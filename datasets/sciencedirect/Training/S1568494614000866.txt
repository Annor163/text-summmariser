@&#MAIN-TITLE@&#
Stochastic feature compensation methods for speaker verification in noisy environments

@&#HIGHLIGHTS@&#
Stochastic feature compensation methods are explored for robust speaker verification.Front-end denoising using Gaussian Mixture Models (GMMs) built from stereo training data.Feature compensation based on joint probability models (GMMs) of noisy and clean features outperform individual model based compensation.Feature compensation using GMM-based cepstral trajectory mapping shows the overall best performance.

@&#KEYPHRASES@&#
Speaker verification,Noisy environment,Minimum mean squared error,Maximum likelihood estimate,Expectation Maximization algorithm,Gaussian Mixture Models,

@&#ABSTRACT@&#
This paper explores the significance of stereo-based stochastic feature compensation (SFC) methods for robust speaker verification (SV) in mismatched training and test environments. Gaussian Mixture Model (GMM)-based SFC methods developed in past has been solely restricted for speech recognition tasks. Application of these algorithms in a SV framework for background noise compensation is proposed in this paper. A priori knowledge about the test environment and availability of stereo training data is assumed. During the training phase, Mel frequency cepstral coefficient (MFCC) features extracted from a speaker's noisy and clean speech utterance (stereo data) are used to build front end GMMs. During the evaluation phase, noisy test utterances are transformed on the basis of a minimum mean squared error (MMSE) or maximum likelihood (MLE) estimate, using the target speaker GMMs. Experiments conducted on the NIST-2003-SRE database with clean speech utterances artificially degraded with different types of additive noises reveal that the proposed SV systems strictly outperform baseline SV systems in mismatched conditions across all noisy background environments.

@&#INTRODUCTION@&#
Speaker verification (SV) is the process of validating the claimed identity of an individual using his or her speech. The task is achieved by classifying a claimant's utterance as true (authentic) or false (impostor) based on its statistical similarities with an enrolled (claimed) speaker's utterance. Acoustic modeling is used in the training stage of SV to effectively capture the distribution of features unique to an enrolled speaker. In the standard Gaussian Mixture Model (GMM)-based SV systems [1,2], acoustic speaker models are GMMs obtained by Maximum a Posteriori (MAP) adaptation [3] of a Universal Background Model (UBM) [2]. During evaluation, given a test speech segment, the log-likelihood ratio of scores obtained from the MAP-adapted GMMs and UBM is compared with an empirically determined threshold for final classification decision. Though the simple GMM-based SV systems perform quite well for clean speech, the performance is severely degraded in the presence of environmental noise [4]. A major challenge in the field of speaker recognition is to make the SV system robust towards its acoustic environment. Apart from channel distortions, additive background noise has been identified as a prominent factor for degraded SV performance [4]. The loss of performance accuracy can be mainly attributed to the mismatch occurring due to the differences in training and recognition environment. Since accurate estimation of noise is infeasible in nature, traditional approaches aim to compensate for environmental noise. Noise compensation can be broadly categorized in two domains i.e., acoustic model level and feature level.Acoustic model adaptation techniques alter the statistical model parameters learned during the training/enrollment phase to reflect the acoustic environment of testing/recognition phase [5]. Popular data-driven model adaptation techniques like Maximum aPosteriori (MAP) [3] and Maximum Likelihood Linear Regression (MLLR) [6] use various amounts of adaptation data to achieve this task. State-of-the-art model compensation techniques like Parallel Model Compensation (PMC) [7] and Vector Taylor Series (VTS) [8] use an analytical relationship of the clean and noisy environment. These methods exploit prior knowledge about the test environment in the form of a statistical model of the noise or reliable estimates of the noise distribution. The model adaptation techniques are usually superior to their feature-level counterparts because they can appropriately capture the uncertainty caused by noise statistics [9]. However, besides depending on available clean speaker models, these methods are computationally intensive and often require high amount of training data [10].Feature compensation techniques map feature vectors extracted during the recognition phase to reflect the acoustic environment of the training/enrollment phase. The wide range of methods explored in this domain can be viewed in three groups. The first group of methods include high-pass filtering techniques like Cepstal Mean Subtraction (CMS) [11,12] and RelAtive SpecTral Amplitude (RASTA) [13]. Despite limited performance improvement, these techniques along with various feature transformation methods like feature warping [14] and nonlinear spectral magnitude normalization [15] find generic application in most speech related tasks. The second group of feature compensation techniques are noise model-based. They assume prior knowledge of the noise spectrum. An estimate of the clean speech parameters is made using either a noise model or representation of the effects of noise in speech. The parameters of the noise model are estimated and applied to the appropriate inverse operation to compensate the recognition signal. Examples include Spectral Subtraction (SS) [16], Codeword Dependent Cepstral Normalization (CDCN) [17], Kalman Filtering [18], and feature-level VTS [19].The third group of feature compensation techniques are entirely data-driven and are stochastic in nature. They are ‘blind’ towards the nature of the corrupting process and are based on empirical compensation methods that use direct spectral comparison. Prior work shows that they often outperform the previous two approaches for feature enhancement [20]. During the training phase, some transformations are estimated by computing the frame-by-frame differences between the vectors representing speech in the clean and noisy environments (stereo data). The differences between clean and noisy feature vectors are modeled by training additive bias vectors on the mean and covariance of either of the two (clean or noisy) probability distributions. During the evaluation phase, the bias vectors are used to transform noisy test feature vectors to their clean feature equivalent based on a minimum mean squared error (MMSE) estimate. Earlier methods like CDCN [17,21] used vector quantization (VQ) codebooks to represent the distribution of clean feature vectors. Due to their quantization-based framework, these algorithms were unable to learn the variance of a distribution and were later replaced by the more flexible Gaussian Mixture Model(GMM)-based normalization techniques e.g., multivaRiate GAussian-based cepsTral normaliZation (RATZ) [22]. Although the RATZ family of algorithms approximated the normalized features, the posterior probability of clean GMM components with respect to the noisy test feature vectors were usually distorted causing poor MMSE estimates. To suppress these distortions, the Stereo-based Piecewise LInear CompEnsation for Environments (SPLICE) algorithm [23] modeled the noisy feature space using GMMs instead. This produced significantly better result in robust speech recognition tasks compared to its predecessors [24]. The effectiveness of SPLICE framework has since then encouraged it's extended applications e.g., speech recognition in non-stationary noisy environments within cars using the Multi Environment Model-based LInear Normalization (MEMLIN) algorithm [25] and word recognition using Noise Adaptive Training [24]. The more recently proposed Stereo-based Stochastic Mapping (SSM) [26] is principally a more accurate version of SPLICE based on joint probability modeling of the noisy and clean feature spaces using GMMs.In the intermediate stages of the MMSE estimation, algorithms like RATZ, SPLICE, MMCN rely on approximations of the conditional distribution of clean and noisy features since its closed form solution is hard to estimate. Though SSM overcomes this limitation by deriving an exact conditional distribution from the joint probability model, a fundamental drawback still exists. Individual frames of an utterance are treated independent of each other during feature transformation. This often resulted in inappropriate dynamic characteristics. Addressing this problem, feature enhancement based on mapping sequence of frames (cepstral trajectory) was proposed in [27]. The motivation was based on successful applications of GMM-based trajectory mapping techniques for voice conversion [28].The family of stochastic feature compensation algorithms till date remains a preferable choice for robust speech recognition tasks due to their relatively lower computational cost and reasonably good performance. An added advantage is their independence of any structural assumption about the nature of noise degradation. However, to the best of the authors’ knowledge, the application of these techniques has not been studied extensively for robust SV tasks. In this paper we propose application of standard stochastic feature compensation methods in a SV framework. Through a comparative study of these methods, we highlight their significance for speaker verification in noisy environment.The rest of the paper is organized as follows. Section 2 provides a brief introduction to stochastic feature compensation. Algorithmic descriptions of stereo-based feature compensation techniques used for a comparative study in the present work, are given in Section 3 and Section 4. The experiments conducted are discussed in Section 5, results and discussion in Section 6 followed by a brief summary and conclusion of the work in Section 7.Since accurate enumeration of the environmental effects on speech is a non-trivial task, a simplified version of speech signal degradation based on additive and convolutional channel noise is used in practice. It is assumed that the noise is statistically independent of speech while the convolutive channel distortions are linear time-invariant. In the cepstral domain a noisy mel frequency cepstral coefficient (MFCC) vector ytis represented in terms of MFCC vectors of clean speech xt, additive background noise ntand channel noise htas follows(1)yt=xt+ht+Clog(1+exp(C−1(nt−xt−ht)))where t, C and C−1 are the time frame index, Discrete Cosine Transform (DCT) matrix and the inverse DCT matrix respectively. Due to the random nature of noise, a given clean feature vector can generate different noisy feature vectors, and vice-versa, which causes an uncertainty. Conventionally, Gaussian Mixture Models (GMMs) are used to represent the MFCC distribution. The additive noise in general alters the distribution by reducing the variance of each Gaussian component while the convolutional noise shifts the mean vectors. State-of-the-art model compensation techniques like PMC and VTS use the analytical relation in Eq. (1) and an available GMM of noise to adapt model parameters of noisy speech. In uncontrolled environments where Eq. (1) is not necessarily valid, faulty adapted parameters can be generated.Stochastic feature compensation (SFC) methods are independent of any mathematical structure of noise degradation. They model stereo training data using GMMs. The effect of noise is represented as additive terms to the mean vectors and covariance matrices of the clean speech GMMs. Given a noisy test feature vector yt, a minimum mean squared error (MMSE) criterion is used to estimate a clean vectorxˆtas follows(2)xˆt=E[x|yt]=∫Xxp(x|yt)dxwhere x is a random variable representing clean feature vectors and p(x|yt) is the conditional probability distribution function (pdf) of x given yt. Depending on the nature of the feature compensation algorithm, the two broad approaches of deriving p(x|yt) can be categorized as (i) independent probability modeling and (ii) joint probability modeling. The independent probability modeling methods construct individual GMMs for clean and noisy data. The effect of noise is represented as additive terms to the mean vectors and covariance matrices of the GMMs. The conditional pdf is derived based on numerical approximations using the additive terms. Alternatively, joint probability models construct a single GMM using stacked noisy and clean feature vectors of the stereo data. This is followed by deriving an exact conditional pdf and estimation of clean speech vectors. In the subsequent sections we briefly discuss five standard stochastic feature compensation techniques that are used for robust speech recognition tasks. Each of the methods discussed in the following subsections differ in the way by which they estimatextˆand derive p(x|yt).Fig. 1illustrates the independent probability model based SFC process. The main steps of the process can be outlined as follows1.Firstly individual GMMs are built for clean vectors Xtand noisy vectors Ytas follows(3)p(xt)=∑j=1Mwx(j)Nx(xt;μx(j),Σx(j))(4)p(yt)=∑j=1Mwy(j)Ny(yt;μy(j),Σy(j))wherew(j), μ(j) and Σ(j) denotes the weight, mean vector and covariance matrix of the jth Gaussian component and M is the total number of components.The conditional pdf p(x|j, yt) is then approximated by means of additive factors rjto the clean or noisy training vectors. The values of the additive terms are determined my maximizing the likelihood of the training data.Given a set of noisy test vectors, the equivalent set of clean vectors are estimated by MMSE.In the following subsections three standard independent probability model based SFC techniques used for robust speech recognition tasks, are discussed briefly.The RATZ algorithm [22], derives the required MMSE clean feature estimate in three stages. In the first stage, the clean feature vectors are used to train a GMM as in Eq. (3) using the standard Expectation Maximization (EM) algorithm [29]. The second stage consists of estimating the statistics of the noise-degraded speech by applying appropriate correction vectors to the mean and covariance matrices of the clean speech pdf. The additive correction vectors, which model environmental effects, are in turn estimated by maximizing the likelihood of the noisy feature vectors. Finally, given a noisy test feature vector, a MMSE estimate of clean speech is made using the correction vectors learned during the training phase. Given a sequence of T noisy MFCC vectors Y=[y1, y2, …yT], the log-likelihood is given by(5)L(Y)=log∏t=1Tp(yt)=∑t=1Tlog∑j=1Mwy(j)Ny(yt;μy(j),Σy(j))=∑t=1Tlog∑j=1Mwy(j)Ny(yt;μx(j)+rj,Σx(j)+Rj)where rjand Rjare the correction vectors for the jth Gaussian component of the clean speech pdf. The complete set of unknown bias vectors is iteratively estimated by maximizing L using an EM algorithm (outlined in Appendix A). The maximum likelihood estimates are given by the following equations(6)rˆj=∑t=1Tp(sy(j)|yt,ϕ)(yt−μx(j))∑t=1Tp(sy(j)|yt,ϕ)(7)Rˆj=∑t=1Tp(sy(j)|yt,ϕ){(yt−μx(j)−rˆj)(yt−μx(j)−rˆj)T−Σx(j)}∑t=1Tp(sy(j)|yt,ϕ)where p(sy(j)|yt, ϕ) is the posterior probability of the latent noisy GMM component sy(j) given yt, ϕ={rj, Rj} is the set of model parameters and T denotes matrix transpose. It was studied by Moreno et al. [22] that in case of stereo recordings, a one-one correspondence of the each Gaussian component of the noisy speech GMM and clean speech GMM can be established. This is done by assuming posterior invariance which states that the posterior probabilities of each GMM component with respect to a clean vector and its noisy equivalent vector are equal. This assumption, although less reliable in low SNR conditions suggest that each Gaussian undergoes the same shift and negligible compression. It gives a convenient approximation of p(sy(j)|yt, ϕ) as follows(8)p(sy(j)|yt,ϕ)=p(sy(j))p(yt|sy(j),ϕ)∑k=1Mp(sy(k))p(yt|sy(k),ϕ)=p(sx(j))p(xt|sx(j))∑k=1Mp(sx(k))p(xt|sx(k))=wx(j)Nx(xt;μx(j),Σx(j))∑j=1Mwx(j)Nx(xt;μx(j),Σx(j))Given the above relation, Eqs. (6) and (7) can now be approximated as(9)rˆj=∑t=1Tp(sx(j)|xt)(yt−xt)∑t=1Tp(sx(j)|xt)(10)Rˆj=∑t=1Tp(sx(j)|xt){(yt−xt−rˆj)(yt−xt−rˆj)T−Σx(j)}∑t=1Tp(sx(j)|xt)Since the above equations do not have ϕ in the right hand side, the solutions are non-iterative. The environmental effects on clean speech x in MFCC domain are modeled as additive linear correction vectors r(x). The MMSE estimate for clean speechxˆtgiven a noisy test vector ytis calculated by Eq. (2). The conditional pdf is solved using numerical approximation as follows(11)xˆt=E[x|yt]=∫Xxp(x|yt)dx=∫X(yt−r(x))p(x|yt)dx=∫Xytp(x|yt)dx−∫Xr(x)p(x|yt)dx=yt∫Xp(x|yt)dx−∫X∑j=1Mr(x)p(x,j|yt)dx=yt−∑j=1Mp(j|yt)∫Xr(x)p(x|j,yt)dx=yt−∑j=1Mp(j|yt)rj∫Xp(x|j,yt)dx=yt−∑j=1Mp(j|yt)rjIn the 6th step it is assumed that r(x) remains constant over the integral and can be approximated by rj.The effectiveness of the RATZ algorithm depends on the posterior invariance assumption made in Eq. (8). However in low SNR conditions this assumption becomes unrealistic since the Gaussian pdfs of noisy speech are compressed in different amounts due to changes in its variance. As an alternative, the SPLICE algorithm proposed in [24] models the noisy feature space as given in(12)p(yt)=∑j=1Mp(j)p(yt|j)where p(j) is the apriori probability of the Gaussian component j mathematically equivalent to the component weightwy(j)and p(y|j) is the multivariate GaussianNy(yt;μy(j),Σy(j))as given in Eq. (4). A distinct advantage of SPLICE compared to other model-based feature enhancement techniques like Spectral Subtraction, is its independence of any stationarity assumption. Feature compensation using SPLICE is based on a two simple assumptions. Firstly, a clean MFCC vector xtgenerated by each discrete Gaussian component j can be approximated in terms of its noisy counterpart yt. This is often termed as piece-wise linear approximation. Secondly the conditional pdf of clean speech vectors given the noisy speech vectors and Gaussian component j is also a multivariate Gaussian distribution. The mean of the resultant distribution is assumed to shifted by the corrective vector rjas follows(13)p(x|j,yt)=Ny(x;yt+rj,Γj)Estimation of the parameters rjand Γjare based on maximum likelihood training similar to that of RATZ (Eq. (5)) using an EM algorithm (outlined in Appendix A). The solutions are given by(14)rˆj=∑t=1Tp(j|yt)(xt−yt)∑t=1Tp(j|yt)(15)Γj=∑t=1Tp(j|yt){(xt−yt)(xt−yt)T−rˆjrˆjT}∑t=1Tp(j|yt)where p(j|yt) is the posterior probability of component j given yt(16)p(j|yt)=p(j)p(yt|j)∑j=1Mp(j)p(yt|j)For stereo training data, the solution of Eqs. (14) and (15) are non-iterative. The MMSE estimate for clean speech from the noisy speech pdf is then given by(17)xˆt=E[x|yt]=∫Xxp(x|yt)dx=∫X∑j=1Mxp(j|yt)p(x|j,yt)dx=∑j=1Mp(j|yt)∫Xxp(x|j,yt)dx=∑j=1Mp(j|yt)(yt+rj)=∑j=1Mp(j|yt)yt+∑j=1Mp(j|yt)rj=yt+∑j=1Mp(j|yt)rjThe approximation of the mean of the conditional pdf in Eq. (13) using additive terms rjis often considered to be a limitation of the SPLICE framework. An accurate estimation of the conditional mean would require joint probability modeling of the clean and noisy vectors followed by estimating MLLR-type transforms [30]. Despite these drawbacks, SPLICE is commonly applied for pre-processing feature vectors in robust speech recognition tasks.The previous techniques discussed so far either models the clean feature space (e.g., RATZ) or the noisy feature space (e.g., SPLICE) using GMMs. A corrective bias vector for each GMM component is trained by weighing the difference between clean and noisy feature vector pairs with normalized posterior probabilities. However, in realistic situations when there are multiple types of environment in the noisy space, estimates based on single GMM posteriors might be erroneous. The MEMLIN algorithm [25] aims to enhance modeling accuracy by modeling both noisy and clean spaces in parallel. The noisy feature space is divided into several basic environments and modeled with individual GMM.(18)pe(yt)=∑sye=1Mp(yt|sye)p(sye)wheresyedenotes the latent Gaussian component for the noisy GMM trained in environment indexed by e,pe(yt|sye)andpe(sye)denote the Gaussian pdf for thesyeth component and its apriori probability respectively, as shown below(19)p(yt|sy)=N(yt;μ(sye),Σ(sye))(20)p(sye)=wyeThe clean feature space is modeled by a single GMM and has a similar structure as that of Eq. (3).(21)p(xt)=∑sx=1Mp(xt|sx)p(sx)The objective is to learn the difference between clean and noisy feature vectors associated with a pair of Gaussians (one for a clean model, another for a noisy model), for each basic environment. The bias vector transformations are computed independently for each basic environment. Alike SPLICE, MEMLIN assumes that each clean feature vector xtis approximated by a linear function of the noisy feature vector ytand an additive bias vectorr(sx,sye). However unlike SPLICE, the additive vectors are now a function of both clean and noisy GMM components for a particular environment. The second assumption approximates the conditional pdf of x given ytas a multivariate Gaussian with covariance matrixΣ(sx,sye)and mean given by a linear transformation of the environment-dependent noisy vector, as follows(22)p(x|yt,sye,sx)=N(x|yt−∑ep(e|yt)r(sx,sye),Σ(sx,sye))where p(e|yt) andr(sx,sye)are the posterior probability of environment e given ytand the additive bias vector respectively. The estimation of these factors are discussed briefly. The term p(e|yt) is trained recursively as follows(23)p(e|yt)=βp(e|yt−1)+(1−β)pe(yt−1)∑epe(yt−1)where (0≤β≤1) is a constant and p(e|y0) is uniform across all environments. Ther(sx,sye)factor is obtained by maximizing the likelihood of noisy feature vector with respect tor(sx,sye), using the standard EM algorithm. Given the stereo training data for environment e which comprises the noisy vectorsYe={yte}te=1Teand clean vectorsXe={xte}te=1Te, the complete data log-likelihood of Yeis given by the following equation(24)L(Ye)=∑te=1Telog∑sye=1Mp(sye)Ny(yte;μ(sye)+r(sx,sye),Σ(sx,sye))Maximizing the above equation with respect tor(sx,sye)gives(25)r(sx,sye)=∑te=1Tep(sx|xte)p(sye|yte)(yte−xte)∑te=1Tep(sx|xte)p(sye|yte)wherep(sx|xte)the posterior probability of Gaussian sxwith respect to clean vector xt. Similarlyp(sye|yte)is the posterior probability of Gaussiansyewith respect to noisy vector yt. These can be easily calculated using Eqs. (21) and (18), respectively as follows(26)p(sx|xte)=p(xte|sx)p(sx)∑sx=1Mp(xte|sx)p(sx)(27)p(sye|yte)=p(yt|sye)p(sye)∑sye=1Mp(yt|sye)p(sye)The resultant MMSE estimatexˆtis computed as a weighted sum of all of the basic environment bias vector transformations.(28)xˆt=∫xxp(x|yt)dx=∫∑e∑sye=1M∑sx=1Mxp(x|yt,sye,sx,e)dx=yt−∫∑e∑sye=1M∑sx=1Mr(sx,sye)p(x|yt,sye,sx,e)dx=yt−∑e∑sye=1M∑sx=1Mr(sx,sye)p(e|yt)p(sye|yt)p(sx|sye,yt,e)The above equation introduces a new factorp(sx|sye,yt,e)known as cross probability model. It compensates for the mismatch that occurs when the Gaussian component sxassociated with clean vector xtis different from the Gaussian componentsyeassociated with corresponding noisy vector yt. For simplicity the time dependency with ytis omitted and the resultant factorp(sx|sye,e)is estimated using relative frequency of occurrence. It is calculated as the ratio of the number of times the most probable pair of decoded Gaussians are{sx,sye}and the number of timessyeis decoded singly. The resultant form is as follows(29)p(sx|sye,e)=∑te=1Tep(sx|xte)p(sye|yte)p(sx)p(sye)∑te=1Te∑sx=1Mp(sx|xte)p(sye|yte)p(sx)p(sye)It can be easily verified that in case of single environment, the variable e can be omitted which simplifies most of the above Equations. In such case the factor p(e|yt) can also be entirely ignored. The single environment version of MEMLIN is often termed as Multivariate Model based Cepstral Normalization (MMCN). More recently proposed sophisticated versions of MEMLIN like Phoneme-Dependent MEMLIN (PD-MEMLIN) [31] and Class-Dependent MEMLIN (CD-MEMLIN) [32] extend the same framework in modeling individual phonemes or non-silence regions of a speech utterance.The only apparent drawback of the independent probability model based SFC methods is the determination of the additive terms which may turn out be inaccurate in degraded environmental conditions. Alternatively, joint probability modeling can be used for feature compensation provided sufficient training data is available.Fig. 2illustrates the joint probability model based SFC process. The main steps of the process can be outlined as follows1.The noisy and clean training vectors are concatenated to produce joint vectors (Z)The joint vectors are modeled using a single GMM which represents the joint pdfThe conditional pdf is derived using parameters of the joint pdf.Given a noisy test vectorYt′, a clean vectorXt′is obtained based on MMSE or maximum likelihood estimate (MLE).Two standard joint probability model based SFC methods are discussed in the following sectionsThe main idea of the SSM algorithm [26] is to estimate the joint probability distribution of noisy and clean feature spaces instead of modeling them independently. This eliminates the need for training the hypothesized additive bias term ‘r’ for each GMM component as employed by previous methods like SPLICE or MEMLIN. Unlike previous methods, concatenated pair of noisy and clean feature vectors are used as training data for GMM building. The desired transformation parameters are derived from the joint probability model (GMM) during the training phase. The improvement in performance accuracy is associated with a demand of larger amount of training data for estimating the model parameters in a higher dimensional space. The clean speech estimated during evaluation phase (xˆ), can be derived iteratively using MAP estimation or non-iteratively using the MMSE criterion. In this discussion we restrict ourselves to the MMSE version of the SSM for the ease of comparison with earlier methods. The details of the algorithm are described in the remaining part of this section.As usual we consider a pair of d dimensional clean and noisy feature vector xtand ytrespectively. A joint vector ztof dimension 2d is defined aszt=[ytT,xtT]T. The joint vector is modeled using a GMM λ(z) as follows(30)p(zt)=∑j=1Mwz(j)N(zt;μz(j),Σz(j))where(31)μz(j)=μy(j)μx(j),Σz(j)=Σyy(j)Σyx(j)Σxy(j)Σxx(j)This model is similar to those defined in Eqs. (3) and (4). The mean vector μz(j) for component j is now a concatenation of individual mean vectors μy(j) and μx(j). The composition of the covariance matrix Σz(j) can be similarly related. Σyy(j) and Σxx(j) are the covariance matrices for the jth component of the noisy and clean GMMs respectively. Apart from these, Σyx(j) and Σxy(j) denote the cross-covariance matrices of y and x for the jth GMM component. The GMM is trained with the standard EM algorithm using the joint vectors z. The training stage essentially comprises deriving the model parameters by partitioning the matrices μz(j) and Σz(j) as shown above. During the evaluation stage, the partitioned parameters are used to formulate the conditional pdf p(xt|yt) required for the MMSE-based prediction ofxˆtas defined in Eq. (2). Unlike previous methods, mathematical derivations show that without any assumption the conditional pdf is another GMM where the mixture weights are posterior probabilities of each Gaussian component with respect to y[29].(32)p(xt|yt,λ(z))=∑j=1Mp(j|yt,λ(Z))p(xt|yt,j,λ(z))where(33)p(j|yt,λ(z))=wy(j)N(yt;μy(j),Σyy(j))∑j=1Mwy(j)N(yt;μy(j),Σyy(j))(34)p(xt|yt,j,λ(Z))=N(xt;Ex(j,t),Dx(j))The mean vector Ex(j, t) and covariance matrix Dx(j) of the jth Gaussian in the conditional pdf are defined as(35)Ex(j,t)=μx(j)+Σxy(j)Σyy(j)−1(yt−μy(j))(36)Dx(j)=Σxx(j)−Σxy(j)Σyy(j)−1Σyx(j)Given a noisy test vector yt, its equivalent clean estimatextˆcan be then derived by the MMSE predictor as follows(37)xtˆ=E[xt|yt]=∫Xxtp(xt|yt,λ(z))dxt=∫X∑j=1Mxtp(j|yt,λ(Z))p(xt|yt,j,λ(z))dxt=∑j=1Mp(j|yt,λ(Z))Ex(j,t)The principle of SSM is similar to SPLICE except for the joint probability distribution of noisy and clean feature spaces. In fact SPLICE with MMSE predictor reduces to its SSM counterpart if the cross-correlation of clean and noisy data is taken into account. SSM bears close resemblance to other model-based non-linear transformation methods like Constrained MLLR [33]. However the difference lies in the fact that the transformations in SSM are learned offline during the training phase while those in case of CMLLR, are done online during evaluation. A comparative study of SSM and other contemporary feature compensation methods can be found in [26].The MMSE estimator of SSM as discussed in Section 4.1 is a mixture of linear transforms weighted by the posterior probability of each GMM component. The parameters for the linear transform are derived from the joint distribution of both spaces. The approach is similar to any conventional GMM-based mapping techniques which has diverse applications [34]. However, a distinct drawback of such frame-wise mapping frameworks is that they fail to capture the correlation of features in the entire sequence. This results in inappropriate dynamic characteristics and an excessively smoothed spectra. The cepstral trajectory based GMM mapping (TRAJMAP) algorithm [28,27] addressed this drawback by applying a Hidden Markov Model (HMM)-based parameter generation algorithm [35] with dynamic features, to the GMM-based mapping framework. Instead of individual frame-wise mapping, an entire sequence of frames (cepstral trajectory) is transformed in parallel. This approach had shown promising results for both noise-compensation [27] and voice conversion applications [28], in past. In this section we discuss the mathematical details of the TRAJMAP transformation framework as used in [28]. A fundamental assumption is that despite noise corruption underlying spectral properties of a speaker remain preserved. The algorithm is used to learn a mapping function from a sequence of vectors in a speaker's noisy utterance to the corresponding sequence of clean vectors in the stereo training data.The cepstral vector trajectory is represented by a sequence of clean MFCC vectors X and noisy MFCC vectors Y where X and Y together constitute the stereo training data.(38)X=[X1T,X2T,…,XTT]T(39)Y=[Y1T,Y2T,…,YTT]Twhere T denotes the total number of vectors in the sequence. Individual vectors of each sequence are a concatenation of the static MFCC, its delta and acceleration coefficients. Each vector in the above sequence are 3d dimensional considering static MFCC vectors of d dimension,(40)Xt=[xtT,ΔxtT,Δ2xtT]T(41)Yt=[y1T,ΔytT,Δ2ytT]TThe GMM λ(Z) of the joint pdf p(Zt|λ(Z)) is trained by a concatenated pair of clean and noisy vector (Zt) from the stereo training data whereZt=[YtT,XtT]T. The aim to map the noisy MFCC trajectory Y to its clean counterpart X by maximizing the following likelihood function(42)p(X|Y,λ(Z))=∑jp(j|Y,λ(Z))p(X|Y,j,λ(Z))=∏t=1T∑j=1Mp(j|Yt,λ(Z))p(Xt|Yt,j,λ(Z))where j={j1, j2, …, jT} is a mixture component sequence. The conditional pdf at each frame is modeled as a GMM. At frame t, the jth mixture component weight p(j|Yt, λ(Z)) and the jth conditional probability distribution p(Xt|Yt, j, λ(Z)) are given by the following expressions(43)p(j|Yt,λ(Z))=wjYN(Yt;μjY,ΣjYY)∑j=1MwjYN(Yt;μjY,ΣjYY)(44)p(Xt|Yt,j,λ(Z))=N(Xt;Ej,tX,DjX)where(45)Ej,tX=μjX+ΣjXY(ΣjYY)−1(Yt−μjY)(46)DjX=ΣjXX−ΣjXY(ΣjYY)−1ΣjYXIn Eqs. (45) and (46) we use similar notations for the conditional mean and conditional covariance matrix as discussed in Section 4.1.The task is to estimate a sequence of clean vectorsXˆfrom the entire sequence of noisy feature vectors Y. This is achieved in two stages. In the first stage, a HMM parameter generation algorithm [35] is used to convert Y to the static MFCC parametersxˆ. In the next stage, the delta and acceleration coefficients are derived from each static MFCC vector ofxˆand concatenated with itself to obtain the resultant sequenceXˆ. In contrast to the MMSE-based methods, the derivation ofxˆis based on a maximum likelihood estimate (MLE) as follows(47)xˆ=argmaxp(X|Y,λ(Z))wherexˆ=[xˆ1T,xˆ2T,…,xˆTT]is the sequence of estimated static feature vectors. A matrix W of dimension 3dT×dT is defined such that it converts the static sequencexˆto the expanded sequenceXˆas follows(48)Xˆ=WxˆwhereXˆis the sequence of denoised MFCC vectors with dynamic (delta and acceleration) coefficients as defined in Eqs. (39) and (41). The composition of the matrix W is discussed as follows(49)W=[W1,W2,…Wt,…,WT]T⊗IDXD(50)Wt=[wt(0),wt(1),wt(2)]t=1,2,…,T(51)wt(n)=[01st,…,0,w(n)(−L−(n))(t−L−(n))th,…,w(n)(L+(n))(t+L+(n))th,…,w(n)(0)(t)th,…,0T−th]Tn=0,1,2In Eq. (49), each submatrix Wtis of size T×3 and ‘⊗’ denotes the Kronecker product. In Eq. (51),w(n)(τ)denotes the weights required for calculating the ΔnMFCC coefficient for the (t+τ)th time frame. τ varies in a frame span of[−L−(n),L+(n)]as shown in the following equations (L+(0)=L−(0)=0andw(0)(0)=1)(52)Δxt=∑τ=−L−(1)L+(1)w(1)(τ)xt+τ(53)Δ2xt=∑τ=−L−(2)L+(2)w(2)(τ)xt+τThe maximum likelihood estimate in Eq. (47) is solved by an EM algorithm which iteratively maximizes an auxiliary function with respect toxˆas follows(54)Q(X,Xˆ)=∑jp(j|Y,X,λ(Z))log(p(Xˆ,j|Y,λ(Z)))The sequence of vectorxˆobtained as a solution of Eq. (54) is given by(55)xˆ=(WT(DX)−1¯W)−1WT(DX)−1EX¯where(56)(DX)−1¯=diag[(D1X)−1¯,(D2X)−1¯,…,(DtX)−1¯,…,(DTX)−1¯](57)(DX)−1EX¯=[(D1X)−1E1X¯T,(D2X)−1E2X¯T,…,(DtX)−1EtX¯T,…,(DTX)−1ETX¯T]T(DX)−1¯in Eq. (56) is a block diagonal matrix of size 3dT×3dT while(DX)−1EX¯in Eq. (57) is a vector of size 3dT×1. The individual constituents of the matrices i.e.,(DtX)−1¯and(DtX)−1EtX¯are given by(58)(DtX)−1¯=∑j=1Mλj,t(DjX)−1(59)(DtX)−1EtX¯=∑j=1Mλj,t(DjX)−1Ej,tX(60)λj,t=p(j|Yt,Xt,λ(Z))The solution given by Eq. (55) is only a sequence of static MFCC vectors i.e., a vector of size dT×1. The full sequence with delta and acceleration coefficients appended with the resultant vector can be obtained by a simple linear operationWxˆ. Detailed derivation of Eq. (55) is provided in Appendix A.All experiments are carried out in the NIST-2003-SRE database [36]. The data consists of conversational speech collected from 356 speakers (149 male and 207 female) over a cellular phone network. Each speech file is sampled at 8kHz with a bit resolution of 16 bits/sample. The training set contains approximately 2min of one-sided conversational speech from each enrolled speaker. The test set contains around 3500 speech segments of approximately 15s each. The primary task is performed, where each test segment is evaluated against 11 speaker models out of which one may be a true trial and the rest are false trials.The purpose of our study is to address the issue of speaker verification in mismatched condition where a speaker enrolls in a clean environment whereas during verification his/her speech is corrupted by background noise. However stereo-data based techniques as described in Section 2 require simultaneous recording of a speaker's training data over two channels i.e., one in clean condition and the other in a noisy environment. Due to unavailability of such data, the noisy utterances in our study are simulated by corrupting the clean speech utterances of the NIST-SRE-2003 by different types of additive noises. The approach is motivated by synthetic generation of stereo-data as described in [37]. We use the standard GMM-UBM framework for speaker verification [2]. Fig. 3shows the block diagram of the feature compensation process in a GMM-UBM based speaker verification system. The various stages of the SV system are discussed in the following sections.Four additive noises (i.e., car, factory, pink and white) collected from the NOISEX-92 database [38] have been used for representing unique background environments. The speech segment from each of the 356 enrolled speakers was degraded by adding a specific type of noise at 0dB and 5dB SNRs, respectively. The noise level was scaled to maintain the desired SNRs of the reconstructed speech segments. 8 different sets of noisy training utterances were obtained (one for each noise at a particular SNR). The default training set of the NIST-SRE-2003 was used as the clean recordings.Each test utterance was similarly reconstructed by noise addition at the two SNRs. Each set of noisy training utterances was used for the following sets of experiments.1.Mismatched condition: The noisy test utterances were evaluated against speaker models built from clean training data.Matched condition: The noisy test utterances were evaluated against speaker models built from noisy training data.Feature compensated: The noisy test utterances were subjected to feature enhancement prior to evaluation against clean speaker models. Each of the four feature compensation techniques discussed in Section 2 were compared with the above two conditions and the proposed method, on the basis of their performance.The simulated stereo training data was used for front-end GMM training as discussed later in Section 5.5. For comparing relative improvements in performance accuracy produced by the various feature compensation schemes, the SV systems under mismatched conditions have been considered as a baseline.The speech signals were pre-emphasized using a first order high pass filter of coefficient 0.97. The resultant signals were processed using short-term frames of 20ms with a frame-overlap of 10ms. We used a simple energy-based thresholding scheme for voiced activity detection (VAD) as described in [39]. For the simulated stereo data used in our work, the VAD was applied in two stages. The average energy of an entire clean speech utterance (Eavg) was calculated and a threshold of 1% of the average energy (0.01×Eavg) was empirically determined. All clean frames with an average energy below the selected threshold were discarded. The frame indices of the discarded frames were recorded for later use. After noise contamination of the clean utterance as described in Section 5.1, the noisy frames corresponding to recorded indices were discarded without any further thresholding.We used the standard MFCC coefficients as features. A 26 channel mel-scaled triangular filterbank constrained in the telephonic bandwidth of 300–3400Hz, was imposed on the DFT magnitude spectra of each frame. 13 cepstral coefficients (C1−C13) excluding the zeroth one (C0), were extracted after Discrete Cosine Transform of the log filterbank energies. The delta and acceleration coefficients computed over a frame span of 2, were appended to form a 39-dimensional feature vector. All feature vectors were subjected to cepstral mean subtraction followed by cepstral variance normalization. The resultant distribution was scaled to zero mean and unit variance.In the remaining part of the paper, we shall refer the MFCC feature vectors extracted from the noisy training data and its clean counterpart as ‘noisy vectors’ and ‘clean vectors’, respectively.Acoustic modeling using the standard GMM-UBM framework was performed in two stages i.e., construction of a Universal Background Model (UBM) and the target speaker models.The SwitchBoard corpus (part II) was used for UBM construction. The data consisting of 20h of conversational speech from 100 male and 100 female speakers (6min from each speaker) was subjected to preprocessing and feature extraction. A 1024-component GMM (UBM) was trained offline using 100 iterations of the EM algorithm.The target speaker models (GMMs) were derived individually, by MAP adaptation of the UBM using each enrolled speaker's training data. The process was repeated twice, once each for the clean and noise-degraded speech of the stereo training data. Model parameters adapted to residual channel noises present in the original NIST training data offers extra robustness to the SV systems. The clean speaker models were used for evaluation in the mismatched condition as well as the feature compensated conditions.The three basic stages of implementing the feature compensation process are discussed below.•Front-end GMM training: The stereo training data corresponding to each speaker was used for building speaker-specific front-end GMMs prior to feature enhancement. For RATZ, SPLICE and MMCN, a pair of 8-component GMMs (clean and noisy) with diagonal covariance matrices were constructed for each speaker using the standard EM algorithm.For SSM and TRAJMAP, individual pairs of noisy and clean MFCC vectors in the aligned sequence were first concatenated to create a single sequence of 78-dimensional MFCC vectors. The joint vectors were used to build speaker specific 8-component GMMs with full covariance matrices. The number of GMM components was empirically determined according to the available training data. However in practical applications without training data constraints, higher number of components can be explored. The conditional GMM parameters required for SSM and TRAJMAP were derived using Eqs. (35), (36) and Eqs. (45), (46) respectively.Feature enhancement: Each noisy test feature vector was transformed using the front-end GMM parameters of each of the 11 target speaker models specified for the evaluation phase of the NIST primary task. In contrast to the actual evaluation process, each of the 11 transformed vectors were scored against the corresponding speaker model and the UBM.The corrective bias vectors of the mean and covariance terms for RATZ were estimated using Eqs. (9) and (10) non-iteratively. This was followed by the MMSE predicted value given by Eq. 11. Only the noisy front-end GMMs trained as in Eq. (4) were used to estimate the bias vectors for SPLICE as given by Eqs. (14) and (15). This was followed by the MMSE estimate given by Eq. (17).We used the simplified single environment version of MEMLIN i.e., MMCN for feature enhancement. The posterior probability factor for each environment given by Eq. (23) could thus be entirely omitted. The cross probability model (Eq. (29)) and the MMSE predictor (Eq. (28)) were likewise simplified.MMSE estimates for SSM and the MLE estimate for TRAJMAP were calculated using Eqs. (37) and (55), respectively. The static MFCCs obtained from TRAJMAP were concatenated with the delta and acceleration coefficients to yield the resultant 39-dimensional vector.Effectiveness of the proposed feature compensation method is demonstrated by a set of diagrams which highlight some characteristics of the transformed and distorted MFCC features, respectively. Since the lower order MFCC coefficients represent the broad spectral shape, the first MFCC coefficient has been considered without loss of generality. Fig. 4(a) shows the histogram of the first MFCC coefficients of an arbitrary test speech utterance from the NIST-2003-SRE and its equivalent noisy signal obtained by white noise addition at 0dB SNR. Fig. 4(b)–(e) shows the effect of enhancing the noisy utterance by applying various feature compensation algorithms. Since the feature vectors were mean and variance normalized, both the distributions are centered around 0. However the area under the overlapping region of the curves is a measure of accuracy in the conversion process. A fully overlapped curve would suggest the ideal situation of perfect conversion. The distortion caused by noise statistics can be observed in Fig. 4(a) in which the peak of noisy distribution is significantly skewed towards the left. The skewness shows a gradual reduction after the application of feature compensation algorithms. The shape of the transformed distributions i.e., spectral shape, is similarly affected by noise addition. The simple noisy distribution shows arbitrary changes in the spectral shape as seen in several regions of the curve. The change in spectral shape is negligible in case of RATZ with minor differences at the peak region. The change in the noisy distribution shows more prominence in case of SPLICE and MMCN. A spectral smoothening effect can be observed at the peak regions for the SPLICE and MMCN-compensated distributions, respectively with slightly more overlap in case of the former. The SSM and TRAJMAP compensated distributions shows comparatively higher resemblances with the clean distribution. The significant increase in the overlapping area of the histograms is apparent. The changes are also reflected on the spectral shape which shows that the transformed distribution captures minute similarities at the peak region.Fig. 5(a)–(e) shows the scatter plots between the first MFCC coefficients (C1) of the given test utterance (x-axis) and its white noise corrupted equivalent (y-axis). The C1s extracted from non-silence frames of the test utterance are represented by blue dots. The black line represents the ideal condition of perfect feature transformation (x=y). The red line is a first order polynomial of the clean feature vectors which best fits the noisy feature vectors in a least square sense. The imperfections in the transformation process can be inferred from the deviation between the two lines in a figure. The distortion of the cepstral distribution due to the addition of white noise is apparent from Fig. 5(a) in which the two lines are significantly deviated from each other. The spread of the data (blue dots) across the black line is a measure of the covariance of the clean and noisy data. Significant changes in the scatter plots can be observed after the application of the feature compensation algorithms. The increased covariance of data is noticed in case of SPLICE and MMCN where the deviation between the red and black lines is relatively lower compared to RATZ. SSM and TRAJMAP show the best fit in terms of covariance of the given data with the latter showing marginal improvements over the former. Despite outliers most of the data points are considerably aligned along the line of best fit with very little noticeable deviation.

@&#CONCLUSIONS@&#
