@&#MAIN-TITLE@&#
Multi-scale hybrid saliency analysis for region of interest detection in very high resolution remote sensing images

@&#HIGHLIGHTS@&#
Novel multi-scale frequency analysis is proposed for intensity feature analysis.Multi-scale analysis is introduced to extract color and orientation features.Spectral information content is proposed for spectral feature analysis.Propose a novel weight multi-scale feature fusion methodSynthesize frequency and space domain features in proposed algorithm frame.

@&#KEYPHRASES@&#
Computer vision,Remote sensing image processing,Region of interest,Visual attention,Saliency analysis,

@&#ABSTRACT@&#
Researchers have recently been performing region of interest detection in such applications as object recognition, object segmentation, and adaptive coding. In this paper, a novel region of interest detection model that is based on visually salient regions is introduced by utilizing the frequency and space domain features in very high resolution remote sensing images. First, the frequency domain features that are based on a multi-scale spectrum residual algorithm are extracted to yield intensity features. Next, we extract the color and orientation features by generating space dynamic pyramids. Then, spectral features are obtained by analyzing spectral information content. In addition, a multi-scale feature fusion method is proposed to generate a saliency map. Finally, the detected visual saliency regions are described using adaptive threshold segmentation. Compared with existing models, our model eliminates the background information effectively and highlights the salient detected results with well-defined boundaries and shapes. Moreover, an experimental evaluation indicates promising results from our model with respect to the accuracy of detection results.

@&#INTRODUCTION@&#
In recent years, the spatial resolution of remote sensing images has increased greatly. There has been considerably more information in such remote sensing images compared with the previously used low spatial resolution remote sensing image; this change presents a great challenge to the analysis and processing of very high resolution (VHR) remote sensing images [1–4]. Compared with the traditional low spatial resolution remote sensing images, the VHR images contain complicated spatial information, clear details, and well-defined geography objects. Moreover, the structure, edge, and texture information in a VHR remote sensing image is abundant and clear. In summary, the background information in a VHR remote sensing image becomes much more complex. Thus, more efficient information processing technology for VHR remote sensing images is required.Among various applications of remote sensing image processing technology, object recognition is one of the most popular. Usually, the first step toward object recognition is object detection, which aims at extracting an object from its background before recognition. However, before performing the feature analysis, how can a machine vision system extract the saliency regions from an unknown background? Traditional approaches are to convert this problem to the detection of specific categories of objects and to use statistical pattern recognition methods that are based on features such as the spectrum and texture. Most of these methods, called top-down approaches [5,6], require a prior knowledge library that is difficult to build and has a large influence on the detection result, and the expansibility becomes the bottleneck in generalized tasks. Moreover, an essential part of these methods is global searching, which is both time-consuming and storage expensive. Thus, the region of interest detection method based on the bottom-up visual attention model, which has the least reference on statistical knowledge of the object in remote sensing images, should be implemented.Currently, the study of the human visual system (HSV) has become an important trend. Visual attention is one of the primary features of HVS to derive important and compact information from a scene [7–10]. Because the surrounding environment includes an excessive amount of information, the visual attention mechanism enables a reduction in the redundant data. The content that draws human beings' attention has a characteristic called visual saliency, which means to stand out from the surroundings. A point that draws a person's attention is called a focus of attention (FOA), and the region centered by the FOA is called a visually salient region. In recent years, many studies have attempted to build computational models to simulate visual attention, and many bottom-up visual attention models have been proposed [11–16].One of the earliest bottom-up visual attention models was proposed by Itti et al. [6,11,12]. Itti [12] constructed this model by using a biologically plausible architecture, which was proposed by Koch and Ullman [7] and is the basis for several models [13–15]. Itti's model obtains the saliency map based on the intensity, color, and orientation conspicuity maps. These conspicuity maps are attained by the across-scale addition of feature maps, while the feature maps capture the center-surround differences between various Gaussian pyramids and oriented pyramid scales [12]. Because the saliency map is computed over coarser scales, local information loss is unavoidable in this algorithm. This model attempts to simulate the visual attention mechanism based on the HVS biological vision principles, has had a significant influence on the study of the visual attention mechanism, and has been improved since its proposal. Inspired by Itti's method, Frintrop et al. [16] present a method, use integral images to speed up the calculations, and compute center-surround differences with square filters.After the introduction of Itti's model, other approaches, which are purely computational and are not based on biological vision principles, are proposed. Achanta et al. [17] attempt to build visual attention models by accounting for color contrast information. They first obtained the Gaussian-filtered image and then transformed the input images from RGB color space to CIE Lab color space. The CIE Lab color space is used for each image location to form a feature vector, and then, the absolute difference between the Gaussian-blurred image and the arithmetic mean vector is calculated to obtain the saliency map. This method generates full-resolution saliency maps, but it works well only on images that have large and homogeneous objects that have clear boundaries. Hence, because of its dependency on the object size and the uniformity, it has limitations with respect to the applications of remote sensing images.Hou and Zhang [18] attempt to obtain a saliency map for images in the transform domain. They first obtained a down-sampled image with a height or width equal to 64pixels, and then, they performed Fourier Transform (FT) of the down-sampled image to obtain the phase and amplitude spectrums. By analyzing the log-spectrum of the images, they extracted the spectral residuals of the down-sampled image in the spectral domain. The saliency map is derived by applying an inverse FT on an exponential function that combines the spectral residual and phase spectrum information. This method can obtain fast saliency detection, but the saliency map has very low resolution, and much detailed information is unavoidably lost.Perazzi et al. [19] proposed a saliency filter method, which obtained perfect results in natural images. It decomposes a given image into compact, perceptually homogeneous elements that abstract unnecessary detail. Based on this abstraction it computes two measures of contrast that rate the uniqueness and the spatial distribution of these elements. From the element contrast it then derives a saliency measure that produces a pixel-accurate saliency map.Another bottom-up visual saliency model, Graph-Based Visual Saliency (GBVS), was proposed by Harel et al. [20]. This method uses a novel application of ideas from graph theory to concentrate mass on activation maps and to form activation maps from raw features. The saliency map yielded by GBVS also has low resolution, and some spatial information is lost. As an improvement of Harel's model, Sun et al. [21] combined edge-based and Graph-Based Visual Saliency computation methods, and obtain better results in VHR remote sensing images. In addition, to such models, significant advancements have also been made in automatic salient object detection [22–27], and the visual saliency model is also applied to image and video compression [28,29].In these methods, the detection results of the visually salient region have low resolution, poorly defined borders, or are sensitive to the object size and uniformity. The VHR remote sensing images contain complicated spatial information and clear details, and the structure, edge, and texture information is abundant and clear. Although the good performance of these methods has been proved in natural images, their limitations will be magnified in VHR images. Even some approaches have been trying to be applied in remote sensing images, like Sun's model [21]. However, current methods of saliency detection fail to address the complex background information that is found in VHR images.The focus of this paper is on the automatically detected and described region of interest for VHR remote sensing images based on visually salient analysis. Thus, the salient regions in VHR remote sensing images should be detected effectively and described accurately. We introduce a new model that offers two advantages over existing methods: eliminate the background information effectively and highlighting the salient detected results with well-defined boundaries and shapes. In our model, a ROI extraction model for VHR remote sensing images is presented, which allows the input image to be processed along four feature channels and involves saliency analysis that combines spatial and frequency analysis.In this model, the intensity, color, orientation, and spectral features are extracted to compute the saliency map. We first extract the intensity and color information from the input remote sensing images. Next, the frequency domain features based on a multi-scale spectrum residual algorithm are extracted to yield intensity features. We subsequently extract the color and orientation features by generating space dynamic pyramids. Then, spectral features are obtained by analyzing spectral information content. In addition, a multi-scale feature fusion method based on a weighted across-scale combination strategy is proposed to generate the saliency map. Finally, the detected visual saliency regions are described using adaptive threshold segmentation. Experimental evaluation depicts the promising results from our model in the accuracy of the detection results.In this model, the early visual features are extracted from the input image, including the intensity, color and orientation. For the color, red/green, green/red, blue/yellow, and yellow/blue are color pairs that exist in the human visual cortex. Orientation features are obtained by using Gabor pyramids. A linear “center-surround difference” operation, including interpolating the coarser aspects to a finer scale and point-by-point subtraction, is used between different levels of the pyramid to compute the multi-scale feature maps.The feature maps are globally prompted using the normalization operation N(•) and are added by using the across-scale combination operation “⊕”. This operation includes reducing each map to scale four and point-by-point addition to generate the conspicuity maps, Ī for intensity,C¯for color and Ō for orientation:(1)I¯=⊕c=24⊕s=c+3c+4NIcs(2)C¯=⊕c=24⊕s=c+3c+4NRGcs+NBYcs(3)O¯=∑θ∈004509001350N⊕c=24⊕s=c+3c+4NOcsθ.The final saliency map is calculated by normalizing and adding the three conspicuity maps, as follows:(4)S=13NI¯+NC¯+NO¯.In 2009, Achanta et al. presented a frequency-tuned approach of computing saliency in images by using the low level features of color and luminance; this method is easy to implement, is fast, and provides full resolution saliency maps. In the Achanta et al. model, the original image is first transformed to the CIE Lab color space, and each pixel location is a [L,a,b]Tvector. The method of finding the saliency map S for an image I of width W and height H pixels can be formulated as:(5)Sxy=Iμ−Iωhcxywhere Iμis the mean image feature vector, andIωhcxyis the corresponding image pixel vector value in the Gaussian blurred version (using a 5×5 separable binomial kernel) of the original image. ‖ ‖ is the Euclidean distance.An image-adaptive threshold is set to binarize the saliency map. The adaptive threshold (Ta) value is determined as two times the mean saliency of a given image:(6)Ta=2W×H∑x=0W−1∑y=0H−1Sxywhere W and H are the width and height of the saliency map in pixels respectively. S(x,y) is the saliency value of the pixel at position (x,y).In 2007, Hou and Zhang presented a simple method for the visual saliency detection by using a spectral residual approach. From the perspective of information theory, effective coding decomposes the image information H(Image) into two parts:(7)HImage=HInnovation+HPriorKnowledge.The amplitude A(f) of the averaged Fourier spectrum of the ensemble of natural images obeys a distribution:(8)EAf∝1/f.On a log–log scale, the amplitude spectrum of the ensemble of natural images, after averaging over the orientations, lies approximately on a straight line. Thus, the saliency parts are the values that log the amplitude spectrum and subtract the average log amplitude spectrum. Therefore, the spectral residual R(f) can be obtained by(9)Rf=Lf−Afwhere L(f) is the log amplitude spectrum of the input image. A(f) is the average log amplitude spectrum that is taken by convoluting the input image with an averaging filter of 3×3. Then Inverse Fourier Transform is used to get the saliency map.In 2009, Harel proposed a Graph-Based Visual Saliency (GBVS) model. This method is composed of two steps: first, activation maps are formed on certain feature channels, and then, they are normalized in a way that highlights the conspicuity and admits a combination with other maps.The initial steps for creating feature maps of the GBVS model are similar to Itti. Then, the dissimilarity of two pixels, M(i,j) and M(p,q), is defined as(10)dijpq=ΔlogMijMpq.Consider now the fully connected directed graph GA, which is obtained by connecting every node of the lattice M. A feature map is M:[n]2→ℜ. The goal is to compute an activation map A:[n]2→ℜ. The directed edge from node (i,j) to node (p,q) will be assigned a weight(11)ω1ijpq=Δdijpq)⋅Fi−p,j−qwhereFab=Δexp−a2+b22σ2.The last step, which is called “normalizing” an activation map, is begun with an activation map A:[n]2→ℜ. A graph GNhas n2 nodes labeled with indices from [n]2. For each node (i,j) and every node (p,q) to which it is connected, we introduce an edge from (i,j) to (p,q) with the weight:(12)ω2ijpq=ΔApq⋅Fi−p,j−q.Again, normalizing the weights of the outbound edges of each node to unity and treating the resulting graph as a Markov chain gives us the opportunity to compute the equilibrium distribution over the nodes. The output saliency map SGBVS is generated by computing the equilibrium distribution over the nodes.In Sun's method, two bottom-up visual saliency computation methods, edge-based and Graph-Based Visual Saliency, are adopted to exploit different kind of features, and two saliency maps are fused using a 2D Gaussian shaped function. This method is proposed for high resolution remote sensing images.For the edge-based saliency map, given an image I, first, the edge map is generated by using a Soble operator. Each pixel in the map has its edge strength, which equal to its edge value.Second, the edge density information is propagated using the distance transform (DT), with which DT maps are generated by applying threshold decomposition on the edge map. The DT is defined following.(13)DTx,yi=mina,b=−1….1Bx+a,y+bi+ωa,bwhere Biis a binary images extracted by threshold decomposition for slice i on the edge map and ωa,bis weight mask of the 8 neighbor. The salient distances from a pixel to its neighbor are weighted by the edge strengths in DT maps. In DT maps, pixels close to edges have low values and the higher edge magnitude the edge has, the lower the DT values the pixels are.Third, the edge-based saliency map is generated by summing all the DT maps Sedge. In this saliency map, lower values indicate greater saliency. Last, the final output saliency map SEdge-based is normalized by the following equation.(14)SEdge‐based=Sedge−maxSedgemaxSedge−minSedgeGraph-based saliency map is computed as Harel's model. Finally, a 2D Gaussian shape mix function is employed to fuse two saliency maps, obtain by edge-based and graph-based methods.Perazzi's method consists of four basic steps. First, it decomposes a given image into compact, perceptually homogeneous elements that abstract unnecessary detail. Based on this abstraction it computes two measures of contrast that rate the uniqueness and the spatial distribution of these elements. From the element contrast it then derives a saliency measure that produces a pixel-accurate saliency map.First, Perazzi et al. used a slightly modified SLIC approach to obtain the image abstraction. The modified SLIC approach instead use K-means clustering in geodesic image distance in CIE Lab color space. Then, element uniqueness is defined as the rarity of a segment i given its position piand color CIE Lab cicompared to all other segments j:(15)Ui=∑j=1Nci−cj2⋅ωpipj︸ωijpwhere by introducing ωij(p) it can combine global and local contrast estimates with control over the influence radius of the uniqueness operator.Third, it defines the element distribution measure for a segment i using the spatial variance Di of its color ci, i.e.(16)Di=∑j=1Npj−ui2⋅ωcicj︸ωijcwhere ωij(c) describes the similarity of color ciand color cjof segment i and j, respectively, pjis again the position of segment j, and ui=∑j=1Nωij(c)pjdefines the weighted mean position of color ci.For the saliency assignment, tt starts by normalizing both uniqueness Uiand distribution Dito the range [0…1]. It combines these terms as follows to compute a saliency value Sifor each element:(17)Si=Ui⋅exp−k⋅Diwhere k=6.Finally, it defines the saliencyS˜iof a pixel as a weighted linear combination of the saliency Sjof its surrounding image elements(18)S˜i=∑j=1NωijSj.Our model is proposed to improve the accuracy of ROI extraction for VHR remote sensing images. The implementing stages of our model are as follows:1)The extraction of intensity features by multi-scale spectrum residuals;The extraction of multi-scale color and orientation features;Spectral feature extraction by analyzing spectral information content.The introduction of weighted multi-scale feature fusion and the generation of a saliency map;The description of the ROI regions.Fig. 1shows the framework of our model.The input image is provided in the form of static color images. Let r, g, and b be the red, green, and blue channels of the input image, and the intensity image I is obtained as:(19)Ixy=rxy+gxy+bxy3.To obtain multi-scale feature maps, the intensity image I(x, y) is used to create the corresponding Gaussian pyramid Iδ(x,y), where (x,y) is the pixel location, δ∈[0⋯n] is the scale, n is the level of the pyramid, and n is computed as:(20)n=log2l/2−1where l is the shorter border of the intensity image I, and Iδ(x,y) is computed as:(21)I0xy=Ixy∗ϕxy(22)Iδ+1′xy=Iδ2x,2yIδ+1xy=Iδ+1′xy∗ϕxyδ∈0,⋯,n−1where “*” denotes the convolution operation, Iδ+1′(x,y)=Iδ(2x,2y) is to implement the down-sampling operation, and ϕ(x,y) is written as:(23)ϕxy=12πσexp−x2+y22σ2.The multi-scale space image is obtained by the Gaussian pyramid operation. Fig. 2shows the multi-scale space image results. Then, to attain the multi-scale spectrum residual feature images, each level Gaussian pyramid image Iδ(x,y) is regarded as the input data. We first perform the Fourier Transform of the input data to obtain the phase and amplitude spectrums as follows:(24)Fδuv=∫−∞+∞∫−∞+∞Iδxye−2πixu+yvdxdyδ∈0⋯n(25)Fδuv=Aδuveiφδuv(26)Aδuv=ReFδuv2+ImFδuv2(27)φδuv=argtanImFδuvReFδuvwhere Fδ(u,v) is the Fourier Transform results, Aδ(u,v) is the amplitude spectrums of the input image, and φδ(u,v) is the obtained phase spectrums.Then, we extract the spectral residuals in the spectral domain. The implementation is as follows:(28)Lδuv=logAδuv(29)Lδ′uv=Lδuv*h(30)Rδuv=Lδuv−Lδ′uv.Finally, the feature maps are derived by applying an Inverse Fourier Transform on an exponential function (this function combines the spectral residual and phase spectrum information) and filtering by Gaussian smoothing. Thus, the multi-scale intensity spectrum residual feature maps are derived as follows:(31)Fδ′uv=expRδuv+φδuv(32)fδxy=∫−∞+∞∫−∞+∞Fδ′uve2πixu+yvdxdy(33)I¯δxy=ϕxy∗fδxywhere h denotes the average filters, and Īδrepresents the multi-scale intensity spectrum residual feature maps.The multi-scale spectrum residual results are attained by the proposed algorithm. To have a more intuitive comparison between different scale spectrum residual images, Fig. 3presents the spectrum residual images on various scales. To further analyze the characteristics of multi-scale spectrum residuals, Fig. 4shows the results of log spectrum, average log spectrum, and log spectrum after the average filter, and the spectrum residuals on different scale-space levels. Spectrum residuals exist on every level of scale-space image. On the fine scale, the spectrum residual results contain more high-frequency information, which means that more detailed information, such as textures and edges, is included. Thus, detailed regions can be detected effectively on the fine scale. On the coarse scale, for the spectrum residual results, more average information and less detailed information are contained, which is helpful for detecting larger regions with details and extracting the shape of salient regions. Therefore, the multi-scale spectrum residual algorithm can effectively extract the salient regions on every level of scale-space image, and the highlighted salient regions will have well-defined boundaries and shapes. Moreover, Fig. 3 also shows the advantages of the proposed multi-scale spectrum residual algorithm.For the color features, the R, G and B channels are first normalized by I to eliminate the influence of the intensity, and the normalization is applied only at the locations where I is larger than 1/10 of its maximum over the entire input image. Then, four broadly tuned color channels are generated:(34)Rxy=rxy−gxy+bxy/2(35)Gxy=gxy−rxy+bxy/2(36)Bxy=bxy−rxy+gxy/2(37)Yxy=rxy+gxy/2−rxy−gxy/2−bxywhere R, G, B and Y refer to red, green, blue, and yellow, respectively (negative values are set to zero).Then, four Gaussian pyramids are generated for these color channels, namely, Rδ, Gδ, Bδ, and Yδ.(38)R0xy=Rxy∗ϕxyG0xy=Gxy∗ϕxyB0xy=Bxy∗ϕxyY0xy=Yxy∗ϕxy(39)Rδ+1′xy=Rδ2x,2yRδ+1xy=Rδ+1′xy∗ϕxyδ∈0,⋯,n−1(40)Gδ+1′xy=Gδ2x,2yGδ+1xy=Gδ+1′xy∗ϕxyδ∈0,⋯,n−1(41)Bδ+1′xy=Bδ2x,2yBδ+1xy=Bδ+1′xy∗ϕxyδ∈0,⋯,n−1(42)Yδ+1′xy=Yδ2x,2yYδ+1xy=Yδ+1′xy∗ϕxyδ∈0,⋯,n−1Then, to simultaneously account for red/green and green/red double-opponency, RGδis created. To simultaneously account for blue/yellow and yellow/blue double-opponency, RGδis created. As a result, we obtain the color feature maps RGδand RGδ.(43)RGδxy=Rδxy−Gδxy(44)BYδxy=Bδxy−YδxyFor the orientation feature maps, they are obtained by using the Gabor pyramids Oδ(θ), where σ∈[0,⋯,n] refers to the scale and θ∈{0°,45°,90°,135°} representing the preferred orientation.The general function of the 2D Gabor filter family can be represented as a Gaussian function that is modulated by a complex sinusoidal signal. A 2D Gabor filter h(x, y) can be formulated as(45)hxyθ=12πσxσyexp−12x^2σx2+y^2σy2exp2πiWx^(46)x^=xcosθ+ysinθy^=−xsinθ+ycosθwhere θ(θ∈{0°,45°,90°,135°}) specifies the orientation of the Gabor filters. Here, σxand σyare the scaling parameters of the filter. W is the radial frequency of the sinusoid. A filter will respond more strongly to an edge or a texture that has a normal that is parallel to the orientation θ of the sinusoid.To attain the orientation feature maps, we first extract the orientation information of the intensity image I with the Gabor filter. Then, the pyramids are built to obtain the multi-scale orientation feature images Oδ(θ). The algorithm steps are as follows:(47)O^xyθ=Ixy∗hxyθθ∈0°,45°,90°,135°(48)O0xyθ=O^xyθ∗ϕxy(49)Oδ+1′xyθ=Oδ2x,2y,θOδ+1xyθ=Oδ+1′xyθ∗ϕxyδ∈0,⋯,n−1.Information content is a valid way to measure saliency based on probability of occurrence. In spectral information content analysis, one-dimensional histogram of intensities in different channels is constructed. Then, spectral feature maps are computed by analyzing spectral information content. Finally, we construct the spectral conspicuity map by fusing these spectral feature maps.Let Xldenote the fusion results between panchromatic and multi-spectral channel images for VHR remote sensing image. l is determined by the number of multi-spectral channel. Xl(i,j) denotes the intensity value of Xllocated at (i,j). To compute the feature map of one channel, the one-dimensional histogram of Xlwith intensity levels in the range [0,255] is a discrete function (50), where rkis the kth intensity value and nkis the number of pixels in the image with intensity rk. The normalized histogram is given by Eq. (51), where p(rk) is an estimate of the probability of occurrence of intensity level rkin Xl.(50)hrk=nk,k=0,1,2,⋯,255(51)prk=nk/M×N,k=0,1,2,⋯,255The information content of each intensity level can be computed by using Eq. (52). Then, we replace the intensity value of each pixel by the information value based on Eq. (53)(52)Lrk=−logprk(53)Elij=LAlijwhere Elis the spectral feature map of channel Xl.The final spectral conspicuity mapE˜is shown in Eq. (54).(54)E˜=∑l=14wlEl(55)wl=−loghlh1+h2+h3+h4(56)hl=∑i∑jAlij∑c=14∑i∑jAcijNow, we have obtained multi-scale intensity, color, and orientation feature maps, and next, we must combine the feature maps that are at different scales into one intensity, color, and orientation conspicuity map, respectively. A normal combination method usually interpolates all of the maps into a specific size and performs point-to-point addition. This type of combination will greatly inhibit small-size salient regions because they appear only at fine scales, while large salient regions will appear at almost all of the scales. In addition, when the scale becomes coarser, the edges of the salient regions will become more and more blurred. In this view, it is necessary to enhance the maps at the finer scale and inhibit the maps at the coarser scale. Therefore, a novel weighted across-scale fusion method is proposed here, which is denoted as W(∙) and includes the following steps:1)Normalize all of the maps into [0, 255];For each map, compute its average intensitym¯;Multiply the map with255−m¯2.With the combination operator W(∙), the intensity, color, and orientation conspicuity maps are calculated as follows:(57)I˜=⊕σ=0nWI¯δxy(58)C˜=⊕σ=0nWRGδxy+WBYδxy(59)O˜=∑θ∈0°,45°,90°,135°W⊕σ=0nWOδxyθwhere “⊕” denotes across-scale addition, which means interpolating the map to level 0 (the size of the original input image) and using point-to-point addition. Ĩ,C˜, and Õ are the intensity, color, and orientation conspicuity maps, respectively. The final saliency map S is computed as:(60)S=NWI˜+WC˜+WO˜+WE˜where N(∙) means to normalize the image into [0, 255]. S is the saliency map of the input. Fig. 5shows the computational results of the conspicuity images of one test image.Once the saliency map is produced, we can generate the ROI. Because the ROIs are distinctive in the saliency map, we can easily use a threshold segmentation method to separate the ROIs from the background. The traditional Otsu's method is used to compute a threshold to transform the saliency map into a binary mask, where one refers to the ROIs.The Otsu thresholding technique [30] is a global thresholding method that has been widely used. An image with the total pixel value index number N can be represented in the gray levels [0,1,2,⋯,L]. The number of pixels at level i is denoted by ni. The probability for level i is as follows:(61)pi=ni/Ni=0,1,⋯L(62)∑i=1Lpi=1.The pixels of the saliency map are divided into two classes, C0 and C1 (background and objects), by a threshold at level k. The probabilities for the class occurrences are given by the following:(63)ω1k=∑i=0kpiω2k=∑i=k+1Lpi=1−ω1k.The class means are calculated as follows:(64)μ1k=∑i=0kipiμ2k=∑i=k+1Lipi.The average pixel value over the entire image is:(65)μ=∑i=0Lipi=ω1kμ1k+ω2kμ2k.The inter-class variance is as follows:(66)σb2k=ω1kμ1k−μ2+ω2kμ2k−μ2=ω1kω2kμ1k−μ1k2.Thus, the threshold segmentation algorithm is computed as follows:1)Compute the histogram and probabilities of each intensity level;Set up the initial ωi(0) and μi(0);Step through all of the possible thresholds t=1…maximum intensity;a) Update ωiand μi;b) Compute σb2(k);The desired threshold corresponds to the maximum σb2(k).Once the threshold is obtained, the saliency can be turned into a binary image and the final detection result is obtained by multiplying the binary image with the original image.

@&#CONCLUSIONS@&#
A novel region of interest detection approach based on the bottom-up saliency detection model is proposed to improve the extraction accuracy for ROI regions. The proposed model is presented to process the input remote sensing image along four feature channels. The intensity features are extracted by multi-scale spectrum residuals, multi-scale color and orientation features are extracted by Gaussian pyramids and Gabor pyramids, and spectral features are obtained by analyzing spectral information content. Thus, our method involves saliency analysis that combines spatial and frequency analysis. Moreover, a weighted across-scale combination and a description of ROI approach are introduced and applied in our model.The proposed method and the other six models are implemented in the experiments. The subjective quality performance based on the finally saliency maps and the ROI results indicates that our model offers two advantages over the other approaches: eliminate the background information effectively and highlighting the salient detected results with well-defined boundaries and shapes. Moreover, to evaluate the objective quality performance, the ROC curve, the ROC area, precision values, recall values, and F-Measure values are presented in the experimental results. Moreover, to evaluate the geometric structure accuracy of ROIs, geometric errors (edge-location, fragmentation, and shape errors) are also compared in this paper. These results also demonstrate the reliability and accuracy of our model. In general, the detection results are satisfying from visual and statistical perspectives.In our future work, the proposed method could be used as an operator to select salient and object potential area in VHR remote sensing images. It is unsupervised and can be employed to pre-segment images and make further image processing more effective. In addition, the results of our model also can be applied to image compression. ROIs can be regarded as the step of priority coding in the process of image compression.