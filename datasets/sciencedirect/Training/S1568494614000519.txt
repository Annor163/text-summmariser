@&#MAIN-TITLE@&#
Automatic parameter tuning for Evolutionary Algorithms using a Bayesian Case-Based Reasoning system

@&#HIGHLIGHTS@&#
An automatic parameter tuning system for evolutionary algorithms is analyzed.The system is based on Bayesian Networks and Case-Based Reasoning methodology.The system is compared to an optimal exhaustive statistical analysis.The behavior of the system is evaluated in two scenarios: static and dynamic.In both scenarios, statistical tests prove the efficiency of the system.

@&#KEYPHRASES@&#
Parameter tuning,Case-Based Reasoning,Bayesian Networks,Evolutionary Algorithms,Geometric constraint solving,

@&#ABSTRACT@&#
The widespread use and applicability of Evolutionary Algorithms is due in part to the ability to adapt them to a particular problem-solving context by tuning their parameters. This is one of the problems that a user faces when applying an Evolutionary Algorithm to solve a given problem. Before running the algorithm, the user typically has to specify values for a number of parameters, such as population size, selection rate, and probability operators.This paper empirically assesses the performance of an automatic parameter tuning system in order to avoid the problems of time requirements and the interaction of parameters. The system, based on Bayesian Networks and Case-Based Reasoning methodology, estimates the best parameter setting for maximizing the performance of Evolutionary Algorithms. The algorithms are applied to solve a basic problem in constraint-based, geometric parametric modeling, as an instance of general constraint-satisfaction problems.The experimental results demonstrate the validity of the proposed system and its potential effectiveness for configuring algorithms.

@&#INTRODUCTION@&#
One of the major issues in applying metaheuristics and, in particular, an Evolutionary Algorithm (EA) [1], is how to choose an appropriate parameter setting. High computational requirements are needed to adjust the control parameters of the algorithm to improve its performance on a particular problem. Numerous studies focused on developing methods for automatically finding the best set of parameter values have been performed in [2] and more recently in [3].Geometric constraint solving, as an instance of general constraint-satisfaction problems, is a basic problem in constraint-based, geometric parametric modeling in the field of Computer-Aided Design [4]. Geometric problems defined by constraints have an exponential number of solution instances. Generally, the user is only interested in one instance that fulfills the geometric constraints. This solution instance is called the intended solution. Selecting a solution instance amounts to selecting one among a number of different roots of a nonlinear equation or system of equations. The problem of selecting a given root has been named in [5] the Root Identification Problem (RIP).In previous work [6,7] EAs have been shown as suitable techniques for solving the RIP, but the success and performance of EAs depends crucially on finding suitable parameter settings. Given the size of a problem and an EA, specific values must be assigned to the set of parameters which determine the evolution of the algorithm. Doing this by hand is a very time consuming process that does not ultimately guarantee finding satisfactory parameters. Users of EAs mostly rely on conventions, ad hoc choices, and experimental comparisons on a limited scale. Parameter settings are commonly chosen in practice by trial and error, tuned by hand [8], taken from other fields, by parameter tuning [9] or by adaptation and self-adaptation mechanisms (parameter control) [2,10,11].Given the great variety of possible approaches, an alternative solution would be to design a system which automatically provides the correct values of the parameters to control the execution of a particular algorithm when applied to a specific problem. Such a system, referred to as Bayesian Case-Based Reasoning (CBR), follows the approach proposed in [12,13] and empirically evaluated to set a basic genetic algorithm [14] that solves the RIP in [15]. Although good results were obtained when the Bayesian CBR system was applied, it would be useful to adapt and study the behavior of the system with the statistically most promising EAs in a given environment. In particular, recent studies on the applicability of a number of metaheuristics to the RIP have determined the algorithms that statistically obtain the best performance [16]. Such studies have originated a great deal of data for adapting, applying and evaluating the previously proposed Bayesian CBR system and comparing the results obtained.The aim of this paper is twofold. First, carrying out an adaptation and a new empirical evaluation of the Bayesian CBR system when it is used to configure EAs, in general, that solve the RIP. Second, a comparison is made between the proposed approach and the method which obtains the statistically best parameter configuration known [16,17].The paper is structured as follows. Section 2 briefly describes some preliminary aspects about parameter tuning. Section 3 presents the main issues about geometric constraint solving systems and the RIP. Section 4 demonstrates the profiling of the generic Bayesian CBR system according to the specific application domain, presents the experiments carried out and compares the results to the statistically best parameter configuration known. Finally, in Section 5, some conclusions and future work are discussed.EAs comprise a wide class of solution methods that have been successfully applied to many optimization problems. The assessment of these methods is commonly based on experimental analysis but the lack of a methodology in these analyses limits the scientific value of their results. Despite the lack of theoretical foundation, the simplicity of EAs has attracted many researchers and practitioners. Many results in the literature indicate that metaheuristics, and in particular EAs, are state-of-the-art techniques for problems for which there is no efficient algorithm [18,19].However, EAs do not always reach an optimal solution, even for long computation times. In addition, it is often difficult to obtain an analytical prediction of either the achievable solution quality within a given computation time or the time taken to find a solution of a given quality. The assessment of these conflicting performance measures is critical for the evaluation of EAs and their application to real problems. Given the lack of theoretical guidelines and the stochastic nature of most metaheuristics, such performance assessments are best carried out by experimentation over representative benchmarks [20–22].In a parameter tuning framework, the experimental design consists of a set of techniques which comprise methodologies for adjusting the parameters of the algorithms depending on the settings used and results obtained [23,24]. A brief survey on these parameter tuning techniques is contained in [25]. The Design of Experiment (DoE) paradigm offers a way of retrieving optimal parameter settings. It is still a tedious task, but it is known to be a robust and well tested suite, which can be beneficial for giving reason to parameter choices besides human experience.Finding a good set of parameter values is a complex optimization task with a nonlinear objective function, interacting variables, multiple local optima, noise (by the stochastic nature of the EA to be tuned), and a lack of analytic solvers. Ironically, it is precisely in these types of problems that EAs are very competitive heuristic solvers. It is therefore logical to use an evolutionary approach to optimize the parameters of an EA. The approaches following such idea are known as belonging to algorithmic parameter tuning, which even comprises search methods specifically designed for parameter tuning and parameter analysis, e.g., Sequential Parameter Optimization [26] or ParamILS, a local search approach for algorithm configuration [27]. Besides the principal parameter tuning algorithms, there are a number of useful ‘add-ons’, i.e., methods for increasing search efficiency, that are independent from the main tuner and can be combined with different tuning algorithms: racing, sharpening, etc. [9,28].The main problems in parameter tuning are, amongst others, that mistakes in settings by users can be sources of errors or sub-optimal performance, that it is usually very time consuming or that good values may become bad during the run [2]. Algorithm configuration is commonly (either implicitly or explicitly) treated as an optimization problem, where the objective function captures performance on a fixed set of benchmark instances.In [12,13] a Bayesian CBR system was introduced as a general framework for solving the parameter tuning problem, conditioned by the current problem instance to be solved. The system estimates the best parameter configuration for maximizing the algorithm performance when it solves an instance-specific domain problem. The proposed system follows the CBR methodology [29–34]. CBR systems have had considerable success in a wide variety of problem solving tasks and domains, including parameter tuning [35,36]. Other characteristics that have made CBR a good solution for the tuning problem are:•CBR is preferable when it is difficult to give explicit laws of behavior, but easy to give examples. In setting parameters for EAs, a number of past examples of good solutions usually exists.CBR is suitable when a completely accurate solution to the problem does not exist. Generally, in tuning an EA, a parameter configuration only works well with a subset of all instances.CBR is suitable when problems or cases tend to repeat themselves. Problem instances are very similar in most of the cases.Finally, CBR is applicable when it is easy to construct new cases from the solution of new problems. It is sufficient to run the EA with the suggested configuration for it to perform and create a new case.Moreover, CBR has two interesting properties related to the parameter tuning problem: its learning capacity, that allows the system to adapt itself to changes and its capacity for autonomy, that allows the system to assist the user completely.The design of the Bayesian CBR system has been carried out in two phases. The first one makes use of Bayesian Networks (BNs) [37–40] to model qualitative and quantitative relationships among the different algorithm parameters of interest to the user. The second phase integrates BNs within a CBR system to solve new domain problem instances taking into account relevant features and past experience of similar instances. A detailed description of the basic system can be found in [12,13].Computer-Aided Design systems were intended to present intelligent assistants to designers, but conventional geometric modelers were and are actually used only for activities that occur near the end of the design process, when analyzing and presenting the results. The computer program merely substitutes the drawing board, and does not support the designer in the time-consuming and error-prone calculations of coordinates and dimensions, or in different decisions that must be made. The program ‘thinks’ in a procedural way, but the declarative description of geometry is much closer to a human [41,42]. For this reason, the designer meets the requirement for numerous additional calculations by extending the design time and often presenting sources of errors. A new generation of geometric modelers offers the solution to this problem. The geometry can be described by defining relations (constraints) among particular geometric elements.A geometric constraint is a relation among geometric objects that should be satisfied [43]. The automatic solution of geometric constraints represents a kind of interface between a declarative description of the geometry that it is closer to the user and a procedural description more suitable to a computer. The user simply specifies an object's shape and size in a declarative way and the system takes care of making the drawing according to the specification. In short, the user specifies what to draw, not how to draw it [44].Various methods of constraint-based geometric design have been presented in the literature [45]. They can be classified into three main groups: the numerical approach, the symbolic approach and the constructive approach[46]. In the numerical approach, constraints are translated into algebraic equations and various numerical techniques, such as Newton–Raphson iteration or homotopy [47], are used to solve these equations. These methods have problems with convergency, and are usually employed as last resort if the solution is not found through a constructive approach [46]. The symbolic approach is similar to the numerical approach, but the algebraic equations are first transformed into the symbolic form by employing general symbolic methods such as Wu–Ritt's characteristic method [48,49] or the Gröbner basis method [50], and are then solved numerically [46]. In the constructive approach, a planning phase is carried out in order to transform the constraint problem into a step-by-step constructive form that is easy to compute, after which the constraint system can be solved efficiently [51].Many techniques that provide powerful and efficient methods for solving systems of geometric constraints have been reported in the literature. For example, see [45,52] and references therein for an extensive analysis of work on constraint solving. Among all the geometric constraint solving techniques, our interest focuses on the constructive approach.Constructive solvers have two major components: the analyzer and the constructor. The analyzer symbolically determines whether a geometric problem defined by constraints is solvable. If the problem is solvable, the output of the analyzer is a sequence of construction steps, known as the construction plan, that places each geometric element in such a way that all constraints are satisfied. After assigning specific values to the parameters, the constructor interprets the construction plan and builds an object instance, provided that no numerical incompatibilities arise.Fig. 1shows a mechanism defined as a geometric constraint problem. This mechanism, known as the crankshaft linkage[53], transforms the linear motion of point p5, along the straight line l1, into a circular motion of point p4 along a circle centered on point p3. As a geometric constraint problem, the crankshaft linkage includes five points, pi, 1≤i≤5, and one straight line l1. The set of geometric constraints is listed in Fig. 2, and includes point to point distance constraint, dpp(), and coincidence constraint, on(). Fig. 3shows the construction plan generated by the constructive solver [54] for the crankshaft linkage.It is usually difficult for human designers to specify exactly the geometric constraints needed to define an object unambiguously. A well constrained problem has a finite number of solutions [55]. However, if an infinite number of solutions exists the problem is considered as under-constrained or if there are no solutions as over-constrained. To define the shape and dimensions of a planar scene consisting of n characteristic points, 2n−3 independent dimensional constraints are necessary [56].The main task of any geometric constraint solver is to find a solution when it exists but, in general, a well-constrained geometric constraint problem has an exponential number of solutions and the number of instances is exponential in the number of geometric elements in the problem, so this is not usually a simple task.Generally, a user is only interested in one instance that fulfills the geometric constraints. Selecting a solution instance amounts to selecting one from among a number of different roots of a non-linear equation or system of equations. The problem of selecting a given root was first considered in [5] and denoted as RIP.Several approaches to solve the RIP have been reported in the literature. Techniques of Artificial Intelligence can be employed in the constructive approach. Generally, this approach requires pre-processing to transform a constraint problem into a new form that is easy to draw.In [6,7] a new technique to automatically solve the RIP was presented. This technique overconstrains the geometric problem by defining two different categories of constraints. One category includes the set of constraints specifically needed to solve the geometric constraint problem. The other includes a set of extra constraints on the geometric elements which identify the intended solution instance. Once the solver has generated the space of solution instances, the extra constraints are used to drive an automatic search all over the space of solutions. This search is performed by an EA and outputs a solution instance that maximizes the number of extra constraints fulfilled [7].The control parameters of the EAs are usually fixed before they are executed, and the values used for these parameters have an important influence on the EA performance [57–59]. The computational effort needed to solve these kinds of problems is very high, so if this method is to be competitive and applicable to final user products, it is necessary to optimize the performance of EAs with an accurate configuration of these control parameters. To achieve this goal the influence that those parameters have on the EAs performance must be previously analyzed.As mentioned in Section 2, several references for achieving the correct control parameters have been reported in the literature of EAs, but it is worth noting that the most suitable choice depends on the application domain [60]. That is the reason why it would be interesting to identify the effects that different control parameters, guiding the executions of this class of algorithms, cause when they are applied to geometric constraint problems.In [16], a preliminary study was conducted to asses the potential behavior of a number of metaheuristics applied to solve the RIP. The study considered single-point and population-based metaheuristics according to the classification given in [61]. Population-based approaches and, in particular, EAs performed better than path-based approaches. The most promising algorithms were specifically Population-Based Incremental Learning (PBIL) [62], and Cross generational elitist selection Heterogeneous recombination and Cataclismic mutation (CHC) [63].Hence, several analyses were made to tune CHC and PBIL algorithms [16,17] in order to obtain the best performance when they are used to solve the RIP. An empirical methodology along with a statistical ANalysis Of VAriance (ANOVA) [64] were applied. Finally, a parameter setting for each problem size and evolutionary algorithm, CHC and PBIL, was estimated by means of a simple linear regression method [65]. These methods obtain the statistically best parameter settings but indeed are very time-consuming.The behavior of an EA depends greatly on the values of the different parameters that control its run [2]. Even though some methods can be designed for parameters’ adjustment, their use does not guarantee a result for a specific problem. Hence, it seems reasonable to have a system that obtains the optimal parameter setting for an EA. The system could learn while gathers information about the finished runs of the algorithm. To put this idea into practice, we have set up the generic Bayesian CBR system[12,13] to assist the user in decisions about parameters’ configuration for the EAs, in particular, when they are used in the RIP. Such a system has the following features:•A priori, it does not require knowledge about the internal mode of operation of EAs. In the absence of this explicit knowledge, the system only requires data from previous EA runs to solve specific instances of the geometric problem.The parameter tuning will be particular for each instance of a geometric problem, depending on its relevant features, which can be supplied by an expert.The Bayesian CBR system has been defined in such a way that it can be applied to any other parameter setting problems for EAs. Moreover, the system has the capacity to select the optimal configuration in a stand-alone way, in the sense of being a system that does not need any user supervision. In short, we are proposing and testing a system for automatically solving the parameter setting problem for EAs.In this section we present an experimental evaluation of the Bayesian CBR system to set the parameters of the CHC and PBIL algorithms when solving the RIP. First, the generic Bayesian CBR system is profiled to adapt it to the specific application domain (by specifying available data sets, the representation of the cases, the parameters and the performance metric). We then evaluate the behavior of the Bayesian CBR system in two scenarios, both static and dynamic, depending on the true state of available information: complete or incomplete (but increasing when the system gathers new information), respectively.For the purposes of experimentation, we have considered the same benchmark [66] used in the comparative study of the application of metaheuristics to the RIP [17]. In that study Joan-Arinyo et al. included 24 different figures or geometric problem instances defined by constraints. In the evaluation process of our Bayesian CBR system each figure generated will be considered a case.Each case or figure consists of a description part, including the features of a given geometric problem instance, and a model solution part, similar to BN (see Fig. 4). The meaning of the attributes that describe each geometric problem can be seen in Table 1, and the solution part of the case is an induced BN that encodes dependence/independence relationships between both the parameter configuration used and the performance value reached by the EAs when this case is solved.The behavior of the EAs has been investigated using only those parameters that were considered in [17] to have the largest influence on the algorithms’ performance. These are shown in Table 2for CHC and Table 3for PBIL.With the range of values in Tables 2 and 3 the number of possible configurations for executing respectively the CHC and PBIL algorithms are 735 (7×7×5×3) and 875 (7×5×5×5). A simple exhaustive evaluation of these configurations is not sufficient due to the probabilistic nature of EAs. The same configuration can give rise to different results, so it is necessary to compile an adequate number of EA executions to capture the expected values for each configuration (solution extensible to other Artificial Intelligence models). In reality, there is no deterministic way to select the optimal parameter setting by means of an analytical formula. We are ideally interested in the underlying joint probability distribution of parameters’ values vs. performance [17] and if there exists a method to assess such a distribution based on experimental results. This is the main reason why in [17] 50 runs are made for each configuration. Then, we have a collection of 36750 (735×50) configurations for each figure, in the case of CHC algorithm, and 43750 (875×50) configurations for the case of PBIL algorithm.The mean run-lengthEˆ(RL)[67] is used as the performance measure to compare and evaluate the quality of the proposed configurations. The computational cost of an EA corresponds, basically, to compute the number of extra predicates fulfilled by the different solution instances generated during the search process, that is, a function fitness evaluation [68]. Therefore as measure unit we choose counting the number of times the function fitness was evaluated. In our context, this is a more machine independent measure of algorithm's performance than measuring actual CPU time [69]. The mean run-length is estimated by the expression presented in Eq. (1),(1)Eˆ(RL)=1z∑i=1zrli+(n−z)zrlmaxwhere z is the number of successful runs and rliis the run-length of the ith successful run. The total number of runs was n=50, each triggered with an initial random seed. The maximum number of evaluations allowed before declaring a run as a failure (best fitness found less than 90% of the optimum or than 90% of a lower bound [16] if the optimum is unknown) is the maximum run-length, rlmax. Since our context requires real time interaction [55], the limit value rlmaxwas set to 30,000 evaluations.For each configuration and run, the parameters’ values used (PS, DR, D, BI for CHC and PS, LR, MP, MS for PBIL) and the performance (the variableEˆ(RL)) have been stored.From these data, we have designed two scenarios for testing the Bayesian CBR system: (1) a static scenario, to ascertain the efficiency of the system when knowledge is complete, i.e., data of EA runs are completely available for all possible configurations and (2) a dynamic scenario, to ascertain the efficiency of the system when not all data are available and it is required a learning mechanism to continue updating their knowledge.The experimentation to validate the Bayesian CBR tuning system has been carried out using the Elvira package [70], a free software that supports structural and parametric learning of BNs from data. Subsequently, PC [71], K2 [72] and VNSST [73] (with metrics BIC, K2 and BDe) have been identified as the batch algorithms applicable for learning a BN from a complete database. These algorithms are limited to working with discrete variables and therefore it has been necessary to pre-process our collections of runs, specifically to discretize the variableEˆ(RL). Though there are some unsupervised discretization methods in the literature, we have defined an ad hoc discretization method, which takes into account the characteristics ofEˆ(RL), as follows:(2)⌊log2(Eˆ(RL)−minimumrl)⌋where minimumrlstands for an estimate of the minimum value ofEˆ(RL)in the collection of runs.Finally, for the inference process, Elvira implements the k-MPE mechanism [37,74], i.e., with Elvira it is possible to obtain the k-most probable parameter configurations that minimizeEˆ(RL). Elvira also supplies the probability for each proposed configuration. This allows the understanding and reproduction of the method.In the following subsections, the evaluation of the Bayesian CBR system is presented. Results in [17] will be considered as a “gold standard” in the sense that they were obtained manually after a thorough statistical analysis of the data. It will constitute the framework within the obtained results in the two different scenarios will be compared.The aim of this test is to ascertain the ability of the Bayesian CBR system to recommend an EA configuration when all information is accessible. In this case, the methodology carried out to test the system is based on all data generated in [17]. With this information (36,750 inputs for CHC and 43,750 inputs for PBIL) the learning algorithms K2, PC and VNSST (with metrics BIC, K2 and BDe) have been applied for each of the 24 figures, having previously discretized the variableEˆ(RL). Some induced BNs can be seen in Fig. 5whereEˆ(RL)is represented by e.Next, the MPE process is applied to each BN solution induced in order to achieve the most probable configuration for the EA that minimizesEˆ(RL). As a result of this inference process, three configurations (K2, VNSST and PC) for the CHC algorithm and another three configurations for the PBIL algorithm have been proposed by our system for each geometric problem.To determine whether significant differences exist between configurations suggested by the Bayesian CBR system for each geometric problem and the configuration recommended in [17], several statistical analyses have been carried out comparing the EAs performance. These analyses consisted of first applying an omnibus method and second, having found significant differences, of using a post hoc method to elucidate which configurations were different from one another. The methods selected were multiple ANOVA as an omnibus test and Tukey Honestly Significant Difference as the post hoc test [75]. To guarantee the validity of any parametric test, and of ANOVA in particular, the population of performances given by the different configurations must meet certain criteria: independence, normality and homoscedasticity. Following [16], the performances of the different configurations derived from the analysis of the algorithms’ parameters fulfill such properties.The results of multiple ANOVA show that significant differences exist among the different configurations for both algorithms: CHC and PBIL. We have applied ANOVA to know if the means of the populations of performance,Eˆ(RL), provided by the different configurations proposed are equal. This is precisely the null hypothesis of ANOVA in the analysis of the behavior of CHC and PBIL over each problem instance. Considering the ANOVA results for the set of problems in the benchmark, the null hypothesis is rejected in all cases because the tests of equality of means have shown a significance level smaller than 0.05 for each problem instance. This means that variations in each individual configuration lead to variations in the mean run-length with a 95% confidence level. Hence, each configuration, and indeed each tuning method, has a different contribution to the performance of CHC and PBIL. Moreover, because of ANOVA's ability to explain the differences found in the performance of the algorithms with the different configurations, it is interesting to measure the percentage of variability in the response,Eˆ(RL), being explained. The coefficient of determination, R2, is the fraction of the total squared error that is explained by the model in ANOVA. Thus, values approaching one are desirable [76]. R2 values over the benchmark for CHC ranged from 0.789 through 0.977 with an average of 0.852 and a standard deviation of 0.068. In the case of PBIL, R2 values ranged from 0.849 through 0.981 with an average of 0.892 and a standard deviation of 0.039. Therefore, the model accounts for most of the configurations affecting the algorithm performance and the differences among them can be explained from ANOVA.However, the results of ANOVA just show whether performance means differ significantly or not. They do not explain which configurations are different from each other. To elucidate this question we have applied post hoc tests. We need to statistically determine the ability of the Bayesian CBR system to provide a configuration at the same level of performance as the configuration recommended in [17]. It means that post hoc tests find no significant differences in the performance provided by some of the Bayesian CBR system configurations with respect to the statistically best configuration known. Table 4presents the results of the post hoc tests indicating the number of homogeneous subset for each configuration to illustrate where the performance differences appear. Each row contains results corresponding to one geometric problem for both algorithms. Each column is associated with one configuration. The three first columns for each algorithm show the results of the Bayesian CBR system.The results from Tukey's test [77] were grouped in homogeneous subsets, so that each subset groups configurations with performances that do not differ significantly. In general, a large number of homogeneous subsets – each grouping just one configuration or a few different configurations – means that the algorithm performance is largely influenced by the configuration change. Homogeneous subsets are sorted according to increasing values of the performance. Thus, the first subset includes the performances corresponding to those configurations for which the algorithms work better. Each cell contains the number of the corresponding homogeneous subsets. As can be seen, one configuration can belong to more than one homogeneous subset. The maximum number of homogeneous subsets is the number of configurations tested for each algorithm, i.e., 4. It can be seen from the results that there are no cases where a configuration reaches four subsets. This indicates how few differences exist among the configurations tested: Bayesian CBR configurations and Joan-Arinyo et al. It should be recalled that Joan-Arinyo et al. made an exhaustive statistical analysis which is an excessively time-consuming task.Notice that the number of homogeneous subsets is only two in 80% of the cases for CHC and close to 90% of the cases for PBIL. It indicates that qualitatively there exist very few differences among the configurations. Moreover, if the configurations provided by the Bayesian CBR system are separately analyzed, the existence of one subset allocating them occurs in more than 50% of the cases for CHC and 70% of the cases for PBIL. In most of the cases, these configurations act as only one class in opposition to or at the same level as the statistically best configuration.When comparing the Bayesian CBR configurations to Joan-Arinyo et al. configurations, more significant differences appear for CHC than for PBIL. Concretely, at least one of the Bayesian CBR configurations does not show statistically significant differences, with a 95% of confidence, from that proposed in Joan-Arinyo et al. in 20.83% and 79.16% of the geometric problems, respectively, for CHC and PBIL. Considering the configurations given by the Bayesian CBR, for CHC there are not noticeable differences, while for PBIL we can highlight PC which, in 62.40% of the geometric problems, does not present statistically significant differences with the best configuration known. In a first approximation, the Bayesian CBR system is at the same level as the best configuration known only for PBIL. The results for CHC suggest the existence of two classes (Bayesian CBR system and Joan-Arinyo et al.) but with significant differences in most of the cases. Because of that, the behavior of CHC and PBIL with the different configurations will be also normalized and quantitatively compared.The differences between the results for CHC and PBIL arise from the influence of the parameters for each algorithm (for a detailed explanation, see [17]). For PBIL all the parameters are equally influential in the algorithm performance for the benchmark tested. However, for CHC, two parameters (divergence rate and best individuals) are involved in the calculations only in certain algorithm phases when the population is reinitialized after finding a local optimum [63]. For our benchmark, more than 80% of the algorithm runs (a similar percentage to the number of cases where there exist significant differences in the post hoc tests) reached a solution without reinitializing the population. This explains why in most of the studied cases such parameters do not have an influence on the CHC algorithm performance, hence the configurations do not have enough influence in the performance. To validate or to reject this result we should study the CHC performance feeding it with larger problems with more local optima.To clarify the differences among the proposed configurations and algorithms, a quantitative analysis of the performance is presented in Fig. 6. An ANOVA box plot is depicted for each algorithm in order to compare the performance of the configurations given by the different tuning methods tested over the benchmark. The ANOVA box plot is used to assess normality and to select those configurations for which the algorithm shows the best performance. The performance was normalized between 0 and 1. The box plot shows the mean run-length for the set of geometric problems in the benchmark vs. the different tuning methods. The tuning methods are in the X axis. Normalized performance values are represented on the Y axis. The thin vertical line shows the run-length range bounded by the minimum and the maximum values and the bold horizontal line inside the boxes indicates the median value. The box itself indicates where most of the cases lie. The lower side is the first quartile while the upper side is the third quartile. Ideally, for normal distributions, the rectangle is in the middle of the range and the median line is in the middle of the rectangle and is coincident with the average value.Both box plots are very similar. The normalized differences between the statistically best configuration and those provided by the Bayesian CBR system are also in the same magnitude. It is reasonable now to put the Bayesian CBR configurations on a level with Joan-Arinyo et al. configuration for both CHC and PBIL. With respect to CBR methods, there are few differences between them in both CHC and PBIL. The dominant CBR method is PC. Hence, PC is used as the CBR method in the dynamic scenario, presented in Section 4.3.In conclusion, the results obtained in the static scenario, following qualitative and quantitative statistical analyses, prove that the Bayesian CBR tuning system is a valid choice for setting the EA parameters with independence of the different influence of such parameters. In this case, the system has been applied to the algorithms CHC and PBIL in order to solve the RIP. Statistical tests have not exhibited significant differences with the best configuration known when we compare the results with the proposed configurations for the whole data set or for each problem individually. Recall that the Bayesian CBR system is automatic and much less time-consuming when compared to an exhaustive statistical analysis for parameter tuning.The aim of this test is to evaluate the learning capacity of the Bayesian CBR system in a dynamic environment. We have simulated the mode of operation of the system in a real situation, in which not all data are available at the beginning, but are incorporated gradually at the same time as several reasoning cycles are run. More specifically, the objective of the test is to evaluate the behavior of the system when it solves a geometric problem already present in the case base. For this purpose, we have built a case base with 24 cases or geometric problems, where each case has a BN solution encoding only 5% of available EA runs for that geometric problem. With this initial case base we have simulated several cycles of reasoning for each geometric problem. In each cycle several configurations have been recommended and run, and later included within the database. When a sufficient number of data for a case was stored onto the database, they were used to update it through a sequential learning process. Sequential learning leads to the update of the BN solution of such a case in order to better reflect the experience obtained. For this experiment, we have used only the PC algorithm for learning the BN solution from data runs of each geometric problem, in view of the positive results achieved in previous experiments with this learning algorithm.The Bayesian CBR system, in each cycle, not only proposes the most probable configuration, but also the k most probable configurations, depending on the state of reliability of the model solution, r(s, t) (see Eq. (3)):(3)r(s,t)=1C*avg(s,t)1+sd′(s,t)where C stands for the number of desirable runs per configuration, avg(s, t) denotes the average of encoded runs and sd′(s, t) represents the standard normalized deviation of executions encoded by the network.Accordingly, in [13] a weight function, which is used to compute the number of recommendations k given by the system at time t (see Eq. (4)), is considered. This function is defined as an exponential decay function with regard to the reliability of the current state:(4)w(s,t)=exp(−a*r(s,t))wherea=−ln(wmax),wmaxis the weight assigned to the maximum value reached by r(s, t) and its value must be positive and tend to 0. In a total reliable state, (r(s, t)=1), the system gives at least one recommendation (the most probable configuration), k=1, whereas in a partial reliable state (r(s, t)<1) the system gives more than one solution up to a maximum number of recommendations given by the expressionk=1+w(s,t)*log2(M), where M is the number of possible configurations.It has been necessary to record the number of runs encoded by each BN solution, the number of desirable runs for each configuration (C=3 in this simulation process) andwmax=0.001, and to compute the reliability of each model solution that would be used in the retrieval and reuse phases of the reasoning cycle (M=875 configurations for PBIL and M=735 configurations for CHC). The simulation process stops for a geometric problem or case when the state of its BN solution indicates that its reliability is high and only one configuration is recommended.To determine whether, in this dynamic scenario, significant differences exist between configurations suggested by the Bayesian CBR system for each geometric problem and the configuration recommended in [17], the same process as in Section 4.2 has been carried out. First, multiple ANOVA has been applied as an omnibus method and we have determined that significant differences between at least two configurations for each geometric problem exist. Second, to evaluate where the significant differences appear, the Tukey Honestly Significant Difference post hoc method has been applied. Again, to guarantee the validity of the test, the population of performances given by the different configurations must meet certain criteria: independence, normality and homoscedasticity. Following [16], the performances of the different configurations derived from the analysis of the algorithms’ parameters fulfill these criteria.Considering the ANOVA results for the set of problems in the benchmark, the null hypothesis is rejected again in all cases because the tests of equality of means have shown a significance level smaller than 0.05 for each problem instance. This means that at least one tuning method (Joan-Arinyo et al. or CBR using PC either at the beginning or the end of the dynamic test) leads to significant variations in the mean run-length with a 95% confidence level. ANOVA will be useful if and only if the percentage of variability in the response,Eˆ(RL), being explained by the results is high. The coefficient of determination, R2, will be used as reference to indicate such variability percentage. R2 values over the benchmark for CHC ranged from 0.754 through 0.947 with an average of 0.831 and a standard deviation of 0.077. In the case of PBIL, R2 values ranged from 0.809 through 0.952 with an average of 0.870 and a standard deviation of 0.051. Therefore, the model accounts for most of the configurations provided by the tuning methods and the differences among them can be explained from ANOVA. Notice a slight variation in the determination coefficient, explaining in all cases more than 75% of the performance yielded by the configurations, with respect to the results of the static test.Next, the post hoc tests are used to determine where the differences among the different tuning methods tested appear. The results of the post hoc tests are shown in Table 5. For each geometric problem and algorithm (CHC or PBIL), the number of homogeneous subsets is presented for each configuration. Clearly, the first subset always contains the configuration given by the exhaustive statistical study. The number of homogeneous subsets is only two in more than 50% of the cases for CHC and in 50% of the cases for PBIL. For the remaining problem instances, the number of subsets is three. It indicates that qualitatively there exist very few differences among the configurations. The average of homogeneous subsets has increased with respect to the static scenario because of the lack of available data in the dynamic scenario. Likewise, if both configurations provided by the Bayesian CBR system are separately analyzed, the existence of one subset allocating them occurs in 25% of the cases for CHC and 37.5% of the cases for PBIL. This is reasonable because not all data are available at the beginning, but are incorporated gradually in the scenario leading in most of the cases to a different state of the system.The configurations yielded by the Bayesian CBR system do not present significant differences (with a 95% of confidence) with the best performance configurations in 29.15% of the cases for CHC and in 16.66% of the cases for PBIL. We must consider the absence of performance data associated with the dynamic test procedure. For the CHC algorithm such a lack of performance data is less noticeable because the change in parameters is less effective than in PBIL, as we have indicated in Section 4.2. If the Bayesian CBR results are compared to each other, the end configurations are statistically better or equal, with a 95% of confidence, than the initial configurations in 58.3% of the cases for CHC and in 75% of the cases for PBIL. The more effective the configuration change is for the algorithm the greater is the Bayesian CBR capacity for providing a better configuration.Finally, to clarify the differences among the tuning methods and algorithms, a quantitative analysis of the performance will be useful. Fig. 7presents a box plot for each algorithm to summarize the performance of the configurations given by the different tuning methods according to our benchmark. The performance has been normalized between 0 and 1. The supremacy of Joan-Arinyo method and the different CBR results depending on the algorithm can be checked. The effectiveness of the Bayesian CBR system is demonstrated by a normalized performance median greater than 0.6 for CHC and 0.5 for PBIL. The relative differences in performance between the statistically best configuration and those provided by the Bayesian CBR system have decreased with respect to the static scenario. However, the lack of available data has not drastically affected in the magnitude of the performance.The results of the dynamic tests demonstrate the suitability of the Bayesian CBR system for setting the control parameters of the two algorithms that solve the RIP. A high level of efficiency is achieved because the adjustment model requires much less runs than the exhaustive statistical analysis of Joan-Arinyo et al. There is no evidence of a generalized decrease in the quality of the results with respect to the static scenario.

@&#CONCLUSIONS@&#
This paper analyzes the performance of an automatic parameter tuning system based on BN and CBR methodology, in order to determine suitable parameter settings for EAs applied to solve problems with high computational requirements. A basic problem in parametric modeling, the RIP, as an instance of general constraint-satisfaction problems was selected for the experimentation. The algorithms CHC and PBIL were chosen as representative of EAs because of previous studies conducted to assess the potential behavior of a number of metaheuristics related to the problem.The behavior of the Bayesian CBR system was evaluated in two scenarios: static and dynamic, depending on the true state of available information; either complete or incomplete. The benchmark considered was previously used in a study to determine the best statistical parameter settings for the algorithms through an exhaustive statistical analysis, proposed by Joan-Arinyo et al., using ANOVA. The results of the exhaustive analysis (statistically optimal) were compared to those of the Bayesian CBR system using an omnibus test (ANOVA) and a post hoc test (Tukey Honestly Significant Difference).In both scenarios, static and dynamic, the results of the statistical tests prove that the Bayesian CBR tuning system is a valid choice to set the EA parameters with independence of the influence of such parameters, and with the advantage of being automatic and much less time-consuming than the statistically optimal exhaustive method. The Bayesian CBR system also has the advantage of not requiring a complete state of information. Furthermore, as the system increases its knowledge, the results improve.Finally, two different sources of randomness are present in our study: the non-deterministic nature of the algorithm itself, and the random selection of problem instances. We plan to conduct a study to assess the effect of each source of randomness on the final behavior of the system. Knowing the effect of each source of randomness would allow us to fit our system to specific requirements in general constraint-satisfaction problems.