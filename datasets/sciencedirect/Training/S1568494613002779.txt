@&#MAIN-TITLE@&#
Evolving intelligent algorithms for the modelling of brain and eye signals

@&#HIGHLIGHTS@&#
Three evolving and stable intelligent algorithms are described.The evolving and stable intelligent algorithms are used for the modelling of brain signals.The evolving and stable intelligent algorithms are used for the modelling of eye signals.

@&#KEYPHRASES@&#
Evolving intelligent systems,Stable intelligent systems,Modelling,Brain signals,Eye signals,

@&#ABSTRACT@&#
In this paper, the modelling problem of brain and eye signals is considered. To solve this problem, three important evolving and stable intelligent algorithms are applied: the sequential adaptive fuzzy inference system (SAFIS), uniform stable backpropagation algorithm (SBP), and online self-organizing fuzzy modified least-squares networks (SOFMLS). The effectiveness of the studied methods is verified by simulations.

@&#INTRODUCTION@&#
In recent years, there are two important topics which are related with the modelling, they are the evolving intelligent systems and stable intelligent systems.The evolving intelligent systems, are characterized by abilities to adjust their structure as well as parameters to the varying characteristics of the environment (with the term of environment embracing processes/phenomena with which the system has to interact and or deal with the users using the system) [6,46,20]. Some important results are given by [5,7–11,13,16,17,19,21–27,29,31,36–38]. In [5], two novel approaches for on-line evolving fuzzy classifiers are presented. In [7], an evolving system that considers the evolving clustering, online structure evolution and simplification is introduced. A hybrid evolving architecture for dealing with incremental learning is introduced by [8]. The suitability of various nature-inspired meta-heuristics to the problem of software testing is investigated by [9]. In [10], the authors follow an approach consisting of combining spectral clustering and ant colony optimization in a two-stage algorithm. In [11], the authors investigate self-adaptation of classification systems at three levels. The problem of the classification of streaming data from a dimensionality reduction perspective is addressed by [13]. A new approach for creating and recognizing automatically the behavior profile of a computer user is presented by [16]. The implementation of a zero-order Takagi–Sugeno–Kang (TSK)-type fuzzy neural network (FNN) is proposed by [17]. An evolving fuzzy granular framework to learn from and model time-varying fuzzy input-output data streams is introduced by [19]. A class of evolving fuzzy rule based system as an approach for multivariable Gaussian adaptive fuzzy modeling is considered by [21]. A new approach for evolving fuzzy modeling using tree structures is proposed by [22]. A new algorithm for incremental learning of a specific form of Takagi–Sugeno fuzzy systems is introduced by [23]. New approaches to handling drift and shift in on-line data streams with the help of evolving fuzzy systems (EFSs) are presented by [24]. In [25], the authors examine approaches for reducing the complexity of evolving fuzzy systems (EFSs) by eliminating local redundancies during training. A new methodology for conducting active learning in a single-pass on-line learning context is introduced by [26]. New dynamic split-and merge operations for evolving cluster models, which are learned incrementally and expanded on-the-fly from data streams are considered by [27]. In [29], the authors address option pricing using an evolving fuzzy system model and Brazilian interest rate options data. The use of evolving classifiers for activity recognition from sensor readings in ambient assisted living environments is described by [31]. In [36], a sequential adaptive fuzzy inference system called SAFIS is developed based on the functional equivalence between a radial basis function network and a fuzzy inference system (FIS). The performance evaluation of the recently developed sequential adaptive fuzzy inference system (SAFIS) algorithm for classification problems is presented by [37]. In [38], two adaptive fuzzy control schemes including indirect and direct frameworks are developed for suppressing the wing-rock motion. The above systems are evolving and soft; however, they are not guaranteed to be stable.The stable intelligent systems are characterized to be systems where some kind of stability is guaranteed, i.e., for bounded inputs in the algorithms, there are also bounded outputs and bounded parameters. Some important results are given by [1–4,32,33,35,40,41,47,50,51]. In [1], the authors proposes a new H∞ weight learning algorithm (HWLA) for nonlinear system identification via Takagi–Sugeno (T–S) fuzzy Hopfield neural networks with time-delay. New results on stability for Takagi–Sugeno fuzzy delayed neural networks with a stable learning method are introduced by [2]. In [3], an error passivation approach is used to derive a new passive and exponential filter for switched Hopfield neural networks with time-delay and noise disturbance. The model predictive stabilization problem for Takagi–Sugeno (T–S) fuzzy multilayer neural networks with general terminal weighting matrix are investigated by [4]. In [32,33], two stable intelligent controllers for nonlinear systems with dead-zone are addressed. Two stable neural networks are introduced by [35,40]. The aforementioned studies are stable and soft; nevertheless, they are not evolving.There is research where evolving and stable characteristics are possible and also combined whenever assuring some sort of convergence to optimality given by [28,39,41]. That systems are novel because they merge the main characteristics of the above techniques into one algorithm which has the main characteristics to be evolving, soft, and stable. See Fig. 1.This paper presents the comparison of three intelligent algorithms for the modelling of brain and eye signals. The signals could be applied for the patients who cannot move their bodies; therefore, they could use their brains or their eyes to say what they want or need. The algorithms are the SAFIS algorithm [36] which is an evolving intelligent system, SBP [40], which is a stable intelligent system, and SOFMLS [39], which is an evolving and stable intelligent system.The paper is organized as follows. In Section 2, the SAFIS, SBP, and SOFMLS algorithms are detailed. In Section 3, the encephalography (EEG) and electro-oculogram (EOG) signals are described. In Section 4, the comparison of three algorithms for the modelling of brain and eye signals is presented. Section 5 presents conclusions and suggests future research directions.In this section the three algorithms of this paper are described.A sequential adaptive fuzzy inference system (SAFIS) is developed based on the functional equivalence between a radial basis function network and a fuzzy inference system (FIS). In SAFIS, the concept of “Influence” of a fuzzy rule is introduced and using this the fuzzy rules are added or removed based on the input data received so far. If the input data do not warrant adding of fuzzy rules, then only the parameters of the “closest”(in a Euclidean sense) rule are updated using an extended Kalman filter (EKF) scheme.The SAFIS algorithm is summarized as below [36]:Given the growing and pruning thresholds eg, epfor each observation (xk, yk) wherexk∈RNx,yk∈RNyand k=1, 2, …, do(1)Compute the overall system output:(1)yˆk=∑n=1NhanRn(xk)∑n=1NhRn(xk)whereRn(xk)=exp−1σn2xk−μn2where Nhis the number of fuzzy rules.Calculate the parameters required in the growth criterion:(2)ɛk=maxɛmaxγn,ɛmin,0<γ<1(3)ek=yk−yˆkApply the criterion for adding rules:If(4)xk−μrn>ɛkand(5)Einf(Nh+1)=ek1.8nxk−μrnNx∑n=1Nh+1(1.8σn)Nx>egallocate a new rule with(6)aNn+1=ekμNh+1=xkσNh+1=nxk−μrnElse, adjust the system parameters arn, μrn, σrnfor the nearest rule only by using the extended Kalman filter (EKF) method:(7)Kk=Pk−1BkRk+BkTPk−1Bk−1θk=θk−1+KkekPk=I−KkBkTPk−1+qIwhereθk=θ1⋯θrn⋯θNhT=[a1,μ1,σ1,…,arn, μrn, σrn,…,aNh, μNh, σNh].Check the criterion for pruning the rule:If(8)Einf(rn)=arn1.8σrnNx∑n=1Nh+1(1.8σn)Nxremove the rnth rule, reduce the dimensionality of EKF.end ifend if.Remark 1The significance of a neuron proposed in GAP-RBF is defined based on the average contribution of an individual neuron to the output of the RBF network. Under this definition, one may need to estimate the input distribution rangeS(X)=arn∑n=1Nh+1(1.8σn)Nx. However, the influence of a rule introduced in this paper is different from the significance of a neuron proposed in GAP-RBF. In fact, the influence of a neuron is defined as the relevant significance of the neuron compared to summation of significance of all the existing RBF neurons. As seen from Eq. (8), with the introduction of influence one need not estimate the input distribution range and the implementation has been simplified.Remark 2In parameter modification, SAFIS utilizes a winner rule strategy similar to the work done by Huang et al. The key idea of the winner rule strategy is that only the parameters related to the selected winner rule are updated by the EKF algorithm in every step. The ‘winner rule’ is defined as the rule that is closest (in the Euclidean distance sense) to the current input data as in. As a result, in SAFIS, a fast computation is achieved.Remark 3In SAFIS, some parameters need to be decided in advance according to the problems considered. They include the distance thresholds (ɛmax, ɛmin, γ), the overlap factor (n) for determining the width of the newly added rule, the growing threshold (eg) for a new rule and the pruning threshold (ep) for removing an insignificant rule. A general selection procedure for the predefined parameters is given as follows: max is set to around the upper bound of input variables; ɛmin is set to around 10% of ɛmax; γ is set to around 0.99. epis set to around 10% of eg. The overlap factor (n) is utilized to initialize the width of the newly added rule and chosen according to different problems. is suggested to be chosen in the range [1.0, 2.0]. The growing threshold egis chosen according to the system performance. The smaller the eg, the better the system performance, but the resulting system structure is more complex.The stable backpropagation (SBP) algorithm is developed with a new time-varying rate to guarantee its uniformly stability for online identification and its identification error converges to a small zone bounded by the uncertainty. The weights’ error is bounded by the initial weights’ error, i.e., overfitting is eliminated in the mentioned algorithm [44].The SBP algorithm is as follows [40]:(1)Obtain the output of the nonlinear system y(k) with equation (9). Note, that the nonlinear system may have the structure represented by equation (9); the parameter N is selected according to this nonlinear system.(9)y(k)=f[Xk]whereXk=x1(k)…,xi(k),…,xN(k)T=y(k−1),…,y(k−n),uk−1,…,uk−mT∈RN×1(N=n+m) is the input vector,u(k−1)∈Ris the input of the plant,y(k)∈Ris the output of the plant, and f is an unknown nonlinear function, f∈C∞.Select the following parameters; V1 and W1 as random numbers between 0 and 1; M as an integer number, and α0 as a positive value smaller or equal to 1; obtain the output of the NNyˆ(1)with the following equation.(10)yˆ(k)=VkΦk=∑j=1MVjkϕjkΦk=ϕ1k,…,ϕjk,…,ϕMkTϕjk=tanh(∑i=1NWijkxi(k))For each iteration k, obtain the output of the NNyˆ(k)with Eq. (10), obtain the identification error e(k) with Eq. (11), and update the parameters Vjk+1 and Wijk+1 with Eq. (12).(11)e(k)=yˆ(k)−y(k)(12)Vjk+1=Vjk−αkϕjke(k)Wijk+1=Wijk−αkσijke(k)where the new time varying rate αkis:αk=α02(1/2)+∑j=1Mϕjk2+∑j=1M∑i=1Nσijk2where i=1, …, N, j=1, …, M, σijk=Vjksech2(∑i=1NWijkxi(k))xi(k)∈R.Remark 4There are two conditions for applying this algorithm for nonlinear systems: the first one is that the nonlinear system may have the form described by (9), and the second one is that the uncertainty μ(k) may be bounded.Remark 5The value of the parameter used for the stability of the algorithmμ¯is unimportant, because this parameter is not used in the algorithm. The bound of μ(k) is needed to guarantee the stability of the algorithm, but it is not used in the BP algorithm(10)–(12).Remark 6The proposed NN has one hidden layer. It was reported in the literature that a feedforward neural network with one hidden layer is enough to approximate any nonlinear system.Remark 7Note that the behavior of the algorithm could be improved by changing the values of Morα0.An online self-organizing fuzzy modified least-square (SOFMLS) network has the ability to reorganize the model and adapt itself to a changing environment where both the structure and learning parameters are performed simultaneously. The stability of the mentioned algorithm is guaranteed and the bound for the average identification error is found.The SOFMLS algorithm is as follows [39]:(1)Select the following parameters: the parameter of the modified least square algorithm isR2>0∈R, the parameter of the clustering algorithm is0<r<1∈R, and the parameter of the pruning algorithm is du∈N, (L=L+ΔL, ΔL=5du).For the first data k=1 (where k is the number of iterations), M=1 (where M is the number of rules or clusters), the initial parameters of the modified least square algorithm areP1=cI∈R3M×3M(where0<c∈R),v1(1)=y(1),c1(1)=∑i=1Nxi(1)N, andw1(1)=rand∈(0,1)(v1is the initial parameter of the consequent part, c1 andw1are the centers and widths of the membership function of the antecedent part), and the initial parameter of the clustering and pruning algorithms is d1(1)=1 (where d is the density parameter).For the other data where k≥2, evaluate the fuzzy network parameters zj(k−1) and b(k−1) with (13), evaluate the output of the fuzzy networkyˆ(k−1)with (13)–(15), evaluate the identification error e(k−1) with (16), update the parameters of the modified least square algorithmvj(k), cj(k), andwj(k)with (17), and evaluate the parameter of the clustering and pruning algorithm p(k−1) with (18).(13)b(k−1)=∑j=1Mzj(k−1)zj(k−1)=exp−γj2(k−1)γj(k−1)=∑i=1Nwj(k−1)xi(k−1)−cj(k−1)N(14)ϕj(k−1)=zj(k−1)b(k−1)(15)yˆ(k−1)=∑j=1Mvj(k−1)ϕj(k−1)=VT(k−1)Φ(k−1)(16)e(k−1)=yˆ(k−1)−y(k−1)(17)θ(k)=θ(k−1)−1Qk−1PkBk−1e(k−1)Pk=Pk−1−1Rk−1Pk−1Bk−1Bk−1TPk−1(18)p(k−1)=max1≤j≤Mzj(k−1)The updating of the clustering algorithm is as follows:If p(k−1)≥r, then a rule is not generated, the winner rule j* is presented when zj(k−1)=p(k−1), the value of the densitydj*(k)of this rule is updated with (19). The winner rule is a rule that increments its importance in the algorithm. Go to 3.(19)dj*(k)=dj*(k)+1If p(k−1)<r, then a new rule is generated (M=M+1), wherer∈0,1, (e.g., the number of rules is increased by 1), the initial values of cM+1(k),wM+1(k),vM+1(k), and dM+1(k) are assigned to the new rule with (20), the missing parameters are added to havePk∈R3(M+1)×3(M+1)with diagonal elements (where Pk,vj(k), cj(k), andwj(k)are the parameters of the modified least square algorithm, dj(k) is the parameter of the density, j=1…M). Go to 3.(20)cM+1(k)=∑i=1Nxi(k)NwM+1(k)=∑i=1Nxi(k)−cj*(k)NvM+1(k)=y(k)dM+1(k)=1The updating of the pruning algorithm is as follows:For the case where k=L, the pruning algorithm works (the pruning algorithm is not active at each iteration) and evaluates the minimum density dmin(k) with (21), and L is updated as L=L+ΔL.(21)dmin(k)=min1≤j≤Mdj(k)If M≥2 and dmin(k)≤du, then this rule is pruned, where du∈N is the density threshold, and the looser rule j* is presented when dj(k)=dmin(k). The looser rule is the least important rule of the algorithm, the values of cj(k),wj(k),vj(k), and dj(k) are assigned with (22) and (23) to prune the looser rule j*, and in the same way, the values of Pkare assigned to prune the looser rule j* (where Pk,vj(k), cj(k), andwj(k)are the parameters of the modified least square algorithm and dj(k) is the density parameter, j=1…M), and M is updated as M=M−1 (e.g., the number of rules is decreased by 1). Go to 3.(22)cj−1(k)=cj(k)wj−1(k)=wj(k)vj−1(k)=vj(k)dj−1(k)=dj(k)(23)cM(k)=0wM(k)=0vM(k)=0dM(k)=0If dmin(k)>duor M=1, then this rule is not pruned. Go to 3.Remark 8The networks of many earlier studies, use membership functions, as shown in this study, and they also use the function γj(k−1). First, in the antecedent part of the networks of the aforementioned references, 2N parameters are used for each rule of the multidimensional membership functions, while in the antecedent part of the network used in this study, 2 parameters are used for each rule of the unidimensional membership functions (13). Second, the networks of the aforementioned references use 1/σij(k−1), which can cause singularity in online learning, while the network used in this study useswj(k−1)=1/σj(k−1)to avoid singularity. Some authors use the sum inference, product inference, or norm inference; however, in this study, the mean inference γj(k−1) (13) is used.Remark 9The idea to take the maximum of zj(k−1) as in (18) to obtain the winner rule is taken from the competitive learning of the adaptive resonance theory (ART) recurrent neural network (in the case of the ART network, the winner rule is the winner neuron).Remark 10In an earlier research, the second derivative of an objective function is used to find the unimportant rule. In this study, the density parameter is used to find the unimportant rule. In another study, the density as the population is considered, the population of each cluster is monitored, and if it amounts to less than 1% of the total data samples, the cluster is ignored at this iteration. The rule is ignored asvdmin(k)=0, and subsequently, this weight is ignored in the termyˆ(k−1)of (15). The cluster is ignored in the algorithm at this iteration, but the rule is not pruned; thus, the network cannot decrease. In other earlier work, two threshold parameters are considered: one for adding rules and the other for removing rules; however, they did not use the density parameter.Remark 11The parameter M (number of rules) is finite, because the algorithm adds the necessary rules and prunes the unnecessary rules to adapt itself to the changing environment. The number of rules M is changed by the clustering and pruning algorithms, and M changes only the dimension ofBk−1Tand θ(k−1); thus, the stability result is preserved.Remark 12The value of the parameter used for the stability of the algorithmμ¯is unimportant, because this parameter is not used in the algorithm. The bound of μ(k−1) is needed to guarantee the stability in the algorithm.Remark 13The parameters L and ΔL are needed in (21), because the pruning algorithm is not active at each iteration. The initial value of L is ΔL, and the pruning algorithm works at the first time when k=L, and consequently, L is increased by ΔL. The pruning algorithm works for each ΔL iterations. The parameter ΔL was determined empirically as 5du; thus, the pruning algorithm has only duas the designing parameter. Note that the behavior of the algorithm could be improved by changing the values of c, R2, r, or du.This section describes the characteristics of the brain and eye signals.The difference of the potential in one membrane is obtained by the exchange between the ions (Na+,Cl−,K+) being in the same. The neurons have a potential difference's between the inside and outside which is called rest potential, this potential represents constant changes because of the impulses given by the neighbor neurons [30,48]. This potential difference's can be measured in the brain cortex using electrodes which convert the ion flow in electric flow. The characteristic of the encephalography signal (EEG) is of 5–300μV in amplitude and of 0–150Hz in frequency [34,43].The EEG signals are waves similar to periodic but the waves can change from one time to other, and they have some characteristics which allow the modelling [14], [18], as are the amplitude, the frequency, the morphology, the band, the rhythm, and the duration [30,43].The following paragraphs show the characteristics which are considered for an adult in vigilance [30,43].Alpha signal. Is the normal rhythm of the bottom, is the most stable and typical in the human. It is found in the frequencies of 8–12±1Hz. The amplitude is between 20 and 60μV. It can be seen generally in posterior regions with more amplitude in the occipital lobes. See Fig. 2. It is more evident when the patient is awake with closed eyes and in physical and mental rest, it is stopped when the eyes are opened or with the mental activity [30,12].Beta signal. It is found in the frequencies >13Hz, in general between 14−35Hz. The amplitude is usually low, from 5−10μV and is symmetric [30], [12]. See Fig. 2.Theta signal. It has a frequency of 4–8Hz; is of half of low voltage, and is found in the temporal regions [30,12]. See Fig. 2.Delta signal. It is found in the second and the third stages of the dream. It has a frequency of 0.5–3.5Hz and the amplitude is generally higher than 75μV [30,12]. See Fig. 2.The electro-oculograms (EOG) are the signals obtained as a result of the eye movements of a patient and these EOG are detected using three electrodes, one electrode on the temple, one above and other underneath of the eye. Usually, the detected signals are by direct current (DC) coupling to specify the direction of the gaze. In the experiments of this paper, three electrodes are placed on the dominant side of the patient eye according to the optimum positions suggested by [15,42,49].Fig. 3shows the relationship between real eye movements (input) and the EOG signals (output) of the system. Denoted the upper and lower Thresholds of the vertical channel Ch.V as V1 and V2, respectively, and denote the upper and lower Thresholds of the horizontal channel Ch.H as H1 and H2, respectively. When the EOG potential exceeds one of these Thresholds, the output assumes ON, and when the EOG potential does not exceed one of these Thresholds, the output assumes OFF. The process of transforming the EOG signals from the intention of the patient is as follows [42,49]:1.Output Up is when it is obtained an Up behavior, first, Threshold V1 of the vertical channel becomes ON while Threshold V2 is OFF, second, Threshold V2 of the vertical channel becomes ON while Threshold V1 becomes OFF. H1 and H2 of the horizontal channel remain OFF all the time.Output Down is when it is obtained a Down behavior, first, Threshold V2 of the vertical channel becomes ON while Threshold V1 is OFF, second, Threshold V1 of the vertical channel becomes ON while Threshold V2 becomes OFF. H1 and H2 of the horizontal channel remain OFF all the time.In this section, the three above detailed algorithms are applied for the modelling of brain and eye signals. The aforementioned signals could be applied for patient who cannot move their bodies; consequently, they could use their brains or their eyes to say what they want or need. The SAFIS of [36], SOFMLS of [39], and SBP of [40] are compared for the modelling of brain signals in the example 1, and for the modelling of eye signals in the example 2. The root mean square error (RMSE) of [39,40,43,45], is used for the comparison results:(24)RMSE=1N∑k=1Ne2(k)12where e(k) is the learning error of (3)–(11).Consider real data of brain signals [43] where 5528 pairs (u(k), y(k)) of 5.528s are used for the learning, 1844 pairs (u(k), y(k)) for 1.844 s are used for the testing. The alpha signal is obtained in this study because it has more probabilities to be found. The acquisition system is applied with a 28 years old healthy man when its eyes are closed. The inputs of all the intelligent systems are y(k), y(k+1), y(k+2), y(k+3), and the output of the intelligent systems is y(k+4).Considering the remark 3, the parameters for the SAFIS algorithm [36] are Nx=4, γ=0.997, ɛmax=2, n=2, ɛmin=0.2, eg=0.03, ep=0.003. Considering the remark 7, the parameters of the SBP algorithm [40] are N=4, M=4, α0=0.25. Considering the remark 13, the parameters of the SOFMLS algorithm [39] are N=4,P1=cI∈R3x3, where c=1, R2=0.1, r=0.973, and du=6.Fig. 4shows the comparison results for the learning of the three algorithms. Fig. 5gives the illustration of the rule (neuron) evolution for the three algorithms during learning. Fig. 6shows the comparison results for the testing of the three algorithms. Table 1shows the RMSE comparison results for the algorithms using (24).From Figs. 4–6, and Table 1, it can be seen that the SOFMLS presents the smallest learning and testing RMSE, the SAFIS presents the biggest learning RMSE, the SBP presents the biggest testing RMSE, the SOFMLS and SBP give the smallest number of neurons, and the SAFIS gives the biggest number of neurons.Consider real data of eye signals of the up behavior [42] where 3572 pairs (u(k), y(k)) of 3.572s are used for the learning, 1192 pairs (u(k), y(k)) for 1.192s are used for the testing. The up signals are used in this paper. The acquisition system is applied with a 25 years old healthy man when its eyes are moving two electrodes are used to find the signals as described in the aforementioned section. The inputs of all the intelligent systems are y(k), y(k+1), y(k+2), y(k+3), and the output of the intelligent systems is y(k+4).Considering the remark 3, the parameters for the SAFIS [36] are Nx=4, γ=0.986, ɛmax=0.1, n=2, ɛmin=0.01, eg=0.01, ep=0.001. Considering the remark 7, the parameters of the SBP [40] are N=4, M=3, α0=0.25. Considering the remark 13, the parameters of the SOFMLS [39] are N=4,P1=cI∈R3x3, where c=1, R2=0.1, r=0.973, and du=6.Fig. 7shows the comparison results for the learning of the three algorithms. Fig. 8gives the illustration of the rule (neuron) evolution for the three algorithms during learning. Fig. 9shows the comparison results for the testing of the three algorithms. Table 2shows the RMSE comparison results for the algorithms using (24).From Figs. 7–9, and Table 2, it can be seen that the SOFMLS presents the smallest testing RMSE, the SBP presents the smallest learning RMSE, the SAFIS presents the biggest learning RMSE, the SBP presents the biggest testing RMSE, the SOFMLS gives the smallest number of neurons, and the SAFIS gives the biggest number of neurons.Consider real data of brain signals [43] where 5528 pairs (u(k), y(k)) of 5.528s are used for the learning, 1844 pairs (u(k), y(k)) for 1.844s are used for the testing. The alpha signal is obtained in this study because it has more probabilities to be found. The acquisition system is applied with a 28 years old healthy man when its eyes are closed. The inputs of all the intelligent systems are y(k), y(k+1), y(k+2), y(k+3), and the output of the intelligent systems is y(k+4).Considering the remark 3, the parameters for the SAFIS algorithm [36] are Nx=4, γ=0.99, ɛmax=1, n=2, ɛmin=0.1, eg=0.01, ep=0.001. Considering the remark 7, the parameters of the SBP algorithm of [40] are N=4, M=3, α0=0.5. Considering the remark 13, the parameters of the SOFMLS algorithm [39] are N=4,P1=cI∈R3x3, where c=1, R2=0.05, r=0.93, and du=6.Fig. 10shows the comparison results for the learning of the three algorithms. Fig. 11gives the illustration of the rule (neuron) evolution for the three algorithms during learning. Fig. 12shows the comparison results for the testing of the three algorithms. Table 3shows the RMSE comparison results for the algorithms using (24).From Figs. 10–12, and Table 3, it can be seen that the SOFMLS presents the smallest learning and testing RMSE, the SAFIS presents the biggest learning RMSE, the SBP presents the biggest testing RMSE, the SOFMLS gives the smallest number of neurons, and the SAFIS gives the biggest number of neurons.Consider real data of eye signals of the down behavior [42] where 3572 pairs (u(k), y(k)) of 3.572s are used for the learning, 1192 pairs (u(k), y(k)) for 1.192s are used for the testing. The up signals are used in this paper. The acquisition system is applied with a 25 years old healthy man when its eyes are moving two electrodes are used to find the signals as described in the aforementioned section. The inputs of all the intelligent systems are y(k), y(k+1), y(k+2), y(k+3), and the output of the intelligent systems is y(k+4).Considering the remark 3, the parameters for the SAFIS [36] are Nx=4, γ=0.99, ɛmax=0.1, n=2, ɛmin=0.01, eg=0.01, ep=0.001. Considering the remark 7, the parameters of the SBP [40] are N=4, M=3, α0=0.5. Considering the remark 13, the parameters of the SOFMLS [39] are N=4,P1=cI∈R3x3, where c=1, R2=0.05, r=0.96, and du=6.Fig. 13shows the comparison results for the learning of the three algorithms. Fig. 14gives the illustration of the rule (neuron) evolution for the three algorithms during learning. Fig. 15shows the comparison results for the testing of the three algorithms. Table 2 shows the RMSE comparison results for the algorithms using (24).From Figs. 13–15, and Table 4, it can be seen that the SOFMLS presents the smallest learning and testing RMSE, the SAFIS presents the biggest learning RMSE, the SBP presents the biggest testing RMSE, the SOFMLS and SBP give the smallest number of neurons, and the SAFIS gives the biggest number of neurons.Remark 14In the simulations, selecting different parameters for the algorithms of each example, the results present small variations.Remark 15The SAFIS algorithm is applied in two synthetic examples and in the Makey-Glass time series prediction problem [36]. The SBP algorithm is applied in a synthetic example and in the prediction of the loads distribution in a warehouse [40]. The SOFMLS algorithm is applied in two synthetic examples and in the Box-Jenkins furnace. This study is novel because it shows that the three algorithms can be used for the modelling of other different kind of systems which are the real brain and eye signals.Remark 16There is not a winner algorithm because the assumed tuning parameters for each method play their important role.

@&#CONCLUSIONS@&#
