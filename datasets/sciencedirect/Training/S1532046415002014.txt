@&#MAIN-TITLE@&#
Textual inference for eligibility criteria resolution in clinical trials

@&#HIGHLIGHTS@&#
Manual chart review for clinical trial screening is a laborious process.We present the task of identifying criteria relevant sentences in clinical notes.This is formulated as a search-based textual entailment problem.We describe creation of a dataset and implementation of four baseline methods.Method using similarity between UMLS concept pairs performs the best.

@&#KEYPHRASES@&#
Natural language processing,Electronic health records,Textual inference,Clinical trials,

@&#ABSTRACT@&#
Clinical trials are essential for determining whether new interventions are effective. In order to determine the eligibility of patients to enroll into these trials, clinical trial coordinators often perform a manual review of clinical notes in the electronic health record of patients. This is a very time-consuming and exhausting task. Efforts in this process can be expedited if these coordinators are directed toward specific parts of the text that are relevant for eligibility determination. In this study, we describe the creation of a dataset that can be used to evaluate automated methods capable of identifying sentences in a note that are relevant for screening a patient’s eligibility in clinical trials. Using this dataset, we also present results for four simple methods in natural language processing that can be used to automate this task. We found that this is a challenging task (maximum F-score=26.25), but it is a promising direction for further research.

@&#INTRODUCTION@&#
Clinical trials play an important role in medical research. The successful completion of a trial is dependent on achieving a significant sample size of patients enrolled into the trial within a limited time period. However, the process of eligibility determination is extremely challenging and time-consuming, often mandating manual chart review [1,2]. Such reviews can involve repeated readings of the patients’ electronic health record (EHR) for multiple trials, across every visit. This limits the number of patients that can be evaluated. Although institutions participating in clinical trials spend significant resources in conducting eligibility screening, the cost of screening is generally not fully compensated through the contracts supporting the trials [3].The eligibility requirements of a patient, for a clinical trial, are specified in the form of inclusion and exclusion criteria. These are detailed descriptions of the characteristics a patient must or must not have in order to participate in the trial. Patients can be pre-screened for eligibility by referring to either structured or unstructured data in the EHR, or a combination of both. While structured data such as diagnosis codes, laboratory results, medication orders, procedure information, and problem lists are useful for eligibility determination, clinical notes still remain the preferred means of documentation for physicians [4]. These notes frequently contain nuances of clinical presentation and care that are critical for making an eligibility screening determination that the structured data does not. Moreover, the criteria are specified in natural language. Hence, not all criteria can be translated into queries that leverage structured data. Köpcke et al. [5] found that there was a significant gap (65%) between the structured data documented for patient care and the data required for eligibility assessment.Thus, the use of clinical notes from the EHR is imperative for eligibility determination and clinical trial coordinators often undertake lengthy reviews of the EHR, specifically clinical notes, to assist with determining patient eligibility. Since this process is expensive in terms of time and effort, it would be desirable to speed up this step in the eligibility screening workflow. The NLP community has developed techniques for problems such as question answering [6], textual entailment [7], textual inference [8] and information retrieval [9], which can be used for identifying text of interest from the entire document.In this paper, we describe the creation of a dataset and evaluate baseline methods for identifying text in clinical notes that is pertinent to a given eligibility criterion for a trial. We compare simple methods in natural language processing (NLP) that can identify specific sentences in clinical notes that are relevant to a trial’s eligibility criteria. Such methods may be used to direct trial coordinators to specific parts of the notes, making the process of manual review easier, in the future. Recently, Köpcke et al. [10] reviewed 79 Clinical Trial Recruitment Support Systems (CTRSS) across 101 publications and found that success of a CTRSS depends on its successful workflow integration, rather than on sophisticated reasoning and processing algorithms. Although it needs formal validation, we believe that our proposed approach can be incorporated into the existing workflow seamlessly and will therefore help expedite the eligibility determination process.In order to evaluate the performance of an automated system that can identify text relevant to eligibility criteria in clinical notes, we created a new gold standard dataset, as outlined below. The 2014 i2b2 challenge [11] had two tracks with a training set of 790 notes, with annotations for protected health information and risk of heart disease, respectively. The third track evaluated the usability of systems developed for the i2b2 challenges. The fourth track allowed participants to demonstrate novel use of the data made available through the challenge. This work was a submission to this fourth track, proposing the use of NLP for expediting clinical trial eligibility screening. Since a limited amount of time was available to attempt the challenge, 80 notes were annotated for four criteria (20 notes per criterion) by our team of annotators (described in Section 2.3). Every annotator marked all full sentences in a note that were relevant to an eligibility criterion. These annotations are similar to other NLP shared tasks [7,12] such as the Recognizing Textual Entailment (RTE) challenges in the open domain. Therefore, systems developed for these annotations can leverage NLP research from such shared tasks for a challenging domain-specific real world problem and identify interesting research questions. There is at present no other publicly available dataset in the clinical domain with such annotations.The National Library of Medicine and the National Institutes of Health maintain the website www.clinicaltrials.gov, a registry and a database of publicly and privately supported clinical studies across the globe. The 2014 i2b2 challenge provided notes for coronary artery disease (CAD) patients from three cohorts: (1) patients with no CAD, (2) patients who developed CAD over the course of provided notes for that patient, and (3) patients who had CAD from beginning of their record. Therefore, we downloaded from clinicaltrials.gov study record data for all trials associated with the search term “coronary artery disease” as XML files (5054 trials in the May 2014 download). We extracted eligibility criteria for these trials and segmented them into individual sentences using a combination of the Ling-pipe [13] sentence chunking module trained on MEDLINE biomedical abstracts and user defined rules to identify sentence boundaries. Each sentence was assumed to be a single criterion. The first author (CS) and the physician (CH) in the team considered the following factors while selecting the criteria: (1) the resolution of the criterion would require a healthcare professional to refer to the notes of the patient, (2) it would be hard to assess patient eligibility for the criterion by referring only to the structured data associated with the patient’s EHR, and (3) the criterion is commonly used across clinical trials for CAD. The criteria were selected to reflect realistic situations where clinical trial coordinators spend substantial time reading notes to assess eligibility, and would benefit from a NLP system such as the one proposed in this study.The segmented criteria sentences described above were analyzed using MetaMap [14] to identify concepts from the Unified Medical Language System (UMLS). Ignoring concepts such as “patient,” “study,” “trials,” and “subject,” we found that medical history (C0262926) and lesion (C0221198) were the most common concepts in these criteria. We chose one eligibility criterion associated with each of these two concepts (Criteria 1 and 2). We also found that angina was the most common concept in the semantic category ‘Sign or Symptom’ across the eligibility criteria for CAD trials. It is a common practice among physicians to specify angina using grades as specified by the Canadian Cardiovascular Society Angina Grading Scale [15]. We chose classification of patients into Class 1 (Criterion 3) and Class 3 angina (Criterion 4) as the other two eligibility criteria in our study. These criteria are shown in Table 1.Many studies have used physicians to create gold standard datasets in the clinical domain. However, Raghavan et al. [16] showed that individuals with varying level of clinical expertise could also generate high quality annotations. Our annotation team consisted of two senior undergraduate nursing students, a second-year graduate-entry nursing student, and a physician. We developed detailed annotation guidelines for the annotators for each criterion. The annotation process was carried out using CLINical TrIals CrIteria ANnotator (CLINICIAN), a tool developed at our institution. CLINICIAN is a secure web-based tool and has a simple user interface.After having logged in, annotation for a single note has the following workflow: the user selects a criterion of choice (screenshot shown in Fig. 1a) to annotate notes for. This brings up a webpage (screenshot shown in Fig. 1b) that displays the criterion statement at the top and a note beneath it. The user first determines whether the note has any text that is relevant to the criterion and marks the note as “Not relevant” otherwise. If found to be relevant, the user highlights all complete sentences in the note that contribute toward determining, whether the patient meets the criterion, based on that particular note. After selecting all relevant sentences, the user clicks on one of the three buttons “Yes,” “No,” or “Maybe,” indicating an assessment of whether the patient meets the criterion. This brings up the next note and the user repeats the workflow. Table 2summarizes the distribution of these annotations in terms of mean (μ) and standard deviation (σ) across 80 notes in the dataset and the appendix lists the details.In order to ensure that these guidelines covered all cases, we carried out four training rounds. Each round involved annotations of ten notes per criterion (40 in total). All four annotators worked on the same set of 40 notes. This was followed by a discussion about the correctness of these annotations. Decisions associated with correctness of annotations were made through group discussions led by the physician. The annotation guidelines were updated after every training round to restrict differences amongst annotators. After completing the training, we calculated reliability of agreement amongst the four annotators based on annotations obtained for 20 notes per criterion (80 in total). We ensured that notes from the training rounds were not used in the agreement testing set. We used Fleiss’ kappa [17] as a metric of agreement, since more than two annotators were involved.We calculated three metrics of agreement. As outlined in the previous section, if a note was found to be relevant, it was annotated for criterion eligibility. Thus, every note could be relevant (“Yes,” “No,” “Maybe”) or “Not Relevant.” The first metric calculated agreement on whether a note was relevant. The second metric calculated agreement for assessment of criterion eligibility criteria across all four answers, “Yes,” “No,” “Maybe,” and “Not Relevant.” All notes were segmented into constituent sentences using the Ling-pipe [13] sentence chunking module trained on MEDLINE biomedical abstracts. We calculated agreement on sentences found to be relevant by the annotators. A sentence was considered relevant only if the annotator highlighted it. We can observe in Table 3that the annotators achieve high agreement considering overall performance across all three metrics.

@&#CONCLUSIONS@&#
