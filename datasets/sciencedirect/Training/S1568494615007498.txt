@&#MAIN-TITLE@&#
Fuzzy inference-based fall detection using kinect and body-worn accelerometer

@&#HIGHLIGHTS@&#
A new approach for reliable fall detection.In case of potential fall a threshold-based algorithm launches the fuzzy system to authenticate the fall event. The fuzzy system consists of two input Mamdani engines and a triggering alert Sugeno engine.The output of the first engine is a fuzzy set, which assigns grades of membership to the possible values of dynamic transitions, whereas the output of the second one is another fuzzy set assigning membership grades to possible body poses.Since the Mamdani engines perform fuzzy reasoning on disjoint subsets of the linguistic variables, the total number of the fuzzy rules needed for input–output mapping is far smaller.

@&#KEYPHRASES@&#
Human activity analysis,Fuzzy logic,Fall detection,

@&#ABSTRACT@&#
In this paper, we present a new approach for reliable fall detection. The fuzzy system consists of two input Mamdani engines and a triggering alert Sugeno engine. The output of the first Mamdani engine is a fuzzy set, which assigns grades of membership to the possible values of dynamic transitions, whereas the output of the second one is another fuzzy set assigning membership grades to possible body poses. Since Mamdani engines perform fuzzy reasoning on disjoint subsets of the linguistic variables, the total number of the fuzzy rules needed for input–output mapping is far smaller. The person pose is determined on the basis of depth maps, whereas the pose transitions are inferred using both depth maps and the accelerations acquired by a body worn inertial sensor. In case of potential fall a threshold-based algorithm launches the fuzzy system to authenticate the fall event. Using the accelerometric data we determine the moment of the impact, which in turn helps us to calculate the pose transitions. To the best of our knowledge, this is a new application of fuzzy logic in a novel approach to modeling and reliable low cost detecting of falls.

@&#INTRODUCTION@&#
Human behavior understanding is becoming one of the most active and extensive research topics in artificial intelligence and cognitive sciences. Automatic activity recognition is a process the objective of which is to interpret the behavior of the observed entities in order to generate a description of the recognized events or to raise an alarm. The capture of data associated to these entities can be achieved by sensors such as cameras that collect images of a specific scene, or inertial sensors that measure physical quantities of the moving object regardless of illumination or scene clutter. One of the biggest challenges in decision making about the alarm on the basis of such sensor readings is to cope with uncertainty, complexity, unpredictability and ambiguity [1,2].Traditional machine learning techniques pose some limitations in modeling human behavior due to the lack of any reference to the inherent uncertainty that human decision-making has. On the other hand, fuzzy logic poses the ability to imitate the human way of thinking to effectively utilize modes of reasoning that are rough rather than exact. With fuzzy logic we can indicate mapping rules in terms of linguistically understandable variables rather than numbers. Processing the words gives us the opportunity to express imprecision, uncertainty, partial truth and tolerance [3]. In consequence, fuzzy logic-based inference engines are capable of achieving robustness and close resemblance with human-like decision making in ambiguous situations.Recognition and monitoring of activities of daily living (ADLs) is important ingredient of human behavior understanding [4–6]. Several approaches have been proposed to distinguish between activities of daily living and falls [7–10]. Falls are a major health risk and a significant obstacle to independent living of the seniors and therefore significant work has been devoted to ensure robustness of assistive devices [11]. However, regardless of a lot of efforts undertaken to obtain reliable and unobtrusive fall detection, current technology does not meet the seniors’ needs. The main cause for not accepting of currently available technology by elderly is that the existing devices generate too much false alarms. In consequence, some ADLs are mistakenly indicated as falls, which in turn leads to substantial frustration of the users of such devices.The most common method for fall detection consists in using a body-worn tri-axial accelerometer and proving whether acceleration's amplitude crosses a fixed threshold [12]. Typically, such algorithms distinguish poorly between activities of daily living and falls, and none of which is commonly accepted by elderly. The main reason of poor separability and high false ratio of devices using only accelerators is lack of adaptability together with insufficient capabilities of context understanding. In order to improve the recognition performance the use of both accelerometer and gyroscope has been investigated by several research groups [13,14]. However, it is not easy to achieve low false alarm ratio since several ADLs like quickly lying down on a bed share similar kinematic motion patterns together with real falls. It is worth noting that despite of several shortcomings of the wearable devices for fall detection, the discussed technology has a great potential to provide support for seniors. It is also the only technology that was successfully used in large scale collection of motion patterns for research in the field of fall detection. Nowadays wearable devices like smart watches, which are frequently equipped with miniature inertial sensors, are unobtrusive and can deliver motion data during dressing up, bath as well as standing up the bed, i.e. during critical phases, in which considerable number of accidental falls and injuries take place.Several methods have been developed so far to detect falls using various kinds of video cameras [15,2,16]. In general, video-based fall detection systems show some potential and reliability in detecting falls in public places. However, in home environments the RGB cameras are less useful since they do not preserve privacy. Moreover, video camera-based algorithms cannot extract the object of interest all time of the day, especially in dark rooms. As indicated in [2], such algorithms only work in normal illumination conditions, whereas the fall risk of adults is much larger in low lighting conditions. For that reason, in [15,2] in order to recognize different activities in various environments, both controlled as well as unstructured, an infrared illumination was utilized to enable the web cameras to deliver images of sufficient quality in poor lighting conditions. It is also worth noting that the currently available prototype devices require time for installation, camera calibration and they are not cheap since a considerable computational power is needed to execute in real-time the time consuming algorithms. While these techniques might give good results in several scenarios, in order to be practically applied they must be adapted to non-controlled environments in which neither the lighting nor the subject undergoing tracking is under full supervision. Additionally, the lack of depth information can lead to lots of false alarms.Recently, Microsoft introduced the Kinect sensor, which delivers dense depth maps under difficult lighting conditions. This motion sensing device features an RGB camera and depth sensor, which consists of an infrared projector combined with a monochrome CMOS sensor capturing 3D data under any ambient light conditions with not direct natural illumination. The depth information is then utilized to estimate a skeletal model of any humans in Kinect's view using a Random Forest classifier, which assigns each pixel as being a body part or background [17]. The RGB camera data is not used for this due to its high variability in poor lighting conditions. Pixels corresponding to each body part are then clustered and fitted to a skeletal model on a frame-by-frame basis. The Kinect has strong potential in human behavior recognition in a wide range of illuminations, which occur during a typical 24-hour day-night cycle. Depth information is very important cue since the entities may not have consistent color and texture but they must occupy an integrated region in the 3D space. In particular, owing to this property the person can be reliably extracted at low computational cost.In the area of fall detection, the Kinect sensor has been introduced quite recently [18,14,10]. In [18], an overall fall detection success rate of 98.7% has been obtained using the distance of gravity center to floor level and velocity of a moving body. In [14], we presented a method for fusing the features extracted on the depth maps with data from inertial sensors. A Takagi-Sugeno (TS) fuzzy inference system has been used to trigger a fall alarm using the information about person's motion and the distance of gravity center to the floor. The discussed methods are resistant to changes of light conditions since they utilize the center of the gravity, which is extracted on depth maps only. As demonstrated in recent work, fusion of depth camera and body-worn inertial sensors can improve recognition of human activities [19] as well as detection of nocturnal epileptic seizure [20]. However, as indicated in [21], although there already exist several methods to detect human activities based either on wearable sensors or on cameras, there is little work that is devoted to combining the two modalities. In [22], a two-stage system is used for fall detection. The first stage is responsible for characterizing the vertical state of a segmented 3D object for individual frames, and events through a temporal segmentation of the vertical state time series of the tracked 3D objects. In the second stage the system employs an ensemble of decision trees and the features extracted from an on ground event to acknowledge belief that a fall preceded it. Overall, the depth information is very advantageous in real-time tracking of human faces [23], since the head trajectories resulting from the tracking are very useful in behavior recognition, particularly in fall detection [24].In our previous work [14], the acceleration data from the accelerometer, the angular velocity data from the gyroscope, and the center of gravity data of a moving person that was determined on the basis of Kinect depth maps were used as inputs in a fuzzy inference module to fuse information and then to generate alarms when falls occurred. The work described in here builds on that previous research by rebuilding the fuzzy engine and extending it about new modules. The system uses an accelerometer and depth data. Instead of using a single inference module, we utilize two input fuzzy inference engines, which are responsible for inferring separately the static poses and dynamic transitions, and which outputs are used by the output fuzzy engine, enhancing the confidence of the inference. In consequence, multiple independent measurements and the corresponding features are authenticated by the measurements of the another sensor as well as different features. This way a cooperative arrangement emerges and confidence of the fall alert is enhanced. This is because static actions such as lying on the floor are different from dynamic transitions, and in particular different measurements are needed to describe them. As a result, considerably smaller number of rules is required to describe how the Fuzzy Inference System (FIS) should make a decision regarding classifying the input data.To overcome difficulties related to the high dimensional input spaces, an idea of hierarchical fuzzy systems has been proposed in [25]. Based on discussed linear hierarchical structure, in which the number of the rules increases linearly with the number of the input variables, it has been proven in [26] that hierarchical fuzzy systems are universal approximators. However, such hierarchical fuzzy structures can become universal approximators only when there is sufficient number of free parameters. In [27] it has been proven that universal approximation property can be obtained by increasing the number of hierarchical levels. Our approach differs from the above approaches since in the input level we utilize fuzzy reasoning on disjoint subsets of the linguistic variables, which express different modalities of the observations. In contrast to these hierarchical models as well as hierarchical structure of rules [28], our output engine does not operate on raw input variable(s), but it only operates on fuzzy sets and the membership grades inferred at the preceding level of knowledge extraction.The next contribution of this paper is employing the information about the time of impact to extract very informative temporal features. Thanks to relatively high sampling rate of the accelerometer the system precisely determines the time instants characterizing the fall, which are then used to calculate depth map-based temporal features. We demonstrate experimentally that such features are very informative as well as that fusion and manipulation of linguistic variables and rules is easy. Our modular system reduces cost and has flexibility, increased system reliability and good scalability. The subsequent contribution is our fuzzy architecture with reduced power consumption. Owing to combination of crisp and fuzzy relations, i.e. alerts produced by the accelerometer in case of rapid motion, there is no need to perform fuzzy inference frame-by-frame. Instead, we collect the depth maps in a circular buffer and process them if there is evidence of a fall. Reduced power consumption was a key feature to be incorporated into the system design. The resulting easy-to-install fall detection system is unobtrusive and preserves privacy, and operates all time of the day.The contribution of this work is a fuzzy inference system consisting of fuzzy inference subsystems, which are responsible for drawing conclusions about the static poses and dynamic transitions. The accelerometer filters at low computational cost slow motions and gives time stamps at which depth-based features describing rapid motion should be calculated. The proposed linguistically understandable classifiers can be generalized to other applications especially when sensor fusion is involved. The proposed method is general and can be used to fuse heterogeneous sensors.Surprisingly from our findings, there is very minimal or almost no research that had tackled the problem of activity recognition on the basis of different modalities using the fuzzy approach. Fuzzy logic has been used in several systems for human fall detection. In [29], a fuzzy logic-based posture recognition method identifies four static postures with an accuracy of 74.29% on 62 video sequences. The algorithms can detect emergency situations such as a fall within a health smart home. An approach [30] uses fuzzy logic to generate on ground, in between, and upright state confidences from the body orientation and the spine height features. The confidences are then thresholded to trigger a fall alarm. With one false alarm the fall detection accuracy of 98.6% was reported on a set of 40 falls and 32 non-falls collected in a laboratory setting. The method relies on skeletal joint data, which can be extracted by Microsoft Kinect SDK/OpenNI. However, as indicated in [22], the software for skeleton extraction has a limited range of the skeletal tracking, approximately 1.5–4m from the motion sensor. Such range is insufficient to capture falls in many areas of typical senior rooms. Moreover, in many typical ADLs the Kinect has difficulties in tracking all joints [31]. Thus, recent systems for reliable fall detection [14,2,32] do not take into account the Kinect RGB images and only rely on depth maps to delineate the person(s).The remaining part of this paper is organized as follows. In Section 2, we outline the architecture and main ingredients of the fuzzy system. Section 3 is devoted to presentation of data processing. In Section 4, we discuss descriptors that are used to distinguish between falls and daily activities. Section 5 presents the fuzzy system. Experimental results are discussed in Section 6. Section 7 provides concluding remarks.Although fuzzy inference has already been used to provide reliable representation of person falls, our approach differs significantly from the most significant work in this area [33,2] in several aspects. First of all, the final decision is taken on the basis of reasoning from a fuzzy knowledge and two linguistic variables, which are described by fuzzy sets, provided by two Mamdani-type fuzzy engines. Each engine is responsible for extracting different kinds of knowledge for later reasoning by a Sugeno-type fuzzy inference engine, which provides a crisp decision on either fall on no-fall. The first fuzzy engine performs reasoning about human pose, whereas the second one is responsible for reasoning about motion of the person. Secondly, two different sensors providing necessary redundancy are fused using fuzzy rules. Thirdly, the reasoning is done not for every frame, but it is executed only in case of a possible fall, which is detected with low computational cost through thresholding of the accelerometric data, see Fig. 1. The data needed to perform the inference about the speed of the pose transition are stored in a circular buffer, which delivers them to authenticate a possible fall event if necessary. Thanks to use of the disjoint linguistic variables in the first stage we reduced the computational overheads. Such a processing architecture has been designed to consume least amount energy while achieving reliable fall detection in real-time.At the beginning of this section, we discuss how the accelerometric data are used to trigger the processing of the depth maps. Afterwards, we present processing of depth data.On the basis of the data acquired by the IMU (Inertial Measurement Unit) device the algorithm indicates a potential fall. The decision is taken on the basis of the thresholded total sum vector SVtotal, which is calculated from the sampled data in the following manner:(1)SVtotal(t)=Ax2(t)+Ay2(t)+Az2(t)where Ax(t), Ay(t), Az(t) is the acceleration in reference to the local x-, y-, and z-axes at time t, respectively. It contains both the dynamic and static acceleration components, and thus it is equal to 1g for standing.Fig. 2illustrates sample plots of acceleration change curves for falling along with daily activities like going down the stairs, picking up an object, and sitting down–standing up. As we can observe on the discussed plots, during the falling phase the acceleration attained the value of 6g, whereas during walking downstairs it attained the value of 3g. As we already mentioned, it is equal to 1g for standing. The sensor signals were acquired at a frequency of 256Hz and resolution of 12 bits. The data were acquired by x-IMU device [34], which was worn near the pelvis by a middle aged person. Such placement of the inertial device has been chosen since this body part represents the major component of body mass and undergoes movement in most activities.In practice, it is not easy to construct a reliable fall detector with almost null false alarms ratio using the inertial data only. Thus, our system employs a simple threshold-based detection of falls, which are then verified through fuzzy inference on both depth and accelerometric data. The critical issue in threshold-based approach is the selection of a appropriate threshold since if the value is too high the system (having sensitivity < 100%) might miss some real falls but never triggers false alarms (with 100% specificity), while if the threshold value is too low the system will detect all actual falls (100% sensitivity) but, at the same time, it may trigger some false alarms (specificity < 100%). Thus, choosing the threshold for accelerometric data to be utilized in a fall detector is a compromise between sensitivity and specificity. In our approach, if the value of SVtotalis greater than 3g then the system begins the extraction of the person and then executes fuzzy inference engine responsible for the final decision about the fall. Since the smallest acceleration measured from a fall is about 3g [35], the assumed threshold allows us to filter all falls for further depth-based authentication.The depth maps acquired by the Kinect sensor are continuously stored in a circular buffer. In a basic mode of person extraction, the depth image sequence from the circular buffer is utilized to extract a depth reference image, which is in turn employed to delineate the person. The extraction of the person is achieved through differencing the current depth image from such a depth reference image. In the current implementation the depth reference image is updated on-line, which makes possible to perform fall detection in dynamic scenes. Each pixel in the depth reference image assumes a temporal median value of the past depth values from the circular buffer. In the initialization stage the system collects a number of the depth images, and for each pixel it assembles a list of the pixel values from the former images, which is then sorted in order to determine the temporal median. Given the sorted lists of pixels the depth reference image of the scene can be updated quickly by removing the oldest pixels and updating the sorted lists with the pixels from the current depth image and then extracting the median value. For typical human motions and typical scene modifications, for instance, as a result of a movement of a chair, satisfactory results can be attained on the basis of fifteen depth maps. In order to avoid enclosure of the person into depth reference image, for example, if he/she is at standstill for a while, we take every fifteenth depth image acquired by the Kinect sensor. This means that for Kinect sensor acquiring the images at 30Hz, the depth reference image is entirely refreshed in 7.5s. The person is extracted with 30fps through differencing the current depth image from the depth reference image updated in such a way.In the basic mode of person extraction, in order to prevent disappearance of the person (on the binary image indicating the foreground objects) if he/she is not in motion for a while, i.e. to avoid assigning the person to the depth reference map, we perform updating of the depth reference image only when the person is in motion. The scene change can be inferred on the basis of differencing the consecutive depth maps, which is in fact the simplest method of motion detection.Since the person is the most important subject in the fall detection, we consider also motion data from the accelerometer to sense the person's movement and scene changes. When the person is at rest, the algorithm acquires new data. If the person is not at rest during assumed in advance period of time, the algorithm extracts the foreground and then it determines the connected components to decide if a scene change took place. In the case of the scene change, for example, if a new object appears in the scene, the algorithm updates the depth reference image. We assume that the scene change takes place, when two or more blobs of sufficient area appear in the foreground image. If no substantial scene change is detected then there is no necessity to update the scene reference depth map, and in such a case the algorithm acquires a new depth map. As mentioned in the previous subsection, the person is detected on the basis of depth images updated in such a way only if SVtotalexceeds the threshold. Prior to the person detection the system examines if the depth image has been updated on the basis of minimum ten consecutive images acquired before the fall, i.e. whether the depth image has been refreshed for the duration of five seconds preceding the beginning of fall. If not, the system takes the depth images from the circular buffer and updates the depth reference map. In order to cope with such circumstances we continuously store in the circular buffer 150 depth images. If the depth image has not been properly refreshed just before the fall the alarm is triggered with the delay.It is worth noting that the procedure responsible for extraction of the depth reference map can be replaced by a block executing another algorithm, for instance relying on the well-known mixture of distributions [36], which were recently used to delineate the person in a system for fall detection [22]. Fig. 3depicts sample depth maps and binary images with the extracted person. As already mentioned, in order to preserve the privacy of the user as well as to make the system ready to work any time, the RGB images corresponding to the depth maps are not acquired by our fall detection system.As illustrated in Fig. 4, in certain circumstances the algorithm presented above can oversegment the person. When the algorithm detects such an oversegmentation it switches from the basic mode of person extraction to region growing based person extraction. Algorithm 1 presents the person extraction in the second mode. The input of the function Person Ex R G is current depth image Dxy, a depth reference image Bxyand a circular buffer Qxyz. At the beginning, in the first call of the function, the algorithm determines the seed region for the region growing through differencing Bxyfrom the previous depth map, i.e. depth image acquired just before the person oversegmentation. A roi region surrounding the person and the segmented object is determined as well in order to restrict the processing area of the depth maps. In each call of the function, it extracts foreground Fxyand updates the roi. Afterwards, starting from the seed, it delineates the person blob and stores it in the image Pxy. The growing of the person region is executed until the blob area is smaller than a prespecified value or the depth/distance values are within a predefined range from the seed region. The values used in the stop condition are scaled regarding to the distance of the seed to the camera. Finally, given the delineated person, the discussed algorithm updates the seed region for the next call of the Region Growing.Algorithm 1Person extraction using region growingPrecondition:roi and seed are declared as static variables1:functionPxy=PersonExRG(Dxy, Bxy, Qxyz)2:[roi, seed]=init(Qxyz, Bxy)▷ called only once3:Fxy=|Dxy−Bxy|4:roi=ROI(Fxy, roi)5:Pxy=RegionGrowing(Dxy, roi, seed)6:seed=UpdateSeed(Pxy)7:end functionAs already mentioned, in the basic mode of person detection, the depth reference image is entirely refreshed in 7.5s. When the person delineation is done in the second mode the depth image should be updated as fast as possible. This has been achieved through updating the depth reference image in roi region on the basis of person-free areas. Such person-free areas can be calculated straightforwardly through differencing the maps Pxywith the extracted person from the current depth image Dxy, see 3rd line in Algorithm 2. Such maps, where the areas belonging to person assume zero values, are then pushed on the stack, see call of Sadd function. The function LastPixel extracts the most recent non-person pixel in the roi area. After the switch from the region growing-based person detection to the basic mode, the depth reference image in the roi area is replaced by the Bxymap. A sample movie at http://fenix.univ.rzeszow.pl/~mkepski/demo/personseg.mp4 compares the person extraction in both modes.Algorithm 2Update of the depth reference mapRequire:Sxyis declared as static variable1:function[Bxy,Dxy′]=UDRM(Dxy, Bxy, Qxyz, Pxy, roi)2:Sxy=init(Qxyz)▷ called only once3:Dxy′=Dxy(roi)−Pxy(roi)4:Sxy=Sadd(Dxy′,roi)5:Bxy=LastPixel(Sxy, roi)6:end functionThe switch from the basic mode of the person extraction to the mode based on the region growing is realized on the basis of the value of binary variable regionGrMode, see Algorithm 3. The simplest way to detect a connection of the person blob with another blob, i.e. to detect a situation in which the algorithm should switch from the basic mode to the second mode, is to compare the area of the current foreground with the area of the foreground in the previous map, see call of function Thresh. The region growing based mode should be finished if all pixels in the depth reference image Bxyare updated. This is achieved through summing the values of person-free areasDxy′in a binary image Lxy, see 7th line in Algorithm 3. Having on regard that inDxy′image the pixels belonging to person areas assume the value 0, the algorithm switches to the basic mode of person detection if all Lxypixels in the roi area have depth values different from zero.Algorithm 3Setting conditional variable regionGrModeRequire:LxyandFxy′are declared as static variables1:functionregionGrMode=RGM(Dxy, Bxy)2:Fxy′=init▷ called only once3:Fxy=|Dxy−Bxy|4:regionGrMode=Thresh(Fxy,Fxy′)5:Fxy′=Fxy6:end function7:functionregionGrMode=updateRGM(Dxy′,roi)8:Lxy=init()▷ called only once9:Lxy=OR(Lxy,Dxy′(roi))10:regionGrMode=IFstop(Lxy)11:end functionThe extraction of the person is executed only when the condition SVtotal>threshold is true. After the person extraction, the floor equation coefficients are uploaded to delineate the ground in the point cloud and then to extract features describing the activities. In particular, thanks to the floor extracted in advance, some point cloud features are extracted with regard to the floor. The depth and point cloud features are then employed to decide if a fall occurred. If the SVtotalis smaller or equal to the assumed threshold then new data from the accelerometer is acquired.In this section, we explain the extraction of the ground plane that is used in calculating the fall descriptors. The descriptors are discussed in the second part of the section.After the transformation of the depth pixels to the 3D point cloud, the ground plane described by the equation ax+by+cx+d=0 was recovered. Assuming that the optical axis of the Kinect camera is almost parallel to the floor, a subset of the points with the lowest altitude has been selected from the entire point cloud and then utilized in the plane estimation. The parameters a, b, c and d were estimated using the RANdom SAmple Consensus (RANSAC) algorithm. RANSAC is an iterative algorithm for estimating the parameters of a mathematical model from a set of observed data, which contains outliers [37]. The distance to the ground plane from the 3D centroid of point cloud corresponding to the segmented person has been determined on the basis of an expression for point–plane distance:(2)D(t)=|axc(t)+byc(t)+czc(t)+d|a2+b2+c2where xc, yc, zcstand for the coordinates of the person's centroid. The parameters should be re-estimated subsequent to each change of the Kinect location or orientation.The following features are extracted on the depth images in order to authenticate the fall hypotheses, and which are calculated if person's acceleration is above the preset threshold:•H/W – a ratio of height to width of the person's bounding box in the depth maps.H/Hmax– a proportion expressing the height of the person's surrounding box in the current frame to the physical height of the person, projected onto the depth image.D – the distance of the person's centroid to the floor.max(σx, σz) – largest standard deviation from the centroid for the abscissa and the applicate, respectively.Fig. 5depicts a person in depth images together with the H/W and H/Hmaxfeatures. In order to determine the box enclosing the person the algorithm seeks for the largest blob in the binary image representing the foreground objects. As we can observe on the discussed depth maps with graphically marked features, the depth features assume quite different values during an example fall event and typical daily activities like walking and sitting on a chair.The P40 descriptor is calculated on 3D point clouds. On the basis of the extracted person in the depth image a corresponding person's point cloud is determined in 3D space, see Fig. 6. Afterwards, a cuboid surrounding the person's point cloud is determined. Then, a sub-cuboid of 40cm height and placed on the floor is extracted within such a cuboid. Finally, a ratio of the number of the points contained within the cuboid of 40cm height to the number of the points being within the surrounding cuboid is calculated. The distance of each point to the floor is calculated on the basis of (2). The 40cm height of the cuboid has been chosen experimentally to include all 3D points belonging to a lying person on the floor.At the beginning of this sections we outline Mamdani and Takagi-Sugeno fuzzy modeling. Then we justify reasons why we use fuzzy reasoning to discriminate between daily activities and falls. Afterwards, we discuss the proposed fuzzy engine.Fuzzy inference systems have become one of the most well-known applications of fuzzy logic. One of the reasons for significant interest on fuzzy inference systems is the ability to express the behavior of the system in an interpretable way for humans as well as to incorporate human expert knowledge and intuition with all its nuances, since domain experts are able to determine the main trends of the most influential variables in the system. This is because expert rules are based on evidence from data, a priori knowledge as well as intuition along with large experience and expertise. In consequence, typically they present a high level of generalization. Moreover, such a representation is highly interpretable. Assuming there are enough rules, a collection of fuzzy rules can accurately represent arbitrary input–output mappings [38]. In general, as the complexity of a system increases, the usefulness of fuzzy logic as a modeling tool increases.A fuzzy inference system consists of three components: a rule-base, which contains a pool of fuzzy rules; a database of the membership functions used in the fuzzy rules; and a reasoning mechanism, which performs the inference. Two main types of fuzzy modeling schemes are the Mamdani and Takagi-Sugeno model [39]. In the Mamdani scheme each rule is represented by if−then conditional propositions [40]. A fuzzy system with two inputs x1 and x2 (antecedents) and one output y (consequent) is described by a collection of r conditional if−then propositions in the form:(3)ifx1isA1kandx2isA2kthenykisBk,fork=1,2,…,rwhereA1kandA2kare fuzzy sets representing the kth antecedent pairs and Bkis fuzzy set representing the kth consequent. If we adopt max and min as our choice for the T-conorm and T-norm operators, respectively, and use max–min composition, the aggregated output for the r rules is given as follows:(4)μBk(y)=maxk[min[μA1k(in(i)),μA2k(in(j))]],k=1,2,…,rFor max-product (or correlation-product) implication technique, the aggregated output for a set of disjunctive rules can be determined in the following manner:(5)μBk(y)=maxk[μA1k(in(i))·μA2k(in(j))],k=1,2,…,rwhere the inferred output of each rule is a fuzzy set scaled down by its firing strength via algebraic product. Such a truncation or scaling is conducted for each rule, and then the truncated or scaled membership functions from each rule are aggregated. In conjunctive system of rules, the rules are connected by and connectives, whereas in case of disjunctive system the rules are connected by the or connectives. This kind of fuzzy system is also called a linguistic model because both the antecedents and the consequents are expressed as linguistic constraints. As a consequence, the knowledge base in Mamdani FIS is easy to understand and to maintain. The model structure is manually designed and the final model is neither trained nor computationally optimized, even though some heuristic tuning of the fuzzy membership functions is common in practice. Mamdani's model expresses the output using fuzzy terms based on the provided rules. Since this approach is not exclusively reliant on a dataset, a model that presents a high level of generalization can be obtained even when a small amount of experimental data is in disposal. All the existing fuzzy systems that are used as universal approximators are Mamdani fuzzy systems.Takagi-Sugeno (TSK) fuzzy model consists of if−then rules that embody the fuzzy antecedents, and a mathematical function acting as the rule consequent part. A typical rule in a TSK model with two inputs x and y and output z, has the following form:(6)ifxisAandyisBthenz=f(x,y)where z=f(x, y) is a crisp function in the consequent. Typically f(x, y) is a polynomial function in the inputs x and y. In a TSK model each rule has a crisp output that is given by a function. As a result the overall output is determined via a weighted average defuzzification. It is a data driven approach in which the membership functions and rules are generated using an input–output data set. The Takagi-Sugeno model is typically constructed in two steps consisting of extracting the fuzzy rules and then optimizing the parameters of the linear regression models. The final output is a weighted average of a set of crisp values. The main difference between the two approaches lies in the consequent of fuzzy rules, since Mamdani fuzzy systems utilize fuzzy sets as rule consequent, whereas Takagi-Sugeno fuzzy systems employ linear functions of input variables as rule consequent. The first two stages of the fuzzy inference process, namely, fuzzification of the inputs and applying the fuzzy operator, are exactly the same. The main difference between them is that the Sugeno output membership functions are either linear or constant. Such a constant membership function gives us a zero-order Sugeno fuzzy model that can be viewed as a special case of the Mamdani FIS, in which each rule's consequent is specified by a fuzzy singleton.

@&#CONCLUSIONS@&#
We have demonstrated a flexible framework for combining different modalities in order to improve the fall detection. The features are extracted on both depth maps and accelerometric data and then used along with fuzzy inference to determine the state of the resident. In the proposed architecture an accelerometer is utilized to indicate an eventual fall. A fall hypothesis is then authenticated by a two-stage fuzzy system, which fuses depth maps and accelerometric data. The aim of the first stage, which is composed of two Mamdani engines, is to infer separately the static pose and dynamic transition. The second stage of Takagi-Sugeno engine provides a crisp decision on either fall or no-fall. This resulted in reduced computation complexity due to the disjoint linguistic variables at the first stage, and reduced computation cost due to extracting of the depth features only when a fall is likely to have just happened as indicated by the acceleration measurement values. The proposed linguistically understandable classifier can be generalized to other applications especially when sensor fusion is involved or human activity summarization is required. As demonstrated experimentally, it can be particularly useful in fall detection since frequently a reduced amount of training data is in disposal. We showed that fusion and manipulation of linguistic variables and rules is easy. We demonstrated experimentally that the proposed framework permits reliable and unobtrusive fall detection in real-time and at low computational cost.