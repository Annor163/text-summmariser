@&#MAIN-TITLE@&#
Modified binary PSO for feature selection using SVM applied to mortality prediction of septic patients

@&#HIGHLIGHTS@&#
Septic shock patients’ outcome prediction.Modified binary particle swarm optimization method.Feature selection with the simultaneous optimization of SVM kernel parameters.Improved results for sepsis outcome prediction.

@&#KEYPHRASES@&#
Feature selection,Wrapper methods,Particle swarm optimization,Premature convergence,Sepsis,Support vector machines,

@&#ABSTRACT@&#
This paper proposes a modified binary particle swarm optimization (MBPSO) method for feature selection with the simultaneous optimization of SVM kernel parameter setting, applied to mortality prediction in septic patients. An enhanced version of binary particle swarm optimization, designed to cope with premature convergence of the BPSO algorithm is proposed. MBPSO control the swarm variability using the velocity and the similarity between best swarm solutions. This paper uses support vector machines in a wrapper approach, where the kernel parameters are optimized at the same time. The approach is applied to predict the outcome (survived or deceased) of patients with septic shock. Further, MBPSO is tested in several benchmark datasets and is compared with other PSO based algorithms and genetic algorithms (GA). The experimental results showed that the proposed approach can correctly select the discriminating input features and also achieve high classification accuracy, specially when compared to other PSO based algorithms. When compared to GA, MBPSO is similar in terms of accuracy, but the subset solutions have less selected features.

@&#INTRODUCTION@&#
Advances in computer science and acquisition systems have made possible to easily collect and store large databases containing long time series. These databases have become increasingly frequent in distinct fields, including astronomy [1], molecular biology [2], finance [3], marketing [4] and health care [5], and are often used for purposes of data mining and knowledge discovery.Knowledge is only valuable when it can be used efficiently and effectively. Therefore, extensive research has been made in the urge to find new computational theories and tools, that can aid to the extraction of useful information (knowledge) from these rapidly growing databases. The field of science concerned with automated knowledge discovery is called knowledge discovery in databases (KDD) [6].Mathematical modeling is the description of a system using mathematical language. For complex and partly understood systems, nonlinear models based on artificial intelligence techniques can be used. Modeling may help in medical diagnosis; a good example is the prediction of sepsis outcome using learning machines. An usual drawback, encountered when modeling real systems, is noise and redundancy in data. Hence, it is very important, when preprocessing data, to choose the optimal feature subset.Feature selection (FS) or variable selection, is the technique of selecting a subset of relevant features (variables) for building more robust learning models. It not only reduces the processing cost, but also improves the model built from the selected data [7,8]. Thus, its use in healthcare modeling problems helps to prevent medical complications and even patient death. In feature selection the goals are to maximize the model performance and to minimize the number of used features. The solution is dependent on the chosen combination of features, making the FS problem NP-Hard. Metaheuristics such as particle swarm optimization (PSO) [9], evolutionary algorithms (EA) [10], or ant colony optimization (ACO) [11,12] have shown to be well suited for this kind of problems, due to their randomized nature [13]. They are able to find good solutions, without having to try all possible combinations.The medical condition studied in this paper is sepsis, a common clinical condition defined by a whole-body inflammatory state, called systemic inflammatory response syndrome (SIRS). This clinical condition has different degrees of severity that can lead to severe sepsis and later to septic shock. A patient is considered to be in septic shock if he/she has sepsis associated with arterial hypotension despite “adequate” fluid resuscitation [14]. This advanced stage of sepsis carries a high burden, which translates into a high mortality rate (about 50%) and high costs of treatments, when compared with other intensive care unit (ICU) patients [15,16]. It is now considered to be the most common cause of death in noncoronary critical care units. Approximately 150,000 persons die annually in Europe and more than 200,000 in the United States [17].This paper proposes a modified binary particle swarm optimization (MBPSO) method for feature selection with the simultaneous optimization of SVM kernel parameter setting, applied to mortality prediction in septic patients. An enhanced version of binary particle swarm optimization, designed to cope with premature convergence of the BPSO algorithm is proposed. MBPSO control the swarm variability using the velocity and the similarity between best swarm solutions. This paper uses support vector machines in a wrapper approach, where the kernel parameters are optimized at the same time. The MBPSO is used as a wrapper method, that is solely a feature selection method in which every candidate solution is evaluated using a learning machine [18], such as neural networks (NN), support vector machines (SVM) or fuzzy modeling (FM) [19]. The chosen classification technique, in this case, is support vector machines, once this method has universal function approximation properties [20]. Further, SVM are widely used in computational biology due to their high accuracy, their ability to deal with high-dimensional and large databases, and their flexibility in modeling diverse sources of data [21]. These characteristics are suitable to the problem proposed in this paper. Dealing with clinical data can be problematic, since the available data is usually high-dimensional and very heterogeneous, which means dealing with a large number of features and different types of data. In clinical databases, which is the environment where it is intended to apply the developed method, usually the higher the number of studied features the less the number of patients and samples available, once not all the patients have the same physiological variables registered. This means that the used number of features is always a trade-off between the number of features and the number of patients, which typically translates into tens of features.We will start by introducing the problem formulation for feature selection in Section 2, where the main concepts of support vector machines are also presented. Then, the classical binary PSO is presented in Section 3. In Section 4, the proposed MBPSO approach is described and its advantages are assessed using a group of benchmark databases. In Section 5, we will apply the MBPSO algorithm to the sepsis outcome prediction problem, and the conclusions are presented in Section 6.The KDD process comprises a series of steps to extract knowledge from data. The first step is selection and it consists of acquiring the most useful target data from the available databases. The target database has to be adequately chosen so that it contains sufficient information regarding the system we want to describe. The next two steps (feature construction and feature selection), are part of the feature extraction process and are used with the purpose of extracting the most relevant features of the target data. Feature construction (also called data preprocessing) [22], comprehends all the methods that involve some degree of modification to the original feature, e.g. data standardization, normalization and noise filtering. The objective of this crucial preprocessing step is to make the underlying information in data easier to identify. In opposition, feature selection does not induce a transformation to the features, it simply searches for the optimal feature subset discarding the features with lowest informative potential.There is a large number of available feature selection techniques, but there are three aspects that roughly differentiate them [22]:•feature subset generation (or search strategy);evaluation criterion definition (e.g. relevance index or predictive performance);evaluation criterion estimation (or assessment method).The first refers to the applied search strategy to evaluate the solutions in the space of possible feature combinations. The last two correspond to the evaluation criterion, i.e. the method and measures used to assess the quality of each feature subset. Based on the subset evaluation procedure we may divide feature selection algorithms into three classes, wrapper methods, embedded methods and filter methods[23].Wrappers use a search algorithm to search through the space of possible features and evaluate each subset by running a model on the subset. Wrappers can be computationally expensive and have a risk of over-fitting the model. Filters are similar to wrappers in the search approach, but instead of evaluating against a model, the features are selected by evaluating a performance measure that does not require building a model.In embedded feature selection methods, similarly to wrapper methods, feature selection is linked to the classification stage. This link is in this case much stronger, as feature selection in embedded methods is included into the classifier construction. Recursive partitioning methods for decision trees such as ID3, C4.5 and CART are examples of such methods.The main advantage of the wrapper method over embedded methods is a better coverage of the search space. And the main advantage of the wrapper method over filter methods is that in wrappers the predictive performance of the final selected subset is correlated with the chosen relevance measure, or in this case the classifier. When the objective is to obtain a model as accurate as possible and the time to obtain it is not an issue, then the wrapper method is an advantageous choice.The main characteristic of wrapper methodologies is the use of the predictor as part of the selection procedure. They use a learning machine to score the subsets according to their predictive performance [22]. Wrappers are constituted by three main components:1.Learning machine;Feature evaluation criteria;Search method.Wrapper approaches are aimed to improve the results of the specific predictors they work with. Nevertheless, wrapper methods have the associated problem of having to train a classifier for each tested feature subset. This means testing all the possible combinations of features will be virtually impossible and a search method is imperative. During the search, subsets are evaluated without incorporating knowledge about the specific structure of the classification or regression function [22].Many popular search approaches use greedy hill climbing, which iteratively evaluates a candidate subset of features, then modifies the subset and determines whether or not the new subset is an improvement over the old. Evaluation of the subsets requires a scoring metric that grades a subset of features. In the case of wrapper methods, the feature subsets are often evaluated by the accuracy of the produced model. Exhaustive search is generally impractical, so at some defined stopping point, the subset of features with the highest score discovered up to that point is selected as the satisfactory feature subset. The stopping criterion varies with the selected feature selection algorithm. Possible criteria include: a subset score exceeding a threshold, the maximum allowed run time of an algorithm has been surpassed, etc.In this work, support vector machines are used as a modeling tool to evaluate the subsets of features. Feature evaluation criteria is still an open discussion [22,24], as accuracy not always dictate a suitable performance score for a specific predictor, especially in medical applications, where in most cases the existing classes are unbalanced. The modeling technique and the used performance measures are described in the following. The search method is one of the focus issues in this paper and will be described in detail in Section 4.The sepsis problem is a very complex medical state, thus one of the main criteria for choosing a modeling technique is the capability of effectively representing highly nonlinear problems. Support vector machines is a method for learning separating functions into two-class classification problems [19]. The algorithm maps the nonlinear inputs to a high dimension feature space. In this feature space a linear classification surface is constructed. SVM are an easy to use classification technique even though users usually get unsatisfactory results at first due to poor parameter setup. The first version of SVM was introduced as a linear classifier. Given some training dataD, a set of n points of the form(1)D={(ui,yi)|ui∈ℝNt,yi∈{−1,1}}i=1nwhereuiare the input variables, yiis either 1 or −1, indicating the class to which the inputuibelongs, and Ntis the total number of available input variables. Eachuiis a Nt-dimensional real vector. We want to find the maximum margin hyperplane that divides the points having yi=1 from those having yi=−1. However, most classification problems can only be separated using a nonlinear classifier. The solution is to use a transformed feature spaceF, mapping observations from a general setDinto an inner product spaceF:(2)φ:ℝNt→F,u→φ(u),with φ being the nonlinear mapping function betweenℝNtandF. After this operation one works with the new database:(3)(φ(u1),y1),…,(φ(un),yn)∈F×Y.The discriminant function will be in the form:(4)f(u)=w,φ(u)+b,whereware the normal vectors to the hyperplanes and b the respective offsets. In the feature space,F, data is linearly separable. Further, when viewed in the original input spaceD, f is a nonlinear function if φ(u) is a nonlinear function. Using the Lagrangian formulation of the problem, the training data will only appear in the form of dot products between vectors. This is a crucial property which allows the generalization of the linear procedure to the nonlinear case [25]. The Lagrange formulation of the dual problem inFbecomes:(5)maximizeαLD=∑i=1nαi−12∑i=1n∑j=1nαiαjyiyj(φ(ui)·φ(uj))subjectto:0≤αi≤C,i=1,…,n,∑i=1nαiyi=0.where LDis the objective function of the Lagrangian formulation of the dual problem, and α are the Lagrange multipliers. A kernel function is defined as:(6)k(ui,uj)=φ(ui)·φ(uj).With this function, it is only necessary to have the feature vectorui, since the scalar product φ(ui)·φ(uj) is directly calculated by computing the kernel k(ui,uj).Then, the optimization model in (5) can be solved using the method for solving the optimization in the separable case. This paper uses the sequential minimal optimization (SMO) [26] to solve the QP problem of SVM. The optimal hyperplane has the form:(7)f(u,α*,b*)=∑i=1nyiαi*φ(ui),φ(u)+b*,=∑i=1nyiαi*k(ui,uj)+b*.Depending upon the applied kernel, the bias b can be implicitly part of the kernel function. Therefore, if a bias term can be accommodated within the kernel function, the nonlinear SVM classifier is given by:(8)f(u,α*,b*)=∑i=1nyiαi*φ(ui),φ(u)=∑i=1nyiαi*k(ui,uj).Some kernel functions include polynomial, radial basis function (RBF) and sigmoid kernel [25]. This paper uses the RBF kernel function (9) for the SVM classifier because this kernel function can analyze higher-dimensional data and only requires two parameters, C and γ[27].(9)k(ui,uj)=exp(−γ∥ui−uj∥2).In order to improve classification accuracy, these kernel parameters in the kernel functions should be properly set.In this work, feature selection is used with the purposes of improving modeling quality. The problem in study is a classification problem. The goals are the maximization of model performance and the minimization of the number of used features. The objective function is defined as a fitness function (as in EA), being our goal its maximization. The most suitable representation to the proposed task is [12]:(10)f(ui)=α1−P+(1−α)1−NfNt,where P is the classifier performance measure and Nfis the size of the tested feature subset. The term on the left side of the equation accounts for the overall accuracy and the term on the right for the percentage of used features. Note that both terms in the objective function are normalized. Constant α ∈[0, 1] define the weights of the related goal; performance and subset size.Traditionally, accuracy has been used to evaluate classifier performance. This measure is defined as the total number of good classifications over the total number of available examples. Usually most of the classification problems have two classes, positive and negative cases [28]. Thus, the classified test points can be divided into four categories that usually are represented in the well known confusion matrix: true positives (TP), true negatives (TN), false positives (FP) and false negatives (FN). Given the four categories of the confusion matrix, accuracy is defined as:(11)Accuracy=TP+TNTP+FP+TN+FN.This criterion is limited, especially if one of the classes is much larger than the other. With a unbalanced classification problem, misclassifications in the minority class will not have a large impact in the accuracy value. Further, a good classification of a class might be more important than classifying other classes, and this cannot be assessed with accuracy. To account for these issues, additionally two other performance measures, commonly used in binary classification problems, were considered [29]:(12)Sensitivity=TPTP+FN,(13)Specificity=TNFP+TN.Sensitivity is equal to the TP rate, i.e. the ratio of true positives that were identified in all the positive class samples. Analogously, specificity is the rate of TN. Sensitivity and specificity describe how well the classifier discriminates the positive and the negative classes, respectively.The problem of finding an optimal feature subset using the objective function (fitness function) in (10) is highly complex. Thus, metaheuristics are used to search the space of feature combinations.To solve this problem several search meta-heuristics have been proposed, as e.g. genetic algorithms (GA) [10], particle swarm (PSO) [9,27] and ant colony optimization (ACO) [12,30]. These methods are able to find fairly good solutions without searching the entire workspace. The feature selection technique proposed in this paper is a modified version of the binary PSO algorithm that optimize simultaneously the SVM model parameters.Particle swarm optimization is a simple metaheuristic with biological inspiration in swarming behavior of some species [11]. Contrarily to evolutionary algorithms, it does not include any genetic operators, which makes it simpler than EA and also reduces the number of parameters to adjust. It has been applied to very different problems such as flow shop [31], antenna design [32] or healthcare [13,28].The tendency to form swarms appears in many different organisms, for instance, in birds and fish. However, in a swarm there are no leaders, it is simply the result of the interaction between the behaviors of its individuals (local interactions) [33]. Swarming offers several advantages: protection against predators, more efficient reproduction (easier to find a mating partner), food search and gathering. Nevertheless, the search efficiency provided by swarming is what underlies particle swarm optimization (PSO) algorithms [11].Initially, particle swarm optimization have been developed for continuous problems. Later, it was also extended for discrete problems, which resulted in the commonly known binary PSO [11]. The binary version of particle swarm optimization has the tendency to prematurely converge as noted in [13,31,34], especially in more challenging optimization tasks. There are various approaches to cope with this problem: use the mutation operator from EA [13], reset the swarm best if the fitness stagnates [34] or using perturbation mechanisms [31]. More recently, an hybrid PSO have been proposed in [9], but this algorithm has the disadvantage of being computationally heavy, which is not desired for a feature selection wrapper approach. A BPSO for feature selection and parameter determination of SVM is proposed in [27]. This paper proposes a modified binary PSO with SVM parameter determination.In PSO, each particle (corresponding to an individual in EA) is a candidate solution of the optimization problem. A particle has a position and a velocity in the search space, where the method for updating the velocity depends on the particle itself and on the other particles. The first step when using a metaheuristic is to select the most suitable encoding scheme for the problem under study.The most common and generic encoding schemes are real and integer encoding. The use of each of them depends on the problem in hand. For the feature selection problem it is common to use a binary encoding. In this case, particles have their positions and velocities described by:(14)xj=(x1j,x2j,…,xNj),(15)vj=(v1j,v2j,…,vNj),where xijandvijare binary variables, i is the particle index, and N is the total number of particles. Each particle has a position vector, in the binary space, with length equal to the number of data inputs (features). Thus, xijis binary, i.e. xij∈{0, 1}, with j=1, …, Nt, where Ntis the total (initial) number of features of the dataset. The variable xijcorresponds to input uj, where j=1, …, Nt. If feature ujis to be selected then xij=1, if not xij=0, as depicted in Fig. 1.The first step is to normalize the data, which is a crucial step in the classification process. By mapping the data between [0, 1] (or in another interval) we remove the scale effects, allowing underlying characteristics to be compared. Further, several parameters have to be selected, namely: number of particles N and number of iterations I. A common value for N is between 20 and 40. The positions of the first swarm can be initialized randomly by doing:(16)xij←1,ifr≥0.50,otherwisei=1,…,N,j=1,…,Nt,the velocities are given by:(17)vij=−vmax+2svmax,i=1,…,N,j=1,…,Nt,where r and s are random numbers ∈[0, 1], andvmaxis the maximum value for the velocity. The above positions and velocities are iteratively updated based on the SVM classifier performance and the number of selected features. This process can be divided in to the following steps:Step 1.Evaluate each particle in the swarm: in this step an SVM model is generated using the feature subset corresponding to the particle position. Then, the objective function defined in (10) is computed to evaluate the solutions.Swarm and particle best values: the particle best, xpb, corresponds to the position of the particle that had the best fitness f in all iterations. Swarm best, xsb, is the best position achieved in all iterations by all the particles. This verification can be summarized by:(18)xipb←xi,iff(xi)>f(xipb)(19)xsb←xi,iff(xi)>f(xsb)Update velocities: velocity directs the movement in the search space taking into account the performance of the own particle and of the swarm, and it is updated as follows:(20)vij←wvij+c1qxijpb−xijΔt+c2rxjsb−xijΔt,i=1,…,N,j=1,…,Nt.The term involving the constant c1 is called the cognitive component and it measures the degree of self-confidence of a particle, the degree at which it trusts its performance. The term involving c2 is the social component and it relies in the capability of the swarm to find better candidate solutions. The parameters q and r are uniform random numbers ∈[0, 1]; Δt is the time step of each iteration; the termwis the inertia weight and it controls the influence of the previous velocity in the new velocity. Ifw>1the particle favors exploration over exploitation, else ifw<1the particle gives more importance to the current best positions (particle best and swarm best). Thus, it is common to start with a value larger thanw=1.4and then reduce it by a factor β∈]0, 1[ in each iteration. Further, after velocities have been update, the restrictionvij<vmaxis applied; this is a crucial step for the swarm to maintain coherence.Update particle position: the logistic function of the velocity is used as the probability distribution for the position [11,31]:(21)S(vij)=11+e−vijThus the particle position is calculated for each variable by:(22)xij←0,ifr>S(vij)1,otherwisei=1,…,N,j=1,…,NtContinue the iterative process: return to Step 1 if convergence or iteration limit is not achieved.Unlike the continuous version of the PSO, in BPSO, particle position is restricted to the Hamming space. Thus, there is no risk of swarm divergence. However, the problem of premature convergence of the swarm arises. According to (21) and (22), if the velocity reaches a high value (e.g.vij=12) it would become very difficult for new solutions to be tried. Hence, a velocity threshold is imposed, such that the probability of choosing a determined position is limited. Namely, the valueS(vmax)should be smaller than one, enabling some random “mistakes” to be made during position update. Note that in this situation the velocity clamping mechanism is similar to the mutation operator in the GA, withvmaxplaying the role of the mutation rate.The value ofvmaxcould, in principle, vary during search to initially favor exploration and later exploit the best solutions. However, to the best of our knowledge, such matter has never been addressed and will be regarded in the proposed modified binary PSO.The binary version of particle swarm optimization (BPSO) has the tendency to prematurely converge, as noted in [35], especially in more challenging optimization tasks. There are various approaches to cope with this problem: using the mutation operator from EA [13], reset the swarm best if the fitness stagnates [34] or using perturbation mechanisms [31]. The modified BPSO algorithm proposed in this paper, associates the benefits of local search (mutations) [31,36] with resetting the swarm best mechanism [34].These mechanisms have individual characteristics that contribute to an improvement of the solutions when there is premature convergence of the binary PSO.The importance of preserving the previous value of the velocity when calculating its current value has been discussed in Step 3. However, if a suboptimal solution happens to be better than any of the previously found, the swarm might converge towards that position. In such situation, the velocity of the particles will increase untilvmax. Therefore, it will be difficult to divert from this undesirable suboptimal position with a small change in velocity.In order to explore untried areas of the search space, it was suggested in [36] to introduce small random mistakes in the current particle positions:(23)xij=¬xijifr≤rmut,xij=xijotherwise,,i=1,…,N,j=1,…,Nt.where rmutis the probability of random mutation. After updating the particle position as in (21) and (22), each of the bits of the position vector is mutated with a probability rmut. Common values for the mutation probability are rmut=1/Nt, which means that at least one of the bits in the position vector will be flipped. The mutation rate should be sufficient to introduce some variability in the swarm without increasing too much the randomization of the algorithm.This mechanism has proven to be effective in improving the solutions found by the original BPSO [36]. Nevertheless, in problems with higher dimensionality, increasing the value of rmutmight not be enough to avoid premature convergence.This mechanism is used in the improved binary PSO (IBPSO) in [34]. Each particle adjusts its position according to two values, its own best solution so far, xpb, and the swarm best solution xsb. The particle best is a local search value, whereas the swarm best constitutes a global search value. If the xsbvalue is itself trapped in a local optimum, this will limit the search of the entire swarm. Hence, when resetting xsb, the binary PSO will not be trapped in a local optimum, and better classification results can be achieved by searching for a new xsbvalue in a region with a lower number of features. This process is depicted in Fig. 2. In this approach, all bits of xsbare equal to 0 except one, which is randomly chosen and is set to 1. The algorithm is considered to have prematurely converged, see Fig. 2(a), when the value of f(xsb) stays constant for a chosen number of iterations Imax. The following step is to reset the swarm best, which consists of selecting any features. Thus, all the bits in thexsbare changed to be zero. In most cases, the algorithm will converge towards a better solution, due to the reduction in the number of selected features while maintaining classification accuracy.We combined the above mechanisms in a novel PSO (NPSO) approach, which was presented in [37]. The algorithm in [37] has demonstrated to have superior performance than the original BPSO and IBPSO when applied to feature selection. Nevertheless, NPSO had several disadvantages. One of them is the addition of two adjustable parameters, the random mutation probability rmut, and the maximum allowed number of iteration with a constant swarm best fitness f(xsb). Therefore, this paper proposes the modified BPSO algorithm, as described in the next section.One of the major problems when using wrapper methods for feature selection is the process of selecting a proper set of parameters for the used model. This problem can be surpassed by automatically optimizing the SVM model parameters in parallel with the feature selection process [10]. The proposed modified binary PSO includes the SVM model parameters in the encoding of the particles and is described in the following.In this paper, the RBF kernel function, defined in (9), is used in the SVM classifier, once the RBF kernel function can analyze higher dimensional data and only requires two parameters, C and γ to be defined [38]. When the RBF kernel is selected, the parameters C and γ, and the features used as input attributes must be optimized using our proposed MBPSO system.Therefore, the particle is in this case comprised of three parts, the features mask, C and γ. Fig. 3shows the representation of particle i, where j=1, …, NPand NPis the size of the particle. A part of the binary sequence refers to the feature selection (as described above) and the rest of the sequence to the model parameters. Nevertheless, the parameters are usually real valued, thus it is necessary to decode the binary strings in to floating point values. For each parameter of the model there is a specific part of the binary string that is decoded independently of a selected range.The MBPSO combines the described reset swarm best mechanism with a local search operator that displaces the particle best positionxpband uses an operator similar to the mutation mechanism, but instead of using a probability of mutation, the change in the particles are made controlling the value ofvmax. These modified mechanisms were first introduced in [39] and are used by the proposed MBPSO, once these mechanisms have shown better performance.Thexsbis the only liaison between all the particles. Hence, the reset swarm best mechanism was kept from the NPSO, since it shown to be effective in preventing the premature convergence ofxsb.The criteria to reset the swarm best is to verify if the fitness of thexsbstays constant for a given number of iterations. Usually, this value is around three iterations [34]. It is a small value to avoid the convergence of the rest of the swarm to a suboptimalxsb. The refinement of the best solution is difficult, and as a consequence NPSO takes a high number of iterations to converge to an acceptable solution. In [39], we have introduced a mechanism of local search that consists of displacing thexipbvalues when resetting the swarm best:(24)xijpb=¬xijpbifr≤drxijpb=xijpbotherwise,i=1,…,N,j=1,…,NP,where dr is the displacement rate, i.e. the probability of each bit in thexipbbeing flipped, and r is a random number ∈[0, 1]. As can be seen in Fig. 4, even if most of the particles converge to a suboptimalxsb(a), when this mechanism is applied the particles are displaced (b). The algorithm will be able to converge towards a better solution (c). Therefore, we will be able to refine the results around the best position reducing the risk of causing premature convergence of the algorithm.When the velocity reaches a high value (e.g.vij=10) the probability of selecting a given bit is very close to one. The NPSO uses a mechanism similar to the mutations in EA to make small “mistakes” during position update, avoiding premature convergence. However, in the proposed MBPSO a more effective approach is applied, by controlling the value ofvmax.It should be noticed that the probability of selecting a given bit whenvij=vmaxis:(25)xij=1,p=S(vmax)xij=0,p=S¯(vmax)=1−S(vmax)and it can be proven that whenvij=−vmaxthe probability of selecting each bit is:(26)xij=1,p=S(−vmax)=1−S(vmax)xij=0,p=S¯(−vmax)=1−S(−vmax)=S(vmax)with i=1, …, N and j=1, …, NP. This means that the probability of making a “mistake” when the velocity achieves its maximum absolute value is equal to1−S(vmax)=S(−vmax). Therefore, we can control the value ofvmaxin order to introduce variability in the swarm, instead of using the mutation operator as in NPSO. This approach has several advantages:•localized variability – the variability is introduced where it is most important, when the particle position stagnates due to the saturation of the velocity;removing an extra operator – replacing the mutation operator with the control ofvmaxsimplifies the algorithm. There will be less parameters to adjust and the algorithm will be computationally more efficient.The proposed algorithm is summarized in Algorithm 1.To demonstrate the improvements achieved by the proposed MBPSO approach, feature selection was performed in 6 benchmark datasets. The selected benchmark databases are listed in Table 1, which are available in the UCI repository [40]. This repository has been widely used by researchers as a primary source of machine learning databases (it has been cited over 1000 times). Further, the databases in this repository have a good balance between classes and a large diversity in feature number and sample size. Four methods are applied for searching the optimal subset of features:1.The binary PSO (BPSO) using the mutation operator presented in [36];The improved binary PSO (IBPSO) using the reset swarm best mechanism proposed in [34];The genetic algorithm (GA) introduced in [10];The proposed modified binary PSO (MBPSO) approach introduced in this paper.Algorithm 1Feature selection with the MBPSO1:Initialize algorithm parameters2:Normalize and divide data (train+test)3:Randomly initialize swarm particles4:while (number of iterations or stopping criteria are not met) do5:Evaluate the fitness of all the individuals6:fori=1 to number of particles do7:if fitness of xiis greater than the fitness ofxipbthen8:xipb=xi9:end if10:if fitness of xiis greater than the fitness of xsbthen11:xsb=xi12:end if13:if xsbconstant for Imax iterations then14:reset xsb15:update xpbusing displacement rate dr as in (24)16:end if17:forj=1 to dimension of particle's position do18:vij←wvij+c1qxijpb−xijΔt+c2rxjsb−xijΔt19:if|vij|>vmaxthen20:vij=vmax×sgn(vij)21:end if22:S(vij)=11+e−vij23:if (r<S(vij)) then24:xij=1 elsexij=025:end if26:end for27:Introduce variability in the swarm using (25)28:end for29:end whileRecall that SVM are used in combination with the optimization algorithms in a wrapper methodology, and the model parameters are optimized simultaneously in the feature selection optimization process. After FS, 10-fold cross validation is applied to the best subset in each algorithm and the best results are presented in Table 2.In what concerns the comparison of the proposed MBPSO to other binary PSO algorithms, Table 2 shows clearly that the overall accuracy increases due to the proposed mechanisms to avoid premature convergence, while the number of selected features is smaller or similar using MBPSO.Table 2 shows that the MBPSO introduces in general a slight increase in the specificity, and in some cases a significant improvement in the sensitivity. This is important since correct classifications for the positive cases are being made more accurately, and in many classification applications the positive class has a greater importance than the negative one. This is generally the case for medical applications, and for the case study presented in Section 5 is even more important, once the failure in correct classifications of the positive class means that a patient will have a larger risk of death.Although the improvement of the results using MBPSO is modest, it is statistically significant as the p-value is <0.05. Comparing MBPSO with all the other binary PSO based algorithms, only for the WBCO database was not possible to improve the results in terms of accuracy.The MBPSO approach is better than the IBPSO, due to the mechanisms that introduce variability in the swarm. Nevertheless, these differences are still dependant of the adjustment of the dr, since this parameter has to be well adjusted in order to compensate an increase in the resetxsbiterations.To further test the proposed MBPSO algorithm, the comparison with other metaheuristics used for feature selection is very important. The application of GA to feature selection is well studied in the literature, so it presents a solid ground of comparison. From Table 2 it is possible to observe that the MBPSO results in terms of accuracy are similar or slightly better than GA. However, MBPSO can find smaller feature subsets. In terms of sensitivity, GA is better, except for the Colon database. On the other hand, GA has a lower specificity.Sepsis is the 10th leading cause of death in the United States, being one of the only two infectious diseases listed in the top 15 causes of death [41]. According to a study conducted by Emory University School of Medicine and the Centers for Disease Control and Prevention, the rate of patients with sepsis in the US population has increased more than 300 percent between 1979 and 2000 [42]. Furthermore, given that one of the most important risk factors of sepsis is the advanced age, the disease rates will very likely continue to grow with the increasing ageing of the population [43].This disease is not only responsible for a high number of casualties, but it also represents a large financial burden to the health care system. For example in USA, a sepsis patient stays on average 19.6 days in an intensive care unit (ICU), with an average cost of $22,100. This reflects in a total of approximately $1.7 billion per year for treating patients with severe sepsis [43]. An estimated 20–30 percent of these costs are related to the treatment of the illness, while 70–80 percent are associated with productivity losses due to mortality [15].It has been proven that an early goal directed therapy (EGDT) is more effective in reducing mortality risk of patients with sepsis than standard care [44]. However, it involves the identification of high-risk patients, namely the prediction of the transition between sepsis and more severe states of sepsis. For the case of neonates, it was possible to identify some biomarkers that enable the detection and early diagnosis of sepsis [45]. However, in the case of adult patients this identification was not yet possible, which drastically reduces the applicability of EGDT.The Acute Physiology and Chronic Health Evaluation (APACHE II) is a classification system for disease severity, designed to be used in the evaluation of adult patients. The Simplified Acute Physiology Score (SAPS II) was developed with the primary goal of simplifying the APACHE II score [46]. The Sequential Organ Failure Assessment (SOFA) score is a scoring system, which evaluates the extent organ function or rate of failure. Finally, Multiple Organ Dysfunction Score (MODS) has similar purposes to the SOFA score, but is computed in a different manner.More recently, there has also been some work developed to determine disease severity using machine learning algorithms. This approach consists of using knowledge discovery techniques in order to predict the health condition of sepsis patients. Most of the developed tools run on static data (e.g. age, gender, blood pressure, hearth rate, temperature, pH), and are used to classify if it is probable that a patient will evolve to a more severe health state. The number of developed methods is scarce, the most known approaches are [46–49]. In [46,47], an application of a knowledge-based neural network technique is presented, and uses 12 of most frequently measured variables (in patients with sepsis) to predict the outcome of a sepsis patient (decease/survive). In [48], an extension of [46,47] is presented, using 28 variables instead of the initial 12, achieving considerably better results. Finally, in [49], an early warning system (EWS) is proposed, where a multivariate logistic regression model is used to provide prior warnings for septic shock.The methodology used in this work, in what concerns the application of feature selection methods to the problem of sepsis outcome prediction, is a follow up of the work developed in [48], and aims not only to derive an accurate classifier, but also shade some light in which physical variables are more likely to dictate the patient state of health under a sepsis clinical state.In such a case the most important issue is to have a classifier with the best accuracy possible. The models generated by SVM will be optimized through feature selection using the proposed MBPSO algorithm. Thus, in this section this method will be validated against GA, in order to prove its multiple advantages.This paper uses the public available MEDAN database [46], containing data of 382 patients with abdominal septic shock, recorded from 71 German intensive care units (ICUs), from 1998 to 2002. This database holds: personal records, physiological parameters, procedures, diagnosis/therapies, medications, and respective outcomes (survived or deceased). For the purpose of the present work, we will focus exclusively in physiological parameters, which include a total number of 103 different variables/features, which is the same database used in [47,48].We follow the preprocessing procedures done in [48]. From the initial set containing a total of 103 features, two different subsets of features were chosen as inputs for our models. One, was defined as in [47], for comparison purposes. It contains the 12 most frequently measured variables: creatinin (mg/dl), calcium (mmol/l), arterial, pCO2 (mmHg), pH, haematocrit (%), sodium (mmol/l), leukocytes (1000/ml), haemoglobin (g/dl), central venous pressure (CVP) (cmH2O), temperature (C), heart frequency (1/min) and systolic blood pressure (mmHg). A total of 121 patients were found to have data regarding these features. However, the previous subset of features was considered to be too narrow, and in [48] a second data subset was defined including a total of 28 variables, which were found to be present in a total of 89 patients. These two sets of features are described in Table 3.All the optimization algorithms presented in this work were applied to each of these subsets, using SVM as learning machines. The reason to apply feature selection techniques to these not so large subsets is related to the clinical relevance of finding the specific variables that relate the most with the prediction of a patient's outcome. The accuracy, or the correct classification rate, is not always the best way to evaluate the performance of the classifier. For this particular application, the main interest is to correctly classify which patients are more likely to die, in order to rapidly act in their best interest. Bearing this in mind the classifier should classify as accurate as possible the death cases, or the true positives, and have a low number of false negatives. In other words, the classifier should maximize sensitivity. In order to evaluate the performance of the developed prediction models, upon the described subsets of data, three different criteria were used: accuracy, sensitivity and specificity.The benefits of feature selection over the 12 feature dataset are summarized in Table 4. Comparing the results in terms of accuracy before and after feature selection, a slight increase can be observed. However, the value of sensitivity increased considerably after feature selection, without a noticeable decrease in specificity. This is an increase in the quality of the model, once the sensitivity is very important for this application. Further, a small increase in the overall accuracy was registered. This is due to the reduction in the feature subset size from 12 (initial) to 3 (after FS) features on average, which implies a much less complex model. The best overall accuracy was achieved by MBPSO, with a significant improvement in terms of sensitivity, and using only two features.The results for the 28 variables dataset are depicted in Table 5. The results show a considerable increase in the predictive performance of the generated classifier.Regarding Table 5, it can be seen that the accuracy over the 28 features dataset improved after feature selection. Also, the initial sensitivity was improved, while maintaining a good specificity. This can probably be justified by the reduction of the number of redundant and noisy feature used to generate the SVM surface. In terms of the number of selected features, only 20% of the initial features are used in the model with the best performance.The accuracy is substantially better when using 28 features, meaning that some meaningful features were left out in the first 12 features dataset. However, the average number of selected features more than doubles when using 28 features instead of 12. Regarding the performance of the used optimization algorithms, in the improvement of sepsis outcome prediction, both GA-SVM and MBPSO-SVM have considerably better accuracies than the remaining algorithms, including the initial set of features. These differences are statistically relevant with more than 95% of confidence, since the calculated p-values, with respect to the accuracy without feature selection (No-FS), are much smaller than 0.05. There is no significant difference in performance between GA-SVM and MBPSO-SVM. The set of initial features is in both cases reduced, on average, to 1/4 of the initial size. However, the MBPSO running time was on average approximately half of GA run time (for example for the 12 feature data set the running time was 8000s for the GA vs 4500s for the MBPSO).A comparison was made between the studied wrapper methods and other FS methods already applied to the problem of sepsis outcome prediction. These wrapper approaches are: neuro-fuzzy modeling using only the 12 feature dataset [47], bottom-up (BU) using fuzzy modeling and neural networks [48] and ant feature selection (AFS) using fuzzy modeling and neural networks [48]. The results are depicted in Tables 6and 7for the 12 and 28 features datasets respectively.The differences between the proposed approaches and the other methods are significant. The GA-SVM and MBPSO-SVM have superior performances in all the measures that evaluate the generalization capabilities of the generated model (e.g. accuracy, sensitivity and specificity). The best performance of all wrapper methods is achieved over the 28 feature database by the GA-SVM, but with similar performance to MBPSO-SVM.This comparison is solely illustrative, since the methods use different modeling techniques. However, this might be an indicator that SVM have better generalization properties over this database than fuzzy modeling or neural networks.

@&#CONCLUSIONS@&#
The proposed MBPSO algorithm was tested over 6 benchmark databases, showing a better performance than the state of the art methods for PSO and similar or better results than GA. Some limitations to the proposed MBPSO algorithm can arise if there is a poor adjustment in the parameters, as it has two more adjustable parameters than the standard version of binary PSO. Nevertheless, it has only one more parameter than genetic algorithm which has a similar overall performance. Despite all the mechanisms for premature convergence, the proposed MBPSO algorithm was able to correctly estimate the parameters of support vector machines, in parallel to feature selection. The generation of models for the mortality risk evaluation in patients with sepsis is a very important topic in medicine. The application of SVM based feature selection to the case study of sepsis outcome prediction shown to be superior than all the previously used methods [47,48,50].Future work considers experimenting the introduced algorithm with other medical databases in order to more consistently compare its performance with other feature selection techniques.