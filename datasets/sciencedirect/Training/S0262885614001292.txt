@&#MAIN-TITLE@&#
A tensor-based deep learning framework

@&#HIGHLIGHTS@&#
We propose a novel deep learning model for action recognition.The deep learning model exploits multilinear algebra.The method derives spatio-temporal features.

@&#KEYPHRASES@&#
Deep learning,Hierarchical Temporal Memory (HTM),Tensor algebra,L,1-norm,Support Vector Clustering,Spatio-temporal features,

@&#ABSTRACT@&#
This paper presents an unsupervised deep learning framework that derives spatio-temporal features for human–robot interaction. The respective models extract high-level features from low-level ones through a hierarchical network, viz. the Hierarchical Temporal Memory (HTM), providing at the same time a solution to the curse of dimensionality in shallow techniques. The presented work incorporates the tensor-based framework within the operation of the nodes and, thus, enhances the feature derivation procedure. This is due to the fact that tensors allow the preservation of the initial data format and their respective correlation and, moreover, attain more compact representations. The computational nodes form spatial and temporal groups by exploiting the multilinear algebra and subsequently express the samples according to those groups in terms of proximity. This generic framework may be applied in a diverse of visual data, while it has been examined on sequences of color and depth images, exhibiting remarkable performance.

@&#INTRODUCTION@&#
A prerequisite of the human–robot interaction is the ability of the robot to comprehend the respective inputs given by humans. Similarly to humans, the most intensive sensory input of a robot is the visual one. The latter suggests that the robots should collaborate with humans they ought to be endowed with subsystems suitable for action, gesture or even emotion recognition. An interest point for such desired recognition tasks is the fact that the corresponding data encapsulate both spatial and temporal information. The temporal dimension of those problems may be administered in an appropriate manner and, consequently, the utilization of mainstream algorithms that are suitable for object recognition is considered to be improper for the aforementioned tasks.A well-known approach to deal with video sequences is to utilize motion descriptors [1–5], however the formation of Bag-of-Words models does not preserve the temporal coherence. A solution to the latter disadvantage can be provided by deep learning architectures, where the acquisition of spatio-temporal contingencies and, thus, the derivation of respective features are considered to be an indispensable goal. Deep learning is an emerging area in the field of machine learning and its stepping stone are contemporary neuroscience discoveries concerning the representation of information and the feature derivation. More precisely, the findings in Ref. [6] suggest that the neocortex in mammals does not process the sensory input data in a shallow manner, but they traverse through a composite multi-layered structure of computational units which, through repetition, learn to express the signals according to the consistencies they display. Additionally, the works in Ref. [7,8] provide affirmation for the existence of a mutual cortical architecture, not only between different modalities such as visual, auditory and somatosensory, but also among other species. The deep learning models have already been applied in a diversity of recognition tasks namely object recognition [9–12], human tracking [13], segmentation [14], brain–computer interaction (BCI) [15] and in action recognition [16–18] exhibiting remarkable performance.Tensor or multilinear algebra can provide a complete mathematical framework for analyzing the multifactor formation of image and video sets. Moreover, it administers methodologies for decomposing such sets and, thus, unfolding the factors or modes. Similarly to matrices which are considered to be linear operators over a vector space, tensors constitute multilinear operators over a set of vector spaces. Consequently, linear analysis is a special case of tensor analysis which offers a generalized mathematical framework appropriate to confront a plethora of machine learning problems. However, the vectorization of a tensor neglects the proximity information, i.e. when a video sample is processed, a patch of pixels is separated from its neighbor ones and, therefore, lacks spatio-temporal coherence. Additionally, the vectorization procedure, besides the fact that stacks the rows or columns of the original tensorial samples in an inconsistent manner, also leads to the formation of high dimensionality vectors. Thus, combining the high dimensional data representation with the small number of data samples conducts to small size problems, able to be competently handled by tensors [19]. Tensor based frameworks maintain most of the authentic constraints of the data and, thus, they are in position to improve the characterization or classification of them. Especially in the case where the number of training samples is restricted, the constraints aid the derivation of a reasonable solution in such problems. The second and the third order tensors suffice to describe image and video samples, respectively.The motivation of the proposed model relies on three main facts, as follows. Firstly, tensor based approaches avoid over-training, especially in small size problems [20]. The over-training occurs in most deep architectures and further measures should be considered, which attempt to confine such phenomena [21]. Secondly, multilinear algebra reduces the computational complexity and the requirements in terms of memory [22,23]. Such considerations are frequently encountered in deep learning models thus, cost expensive solutions, namely the developing of computing clusters with thousands of machines are performed [24]. Last, with the aim to reduce the number of parameters that have to be estimated, multilinear units are more efficient than vectorized non-linear ones.This paper presents an unsupervised deep learning methodology inspired by the HTM notion [25,26], i.e. a network of computational nodes, placed in a tree shaped hierarchy and divided into distinguished levels. Each of these nodes incorporates notions from tensor algebra to infer spatio-temporal features. To the best of our knowledge this is the first attempt to graft tensor based approaches in such architectures. The insight behind this approach is the avoidance of high dimensionality feature vectors appearing in previous HTM architectures [25,11,26,12], thus, resulting to more efficient method in terms of data representation and classification accuracy. The experimental evaluation procedure justifies this insight in terms of classification accuracy. The proposed methodology is an unsupervised multi-level architecture capable of extracting spatio-temporal features from video samples. Every node – similarly to the HTM notion [25,26] – incorporates two distinct procedures: the spatial and the temporal one, while the procedures themselves attain the training and the testing (inference) mode. Thereupon, when the spatial procedure is in training mode, the node treats every frame patch as a second-order tensor and attempts to learn and quantize the input space. This quantization is accomplished by deriving representatives (i.e. quantization centers) through a competitive learning scheme. Then, the node switches to testing mode and expresses each frame patch in terms of similarity utilizing the corresponding representatives. Afterwards, the node initiates the temporal procedure in training mode and for each respective frame patch it forms a matrix that contains the temporal changes among the frame patches of the same video sample. Those matrices are utilized to assemble temporal clusters through a tensorized extension of a vector based clustering technique. Last, the node alternates to testing mode, in which every frame patch acquires – through a membership function – the degree of closeness with respect to the derived temporal clusters. Regarding the proposed deep learning model, each level is successively trained and a level starts its training only after the nodes of the previous level have completed the respective operation. In addition, the upper level uses the concatenated outputs from its corresponding children nodes as input and treats those concatenations as a dictionary. The proposed technique exceeds in terms of classification accuracy not only previous HTM-based models but also other state-of-the-art methodologies (see Section 4 for details).The remaining of this paper is organized as follows: In Section 2 the most significant work over the deep learning field and methodologies that exploit multilinear algebra is presented; Section 3 describes the architecture of the proposed deep learning model, including the operation of a single node; the experimental validation of the presented methodology occurs in Section 4 and, last, conclusions are drawn in Section 5.

@&#CONCLUSIONS@&#
