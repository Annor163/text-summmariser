@&#MAIN-TITLE@&#
Soft label based Linear Discriminant Analysis for image recognition and retrieval

@&#HIGHLIGHTS@&#
The proposed label propagation can handle the outliers and multi-density distributed data.The out-of-sample problem of label propagation is solved by using LDA criterion.The proposed method is examined based on extensive simulations for image classification and retrieval problems.Simulation results show the superiority of proposed method to other state-of-art methods.

@&#KEYPHRASES@&#
Linear Discriminant Analysis,Semi-supervised dimensionality reduction,Soft label,Label propagation,

@&#ABSTRACT@&#
Dealing with high-dimensional data has always been a major problem in the research of pattern recognition and machine learning. Among all the dimensionality reduction techniques, Linear Discriminant Analysis (LDA) is one of the most popular methods that have been widely used in many classification applications. But LDA can only utilize labeled samples while neglect the unlabeled samples, which are abundant and can be easily obtained in the real world. In this paper, we propose a new dimensionality reduction method by using unlabeled samples to enhance the performance of LDA. The new method first propagates the label information from labeled set to unlabeled set via a label propagation process, where the predicted labels of unlabeled samples, called soft labels, can be obtained. It then incorporates the soft labels into the construction of scatter matrixes to find a transformed matrix for dimensionality reduction. In this way, the proposed method can preserve more discriminative information, which is preferable when solving the classification problem. Extensive simulations are conducted on several datasets and the results show the effectiveness of the proposed method.

@&#INTRODUCTION@&#
Dealing with high-dimensional data has always been a major problem in the research of pattern recognition and machine learning. Typical applications of these include face recognition, document categorization, and image retrieval. Finding a low-dimensional representation of high-dimensional space, namely dimensionality reduction is thus of great practical importance. The goal of dimensionality reduction is to reduce the complexity of input space and to embed high-dimensional space into a low-dimensional space while keeping most of the desired intrinsic information [1,2,13,14]. Among all the dimensionality reduction techniques, Principle Component Analysis (PCA) [3] and Linear Discriminant Analysis [4] are the most popular methods and have been widely used in many classification applications. The objective of PCA is to pursue a set of orthogonal basis that can capture the directions of maximum variance in the dataset for optimal reconstruction. While the objective of LDA is to find the optimal projection that maximizes the between-class scatter matrix Sbwhile minimizes the within-class scatter matrix Swin the low-dimensional subspace. Given that the within-class scatter matrix is nonsingular, the optimization problem of LDA can be solved by generalized eigen-value decomposition (GEVD), i.e. to find the d largest eigenvectors corresponding to the eigenvalues ofSw-1Sb[27]. However, for many applications where the number of dimensionality is much larger than that of the samples, the within-class scatter matrix tends to be singular. Hence the optimal projection matrix may be found incorrect. This is the so-called small sample problem and many variants of LDA have been proposed to solve it, which include Regularized LDA [23], Null-space LDA [24], Uncorrelated LDA [25], SRDA [26].In general, LDA is supervised, which means it requires label information. Although supervised methods generally outperform unsupervised methods, a large number of labeled samples are needed in order to achieve satisfactory results [27]. But in many cases, labeling large number of samples is time-consuming and costly. On the other hand, unlabeled samples are abundant and can be easily obtained in the real world. Thus, semi-supervised learning methods [5–12,41–45], which incorporate both labeled and unlabeled samples into learning procedure, have become more effective than only relying on supervised learning. Many semi-supervised methods have been proposed in the past few years, which include Gaussian Fields and Harmonic Functions (GFHF) [5], Learning with Local and Global Consistency (LLGC) [6], Semi-supervised Discriminant Analysis (SDA) [9] and Laplacian regularized Least Square (LapRLS) [11].The above methods can be divided into two categories: transductive method and inductive method. Two well-known transductive methods are GFHF and LLGC. These methods work in a transductive way by propagating the label information from labeled set to unlabeled set. But they cannot predict the class labels of new-coming samples hence suffering out-of-sample problem. In contrast, inductive methods, such as SDA and LapRLS can solve such problem. These methods firstly construct a manifold regularized term to preserve the geometrical structure with both labeled and unlabeled set [17]. Then, they arm to find a transformed matrix to perform dimensionality reduction by incorporating the manifold regularized term into the original objective function of supervised algorithms. Hence, the new-coming samples can be projected into a low-dimensional subspace by using such transformed matrix and the out-of-sample problem can be naturally solved, which is more practical in real-world applications.In this paper, we propose a new semi-supervised dimensionality reduction method to enhance conventional LDA performance by incorporating the soft labels into the scatter matrixes. The proposed method first propagates the label information from labeled set to unlabeled set via label propagation, where the predicted class labels of unlabeled samples, called soft labels, can be obtained. It then finds a transformed matrix to perform dimensionality reduction by incorporating the soft labels into the scatter matrixes. The proposed method can be viewed as a unified framework to extend many existing LDA algorithms to their semi-supervised versions. Its basic ideas are different from those semi-supervised algorithms such as SDA and LapRLS. These methods use the labeled and unlabeled samples in a simple manner to construct a manifold regularized term and add it to the objective function of supervised algorithms. But in the proposed algorithm, by incorporating the soft labels into training, it can well preserve the probability distribution of samples both in labeled and unlabeled set hence obtaining a better subspace for dimensionality reduction. We also analyze our proposed algorithm under a least square framework. It can be concluded that given a certain class indicator, the optimization problem of the proposed method can be equivalent to a weighted least square problem.The main contribution of this paper is summarized as follows:(1)We present an effective label propagation procedure, which is based on a new local reconstruction graph with symmetrization and normalization. The symmetrization can add more connections of a sample with its neighborhoods and the normalization can handle with the case when the density of dataset varies dramatically.The proposed SL-LDA can preserve more discriminative information in the soft labels of unlabeled samples than other methods, which is good to the performance for classification. It can also be easily extended to its kernel version by using kernel tricks [21,22].Motivated by the equivalence between LDA and the least square problem [18–20], we extend this relationship and further analyze SL-LDA under a weighted least square problem (W-LS). Based on such relationship, we then propose a more efficient approach for calculating the optimal solution of SL-LDA, which is a linear system of equation and can be performed on large-scale dataset.This paper is organized as follows: In Section 2, we will present an effective label propagation procedure with outlier detection and dealing with noisy labels. In Section 3, we will introduce our soft label based Linear Discriminant Analysis (SL-LDA) for semi-supervised dimensionality reduction. We will also build a close relationship between SL-LDA and W-LS in this section and propose a more efficient approach for solving SL-LDA. The simulation results based on extensive datasets are shown in Section 5 and the final conclusions are drawn in Section 6.Let X=[Xl,Xu]∊RD×(l+u) be the labeled and unlabeled set, where each sample in Xlis associated with a class label ci, i∊[1,2,… ,c]. The goal of label propagation is to propagate the label information of labeled set to the unlabeled set according to the distribution associated with both labeled and unlabeled set [5–8,39,46], through which the predicted labels of unlabeled set, called soft labels can be obtained.In label propagation, a similarity matrix must be defined for evaluating the similarities between any two samples. The similarity matrix can be approximated by a neighborhood graph associated with weights on the edges. Officially, letG^=(V^,E^)denote this graph, whereV^is the vertex set ofG^representing the training samples,E^is the edge set ofG^associated with a weight matrix containing the local information between two nearby samples. There are many strategies to define the weight matrix W, a typical one is to use Gaussian function as [5,6,39]:(1)wij=exp(-‖xi-xj‖2/σ)xi∈Nk(xj)orxj∈Nk(xi),where Nk(xj) is the k neighborhood set of xj, σ is the Gaussian function variance. However, σ is hard to be determined and even a small variation of σ can make the results dramatically different [7]. Wang and Zhang have proposed another strategy to constructG^by using the neighborhood information of samples [7]. This strategy assumes that each sample can be reconstructed by a linear combination of its neighborhoods [2], i.e.xi≈∑j:xj∈Nk(xi)wijxj. It then calculates the weight matrix by solving a standard quadratic programming (QP) problem as:(2)minxi-∑j:xj∈Nk(xi)wijxjF2s.t.wij⩾0,∑j∈Nk(xi)wij=1.The above strategy is empirically better than the Gaussian function, as the weight matrix can be automatically calculated in a closed form once the neighborhood size is fixed. In addition, in order to make a connection to the normalized graph, we symmetrize W as W←(W+WT)/2 or wij←(wij+wji)/2. The advantage of this step is that it considers the node degree of each sample and a sample with large node degree can connect more neighborhoods. Then similar to [39], we normalize W asW∼=D-1/2WD-1/2orw̃ij=wij/diidjjwhere D is the diagonal matrix satisfyingdii=∑j=1l+uwij. The normalization can strengthen the weights in the low-density region and weaken the weights in the high density region, which is good for handling the case that the density of dataset varies dramtically. Finally, to satisfy the sum-to-one constraint as Eq. (2), the weight matrixW∼is set asW∼←W∼D∼-1orw̃ij←w̃ij∑j=1l+uw̃ij, whereD^is the diagonal matrix satisfyingd̃ii=∑j=1l+uw̃ij. The basic steps for graph construction can be seen in Table 1.We will then predict the labels of unlabeled samples based on a label propagation process, through which the soft labels of unlabeled set can be obtained. Let Y=[y1,y2,… ,yl+u]∊R(c+1)×(l+u) be the initial labels of all samples, for the labeled sample xj, yij=1 if xjbelongs to the ith class, otherwise yij=0; for the unlabeled sample xj, yij=1 if i=c+1, otherwise yij=0. Note that we add an addition class c+1 to detect outliers hence the sum of each column of Y is equal to 1 [39].Denote a stochastic matrixP=D∼-1W∼, whereD∼is the diagonal matrix with the each element beingd̃ii=∑j=1l+uw̃ij. Then, we consider an iterative process for label propagation. At each iteration, we hope the label of each labeled sample is partly received from its neighborhoods and the rest is from its own label. Hence the label information of the data at t+1 iteration can be:(3)F(t+1)=F(t)PIα+YIβ.where Iα∊R(l+u)×(l+u) is an diagonal matrix with each element being αj, Iβ=I−Iα,αj(0⩽αj⩽1)is a parameter for xjto balance the initial label information of xjand the label information received from its neighbors during the iteration. According to [39], the regularization parameter αjfor the labeled data xjis set to αl; for the unlabeled sample xj, it is set to αuin the simulations. In this paper, we simply set αl=0 for labeled sample, which means we constrain Fl=Yl, and determine the value of αufor unlabeled sample based on fivefold-cross validation. By the iterative process in Eq. (3), we have:(4)F(t)=F(0)(PIα)t+YIβ∑k=0t-1(PIα)k.Based on the properties of matrix, i.e. limt→∞(PIα)t=0 andlimt→∞∑k=0t-1(PIα)k=(I-PIα)-1, the iterative process of Eq. (4) converges to:(5)F=limt→∞F(t)=YIβ(I-PIα)-1.It can be easily proved that the sum of each column of F is equal to 1 [39] (Corollary 1, we also include the proof in Appendix 6.1 to make our paper self-contained). This indicates that the elements in F are the probability values and fijcan be seen as the posterior probability of xjbelonging to the ith class. When i=c+1, fijrepresents the probability of xjbelonging to the outliers. The basic steps for the proposed label propagation process can be seen in Table 1. A simple example of the label propagation process with outlier detection can be seen in Fig. 1. However, in many real-world applications, there may exist noisy labeled samples due to the careless of the annotator, our method can also automatically erase such noisy labeled samples by setting αl>0. A case in point is given in Fig. 2, where we generalize a two-moon dataset with wrong labeled samples.In order to analyze the sensitivities in terms of how to bootstrap the label propagation, we compare the performance of the proposed label propagation with those of GGSSL and LNP. Specifically, we in Fig. 3first show the parameter sensitivities of the proposed label propagation with comparison to that of GGSSL. In Fig. 3, we generalize a two-cycle dataset with two samples labeled per class. The marker size of each unlabeled data point is proportional to its predicted label. From Fig. 3, we can see that the performance of GGSSL is sensitive to the Gaussian variance and even small variation can make the results dramatically different. However, the proposed label propagation has no such parameter and can be more robust than GGSSL. We can also see that the proposed label propagation can perform well in a wide range of αuthat is close to 1. In Fig. 4, we generalize a one-Swiss-roll dataset with one sample labeled in the core. The density of dataset is dense in the core of Swiss-roll while is sparse in the around. Hence the one-Swiss-roll dataset is a varied-density dataset. Similar to Fig. 3, the marker size of each unlabeled data point is proportional to its predicted label. From the simulation results in Fig. 4, we can see that both GGSSL (by carefully adjusting the Gaussian variance) and the proposed label propagation can better handle the varied-density dataset than LNP due to the normalization process. The proposed method is slightly better than GGSSL.By the above presentation, we design an effective label propagation procedure. But label propagation can only predict the labels of unlabeled samples and cannot predict the labels of new-coming samples. The reason is that label propagation is a transductive learning procedure and cannot learn a model. To solving this problem, a typical method is to regression the soft labels with a linear model as(6)F=VTX+bT,where W is the projection matrix and b is a bias term. Then, the label of a new-coming sample x′ can be handled by f′=VTx′+bT. Next, we will present a new effective method to calculate the projection matrix, which is based on the LDA criterion and arms to preserve more discriminative information.In general, LDA is a supervised method, which means the label of each sample is fixed, i.e. the probabilityfijlof xjbelonging to the ith class is either 0 or 1. The total-class, within-class and between-class scatter matrixes St, Swand Sbare then constructed based on such fixed label (we call it hard label):(7)St=∑i=1c∑x∈ci(x-μ)(x-μ)T,Sw=∑i=1c∑x∈ci(x-μi)(x-μi)T,Sb=∑i=1cli(μi-μ)(μi-μ)T,where liand μiare the number and mean of labeled set in the ith class, μ is the mean of all classes. The goal of LDA is thus to maximize the between-class scatter matrix while minimize the within-class scatter matrix in the reduced subspace as:(8)J(V)=maxVTr((VTSwV)-1VSbV)However, since LDA is a supervised method, it needs a large number of labeled samples in order to achieve satisfying classification accuracies [4]. However, labeling large number of samples is time-consuming and impractical. Hence, to enhance the performance of conventional LDA, we consider increasing the labeled samples for training by estimating the labels of unlabeled samples. Fig. 5illustrates our expectation for the improvement of LDA, in which we generalize a two-line dataset and show the boundary in the 1d reduced space (the boundary is orthogonal to the projection direction).Note that via label propagation process, we obtain the soft labels of unlabeled samplesfiju0⩽fiju⩽1. These soft labels have key probability information for discriminative learning and can be divided into three categories: (1) forfijuthat is close to 1, ∀i∊[1,2,… ,c], it can be taken into account as this means that xjis a newly added labeled sample in the ith class; (2) forfijuthat is close to 1 when i=c+1 , it can be neglected as this means that xjmay belong to the outliers; and (3) for otherfiju0<fiju<1, it can also be taken into account as it may have some useful probability information of xj. Hence, in order to use the estimated labels of unlabeled samples to enhance the performance of LDA, we propose a soft label based LDA, called SL-LDA, by incorporating the soft label into the construction of scatter matrixes.In this subsection, we first define the soft label based total-class, within-class and between-class scatter matrixes. Specifically, letμi∼,μ∼are the soft means of dataset in the ith class and in all classes as:(9)μi∼=∑j=1l+ufijxj∑j=1l+ufijμ∼=∑i=1c∑j=1l+ufijxj∑i=1c∑j=1l+ufij.Note that when the soft label fijbecomes hard label, i.e. fij∊{0,1}, the soft meansμi∼andμ∼will fell back to μiand μ, i.e.μi∼=μiandμ∼=μ. In addition, following Eq. (9), we have:(10)μi∼=∑j=1l+ufijxj∑j=1l+ufijμ∼=∑i=1c∑j=1l+ufijxj∑i=1c∑j=1l+ufij→∑j=1l+ufijμi∼=∑j=1l+ufijxj∑i=1c∑j=1l+ufijμ∼=∑i=1c∑j=1l+ufijxj→∑j=1l+ufijμi∼μi∼T=∑j=1l+ufijxjμi∼T=∑j=1l+ufijμi∼xjT∑i=1c∑j=1l+ufijμ̃μ̃T=∑i=1c∑j=1l+ufijxjμ∼T=∑i=1c∑j=1l+ufijμ̃xjT.Then following Eq. (10), we define the soft label based total-class, within-class and between-class scatter matrixesSt∼,Sw∼andSb∼as:(11)St∼=∑i=1c∑j=1l+ufij(xj-μ∼)(xj-μ∼)T=∑i=1c∑j=1l+ufijxjxjT-∑i=1c∑j=1lfijμ∼μ∼T=∑j=1l+uEjjxjxjT-∑j=1l+u∑k=1l+uEjjEkkeEeTxjxkT=XEXT-XEeTeEeEeTXT,(12)Sw∼=∑i=1c∑j=1l+ufij(xj-μi∼)(x-μi∼)T=∑i=1c∑j=1l+ufijxjxjT-∑i=1c∑j=1l+ufijμi∼μi∼T=∑j=1l+uEjjxjxjT-∑j=1l+u∑k=1l+u∑i=1cfijfikGiixjxkT=XEXT-XFcTG-1FcXT,(13)Sb∼=∑i=1c∑j=1l+ufij(μi∼-μ∼)(μi∼-μ∼)T=∑i=1c∑j=1l+ufijμi∼μi∼T-∑i=1c∑j=1l+ufijμ∼μ∼T=∑j=1l+u∑k=1l+u∑i=1cfijfikGiixjxk-∑j=1l+u∑k=1l+uEjjEkkeEeTxjxkT=XFcTG-1FcXT-XEeTeEeEeTXT,where E∊R(l+u)×(l+u) and G∊Rc×care two diagonal matrixes with each element satisfyingEjj=∑i=1cfijandGii=∑j=1l+ufij,Fc∈Rc×(l+u)is a matrix formed by the first c rows of F, e=[1,1,… ,1]∊R1×(l+u) is a unit vector. The above scatter matrixes can be the extensions to the ones in LDA: (1) Following Eqs. (11)–(13), it can be easily noted that the equality of scatter matrixes still holds, i.e.St∼=Sw∼+Sb∼. (2) When the soft label becomes hard label, i.e. fij∊{0,1}, the soft label based scatter matrixes will be the conventional ones as in Eq. (7). (3) By incorporating the soft labels of unlabeled samples into the construction, the soft labeled based scatter matrixes can preserve more discriminative information and alleviate the effects of outliers. Hence by maximizing the between-class scatter matrix while minimizing the within-class scatter matrix in the reduced subspace, i.e.(14)J(V)=maxVTrVTSw∼V-1VSb∼V,the optimal solution of SL-LDA can then be obtained by the generalized eigen-value decomposition (GEVD) as:(15)Sw∼+αI-1Sb∼V∗=V∗Λor(St∼+αI)-1Sb∼V∗=V∗Λ,where Λ is the eigenvalue diagonal matrix, αI is a multiply of identity matrix for avoiding the singularity ofSw∼orSt∼.Least square is another popular technique which has been widely used for regression and classification [28]. Let T∊Rc×(l+u) be a class indicator matrix and b∊R1×cbe the bias term, the goal of least square is then to fix a linear model t=VTx+bTby regressing X on T. It has been regoriously proved that the conventional LDA can be equivalent to least square given a certain class indicator [18–20]. In this subsection, we further extend this relationship and analyze SL-LDA under a weighted least square framework (W-LS). Based on such relationship, we then, in the next subsection, propose an efficient approach for solving SL-LDA. Let we consider a weighted and regularized least square problem:(16)J(V,b)=minV∑i=1c∑j=1l+ufijVTxj+bT-tjF2+α‖V‖F2.For convenience, we write Eq. (16) in matrix form as(17)J(V,b)=Tr(VTX+bTe)E(VTX+bTe)T-2TrTE(VTX+bTe)T+αTr(VTV).By setting the derivative w.r.t. Vand b to zero, we have:(18)XEXTV+XE(eTb-TT)+αV=0eEeTb+eE(XTV-TT)=0Following Eq. (18), we can obtain the optimal solution of W-LS as:(19)V∗=(XLsXT+αI)-1XLsTTwhere Ls=E−EeeTE/eEeTand following Eq. (11), we haveXLsXT=St∼. In addition, to establish the connection to SL-LDA, we choose a class indicator as T=G−1/2FE−1, that is for each tj, it satisfies:(20)tij=fij/EjjFii.It can be easily verified thatXLsTTTLsXT=Sb∼(Corollary 2, see in Appendix 6.2). Hence if we letHb∼=XLsTT∈RD×c, it then followsHb∼Hb∼T=Sb∼. The optimal solution of W-LS in Eq. (20) can thus be rewritten asV∗=St∼+αI-1Hb∼.We next build the relationship between SL-LDA and W-LS. This relationship is based on the following theorem:Theorem 1Given two matrix A and B, then AB and BA have the same non-zero eigenvalues. For each nonzero eigenvalue of AB, if the corresponding eigenvector of AB is v, then the corresponding eigenvector of BA is u=Bv.The proof of Theorem 1 can be seen in [28]. But in order to make our paper self-contained, we also include the proof in Appendix 6.3. Recall that the optimal solution of SL-LDA is formed by the d eigenvectors of(St∼+αI)-1Sb∼=(St∼+αI)-1Hb∼Hb∼T, according to Theorem 1, it has the same nonzero eigenvalues to auxiliary matrix as:(21)M=Hb∼T(St∼+αI)-1Hb∼∈Rc×c.According to Theorem 1 again, if U is the eigenvectors of M corresponding to nonzero eigenvalues, then(St∼+αI)-1Hb∼Uis the eigenvectors of(St∼+αI)-1Hb∼Hb∼T. Hence we have the following theorem:Theorem 2LetVSL-LDA∗be the optimal solution of SL-LDA as in Eq. (13),VW-LS∗be the optimal solution of weighted least square as in Eq. (18), it then follows:VSL-LDA∗=VW-LS∗U, where U are eigenvectors corresponding to the nonzero eigenvalues of M defined in Eq. (21).Theorem 2 has given the relationship between SL-LDA and W-LS, which leads to the following theorem:Theorem 3GivenVSL-LDA∗=VW-LS∗Uas in Theorem 2, we haveVSL-LDA∗VSL-LDA∗T=VW-LS∗VW-LS∗T. Accordingly, for any two samples xi and xj, it follows:The proof of Theorem 3 is given in Appendix 6.4. Following Theorem 3, it indicates that when applying a distance-based classifier such as KNN, SL-LDA and W-LS can achieve the same classification results, which leads us to propose a more efficient approach for solving SL-LDA. Specifically, instead of directly solving the GEVD problem as in Eq. (14), we can solve a W-LS problem as in Eq. (16) to obtain the projection matrix of SL-LDA. We next show the efficiency of the approach. Since (XLsXT+αI)X=X(LsXTX+αI), we can rewrite the optimal solution of W-LS in Eq. (19) as:(22)V∗=(XLsXT+αI)-1XLsTT=(XLsXT+αI)-1X(LsXTX+αI)(LsXTX+αI)-1LsTT=(XLsXT+αI)-1(XLsXT+αI)X(LsXTX+αI)-1LsTT=X(LsXTX+αI)-1LsTT.Note that for calculating the inverse of (LsXTX+αI), the computational complex is O(l+u)3, while for calculating the inverse of (XLsXT+αI), the computational complex is O(D3). Another efficient approach for solving the problem in Eq. (16) is LSQR [40] and the computational complex is O((l+u)sd), where s is the nonzero entries in X. Hence given X is high-dimensional and sparse, solving the W-LS problem in Eq. (19) can be more efficient than the GEVD method. The basic steps of the efficient approach are given in Table 2.After we have obtained the projection matrixVSL-LDA∗, given new coming sample x, its output in the reduced space can be obtained byz=VSL-LDA∗Tx. Note that the proposed SL-LDA is a linear method. It can be easily extended to the kernel version by using KPCA plus LDA trick [21,22].In this section, we evaluate our algorithms with two synthetic dataset and several UCI datasets and real-world datasets. For the synthetic datasets, we evaluate our algorithm using two-moon and two-Swiss-roll datasets. For other datasets, we focus on solving the classification problems based on 8 UCI datasets and 5 real-world datasets which are all benchmark datasets. Furthermore, we compare our algorithm with other state-of-the-art supervised and semi-supervised algorithms. For the comparative study, we randomly split each dataset into training set and test set. We also randomly select samples from the training set to form labeled and unlabeled set. All the training sets are preliminarily processed with a PCA operator to eliminate the null space before performing dimensionality reduction [3]. All algorithms used the training set in the output reduced space to train a nearest neighborhood classifier for evaluating the accuracy of test set.In this toy example, we generate a dataset with two classes, each follows a half-moon distribution. In each class, two samples are selected as labeled set and the remainings are as unlabeled set. Since the distribution of two-moon dataset is nonlinear, to handle this problem, we first perform KPCA to the two-moon dataset; we then use the output in the full-rank KPCA to train the linear methods [21,22]. Fig. 6shows the gray images of decision surfaces and boundaries obtained by LDA, SDA and SL-LDA. The gray value of each pixel represents the difference of distance from the pixel to its nearest labeled samples in different classes after dimensionality reduction. The decision boundaries are then formed by the pixels with the values equal to 0. In this example, we set the reduced dimensionality as 1. From Fig. 6 we can observe that for the two-moon dataset, the decision boundary learned by LDA cannot classify the two classes. This indicates that given insufficient labeled samples, LDA fails to find the precise boundary between different classes. In contrast, by using the unlabeled samples to construct the manifold term for preserving the geometrical structure embedded into the dataset, SDA can find the precise decision boundary. In addition, the proposed SL-LDA can achieve the best performance, as the decision boundary learned by SL-LDA is more precise than those obtained by SDA. There are also less misclassified samples in SL-LDA compared to SDA. The improvement is reliable due to the fact that SL-LDA preserves more discriminative information from soft labels which are obtained from label propagation process.In order to further evaluate the effectiveness of our algorithm, we generate a more challenging two-Swiss-roll dataset with two classes. Each follows a Swiss-roll distribution with the same core but increased radius. In each class, only one sample is selected as labeled set and the remaining as unlabeled set. We then investigate the effectiveness of different methods based on the dataset. Note that the two-Swiss-roll dataset is also nonlinear. We use the same method to handle the nonlinear problem as in two-moon dataset. Other parameters are set the same as in two-moon dataset. Fig. 7shows the gray images of decision surfaces and boundaries obtained by LDA, SDA and SL-LDA, where the gray value of each pixel and boundary represents the same means as in two-moon dataset. In Figs. 7a and b, we can see that both LDA and SDA have failed to discover the boundaries between two classes. This fact verifies that for two-Swiss-roll dataset, the limited labeled samples cannot provide sufficient discriminative information, hence causing the performance of classification unsatisfied. Moreover, in Fig. 7b, we can see that even with sufficient unlabeled samples to preserve the geometrical structure, SDA does not perform well. This is mainly because compared with the two-moon dataset, the two-Swiss-roll dataset has more complex geometrical structure as the samples in two classes parallel revolve around the same core. Hence, there is not a clear boundary that can divide the two classes into two sides. In Fig. 7c, we can see that though there are still some miss-classified samples, the proposed SL-LDA can achieve the best performance by incorporating more discriminative information obtained from soft labels.For classification problem, we use 8 UCI datasets and 5 real-world datasets to evaluate the performance of methods. The detailed information of dataset can be shown in Table 3. For each dataset, we randomly select l samples from each class as labeled set and u samples as unlabeled set. The test set is then formed by the selected or all remaining samples. The data partitioning for each dataset is also given in Table 3. We also show some sampled images of real-world datasets as in Fig. 8.Next, we compare our method with other supervised and semi-supervised dimensionality reduction methods. These methods include RLDA [23], RLS [28], CCA [15], SDA [9], LapLDA [10], LapRLS [11], SSCCA [12] and the proposed SL-LDA. Note that the original CCA [15] only deals with binary classification, therefore, we choose a variant version of CCA and SSCCA by using the c−1 label coding for solving multi-class classification problem [16]. We also compare our method with three label propagation methods, i.e. SSKDE [41], First-order LNP [7] and Second-order LNP [8], and we denote them as LNP1 and LNP2, respectively. The simulation settings are as follows: For all the methods that have Tikhonov regularized term, we set the same value of parameter αtin each dataset. For all the semi-supervised methods that have manifold regularized term, we set the same value of parameter αmand the same neighborhood size for constructing the neighborhood graph in each dataset. For the proposed SL-LDA, there are three parameters, i.e. the balancing parameter αjin Eqs. (3) and (4), the neighborhood size and the Tikhonov regularized parameter αt. For the first parameter, we use 5-fold cross validation to determine the value ofλand the candidate set forλis {10−9,10−6,10−3,100,103,106,109}; for other two parameters, we choose the same values as in other methods in each dataset.The training set in all datasets are preliminarily processed with PCA operator to eliminate the null space before performing dimensionality reduction. For supervised methods such as RLDA, RLS and CCA, we use only labeled set to train the learner. For semi-supervised dimensionality reduction methods, we use all the training set with both labeled and unlabeled set to train the learner. All algorithms used labeled set in the output reduced space to train a nearest neighborhood classifier for evaluating the classification accuracy of test set.The average accuracies over 20 random splits with the above parameters for each dataset are shown in Table 4. From the simulation results, we can obtain the following observation: (1) the semi-supervised dimensionality reduction methods are better than the corresponding supervised methods. For example, SDA, Lap-RLS and SSCCA outperform RLDA, RLS and CCA by about 5–8% in the Ionosphere, Wine and Waveform datasets. For other datasets, it can outperform by 2–3%. This indicates that by incorporating the unlabeled set into the training procedure, the classification performance can be markedly improved, as the manifold structure embedded in the dataset is preserved. (2) The proposed SL-LDA delivers the accuracies much better than those delivered by other semi-supervised dimensionality reduction methods such as SDA, Lap-LDA, Lap-RLS and SSCCA by about 2–3% in most datasets. The improvement can even achieve almost 5% in Letters and CMU-PIE datasets. This enhancement is believed to be true due to the reason that by taking into account the soft labels of unlabeled samples, which are obtained from label propagation process, the SL-LDA can preserve more discriminative information. While in SDA and Lap-RLS, these algorithms can only use unlabeled samples in a simple manner to preserve the manifold structure but cannot preserve more discriminative structure. (3) We also compare the proposed SL-LDA with two popular label propagation methods LNP1 and LNP2. Note that in these two methods, the predicted label of a test sample is obtained by reconstructing from the labels of its neighborhoods, where the reconstruction weights are calculated following Eq. (2)[9,10]. From Table 4, we can observe the proposed SL-LDA can achieve better performance than LNP1 and LNP2 by about 2% in the dataset. The reason for it is that SL-LDA is an inductive learning method and can learn a linear model. As the projection matrix can be obtained by incorporating the soft labels into the linear model, the new sample can be processed with the projection matrix and preserves more discriminative information. While in LNP, the label of a new sample can only be predicted using the label information of its neighborhoods. (4) In order to further show further show the superiority of the proposed method, we conduct more simulations for classification by varying the number of labeled samples. Table 5shows the average classification accuracy over 20 random splits on test set of COIL100 and CMU-PIE datasets with 2, 4, 6, 8, 10 samples per class selected as labeled set. Other parameters are set the same as above. From the simulation results we can observe that the proposed method can deliver better classification accuracy than those obtained by other supervised and semi-supervised methods by about 2–3% in most cases of fixed labeled number. The superiority can even achieve 5% when only 2 samples per class are selected as labeled set both in COIL100 and CMU-PIE datasets. This indicates that the proposed method is more suitable for the case when few samples are labeled. Since labeling large number of samples is time-consuming and costly, our proposed method is more practical than other semi-supervised methods in real-world application.In addition, in order to further verify the above analysis, we demonstrate the sub-manifold visualization of the proposed method and compare it with other state-of-the-art methods such as PCA, LPP, LDA, SDA and Lap-RLS. In this study, we randomly choose the first five classes of COIL100 dataset for simulation. In this dataset, we randomly select 2 samples per class as labeled set and 20 samples per class as unlabeled set. The remaining 50 samples per class are selected as test set. Hence the number of labeled samples is quite few compared with that unlabeled samples. Our goal is to visualize the test set in the 2d subspace by projecting the test set on the 2d projection matrix learned by different methods. The simulation results are shown in Fig. 9. From the simulation results, we can see that the proposed method can achieve the best performance than other methods, in a way that the sub-manifold of each object is closely conglomerated, while those belonging to different objects are clearly separated. The main reason is that the proposed method can provide more discriminative information compared with other state-of-art methods.The goal of content-based image retrieval (CBIR) is to retrieve relevant images from the dataset by using semantic similarity between imagery contents [47]. In this subsection, we evaluate the CBIR performance of the proposed SL-LDA and compare it with those of RLDA, RLS, SDA and Lap-RLS based on a large-scale image dataset, called Corel dataset [34]. In this dataset, we choose a subset from the dataset for simulation, which contains 50 concepts with each having 100 images [38]. Some samples of Corel dataset is shown in Fig. 10.In CBIR, low-level image representation is of great importance to characterize the feature information for each image. Current visual features include color, texture, shape and different combinations of the aforesaid methods. In this work, we combine a 48-dimensionality color histogram in HSV space [35] and a 48-dimensionality color texture moment (CTM) [36,37] to represent each image. For color histogram, in order to avoid the computation burden, we only consider 16 bins to calculate the histograms for each of the three HSV channels. Specifically, the histogram of the hue channel is calculated as follows:(23)hHi=nHi/nT,i=1,2,…,16,where Hiis the ith quantized level of the hue channel,nHiis the total number of pixels with their hue values in that level and nTis the total number of pixels in the images. The histograms of the Saturation (hSi) and Value (hVi) channels can also be calculated as in Eq. (23). For color texture moment (CTM), local Fourier transform (LFT) is adopted as a texture representation strategy and eight characteristic maps are derived for describing different aspects of co-occurrence relations of images pixels in the three channels of the color space (SVcosH, SVsinH and V). Then, CTM calculates the first and second moments of these maps to represent the natural color image pixel distribution.We next design an automatic feedback strategy to model the retrieval process. For each query image submitted by the user, the system retrieves and ranks the images in the database. The top 10 images are then selected as the feedback images and their feedback information can be used for re-ranking. It is worth noting that for the sake of feedbacks, users can either mark them as relevant or irrelevant ones or provide their class labels. Since our method is more proper to the class labels, we focus on the latter ones. The above automatic feedback strategy is performed for four iterations for each query image.But in a real image retrieval system, the query image is usually not in the image database. To simulate this environment, we divide the dataset into two non-overlapping subsets, i.e. the first one includes 4500 images used for training the system and the second one includes 500 images used as query images. For the image retrieval results, we use accuracy-scope curve [37] to evaluate the performance of different methods under a fixed iteration. The scope is the number of top-ranked images presented to the user and the accuracy is the ratio between the numbers of relevant images to the given scope. The accuracy-scope curve describes the accuracies with varied scopes and thus gives an overall performance evaluation of the algorithms. We then fix the scope by 10, 20, 50 and evaluate the performance of different methods with varied iterations.Fig. 11shows the average accuracy-scope curves of different methods at Iteration 2 and Iteration 4. Fig. 12shows the average accuracies with varied iterations under the fixed scope 10, 20 and 50. From Figs. 11 and 12, we observe: (1) All of the five methods i.e. RLDA, RLS, SDA, Lap-RLS and SL-LDA can achieve better performance when iteration increases from 2 to 4, which indicates that the user-provided feedbacks are helpful to enhance the performance of image retrieval. (2) The semi-supervised dimensionality reduction methods are better than their corresponding supervised methods due to the utilization of the unlabeled set, i.e. SDA and Lap-RLS can achieve approximately 4–5% improvements to RLDA and RLS in most cases. (3) By iteratively adding the user’s feedbacks, the proposed SL-LDA can achieve much better retrieval results than other methods over the entire scope and iterations. The improvement can be the largest by approximately 5% compared to other methods when the scope is fixed to 10, 20 in the last round of feedback, shown in Fig. 12. This is demonstrated by incorporating the soft labels of unlabeled samples into learning, SL-LDA can preserve more discriminative information than other methods hence enhance the performance of image retrieval.

@&#CONCLUSIONS@&#
In this paper, we propose a new semi-supervised dimensionality reduction method by incorporating the soft label of unlabeled samples into learning. The soft labels can be obtained by a specially designed label propagation, which is based on a new local reconstruction graph with symmetrization and normalization. The symmetrization can add more connections of a sample with its neighborhoods and the normalization can handle with the case when the density of dataset varies dramatically. As an extension to the conventional LDA, the proposed SL-LDA can preserve more discriminative information embedded in the soft labels than other state-of-the-art methods, which can be beneficial to the performance of classification. In addition, motivated by the equivalence between the conventional LDA and least square problem [18–20], we further extend this relationship and analyze SL-LDA under a weighted least square problem (W-LS). We then propose a more efficient approach for solving SL-LDA. Theoretical analysis and extensive simulations show the effectiveness of our algorithm. The results in simulations demonstrate that our proposed algorithm can achieve great superiority compared with other existing methods.Denote en=[1,1,… ,1]∊R1×nas the unit vector, to prove Corollary 1, we need to prove ec+1F=el+u. Actually, we first have el+uP=el+uand ec+1Y=el+u, then:(24)ec+1Y=el+uel+uP=el+u→ec+1YIβ+el+uPIα=el+u⇒ec+1YIβ=el+u(I-PIα)⇒ec+1YIβ(I-PIα)-1=el+u⇒ec+1F=el+uTo prove Corollary 2, we first rewriteSb∼as:(25)Sb∼=∑i=1c∑j=1l+ufij(μi∼-μ∼)(μi∼-μ∼)T=XFTG-1-XEeTeceEeTGXFTG-1-XEeTeceEeTT=XFT-XEeTeFTeEeTG-1XFT-XEeTeFTeEeTT=XE-XEeTeEeEeTE-1FTG-1FE-1XE-XEeTeEeEeTT.The second equation holds as[μ1∼,μ2∼,…,μc∼]=XFTG-1andμ∼=XEeT/eEeT, the third equation holds as ecG=eFT. Since Ls=E−EeTeE/eEeTand T=G−1/2FE−1, following Eq. (25), we haveXLsTTTLsXT=Sb∼.Let τ be a nonzero eigenvalue of AB and v be its corresponding eigenvector, it follows ABv=τv≠0→Bv≠0 and ABv=τv→BABv=τBv. Hence, we have that Bv is the eigenvector of BA corresponding to the same nonzero eigenvalue τ.On the other hand, if τ is a nonzero eigenvalue of BA and u be its corresponding eigenvector, similarly, we can also verify that Au is an eigenvector of AB corresponding to the same nonzero eigenvalue τ. As analyzed above, AB and BA have the same nonzero eigenvalues, and for each nonzero eigenvalue, if the corresponding eigenvector of AB is v, then the corresponding eigenvector of BA is u=Bv.Let the rank of auxiliary matrix M be q, then, U∊Rc×qis an orthogonal matrix formed by the eigenvectors corresponding to the q nonzero eigenvalues of M. Sinceq⩽c, there exists an orthogonal matrix U⊥∊Rc×(c-q) formed by the eigenvectors corresponding to the c-q zero eigenvalues of M, which satisfies UTU⊥=O and follows:(26)U⊥TMU⊥=O→U⊥THb∼TSt∼+αI-1Hb∼U⊥=O→Hb∼U⊥=O.The third equation holds as(St∼+αI)is positive definite. Recall that the optimal solution of W-LS is(St∼+αI)-1Hb∼, it followsVW-LS∗U⊥=(St∼+αI)-1Hb∼U⊥=O. In addition, letU∼∈Rc×cbe the orthogonal matrix formed byU∼=[U,U⊥], it followsU∼TU∼=U∼U∼T=I. Hence we have(27)VW-LS∗VW-LS∗T=VW-LS∗U∼U∼TVW-LS∗T=VW-LS∗UUTVW-LS∗T+VW-LS∗U⊥U⊥TVW-LS∗T=VW-LS∗UUTVW-LS∗T.SinceVSL-LDA∗=VW-LS∗U, following Eq. (27), we thus proveVW-LS∗VW-LS∗T=VSL-LDA∗VSL-LDA∗T.