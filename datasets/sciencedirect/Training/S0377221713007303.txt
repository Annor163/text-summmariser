@&#MAIN-TITLE@&#
An interactive evolutionary multi-objective optimization algorithm with a limited number of decision maker calls

@&#HIGHLIGHTS@&#
Interactive evolutionary algorithm for handling many objective optimization problems.Incorporates decision maker preferences in intermediate steps of the algorithm.Polyhedral cones and sets are used to modify domination and perform a focussed search.Successfully handles optimization problems up to five objectives.

@&#KEYPHRASES@&#
Evolutionary multi-objective optimization,Multiple criteria decision-making,Interactive multi-objective optimization,

@&#ABSTRACT@&#
This paper presents a preference-based method to handle optimization problems with multiple objectives. With an increase in the number of objectives the computational cost in solving a multi-objective optimization problem rises exponentially, and it becomes increasingly difficult for evolutionary multi-objective techniques to produce the entire Pareto-optimal front. In this paper, an evolutionary multi-objective procedure is combined with preference information from the decision maker during the intermediate stages of the algorithm leading to the most preferred point. The proposed approach is different from the existing approaches, as it tries to find the most preferred point with a limited budget of decision maker calls. In this paper, we incorporate the idea into a progressively interactive technique based on polyhedral cones. The idea is also tested on another progressively interactive approach based on value functions. Results are provided on two to five-objective unconstrained as well as constrained test problems.

@&#INTRODUCTION@&#
The evolutionary multi-objective optimization (EMO) algorithms have demonstrated their ability in solving complicated multiple objective problems (Deb, 2001; Coello, VanVeldhuizen, & Lamont, 2002). They have been successful in handling two to three objective test problems, but thereafter, the deterioration in performance becomes noticeable (Deb, Thiele, Laumanns, & Zitzler, 2005; Deb & Saxena, 2006; Knowles & Corne, 2007) both in terms of convergence1In evolutionary multi-objective optimization, convergence refers to the proximity of the solutions to the Pareto-optimal frontier.1and diversity2In evolutionary multi-objective optimization, diversity refers to the spread of solutions approximating the Pareto-optimal frontier.2(Deb, 2001). The deterioration in performance while solving problems with larger number of objectives is primarily due to stagnation in search as the Pareto-dominance looses its discriminatory potential in higher dimensions. Moreover, the requirement of an exponentially increasing population size to explore the Pareto-optimal front leads to a huge computational expense. Difficulty in visualization of the objective space further leads to additional challenges related to performance evaluation of the algorithm as well as decision making. These difficulties are inherent to an optimization problem with a larger number of objectives, and efficient procedures are required. In this paper, we introduce a methodology, which can be integrated with any evolutionary multi-objective optimization algorithm allowing it to effectively handle problems with multiple objectives.The EMO algorithms aim for well spread solutions close to the Pareto-optimal front for two to three objective problems. The decision maker (DM) is expected to choose the most suitable point from an array of approximately Pareto-optimal points found by the EMO algorithm. However, in this paper we propose to integrate the DM with the optimization run of an EMO algorithm in a way such that the preferences of the DM can be incorporated into the intermediate generations of the algorithm. Such an integration leads to progress towards the most preferred point.3The most preferred point is the point on the Pareto-optimal front which gives maximum utility/satisfaction to the DM when compared with other points on the front.3This point is of course unknown at the start of the optimization run and the proposed algorithm tries to get as close to this point as possible, based on the preference information provided by the DM. Such a procedure, where a DM is involved in the intermediate generations of an EMO algorithm, is called a progressively interactive EMO approach (PI-EMO) (Deb, Sinha, Korhonen, & Wallenius, 2010; Branke, Greco, Slowinski, & Zielniewicz, 2009). A progressively interactive approach is a DM-oriented approach, which allows the DM to guide the algorithm towards the most preferred point. The working of such an approach can be observed from Fig. 1for a two-objective maximization problem. The advantage associated with seeking the most preferred point, instead of the entire Pareto-optimal front, is that it saves us from the intricacies involved in exploring the entire multi-dimensional front.This paper proposes a simple scheme, which could be integrated into any progressively interactive EMO algorithm to approach the most preferred point in limited number of interactions with the decision maker. The budget of DM calls4A DM call is an event where the algorithm seeks preference information from the DM.4is taken as input at the start of the optimization run, and then the decision maker is invited to provide preference statements whenever required. The concept is integrated with a progressively interactive EMO approach based on polyhedral cones (PI-EMO-PC) (Sinha, Deb, Korhonen, & Wallenius, 2010), which is elaborately discussed in the paper. The scheme of limited budget of DM calls is generic, and is also demonstrated on another progressively interactive EMO approach based on value functions (PI-EMO-VF) (Deb et al., 2010). Results are provided for three unconstrained and two constrained test problems having two to five objectives.A multi-objective optimization problem inherently consists of two tasks, namely, search and decision making. These two tasks can be combined in various ways to generate procedures, which can be classified into three broad categories, i.e. apriori approach, aposteriori approach and interactive approach. In this section, we provide a review for the methods falling in each of these categories. Discussion about preference-based methods can also be found in the review papers by Rachmawati and Srinivasan, 2006 and Branke, 2008.In the apriori approach, preferences are elicited before the start of the algorithm; then the optimization task is executed by incorporating the preference information, and the most preferred solution is identified. Biased niching based EMO (Branke & Deb, 2004), the reference direction based EMO (Deb & Kumar, 2007), reference point-based EMO approaches (Deb, Sundar, Uday, & Chaudhuri, 2006; Thiele, Miettinen, Korhonen, & Molina, 2009), and the light beam approach based EMO (Deb & Kumar, 2007) represent some of the efforts in the direction of utilizing preference information before the start of an EMO algorithm. Once the information is available in the form of a reference direction or reference point, the algorithm finds the most-preferred point without any further interaction with the DM. Deb et al., 2006 used the concept of the reference point, but did not apply an Achievement Scalarizing Function; they rather used a weighted Euclidean distance to rank population members. In a later study, Thiele et al., 2009 implemented a similar idea using the Achievement Scalarizing Function. Another simple algorithm based on this approach is to modify dominance based on ranks obtained for a few alternatives from the DM. Such an approach was used by Greenwood, Hu, and D’Ambrosio, 1996 in their study. Tiwari, Wiecek, and Fadel, 2008 used pre-determined preference cones in an evolutionary algorithm to converge to a part of the Pareto-optimal frontier. They considered two objective test problems in their study.Information in an apriori approach is elicited towards the beginning, therefore, the solution obtained after executing the algorithm is usually not the best solution and may not even be close to the most preferred solution. The preference structure of the DM at the beginning might be different from the preference structure at the Pareto-optimal front. Therefore, the approach is highly error prone, as even slight deviations in providing preference information at the beginning may lead to entirely different solutions.Most of the evolutionary multi-objective algorithms (Deb, Agrawal, Pratap, & Meyarivan, 2002; Zitzler, Laumanns, & Thiele, 2001) which aim to find the entire frontier, are classic examples of the aposteriori approach. In such methods, the decision making aspect is ignored and the entire Pareto-optimal frontier is generated before incorporating the DM. However, as already mentioned, there are enormous difficulties in finding the entire Pareto-optimal front for a problem having a large number of objectives. Choosing the most preferred solution from this front makes the problem even more challenging.Realizing the various difficulties associated with the above two approaches, in recent years there has been interest towards development of interactive EMO algorithms, particularly for problems having a large number of objectives. A variety of interactive methods have been presented in the literature, but our focus is on progressively interactive techniques. As already mentioned, progressively interactive methods converge towards a particular region of the frontier by incorporating preferences obtained from the DM in the dominated regions of the objective space. Preference elicitation is performed during the course of optimization such that a progress towards the most preferred point is made. Some of the recent work in the direction of progressively interactive techniques are Phelps and Koksalan, 2003, Fowler et al., 2009, Jaszkiewicz, 2007, Branke et al., 2009, Koksalan and Karahan, 2010, Deb et al., 2010. Next, we briefly highlight the salient features of these studies.Phelps and Koksalan, 2003 periodically accept preferences from the DM and construct a linearly weighted sum of objectives, which is optimized in the subsequent generations using an evolutionary algorithm. Fowler et al., 2009 send a few solutions to the DM for ranking, and construct a convex preference cone, which is used to rank the members not considered by the DM. The study assumes a quasi-concave preference structure for the DM, and presents the results on multi-dimensional knapsack problems. Jaszkiewicz, 2007 also uses linear value functions, however, his strategy is to select a set of compatible linear value functions from randomly generated linear value functions. The selected value functions are then used within the EMO algorithm to explore the preferred regions on the frontier. In fact, many algorithms use linear value functions, but they have limitations in handling problems where the most preferred point lies on a non-convex part of the Pareto-optimal front. Branke et al., 2009 implemented the GRIP (Figueira, Greco, & Slowinski, 2009) methodology, where the preference information from the DM is used to construct all possible additive value functions conforming to the preferences. This guides the search of the EMO algorithm towards interesting regions of the frontier. They extended the idea in a later study (Branke, Greco, Slowinski, & Zielniewicz, 2010). In their approach, the DM is frequently invited to provide preferences, and the implementation has been done on two-objective test problems. It has not been demonstrated in the study, whether the method scales efficiently when handling higher number of objectives. A study by Koksalan and Karahan, 2010 defines a territory around preferred solutions elicited from the DM, and generates more solutions around them, obtaining a denser coverage of the interesting regions. Interaction with the DM is again performed, and based on this idea the algorithm gets close to the most preferred point. Some other studies, which utilize preference statements in an EMO interactively, are Kaliszewski, Miroforidis, and Podkopaev, 2011, Said et al., 2010. Kaliszewski et al., 2011 have developed a decision support scheme which could be used with an EMO. It operates by defining upper and lower bounds for solutions, and the process continues until the bounds are tight. Said, Bechikh, and Ghedira, 2010 use a variant of the Pareto-dominance criterion, which has the ability to create a strict partial order. This helps in guiding the search towards the preferred parts of the Pareto optimal frontier. The DM’s preferences are elicited as a set of aspiration levels.In a recent study (Deb et al., 2010), the authors have proposed a progressively interactive EMO method, which uses implicitly defined value functions (PI-EMO-VF). In this approach the information from the DM is elicited and used to construct a polynomial value function. At different stages of the algorithm, the implicitly constructed value function maps the preference information provided by the DM, and is subsequently used for making decisions. Another recently suggested methodology (Sinha et al., 2010) eliminates the process of constructing a value function by using polyhedral cones. This method uses preference information from the DM in the form of the best point from a given set. This information is used to formulate a polyhedral cone, which is used to modify the domination criterion, and drive the EMO procedure towards the region of interest.Most of the progressively interactive methods follow a similar framework. In this section, we describe a general framework for such methods, and discuss what makes these methods different. Flowchart in Fig. 2shows the steps followed by a progressively interactive approach.For most of the progressively interactive methods, an initial set of solutions is generated either randomly or using evolutionary search with a standard domination criterion. Once an initial set of solutions is available, DM preferences are elicited on a selected set of solutions to construct a domination criterion, which helps an evolutionary algorithm perform a focussed search in the region of interest. The DM preferences are elicited alternately with the evolutionary search until the termination criterion is met. The algorithms usually produce a single most preferred point as the final outcome. Each step in the approach can be performed in different ways, for example, preference information can be accepted in terms of best point from a given set, or rank ordering a set of points, or pairwise comparisons of given pairs. Similarly, domination condition can be constructed in various ways for example in the context of this paper it may be a polyhedral-based domination criterion or a value function-based domination criterion. Other things which make algorithms different are, how long the evolutionary search is performed and when are the preferences elicited. Therefore, the existing approaches differ from each other in one or more of the following ways:1.How are the preference statements elicited?When are the preference statements elicited?How are the preference statements converted into an effective domination criterion?The main focus of this paper is to tackle point 2 from the above list in an effective manner, so that a limited number of DM calls can be efficiently utilized. Now, we provide a brief description how a fixed budget DM call scheme can be implemented for multi-objective problems, and then show its working on a specific progressively interactive approach in the next section.The proposed fixed budget scheme requires the ideal pointI→to start. The ideal point is determined by maximizing each of the objectives individually. An individual maximization of the objectives gives at most M pointsP1→,P2→,…,PM→. A two-objective case has been shown in Fig. 3where maximization of the objectives gives 2 pointsP1→andP2→. Once the ideal point is known, the initial random population is created. The point(Pib→)closest5Euclidean distance from the ideal point is computed in the objective space for each of the randomly generated points. The point with the shortest Euclidean distance is denoted asPib→.5to the ideal point(I→)is chosen and its distance from the ideal point is denoted asDI=|I→-Pib→|.DI→=I→-Pib→denotes the vector fromPib→toI→. This distance DIis divided intodI=DITDMequal parts (step-size). This is done to make sure that the preference information from the DM is elicited only after fixed progress has been made. The distance traversed is measured by projecting the progress of one of the points (discussed later) from the population on the vectorDI→.Thereafter, an evolutionary search is alternated with decision making instances, until the termination criterion is met. This simple mechanism ensures that the decision maker is invited only after a significant progress is made, and also guarantees that the search would terminate in less than or equal to the maximum allowed number of DM calls. It is also possible to implement the scheme by using a variable step-size instead of keeping it fixed. It might be interesting to study the performance of the scheme with larger step-sizes in the beginning and smaller step sizes in the end. In this paper, we perform the study with a fixed step-size, and then show the advantages of variable step-size towards the end.When the above scheme is integrated with a standard EMO algorithm (such as NSGA-II (Deb et al., 2002), SPEA-II (Zitzler et al., 2001) and others), and the domination criterion is based on the preferences of the decision maker, the approach leads to the most preferred solution in a fixed budget of DM calls. In this paper, we incorporate the fixed budget scheme in a progressively interactive approach based on polyhedral cones. The approach approximates the preferred region of the decision maker by utilizing the preference statements of the decision maker without requiring any significant parameter to be set, apart from the standard parameters used in evolutionary algorithms. The steps involved in the algorithm are discussed in detail in the next section. Later, we also test the fixed budget scheme on the value function-based approach and compare the performance with the polyhedral cone-based approach.In this section, we propose a progressively interactive EMO algorithm (PI-EMO-PC)6For an earlier version, see Sinha et al., 2010, where there is no mechanism to control the number of interactions with the DM.6which uses the notion of polyhedral cones and sets to converge towards the most preferred point of an M objective problem. Though the polyhedral cones and sets provide only an approximation of the preferred region, they are sufficient to guide an EMO algorithm towards the most preferred point by eliciting preference information from the DM at various stages of the algorithm. Before discussing the step-wise procedure for the algorithm, we introduce some concepts used in the paper.A cone is a nonempty setC⊂RMfor which c∈C⇒λc∈C where λ>0. A cone is convex if c(1), c(2)∈C⇒c(1)+c(2)∈C. In the following definitions we define a polyhedral cone and a polyhedral set.Definition 1Polyhedral cone: IfA∈RL×Mis a matrix, then a polyhedral coneC(A)⊂RMdetermined by A is defined as:(1)C(A)≔{c∈RM:Ac⩾0}Polyhedral set: IfA∈RL×Mis a matrix, andb∈RLis a vector, then a polyhedral setC(A,b)⊂RMdetermined by A and b is defined as:(2)C(A,b)≔{c∈RM:Ac⩾b}A polyhedral cone is a special kind of cone which is defined by a solution set of a homogeneous system of linear inequalities. In M dimensions polyhedral cones may be constructed with any finite number of bounding hyperplanes. However, in the context of this paper we consider only polyhedral cones constructed with M different hyperplanes in M dimensions. A polyhedral set is an intersection of a finite number of halfspaces and is always convex.In evolutionary algorithms, at any generation we have a population of non-dominated solutions. If one of the solutions is known to be the best in the population, we utilize it to create a polyhedral cone and set. Since we aim to produce a single point on the Pareto-front, close to the most preferred point, we require two important features in the preference-based approach. The first feature being identification of a proper search direction, and the second feature being, identification of the preferred region. These two features are important for a preference-based approach to converge to one of the preferred points on the Pareto-optimal front. In the proposed approach, we use a polyhedral cone at each DM call, and choose a search direction based on the cone to perform a local search (LS). A preferred region is also determined for subsequent exploration by EMO.Now, we discuss our PI-EMO-PC procedure in the form of a sequence of steps, integrated with the fixed budget scheme:Step 0:Determine the ideal pointI→by maximizing each of the objectives individually using any single-objective optimization algorithm.Initialize a population Par0, and set population iteration counter t=0. Initialize archive set7The initial size of the archive (A) is 0. The maximum size the archive can have is ∣A∣max, a subjectively determined parameter.7A. Determine the point closest(Pib→)to the ideal pointI→and find the vectorDI→=I→-Pib→. Store the distancedI=DITDM, where TDMis the maximum number of DM calls.Increment the counter as t←t+1 and execute the EMO algorithm generation with the usual Pareto-domination criterion. The best memberPcb→in the current population is the one closest to the ideal point which dominatesPib→. All the feasible non-dominated solutions found at the end of the generation are added to the archive A.If projected distance ofPcb→-Pib→onDI→is less than dIthen go to Step 2; otherwise store the function evaluations required during the improvement as fI. Initialize c as 1 and call the DM to choose the best solutionAc→best, from the archive Atusing VIMDA (Korhonen & Karaivanova, 1999). Here c refers to the DM call. Choose the end points8In a maximization problem, end points in a set of non-dominated solutions are those members, which have maximum value for one of the objectives. However, it may not be trivial. Refer Section 4.3 for further details.8from the non-dominated front of the current parent population Partas rest of the solutions. This makes the chosen number of points for polyhedral cone construction as η=M+1.Construct the sides of the polyhedral cone from the chosen set of η=M+1 points, described in Section 4.3. Set the function evaluation counter f=0.An offspring population, Offt, is created from the parent population, Part, by executing an iteration of the EMO algorithm. Parent selection in the EMO algorithm is performed using modified domination criterion (discussed in Section 4.5), and then crossover-mutation operators are used to generate the offsprings. Increment the function evaluation counter f by number of function evaluations done in this step. Update the archive by including the offspring population into it and then removing the dominated solutions from the archive. If the archive size exceeds ∣A∣max, k-mean clustering is used to keep the diverse set of ∣A∣maxclusters; rest of the solutions are deleted.From parent population, Part, and offspring population, Offt, a new population, Part+1, is determined, using modified domination criterion and EMO algorithm’s diversity preserving operator. The iteration counter is incremented as t←t+1 and the algorithm moves to Step 5 if f is less than fI; otherwise proceed to Step 7.Increment c by 1 and call the DM to choose the best solutionA→cbest, from the archive At. The previous best solution chosen by the DM is stored asA→c-1best. The polyhedral cone is constructed and the search direction is determined. Projected distance of vectorA→cbest-A→c-1bestonDI→is computed and stored as dA.If the projected distance dAis less than dI, a local search is performed withA→cbestas the reference point along the search direction. The best solution,A→cbest, chosen by the DM is updated at each iteration of the local search. The local search is stopped if|A→cbest-A→c-1best|>dI, the function evaluation counter is reset as f=0, and the algorithm proceeds to Step 5. If local search is unable to produce better solutions, then the algorithm is terminated and the current best solution is chosen as the final outcome.The PI-EMO-PC algorithm requires the parameters ∣A∣maxand TDMin addition to the EMO algorithm’s parameters to be specified.While solving a problem it will often occur that not the entire budget of DM calls is utilized, i.e. c<TDMat the end of the algorithm. In this case we recommend to move back toAαbest, whereα=c-TDM-c2, updatedI=dI2and start the algorithm from that point again. To implement fine-tuning, it is necessary to store the population members and the archive set at the end of each local search, i.e. Step 8 of the algorithm above. The stored parent population and the archive set are used to start the fine-tuning algorithm with a smaller dIvalue.The above fine-tuning procedure can also be utilized if the DM is unhappy with the final solution. In that case the DM specifiesTDM′and the algorithm moves back toAαbestwhereα=c-TDM′2and dIis updated asdI=dI2.At an instance of a DM call, the DM is provided with the archive set from which the best point is chosen. The chosen point forms the apex of the polyhedral cone in M-dimensional space, and we require M additional points to construct the complete cone. The end points of the non-dominated part of the parent population at that instant are chosen as the remaining points for constructing the bounding hyperplanes of the polyhedral cone. In most of the cases, the number of end points will be equal to M, and including the apex we will have M+1 points for constructing the polyhedral cone. However, there could be a few anomalies, where the total number of points is not equal to M+1; we handle such cases later in this sub-section.Choosing a set of M−1 points from the set of M end points and the apex gives us one of the bounding hyperplanes for the polyhedral cone. By considering all possible combinations, we are able to construct M different bounding hyperplanes for the polyhedral cone. All the chosen points are non-dominated with respect to each other, therefore if we choose any of the M bounding hyperplanes, then one of the two normals to that plane has non-negative components. This property is important when we decide the search direction for local search in the next sub-section.For instance, Figs. 4 and 5show the polyhedral cones in two and three dimensions. The equation of each hyperplane can be written aspi⊤f+qi=0,i∈{1,…,M}. If a given pointf(0)=(f1(0),…,fM(0))in the objective space haspi⊤f(0)+qi>0∀i∈{1,…,M}, then the point lies inside the polyhedral cone; otherwise it lies outside. In Fig. 4, the shaded region haspi⊤f+qi<0foratleastonei∈{1,…,M}and the unshaded region haspi⊤f+qi>0∀i∈{1,…,M}. Similarly, in Fig. 5, the unshaded polyhedral cone representspi⊤f+qi>0∀i∈{1,…,M}.There could be a scenario, where we do not have M+1 points to construct the polyhedral cone. Such a scenario could occur under two circumstances. Firstly, if the DM chooses an end point along a particular objective as the best point, and secondly, if there is an end point which is common along two or more objectives. To tackle such a situation, we propose the creation of artificial points in order to construct the polyhedral cone. If the DM chooses an end point (sayf1(0),…,fi(0),…fM(0)) along a particular objective i as the best point, then we create an artificial point asf1(0),…,fi(0),…fM(0)-(δ1,…,δi,…,δM), with δi=0 and δj≠i=10−5. The created artificial point is chosen as the new end point. If a single point happens to be the end point for two or more objectives, then for each of the objectives we create an artificial point as an end point using a similar strategy.A different scenario could be that we have more than one end-point corresponding to a particular objective. This would rarely happen, but under such a situation one of those points is chosen randomly as the end point corresponding to that objective.Once the polyhedral cone is determined, it provides an idea for a search direction. The non-negative normal unit vectors(Vi^)of all the M hyperplanes can be summed up to get a search directionW→=∑i=1nVi^.W^is the unit vector alongW→and Wi∀i∈{1,…,M} represent the components of the vectorW→. Since all the unit vectors are non-negative, therefore the final search direction would not lead to deterioration in any of the objectives. This direction has been used to determine if the optimization process should be terminated or not. To implement this idea we perform a single-objective search along the identified direction.We solve the following Achievement Scalarizing Function (ASF) problem (Wierzbicki, 1980) for the best point from the archive(A→cbest). In the following equation,zb=A→cbest:(3)Maximizemini=1Mfi(x)-zibWi+ρ∑j=1Mfj(x)-zjbWj.subjecttox∈S.In the above formulation,Sdenotes the feasible decision variable space of the original problem. The second term has a small constant ρ (=10−10 is suggested), which prevents the method from converging to a weakly Pareto-optimal point. The sequential quadratic programming (SQP) optimization method is used to solve the above problem and the intermediate solutions in the objective space (z(i), i=1,2,…) are recorded. If at any intermediate point,z(i)-A→c-1best·DI→DIis larger than dI, the Achievement Scalarizing Function optimization is stopped and we continue with the EMO algorithm. In this case, we replaceA→cbestwith z(i) in the archive set, and update the archive set At, by deleting the dominated members.A→cbestreplaces the member closest to it in the parent population Part. Fig. 6depicts this scenario. On the other hand, if at the end of the SQP run, the final SQP solution (say, zT) does not meet the criterionz(i)-A→c-1best·D→IDI>dI, we terminate the EMO algorithm and declare zTas the final preferred solution. This situation indicates that local search is unable to find any solution in the search space along the search direction, which can further improve the best solution obtained so far. Hence, we can terminate the optimization. Fig. 7shows such a situation, warranting a termination of the PI-EMO procedure.The polyhedral set, obtained as a result of the constructed polyhedral cone, has been used to modify the domination criterion in order to emphasize and create preferred solutions. The polyhedral set from the most recent decision-making interaction is given bypi⊤f+qi⩾0∀i∈{1,…,M}. The bounding hyperplanes of the polyhedral set are used to eliminate regions, and focus the search entirely on the region preferred by the DM. The ith hyperplane of the polyhedral set divides the region into two half-spaces,pi⊤f+qi⩾0andpi⊤f+qi<0. The region represented bypi⊤f+qi<0is assumed to be less preferred over region represented bypi⊤f+qi⩾0. Therefore, the intersection of the regions represented bypi⊤f+qi⩾0∀i∈{1,…,M}gives the preferred region in which we perform a focused search after interaction with the DM. Based on this idea we develop a modified domination criterion in this sub-section.Once the polyhedral set is known, any two feasible solutions, x(1) and x(2), are compared with their corresponding objective function values, f(1) and f(2), by using the following modified domination criterion:1.Ifpi⊤f(1)+qi≥0andpi⊤f(2)+qi⩾0∀i∈{1,…,M}, then the two points are compared based on the usual Pareto-domination principle.Ifpi⊤f(1)+qi<0andpi⊤f(2)+qi<0foratleastonei∈{1,…,M}, then the two points are compared based on the usual Pareto-domination criterion.Ifpi⊤f(1)+qi≥0∀i∈{1,…,M}, andpi⊤f(2)+qi<0foratleastonei∈{1,…,M}, then the former dominates the latter.Fig. 8illustrates the region dominated by two points A and B. The set formed by the linear equations has been shown. Point A lies in the region in whichpi⊤f+qi<0foratleastonei∈{1,…,M}. The region dominated by point A is shaded. This dominated area is identical to that which can be obtained using the usual Pareto-domination criterion. However, point B lies in the regionpi⊤f+qi>0∀i∈{1,…,M}. For this point, the dominated region is different from that which would be obtained using the usual Pareto-domination criterion. In addition to the usual region of dominance, the dominated region includes all points which havepi⊤f+qi<0foratleastonei∈{1,…,M}.The constrained handling mechanism as defined in Deb et al., 2002 is used. When two solutions under consideration for a domination check are feasible, then the above modified domination criterion is used. If one point is feasible and the other is infeasible, then the feasible solution is considered to be dominating the infeasible solution. If both points are infeasible, then the one having smaller overall constraint violation is declared to be dominating the solution with higher overall constraint violation.The PI-EMO-PC procedure has been implemented on the NSGA-II algorithm. However, it is possible to integrate the procedure with any other EMO algorithm. Firstly a few generations of the algorithm, as discussed in the steps above, are performed according to the usual NSGA-II algorithm (Deb et al., 2002) and the archive set is maintained. Thereafter, the NSGA-II algorithm is modified by using the modified domination criterion (discussed in Section 4.5) in the elite-preserving operator and tournament selection for creating the offspring population. The NSGA-II recombination operator (SBX) has been used in this study without any modification. The crowding distance operator of NSGA-II is not used in this implementation because of its poor performance (Deb, Mohan, & Mishra, 2003) in higher dimensions. It has been replaced with k-mean clustering for maintaining diversity among solutions of the same non-dominated front.An archive A is maintained which contains all the non-dominated members found in the current as well as the previous iterations of the optimization run. Archiving makes sure that most of the non-dominated solutions that are generated at the intermediate steps of the algorithm are never lost, even if the DM makes an error while providing preference information at a particular DM call.For local search (discussed in Section 4.4), the SQP code of KNITRO (Byrd, Nocedal, & Waltz, 2006) software has been used in this paper to solve the single objective optimization problem. The SQP algorithm is terminated either whenz(i)-A→c-1best·DI→DI>dIor when the Karush–Kuhn–Tucker (KKT) error measure is less than or equal to 10−6. If the local search terminates due to the KKT measure, then the overall PI-EMO-PC algorithm gets terminated.As already discussed, we need to find out the ideal point before the start of the algorithm. We use the parent centric crossover (PCX) based single-objective algorithm (Sinha, Srinivasan, & Deb, 2006) for the single-objective optimization. The algorithm is executed for a maximum of 2000 function evaluations for each of the objectives ignoring all the other objectives, but keeping the constraints. These function evaluations are counted separately from the function evaluations required to solve the problem. While presenting the results later on, we assume that the ideal points are given. One could use any single objective optimization algorithm (Powell, 2006; Hansen & Ostermeier, 2001) to find an approximation to the ideal point. If the number of function evaluations is very high, then the ideal point can be replaced with an aspiration point elicited from the DM.

@&#CONCLUSIONS@&#
In this paper, a progressively interactive EMO algorithm has been proposed which uses the idea of polyhedral cones and polyhedral sets to move towards the most preferred point. Before the start of the optimization the DM is expected to provide information about the maximum number of times she will be available to provide preference information. This is called the budget of DM calls, and the algorithm tries to find the most preferred point within this budget by accepting preference information in the intermediate steps of the algorithm. Preference information is accepted in terms of the best member from the given archive set. Eliciting preferences progressively provides the algorithm information to focus the search on a specific area of the search space. The algorithm has been rendered self-adaptive such that it makes a decision when to switch to a local search and when to elicit preference information from the DM. It brings the DM and the algorithm together, providing more control to the DM over the optimization process. The procedure successfully handles two to five-objective constrained and unconstrained test problems. A parametric study has been done for the parameters, maximum number of DM calls, and maximum archive size. It has been shown that the budget of DM calls is critical in defining the accuracy of the optimization, with higher values leading to better accuracy. The performance of the algorithm shows that such progressively interactive techniques are capable of handling optimization problems with a relatively large number of objectives efficiently.