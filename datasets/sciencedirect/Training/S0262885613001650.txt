@&#MAIN-TITLE@&#
Modeling and correction of multipath interference in time of flight cameras

@&#HIGHLIGHTS@&#
Multipath Interference (MpI) is a non-systematic error in Time of Flight (ToF) imaging.MpI causes severe distortions in ToF measurements.We propose a ray tracing model that renders realistic ToF images, including MpI.We use nonlinear optimization to find the scene that best renders the input image.We present results with real and synthetic datasets. Error is reduced in all cases.

@&#KEYPHRASES@&#
Time of Flight (ToF),Multipath Interference (MpI),Iterative method,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
A Time of Flight (ToF) camera is a sensor that gives, for every pixel, the distance between the camera and the scene in a particular direction. As a result, the camera produces a range image (i.e. 2.5D image) of the scene. To get distances, ToF cameras measure the time-of-flight of a light signal that impacts the scene and returns back to the sensor. In practice, ToF cameras use modulated infrared light sources, measuring time-of-flights with signal phase shifts (see [1]).Currently, ToF cameras represent one of the leading technologies in range imaging. Commercial ToF cameras have reduced their cost while showing improvements in terms of frame rate, size or accuracy. These improved features make ToF cameras a suitable solution in many applications [2,3] and also a promising candidate for small and portable applications, such as mobile phones or medical endoscopes (see [4,5]). Compared to multiple camera systems, which need complex processing, ToF technology gives fast and reliable 3D information at the hardware level.However, the main drawback of ToF technology is still the reduced signal to noise ratio it shows in practice. We can distinguish between systematic and non-systematic error sources in ToF technology. Systematic errors have been discussed widely in the literature and most of them can be calibrated (see [6] for a comprehensive survey). Most commercial cameras include hardware compensation for systematic errors. Contrary, non-systematic error sources cannot be calibrated and they depend on the scene. We highlight in this paper the effect of Multipath Interference (MpI) [7]. Due to multiple reflections of the light, a mixture of incoherent light signals arrives at each pixel from different paths apart from the direct path. This effect causes significant distortions in the estimation of depth, especially in the vicinity of corners and convex areas of the scene.In the literature there are several studies which show the impact of MpI in scenes registered by ToF cameras. In [7] they show how MpI severely distorts corner-like scenes. In [8] they measure the impact of MpI when building maps using mobile robots equipped with ToF cameras.Several works in the literature propose hardware modifications in the ToF cameras to remove multipath distortion. Falie and Buzuloiu [9,10] propose a method based on infrared structured light. Dorrington et al. [11] propose a method to correct multipath interference using light with two different wavelengths. These works are not able to completely remove MpI and they require hardware modifications in the camera, which are not available in commercial sensors.Fuchs [12] proposes a method to automatically correct MpI from the camera measurements that does not require hardware modifications. The author proposes a model that predicts the multipath distortion given the information captured by a ToF camera (i.e. signal amplitude and depth for each pixel). The main assumption of Fuchs [12] is that, despite the fact that measurements are contaminated with MpI, the signal distortion can be accurately predicted from them. The main drawback of Fuch's work is precisely the model used to predict MpI. It is not demonstrated under which circumstances the predicted MpI is accurate. In fact, we show in this paper that Fuch's method fails in general scenes when the surfaces have different albedos. In addition Fuch's work requires the adjustment of scene-dependent parameters which is a serious limitation in practice. There are no hints about how to set these parameters and which is their influence in the solution.This paper proposes a method that removes multipath distortion from the measurements of a ToF camera without any prior knowledge about the scene. We also do not require hardware modifications in the camera. Our method optimizes a cost function whose global minimum represents the correction needed to obtain the real depth of each pixel, free from multipath distortion. We use a radiometric generative model based on certain properties about the radiometry of the camera and the scene. Unlike the proposal of Fuchs [12], our method is accurate given that the required conditions about the scene are approximately met. The results show that our approach is very accurate, even in complex scenes.The paper is divided in the following sections: Section 2 details the radiometric model used to predict ToF measurements of a known scene. The iterative correction algorithm is presented in Section 3. In Section 4, we show the results for both synthetically generated and real scenes.Scalar real values are represented in lowercase letters (e.g. a) and complex values, used to define phasors, are denoted by uppercase letters (e.g. S). Bold typography indicates vectors and matrices, using lowercase letters for vectors (e.g.p) and uppercase letters for matrices (e.g.A). The vector norm ∥x∥=(∑ixi2)1/2 is the Euclidean norm. Calligraphic fonts are reserved to define sets or regions:Rpδ=qs.t.p−q<δ.We use the Λ modifier in signals, scalar amplitudes or vectors, to represent real measurements that may contain noise and distortions (e.g. Ŝ is the measured magnitude of S). In the same way, predicted magnitudes using generative models are represented by the superscript *. Thus, the term Ŝ⁎ would refer to the prediction of the magnitude Ŝ.In this section we present a generative model capable of rendering realistic ToF measurements, including multipath interference. We use general properties of light and surfaces to render realistic images as if they were captured in a ToF camera. The radiometric model developed in this section takes the scene geometry as an input (depth and point normals) and gives the depth map and the intensity image measured in a ToF camera as an output. In Section 3 we use this generative model to find the scene depth map without multipath distortion. We use a fitting algorithm that obtains the scene depth so that the generative model renders a depth image that is as close as possible to the one measured in the ToF camera. The generative model presented in this section is thus the core of our correction algorithm.The way light propagates, reflects in the scene, and is then captured back in the camera is a very complex phenomenon. It requires details about how surfaces and camera optics reflect and scatter light. In practice, such information is never available. This paper proposes a radiometric model based on the following general assumptions (see [13] for more details):•All the surfaces in the scene are perfect Lambertian reflectors.A single isotropic infrared light source illuminates the scene. This light source is located at the projection center of the camera. We consider that the distance between the light source and any of the pixels is negligible.Camera response is linear and narrow band at the infrared wavelength of the light is emitted.Camera optics corresponds to an ideal pin-hole lens model. Each pixel captures infrared light along a unique line of sight that passes through the optical center.Despite these assumptions are approximations; they have been extensively used in other areas of computer vision, such as color research [14] or Shape-from-Shading methods [15]. With Lambertian surfaces we avoid the need to know the Bidirectional Reflectance Distribution Functions (BRDFs) of each surface. Unlike the Lambertian model, real BRDFs are difficult to model and require calibration of the surface response under different light conditions and orientations. The assumptions about the camera response and optics are also usual in computer vision [16] and photogrammetry [17]. If properly calibrated, including radial and tangential distortion, a ToF camera fits the pin-hole model with high accuracy. In addition, considering the light source as an isotropic light source at exactly the center of the camera is a common approximation that gives accurate results [12].To illustrate the radiometric model, Fig. 1shows a scene with two 3-dimensional points piand pj. These points belong to surfaces with local orientations defined by normal vectors niand njand infinitesimal areas dAiand dAj. A ToF camera consisting of two pixels, with indexes i and j, captures the scene. Pixels i and j capture the light reflected from points piand pjrespectively.The ToF camera emits modulated infrared light and then computes the phase shift between the signal emitted (e.g. signal emitted in the direction of pixel i) and the reflected signal in the scene (e.g. signal received from pi). We can write the information taken at pixel i using a complex number (i.e. phasorial signal):(1)S^i=a^iejΔφ2∥p^i∥,where âiis the signal amplitude andΔφ2∥p^i∥is the signal phase shift. This phase shift is a linear function of the distance betweenp^iand the sensor. In real scenarios, multipath of light distorts phase and amplitude measurements and hence∥p^i∥≠∥pi∥. The time varying cosine representation of phasor (1) is:(2)s^it=a^icoswt+Δφ2∥p^i∥,where w is the modulation frequency used for the infrared source.To model the multipath effect, we decompose the signal Ŝiinto the direct path signal (Si) and the signal due to multipath (Sij), which in Fig. 1 comes only from point pj.(3)S^i=Si+Sij.In this model the light travels only once from point pjto pi. In general we can model a higher order of light bounces between the two points. However, that would clearly increase the complexity of the model. We study in Section 4 the impact in the results of considering only the first bounce of the light.The two components of Ŝiare detailed below:•Direct path term (Si):The direct path term of the signal decomposes into the following multiplicative complex factors:(4)Si=S0G0→iGi→0.The term S0=aoutdenotes the light emitted by each pixel. At emission, light is coherent in phase and thus aout∈ℝ.The complex number G0→i11Notation: sub-index x→y means light traveling from x to y, where x (same for y) means point px. p0 is the camera center of projection.models the amplitude decrease and phase shift suffered by the light when traveling from the sensor and impacting with point pi:(5)G0→i=ρidAicosαi∥pi∥2ejΔφ∥pi∥.Point piis embedded in a surface with differential area dAiand orientation αirelative to the light source. The term cos(αi) accounts for the foreshortening effect between the patch and the light source. The albedo factor ρigives an idea of the amount of energy absorbed by the surface. In fact, the product (S0G0→i) can be seen as a new point light source placed at pi.The term Gi→0 models the gain and phase shift when the light at picomes back to the camera sensor and it is registered on pixel i:(6)Gi→0=ρ0dA0cosα0∥pi∥2ejΔφ∥pi∥,where ρ0dA0 cos(α0) is the linear response of the camera. We assume that α0=0 and dA0 are replaced by pixel area. Jointly with ρ0 they are considered unknown camera gain factors.•Multipath term (Sij):The energy that hits point pj, apart from being irradiated towards pixel j (direct path Sjin pixel j), also impacts on pixel i (traveling a distance dij=∥ pi−pj∥ and being added to the light that reaches pixel i).The multipath signal is factorized into the following terms:(7)Sij=S0G0→jGj→iGi→0where Gj→iis the amplitude decrease and phase shift of the light suffers when traveling from pjto point pi:(8)Gj→i=ρidAicosαijdij2ejΔφdijand αijis the angle between the surface at piand the vector pi−pj.The term Ŝi(measured signal in pixel i) can be rewritten using Eqs. (4) and (7):(9)S^i=Si+Sij=S0G0→i+G0→jGj→iGi→0where the terms S0 and Gi→0 are common to both the direct path term and the multipath term.In a real scenario, the light reflects from a dense amount of points and arrives at point pi. The equations must be integrated over all the geometry of the scene, removing surface differentials. We assume that the scene is piecewise planar between the points measured by the camera pixels. As a consequence, the scene geometry is approximated with planar surface patches. These planar patches are assumed to be small enough to substitute surface differentials for small areas.The signal received at pixel i can be expressed as:(10)S^i=Si+∑pj∈RpiSij=S0G0→i+∑pj∈Rpi)G0→jGj→iGi→0where we defineR(pi) as the set of point (patches) that are reflecting the light towards point pi. This set can be found using surface orientations. A surface patch at point pjwith normal vector njand ni⊤inj<0 does not reflect light towards point pi, so it must not be included inR(pi).In this section we propose a method that takes a single ToF image (amplitude and depth maps) and gives as a result the depth of the scene without multipath distortion. Scene geometry is not known “a priori”. Our method can be applied to any scene where the radiometric model of Section 2 is capable of rendering accurate ToF measurements. Section 4 shows that this model can be applied even to scenes containing non-Lambertian objects and high differences in albedos.The correction algorithm can be summarized as follows: If the true depth values for each pixel are found, then the generative model presented in Section 2 is able to reproduce camera measurements. Following this idea we propose an optimization method that finds the scene depth (free from MpI) that makes the generative model render the closest ToF depth map to camera measurements.Fig. 2graphically shows the signals involved during our correction algorithm. We display a corner-like scene (blue curve) seen by a ToF camera. We denote as Sxthe phasor corresponding to the direct path of pixel x. The phase of Sxgives the sought-after true depth of pixel x. Sxis not directly visible from the camera measurements due to noise and multipath. We denote as Ŝx(red curve) the measurements obtained in the camera for pixel x. From Ŝxwe can obtain a distorted depth for pixel x, as it is clearly seen in the roundish shape of the red curve. We propose a generative model that predicts ToF measurementsS^x∗(orange curve) given an estimation of the direct path depths Sx∗ (green curve). The correction algorithm iteratively finds the values of Sx∗ so thatS^x∗is as close as possible to the measurementsS^xin terms of phase (we show arrows between both signals). If the model is accurate enough, we expect the phase of Sx∗ being also as close as possible to that of Sx, thus obtaining a good estimation of the depth values without MpI.We describe the 3D point pi, measured at pixel i using a scalar depth correction λi:(11)pi=p^i1−λi,wherep^iis the noisy measurement of piand λiis the correction in depth along the line of sight of pixel i.We first present the proposed model using the simplified two point scenario shown in Fig. 1. LetS^i∗λiλjdefined as the signal in pixel i in function of depth corrections λiand λjfor points piand pjrespectively:(12)S^i∗λiλj=Si∗λi+Sij∗λiλj,where(13)Si∗λi=S0G0→i∗Gi→0∗Sij∗λiλj=S0G0→j∗Gj→i∗Gi→0∗.Following Eqs. (5), (6), and (8), the gain terms can be expressed as:(14)G0→i∗=ρi∗dAi∗cosαi∗p^i21−λi2ejΔφp^i1−λi(15)Gi→0∗=ρ0dA0cosα0p^i21−λi2ejΔφp^i1−λiand(16)Gj→i∗=ρi∗dAi∗cosαij∗dij∗2ejΔφdij∗wheredij∗=p^i1−λi−p^j1−λj.In this two-point example, given λiand λj, the proposed model predicts the signals measured in the camera considering a scene with geometry given byp^i⋅1−λiandp^j⋅1−λj. There are some parameters that must be known to evaluate functionsS^i∗:•Surface albedo factors ρi∗, included in factors Gi→j∗. These parameters are scene-dependent. Their estimation is explained below.Camera response and gain, included in terms Gi→0∗. In general, unless the camera response is calibrated using light and surface patterns, those parameters are unknown. As it is explained later, the way the generative model is matched to the measurements makes unnecessary to obtain these parameters.Including surface albedos ρiin the model is mandatory as surfaces involved in a real scene have very different albedos (e.g. black and white colors in a checkerboard). In most cases, albedo determination from images is a difficult task in computer vision, especially with uncontrolled light scenarios (e.g. see [18,19]).In this case we propose a method to approximate the scene albedos using ToF measurements and taking into account the radiometric model detailed in Section 2. Let ρibe the albedo factor for the surface detected at pixel i. This factor can be expressed as the product between a reflectivity term ρireland a global scale factor ψ:(17)ρi=ψ⋅ρirel,∀i.Depth and normals of each pixel are used to decouple foreshortening and amplitude fall-off to obtain a value of ρirelin terms of the amplitude detected at pixel i, namelya^i:(18)ρirel=a^ip^i4dAicosαiknwhere ρirelis indeed the amplitude of the signal arriving at pixel i in a common plane parallel to the camera sensor (we use the plane in the coordinate origin). The scalar knis chosen so that ρirel≤1.The global scale factor depends on the camera and light source used. ψ is a camera-dependent parameter and it can be calibrated. In Section 4 we describe a calibration algorithm.Finally, we show in Section 4 that despite the fact that albedos are computed using noisy image amplitudesa^i, they are fair enough to get accurate depth corrections.In this section we present an optimization approach that fits the generative method to ToF measurements, obtaining as a consequence the scene without MpI.In Fig. 3we show a flowchart that explains the fitting algorithm; note that we refer to the involved signals using the same color assignment as used in Fig. 2. The inputs are depth and amplitude measurements taken from a single frame capture in the sensor (green box in the diagram). We call “Solution(k)” in the diagram to the estimation of the depths without MpI at iteration k of the algorithm. At k=0, sensor measurements are used as initialization. Given “Solution(k)”, we simulate the depths with MpI. This is called “Solution(k)+MpI” in the diagram. We use the radiometric model explained before to simulate MpI, using amplitude measurements to estimate albedos. The iterations stop when the differences in depth between the measurements and “Solution(k)+MpI” are below some predefined threshold γ. After the method stops, “Solution(k)” contains the depths for each pixel without multipath errors.To have guarantees of convergence we implement diagram 3 using a cost function that is optimized with iterative methods. The cost function involves the depth correction of all pixels in the image:(19)Λ=λ1…λN,grouped in the column vector Λ for an image with N pixels. We propose the following sum of squares cost function:(20)min︸Λ∑i=1NfiΛ2where:(21)fiΛ=ϕS^i−ϕS^i∗Λ=ϕS^i−ϕSi∗λi+∑j∈RiSij∗λiλj,and ϕ(S) is the phase of the phasor S.As is displayed in Fig. 4the optimization approach is forcing every two phasors ŜiandS^i∗to be as parallel as possible.The cost function (20) is non-linear and non-convex in terms of the unknown vector Λ. In this paper a numerical optimization algorithm is used to reach a local minimum of the cost. In particular the Levenberg–Marquardt algorithm is proposed, using as initialization Λ=0.Although the global minimum of the cost is not ensured, in Section 4 we show with experiments that the ToF measurements give a good initialization so that only a few iterations are needed to reach the sought-after local minimum.

@&#CONCLUSIONS@&#
