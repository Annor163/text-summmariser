@&#MAIN-TITLE@&#
Fully automated assessment of the severity of Parkinson's disease from speech

@&#HIGHLIGHTS@&#
Development of better models for extraction of features related to pitch, jitter and shimmer.Analysis of the effectiveness of different tasks for eliciting speech.Demonstration of the feasibility of administering a battery of PD tests on a portable platform and automating the analysis of elicited speech.

@&#KEYPHRASES@&#
Parkinson's disease,Pitch estimation,Jitter,Shimmer,

@&#ABSTRACT@&#
For several decades now, there has been sporadic interest in automatically characterizing the speech impairment due to Parkinson's disease (PD). Most early studies were confined to quantifying a few speech features that were easy to compute. More recent studies have adopted a machine learning approach where a large number of potential features are extracted and the models are learned automatically from the data. In the same vein, here we characterize the disease using a relatively large cohort of 168 subjects, collected from multiple (three) clinics. We elicited speech using three tasks – the sustained phonation task, the diadochokinetic task and a reading task, all within a time budget of 4min, prompted by a portable device. From these recordings, we extracted 1582 features for each subject using openSMILE, a standard feature extraction tool. We compared the effectiveness of three strategies for learning a regularized regression and find that ridge regression performs better than lasso and support vector regression for our task. We refine the feature extraction to capture pitch-related cues, including jitter and shimmer, more accurately using a time-varying harmonic model of speech. Our results show that the severity of the disease can be inferred from speech with a mean absolute error of about 5.5, explaining 61% of the variance and consistently well-above chance across all clinics. Of the three speech elicitation tasks, we find that the reading task is significantly better at capturing cues than diadochokinetic or sustained phonation task. In all, we have demonstrated that the data collection and inference can be fully automated, and the results show that speech-based assessment has promising practical application in PD. The techniques reported here are more widely applicable to other paralinguistic tasks in clinical domain.Parkinson's disease (PD), which is characterized by tremor and impaired muscular co-ordinations, currently has no cure and hence screening for early detection and monitoring its progression are important tools for managing the disease in the growing population of the elderly. The disease is associated with low levels of dopamine in the brain and the symptoms are managed by artificially increasing amounts of dopamine with drugs (e.g., L-dopa) and in severe cases by electrically stimulating specific regions in the mid-brain. The severity of the disease is typically assessed in a clinic with a battery of tests – the Unified Parkinson's Disease Rating Scale (UPDRS) – consisting of clinician-scored motor evaluations and self evaluation of the activities of daily life including speech, swallowing, handwriting, dressing, hygiene, falling, salivating, turning in bed, walking, and cutting food. The UPDRS scores range from 0 to 176, with 0 corresponding to a healthy state and 176 to a severe affliction (MDSTF, 2011). The assessment is time-consuming and is performed by trained medical personnel, which becomes burdensome when, for example, frequent re-assessment is required to fine-tune dosage of drugs or the parameters of the electrical pulse train in deep brain stimulation. Not surprisingly, there has been a growing interest in creating tools and methods for alternative home-based assessments of this disease. Easier methods of assessment can play a crucial role in screening for early detection of PD, the second most common neurodegenerative disease in US.Since speech production involves complex motor coordination, the disease exhibits tell tale symptoms which are well-known to speech pathologists, although the exact pathophysiological cause remains unclear. For several decades now, researchers have been interested in measuring these symptoms in speech more objectively with the hope of augmenting or simplifying the assessment. Speech tasks can be administered remotely, avoiding the need for driving to the clinic, which can be challenging for those with severe PD-related motor tremors. Speech can be elicited, recorded and analyzed automatically relatively easily at much lower cost than in-person clinical assessment. Furthermore, speech-based assessment can monitor changes objectively over time more accurately.While there has been considerable interest in analyzing speech in PD, spanning about five decades, only recently it has attracted the attention of computational speech researchers. Early studies, reported in clinical journals, employed relatively simple analysis of speech samples. Here, we set the context of this work by reviewing a sampling of previous work in Section 2. As evident from this review, previous studies have several limitations. With a few exceptions, most studies have been conducted on relatively smaller cohorts, recruited from a single clinic and often narrowly focused on characterizing pathology related to production of vowels. Data collected from a single clinic can suffer from bias due to the subjective nature of clinical assessments.In this article, we investigate the accuracy of automatically inferring the severity of PD from speech samples in a relatively large cohort collected from multiple clinics. The data collection is described in detail in Section 3. One of the aims of this paper is to investigate the utility of current speech processing and machine learning techniques with publicly available tools for inferring the severity of Parkinson's disease. In Section 4, we extract a number of potential speech features using standard speech processing algorithms and apply several machine learning algorithms to predict the clinical ratings from the speech features. Standard pitch detection algorithms do not have the necessary time-frequency resolution to capture the fine tremor observed in PD. We recently developed a pitch estimation algorithm that addresses this problem, which incidentally won the 2013 Interspeech Challenge on detecting and diagnosing Autism Spectral Disorders (Asgari and Shafran, 2013). We describe our method and our evaluation on PD in Section 5. Finally, we summarize the contributions of this paper.The earliest work on measuring speech abnormalities objectively in PD can be traced back to Canter's dissertation. Taking advantage of then newly available instruments to measure pitch using a direct-writing oscilloscope (Sanborn, Model 450) and vocal intensity using a high-speed level recorder (Bruel and Kjaer, Model 2304), Canter compared speech from 17 patients, who were off medication for 48h, with 17 age-matched controls (Canter, 1963, 1965a,b). PD subjects exhibited higher median pitch and lower range than the controls. They lacked the necessary control to generate soft sounds. At the other end of the scale, they had lower intensity of maximal loudness. They were able to sustain phonation for markedly shorter duration, about 50% of the control. In articulation, PD subjects were slower and plosives lacked precision, often confused with fricatives. Canter also noted that the rate of speech and intelligibility were significantly different from the controls. Most of his conclusions, even though deduced from manual measurements from plots, have been subsequently confirmed by measurements with more sophisticated instruments and again with small number of subjects (Titze, 1994; Darley et al., 1975). One exceptionally large sample study collected speech from about 200 PD subjects (Logemann et al., 1978). They confirmed Canters findings and in addition observed characteristic types of misarticulations (e.g., back-of-tongue, tongue tip, lips). The above mentioned studies were conducted by clinical researchers who compiled the features with manual measurements and perceptual ratings.There have been very few studies on employing automatic speech processing to classify PD subjects from controls or inferring the severity of the diseases. Here we describe a few representative studies. Guerra and Lovely attempted to emulate the ratings of speech pathologists (Guerra and Lovey, 2003). Specifically, they created a linear regression with automatically extracted features but whose coefficients were manually set to emulate perceptual measures such as harsh voice, breathy voice and audible inspirations. While their approach was meant to facilitate easy clinical adoption, their method was ad hoc and they were only able to map a few perceptual measures. Little and his colleagues focused their attention on relatively simple analysis of phonation of a single vowel, collected from 42 subjects, including 10 controls (Tsanas et al., 2010). They evaluated the efficacy of their algorithm using cross validation and reported a mean absolute error (MAE) of 6.6 in inferring the severity of PD. Their results are overly optimistic because their test and training sets have significant overlap, with same speakers contributing large number of samples to both. Their assessment erroneously assumed speech frames from each session were independent. So, they model not just the difference between speakers due to PD but also due to normal variations in speaker traits. More recently, Bocklet et al. (2011) applied a more rigorous machine learning approach to classify 23 PD subjects from 23 control subjects. They extracted 292 prosodic features, adapted a 128 component Gaussian mixture model or universal background model using maximum a posteriori criterion and found that they were able to perform the classification with good accuracy. However, their task was classification, not assessing the degree of severity which is more relevant in a monitoring application such as the one we are interested in.Taken together, there has been continuous interest spanning several decades in characterizing the speech abnormalities in PD. However, early studies were focused on measuring group differences of speech features and recent studies have been performed on small samples.Speech data for this work was collected as a part of a larger study whose goal is to develop an objective measure of severity for Parkinson's disease. As a clinical reference, the severity of subjects’ condition were measured by clinicians using the UPDRS. In this study, we focused on motor sub-scale (mUPDRS), which spans from 0 for healthy individual to 108 for extreme disability. The data was collected from multiple (three) clinics to alleviate potential bias due to clinic-specific practices. The objective measures were collected using a battery of tests administered on a portable platform developed by Intel, the Kinetics Foundation and a consortium of neurologists (Goetz et al., 2009). The platform measured fine motor control of hands and foot (via a foot-tapper) and recorded speech via a close-talking microphone. Subjects were prompted to respond to different motor and speech tasks in a prearranged sequence. The tasks were administered by trained PD clinicians who were familiar with the device. Note, the key motivation for developing the portable platform was to create a home-based assessment platform and for that purpose the data collection was fully automated.The battery of speech tasks, listed below, were compiled to measure different aspects of speech production. The tasks were chosen so that they can be administered automatically and performed within a short time budget of 4min.1.Sustained phonation task: Subjects were instructed to phonate the vowel /ah/ for about 10s, keeping their voice as steady as possible at a comfortable and constant vocal frequency. Speech pathologists often rate voice quality during this task.Diadochokinetic (DDK) task: Subjects were asked to repeat the sequence of syllables /pa/, /ta/ and /ka/ continuously for about 10s as fast and as clearly as they possibly could. This task is often employed by speech pathologists to judge articulatory precision, control and speed.Reading task: Subjects were asked to read 3 standard passages, which are referred to in the literature as “The North Wind and The Sun”, “The Rainbow Passage”, and “The Grandfather Passage”. For example, the first story begins as – The North Wind and the Sun were disputing which was the stronger, when a traveler came along wrapped in a warm cloak. This reading task allows measurement of vocal intensity, voice quality, and speaking rate including the number of pauses, the length of pauses, the length of phrases, the duration of spoken syllables, the voice onset times (VOT), and the sentence durations.The empirical evaluations in this paper were performed with data from 168 subjects, all of whom were diagnosed with PD. Thus, this corpus has more subjects than previous studies on automated assessments – 42 PD subjects and 10 controls (Tsanas et al., 2010), 23 PD subjects and 23 controls (Bocklet et al., 2011). The severity of the disease in our subjects ranged from 0 (control) to 55 on the UPDRS scale in our subjects, with a mean of 22.9 and standard deviation of 9.3. Our subject pool exhibits slightly higher mean and range of severity of the disease than the most closely related previous work by Bocklet and colleagues whose PD subjects had a mean UPDRS score of 17.5 and a standard deviation of 7.3. As we shall later see in Section 8, the diversity of subjects influences how well the automated assessment can generalize, for example, across clinics.In this section, we describe our baseline system and investigate how the accuracy of inferring the severity of PD depends on different learning strategies.The larger goal of the project is to assess and to monitor the severity of PD using both motor and verbal responses and our task is to focus on the latter. Specifically, to learn a regression on the features extracted from the elicited speech. Let the speech data collected from a subject be x(i) and y his or her clinical rating (UPDRS score). Thus, we need to learn a regression function f(x;β), parametrized byβ, that approximates the UDPRS scores from the available training data D={(xi, yi);i=1, …, n}.(1)y=f(x;β)+ϵAssuming an independent identically distributed regression error ϵ, the optimal function can be learned by minimizing the average loss.(2)βopt=argminβ∑i=1nℓ(yi,f(xi;β))The learned parameters will depend on the choice of the loss function ℓ, which may be absolute loss, squared loss and hinge loss. The simplest regression function is a constant and that would correspond to guessing the same severity (chance) for all subjects. For absolute and squared loss, the optimal guess would be the median and mean UPDRS respectively. In most literature on this topic, the performance is reported in terms of mean absolute error (MAE). On our data set, the median UPDRS score is 21 and the mean absolute error of 7.5.The most popular regression function is a linear function, where the parameters {βi;i=0, …, d} are regression coefficients and d is the dimension of features xi.(3)f(x;β)=β0+∑i=1dxiβiThe number of potential speech features that can be extracted from the speech samples elicited through the four tasks can be large, much larger than the number of subjects in the study. In such a scenario, the learning task, in Eq. (2), is an ill-posed problem without a unique solution for the linear function. The simplest solution to this problem is to augment the cost function with a regularization term r(β) that penalizes large values of the regression coefficients, driving them to zero when they are not useful.(4)βopt=argminβ∑i=1nℓ(yi,f(xi;β))+λr(β)The trade-off between accurate inference of the severity of PD and discarding irrelevant features is controlled by the regularization weight λ, which is often picked using cross-validation (Tibshirani, 1996).Before delving into our experiments on different modeling strategies, we briefly describe the features extracted for this task. Classic perceptual characteristics associated with PD are reduced loudness; monotonous pitch; monotonous loudness; reduced stress; breathy, hoarse voice quality; imprecise articulation; and short rushes of speech. In general, we can divide the aforementioned problems into three major categories: loudness related problems, pitch related problems and articulatory related problem. In our experiment, we used a diverse set of features to capture cues associated with these categories. For our baseline system, we adopted the baseline features defined in INTERSPEECH 2010 Paralinguistic Challenge (Schuller et al., 2010) using openSMILE toolkit (Eyben et al., 2010).The features, comprised of 1582 components, can be broadly categorized into three groups: (1) loudness related features such as RMS energy and PCM loudness, (2) voicing related features like pitch frequency, jitter, and shimmer, and (3) articulatory related features such as mel-frequency cepstral coefficients and line spectral frequencies. The above configuration provides 38 features along with their derivatives to form the frame-level acoustic features. The derivatives allow us to capture local dynamics of pitch and other features. The features computed at the frame-level were summarized into a global feature vector of fixed dimension for each recording using 21 standard statistical functions including min, max, mean, skewness, quartiles and percentile.We investigated three forms of regularization, L2-norm in ridge regression, L1-norm in lasso, and hinge loss function in support vector machine. We describe them briefly here and more details can be found in standard textbooks (Hastie et al., 2001).Ridge regression In ridge regression, the loss function ℓ() and the regularizer r() are the squared loss and the L2-norm, respectively.ℓ(yi,f(xi;β))=(yi−f(xi;β))2,r(β)=||β||22Lasso regression In lasso regression, the loss function is again the squared loss but the regularizer is the L1-norm. The L1-norm is well-known for finding sparse solution, assigning zero values to useless regression coefficients (Tibshirani, 1996).ℓ(yi,f(xi;β))=(yi−f(xi;β))2,r(β)=||β||1Linear support vector regression Support vector regression uses the ϵ-insensitive loss function and the L2-norm for regularization.ℓ(yi,f(xi;β))=max(0,||yi−f(xi;β)−ϵ||1),r(β)=||β||22These three learning strategies were evaluated on our data set using leave-one-out cross-validation with the scikit-learn toolkit (Pedregosa et al., 2011) and the performances are shown in Table 1. We consider all the speech elicited from each battery of speech tasks on a subject as one sample for our cross-validation evaluation. The ridge regression and the support vector regression are significantly better than chance with a p-value of less than 0.001, according to cross-validated paired t-test (Dietterich, 1998), and is denoted by (†) in the table. Ridge regression is much faster to learn than the support vector regression.Tremor observed in Parkinson's disease can be as low as 10Hz (Titze, 1994). Even though there are a large number of pitch estimators in the literature (Talkin, 1995; Boersma and Weenink, 1995; De Cheveigné and Kawahara, 2002; Hermes, 1988; Sun, 2002; Drugman and Alwan, 2011; Kawahara et al., 2008), most of them are not well-suited for measuring the small tremor in PD. The key issue is that resolutions as low as 10Hz can be obtained only using large time windows. But speech, unlike for example elephant vocalizations, is highly non-stationary and violates the stationarity assumption when long windows are considered. The autocorrelation based methods wrongly assume the pitch is constant over the duration of the frame (Talkin, 1995; Boersma and Weenink, 1995; De Cheveigné and Kawahara, 2002). Methods that locate peaks in frequency domain, power spectrum or cepstral domain also suffer from similar drawbacks (Hermes, 1988; Sun, 2002; Drugman and Alwan, 2011; Kawahara et al., 2008). For example, at 16kHz sampling, a 25ms frame would correspond to 400 sample points and a frequency resolution of about 20Hz or more. Increasing the resolution with longer frames runs afoul of the stationarity assumption as the frame includes sounds corresponding to different phones. The pitch estimator needs to measure tremors of the order of 10Hz using standard 25ms time frames, which most current methods cannot do.One notable exception is the harmonic model of speech where the harmonic coefficients are allowed to vary with time within the frame. This model takes into account the harmonic nature of voiced speech and can be formulated to estimate pitch candidates with maximum likelihood criterion (Asgari and Shafran, 2013).The popular source-channel model of voiced speech considers glottal pulses as a source of periodic waveforms which is being modified by the shape of the mouth assumed to be a linear channel. Thus, the resulting speech is rich in harmonics of the glottal pulse period. The harmonic model is a special case of a sinusoidal model where all the sinusoidal components are assumed to be harmonically related, that is, the frequencies of the sinusoids are multiples of the fundamental frequency (Stylianou, 1996). The observed voiced signal is represented in terms of a harmonic component and a non-periodic component related to noise.Let y=[y(t1), y(t2), …, y(tN)]Tdenote the speech samples in a voiced frame, measured at times t1, t2, …, tT. The samples can be represented with a harmonic model with an additive noise n=[n(t1), n(t2), …, n(tN)]Tas follow:(5)s(t)=a0+∑h=1Hahcos(2πf0ht)+bhsin(2πf0ht)y(t)=s(t)+n(t)where H denotes the number of harmonics and 2πf0 stands for the fundamental angular frequency. The harmonic coefficients ahand bhin this equation are assumed to be constant for each frame, but this assumption is relaxed while computing jitter and shimmer, as described later in Section 5.6. The harmonic signal can be factored into coefficients of basis functions, α, β, and the harmonic components which are determined solely by the given angular frequency 2πf0.(6)s(t)=[1Ac(t)As(t)]a0αβAc(t)=[cos(2πf0t)⋯cos(2πf0Ht)]As(t)=[sin(2πf0t)⋯sin(2πf0Ht)]α=[a1⋯aH]Tβ=[b1⋯bH]TStacking rows of [1Ac(t)As(t)] at t=1, …, T into a matrix A, Eq. (2) can be compactly represented in matrix notation as:(7)y=Am+nwhere y=Am corresponds to an expansion of the harmonic part of the voiced frame in terms of windowed sinusoidal components, andΘ=[f0,b,σn2,H]is the set of unknown parameters.Assuming the noise samples n are independent and identically distributed random variables with zero-mean Gaussian distribution, the likelihood function of the observed vector, y, given the model parameters can be formulated as the following equation. The parameters of the vector m can then be estimated by maximum likelihood (ML) approach.(8)L(Θ)=−D2log(2πσn2)−12σn2||y−Ab||2mˆML=(ATA)−1ATyUnder the harmonic model, the reconstructed signalsˆis given bysˆ=Am. The pitch can be estimated by maximizing the energy of the reconstructed signal over the pre-determined grid of discrete f0 values ranging from f0minto f0max.(9)fˆ0ML=argmaxf0sˆTsˆThe frame-based pitch estimation does not prevent pitch from varying drastically from one frame to the next. In natural speech, however, pitch often varies smoothly across frames. This smoothness constraint can be enforced using a first order Markov dependency between pitch estimates of successive frames. Adopting the popular hidden Markov model framework, the estimation of pitch over utterances can be formulated as follows.The observation probabilities are assumed to be independent given the hidden states or candidate pitch frequencies here. A zero-mean Gaussian distribution defined over the pitch difference between two successive frames is a reasonable approximation for the first order Markov transition probabilities (Tabrikian et al., 2004),P(f0(i)|f0(i−1))=N(f0(i),σt2). Putting all this together and substituting the likelihood from the Eq. (9), the pitch over an utterance can be estimated as follows.(10)Fˆ0=argmaxF0∑i=0MsiˆTsiˆ|f0(i)+logN(f0(i),σt2)Thus, the estimation of pitch over an utterance can be formulated as an HMM decoding problem and can be efficiently solved using Viterbi algorithm.Like in other pitch detection algorithms, pitch doubling and halving are the most common errors in harmonic models too. The harmonics of f0/2 (halving) include all the harmonics of f0. Similarly, the harmonics of 2f0 (doubling) are also the harmonics of f0. The true pitch f0 may be confused with f0/2 and 2f0 depending on the number of harmonics considered and the noise.In many conventional algorithms, the errors due to halving and doubling are minimized by heuristics such as limiting the range of allowable f0 over a segment or an utterance. This requires prior knowledge about the gender and age of the speakers. Alternatives include median filtering and constraints in Viterbi search (Talkin, 1995), which remain unsatisfactory.We propose a method to capture the probability mass in the neighborhood of the candidate pitch frequency. The likelihood of the observed frames falls more rapidly near candidates at halving f0/2 and doubling 2f0 than at the true pitch frequency f0. This probability mass in the neighborhood can be captured by convolving the likelihood function with an appropriate window. Fig. 1illustrates the problem of halving and demonstrates our solution for it. The dotted line shows the energy of the reconstructed signalsˆfor a frame. A maximum of this function will erroneously pick the candidate f0/2 as the most likely pitch candidate for this frame. However, notice that the function has a broader peak at f0 than at f0/2. The solid line shows the result of convolving the energy of the reconstructed signal with a hamming window. In our experiments, we employed a hamming window with the length off0min/2wheref0minis the minimum pitch frequency. The locally smoothed likelihood shows a relatively high peak at the true pitch frequency f0 compared to f0/2, thus overcoming the problem of halving.Another problem with the harmonic model is the need to specify the number of harmonics considered. This is typically not known a priori and the optimal value can be different in different noise conditions. Davy proposed a sampling-based method for estimating the number of harmonics (Godsill and Davy, 2002). Their approach is based on Monte Carlo sampling and requires computationally expensive numerical approximations. Mahadevan employs the Akaike information criteria (AIC) for tackling the problem of model order selection (Mahadevan and Espy-Wilson, 2011). Here, we follow a Bayesian approach trying to maximize the likelihood function given by:(11)Hˆ=argmaxHp(y,ΘH)where ΘHdenotes the model constructed by H harmonics. The likelihood function increases as a function of increasing model order and often leads to the overfitting problem. We adopt the Bayesian information criterion (BIC) as a model selection criterion, where the increase in the likelihood is penalized by a term that depends on the model complexity or the number of model parameters. For the harmonic model, we include a term that depends on the number of data points in the analysis window N.(12)BIC(H)=−2logp(y,ΘH)+HlogNThus, in our proposed model selection scheme, we compute the average frame-level BIC for different model orders, ranging from H=2, …, Hmax. For a given task or noise condition, we choose the number of harmonic that minimizes the average frame-level BIC.Speech pathologists often measure cycle-to-cycle variations in glottal pulse to characterize abnormalities in voice production (Titze, 1994). In early work, the cycle-to-cycle variation in fundamental time period Tiand amplitude Aiwere measured manually and computed using a variant of the equation below.(13)J=1/(N−1)∑i=1N−1|Ti−Ti+1|1/N∑i=1NTi(14)S=1N−1∑i=1N−1logAi+1AiAutomatic measurement of time period and amplitude of each cycle is prone to errors due to noise. Most automated methods sidestep this problem by measuring the variation across frames using the average time period and amplitude per frame (Nöth et al., 2011; Haderlein et al., 2011; Schuller et al., 2012). However, this approach ignores variations within frame.The harmonic model allows an alternate method to measure jitter and shimmer that is less sensitive to noise and can also capture variation within a frame. The key idea of our approach is to reconstruct two versions of the input waveform in each frame – a version where the amplitudes of the harmonics are constant, as in Eq. (6), and another without that assumption. Both reconstructions are estimated to minimize the effect of noise.The model for the voiced speech that allows harmonic amplitude to vary with time can be represented as follows (HM-VA) (Godsill and Davy, 2002).(15)s(t)=a0+∑h=1Hah(t)cos(2πf0ht)+∑h=1Hbh(t)sin(2πf0ht)Note, unlike the previous model defined by Eq. (5), here the harmonic coefficients ah(t) and bh(t) are allowed to vary with time within a frame. Thus, this model is capable of capturing cycle-to-cycle variations within a frame. The cycle-to-cycle variations in harmonic amplitudes cannot be arbitrary and can be constrained by imposing smoothness constraints to improve the robustness of their estimation. One easy method for imposing smoothness is to represent the harmonic coefficients as a superposition of small number of basis functions ψias in Eq. (16) (Godsill and Davy, 2002).(16)ah(t)=∑i=1Iαi,hψi(t),bh(t)=∑i=1Iβi,hψi(t)We represent this smoothness constraints within a frame using four (I=4) Hanning windows as basis functions. For a frame of length M, the windows are centered at 0, M/3, 2M/3, and M. Each basis function is 2M/3 samples long and has an overlap of M/3 with immediate adjacent window. The parameters of this model can be expressed, once again, as a linear model, similar to Eq. (7), but this time the A and m have dimensions four times the original dimensions. Given the fundamental frequency from Eq. (10), we compute ah(t) and bh(t) using a maximum likelihood framework.Now, the cycle-to-cycle variation associated with jitter and shimmer can be computed from the estimated parameters of the two models, with constant amplitudes, ahand bh, and the time-varying amplitudes, ah(t) and bh(t), of the harmonics. Shimmer can be considered as a function f(t) that scales the amplitudes of all the harmonics in the time-varying model.(17)ch(t)=chf(t)+e(t),t=1,…,T,h=1,…,Hwherech=∑h=1Hah2+bh2denotes the constant amplitude of the harmonic components in the harmonic model and ch(t) is the counterpart where the amplitudes vary within a frame. Once again, assuming uncorrelated noise, f(t) can be estimated using maximum likelihood criterion.(18)fˆ(t)=∑h=1Hchch(t)∑h=1Hch2The larger the tremor in voice, the larger the variation in f(t). Hence, we use the standard deviation of f(t) as a summary statistics to quantify the shimmer per frame.For computing jitter, we first create a matched filter by excising a one pitch period long segment from the signal estimated with the constant-amplitude harmonic model from the center of the frame. This matched filter is then convolved with the estimated signal from the time-varying harmonic model and the distance between the maxima defines the pitch periods in the frame. The perturbation in period is normalized with respect to the given pitch period and its standard deviation is an estimate of jitter.Researchers have used HNR in the acoustic studies for the evaluation of voice disorders. Given the reconstructed signal as the harmonic source of vocal tract, the noisy part is obtained by subtracting the reconstructed signal from the original speech signal. The noisy part encompasses everything in the signal that is not described by harmonic components including fricatives. In our model, HNR and the ratio of energy in first and second harmonics (H1/2) can be computed from the HM-VA as follows.(19)ch(t)=∑i=1Iah(t)2+bh(t)2HNR=log∑t=1N∑h=1Hch(t)2−log∑t=1N(y(t)−s(t))2H1/2=log∑t=1Nc1(t)2−log∑t=1Nc2(t)2Thus, we compute jitter, shimmer, harmonic-to-noise ratio and the ratio of energy in first and second harmonics using reconstructed signal that is less prone to noise related errors. The effectiveness of these two measures are evaluated in the experiments below.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
