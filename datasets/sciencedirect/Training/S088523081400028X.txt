@&#MAIN-TITLE@&#
Paraphrastic language models

@&#HIGHLIGHTS@&#
Paraphrastic language models proposed.Statistical paraphrase learning from standard texts.Improved LM context coverage and generalization performance.Combination with word and phrase level neural network LMs.Significant error rate reductions of 5–9% relative.

@&#KEYPHRASES@&#
Language modelling,Paraphrase,Speech recognition,

@&#ABSTRACT@&#
Natural languages are known for their expressive richness. Many sentences can be used to represent the same underlying meaning. Only modelling the observed surface word sequence can result in poor context coverage and generalization, for example, when using n-gram language models (LMs). This paper proposes a novel form of language model, the paraphrastic LM, that addresses these issues. A phrase level paraphrase model statistically learned from standard text data with no semantic annotation is used to generate multiple paraphrase variants. LM probabilities are then estimated by maximizing their marginal probability. Multi-level language models estimated at both the word level and the phrase level are combined. An efficient weighted finite state transducer (WFST) based paraphrase generation approach is also presented. Significant error rate reductions of 0.5–0.6% absolute were obtained over the baseline n-gram LMs on two state-of-the-art recognition tasks for English conversational telephone speech and Mandarin Chinese broadcast speech using a paraphrastic multi-level LM modelling both word and phrase sequences. When it is further combined with word and phrase level feed-forward neural network LMs, a significant error rate reduction of 0.9% absolute (9% relative) and 0.5% absolute (5% relative) were obtained over the baseline n-gram and neural network LMs respectively.

@&#INTRODUCTION@&#
Natural languages are known to have layered structures, a hidden and deeper structure that represents the meaning and core semantic relations within a sentence, and a surface form found in normal written texts or spoken language, as formulated in linguistic theories such as generative grammar Chomsky (1966), Jackendoff (1974). The mapping from the meaning to the observed surface form involves a natural language generation process. As multiple surface realizations can be used to convey identical or similar semantic information, this mapping is often one-to-many. These different surface realizations are paraphrastic to one another. They were created by using different syntactic, lexical and morphological rules in the generation process. Functionally these paraphrase variants represent different styles, dialects or other speaker specific characteristics. Due to their presence, only modelling the observed surface word sequence can result in poor context coverage, for example, when using standard n-gram language models (LMs).One approach to handle this problem requires directly modelling paraphrase variants when constructing the LM. As alternative expressions of the same meaning are now considered, the resulting language model's context coverage and generalization performance is expected to improve. Along this line, the use of word level synonym features Cao et al. (2005), Hoberman and Rosenfeld (2002), Jelinek et al. (1990), Kneser and Peters (1997) has been investigated in early research for n-gram and class n-gram based Brown et al. (1992) language models. However, there are two issues associated with these existing approaches. First, the paraphrastic relationship between longer span syntactic structures, such as phrases, is largely ignored. A more general form of modelling that can also capture a higher level and longer span paraphrase mapping should be more effective. Second, previous research focused on using manually derived expert semantic labelling provided by resources such as WordNet Fellbaum (1998). As manual annotation is usually very expensive to produce, these methods cannot be applied to large corpora or languages without suitable WordNet-type resources. Hence, automatic, statistical paraphrase induction and extraction techniques are required.In order to address these issues, this paper presents a novel form of language model, the paraphrastic language model (PLM). It provides a highly flexible and general form of paraphrase modelling that can be used at either the word, phrase or sentence level. The paraphrastic relationship between longer span syntactic structures can thus be effectively captured. A phrase level paraphrase model statistically learned from standard text data is used to generate multiple paraphrase variants for the training data. Language model probabilities are then estimated by maximizing the marginal probability of these variants. By linking language generation and modelling, paraphrastic LMs exploit an intuitive and interpretable parameter smoothing scheme to improve generalization performance. In order to leverage the complementary characteristics of paraphrastic LMs and feed-forward neural network LMs (NNLMs) Bengio et al. (2003), Kuo et al. (2012), Le et al. (2013), Park et al. (2010), Schwenk (2007), the combination between the two is also investigated.This paper extends previous research summarized in Liu et al. (2012b, 2013c). A more complete study of using paraphrastic language models for speech recognition is presented. Various important aspects of this work, including the theory and implementation of the statistical paraphrase learning algorithm, the generation of paraphrase lattices and the construction of phrase and multi-level paraphrastic LMs, are covered in detail in this paper. These are further augmented by a full set of experimental results presented to demonstrate the advantages of paraphrastic LMs over existing modelling methods. This paper shows the applicability of paraphrastic LMs to multiple languages and genres, the scaling behaviour on varying amounts of training data, and their complementarity to other established language modelling techniques.The rest of the paper is organized as follows. Paraphrastic language models are introduced in Section 2. A statistical n-gram phrase pair based paraphrase extraction scheme is presented in Section 3. Paraphrase lattice generation using a weighted finite state transducer (WFST) approach is described in Section 4. The estimation of paraphrastic LMs is presented in Section 5. The combination between paraphrastic LMs and feed-forward neural network LMs is proposed in Section 6. In Section 7 a range of paraphrastic LMs are evaluated on two state-of-the-art speech recognition tasks for English conversational telephone speech and Chinese broadcast speech respectively. Section 8 is the conclusion and possible future work.As discussed above, in order to capture the paraphrase mapping between longer span syntactic structures, a more general form of modelling is required. To address this issue, the particular type of LMs proposed in this paper can flexibly model paraphrastic relationships at the word, phrase and sentence level. As LM probabilities are estimated in the paraphrased domain, they are referred to as paraphrastic language models (PLMs) in this paper. For a surface word sequenceW=<w1,w2,…,wi,…,wL>of L words in the training data, for example, “And I generally prefer”, rather than maximizing the surface word sequence log-probabilitylnP(W)as for conventional LMs, the marginal probability over its paraphrase variant sequences,{W′}, such as “And I just like” or “I mean I want”, is maximized Liu et al. (2012b, 2013c),(1)F(W)=ln∑ψ,ψ′,W′P(W|ψ)P(ψ|ψ′)P(ψ′|W′)PPLM(W′)where•PPLM(W′)is the paraphrastic LM probability to be estimated;P(ψ′|W′)is a word to phrase segmentation model assigning the probability of a phrase level segmentation,ψ′=<v1′,v2′,…,vK′>of K phrases, given a paraphrase word sequenceW′=<w1′,w2′,…,wi′,…,wL′′>of L′ words in total.P(ψ|ψ′)=∏iP(vi|vi′)uses a phrase to phrase paraphrase model to compute probability of a phrase sequenceψ=<v1,v2,…,vi,…,vK>being paraphrastic to another oneψ′=<v1′,v2′,…,vi′,…,vK′>;P(W|ψ)is a phrase to word segmentation model that converts a phrase sequenceψto a word sequenceW, and by definition is a deterministic, one-to-one mapping, thus considered non-informative.As multiple word to phrase segmentations are possible, ambiguity can occur. If there is no clear reason to favor one phrase segmentation over another,P(ψ′|W′)may be treated as non-informative. This is the approach adopted in this work. The input word vocabularyVwassociated with the word to phrase segmentation model,P(ψ′|W′), is a subset of the output phrase level vocabularyVv. By definition, this allows single word phrases to be generated in addition to multi-word based ones.By differentiating Eq. (1) with respect to the paraphrastic LM log probabilities, it can be shown that the sufficient statistics for a maximum likelihood (ML) estimation ofPPLM(W′)are accumulated for each paraphrase word sequence and weighted by its posterior probability. For a particular n-gram predicting wordwifollowing history hi, the associated statisticsC(hi,wi)are(2)C(hi,wi)=∑ψ,ψ′,W′P(W′|ψ′)P(ψ′|ψ)P(ψ|W)CW′(hi,wi)=∑W′P(W′|W)CW′(hi,wi)whereCW′(hi,wi)is the count of subsequence <hi,wi> occurring in paraphrase variantW′. These sufficient statistics are then use to estimate the paraphrastic LM probabilities {PPLM(·|hi)} under a positive and sum-to-one constraint. By discounting and re-distributing statistics to alternative paraphrases of the same word sequence, paraphrastic LMs estimated using such statistics are expected to have a richer context coverage and improved generalization performance. This advantage can be exploited by various forms of LMs that do not explicitly capture the paraphrastic variability in natural languages. In this paper, the estimation of paraphrastic n-gram LMs is considered and will be described in detail in the following sections.At the same time as improving generalization and coverage, paraphrastic count smoothing can also increase modelling confusion compared to conventional LMs trained on just the surface word sequence. One approach to balance the specific, but poorer coverage word-based n-gram LMs with a more generic LM is to linearly interpolate the LM probabilities. This is commonly used with class-based LMs Brown et al. (1992), Niesler and Woodland (1996), Niesler et al. (1998) and is used in this paper with paraphrastic LMs. LetP(w˜|h˜)denote the interpolated LM probability for any in-vocabulary wordw˜following an arbitrary historyh˜is given by(3)P(w˜|h˜)=λNGPNG(w˜|h˜)+λPLMPPLM(w˜|h˜)where λNGand λPLMare the interpolation weights assigned to the conventional LM distribution PNG(·) and the paraphrastic LM PPLM(·). These interpolation weights are positive under a sum-to-one constraint, and can be optimized on the perplexity of some held-out data.The word level paraphrastic LMs investigated in this paper are in the same form as conventional back-off n-gram models. The only difference between the two lies in the way the sufficient statistics used in LM estimation are derived. Instead of the normal n-gram counts computed directly from the surface word sequence, integer quantized paraphrastic word n-gram counts accumulated from explicitly generated paraphrase variants were used in this work to train word level paraphrastic LMs. These statistics were used by a conventional n-gram probability estimation and smoothing procedure, which makes no assumption over the underlying means used to derive the sufficient statistics. It also guarantees the resulting LM probabilities to be positive and sum-to-one, as well as those obtained after the linear interpolation in Eq. (3).The same form of linear interpolation can also be used between phrase level conventional and paraphrastic LMs.11In this paper the interpolation weights assigned to the phrase level conventional and paraphrastic LMs are determined using their probabilities computed on phrase segmented held-out data based on the longest available phrase segmentation.As multiple word to phrase segmentations are possible for the same word sequence in general, the exact phrase sequence level perplexity evaluation is non-trivial for both the conventional and paraphrastic phrase level LMs. Hence, in this paper only the perplexity performance of word level paraphrastic LMs are evaluated.In order to increase the context span for paraphrastic LMs, a phrase level paraphrastic LM can also be trained. This can be obtained by optimizing a simplified form of the criterion given in Eq. (1), where the word to phrase segmentation modelP(ψ′|W′)is dropped,(4)F(W)=ln∑ψ,ψ′P(W|ψ)P(ψ|ψ′)PPLM(ψ′).Thus for a particular phrase level n-gram predicting phraseψifollowing its historyhˆi, the associated phrase level statisticsC(hˆi,ψi)are accumulated as(5)C(hˆi,ψi)=∑ψ′P(ψ′|W)Cψ′(hˆi,ψi)whereC(hˆi,ψi)is the count of phrase level subsequence <hˆi,ψi> occurring in phrase level segmented paraphrase variantψ′. These are then used to estimated the phrase level paraphrastic n-gram LM probabilities{PPLM(·|hˆi)}in this paper.In order to incorporate richer linguistic constraints, it is possible to train and form a log-linear combination of LMs that model different units, for example, words and phrases. LMs built at word and phrase level are combined to yield a multi-level LM to further improve discrimination Liu et al. (2010, 2013a,b). This requires word level lattices to be first converted to phrase level lattices before the log-linear combination is performed. The log-linear interpolation weights determine the contribution from component LMs. These were empirically set as 0.6 and 0.4 for word and phrase level LMs, and kept fixed for all experiments of this paper.22In practice, this setting was found to give comparable performance to an equal log-linear weighting of word and phrase level LMs. The error rate was found insensitive to the setting of these weights when they are varied in the region from (0.3:0.7) to (0.7:0.3).As discussed above, paraphrastic LMs directly target expressive richness related variability in natural languages. A central part of this generative modelling framework uses a statistically trained phrase level generative model to explicitly produce multiple paraphrase variants for each training data sentence. This allows automatically discounted paraphrastic counts to be obtained to estimate LM probabilities. The overall general procedure of constructing a paraphrastic LM is summarized below.1:Estimation of the phrase to phrase paraphrase modelP(v|v′)in Eqs. (1) and (4) for both word and phrase level paraphrastic LM training;2:Construct the word to phrase segmentation modelP(ψ′|W′)and the phrase to word segmentation modelP(W|ψ)that produces or accepts the phrases allowed by the resulting phrase level paraphrase modelP(v|v′);3:for every sentence in the training data do4:Generate paraphrase variants using the above word to phrase segmentation modelP(ψ′|W′), the phrase level paraphrase modelP(v|v′)and the phrase to word segmentation modelP(W|ψ)(for word level paraphrase lattices only);5:Accumulate paraphrastic n-gram counts at word level in Eq. (2), or phrase level in Eq. (5), over all generated paraphrase variants and weighted by their posteriors;6:end for7:Word or phrase level paraphrastic LM training using the above accumulated sufficient statistics.In the following sections, each step of the above paraphrastic LM training procedure is described in detail.A phrase level paraphrase model is used in paraphrastic LMs, as discussed in Sections 1 and 2. In order to obtain sufficient phrase coverage, an appropriate technique to learn a large number of paraphrase phrase pairs is required. Since it is impractical to obtain expert semantic labelling at the phrase level, statistical paraphrase extraction schemes are needed.Statistical paraphrase induction methods can be categorized into two major types, depending on the nature of the data being used Androutsopoulos and Malakasiotis (2010), Madnani and Dorr (2010). The first category uses comparable or parallel text data. Coarse grained alignment Barzilay and Lee (2003), or statistical machine translation based extraction methods Brown et al. (1990) are used to learn the paraphrastic relationship among words and phrases. As these methods assume a partial or complete semantic overlap between sentences, highly specialized training material is required. Hence, it is expensive to obtain and use on a large scale. The second category of techniques perform paraphrase pair extraction using standard text data Lin and Pantel (2001), Pasca and Dienes (2005). These are motivated by distributional similarity theory Harris (1954), which postulates that phrase pairs often sharing the same left and right contexts are likely to be paraphrases of each other. As standard text data in large amounts can be used, wide phrase coverage can be obtained.In order to exploit the advantages of standard text data based statistical paraphrase induction techniques as discussed in Section 3.1, an n-gram based paraphrase induction algorithm given below is used in this paper to estimate the paraphrase model Liu et al. (2012b). When this distributional similarity based paraphrase learning algorithm is used, the minimum and maximum phrase length are set as Lmin=1 and Lmax=4, and the left and right context length set as LN=3. In practice these settings are found to produce a good balance between the coverage and quality of the extracted paraphrases.33Decreasing the setting of context length LNweakens the constraint used in the paraphrase learning algorithm. This results in an increase in the number of phrase pairs extracted but also a deterioration in their quality. No performance improvement was observed by further increasing LN, or the maximum phrase length Lmax.Unless otherwise stated, these are thus kept fixed for all experiments in this paper.1:initialize phrase pair list V={};2:initialize n-gram subsequence list U={};3:for every sentence in training data do4:find and add all subsequences<cl,v,cr>such thatLcl=LN,Lcr=LNandLmin≤Lv≤Lmaxinto U.5:end for6:for every<cl,v,cr>in Udo7:for every other<cl′,v′,cr′>in Udo8:ifcl=cl′,cr=cr′andv≠v′then9:if<v→v′>and<v′→v>not in Vthen10:add phrase pairs<v→v′>,<v′→v>to V;11:end if12:increase co-occurrence countsC(v→v′)andC(v′→v)both by 1;13:end if14:end for15:end for16:for every phrase pair<v→v′>in Vdo17:estimate paraphrase probp(v′|v)=C(v→v′)∑v¯C(v→v¯)18:end forThe above algorithm can be extended to incorporate additional useful information. For example, it is possible to build domain or style dependent paraphrastic LMs via a directed paraphrasing by restricting the choice of target phrases being used. In common with other paraphrase induction methods, the above scheme can also produce phrase pairs that are non-paraphrastic, for example, producing antonyms. However, this is less of a concern for language modelling since the primary aim is to improve context coverage.In general, it is possible to allow the extracted paraphrases to contain out-of-vocabulary (OOV) words that are not modelled by the conventional LM. This can improve the resulting paraphrastic LM's vocabulary coverage. In this paper, a common vocabulary is used for both the standard LMs and paraphrastic LMs. This requires the above n-gram based paraphrase learning algorithm to be modified so that all phrase pairs that contain OOV words are discarded in the accumulation of co-occurrence counts.In order to train paraphrastic LMs, multiple paraphrase variants are required to compute the sufficient statistics given in Eq. (2). As all four components of the paraphrastic LM given in Eq. (1) can be efficiently represented by WFSTs Mohri (1997), WFST based paraphrase variant generation was used in this work, rather than designing special purpose decoding tools. For each training data sentence, the paraphrase word latticeTW′is generated using a sequence of WFST composition operations, before being projected onto the word sequence level and compressed via the determinization operation. This is given by(6)TW′=det(πW′(TW:W∘TW:ψ∘Tψ:ψ′∘Tψ′:W′))whereTW:Wis the transducer containing the original word sequence,TW:ψis the word to phrase segmentation transducer,Tψ:ψ′the phrase to phrase paraphrase transducer andTψ′:W′the phrase to word transducer. ∘, det(·) and π(·) denote the WFST composition, determinization and projection operations.An example of a word to phrase segmentation transducer is shown in Fig. 1(a), which can generate seven phrases. These include, the sentence start “<s>” and end symbol “</s>”, single word phrases “and”, “I”, “generally” and “prefer”, as well as a two word phrase “and_I”. Here “<e>” denotes the null symbol. When used for paraphrase lattice generation, in order to obtain a sufficient depth of the resulting lattices, all possible word to phrase segmentations are allowed in Eq. (6) to be further transformed to their associated phrase level paraphrases. The phrase to word transducer can be derived by taking the word to phrase transducer's inverse (swapping input and output symbols). As mentioned in Section 2, both the phrase to word, and word to phrase segmentation models are considered non-informative in this paper for paraphrastic LM training.An example part of a phrase to phrase paraphrase model is shown in Fig. 1 (b), where an input phrase “prefer” is paraphrased into a total of 12 single word phrases including “appreciate”, “like”, “want”, “need”, “love” and “wish”, as well as multi-word phrases such as “really_like” and “really_appreciate” and “would_like”. When using the paraphrase phrase pair extraction method presented in Section 3, it is possible that some phrases may have no paraphrases available in the training data. In order to ensure the resulting paraphrase lattice is fully connected, self-reflexive arcs that map the input phrases to the same output are also included in the paraphrase transducer with zero cost. For the example shown in Fig. 1(b), this is represented by the top arc in the transducer that maps the input phrase “prefer” to itself. It should noted that including this self-reflexive arc will not lead to over counting in the paraphrastic counts accumulation in Eqs. (2) and (5), as both are computed using properly normalized paraphrase sequence posterior probabilities derived from a lattice forward-backward pass.It is also possible to construct standard, non-paraphrastic phrase level LMs using the phrase segmentation transducer shown in Fig. 1(a). First, a phrase level latticeTψcontaining all possible segmentations for the original word sequenceWis derived by the following WFST operations,(7)Tψ=ω≐1(det(πψ(TW:W∘TW:ψ)))where ω≐1(·) denotes the operation of setting all the WFST arc weights to 1. For an example sentence “And I generally prefer”, the resulting phrase level segmented WFST lattice is shown in Fig. 2. In order to obtain a maximum phrase level context span and linguistic constraints, the shortest path inTψ, for the same example shown in Fig. 2, “And_I generally prefer”, which contains the longest available phrase segmentation, is used to train a conventional, non-paraphrastic phrase sequence level LM.In order to improve the efficiency in paraphrase lattice generation, a relative beam of 5.0 based WFST pruning was applied. Combined with the pruning, a bi-gram LM was used to further improve the paraphrasing efficiency and deweight the statistics accumulated from very unlikely paraphrase sequences.44Initial experiments show when no such bi-gram LMs were used in paraphrase lattice generation, a small performance degradation was found in the resulting paraphrastic LM.The WFST based paraphrase generation approach in Eq. (6) is thus modified as(8)TW′=det(πW′(TW:W∘TW:ψ∘Tψ:ψ′∘Tψ′:W′)∘GW′)whereGW′is the acceptor representing the bi-gram LM estimated on the surface word sequence. In practice this was found to provide a good balance between improving the paraphrase generation quality and retaining sufficient depth in the lattices.As discussed in Section 2, phrase level paraphrastic lattices are used to accumulate the sufficient statistics in Eq. (5) to train phrase level paraphrastic LMs. These phrase level paraphrase latticesTψ′can be generated using a sequence of WFST operations similar to those above used for word level paraphrase lattice generation given in Eq. (8), except that the phrase to word segmentation transducerTψ′:W′is now dropped, and a fixed phrase level bi-gram LM acceptorGψ′, trained on phrase segmented texts derived using the WFST operation in Eq. (7) associated with the longest available phrase segmentation, is used instead.Using the above WFST based decoding approach for an example sentence “And I generally prefer”, and a paraphrase model trained on 545 million words of conversational English data, an example paraphrase lattice after pruning is shown in Fig. 3.Inside the lattice, the following paraphrase variants are among those generated: “And I just like”, “I mean I want”, “I guess I prefer”, “You know I need”, “And I appreciate”, “I probably have”, “‘Cause I like”, “Well I need” and “So I like”. As the n-gram based paraphrase extraction method presented in Section 3 can also produce phrase pairs that are non-paraphrastic, antonyms such “hate” for word “prefer”, previously shown in the 9th arc from the top in the phrase level paraphrase transducer of Fig. 1(b). Hence, word sequences such as “And you know I hate” were also found in paraphrase lattice before pruning. Using the same above paraphrase model, the lattice density measured on the word level paraphrase lattices generated for the 20M word Fisher data is 5.1arcs on average for every word in the surface word sequence.In order to improve phrase coverage, expert semantic labelling provided by resources such as WordNet Fellbaum (1998) can be used. The expert semantic labelling by WordNet, including synonyms, hypernyms, hyponyms and pertainyms, were used to extract manually derived paraphrases, for example, “choose” and “favor” for word “prefer”, in addition to those automatically learnt and shown in Fig. 1(b). The semantic similarity based HowNet Dong and Dong (2006) for the Chinese language was also used in this paper to generate expert paraphrases. Multi-character words that share the same semantic class labelling and part-of-speech tagging are considered as paraphrases to each other. As for both expert resources these paraphrase phrase pairs are not statistically derived, the resulting paraphrase model are treated as non-informative, and all the arcs in the paraphrase transducer, for example, shown in Fig. 1(b), carry zero cost. In order to reduce the statistics contribution from very unlikely paraphrase sequences, the bi-gram LM introduced in Eq. (8) is again used in paraphrase lattice generation. Due to their different nature, statistically learned and expert derived paraphrase pairs were used to generate separate sets of lattices, and paraphrastic LMs. These models are then used in the interpolation with standard LMs in Eq. (3). When combined with conventional LMs using Eq. (3), their interpolation weights are optimized on the perplexity of some held-out data, as discussed in Section 2.After paraphrase lattices are generated using the WFST based decoding algorithm discussed in Section 4, the n-gram statistics given in Eqs. (2) and (5) required for paraphrastic LM estimation are accumulated from these word or phrase level paraphrase lattices via a forward-backward pass. These sufficient statistics are then use to estimate back-off word or phrase level n-gram Katz (1987) probabilities. In order to improve generalization to contexts that cannot be found in either the training data or the associated paraphrases, appropriate parameter smoothing and discounting methods, for example, modified KN smoothing Chen and Goodman (1999), are required.As the standard modified KN smoothing algorithm was originally derived only for integer based n-gram counts, it cannot be directly used on the fractional paraphrastic LM counts. To address this issue, it is possible to extend the modified KN smoothing method itself to handle fractional statistics along the line of the work proposed in Tam and Schultz (2008).55A fractional Kneser–Ney smoothing scheme was investigated in Tam and Schultz (2008) for correlated bi-gram latent semantic analysis (LSA) models. In the experimental results presented in that work, a small error rate reduction of 0.1% absolute was reported over interger counts based Witten–Bell smoothing, where the baseline bi-gram LSA model using gave an error rate of 23.8%In this paper, a simplified approach was used to handle the same issue. Word or phrase level fractional n-gram counts were quantized into integers before n-gram estimation. First, a minimum float count cut-off of 0.001 was applied to discard all rare paraphrastic counts below such threshold. All the retained fractional counts were then increased by 1 before rounding to the nearest integer. The resulting integer quantized statistics were used in n-gram estimation and modified KN smoothing with the SRILM toolkit Stolcke (2002). Independent of the nature of the underlying means used to derive the integer based n-gram sufficient statistics, being computed directly from the surface word sequence as in conventional LMs, or from paraphrase lattices as in this work, this well established n-gram probability estimation and smoothing procedure guarantees the resulting LM probabilities to be under the positive and sum-to-one constraints. In practice this approach was found to outperform alternative smoothing methods such as Witten-Bell smoothing in terms of both perplexity and error rate. The resulting paraphrastic n-gram LMs were then linearly combined with the conventional LM using the form of interpolation given in Eq. (3).In order to handle the data sparsity problem, language modelling techniques based on a continuous vector space representation of word sequences, such as neural network LMs (NNLM), can be used. Depending on the network architecture being used, these can be categorised into feed-forward n-gram based NNLMs Bengio et al. (2003), Kuo et al. (2012), Le et al. (2013), Park et al. (2010), Schwenk (2007), and recurrent NNLMs Mikolov et al. (2010), Sundermeyer et al. (2012). In this paper, the combination between paraphrastic LMs and feed-forward n-gram based NNLMs is considered Liu et al. (2013c).Both paraphrastic LMs and feed-forward NNLMs can improve LM generalization. However, there are major differences between them that can also be exploited as complementary characteristics. First, paraphrastic LMs can be trained using large amounts of training data. In contrast, to reduce computational cost, feed-forward NNLMs are normally trained using only a small in-domain data set, for example, audio transcripts, and optionally a re-sampled subset of out-of-domain data Schwenk (2007) available in large quantities. Secondly, paraphrastic LMs re-distribute sufficient statistics to variable length paraphrase variants of the same sentence. The resulting sequence level smoothing of LM probabilities is different to the n-gram level smoothing used by NNLMs. Finally, the paraphrastic LMs considered in this paper are based on n-gram models. Despite being more efficient than NNLMs in probability computation, their generalization ability remains limited for unseen contexts that cannot be found in either the training data or the associated paraphrases. Hence, in order to leverage the strengths of both models, the combination between paraphrastic LMs and NNLMs is investigated in this paper. The particular form of combination considered in this paper is a linear interpolation between the paraphrastic LM, the feed-forward NNLM and the conventional n-gram LM. The interpolated LM probabilities given in Eq. (3) are therefore modified as,(9)P(w˜|h˜)=λNGPNG(w˜|h˜)+λPLMPPLM(w˜|h˜)+λNNPNN(w˜|h˜)where λNNis the interpolation weight assigned to the neural network LM. In the same fashion as in Eq. (3), component LM interpolation weights can be optimized on held-out data.For the multi-level paraphrastic LMs discussed in Section 2, the above interpolation needs to be performed at both the word and phrase level prior to the log-linear combination between the word and phrase level LMs. In addition to a word level neural network LM, a neural network LM constructed using phrase level segmented training data is also required. The phrase level segmented data can be obtained using the WFST operations given in Eq. (7) and 1-best search in the resulting phrase lattices as described earlier in Section 4.To reduce computational cost, conventional feed-forward NNLMs only model the probabilities of a small and more frequently occurring subset of the complete vocabulary, commonly referred to as the shortlistSchwenk (2007). The output layer normally only contains nodes for in-shortlist words. A similar approach may also be used at the input layer when a large vocabulary is used. Two issues arise when using this conventional NNLM architecture. First, NNLM parameters are trained only using the statistics of in-shortlist words thus introduces an undue bias to them. Secondly, as there is no explicit modelling of probabilities of out-of-shortlist (OOS) words in the output layer, statistics associated with them are also discarded in NNLM training. To handle these issues, alternative network architectures that model a full vocabulary at the output layer can be used Park et al. (2010), Le et al. (2013). In this paper, an NNLM architecture with an additional output node explicitly modelling the probability mass of OOS words Park et al. (2010) is used. This ensures that all training data are used in NNLM training, and the probabilities of in-shortlist words are smoothed by the OOS probability mass, thus obtaining a more robust parameter estimation. The architecture of a 4-gram feed-forward NNLM of this form is illustrated in Fig. 4.

@&#CONCLUSIONS@&#
