@&#MAIN-TITLE@&#
Efficient training for dendrite morphological neural networks

@&#HIGHLIGHTS@&#
An efficient training algorithm for a dendrite morphological neural network.Convergence in a finite number of steps.Perfect classification of the training data.No overlap between hyper-cubes with distinct class labels.The algorithm can be applied to classification problems of p classes and n attributes.

@&#KEYPHRASES@&#
Dendrite morphological neural network,Efficient training,Pattern recognition,Classification,

@&#ABSTRACT@&#
This paper introduces an efficient training algorithm for a dendrite morphological neural network (DMNN). Given p classes of patterns, Ck, k=1, 2, …, p, the algorithm selects the patterns of all the classes and opens a hyper-cube HCn(with n dimensions) with a size such that all the class elements remain inside HCn. The size of HCncan be chosen such that the border elements remain in some of the faces of HCn, or can be chosen for a bigger size. This last selection allows the trained DMNN to be a very efficient classification machine in the presence of noise at the moment of testing, as we will see later. In a second step, the algorithm divides the HCninto 2nsmaller hyper-cubes and verifies if each hyper-cube encloses patterns for only one class. If this is the case, the learning process is stopped and the DMNN is designed. If at least one hyper-cube HCnencloses patterns of more than one class, then HCnis divided into 2nsmaller hyper-cubes. The verification process is iteratively repeated onto each smaller hyper-cube until the stopping criterion is satisfied. At this moment the DMNN is designed. The algorithm was tested for benchmark problems and compare its performance against some reported algorithms, showing its superiority.

@&#INTRODUCTION@&#
Pattern classification is a relevant problem in Artificial Intelligence. If a machine needs to efficiently interact with its environment, this problem should be correctly solved. The pattern classification problem can be stated as follows: given a patternX=(x1,x2,…,xn)T, or a distorted versionX˜, find, somehow, its membership classC1,C2,…,Cp. Hundreds of proposals have been described in the literature. A group of them (the distance based methods) make use of a distance between pattern X and the prototypeZk,k=1,2,…,p, of each class Ck, and the pattern is assigned to the class Ck, for whichd(X,Zk)is minimal. Another group (the decision based methods) uses a set of separation functions: f1, f2, …, among classes. One more group (the probability based methods) utilizes the a priori conditional probabilities:p(Ck|X), and assigns X to the class for whichp(Ci|X)>p(Cj|X),i≠j. The methods belonging to the syntactical approach make use of structural pattern descriptions to determine the index class of X. In short, those methods based on the Artificial Neural Network (ANN) approach utilize the knowledge codified into the weights of the ANN to classify X.The so called dendrite morphological neural networks (DMNN) belong to the ANN approach. DMNN were first described by Ritter and colleagues in [19,22]. DMNN emerge as an improvement of classical morphological neural networks (MNN), originally introduced by Davidson in [5] and then re-discussed by Ritter and Sussner in [21]. Morphological neural networks are closely related to Lattice Computing [9], which can be defined as the collection of Computational Intelligence tools and techniques that either make use of lattice operators inf and sup for the construction of the computational algorithms and replace the multiplication algebra operator of the real numbers by the addition operator. Algorithms and applications of Lattice Computing can be found in [12,8,7,31,29,30,13,24,4,3,25,34,6,15,17,27]. Morphological perceptrons with competitive learning, a variation of standard morphological perceptrons are discussed in [28]. Processing at dendrites level and not only at the cell body level allows neurons to power their processing capacities [26]. This fact is taken into account by Ritter and colleagues in the DMNN proposal.A key issue in the design of a DMNN is its training; this is in the selection of the number of dendrites and the values of synaptic weights for each dendrite. Diverse algorithms to automatically train a DMNN can be found in [19,22,2,32,20,23,4].In this paper, a novel algorithm for the automatic training of a DMNN is presented. At the first step, the algorithm takes the complete training set and opens a hyper-cube such that all the patterns rest inside. The size of HCncan be chosen such that the border elements remain in some of the faces of HCn, or can be chosen for a bigger size by adding a margin M on each side of the hyper-cube. An evolutive algorithm was used to determine the optimal value of M. At the second step, the algorithm divides the hyper-cube into 2nsmaller hyper-cubes, (with n the dimensionality) and verifies if each hyper-cube contains only patterns from one class C. If this is the case, the algorithm provides the designed DMNN and stops, otherwise it divides each smaller hyper-cube until the stopping criterion is satisfied. For testing, the hard limiter activation function of the original DMNN was changed by the maximum function to improve the results.The rest of the paper is organized as follows. In Section 2 the basics and the most important concepts on dendrite morphological neural networks are given. Section 3 is dedicated to explain the philosophy and functionality of the proposed training algorithm. Section 4 is focused to present the experimental results where the proposed training algorithm is tested and compared with other reported methods by using some examples. Finally, Section 5 is oriented to provide the conclusions and directions for further research.Morphological neural networks use lattice operations ∨ (maximum), or ∧ (minimum), and + from the semi-rings(R−∞,∨,+)or(R∞,∧,+)whereR−∞=R∪{−∞}andR∞=R∪{∞}. The neuron computation in a MNN for inputx=(x1,x2,…,xn)is given by(1)τj(x)=aj⋁i=1nbij(xi+wij)or(2)τj(x)=aj⋀i=1nbij(xi+wij)wherebij=±1denotes if the ith neuron causes excitation or inhibition on the jth neuron,aj=±1denotes the output response (excitation or inhibition) of the jth neuron to the neurons whose axons contact the jth neuron and wijdenotes the synaptic strength between the ith neuron and the jth neuron. Parameters bijand ajtake +1 or −1 value if the ith input neuron causes excitation or inhibition to the jth neuron.The computation performed by the kth dendrite can be expressed by the formula(3)Dk(x)=ak⋀i∈I⋀l∈L(−1)1−l(xi+wikl)wherex=(x1,x2,…,xn)∈Rncorresponds to the input neurons,I⊆{1,…,n}denotes to the set of all input neurons Niwith terminal fibers that synapse on the kth dendrite of a morphological neuronN,L⊆{0,1}corresponds to the set of terminal fibers of the ith neuron that synapse on the kth dendrite of N, andak∈{−1,1}denotes the excitatory or inhibitory response of the kth dendrite.Clearly,I≠0yL≠0since there is at least one axonal fiber coming from at least one of the input neurons with synapse dendrite k. The activation function used in a MNN is the hard limiter function that assigns 1 if the input is greater or equal to 0 and assigns 0 if the input is lesser than 0. A more detailed explanation can be found in [19,23].A key issue in the design of a DMNN is its training; this is in the selection of the number of dendrites and the values of synaptic weights for each dendrite. For purposes of explaining the algorithm, a simple example of three classes with two attributes is used. Fig. 1shows the example patterns, points of C1 are shown as red dots, patterns of C2 as green dots and points of C3 as blue dots.Given p classes of patterns, Ck,k=1,2,…,p, each with n attributes, the algorithm applies the following steps:Step (1)Select the patterns of all the classes and open a hyper-cube HCn(with n the number of attributes) with a size such that all the elements of the classes remain inside HCn. The hyper-cube can be one whose coordinates match the patterns of class boundaries; it can be called the minimum hyper-cube (MHC). For having better tolerance to noise at the classification time, add a margin M on each side of the MHC. This margin is a number greater or equal to zero and is estimated as a function of the size T of the MHC. IfM=0.1Tthen the new hyper-cube will extend that ratio to every side of the MHC. Fig. 1 presents the box that covers all the patterns, withM=0.1T.Divide the global hyper-cube into 2nsmaller hyper-cubes. Verify if each generated hyper-cube encloses patterns from only one class. If this is the case, label the hyper-cube with the name of the corresponding class, stop the learning process and proceed to step 4. For the example, the first division of the box is presented in Fig. 2.The step 3 has two stages: (a) if at least one of the generated hyper-cubes (HCn) has patterns of more than one class, then divide HCninto 2nsmaller hyper-cubes. Iteratively repeat the verification process onto each smaller hyper-cube until the stopping criterion is satisfied. Fig. 3shows all the boxes generated by the training algorithm.(b)Once all the hyper-cubes were generated, if two or more hyper-cubes of the same class share a common side, they are grouped into one region. Fig. 4presents the application of this simplification procedure that automatically reduces the number of hyper-cubes.Based on the coordinates on each axis, calculate the weights for each hyper-cube that encloses patterns belonging to Ck. By taking into account only those hyper-cubes that enclose Ckitems, at this moment the DMNN is designed. Fig. 5shows a dendrite morphological neural network with an input layer that separates the three classes: C1, C2 and C3. The neurons of the input layer are connected to the next layer via the dendrites. The black and white circles denote excitatory and inhibitory connection, respectively. The geometrical interpretation of the computation performed by a dendrite is that every single dendrite determines a hyper-box which can be defined by a single dendrite via its weight values wijas the example shows.For testing, two noisy patterns,X˜1=(4.58.5)of the class C1 andX˜2=(43.5)of the class C3, were selected as examples at this stage. When Eq. (3) is applied to the first dendrite, the following results are obtained:D11(X˜1)=D11(4.58.5)=[(4.5−0.3)∧−(4.5−6.6)]∧[(8.5−7.4)∧−(8.5−9.8)]=[2.1∧1.1]=1.1D11(X˜1)=D11(43.5)=[(4−0.3)∧−(4−6.6)]∧[(3.5−7.4)∧−(3.5−9.8)]=[2.6∧−3.9]=−3.9In the same way, all the other dendrites are calculated; forX˜1:D12=−2.1,D21=−1.1,D31=−1.1,D32=−3.5and forX˜2:D12=−1.6,D21=−1.5,D31=−1.5,D32=1.5. With these values and by means of Eq. (1), the classification is obtained:τ(X˜1)=(D11∨D12∨D21∨D31∨D32)=(1.1∨−2.1∨−1.1∨−1.1∨−3.5)=1.1.τ(X˜2)=(D11∨D12∨D21∨D31∨D32)=(−3.9∨−1.6∨−1.5∨−1.5∨1.5)=1.5.Thereforeτ(X˜1)=1.1≥0corresponds to D11 (index of C1) thusy(X˜1)=1, the input pattern is classified into class C1, as was expected. Moreover,τ(X˜2)=1.5≥0corresponds to D32 (index of C3) thusy(X˜2)=3, the input pattern is correctly classified into class C3. If there is the case that the neuron value (τ) is not greater or equal to zero, then the pattern is not classified into any class.In this section, validation experimental results are presented. These results demonstrate a superior learning performance of the proposed algorithm over the training algorithm for a DMNN proposed by Ritter. Furthermore, comparisons with Multilayer Neural Networks with one hidden layer, Support Vector Machines and Radial Basis Networks were made. The experiments were performed using synthetic 2-dimensional datasets and real datasets.The training algorithm was applied to synthetic datasets, the first dataset was generated and forms two Archimedean spirals defined by the expressions(4)xc(θ)=2(−1)1−cπθcos(θ)(5)yc(θ)=8(−1)1−c3πθsin(θ).wherec∈{0,1}denotes the spiral class label, θ is the angle in radians, and (xc, yc) are the coordinates of the spiral points. A more detailed explanation can be found in [22].Using the above formulas, samples for the training dataset were generated. Fig. 6shows the spirals training dataset with 50 samples. In the following figures, solid dots represent the training points for the classes Ck. Empty circles denote the test samples and asterisks represent the test samples classified in each class.The second dataset was the Ripley's synthetic dataset that consists of two classes [18]. The data is divided into a training dataset and a test set consisting of 250 and 1000 samples, respectively, with the same number of samples belonging to each of the two classes. The training dataset appears in Fig. 7.In the original proposed training algorithm for a single layer morphological perceptron (SLMP-PO) the value of factor M was selected empirically. In a second approach, the value of M that minimizes the error was determined by means of an evolutionary algorithm, differential evolution DE/rand/1/bin [16], with the parameters shown in Table 1.To ensure that there is no sensitivity to “F” and “CR” parameters, these parameter values were generated randomly (using a uniform distribution) per run between the intervals shown in Table 1. The intervals for both parameters were defined empirically. As the results reveal (see Table 2), the application of the differential evolution algorithm to find the best value of M, helps significantly to decrease the classification error of the proposed algorithm.Besides the proposal to find the value of M, another improvement was made. The original DMNN model uses a hard limiter as the activation function. This activation function was changed by the maximum function. This decreases the error into a considerable percentage (see Table 2). This occurs because in the proposed algorithm some testing patterns not allocated to any class might exist. With the use of the maximum function, those patterns are assigned to the closest class (see Fig. 11).Table 2 presents the classification errors obtained with the original proposed algorithm and the improvements for the spiral problem. To get the test datasets, noise was added to the training spiral patterns with a normal distribution with mean on the center of the spiral point and different values of standard deviation (σ), every testing dataset has 500 samples. All the algorithms were implemented using MATLAB 7.11 on a desktop computer with Intel i7 2.2GHz processor, with 8GB in RAM. The classification errors obtained show the advantages of the improvements as can be seen in the following table.To verify the efficiency of the final proposed training algorithm for the single layer morphological perceptron (SLMP-P), training algorithm proposed by Ritter (SLMP-R) [22] and a multilayer perceptron (MLP) were used for comparison. It should be mentioned that the algorithm used in the following experiments is the SLMP-P2 which is referred to as SLMP-P. In all cases, the classification errors obtained by the proposed algorithm were smaller than those obtained by the Ritter's algorithm and the MLP. The results obtained with the final improved algorithm are very satisfactory. The number of dendrites generated by the proposed algorithm is bigger than the number of dendrites of the Ritter training algorithm, this is because in the proposed algorithm the generated hyper-cubes cover all the classes.Fig. 8shows an error percentage comparison graphic of the three algorithms. This graphic shows the superior training performance of the proposed training algorithm over the SLMP-R and the MLP (Table 3).For the MLP with one hidden layer, the training parameters were established as: learning rate=0.25, momentum=0.2, the activation function used was the logistic sigmoid. Hecht-Nielsen [11] suggested that an upper limit for the number of hidden layer neurons should be smaller than 2N+1 (for a special activation function) where N is the number of input neurons. Thus, this limit was considered as a starting point (4 neurons) but in this problem the error obtained with 4 neurons was considerable (between 40% and 50%). It was found that with 9 neurons in the hidden layer the error was acceptable.Figs. 9, 10, 12 and 13 show the boxes formed by the training process using 50 samples and the test classification results obtained by the SLMP-R and the SLMP-P, respectively. From here, all classification result graphs show the test patterns and the training hyper-cubes to the right; to the left, the test samples and the classification results (asterisks) are presented.Fig. 11shows the results for the algorithm SLMP-P1 (hard limiter) to the right, in this case the patterns represented with yellow asterisks are not classified in any class, as can be seen there is an error. However, if the hard limiter activation function is changed by the maximum function, the error obtained in the spiral problem (σ=0.2) is zero and all the test patterns are assigned to one class.Figs. 12 and 13show the results with the SLMP-P for the spiral problem.The classifications results of the algorithms for Ripleys dataset appear in Figs. 14, 15, 16 and 17.Table 4presents the classification errors for the Ripley's dataset obtained by each one of the algorithms. As can be seen, for this problem, the error obtained with the proposed training algorithm is larger that the error achieved in the spiral problem. This is because some samples of the data of C1 are mixed in the class C2 and vice versa. However, the SLMP-P improves the results of the SLMP-R and the MLP, so the performance of the proposed algorithm is satisfactory.In this subsection, the proposed training algorithm was applied to real data with p classes and n features for each class, and the results were compared with a Multilayer Perceptron (MLP), a Support Vector Machine (SVM) and a Radial Basis Network (RBN); the training algorithm proposed by Ritter can only be applied to the bi-class classification problems. First, object recognition experiments were considered and later some datasets from the UCI Machine Learning Repository [1] were used.The proposed algorithm was tested to solve the 5 classes and 2 attributes problem presented in [33]. This problem consists in classifying the objects: sheave, milano tail, screw, eyebolt and spike. The distinctive features used were the first and the second Hu moments. Fig. 18shows the object images and their Hu moments.The training and test datasets have 50 samples each. The proposed algorithm achieved a misclassification percentage of 0% with a value of M=0.7 and 6 dendrites. Fig. 19shows the results.To test the performance of the training algorithm in real images, a subset of the ETH80 database [14] was used. This dataset contains high-resolution color images of 80 objects from 8 different categories, the selected subset has 10 objects and each object is represented by 41 images from viewpoints spaced equally over the upper viewing hemisphere. The size of the images is of 128×128pixels and the images were converted to grayscale. The 10 objects subset is shown in Fig. 20and has the following objects: apple, car, cup-1, cup-2, dog-1, dog-2, pear-1, pear-2, tomato and cow.For object representation, the first four Hu moments obtained from binary images were used. The results were compared with a one hidden layer MLP, a SVM and a RBF. These algorithms were applied using the software WEKA 3-6-9 [10]. For the MLP with one hidden layer, the training parameters were established as: learning rate=0.3 and momentum=0.2, the activation function used was the sigmoid. For the SVM, a Polynomial Kernel of degree 2 was used and for the RBN, two clusters were used. For selecting the Polynomial Kernel degree for the SVM and the clusters number for the RBN, these values were manually changed and the ones that generated the best results, were selected. The misclassification percentage obtained with the different neural networks are presented in Table 5.Hence, as can be seen in Table 5, the modified DMNN improves the results of the remaining algorithms; however, the misclassification percentage could be reduced if more features to describe the objects are used; further research and experimentation is needed to verify this idea.On the other hand, the classification performance of the DMNN was evaluated on seven well-known datasets, which can be found in the UCI Machine Learning Repository [1]. The Iris dataset was the first considered problem. This dataset has 3 classes (Iris setosa, Iris virginica and Iris versicolor) and 4 features (the length and the width of the sepals and petals, in centimeters). The dataset has 50 samples per class. The original dataset was divided into two subsets of the same size, taking samples randomly for training and testing.To display the results, only three attributes were used (length and width of the sepals and length of the petals), in this way the patterns are in the 3D space and the results can be viewed graphically. Fig. 21presents the result of the training and Fig. 22shows the classification results. The algorithm achieved a 4% of misclassification error with M=0.07 and 22 dendrites.The algorithm was applied to the Iris dataset with 4 attributes, the Glass Identification, the Liver Disorders, the Page Blocks, the Image Segmentation and Letter Recognition datasets [1]. The Glass Identification dataset used has 6 classes, 9 attributes and 214 samples (class 6 was not considered as it has very few samples). The Liver Disorders dataset has 2 classes, 6 attributes and 345 samples. The Page Blocks dataset has 5 classes, 10 attributes and 5473 samples. Like the Iris dataset, all the previous datasets were divided into two subsets to get the training and test patterns. The Image Segmentation dataset has 7 classes, 19 attributes and 210 samples for training and 2100 samples for testing. On the other hand, the Digits Handwriting Recognition dataset has 10 classes, 16 attributes and 10,992 samples with a training dataset of 7494 samples and a testing set of 3498 samples. The Letter Recognition dataset has 27 classes, 16 attributes and 20,000 samples, generally 16,000 samples are for training and 4000 for testing, in this work only 30% of the training and testing samples were used. For the Image segmentation and Letter recognition datasets, 10 attributes were used and 5 features were utilized for the Letter recognition dataset. For selecting the more useful attributes, the ranking algorithm “InfoGainAttributeEval” of the software WEKA was used. This algorithm evaluates the worth of an attribute by measuring the information gain with respect to the class. Table 6presents the classification results for the different problems and a comparison with a MLP, a SVM and a RBN, with the parameters specified in the same way as for the ETH80 subset classification problem. As can be seen, in all cases the proposed algorithm obtains the lowest classification error. Furthermore, the proposed algorithm does not manually set parameters so that the training process is very simple for the user.The experiments reveal that the proposed algorithm has the following features:1.Convergence in a finite number of steps.Perfect classification of the training data.No overlap between hyper-cubes with distinct class labels.Once the value of M is set, if there are the same training patterns, the hyper-cubes generated are always the same.No dependency of class presentation order.No areas of indecision in the testing.The algorithm can be applied to classification problems of p classes and n attributes.

@&#CONCLUSIONS@&#
A novel and efficient training algorithm for a dendrite morphological neural network was presented. Comparisons of the proposed training algorithm with the SLMP-R, MLP, SVM and RBN were performed and the results demonstrated the advantages of the SLMP-P. The experiments revealed that the margin M included in the SLMP-P allows the algorithm to be less sensitive to noise than the SLMP-R and the value of M that minimizes the error can be found by an evolutive algorithm. Furthermore, the change of the hard limiter activation function, used in the original dendrite morphological neural network model, by the maximum function is very useful to reduce the error, because with this activation function, the testing patterns not allocated to any class are assigned to the closest class. Moreover, unlike the MLP, there are no convergence problems and always there is a perfect training data classification.On the other hand, validation experimental results show that the training algorithm is used to solve real problems; however, to solve problems with many class attributes, the algorithm could be slow but this inconvenience could be reduced because due to its nature, the algorithm can be parallelized. Hence, future work will be focused on the implementation of the algorithm on a parallel architecture to handle multiclass problems with lots of describing features and in the algorithm application in problems like real time object recognition.