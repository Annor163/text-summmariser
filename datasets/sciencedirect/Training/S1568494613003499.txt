@&#MAIN-TITLE@&#
The software project scheduling problem: A scalability analysis of multi-objective metaheuristics

@&#HIGHLIGHTS@&#
We have addressed the Software Scheduling Problem (SPS), which is of outmost importance for current software companies as the software projects are becoming very large.A scalability analysis of eight multi-objective metaheuristics (NSGA-II, SPEA2, PAES, DE-PT, MO-GA, MOABC, MOCell, GDE3) is performed over a set of SPS instances with up to 256 employees and 512 tasks.PAES has shown to be the most scalable algorithm: it is able to provide the project manager with the more diverse project schedules.

@&#KEYPHRASES@&#
Software project scheduling,Scalability analysis,Multi-objective optimization,

@&#ABSTRACT@&#
Computer aided techniques for scheduling software projects are a crucial step in the software development process within the highly competitive software industry. The Software Project Scheduling (SPS) problem relates to the decision of who does what during a software project lifetime, thus involving mainly both people-intensive activities and human resources. Two major conflicting goals arise when scheduling a software project: reducing both its cost and duration. A multi-objective approach is therefore the natural way of facing the SPS problem. As companies are getting involved in larger and larger software projects, there is an actual need of algorithms that are able to deal with the tremendous search spaces imposed. In this paper, we analyze the scalability of eight multi-objective algorithms when they are applied to the SPS problem using instances of increasing size. The algorithms are classical algorithms from the literature (NSGA-II, PAES, and SPEA2) and recent proposals (DEPT, MOCell, MOABC, MO-FA, and GDE3). From the experimentation conducted, the results suggest that PAES is the algorithm with the best scalability features.

@&#INTRODUCTION@&#
In the current software industry, the market is becoming highly competitive, so software companies have to elaborate accurate project plans in order to have success. In order to plan a software project, companies need, on the one hand, to estimate the project workload (using for example the COCOMO model [1]) and, on the other hand, to establish the project schedule and to assign resources to tasks. Tasks may be anything from maintaining documents to typing source code, and resources include people, machines, time, etc. Like other similar projects (e.g., civil engineering problems), scheduling and staffing management might be performed with traditional techniques such as Program Evaluation and Review Technique (PERT), Critical Path Method (CPM), and those enclosed within the class known as the Resource-Constrained Project Scheduling Problem (RCPSP) [2]. Although relevant and helpful, these methods are also becoming hardly applicable to the today's software projects, specially PERT/CPM, which are not applicable to the complex networks of activities and their precedence and durations that usually arise [3]. What makes the difference of software projects from those from other areas (production scheduling, RCPSP, etc.) is that only one type of resources are allocated, i.e., employees, having each of them different skills and being able to get involved in several tasks during a working day [4]. Employees’ salary is also important as it allows the project to be assigned with a cost (and not only its duration).When a software company starts scheduling any given project, two main goals are always in conflict: reducing the project cost, that is, completing the entire development at the lowest investment and minimizing the project duration so that new projects can be addressed. Of course, these two goals have a direct impact on the company's income. In this paper we focus on the assignment of employees to tasks in a software project so as to minimize these two objectives, that is, it is a multi-objective optimization problem, which has been called the Software Project Scheduling (SPS) problem [5]. Because of their computational complexity and the size of actual projects, this problem cannot be efficiently solved using complete algorithms since enumerating the entire search space would require years of computation [6,7]. Metaheuristics [8] become the choice here. These are a broad family of approximate techniques that can be considered as general search algorithms, including—but not restricted to—Evolutionary Algorithms, Ant Colony Optimization, Particle Swarm Optimization, Simulated Annealing, Tabu Search, and Iterated Local Search. As opposed to exact techniques, metaheuristics do not guarantee to find optimal solutions to the problems, but they allow to reach good solutions in a reasonable amount of time. A metaheuristic can be defined as a high level strategy which controls a number of subordinated techniques (usually heuristics) in the search for an optimum. Using this kind of algorithms does make sense when no efficient algorithm is known to find the optimal solutions or when the complete enumeration is not viable due to time or memory constraints. Researchers have therefore used these algorithms to solve software engineering optimization problems, as it is clearly shown in [9], where a comprehensive review and analysis of the literature is carried out.The SPS problem is multi-objective in nature [10] and such a formulation has been used (no aggregation of the objective values into one single value has been considered). Contrary to single-objective optimization, the solution of a multi-objective problem such as SPS is not one single solution, but a set of nondominated solutions known as the Pareto optimal set, which is called Pareto border or Pareto front when it is plotted in the objective space [11]. Whatever solution of this set is optimal in the sense that no improvement can be reached on an objective without worsening at least another one at the same time. That is, in the context of the SPS problem, it is not possible to reduce the project cost without increasing its duration (or vice versa). The main goal in the resolution of a multi-objective problem is to compute the set of solutions within the Pareto optimal set and, consequently, the Pareto front. Many metaheuristics have been proposed in the literature to deal with multi-objective problems [12].Previous works in the literature have addressed the multi-objective SPS problem with metaheuristics [10], where 36 instances with up to 30 tasks and 15 employees have been tackled. But actual, relevant, contemporary software projects involve both more people and more tasks. This work evaluates the issue of the scalability of eight multi-objective metaheuristics for the SPS problem, three classical methods—NSGA-II [13], SPEA2 [14], and PAES [15]—plus five novel algorithms—DEPT [16], MO-FA [17], MOABC [18], MOCell [19], and GDE3 [20]—on a set of 36 instances with an exponential increase in both the number of tasks (from 16 to 512) and the number of employees (from 8 to 256). Two quality indicators, the hypervolume (HV) [21] and the attainment surfaces [22], have been used to measure the quality of the resulting Pareto fronts. This piece of research has to be considered as an extension of a previous conference publication [23] and, as a consequence, we want to clearly state the novel contributions included here. They follow two different lines: on the one hand, we have included four new algorithms in the comparison (SPEA2, MOABC, MOCell, and GDE3, i.e., doubling the experimentation conducted) to provide the reader with insights about their scalability capabilities; on the other hand, we have thoroughly analized the solutions of the different algorithms to find correlations between their features and the region in the objective space in which these solutions are located. In other words, we want to know what “metaheuristic algorithms do” to obtain a solution with some concrete values for the objective functions, i.e., the characteristics of the resulting project schedules.The paper is structured as follows. Section 2 provides the reader with the formulation of the SPS problem. Section 3 briefly describes the eight multi-objective metaheuristics used. The experimentation performed to assess the performance of these algorithms is detailed in Section 4. In Section 5, we show how a project manager can profit from the results of this paper and Section 6 includes the main conclusions of this work and devises the future lines for further research.We follow here the same formulation proposed in [5]. Thus, the resources considered are people with a set of skills and a salary. These employees have a maximum degree of dedication to the project. Formally, each person (employee) is denoted with ei, where i goes from 1 to E (the number of employees). Let SK be the set of skills, and sithe ith skill with i varying from 1 to S=|SK|. The skills of the employee eiwill be denoted witheiskills⊆SK, the monthly salary witheisalary, and the maximum dedication to the project witheimaxded. The salary and the maximum dedication are real numbers. The former is expressed in abstract currency units, while the latter is the ratio between the amount of hours dedicated to the project and the full working day length of the employee. The tasks are denoted with ti, where i goes from 1 to T (the number of tasks). Each task tihas a set of required skills associated with it, which we denote withtiskills, plus an efforttieffort, expressed in person-month (PM). The tasks must be performed according to a Task Precedence Graph (TPG) that indicates which tasks must be completed before a new task is started. The TPG is an acyclic directed graph G(V, A) with a vertex set V={t1, t2, …, tT} and an arc set A, where (ti, tj)∈A if the task timust be completed, with no other intervening tasks, before task tjcan start. The objectives of the SPS problem are to minimize the cost and the duration of the project, which are defined in Eqs. (7) and (8), respectively. The constraints are: first, that each task must be performed by at least one person; second, the set of required skills of a task must be included in the union of the skills of the employees performing the task; and, third, no employee must exceed her/his maximum dedication to the project.A solution can be represented with a matrix X=(xij) of size E×T, where xij≥0. The element xijis the degree of dedication of the employee eito the task tj. In order to compute the project duration, denoted with pdur, we need to calculate the duration of each individual task (tjdur). This is calculated in the following way:(1)tjdur=tjefforttjahrwheretjahris the amount of human resources spent on task tjand is defined as the sum of the dedication degree that the employees have on the task, that is:(2)tjahr=∑i=1ExijAt this point we can also define the participation of employee eiin the project,eipar, as the fraction of the total workload of the project that was performed by the employee, that is:(3)eipar=∑j=1Txijtjdur∑j=1Ttjeffort=∑j=1T(xij/(∑k=1Exkj)tjeffort)∑j=1TtjeffortFrom the right-most expression it is clear that:(4)∑i=1Eeipar=1The next step is to compute the starting and ending time for each task (tjstartandtjend), which are defined according to the following expressions:(5)tjstart=0if∄ti,(ti,tj)∈Amaxti,(ti,tj)∈A{tiend}otherwise(6)tjend=tjstart+tjdurThe project duration, pdur, is the maximum ending time ever found:(7)pdur=maxj=1T{tjend}.The project cost pcostis the sum of the salaries paid to the employees for their dedication to the project. These charges are computed by multiplying the salary of the employee by the time spent on the project. The time spent on the project is the sum of the dedication multiplied by the duration of each task. In summary:(8)pcost=∑i=1E∑j=1Teisalary·xij·tjdurIn order to check the validity of a solution we must first check that all tasks are performed by somebody, i.e., no task is left undone. That is:(9)tjahr>0∀j∈{1,2,…,T}The second constraint is that the employees performing the task must have the skills required by the task:(10)tjskills⊆⋃{i|xij>0}eiskills∀j∈{1,2,…,T}Finally, in order to compute the overwork poverwe first need to compute the working function for each employee, defined as:(11)eiwork(τ)=∑{j|tjstart≤τ≤tjend}xijIfeiwork(τ)>eimaxdedthe employee eiexceeds her/his maximum dedication at instant τ. The overwork of the employeeeioveris:(12)eiover=∫τ=0τ=pdurramp(eiwork(τ)−eimaxded)dτwhere ramp is the function defined by:(13)ramp(x)=xifx>00ifx≤0The definite integral in (12) always exists and can be easily computed because its integrand is piecewise continuous. The total overwork of the project is then the sum of the overwork for all the employees, i.e.:(14)pover=∑i=1EeioverThis total overwork must be zero:(15)pover=0Algorithm 3.1Template of a multi-objective metaheuristic1:S(0)← GenerateInitialSolutions() // S can contain only a solution2:Evaluation(S)3:A(0)← Update(A(0), S(0))4:t← 05:while notStoppingCriterion()do6:t←t+17:S(t) ← Variation(A(t−1), S(t−1))8:Evaluate(S(t))9:A(t) ← Update(A(t), S(t))10:end while11:Output:AIn this section, we briefly describe the eight metaheuristics used in this study, namely NSGA-II, SPEA2, PAES, DE-PT, MO-FA, MOABC, MOCell, and GDE3. They all are either population-based metaheuristics [8], i.e., they operate on a set of solutions at every iteration, or include an external archive for storing the nondominated solutions found during the search (e.g., PAES is a trajectory-based metaheuristic using such a mechanism), or both (e.g., SPEA2, MOCell are population-based algorithms with external archive). A general template for a multi-objective metaheuristic is displayed in Algorithm 3.1. The general operation of these algorithms begins by generating the initial solutions, S (usually in a random manner), and updating the set of non-dominated solutions found in this first sampling, A (lines 1–3). Then, the search loop starts. It lies in stochasticly varying the solutions included in S and A, and generating a (hopefully improved) new set of solutions (line 7) from which those that are non-dominated are retrieved (line 9). The matching of this general scheme on the eight algorithms used in this work is briefly presented in the following subsections (for a detailed description, interested readers are referred to the references provided for each one).The Non-dominated Sorting Genetic Algorithm II, NSGA-II, was proposed by Deb et al. [13]. It is a genetic algorithm based on generating a new population from the original one by applying the typical genetic operators (selection, crossover, and mutation); then, the individuals in the new and old population are sorted according to their rank, and the best solutions are chosen to create a new population. In case of having to select some individuals with the same rank, a density estimation based on measuring the crowding distance to the surrounding individuals belonging to the same rank is used to get the most promising solutions. From Algorithm 3.1, S and A are considered to be one single set P=S∪A so that, at each iteration, the non-dominated solutions found are used to generate new solutions within the evolutionary loop.The Strength Pareto Evolutionary Algorithm 2, SPEA2, was proposed by Zitler et al. [14]. In this algorithm, each individual has a fitness value that is the sum of its strength raw fitness plus a density estimation. SPEA2 fits perfectly in the general template of Algorithm 3.1, having a population of solutions plus an external archive. That is, the algorithm applies the selection, crossover, and mutation operators with solutions from S to fill the archive A of individuals; then, the nondominated individuals of both the original population and the archive are copied into a new population. If the number of nondominated individuals is greater than the population size, a truncation operator based on calculating the distances to the kth nearest neighbor is used. This way, the individuals having the minimum distance to any other individual are chosen.The Pareto Archived Evolution Strategy, PAES, is a metaheuristic proposed by Knowles and Corne [15]. The algorithm is based on a simple (1+1) evolution strategy. To find diverse solutions in the Pareto optimal set, PAES uses an external archive of nondominated solutions, which is also used to decide about the new candidate solutions. That is, from one single solution PAES is able to approximate the entire Pareto front. In this case |S|=1 in Algorithm 3.1, and the variation operator works with one single solution (a mutation operator) to generate new non-dominated ones. An adaptive grid is used as a density estimator in the archive.The Differential Evolution (DE) is an evolutionary algorithm created by Price and Storn [24]. The fundamental idea behind DE is a scheme for generating new possible solutions (trial individuals) taking advantage of the differences among the population (target individuals), according to its simple formulae of vector-crossover and mutation. We have defined a new multiobjective version that incorporates the Pareto Tournaments concept (DEPT) [16] to choose the best solution between two given ones based (in this case, the target and the trial individuals). Ties are broken in the tournament (in case on non-dominance) by using the crowding distance of NSGA-II. As it can be seen, this algorithm considers both the solution set S and the archive A from Algorithm 3.1 to be the same set. Non-dominated solutions are generated by using the DE crossover with solutions in this set.The Firefly Algorithm (FA) is one of the latest nature-inspired optimizers proposed. This algorithm is defined by Yang [17] and it is inspired by the flash pattern and characteristics of fireflies. To solve the SPS problem we have developed the Multiobjective Firefly Algorithm (MO-FA) in which the attractiveness of a firefly, determined by its brightness, is associated with the objective functions. So, by generating a set of fireflies, the search loop of the algorithm evolves by moving dominated fireflies toward brighter (non-dominated) ones. As DEPT, MO-FA considers the solution set and the archive to be unified. The parameter values have been established as proposed in [17].The Artificial Bee Colony (ABC) is a new population-based algorithm proposed by Karaboga [18] and inspired by the collective behavior of the honey bee swarms. A multi-objective version of ABC (MOABC) has been devised. It keeps the general outline of the ABC algorithm but incorporating the concept of dominance and crowding distance from NSGA-II. In fact, it uses the same idea: the bee colony is split into two sub colonies. As in NSGA-II, the first colony is used to generate the second one and finally those non-dominated solutions from the entire one are retrieved. That is, the matching with Algorithm 3.1 is the same as NSGA-II: S and A are considered as one single set which is iteratively improved with the ABC search operators.The Multi-Objective Cellular Genetic Algorithm, MOCell, is a cellular genetic algorithm (cGA) [19]. Like many multi-objective metaheuristics, it includes an external archive to store the nondominated solutions found so far. The matching with Algorithm 3.1 is therefore straightforward. The archive is bounded and uses the crowding distance of NSGA-II to keep diversity in the Pareto Front. We have used here an asynchronous version of MOCell, called aMOCell4 in [25], in which the cells are explored sequentially (asynchronously). The selection is based on taking an individual from the neighborhood of the current solution (called cell in cGAs) and another one randomly chosen from the archive. After applying the genetic crossover and mutation operators, the new offspring is compared with the current one, replacing it if better; if both solutions are nondominated, the worst individual in the neighborhood is replaced by the current one. In these two cases, the new individual is inserted into the archive.The Generalized Differential Evolution 3, GDE3 [20], is an improved version of the Generalized Differential Evolution (GDE) algorithm [26]. It starts with a population of random solutions, which becomes the current population. At each generation, an offspring population is created using the differential evolution operators; then, the current population for the next generation is updated using the solutions of both, the offspring and the current population. Before proceeding to the next generation, the size of the population is reduced using nondominated sorting and a pruning technique aimed at diversity preservation, in a similar way as NSGA-II, although the pruning used in GDE3 modifies the crowding distance of NSGA-II in order to solve some of its drawbacks when dealing with problems having more than two objectives. Therefore, the matching with Algorithm 3.1 is the same as NSGA-II: S and A are merged into one single set.This section is aimed at presenting the experiments conducted to evaluate the scalability capabilities of the previously described algorithms on 36 instances of the SPS problem. Recall that we are looking at both the (financial) cost and the duration of the project scheduling given by the aforementioned multi-objective metaheuristics. These are the two conflicting objective functions that guide the search of these algorithms.For the empirical study we have used a total of 36 instances.11http://mstar.lcc.uma.es/problems/swscheduling.htmlEach instance represents a different software project. The number of employees and tasks scales up from 8 to 256 and from 16 to 512, respectively. The total number of skills in the project, S, is 10 and the number of skills per employee ranges from 6 to 7. We denote the instances with iT-E, where T and E are the number of tasks and employees, respectively. For example, the instance i128-32 has 128 tasks and 32 employees. The maximum dedication for all the employees is 1 (full working day) in the 36 instances.

@&#CONCLUSIONS@&#
In this paper we have analyzed the scalability of eight multi-objective metaheuristic algorithms when they are applied to the Software Project Scheduling problem. The efficient resolution of the problem is important in the context of software companies, which have to deal with large software projects. We have performed an experimental evaluation using a benchmark of 36 automatically generated instances with increasing size and both, the hypervolume indicator and the attainment surfaces, have been used in order to evaluate the quality of the approximated fronts. The results have reported encouraging conclusions taking into account the property of the algorithms being investigated (i.e., scalability). PAES has shown to be not only the algorithm that scales the best but also the one with the best solution quality (in terms of HV). The attainment surfaces have allowed us to refine this outstanding behavior by graphically displaying the reason of these high HV values: PAES is able to reach projects with low cost and long durations, but it is outperformed by other algorithms in a narrow region of the objective space related to high-cost short-duration schedules. We have also gone one step further by analyzing the properties of the obtained solutions by computing the correlations between objectives, the assigned dedication of the employees, and the tasks.A future line of research would be the design of a new version of the problem including some real-world issues that are not present in the current formulation, like communication overhead in a group of employees or solution robustness. Especially interesting is the design of new hybrid algorithms that combine together the best features of the algorithms analyzed in this work as well as a theoretical analysis of the problem landscape so as to devise advanced operators and algorithms.