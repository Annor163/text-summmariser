@&#MAIN-TITLE@&#
Color constancy by combining low-mid-high level image cues

@&#HIGHLIGHTS@&#
We integrate image statistics, regions and scene characteristics.A Bayesian framework is adopted to combine all cues in a principled way.Results of different cues combination are analyzed.We demonstrate that all cues combined together produces the best result.

@&#KEYPHRASES@&#
Color constancy,Low-level image statistics,Intermediate-level regions,High-level scene detection,Bayesian framework,

@&#ABSTRACT@&#
In general, computational methods to estimate the color of the light source are based on single, low-level image cues such as pixel values and edges. Only a few methods are proposed exploiting multiple cues for color constancy by incorporating pixel values, edge information and higher-order image statistics. However, expanding color constancy beyond these low-level image statistics (pixels, edges and n-jets) to include high-level cues and integrate all these cues together into a unified framework has not been explored.In this paper, the color of the light source is estimated using (low-level) image statistics, (intermediate-level) regions, and (high-level) scene characteristics. A Bayesian framework is proposed combining the different cues in a principled way.Our experiments show that the proposed algorithm outperforms the original Bayesian method. The mean error is reduced by 33.3% with respect to the original Bayesian method and the median error is reduced by 37.1% on the re-processed version of the Gehler color constancy dataset. Our method outperforms most of the state-of-the-art color constancy algorithms in mean angular error and obtains the highest accuracy in terms of median angular error.

@&#INTRODUCTION@&#
Color is an important cue in computer vision and imaging related topics, like human–computer interaction [1], color feature extraction [2] and color appearance models [3]. Differences in illumination cause measurements of object colors to be biased towards the color of the light source. Fortunately, humans tend to perceive object colors more or less constant despite large differences in illumination color. Color feature extraction and color appearance models would benefit from a similar color constancy ability.Typically, computational methods to estimate the color of the light source are based on single, low-level image cues. For instance, pixel values are used in well-established methods like the Grey-World method [4] and the Gamut mapping [5]. However, using only pixel information ignores a considerable amount of information that is available in pixel derivatives. To this end, edge information is exploited in the context of color constancy such as the Grey-Edge method [6]. Hirakawa et al. [7] decompose an input color image into distinct spatial sub-bands, and then model the color statistics separately in each subband. This approach is similar to modeling edge distributions. Only a few methods are proposed exploiting multiple image cues for color constancy. The generalized Gamut mapping [8] incorporates pixel values, edge information and higher-order statistics. But extensions beyond low-level statistics are cumbersome at best.For mid-level image cues, segmentation is used to improve the accuracy of color constancy algorithms, e.g. [9]. Image segmentation reduces the effect of large, uniformly colored regions. Furthermore, it reduces the computational complexity as the number of regions is typically a fraction of the number of pixels. The exemplar-based method of [10] exploits region information. First, nearest neighbor surfaces for each region are computed. Then, the illuminant is estimated by integrating the neighboring votes. Learning-based approaches are proposed using multiple types of cues by selecting the most appropriate estimation algorithm based on specific criteria, e.g. image statistics [11], scene geometry information [12] or scene information [13]. However, these methods do not scale easily for multiple cues.Recent advances in automatic scene segmentation and recognition have now reached a level of accuracy to adequately classify semantic cues (e.g. objects and scenes) in images and videos. Color constancy can largely benefit from semantic cues to determine the possible light sources under which an image is recorded. For example, for images of outdoor scenes (e.g. streets, landscapes and forests), the possible light sources are daylight variants. This argument holds also for other concepts (offices, meeting rooms and shops) where a restricted set of indoor lighting is possible. Based on scene classification, the context of these concepts can be derived and consequently the range of possible light sources can be learned. For example, Bianco et al. [13] show that the image statistics of different concepts will result in different “preferred methods”, e.g. the Shades of Grey method [14] will generally result in a better performance when applied to indoor images, while the second-order Grey-Edge is more suited for outdoor images [13].In this paper, color constancy is computed by integrating (low-level) image statistics, (intermediate-level) regions and (high-level) scene characteristics. A Bayesian framework is adopted to combine all cues in a principled way. More cues can easily be added and no prior color constancy algorithms are assumed. An overview of the proposed system is shown in Fig. 1. First, the input image is decomposed into several components. Low-level components involve pixel-values, edges and higher-order derivatives. Medium-level components are blobs and super-pixels. High-level semantic cues involve concepts (e.g. indoor/outdoor). Then, all these components are fused by the proposed Bayesian framework.The remainder of this paper is organized as follows. In Section 2, we give a brief overview of the Bayesian approach to color constancy. In Section 3, we describe a Bayesian framework used for multiple visual cues as well as the visual cues used. In Section 4, experimental results are given followed by our conclusion in Section 5.Most illuminant estimation algorithms are based on simplifying assumptions. The White-Patch algorithm [15] assumes that the maximum response in RGB channels is caused by a perfect white reflectance. The Grey-World algorithm [4] assumes that the average reflectance in a scene is achromatic. Bayesian approaches [16,17], on the other hand, do not explicitly model these assumptions but rather model the variability of the surface reflectances and illuminants as random variables. An illuminant estimate is obtained from the posterior distribution conditioned on the image data. The advantage of such methods is the adaptability to the data. This approach provides a robust and elegant way to combine multiple cues and learn the light sources.To be precise, let a pixel in a linear RGB image be denoted byy,with three color channels: (yr, yg, yb). Under a Lambertian surface assumption, the relation between the image pixely,the reflectancex=(xr,xg,xb)and the light sourcel=(lr,lg,lb)is given by(1)yr=lrxryg=lgxgyb=lbxb,which can be written in matrix form as(2)L=diag(l)y=Lx.If the observed data are:(3)Y=(y(1),…,y(n)),with unknown reflectances:(4)X=(x(1),…,x(n)),the posterior probability for the illuminant is given by(5)p(l|Y)∝p(Y|l)p(l)=∫X(∏ip(y(i)|L,x(i))p(X)dX=|L−1|np(X=L−1Y)p(l)In [16], the illuminants and the reflectances are assumed to be independent. The prior distribution of reflectances is estimated by an empirical distribution, which improves over a Gaussian prior. For the illuminant prior, instead of assuming to be uniform over a constrained set [16], Gehler et al. [17] propose to use an empirical distribution which is obtained from the illuminants of the training set. We adopt the approach of Gehler to model the illuminant prior. It has the advantage of steering the algorithm towards frequently occurring illuminants.The goal of Bayesian color constancy is to compute the color of the light source that minimizes the Bayesian risk. The estimated illuminant is selected by computing the posterior probability of the model under all admissible illuminants for any given image.In general, image features can be divided into low-level features (e.g. corners and edges), medium-level features (e.g. regions) and high-level features (e.g. objects and scene types). In this section, we will discuss different levels of features for color constancy and show how to incorporate them into the Bayesian framework.Traditionally, pixel values are used as image cues for illumination estimation [4,5,15]. However, these approaches ignore a considerable amount of information that is available in the image derivatives, i.e. correlations between pixels. It is shown that the n-jet describes the derivative structure of an image [18,19]. The n-jet description encompasses pixel values, edges and higher-order structures. In this paper, we will consider the n-jet up to the second order(6)J={f,fx,fy,fxx,fxy,fyy},wheref=R,G,Band the derivatives are computed by a convolution with a Gaussian at the scale of the derivative filter(7)f⊗∂∂xGσ=∂∂x(f⊗Gσ).Note that the diagonal model consists of strictly positive elements, but the first and second-order derivatives can contain negative as well as positive values. Instead of using the derivative features, we compute ∇f for the 1-jet and ∇∇f for the 2-jet as follows:(8)∇f=fx2+fy2∇∇f=fxx2+4*fxy+fyy2,We use these higher-order image statistics, in terms of n-jet, for the Bayesian color constancy.According to the Bayesian framework, the posterior probability for the illuminat using pixel information is given by(9)p(l|Ypixel)∝|L−1|np(Xpixel=L−1Ypixel)p(l),where the empirical distributions for pixel valuesp(Xpixel)is learned by computing histograms in RGB space using 32 × 32 × 32 bins similar to [17]. The probability of each reflectance is a function of the number of appearances in the learning set. Similar to the posterior probability using pixel information, the posterior probability using n-jets information is written as follows:(10)p(l|Yn-jet)∝|L−1|np(Xn-jet=L−1Yn-jet)p(l),Given an image, the n-jet information is computed for each channel, and then normalized to [0, 1]. The empirical distributions for the n-jet informationp(Xn-jet)is also learned by computing histograms of 32 × 32 × 32 bins.Rather than using the entire image, segmentation is used to improve the accuracy of color constancy algorithms, e.g. [9]. This preprocessing step leads to improved results. For example, the gray world approach is sensitive to large, uniformly colored surfaces. The same problem also holds for the Bayesian approach. Large regions may considerably skew the reflectance distributions. Segmenting the image reduces the effects of large, uniformly colored regions. By balancing surfaces to a more equal distribution, segmentation will also reduce the complexity of the algorithms as the number of regions is typically only a fraction of the number of pixels.Many methods are proposed to segment an image in a bottom-up way such as Normalized Cuts (NCuts) [20], the Felzenszwalb and Huttenlocher (FH) [21] and Mean-Shift [22]. Felzenszwalb and Huttenlocher is typically used in high recall settings to create an oversegmentation of superpixels. Mean Shift and Normalized Cuts provide better precision, but often produce artifacts by breaking large uniform regions (e.g. sky) into chunks.In this paper, we use a hierarchical segmentation method proposed by van de Sande et al. [23]. Firstly, the image is over-segmented into several initial regions. Starting from the initial regions, a greedy algorithm is used to iteratively group the two most similar regions together and calculates the similarities between this new region and its neighbors. The process stops when the whole image becomes a single region. The method uses size and appearance features to measure the similarity, encouraging small regions to merge early and preventing a single region from gobbling up all others one by one.Images are segmented based on the RGB color space as well as opponent color space, normalized rgb color space, and the hue channel, generating different hierarchical segmentations for each color space. The hierarchical segmentation method uses a graph-based approach to generate the initial segments. Parameter k denotes the minimum size of the segments. Two initial segmentations are used fork=100,200. For each image, we generate eight hierarchical segmentation trees of about 15,000 regions. Then, images are corrected with the given light source colors, transferring to the ones that are taken under the white illuminant. The mean color of these regions is computed on the corrected images and normalized to [0, 1] in RGB color space. The posterior probability for the illuminant using mean region color information is given by(11)p(l|Yregion)∝|L−1|np(Xregion=L−1Yregion)p(l),Similar to the low-level features, we learn the empirical distributionsp(Xregion)for region values by computing the histogram with the same size.Finally, high-level image features are considered. Concepts will enforce different priors on the set of possible illuminants. Moreover, the empirical distribution will differ for various concepts. Bianco et al. [13] show that the statistics of different concepts will result in different preferred methods, e.g. the Shades of Grey method will generally result in a better performance when applied to indoor images, while the second-order Grey-Edge is more suited for outdoor images [13]. Lu et al. [12] show that different 3D geometry models (stages) prefer different color constancy methods. As opposed to previous methods, our approach uses high level visual features to model the priors on the set of possible illuminants and the empirical distributions of different concepts.For indoor and outdoor images, the images are usually different and they are usually taken under different light sources. Furthermore, the empirical distribution of indoor and outdoor concepts is different [17]. In our algorithm, for each category of concepts (indoor and outdoor), an empirical distribution is modeled based on low-level and medium-level cues. The distributions of different illuminant conditions are also learned.Given the concept in an image, the posterior probability for the illuminant is defined as follows:(12)p(l|Yconcept)∝p(P1…N|l;C)p(l;C)p(P1…N;C)=p(l;C)p(P1…N;C)∏i=1Np(Pi|l;C)=p(l;C)p(P1…N;C)∏i=1N[p(l|Pi;C)p(Pi;C)p(l;C)]∝∏i=1Np(l|Pi;C)p(l;C)N−1where P1 …Nrepresent the image cues that may contain empirical distribution information for color constancy, and C represents different concept categories (e.g. indoor and outdoor scenes). Pirepresents low-level and mid-level features: Ppixel, P1-jet, P2-jet and Pregion. For different low-level and mid-level features, we assume that they are conditionally independent and contain information about the illuminant distribution. We exploit the posterior probabilityp(l|Pi;C)for the illuminant using different low-level and mid-level features for different concepts. The indoor and outdoor images are usually taken under different illuminant conditions. Therefore, we also learn the illuminant distribution p(l; C) for different concepts.Besides classifying images as indoor and outdoor, we also consider the 3D geometry (stages) of images. For each category of stages, the stage information is manually labeled. The empirical distribution and the illuminant conditions are modeled in the same way for indoor and outdoor image categories.

@&#CONCLUSIONS@&#
