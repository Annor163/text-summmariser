@&#MAIN-TITLE@&#
Scene parsing using graph matching on street-view data

@&#HIGHLIGHTS@&#
An effective scene parsing framework via graph matching guidance on street-level data is proposed.Graph matching is introduced to partially match image components taking into account the regional similarity of scenes.The proposed algorithm can be applied to small training and testing sets, and achieves competitive parsing performance.

@&#KEYPHRASES@&#
Scene parsing,Graph matching,Markov random field,Street view,

@&#ABSTRACT@&#
Scene parsing, using both images and range data, is one of the key problems in computer vision and robotics. In this paper, a street scene parsing scheme that takes advantages of images from perspective cameras and range data from LiDAR is presented. First, pre-processing on the image set is performed and the corresponding point cloud is segmented according to semantics and transformed into an image pose. A graph matching approach is introduced into our parsing framework, in order to identify similar sub-regions from training and test images in terms of both local appearance and spatial structure. By using the sub-graphs inherited from training images, as well as the cues obtained from point clouds, this approach can effectively interpret the street scene via a guided MRF inference. Experimental results show a promising performance of our approach.

@&#INTRODUCTION@&#
Scene parsing, which is the segmentation and classification of regions in an image with different semantics, is of great importance in the computer vision community. For decades, various approaches have been developed to parse scenes on image sets, so that algorithms can learn and infer results from the significant amount of information provided by images. However, there are difficulties when only images are used. First, it is difficult to train a sufficiently effective classifier to label the regions due to the diversity of the categories. Second, shadows heavily influence the labeling in most of the images, such that regions on two sides of the shadow edge are labeled with different categories although they represent the same object. In contrast, point clouds can provide extra cues that images cannot convey. For example, given depth and height measurement, the 3D shape of an object can be estimated precisely; shadows will never affect the segmentation of the point clouds. Methods integrating both images and LiDAR data have been explored over the past few years. In this paper, we focus our attention on the urban street view with data from both images and point clouds collected at street level; the data were provided by Google.Many efforts have been made to accurately parse images into a variety of categories. These methods are mostly based on the 2D global or local features. Liu et al. [1] proposed a scene parsing framework based on dense image alignment over dense scale-invariant feature transform (SIFT) images, which has been a successful technique and performs very well. However, this method works on a pixel-wise level and the belief propagation optimization over the graph, with respect to pixels, has a high computational complexity. In addition, a large image database is needed [2], which makes this method difficult to use in practice. Farabet et al. [3,4] employed a multi-scale convolutional network to compute dense feature vectors centered on each pixel. After a max-pooling stage, a classifier is trained to estimate the histograms of all object categories. Another method is a scene segmentation approach that matches object boundaries or edges across scenes [5], which does not need to extract features from images explicitly. Similarly, Russell et al. [6] also proposed a scene segmentation method by matching image composites across scenes. Thus different visual clues can be collected to help enhance the parsing accuracy. Compared to the algorithms that work on pixel level, on the other hand, some attempts over superpixels have been made [7–9]. These methods are based on the fact that representation over superpixel can reduce the computational complexity remarkably, and also can naturally form an aggregate of pixels with a similar local appearance. Since the performance of the superpixel segmentation significantly influences the parsing results, state-of-the-art segmentation approaches [10,11] are always employed. The approach proposed by Tighe and Lazebnik [7] is a typical non-parametric parsing scheme over superpixels using the segmentation method in [11], in which a training stage is not necessary.More accurate parsing performance can be achieved by integrating images and 3D information than when images alone are used. Some approaches [12–14] use structure-from-motion method [15], which allows 3D information in different scenes to be effectively estimated from image sequences (e.g., stereo, video). Especially, Xiao and Quan [14] developed a method in which a refined dense depth map was computed, providing extra information such as surface normal, planarity, height and distance to camera path. However, methods in this category heavily rely on the estimation accuracy of the 3D information and have a high computational complexity. An alternative to estimate the 3D information is to gather data using LiDAR sensors [16–19]. Zhao et al. [16,17] used Velodyne LiDAR sensors and cameras mounted on vehicles to capture images and LiDAR data. They detected a large range of objects as “obstacle” (e.g., car, tree, pedestrian and any other objects limited in a bounding box); Fuzzy logic inference was employed to classify them in various categories. Instead of using 3D information, Ardeshir et al. [20] proposed a method to conduct scene understanding with the help of location and address. Wang et al. [21] introduced a holistic scene understanding framework by integrating object detection, pose estimation, depth reconstruction and semantic segmentation. Since the main objects in street scenes are buildings, it is anticipated that the parsing scheme can also work on building facades. Unlike regular image parsing tasks with several labels, facade parsing aims at identification of targets with common shape and symmetry. Some researchers propose to implement facade parsing by using only images [22,23]. Other methods [24,25] try to interpret building facades using range data.Almost all the aforementioned methods use the information in pixel or superpixel, either to train a classifier or to minimize the energy function. The information in pixel or superpixel can be considered as lower level vision features, while these features include SIFT, HOG or Color Histogram, which tend to be isolated without spatial relationship. However, in pictures of real world, pixels or superpixels appear in a more organized structure which can be referred to as higher level features, namely “objects”. One object always includes a collection of typical features and their spatial relationship, and this observation inspires many computer vision applications by representing an object using a graph (i.e., undirected graph with attributes on both nodes and edges). In street view images, similar objects appear repeatedly across scenes, such as buildings, cars and pedestrians. One can anticipate that matching similar objects in terms of structured graphs regardless of locations where they appear in the pictures, will make the parsing more effective and reliable. Compared with the methods that employ information at pixel or superpixel level, matching at object level can be considered as employing “higher” level visual cues from images. Moreover, identifying similar objects is a more natural way as how human beings recognize a scene. To the best of our knowledge, extensive research has not been conducted on scene parsing based on graph matching, though this has been shown to be successful in object matching, recognition and retrieval [26–29]. A similar method was proposed by Gould and Zhang [30], in which a correspondence graph is established via patch similarity to achieve the annotation transferring. Our work is inspired by [6] which collects low-level visual features across scenes.A parsing algorithm via Markov Random Field (MRF) optimization on a guided graph derived from a graph matching of superpixels is proposed. In this scheme, the structure of the guided graph has advantages of both data fidelity and inherited sub-graphs from training images. The following contributions are made in this paper:1.A graph matching scheme [31] is employed to interpret the semantic structure of the query scene with respect to the guidance scene. Taking into account the fact that similar images share a similar semantic structure, this step involves the calculation of sub-graphs in the query scene, which correspond to matched structures in the training scenes, providing more plausible evidence for the labeling stage.Apart from normal MRF optimization, this approach takes both the query scene itself and the structures inherited from training scenes as input; consequently, the optimization is implemented on an enhanced graph structure where the label distribution is constrained and the inherited adjacency relationship is considered.The rest of the paper is organized as follows. In Section 2, we introduce and elaborate on the proposed approach. Specifically, details how to find correspondence via graph matching and how to use the matching results as guidance to infer the labels of the query images are presented in in Sections 2.3 and 2.4, respectively. In Section 3, we present the experiments that was conducted on datasets from several cities with different street view styles. Experimental results show that approach achieves the promising performance. The conclusion and future research are presented in Section 4.

@&#CONCLUSIONS@&#
