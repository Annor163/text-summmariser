@&#MAIN-TITLE@&#
Hierarchical temporal graphical model for head pose estimation and subsequent attribute classification in real-world videos

@&#HIGHLIGHTS@&#
A hierarchical temporal model is used to estimate head pose in real-world videos.Head pose classification in (un)constrained databases shows superior performance.Proposed model is used to classify facial traits in real-world videos.Trait classification with and without using the estimated pose angle is performed.Facial trait classification using the proposed model show superior performance.

@&#KEYPHRASES@&#
Face,Attribute,Pose,Probabilistic,Real-world,Unconstrained,Video,Hierarchical,Graphical,Temporal,

@&#ABSTRACT@&#
Recently, head pose estimation in real-world environments has been receiving attention in the computer vision community due to its applicability to a wide range of contexts. However, this task still remains as an open problem because of the challenges presented by real-world environments. The focus of most of the approaches to this problem has been on estimation from single images or video frames, without leveraging the temporal information available in the entire video sequence. Other approaches frame the problem in terms of classification into a set of very coarse pose bins. In this paper, we propose a hierarchical graphical model that probabilistically estimates continuous head pose angles from real-world videos, by leveraging the temporal pose information over frames. The proposed graphical model is a general framework, which is able to use any type of feature and can be adapted to any facial classification task. Furthermore, the framework outputs the entire pose distribution for a given video frame. This permits robust temporal probabilistic fusion of pose information over the video sequence, and also probabilistically embedding the head pose information into other inference tasks. Experiments on large, real-world video sequences reveal that our approach significantly outperforms alternative state-of-the-art pose estimation methods. The proposed framework is also evaluated on gender and facial hair estimation. By incorporating pose information into the proposed hierarchical temporal graphical mode, superior results are achieved for attribute classification tasks.

@&#INTRODUCTION@&#
The amount of real-world video sequences has increased substantially as the cost of the cameras has decreased in recent years, leading to a considerable increase in the range of possible real-world video applications, including video indexing, human tracking, face recognition and verification, social networking and human computer interaction. Despite the huge literature on optimizing and automating such applications, due to the challenges presented by real-world environments (see Fig. 1), it is still hard to develop these applications. Recently there has been a lot of effort in the computer vision community, to further improve these systems in the context of real-world scenarios [1–22]. Some of this effort is to make face recognition/verification, facial attribute classification and human computer interaction to benefit from head pose estimates as prior information in order to boost their performance [1,23–34]. For instance, for face verification/facial attribute classification tasks, facial fiducial points are used to estimate pose, so as to be able to map face images from the real-world environment to a common coordinate system [24,35,36,32].Head pose estimation methods based on 2D images can be divided into several main groups, namely geometric, tracking, appearance template and manifold subspace embedding methods [38]. Geometric methods[39] use the facial landmark locations to estimate the head pose from their relative configuration. Tracking methods[40–42] use the relative movement between consecutive video frames to estimate the global head movement. Appearance template methods[43,44,28] use image-based comparison techniques to match a test image to a set of training images with corresponding pose labels. [28], for instance, estimates the 3D head pose from high quality depth images using random regression forests. It achieves this through regression between depth features and continuous head pose angles. However, this approach uses every image region for the head pose estimation, thus it is not robust to real-world facial occlusion, such as from coffee cups and facial hair, which can introduce additional spurious features to the face representation. Manifold, subspace embedding methods[45,44,46–51], on the other hand, use linear and nonlinear subspace techniques to project an image onto the head pose manifold, which is learned from a training set. When such techniques are used for video frames, they implicitly model a given video sequence temporally by mapping similar frames onto nearby locations in the manifold. The highest accuracies published in the head pose literature are provided by the manifold learning methods (see for example [51]). However, most of the aforementioned approaches are developed for constrained environments, and have several stages which are not compatible with real-world unconstrained environments (e.g. no extreme head pose or major occlusion is allowed). For instance, they often assume that the entire set of facial features typical for near frontal poses is always visible. Facial features are often manually labeled in the testing data, rather than automatically extracted. Most approaches are trained and tested on images which do not exhibit wide appearance variation. The testing databases mostly contain images with solid or constant background, limited range of facial expressions, no random illumination, and limited or no facial occlusion (e.g. CMU Multi-PIE [52], FERET [53] and CAS-PEAL [54]). Finally, the current tracking methods rely on video sequences with a known initial head pose, and typically, they must be reinitialized (at times, manually) whenever the tracking fails (due to a failure in the face detection or due to occlusion). However, many of these requirements and assumptions are not practical in the context of real-world videos.Different methods have been proposed to solve the problem of head pose estimation in real-world environments, such as [27,30,33,23,55,29,32]. [27,30] treat the problem as a classification problem (assigning a face image to one of very coarsely defined pose bins), and address it in the context of single, low resolution video frames of crowded scenes under poor and/or limited (e.g. indoor) lighting. Approaches such as [23,29] use relatively high quality video frames/images and perform classification on finer pose bins. Some other approaches, on the other hand, define the pose estimation problem as a continuous pose angle estimation task [55,32,33]. Refs. [55,56] shows that when facial landmarks are located on the face, they can be successfully used to estimate head pose. However, such approaches have the vulnerability that it is difficult to extract such features when a major facial occlusion is present (see Fig. 2) or when the pose angle is more than 45°, leading to occlusion of facial landmark regions (e.g. eyes) in the image. Finally, most of the aforementioned approaches either focus on only a specific set of features (e.g. facial landmark points) to represent faces, or do not leverage the temporal pose information available between consecutive video frames.The problem addressed in this paper is the automatic head pose (yaw angle) estimation in real-world videos that are affected by the joint occurrence of arbitrary face scales, extreme head poses, non-uniform illumination conditions, partial occlusions, motion blur, background clutter, wide variability in image quality, and subject variability (see Figs. 1 and 2). We propose a novel hierarchical, temporal graphical model, which uses a number of complementary robust local invariant facial features, and leverages the dependencies between consecutive video frames, in order to substantially improve head pose estimation in real world scenarios. Furthermore, at each frame, the system assesses the probability density function over the pose angles, ranging from −90° to +90° (Fig. 2). The proposed hierarchical and temporal graphical model (Figs. 4 and 3) uses spatial codebook representations inferred from different local features, which have a high degree of invariance to various transforms, such as changes in scale, viewpoint, rotation and translation. The local feature detectors and descriptors are chosen such that they extract complementary information from the tracked face image (see Fig. 2): (i) facial edge points inferred from eyebrows, mouth etc. (Geometric Blur features [57]) (ii) facial anatomical regions extracted from eyes, forehead, cheeks etc. (Boundary Preserving Local Region features [58]) (iii) densely sampled patch-based features over the whole face image (SIFT and CSIFT [59,60]), and (iv) facial landmarks [55,56]. Some of the reasons for employing multiple features include: (i) to ensure that if one type of feature is not detected from a face, the other(s) can compensate for it, in order to robustly estimate head pose, and (ii) to complement each other. To calculate the pose distribution for each feature type, the codebook statistics are employed. These statistics are used in a graphical model to estimate the single video frame pose probability distribution. Next, the framework temporally models these head pose probabilities over a video sequence. Finally, the method performs non-parametric density estimation to obtain continuous head pose probabilities.The proposed hierarchical temporal graphical model is a general framework, which can use any type of feature and can be adapted to any face related inference task while providing the estimated pose distribution as a prior knowledge. To demonstrate this, we show how one can adapt the proposed graphical model for a different facial classification task: facial attribute (soft biometric trait) estimation. Facial attributes are the contextual information which describe a subject, such as gender, age, skin color, hair color, facial hair and eye color. The crucial difference between the pose estimation and attribute estimation task is that pose is a dynamic class which changes over a video sequence whereas facial attribute is a static variable which does not change throughout a video sequence. The framework is adapted for each domain and pose is added as a feature to the system. The hypothesis is that the addition of pose information will improve the classification results.We provide qualitative and quantitative results of the two set of experiments performed on 18,000 video frames from the McGill Real-world Face Video Database [37] (see Fig. 1), which is a publicly available dataset. This database provides the original frame, detected, aligned and masked face image, and labels for different facial classes (gender, head pose (yaw) and facial hair) over real-world videos recorded from subjects (completely free in movement) in natural scenes. Each video is collected under different illumination and background conditions, and the subject can exhibit a full range of head poses (in yaw, pitch and roll) and arbitrary facial occlusions. A comparison of some of the leading publicly available face databases is provided in Table 1in order to justify our choice. The first set of experiments aims to evaluate pose estimation using the proposed hierarchical temporal graphical model, and the second set of experiments assesses the adaptation of the proposed hierarchical temporal graphical model to the attribute classification task, which leverages the estimated pose distribution. Our results are compared to the leading alternative approaches [23,55,51,33,32] in the pose estimation and in facial attribute (trait) classification [24,55,70]. Previous work on facial trait, e.g. gender, estimation, includes [21] which first uses mutual information for feature selection, and then fuses the selected features to perform gender classification from “single face images”. For a recent survey of the field, please refer to [22]. We also evaluate the proposed pose estimation methodology on a constrained database, i.e. the CMU Multi-PIE face database [52], which contains face images from different subjects from 13 viewpoints, different facial expressions and illumination conditions. Although the proposed method is applicable to any facial trait, we test the proposed graphical model on facial classes that have recently received attention, and which are relatively challenging: gender, and the presence of facial hair. Gender is a global trait, to which informative features are spread over the face, whereas facial hair is a more local trait and only particular regions of the face give evidence. The experimental results show that the proposed graphical model outperforms the alternative approaches in both the head pose and facial attribute estimation tasks.The rest of the paper is organized as follows: Section 2 introduces the proposed hierarchical temporal probabilistic model for the task of pose estimation. Section 3 shows how to adapt the proposed model for the attribute classification problem while taking into account the pose distribution. Section 4 explains some of the methodology and implementation details. Section 5 explains the experimental results, with the summary and discussion presented in Section 6.We now describe the proposed hierarchical probabilistic model (Fig. 3). In Level 1 the framework models the relationship between the statistics learned from different local invariant features on the detected face, and their corresponding head pose distributions. In Level 2, each frame pose distribution is inferred based on each feature-based head pose estimates inferred at Level 1. Finally, at the highest level (Fig. 4), the model leverages the temporal information to estimate the most likely head pose configuration over the entire video sequence. The proposed graphical model is undirected. Eqs. (11)–(14) present the expression of the edge potentials (derived from joint distributions).As the first step, the automatic face localization/tracking framework in [37] is utilized over each video sequence to obtain a set of located and tracked face image,Xintt,t∈1,2,…,T, where T is the maximum number of frames with successfully tracked faces. Note that while the framework in [37] is employed as it is shown to be robust to challenges in the real-world, other methods can also be employed to track and locate faces in videos. The ultimate goal here is to estimate the set of pose probability density functions throughout the video,{Y∼1,Y∼2,…,Y∼t,…,Y∼T}, given the single frame based pose distributions{Y1,Y2,…,Yt,…,YT}, whereY∼t={ϕ1,ϕ2,…,ϕM}is the set of head pose angles for each video frame. Given the graphical model in Fig. 4, the pose posterior distributionp(Y∼t|Yt-1,Yt,Yt+1)is estimated via:(1)p(Y∼t|Yt-1,Yt,Yt+1)=1Z(Yt-1,Yt,Yt+1)exp-U1,where Z is the normalization function, which is constant with respect toY∼t. The energy functionU1is defined as:U1=γ1ϱ(Y∼t,Yt-1)+γ2ϱ(Y∼t,Yt)+γ3ϱ(Y∼t,Yt+1), where the weights{γ1,γ2,γ3}are learned from the training data. The potential functionϱmodels the unary cliques defined between pose distributions estimated at framet,t-1andt+1, and the t-th frame temporal pose distributionY∼t.ϱis defined by the corresponding probability distribution functions:ϱ(Y∼i,Yj)=-logp(Y∼i|Yj)p(Yj), wherei=tandj∈{t-1,t,t+1}. The estimation of the potential function requires the computation of the posterior distributionp(Y∼i|Yj). This be achieved with using any probabilistic classifier, such as Bayesian, Relevance Vector Machine (RVM) or Random Forests (RF) [71]. However, one must first estimate the random variableYj∈{t-1,t,t+1}, which is achieved at the Level 2 of the proposed hierarchical graphical model. At Level 2,Ytis inferred through the estimation of the posterior distributionp(Yt|ypatcht,yedget,yregiont,ylandmarkt), where{ypatcht,yedget,yregiont,ylandmarkt}are the patch, edge, region based and facial landmark based pose distributions. This is done by the proposed fully connected graph in Level 2. The parameterization and learning of this graph are explained in the following subsections (see Fig. 3).Before Levels 1 and 2 of the hierarchical model are explained in detail, the terminology used in the proposed model is first defined.Xinttis the set of intensity values in RGB space at time t for the pixels in the image of the tracked face:Xintt=xi,jt|∀i∈1,…,R,∀j∈1,…,C, where the image has sizeR×Candxi,jtis the intensity value of an individual pixel at location(i,j). Given the challenges presented by real-world environments, local invariant features are used due to their high degree of robustness to various transforms, such as changes in scale, viewpoint, rotation and translation. Extensive analysis of a number of different local invariant features (e.g. [72,73]) shows that using both densely and sparsely detected features, and representing these features with complementary descriptors is beneficial for classification/detection tasks.The collection of the different feature representations (see Fig. 2) inferred from the tracked faceXinttis denoted byXt=Xpatcht,Xedget,Xregiont,Xlandmarkt, whereXpatchtis the densely sampled patch representation,Xedgetis the sparsely sampled edge representation,Xregiontis the dense region, andXlandmarktis the facial landmark representation. Here,Xpatchtis the collection of image patches extracted fromXintt:(2)Xpatcht=xpt|∀p∈1,…,P,where P is the total number of patches andxptdenotes a single patch with index p, which contains two pieces of information: (1) the set of pixels in the patch,xi,jtp, and (2) the location of the patch center,(rp,cp):(3)xpt=xi,jtp,rp,cp|xi,jtp⊂Xintt,rp∈1,…,R,cp∈1,…,C.Xedgetdenotes the collection of distinct points lying on the edge map inferred fromXintt:(4)Xedget=xet|∀e∈1,…,E,where E is the total number of detected edge points, andxetis a single edge point with edge index e, which contains two pieces of information: (1) the set of pixelsxi,jtethat describes the e-th distinct edge point, and (2) location of the distinct edge point(re,ce):(5)xet=(xi,jte,re,ce)|xi,jte⊂Xintt,re∈1,…,R,ce∈1,…,C.Xregiontdenotes the collection of facial regions extracted fromXintt:(6)Xregiont=xrt|∀r∈1,…,R,whereRis the total number of facial regions extracted fromXintt, andxrtis the rth single face region, which includes three pieces of information: (1) its set of pixelsxi,jtr, (2) the location of the region center(rr,cr), and (3) the region scale (size)sr:(7)xrt=xi,jtr,rr,cr,sr|xi,jtr⊂Xintt,rr∈1,…,R,cr∈1,…,C,sr∈Z+.Xlandmarktdenotes the collection of facial landmarks extracted fromXintt:(8)Xlandmarkt=xflt|∀l∈1,…,L,where L is the total number of facial landmarks extracted fromXintt, andxfltis theflth single facial landmark, which contains the facial landmark location(rfl,cfl).To model each of these representations, one can use different features. Here, the features chosen are: (i) densely sampled “SIFT” [59] and “Color SIFT (CSIFT)” [60] features for modeling the face image patches, (ii) sparsely sampled “Geometric Blur (GB)” [57] features for modeling the distinct facial edge points, (iii) “Boundary Preserving Local Region (BPLR)” [58], and (iv) facial landmark [55,56] features for modeling the facial anatomical regions.For the landmark features, we use the location information directly. For the remaining features, rather than using the pixel intensities directly for each feature type, the corresponding descriptor d is extracted from each feature point’s intensity representation, such asxi,jtp,xi,jteorxi,jtr. For the patch representation, SIFT and CSIFT descriptors are used,dsift,p=ktanddcsift,p=kt. In the case of edge features, the GB descriptordGB,e=ltis chosen. Pyramids of Histograms of Oriented Gradients are used as the region descriptor, i.e.dPHOG,r=mt. A visual vocabulary (codebook) is learned for each feature type, using the corresponding feature descriptors and an appropriate mapping function which takes the extracted feature’s location information into account. Next, each extracted feature is represented by a visual word (codeword), from which codeword statistics will be learned. Occurrence statistics, for example, model how likely it is to observe a codeword for a pose value of interest whereas co-occurrence statistics measure which codewords are likely to occur together for a given class value. These statistics will be later used in the potential functions. Note that in the rest of the formulation, instead of the pixel intensity values, the corresponding descriptors d are used inxp=kt,xe=ltandxr=mt.We now define each of the facial feature types used in the proposed model:The Patch-based Representation:The patch representation achieves a dense sampling of the given facial image. To model each image patch, SIFT and CSIFT vocabularies are used. This choice is motivated by the observed performance increase when CSIFT is combined with SIFT, as in [74]. To map the k-th image patchxp=ktto a visual wordfsift,p=ktlearned using the SIFT and CSIFT descriptors, a mapping function g is used such thatg:xp=kt→fsift,p=ktandg:xp=kt→fcsift,p=kt(that isXpatcht→Fpatcht). Rather than using the patch index p as the spatial information [37], here we use the patch location(rp,cp)in the coding and pooling phases to leverage the spatial information. By adding two more dimensions to the descriptor space, this permits modeling the spatial inter-patch relationship. Because faces are aligned in the preprocessing step, this mapping provides better modeling for the face vocabulary.The Edge-based Representation:To detect the key facial edge points and to calculate the corresponding descriptor around each edge point, the Geometric Blur (GB) framework ([75,57]) is used. Geometric blur is shown to be effective when applied to sparse edge points. Thus, first the oriented edge filter responses are extracted from face images. Then, rejection sampling over the edge map is used to obtain sparse interest points along edges. Once these interest points are detected, GB descriptors are calculated around each point [57]. Unlike uniform Gaussian blur-based descriptors, GB models the amount of blur proportional to the distance from the corresponding point. Here the motivation is that the distortion due to the affine transformations should be modeled properly: the amount of blur varies linearly with distance from corresponding points. To map the l-th distinct edge pointxe=ltto a visual wordfGB,e=ltlearned using GB descriptors, we use the mapping functiong:xe=lt→fGB,e=lt(that isXedget→Fedget). Similar to the facial patch occurrence model, the location information(re,ce)is used in the coding and pooling steps.The Region-based Representation:Anatomical regions, such as the mouth, the eyes, the ears and the eyebrows, can provide some pose information. Local region detectors and descriptors are used to model such anatomical regions. To achieve this, boundary-preserving local regions (BPLRs) [58]xr=mtare computed. BPLRs are densely sampled local regions obtained from a given face image, and they preserve the shape of the facial structure on which they are detected. The BPLR detection is achieved based on the following steps [58]. The algorithm first obtains multiple overlapping segmentations from a given face image, for which the corresponding distance transform maps are computed. Then, it divides each segment into regular grid cells, and samples an element feature in each cell. The cell position and scale information is determined by the maximal distance transform value in the cell. Then, it links all elements using a minimum spanning tree, which extends connections and integrates multiple segmentations. Finally, it outputs a set of overlapping regions which contains a group of linked elements within the tree, namely BPLRs. Once the BPLRs are extracted, Pyramids of Histograms of Oriented Gradients (PHOG) descriptors are computed over the gPb (globalized probability of boundary)-edge map for each detected BPLR. The mapping functiong:xr=mt→fPHOG,r=mtnot only uses the spatial information in the coding and pooling steps, but also the scale (size) information for each extracted BPLR.The Facial Landmark-based Representation:The facial landmarks by Xiong and De la Torre [55,56] have been shown to successfully estimate head pose. Thus, if they are detected in a video frame, we incorporate the facial landmarks location information into our graphical model as another feature type.As Eq. 1 shows, inferring the video based pose distributionY∼trequires the estimation of single frame-based pose distributionYt. Level 2 provides the inferredYts to Level 3 through the estimation the posterior distributionp(Yt|ypatcht,yedget,yregiont,ylandmarkt)for the face image in t-th video frame.{ypatcht,yedget,yregiont,ylandmarkt}are the patch, edge, region based and facial landmark based pose distributions. This is done by learning different combinations of patch, edge, region and landmark classifier pose distributions. To infer the posterior probabilityp(Yt|ypatcht,yedget,yregiont,ylandmarkt), given the hierarchical model in Fig. 3, the following expression is derived:(9)p(Yt|ypatcht,yedget,yregiont,ylandmarkt)=1Z(ypatcht,yedget,yregiont,ylandmarkt)exp-U2,whereas beforeZdenotes the normalization function and the energy functionU2is defined as:(10)U2=β1ν1(Yt,ypatcht)+β2ν1(Yt,yedget)+β3ν1(Yt,yregiont)+β4ν1(Yt,ylandmarkt)+β5ν2(Yt,ypatcht,yedget)+β6ν2(Yt,ypatcht,yregiont)+β7ν2(Yt,yedget,yregiont)+⋯+β11ν3(Yt,ypatcht,yedget,yregiont)+⋯+β15ν4(Yt,ypatcht,yedget,yregiont,ylandmarkt).β1,…,β15are the weights for each possible clique and potential function combinations (of the fully connected graph), which are learned using the optimization strategy in [76], namely the de-randomized evolution strategy with covariance matrix adaptation. The potential functionνmodels the possible cliques of t-th frame pose distribution from three different feature representations, such as unary, pairwise, triplet and fourth order cliques. In the interests of maintaining clarity we do not show all 15 potential functions here, we show only a subset.νis defined by the corresponding probability distribution functions:(11)ν1(Yt,ypatcht)=-logp(Yt|ypatcht)p(ypatcht),(12)ν2(Yt,ypatcht,yedget)=-logp(Yt|ypatcht,yedget)p(ypatcht,yedget),(13)ν3(Yt,ypatcht,yedget,yregiont)=-logp(Yt|ypatcht,yedget,yregiont)p(ypatcht,yedget,yregiont),(14)ν4(Yt,ypatcht,yedget,yregiont,ylandmarkt)=-logp(Yt|ypatcht,yedget,yregiont,ylandmarkt)p(ypatcht,yedget,yregiont,ylandmarkt).ypatcht,yedget,yregiont,ylandmarktare the pose distributions, which are inferred through Section 2.3. The estimation of the joint probabilityp(Yt,ypatcht)is achieved using the training database:p(Yt,ypatcht)∝k(Yt,ypatcht)+dt, wherek(Yt,ypatcht)is the count of the joint occurrence event(Yt,ypatcht), anddtis the Dirichlet regularization parameter required to compensate for the sparsity. Because a uniform prior is assumed,dtis constant for all t. Note that probabilities in other cliques are calculated in a similar fashion. The posterior probabilities, such asp(Yt|ypatcht),p(Yt|ypatcht,yedget),p(Yt|ypatcht,yedget,yregiont)andp(Yt|ypatcht,yedget,yregiont,ylandmarkt)on the other hand, are calculated using Random Forests (RF) [71]. Once the Level 2 probabilitiesY1,Y2,⋯,Yt,⋯,YTare estimated,Y∼tcan be calculated using Eq. 1. Note thatYtandY∼tare real-valued vectors representing the entire head pose distribution. Next, the entire pose density is estimated for 1° intervals in the range[-90°,+90°]overY∼t. Gaussian kernel-based non-parametric model fitting is used since the initial pose densities do not follow any known parametric distribution.To inferypatcht,yedget,yregiont,ylandmarkt, we need to model the posterior distributionsp(ypatcht|Xpatcht),p(yedget|Xedget),p(yregiont|Xregiont)andp(ylandmarkt|Xlandmarkt). The posterior distribution for the patch-based features is given by:(15)p(ypatcht|Xpatcht)=1Z(Xpatcht)exp-Upatch,whereZis the normalization function and, given the proposed graphical model, the energy function U is defined as:(16)Upatch=λ1φ1,sift(ypatcht,Xpatcht)+λ2φ1,csift(ypatcht,Xpatcht)+λ3φ2,sift(ypatcht,S(Xpatcht))+λ4φ2,csift(ypatcht,S(Xpatcht)),where the potentialφ1models the occurrence relationship between patch features (e.g., SIFT or CSIFT) and the t-th frame patch-based pose distributionypatcht. The pairwise potentialφ2, on the other hand, models the co-occurrence relationship between pairs of patch features andypatcht. The weightsλ1,λ2,λ3,λ4are learned from the training data using 2-fold cross validation. Note that these weights are learned locally.For edge-based features, the posterior distribution is defined as:(17)p(yedget|Xedget)=1Z(Xedget)exp-Uedge,whereZis the normalization function and the energy functionUedgeis defined as:(18)Uedge=ζ(yedget,Xedget),whereζis the edge related potential function, which models the relationship between edge features and the t-th frame edge-based pose distributionThe region and landmark posterior distributions, i.e.p(yregiont|Xregiont)andp(ylandmarkt|Xlandmarkt), are modeled via the potential functions ofΦ1(Yregiont,Xregiont)andω(Ylandmarkt,Xlandmarkt), in a similar fashion to the edge-based features.The following expressions are used to model the occurrence potential function for SIFT and CSIFT based vocabulary (recall that using the codebook mappingg:Xpatcht→Fpatchtfor SIFT and CSIFT separately):(19)φ1,sift(ypatcht,Xpatcht)=-logp(ypatcht|Fpatcht)p(Fpatcht),and(20)φ1,csift(ypatcht,Xpatcht)=-logp(ypatcht|Fpatcht)p(Fpatcht),where uniform priors are assumed. Any classifier can be used to model the posterior probabilitiesp(ypatcht|Fpatcht)for SIFT and CSIFT. In this work, we choose to use a Random Forest (RF) [71] to perform inference. A RF is a discriminative classier that consists of an ensemble of decision tree classifiers, where the final classification is determined by summing the votes cast by each individual tree. Due to the random selection of a subset of training data and features, RF is less prone to overfitting than traditional single decision trees. Also the RF classifier is computationally efficient and provides probabilistic outputs.Co-occurrence statistics of densely sampled patch-based visual words also provide discriminative information about the pose class. Thus, the local co-occurrence statistics are extracted from a neighborhood regionN(xp=kt)defined around each image patchxp=ktinXpatcht, namelyS(Xpatcht). Similar to the occurrence statistics, co-occurrence statistics are defined over the visual vocabulary space:S(Fpatcht). The following expressions are proposed to model the “patch co-occurrence” potential function:(21)φ2,sift(ypatcht,S(Xpatcht))=-logp(ypatcht|S(Fpatcht))p(S(Fpatcht)),Note thatφ2,csiftis calculated similarly.The following expressions model the facial edge potential function using the mapping defined earlier, i.e.g:Xedget→Fedget:(22)ζyedget,Xedget=-logpyedget|FedgetpFedget,wherep(Fedget)is assumed to be uniform, and the posterior probabilityp(yedget|Fedget)is also estimated using a Random Forest classifier.Modeling the potential functionsΦ1yregiont,Xregiontandωylandmarkt,Xlandmarktis achieved by using a similar method to estimate the edge-based potential functions. To estimate the posterior probabilitiesp(yregiont|Fregiont)andp(ylandmarkt|Flandmarkt), a Random Forest classifier is also used.During the training, the model is provided withYt∼,Yt,yedget,ypatcht,yregiont,ylandmarkt, the given head pose ground truth distribution. During training, the weightsγi∈{1,2,3},λi∈{1,…,4},βi∈{1,…,15}, and the conditional, joint and prior probabilities used to calculate the potential functions are learned. When a test video is given, first the occurrence and co-occurrence statistics are computed from different feature representations to be able to estimateypatcht,yedget,yregiont,ylandmarkt. Since the weights are learned and theytare already estimated, the head pose distribution for each video frameYtcan also be estimated. Then, for each possible value ofYt∼, the pose distributionp(Y∼t|Yt-1,Yt,Yt+1)is estimated.In Section 2, we show how to estimate pose distributionY∼tfor a video frame at time t via the proposed hierarchical temporal graphical model. Now, we are interested in adapting the proposed model to the facial attribute (trait) classification (e.g. gender, facial hair) task while leveraging the estimated pose distributionY∼t. Fig. 5shows all the levels of the adapted graphical model. Levels 1, 2 and 3 are similar to the model explained in Section 2. Compared to the hierarchical model explained in Section 2, there are two additional levels: Level 4 and Level 5. Level 4 is defined as the fully connected graph between (i) the single frame based attribute distributionGtat the frame t, and (ii) the temporal pose distributionY∼testimated by the proposed model in Section 2, the patch-based, edge-based, region-based and landmark-based facial attribute distributions, defined asgpatcht,gedget,gregiont, andglandmarkt. Level 4 probabilities are estimated in a similar fashion to Section 2.2. However, in this fully connected graph there are 31 weights and potential functions to learn from the training data. Level 5, on the other hand, models the temporal aspect of the facial trait. The goal here is to infer the most probable facial attribute valuezˆgiven the attribute distributions estimated from the entire video sequence. To this end, we define a classifier which computes the posterior probability of the attribute (trait) class Z given the inferred attribute distributions from all framesG={G1,G2,…,Gt,…,GT}:(23)zˆ=maxz∈Zlogp(z|{G1,G2,…,Gt,…,GT})p(z‾|{G1,G2,…,Gt,…,GT}),wherezˆis the binary classification decision defined over the classifierp(z|G),zandzˆare opposing attribute values (e.g., in the case of gender,z=maleandzˆ=female). Alternatively, using the conditional independence structure shown in the proposed graphical model (see Fig. 5), Eq. (23) can be written as(24)zˆ=maxz∈Zlogp(G1|z)p(G2|z)⋯p(Gt|z)⋯p(GT|z)p(z)p(G1|z‾)p(G2|z‾)⋯p(Gt|z‾)⋯p(GT|z‾)p(z‾),wherep(z)is the a priori probability on the class attribute value z.p(Gt|z)is the likelihood for the class attribute Z over the attribute distribution estimated at t-th frame in the video sequence. Eq. (24) states that the estimated facial attribute in each frame is conditionally independent given the video trait hypothesis z. The temporal modeling of individual frame-based class beliefs, in Eq. (24), is achieved via multiplication of the individual likelihood functionsp(Gt|z). For example, in gender classification,p(Gt|z)models the likelihood that the subject is male given the facial trait estimate at a single frame.In the context of real-world videos, it is often the case that features change rapidly from frame to frame, not just due to occlusion and fast movement, but because the conditions are changing (e.g. changes in lighting, scale, orientation). To overcome this inconsistency in the presence of features over frames, our framework models frame-to-frame feature dependencies through information passed between the nodes at the highest level (level 5). We investigated different clique and local connectivity models in level 5, including a fully connected graphical model. It is observed that the fully connected model did not improve the results and was intractably expensive. Thus, we employed the simpler, but computationally efficient and effective temporal model explained in here.The patch-based representation was obtained by setting the total number of patches P to 2304, i.e. 48×48 grid defined overXintt. Rejection sampling was done over the edge map to obtain 300 sparse interest points along edges. Each face image was represented using three types of normalized histograms in Level 1 of the proposed model: (1) densely sampled SIFT and C-SIFT based texture histogram using a codebook with size 2000 and 6000 respectively, (2) GB based shape (edge) histogram using a codebook with size 4000, and (3) BPLR based region histogram using a codebook with size 8000. Note that the codebook sizes were empirically decided using the training data as the ones which maximized the single frame based trait classification accuracy using the proposed approach. In the facial landmark based representation,L=49.The co-occurrence based patch features that were used in the potential functionsφ2,sift(ypatcht,{xp=kt,xp=jt})andφ2,csift(ypatcht,{xp=kt,xp=jt})(see Section 2.3.1) had very high dimensionality, e.g.4×106for SIFT. As such, it was very hard to track and model each co-occurrence feature due to memory and sparsity issues. Thus, we decided to reduce the co-occurrence feature space dimension while maintaining the main characteristics of the features as much as possible. A Random Forest classifier was first used to select the top 1% of the occurrence features (codewords) based on the feature importance score [71]. Then, PCA was applied over the co-occurrence features of these selected occurrence codewords. During this process, K eigenvectors with the highest eigenvalues were chosen in a fashion that at least 99% of the original feature space information was preserved.While estimating the optimal set of weights in Level 2 of the pose graphical model, and Level 2 and 3 of the attribute graphical model, different global search (and multiple start local search) algorithms were evaluated, such as simulated annealing, genetic algorithm and pattern search (direct search). Among all, the optimization strategy in [76], i.e. CMA-ES (de-randomized evolution strategy with covariance matrix adaptation), was found to provide the best and the most consistent estimation performance. The CMA-ES is an evolution strategy, which is a stochastic, derivative-free method, developed for numerical optimization tasks. CMA-ES performs well on difficult optimization problems, such as functions with discontinuities, noise, local optima or outliers. CMA-ES is based on a biological evolution strategy containing the mutation (variation) and generation (iteration) steps. Each iteration generates candidate solutions from the current individuals (a subset of candidate solutions from the previous iteration) using a stochastic approach. Some of these individuals are later selected as the parents of the next generation. This selection is defined based on the performance on the defined objective function. The iteration continues until a candidate solution is reached. During the iteration phase, the CMA-ES estimates a positive definite matrix, i.e. covariance matrix, without using gradient information. This allows CMA-ES to be feasible on multi-model, noisy or non-continuous problems. Another benefit of using CMA-ES is that it does not require any prior information, such as the distribution shape, a difficult task considering the dimensionality of the weight space, i.e. 15 and 31. For details of the CMA-ES algorithm, please see [76].

@&#CONCLUSIONS@&#
