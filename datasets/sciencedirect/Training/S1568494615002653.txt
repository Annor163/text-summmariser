@&#MAIN-TITLE@&#
Classification of the cardiotocogram data for anticipation of fetal risks using machine learning techniques

@&#HIGHLIGHTS@&#
Cardiotocography (CTG) is used as a technique of measuring fetal well-being.The classification performances of different classification methods are examined.The research showed us there is no big difference in accuracies of the classifiers.The accuracy achieved by k-NN is 98.4% (minimum) and by RF is 99.18% (maximum).

@&#KEYPHRASES@&#
Cardiotocogram,Support vector machines,Artificial neural network,Radial basis functions,Decision trees,k-Nearest neighbor and Random Forest,

@&#ABSTRACT@&#
The aim of the research is evaluating the classification performances of eight different machine-learning methods on the antepartum cardiotocography (CTG) data. The classification is necessary to predict newborn health, especially for the critical cases. Cardiotocography is used for assisting the obstetricians’ to obtain detailed information during the pregnancy as a technique of measuring fetal well-being, essentially in pregnant women having potential complications. The obstetricians describe CTG shortly as a continuous electronic record of the baby's heart rate took from the mother's abdomen. The acquired information is necessary to visualize unhealthiness of the embryo and gives an opportunity for early intervention prior to happening a permanent impairment to the embryo. The aim of the machine learning methods is by using attributes of data obtained from the uterine contraction (UC) and fetal heart rate (FHR) signals to classify as pathological or normal. The dataset contains 1831 instances with 21 attributes, examined by applying the methods. In the paper, the highest accuracy displayed as 99.2%.

@&#INTRODUCTION@&#
During pregnancy, it is not easy to acquire direct information about the fetus. Therefore, obstetricians should use indirect information related to a fetal state. Most important one is measuring the fetal heart rate (FHR) [1].The term ‘electronic fetal monitoring’ is the other alternative name of CTG, however, it is conceivable as less specific appellation since CTG contains observation about the contractions of the mother and different kind of fetal monitoring too. One type of fetal assessments in pregnancy can be the antenatal CTG and it checks the fetal heart rate regarding as a biological indicator of baby health [2]. CTG is usually applied during the last trimester it means after the 28th week of pregnancy. The idea behind the usage of CTG is a monitoring test for the diagnosis of babies having either acute or chronic fetal hypoxia and getting information about the baby under the risk of developing hypoxia [2].The accepted parameters for the fetus are stated as follows (they are all in beats per minute). Baseline fetal heart rate between 110 and 160. Baseline variance is larger than 5 [3].Nowadays, cardiotocography is the most preferable oblique, investigative method, in practical usage, to observe fetal well-being. Cardiotocogram (CTG) comprises of two different signals, its uninterrupted recording of instant fetal heart rate (FHR) and uterine activity (UC). The information, which is obtained from CTG, is used for early identification of a pathological state (i.e. inherited heart deficiency, fetal suffering or hypoxia, etc.) and may assist the obstetrician to anticipate future complications and interfere before there is a permanent harm to the fetus. During the delivery the baby who is exposed to hypoxia may cause of death or temporary disablement. Due to improper diagnosis of the FHR pattern recordings and improper treatments applied to the fetus can cause more than half of these deaths [4].While its practicality, there has been some discrepancy as to the utility and the success of CTG monitoring, particularly in low-risk pregnancies. If there is an incorrectly analyzed fetal pain, then, it may be referred to needless treatments or if there is an improper analysis of fetal welfare then it may be rejected required treatments [5].The classification uses a performance evaluation measure, but it is not enough to decide for the vital case especially in medical diagnosis. Therefore, it is also suggested another type of performance evaluation tools such as the ROC (receiver operation characteristics) [6] and F1-measure [7].In this study, it is used eight different machine-learning methods to examine the CTG data categorized as healthy or unhealthy according to the decisions of three obstetricians. This study presents a comparison of the performances of the machine learning methods using the open source software WEKA [8] in terms of accuracy, specificity, sensitivity, F-measure and ROC curve.The analysis shows the performance of the methods over the datasets from UCI [9] including CTG data with some indicative features. Three expert obstetricians decided to condition of the CTG data, whether normal or pathological checking the status of the embryo. The UCI cardiotocography data [9] was obtained by the automatic SISPORTO 2.0 [10] software. It is isolated from the suspicious entries and normal and pathologic class added to the NP feature. The Table 1gives an explanation for each property of the respective features in the data. The CTG data has 21 features, 8 of them are continuous and 13 are discrete. The classification of the data with respect to condition of the fetus either normal or pathological. After giving a short explanation for each of the algorithms which are ANN, SVM, SL, RBF, C4.5, CART and RF decision trees. WEKA classification algorithms available at [11] were used for the algorithms. Each algorithm performance was tested using 10-fold cross validation during the examination for CTG dataset that tested around 1831 entries accurately. The CTG data entered in the WEKA classification module and trained several times by varying the respective parameters of the each algorithm to maximize the classification performance. The performances of the algorithms recorded and then tabulated.Logistic regression analyzes the effect of several dynamics in a two-grouped outcome via approximating the probability of the experimental event [12]. The outcome of logistic regression is a binary event, like spam versus not spam, or malignant versus benign. The mathematical background of logistic regression is the concept logit that is described as “the natural logarithm of the odds ratio”. In the literature, you can come up against the researchers who were calling the logistic regression as the logit model or logistic model formulated as follows,(1)logp(x)1−p(x)=β0+x⋅βWhy? Because, to achieve a goal, such as predicting the dichotomous outcome, instead of using a complicated equation, it opts linear one like its cousin linear regression. Solving for p, gives us a new formula depends on the variable x(2)p(x;w,n)=eβ0+x⋅β1+eβ0+x⋅β=11+e−(β0+x⋅β)That equation includes all necessary coefficients, which are accepted as the inputs of the model. In the literature, it is encountered with two different types of usage of logistic model: Of course, to categorize the data according to its classes by calculating the ratio of probabilities success and failure in the form of the odds ratio. Secondly, to find out the relationships between the variables and how effects each other [13].The k-nearest-neighbors are an example of a classification method with the independence of the parameters. The method can be classified as by the point of implication as simple, but efficient in many datasets [14].The k-NN algorithm is defined by three terms (S, k, T) where S represents a resemblance measure which links to each pair of data in an appropriate N-dimensional space at (real or integer) number, k represents the number of nearest data that are trained to carry out the classification and T represents the vector of M training data applied by the classifier to actually carry out the classification [15].k-NN uses distance metrics usually Euclidean distance to perform the similarity measure formulated by(3)dE=∑i=1Nxi2−yi2If t is a data sample, which classification implemented and its k nearest neighbors turn up, then this makes a neighborhood of t. While using the method to classify the data sample in the neighborhood about t, the distance is whether considered or not. Of course, to use k-NN choosing the right value for k is critical; due to the rate of success of classification directly appertain to this value. The k-NN method can pass for biased by k. It is possible to choose the k-value in so many different ways. Of course, the smooth and efficient one is applying the algorithm several times changing the value of k to get the highest accuracy. To decrease the dependency level of choice of k for k-NN method, Wang [16] recommended checking multiple sets of nearest neighbors instead of one set of nearest neighbors.k-NN can be applied by using the following procedure,1.Keep all the outputs of the P nearest neighbors to examine a part r in the vector setm=m1,…,mpby applying the steps P times:a.Take the next part riin the data, where i represents the current iteration in the given set {1, …, Q}b.If r is not assigned or r<d(r, mi):r←d(r, mi), t←sic.Repeat until the last element of the data (i.e. i=Q)d.Keep r as the vector c and t as the vector m2.Compute the average output for the vector m by the formula as follows:m¯=1P∑i=1Pmi3.Assignm¯as the output value of the examination data part r.The RBFN is derived from the methods for executing full interpolation of a subpart of data points in a high dimensional feature space [17]. A usual radial function is the Gaussian that is having an arbitrary value c, is as follows(4)h(x)=e−(x−c)2r2The parameters of the function are c for the center and r for the radius.The RBFN shows similarity in network design with the typical regularization network [18]. Since the basis functions are radially symmetrical due to the stabilizer having radial symmetry, it calls the RBFN. From the point of approximation theory, the regularization network needs three properties [19] expressed as follows,•It can converge any multivariate continuous function on a compact set to a random precision, indicated an infinitely many units.The regularization network shows the best-approximation attribute since the unidentified coefficients are linear.The result calculated by the regularization network is optimum. Optimality shows that the regularization network reduces a function that measures how much the solution diverges from its actual value as denoted by the training data [20].Fig. 1illustrates a typical RBF network [21]ANNs are computing structures inspired from the biological neural networks. ANN constitutes of the concomitant operating units. They are capable of learning by adjusting the weights of the interconnections through the input data [22].Fig. 2demonstrates the typical feed forward neural network.ANN consists of neurons, each of which has one or more weighted inputs and one or more output that are weighted when connecting to other neurons. Neuron collects the weighted inputs and transports the net input through an initiation function in order to create a result [23].(5)hi=σ∑j=1NVijxj+Tihiddenwhere σ() represents activation function, N symbolizes the number of input neurons Vijthe weights, xjinputs for the input neuron, andTihiddenthe threshold value for the hidden neurons [24].The limitations of perceptron can overcome by feed forward multilayer networks with non-linear node functions. However, the simple perceptron learning algorithm cannot be converted just when it is transported from a single layer of perceptrons in multiple layers of perceptrons. These feed forward networks are sometimes given the name “multilayer perceptrons” (MLPs). The expression “back propagation network” is sometimes referred as the definition of feed forward neural networks trained by the back propagation learning method [25].Support vector machines (SVM) were developed by [26] for using binomial classification. It gained popularity amongst the other machine learning algorithms. Because of, implementing it in wide areas of biomedical science (i.e. classification of protein compounds as high as 90%) to computer science (i.e. classification of handwritings, images, text and hypertext) without any need to look for different machine learning algorithm. It needs a hyperplane to split multidimensional data into two classes. The following formula is used two linearly separable data if the ordered (w, b) exists(6)wTxi+b≥1,forallxi∈C+orwTxi+b≤−1,forallxi∈C−The data gathered from the real life usually are nonlinear that means it is not easy to find a line to separate data into two groups. SVM accomplishes this nonlinearity problem by coming up with the concept of a “kernel-induced feature space”, which transfers the data into higher dimensional space to convert it as separable data. It comes to mind how SVM overcomes that time-consuming complexity of transformation and over fitting data problem. Since during the computation, it is used just dot product in a higher dimensional space, which provides SVM avoiding such computational and over fitting problem. Besides the classification, the other implementation purpose of SVM is regression [27].Classification and regression trees (CART) considered as a decision tree method applied for classification purposes using the diachronic data. CART as a machine learning method is required to be determined number of classes. CART was proposed in 80s by [28]. In order to build decision trees, CART needs learning example. Decision trees are symbolized by a group of queries, which is splitting the learning sample into small and insignificant parts. In order to achieve the best split, the question that separates the data into two homogeneous parts, CART method examines for all possible variables and values [29].CART uses Gini index to select the attribute which has maximum information. If a data A with n classes has Gini index that is defined as,(7)Gini(A)=1−∑k=1npj2Most experimental learning systems need to know the type of classes exists in a given dataset. In addition, for each system, there is a vector of attribute values, and a mapping function to correspond from attribute values to classes. The attributes applied to define examples can be assembled into continuous attributes, whose figure are numeric, and discrete attributes with ungraded nominal values. C4.5 [30] can be given an example of that system which learns decision-tree classifiers [31].C4.5 utilize “Information Gain” to get a new measurement that is called as “Gain Ratio”. They are defined as in the following formula.(8)Entropy(P)=−∑i=1Npilog(pi)(9)GainRatio(p,T)Gain(p,T)SplitInfo(p,t)where splitInfo is defined as(10)SplitInfo(p,test)=−∑j=1Np′jp⋅logp′jpp is the probability distribution of the given data and log uses base as 2 due to measure information as ‘bit’.Random Forest (RF) gives a particular synthesis of classification accuracy and being a model having an exposition between the traditional artificial intelligence algorithms. The arbitrary instantiation and way of creating group applied in Random Forest give it a chance to succeed reliable prediction besides better generalizations. The property of generalization is derived from the bagging sketch, which improves the generalization by diminishing variance, even though dependent processes like boosting perform this by diminishing bias [32].Three features of Random Forest obtain the primary focus [33]:•It gives the opportunity of reliably classification in a different field of technique.It provides evaluating the significance of each attribute having model training.The trained model is used to measure pairwise proximity among samples.Due to use of the WEKA workbench, it allows to change and find the appropriate parameters. Firstly, the default values of algorithms’ parameters are tested and according to results of confusion matrices, it is tested to find the best accuracy that is achieved by the respective classifier.ANN in the workbench has seven parameters, but we have tested only three parameters as follows number of hidden layers was assigned 11, learning rate was 0.21 and momentum was 0.2.SVM has four parameters, but it is used only tested two parameters that were kernel as RBF, cost value was 9000.Logistic regression has four parameters just two were tested they were M as 1000 and heuristic stop as 151 respectively.RBF has three parameters just two were tested, which were clustering seed as 121 and number of clusters as 15.C4.5 (J48) has four parameters we were tested three of them such as confidence factor as .025, seed as 100 and number of folds as 6.CART has four parameters we have tested three of them in order number of folding for pruning as 0.2, seed as 100 and minimum number of object as 2RF has four parameters we tested three of them such as N as 13, F as 4 and T as 29.And the last one k-NN has three parameters and just two were tested k as 11 and linear search algorithm as Euclidean distance.Predicting performance of a machine learning method based on inadequate data is difficult. Therefore, Cross-validation becomes the favorite when the researcher got a small amount of data [34]. When machine-learning algorithms are used, decisions must be made on how to divide data for training and testing. With the aim of calculating the performance of machine learning methods, the entire CTG data split training and testing sets, and 10-fold cross-validation, which is a famous method for evaluation, is applied afterwards. The classification of data examined by a training set as forming a model, the verification of the model performed by the test set.The number of true negatives (TN), false negatives (FN), true positives (TP), and false positives (FP) are used to compute the efficiency of the classifier. The sensitivity and specificity are statistical measurements of checkout tests. Sensitivity states in the rate of positive test result,Sensitivity=TPTP+FN×100%Specificity indicates to the ratio of a negative test result, which has the following formulaSpecificity=TNTN+FP×100%Accuracy shows overall measure, which is:Accuracy=TP+TNTP+FP+TN+FN×100%ROC represents the classifier performance without considering class distribution or error costs. A receiver operating characteristics is made by drawing all specific values versus correspondent sensitivity values [35,36]. The approximation excellence relies upon the amount of thresholds tested. Regardless of its good sides, the ROC plot does not provide a standard for the classification of cases. On the other hand, there are methods that can be applied to get decision formulas by using the ROC drawing [36,37].Deciding the suitable threshold from a ROC curve is contingent upon on getting figures for the notional costs of false-negative and false positive errors.There is another statistical measurement call F-measure; to evaluate characterization of the performance, has the following formula:F-measure=2TP2TP+FP+FNIn Table 2, the applied measurements are given with their mathematical expressions.

@&#CONCLUSIONS@&#
