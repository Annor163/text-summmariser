@&#MAIN-TITLE@&#
Non-parametric particle swarm optimization for global optimization

@&#HIGHLIGHTS@&#
Proposing an improved PSO scheme called non-parametric particle swarm optimization (NP-PSO).Combining local and global topologies with two quadratic interpolation operations to increase the search ability in NP-PSO.Removing PSO parameters in the proposed method.Having the best performance of NP-PSO in solving various nonlinear functions compared with some well-known PSO algorithms.

@&#KEYPHRASES@&#
Optimization problems,Particle swarm optimization,Global and local optimum,Non-parametric particle swarm optimization,

@&#ABSTRACT@&#
In recent years, particle swarm optimization (PSO) has extensively applied in various optimization problems because of its simple structure. Although the PSO may find local optima or exhibit slow convergence speed when solving complex multimodal problems. Also, the algorithm requires setting several parameters, and tuning the parameters is a challenging for some optimization problems. To address these issues, an improved PSO scheme is proposed in this study. The algorithm, called non-parametric particle swarm optimization (NP-PSO) enhances the global exploration and the local exploitation in PSO without tuning any algorithmic parameter. NP-PSO combines local and global topologies with two quadratic interpolation operations to increase the search ability. Nineteen (19) unimodal and multimodal nonlinear benchmark functions are selected to compare the performance of NP-PSO with several well-known PSO algorithms. The experimental results showed that the proposed method considerably enhances the efficiency of PSO algorithm in terms of solution accuracy, convergence speed, global optimality, and algorithm reliability.

@&#INTRODUCTION@&#
PSO [1] is a population-based algorithm inspired by the social behavior of bird flocking or fish schooling. In the algorithm, a member in the swarm, particle, represents a potential solution which is a point in the search space. The global optimum is regarded as the location of food. Each particle adjusts its flying direction according to the best experiences obtained by itself and the swarm in the solution space. The algorithm has a simple concept and is easy to implement. Hence, it has received much more attention to solve real-world optimization problems [2–7], nevertheless, PSO may easily get trapped in local optima and shows a slow convergence rate when solving the complex and high dimensional multimodal objective functions [8].A number of variant PSO algorithms have been proposed in the literature to overcome the problems. The algorithms have improved the performance of PSO in different ways using various types of topologies, selecting parameters, combining with other search techniques and so on.A local (ring) topological structure PSO (LPSO) [9] and Von Neumann topological structure PSO (VPSO) [10] were proposed by Kennedy and Mendes to avoid trapping into local optima. According to Kennedy [9,11], PSO with a small neighborhood might have a better performance on complex problems, while PSO with a large neighborhood would perform better on simple problems. Suganthan [12] applied a dynamically adjusted neighborhood where the neighborhood of a particle gradually increases until it includes all particles. Dynamic multi-swarm PSO (DMS-PSO) [13] was suggested by Liang and Suganthan where the neighborhood of a particle gradually increases until it includes all particles. Hu and Eberhart [14] applied a dynamic neighborhood where m nearest particles in the performance space is chosen to be its new neighborhood in each generation. Mendes et al. [15] presented the fully informed particle swarm (FIPS) algorithm that uses the information of entire neighborhood to guide the particles for finding the best solution. Parsopoulos and Vrahatis combined the global and local versions together to form the unified particle swarm optimizer (UPSO) [16]. Gao et al. [17] used PSO with a stochastic search technique and chaotic opposition-based population initialization to solve complex multimodal problems. The algorithm, CSPSO, finds new solutions in the neighborhoods of the previous best positions to escape from local optima.The fitness-distance-ratio-based PSO (FDR-PSO) was introduced by Peram et al. [18]. In the algorithm, each particle moves toward nearby particle with higher fitness value. Liang et al. [8] developed comprehensive learning particle swarm optimization (CLPSO) that focused on avoiding the local optima by encouraging each particle to learn its behavior from other particles on different dimensions.In another research, a selection operator was firstly proposed for PSO by Angeline [19]. Other researchers applied apart from crossover [20], and mutation [21] operations from GA into PSO. An adaptive fuzzy particle swarm optimization (AFPSO) [22] proposed to adjust the parameters in PSO based on fuzzy inferences.Beheshti et al. proposed the median-oriented PSO (MPSO) [23] based on the information from the median particle. Also, they introduced centripetal accelerated PSO (CAPSO) [24] according to Newton's laws of motion to accelerate the learning procedure and convergence rate of optimization problems. Other variant PSO algorithms have been recently developed based on different techniques [25–28].Although the aforementioned algorithms have obtained satisfactory results in many optimization problems; there are still some disadvantages. For example, LPSO presents a slow convergence rate in unimodal functions [23,24]. CLPSO is not a good choice for solving unimodal problems [8]. Also, the majority of the algorithms require several parameters to tune, and setting the parameters can be a challenging for optimization problems. Moreover, some of the algorithms have a better performance than the PSO but their structures are not as simple as PSO.To overcome the drawbacks, this study introduces a non-parametric particle swarm optimization (NP-PSO) algorithm. The proposed method performs a global and local search over the search space with a fast convergence speed using two quadratic interpolation operations. There is no need to tune any algorithmic parameter in the NP-PSO algorithm. It means that all PSO parameters are removed in the proposed algorithm.The remainder of this study is organized as follows. In Section 2, a brief overview of PSO is provided. The proposed algorithm, NP-PSO in more details is described in Section 3. In Section 4, NP-PSO is used to solve several unimodal and multimodal benchmark functions and its performance is compared with some PSO algorithms in the literature. Finally, conclusions and further research directions are presented in Section 5.PSO is a population-based meta-heuristic algorithm that applies two approaches of global exploration and local exploitation to find the optimum solution. The exploration is the ability of expanding search space, where the exploitation is the ability of finding the optima around a good solution. The algorithm is initialized by creating a swarm, i.e., population of particles (N), with random positions. Every particle is shown as a vector,(X→i,V→i,P→besti), in a D-dimensional search space whereX→iandV→iare the position and velocity, respectively.P→bestiis the personal best position found by the ith particle:(1)X→i=(xi1,xi2,…,xiD)fori=1,2,…,N.(2)V→i=(vi1,vi2,…,viD)fori=1,2,…,N.(3)P→besti=(pbesti1,pbesti2,…,pbestiD)fori=1,2,…,N.The best position obtained by the swarm,P→g, is obtained to update the next particle velocity.(4)P→g=(pg1,pg2,…,pgD).Based onP→bestiandP→g, the next velocity and position of the ith particle are computed using (5) and (6), respectively as follows:(5)vid(t+1)=w×vid(t)+C1×rand1×(pbestid(t)−xid(t))+C2×rand2×(pgd(t)−xid(t)),(6)xid(t+1)=xid(t)+vid(t+1),wherevid(t+1)andvid(t)are the next and current velocity of the ith particle respectively. w is inertia weight, C1 and C2 are acceleration coefficients, rand1 and rand2 are random numbers in the interval [0,1].xid(t+1)andxid(t)are the next and current position of the ith particle.Also,|vid(t+1)|<vmaxand vmax is set to a constant bounded based on the search space bound. In (5), the second and the third terms are called cognition and social term, respectively. The two models applied to chooseP→gare known asP→gbest(for global topology) andP→lbest(for local topology) models. In global topology, the position of each particle is affected by the best-fitness particles of the entire population in the search space; in the local model, each particle is influenced by the best-fitness particles in its neighborhood. In this study, the local topology is called LPSO.A large value of w is more appreciate of the global exploration; while a small value facilities a local exploitation. Shi and Eberhart [29] proposed a linearly decreasing inertia weight. They also designed fuzzy methods to nonlinearly change the inertia weight [30]. Ratnaweera et al. [31] proposed the HPSO-TVAC algorithm, which used linearly time-varying acceleration coefficients. In the algorithm, a larger C1 and a smaller C2 set at the beginning, gradually reversing their relationship throughout the search. By analyzing the convergence behavior of the PSO, a PSO variant with a constriction factor was presented by Clerc and Kennedy [32] as follows:(7)χ=22−φ−φ2−4φ,(8)φ=C1+C2=4.1,(9)vid(t+1)=χvid(t)+C1×rand1×(pbestid(t)−xid(t))+C2×rand2×(pgd(t)−xid(t)),where C1 and C2 are set to 2.05.χis mathematically equivalent to the inertia weight (w) as Eberhart and Shi pointed out [33].NP-PSO tends to overcome the disadvantages of PSO by avoiding local optima, accelerating the convergence speed and removing algorithmic parameters. According to [23,24], PSO has shown a better performance than LPSO in unimodal problems and LPSO provides a good results in multimodal. Hence, both local and global topologies are applied in NP-PSO. Also, the search of new area is improved by creating new particles in different areas. In the algorithm, each particle uses the best position found by its neighbors (P→lbest) to update its velocity as shown in (11).(10)P→lbest=(plbest1,plbest2,…,plbestD).(11)vid(t+1)=vid(t)+rand1×(pbestid(t)−xid(t))+rand2×(plbestd(t)−xid(t)),The next position of each particle is computed based on the current position,xid(t), the next velocity,vid(t+1), and the best position found so far,X→gbest, as follows:(12)xid(t+1)=xid(t)+vid(t+1)+rand3×(xgbestd(t)−xid(t)).(13)X→gbest=(xgbest1,xgbest2,…,xgbestD).At the beginning, theX→gbestis initialized by the global best position obtained by the swarm:(14)P→gbest=(pgbest1,pgbest2,…,pgbestD).Then, the two new particles are created as (15) and (16). If every position of the new particles is better than theX→gbest, theX→gbestis updated.(15)X′→1=12(X→j2−X→gbest2−X→k2)×f(X→j)×f(X→k)(X→j−X→k)×f(X→gbest)+(X→k−X→gbest)×f(X→j)+(X→gbest−X→j)×f(X→k),(16)X→′2=12(X→j2−X→k2)×f(X→gbest)+(X→k2−X→gbest2)×f(X→j)+(X→gbest2−X→j2)×f(X→k)(X→j−X→k)×f(X→gbest)+(X→k−X→gbest)×f(X→j)+(X→gbest−X→j)×f(X→k),whereX→jandX→kare two particles selected randomly from the swarm. f(x) is the fitness value of X.In (14) and (15),X→gbest,X→j, andX→kmust be different from each other. If the positionX′→1orX→′2is better than theX→gbest,X→gbestis replaced by the new particleX′→1orX→′2in the swarm. In other word, the new particles are accepted in the swarm if their positions are better than the best solution obtained by the swarm.Eq. (15) has been proposed in this study and (16) is based on the quadratic interpolation in [34].|xid(t+1)|<xmaxand xmax is set to a constant based on the search space bound. The steps are shown as the flowchart of NP-PSO in Fig. 1.In this section, the proposed NP-PSO algorithm is compared with six well-known PSO algorithms in the literature. The performance of algorithms is evaluated by various unimodal and multimodal functions in different dimensions. Nineteen (19) benchmark functions [35–37] are selected in this study.The minimization functions applied in the experimental study are including unimodal, multimodal, rotated, shifted-rotated, hybrid, and composition functions as detailed in Table 1. In the table, Range and n are the feasible bound and the dimension of each function, respectively. Foptis the optimum value of function.In unimodal functions, the convergence rate of search algorithm is more important than the final results because several methods have been designed to optimize these functions. In multimodal functions, finding an optimal (or a good near-global optimal) solution is more interesting since these functions are more difficult to optimize because of the number of local optima. In these functions, the number of local optima exponentially increases as the dimension increases. Therefore, the search algorithms should be able to obtain good solutions and not become trapped in a local optimum.In rotation functions, the rotation increases the function complexity and does not affect the shape of function. The variableY→is computed using an orthogonal matrix M[38] and applied to obtain the fitness value of rotated function. In shifted functions, the global optimumX→*=(x1*,x2*,…,xD*)is shifted to the new positionO→=(o1,o2,…,oD).In the hybrid function, some different basic functions are used to construct these functions as shown in Appendix A. The composition function merges the properties of some functions and maintains continuity around the global/local optima [37]. The details of composition functions are presented in Appendix A.Among the benchmarks used in this study, functions 1–3 are unimodal functions and functions 4–7 are in the class of multimodal functions. Functions 8–11 are rotated and functions 12–15 are shifted-rotated unimodal and multimodal functions. Function 16 is the hybrid function and functions 17–19 are composition functions. All the test functions are shown as follows:1. Sphere model (unimodal function)F1(x)=∑i=1Dxi22. Schewefel’ s Problem 2.22 (unimodal function)F2(x)=∑i=1D|xi|+∏i=1D|xi|3. Schwefel's Problem 1.2 (unimodal function)F3(x)=∑i=1D∑j=1ixj24. Rosenbrock function (multimodal function)F4(x)=∑i=1D−1[(100(xi2−xi+1)2+(xi−1)2]F4 is unimodal in a 2-dimension or 3-dimension search space but can be treated as a multimodal function in high-dimensional cases.5. Griewank function (multimodal function)F5(x)=14000∑i=1Dxi2−∏i=1Dcosxii+16. Rastrign function (multimodal function)F6(x)=∑i=1Dxi2−10cos(2πxi)+107. Non-continuous Rastrigin's function (multimodal function)F7(x)=∑i=1D[yi2−10cos(2πyi)+10],whereyi=xixi≤0.5round(2xi)2xi≥0.58. Rotated Schwefel's Problem 1.2 with noise in fitness (unimodal function)F8(x)=∑i=1D∑j=1iyj2×(1+0.4|N(0,1)|),y=M×x9. Rotated Schewefel’ s Problem 2.22 (unimodal function)F9(x)=∑i=1D|yi|+∏i=1D|yi|,y=M×x10. Rotated Ackley's function (multimodal function)F10(x)=−20exp−0.21D∑i=1Dyi2−exp1D∑i=1Dcos2πyi+20+e,y=M×x11. Rotated discus functionF11(x)=106yi2+∑i=1Dyi2+F11*,y=M(x−o),F11*=30012. Shifted rotated Schwefel's Problem 1.2 (unimodal function)F12(x)=∑i=1D∑j=1izj2,z=(x−o)×M13. Shifted rotated Ackley's function (multimodal function)F13(x)=−20exp−0.21D∑i=1Dzi2−exp1D∑i=1Dcos2πzi+20+e,z=(x−o)×M14. Shifted rotated Rosenbrock function (multimodal function)F14(x)=∑i=1D−1[(100(zi2−zi+1)2+(zi−1)2],z=(x−o+1)×M15. Shifted and Rotated HappyCat Function (Multimodal Function)F15(x)=∑i=1Dzi2−D1/4+0.5∑i=1Dzi2+∑i=1Dzi/D+0.5+F15*,z=M5(x−o)100,F15*=130016. Hybrid function 5 (multimodal function)N=5p=[0.1, 0.2, 0.2, 0.2, 0.3]g1: Expanded Scaffer's F6 function: f10g2: HGBat function: f4g3: Rosenbrock's function: F4g4: Modified Schwefel's function: f9g5: High conditioned elliptic function: f117. Composition function 6 (multimodal function)N=5σ=[10, 20, 30, 40, 50]λ=[2.5, 10, 2.5, 5e−4,1e−6]bias=[0, 100, 200, 300, 400]g1: Rotated expanded Griewank's plus Rosenbrock's function: f12g2: Rotated HappyCat function: F15g3: Rotated modified Schwefel's function: f13g4: Rotated expanded Scaffer's F6 function: f14g5: Rotated high conditioned elliptic function: f1518. Composition function 7 (multimodal function)N=3σ=[10, 30, 50]λ=[1, 1, 1]bias=[0, 100, 200]g1: Hybrid function 1: f16g2: Hybrid function 2: f17g3: Hybrid function 3: f1819. Composition function 8 (multimodal function)N=3σ=[10, 30, 50]λ=[1, 1, 1]bias=[0, 100, 200]g1: Hybrid function 4: f19g2: Hybrid function 5: F16g3: Hybrid function 6: f20In this section, the results of proposed method are compared with CLPSO, PSO, LPSO, UPSO, FIPS, and FDR-PSO. The algorithms are evaluated using the benchmark functions with dimensions 10 and 30. Maximum iterations is set at 5000 for D=10, and 10,000 for D=30. Population size is set to 30 (N=30) except for F11, and F16–F19 is set to 50 because the functions are more complex. Also, the ring topology is used as the neighborhood structure in the lbest model and the number of neighbors is set to 2. The parameters settings of PSO algorithms used are listed according to their references as follows:•PSO with inertia weight [29],LPSO with inertia weight [12],CLPSO [8],UPSO [16],FIPS [15];FDR-PSO [18].The algorithms are run independently 30 times for the benchmark functions and the average best solution, the standard deviation (STD), the median of the best solution in the last iteration, the average iteration for finding the best solution, and rank of each algorithm are reported in Tables 2 and 4. The algorithms are ranked based on the average best solutions in the tables. Also, the non-parametric Wilcoxon rank sum test [39] is conducted between the NP-PSO's result and the best results achieved by the other six PSO versions for each problem to determine whether the results generated by NP-PSO are statistically different from the results obtained by the other algorithms. The results of Wilcoxon rank sum test are shown in Tables 3 and 5, where h-value=1 indicates that the performances of the proposed method is statistically different with 95% certainty, h-value=−1 represents that the compared algorithm are significantly better than the proposed algorithm, and h-value=0 denotes that the results of the two considered algorithms are not significantly different. In Tables 3 and 5, rows 1 (Better), 0 (Same), and −1 (Worse) give the number of functions that the NP-PSO performs signiﬁcantly better than, almost the same as, and significantly worse than the compared algorithm, respectively.Table 2 shows the experimental results for all benchmark functions with dimension D=10. As illustrated, the NP-PSO algorithm surpasses the other algorithms in minimizing functions 1, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15 and 19 and especially significantly improves the results of functions 3, 14 and 19. According to the results of Wilcoxon rank sum test, these results are different from the other compared algorithms for the functions, however, CLPSO, FDR-PSO, and FIPS provide the best in functions 7, 11, and 17, respectively. The NP-PSO performs better on more complex problems when the other algorithms miss the global optimum. The proposed method achieves the global optimum for functions 1, 2, 3, 6, 8, 9, 10, and 13 in the minimum average iteration compared with the other algorithms. The superior convergence rate of NP-PSO is shown in Fig. 2. The results in this figure illustrate that NP-PSO tends to find the global optimum for F1, F9 and F14 faster than the others and obtains the highest accuracy for these functions from among all the algorithms.The minimization results of the benchmark functions with dimension D=30 are presented in Table 4. As seen in the table, the NP-PSO algorithm outperforms the other algorithms for functions 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14, 15, and 19. The largest difference in performance between the proposed algorithm and the other algorithms occurs for the functions 1, 2, 3, 4, 8, 9, 10, 14, and 19. From the results of Wilcoxon rank sum test in Table 5, it can be concluded that the NP-PSO provides the best results in almost all functions, except in F14, F17 and F18. In these functions, FDR-PSO, FIPS and UPSO obtain the better performance than NP-PSO and NP-PSO has the second rank in these functions. Fig. 3illustrates the progress of the average best solution over 30 run for F3, F10 and F19. As seen, the proposed method presents a higher convergence rate than the other algorithms.Fig. 4illustrates the population distribution observed at various stages in a NP-PSO process for function 10 with D=30. As shown in Table 4, only NP-PSO can obtain the global optimum in the lower iteration among the other algorithms for this function. It can be seen in Fig. 4 that following the initialization, the particles start to explore the search space without an evident control center. Then, the learning mechanisms of the NP-PSO pull many particles to swarm together toward the optimal region and the population converges to the best particle. From this simple investigation, it can be observed that the population distribution information can significantly vary during the run time, and the NP-PSO has the ability to jump from local optima and converges to the global optimum.In Tables 2 and 4, NP-PSO has the first final rank compared with the other algorithms. The sum rank of NP-PSO is significantly better than the other algorithms. These all indicate that the proposed algorithm, NP-PSO, is more powerful and robust than the others for solving complex unimodal and multimodal functions. In these tables, the standard PSO (PSO) and local topology of PSO (LPSO) provide the poor results, respectively. Also, the rank in the tables shows that CLPSO algorithm has a worse performance for the unimodal functions.This study presents a non-parametric PSO algorithm (NP-PSO) to improve the search ability and the convergent efficiency of PSO. The algorithm does not use any algorithmic parameter. It is simple and easy to implement as the original PSO. The method enhances the global exploration and the local exploitation using both the local and global topologies, and two quadratic interpolation operations. The new strategies increase the particles’ abilities to fly in the larger potential space. To evaluate the algorithm, six well-known PSO algorithms have been selected for solving minimization functions. From the analysis and experiments, it can be concluded that the NP-PSO is the best choice to solve complex unimodal and multimodal functions compared to the six PSO algorithms. The proposed method generates the best solutions in a lower iteration among the other algorithms.For further research, a binary NP-PSO (BNP-PSO) and multi-objective NP-PSO (MONP-PSO) can be proposed to evaluate the performance of NP-PSO for solving binary and multi-objective optimization problems, respectively. Also, Eqs. (15) and (16) can be applied in the conventional PSO algorithms to assess the efficiency of these equations.

@&#CONCLUSIONS@&#
