@&#MAIN-TITLE@&#
On the performance of ACO-based methods in p2p resource discovery

@&#HIGHLIGHTS@&#
We search for an optimal aco algorithm in p2p environment.We formulate criteria for such an algorithm and examine the candidates.The impact of underlying topology is negligible if not exploited directly.The hybrid approach converges better and quicker than the mutli-agent approach.The mutli-agent approach converges quicker, but worse than single-agent approach.

@&#KEYPHRASES@&#
Semantic search,Ant colony optimization,Peer-to-peer,

@&#ABSTRACT@&#
Over the recent years peer-to-peer (p2p) systems have become increasingly popular. As of today most of the internet IP traffic is already transmitted in this format and still it is said to double in volume till 2014. Most p2p systems, however, are not pure serverless solutions, nor is the searching in those networks highly efficient, usually achieved by simple flooding. In order to confront with the growing traffic we must consider more elaborate search mechanisms and far less centralized environments. An effective proposal to this problem is to solve it in the domain of ant colony optimization metaheuristics. In this paper we present an overview of ACO algorithms that offer the best potential in this field, under the strict requirements and limitations of a pure p2p network. We design several experiments to serve as an evaluation platform for the mentioned algorithms to conclude the features of a high quality approach. Finally, we consider two hybrid extensions to the classical algorithms, in order to examine their contribution to the overall quality robustness.

@&#INTRODUCTION@&#
Since the introduction of email, one of the most successful examples of a large-scale distributed application in its time, the research field of distributed computing [1] has experienced an enormous development. The reasons for using such systems are, firstly, because the nature of the application requires that several computing nodes produce and share data across a communication network and, secondly, because of practical reasons with respect to a centralized system in terms of scalability, reliability or expandability.Distributed environments have drawbacks however, namely, it is quite difficult to propose a resource discovery mechanism that would be as efficient as the optimum of a centralized depository. It is also more of a challenge to obtain a complete answer, as well as estimate the completeness of it. Even so, the benefits of a distributed environment overweigh the drawbacks and that is why the search for efficient resource discovery and search algorithms is crucial.These drawbacks are specially challenging in p2p distributed environments. Abstractly speaking, p2p systems are networks of interconnected peers, where some provide resources of any nature whereas others wish to obtain them. The roles of peers (sometimes referred to as nodes) are variable. One example of a real life use of p2p might be en-masse concurrent calculations which have been done with great success. Distributed computing projects, such as SETI@home [2] or Collatz Conjecture [3], peaked at hundreds of teraflops of computing power with relatively low costs. What stimulates the development of distributed techniques is the comparison of these results with super-computers such as ORNL Supercomputer [4], which involve immense investments and oscillate at about 10–15000 teraflops. Other examples of p2p application are: Sharing of storage and content distribution [5–7], where the desired content is treated as the resource, sharing of bandwidth, streaming or even anonymous communication solutions, both text and VoIP [8].Nodes in p2p systems are in possession of resources. A query in such a system is, in short, a process of demanding resources from a subset of peers and then returning to the sender peer with results. There are several degrees of distribution in p2p environment to be considered. The most extreme one is the case of p2p systems with a very high distribution degree which implies the following:1.The content, information or process of global scope is very highly undesirable.The cost of the exchange of data between two peers of the system is considerable. So much so, the system would rather obtain no data than obtain low relevancy data.In this respect, a remarkable computing strategy to address the problem of effective searching in highly distributed p2p systems has been ant colony optimization (ACO, introduced in [9]). With the query masquerading itself as an ant in search of food and depositing chemical substance as trails, which can be read by other ants, one can achieve a very good implementation of p2p search. The biggest benefits of the use of ACO are: no (or a very small amount of) global information, generic nature, quick convergence to near-optimal solution and robustness in terms of system load.There has been several ACO proposals addressing this issue (ant colony system—ACS [10], min–max [11], neighboring-ant search—NAS [12], Semant [13] and various mixed solutions [14]), albeit there are factors whose impact has not been thoroughly studied such as the existence of long distance connections between peers in unstructured environments and the consideration of hybrid strategies that take advantage of underlying structured topologies. Therefore, in this paper we will show that there is still room for improvement in the area of ACO based p2p search systems and we will propose an implementation of a p2p version of ACS which is competitive in both unstructured network topologies with varying number of long distance connections and structured hypercube ones if a hybrid strategy is defined.In Section 2 we will analyze the problem in detail and describe the use of ACO in p2p search. It will contain the mathematical base and the concepts to consider; in Section 3 we present motivations for choosing specific algorithms for the comparative study. In Section 4 we will propose the experimental dimensions to be analyzed, the designed experiments and present their results with comments. Finally, in Section 5 we will formulate the final discussion.Ant colony optimization is a swarm intelligence approach to problem-solving introduced by Marco Dorigo in his work on distributed optimization in 1991 [15]. The core idea of ACO is twofold, firstly – as properly named – it uses a swarm of simple and stochastic automata to solve complex problems and, secondly, the communication between these is through stigmergy and therefore indirect. Such a communication method has shown to provide interesting results, especially with the emphasis on finding the shortest path [16] or paths optimizing a given function [17,18]. The automata, or agents, in ACO are called ants. Each ant has the simple task of finding the required resource (search phase) and bringing it back to its nest (returning phase); without the loss of generality one can limit the world, in which ants live, to a bidirectional graph of finite size with nodes representing possible locations of resources and edges representing trails.Ants follow a simple and non-deterministic search algorithm that can be summarized in the following (the micro-scale algorithm—ant's behavior):AlgorithmMicro-scale algorithm1:Consider the Ant A1 that finds itself in a node ne(emitting node) of graphG=V,E, where V is a set of vertices and E is a set of edges, with the task of finding the set of resourcesr|r∈qG, whereqGis a perfect response of the graph G to the query q2:A1 checks the node in which it currently resides for the presence of resource r3:If a resource r is found it is added to the ant's basket4:If A1 has found enough resources to fill its basket or any other pre-established condition or set of conditionsDAholds true, A1 proceeds to step 85:A1 performs the step transition based on available, local knowledge:STA1,nj,qBeing in node nj, it chooses noden∈ni|nj,ni∈Eas the next destination. Adds n to the stack of nodes visited,nAi6:A1 performs local pheromone update—metaphore of the natural process of pheromone evaporation7:Proceed to step 18:A1 converts from being a forward ant to being a backward ant9:(optional, indication of a hybrid ACO) A1 performs optimization of the stacknA110:A1 performs an evaluation of the trail found based on a quality measure functionQMA111:A1 returns to the emitting node following the stacknA1, at every step performing an update of the locally stored pheromone trails using global pheromone update rule—metaphore of pheromone deposition.12:When A1 returns to the emitting node, it deposits found resources from the basket and the algorithm concludesEnd AlgorithmAs it can be deduced from the micro-scale algorithm, there are several key factors that define ACO algorithms:1.Graph topology (or the lack of it).State transition functionSTA,n,qwhere A stands for ant, n is the current node and q is the carried query.Local pheromone update function—the model of pheromone evaporation.Global pheromone update function—the model of depositing pheromones after the search concludes.Quality measure functionQMAwhere A stands for ant. Here, in fact, the ant is the evaluated element—seeing how it represents the query, the route over the graph and the resources found.Query completion requirementDA, where A stands for ant.Post-processing algorithms—such as route optimization, loop detection and removal, graph topology exploitation, etc.One of the most popular implementations of the ACO metaheuristic is ant colony system [9]. It is an extension and improvement over the ant system (AS) [15]. It has been chosen as the principle candidate for p2p search. In the particular case of ACS the query completion is achieved by either collecting between Rmin and Rmax resources or making ttl steps (time to live—the maximum amount of state transitions); formally:(2.1)DA=1,ifr∈Rmin,Rmaxorh≥ttlmax0otherwisewhere r is the amount of resources found and h is the amount of steps taken.If no resources were found the ant has a choice whether to finish the algorithm empty or perform the route back to the emitting node, without any pheromone updates and inform the node about the failure. For our purposes we chose the latter solution.Consequently, the macro-scale algorithm (the system's large scale behavior) for ACO p2p search could be defined as follows:AlgorithmMacro-scale algorithm1:Query q is requested upon a node n2:Bring to life a forward ant FAqin the node n and supply it with q3:Let FAqperform the micro-scale algorithm4:Until δt time units have passed, consider the q resolution pending5:If backward ant BAqis received in the node n after less than δt time units have passed, consider the basket of BAqthe graph's response to q and dispose of BAq6:If BAqis not received within δt time units, consider the q resolved with no resultsEnd AlgorithmTaking as basis the previous generic algorithmic schema, the definition of an ACO-based query resolution algorithm in p2p environments must conform to the following additional query–resource (q–r) principles for it to be considered p2p compliant:1.Every node may have any amount of resources, including zero resources.Every node may issue a query—that is, a request for a set of resources of any nature; one that may be constructed of resources residing in one or many nodes within the network.Every node may not be aware of the content of any other node but itself.Every node must be connected to a set of nodes via bidirectional links of high traveling cost. A degenerated (disconnected) node may be connected to zero other nodes.Every query is propagated among nodes, collecting resources that correspond to the request issued.The destination (the final) node of a query is never known a priori nor is it deterministic.The trail of a query is never known a priori nor is it deterministic.The previous list of requirements will serve to filter out algorithms that have no applicability in the field of p2p. Omitting of any or all of these principles is possible. Such a system would, however, suffer from lower generality and it would be incomparable to the real world p2p networks. Once the generic approach for ACO p2p searching has been introduced we can discuss, within the dimensions mentioned above, some of the more prominent ACO algorithms proposed in the literature. In the following subsections we will describe in moderate detail some of the principle ACO and ACO-like algorithms, besides the well-known ACS strategy, and then formulate the subset of those best applicable for p2p and proceed to in-detail study.The Semant algorithm [19] is our second candidate for p2p search, it uses a very similar approach to the classical ACS, however it adds several extensions. One of the most prominent is the use of a 2-dimensional pheromone table stored in every node, that is a (keyword, outgoing link) pair, rather than the typical 1-dimensional pheromone per outgoing link. This can be understood as an additional layer (overlay) of pheromones per every taxonomy entity used in the query routing. For more details on the concept, and our variation of it, see Section 3.1.Semant maintains the exploration–exploitation dilemma approach from the ACS. The exploitation is now expressed as in (2.2)(2.2)s=argmaxu∈Jkrτcu×ηuβ,where τcuis the pheromone level from the current node to the u node within the c concept overlay and ηuis the cost of traveling to u. The result s is the next node to be visited, and, as in ACS, it is deterministic. The major change is the exploration phase: every edge that origins in the current node is assigned a probabilitypcr∈0,1according to (2.3), but in this case a resolution of probability is done per link, which means that pcris the probability that the r destination node in the c concept overlay will be returned as the next step:(2.3)pcr=τcr×ηrβ∑v∈Jkrτcv×ηvβThe consequences of such an approach are twofold: firstly, there might be more than one link as a result of this, and secondly, there might be no links. In the first case, the original FA is sent to one of the chosen links and a clone of the FA, called FAclwill be sent to every ith link, i>1. In the second case, a result will be obtained by falling back to the exploitation phase. This behavior is formally described by Eq. (2.4) and constrained by (2.5):(2.4)GOj=1,ifp≤pcj0,otherwise(2.5)∑j∈Jkrpj=1,where GOjis a function that expresses the fact of an ant (or its clone) choosing to go to the j node (value 1) or not (value 0) and p is a random variable.The pheromone management is also different to the ACS approach. The pheromone deposition is a linearly growing function, that is, the act of dropping n units of pheromone will increase the value by n. Hence the maximum value ofτr,uis more of an issue to consider.(2.6)τr,u←τr,uδτr,uIn (2.6)δτr,urepresents the quality measure and is calculated as(2.7)δτr,u=wd×RRmax+1−wd×ttlmax2×h,where Wdis a parameter that expresses the balance between both components of the equationWd∈0,1, 0 is the amount of resources found, Rmax is the maximum resources allowed, ttlmax is the maximum number of steps allowed and h is the number of steps taken. Here, as in ACS, the pheromone levels can be limited by phmax and phmin. Since Semant uses linear growth of pheromone, instead of weighted growth, the phmax must be set to a very high value, in order to avoid issues with having all the paths at its maximum value—thus not providing any information.The evaporation process is very similar to ACS and somewhat simplified:(2.8)τr,u←1−ρ×τr,uThe query completion is achieved in an identical manner to ACS.Another proposition of an extension of the basic AS was proposed by Gómez Santillán et al. [12]. It is based on exploiting the node distribution and several look-ahead heuristics. For in depth look consult the work [12]. Here we will focus on the pseudocode governing the behavior of neighboring-ant search (NAS), provided by the authors of NAS:AlgorithmNAS algorithm1:for each query in rkcreate a search agent k with TTLk=maxTTLand Hitsk=02:while (Hitsk<maxResults and TTLk>0)3:if (the unvisitedsk∈rk∪Γrkhas the searched resource)4:rk=append skto pathk5:Hitsk=Hitsk+16:Local pheromone update7:Global pheromone update8:else9:if (rkis a leaf node or does not have an unvisited neighbor)10:remove the last node from pathk11:else12:sk= apply the transition rule with the DDC function13:14:Local pheromone update15:end if16:end if17:TTLk=TTLk−118:kill the search agentEnd AlgorithmAt step 5 clearly the NAS algorithm takes advantage of basing its routing decisions on the content of the neighboring nodes. This, yet again, violates the third of the q–r principles. Furthermore at step 10 it permits removing nodes from the path hence improving the overall quality measure by simulating the path shorter than it actually was. And finally, NAS generates a BA (called retrieval agent) at every occurrence of a resource, putting a great additional load on the system. A remark is made in [20], where Michlmayr notices that the less resources a single agent carries (the Rmin variant of Semant), the better overall score of the results obtained, but the smaller the value of a single query. In other words: there is an increase of measured quality (which will be defined formally in Section 4.2) but at the cost of the real value for the user (less results at a time), and for the system (more load). All these factors contribute to the fact that in [12] NAS achieves results better by approximately one order of magnitude and simply it is not comparable with an ACO algorithm of a more pure nature. The fact of examining the content of neighboring nodes should improve the results by a factor of average node degree, that is, an average of the degrees of all the nodes in the system. This is because within, what is calculated as one step, they analyze all the neighbors, therefore making several steps in one; in the terms of means this translates into makingn=averagenoderadesteps and later reporting it as one step. Worth mentioning is the fact that NAS guides the FA (search agents) toward nodes with high degrees, further exploiting the proposed approach.The most straightforward algorithm is the k-walker explored in detail in [21]. It has to be emphasized that it is not an ACO algorithm, but it serves as a good benchmark, a reference in a given test; it is also proposed as such in the experimental study of Semant [20], which we follow closely in order to maximize the fidelity of its result recreation. Moreover, the random behavior is a firm minimum performance expectance; a way to discard an algorithm if it fails to surpass it in every measure. The reason why it has been included in this section is that it can be easily expressed in the ACO's terms, degenerated but valid, and follow the general flow of ACO. By doing this we also show how we implemented it using our ACO testing middleware.The state transition consists of one phase—the dilemma of exploration versus exploration is removed. On the first step k-walker generates k forward ants and each one of them makes a random decision on how to continue; on every other it simply takes a random decision (2.9). The probability of choosing to go from the node r to the node s ispkr,s(2.10)(2.9)s=S,ifh>0S1,…,Sk,ifh=0,where h is the number of steps taken.(2.10)pkr,s=1Jkr,ifs∈Jkr0otherwiseThe pheromone management is not relevant becauseτr,udoes not appear in (2.10). Therefore for evaporation, as well as deposition are as follows:(2.11)τr,u←τr,u,which indicates that there is no pheromone evolution.The query completion is achieved in an identical manner to ACS.Other ACO algorithms that are highly worth mentioning are the following:1.AntNet [22]AntHocNet [23]Ant-based control [24]Their core ideas however do not fit our established q–r principles. The AntNet and AntHocNet are mainly used for packet routing where the destination is well known and only the path is to be discovered. This stands in high contrast to the q–r principles (point 6), in which the destination in not known but merely described by the combination of query, resources and the algorithm. The same problem occurs with ant based control, which is used in circuit switching environments.For the sake of completeness another approach deserves a mention – it is the most straightforward and most redundant approach of all – namely: the k-flooding [25]. It has been used for many years in the Gnutella protocol [5] and it is basically sending the query to all the neighbor nodes until the kth depth. Flooding has a somewhat limited variant; called t-top k-flooding, in which case the flood is sent to only the t best neighbors. As shown in the work by Jun-qing et al. [26] they are vastly inefficient compared to ACO and will not be considered in this work.Once the main algorithmic ACO-based approaches for solving the proposed problem have been presented, the following comparative table relates them according to some dimensions that are important for selecting the candidate algorithms that will be included in our experimental study.From Table 1we can easily conclude that in the pure form only three algorithms qualify, in terms of q–r principles, for further study. This was the reason why we chose to increase the test sample by introducing a set of extensions to the classical approach. In Section 3 we present the mentioned extensions in detail.In order to adapt ACS closely to the requirements of p2p environments, we have decided to introduce two extensions that can be used separately or combined at will: a semantic and a hybrid extension. The semantic extension is based on that proposed by Semant's authors, and the hybrid extension is aimed at exploiting hypercube topology.Based on the algorithms selected in Section 2.5 and with the use of the extensions explained below we create four new algorithms, namely: semantic ACS, hybrid-semantic ACS, hybrid semant and hybrid k-random walks. All of them are compliant with our q–r principles and they are a combination of the corresponding classical algorithm and one or both extensions.The first extension is the notion of the routing concept, mentioned already while discussing Semant in Section 2.1. Its theoretical base was presented in [27], where the idea of several overlay networks superposed over the physical node network is introduced.Every node n keeps a 2-dimensional matrix Ω:Nn×Rn, with real, positive values, where Nnis the space of outgoing links from the node n and Rnis the space of routing concepts maintained by this particular node n. This matrix is referred to as routing table, or routing matrix. The n′th, rth element of Ω corresponds to the pheromone value of the n′th outgoing link for the rth routing concept, which can be written asΩn′r=τ. There are two functions defined:(1)phUpdaten′,r,τ, that establishes the value Ωn′r for the n′∈Nnand r∈Rnas τ.phRetrieven′,r, that returns the value Ωn′r for the n′∈Nnand r∈Rn.In both cases, if r∉Rn, the matrix is redefined asΩ†:Nn×Rn†, the space of routing concepts is redefined asRn†:Rn∪r, and∀n′∈NnΩn′r=phinitwhere phinitis the initial value of the pheromone. Both functions are undefined for n′∉Nn. Node n, at initialization, hasRn=rdef, and∀n′∈NnΩn′rdef=phinit, where rdefis the default routing concept. In other words: If the requested routing concept is not present in the routing table the table is extended by adding a new row for the missing routing concept all with initial values. Additionally, the routing table maintains the pheromone value only for the immediate neighbors.Note that it stands in contrast to the AntNet [22] algorithm, where the second dimension in the routing table is also used, but it monitors all the accessible nodes from a given node n, both directly and indirectly: (node, outgoing link). Such an approach would be not good, both memory- and efficiency-wise, in a p2p environment due to the typical size range and high dynamism.Routing concept is a generalization of the pheromone-per-keyword approach, introduced in [13]. Firstly, the routing concept does not have to be a keyword but any type of distinction will serve, such as a taxonomy of entities, numerical values, etc. Additionally, it allows the query to choose, at every step, into which overlay network it should be injected, rather than have one fixed at query's creation. The routing table dimension in each node can grow and shrink at will, without any a priori limitations. If no routing concept is chosen the default routing concept will be used—eliminating the overlay network concept for a given query. If the routing concept remains unchanged during the querying process the approach is reduced to the one described in Semant. ACS can be extended easily by adding the routing concept functionality.In its most general approach hybrid setups are very complex and elaborate systems of coordinated algorithms of mutual interaction. In order to properly apply hybrid extension we, firstly, faced a design decision, as there are several large classes of hybrid techniques. According to [28] the most suitable hybrid class for evolutionary-class metaheuristic is HRH (high-level relay hybrid). In such a setup algorithms that form components of the hybrid are self-contained and sequentially executed, forming processing (initial phase) and postprocessing (intermediate and final phases). As the author mentions, coarse grain evolutionary components, such as ACO, are not suitable for finding near-optimal solutions under difficult conditions. Local search technique is a good complement in this case.Another view of Hybrid taxonomy is provided in [29]. There the author establishes a slightly more in-depth, yet similar, taxonomy based around four key questions: the class of the hybridized components (metaheuristics, search techniques, seeding techniques, etc.), the level of hybridization (weak, strong), the order of execution (parallel, interleaved, batch) and control strategy. In this light ACO-applicable hybrid systems would be denominated weak-coupled, batch, integrative solution, similar to the previous suggestion of HRH. The author also decomposes the hybrid strategies into level-by-level processing approaches, where four elements are extracted: output function (OF), improvement method (IM), solution combination method (SCM) and input function (IF), which can be depicted as OF+IM+SCM+IF. The ACO itself is fully expressed in the above terms; we, however, add the TRO (taboo route optimization, see below) in the SCM phase.Our attempted solution is akin to the one by Duan et al. [30], which propose the following structure: DE+HS+HJ, where DE is the differential evolution algorithm representing the evolutive component (ACO in our case), HS is the harmony search (which has been omitted in our approach), and finally HJ is the Hooke and Jeeves direct search method, a local search algorithm that performs the final refinement. For HJ we use a domain-and-topology bound solution which we named TRO.The TRO consists of path shortening with the assumption of an underlying hypercube topology and exploits the fact that optimal paths in hypercube-based networks are well-known and solved problems. During the process of converting a forward ant into a backward ant the route optimization will be executed. The taboo route optimization draws its name somewhat from the taboo search [31], as they have several concepts in common. In our case the taboos, however, are not solutions but components of the solutions to be maintained in the final result. Also they are not established within the search process, but injected in the initial step.AlgorithmTRO1:Nodes within the pathp:n1,n2,…,nNthat was covered by a forward ant FA, will be analyzed and all those that have provided required resources will be marked as taboo. The first and the last node will be marked as well. As a result we obtain the pathpt:n1t,…,nnr1,…,nr1t,…,nNt, where:-nnriis read as the ith node that has not provided any resources, and-nriis read as the ith node that has provided resources2:A subpath sptwill be composed of only marked nodesnitof the path ptand nodes will be renamednsp1,nsp2, etc.spt:nspi,nspi+1,…,nspM3:For every pairnspi,nspi+1of nodes form the subpath spt, a topology-based optimal path between them will be found, namedpii+1and expressed as a sequence of nodes. The path resolving is performed according to the standard deterministic routing approach [32]. Ifpii+1is shorter than the number of nodes interposed betweennspi,nspi+1in the original path p, the appropriate section within p will be replaced bypii+1; in other case,pii+1will be discarded4:Once the process is complete the newly created path will be named poptimand will replace the original path p, which was provided to the backward ant BA by the forward ant FA; thus BA will follow the path poptimon its way to the emitting nodeAlgorithm EndAll the backward ant pheromone duties will be performed on the way as shown in Section 2.Based on our previous comparative study, we decided to choose for further testing only those algorithms that were compliant with our established q–r principles (see Section 2). Of those that remain, the pure ACS was also rejected due to the fact that it would inevitably score less than the Semantic ACS, this way we place Semant and Semantic ACS on equal footing. In the following section, we will describe the network topologies, the quality metrics used and the test setup that were used in the experimental study.In its purest form the ACO algorithms do not use any additional path processing. Nevertheless the idea of local path optimization was introduced early, in [9] to improve both: the speed of the path convergence, as well as the quality of the solution obtained. One of the approaches is to use advantages the topology may provide; some network topologies include ring [33], toroid [34], hypercube [35] and others in which corresponding local path optimizations apply. In this respect, we will verify whether the topology has no impact on either the speed or quality of convergence unless followed by a local optimization algorithm that uses it explicitly as it has been stated in [12,20] by taking into consideration the three topologies described next.The world consists of n nodes, where n is an even number. The nodes are organized in a fully connected grid with a toroidal topology [36], where both dimensions d1, d2 of the creating rectangle are chosen to fulfilld1−n2≅0to minimize the medium distance. Additionally, every node n1 has one long distance connection (LDC) that connects it directly to another node n2 with the toroidial distancen2,n1>2. The probability of a node n1 having LDC of length len is proportional to lon−1. The above description is taken directly from the guidelines in [19]. This kind of world will be named sem-n, where n stands for the number of nodes. In [12,20] the sem-n world is approached as if unstructured. The average degree of a node is(4.1)avsem-n=5This kind of world resembles the sem-n world in every detail with the exception of LDC connections. In the sem-n world every node has exactly one LDC connection, while in this world there will be extra m LDC connections distributed randomly and evenly among all the nodes. The length-wise distribution of LDC connections from sem-n applies. This kind of world will be named ldc-n-m where n stands for the number of nodes and m for the number of additional LDC links. The average degree of a node in ldc-n-m is:(4.2)avldc-n-m=5n+2mn=5+2mnNote that:(4.3)sem-n≡ldc-n-0and consequently:(4.4)avsem-n=avidc-n-0The hypercube world is a hypercube manifold of degree d[37]. Therefore, it will have n=ednodes. This kind of world will be named hc-d, where d stands for the degree of the world. In this case the average degree of a node is, unsurprisingly:(4.5)avhc-d=dAdditionally note that:(4.6)avhc-10≅avldc-1024-2400We have decided to adopt a common efficiency quality measure which is widely used by several authors such as in [10,19], where it is defined as a Hop per Hit (dimensionless) ratio; hop is the number of steps taken by an agent and hit is the number of resources found and it reflects rather well the quality of a resolution of queries. It is, yet again, a measure taken from the Semant study [20], which we must use in order to remain comparable. What is irrelevant in our testing is the absolute execution time. In the real setups the evolution takes place in the span of days, even weeks. So in our case it is, for all practical purposes, highly accelerated and the execution time transmits no information. One might argue the importance of the time factor in an attempt to solve a local problem by applying ACO algorithm; it is, however, not our case. In the field of p2p query routing the true value is how quickly the system evolves in terms of iterations, rather than time units. Moreover, it is important to add supplementary views in order to fully understand the undergoing processes. These will be provided by two additional metrics: Hit per Ant (dimensionless) and Ants (dimensionless).Hit per Ant reflects the amount of resources found by a single agent. This permits to compare easily multi-ant algorithms with single-ant ones. There is a question to consider here: with comparable Hop per Hit values, could low Hit per Ant be considered inferior to high Hit per Ant? We argue that the answer to this question is affirmative, because a low Hit per Ant value reflects the fact that each agent finds a small amount of resources. In consequence, in order to achieve a comparable Hop per Hit value an algorithm with a low Hit per Ant ratio will have to use more ants in the process, which will directly affect the system load in a p2p environment. To better understand compare ten ants, each one performing one step, with one ant performing 10 steps.On the other hand, the amount of ants used, denoted as Ant, which could be read as Ants per Query, shows how many ants are created by a single request. This measure can be used to analyze whether the system evolves toward using less ants, which is a favorable situation if a scalable p2p system is to be capable of processing a vast number of queries per unit of time. In this respect, we will consider a forward and a backward ant as separate beings so the absolute minimum for this measure is two ants if a backward ant's creation is forced and one, if it is not.Every test will consist of an amount n of queries of random nature, with a given taxonomy (see Section 4.3.1), released from random nodes within the given world. The query will be propagated following the rules of the algorithm that is being tested (see Section 2). For every query a set of data will be stored: the birth (creation) nanosecond, the death nanosecond, the query as text, the number of hops made, the number of resources found and the location of resources found. Based on this we will sort the full data, collected over the n iterations, by birth nanosecond, calculate the quality measures (see Section 4.2) and present the results as graphs. The amount of queries will be fixed at n=100, 000. Each test run will be repeated three times to assure consistency. The decision to limit the execution repetition at three was taken due to time and disk space constrains. The full set of crude data, as it is, occupies more than 60GB of disk space and is a result of more than 250h of pure processing time. Additional limiting factor was a high consistency of independent executions leading us to believe that more repetitions would not improve accuracy.Our testing platform is a highly configurable Java-based engine that supports all the above algorithms. Tests will be run on Intel Pentium 4630 at 3.00GHz with 4GB of ram on a 32bit Windows 7 machine. The scalability of the solution is not an issue to be addressed in our case, as all the real-life implementations will be very highly distributed and slow-evolving. While testing the limits of our software in the mentioned machine we managed to easily generate graphs of up to 60000 nodes and release onto them more than one million ants. A typical user would own not more than several nodes and process only singular ants at a time.As in [20], the ACM Computing Classification System [38] will be the taxonomical vocabulary used. Every resource in the network N is described by one, and only one leaf taxonomical concept t (referred to as the taxonomical entity) of the ACM classification. A resource has therefore only two properties: its owner node n and a taxonomical label t. It is depictured asrn,t. It must be pointed out that two resourcesr1n1,t1andr2n2,t2, unless explicitly r1=r2, are not considered equal, even if n1=n2 and t1=t2. The consequence of such an approach leads to valuing higher those nodes that provide many resources of the same t.The distribution of resources within the network follows strictly the approach by Semant test setup [20]. The resources are evenly distributed among the nodes, as well as among the entities in the taxonomy tree. Additionally, every node is a designated expert in a given field (there can be multiple experts in each field), which is expressed by the composition of resources in it. Of all the resources units in a node, 60% is labeled with the field in which the node is considered an “expert”, further 20% is labeled with another field that is closely related in the taxonomical tree to the expert field, and the last 20% is purely random, but with the restriction to be outside the expert field. This is said to resemble real-world distribution more, reflecting the fact that people have specific interests and hobbies [39].Every query q will only carry one of the ACM classification leaf entities and it will be fully defined by it. In this case, however,q1t1=q2t2iff t1=t2; the benefit of such an approach is to be able to compare results of two queries released at different time points in the testing process and to show relative improvement between them. Routing concept overlay (explained in Section 3.1) for a queryq1t1will be t1 and will remain so during the entire querying process.The resolution of a queryqrtrin a node niconsists of finding all the resources that have been labeled with tr, that is, all the resourcesrr∈r|∃rni,tr.During the evaluation process every node of the network N of size n has a probability of being chosen to generate a query q with the probability of 1/N. In the Semant [19] evaluation setup every node has a probability of 0.1 to generate a query at every time unit. This leads to the conclusion that in order to have an equivalent test to the Semant test of T time units one must execute n×0.1×T sequential iterations. A scale factor of ×10 will be used in order to convert between time units of Semant and iterations used in this work. Every query q will carry a randomly chosen taxonomy entity t with the guarantee that there exists a resource within the network N that is described by t.In Table 2 we summarize the recommended parameters for ACS and Semant algorithms.Within each experiment the obtained data will be processed and presented two-fold.Firstly we will intend to simply plot the data points of all the three measures mentioned in Section 4.2. Due to the large amount of data and its high variability we chose to use simple rolling average of the size 64 as an impulse filter and a data-set compacting method. This will serve as a graphical confirmation of consistency between independent executions; as well as allowing us to formulate initial observations.Secondly we choose to perform statistical analysis to back up the graphical observations. Here, again, we use rolling average in order to limit the amount of data involved in calculations. The process will be performed over all the independent executions within an experiment; having in mind that each configuration is executed three times. The statistical analysis will consist of stating the H0 and H1 hypothesis as follows:-H0: There is no statistically relevant difference between the algorithms ACS, SemAnt and RandomWalker k−2, in terms of Hop per Hit measure.H1: There exists a statistically relevant difference between the algorithms ACS, SemAnt and RandomWalker k−2 in terms of Hop per Hit measure.For the hypothesis’ evaluation we will use the Friedman test, for the mutual comparison between the algorithms and the Wilcoxon signed-rank test method with the Bonferroni correction applied. All the statistical tests will be performed at σ<0.05. Evaluation techniques we apply have been proposed by Derrac et al. in [40] specifically for such cases of studies. The only exception to the above description is the Experiment 1, where we attempt to recreate the Semant's results in terms of Hop per Hit. There is only graphical data provided by the authors of Semant and therefore we must rely on graphical analysis solely.We will use the T-means test to express difference between any given pair of algorithms if necessary.In this section we will analyze the performance of the chosen algorithms in an unstructured world. Firstly we show that we have managed to recreate the results of Semant using our testing platform and, then, we extend the comparison. The topology used in the work [20] is always sem-1024 (see Section 4.1.1 for more details) and all the execution parameters are described in Table 2. In order to make any subsequent results viable we must first demonstrate that the implementation of the environment of Semant is comparable to the results obtained in the original work. To achieve this we have developed a testing platform and applied the strictest details that are provided in the original Semant work.Being able to recreate results presented in [20] leads to the conclusion that the implementation we have created is a correct one albeit there is a slight and irrelevant difference in the results of random k-walker, most likely due to an implementation decision taken by authors and not specified explicitly in their work. Additionally, in this experiment we have included a comparison between these two approaches and our extension of ACS taking advantage of the routing concept idea presented above.

@&#CONCLUSIONS@&#
In this work we have proven several important facts about the use of ant-based methods in the p2p environments. We limited the scope of possibilities by choosing a set of reasonable prerequisites and picked two algorithms (in five variants) of eight taken in consideration. Random behavior was selected as the background and the baseline.As underlying structures we have elected multidimensional hypercubes, toruses and toruses with additional links—in every case the only factor impacting the results was the average degree of the node, which translates directly into the network's link density. There was no perceptible difference between a hypercube of average node degree 10 and torus of average node degree ∼10. This conclusion can be taken a step further. As the average node degree has a highly disproportional influence on the results, only very slightly demonstrating itself in extremes, we state that unexploited underlying topology is irrelevant to the results. On the other hand, the topology-aware hybrid route optimization makes a big difference, scoring results unobtainable in other approaches. Therefore, exploiting the underlying topology is relevant to the results and can have a very positive impact. Similar conclusions, with no empirical backup demonstrated, have been suggested in [12,20].Another conclusion to notice is that the addition of the hybrid path optimization has by far more impact on the results than then original difference between ACS and Semant. One can understand this as a confirmation of the superiority of hybrid methods over slight tweaks in parameters of the classical algorithms. This is a practical implication to the question of ant-based p2p search and must be always taken into consideration when constructing a p2p search mechanism.We have shown that the classical approach of ACS, extended with the routing concept notion, scores better than the elaborate construction of Semant. Under all circumstances it has achieved superior convergence and lower use of system resources. The confrontation of a single ant (ACS) versus multi-ant (Semant) algorithms reveals a profound difference. Multi-ant algorithms, represented by Semant, have the tendency to quickly penetrate the world and seed pheromone values more rapidly. However, as was stated earlier, this process penalizes the results in the long run because the multi-ant mechanism continues to emit additional agents even when there is no need. These unnecessary agents return to their corresponding initial nodes and mostly find no new resources, therefore putting an additional strain on the system for no benefit.In summary, the above conclusions must be taken into account when attempting the construction of a high quality ACO adaptation in the field of p2p.It needs to be pointed out that the notion of quick convergence, as opposed to the quality convergence, might prove to be more useful in dynamic systems. The reasoning is that, even though one algorithm might theoretically reach a better state of convergence, it would never do so due to the environment changing constantly and spoiling what was established; whereas the quick one – although not as good – would keep the average convergence in a better state. This will form the bulk of our future work: examining the behavior of the mentioned algorithms under the strain of variability. It will include resources disappearing and reappearing, nodes reattaching themselves to other points in the system, nodes disconnecting from the system completely, etc.