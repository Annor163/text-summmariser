@&#MAIN-TITLE@&#
Learning vector representation of medical objects via EMR-driven nonnegative restricted Boltzmann machines (eNRBM)

@&#HIGHLIGHTS@&#
We introduce a novel framework called eNRBM to model electronic medical records.Medical objects such as disease and intervention are embedded into a vector-space.The embedding facilitates manipulations and visualization using existing tools.eNRBM learning is guided by clinical structures extracted from EMRs.eNRBM displays factors grouping, and predicts suicide risk better than clinicians.

@&#KEYPHRASES@&#
Electronic medical records,Vector representation,Medical objects embedding,Feature grouping,Suicide risk stratification,

@&#ABSTRACT@&#
Electronic medical record (EMR) offers promises for novel analytics. However, manual feature engineering from EMR is labor intensive because EMR is complex – it contains temporal, mixed-type and multimodal data packed in irregular episodes. We present a computational framework to harness EMR with minimal human supervision via restricted Boltzmann machine (RBM). The framework derives a new representation of medical objects by embedding them in a low-dimensional vector space. This new representation facilitates algebraic and statistical manipulations such as projection onto 2D plane (thereby offering intuitive visualization), object grouping (hence enabling automated phenotyping), and risk stratification. To enhance model interpretability, we introduced two constraints into model parameters: (a) nonnegative coefficients, and (b) structural smoothness. These result in a novel model called eNRBM (EMR-driven nonnegative RBM). We demonstrate the capability of the eNRBM on a cohort of 7578 mental health patients under suicide risk assessment. The derived representation not only shows clinically meaningful feature grouping but also facilitates short-term risk stratification. The F-scores, 0.21 for moderate-risk and 0.36 for high-risk, are significantly higher than those obtained by clinicians and competitive with the results obtained by support vector machines.

@&#INTRODUCTION@&#
Modern electronic medical records (EMRs) have changed the landscape of clinical data collecting and sharing, facilitating efficient care delivery [1]. The data in EMR offers insights into key questions: What are the comorbidity patterns? [2] What are the relationships between diseases and interventions under multimorbidity? What is the risk of adverse events for this patient? [3] However, it remains an open problem in formulating efficient mining techniques to discover these answers [4]. This is partly due to the complexity of the EMR data. The EMR contains a mixture of static, temporal, type-specific data packed in irregular episodes. Huge effort is required for extracting meaningful features [4] and developing prognostic models from EMR [5].We hypothesize that the answers lie in unsupervised learning of EMR representations [4,6]. Unsupervised learning lets clinical patterns emerge through the learning process. We approach the problem by utilizing a recent advancement in deep learning [7,8]. In particular, we adopt restricted Boltzmann machines (RBM) [9] as a generative model of EMR. RBM has a bipartite structure, in which an input layer is connected to a representation layer. The input layer consists of observed clinical variables over multiple periods of time. The representation layer is composed of unobserved binary factors, which act as the underlying aspects of illness and healthcare processes. These aspects jointly generate clinical observables. The RBM transforms raw, high-dimensional and mixed-type EMR data into a homogeneous representation. Clinical objects such as disease, procedure and health trajectory are embedded in the same vector space. The embedding facilitates visualization, manipulation and risk prognosis. See Fig. 1for a graphical illustration of the RBM-based framework.The standard RBM, however, suffers from two key limitations that hinder its usability in the clinical context. First, the embedding coefficients can be either positive or negative, making interpretation of group membership difficult. Second, the RBM assumes unstructured inputs but ignores explicit structures inherent in the EMR, leading to incoherent grouping.We modify the RBM to overcome these limitations. First, the embedding coefficients are constrained to be nonnegative. This leads to model sparsity where only a few embedding coefficients are non-zeros. Each latent factor corresponds to a small group of features which potentially play the role of a derived phenotype. Second, model learning is guided by clinical structures derived from the disease taxonomy, the procedure hierarchy and the temporal progression of illness and care. These two modifications result in a novel model called EMR-driven nonnegative RBM (eNRBM).We validate the proposedeNRBMon a large cohort of 7578 mental health patients in several tasks, including disease/procedure embedding and visualization, comorbidity grouping, and short-term suicidal risk stratification. We demonstrate thateNRBM-based embedding leads to meaningful grouping of diseases and interventions. The merit of the proposed method is highlighted by comparing the predictive performance on risk stratification against support vector machines.The rest of the paper is organized as follows. Section 2 introduces restricted Boltzmann machines. Section 3 presents the main contributions of the paper: (a) an introduction of the RBM as a generative model of the EMR; (b) introducing medical object embedding; (c) introducing nonnegative coefficients into the RBM leading to coherent feature grouping and more compact representations; and (d) adding structural constraints into the RBM by exploiting inherent structures in the EMR. This is followed by an experimental section which demonstrates the capacity of the proposed methods on a large cohort of mental health patients. Finally, Section 5 discusses findings, limitations and future work.Restricted Boltzmann machine (RBM) is a type of neural networks. As illustrated in Fig. 2, a RBM is a bipartite graph consisting of: (i) an input layer of visible units that encode the observables (e.g., disease occurrences), (ii) a latent layer of hidden units, and (iii) weighted connections between every visible unit to hidden units [8,9]. The RBM differs from standard neural networks in important ways. First, it is stochastic rather than deterministic: Variables are randomly distributed according to a joint distribution specified by the model. Second, the network is undirected allowing information to propagate in both directions (feedforward and feedback modes). And finally, learning is unsupervised without labels.Letvdenote the set of visible variables:v=v1,v2,…,vN∈0,1Nandhthe set of hidden factors:h=h1,h2,…,hK∈0,1K. LetW∈RN×Kbe the weight matrix connecting the hidden and visible units. The connection weightWnkmeasures the association strength between the visible unit i and the hidden unit k, that is the tendency of these two units being co-active. The interaction between variables defines an energy function:(1)Ev,h=-a⊤v+b⊤h+v⊤Whwherea,bare the bias coefficients of hidden and visible units, respectively. The model admits the Boltzmann distribution:(2)Pv,h∝e-Ev,hThe RBM is a generative model of data whose density isP(v)=∑hP(v,h).The parameters are often estimated by maximizing the data likelihoodP(v). For example, an update rule for mapping weights is(3)Wik←Wik+ηviρkP∼-vihkPwhereρkrepresentsP(hk=1|v),P∼denotes empirical distribution of the visible data,·Pdenotes expectation with respect to distribution P, and η is learning rate. The data expectationviρkP∼is easy to evaluate. The model expectationvihkPis computationally difficult but can be efficiently approximated by short Markov chains starting from the observationsvin a procedure known as “contrastive divergence” [10].The EMR data broadly consist of two types: static information (such as gender, ethnic background) and healthcare trajectory. The trajectory is recorded as a series of time-stamped events (such as admission, diagnosis or intervention).1Demographic factors such as age, location and income do change over time, but they might be considered as static at the present time if their interaction with clinical variables are not obvious.1We are mainly interested in discrete events and assume that continuous and real-valued data such as EEG signals and blood sugar readings have been discretized through existing methods such as temporal abstraction [11]. Static elements naturally form a vector. The entire trajectory is divided into disjoint intervals of predefined lengths. Events occurring within each interval are aggregated and arranged as a sparse vector. All intervals form a temporal matrix, as illustrated in the data layer of Fig. 1.In RBM-based modeling of EMRs, as illustrated in Fig. 1, all data elements share the same hidden representation layer. The hidden layer is utilized in the tasks of interest (e.g., visualization of patients, diagnosis of a present disease, or prognosis of future risk). Thus, the hidden layer is a mediator between history (recorded illness), present (diagnosis) and future (prognosis). It “explains” the data through:(4)P(vi1|h)=σai+∑kWikhkwherevi1representsvi=1, andσ(x)=1+e-x-1. As all hidden units jointly represent the data, the representation is said to be fully distributed. This makes the representation highly compact: The model can be considered as a giant mixture of2Kcomponents with onlyKN+K+Nparameters.This mixture view is attractive because healthcare is a complex process, and the recorded events are the result of interaction between multiple processes (e.g., the underlying illness, comorbidity, diagnostic decision and intervention), each of which can be captured by one or more hidden units.The RBM embeds medical objects (e.g., diagnosis codes) and health trajectories into a vector space. Each object i is represented by a row vectorWi•inRK. The vector embedding facilitates algebraic manipulations such as similarity calculation and retrieval, translation and rotation, and 2D projection for visualization. See Fig. 4 for an example of diseases embedded in 2D. An entire health trajectory can also be represented in the same space through probabilistic projection:(5)ρk=Phk=1|v=σbk+∑iWikviwhereσxis the sigmoid function defined in Eq. (4). The posterior vectorρ=ρ1,ρ2,…,ρKrepresents the entire patient trajectory. This can then be used for classification and prognosis (see Section 4.5 for a demonstration).For a typical EMR, a practical issue arises since the input features are not binary but counts. We employ a simple solution: features are normalized into the range[0,1]and treated as empirical probability. A more theoretical drawback is that the RBM is not effective in organizing features, and does not takes the inherent structures of the EMR into account. In what follows, we show how to modify RBM to tackle these problems.This subsection presents modifications to RBMs for promoting the grouping of features and enhancing interpretability. We introduce two constraints into the parameter structure: nonnegative weights and EMR-driven smoothness, resulting in a novel model called EMR-driven nonnegative RBM (eNRBM).The first modification is to constrain the connection weightsWikto be nonnegative. To enforce nonnegativity, we augmented the data log-likelihoodlogP(v)with a barrier functionBWik=Wik2ifWik<0and 0 otherwise. Minimizing the augmented log-likelihood would drive negative weights toward zeros.This leads to several interesting properties. First, the mapping matrixWis sparse, that is, only few elements are non-zeros. Second, hidden factors must “compete” to generate data, and thus creating an “explaining away” effect (where only a few latent factors are plausible explanation of the data). The result is a parts-based representation where each hidden unit is responsible to explain a part of the EMR [12].The “explaining away” effect also leaves some hidden units unused (with near-zero mapping weight vectorsW•k). Thus it offers a natural way to estimate the intrinsic dimensionality of the data. A hidden unit k is declared “dead” ifW•k11N-1⩽τfor small τ. This capacity is not seen in standard RBMs.The other modification is based on the inherent structures in the EMR. Due to the progressive nature of health, events often repeat over time. Thus, a disease occurring in consecutive time-intervals results in related features. Other structures are in the hierarchical organization of diseases and interventions, including the disease taxonomy ICD-102http://apps.who.int/classifications/icd10.2and the procedure cube ACHI.3https://www.aihw.gov.au/procedures-data-cubes/.3For example, two diseases that share the same parent in the taxonomy, by definition, possess similar characteristics.Here we introduce a novel regularization scheme to realize these structures. Assume that the structures can be encoded into a feature graph G whose edges indicate the relatedness between features. Letγij>0be the relation strength between feature i and j, the relatedness can be realized by minimizing the following smoothness objective:(6)Ω(W)=∑ijγij∑kWik-Wjk2In model estimation, this objective is added to the data log-likelihood, in addition to the nonnegativity constraint mentioned above. The details are presented in Appendix A.In our implementation, we construct the feature graph as follows. An edge is created if any of the following requirements are met:•Two codes share the same two-character prefix. In particular, we use the first two numbers or letters (using ICD-10 for diseases, and ACHI for procedures). For example, F10 (mental disorder due to alcohol) and F17 (mental disorder due to tobacco) are linked since they are children of F1 (mental disorders due to psychoactive substance use). However, F10 and F20 (schizophrenia) do not share a direct relation. We feel that this balances well between the relatedness and specificity of the disease classification.A code is recorded in consecutive intervals. For example, if F10 is recorded in [0–3] months and [3–6] months prior to a specified date, this constitutes an edge. This is because two close events of the same type would behave similarly.Our focus is on mental health patients who were under assessment for suicidal risk. Mental health is a global burden that accounts for 14% of the world health expenditure [13]. Among mental health problems, suicidal risk is devastating: suicidal thoughts occur in 10% of the population in their lifetime [14], and suicide attempts happen in 0.3% of the population each year [15]. The risk of suicide has led to mandatory assessments. However, suicide risk assessments are often inaccurate leading to concern over practicality [16,17].We used a mental health cohort previously extracted from Barwon Health, a large regional hospital in Australia [18,19]. Data was collected between January 2009 and March 2012. The dataset contains 7578 patients (49.3% male, 48.7% under 35) who underwent collectively 17,566 assessments. Any patient who had at least one encounter with the hospital services and one risk assessment was included. Most patients had one assessment (62%), but 3% of patients had more than 10 assessments. Diagnoses are coded using ICD-10. More details are described in [19].Each assessment was considered as a data point from which a prediction would be made. The future outcomes within 3months following an assessment were categorized into three ordinal levels of risk according to [18]: no-risk, moderate-risk (non-fatal consequence), and high-risk (fatal consequence). The risk classes were decided using a look-up table from the ICD-10 codes. If there were more than one outcome classes, the highest risk class would be chosen. There were 15,272 (86.9%) no-risk outcomes, 1436 (8.2%) moderate-risk and 858 (4.9%) high-risk.

@&#CONCLUSIONS@&#
