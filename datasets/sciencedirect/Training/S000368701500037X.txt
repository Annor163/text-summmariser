@&#MAIN-TITLE@&#
Movement transformation on multi-touch devices: Intuition or instructional preparation?

@&#HIGHLIGHTS@&#
We evaluate the usability of a touch pad device.An action-regulatory framework is introduced.Users cannot generalize gestures between touch pads and touch screens.

@&#KEYPHRASES@&#
Intuitive interaction,Multi-touch,Input devices,

@&#ABSTRACT@&#
Multi-touch technology is a key part of computer interaction today, yet little is known about the distinction between direct and indirect input devices in terms of intuitive interaction. An experimental study aims to identify the difficulties of interaction with indirect multi-touch devices by applying the action regulation theory and the principle of movement transformation to common computer tasks involving gesture utilization. An analysis of the data acquired from 54 subjects working with an Apple Magic Trackpad implies that gestures on indirect multi-touch devices are not utilized intuitively without instructions that bypass conceptual difficulties of indirect gesture usage. It is shown that gesture use influences product assessment measured by User Experience questionnaires and that prior experience with direct multi-touch devices does not influence gesture usage or product assessment. We advise that product developers utilize video instructions to create a sense of intuitive interaction.

@&#INTRODUCTION@&#
Since multi-touch technology has been incorporated into most brands and models of cell phones, tablets, even public transportation ticket machines, it is an omnipresent issue in everyday life and becomes growingly important in the working environment. For example, the recent push by Microsoft towards touch-controlled or at least touch-optimized interfaces with the current and upcoming iterations of their operating system Windows makes the ability to use these devices important and worth investigating. Multi-touch technology has often been declared as more intuitive than more traditional input devices (Apple Inc., 2014a; Apple Inc., 2014b). Multi-touch gestures can be defined as movements with two or more fingers touching a touch screen or pad and creating more than one point of contact at a time (Han, 2006).At this point it becomes crucial to separate touch screen devices and touch pads, as are common today in laptops and as stand-alone input devices for computers in general. While touch screens have no separation between the area of execution and the area of effect (with their unique drawbacks because of it), many touch pads involve the same gesture functionality one is accustomed to on touch screens, but separating the area of execution and the area of effect. Developers don't currently seem to separate the two different devices, it rather seems they want to treat the operational concepts as equal (Apple Inc., 2014b). An inherent shortcoming of touch pads, the user has to be cognitively aware of one differentiation to touch screens no matter the situation: that his/her actions will not take place at the same space they were performed. Movement transformation in order to perform an action is hindered by this obstacle, quite possibly to a point where multi-touch gestures, close to reality in their interaction with virtual instead of physical objects, might lose their appeal towards the user. Similar results have already been obtained by (Schmidt et al., 2009) with focus on multi-touch input for large surfaces.Both multi-touch and traditional input devices are subject to the same regulatory principles of cognitive and motor action on the side of the user, as described in the action regulation theory (Hacker, 1994). It specifies three levels of regulation, namely the intellectual, the perceptive-conceptual and the sensorimotor level. Given the different task scenarios of multi-touch and traditional input devices, one can assign stages of device usage to the levels of action regulation theory. A simple gesture on a multi-touch device for example can be broken down into the different levels as is shown in Table 1.According to the TOTE-model (Miller et al., 1973), the user will undergo a loop of indicative and executive processes until the desired outcome is achieved. If that qualitative (i.e. “This is not the page I want to see”) or quantitative goal (i.e. “I am still three pages away from the page I want to see”) is not accomplished, the user will perform transformations of the implemented movement variables. (Sangals, 1997) defines the transformation of sensorimotor variables as a key part of action since it is most commonly influenced by specific types of sensorimotor motion, either using endogenous bodily movements or tools to help achieve task completion. The reference also specifies motion as a tool itself to achieve verbal, cognitive or even social goals.The theoretical framework presented above provides researchers and usability engineers with a focus on the movement transformations required to be conducted on the side of the user. The movement transformation loops of the TOTE-model require constant comparisons between the status quo and the desired outcome, even more so for complex tasks (Palinko et al., 2010). One can infer that in order to achieve complex goals, the user will be cognitively involved in the process of carrying out the task as long as it integrates more than merely sensorimotor levels of action. As far as we know, no competing framework has been proposed in the literature that involves different levels of action regulation or includes hierarchical cognitive functioning in research on intuitive interaction.For gesture-controlled devices, (George, 2010) specifies a conversation metaphor in which the interface is “a language medium in which the user and system have a conversation about an assumed, but not explicitly represented world”. The authors state that in modern, “natural” user interfaces, symbolic gestures resemble an indirect learning process assigned to triggering an action. In that way, we agree with the authors' reasoning that this puts multi-touch gestures a lot closer to learning command line interface actions or using graphical user interface alternatives than being presented a natural or even intuitive interaction. To use two fingers on a multi-touch surface and rotate them in order to activate the function of rotating an image file on screen for example requires a learning success on the side of the user, just as learning the placement of the rotating functionality in a graphic user interface would. References (Hutchins et al., 1985) and (Jetter et al., 2010) describe ways to implement such learning opportunities in a way that preserves cognitive resources of the user, especially through direct interface manipulation in terms of feedback. The goal is to ease the user into understanding a function on higher levels of the action regulation theory in order to use it correctly. To convey a sense of intuition while using a device, the conceptual difficulty of the functionality to be learned and the user's pre-existing experience of working with such a device have to be taken into account. Intuitive interaction with a product is defined as the unconscious application of knowledge on the side of the user (Mohs et al., 2006). Task difficulty in intuitive interaction can therefore be understood as the amount of effort it takes to transfer knowledge about functionality from one action (i.e. rotating a picture on your desk in real life) to another (i.e. using a rotation gesture on a multi-touch device), without that process turning into conscious knowledge transfer. Whether the user can intuitively utilize an interface functionality can be considered part of the perceptive-conceptual level of action regulation theory.The issue of movement transformation on devices separating the area of execution from the area of effect becomes especially relevant once the transition of prior knowledge is not supposed to take place from real world to virtual applications, but from one virtual world with certain properties to another – this is the case for gesture prior knowledge between touch screen and touch pad devices.Sutter (2006) enhances the concept of perceptive-conceptual difficulty provided here to sensorimotor variables of movement transformation, indicating a difference in sensorimotor difficulty as well as in a perceptional one. The assessment is based on (Heuer, 1983) and describes how paths of movement in various input devices (i.e. trackballs and touchscreens) can lead to different difficulties in movement transformation. In the case of this study, paths of movement between direct and indirect multi-touch devices are identical (meaning that using a zoom gesture on a touch pad is implemented the same way it is on a touch screen, which corresponds to identical paths of movement), hence focus will be laid on perceptive-conceptual difficulty. If different paths of movement were used to trigger the same functionality between direct and indirect devices, one would also need to investigate whether or not one gesture is more effortlessly triggered on a sensorimotor level than the other. An example of a study that does not take paths of movement into account is provided in (Frisch et al., 2009). Here, the authors let subjects choose their preferred gesture on a multi-touch tabletop device, being able to use their fingers (single or multi-touch), a stylus pen or any of the possible combinations they like. Granted, Frisch et al. do not focus on differences on reported usability levels, so their approach is different to ours. It does provide a comparison of hypothetical frameworks though: the authors observed the frequency of gestures that were used. If they also wanted to measure if gestures used more often were also rated more intuitive or usable, their approach would not be able to explain whether differences in usage or usability assessment originated in difficulties on the perceptive-conceptual or sensorimotor level, due to different paths of movement. We consider that differentiation a benefit to our framework. As stated before, our focus lies on the perceptive-conceptual level of action regulation since we can neglect possible differences on the sensorimotor level.It follows from the passages above that touch pads, their interaction with the user being indirect and involving the perceptive-conceptual level of action regulation more strongly, cannot be expected to be as intuitively usable as touch screens with their direct possibility of interaction. We take the view that the difference between area of execution and area of effect could be a key issue in learning if and how multi-touch functionality does not work as well on touch pads than it does on touch screens. Implementing Action Regulation Theory and the TOTE-model, enhanced perceptional difficulty is to be expected in a task while manipulating one area motorically and scanning another one for the effect visually. This article however does not draw a direct comparison between the two device categories, it merely investigates under which circumstances touch pad devices are used intuitively. It is designed as a preliminary study to study the effects of differing areas of execution and effect solely for touch pad devices. Three research questions have been investigated to examine this statement: First, participants were hypothesized to use more multi-touch gestures on a touch pad if they experienced a more “informative” instructional condition (see Method section for further information). If confirmed this hypothesis would establish that touch pads, being interacted with indirectly, are not intuitively used by themselves. Our second hypothesis states that participants in more informative instructional conditions assign higher ratings of intuition and usability to the Magic Trackpad, measured via the INTUI and AttrakDiff questionnaires. The perception of intuitive interaction is separated into four subcomponents by (Ullrich and Diefenbach, 2010): effortlessness, gut feeling, verbalization and magical experience. The four components are measured using the INTUI questionnaire. According to (Hassenzahl et al., 2008), product categories differ with regards to their desired qualities. Pragmatic products for example are judged mainly in terms of their usability and their ability to let the user achieve his/her goals using the product. Products that are perceived as hedonic on the other hand lay emphasis on social appraisal and brand-associated values. (Hassenzahl, 2001) points out that the assessment of pragmatism or hedonism is not mutually excludable – a product can be majorly pragmatic, majorly hedonic or assessed as both. This assessment affects the manifestation of the subcomponents, but not necessarily the overall perceived “intuition” of the product (Ullrich and Diefenbach, 2010). In addition to the INTUI questionnaire, the AttrakDiff 2 mini (Hassenzahl et al., 2008) was handed out to assess the dimensions “Pragmatic Quality”, “Hedonic Quality”, its two subcomponents “Hedonic Quality – Identity” and “Hedonic Quality – Stimulation” and the dimension “Attractiveness”. If confirmed, this hypothesis would make the case for more instruction-oriented marketing approaches when it comes to touch pad devices, otherwise the user might not use the device correctly and (possibly inter-relatedly) not judge it as intuitive, usable or fun to work with. The third hypothesis focuses on prior experience with touch screens and posits that there is no significant correlation between touch screen experience and the use of multi-touch gestures or intuition ratings on a touch pad. This hypothesis should be able to support the claim that touch pads only “work” given a certain level of instruction because transition of prior knowledge from touch screens to touch pads apparently does not happen.For example, a hypothetical subject being accustomed to touch screens might still find himself/herself at a low instructional level, not know how to use the device's multi-touch capabilities and judge it as unintuitive. In this scenario, the low instructional level matches the assumption that users who know how to use touch screen devices do not need any special instruction to transfer their knowledge to touch pad devices. Our hypothetical subject would show that these two domains are not interchangable because he/she could not make use of the gesture functionality the device offers and does not judge it as intuitive.

@&#CONCLUSIONS@&#
