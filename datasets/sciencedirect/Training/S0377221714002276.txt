@&#MAIN-TITLE@&#
Minmax robustness for multi-objective optimization problems

@&#HIGHLIGHTS@&#
Minmax robustness is extended to multi-objective optimization.The concept of minmax robust efficiency is introduced.Weighted sum scalarization and epsilon-constraints are adjusted to the new concept.Problems with objective-wise uncertainty are investigated more closely.The concepts are illustrated with a few LP-examples.

@&#KEYPHRASES@&#
Multi-objective optimization,Robustness and sensitivity analysis,Scenarios,Uncertainty modelling,

@&#ABSTRACT@&#
In real-world applications of optimization, optimal solutions are often of limited value, because disturbances of or changes to input data may diminish the quality of an optimal solution or even render it infeasible. One way to deal with uncertain input data is robust optimization, the aim of which is to find solutions which remain feasible and of good quality for all possible scenarios, i.e., realizations of the uncertain data. For single objective optimization, several definitions of robustness have been thoroughly analyzed and robust optimization methods have been developed. In this paper, we extend the concept of minmax robustness (Ben-Tal, Ghaoui, & Nemirovski, 2009) to multi-objective optimization and call this extension robust efficiency for uncertain multi-objective optimization problems. We use ingredients from robust (single objective) and (deterministic) multi-objective optimization to gain insight into the new area of robust multi-objective optimization. We analyze the new concept and discuss how robust solutions of multi-objective optimization problems may be computed. To this end, we use techniques from both robust (single objective) and (deterministic) multi-objective optimization. The new concepts are illustrated with some linear and quadratic programming instances.

@&#INTRODUCTION@&#
There is a still a gap between theory and practice in optimization, being evident in the fact that optimization methods are still not used for many real-world problems for which they could make an impact. Among others, there are two reasons why mathematical optimization does often not lead to solutions that are applicable in practice: First, most real-world problems are of multi-objective nature, and second, the input data is often not known beforehand. Both aspects have been considered extensively in multi-objective and in robust optimization. However, the combination of both aspects has not been well researched so far.As an example, consider the well known shortest-path problem: In this problem one often has multiple objectives such as the length of the path and its costs (as studied for more than 30years, e.g., Martins, 1984). Additionally, arc lengths might be uncertain (compare, e.g., Aissi, Bazgan, & Vanderpooten, 2009; Yu & Yang, 1998) due to unknown traffic conditions, and costs might be unknown since they depend, e.g., on fuel prices. Shortest path problems have many applications, for example, they can be used to compute passengers’ paths in timetable information systems. Within this setting, multiple criteria refer to the length of the journey, its price, or to the number of transfers, and have been algorithmically treated in Disser, Müller-Hannemann, and Schnee (2008) and Müller-Hannemann and Schnee (2007). On the other hand, timetable information has also been studied under uncertainties which are given by unknown delays in public transportation (Goerigk, Heße, Müller-Hannemann, Schmidt, & Schöbel, 2013; Goerigk, Knoth, Müller-Hannemann, Schmidt, & Schöbel, 2013). However, an uncertain and multi-objective formulation has not been presented so far. In this paper we propose the concept of robust efficiency in order to handle uncertainties in general multi-objective optimization problems.For estimating the effects of uncertainties, it is necessary to evaluate how sensitive an (optimal) solution is to disturbances of the input data. In the literature on single objective optimization, this is often done as an a posteriori step called sensitivity analysis (for an overview see Saltelli, Chan, & Scott (2000)). Sensitivity analysis provides ranges for input data within which a solution remains feasible or optimal. It does not, however, provide a course of action for changing a solution should the disturbance be outside these ranges. In contrast, stochastic programming and robust optimization techniques take the uncertainty into account during the optimization process in single objective optimization. While stochastic programming (see Birge & Louveaux (2011) for an introduction) minimizes the expected objective value of the solution based on a probability distribution of the uncertain data, robust optimization hedges against the worst case. Hence it does not require any probabilistic information. Whether robust or stochastic optimization is the more appropriate way of dealing with uncertainty usually depends on the application.Robust optimization is applied to problems where a solution is required which hedges against all possible scenarios, i.e., realizations of the uncertain input data. For single objective optimization problems, many different approaches to model robustness have been suggested in the literature. Minmax robustness (also called strict robustness) has been first mentioned in Soyster (1973) and has been extensively researched, see, e.g., Ben-Tal et al. (2009) for many results. It requires that a solution should be feasible no matter what happens, i.e., for all possible scenarios, and aims at minimizing the objective function in the worst case. A second prominent concept is to minimize the worst case regret as suggested, e.g., in Kouvelis and Yu (1997). Since both of these concepts are rather conservative, alternative concepts have recently been proposed in the literature. See the overview in Goerigk and Schöbel (2013).As pointed out before, many real world optimization problems require the minimization of multiple conflicting objectives (Stewart et al., 2008), e.g., the minimization of production time versus the minimization of the cost of manufacturing equipment, or the maximization of tumour control versus the minimization of normal tissue complication in radiotherapy treatment design. Naturally, the issue of uncertain data affects these multi-objective optimization problems in the same way that it affects single objective ones. Therefore, being able to find robust efficient solutions would be very valuable in these applications. While the need for dealing with uncertainty in multi-objective optimization has been realized, hardly any of the classical robustness concepts have been extended and applied to multi-objective optimization problems. Most of the research in this area m rather deals with Branke’s concept of robustness for single objective optimization (Branke, 1998). He proposes to replace the objective function f by its mean functionf¯which maps any point x to the average function value in a pre-defined neighbourhood of x. A minimizer off¯is then more robust than a minimizer of f in the sense that the function values in its neighbourhood do not change too much.Based on Branke’s idea for single objective optimization problems, Deb and Gupta (2006) introduce two concepts of robustness for multi-objective optimization problems. The first one replaces all objective functions by their mean functions as Branke (1998) does for single objective optimization problems. Efficient solutions to the resulting optimization problem are called robust solutions of the original problem. Deb and Gupta’s second concept minimizes the original objective functions but adds constraints to the problem that restrict the variation between the original objective functions and their mean functions to a predefined limit. The latter approach proves to be more pragmatic and enables the user to control the desired level of robustness.Barrico and Antunes (2006) extend Deb and Gupta’s first concept by introducing the degree of robustness of a solution. This concept uses a predefined neighbourhood of a feasible solution and measures how much this neighbourhood can be extended without containing solutions whose function values vary from the function value of the original solution by a predefined ratio. Gunawan and Azarm (2005) measure robustness for multi-objective optimization problems by a so called sensitivity region in the space of the uncertain parameters. This region defines the allowed variation of the uncertain parameters without violation of predefined variation constraints of the objective functions. Since the sensitivity region may be asymmetric, Gunawan and Azarm (2005) introduce a worst-case-sensitivity region by fitting a ball of maximal radius around the undisturbed scenario into this region. More detailed overviews of these robustness concepts for multi-objective optimization problems can be found in Steponavice and Miettinen (2012) and Witting (2012).There are not many extensions of the classical concepts of robustness from single objective to multi-objective optimization problems. While in single objective uncertain optimization the concept of minmax robustness has been extensively studied (see, e.g., Ben-Tal & Nemirovski, 1998; Ghaoui & Lebret, 1997; Soyster, 1973) for older contributions and Ben-Tal et al. (2009) for a recent survey, no general extension to multi-objective optimization exists. However, very recently, research interest on minmax robustness for multi-objective optimization can be observed. In a first approach, Kuroiwa and Lee (2012) extended the concept of minmax robustness to multi-objective optimization problems by replacing the objective vector of the uncertain multi-objective optimization problem by the vector consisting of the worst cases of the respective components. Other publications consider similar concepts of robustness. Fliege and Werner (2013) apply the same concept to portfolio selection problems, Yu and Liu (2013) apply it to an uncertain multi-objective game theory problem, and Chen et al. (2012) discuss uncertain multi-objective optimization problems with application in proton therapy for cancer treatment and use the same concept for handling the uncertainties. However, this approach is a bit restrictive and does not completely reflect the variety of multi-objective solutions as we will see later on in this paper.Doolittle, Kerivin, and Wiecek (2012) follow a similar approach as Kuroiwa and Lee (2012). The authors reformulate the uncertain multi-objective optimization problem by replacing each of the objective functions by a new variable and adding additional constraints to the problem formulation (cf. the same approach for single-objective optimization problems in Ben-Tal & Nemirovski (1998)). This approach results in the same concept as presented by Kuroiwa and Lee (2012).Another interpretation of minmax robustness for uncertain multi-objective optimization problems has been introduced by Avigad and Branke (2008) who present an evolutionary algorithm to find solutions to unconstrained uncertain multi-objective optimization problems. They interpret the worst case objective vectors for a given solution to an uncertain multi-objective optimization problem as the set of efficient solutions to the problem of maximizing the objective function at the considered solution over the uncertainty set. The concepts we present in this paper follow a similar approach.In this paper we extend the concept of minmax robustness from single objective to multi-objective optimization in a general way. First applications of our new approach have been described in cutting stock optimization (Ide, Tiedemann, Westphal, & Haiduk, 2013) and in Kuhn, Raith, Schmidt, and Schöbel (2012) where a special bi-objective shortest path problem is considered under uncertainty, and applications in aircraft routing with uncertain weather conditions and transportation of hazardous materials are mentioned.After introducing the necessary preliminaries from multi-objective and robust optimization we define a multi-objective uncertain optimization problem and its robust counterpart in Section ‘Robust counterparts of uncertain multi-objective optimization problems’. In Section ‘Finding robust efficient solutions for uncertain multi-objective optimization problems’ we apply both techniques from robust (single objective) and (deterministic) multi-objective optimization in order to find robust solutions of multi-objective optimization problems. The special case of uncertain multi-objective optimization problems in which the uncertain parameters in the different objective functions are independent of each other is investigated in Section ‘Objective-wise uncertainty’. First insights into the complexity of the concepts of the paper are given in Section ‘Illustrations of robust efficient solutions’ using linear and quadratic optimization instances. The paper ends with some conclusions and suggestions for further research in Section ‘Conclusion’.We recall some notation of multi-objective optimization used in this paper. Multi-objective optimization deals with the problem of minimizing a functionf:X→Rksubject to some constraints defining the feasible setX. A multi-objective optimization problem is given byminf(x)s.t.x∈X.The goal to minimize a vector valued function and the absence of a total order onRkmake it necessary to define the meaning of minimum with respect to weaker ordering relations. In this paper we will use the ordering relations≦,⩽,<, see Ehrgott (2005). Lety1,y2∈Rk. Theny1≦y2ify1is smaller or equal toy2in every component,y1⩽y2ify1is smaller or equal toy2in every component and smaller in at least one component, andy1<y2ify1is smaller thany2in every component. Notice that this implies the equivalence of the relations ⩽ and < inR.Throughout the paper we have to distinguish between these three relations in proofs and theorems. Since most of the proofs and theorems are formulated analogously for each relation, we shorten the text by using a [././.] notation. Furthermore, we define the conesR≧k,R≥kandR>kbyR[≧/≥/>]k≔{x∈Rk:x[≧/⩾/>]0}.Using the ordering relation⩽the goal of a multi-objective optimization problem is to find all feasible solutionsx¯∈Xwhich are efficient, that is, where there is no solutionx∈Xsuch thatf(x)⩽f(x¯). Replacing ⩽ by < or≦,x¯is called weakly efficient and strictly efficient, respectively. Notice that(1)x¯is[strictly/·/weakly]efficient⇔f(x¯)-R[≧/≥/>]kdoesnotcontainanyf(x)withx∈X.This relationship will help us to develop the concept of robust efficiency in Section ‘Robust counterparts of uncertain multi-objective optimization problems’. In Section ‘Finding robust efficient solutions for uncertain multi-objective optimization problems’ we will review some techniques for computing efficient solutions.We also introduce some notation from robust optimization. Given a set of scenarios (also called uncertainty set)U⊆Rm, an uncertain optimization problemP(U)is given as the family(P(ξ),ξ∈U)of optimization problemsP(ξ)minf(x,ξ)s.t.x∈X,wheref:Rn×U→Ris the objective function,X⊆Rnis the feasible set, andξ∈Uindicates particular values for the parameters of the objective function. We callξa scenario andP(ξ)an instance ofP(U).Since an uncertain optimization problem is defined as a family of optimization problems with different objective functions defined by each scenarioξ∈U, we need to clarify how to evaluate a feasible solution, i.e., which feasible solutionsx∈Xare considered robust optimal solutions of the uncertain optimization problem. This depends on the definition of robustness. As indicated in Section ‘Introduction’, many different concepts of robustness, interpreting optimality for an uncertain optimization problem, can be found in the literature. In this paper we concentrate on the concept of minmax robustness introduced by Soyster (1973), and extensively studied by, e.g., Ben-Tal et al. (2009). This concept adopts a pessimistic point of view by trying to hedge against the worst case, i.e., the goal is to minimize the objective function for the worst case scenario over all feasible solutions. Having specified this goal, we can now transform the uncertain optimization problem into its so called robust counterpart which is a deterministic optimization problem. It can be written asminsupξ∈Uf(x,ξ)s.t.x∈X.We call an optimal solution to this problem robust optimal.We note that because the novelty considered in this paper concerns the multiple objectives, we will only consider uncertain objectives and assume the feasible set to be deterministic. We can do this without loss of generality for the problems considered in this paper because as done in the concept of minmax robustness, we only call a solution robust, if it is feasible for every scenario.The goal of this section is to extend the concept of minmax robustness introduced in Section ‘Uncertain optimization problems and minmax robustness’ to multi-objective optimization problems. Assume that we are given a multi-objective optimization problemminf(x)s.t.x∈Xwith an objective functionf:Rn→Rkand a feasible setX⊂Rn. As in the single objective case in Section ‘Uncertain optimization problems and minmax robustness’, we assume that the objective function f may depend on scenariosξwhich are unknown or uncertain. As in uncertain single objective optimization, given an uncertainty setU⊆Rm, an uncertain multi-objective optimization problemP(U)is given as the family(P(ξ),ξ∈U)of multi-objective optimization problemsP(ξ)minf(x,ξ)s.t.x∈X,with objective functionf:Rn×U→Rkand feasible setX⊆Rn. Again we callξ∈Ua scenario andP(ξ)an instance ofP(U).Obviously for|U|=1,P(U)reduces to a (deterministic) multi-objective optimization problem, whereas fork=1,P(U)becomes an uncertain single objective optimization problem. Throughout the paper we will use these two special cases to justify and compare our concepts and results.Given an uncertain multi-objective optimization problemP(U), the same question as in single objective optimization arises, namely, how to evaluate feasible solutionsx∈X. In uncertain multi-objective optimization problems, we cannot evaluate solutions by just taking the worst case over all scenarios because we obtain a vector of objective values for each scenario. The set of objective values of x isfU(x)≔{f(x,ξ):ξ∈U}⊆Rk.With this notion we are able to extend the definition of efficiency as given in (1).Recall that we call a solutionx∈X[strictly/·/weakly] efficient for a deterministic multi-objective optimization problem if there exists no other solutionx¯∈X,x≠x¯such thatf(x)∈f(x¯)-R[≧/≥/>]k. A straightforward extension of this concept is as follows.We call a feasible solutionx¯∈Xrobust efficient, if the setfU(x¯)-R≥kdoes not contain any other setfU(x)withx≠x¯∈X. Thus, we consider all possible objective values of a solutionx¯under all the various scenarios, namely the setfU(x¯). If this set does lie to the “upper right” of the set of all possible realizationsfU(x)of another solution x (and hence is worse), we call the according solutionx¯robust efficient. Formally, we have Definition 3.1.Definition 3.1Robust efficiencyGiven an uncertain multi-objective optimization problem we call a feasible solutionx¯∈X•robust weakly efficient (from now on rwe), if there is nox∈X⧹{x¯}such thatfU(x)⊆fU(x¯)-R>k;robust efficient (from now on re), if there is nox∈X⧹{x¯}such thatfU(x)⊆fU(x¯)-R≥k;robust strictly efficient (from now on rse), if there is nox∈X⧹{x¯}such thatfU(x)⊆fU(x¯)-R≧k.In deterministic multi-objective optimization there is a relationship between weak efficiency, efficiency and strict efficiency. This relationship also holds for robust efficiency.Lemma 3.2LetP(U)be an uncertain multi-objective optimization problem. Then we have:x¯isrse⇒x¯isre⇒x¯isrwe.The robust counterpart of an uncertain multi-objective optimization problem is the problem of identifying allx∈Xwhich are rwe, re, or rse. We denote this analogously to the robust counterpart of a single objective optimization problem, namely asminsupξ∈Uf(x,ξ)s.t.x∈X,wheresupξ∈Uf(x,ξ)is defined as the set of [weakly/·/strictly] efficient solutions of the multi-objective maximization problemmaxf(x,ξ)s.t.ξ∈U.Thus Definition 3.1 can be seen as a connection between the interpretation of the supremum of a set as a set itself, namely as the set of its non-dominated points with regard to multi-objective maximization (as in Löhne (2011)), and a particular set ordering, namely calling a setfU(x)dominatingfU(x¯), if it is contained infU(x¯)-R[≧/≥/>]k.We illustrate Definition 3.1 with a small example.Example 3.3Consider Fig. 1. The left picture refers to some uncertain multi-objective optimization problem with feasible setX={x1,…,x5}, i.e. consisting of only five feasible solutions. The five setsfU(x1),…,fU(x5)are depicted as polygons. The picture in the middle shows the sets of their suprema. By adding-R≧kto each of these sets, in the right picture we can see that none offU(x1)-R≧k,fU(x2)-R≧k,fU(x3)-R≧kandfU(x5)-R≧kdoes contain any other setfU(xi), thusx1,x2,x3andx5are all robust strictly efficient and hence robust efficient.fU(x4)-R>kon the other hand containsfU(x1),fU(x2)andfU(x3), thusx4is not robust weakly efficient, and hence also not robust efficient.Before we analyze properties of our definition of robust efficiency we prove some inclusions which are used throughout this paper.Lemma 3.4Given an uncertain multi-objective optimization problemP(U).(a)For allx′,x¯∈XfU(x′)⊆fU(x¯)-R[≧/≥/>]k⇔fU(x′)-R≧k⊆fU(x¯)-R[≧/≥/>]k.For allx′,x¯∈XfU(x′)-R≧k⊆fU(x¯)-R[≧/≥/>]k⇔∀ξ∈U∃η∈U:f(x′,ξ)[≦/⩽/<]f(x¯,η).For allx′,x¯∈XfU(x′)-R≧k⊆fU(x¯)-R≧k⇒supξ∈Ufi(x′,ξ)≦supη∈Ufi(x¯,η)∀i∈{1,…,k}.Ifmaxξ∈Ufi(x,ξ)exists for allx∈X, then for allx′,x¯∈XfU(x′)-R≧k⊆fU(x¯)-R>k⇒maxξ∈Ufi(x′,ξ)<maxη∈Ufi(x¯,η)∀i∈{1,…,k}.These inclusions are needed in proofs throughout the paper. Lemma 3.4(a) and (b) together provide the insight, that a solution x is considered worse than another solutionx′if and only if for every scenarioξ∈Uthere is a scenarioη∈Uin whichx¯attains a worse objective value thanx′inξ. Lemma 3.4(c) and (d) basically mean that the anti-ideal point (i.e., the point composed of the respective suprema of the components overU) offU(x′)is considered better than the anti-ideal point offU(x¯)iffU(x′)is considered better thanfU(x¯).We now provide the technical proof of Lemma 3.4:Proof of Lemma 3.4(a)“⇐” holds sincefU(x′)⊆fU(x′)-R≧k.“⇒” For everyξ∈Uandμ∈R≧kwe have to showf(x′,ξ)-μ∈fU(x¯)-R[≧/≥/>]k.SincefU(x′)⊆fU(x¯)-R[≧/≥/>]kalsof(x′,ξ)∈fU(x¯)-R[≧/≥/>]k. Together withμ∈R≧kf(x′,ξ)-μ∈fU(x¯)-R[≧/≥/>]k.“⇒” Assume the contrary. Then∃ξ∈U∀η∈U:f(x′,ξ)[≦/≰/≮]f(x¯,η)⇒∃ξ∈U∀η∈U:f(x′,ξ)∉fU(x¯)-R[≧/≥/>]k⇒fU(x′)-R≧k⊈fU(x¯)-R[≧/≥/>]k.“⇐” Assume the contrary. Then∃ξ∈U∃μ∈R≧k:f(x′,ξ)-μ∉fU(x¯)-R[≧/≥/>]k⇒∃ξ∈U:f(x′,ξ)∉fU(x¯)-R[≧/≥/>]k⇒∃ξ∈U∀η∈U:f(x′,ξ)[≦/≰/≮]f(x¯,η).Assume the contrary. Then there exists ani∈{1,…,k}such thatsupξ∈Ufi(x′,ξ)>supη∈Ufi(x¯,η). Thus, there actually exists a scenarioξ∗∈Usuch thatfi(x′,ξ∗)>supη∈Ufi(x¯,η). But this means∀η∈U:f(x′,ξ∗)≦f(x¯,η),which is a contradiction to (b).Assume the contrary. Then there exists ani∈{1,…,k}such thatmaxξ∈Ufi(x′,ξ)⩾qmaxη∈Ufi(x¯,η). Moreover, since the inequality is strict, there actually exists a scenarioξ∗∈Usuch thatfi(x′,ξ∗)⩾qmaxη∈Ufi(x¯,η). But this means∀η∈U:f(x′,ξ∗)≮f(x¯,η),which is a contradiction to (b).□Next, we show that for|U|=1andk=1, i.e. in the deterministic multi-objective case and the uncertain single objective case, Definition 3.1 coincides with the definition of efficiency and robust optimality, respectively.Lemma 3.5GivenP(U)with|U|=1. Then•x¯∈Xis re if and only if it is efficient,x¯∈Xis rse if and only if it is strictly efficient,x¯∈Xis rwe if and only if it is weakly efficient.Let|U|=1. Thenx¯isr[s/·/w]e⇔3.4.a∄x′∈X⧹{x¯}:fU(x′)-R≧k⊆fU(x¯)-R[≧/≥/>]k⇔∄x′∈X⧹{x¯}:f(x′)-R≧k⊆f(x¯)-R[≧/≥/>]k⇔3.4.a∄x′∈X⧹{x¯}:f(x′)∈f(x¯)-R[≧/≥/>]k⇔x¯is[strictly/·/weakly]efficient.□GivenP(U)withk=1. Then(a)x¯∈Xis re forP(U)if and only if it is rwe.Ifx¯∈Xis rwe forP(U), it is a robust optimal solution.Ifx¯∈Xis the unique robust optimal solution, it is rse forP(U).Ifmaxξ∈Uf(x′,ξ)exists for allx′∈Xthenx¯is re forP(U)if and only if it is a robust optimal solution and rse forP(U)if and only if it is the unique robust optimal solution.(a)Holds sinceR≥=R>by definition.x¯isrwe⇔3.4.a∄x′∈X⧹{x¯}:fU(x′)-R≧⊆fU(x¯)-R>⇒k=1∄x′∈X⧹{x¯}:supξ∈Uf(x′,ξ)<supξ∈Uf(x¯,ξ)⇔x¯isarobustoptimalsolution.x¯istheuniquerobustoptimalsolution⇔∄x′∈X⧹{x¯}:supξ∈Uf(x′,ξ)⩽supξ∈Uf(x¯,ξ)⇒3.4.c∄x′∈X⧹{x¯}:fU(x′)-R≧⊆fU(x¯)-R≧⇔x¯isrse.Ifmaxξ∈Uf(x′,ξ)exists for allx′∈X, then∄x′∈X⧹{x¯}:fU(x′)-R≧⊆fU(x¯)-R≧⇒k=1∄x′∈X⧹{x¯}:maxξ∈Uf(x′,ξ)⩽maxξ∈Uf(x¯,ξ)and∄x′∈X⧹{x¯}:maxξ∈Uf(x′,ξ)<maxξ∈Uf(x¯,ξ)⇒3.4.d∄x′∈X⧹{x¯}:fU(x′)-R≧⊆fU(x¯)-R>.□Lemmas 3.5 and 3.6 show that our concept of robust efficiency is sound, because in the cases where the problem reduces to a deterministic multi-objective optimization problem or to an uncertain single objective optimization problem, respectively, our new definitions turn out to be the common definitions of efficiency and robust optimality.Having introduced the definition of robust efficient solutions for uncertain multi-objective optimization problems in Section ‘Robust counterparts of uncertain multi-objective optimization problems’, we now apply knowledge from deterministic multi-objective optimization (see Section ‘(Deterministic) multi-objective optimization’) and single objective robust optimization (see Section ‘Uncertain optimization problems and minmax robustness’) in order to find robust efficient solutions to an uncertain multi-objective optimization problemP(U). Throughout this section we show that insights from deterministic multi-objective optimization can be extended to robust multi-objective optimization.The most common approach to computing efficient solutions for a deterministic multi-objective optimization problem is the weighted sum scalarization. The general idea is to form a single objective optimization problem by multiplying each objective function with a non-negative weight and summing up the weighted objectives. The weighted sum problemWP(λ)for a given (deterministic) multi-objective optimization problemPand a weight vectorλ∈R≥kisWP(λ)min∑i=1kλifi(x).s.t.x∈XOptimal solutions ofWP(λ)are efficient solutions ofPdepending on assumptions onλand uniqueness of the solution.Theorem 4.1see Ehrgott (2005), Theorem 3.6Given some multi-objective optimization problemP, letWP(λ)be its weighted sum scalarization. Ifx¯∈Xis a [·/·/unique] minimizer ofWP(λ)for someλ∈R[≥/>/≥]k, thenx¯is [weakly/·/strictly] efficient forP.We now generalize the weighted sum scalarization by reducing the robust counterpart of an uncertain multi-objective optimization problem to a single objective uncertain optimization problem in order to be able to compute robust efficient solutions by computing robust optimal solutions of the uncertain single objective problem. To this end, we introduce the robust version of the weighted sum scalarization problem of an uncertain multi-objective optimization problemP(U)asWP(U)(λ)minsupξ∈U∑i=1kλifi(x,ξ).s.t.x∈XWe again investigate the special casesk=1and|U|=1.Remark 4.2Ifk=1,WP(U)(λ)is the robust counterpart of the single objective optimization problemWP(λ). If|U|=1,WP(U)(λ)is identical toWP(λ).Motivated by the second part of Remark 4.2 we extend Theorem 4.1 to uncertain multi-objective optimization problems and we do indeed find a similar result.Theorem 4.3Given an uncertain multi-objective optimization problemP(U), the following statements hold.(a)Ifx¯∈Xis the unique optimal solution toWP(U)(λ)for someλ∈R≥k, thenx¯is rse forP(U).Ifx¯∈Xis an optimal solution toWP(U)(λ)for someλ∈R>kandmaxξ∈U∑i=1kλifi(x,ξ)exists for allx∈X, thenx¯is re forP(U).Ifx¯∈Xis an optimal solution toWP(U)(λ)for someλ∈R≥kandmaxξ∈U∑i=1kλifi(x,ξ)exists for allx∈X, thenx¯is rwe forP(U).Assumex¯is not r[s/·/w]e forP(U). Then there exists anx′∈Xsuch thatfU(x′)⊆fU(x¯)-R[≧/≥/>]k⇒3.4.a&b∀ξ∈U∃η∈U:f(x′,ξ)[≦/⩽/<]f(x¯,η).Now chooseλ∈R[⩾/>/⩾]karbitrary but fixed.⇒∀ξ∈U∃η∈U:∑i=1kλifi(x′,ξ)[≦/</<]∑i=1kλifi(x¯,η)⇔∀ξ∈U:∑i=1kλifi(x′,ξ)[≦/</<]supη′∈U∑i=1kλifi(x¯,η′)⇔supξ′∈U∑i=1kλifi(x′,ξ′)[≦/</<]supη′∈U∑i=1kλifi(x¯,η′).The last equivalence holds because for (b) and (c)maxξ′∈U∑i=1kλifi(x′,ξ′)exists. But this means thatx¯is not [the unique/an/an] optimal solution toWP(U)(λ)forλ∈R[⩾/>/⩾]k. □This leads to our first method for computing r[s/·/w]e solutions to the uncertain multi-objective optimization problemP(U).Method 1: Weighted sum scalarization for uncertain multi-objective optimizationInput:Uncertain multi-objective problemP(U), solution setsSOLrse=SOLre=SOLrwe=∅.Choose a setΛ⊆R≥k.IfΛ=∅: STOP. Output: Set of rse solutionsSOLrse, set of re solutionsSOLreand set of rwe solutionsSOLrwe.Chooseλ∈Λ, setΛ≔Λ⧹{λ}.Find an optimal solutionx¯toWP(U)(λ).(a)Ifx¯is the unique optimal solution toWP(U)(λ), thenx¯is rse forP(U), thusSOLrse=SOLrse∪{x¯}.Ifmaxξ∈U∑i=1kλifi(x,ξ)exists for allx∈X, thenx¯is rwe forP(U), thusSOLrwe=SOLrwe∪{x¯}.–If furthermoreλ>0, thenx¯is re forP(U), thusSOLre=SOLre∪{x¯}.Go to step 2.Note that the set of robust efficient solutions found by Method 1 depends on the choice ofΛin Step 1. For each of theλ∈Λ, we obtain a new single objective robust optimization problemWP(U)(λ), and the complexity of solvingWP(U)(λ)depends on the structure of the underlying deterministic single objective optimization problem. However, for the special case of an uncertain multi-objective linear programming problemP(U), for eachλ∈Λ,WP(U)(λ)turns out to be a linear single objective optimization problem with 1 additional variable and|U|additional linear constraints.We illustrate the weighted sum method with a brief example that continues Example 3.3.Example 4.5Consider Fig. 2. In both pictures we can see a weight vector for the two objective functions and we want to minimize the supremum of the weighted sum over the setsfU(xi)-R≧k. In the left picture we getx1as the unique optimal solution, in the right picture we getx3as the unique optimal solution. Thusx1andx3arerse. Unfortunately the weighted sum scalarization does not yield allrsesolutions, e.g., in this examplex2is not an optimal solution for any weight vectorλ⩾0.Another scalarization method for computing efficient solutions for deterministic multi-objective optimization problems is the∊-constraint scalarization. We summarize the most important results for this method. LetPbe a deterministic multi-objective optimization problem. Fori∈{1,…,k}and∊∈Rk, the corresponding∊-constraint optimization problem is defined as∊CP(∊,i)minfi(x)s.t.fj(x)≦∊j∀j≠ix∈X.Note that to define∊CP(∊,i), the i-th component of∊is not needed. We do, however, need∊∈Rkin some parts of the following theorems so that we simplify notation and assume∊∈Rk. If we explicitly want to exclude the i-th component from a vector∊we denote this as∊-i∈Rk-1.Theorem 4.6see Ehrgott (2005), Propositions 4.3, 4.4, Theorem 4.5LetPbe a deterministic multi-objective optimization problem with objective function f and feasible setX.(a)Ifx¯∈Xis an optimal solution to∊CP(∊,i)for some∊∈Rkandi∈{1,…,k}, then it is weakly efficient forP.Ifx¯∈Xis the unique optimal solution to∊CP(∊,i)for∊∈Rkand somei∈{1,…,k}, then it is strictly efficient forP.We now use the∊-constraint approach to reduce a multi-objective uncertain optimization problem to a single objective uncertain optimization problem. To this end we define the robust∊-constraint version∊CP(U)(∊,i)ofP(U), thus an∊-constraint problem for an uncertain multi-objective optimization problem:∊CP(U)(∊,i)minsupξ∈Ufi(x,ξ)s.t.fj(x,ξ)≦∊j∀j≠i,∀ξ∈Ux∈X.We now extend the results of Theorem 4.6 to uncertain multi-objective optimization problems.Theorem 4.7Given an uncertain multi-objective optimization problemP(U), the following statements hold.(a)Ifx¯∈Xis the unique optimal solution to∊CP(U)(∊,i)for some∊∈Rkand somei∈{1,…,k}, thenx¯is rse forP(U).Ifx¯∈Xis an optimal solution to∊CP(U)(∊,i)for some∊∈Rkand somei∈{1,…,k}andmaxξ∈Ufi(x,ξ)exists for allx∈X, thenx¯is rwe forP(U).(a)Assume thatx¯is notrseforP(U). Then there exists anx′∈Xsuch thatfU(x′)⊆fU(x¯)-R≧k⇒3.4.a&b∀ξ∈U∃η∈U:f(x′,ξ)≦f(x¯,η)⇒supξ∈Ufi(x′,ξ)≦supη∈Ufi(x¯,η)and∀ξ∈U∃η∈U:f-i(x′,ξ)≦f-i(x¯,η)≦∊-i.But thenx′is feasible for∊CP(U)(∊,i)and has an equal or better objective value thanx¯. This is a contradiction to the assumption thatx¯is the unique optimal solution to∊CP(U)(∊,i).Assume thatx¯is notrweforP(U). Then there exists anx′∈Xsuch thatfU(x′)⊆fU(x¯)-R>k⇒3.4.a&b∀ξ∈U∃η∈U:f(x′,ξ)<f(x¯,η)⇒maxξ∈Ufi(x′,ξ)<maxη∈Ufi(x¯,η)and∀ξ∈U∃η∈U:f-i(x′,ξ)<f-i(x¯,η)≦∊-i.But thenx′is feasible for∊CP(U)(∊,i)and has a better objective value thanx¯. This is a contradiction to the assumption thatx¯is an optimal solution to∊CP(U)(∊,i). □Theorem 4.7 leads to our second method for computing r[s/w]e solutions to the uncertain multi-objective optimization problemP(U).Method 2:∊-constraint method for uncertain multi-objective optimizationInput:Uncertain multi-objective problemP(U), solution setsSOLrse=SOLrwe=∅.Choose a setE⊆Rk.IfE=∅: STOP. Output: Set of rse solutionsSOLrseand set of rwe solutionsSOLrwe.Choose∊∈E, setE≔E⧹{∊}.For everyi∈{1,…,k}, find an optimal solutionx¯ito∊CP(U)(∊,i).(a)Ifx¯iis the unique optimal solution to∊CP(U)(∊,i), thenx¯iis rse forP(U), thusSOLrse≔SOLrse∪{x¯i}.Ifmaxξ∈Ufi(x,ξ)exists for allx∈X, thenx¯iis rwe forP(U), thusSOLrwe≔SOLrwe∪{x¯i}.Go to Step 2.Note that the set of robust efficient solutions found by Method 2 depends on the choice ofEin Step 1. For each of the∊∈E, we obtain k new uncertain single objective optimization problems∊CP(U)(∊,i)(i=1,…,k). Clearly, the complexity of solving these∊CP(U)(∊,i)problems depends on the structure of the underlying deterministic single objective optimization problem. However, considering once again the special case of an uncertain multi-objective linear programming problemP(U), we can see that for each∊∈Eand for eachi∈{1,…,k},∊CP(U)(∊,i)turns out to be a single objective linear optimization problem with 1 additional variable andk·|U|additional linear constraints.We again use Example 3.3 to illustrate the results of Theorem 4.7.Example 4.9Consider Fig. 3. In the left picture we see thatx1andx2are robust efficient as optimal solutions to minimizingf2subject to two different upper bounds onf1. The right picture shows thatx5is robust efficient as optimal solution to minimizingf1subject to an upper bound onf2. Unfortunately, and contrary to what we know from deterministic multi-objective optimization, the∊-constraint method does not find all rwe solutions. Here it is not possible to identifyx3as robust efficient becausex2is better thanx3in the supremum of every component and feasible for∊CP(U)(∊,i)wheneverx3is feasible.Methods 1 and 2 are adaptations of well known scalarization methods from multi-objective optimization to uncertain multi-objective optimization problems. Therefore they require the solution of a single objective uncertain problem for each setting of the parametersλ, respectively∊and i. The next method in Section ‘Approach 3: Objective-wise worst case’ on the other hand, is the adaptation of a robust optimization technique to the uncertain multi-objective optimization problemP(U)and results in the necessity to solve a deterministic multi-objective optimization problem.Instead of interpreting the supremum in the robust counterpart of our uncertain multi-objective optimization problemP(U)as a multi-objective maximization problem, we can also interpret it as a point rather than a set. Doing so we formulate a new problem, the objective-wise worst case problemOWCP(U)minfUowc(x),s.t.x∈X,wherefUowc(x)≔supξ∈Uf1(x,ξ)supξ∈Uf2(x,ξ)⋮supξ∈Ufk(x,ξ).OWCP(U)is the (deterministic) multi-objective minimization of the objective-wise supremum. This interpretation has been introduced as a first concept of robustness for multi-objective optimization problems by Kuroiwa and Lee (2012). It has the obvious advantage that computingfUowc(x)for given x is much easier than solving a multi-objective optimization problemmax{f(x,ξ):ξ∈U}as before: It only involves solving k deterministic single objective optimization problems.OWCP(U)then is a deterministic multi-objective optimization problem and can be solved with any method of deterministic multi-objective optimization. In other words, using theOWC-method implies that rather than considering set dominance as required in Definition 3.1, we can consider the standard dominance of points (the objective wise worst case scenarios). This reduces the uncertain multi-objective optimization problem to a deterministic one.Remark 4.10(a)For|U|=1OWCP(U)reduces to a classical deterministic multi-objective optimization problem.Fork=1OWCP(U)reduces to the (single objective) robust counterpart ofP(U).We now analyze how optimal/robust solutions ofOWCP(U)are related to robust efficient solutions as defined in Section ‘Robust counterparts of uncertain multi-objective optimization problems’.Theorem 4.11(a)Letx¯∈Xbe a strictly efficient solution toOWCP(U). Thenx¯is rse forP(U).Letmaxξ∈Ufi(x,ξ)exist for alli∈{1,…,k}and allx∈X. Letx¯be a weakly efficient solution toOWCP(U). Thenx¯isrweforP(U).(a)Assume thatx¯is not rse forP(U). Then there exists anx′∈Xsuch thatfU(x′)⊆fU(x¯)-R≧k⇒3.4.a&b∀ξ∈U∃η∈U:f(x′,ξ)≦f(x¯,η)⇒supξ∈Ufi(x′,ξ)≦supη∈Ufi(x¯,η)∀i∈{1,…,k}.But the latter means thatfUowc(x¯)is equal to or dominated byfUowc(x′), thusx¯is not strictly efficient forOWCP(U).Assume thatx¯is notrweforP(U). Then there exists anx′∈Xsuch thatfU(x′)⊆fU(x¯)-R>k⇒3.4.a&b∀ξ∈U∃η∈U:f(x′,ξ)≦f(x¯,η).Sincemaxξ∈Ufi(x,ξ)exists for alli=1,…,kand allx∈X, this impliesmaxξ∈Ufi(x′,ξ)<maxη∈Ufi(x¯,η)∀i=1,…,k.But the latter means thatfUowc(x¯)is strictly dominated byfUowc(x′), thusx¯is not strictly efficient forOWCP(U). □Theorem 4.11 leads to our third method for computing r[w/s]e solutions to the uncertain multi-objective optimization problemP(U).Method 3:OWC-method for uncertain multi-objective optimizationInput:Uncertain multi-objective problemP(U), solution setsSOLrse=SOLrwe=∅.Find a setSOLweof weakly efficient solutions toOWCP(U).IfSOLwe=∅: STOP. Output: Set of rse solutionsSOLrseand set of rwe solutionsSOLrwe.Choosex¯∈SOLwe, setSOLwe≔SOLwe⧹{x¯}.(a)Ifx¯is strictly efficient forOWCP(U), thenx¯is rse forP(U), thusSOLrse≔SOLrse∪{x¯}.Ifmaxξ∈Ufi(x,ξ)exists for alli∈{1,…,k}andx∈X, thenx¯is rwe forP(U), thusSOLrwe≔SOLrwe∪{x¯}.Go to Step 2.We can not estimate the complexity of Method 3 in general since it is highly dependent on the problem structure ofP(U)and the algorithm for solving the deterministic multi-objective optimization problemOWCP(U). E.g., ifP(U)is a linear uncertain multi-objective optimization problem, and the (deterministic) weighted sum method is used for solvingOWCP(U), for each weightλ∈R≥k, we obtain a linear single objective optimization problem with k additional variables andk·|U|additional constraints. If we use the (deterministic)∊-constraint method for solvingOWCP(U), for each∊∈R≥k, we obtain k linear single objective optimization problems, each with 1 additional variable andk·|U|additional constraints.Once again we use Example 3.3 to explain the objective-wise worst case problem.Example 4.13Consider Fig. 4. The strictly efficient solutions ofOWCP(U)arex1,x2andx5, thus they arerse, too. Note that these are exactly the same robust solutions we obtained using the∊-constraint method in Section ‘Approach 2:∊-Constraint scalarization’.Interestingly, the approach of solving the uncertain multi-objective optimization problemP(U)by applying the∊-constraint method toP(U)and then solving a single objective minmax problem yields the same result as first forming the minmax version ofP(U)and then applying the∊-constraint method as Theorem 4.15 shows.Lemma 4.14Given an uncertain multi-objective optimization problemP(U). Then∊CP(U)(∊,i)is equivalent to∊COWCP(U)(∊,i).The result can easily be seen by simply writing down both problems. The∊-constraint version ofOWCP(U)isminsupξ∈Ufi(x,ξ)s.t.supξ∈Ufj(x,ξ)⩽∊j∀j≠ix∈Xand the∊-constraint version ofP(U)isminsupξ∈Ufi(x,ξ)s.t.fj(x,ξ)⩽∊j∀j≠i,∀ξ∈Ux∈X.Obviously these problems are equivalent. □Lemma 4.14 allows us to relate Methods 2 and 3 for computing r[s/w]e solutions.Theorem 4.15Every r[s/w]e solutionx¯∈XtoP(U)found by the∊-constraint method (Method 2) can also be found by theOWC-method (Method 3). Furthermore, every r[s/w]e solutionx¯∈XtoP(U)found by theOWC-method using the (deterministic)∊-constraint method can also be found by Method 2.Since we can find the r[s/w]e solutionx¯with Method 2, there exist parameters∊∈Rkandi∈{1,…,k}such thatx¯is [the unique/an] optimal solution to∊CP(U)(∊,i). Now, by Theorem 4.14,∊CP(U)(∊,i)is equivalent to the∊-constraint version ofOWCP(U),∊COWCP(U)(∊,i). Thereforex¯is [the unique/an] optimal solution to∊COWCP(U)(∊,i)and thus a r[s/w]e solution found with Method 3 if we choose the∊-constraint method with parameters∊and i for finding weakly efficient solutions toOWCP(U)in Step 1.With the same argumentation, we see that [the unique/an] optimal solution to∊COWCP(U)(∊,i)is also [the unique/an] optimal solution to∊CP(U)(∊,i), proving the second part of the theorem.That the reverse direction of the first part does not hold can be seen for example in the deterministic case, where not all weakly efficient solutions can be found by the classical∊-constraint method and thus by Method 2, since for|U|=1it reduces to the classical version. Nonetheless, these solutions can be found by Method 3 as we did not specify which algorithm it uses for finding weakly efficient solutions in Step 1. □Methods 1 and 2 are straightforward extensions of the well-known weighted sum scalarization and∊-constraint method from deterministic multi-objective optimization. An important difference is that Methods 1–3 are not always able to find all robust efficient solutions to an uncertain multi-objective problem, while the∊-constraint method for deterministic multi-objective optimization is able to find all efficient solutions (compare Ehrgott, 2005). In the following we show an example in which solutions exist which are not found by any of the three methods.Example 4.16Consider the feasible setX≔{x1,x2,x3,x4}and the uncertainty setU≔{ξ1,ξ2}. Letf:X×U↦R2be given by the plot in Fig. 5. Then we see that all feasible solutions are robust [strictly/·/weakly] efficient. On the other hand,x1is not an optimal solution toWP(U)(λ)for anyλ∈R[≧/≥/>]2since eitherx3orx4are better. Furthermore,x1is not optimal for∊CP(U)(∊,i)for any∊∈R⩾2qandi∈{1,2}since herex2is always feasible ifx1is feasible and has a better objective value. Also,x1is not optimal forOWCP(U)sincefUowc(x1)is dominated byfUowc(x2). Therefore, none of the methods 1, 2, nor 3 findsx1as a robust efficient solution, even though it is robust strictly efficient.In this section we introduce a special class of uncertain multi-objective optimization problems, namely problems where the uncertainties of the objective functions are independent of each other.Definition 5.1Objective-wise uncertaintyWe say a problemP(U)withU⊂Rmis objective-wise uncertain (owu), if the uncertainties of the objective functionsf1,…,fkare independent of each other, namely ifU=U1×⋯×UkwithUi∈Rmisuch that∑i=1kmi=mandf(x,ξ)=f1(x,ξ1)f2(x,ξ2)⋮fk(x,ξk),whereξi∈Ui.This definition has been motivated by the notion of constraint-wise uncertainty as considered, e.g., in Ben-Tal et al. (2009) and is likely to appear in practical instances, e.g., if the coefficients of the objective functions are noisy. We note that many studies on robustness deal with interval-wise uncertainty which is also assumed to be independent and hence a special case of our assumption. For the resulting class of problems we are able to obtain further results for computing robust efficient solutions.The first important insight is that the existence of a worst case for every objective functionfi, does in fact imply that a worst case scenario exists for the uncertain multi-objective optimization problemP(U).Lemma 5.2IfP(U)is owu andmaxξ∈Ufi(x,ξ)exists for allx∈Xandi∈{1,…,k}thenξmax(x)≔argmaxξ1∈U1f1(x,ξ1)argmaxξ2∈U2f2(x,ξ2)⋮argmaxξk∈Ukfk(x,ξk)∈U.For allx∈Xandi∈{1,…,k}argmaxξi∈Uifi(x,ξi)∈Uiand thereforeξmax(x)∈U1×⋯×Uk=U.□We interpret this property of owu problems for Method 3.Corollary 5.3Given an owu problemP(U), wheremaxξ∈Ufi(x,ξ)exists for allx∈Xandi∈{1,…,k},fUowc(x)=supξ∈Uf1(x,ξ)supξ∈Uf2(x,ξ)⋮supξ∈Ufk(x,ξ)=maxξ∈Uf1(x,ξ)maxξ∈Uf2(x,ξ)⋮maxξ∈Ufk(x,ξ)=f(x,ξmax(x)).The most important result of this section is that owu problems are in fact equivalent to deterministic multi-objective optimization problems.Theorem 5.4Given an owu problemP(U), wheremaxξ∈Ufi(x,ξ)exists for allx∈Xandi∈{1,…,k}. Thenxisr[s/·/w]eforP(U)⇔xis[strictly/·/weakly]efficientforOWCP(U).SinceP(U)is owu andmaxξ∈Ufi(x,ξ)exists for allx∈X, due to Lemma 5.2ξmax(x)∈Ufor allx∈X. ThusfU(x)-R[≧/≥/>]k⊇f(x,ξmax(x))-R[≧/≥/>]kfor allx∈X. On the other hand obviouslyf(x,ξ′)≦f(x,ξmax(x))for allξ′∈U, thusfU(x)-R[≧/≥/>]k⊆f(x,ξmax(x))-R[≧/≥/>]kfor allx∈X.The last two inclusions together yieldfU(x)-R[≧/≥/>]k=f(x,ξmax(x))-R[≧/≥/>]k=fUowc(x)-R[≧/≥/>]kfor allx∈Xdue to Corollary 5.3. Thereforexisr[s/·/w]eforP(U)⇔xisr[s/·/w]eforOWCP(U).SinceOWCP(U)is a deterministic multi-objective optimization problem,xisr[s/·/w]eforOWCP(U)⇔xis[strictly/·/weakly]efficientforOWCP(U)due to Lemma 3.5. □This result allows us to find all re solutions with Method 3 for owu problems. To see this, we first recall another result from the (deterministic)∊-constraint method:Theorem 5.5see Ehrgott (2005), Theorem 4.5LetPbe a deterministic multi-objective optimization problem with objective function f and feasible setX. Thenx¯∈Xis efficient forPif and only if there is an∊∈Rksuch thatx¯is an optimal solution to∊CP(∊,i)for alli∈{1,…,k}.From Theorems 4.14 and 5.5 we now can directly deduce Corollary 5.6.Corollary 5.6Given an owu problemP(U), wheremaxξ∈Ufi(x,ξ)exists for allx∈Xand alli∈{1,…,k}. Thenx¯∈Xis re forP(U)if and only if there is an∊∈Rksuch thatx¯is an optimal solution to∊CP(U)(∊,i)for all i.Recall that Example 4.9 shows that the∊-constraint method for uncertain multi-objective optimization problems need not find all re solutions. Nevertheless, Corollary 5.6 confirms that the∊-constraint method finds all re solutions for owu problems, just as in deterministic multi-objective optimization.One goal in robust single objective optimization is to simplify the robust counterpart of an optimization problem in order to compute robust solutions. A general property is Theorem 5.7:Theorem 5.7Ben-Tal and Nemirovski (1998)LetP(conv(U))be an uncertain (single objective) optimization problem, whereU={ξ1,…,ξm}andf(x,ξ):Rn×conv(U)→Ris quasiconvex inξ. Thenx¯isrobustoptimalforP(U)⇔x¯isrobustoptimalforP(conv(U)).In Section ‘Finding robust efficient solutions for uncertain multi-objective optimization problems’ we extended results from deterministic multi-objective optimization. We now want to extend Theorem 5.7 from single objective robust optimization to robust multi-objective optimization. Such an extension is only possible for owu problems, as we shall see in Examples 5.10 and 5.11, Examples 5.12 and 5.13. Before we state a similar result to Theorem 5.7 we need a lemma.Lemma 5.8P(U)is owu if and only ifP(conv(U))is owu.“⇐” is trivial sinceU⊂conv(U).“⇒” LetP(U)be owu. Thenconv(U)=conv(U1×⋯×Uk)=conv(U1)×⋯×conv(Uk),which holds since for two setsA,B,conv(A×B)=conv(A)×conv(B).□Now we can extend Theorem 5.7.Theorem 5.9Given an owu problemP(conv(U)), whereU={ξ1,…,ξm}andf1(x,ξ),…,fk(x,ξ):Rn×conv(U)→Rare quasiconvex inξ. Thenx¯isr[s/·/w]eforP(U)⇔[x¯isr[s/·/w]eforP(conv(U)).We show that forx′,x¯∈XfU(x′)-R≧k⊆fU(x¯)-R[≧/≥/>]k⇔fconv(U)(x′)-R≧k⊆fconv(U)(x¯)-R[≧/≥/>]k.“⇐”fconv(U)(x′)-R≧k⊆fconv(U)(x¯)-R[≧/≥/>]k⇒3.4.b∀ξ′∈conv(U)∃ξ¯∈conv(U)s.t.f(x′,ξ′)[≦/⩽/<]f(x¯,ξ¯)⇒∀ξ′∈U∃ξ¯∈conv(U)s.t.f(x′,ξ′)[≦/⩽/<]f(x¯,ξ¯)⇒∀ξ′∈U∃λ1,⋯,λm∈R≧,∑i=1mλi=1s.t.f(x′,ξ′)[≦/⩽/<]fx¯,∑i=1mλiξi.Butfx¯,∑i=1mλiξi≦f1(x¯,ξ1∗)⋮fk(x¯,ξk∗)for someξ∗∈Usincefiis quasiconvex inξfor every i andP(U)is owu. But this means that∀ξ′∈U∃ξ∗∈Us.t.f(x′,ξ′)[≦/⩽/<]f(x¯,ξ∗),thusfU(x′)-R≧k⊆fU(x¯)-R[≧/≥/>]k.“⇒”fU(x′)-R≧k⊆fU(x¯)-R[≧/≥/>]k⇒∀ξ∈U∃ξ¯∈Us.t.f(x′,ξ)[≦/⩽/<]f(x¯,ξ¯).But then for every∑i=1mλiξi∈conv(U), since f is quasiconvex inξ, it holds thatfx′,∑i=1mλiξi≦f(x′,ξmax(x′))[≦/⩽/<]f(x¯,ξ∗),for someξ∗∈U⊆conv(U)sinceP(U)is owu and thusξmax(x′)∈U. This completes the proof. □The assumptions of Theorem 5.9 cannot be weakened, as we show in the following four examples. First, Examples 5.10 and 5.11 show a function which is quasiconvex inξbut, whereP(U)is not owu. We identify solutions which are rse for eitherP(U)orP(conv(U))but not even rwe for the other.Example 5.10LetX=U={0,1}andf:X×conv(U)→R2given byf(x,ξ)≔(-0.8ξx+0.5x+0.5,0.8ξx-0.3x+0.5)T.Then f is affine (thus quasiconvex) inξbutP(U)is not owu. By plotting the function in Fig. 6, we can see thatx=1is rse forP(U)but not even rwe forP(conv(U)), since here it is dominated byx=0.LetX=U={0,1}andf:X×conv(U)→R2given byf(x,ξ)≔(1.8+0.2x-ξ,0.8+0.2x+ξ+x·(ξ10-ξ))TThen f is quasiconvex inξbutP(U)is not owu. Again by plotting the function in Fig. 7, we can see thatx=1is rse forP(conv(U))but not even rwe forP(U), since here it is dominated byx=0:On the other hand, Examples 5.12 and 5.13 illustrate uncertain multi-objective optimization problemsP(U)that are owu but where f is not quasiconvex. Again we find solutions which are rse for eitherP(U)orP(conv(U))but not even rwe for the other.Example 5.12LetX={0,1},U=R×{-2,2}andf:X×conv(U)→R2given byf(x,ξ)≔2+x2+x-(-1)x·2-ξ22.ThenP(U)is owu but f is not quasiconvex inξ. By plotting the function in Fig. 8, we can see thatx=1is rse forP(U)but not even rwe forP(conv(U)), since it is dominated byx=0.This example is pretty much the same as Example 5.12, the difference is one minus sign inf2. Because of this change,x=1is rse forP(conv(U))but not even rwe forP(U). LetX={0,1},U=R×-2,2andf:X×conv(U)→R2given byf(x,ξ)≔2+x2+x+(-1)x·2-ξ22.ThenP(U)is owu but f is not quasiconvex inξ. Again, by plotting the function (which we omit here), we can see thatx=1is rse forP(conv(U))but not even rwe forP(U), since it is dominated byx=0.Together with Section ‘Completeness of the objective-wise worst case method’ we can deduce that an owu problemP(conv(U))is equivalent to the deterministic problemOWCP(U)with a considerably smaller uncertainty setU. This smaller uncertainty set also simplifies the calculation of a worst case, especially since (due to Lemma 5.2) we can do this for each objective function at a time.So far we introduced the concept of robust efficiency and presented various methods to compute robust efficient solutions. This raises many questions on the structure of the set of robust efficient solutions, e.g., if it forms a set of robust nond-dominated points on the boundary of the feasible set in objective space similar to the non-dominated set in deterministic multi-objective optimization, and where in the feasible set robust efficient solutions may be located.Unfortunately, as demonstrated in the master thesis of Majewski (2013), it turns out that even in the case of multi-objective linear programming, the structure of robust efficient solutions does not satisfy well-known properties of deterministic multi-objective optimization.•While in deterministic linear multi-objective optimization there always exists an efficient solution at an extreme point of the feasible set (compare, e.g., Ehrgott, 2005), this is not the case for robust efficient solutions in uncertain linear multi-objective optimization, not even for owu functions.Even worse, there exist examples of uncertain bi-objective affine optimization problems on the box[0,1]2in which all robust efficient solutions lie in the interior of the feasible set; a situation which cannot occur in deterministic affine multi-objective programming (compare, e.g., Ehrgott, 2005).Hence, general conclusions are difficult to be drawn and a general intuition of a robust efficient solution may be hard to obtain. However, we can illustrate the solutions found by the different methods for particular instances.Fig. 9shows the feasible set of an optimization problem with four linear objective functions, feasible set[-1,0]2, and six scenarios for the uncertain parameters. The solutions found by the∊-constraint method (Method 2) are indicated for different step sizes of the chosen∊(but with values of∊taken from the same set).Fig. 10shows two other feasible sets of uncertain multi-objective optimization problems. The solutions obtained with the weighted sum scalarization method (Method 1) are indicated in black, the solutions of the∊-constraint method in grey. As one can see on the left, the solutions obtained with Method 1 seem to lie on the boundary of the feasible set, while Method 2 also finds solutions in the interior of the feasible set.In conclusion, robust efficient solutions can lie basically anywhere in the feasible set (even for uncertain multi-objective linear optimization problems) and none of our three methods is clearly superior to the others. These results highlight again the complexity of uncertain multi-objective optimization.We finally present a bi-objective example in order to illustrate the advantages of using robust efficient solutions over optimal solutions to the various scenarios.Fig. 11illustrates a bi-objective convex quadratic optimization problem. We investigate two types of solutions. First, we choose one nominal scenario (which might be the undisturbed, or the most likely scenario) and compute the efficient solutions for this particular case. If the nominal scenario occurs this is the best we can do, and it is what is often done in practice: Choose the most likely scenario and solve the optimization problem with this fixed scenario. We call this set of solutions the nominally efficient solutions Nom. We then use the approach of our paper and compute robust efficient solutions Rob to the uncertain bi-objective convex quadratic optimization problem assuming six other scenarios. In Fig. 11 the objective vectors of nominally efficient solutions Nom and robust efficient solutions Rob are compared to each other as follows.•Fig. 11(a) shows the objective values of Nom (black) and of Rob (grey) in the nominal scenario. It can be seen that under the nominal scenario, the nominally efficient solutions are slightly better than the robust efficient solutions, in particular, the objective vectors of robust efficient solutions under the nominal scenario are each dominated by objective vectors of nominally efficient solutions under the nominal scenario.The worst case objective values of Nom (black) and Rob (grey) are shown in Fig. 11(b). These have been computed as follows: Let x be a solution in Nom or Rob. Then we computedfU(x)-R≧k. This is shown as the grey polygonal line in Fig. 11(b) for a specificx∈Rob. The upper right vertices of the setsfU(x)-R≧kbelong to the worst case scenarios – these are depicted in the figure for allx∈Nomandx∈Rob. We can see that using robust efficient solutions over nominally efficient solutions provides very significant improvement of the worst cases in our example.Fig. 11(c) and (d) show the objective values of Nom (black) and of Rob (grey) under all seven scenarios, i.e., the set{fU(x):x∈Nom}is shown in Fig. 11(c) and the set{fU(x):x∈Rob}is shown in Fig. 11(d).

@&#CONCLUSIONS@&#
