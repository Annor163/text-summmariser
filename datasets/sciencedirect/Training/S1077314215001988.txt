@&#MAIN-TITLE@&#
Action recognition by using kernels on aclets sequences

@&#HIGHLIGHTS@&#
We propose a string kernel based method for action recognition.The similarity between strings is evaluated by a soft alignment kernel.The experiments have been performed over three standard datasets.

@&#KEYPHRASES@&#
Human action recognition,Strings,String kernel,Soft assignment,

@&#ABSTRACT@&#
In this paper we propose a method for human action recognition based on a string kernel framework. An action is represented as a string, where each symbol composing it is associated to an aclet, that is an atomic unit of the action encoding a feature vector extracted from raw data. In this way, measuring similarities between actions leads to design a similarity measure between strings. We propose to define this string’s similarity using the global alignment kernel framework. In this context, the similarity between two aclets is computed by a novel soft evaluation method based on an enhanced gaussian kernel. The main advantage of the proposed approach lies in its ability to effectively deal with actions of different lengths or different temporal scales as well as with noise introduced during the features extraction step. The proposed method has been tested over three publicly available datasets, namely the MIVIA, the CAD and the MHAD, and the obtained results, compared with several state of the art approaches, confirm the effectiveness and the applicability of our system in real environments, where unexperienced operators can easily configure it.

@&#INTRODUCTION@&#
In the last years the analysis and the recognition of human motion from images and videos have interested several scientists working in pattern recognition and computer vision fields [11,23,30,46,47]. The large interest shown by many researchers toward action recognition is motivated by the virtually infinite applicative domains where it is possible to exploit detailed information regarding actions for achieving real time situation awareness, augmented reality, improved human-machine interaction, etc. Examples range from surveillance systems and ambient assisted living with the possibility of identifying some events of interest occurring into a video (an aggression, a robbery, a tumble of an elder person, etc), to video retrieval by allowing a human operator to easily retrieve specific types of actions performed by persons in a video footage, and to computer gaming for delivering more and more advanced, realistic, and engaging experience for the player.Despite the difficulty to define a strict partition among the types of movements, three main classes are identified in [1,37] with different nomenclatures: gestures, actions and activities. Gestures [36] are elementary movements of a single body part, such as a hand, an arm, the face or the head. An action [42,51] describes a movement of the whole human body, such as walking or drinking. Finally, an activity [1] is evaluated over a longer period of time, often analyzing object’s trajectories, and may involve more persons and/or objects as two persons fighting, a person leaving a baggage, and so on. In this paper, we focus on the recognition of the second class of movement, namely actions.Methods available in the literature formalize the action recognition as a traditional pattern recognition problem, where, as stated in [42], a set of features is computed on the sequence of images and is used to feed a classifier. Several issues make this problem very challenging: indeed, a method for recognizing human actions has to be sufficiently general to deal with intra-class variations (actions performed by different persons, but typically in different ways), and at the same time should be able to distinguish among different actions even when performed by the same person. Furthermore, the same class of actions can be performed in different places, so implying different conditions (for instance related to the illumination of the environment or the angle under which the camera takes a person performing the action) which may change the appearance of the subject under test. Finally, algorithms should be able to recover actions having different durations and performed at different paces.Within this framework, the contribution of the scientific community has been mainly devoted to the definition of novel feature vectors, aiming at describing the spatio-temporal movement of humans so as to be sufficiently general to deal with different persons and actions performed under different conditions, but at the same time sufficiently discriminant to discern among the different actions of interest. In particular, according to [42], two different categories of descriptors can be defined, namely local and global. Local descriptors are based on a bottom-up approach: a set of spatio temporal interest points is detected by locally analyzing each pixel of the image and assigning a local patch to each of these points. These patches are then combined, for instance using a traditional bag of features approach [18,24,29,30,48]. The main advantage of local descriptors is that they are less sensitive to partial occlusions and are typically computed on the whole image, so without the need to detect moving objects and to track them in successive frames. This is a useful property in all those cluttered and/or crowded scenes when reliable object detection and tracking cannot be achieved. However, these advantages are counter balanced by the high computational effort required to extract these descriptors. Furthermore, the performance that can be achieved when using local descriptors strongly depends on the amount and on the reliability of the interest points detected by the system: in some cases, for instance due to camera movements, interest points extraction may be unreliable thus requiring specific pre-processing steps.On the other hand, global descriptors are evaluated by using a top-down approach; the person of interest is first located in the scene and its blob is extracted using detection and/or tracking techniques. Starting from the blob, the action is globally described by features derived from silhouettes, edges or optical flow [10,13,25,26,28,39,50]. The main advantage of this family of approaches is that the feature vector extracted from the blob is solely based on the useful information strictly related to the movement of the person. Of course, it is evident that the higher is the accuracy in the extraction of the blob, the better will be the overall performance of such approaches. However, their main limitation is in general their sensitivity to noise, partial occlusions and variations in viewpoint. In order to overcome such limitations, [35] propose a representation that is a trade off between the global and local approaches through a grid-based description: the minimum box enclosing the silhouette is partitioned into cells in order to also consider a kind of local information about parts of the objects.Unfortunately the low level representation provided by both local and global descriptors is sensitive to errors introduced by either interest point or blob detection methods. In order to avoid this problem, a common solution recently adopted in the literature consists in introducing a high-level representation, able to improve the discriminative power of the overall recognition system. For instance, in [44] the authors adopt a classification strategy based on Hidden Markov Model (HMM): they create a two-layers Maximum Entropy Markov strategy for modeling an activity as a set of sub-activities and exploit a dynamic programming approach for the inference. In general, the main drawback of HMM based approaches lies in the large amount of labeled data required during the training step, which is often not simple to obtain in real environments. Furthermore, the performance of HMM strongly depends on a good configuration of hidden states, which is difficult to achieve and in general requires an expertise in this field for a proper choice.In [20] the high level representation is obtained directly from the data, by exploiting regularities in the training set: in particular, this is obtained through a Deep Belief Network trained by a Restricted Boltzmann Machine. Given a suitably large dataset, the deep representation typically outperforms a hand-crafted description scheme, without requiring an heavy effort in feature design. However, as in the case of the HMM architecture, the parameters used to train the network strongly influence the achieved performance, and usually require the intervention of an expert of the paradigm.A high level representation based on bag of words has been exploited in [19]: the action is represented by an histogram, which encodes the occurrence of feature vectors in a given temporal window (the so called visual words) according to a dictionary extracted during the learning phase. In this way, the similarity between two actions is expressed in terms of similarity between histograms rather than in terms of similarity between low level feature vectors, so making the system less sensitive to errors introduced during the features extraction step. One of the main advantages of this approach lies on the simplicity of its configuration (only few parameters need to be set up). However, although the promising performance obtained by Foggia et al. [19], a larger experimentation has highlighted its main limitation: this method bases its decision only on the occurrence or on the absence of relevant visual words within a given temporal window, then the temporal information cannot be fully taken into account. It is important to highlight that the temporal information is instead a very important feature, since human vision implicitly analyzes the sequence of the motion patterns composing the actions in order to distinguish them [27].In order to overcome this limitation, a temporal bag of words has been recently exploited [5,22,43]: the common idea is to encode in the histogram information about the relative position of a frame with respect to the other ones. For instance, in [43] the video is partitioned into fixed length bins and a single histogram is extracted for each bin; the video is finally represented as a k-dimensional feature vector, where each dimension encodes a single bin. The partitioning of the video is finally iterated in a hierarchical way by considering different bins’ lengths, so as to deal with actions performed at different rates. However, the bigger is the number of layers, the higher will be the performance but at the same time the higher will be the computational effort required by the system. In [5] a n-grams based approach is exploited: a set of grams, namely the possible sequences of l words, beingl={1,…,n},are used to build the codeword so as to consider both the temporal and causal information; furthermore, randomly generated sequences are added to the codeword using regular expressions. Finally, the action is represented by computing the histogram of occurrences, as in the traditional bag of words approach. A similar strategy has been also proposed in [22], where the authors define the grams to be considered by a graph based approach. Although being very promising, the main limitation of such approaches is that only small values of n, typically lower than 3, can be used in real time applications, thus allowing to evaluate only local temporal information (three consecutive frames) instead than global one.In order to explicitly exploit the temporal evolution of actions, several researchers have recently explored a string based representation. In [4], for instance, the string is built as a sequence of histograms, one for each frame. A different approach has been exploited in [21], where a set of spatio-temporal interest points (STIPs) is detected from a fixed length time window; in each window, the STIPs are used to form a feature graph where nodes and edges encode respectively STIPS and their spatio-temporal distance. The string is finally built by concatenating the so obtained feature graphs. Although being very promising, such approaches are very expensive from two points of view: on one side, for each frame they need to store a large amount of information (an histogram and a graph, respectively) for encoding a single symbol of the string; on the other side, the matching between such symbols is computationally intensive. In [8] the authors propose a very compact representation of the actions based on strings and a novel similarity measure based on the Dirac kernel, which allows to overcome limitations due to the computational effort of previous string based methods.Starting from the seminal work in [8], in this paper we propose an efficient and robust method for recognizing human actions, providing the following original contributions:•the temporal dimension of the action is exploited by means of a very compact representation based on strings, which allows to reduce the amount of memory required for storing the past information about the action without losing relevant information;a novel similarity measure based on a soft global alignment kernel is proposed for managing actions of different lengths and performed at different speeds;an extensive experimentation is carried out over three publicly available datasets and the achieved results demonstrate the effectiveness of the proposed approach both in terms of accuracy and time required for the elaboration; furthermore, a sensitivity analysis has confirmed its robustness with respect to the configuration parameters.The rest of the paper is organized as follows: we introduce the proposed method first focusing on its rationale in Section 2 and then providing a detailed description in Section 3; in Section 4 we report and comment the results of the experimental analysis carried out on three public datasets and the comparison with state of the art action recognition methods; finally, in Section 5, we draw final conclusions and delineate future direction of our research efforts in this area.In order to model motion patterns composing actions, we introduce the concept of aclet, considered as the atomic unit of an action. Each aclet is encoded through a symbol corresponding to an entry in an alphabet learnt during a preliminary learning step. This alphabet is obtained from quantization of the low level feature vectors computed on the silhouette at each time instant. The main advantage deriving from the introduction of aclets to represent feature vectors is that an aclet allows to encode information associated to a frame into a single value instead of in a high dimensional feature vector, so significantly decreasing the amount of memory required to maintain the history of the action. The sequence of aclets is then used to build a high level representation based on strings, able to explicitly take into account the temporal information about the sequence of aclets. This is a very important feature, since it allows to distinguish among similar actions whose main difference pertains the sequence of sub-actions (such as, for instance, sit down and stand up and stand up and sit down).In order to deal with the uncertainty introduced during the extraction of aclets, we introduce a novel similarity measure based on the global alignment kernel framework, which allows to evaluate all possible alignments between two strings. This framework has been successfully used in other applications domains, ranging from gesture recognition [41] to emotional expression classification [34] and trajectory analysis [9]. The main improvement of the proposed approach with respect to the method in [8] regards the typology of assignment performed between a feature vector and an aclet. In fact, in [8] an hard assignment has been considered: each feature vector is assigned to a single aclet and the similarity between two aclets is evaluated by a traditional Dirac kernel. Unfortunately, such a strategy induces a strong sensitivity to the parameters required to set up the system. In order to mitigate this problem, in this paper we introduce a novel similarity measure evaluating the closeness of two aclets (soft assignment) and not only if two aclets are exactly the same (hard assignment); this is achieved by measuring the similarity between two aclets using an enhanced version of the gaussian kernel, which allows to implicitly assign a single feature vector to more than one aclet, so as to increase the overall robustness of the proposed approach with respect to errors introduced during features extraction step, as well as to make it less dependent on large variations of the configuration parameters. This is due to the fact that soft assignment can be seen as an estimation of the posteriori probability that a feature vector belongs to an aclet [33], thus it is able to strongly mitigate the quantization error introduced by the traditional hard assignment.Another important contribution provided by this paper with respect to [8] is a more significant experimentation carried out on three public datasets, together with a sensitivity analysis that shows the strong improvements of the proposed approach with respect to the state of the art. Furthermore, as we will show, our method is able to outperform state of the art methodologies while remaining simple to configure: indeed, only a few parameters need to be selected during the configuration steps, implying that our approach is especially suited for working in real environments where unexperienced operators may configure the application.An overview of the proposed method is shown in Fig. 1, where the two levels of representation used by our method are highlighted. In the first stage the low level representation is extracted by analyzing a sequence of depth images. Then, actions are modeled at the second level of the representation through strings whose symbols are named aclets. Each aclet is defined from feature vectors extracted in the first stage. Each string is finally used to feed a classifier, able to identify the action occurring in the video. In this section, details about each module are provided.In the last years, the scientific community has deeply investigated the possibility to use depth images instead of traditional optical cameras. In fact, depth images allow to limit the loss of 3D information due to the use of traditional optical cameras: the silhouette of a person can be extracted in an easier and more reliable way since depth images are more robust to illumination changes or bad lighting conditions [2,52]. Starting from this consideration, we decided to extract the feature vector by processing depth images provided by a Kinect sensor. Here we adopt the low level description based on global features recently introduced in [10]: the system first detects the human silhouette by a traditional background subtraction method [14] applied over the depth image. Then, starting from the foreground image containing the silhouette, it computes three derived images: the average depth image (ADI), the motion history image (MHI) and the depth difference image (DDI). The main advantage provided by this representation is that the movement of a person in both spatial (x,y) and depth (z) dimensions can be represented in a very compact way, so significantly decreasing the amount of data to be processed without losing relevant information. For each derived image, an example is shown in Fig. 2. Finally, for each frame, the feature vector is computed by analyzing ADI, MHI and DDI.Given an observation window of N temporally adjacent images, the ADI(x, y, t) [35] is used to capture the motion information in the depth dimension. Let be FM(x, y, t) the value of the foreground mask of the pixel x, y at time t, being FM(x, y, t) ∈ {0, 1}, and D(x, y, t) the homologous value in the depth image. ADI(x, y, t) is the average depth at position (x, y) over the non zero values of the foreground mask at timest−N+1,…,t:(1)ADI(x,y,t)=∑k=t−N+1tD(x,y,k)FM(x,y,k)∑k=t−N+1tFM(x,y,k).The MHI[17], is used to capture into a single static image the sequence of motions. The value of MHI(x, y, t) is updated as follows:(2)MHI(x,y,t)=255if point (x, y) passed from background to foreground at time t, and(3)MHI(x,y,t)=max{MHI(x,y,t−1)−τ,0}otherwise; τ is a constant value set to(256/N)−1,corresponding to an observation over N frames.Finally, the DDI[35] is used to evaluate motions changes in the depth dimension:(4)DDI(x,y,t)=Dmax(x,y,t)−Dmin(x,y,t),where Dmax(x, y, t) and Dmin(x, y, t) are respectively the maximum and minimum depth for position (x, y) over the images at timest−N+1,…,t. Note that only non-zero pixels values of the FM across N frames are evaluated. In our experiments N has been set to one second, as it is the minimum time required by a person to move a part of his body into a meaningful way.Once obtained the above derived images, we extract three different types of features. Both the MHI and the ADI are represented through the seven translation, scale and orientation invariant Hu moments [6]. Hu moments, computed over both MHI and ADI, result in a 14 sized feature vector.On the other side, the DDI is represented through two different kinds of features: theRtransform [45] and the min–max depth Variations[35]. The former is an extended Radon transform, which captures the geometric information of interest points. Its main advantage lies in the geometric invariance, to both scale and translation; furthermore, it is robust to the errors of the detection phase, such as disjoint silhouettes and holes in the shape. As experimentally demonstrated in [50],Rtransform outperforms methods using silhouette-based moment descriptors while requiring low computational costs.Finally, in order to enrich the above global description, we also evaluate local information using a grid based approach: we first find the bounding box containing the silhouette in order to capture the complete motion of an action in it. Then, the box is hierarchically partitioned into cells of equal size (1 × 1, 2 × 1, 1 × 2, 2 × 2, 3 × 3, and 6 × 6) and the maximum and the minimum depth values in each cell are computed (Fig. 3). The computation of this min–max depth variations results in a feature vector composed by 108 elements.Note that the proposed description allows us to maintain all the advantages of the global representation, without suffering of the typical problems of this kind of descriptors, already discussed in the introduction: in fact, the silhouette, extracted from depth images instead of traditional intensity images, allows to significantly reduce the sensitivity to noise or partial occlusions. Furthermore, theRtransform captures local properties related to the alignment of image’s subregions, while Hu moments and min–max depth variations provide a global representation of pixels’ distribution.As outlined at the beginning of this section, an action is modeled as a string whose symbols, namely the aclets, belong to an alphabet C of finite size. The symbols in C are determined during the training phase of our method by performing a non-linear quantization of the feature space by K-means clustering.A finite alphabet of symbols, each one associated to an aclet, is thus generated by assimilating the ith cluster to its centroid ci:(5)C=(c1,…,c|C|),being |C| the number of clusters.It is important to highlight that, as we will show in the experimental section, the cardinality of the alphabet is not a critical parameter, as it can be chosen in a large range without significantly affecting the overall performance. Furthermore, the construction of C only requires the labeling of actions, thus limiting dramatically the human intervention in the ground truthing phase. The latter two points are noticeable as they strongly simplify the use of this method in real scenarios.During the operating phase, the obtained alphabet C allows to determine for each low level feature vector vithe closest cluster centroid cjand then to associate the ith symbol si:(6)si=argminj∥vi−cj∥forj∈{1,…,|C|}.In order to model the fact that different actions may have different durations, for instance, the action sit down may takes a few seconds, while the action boxing may have a duration of tenth of seconds, in our approach we represent the kth class of action through a string of length lkwithk∈{1,…,K},being K the cardinality of the set of considered classes of action. Values lkare automatically learnt during the training phase by computing the average string length associated to the actions belonging to that class.At time t, the following strings are calculated(7)Stk=<st−lk,…,st>,k=1,…,Kby concatenating the last lkobtained symbols. The obtained strings are then used to feed a classifier and to evaluate the particular action a person is performing.The similarity between two actions, each represented as a string as introduced in the previous subsection, is evaluated within a kernel framework. The main advantage in the use of a kernel based approach is that kernel functions encode a similarity measure corresponding to a scalar product in some Hilbert space defined by the kernel. This last property allows to combine the rich description provided by strings with efficient machine learning methods such as SVM using the kernel trick [3]. This trick allows to replace scalar products by calls to our kernel in any method which can be expressed solely through scalar products.In this paper, we propose a novel similarity metric based on the general framework of the fast global alignment kernel (FGAK) [15]. This kernel computes a soft-minimum of all alignment scores. Hence this kernel, as argued by Cuturi [15], takes into account a richer statistic on both strings being compared than their minimal alignment score.Furthermore, one of the main advantages with respect to the other kernel based approaches is that it avoids the diagonal dominance of the Gram matrix, which is an undesirable property. As a matter of fact, a Gram matrix with a large diagonal dominance corresponds to a set of objects mainly similar to themselves according to kernel values. Consequently, a kernel with such kind of Gram matrix does not allow efficient generalization from training set. Furthermore, the FGAK allows to properly deal with strings having different lengths and different temporal scales.Formally, given two strings,X=〈x1,…,xn〉andY=〈y1,…,ym〉of length n and m respectively, an alignment between X and Y is a pair of increasing integral vectors (π1, π2) of lengthp<n+m,such that1=π1(1)≤⋯≤π1(p)=nand1=π2(1)≤⋯≤π2(p)=m,with unary increments and no simultaneous repetitions. Let A(n, m) be the set of all possible alignments between X and Y. The global alignment kernel kGAis defined as:(8)kGA(X,Y)=∑π∈A(n,m)∏i=1|π|k(xπ1(i),yπ2(i),π1(i),π2(i)).The kernel k that we consider in our approach is given by a combination of the triangular kernel ktand a soft weighted kernel kw:(9)k(xi,yj,i,j)=kt(i,j)·kw(xi,yj)1−kt(i,j)·kw(xi,yj),where xiand yjencode two symbols and i and j represent the position of these symbols inside strings X and Y, respectively.In particular, kthas been introduced in order to speed up the computation of the kernel by considering only a small but feasible subset of all the possible alignments induced by the global alignment kernel; it allows to make the kernel computation faster, so as to obtain a Fast GAK: if the indices of two symbols differ by more than T, their kernel value is equal to 0. ktis defined as follows:(10)kt(i,j)=(1−|i−j|T)+,where T is the order of the kernel and+refers to the fact thatkt(i,j)=0if|i−j|≥T.The weighted kernel kwon the other side, implicitly induces the soft assignment and allows to avoid the problems related to the hard evaluation of the traditional Dirac kernel δ(xi, yi), used in [8], and defined as:(11)δ(xi,yi)={0ifxi≠yi1ifxi=yiAs we can see, the traditional Dirac kernel does not take into account how close two clusters are. In order to better understand this problem, we can consider the following example. Let us consider the alphabet C shown in Fig. 4. The similarity between symbols s1 and s4, associated to feature vectors v1 and v2 respectively, is equal to 0, sinceδ(s1,s4)=0. Similarly, we can note thatδ(s1,s3)=0,being s1 and s3 the symbols associated to the feature vectors v3 and v4 respectively. However, although belonging to two different clusters, v3 and v4 are located in two close clusters and their similarity should not be Null.Starting from the above considerations, in this paper we introduce a novel kernel with a double purpose: in fact, on one side, we should avoid the crisp behavior induced by the hard evaluation of the Dirac kernel; on the other side, we cannot make its computation too heavy so as to still allow a computation in real time. For these reasons, we propose a novel similarity metric based on kw, that is an enhanced version of the traditional gaussian kernel [15]. Let be d(xi, yi) the distance between the cluster centroidscxiandcyi,associated to the symbols xiand yi, respectively:(12)d(xi,yi)=∥cxi−cyi∥2.The kernel kw(xi, yi) can be computed as follows:(13)kw(xi,yi)=e−ϕσ(xi,yi),where:(14)ϕσ(xi,yi)=12σ2d(xi,yi)+log(2−e−d(xi,yi)2σ2).This kernel performs a soft evaluation, since it is based on the distance between cluster centroids. Such distances can be computed only once, during the setup of the system, so significantly decreasing the computational effort required by the use of the kernel at run time.It is important to highlight that the proposed kernel is a Mercer kernel. In fact, as demonstrated in [16], kGAis positive definite if both k andk/(1+k)are positive definite. Using Eq. 9 our kernel k may be written as:k(x,y,i,j)=∑n=1+∞(kt(i,j)kw(x,y))nand is thus definite positive as the limit of a sum of definite positive kernels. Moreover:k(x,y,i,j)1+k(x,y,i,j)=kt(i,j)kw(x,y)is trivially definite positive as a tensor product of definite positive kernels. Hence [15], Remark 1, our global kernel kGAis definite positive. This is a an important property: indeed, as described in [31], positive definite kernels usually strongly outperform not positive definite ones in kernel machines (for instance in combination with support vector machines).The kernel defined by Eq. 8 is finally normalized so as to obtain a similarity valuekGANin the interval [0, 1]:(15)kGAN(X,Y)=kGA(X,Y)kGA(X,X)*kGA(Y,Y).Computational Cost: it can be shown that the cost for computing kGAisO(Tmin(m,n))where T is the order of kernel kt(Eq. 10 and [15]). This cost constitutes only a marginal overhead with respect to the computation of the low level processing module, which instead depends on the original image resolution, which is significantly higher.The aim of the proposed method is to identify a particular action a* occurring in a video stream, among the |A| different classes aithe system has been trained on, beingi=1,…,|A|.In order to confirm the generality of the proposed approach with respect to different classification paradigms, we considered two different well-known and widely adopted classifiers, namely a multi-class support vector machine (SVM) and a k nearest neighbors (k-NN). As for the SVM, we design |A| different classifiers, one for each class, using a one versus all approach. The ith classifier is trained on the whole training data set in order to classify the members of ith class against the rest. It means that the training set is relabeled: the samples belonging to the ith class are labeled as positive examples, while samples belonging to other classes are labeled as negative ones. During the operating phase, a new string is assigned to the class whose distance to the margin is maximum.As for the k-NN, during the training step the different strings encoding the prototypes are extracted, one for each action. During the operating phase, the stringsStkrepresenting each action at the generic time t are compared with the prototypes using the proposed kernel and the decision is finally taken by a majority vote of their K nearest neighbors. The number of nearest neighbors has been experimentally set to 5.

@&#CONCLUSIONS@&#
