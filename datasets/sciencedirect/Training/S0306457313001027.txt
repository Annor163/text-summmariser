@&#MAIN-TITLE@&#
Mining a Persian–English comparable corpus for cross-language information retrieval

@&#HIGHLIGHTS@&#
We propose novel method for mining high quality translation from comparable corpus.We introduce Term Association Network (TAN) for mining Translation knowledge.We propose a new method for term translation validity using cross outlier detection.Results show that proposed methods significantly outperforms dictionary-based method.Our methods are specially effective in translating OOV terms by expanding query words.

@&#KEYPHRASES@&#
Comparable corpora,Cross-language information retrieval,Term association network,Translation validity check,

@&#ABSTRACT@&#
Knowledge acquisition and bilingual terminology extraction from multilingual corpora are challenging tasks for cross-language information retrieval. In this study, we propose a novel method for mining high quality translation knowledge from our constructed Persian–English comparable corpus, University of Tehran Persian–English Comparable Corpus (UTPECC). We extract translation knowledge based on Term Association Network (TAN) constructed from term co-occurrences in same language as well as term associations in different languages. We further propose a post-processing step to do term translation validity check by detecting the mistranslated terms as outliers. Evaluation results on two different data sets show that translating queries using UTPECC and using the proposed methods significantly outperform simple dictionary-based methods. Moreover, the experimental results show that our methods are especially effective in translating Out-Of-Vocabulary terms and also expanding query words based on their associated terms.

@&#INTRODUCTION@&#
The researches on Cross Language Information Retrieval (CLIR) have recently received much attention, due to the fast growth of the World Wide Web and the availability of information in different languages on the Web. One of the main issues in CLIR is where to obtain the translation knowledge (Oard & Diekema, 1998). Multilingual corpora are widely used for this purpose which are actually available in many language pairs and extracting translation knowledge from multilingual corpora has been extensively studied using various statistical methods. They can be either in the form of parallel or comparable corpora. However, there are limitations in obtaining parallel corpora in all domains and languages while comparable corpora are much easier resources to obtain. Thus, recently, there has been considerable interest in using comparable corpora as translation resources (e.g. Fung & Yee, 1998; Sadat, 2010b; Talvensaari, Laurikkala, Järvelin, Juhola, & Keskustalo, 2007; Tao & Zhai, 2005). In this paper we use a Persian–English comparable corpus, University of Tehran Persian–English Comparable Corpus (UTPECC) (Hashemi, Shakery, & Faili, 2010), to do English–Persian cross language information retrieval by extracting term associations from the comparable corpus. These association terms my contain both translations of a term, and terms that are actually related to its (correct) translations. In this paper, all selected translations and related terms for a term are referred to as its “translations”. As a basic method, we obtain term associations based on co-occurrence of terms in the alignments. We further propose a novel way of extracting translations in different languages based on Terms Association Network (TAN) which exploits term associations in monolingual data as well as bilingual term associations to better detect translation knowledge. TAN method uses a network of terms with implicit mutual information links between terms in the same language and term association links between terms in different languages. The main contribution of this paper lies in combining these term association links as a network, which in turn improves CLIR effectiveness. Basically, we use the neighborhoods of terms in this network to re-score the translation alternatives. Two terms are translations of each other if their neighborhoods in the same language are strongly connected and vice versa they are not likely to be translations if their neighborhoods are not strongly connected. Also, in order to discard misleading translation candidates, we do translation validity check using cross-outlier detection method. Intuitively, if the distribution of the weights of a term’s translations is different from that of its neighbors’ translations, the term is considered as an outlier.We evaluated our methods on Hamshahri and INFILE data sets by doing cross-language information retrieval using the cross-lingual term associations extracted from the comparable corpus. We use the extracted translation knowledge to construct a query language model in the target language corresponding to each query in the source language and rank the documents based on the KL-divergence between the query language model and document language models. Experiments show promising results for extracting translation knowledge from the UTPECC by (1) translating Out Of Vocabulary (OOV) terms, such as proper nouns, which are not in our dictionaries, (2) expanding query words with their related terms and also (3) using probability scores of extracted translations. Also, using comparable corpora helps to complement dictionaries by translating OOV terms and finding related terms to expand query words.The rest of the paper is organized as follows. We first present some previous work done on exploiting comparable corpora in Section 2. We then introduce our translation extraction and query translation methods in Sections 3 and 4. We present the experiment results in Section 5 and finally bring the conclusions and future work of our study in Section 6.Using comparable corpora as a language resource has been studied extensively in the existing literature, in fields such as cross-language information retrieval (Picchi & Peters, 1996; Sadat, 2010a, 2010b; Sadat, Yoshikawa, & Uemura, 2003; Talvensaari et al., 2007), cross-lingual document association (Tao & Zhai, 2005; Vu, Aw, & Zhang, 2009), in extracting parallel sentences (Abdul-Rauf & Schwenk, 2009; Munteanu & Marcu, 2005) and extracting word translations (Fung, 1995; Fung & Yee, 1998; Hassan, Fahmy, & Hassan, 2007; Laroche & Langlais, 2010; Morin, Daille, Takeuchi, & Kageura, 2007; Morin, Daille, Takeuchi, & Kageura, 2010; Otero & Campos, 2010; Rapp, 1995; Rapp & Zock, 2010; Tanaka & Iwasaki, 1996; Tao & Zhai, 2005; Udupa, Saravanan, Kumaran, & Jagarlamudi, 2008; Yu & Tsujii, 2009). Most of the early work in extracting word translations employ an initial lexicon of seed words (e.g. Franz, McCarley, & Roukos, 1999; Fung & Yee, 1998; Picchi & Peters, 1996; Rapp & Zock, 2010; Sadat et al., 2003) and some of them are based on linguistic knowledge such as language morphologies (e.g. Sadat, 2010b; Yu & Tsujii, 2009). We follow the research on extracting word translations without requiring any additional linguistic resources (e.g. Talvensaari et al., 2007; Tao & Zhai, 2005) and use the extracted knowledge to translate queries for cross-language information retrieval. In order to do CLIR, we construct query language models based on scores of extracted related terms from the comparable corpus. Trieschnigg, Hiemstra, de Jong, and Kraaij (2010) use a similar approach to train their translation model in CLIR using a corpus.The problem of exploring a comparable corpus and its combination with a dictionary to do CLIR has been studied before in Talvensaari et al. (2007) and Sadat (2010b). However, our methods in both extracting translation knowledge and constructing query translations are different from theirs. Talvensaari et al. (2007) have built a comparable corpus query translation program (Cocot) which we use as our baseline. They also combine the comparable corpus results with dictionary-based query translation (UTACLIR) and construct queries in the InQuery format, while we use a simple dictionary and construct query language models. Sadat (2010b) presents a two-stage corpus-based translation model which aims to find translations of a source word in the target language corpus and also translations of the target words in the source language corpus. The two stages contain bi-directional extraction of bilingual terminology from comparable corpora and selection of best translation alternatives based on a morphological analyzer. She has also exploited the linear combination of comparable corpus results with bilingual dictionaries. In both works, the impact of comparable corpora on cross-language information retrieval especially combining the extracted translations with dictionaries has been shown to be effectively positive.In this section, we propose a process for learning cross-lingual term associations from comparable corpora. As a first step, we extract translation knowledge using term co-occurrences in the comparable corpus alignments. In the second step, we propose a method based on term association network which exploits term associations in monolingual data as well as bilingual term associations to better extract translation knowledge. We further propose to use cross-outlier detection to filter out misleading translation candidates which are detected as outliers. We will present each step in more detail in the rest of this section.As the basic method, in order to extract term associations from the comparable corpus, we use the method used in Cocot, the comparable corpus query translation program which is proposed in Talvensaari, Pirkola, Järvelin, Juhola, and Laurikkala (2008). The intuition of this method is to use term co-occurrences in the alignments to extract term associations. The algorithm first calculates a weight mikfor each term siin document dkas:(1)mik=0iftfik=00.5+0.5×tfikMaxtfk×lnNTdlkotherwisewhere tfikis the frequency of siin document dk, Maxtfkis the largest term frequency in dkand dlkis the number of unique terms in the document. NT can be either the number of unique terms in the collection or its approximation. In our experiments, these frequencies are computed after stopword removal. This tf.idf modification is adopted from Sheridan and Ballerini (1996) who also used it in similarity thesaurus calculation. The weight of a target term tjin a set of ranked target documents D is calculated as:(2)Mj=∑r=1|D|mjrln(r+1),where D is the set of target documents aligned with a source document containing tj. The documents in D are ranked based on their alignment scores. Less similar documents, which appear lower in the list, are trusted less for translation and their weights are penalized. This penalization is achieved by ln(r+1) in the denominator.Finally, the similarity weight between a source term siand a target term tjis calculated as(3)wc(si→tj)=∑〈dk,D〉∈Amik×Mj‖si‖×(1-α)+α×‖tj‖‖T‖‾,where mikis the weight of source term siin the source document dk, Mjis the weight of target term tjin the set of target documents D which are aligned with the source document dk, A is the set of all alignments,‖si‖is si’s norm vector,‖T‖‾is the mean of the target term vector lengths, and α is a constant between 0 and 1 (we chose α=0.2 same as (Talvensaari et al., 2008)). In this formula, we use pivoted vector normalization scheme, instead of standard cosine normalization, to prevent harsh penalization of long feature vectors (that is, words with high document frequency) (Hashemi et al., 2010).Cocot only considers term co-occurrences in the alignments of source and target documents of the comparable corpus to extract translations, but we perceive that we can also consider term co-occurrences in the monolingual data to get better translation knowledge. Some terms in each language are about to happen in the same topics, such as ‘tennis’ and ‘Wimbledon’. We can use this information to improve the quality of extracted translations. Intuitively, two terms have a high chance of being translations of each other if the terms in their neighborhoods in the same language are highly correlated with each other. In other terms, if their neighborhoods are not correlated, they are less likely to be translations of each other even though they are obtained as translations. Thus, we propose to construct a network of related terms that consists of all the terms in the source language and all the terms in the target language. The edges between two terms in the same language are based on their co-occurrences in that language and the edges between terms in different languages are based on the mined associations. In our experiments we use mutual information as the links between terms in the same language and the normalized values extracted by the Cocot method as the links between terms in different languages. Fig. 1shows the structure of the network. In this figure, the thickness of the lines shows the importance of the edges. Intuitively, if two terms are translations of each other, their neighborhoods are strongly connected and if their neighborhoods are not strongly connected, they are not likely to be translations.As an example, in the extracted translations by Cocot in our experiments, the highest ranked translation for the term “Wimbledon” is not correct, and the correct translation is scored very low. This mistranslation can be because of the small number of aligned documents containing the term ‘Wimbledon’ in the comparable corpus. Our goal is to improve the translation quality of the list of terms extracted by the basic method by reranking the translation terms using the term network. As can be seen from Fig. 1 which is part of the network for the term ‘Wimbledon’, its neighborhood in its own language is more strongly connected to the neighborhood of its correct translation compared to the neighborhood of the incorrect translation. This observation leads us to rerank the list of translations based on the strength of the connection between their neighborhoods and the neighborhood of the source language term.Using this network, we propose to update the weight of the translation link between source and target terms s and t using general function f as:(4)w(s,t)=f(w0(s,t),sim(NS(s),NT(t)))where w0(s,t) is based on mined association between s and t, NS(s) is the set of source language terms in the neighborhood of s, NT(t) is the set of target language terms in the neighborhood of t and sim(NS(s), NT(t)) indicates the similarity of two neighborhoods, which can be computed based on the relevance scores of source language terms in NS(s) and target language terms in NT(t). In order to calculate the similarity of neighborhoods, we consider the paths between two terms (s and t) that go through their neighborhoods (NS(s) and NT(t)). The stronger the links between two neighborhoods, the more similar the neighborhoods will be.In our experiments, we consider one specific case of this model which consists of weights of all the paths with length one, two or three between the two specified source and target terms. The updated translation weight between source and target terms s and t is calculated as:(5)w(s,t)=f(w0(s,t),sim(NS(s),NT(t)))=αw0(s,t)+β∑si∈NS(s)wMI(s,si)·w0(si,t)+∑tj∈NT(t)w0(s,tj)·wMI(tj,t)+γ∑si∈NS(s),tj∈NT(t)wMI(s,si)·w0(si,tj)·wMI(tj,t)where α, β and γ are the parameters that control the influence of paths with lengths one, two and three respectively. In our experiments, we have set α=β=γ=1. wMIis the normalized expected mutual information weight (Manning, Raghavan, & Schütze, 2008) of co-occurring terms in the same language. We considered confidence intervals of mutual information in our estimates to select trustworthy neighbors. w0 is the normalized weight of the correlated terms in different languages. Using Cocot method, we calculate the similarity of source language term s to target language term t (i.e. wc(s→t)). In addition, we observe that we can also consider the similarity of target and source language terms as well (i.e. wc(t→s)). If term t is calculated as a translation for term s but s is not obtained as a translation for term t then they are unlikely to be translations of each other. So, we define w0(t,s) as the combined similarity of source and target language terms:(6)w0(s,t)=g(wc(s→t),wc(t→s))In our experiments, we use the sum of normalized similarity weights as function g. We examined the product of the weights as function g as well, and the results were similar.We normalize the mutual information scores between terms in the same language and raw translation association scores between terms in different languages before combining them using our proposed method. One basic normalization method is to use raw scores’ probability estimation, but one of its deficiencies is that it trusts low scores too much. Intuitively, high scores are more trustable than low ones, either in the extracted translations or terms co-occurring in the same language. So, the normalized value should drop sharply as the scores become smaller. To penalize low scores, we use exponential transformation in our experiments.In our obtained term associations, there exist some terms whose extracted translations are not correct, which leads to decrease in the accuracy of the extracted translations. These terms may for instance be high frequency terms having high entropies in the comparable corpus or low entropy terms which have appeared in only few aligned documents. Intuitively, it sounds reasonable to detect these exceptions using outlier detection methods to enrich translations either by omitting them from the created thesaurus or by obtaining their translations from other resources such as dictionaries.In this section, we introduce our approach to detect incorrectly translated terms (outliers) based on the distribution of term association weights in the same language and their related terms. We use a modified version of the method proposed by Papadimitriou and Faloutsos (2003) for cross-outlier detection using probabilistic criteria for automatic recognition of outliers.We consider the problem of detecting outliers in one language (L1) with respect to their extracted translations in the other language (L2). Thus, our goal is to find terms in L1 that “arouse suspicions” with respect to terms in L2 (Papadimitriou & Faloutsos, 2003). Intuitively, if the weight distribution of a term’s translation is different from that of its neighbors’ translations, the term is considered as an outlier. For instance, Fig. 2shows two sample terms and their neighborhood distributions and their translation relations. As can be seen, this method can detect the term ‘Persian’ as an outlier based on its neighborhood and on the other hand the term ‘Tehran’ is not detected as an outlier since it has a similar distribution with its neighborhood.Considering term t in language L1, we first collect its neighborhood of radius r in L1 (i.e.NL1(t,r)). For each term in the neighborhood of t, we define a locality neighborhood with radius α over its extracted translations in language L2 (see also Fig. 2). Then we calculatewˆL1,L2(t,r,α)which is the average of locality weights over all terms in the neighborhood as:(7)wˆL1,L2(t,r,α)≔∑q∈NL1(t,r)wL2(q,α)nL1(t,r)wherewL2(q,α)is the sum of the weights of the extracted translations of term q in language L2 in radius α andnL1(t,r)is the number of members ofNL1(t,r). Our outlier detection method relies on standard deviation of the calculated weights of neighbors in radius α which are terms in language L2. Thus, the standard deviation,σˆL1,L2(t,r,α), for term t in L1 with radius r and α is defined as:(8)σˆL1,L2(t,r,α)≔∑q∈NL1(t,r)(wL2(q,α)-wˆL1,L2(t,r,α))2nL1(t,r)Finally, a term t in L1 is considered as an outlier at radius r with respect to the extracted translations in L2 if:(9)|wˆL1,L2(t,r,α)-wL2(t,α)|>kσσˆL1,L2(t,r,α)where kσis a constant that determines what is a significant deviation. Typically kσ=3. It is noteworthy to mention that we only consider the left-hand side of Eq. (9) to sort terms based on how much they can be considered as outliers. So, considering only the left-hand side of the equation, we set kσ=3 and calculate the standard deviation for each term t in L1 using Eq. (8). Next, all the terms in L1 are sorted and the topmost terms are then selected as outliers. The two radii r and α are dynamically selected based on the neighborhoods or translations of each term (see Section 4.2). Our experiment results show that the average and standard deviation with respect to radius r give us useful information about the vicinity of the terms and therefore we can detect terms that could not be translated.One of our main goals in this research is to do cross-language information retrieval using the obtained cross-lingual associations from the comparable corpus. To achieve this goad, we translate queries from the source language to the target language and obtain translation knowledge from our comparable corpus using the explained methods in Section 3. Since some terms’ translations may not be obtained from comparable corpus, we detect them as outliers and use bilingual dictionaries to obtain their translations.In order to use the translation knowledge to do CLIR, we present three methods to translate queries by constructing their query language models in the target language. In the first two methods, we directly use the extracted translations to translate query words in L1 to their corresponding terms in L2. As the third method, we propose to use the extracted translation knowledge along with dictionary translations to construct the query language models. In the following, we present the details of the methods.Given query Q in language L1, for each query word, we use top k of its extracted associated terms in language L2 as its translations and construct the query language model in L2. In our proposed methods, we consider all query words to be equally important, so they will have equal weights in the query language model. Moreover, in the constructed query language model each translation term has a weight which is based on the weights of the extracted term associations.We use the method proposed in Shakery (2008) to construct a basic translation of the query in L2. Considering Q=q1,…,qnin L1 andt1i…tkias the top-k related terms in L2 to the query word qi(qi∈Q), the query language model in L2 is constructed as:(10)P(t|Q)=∑i=1n1np(t|qi)∑j=1kp(tj|qi)where p(tl∣qj) is the calculated weight using Eq. (5) and p(tl∣qj)>0 if tlis in the top k associated terms of qjand p(tl∣qj)=0 otherwise.We use the same method in the case of omitting outliers. We just do not consider the query words that are detected as outlier when constructing query language model. For instance, if the query consists of three terms and one of them is an outlier, we construct the language model based only on two of query words without considering the outlier.In the basic query translation method, for all the query words, we equally select the top k associated terms in L2. But we observe that the quality of associated terms for each term is different. For instance, as can be seen from Fig. 3, the translations of “sanction” could be found in the first position and the other next terms (“Iraq”, “Organization” and “US”) are not that much associated in meaning to “sanction”. On the other hand, considering the term “atomic”, we can see that the first few associated terms (“Agency”, “ElBaradei”, “Nucleus”, “Energy”, “Uranium” and “Rich”) are somehow related to “atomic” and can be used in the same context. So, they would be probably useful for query expansion. Thus, if we select k (e.g. k=3) equally for all the terms then for some terms (e.g. “sanction”) the selected translated terms may not be correct and for some other terms (e.g. “atomic”) some useful information may be ignored mistakenly. Intuitively, we can use the weight distribution of the associated terms to select the number of related terms for each term dynamically.We propose a method to select the best associated terms based on the knee point of the diagram as follows:(11)▵i=Ii-Ii+1where Iiis the association weight and ▵iis difference of two consecutive association scores. We determine the point with the maximum slope as:(12)kˆ=argmaxi∑▵ii+1wherekˆis the estimated cut off for the most related extracted translations. The presented method can also be used to dynamically select the most associated neighbors to a term in a same language when the scores are calculated based on mutual information. After dynamic selection of topkˆrelated terms for each query word, we construct the query language model using the method explained in Section 4.1.We do not expect to extract suitable translation knowledge for all the terms from the comparable corpus. For instance, since we use distribution of term frequencies, translations for the high frequency terms could not be obtained. So, we construct query language model for several combined CLIR approaches based on dictionary-based query translations and extracted translations from comparable corpora.In this combined method, in order to translate each query word, we use its dictionary translations as well as the extracted translations from comparable corpus. Since dictionaries are reliable translation resources, we give their translations higher weights compared to our extracted translations from comparable corpus. The weights of the dictionary translations are set to be the highest extracted translations from the comparable corpus for that term plus a small amount which is the difference between the highest and the second highest extracted translations from comparable corpus for that term multiplied by the factor 2. Afterwards, the query language model construction is based on the method presented in Section 4.1.In this method, first the query words are translated using dictionary and then for those query words which could not be found in dictionary, we use their extracted translations from comparable corpus. Formally, let Q=q1,…,qnbe the query. Since each query word is translated either by dictionary or by comparable corpus, the query language model in L2 is constructed as(13)P(t|Q)=1n×kift∈Dic∑i=1n1np(t|qi)∑j=1kp(tj|qi)otherwisein which k is the top translations for term t that can be calculated dynamically for comparable corpus extracted translations.In this method, first the query words are translated using comparable corpora then those query words which are detected as outliers, are translated by dictionary. The query language model construction is the same as previous section.

@&#CONCLUSIONS@&#
In this work, we presented methods to mine translation knowledge from comparable corpora. The most notable presented method was based on network of terms (TAN) which consists of correlations of terms and their neighborhoods. Cross-language information retrieval experiments show that using TAN method and selecting dynamic number of translation terms for each query word significantly outperforms the basic translation extraction method. Therefore, neighborhood information of terms could help to obtain more accurate translation terms from comparable corpus.Furthermore, we have done translation validity check using our proposed outlier detection method and have shown that we can detect mistranslated terms by their neighborhoods. The results show that detecting good outliers and translating only a few number of them with dictionary or even omitting them from the query words improves the retrieval performance. Also, our experiments in combining extracted translations from comparable corpus with the dictionary translations show that using only dictionary performs poorly, because of lack of proper nouns which are essential query keywords. Thus, one of the success reasons of comparable corpus is the ability to translate these OOV terms. Another advantage of comparable corpus is finding good query expansion terms.In our future work, it will be interesting to use the extracted translation knowledge to improve the quality of the created corpus, by using the extracted term associations as an additional resource to translate source language keywords and also improving its quality through an iterative construction process. In this research we present the TAN method and calculate the similarity of neighborhoods by the paths of lengths one, two and three. Thus as an interesting future direction, we are going to test more complex methods to compare terms’ neighborhoods in the TAN model and run the experiments based on the new extracted translations. Also, we will tune the parameters of the created network to investigate the importance of each source of evidence for reranking the translations.