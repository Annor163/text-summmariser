@&#MAIN-TITLE@&#
Feature selection methods and their combinations in high-dimensional classification of speaker likability, intelligibility and personality traits

@&#HIGHLIGHTS@&#
We study high-dimensional feature selection for paralinguistic classification.New supervised and unsupervised methods overfit less than forward selection.Combined selection methods and kNN challenge popular high-dimensional classifiers.In each task, the best methods show improved performance using much fewer features.We report features for speaker traits that we found using the selection methods.

@&#KEYPHRASES@&#
Feature selection,Pattern recognition,Machine learning,Computational paralinguistics,

@&#ABSTRACT@&#
This study focuses on feature selection in paralinguistic analysis and presents recently developed supervised and unsupervised methods for feature subset selection and feature ranking. Using the standard k-nearest-neighbors (kNN) rule as the classification algorithm, the feature selection methods are evaluated individually and in different combinations in seven paralinguistic speaker trait classification tasks. In each analyzed data set, the overall number of features highly exceeds the number of data points available for training and evaluation, making a well-generalizing feature selection process extremely difficult. The performance of feature sets on the feature selection data is observed to be a poor indicator of their performance on unseen data. The studied feature selection methods clearly outperform a standard greedy hill-climbing selection algorithm by being more robust against overfitting. When the selection methods are suitably combined with each other, the performance in the classification task can be further improved. In general, it is shown that the use of automatic feature selection in paralinguistic analysis can be used to reduce the overall number of features to a fraction of the original feature set size while still achieving a comparable or even better performance than baseline support vector machine or random forest classifiers using the full feature set. The most typically selected features for recognition of speaker likability, intelligibility and five personality traits are also reported.

@&#INTRODUCTION@&#
Automatic paralinguistic analysis of speech signals aims to uncover aspects of speech that are not related to the linguistic content of the signal (Schuller et al., 2013). Typical problems include the recognition of a speaker's age, gender, emotional state and possible altered states such as sleepiness or intoxication. The two main approaches to these classification and regression problems are (1) to design the system specifically for the paralinguistic analysis task using expert knowledge in the domain or (2) to generate a large number of suitably high-level features, typically depicting some aspect of the speech signal over several seconds or one utterance, and then apply generic machine learning methods to the high-dimensional feature data. This study is concerned with the latter approach, focusing on automatic selection of useful signal features with the goal of improving classification performance from a large, non-selective baseline feature set and in order to gain better understanding of the given paralinguistic analysis tasks.In machine learning, high-dimensional feature spaces are sparsely populated by limited training data since the number of data points inside a volume unit decreases with an increasing feature space dimensionality. This effect, commonly referred to as the curse of dimensionality, weakens the reliability of trained analysis systems (Duda et al., 2001; Theodoridis and Koutroumbas, 2003) as overfitting them to the data becomes easier. However, if the complete feature set F is comprehensive and contains informative features, then certain feature subsets out of the vast amount of possible subsets (2|F|−1) should define lower-dimensional feature spaces in which learning is more reliable with the limited training data. Provided that such feature spaces can be found, even a basic analysis system could perform better than a complex system operating in the complete feature space due to the lessened effect of the curse of dimensionality. When the high-dimensional analysis problem is viewed in this way, it becomes one of finding these feature subsets by means of feature selection. Therefore, this study investigates a feature-selection approach to tackle paralinguistic classification of speech when a large and varied set of high-level features is available (Schuller et al., 2013). More specifically, the focus is on a problem of finding a robust subset of features when the overall number of potential features highly exceeds the number of data samples available for training and evaluation of the system, making the process of feature selection highly susceptible to overfitting. The remainder of Section 1 discusses issues related to feature selection in pattern recognition (Section 1.1) and outlines the specific aims of the study (Section 1.2).Feature selection algorithms are often used to reduce feature space dimensionality in pattern classification and regression. This study focuses on feature selection for supervised classification, i.e., classification of data patterns – consisting of the selected features – into predefined categories that have meaning in the real world. Automatic feature selection can be formulated as the problem of finding the best possible subset S of features from an initial, and possibly a very large, set of features F (i.e., S⊂F). The learning of a more compact set of features can induce some or all of the following benefits (Blum and Langley, 1997; Reunanen, 2003):1.Enhanced classification performance due to the removal of noisy or unreliable features.Lower computational costs in the final system due to reduced dimensionality in feature extraction, model training and classification.Simpler classifiers with less input variables, which often leads to better generalization ability towards new samples.Better hands-on understanding of the classification problem through discovery of relevant and irrelevant features.Since the ultimate goal is to perform classification of data samples, one could define the optimal subset of features as the one that provides the best classification ability in the given task as measured by a criterion function G(S, D, M)=c, where D denotes the data set used and M denotes the classification model (with its parameters) applied in the task. Value c of the criterion function can correspond to the overall classification performance on D in the given task (in the so-called wrapper methods) or be heuristically defined otherwise (as in the filter methods; Blum and Langley, 1997; Guyon and Elisseeff, 2003; Kohavi and John, 1997). However, the above definition already contains at least two potential problems. First, the number of possible feature subsets grows exponentially as a function of the initial feature pool size, making exhaustive search of the best subset impossible for all but the simplest selection problems (the search problem). Second, the value of the criterion function has to be computed from a finite number of data points D available for the selection process and there are no guarantees that the locations of local or global maxima of the criterion function with respect to S will remain the same for previously unseen data, i.e., when D is replaced by other data (the generalization or overfitting problem; Reunanen, 2003). In general, the more there are features to be considered in F and the less there are labeled data D available for the feature selection process, the higher the risk that the chosen feature set (out of a very large population of candidate feature sets) performs well on a given small training data set, “by chance”, but its predictive power generalizes poorly to new data sets.In order to tackle the search problem, all practical algorithms apply some heuristics to guide the search process, either explicitly as with the wrapper methods or indirectly as with the filter methods (Blum and Langley, 1997). Sequential backward elimination (SBE), originally described by Marill and Green (1963), starts from the complete set of features and sequentially eliminates the one whose elimination results in the best score G(S, D, M). A feature set of size d is thus given by(1)Sd=Sd+1\argmaxfG(Sd+1\f,D,M).Sequential forward selection (SFS), proposed by Whitney (1971), works in the opposite direction: starting from an empty set, the feature set is iteratively updated by including, in each step, the feature f which results in maximal score G(S, D, M). Thus, the feature set of size d is given by(2)Sd=Sd−1∪argmaxfG(Sd−1∪f,D,M).Typically, these methods are used as wrapper feature selection methods such that the criterion function G(S, D, M) is evaluated using an actual classifier M which is trained and evaluated on different parts of the data set D. They are greedy search algorithms, as they always exclude or include the most promising feature. Thus, the contribution of including or excluding a new feature is measured with respect to the set of previously chosen features using a hill-climbing scheme in order to optimize the criterion function G(S, D, M), making these approaches susceptible to its local maxima with respect to S (Blum and Langley, 1997). The feature sets found by SFS and SBE are nested, i.e., each feature set found by them is a subset of each larger feature set found earlier or later in the search.Pudil et al. (1994) proposed the improved “floating” versions of SBE and SFS, which after exclusion or inclusion of a new feature, respectively, include or exclude as many previously excluded/included features as possible without decreasing the previous scores G(Sd, D, M) associated with each feature set size. The sequential floating search algorithms do not have the nesting property and are not strictly greedy, potentially improving their performance (Pudil et al., 1994).In the family of so-called embedded algorithms (see e.g., Blum and Langley, 1997 for a review), feature selection occurs by the internal mechanisms of the classification algorithm. For example, decision trees (e.g., CART by Breiman et al., 1984) and their variants such as random forests (Breiman, 2001) carry out recursive partitioning of the data based on features that are the most useful in distinguishing between different data classes.Finally, the filter methods attempt to perform feature selection by replacing the role of the classifier M in the criterion function G(S, D, M) by a heuristic method to assess the relevance of features and their combinations, i.e., independently of the classifier used. This can be accomplished by using, e.g., measures of class separability in the feature distributions such as divergence or Bhattacharyya or Mahalanobis distance (Marill and Green, 1963; Theodoridis and Koutroumbas, 2003).In the simplest type of filter algorithms, features are individually assigned scores which are assumed to reflect their usefulness in the intended classification task. The features can then be ranked by their importance according to the score. The ranking can be used to sequentially select a given number of best features, or the filter algorithm itself can provide a means to determine the size of the feature set, e.g., a threshold value for the score above which the features are considered useful in the task. Other filter algorithms are feature subset selection algorithms (Kohavi and John, 1997; Theodoridis and Koutroumbas, 2003), in that they consider feature subsets jointly using some criterion, but in contrast to the wrapper methods which also perform feature subset selection, the filter methods do not evaluate the subsets directly using the target classifier. Examples include correlation-based feature selection (CFS; Hall, 1999) and the minimal-redundancy–maximal-relevance (MRMR) approach (Peng et al., 2005). CFS and MRMR analyze correlation and mutual information, respectively, and attempt to maximize it between the features and the class information while simultaneously minimizing it between features in the selected feature set. Filter methods are typically faster to compute than wrapper methods, but do not have direct access to the performance on the actual classification task and do not take into account the interaction between the chosen features and the classifier used. On the other hand, this reduces the risk that the feature selection overfits to the training data when used in conjunction with classifiers that themselves are prone to overfitting. Also, in order to alleviate the overfitting problem, combinations of different feature selection algorithms have recently been used in some studies (Pohjalainen et al., 2012; Saeys et al., 2008).As can be seen from the above discussion, the generalization problem is inherently tied to the search problem, especially in the case of wrapper algorithms. The more the search relies on sequential steps based on the local maximum gain in the criterion function, the more important it becomes that the criterion function of the available data truly follows the topology of all of the data that the classifier will be applied to. The practical way to alleviate this problem is to have more data for training and development sets and to ensure that the division into training and development data is performed carefully so that there are no unrealistic similarities between them (e.g., no same talkers or identical linguistic content when performing classification of spontaneous speech). However, data collection and labeling is often expensive and ideally the feature selection algorithms should provide useful results with as little data as possible.The practical measure of overfitting is to test the system on a set of held-out test data (Reunanen, 2003) after the feature selection and classifier training has been performed using the training and development sets. However, the use of an independent test set does not provide help in the selection of good features as such, because any observations on the test set performance that are propagated back to the system design will basically endanger the validity of the test set itself, again leading to the potential problem of overfitting. Instead, a proper use of held-out validation set simply shows whether the proposed methods generalize well or not.In this study, new feature selection methods based on various criteria, as well as methods for their combination, are proposed and applied with a basic nearest-neighbor classifier in a set of challenging paralinguistic speaker trait recognition tasks (Schuller et al., 2012). The data is characterized by a very large number of features and a small number of instances, making the tasks especially critical for robustness and generalization capabilities of feature selection algorithms. Therefore, the avoidance of overlearning in the selection phase is a central concern.The results obtained are compared against various baselines in both feature selection and pattern classification. In feature selection, the goal is to validate the proposed selection methods as well as to investigate the benefits of combining different types of selection criteria. In classification, the goal is to compare the feature selection approach, whose basic idea was outlined in the beginning of the introduction, against high-dimensional pattern classification systems. Moreover, analyses of the types of acoustic features selected for binary classification of speaker likability, intelligibility and the Big Five personality traits are aimed at uncovering information related to these speech analysis tasks. The study is a continuation of the authors’ previous work in the Interspeech 2012 Speaker Trait Challenge (Pohjalainen et al., 2012; Schuller et al., 2012).As the source of evaluation material, this study uses three databases, which are briefly described.The Speaker Likability Database (SLD; Burkhardt et al., 2011) is a subset of the German Agender database originally recorded to study automatic age and gender recognition from telephone speech (Burkhardt et al., 2010). The speech has been recorded over fixed and mobile telephone lines using a sampling rate of 8kHz. A set of 800 speakers, each speaking one utterance, comprises the SLD subset. The speaker population has been balanced for age and gender and 18 utterance types are included. In the generation of the labelings, listeners judged the likability of each speaker on a seven-point scale and the likability of each utterance was established using evaluator-weighted estimator (EWE; Grimm and Kroschel, 2005) which weights the reliability of each listener based on the cross-correlation of his or her ratings with ratings averaged over listeners. The EWE rating was discretized into two categories, herein referred to as “likable” and “not-likable”, based on the overall median EWE rating.The “NKI CCRT Speech Corpus” (NCSC; van der Molen et al., 2012) was used as material in recognizing speaker intelligibility. The corpus contains recordings of utterances spoken in Dutch by 55 speakers (45 males and 10 females) who underwent concomitant chemo-radiation treatment (CCRT) for inoperable tumors of the head and neck. Recordings were made both before and after CCRT. Thirteen expert listeners rated the intelligibility of each recording on a seven-point scale and, similarly to the likability data, EWE ratings were computed and discretized to “intelligible” and “not-intelligible” categories based on the global median.The “Big Five” or “OCEAN” personality characteristics – openness to experience, conscientiousness, extraversion, agreeableness and neuroticism – were recognized using the “Speaker Personality Corpus” (SPC; Mohammadi and Vinciarelli, 2012) as material. The corpus consists of 640 speech audio clips, with a single speaker in each clip of approximately 10s, randomly extracted from news in French broadcast by Radio Suisse Romande, the Swiss national broadcast service, during February 2005. The total number of speakers is 322 with the most frequent speaker appearing in 16 clips. The personality assessment was performed by 11 judges, each of whom listened to all the clips, by filling the BFI-10, a 10-item personality assessment questionnaire (Rammstedt and John, 2007). In order to generate the data labelings for each of the five personality traits, each clip was labeled as representing a personality trait if at least six judges (the majority) gave it a score that was higher than their personal average score for the same trait. Otherwise the clip was labeled as not representing the trait.For each utterance in each of the three databases, 6125 long-term, utterance-level features have been extracted by the organizers of the Interspeech 2012 Speaker Trait Challenge (Schuller et al., 2012) using the openSMILE feature extractor (Eyben et al., 2010). For the present work, only these data sets consisting of 6125 features per utterance and the associated class labelings of the data instances (utterances) were used.Each of the three databases was partitioned in the Speaker Trait Challenge into three data sets: a training set (denoted by Train), a development set (denoted by Development) and a test set (denoted by Test). The grounds for partitioning the utterances of each of the three databases into the Train, Development and Test sets are given by Schuller et al. (2012). Table 1shows the number of instances for each such subset in the likability, intelligibility and personality data.The problem in each task is to automatically classify data points, representing speech audio clips, with respect to traits X∈{likable, intelligible, open, conscientious, extraverted, agreeable, neurotic} as either “X” or “not-X”, i.e., whether the trait is present or not. Evaluation of the system is performed by comparing the generated hypotheses against the ground truth labelings associated with the data sets.

@&#CONCLUSIONS@&#
Classification of high-dimensional paralinguistic speaker trait data was approached with a special focus on feature selection. Several new feature selection algorithms with different supervised, partially supervised and unsupervised selection criteria were presented, as well as methods for combining the algorithms. These were evaluated and compared against widely used baseline methods from the perspective of both feature selection and pattern classification. In addition, combined selection methods were used to identify the most relevant features for the seven analysis tasks consisting of speaker likability, intelligibility and the Big Five personality traits.The results demonstrate five things: (1) the proposed methods are more resistant against overlearning in feature selection than conventional, hill-climbing forward selection; (2) a huge reduction of feature space dimensionality is achieved without sacrificing performance on the held-out test data, indicating potential computational savings; (3) furthermore, the nearest-neighbor classification performance is improved by many individual and combined feature selection methods, suggesting a potentially improved ability of classifiers to generalize with limited training data when using the proposed methods for feature selection; (4) due to different amounts of overfitting generally shown by different feature selection algorithms, the performance of any given method is difficult to predict without independent evaluation data; and (5) by combining supervised and unsupervised feature selection methods and a basic classifier, the performance of state-of-the-art high-dimensional pattern classification methods can be reached. In future research, potential directions suggested by the results are automatic selection of a feature selection method using independent evaluation data and the application of the proposed methods to various practical analysis problems with different classifiers.