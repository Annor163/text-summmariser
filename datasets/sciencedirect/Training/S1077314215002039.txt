@&#MAIN-TITLE@&#
Semantic super-resolution: When and where is it useful?

@&#HIGHLIGHTS@&#
We highlight the need for and advantages of using semantic information for single-image super-resolution.Two well-known state-of-the-art methods and one baseline method are extended to use this semantic information.We give a theoretical analysis and explore the practical benefits on three different datasets.

@&#KEYPHRASES@&#
Single-image super-resolution,Image enhancement,Semantic segmentations,Sparse coding,

@&#ABSTRACT@&#
Recent algorithms for exemplar-based single image super-resolution have shown impressive results, mainly due to well-chosen priors and recently also due to more accurate blur kernels. Some methods exploit clustering of patches, local gradients or some context information. However, to the best of our knowledge, there is no literature studying the benefits of using semantic information at the image level. By semantic information we mean image segments with corresponding categorical labels. In this paper we investigate the use of semantic information in conjunction with A+, a state-of-the-art super-resolution method. We conduct experiments on large standard datasets of natural images with semantic annotations, and discuss the benefits vs. the drawbacks of using semantic information. Experimental results show that our semantic driven super-resolution can significantly improve over the original settings.

@&#INTRODUCTION@&#
Single-image super-resolution (SISR) is a branch of image enhancement that tries to add high-frequency information to low-frequency images in order to improve their sharpness during upsampling. Because SISR is an ill-posed problem these algorithms use different kinds of image priors to guide the creation of a high-resolution (HR) output. One popular class of priors assumes continuity between intensity values of neighboring pixels. They encourage the algorithm to find solutions that have as little change in neighboring intensity values as possible while still being faithful to the low-resolution (LR) input. This tendency of slow spatial variation is also observed in natural images. Some SISR algorithms get their priors from a database of image examples [1–6]. These algorithms are trained on a collection of natural images in which (by down- and upsampling) they find numerous examples of corresponding local LR/HR combinations, either by working with small intensity patches [1] or by analyzing gradients [7].In this paper, we would like to highlight an often overlooked problem that comes with the ill-posed nature of the SR process. The transformation from an HR image patch to an LR image patch brings with it an inherent loss of information, meaning that a large number of different possible HR patches, when downsampled to LR space, are transformed onto one single LR point. This brings with it a certain ambiguity concerning the inverse transformation from LR to HR. Some algorithms simplify this and assume that finding the best matching LR patch in the database will result in the retrieval of the best available HR patch. Others (e.g. Freeman et al. [1]) try to enforce continuity along neighboring HR patches by choosing the proper candidate from k nearest neighbors instead of the closest match. This however gives no guarantee of a more correct solution, but rather a self-consistent one.When we only look at local features, such as the ones depicted in Fig. 1, we have a difficult time estimating what the HR versions should look like exactly. We can make a guess, e.g.by saying that edges should look sharper, but this still leaves room for many different solutions. If we then add semantic information about the scene by looking at a zoomed out version of the image (e.g.the top image in Fig. 1) we can make a much more educated guess. In the case on top: the local feature is part of a car, more specifically the right tail light. Using this information we should be able to get a better HR patch. If however the zoomed out image is the one at the bottom of Fig 1, then we should reach an entirely different HR patch depicting an eye. Torralba [8] shows some very interesting examples of this problem for object detection. His example of a vague ‘blob’ in a blurry image can be interpreted as different things depending on the context in which it is shown (in the street, inside) or on its orientation (horizontal: car, vertical: person crossing the street).In the following sections, we will analyze when and where using semantic context information can help super-resolution most, and how this information can be incorporated as an extra prior in standard methods with minimum changes. Deriving substantially new methods or sophisticated methods for certain semantic contexts is out of the scope of our study. Moreover, we assume that the semantic information is known at pixel-level for most of our experiments. We then also evaluate the framework under real conditions, when the semantic information is less accurate. We have a number of hypotheses to check.1.How can we adapt current super-resolution methods to use semantic information?How can semantic information push the theoretical limits of what super-resolution can do?How can semantic information help super-resolution in practice?When is using semantic information desirable?

@&#CONCLUSIONS@&#
