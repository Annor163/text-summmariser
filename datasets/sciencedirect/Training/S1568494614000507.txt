@&#MAIN-TITLE@&#
Churn prediction using comprehensible support vector machine: An analytical CRM application

@&#HIGHLIGHTS@&#
Support vector machine (SVM) is made transparent by hybridizing it with NBTree.Its effectiveness is tested on a medium scale unbalanced dataset related to bank credit card customer churn in analytical CRM.Feature selection is carried out by SVM-RFE algorithm.Proposed hybrid outperformed other techniques. Feature selection resulted in rules with short lengths and better comprehensibility of the system.The generated rules act as an early warning expert system to the bank management.

@&#KEYPHRASES@&#
Churn prediction,Support vector machine,Rule extraction,Naive Bayes Tree,Machine learning and customer relationship management,

@&#ABSTRACT@&#
Support vector machine (SVM) is currently state-of-the-art for classification tasks due to its ability to model nonlinearities. However, the main drawback of SVM is that it generates “black box” model, i.e. it does not reveal the knowledge learnt during training in human comprehensible form. The process of converting such opaque models into a transparent model is often regarded as rule extraction. In this paper we proposed a hybrid approach for extracting rules from SVM for customer relationship management (CRM) purposes. The proposed hybrid approach consists of three phases. (i) During first phase; SVM-RFE (SVM-recursive feature elimination) is employed to reduce the feature set. (ii) Dataset with reduced features is then used in the second phase to obtain SVM model and support vectors are extracted. (iii) Rules are then generated using Naive Bayes Tree (NBTree) in the final phase. The dataset analyzed in this research study is about Churn prediction in bank credit card customer (Business Intelligence Cup 2004) and it is highly unbalanced with 93.24% loyal and 6.76% churned customers. Further we employed various standard balancing approaches to balance the data and extracted rules. It is observed from the empirical results that the proposed hybrid outperformed all other techniques tested. As the reduced feature dataset is used, it is also observed that the proposed approach extracts smaller length rules, thereby improving the comprehensibility of the system. The generated rules act as an early warning expert system to the bank management.

@&#INTRODUCTION@&#
CRM is a process or methodology used to learn more about customers’ need and behaviours in order to develop stronger relationship with them. CRM involves the continuous use of refined information about current and potential customers in order to anticipate and respond to their needs and draws on a combination of business process and Information Technology to discover the knowledge about the customers and answer questions like, “who are the customers?”, “what do they do?” and “what they like?”. Therefore the effective management of information and knowledge is central and critical to the concept of CRM for;•Product tailoring and service innovation (web-sites tailored to customer needs, taste experience and the development of mass customisation).Providing a single and consolidated view of the customer.Calculating the lifetime value of the customer.Designing and developing personalized transactions.Multichannel based communication with the customer.Cross-selling/up-selling various products to customers.Different definition of CRM put emphasis on different perspectives. CRM's technological perspective was stressed in [1], its knowledge management perspective was emphasized in [2] and its business re-engineering and continuous improvement perspective is presented in [3].We can think about CRM at three levels, strategic, analytical and collaborative.Strategic CRM: It is focused on development of a customer-centric business culture. Product, production and selling are the three major business orientations identified by Kotler [4].Analytical CRM: Analytical CRM builds on the foundation of customer information. Customers’ data may be found in enterprise wide repositories, sales data (purchasing history), financial data (payment history and credit score), marketing data (campaign response, loyalty scheme data) and service data. With the application of Data Mining, the industry can then interrogate this data and intelligent interrogation provides answers to questions, such as, “who are our most valuable customers?”, “which customer have the highest propensity to switch to competitors?”, “which customers would be most likely to respond to particular offer?” and so on.Collaborative CRM: Staff members from different departments can share information collected when interacting with customers [5].Churn prediction problem is an analytical CRM application and using the extracted rules from SVM, service providers can get transparent and efficient insight about their customers and can make better policies to retain their existing customers.Over the decade and half, the number of customers with banks and financial companies is increasing by the day and this has made the banks conscious of the quality of the services they offer. The phenomenon, called ‘churn’ i.e. shifting loyalties from one service provider to another occurs due to reasons such as availability of latest technology, customer-friendly bank staff, low interest rates, proximity of geographical location and varied services offered. Hence, there is a pressing need to develop models that can predict which existing ‘loyal’ customer is going to churn out or attrite in near future.Service organizations need to be proactive in understanding the customers’ current satisfaction levels before they attrite [6]. Research indicates that the online bank customers are less price-conscious than traditional bank customers with less probability of churning out [7]. Targeting customers on the basis of their (changing) purchase behaviour could help the organizations do better business and loyalty reward programmes helps the organizations build stronger relationships with customers [8].In the financial services industry two “critical” churn periods are identified [9], the first period is the early years after becoming a customer and the second period is after being a customer for some 20 years. A comparative study on Logistic Regression and Neural Network for subscriber data of a major wireless carrier is carried out and it is concluded that using sophisticated neural net $93 could be saved per subscriber [10].Machine learning techniques such as; multilayer perceptron, Hopfield neural network, self-organizing neural networks [11], decision tree [12], multivariate regression analysis [13], logistic regression and random forest [14], emergent self-organizing feature maps (ESOM) [15], neural networks [10], SVM [18], genetic algorithms and rough set theory [16], ensemble with majority voting [17] and hybrid neural networks [18] are employed to solve churn prediction problems. Glady et al., proposed a churn prediction model using customer life time value (CLV), which is defined as the discounted value of future marginal earning, based on customers’ activity [19]. Hu presented a comparative study of different machine learning algorithms [20]. The trend in marketing towards building relationships with customers continues to grow and marketers have become increasingly interested in retaining customers over the long run [21]. Hyung-Su and Young-Gul suggested a performance measurement framework called CRM score card to diagnose and asses a firm's CRM practice [22].Churn prediction problem is one of the most important applications of analytical CRM in finance. Banks would be interested to know their about-to-churn customers and the proposed rule extraction approach do not only provide better predictions but also comprehensibility of the system is improved. Feature selection using SVM-RFE algorithm in the first phase reduces the dimensionality of the data by yielding the key attributes in the data. Thus, less number of rules and smaller rules are extracted resulting in the improvement of the comprehensibility of the system. During the research study in this Paper, various standard balancing techniques are employed such as, random under-sampling, random over-sampling and SMOTE.Remaining paper is organized as follows. Section 2 provides the details about SVM and literature survey of rule extraction from SVM. Proposed rule extraction approach is then detailed in Section 3. Dataset description and experimental setup followed during this research study is presented in Section 4. Next section provides the detailed empirical analysis and observations. Section 6 concludes the paper.The SVM is a learning procedure based on the statistical learning theory [23]. It has been used in wide variety of applications such as gene analysis [24], financial time-series forecasting [25], marketing [26], patent classification [27], face recognition [28] and predicting longitudinal dispersion coefficients in natural rivers [29].For classification problems, the main objective of SVM is to find an optimal separating hyperplane that correctly classifies data points as much as possible and separates the points of two classes as far as possible, by minimizing the risk of misclassifying the training samples and unseen test samples [30]. The training points that are closest to the optimal separating hyperplance are called support vectors and other training examples are irrelevant for determining the binary class boundaries as shown in Fig. 1. To deal with non-linearly separable datasets problems, SVM first projects the data into a higher dimensional feature space and tries to find the linear margin in the new feature space.Given a set of pointsb∈ℜwith i=1,…,N each pointxibelongs to either of two classes with the labelyi∈{−1,+1}[31].The optimization problem for the SVM can be depicted as follows:(1)min12〈w,w〉Subject toyi(w⋅xi+b)≥1∀xiThe SVM classification function for classifying linearly separable data can be written as:(2)f(x)=〈w,x〉+b=∑i=1lyiαi〈xi,x〉+b;This is also known as hard margin, where no room is given for errors. It is observed that most of the time it is linearly non-separable. Hence slack variableξis introduced to allowξerror and the optimization function takes the form of (3) as shown below:(3)min12〈w,w〉+C∑i=1lξiSubject toyi(w⋅xi+b)≥1∀xiTo deal with the problem of non-linearly separable dataset, SVM first projects the data into a higher dimensional feature space using various kernels and tries to find the linear margin in the new feature space. The optimization function can be depicted as shown below:(4)min12〈w,w〉+C∑i=1lξiSubject toyi(w⋅φ(x)+b)≥1∀xiThe optimal hyperplane separating the binary decision classes is given by (5):(5)f(x)=∑i=1lyiαiK(xi,x)+bwhereK(Xi,X)=φ(Xi)Tφ(X)is taken with a semi-positive definite kernel satisfying the Mercer theorem [32].For kernel functionK(.,.)one typically has the following choices:(6)K(X,Xi)=(XiT⋅X),(Linear kernel)(7)K(X,Xi)=XiT⋅X+1Cd,(Polynomial kernel of degreed)(8)K(X,Xi)=exp−(X−Xi)2σ2,(RBF kernel)(9)K(X,Xi)=tanh(κXiT⋅X+θ),(MLP kernel)whered,C,σ,κandθare constants.The algorithm is similar to the classical recursive partitioning schemes, except that the leaf nodes created are Naive-Bayes categorizers instead of nodes predicting a single class. A threshold for continuous attributes is chosen using the standard entropy minimization technique. The utility of a node is computed by discretizing the data and computing the 5-fold cross validation accuracy estimate of using Naive-Bayes at the node.Intuitively, it is attempted to approximate whether the generalization accuracy for a Naive-Bayes classifier at each leaf is higher than a single Navie-Bayes classifier at the current node. To avoid splits with little value, it is defined that a split to be significant if the relative reduction in error is greater than 5% and there are at least 30 instances in the node.Input: a set T of labelled instances.Output: a decision tree with Navie-Bayes categorizers at the leaves.1For each attribute Xi, evaluate the utility, u(Xi), of a split on attribute Xi, for continuous attributes, a threshold is also found at this stage.Let j=argmaxi(ui), i.e. the attribute with the highest utility.If ujis not significantly better than the utility of the current node, create a Naive-Bayes classifier for the current node and return.Partition T according to the test on Xj. If Xjis continuous, a threshold split is used; if Xjis discrete, a multi-way split is made for all possible values.For each child, call the algorithm recursively on the portion of T that matches the test leading to the child.Although SVM generally predicts well, it is still a black box model, i.e. the knowledge learnt by SVM during training is not directly interpretable by the user. Many researchers tried to treat this accuracy vs. comprehensibility trade-off by converting the black box, high accurate models to transparent models via rule extraction in the context of neural networks [33].Extensive work was reported in developing rule extraction techniques for neural networks [34] but less work is reported in rule extraction from SVM. In general, rule extraction techniques are divided into two major groups i.e. decompositional and pedagogical. Decompositional techniques view the model at its minimum (or finest) level of granularity (at the level of hidden and output units in case of ANN). Rules are first extracted at individual unit level, these subset of rules are then aggregated to form global relationship. Pedagogical techniques extract global relationship between the input and the output directly without analyzing the detailed characteristics of the underlying solution. The third group for rule extraction techniques is eclectic which combines the advantages of the decompositional and pedagogical approaches. Using rule extraction a learning system might discover salient features in the input data whose importance was not previously recognized [35].We now briefly review the works reported in rule extraction from SVMs. SVM+prototype determine the prototype vectors (also known as cluster centres) for each input class using K-means clustering algorithm [36]. Based on the centre of the cluster, named prototype, and the farthest support vector, interval or ellipsoid, if-then rules can be created. The main drawback of this algorithm is that the extracted rules are neither exclusive nor exhaustive which results in conflicting or missing rules for classification of new data instances. Fung et al. developed a rule extraction algorithm, which extracts non-overlapping rules by constructing hyperplane with axis parallel surface [37]. They first transformed the problem to a simpler, equivalent variant and constructed the hyper-cubes by solving linear programs and each hypercube is then transformed to an if-then rule.RulExtSVM [38] comprising three steps is used for extracting if-then rules using intervals defined by hyper-rectangular forms. First step is the generation of a hyper-rectangle using the intersection of the support vectors with the SVM decision boundary. During second step, the initial rule set is tuned in order to improve rule accuracy. In the final step redundant rules have been removed to obtain more concise rule set. The disadvantage of this algorithm is the construction of as many hyper-rectangles as the number of support vectors that can be computationally very expensive. Later, a hybrid rule extraction technique is proposed where after developing the SVM model using training set, they used the developed model to predict the output class labels for the training instances [39,40]. Using decision tree (C4.5) rules are generated. The quality of the extracted rules is then measured using the Area Under the Receiver Operating Characteristic Curve (AUC) [41].Hyper-rectangle rules extraction (HRE) approach first constructs hyper rectangles according to the prototypes and the support vectors (SVs), then these hyper-rectangles are projected onto coordinate axes and if–then rules are generated [42]. Fuzzy rule extraction (FREx) applies triangular fuzzy membership function and determines the projection of the support vectors in the coordinate axes and each support vector is then transformed into a rule [43]. Later, a multiple kernel-support vector machine (MK-SVM) is proposed to improve the explanation capacity of SVM [44]. SQRex-SVM is used to directly extract the rules from support vectors extracted using SVM [45].Recently, Farquad et al. proposed a hybrid rule extraction approach using SVM and the extracted rules are tested for bankruptcy prediction in banks [46,47]. They first extracted the support vectors and then used these support vectors along with their corresponding actual target values to train fuzzy rule based system, decision tree and radial basis function network. They concluded that the hybrid SVM+FRBS (fuzzy rule based systems) outperformed the stand-alone classifiers. Then a new active learning-based approach (ALBA) to extract rules from SVM models is proposed [48]. Using the support vectors extracted they generate more number of training instances which are near to support vectors. In other words more number of instances are generated which are near the decision boundary of SVM. Late, using C4.5 and RIPPER (repeated incremental pruning to produce error reduction) rules are generated. Most Recently, Farquad et al. proposed an eclectic rule extraction approach to extract rules for solving regression problems [49]. Where, they employed CART, ANFIS and DENFIS for rule extraction purpose.Farquad et al. proposed an eclectic procedure for extracting rules from SVM which deals with unbalanced and medium scale problems [50]. They first extracted the support vectors and their corresponding target values are then replaced by the predictions given by SVM. This modified data set is then fed to NBTree to extract rules. Their approach was evaluated using bank credit cards data for predicting churn without employing any balancing technique. They concluded that using support vectors the comprehensibility of the rules is improved. In this paper, similar framework is proposed with the addition of feature selection module and standard data balancing techniques for churn prediction problem. The study presented in this paper can be considered as an extensive analysis of rule extraction from SVM and evaluation of the efficiency of feature selection using SVM-RFE applied to churn prediction problem.Churn prediction in bank credit card customers’ problem is solved using the proposed approach. The churn prediction dataset is highly unbalanced with 93:7 class distributions where 93% of the samples are available for loyal customers and only 7% of the data is available to learn about churn customers. The churn prediction dataset is obtained from Chile in 2004, information about the dataset attributes is presented in Table 1. Balancing techniques such as, SMOTE, random under-sampling, random over-sampling and combined under-sampling and over-sampling are employed. The efficiency of SVM for feature selection and rule extraction from SVM using unbalanced and balanced data is analyzed. Extracting support vectors and feature selection using SVM generates vertically and horizontally reduced data. This newly generated data is then used for rule generation.The proposed hybrid approach is composed of three phases and is depicted in Fig. 2.1Feature selection using SVM-RFE.Support vector extraction using SVM.Rule generation using NBTree.SVM-RFE (recursive feature elimination) [24] algorithm is employed for feature selection purpose. Nested subsets of features are selected in a sequential backward elimination manner, which starts with all the features variables and removes one feature variable at a time. At each step, the coefficients of the weight vector w of a linear SVM are used to compute the feature ranking score. The feature say, the ith feature with the smallest ranking score c1=(w)2 is eliminated, where w represents the corresponding component in the weight vector w. Using c=(w)2 as the ranking criterion corresponds to removing the feature whose removal changes the objective function the least. This objective function is chosen to be J=1/2||w||2 in SVM-RFE.Dealing with churn prediction data, sensitivity of the classifier is considered the most important factor for developing the SVM model and for extracting support vectors. Later, the corresponding actual target values of support vectors are replaced by the predicted target values of SVM, resulting in Case-SP dataset whereas support vectors with corresponding actual target values is called Case-SA dataset. For comparative study, the corresponding actual target values of training instances are also replaced by the predictions of SVM model, resulting in Case-P dataset. By using the newly generated Case-P and Case-SP we ensure that the rules extracted actually represent the knowledge learnt by the SVM.NBtree is employed for rule generation purpose [51]. It attempts to utilize the advantages of both decision trees (i.e. segmentation) and naïve bayes (evidence accumulation from multiple attributes). A decision tree is built with univariate splits at each node, but with Navie-Bayes classifiers at the leaves instead of the predictions for single class. It is concluded that NBTree's induction process is useful for larger datasets [51]. Rules are generated under 10-fold cross validation method of testing and the generated rules are later tested against the validation set.The churn prediction dataset is obtained from a Latin American Bank that suffered from an increasing number of churns with respect to their credit card customers and decided to improve its retention system. Two groups of variables are available for each customer: socio-demographic and behavioural data, which are described in Table 1. The dataset comprises of 22 variables, with 21 predictor variables and 1 class variable. It consists of 14,814 instances, of which 13,812 instances are pertaining to loyal customers and 1002 instances represent churned customers. Thus, there are 93.24% loyal customers and 6.76% churned customers. Hence, the dataset is highly unbalanced in terms of the proportion of churners vs. non-churners [52].In many real time problems, almost all the instances belong to one class, while far fewer instances are labelled as the other class, usually the more important class. It is obvious that traditional classifier seeking an accurate performance over a full range of instances are not suitable to deal with imbalanced learning task, since they tend to classify all the data into majority class, which is usually not the objective of the study and less important. Research studies show that many standard machine learning approaches result in poor performance, specifically dealing with large unbalanced datasets [53,54].Under-sampling is a technique in which some of the samples belonging to the majority class are removed randomly and combined with the minority class samples. For example, 25% under-sampling means that the majority class is reduced to 25% of its original size in other words, 25% of the available majority class instances are removed randomly from data. 50% under-sampling means that the majority class is reduced to 50% of its original size.Oversampling is a technique in which the samples belonging to the minority class are replicated a few times and combined with the majority class samples. For example, 100% over-sampling means that the minority class instances are replicated once in other words, minority class instances are doubled, and 200% over-sampling means that the minority class is replicated twice.SMOTE is an approach in which the minority class is oversampled by creating synthetic (or artificial) samples, rather than by oversampling with replacement. The minority class is oversampled by taking out each sample and introducing synthetic samples along the line segments that join any/all of the k minority class nearest neighbours. SMOTE is used to widen the data region that corresponds to minority samples. This approach effectively forces the decision region of the minority class to become more general [55].The available large scale unbalanced dataset is first divided into two parts of 80:20 ratios. 10-fold cross validation (10-FCV) is performed on the 80% of the data and 20% of the data is named as validation set and kept aside. Using validation set, efficiency of the rules generated during 10-FCV is evaluated. The class distribution proportion of the data is maintained in training data and validation data i.e. 93.24% for good customers and 6.76% for churn customers of the banks by resorting to stratified random sampling. Fig. 3depicts the segmentation of the data and the 10-FCV structure maintained for the experiments carried out in this paper.

@&#CONCLUSIONS@&#
