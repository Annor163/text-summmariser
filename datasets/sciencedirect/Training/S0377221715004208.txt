@&#MAIN-TITLE@&#
Benchmarking state-of-the-art classification algorithms for credit scoring: An update of research

@&#HIGHLIGHTS@&#
Large-scale benchmark of 41 classifiers across eight real-word credit scoring data sets.Introduction of ensemble selection routines to the credit scoring community.Analysis of six established and novel indicators to measure scorecard accuracy.Assessment of the financial impact of different scorecards.

@&#KEYPHRASES@&#
Data mining,Credit scoring,OR in banking,Forecasting benchmark,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Credit scoring is concerned with developing empirical models to support decision making in the retail credit business (Crook, Edelman, & Thomas, 2007). This sector is of considerable economic importance. For example, the volume of consumer loans held by banks in the US was $1132bn in 2013; compared to $1541bn in the corporate business.11Data from the Federal Reserve Board, H8, Assets and Liabilities of Commercial Banks in the United States (http://www.federalreserve.gov/releases/h8/current/).In UK, loans and mortgages to individuals were even higher than corporate loans in 2012 (£11,676 m c.f. £10,388 m).22Data from ONS Online, SDQ7: Assets, Liabilities and Transactions in Finance Leasing, Factoring and Credit Granting: 1st quarter 2012 (http://www.ons.gov.uk).These figures indicate that financial institutions require formal tools to inform lending decisions.A credit score is a model-based estimate of the probability that a borrower will show some undesirable behavior in the future. In application scoring, for example, lenders employ predictive models, called scorecards, to estimate how likely an applicant is to default. Such PD (probability of default) scorecards are routinely developed using classification algorithms (e.g., Hand & Henley, 1997). Many studies have examined the accuracy of alternative classifiers. One of the most comprehensive classifier comparisons to date is the benchmarking study of Baesens et al. (2003).Albeit much research, we argue that the credit scoring literature does not reflect several recent advancements in predictive learning. For example, the development of selective multiple classifier systems that pool different algorithms and optimize their weighting through heuristic search represents an important trend in machine learning (e.g., Partalas, Tsoumakas, & Vlahavas, 2010). Yet, no attempt has been made to systematically examine the potential of such approach for credit scoring. More generally, recent advancements concern three dimensions: (i) novel classification algorithms to develop scorecards (e.g., extreme learning machines, rotation forest, etc.), (ii) novel performance measures to assess scorecards (e.g., the H-measure or the partial Gini coefficient), and (iii) statistical hypothesis tests to compare scorecard performance (e.g., García, Fernández, Luengo, & Herrera, 2010). An analysis of the PD modeling literature confirms that these developments have received little attention in credit scoring, and reveals further limitations of previous studies; namely (i) using few and/or small data sets, (ii) not comparing different state-of-the-art classifiers to each other, and (iii) using only a small set of conceptually similar accuracy indicators. We elaborate on these issues in Section 2.The above research gaps warrant an update of Baesens et al. (2003). Therefore, the motivation of this paper is to provide a holistic view of the state-of-the-art in predictive modeling and how it can support decision making in the retail credit business. In pursuing this objective, we make the following contributions: first, we perform a large scale benchmark of 41 classification methods across eight credit scoring data sets. Several of the classifiers are new to the community and for the first time assessed in credit scoring. Second, using the principles of cost-sensitive learning, we shed light on the link between the (statistical) accuracy of scorecard predictions and the business value of a scorecard. This offers some guidance whether deploying advanced—more accurate—classification models is economically sensible. Third, we examine the correspondence between empirical results obtained using different accuracy indicators. In particular, we clarify the reliability of scorecard comparisons in the light of recently identified limitations of the area under a receiver operating characteristics curve (Hand, 2009; Hand & Anagnostopoulos, 2013). Finally, we illustrate the use of advanced nonparametric testing procedures to secure empirical findings and, thereby, offer guidance how to organize future classifier comparisons.In the remainder of the paper we first review related work in Section 2. We then summarize the classifiers that we compare (Section 3) and describe our experimental design (Section 4). Next, we discuss empirical results (Section 5). Section 6 concludes the paper. The online appendix33Supplementary material associated with this article can be found in the online version, at doi:10.1016/j.ejor.2015.05.030.provides a detailed description of the classification algorithms and additional results.

@&#CONCLUSIONS@&#
We set out to update Baesens et al. (2003) and to explore the relative effectiveness of alternative classification algorithms in retail credit scoring. To that end, we compared 41 classifiers in terms of six performance measures across eight real-world credit scoring data sets. Our results suggest that several classifiers predict credit risk significantly more accurately than the industry standard LR. Especially heterogeneous ensembles classifiers perform well. We also provide some evidence that more accurate scorecards facilitate sizeable financial returns. Finally, we show that several common performance measures give similar signals as to which scorecard is most effective, and recommend the use of two rarely employed measures that contribute additional information.Our study consolidates previous work in PD modeling and provides a holistic picture of the state-of-the-art in predictive modeling for retail scorecard development. This has implications for academia and industry. From an academic point of view, an important question is whether efforts into the development of novel scoring techniques are worthwhile. Our study provides some support but also raises concerns. We find some advanced methods to perform extremely well on our credit scoring data sets, but never observe the most recent classifiers to excel. ANNs perform better than ELMs, RF better than RotFor, and dynamic selective ensembles worse than almost all other classifiers. This may indicate that progress in the field has stalled (e.g., Hand, 2006), and that the focus of attention should move from PD models to other modeling problems in the credit industry including data quality, scorecard recalibration, variable selection, and LGD/EAD modeling.On the other hand, we do not expect the desire to develop better, more accurate scorecards to end any time soon. Likely, future papers will propose novel classifiers and the “search for the silver bullet” (Thomas, 2010) will continue. An implication of our study is that such efforts must be accompanied by a rigorous assessment of the proposed method vis-à-vis challenging benchmarks. In particular, we recommend RF as benchmark against which to compare new classification algorithms. HCES-Bag might be even more difficult to outperform, but is not as easily available in standard software. Furthermore, we caution against the practice to compare a newly proposed classifier to LR (or some other individual classifier) only, which we still observe in the literature. LR is the industry standard and it is useful to examine how a new classifier compares to this approach. However, given the state-of-the-art, outperforming LR can no longer be accepted as a signal of methodological advancement.An important question to be answered in future research is whether the characteristics of a classification algorithm and a data set facilitate appraising the classifier's suitability for this data set a priori. We have identified classifiers that work well for PD modeling, but cannot explain their success. Nonetheless, our benchmark can be seen as a first step toward gaining explanatory insight in that it provides an empirical fundament for meta-analytic research. For example, gathering features of individual classifiers and characteristics of the credit scoring data sets, and using these as covariates in a regression framework to explain classifier performance (as dependent variable) could help to uncover the underlying drivers of classifier efficacy in credit scoring.From a managerial perspective, it is important to reason whether the superior performance that we observe for some classifiers generalizes to real-world applications, and to what extent their adoption would increase returns. These questions are much debated in the literature (e.g., Finlay, 2011). From this study, we can add some points to the discussion.First, we show that advancements in computer power, classifier learning, and statistical testing facilitate rigorous classifier comparisons. This does not guarantee external validity. Several concerns why laboratory experiments (as this one) may overestimate the advantage of advanced classifiers remain valid; and might be insurmountable (e.g., Hand, 2006). However, experimental designs with several cross-validation repetitions, different performance measures, and appropriate multiple-comparison procedures overcome some limitations of previous studies and, thereby, provide stronger support that advanced classifiers have the potential to increase predictive accuracy not only in the laboratory but also in industry.Second, our results facilitate some remarks related to the organizational acceptance of advanced classifiers. In particular, a lack of acceptance can result from concerns that much expertise is needed to handle such classifiers. Our results show that this is not the case. The accuracy differences that we observe result from a fully-automatic modeling approach. Consequently, certain advanced classifiers do not require human intervention to predict significantly more accurately than simpler alternatives. Furthermore, the current interest in Big Data indicates a shift toward a data-driven decision making paradigm among managers. This might further increase the acceptability of advanced scoring methods.Finally, the business value of more accurate scorecard predictions is a crucial issue. Our preliminary simulation provides some evidence that the “higher (statistical) accuracy equals more profit equation” might hold. Furthermore, retail scorecards support a vast number of business decisions. Consider for example the credit card industry or scoring tasks in online settings. In such environments, one-time investments (e.g., for hardware, software, and user training) into a more elaborate scoring technique will pay-off in the long run when small but significant accuracy improvements are multiplied by hundreds of thousands of scorecard applications. The difficulties of introducing advanced scoring methods including ensemble models are more psychological than business related. Using a large number of models, a significant minority of which give contradictory answers, is counterintuitive to many business leaders. Such organizations will need to experiment fully before accepting a change from the historic industry standard procedures.Regulatory frameworks and organizational acceptance constrain and sometimes prohibit the use of advanced scoring techniques today; at least for classic credit products. However, given the current interest in data-centric decision aids and the richness of online-mediated forms of credit granting, we foresee a bright future for advanced scoring methods in credit scoring.