@&#MAIN-TITLE@&#
Support vector regression based on optimal training subset and adaptive particle swarm optimization algorithm

@&#HIGHLIGHTS@&#
Graphical abstractA parameters selection algorithm of SVR is proposed based on OTS.The APSO algorithm has been combined to determine the size of OTS.The proposed model has the best accuracy and generalization ability by experiments.The optimal parameters setting of SVR and the optimal size of OTS are studied.The obtained results confirm the applicability and superiority of the proposed model.

@&#KEYPHRASES@&#
Support vector regression,Adaptive particle swarm optimization,Optimal training subset,Parameters selection,

@&#ABSTRACT@&#
Support vector regression (SVR) has become very promising and popular in the field of machine learning due to its attractive features and profound empirical performance for small sample, nonlinearity and high dimensional data application. However, most existing support vector regression learning algorithms are limited to the parameters selection and slow learning for large sample. This paper considers an adaptive particle swarm optimization (APSO) algorithm for the parameters selection of support vector regression model. In order to accelerate its training process while keeping high accurate forecasting in each parameters selection step of APSO iteration, an optimal training subset (OTS) method is carried out to choose the representation data points of the full training data set. Furthermore, the optimal parameters setting of SVR and the optimal size of OTS are studied preliminary. Experimental results of an UCI data set and electric load forecasting in New South Wales show that the proposed model is effective and produces better generalization performance.

@&#INTRODUCTION@&#
Recently, support vector regression (SVR), which was developed by Vapnik [20], has become very promising and popular in the field of machine learning due to its attractive features and profound empirical performance for small sample, nonlinearity and high dimensional data application [1–3]. The main technique of SVR is to use the principle of structural risk minimization (SRM) by constructing an optimal regression hyper-plane in the hidden feature space and solving the unique solution of the accordingly dual quadratic programming problem.In the SVR, the model for forecasting is generated from the learning process with the training data set. Then, SVR has been successfully applied to solve forecasting problems in many areas, such as financial time series forecasting [4], short term wind speed prediction [5], face recognition [6], electric load prediction [7], and so on. However, these empirical results indicated that the largest problems encountered in building up the SVR are how to select the three parameters (C, ε, and δ2) and improve the slow learning for large sample. To solve the above problems, many researchers have given some parameter setting algorithms [8–11]. Particle swarm optimization has become a popular parameters selection algorithm [12], Lin et al. utilize particle swarm optimization (PSO) for parameter determination and feature selection of support vector machines (SVM) [13], Huang and Dun present an optimization mechanism that hybridized PSO and SVM to improve the classification accuracy with an appropriate feature subset and SVM's parameters [14]. Considering that the computation complexity is O(K×N2) (K is the number of iteration), parameter setting algorithms will lead to slow learning in large-scale training data set. This paper aims to present a model for solving the above problem.In the large sample learning problem, the training data set contain much redundant information generally. The redundant data not only are useless for SVR learning, but also could lead to low computational efficiency and low accuracy potentially. Thus, discarding the redundant information of training data set can accelerate learning process of SVR's parameters selection. Inspired by that not all of these training data are equally important for a specific forecasting problem, only the support vectors determine the final SVR model. Better computation performance and generalization ability can be achieved by choosing the optimal training subset (OTS) containing support vectors. Therefore, the learning process can be fast and accurate by using the APSO algorithm and OTS selection method.By combining the optimal training subset reconstruction method with APSO, here the author presents a new parameter selection algorithm for SVR, called APSO-OTS-SVR. Some improved techniques in the optimization framework are presented in order to simplify the APSO iteration learning algorithm and increase the learning speed. Based on APSO-OTS-SVR, forecasting models for an UCI data set and electric load forecasting in New South Wales are proposed. Compared with three SVR models, the experimental results show that APSO-OTS-SVR provides a parameters selection and better generalization performance at higher learning speed.The rest of the study is organized as follows. Section 2 proposes APSO-OTS-SVR, and the main steps of it are also given in this section. The experiment design of the forecasting model is given in Section 3. Section 4 presents the experimental results. The final conclusion is drawn in Section 5.This subsection briefly introduces the brief ideas of SVMs for the case of regression. The SVR firstly considers the linear regression problem, then it can be extended to the nonlinear regression utilizing kernel functions technique. And we refer the reader to the excellent surveys for a more thorough coverage of it [15–19].Suppose the training data are(x1,y1),…,(xn,yn)⊂W×ℝ, whereWdenotes the space of the input patterns xi(e.g.W=ℝn), and yiis the associated output values of xi. For linear regression problem, ε-SVR [20] produce a regression function f(x)=ωTx+b by solving the following formulation.(1)minω,b,ξ,ξ*12ωTω+C∑i=1n(ξi+ξi*)(2)s.t.yi−(ωTxi+b)≤ε+ξi(ωTxi+b)−yi≤ε+ξi*ξi,ξi*≥0The parameter C>0 decides the trade-off of the above two terms in Eq. (1), ξidenotes the training error above ε, whereasξi*denotes the training error below −ε, and n represents the number of samples.The first term of Eq. (1) controls the generalization ability of the regression function. The second term penalizes the learning error of f(xi) and yiby using ε-insensitive tube |yi−(〈ω, xi〉+b)|≤ε. This ε-insensitive loss function |ξε| can be described as the following.(3)|ξ|ε:=0,if|ξ|<ε|ξ|−ε,otherwiseThe primal form of Lagrange multiplier method is as following:(4)Lp=12ωTω+C∑i=1n(ξi+ξi*)−∑i=1nαi(ε+ξi−(yi−(ωTxi+b)))−∑i=1nriξi−∑i=1nαi*(ε+ξi*−((ωTxi+b)−yi))−∑i=1nri*ξi*whereαi,αi*≥0andri,ri*≥0are the well-known Lagrange multipliers, they are obtained by maximizing the dual function of Eq. (4). And according to the partial derivatives of ξiandξi*in Eq. (4), one gets:(5)ω+∑i=1nxi(αi−αi*)=0∑i=1n(αi−αi*)=0C−αi−ri=0C−αi*−ri*=0This will lead to the following dual problem by replacing ω of Eq. (4).(6)maxαi,αi*∑i=1nyi(αi−αi*)−ε∑i=1n(αi+αi*)−12∑i=1n∑j=1n(αi−αi*)(αj−αj*)xiTxjsubject to(7)∑i=1n(αi−αi*)=00≤αi≤C0≤αi*≤Ci=1,2,…,nAccording to the Karush–Kuhn–Tucker's (KKT) conditions of solving quadratic programming problem, only some of(αi−αi*)in Eq. (6) are non-zero. These data points on non-zero coefficient are referred to as the support vector. Usually, the ε value determines the number of support vectors, which represents the sparser level of the solution.By using ω of Eq. (5), the regression function can be expressed as follows.(8)f(x)=∑i=1n(αi*−αi)xiTx+bFrom Eq. (8), the support vectors, whose approximation errors will equal to or larger than ε, determine the final regression function. Intuitively, since errors lower than ε are accepted, training data points lying inside the so called “ε-tube” have no impact on the problem solution. In a sense, these data points are redundant, the discarding of them can reduce the problem complexity and accelerate the learning process.For nonlinear regression problems, “kernel trick” is the key to extending the above linear regression of support vector machine [21]. This is done by mapping the input patterns into a higher-dimensional spaceFby a functionφ:W→F. Therefore, the linear regression on the higher-dimensional space will be equivalent to the nonlinear regression on the input patterns. Kernel function that satisfies Mercer's condition can be used in this part [22], and the dot product 〈xi, xj〉 in Eq. (8), therefore, becomes a kernel function 〈φ(xi), φ(xj)〉=K(xi, xj) for nonlinear cases. In this study, the author discusses the use of Gaussian kernel function(9)K(x,x′)=exp−(x−x′)22×δ2The Gaussian kernel parameter δ2 is determined by the user.It should be notice that the forecasting ability of a SVR model depends on a good setting of the three parameters (C, ε, and δ2). Thus, the selection of all three parameters is an important issue. For the above reason, the APSO algorithm is used in the proposed SVR model to optimize the parameter selection.It is difficult to decide the best parameters of the SVR. APSO algorithm is employed here to find the optimum parameters (C, ε, and δ2).Inspired by swarm behaviors such as bird socking and fishes schooling, Kennedy and Eberhart firstly presented a new swarm intelligence (SI) algorithm called particle swarm optimization (PSO) in 1995 [23].In PSO, m particles fly through an n-dimensional search space. Each particle i has two vectors: the velocity vectorVi=(vi1,vi2,…,vin), the position vector Xi=(xi1, xi2, …, xin). Similar to bird socking and fishes schooling, the particles are updated according to itself previous best position Pi=(pi1, pi2, …, pin) and the entire swarm previous best position Pg=(pg1, pg2, …, pgn). That is, particle i adjusts its velocity Viand position Xiin each generation according to the following formula:(10)vid(t+1)=vid(t)+c1*rand()1*(pid−xid)+c2*rand()2*(pgd−xid)(11)xid(t+1)=xid(t)+vid(t+1)where d=1, 2, …, n; c1, c2 are acceleration coefficients with positive values, which are set to be c1=c2=1/2 in this study; rand()1, rand()2 are random numbers uniform distribution U(0, 1). Eqs. (10) and (11) are employed to calculate a new velocity and position for each particle based on its velocityvid(t), its best location pidso far and the swarm's best location pgdso far.The adaptive particle swarm optimization (APSO) algorithm is an improved PSO algorithm proposed by Shi and Eberhart in 1998 [24]. A self-adapting inertia weight parameter ω was introduced to update the velocity expression (10) as follows:(12)vid(t+1)=ω(t+1)*vid(t)+c1*rand()1*(pid−xid)+c2(t+1)*rand()2*(pgd−xid)To get faster convergence and avoid premature searching, the self-adapting inertia weight ω is expressed as [14,25](13)ω(t+1)=ω(max)−ω(max)−ω(end)MaxIteration×t(14)c2(t+1)=c2(t)×tMaxIterationwhere ω(max), ω(end) are the maximum and final values of of the inertia weight, MaxIteration denotes the maximum number of allowable iterations, t is the present iteration number.The mean absolute error function MAE is used to represent the fitness value of the APSO associated with one particle. Thus, the fitness function f can be expressed as follows:(15)f(C,ε,δ2)=MAE(C,ε,δ2)=∑i=1n|(Pi−Ai)|nwhere Piand Aiare the ith forecasted and actual values respectively, and n is the total number of predictions.To eliminate the influence of outliers and get better generalization ability, the author introduces a tolerance coefficient r for outliers as follows:(16)f(C,ε,δ2)=MAEmin(C,ε,δ2,round(n−r×n))where round() is the rounding function, and MAEmin() represents the MAE of the round(n−r×n) minimum numbers of |(Pi−Ai)|, i=1, 2, …, n.To get a fast and accurate SVR, this study uses an optimal training subset for SVR training process. Suppose that the optimal training subset contains k elements, this algorithm has two steps: firstly, how to select k representation data points with maximum information; secondly, how to choose the parameter k. These are solved in the following two subsection, respectively.To extract representation data points with maximum information from the full training set, a type of instance-based algorithm is presented in this subsection. Inspired by that the farther points contain more information than the nearer ones, k representation data selection method is proposed to choose some training pairs with maximum differences from the entire training pairs.The main steps of k representation data selection method can be summarized as the following Algorithm 1.Input:T=(Xi,yi)i=1n: training data set; n: the number of the independent samples; k: number of representation data subset; (X0, y0): initial element for representation data subset.Output: RDS: k-representation data subset.Termination conditions: The number of representation data subset attains k.k representation data selection methodInput training data setT=(Xi,yi)i=1nand select initial element (X0, y0) for representation data set, and set RDS={(X0, y0)} and i=0;REPEAT(a)Select (X*, y*)∈T subject to:(17)∑Xj∈RDS|Xj−X*|=maxXi∈T∑Xj∈RDS|Xj−Xi|Update the training data set and representation data subset:(18)T=T−{(X*,y*)};RDS=RDS∪{(X*,y*)}Set i=i+1;UNTIL termination conditions is satisfied, that is i==k.The key problem for finding the most suitable value of k will be solved in detail in the following subsection.Based on the k representation data selection method, the determination of parameter k becomes the key to finding the optimal training subset. The optimal training subset has two goals: having as small as possible amount points, while incorporating maximum information of the full training set. Thus, this problem can be summarized as the following optimization problem:(19)mink∈N+F(k)=λ*kn+(1−λ)*MAPE(k,T)where k, n are the size of the optimal training subset and the full training set T, respectively, and MAPE(k, T) represents the mean absolute percentage error (MAPE) of SVR. In Eq.(17), the first item denotes the model complexity, and the second item denotes the prediction accuracy of the proposed model based on the k optimal training subset for T. The λ tuning parameter controls the trade-off between the above two items. The large λ is, the more attention we pay to the model complexity.Remark 1The computation complexity of SVR model is O(N2) (N is the number of training data), so the size of the optimal training subset denotes the model complexity.Remark 2Generally, λ is set to a relatively small number. Specially, the proposed model will get the minimum error without considering the model complexity if we set λ=0, which is more suitable for small sized training set. In practice, one may use an experience rule to roughly tune the value of λ.The approximation convexity property is demonstrated in [26]. With this convexity, obtaining the minimum size optimal subset from the full training set becomes a lot easier and faster. In this study, a hill-climbing method is used for finding the exact solution of the above problem. A stable iteration scheme and a model coverage criterion are proposed to solve the optimal training subset for three initial values of k and, consequently, to compute the corresponding goal function value via training the SVR model. The iteration algorithm for computing the minimum optimal training subset can be summarized as the following Algorithm 2.Input:T=(Xi,yi)i=1n: training data set; n: the number of the independent samples; k1, k2, k3: three initial size values for optimal training subset; (X0, y0): initial element for optimal training subset.Output:k: convergence size value for optimal training subset.Termination conditions:max(k1, k2, k3)−min(k1, k2, k3)>3 or iterations larger than 2.Approximation convexity optimization for the determination of parameter kInput training data setT=(Xi,yi)i=1nand select initial element (X0, y0) for representation data set, set step=1/3 and generate initial SVR models as follows:(a)Calculate the representation data subset for size parameter k1, k2, k3 using Algorithm 1; denote as k1−RDS, k2−RDS, k3−RDS, respectively.Train three SVR models using training subset k1−RDS, k2−RDS, k3−RDS, respectively.CalculateF(k1),F(k2),F(k3)according to three trained SVR models.REPEAT(a)IfF(k3)<=F(k2)andF(k2)<=F(k1), thenk=k3, step1=k2−k1, step2=k3−k2, k1=k1+round(step.×step1), k2=k3, k3=k3+round(step.×step2);ifF(k2)<=F(k3)andF(k2)<=F(k1), thenk=k2,step1=k2−k1, step2=k3−k2, k1=k1+round(step.×step1), k3=k3−round(step.×step2);otherwisek=k1, step1=k2−k1, step2=k3−k2, k3=k3−round(step.×step2), k2=k1, k1=k1−round(step.×step1);Calculate the representation data subset for size parameter k1, k2, k3 using Algorithm 1; denote as k1−RDS, k2−RDS, k3−RDS, respectively.Train three SVR models using training subset k1−RDS, k2−RDS, k3−RDS, respectively.CalculateF(k1),F(k2),F(k3)according to three trained SVR models.UNTIL the termination conditions are satisfied.Remark 3To accelerate the training process of Algorithm 2, the author modifies the original termination conditions in [26]: max(k1, k2, k3)−min(k1, k2, k3)=0 or iterations larger than n/2. The new termination conditions guarantee the combination with the optimal size of APSO's best particle.Let the three parameters (C, ε, and δ2) of SVR forecasting model be a particle, which is a D-dimensional vector (D=3), and suppose that there are N particles in the searching space. Hu et al. [27] gave approximation ranges for SVR parameters: C=[4−6, 410], ε=[0, 0.2], δ2=[4−6, 46], so these parameters’ values are optimized in these ranges by APSO. For the optimization problem, training SVR model is an accompaniment to each particle, this study reduces the complexity of training SVR model by extracting optimal training subset. First, use an approximation convexity optimization as discussed in Section 2.3 and the following Eq. (21) to determine the parameter k. Then, train the SVR using the obtained optimal training subset for the forecast process as discussed above. The framework of the proposed forecasting model is as follows:Step 1:Initialization: Randomly generate N particles and velocities: xisdistribute in U[0, 1],visdistribute in U[−1, 1], i=1, 2, …, N and s=1, 2, 3; and set the minimum error e, the maximum iteration number itmax, the maximum velocityvmaxs, the maximum positionxmaxsand the minimum positionxmins, s=1, 2, 3.Transfer the particles and velocities: [xi1, xi2, xi3]=[xi1×48/4(i−1), 0.08+xi2×0.1, xi3×46/4i];[vi1,vi2,vi3]=[vi1×xi1×0.5,vi2×xi2×0.5,vi3×xi3×0.5].Calculate the parameter k and update the size parameter k1, k2, k3 for each particle using Algorithm 2, and train the SVR model using the corresponding optimal training subset.Calculate the fitness of each particle by Eq. (14).Update the size parameter k1, k2, k3 for each particle using the following Eq. (21).Check whether the termination conditions are satisfied: If the swarm best particle achieves the forecasting precision e, or the iteration number reaches the maximum iteration number itmax, continue to Step 7; otherwise, go to Step 8.Generate new particles for the next generation according to the following equation:(20)vid(t+1)=ω(t+1)×vid(t)+c1×rand()1×(pid−xid)+c2×rand()2×(pgd−xid)xid(t+1)=xid(t)+vid(t+1)ω(t+1)=ω(max)−ω(max)−ω(end)MaxIteration×tsubject to:(21)vid=vmaxid,if:vid>vmaxidvid=−vmaxid,if:vid<−vmaxid(22)xid=xmaxid,if:xid>xmaxidxid=xminid,if:xid<xminidgo back to Step 3.Terminate the searching process and output the optimized three parameters, the optimal training subset and the obtained SVR model.Remark 4In this study, the author set the minimum error e=0.015, the maximum iteration number itmax=20, the maximum velocity vmax=0.5×[410;0.2;46], the maximum position xmax=1×[410;0.2;46] and the minimum position xmin=1×[4−6;0.01;4−6].Remark 5To accelerate the parameters selection process of APSO, k1, k2, k3 in Algorithm 2 can then be updated according to the optimal size k of APSO's best particle:(23)k1=k1+step′×(k−k1)k2=k2+step′×(k−k2)k3=k3+step′×(k−k3)where the parameter step′ is set to 0.8 in this study. Thus, the size parameters k1, k2, k3 are updated by the Algorithm 2 and Eq. (21), and the APSO algorithm has been combined to determine the size of OTS effectively.Remark 6The small size OTS represents maximum information of the full training set, so the data points of OTS should be the support vectors of the final model to obtain SVR model containing most of information of the full training data set. And the research [28] study the relation among these parameters: a relatively large C and δ2 corresponds to a relatively small ε, and the setting of small ε can guarantee that all the data points of OTS become the support vectors of the final SVR model. In addition, the sparseness of OTS indicates a relatively large value of δ2. Thus, I add the setting of initial particles as shown in Steps 1 and 2.This section mainly introduces the research methodology for the experiment comparison design of the present study. First, different SVR models are used to compare the performance of APSO-OTS-SVR model by an UCI data set. Second, the real case in New South Wales is collected to test the forecasting model. Third, different statistical metrics are employed to evaluate the prediction capability.In order to compare the APSO-OTS-SVR model, simple description of three comparison models is as follows:Standard SVR model: This is a traditional and classical SVR model called S-SVR. One can simply set the parameters’ value according to some experimental rules, and the standard SVR model does the learning process for all of the training data. In this study, the parameters’ value is set as follows: C=8, ε=0.1, r=2SVR model based on training data subset: In order to reduce the complexity of the standard SVR model, a training data subset is extracted from the full training data set T uniformly. One can simply set the parameters’ value according to some experimental rules, and this SVR model does the learning process for the obtained training data subset. In this study, 20% sample is extracted from T, and this model is called 20%-SVR.SVR model based on training data subset and APSO: In order to set the parameters’ value, the APSO method is used to get the best SVR model based on a randomly chosen training data subset. In this study, 10% sample is extracted from T, and this model is called 10%-APSO-SVR.To examine the forecasting accuracy, three different statistical metrics are employed in this work: the root mean square error (RMSE), the mean absolute error (MAE) and the mean absolute percentage error (MAPE).(24)RMSE=∑i=1n(Pi−Ai)2n,(25)MAE=∑i=1n|(Pi−Ai)|n,(26)MAPE=∑i=1n|(Pi−Ai)Ai|n,where Piand Aiare the ith forecasted and actual values respectively, and n is the total number of predictions.Generalization ability refers to the adaptability of machine learning algorithms for new samples (validation data); and the situation, good performance for training data but bad for validation data, is viewed as over-fitting. To estimate the generalization ability, the electric load forecasting for the validation set P is carried out by all the models, respectively. For the purpose of evaluating the generalization ability of forecasting model, difference in MAPE (DMAPE) between training data set T and valuation data set P is defined in the following:(27)DMAPE=MAPE(T)−MAPE(P),where MAPE(T) is the MAPE of forecasting model for T, and MAPE(P) is the MAPE of forecasting model for P.In this section, the author performs regression experiments on an UCI data set and a real-world electric load data set. To estimate the generalization ability of forecasting models, data set is split into two parts: T denotes the training data set, and P denotes the validation data set. Suppose that T and P satisfy T⋂P=∅. The basic information of the UCI data set is reported in Table 1. And the half-hourly basis electric load data in New South Wales are collected (48 data points per day) as shown in Fig. 1. Different SVR models are trained using electric load in New South Wales from May 10, 2007 to May 17, 2007, and tested using electric load from May 18, 2007 to May 24, 2007 (totally 720 data points). Thus, a week ahead forecasting is performed using different SVR models in this work. The following experiments are to prove the superiority and effectiveness of our new algorithm compared with the existing SVR algorithms in terms of prediction accuracy, running time and generalization ability.The following section presents applications of the APSO-OTS-SVR model and the proposed SVR comparison models in the UCI data set and one regression problem taken from electric load prediction. These algorithms are coded in Matlab 7.0 [29] and case studies are run on a PC with 1 GB of RAM and a 2.01-GHz-based processor.In this subsection, an UCI data set is employed to validate the effectiveness of the proposed model, the data preprocessing of the data set is described as follows:Boston Housing Data Set: The data set is created by Harrison and Rubinfeld [30]. This is a difficult regression task, and it is employed to test the proposed APSO-OTS-SVR model. Consider that the housing values in suburbs of Boston are affected by many other complex factors, this study introduces a tolerance coefficient r=0.9 in the fitness function Eq. (16) to eliminate the influence of outliers.In this study, the attributes were first transformed with a normalization function. Then, four SVR models were applied. After fitting the models, the attributes were post-processed with the inverse of the normalization transform. In Table 2, the final parameters settings of the UCI data set are given by running the APSO-OTS-SVR model. As shown in the table, there are m data points in the OTS, and these data points are support vectors of the final model, totally. This is determined by the property of the OTS. Thus, one can get the conclusion from this result: when the m is relatively small, these m data points can represent the training dataset, reduce the computational complexity, and improve the generalization capability.For these four mathematical models, the performance measures above are used to investigate their forecasting ability, training time is employed to compare complexity, and DMAPE is reported to evaluate their generalization ability. The detailed performance comparison is listed in Table 4 and plotted in Fig. 6.For this real case, as electricity is an energy source that can not be stored, electric utilities must develop a right decision-making system in time depending on the information at hand, which makes the daily operations of a power utility easier in a constantly fluctuating environment. Theoretically, if the manager can estimate the future electric load demand accurately, he can then effectively control price and income elasticities, energy transfer scheduling, unit commitment and load dispatch. Therefore, how to develop an accurate, fast, simple and robust load forecast model is critical to electric utilities and its customers [7].In Table 3, the parameters selection process of the APSO-OTS-SVR model is shown. For each iteration of the APSO algorithm, the global optimum particle (C, ε, δ2) and its’ corresponding fitness value are obtained. (Notice that this learning process is performed under the normalization data set in the range [0, 3].) The parameters selection process seems to flatten within 15 iterations, which indicates that this method is convergent rather quickly as the iteration number gets larger. In the final APSO-OTS-SVR model, there are 20 data points in the OTS, and these data points are support vectors of the final model, totally. This is determined by the property of the OTS.The experimental results from the above two data sets are summarized in Table 4. The first column denotes the model, the second column denotes the learning process evaluation method including accuracy and CPU training time for the training set T, and the third column denotes the generalization ability evaluation method including accuracy and DMAPE for the validation set P. It can be observed from Table 4 that the proposed forecasting model gives more accurate results in both learning and validation step. The 10%-APSO-SVR model only use 10obtains the best accuracy for both T and P except for the APSO-OTS-SVR model. This result indicates that the parameters selection is the key to the performance of SVR model.In the testing step for generalization evaluation, the MAPE of the APSO-OTS-SVR model is 0.368675(0.115358), while the other models’ are 0.521149(0.140488), 0.638978(0.199089) and 0.787582(0.210003), respectively. Similar results of MAEs and RMSEs demonstrate again that the proposed forecasting model gives better generalization ability in electric load forecasting. Also, the small DMAPE value of the proposed forecasting model indicates that the proposed model solves the problem of over-fitting learning for the training data set T. Although the proposed model need the most CPU training time, its’ CPU training time will remain at this level as the training data set get larger. This is because the size of the OTS and support vectors remains at the optimal level, generally. Further, the accuracy and the average CPU training time are acceptable, which shows the applicability of our proposed SVR model.For the electric load forecasting in New South Wales, the results of training under the 20%-SVR model, the 10%-APSO-SVR model, the S-SVR model and the APSO-OTS-SVR model are shown in Figs. 2 and 3. Consider that the original data are relatively large compared with the forecasting error, the errors are also shown in the corresponding figure to show a difference in prediction performance. In this training stage, one can observe the following conclusions according to the above results: First, the SVR models based on APSO have better performance than the other SVR models. This result demonstrates that the performance of SVR model is affected by parameters selection, and APSO algorithm can produce suitable parameters for SVR model. Second, the APSO-OTS-SVR model fit the original data curse better compared with 10%-APSO-SVR model. This result proves that the OTS has good representative for the training set T.To estimate the generalization ability, the electric load forecasting for the validation set P is carried out by the above SVR models, respectively. The forecasting results under the 20%-SVR model, the 10%-APSO-SVR model, the S-SVR model and the APSO-OTS-SVR model are shown in Figs. 4 and 5. The errors are also shown in the corresponding figure. Similar results are obtained as shown in Figs. 4 and 5: the SVR models based on APSO have better forecasting accuracy than the other SVR models. The forecasting curve of the APSO-OTS-SVR model is more adaptive to the full time series than the standard SVR model, which indicates the value of the proposed model construction.Similar results for the UCI data set have been obtained as shown in Table 4. For the sake of simplicity, the author only plots the training and testing stage of the APSO-OTS-SVR model as shown in Fig. 6. The errors are also shown in the corresponding figure.In Table 5, one sided t test by using SPSS is applied for the above two experiments. The hypothesis test is defined as follows:(28)H0:μi≤te×eH1:μi>te×e,i=1,2,3,4where μiis the average absolute error between the forecasting series and the actual series, whose values are shown in Figs. 2–6 (the forecasting model includes the APSO-OTS-SVR model, the S-SVR model, the 20%-SVR model and the 10%-APSO-SVR model, respectively); teis the tolerance coefficient of forecasting model, e is the length of ε-insensitive tube, which can be defined as follows:(29)e=postprocessed(ε)where postprocessed(ε) is the inverse of the normalization transform.Up to confidence 1−2α=0.9, when the one-sided confidence interval (LowerBound, +∞) includes the point te×e, one can get the conclusion that the null hypothesis H0:μi≤te×e is true. It can be seen from Table 5 that: 4.8717∈(4.1851, +∞), (4.5417, +∞), (2.9089, +∞) and (3.2247, +∞), 2.6848∈(2.3221, +∞), 127.4∈(102.4558, +∞) and (111.2945, +∞), and 145.5442∈(106.2487, +∞) and (117.5292, +∞), which means that: up to confidence 1−2α=0.9, all the null hypotheses for the APSO-OTS-SVR model (except for the testing phase of housing dataset) and the null hypotheses for the 10%-APSO-SVR model (only the training phase of housing dataset) are true. However, for other forecasting models, this result shows that there exists a significant difference between the original data and the forecasting data at a 90% confidence level. Thus, the APSO-OTS-SVR model outperforms the three comparison SVR models significantly in the above cases.Several observations can be made from the above results. First, the empirical results obtained demonstrate that the proposed SVR model performs better accuracy than the three comparison SVR models for both training and validation step. Second, the parameters selection is analyzed in this work, and empirically, one can get the conclusion that the optimal combination between SVR parameters (ε, C, and δ2) and OTS size exists for a specific example. Third, the empirical results indicate that the proposed forecasting model gives better generalization ability in both median value of owner-occupied homes and electric load forecasting, and solves the problem of over-fitting learning for the training data set T in terms of difference in MAPE (DMAPE). Finally, the proposed forecasting model is robust and effective for large-scale training data set. The proposed model gives an analysis framework for parameters selection of the SVR model under OTS. These results may be expected since the APSO selection algorithm can reduce the training complexity and avoid over-fitting by computing OTS. Overall, the APSO-OTS-SVR model provides a very powerful tool of easy implementation for forecasting electric load.

@&#CONCLUSIONS@&#
The critical problem encountered in building up the SVR is how to select the three parameters (C, ε, and σ). In an iteration optimization algorithm, the SVR model is learned by solving a quadratic programming problem after the parameter selection of SVR, and its complexity is O(N2) (N is the number of training set). SVR learning process suffers from slow training speed in the case of large-scale training data, and over-fitting is caused probably [31].Based on the above reason, this paper has proposed a parameters selection framework for the SVR model under OTS. The framework inherently has a concentration capable of solving the complexity problem O(K×N2) suffered by SVR model and avoiding the over-fitting of large-scale training set (K is the number of iteration). The contribution in this paper has been threefold, namely the specification of the parameters selection framework for the SVR model under OTS, the simplification iteration process by combining with APSO and OTS, and the relation study between the parameters setting of SVR and the size of OTS. The experimental results demonstrate the applicability and superiority of the proposed APSO-OTS-SVR model. However, it is fair to say that much remains to be done in the way of model construction.