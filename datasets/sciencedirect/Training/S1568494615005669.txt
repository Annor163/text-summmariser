@&#MAIN-TITLE@&#
Error correction method based on data transformational GM(1,1) and application on tax forecasting

@&#HIGHLIGHTS@&#
An error correction method is proposed to improve forecasting accuracy.This algorithm is integrated with LS-SVR and data transformation GM(1,1).This method emphasizes on the importance of making full use of error informationProposed algorithm is well suited for the characteristics of prediction error.The approach is superior to traditional forecasting methods.

@&#KEYPHRASES@&#
Initial forecast,Data transformation,GM(1,1),Error correction,Tax forecasting,

@&#ABSTRACT@&#
Accurate tax forecast is very important to carry on the macroscopic regulation efficiently under the market economy. However, experience shows that it is very difficult to improve the accuracy of forecasting by setting up a single forecasting model. This article describes the deficiencies of the present forecasting methods and puts forward a new approach to improve the accuracy of prediction by introducing error correction. First, this paper carries out initial prediction of tax revenue by using the LS-SVR model. Second, the accelerated translation transform and weighting mean value generating transform are introduced to process the error sequence. On the basis of the processed data, the error predictive method based on data transformational GM(1,1) is constructed and predicts the subsequent error. Third, the correction of preliminary prediction values is calibrated. The case study based on Chinese tax revenue during the last 30 years shows that the presented approach improves the accuracy of forecasting significantly compared with the prediction accuracy before correction, and then the validity of the model is verified.

@&#INTRODUCTION@&#
Taxation has become a major source of government's revenue. With the economic development and the continual improvement on the market mechanism, the tax policy has been playing an increasingly important role in the development of the market economy in China. On the one hand, the policy not only affects citizen's disposal income and consuming behavior, but also has an influence on the enterprise's financial burden and development potential. On the other hand, the state budget formulation, the implementation of macro-control and the expenditure of infrastructure construction are all depended on the total amount of tax revenue. Therefore, forecasting the tax revenue exactly is very important to carry on the macroscopic regulation efficiently under the market economy [1].In recent years, scholars at home and abroad have made extensive and profound studies on forecasting methods and acquired some achievements, but there are still some deficiencies to be improved. Statistical methods have been extensively adopted for time series forecasting in the past decades, among them are moving average, exponential smoothing, and autoregressive integrated moving average (ARIMA) [2]. These methods are based on the assumption that a probability model generates the underlying time series data. Future values of the time series are assumed to be related to past values as well as to past errors. Besides, they cannot depict the influence of external intervention clearly. For example, Autoregressive integrated moving average (ARIMA) models are one of the most popular linear models in time series forecasting, which have been widely applied in order to construct more accurate hybrid models during the past decade. However, ARIMA model cannot reflect the relationship between the predictive variable and economic elements [3–6]. The GARCH model cannot explain the negative correlation phenomenon between tax revenue and tax income fluctuations. What is more, the constraint conditions may result in the emergence of shock phenomenon in the estimation of GARCH model [7,8]. Duncan et al. have developed Bayesian forecasting model, which is on the basis of multi-state Kalman filter and Conditional independence and stratification. The result shows that Bayesian forecasting model is superior to the single-variable multi-state Kalman filter method. Besides, the relative accuracy increases with the reduction of the length of the historical time series [9]. The explanatory variables existing in the error correction model and based on the co-integration theory contain only GDP and tax revenue without consider other factors which affect the interpretation of the model results [10,11].Recently, machine learning techniques have drawn attention and useful forecasting systems based on these techniques have been developed [12]. Neural network is a popular network architecture in use for time series forecasting [13,14]. It often encounters the local minimum problem during the learning process and the number of nodes in the hidden layer is usually difficult to decide. In addition, it lacks the capability of adaptation and the distance measure adopted may affect prediction performance. Furthermore, the neural network training and learning are based on the causal relationship of predictive variables and predictive factors which are implicit in the samples, but this study does not reflect the changes of external environment and its effects on the prediction. The circumstances of prediction object change, the prediction accuracy will be greatly reduced [15,16]. For this, some scholars have made many attempts and put forward the improved neural network method. For example, Pulido et al. describe a hybrid method based on particle swarm optimization for designing ensemble neural networks with fuzzy aggregation of responses to forecast complex time series [17]. And Castillo et al. present several models of interval type-2 fuzzy neural networks (IT2FNNs) with different architectures [18]. In the study of Gaxiola et al., a new back-propagation learning method enhanced with type-2 fuzzy logic is presented [19]. Simulation results show that these optimized neural network approaches have good prediction. Fuzzy theory is incorporated for prediction in the stock market [20–22,19]. It does not need model identification, inspection and relevance judgment. However, sample classification consistency and data characteristics are closely related to the computation speed and the prediction accuracy, so whether the method can ensure a higher forecast accuracy or not is greatly uncertain [23,24]. Also, no learning is offered by fuzzy theory. Support vector regression can provide high accuracy by mapping into a high-dimensional feature space and including a penalty term in the error function. However, the parameters of SVR model are hard to define and the predictive effect of this model is subject to these parameters to a large extent. In practice, they are selected mostly by experience. Moreover, some valuable information of the SVR model will be lost when reconstructing the indicator data [25,26].In recent years, many scholars realize that integration of different models can be an effective way of improving upon their predictive performance, especially when the models in combination are quite different. From this point of view, a novel hybridization of artificial neural networks and ARIMA model is proposed in order to overcome mentioned limitation of ANNs and yield more general and more accurate forecasting model than traditional hybrid ARIMA-ANNs models. In the proposed model, the unique advantages of ARIMA models in linear modeling are used in order to identify and magnify the existing linear structure in data, and then a neural network is used in order to determine a model to capture the underlying data generating process and predict, using preprocessed data [27]. Babu et al. also think that a suitable combination of linear and nonlinear models could provide a more accurate prediction than an individual linear or nonlinear model. The linear autoregressive integrated moving average (ARIMA) and nonlinear artificial neural network (ANN) models are explored in his paper to devise a new hybrid ARIMA–ANN model for the prediction of time series data [28]. These combined predictions improve the predictive effect to a certain extent, but these methods all ignore the usefulness of the error message.In short, the prediction methods mentioned above only consider the key factors that affect the prediction target in the modeling process, and fail to make full use of useful information implied in the error value. Obviously, the error information generated for the foregoing reasons cannot be explained by the model, but the information is valuable. As for the question that how to make full use of the error information, scholars have conducted extensive and thorough research, discussed the value of error information from different perspectives and expanded the application of error correction in fields of stratified resampling [29], fault diagnosis [30], system identification [31] and predictive control [32]. Perhaps even more importantly, the prediction error may form a time sequence, which still contains some valuable information that cannot be explained by initial forecasting model [33]. Actually, the prediction error belongs to a composite stochastic process due to two facts: the ability of model to predict the tax revenue in some certain moment (higher, lower or normal), and the reflection of prediction error in the model. If we are able to dig out the rule of change in the model state series in the light of historical data, we will forecast the prediction error and adjust the initial predicted value to improve prediction accuracy [34]. Considering the forecast error usually has auto-correlation, we can analyze the internal rule of error sequence and forecast the follow-up in the series, and then use the result to modify the initial prediction of tax revenue [35]. Empirical study shows that the GM(1,1) model based on the data transformation can handle error sequence very well, and improve the imitative effect to a certain extent [36,37].On the basis of the above literature research, this paper attempts to introduce error correction method into revenue forecasting research. To begin with, this paper carries out initial prediction of tax revenue. And then the GM(1,1) model based on data transformation is introduced into the handling process of prediction error, so as to analyze the distribution of the prediction error and construct the error predictive model. Finally, the predictive value of error is used to adjust the initial predicted revenue. Our study contributes to several streams of literature. First, we add to the literature that examines the further advanced theory and methods to improve the forecasting accuracy. This rapidly growing area of the literature has received a great deal of attention lately. Although lots of intelligent estimation methods have been put forward in earlier studies, our investigation represents new contributions that error correction method is introduced to improve forecasting accuracy. This method emphasizes on the importance of making full use of error information. Second, our paper extends prior research on the application of GM(1,1), the features of data transform GM(1,1) are well suited for the characteristics of prediction error. We proved that, by means of mathematical reasoning, error sequences could be transformed to a non-negative smooth monotone sequence through data transformation. The new sequences are much less volatile than before, and the smooth degree also becomes small. Third, our paper constructed error correction method based on data transformational GM(1,1). The effectiveness of the proposed approach is demonstrated by the results obtained from a number of experiments conducted on tax forecasting.The remainder of the study proceeds as follows: Section 2 outlines the theory of mathematical modeling, the error correction method based on data transformational GM(1,1) and initial forecast model. Section 3 describes the sample selection, modeling procedures and the simulation results for the proposed methods. Finally, in Section 4, conclusions are presented.In accordance with system theory, time series contains abundant information, and all the other variables trace is involved in the dynamic process of the system. However, the prediction model constructed through the training samples only reflect the general law of the input variables and output variables, but it is easy to cover up individual special characteristics of sample points. As mentioned, the paper takes the prediction of tax revenue as example. The fluctuations of tax revenue are affected by internal and external factors of the market economy. The complexity of the economic market led to the complexity of its impact on tax revenue fluctuation. Therefore, annual tax revenue has its special regularity. If a unified model is used to describe tax revenue for all years, there must be one-sidedness. This is because it lost a large of specific information for each year in the forecasting process, thereby ignoring the particularity of each year.Obviously, the error information generated for the foregoing reasons cannot be explained by the model, but the information is valuable. Since the forecast object is a time series, for illustration purposes expediently, the time domain can be expressed as t∈D−+D+. Where, D− represents historical time domain, in which the real value of the predict object is known to all, and is also the basis for constructing prediction model. While D+ denotes predicted time domain, in this case the true value of the prediction target is unknown.Assuming historical data can reflect the changes in the characteristics of predict object, so a rational prediction model that reflects changed trend of historical data could be build up. And then we can get the predicted values{yˆt:t=1,…,N}and the corresponding error sequences, which is {ɛt:t=1, …, N}, through the above-mentioned model. Although error value cannot be explained by forecasting model, it still contains some valuable information. It is a good supplement for the current prediction methods to correct the prediction error of the prediction object. The advantage is that error information is considered as useful information to be made full use of. Furthermore some other forecasting methods can be introduced into the error process [35,38].So in order to amend the initial predicted value, we bring in the error modification process. In fact, we assume the predict error is a series {ɛt:t=1, …, N} and consider ɛtbeing a new stochastic process, so we can put forward the prediction model for ɛtand get the predicted value of error{εˆt:t=2,…,N+1}.The initial predicted value will be adjusted as follows:(1)y′t=yˆt−εˆtwhereyˆtis the initial predicted value,y′tdenotes the adjusted initial predicted value andεˆtis the predicted value of error.Based on the above, this article puts forward the error correction method based on data transformational GM(1,1) and specific reasons are as follows.First, the error value contains some valuable information, which can be made use of to correct the initial predicted value to increase forecasting accuracy.Second, no matter whether the existing information is poor or rich, the amount of forecast periods is generally small. If the predicted periods are very long, it is quite hard to avoid large error. So the forecast periods are usually small, and the sequence of the prediction error is small.Third, GM(1,1) is a kind of “poor information” prediction method, namely it could forecast the coming value of the time series, and the GM(1,1) is suitable for the prediction of error.Fourth, the error sequence is volatility series, in which there are positive values and negative values. The error correction method based on data transformational GM(1,1) transform the error sequences to a non-negative smooth monotone sequence through the accelerated translation transforms and weighting mean value generating transforms. The new sequences are much less volatile than before, and the smooth degree also becomes small. The example verifies that this method has a better fitting effect and higher prediction precision.Definition 1Let x(0)=(x(0)(1), x(0)(2), …, x(0)(n)) be a raw data sequence, and mapping T1 is said a sort of data transformation called accelerated translation transform. If H is a positive integer,11Generally speaking, the value of H is the difference between the maximum of x(0) sequence and the minimum of x(0) sequence. Empirical analysis shows that the parameter H could be integer that is in close proximity to the range of error sequence, but it needs to satisfy the fact that the sequence after transformation T1 is a monotonic increasing sequence. Of course, for the arbitrary sequence x(0)=(x(0)(1), x(0)(2), …, x(0)(n)), we can attest to the fact that x(0)T1=(x(0)(1)T1, x(0)(2)T, ..., x(0)(n)T11) is a monotonic increasing sequence.and mapping T1 meets x(0)(k)T1=x(0)(k)+(k−1)H k=1, 2, …, n.Let x(0)=(x(0)(1), x(0)(2), …, x(0)(n)) be a raw data sequence, and mapping T2 is said a sort of data transformation called weighting mean value generating transform and satisfiesx(0)(k)T2=(∑i=1kx(0)(i))/k.Nature 1. Weighting mean value generating transform could weaken the randomness of the raw data sequence, namely the new data sequence is much less volatile than before.ProofLet x(0)=(x(0)(1), x(0)(2), …, x(0)(n)) be a raw data sequence.It's corresponding true value sequence is {x∗(k)}={x∗(1), x∗(2), …, x∗(n)}Random error sequence is {ɛk}={ɛ1, ɛ2, …, ɛn}, and then x(0)(k)=x∗(k)+ɛk.From above mentioned results, we can getx(0)(k)T2=∑i=1kx(0)(i)k=∑i=1kx∗(i)k+∑i=1ki=1k.Thus the conclusion proves to be true.Nature 2. If x(0) is a monotone increasing (decreasing) sequence, x(0)T2 is also a monotone increasing (decreasing) sequence and meets x(0)(k)>x(0)(k)T2.ProofBy the above weighting mean value generating transform, we can getWhen x(0) is a monotone increasing sequence,kx(0)(k+1)>∑i=1kx(0)(i),thenk∑i=1k+1x(0)(i)>(k+1)∑i=1kx(0)(i).Namely,∑ik+1x(0)(i)/(k+1)>∑i=1kx(0)(i)/kSo x(0)(k)T2<x(0)(k+1)T2It means that x(0)T2 is also a monotone increasing sequence and meets x(0)(k)>x(0)(k)T2.Evidenced by the same token, if x(0) is a monotone decreasing sequence, x(0)T2 is also a monotone decreasing sequence and meets x(0)(k)>x(0)(k)T2.Nature 3. If the nonnegative x(0) is a monotone increasing sequence and satisfies quasi exponential law, the smooth ratio of x(0)T2 will be less than that of x(0). Namely,x(0)(k)T2/∑i=1k−1x(0)(i)T2<x(0)(k)/∑i=1k−1x(0)(i).ProofAccording to the above conditions, set x(0)(k)=Aeak(A>0, a>0),Then x(0)(k)T2=A(1−eak)/[k(1−ea)]Let f(x)=xea(x−1)/(1−eax)(a>0)If x>0 and f′(x)<0,the f(x) is a monotone decreasing function.So kea(k−1)/(1−eak)<(k−1)ea(k−2)/(1−ea(k−1)).Since a>0, we can get kea(k−1)(1−ea)/(1−eak)>(k−1)ea(k−2)(1−ea)/(1−ea(k−1)).That is to say, x(0)(k)/x(0)(k)T2>x(0)(k−1)/x(0)(k−1)T2.∀k, if k>i, thus x(0)(k)/x(0)(k)T2>x(0)(i)/x(0)(i)T2.So x(0)(k)x(0)(i)T2>x(0)(i)x(0)(k)T2Parameter x(0)(i) accumulates at the same, thenx(0)(k)∑i=1k−1x(0)(i)T2>x(0)(k)T2∑i=1k−1x(0)(i)Form those results, we can getx(0)(k)T2/∑i=1k−1x(0)(i)T2<x(0)(k)/∑i=1k−1x(0)(i).Let ɛ(0)={ɛ(0)(1), ɛ(0)(2), …, ɛ(0)(n)} be an error sequence. First, the error sequence is transformed toε1(0)={ε1(0)(1),ε1(0)(2),…,ε1(0)(n)}, which is a monotone decreasing sequence, through the accelerated translation transform, whereε1(0)(k)=ε(0)(k)T1,k=1,2,…,n. And then, theε1(0)(k)sequence is transformed toε2(0)=ε1(0)T2={ε2(0)(1),ε2(0)(2),…,ε2(0)(n)}, whereε2(0)(k)=ε1(0)(k)T2,k=1,2,…,n. So the sequenceε2(0)is regard as raw sequence, according to which error correction model based on data transformational GM (1,1) is constructed.In this paper it takes the LS-SVR as an example to carry out the initial prediction of tax revenue.Given the data set (xi, yi) i=1, 2, …, n, xi∈Rnis the input vector and yi∈R is the output variable that corresponds to xiSVM regression method is to find a nonlinear map from input space to output space and map the data to a higher dimensional feature space through the map. The linear estimation function is defined as:(2)y=f(x,ω)=ωTϕ(x)+bThe optimization problem of LS-SVR can be described as follows:(3)minω,b,ξJ(ω,b,ξ)=12ωTω+12C∑i=1Nξi2s.t.yi=ϕ(xi)ω+b+ξii=1,…,Nwhere ω is the weight vector, C is the penalty parameter, and is non-linear mapping function, ϕ(x) denotes nonlinear mapping from the input space to high-dimensional feature space, and b is a partial vector. We can easily get the following function Lagrange:(4)L(ω,b,ξi,αi)=12||ω||2+12C∑i=1Nξi2−∑i=1Nαi[ϕ(xi)ω+b+ξi−yi]According to the KTT we get,(5)0ETEϕϕT+C−1Iba=0yThen, we have the LS-SVR regression function model,(6)f(x)=∑i=1NαiK(xi,xj)+bwhere K(xi, xj)=ϕ(x)Tϕ(xi) is called the kernel function. The radial basis function (RBF) is one of the most popular kernel functions for SVM. The RBF can be described in the following way:(7)K(xi,xj)=ϕ(xi)ϕ(xj)=exp(−||xi−xj||/2δ2)where σ2 is the squared bandwidth, which is optimized through an external optimization technique during the training process.In this paper, we obtain the historical data from the China Statistical Yearbook. The whole data set covers the period from 1980 to 2012, a total of 33 pairs of observations. The experimental data should be necessary divided into the two subsets: the training data and the testing data. In the tax data of China, the data from 1980 to 2006 are used as the training data, others are used as the testing data.The data of tax revenue is set as characteristic sequences X0, and then we select the following 7 indexes as relevant factors sequence to make an analysis according to the size of the influencing factors, the comparability of information and the requirements of prediction model. Among these indexes, as determined by the review of domain experts and prior research, some indexes which have a direct impact on the level of tax revenue are very important to measure the development situation of three industries, including the value-added of the first industry (X1), the value-added of the secondary industry (X2), and the value-added of the tertiary industry (X3). Additionally, there are some indexes which show the size of tax revenue directly or indirectly, such as fixed asset investment across the country (X4) and total volume of foreign trade (X5). Furthermore, some can show the people's living standards and have a direct impact on the status of the total tax revenue growth, for instance, total retail sales of social consumption goods (X6). In addition, there are others which reflect the relationship between revenue growth and economic development, like the rural and urban residents’ deposit balance (X7).Thus, one can take advantage of LS-SVR to perform initial forecast, and then construct the error correction method based on data transformational GM(1,1) to revise the preliminary predicted value.These specific procedures can be implemented in five steps (Fig. 1).Step 1: The data set is divided into training data and predicted data. The training data is used to determine the speci1cations of the models and parameters. The predicted data is reserved for out-of-sample evaluation and comparison of performances among various forecasting models.Step 2: Carry out initial prediction of tax revenue by taking the example of the LS-SVR model, and calculate prediction error value;Step 3: The accelerated translation transform and weighting mean value generating transform are introduced to process the error sequence.Step 4: On the basis of the processed data, the error predictive method based on data transformational GM(1,1) is constructed and predicts the subsequent error.Step 5: The predictive value of error is used to adjust the initial predicted revenue and get the final corrected value.We present the results of several experiments with real-world time series datasets to demonstrate the effectiveness of our proposed error correction method based on data transformation GM(1,1). Then several criteria are used to examine the forecasting accuracy of the newly proposed method, including mean absolute percentage error (MAPE) and mean square prediction error (MSPE), which are defined as follows:(8)MAPE=1N∑i=1Nxt−xˆtxt(9)MSPE=1N∑i=1N(xt−xˆt)xt2where N is the number of the testing data, and xtandxˆtare the actual value and predictive value, respectively, for i=1, 2, …, N.Here we implement LS-SVR model to the historical data and meanwhile to get the predicted error. The predictive effect is shown in Fig. 2.From Fig. 2, we can see that the LS-SVR has made considerable fitting results and the trained network has also good verifying precision. In accordance with the above mentioned trained LS-SVR, the initial predicted value and the corresponding prediction error can be calculated, which are shown in the following Table 1.According to the results in Table 1, we need to perform further analysis on the forecast error sequence {ɛt:t=1, …, N} The sequence (7467.90, 3614.22, 5644.81, 6325.18, 4021.50) is regard as training sample of error predictive model, say ɛ(t)_train here. First of all, the sequence aɛ(t)_train will be processed in light with the data transformation in the Definitions 1 and 2. Then we build the error correction method based on data transformational, and the time response of a gray GM(1, 1) model is as follows:εˆ2(1)(t+1)=39011.7000e(0.1810×t)−31543.0000,t=0,1,…,4Taking the inverse of the above formula,εˆ2(0)(t+1)=6458.9581e0.1810×t,t=1,…,4where t=0,εˆ2(0)(1)=16.5348. Theεˆ(0)(t)can be gotten through the calculation and inverse transformation, namely (7468.00, 4159.13, 4640.02, 5076.62, 6729.74). And the predicted value of error is 9913.22.Then we use the predicted value of error to correct the initial predictive value (yˆt), and get the corrected value (y′t), as shown in Table 2. It is obvious that the error has been significantly reduced. It shows that error correction model can enhance the forecast precision.Given that the accuracy of the prediction varies with the value of H, so whether there is a result inconsistent with the above conclusion that the presented approach improves the accuracy of forecasting significantly compared with the prediction accuracy before correction. To be more confident that empirical findings from the study can be generalized, sensitivity analysis of the results was conducted.When the parameter H takes different values, the comparative analysis of prediction accuracy before and after correction is as follows.As indicated in Table 3, when H is a different value, the mean absolute percentage error (MAPE) and the mean square prediction error (MSPE) after correction is significantly less than 8.66% and 3.86% before correction.The absolute value of percentage error of the corrected predictive tax revenue is 3.44%, 3.47%, 3.51%, 3.56%, 3.60%, 3.65%, respectively, while the percentage error of the initial predicted tax revenue is 6.3%. Therefore, we may say that the forecasting method proposed in this paper has relatively reliable robustness with respect to the value of parameter H, and it also verifies the applicability of this method.To further demonstrate the robustness and superiority of the newly proposed method in this paper, we compare the forecasting accuracy of it with that of some well recognized methods proposed in previous literature. It should be noted that in order to compare with more similar and detailed research related with our research, we not only consider the single forecasting methods, i.e., GM(1,1) method, ARIMA method, BP neural network and LS-SVR method, but also the prediction methods based on error correction (i.e., the methods proposed in [38]). Besides, in order to obtain the objective comparative results, we take the same historical daily observations as previous literature for training and the same out-of-sample observations for testing the forecasting performance. Table 4summarizes the comparative results of forecasting performance among the seven methods.As seen from Table 4, it is evident that the forecasting accuracy of the method proposed in this paper appears better than the other methods, due to its lower MAPE(1.83) and MSPE(0.89) values, although all the forecasting performances using the seven methods are basically acceptable. Therefore, according to the comparative results, we can say that the method proposed in this paper indeed has relatively better forecasting performance for tax revenue.In order to analyze the influence of different initial prediction methods on the forecast effect, we conduct a lot of empirical tests and the results still support this conclusion. Namely, after correction, the prediction accuracy has improved significantly. Compared with single forecast method, the forecasting method in this paper has a better predictive effect.22Of course, there are other methods too numerous to mention here, so the conclusion does not mean that the method in this paper is better than any other single forecasting model.

@&#CONCLUSIONS@&#
