@&#MAIN-TITLE@&#
A binary ABC algorithm based on advanced similarity scheme for feature selection

@&#HIGHLIGHTS@&#
A binary artificial bee colony (ABC) algorithm is proposed for feature selection.A comprehensive comparative study of the ABC PSO variants is presented.The superiority of the algorithm is demonstrated on both training and test sets.The times of appearance of each feature over 30 runs for each dataset are presented.The proposed algorithm performs better than the others.

@&#KEYPHRASES@&#
Feature selection,Artificial bee colony,Particle swarm optimization,Classification,

@&#ABSTRACT@&#
Feature selection is the basic pre-processing task of eliminating irrelevant or redundant features through investigating complicated interactions among features in a feature set. Due to its critical role in classification and computational time, it has attracted researchers’ attention for the last five decades. However, it still remains a challenge. This paper proposes a binary artificial bee colony (ABC) algorithm for the feature selection problems, which is developed by integrating evolutionary based similarity search mechanisms into an existing binary ABC variant. The performance analysis of the proposed algorithm is demonstrated by comparing it with some well-known variants of the particle swarm optimization (PSO) and ABC algorithms, including standard binary PSO, new velocity based binary PSO, quantum inspired binary PSO, discrete ABC, modification rate based ABC, angle modulated ABC, and genetic algorithms on 10 benchmark datasets. The results show that the proposed algorithm can obtain higher classification performance in both training and test sets, and can eliminate irrelevant and redundant features more effectively than the other approaches. Note that all the algorithms used in this paper except for standard binary PSO and GA are employed for the first time in feature selection.

@&#INTRODUCTION@&#
Thanks to the rapid development in computer hardware and software, a huge amount of information can be collected and included in datasets through a large number of features (attributes). However, not all features are relevant to the target concept. In other words, datasets may include irrelevant and redundant features besides relevant ones. Unfortunately, these features may adversely affect the classification performance due to the large search space, known as “the curse of dimensionality” [1,2]. Furthermore, more features may introduce more noise to the dataset that can be also detrimental to the classification performance. Thus, it is important to select an appropriate feature subset from the available features to achieve similar or even better classification performance than using all features [3]. The task is terminologically known as “feature selection”. It does not only achieve better classification accuracy, but also improves the efficiency, reduces data complexity and simplifies the structure of the learnt classifiers [2].Feature selection is one of the most difficult tasks in data mining and classification due to the feature interaction and the large search space [4,5]. Feature interaction may appear as two-way, three-way or may involve even more features. For instance, a feature by itself may not have a confident effect to the target, but its effect can be increased when used together with other features. Also, a feature which is individually relevant may become redundant when interconnected with others. The other challenging task is the large search space, 2n, where n is the total number of features. In other words, it is not possible to thoroughly search all possible solutions in most cases. Although a variety of search methods such as sequential forward and backward feature selection (SFS, SBS) [6,7] have been proposed, they may converge to local minima or cost high computational time.To address these problems, evolutionary computation (EC) techniques have been used as a strong alternative to the classical search methods due to their global search potentials. Particle swarm optimization (PSO) [8,9], genetic algorithms (GAs) [10,11], genetic programming (GP) [12,13], and ant colony optimization (ACO) [14,15] have been widely applied to feature selection. In this study, artificial bee colony (ABC) [16] based on foraging behaviours of honey bees is chosen as the main motivation to address feature selection problems on account of the following advantages when compared to the other well-known EC techniques [17]: (1) it can converge more quickly to the target, (2) It is computationally less expensive, and (3) It is one of the most recent EC techniques. The idea of applying ABC to feature selection is not a novel subject, i.e., there exist some studies concerning the ABC based feature selection [18–20]. However, the existing studies unfortunately have not demonstrated a comprehensive experimental study, including comparisons with recent EC variants, on a variety of datasets or thorough performance evaluation and analysis. Therefore, the potential of ABC for feature selection has not been fully demonstrated and the need for the studies based on ABC has not come to an end.The overall goal of this paper is to propose an improved binary version of the artificial bee colony (ABC) algorithm to address feature selection problems. To achieve this goal, the discrete binary ABC (DisABC) algorithm [21] based on the similarity of Jaccard coefficient among individuals is further improved by introducing the neighborhood selection mechanism of the differential evolution (DE) strategy. In other words, the similarity based search approach is re-simulated according to the DE mutation, recombination and selection strategies. The other goal is to put forward a comprehensive comparative study of some variants of the ABC, PSO and GA algorithms on wrapper feature selection in terms of the classification performance and the feature subset size for the future studies of researchers. To establish the second goal, seven algorithms, which are binary PSO (BPSO) [22], new velocity based binary PSO (NBPSO) [23], quantum inspired binary PSO (QBPSO) [24], discrete ABC (DisABC) [21], angle modulated ABC (AMABC) [25], modification rate based ABC (MRABC) [26] and genetic algorithms (GA) [27] are employed, and 10 benchmark datasets, including various classes, instances and features are chosen from the UCI machine learning repository [28]. Further, two recently published ACO studies are considered to evaluate the performance of the proposed ABC variant. To our knowledge, the employed algorithms except for BPSO and GA are used for feature selection for the first time, and a comprehensive comparative analysis on feature selection is not very common in the literature. Specifically, the following points are investigated:•whether integrating a differential evolution search mechanism to the DisABC algorithm improves its global search ability in feature selection tasks,whether the proposed algorithm is able to perform well in both training and test sets in terms of the classification rate when compared with the seven existing algorithms,whether the proposed algorithm can more effectively remove redundant or irrelevant features and can obtain better feature subsets than the seven existing algorithms, andwhether the proposed algorithm performs better than conventional deterministic feature selection approaches.The rest of the paper is organized as follows. Section 2 gives an outline of the basic ABC algorithm and provides a background on recent studies related to feature selection. Section 3 presents the proposed algorithm and Section 4 describes the experimental design. Section 5 presents the experimental results and discussions. Section 6 concludes the study and provides an insight into the future trends.

@&#CONCLUSIONS@&#
The main goal of this study was to propose a new variant of the DisABC algorithm for feature selection. This goal was successfully achieved by introducing DE based neighborhood mechanism into the similarity based search of DisABC. The second goal of this study was to demonstrate a comprehensive comparative study for the future studies of researchers. This goal was achieved by comparing the proposed algorithm with the seven different EC based algorithms, including BPSO, NBPSO, QBPSO, DisABC, AMABC, MRABC and GA, and three classical approaches, including LFS, LFFS and GSBS. It should be noted that while BPSO and GA are the algorithms most widely applied to the feature selection, the other EC based algorithms are implemented for feature selection for the first time.The obtained results show that the integration of DE based similarity search mechanism into the DisABC algorithm effectively improved the global search ability of the algorithm in feature selection, and the proposed MDisABC algorithm achieved the best classification performance in both training and test sets in almost all cases. Further, the proposed MDisABC algorithm is able to remove redundant features effectively while obtaining the highest classification results. The results also show that the proposed MDisABC algorithm outperformed the deterministic non-EC methods, LFS, LFFS and GSBS in terms of the classification accuracy and selected a much smaller number of features than GSBS. The analysis of the features selected by different algorithms reveals that the proposed MDisABC algorithm is the most stable and consistent algorithm among all the algorithms. Moreover, the analysis of the CPU times of the different methods shows that the proposed MDisABC algorithm achieved better accuracy than the existing methods without taking a longer computational time. In the future, the studies of feature selection based on ABC are expected to increase and more ABC based approaches will be developed. There are also some new evolutionary algorithms [74–76] which have not been used in feature selection. We will test the proposed method on large dimensional datasets and consider the feature selection problem in filter approaches [77] using ABC.