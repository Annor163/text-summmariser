@&#MAIN-TITLE@&#
Solving nonlinear systems of functional equations with fuzzy adaptive simulated annealing

@&#HIGHLIGHTS@&#
Efficient and accurate approach to solve nonlinear systems of functional equation.Optimization is carried out by stochastic fuzzy adaptive simulated annealing.Several examples presented and compared with results obtained by other approaches.

@&#KEYPHRASES@&#
Nonlinear equations systems,Fuzzy logic,Global optimization,Simulated annealing,

@&#ABSTRACT@&#
This paper proposes a method for finding solutions of arbitrarily nonlinear systems of functional equations through stochastic global optimization. The original problem (equation solving) is transformed into a global optimization one by synthesizing objective functions whose global minima, if they exist, are also solutions to the original system. The global minimization task is carried out by the stochastic method known as fuzzy adaptive simulated annealing, triggered from different starting points, aiming at finding as many solutions as possible. To demonstrate the efficiency of the proposed method, solutions for several examples of nonlinear systems are presented and compared with results obtained by other approaches. We consider systems composed of n equations on Euclidean spacesℝn(n variables: x1, x2, x3, ⋯, xn).

@&#INTRODUCTION@&#
Finding solutions to systems of nonlinear equations has been a major concern in applied mathematics, in view of the importance of the subject in a large variety of areas, such as engineering, medicine, chemistry and physics [1]. Unfortunately, although there are several methods directed toward exactly solving systems of linear equations, closed form solutions for systems of nonlinear equations are rare, and, sometimes, too specific to be useful in practice [2–4]. On the other hand, there are elegant and useful results, such as the fixed point theorem of S. Banach that allows us to devise methods for finding approximate solutions for a significant class of nonlinear systems, although it could be hard to prove that certain systems are compatible with its hypothesis.Equations can be described as expressions of correspondences between elements of given spaces by means of operators defined on those spaces, as, for example, finite dimensional Euclidean or infinite dimensional function spaces. Using properties of the involved transformations and defining the particular correspondences and the properties of the underlying state spaces, we can look for conditions under which solutions exist and, in that case, derive a procedure to obtain the solutions in closed or approximate form. Solving systems of equations involves finding elements of the state space satisfying all component equations simultaneously – in our case, vectors ofℝn. Therefore, taking into account that closed form solutions are difficult to derive in the case of systems of nonlinear equation, algorithms that are capable of finding solutions efficiently and precisely are of interest, especially in situations that present non-differentiable or even discontinuous operators, when traditional methods are even scarcer.In [1], whose advances are used as comparison references for the numerical results presented in this paper, the authors describe a significant number of methods for solving numerically the aforementioned problem and report an extensive literature review about the subject, proposing a new method that is based on the transformation of a system of nonlinear equations into a multi-objective optimization problem. The transformed problem is then solved by means of Pareto dominance relationship between candidate solutions and an iterative strategy, thereby evolving the respective populations toward better and, hopefully, optimal solutions. The technique uses principles of evolutionary computing, and is able to handle very general systems without demanding additional conditions on the component functions, such as differentiability or continuity, for instance, differently from gradient based paradigms. Although in [1] a number of applications of multi-objective programming to specific problems are described, no previous method for solving general systems of nonlinear equations using computational intelligence techniques seems to be publicly known until the appearance of the cited reference.The general problem we are concerned with in this paper is that of finding approximate solutions to systems of simultaneous nonlinear equations of the form(1)f(x)=f1(x)f2(x)⋮fn(x)=0,x,0∈ℝnf:Ω→ℝn,fi:Ω→ℝΩ=def∏i=1n[ai,bi]⊂ℝn,ai,bi∈ℝ,ai<bix=(x1,x2,…,xn)0=(0,0,⋯,0)where f1, f2, …, fnare nonlinear functions in the class of all real-valued, not necessarily continuous, functions onΩ. At least one of the equations should be nonlinear.Finding a solution for a system of nonlinear equations means to obtain simultaneous solutions for every equation appearing in the system, that is(2)f1(s1,s2,…,sn)=f2(s1,s2,…,sn)=⋯=fn(s1,s2,…,sn)=0for some particular vector(s1,s2,…,sn)∈ℝn, at least.Unfortunately, it is not always possible to obtain exact closed-form solutions for nonlinear equations, in general. From this fact follows the need to synthesize numerical methods capable of finding accurate approximations for their roots. This setting gets worse whenever functional relationships present in the component equations lack crucial attributes, such as continuity or differentiability, taking into account that Newton-like gradient-based methods become unfeasible, given the impossibility to evaluate derivatives, for instance. Here, we propose to approach such a problem by means of a global optimization based method, regarding it as a multi-objective minimization (each equation should be satisfied), but solving it by the minimization of a single objective function that encloses all the constraints. Such a function is simply the sum of the absolute values of left sides of component equations, as we shall see below.The task of global minimization of numerical functions is of paramount importance in several areas of knowledge. In practical cases, the function to be minimized assumes the form of a cost measure that varies with several parameters and is subject to certain constraints, imposed by its environment. Whenever the objective function is “well-behaved,” that is, differentiable, convex or satisfies Lipschitz conditions, there are several techniques that are capable of finding points at which it attains its minimum value, obeying certain imposed constraints.Difficulties arise whenever the given function presents several local minima, thus making the final result dependent on the starting point. Unfortunately, most real problems lead to very complex objective functions that are nonlinear, discontinuous, multi-modal and multi-dimensional among other properties. To solve such a class of problems, stochastic methods seem to be a good, and sometimes the only, alternative. Genetic algorithms (GA), simulated annealing (SA) and particle swarm optimization (PSO) are among the most popular approaches to stochastic global optimization. The difficulty associated to stochastic approaches is mainly related to the speed of convergence. Despite these drawbacks, researchers have found ways to overcome certain limitations of original evolutionary schemes, leading to implementations such as adaptive simulated annealing (ASA) [5], which is a sophisticated and rather effective global optimization method. The ASA technique is particularly well suited to applications involving neuro-fuzzy systems and neural network training, thanks to its superior performance and simplicity.The ASA approach has the benefits of being publicly available, parameterized, and well-maintained, thereby showing an alternative to GA, according to the published benchmarks, which demonstrate its effectiveness [6]. Unfortunately, many stochastic global optimization algorithms share a few problems, such as large periods of poor improvement in their way to the global optimum. In SA implementations, that behavior is mainly due to the cooling schedule, whose speed is limited by the characteristics of probability density functions, which are employed with the purpose of generating new candidate points. In this manner, if we choose to employ the so-called Boltzmann annealing, the temperature has to be lowered at a maximum rate of T(k)=T(0)/ln(k). In the case of fast annealing, the schedule becomes T(k)=T(0)/k, if assurance of convergence with probability 1 is to be maintained, resulting in a faster schedule. The approach based on the ASA technique has an even better default scheme given by(3)Ti(k)=Ti(0)exp(−Cik1/D)because of its improved generating distribution. The constant Ciis a user-defined parameter and D is the number of independent variables of the function under minimization (dimension of the domain). Notice that subscripts indicate independent evolution of temperatures for each parameter dimension. In addition, it is possible to take advantage of simulated quenching, that is,(4)Ti(k)=Ti(0)exp(−CikQi/D)where Qiis termed the quenching parameter. By attributing to Qivalues greater than 1 we obtain a gain in speed, but the convergence to the global optimum is no longer assured [5]. Such a procedure could be used for higher-dimensional parameter spaces, whenever computational resources are scarce. The internal structure of a well-succeeded approach to accelerate the ASA algorithm, using a simple Mamdani fuzzy controller that dynamically adjusts certain user's parameters related to quenching, is described in [7]. It is shown [10] that, by increasing the algorithm perception of slow convergence, it is possible to speed it up significantly, thereby reducing considerably (perhaps eliminating) the user's task of parameter tuning. That is accomplished without changing the original ASA code, resulting in what has been called the fuzzy ASA algorithm. By sampling the progress of the global optimization process (in runtime), the fuzzy controller changes certain internal parameters that govern the evolutionary dynamics, trying for example to escape from local minima attraction basins. This can be accomplished by using well established fuzzy control techniques, provided the parameters to tuned are known, as well as their influence on the overall dynamical system.The effectiveness of the ASA and fuzzy ASA techniques was evidenced through their application to several relevant areas, such as neural network training, fuzzy modeling, machine learning and maximum likelihood estimation [8–11].To find approximate solutions for system (1), we propose the following algorithm:Step 1 – Transform the original problem into a global optimization one, by defining the objective function, C(x), as the sum of the absolute values of component functions fi, and search for the global minima of(5)C(x)=∑i=1n|fi(x)|,x∈ΩΩ=def∏i=1n[ai,bi]⊂ℝn,ai,bi∈R,ai<biStep 2 – Submit C(x) to the fuzzy ASA algorithm, aiming at finding different solutions to the new global optimization problem defined by (5). Here, it is possible to simply run the algorithm several times or use its multistart version, taking into account that, as a stochastic method, fuzzy ASA is able to explore different regions during different activations.Step 3 – If sufficient solutions presenting global error C(x) less than a predefined value were found, then stop. Otherwise, go back to step 2 until reaching the optimum or satisfying a certain stopping criterion (for instance, achieving a maximum number of trials).The reasoning underlying the algorithm is as follows. If it is possible to minimize the sum of absolute errors of all component equations C(x) below a certain desired level, then each individual error is also below that given threshold. Hence, once the acceptable precision beyond which a given vector is considered a solution for system (1) is chosen, solving problem (5) is equivalent to solving the original system of equations (1). It is also possible to place additional restrictions on individual equations by modifying the cost function C(x) and forcing some (or all) equations to satisfy stricter conditions.In any case, the main task is to effectively minimize the function defined by (5) within a given precision considered adequate for the problem at hand. Therefore, the effectiveness of the algorithm is strongly based on the minimization power of the used global optimization algorithm (fuzzy ASA, in our method), particularly in its ability to handle arbitrarily complex landscapes of highly nonlinear cost functions.In this section we present solutions for several examples of nonlinear systems to verify the efficiency and effectiveness of the proposed method. The set of problems addressed in [1], which represents a good and recent standard for comparison, is considered. The first two benchmark systems are 2-dimensional (Examples 1 and 2). Examples 3–8 deal with several different application areas and of varying dimensions. For additional details the reader is referred to [1], which highlights the fact that in [12] the authors applied their method only to systems of dimension 2 (the aforementioned 2 bi-dimensional cases).All tests were carried out using an Intel® Core(TM) 2 CPU @ 2.4GHz computer with 512Mb of RAM, and the programs were coded in the C++ programming language (Borland C++ compiler).It is worth mentioning that, owing to differences among implementations of numerical runtime libraries corresponding to the several programming environments available at present, it is possible for a particular numerical solution to produce slightly different functional results, when submitted to distinct computer programs, coded in different languages.Example 1(2-dimensional – search domain=[0,10]2)(6)f1(x1,x2)=cos(2x1)−cos(2x2)−0.4=0f2(x1,x2)=2(x2−x1)+sin(2x2)−sin(2x1)−1.2=0The results obtained by applying several methods cited in reference [1], including those therein proposed, and by our approach are shown in Table 1(a). In addition, some approximations of solutions obtained through our approach and the respective global approximation error, defined as the sum of the absolute values of all component functions, are presented in Table 1(b).By observing the results, it is possible to conclude that the proposed method was able to approximate several roots of (6) within a very smaller error than did the previously reported ones in the literature. For example, in practical terms the root shown in Table 1(a) could be considered to be exact, when compared to those of the other estimates.Example 2(2-dimensional – search domain=[−10,10]2)(7)f1(x1,x2)=ex1+x1x2−1=0f2(x1,x2)=sin(x1x2)+x1+x2−1=0The results obtained by applying Effati and Nazemi's method [12], the evolutionary approach [1] and our method are shown in Table 2(a). Furthermore, presented in Table 2(b) are some approximations of solutions obtained with our approach and the respective global approximation error, defined as the sum of the absolute values of all component functions.In this test the proposed method was again able to find the true solution (vector (0,1)) within an insignificant absolute error (<10e−15). It is a noticeable fact that, even when started from different initial points, it converged to the same root.Example 3(10-dimensional – interval arithmetic benchmark – search domain=[−2,2]10)(8)x1−0.25428722−0.18324757x4x3x9=0x2−0.37842197−0.16275449x1x10x6=0x3−0.27162577−0.16955071x1x2x10=0x4−0.19807914−0.15585316x7x1x6=0x5−0.44166728−0.19950920x7x6x3=0x6−0.14654113−0.18922793x8x5x10=0x7−0.42937161−0.21180486x2x5x8=0x8−0.07056438−0.17081208x1x7x6=0x9−0.34504906−0.19612740x10x6x8=0x10−0.42651102−0.21466544x4x8x1=0The results obtained by applying the evolutionary approach [1] and our method are shown in Table 3. For the sake of comparison, we considered only a lower bound for the absolute error among all reported results in [1] for the given example (Table VIII/Fig. 6 in [1]). Whereas in [1] several solutions are listed, we have found only one solution (please, refer also to [4,5,9]), because the original equation is equivalent to a fixed point problem, as shown in (9). As the defining mapping Fdis a contraction whenever restricted to a compact subset ofℝ10, containing the hyper-rectangle [−2, 2]10, the contraction mapping theorem [13], due to Stefan Banach, ensures that there is only one fixed point of the given function inside that compact set. In addition, it tells us exactly how to get to it – by following, for example, the orbit{Fdn(x0):n∈ℕ},x0∈[−2,2]10. In other words, in this example the (unique) desired solution of (8) is a global attractor of the discrete dynamical system xn+1=Fd(xn) inside a certain compact subset ofℝ10. It occurs that such a solution lies in [−2, 2]10.(9)x1x2x3x4x5x6x7x8x9x10=Fdx1x2x3x4x5x6x7x8x9x10=def0.25428722+0.18324757x4x3x90.37842197+0.16275449x1x10x60.27162577+0.16955071x1x2x100.19807914+0.15585316x7x1x60.44166728+0.19950920x7x6x30.14654113+0.18922793x8x5x100.42937161+0.21180486x2x5x80.07056438+0.17081208x1x7x60.34504906+0.19612740x10x6x80.4265110+0.21466544x4x8x1Fd:[−2,2]10⊂C→ℝ10,C=compact subset ofℝ10Shown below, Listing 1 can be useful for experimental evidences of the aforementioned facts, taking into account that several random starting points are generated inside the search domain and serve as x0 for several orbits of the discrete dynamical system xn+1=Fd(xn). It is sufficient to run the script under the Matlab® environment to verify that after 15 iterations the solution is found, as stated previously.Listing 1 – Code to illustrate the existence of a fixed point.In the following examples, results obtained by applying the evolutionary approach [1] and our method are presented. For the sake of comparison, only a lower bound for the results reported in [1] is considered (lower bound for the global absolute errors). Also shown are some approximations of solutions obtained by our approach and the global approximation error defined as the sum of the absolute values of all component functions.Example 4(6-dimensional – neurophysiology application – search domain=[−10,10]6)(10)x12+x32−1=0x22+x42−1=0x5x33+x6x43−c1=0x5x13+x6x23−c2=0x5x1x32+x6x42x2−c3=0x5x12x3+x6x22x4−c4=0(11)x1x2+x1−3x5=02x1x2+x1+x2x32+R8x2−Rx5+2R10x22+R7x2x3+R9x2x4=02x2x32+2R5x32−8x5+R6x3+R7x2x3=0R9x2x4+2x42−4Rx5=0x1(x2+1)+R10x22+x2x32+R8x2+R5x32+x42−1+R6x3+R7x2x3+R9x2x4=0whereR=10,R5=0.193,R6=0.002597/(40),R7=0.003448/(40),R8=0.00001799/40,R9=0.0002155/(40),R10=0.00003846/40The results are shown in Table 5(see also [15]). By analyzing the results found by the proposed method, it is possible to see that all estimates cluster around a specific neighborhood that lies far from the non-dominated solutions published in [1]. As the global errors are here much smaller than those ones, we can draw the conclusion that the present method was able to find other attraction regions containing roots for the nonlinear system (11).Example 6(8-dimensional – kinematic application – search domain=[−10,10]8)(12)xi2+xi+12−1=0a1ix1x3+a2ix1x4+a3ix2x3+a4ix2x4+a5ix2x7+a6ix5x8+a7ix6x7+a8ix6x8+a9ix1+a10ix2+a11ix3+a12ix4+a13ix5+a14ix6+a15ix7+a16ix8+a17i=01≤i≤4The coefficients aij(1≤i≤17, 1≤j≤4) are given in Table 6. See also [16]. The results are shown in Table 7. The resulting solutions present very small approximation errors, mainly when compared to previous results in [1], being possible to observe that they lie far from those presented in the cited reference. Once again it is evidenced that the presented method is able to reach several distinct roots with high numerical precision.Example 7(10-dimensional – combustion application – search domain=[−20,20]10)(13)x2+2x6+x9+2x10−10−5=0x3+x8−3.10−5=0x1+x3+2x5+2x8+x9+x10−5.10−5=0x4+2x7−10−5=00.5140437.10−7x5−x12=00.1006932.10−6x6−2x22=00.7816278.10−15x7−x42=00.1496236.10−6x8−x1x3=00.6194411.10−7x9−x1x2=00.2089296.10−14x10−x1x22=0The results are shown in Table 8. Please, refer also to [17]. This test is particularly interesting because the coefficients appearing in the equations range through a wide numerical spectrum and represent a challenge for approximation algorithms. Nevertheless, the proposed algorithm could find good estimates, improving previous results to a great extent.Example 8(n-dimensional – economics modeling application – search domain=[−10,10]n)(14)xk+∑i=1n−k−1xixi+kxn−ck=0,1≤k≤n−1∑l=1n−1xl+1=0This test used n=20 and ci=0, 1≤i≤n−1. The results are shown in Table 9. In this test the algorithm was able to find distinct roots with practically no errors in a relatively high-dimensional state space, confirming again the global minimization power of the proposed method. As can be observed by comparison, a significant improvement is obtained with the proposed approach.The minimization error curve obtained for each of the above examples is displayed in Fig. 1, and the CPU time spent by the proposed algorithm in each example is listed in Table 10.The method advanced in this paper has been effective in solving nonlinear systems of functional equations, finding accurately several solutions and overcoming previous work aimed at the same target. Taking into account that, in practice, there are numerous cases of nonlinear systems that are not solvable analytically (at least, not with known closed form solution), the proposed algorithm can be a valuable tool for researchers of virtually all areas of science and technology. At least in one case (Example 3), the solution was found by another path and shown to be unique in a given domain. This was confirmed with the proposed method, showing that only one solution existed. When compared with results reported in previous work, the proposed algorithm showed greater accuracy and efficiency. Finally, it should be pointed out that an interesting theme for future work could be the application of fuzzy ASA for finding roots of nonlinear systems in ℂn.

@&#CONCLUSIONS@&#
