@&#MAIN-TITLE@&#
Level of interest sensing in spoken dialog using decision-level fusion of acoustic and lexical evidence

@&#HIGHLIGHTS@&#
This study investigates accurate recognition of a user's interest.We use two different classifiers for acoustic level prediction to reduce over-fitting problem.We design robust lexical features from the erroneous ASR output.We use a decision level fusion approach using acoustic and lexical model.

@&#KEYPHRASES@&#
Level of interest,Decision-level fusion,Human–machine interaction,

@&#ABSTRACT@&#
Automatic detection of a user's interest in spoken dialog plays an important role in many applications, such as tutoring systems and customer service systems. In this study, we propose a decision-level fusion approach using acoustic and lexical information to accurately sense a user's interest at the utterance level. Our system consists of three parts: acoustic/prosodic model, lexical model, and a model that combines their decisions for the final output. We use two different regression algorithms to complement each other for the acoustic model. For lexical information, in addition to the bag-of-words model, we propose new features including a level-of-interest value for each word, length information using the number of words, estimated speaking rate, silence in the utterance, and similarity with other utterances. We also investigate the effectiveness of using more automatic speech recognition (ASR) hypotheses (n-best lists) to extract lexical features. The outputs from the acoustic and lexical models are combined at the decision level. Our experiments show that combining acoustic evidence with lexical information improves level-of-interest detection performance, even when lexical features are extracted from ASR output with high word error rate.

@&#INTRODUCTION@&#
Automatic detection of a user's interest in spoken dialogs can benefit many applications, making the systems more adaptive and responsive to a user's behavior. For example, in a tutoring system, if a student is bored or shows no interest in a topic, the system may change the topic or modify the way of its presentation. Similarly for a custom service application, systems with abilities to detect user's level of interest are more likely to provide better service. There have been some previous studies on level of interest detection from speech. Schuller et al. (2009a) adopted a feature-level fusion of various information, including visual, acoustic, linguistic, and temporal evidences. They found that the system combining all evidences achieved better performance than only using single information. In the Paralinguistic Challenge level-of-interest detection subtask at Interspeech 2010 (Schuller et al., 2010), Gajsek et al. (2010) focused on acoustic information to design more robust features, and Jeon et al. (2010) developed a decision-level fusion method using acoustic, lexical, and contextual information. Wang and Hirschberg (2011) used a hierarchical fusion learning method to use the feedback from previous predictions.Research into level-of-interest detection has inherited many of its techniques from the much more extensively investigated field of emotion recognition. Many studies focused on using single information, such as acoustic evidence from speech (Hoque et al., 2006; Batliner et al., 2008; Schuller et al., 2009b; Lee et al., 2011; Jeon et al., 2011), facial expression (Essa, 1997; Ioannou et al., 2005; Metallinou et al., 2010a), and linguistic features (Osherenko and André, 2007). The interacting pattern between partners in a dialog was also shown as a useful cue for emotion recognition (Lee et al., 2009). There also exist studies to combine various information for emotion recognition, for example, fusion of facial and acoustic cues (Busso et al., 2004; Zeng et al., 2007), facial expression, prosody, and head movement (Metallinou et al., 2010b), or body language and prosody (Metallinou et al., 2011). As shown in many studies (e.g., Busso et al., 2004; Zeng et al., 2007; Metallinou et al., 2011), multi-modal processing achieved better performance than relying on any single modality. In particular, combining acoustic and linguistic evidence achieved better performance of emotion recognition than using speech cues only (Lee and Narayanan, 2005; Schuller et al., 2005). The increasing interest in this field has also been manifested by the several challenge tasks recently, such as at Interspeech 2010, 2011, and 2012, and ICMI 2012.This paper is an extension of our previous work (Jeon et al., 2010). We use a decision-level fusion approach to combine acoustic and lexical information. As shown in our previous study (Jeon et al., 2010), the lexical model was greatly affected by speech recognition errors, therefore the combined system has performance degradation on the ASR condition. Based on our previous decision-level fusion system, we propose more robust lexical features to address the performance loss due to ASR errors. We use the bag-of-words model as baseline features, and introduce new features including level-of-interest value for each word, different ways to represent length information, and similarity values. We also utilize multiple ASR candidates (n-best lists) to extract lexical features. In addition, we propose to use acoustic level combination of two different regressors to address a possible over-fitting issue. Our experimental results show that although there is performance loss for ASR conditions, our proposed lexical features can still provide valuable information, and the decision-level fusion system achieves significantly better performance (McNemar test, p<0.05) than our previous approach (Jeon et al., 2010).The remainder of this paper is organized as follows. In the next section, we describe the level-of-interest detection task and the data. We provide details of our approach in Section 3, including the acoustic and the lexical models, and their combination. Section 4 presents our experimental results and discussion. The last section gives a brief summary along with future directions.The data used in this study is from the Interspeech 2010 Paralinguistic Challenge, which is part of the Audiovisual Interest Corpus created at the Technische Universität München (TUM AVIC) by Schuller et al. (2007). In the following we briefly describe how the data was collected and annotated. More information can be found in (Schuller et al., 2007). In the scenario setup of TUM AVIC data, an experimenter and a subject sit on the opposite sides of a desk. The experimenter plays the role of a product presenter and leads the subject through a commercial presentation. The subject's role is to listen to explanations and topic presentations of the experimenter, ask several questions of her/his interest, and actively interact with the experimenter considering his/her interest in the addressed topics. The subject was explicitly asked not to worry about being polite to the experimenter, for example, by always showing a certain level of ‘polite’ attention. Visual and voice data is recorded by a camera and two microphones (one headset and one far-field microphone in this situation). 21 subjects participated in the recordings, three of them Asian, the remaining European. The language used throughout experiments is English, and all subjects are very experienced English speakers. The database comprises 12,839 utterances.11Utterances as used in this work correspond to a speaker and sub-speaker-turns in the TUM AVIC data.The level of interest (LOI) of the utterances was labeled by four male annotators, independently from each other. Five levels were used in the first place: Disinterest (−2), Indifference (−1), Neutrality (0), Interest (+1), Curiosity (+2). Ground truth is then established by averaging the annotators’ labels and then divided by 2. The final LOI values are thus in the range of [−1.1].In the Interspeech 2010 Paralinguistic Challenge, the data comprised of three data sets: training, development, and blind test sets. Table 1shows the information for these data sets, including the number of utterances, the average duration of utterances with standard deviation, the average correlation between the ground truth and 4 annotators with standard deviation,22We cannot estimate the average correlation for the test data since only ground truth data is released.and speaker information. We use the training and development data to evaluate the regressors and tune the parameters, and the test data is used only for the final testing.Note that the shortest utterance is about 0.1 second, while the longest one is longer than 11 seconds. In Fig. 1we plot the histograms for the LOI values of the training and development data. As can be seen, the samples in the training and development data have a skewed distribution, not (roughly) uniformly distributed in the entire region of [−1, 1]. For example, most instances have a label of between 0.4 and 0.8. We will evaluate if this kind of distribution has an impact on the learning methods.For the training and development data, the spoken content has been transcribed, and pause and non-linguistic vocalizations have been labeled. These vocalizations include breathing, consent, hesitation, laughter, coughing, and other human noise. In order to evaluate the performance using ASR output, which is the real testing scenario in human–machine interaction, we generated ASR output for the three data sets using a state-of-the-art SRI recognizer that was developed for broadcast news speech (Stolcke et al., 2006; Zheng et al., 2007). We did not perform any adaptation of acoustic and language models. Because the speakers are non-native English speakers in this data, and the domain is very different from the training data for the recognizer, the word error rate is quite high for this data: over 60% when excluding non-linguistic vocalization tokens, and over 70% when counting all the tokens. When n-best hypotheses from ASR output are used, the oracle word error rate decreases as expected. For example, we find that the oracle word error rate is reduced by more than 10% absolute when using 10-best hypotheses.Since a listener provides feedback in a dialog, we expect that the listener's interest level is dependent on not only how the person says something (represented by acoustic features), but also what the person said (represented by lexical features). Therefore we use both information sources for this task. We adopt a decision-level combination strategy, which achieved better performance than a feature-level combination in our previous work (Jeon et al., 2010). As shown in Fig. 2, our system consists of the acoustic model, lexical model, and a final decision model that combines their outputs. The acoustic and lexical models were independently optimized to achieve the best individual performance using different learning algorithms. Below we explain the details of each component.The acoustic features are extracted using openSMILE (Eyben et al., 2009). This feature set is generated in three steps. First, 38 low-level descriptors (LLDs) as shown in Table 2are extracted and smoothed by a simple moving average filter of three window length, which removes the high frequency components present in the signal. The 60ms Gaussian windows and 10ms step size were used for F0 related low-level descriptors, and the 25ms Hamming windows and 10ms step size were used for the others. Next, their first-order regression coefficients are added, and then 21 functionals (cf. Table 3) are applied. We discarded 16 features which contain no information (e.g., in the presence of unvoiced frames, openSMILE erroneously identifies minimum F0 as zero). Finally, two single features: F0 values for the onset and turn duration, are added, resulting in 1582 features in total. Details of the acoustic features are provided in (Eyben et al., 2009).The training and development data are used to evaluate acoustic regressors. For the baseline regressor, we used the Random Subspace (RS) algorithm (Ho, 1998), which uses multiple trees on randomly chosen subsets of the features and then combines the output from multiple trees. RS was used as the baseline in the Interspeech 2010 Paralinguistic Challenge. We also explored various algorithms for the acoustic model, but did not find any regressor outperforming that baseline. However, we notice that RS has very little error on the training data, which may suggest overfitting, therefore we used the Gaussian Process (GP) algorithm (Seeger, 2004) as a complementary model, which achieved the second best performance and has less performance difference between the training and development sets in our preliminary experiments. Ensemble of multiple models has been known to often outperform individual models. In this study, we use a weighted linear interpolation of the output from the two regression models. We divided the development set into two parts and used them to estimate the combination weights for each other.Given the amount of the training data, we suspect that the feature size might be too large. Therefore we tried to reduce the feature size using feature selection, which we expect may alleviate the potential over-fitting problem and improve the performance. We found when using Correlation-based Feature Selection (CFS) (Hall, 2000), the reduced feature set yielded better performance for the GP model, but not for RS (possibly because RS already uses subsets of features in each tree). Therefore for the acoustic model, we use the combination of GP with full feature space and RS with the reduced feature space. More details about parameters of each algorithm and analysis are described in Section 4.1.1.The interest level cannot just be determined by how the person says something (represented by acoustic features), but it is also dependent on what the person says. Schuller and Rigoll (2009) showed the spoken content also carries cues with respect to a subject's interest level. In this study, we choose to use simple lexical features for level-of-interest detection, rather than more complex features based on syntactic and semantic analysis. This is because some responses include only a few words or non-linguistic vocalizations. In this case syntactic and semantic features are less useful. Furthermore, simple lexical features are less impacted by erroneous ASR output.We first processed the training transcripts (they can be either from human transcripts or ASR output) to create the dictionary needed for the lexical features. Pauses in the middle of an utterance and non-linguistic vocalization tokens are considered to be a type of word in the dictionary. We kept the words that appear more than once, and the others are collapsed into one word (called INFREQUENT). This token will be used for unseen words in testing. The following describes the features we use for each utterance.•Bag-of-words model (BOW): We use a bag-of-words model as the base lexical features, which has been widely used for various text classification tasks (Joachims, 1998). Each utterance is represented as a vector of word features. We build the BOW vector based on the dictionary described above. The size of the BOW vector varies depending on the data used to generate the dictionary, and will be described in more details in Section 4.1.2. The weight of each wordwiis:(1)logTF(wi)=1+log(count(wi))ifcount(wi)>00otherwiseVarious term weighting methods have been studied in previous work in language processing (for example, for tasks such as text categorization and information retrieval). We performed a preliminary evaluation for our task and found using logTF achieved reasonable performance. More thorough studies for term weighting are still needed.Word level-of-interest (W-LOI) (3 features): The second set of features is regarding the LOI values of the words in the utterance. First we obtain statistics of LOI values for each word in the dictionary from the training set, including the average, minimum, and maximum LOI value. These are the averaged/min/max LOI values of all the utterances that contain this word. Based on these statistics, we then derive three W-LOI features for the utterance: average, minimum, and maximum LOI values. They are the average/minimum/maximum LOI value of all the words in an utterance. These features are in the range of [−1, +1].Length information (LEN) (3 features): These features are used to represent the sentence length and speaking rate. We hypothesize that there might be a correlation between the length of a person's utterances and whether a person is bored or interested in something. There are three features in this category: log value of the total number of words in an utterance, the estimated speaking rate, and silence length.33Note that the speaking rate and the silence length are not purely based on lexical information, but require acoustic information (duration). We include them in the lexical features since they are different from other acoustic/prosodic features whose extraction does not need word information.We decide to apply logarithm to the total number of words such that utterances with a large number of words have a smaller impact. The estimated speaking rate for each wordwiis(μwi−duration(wi))/3σwi, whereμwiandσwirefer to the mean and standard deviation of the duration of wordwi. This formula maps most of the speaking rate values into [−1, 1]. Then the speaking rate of an utterance is the sum of the speaking rate for all the words in the utterance. The word duration information is obtained from the force-aligned data for human transcription or ASR output. The silence length refers to the sum of all the silence/pause in the utterance (excluding the start or the end part of an utterance).Similar utterances (SIM) (20 features): These features leverage information from similar instances (this is similar to nearest-neighbor instance-based learning approach). They consider the LOI values of those utterances that are similar to the testing utterance in content. For an utterance, we first find 20 most similar utterances from the training data using the cosine similarity measure of the two utterances, each represented using the bag-of-words vector.44cosine(θAB)=BOWA·BOWB||BOWA||·||BOWB||.Then we use the LOI values from these 20 utterances as features.We evaluate these lexical features on the human transcription as well as the output from the ASR system described in Section 2. Even though we use a state-of-the-art ASR system, the word error rate is quite high. Incorrect words will naturally have an impact on the lexical features, affecting the BOW vectors (missing or inserting wrong words) and a word's term weight and LOI information. In addition to just using the top ASR hypothesis, we also investigate if we can leverage multiple candidates, such as n-best lists, to reduce the performance loss caused by recognition errors. We use 10-best list in this study. To extract lexical features from 10-best list, we first concatenate the 10 hypotheses as a single utterance, generate the dictionary, and then extract lexical features as described above. When an utterance has fewer than 10 hypotheses, we boost the word term frequency by multiplying by (10/#of hypotheses). This boosting only affects the term weighting in the BOW vector and the value of the total number of the words.Using the training and development data, we evaluated Support Vector Regression (SVR) and RS for the lexical model, which performed better than others such as Multilayer Perceptron, GP, or bagging approach in our preliminary experiments. The parameters of each regressor are optimized using the development data (details are in Section 4.1.2).We use a weighted linear interpolation method to combine the hypotheses from the acoustic and lexical models to make the final decision. At the development stage, the development set was divided into two folds in order to estimate the combination weights for the acoustic and lexical models. This division was also used to optimize the weighting values for the two acoustic regressors, RS and GP. The optimal weights found on one fold are applied to the other for testing. For the final system, we combine the training and development data together to train the final models since the training data is rather small. The combination weights for the final system on the test set are estimated based on the entire development data and the models learned from the training data.

@&#CONCLUSIONS@&#
In this paper, we presented a study of using a decision-level fusion of acoustic and lexical information for level-of-interest sensing. We used two different regression algorithms, RS and GP, to complement each other for the acoustic model. Although RS regressor performs the best by itself, we notice that it has a potential over-fitting problem, and thus combine it with a second regressor, GP. We also develop lexical features that are less affected by erroneous ASR output. Beyond the BOW feature, we include estimated LOI values for the words, length-related features, and the LOI vector of most similar utterances. For ASR conditions, we propose to use n-best hypotheses. Incorporating these features yielded significantly better performance than our previous approach on human transcripts, but only slightly better on 1-best ASR output. When n-best hypotheses are used to extract the lexical features, significantly better performance gain is achieved. We demonstrate that a decision-level fusion from the acoustic and lexical models outperforms using one information source, even for the ASR conditions with high word error rate.Although we tried to develop robust lexical features for erroneous ASR output, the contribution of the lexical model on ASR output is less than that using human transcripts because of the word errors. In the future, we will investigate other lexical features. Another important work is to detect non-linguistic vocalization tokens as explained in Section 4.3.2. Furthermore, for ASR conditions, we plan to incorporate recognition confidence information in the lexical model.