@&#MAIN-TITLE@&#
Practical approach to determine sample size for building logistic prediction models using high-throughput data

@&#HIGHLIGHTS@&#
New method to determine sample size for building logistic prediction model.We performed simulations to examine representative null distribution concept.Two real data sets were examined to compare full permutation method.Drastically improvement when compared to the CPU time required for full permutations.

@&#KEYPHRASES@&#
Sample size,Statistical power,Prediction and validation,Permutation,Null distribution,

@&#ABSTRACT@&#
An empirical method of sample size determination for building prediction models was proposed recently. Permutation method which is used in this procedure is a commonly used method to address the problem of overfitting during cross-validation while evaluating the performance of prediction models constructed from microarray data. But major drawback of such methods which include bootstrapping and full permutations is prohibitively high cost of computation required for calculating the sample size.In this paper, we propose that a single representative null distribution can be used instead of a full permutation by using both simulated and real data sets. During simulation, we have used a dataset with zero effect size and confirmed that the empirical type I error approaches to 0.05. Hence this method can be confidently applied to reduce overfitting problem during cross-validation. We have observed that pilot data set generated by random sampling from real data could be successfully used for sample size determination. We present our results using an experiment that was repeated for 300 times while producing results comparable to that of full permutation method. Since we eliminate full permutation, sample size estimation time is not a function of pilot data size. In our experiment we have observed that this process takes around 30min.With the increasing number of clinical studies, developing efficient sample size determination methods for building prediction models is critical. But empirical methods using bootstrap and permutation usually involve high computing costs. In this study, we propose a method that can reduce required computing time drastically by using representative null distribution of permutations. We use data from pilot experiments to apply this method for designing clinical studies efficiently for high throughput data.

@&#INTRODUCTION@&#
One of the main reasons to utilize high dimensional data from microarrays in clinical research is to develop statistical models that predict clinical outcomes such as, time to recurrence, progression of disease and survival of patients. Finding high quality samples is costly and difficult but it constitutes a key task in performing clinical studies. The task of finding minimum number of samples for scientific study is very important to minimize the wastage of valuable resources and retain clinical utility of the experiment. Determination of sample size based on sound technical basis is a significant part of guidelines set by Institutional Review Board (IRB).Several important methodologies were proposed to determine sample size for microarray experiments. Liu and Hwang report a formula suitable for comparison studies with multiple independent samples [1]. Methods which introduce the concept of controlling False Discovery Rate (FDR) in microarray analysis were further developed to estimate power and sample size [2–5]. These methods are aimed at discovering statistically valid biomarkers. However, because of the inherent complexities in the genetic make-up of diseases such as cancer, diabetes and other immune diseases these methods suffer from less-than-desirable accuracy for medical practice. This implies that one has to consider the necessity of multiple parameters in the prediction models, as well as the variations from experimental platform. Recent FDA clearance of Affymetrix™ system as a diagnostic platform presents an example of rapid upgrade in reliability of such platforms for use in clinical settings [6]. Statistical prediction models such as one used in OncotypeDX™ which employs multiple biomarkers validate the value of such predictive models. These models have formed a trend in many clinical trials in combination with co-diagnosis approach [7,8].Recently, Pang and Jung provided an idea to rigorously determine the sample size required to construct such a predictive model [9]. It estimates empirical power of a suggested sample size using simulated data from bootstrapping based on a predictive model developed using pilot project data. A proof was given by Jung and Young [10] that demonstrates the structure of covariance from pilot data and bootstrapped data from the pilot data are approximately identical. They also suggest a method to estimate empirical power when the response variable is of survival type. Since this method constructs individual prediction models from numerous simulated data sets and performs cross-validation and permutation each time, it reduces the problem of over-fitting while adding expensive computation time for repeated calculations.The concept of prediction–validation method is the first of its kind to determine the sample size of multi-dimensional data [9]. However, it remains a concern that it requires lot of time to determine proper sample size of a data set with many variables. This approach would be more practical if the computation complexity could be reduced. We were inspired by an observation that a set of simulated data sets from pilot data seem to generate similar non-centrality parameters when each set was estimated by maximum likelihood method. Thus, it seemed reasonable to assume that a carefully selected single null distribution could be re-used in other sets for adjustingp-values. Our current study provides a method to determine sample size for the case of binary response variables using this idea. We demonstrate empirical evidence by extensive simulation which supports the fact that sample size can be conveniently approximated.

@&#CONCLUSIONS@&#
