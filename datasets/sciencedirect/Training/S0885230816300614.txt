@&#MAIN-TITLE@&#
Information theoretic optimal vocal tract region selection from real time magnetic resonance images for broad phonetic class recognition

@&#HIGHLIGHTS@&#
Information theoretic optimal regions selection from rtMRI images.Forward region splitting algorithm for maximizing mutual information.Articulatory features from the optimal set of regions.Benefit of proposed features for broad phonetic class recognition.

@&#KEYPHRASES@&#
Mutual information,Phonetic recognition,Speech production,Region splitting,

@&#ABSTRACT@&#
We propose an information theoretic region selection algorithm from the real time magnetic resonance imaging (rtMRI) video frames for a broad phonetic class recognition task. Representations derived from these optimal regions are used as the articulatory features for recognition. A set of connected and arbitrary shaped regions are selected such that the articulatory features computed from such regions provide maximal information about the broad phonetic classes. We also propose a tree-structured greedy region splitting algorithm to further segment these regions so that articulatory features from these split regions enhance the information about the phonetic classes. We find that some of the proposed articulatory features correlate well with the articulatory gestures from the Articulatory Phonology theory of speech production. Broad phonetic class recognition experiment using four rtMRI subjects reveals that the recognition accuracy with optimal split regions is, on average, higher than that using only acoustic features. Combining acoustic and articulatory features further reduces the error-rate by ∼8.25% (relative).

@&#INTRODUCTION@&#
The speech signal is encoded with both linguistic and para-linguistic information including speaker's characteristics, background noise condition, and emotional state of the speaker. Seeking representations from the speech signal has been one of the main challenges in speech research (Greenberg and Kingsbury, 1997; Paliwal, 1998). The nature of a representation could change depending on the type of task at hand. For example, a representation for speech recognition is expected to be invariant to recording environment, and speaker's characteristics (Greenberg and Kingsbury, 1997). On the other hand, for speaker recognition, a representation should mostly capture speaker specific information (Perez-Meana, 2007). Representations robust to noise and other channel distortions are also critical for reliable performance of a speech based system in different working conditions.Speech is a time-varying signal with complex spectro-temporal characteristics. The temporal and spectral resolutions necessary to analyze different speech sounds often vary over a short period of time. Thus, most of the representations in the literature have been designed assuming quasi-stationarity of the speech signal within a short-time window. A number of representative features are available exploiting spectral-only, temporal-only, and joint spectro-temporal properties. Spectral features include Filter Bank Analysis (Potter et al., 1947; Nadeu et al., 2001), Mel Cepstrum Analysis (Davis and Mermelstein, 1980; Olli and Kari, 1998; Han et al., 2006), Linear Predictive Coding (LPC) (Itakura, 1975), Perceptually Based Linear Predictive Analysis (PLP) (Hermansky, 1990), and spectral subband centroids (Paliwal, 1998). Temporal features such as temporal envelope cues (Shannon et al., 1995), short-time energy and fundamental frequency are used as features for speech recognition. Benefits of joint spectro-temporal features for speech recognition have also been demonstrated (Kleinschmidt, 2003; Xavier et al., 2008).In spite of several proposed speech signal-based features, obtaining a robust representation still remains a challenge. This has led researchers to look into obtaining features from other modalities that capture the speech production process directly. For example, Krishnamurthy and Childers (1986) have demonstrated the benefit of using representation from electroglottography (EGG) for classification of speech according to different voicing qualities. While EGG captures the glottal vibrations of the speaker, other modalities such as X-ray microbeam (Fujimura et al., 1973) and electromagnetic articulography (EMA) (Schönle et al., 1987) capture the movement of critical points on speech articulators in the vocal tract by placing sensors on the articulators. Representations derived from these sensor data in addition to the speech acoustic have been shown to be useful for speech recognition (Sun et al., 2000; Wrench and Richmond, 2000; Frankel and King, 2001; Ghosh and Narayanan, 2011; Markov et al., 2006). Unlike sensor tracking in EMA, ultrasound captures a complete spatial view of the tongue (Hueber et al., 2007b; Stone and Davis, 1995); representations derived from the ultrasound images have been shown to be effective for the continuous-speech recognition as well as speech synthesis (Hueber et al., 2007a; Denby and Stone, 2004). Features from electromyography (EMG) as well as electropalatography (EPG) have also been shown to be effective for speech recognition (Jou et al., 2006; Manabe et al., 2003; Jorgensen et al., 2003; Schultz and Wand, 2010; Soquet et al., 1999); these signals capture the muscle movements in the face while speaking. Representations from EMA and EPG data together have been shown to improve the recognition accuracy when used jointly with spectral features (Wrench and Richmond, 2000).Unlike ultrasound, in real time magnetic resonance imaging (rtMRI) the complete upper airway of a subject is imaged including the subject's nose and upper palate in addition to the vocal tract region starting from the lips to the glottis (Narayanan et al., 2011). The rtMRI video frames also have regions that are outside the subject's face in the midsagittal plane. Thus, the rtMRI video directly captures time varying dynamics of the changes in the vocal tract shape. Since the vocal tract shapes cause the production of different sounds, a representation from such vocal tract images would be robust to noise present in the speech signal. Representation from the rtMRI images could also be complementary to the spectro-temporal features derived from the speech signal since the relation between the vocal tract dynamics and speech acoustics is highly non-linear (Deng, 2006).rtMRI captures the air-tissue boundaries along the entire vocal tract region from the glottis to the lips. A commonly used protocol for recording rtMRI video captures MR images at a frame rate of 23.18frames/s with a resolution of 68×68pixels with simultaneous audio recorded at 20kHz. Hence the rtMRI has ∼106,352 dimensional data captured per second, which is more than that captured in audio (sampling rate of 20kHz). Thus, the entire image may not be an efficient representation of the corresponding sound due to its large dimensionality. The key information related to the sound may lie only in a few pixels or regions in the image. In this work, we address the problem of automatically obtaining optimal regions from the rtMRI images such that the articulatory features derived from these optimal regions best represent the respective broad phonetic classes. Features derived from the optimal regions in the rtMRI images could potentially remove the redundancy in representing various sounds.

@&#CONCLUSIONS@&#
In this work, we have proposed an optimal region selection algorithm for broad phonetic class recognition using MI as the criterion. The proposed optimal regions are used to compute a set of articulatory features from rtMRI video frames. As shown in Fig. 10, it is found that on average the broad phonetic class recognition accuracy obtained using the proposed features is higher than that using acoustic features computed from the rtMRI audio. Combining these two features improves the recognition accuracy over those using the individual ones. Thus, the proposed region selection based features are not only equivalent but also complementary to the rtMRI acoustic features. In addition to the recognition benefit, some of the articulatory features using the optimal regions are found to correlate well with the articulatory gestures typically used to describe the constriction actions in the Articulatory Phonology theory (Browman and Goldstein, 1990) of speech production.Although the proposed optimal region selection algorithm does not use any a priori information about the location of subject's articulators in the rtMRI image, it is interesting to find that the optimal regions fall on different articulators that are actively used for speech production such as lips, jaw, tongue, velum, and glottis. From Fig. 6, it is interesting to find that for one female subject (F2) from the rtMRI corpus, the optimal regions fall on the laryngeal prominence indicating the movement of the Adam's apple while speaking. The proposed region selection can also be used to automatically identify parts of different speech articulators in the rtMRI video. While there are several pixels in rtMRI images which correspond to different speech articulators, less than ∼6% of the total number of pixels in the image were found to be part of the optimal regions. This suggests that the phonetic representations are localized primarily few pixels of 68×68 rtMRI image corresponding to the regions of critical articulators.It would be interesting to compare the proposed optimal region based articulatory features with those from other modalities such as EMA. EMA directly measures the dynamics of critical articulators, but rtMRI provides a low-resolution image of the entire vocal tract in the midsagittal plane. Comparing representations from these modalities would reflect their relative potential for designing features for speech applications. It would also be interesting to investigate other techniques such as active shape model (Luettin et al., 1996) and broad statistical pixel measure (Raeesy et al., 2013) for selecting optimal regions in the rtMRI image.Algorithm 1Optimal split of ρd(OPTIMALSPLITREGION).1:Inputs:ρ, d, and broad phonetic labels X of multiple rtMRI frames2:ρT=∅, ρES={ρd}, and ρT,c=ρ∖{ρd}3:I*=IQFρES,ρT,ρT,c,X4:(ρS, IT*)=SPLITONELEVELρT,c,ρT,ρES,I*5:ifIT*=I*then6:returnρS7:end if8:whileIT*>I*do9:I*=IT*10:ρ1S←first element (region) of ρS11:ρ2S←second element (region) of ρS12:(ρS,1,I1T*)=SPLITONELEVELρT,c,ρ2S,ρT,ρ1S,IT*13:(ρS,2,I2T*)=SPLITONELEVELρT,c,ρ1S,ρT,ρ2S,IT*14:ifI1T*≥I2T*then15:IT*=I1T*16:ρS=ρS,117:ρT=ρT⋃ρ2S18:else19:IT*=I2T*20:ρS=ρS,221:ρT=ρT⋃ρ1S22:end if23:end while24:ρT=ρT⋃ρ1S25:returnρTand IT*Algorithm 2Optimal split for one time (SPLITONELEVEL).1:Inputs:ρ′, ρ″, I2:imin←min{i|(i, j)∈ρ″}3:imax←max{i|(i, j)∈ρ″}4:jmin←min{j|(i, j)∈ρ″}5:jmax←max{j|(i, j)∈ρ″}6:Imax=I;ρbest″={ρ″}7:fork=imin+1toimax−1do8:ρ1″={(i,j)|(i,j)∈ρ″andi≤k}9:ρ2″={(i,j)|(i,j)∈ρ″andi>k}10:It=IQFρ′,ρ1″,ρ2″,X11:ifIt>Imaxthen12:Imax=It13:ρbest″={ρ1″,ρ2″}14:end if15:end for16:fork=jmin+1tojmax−1do17:ρ1″={(i,j)|(i,j)∈ρ″andj≤k}18:ρ2″={(i,j)|(i,j)∈ρ″andj>k}19:It=IQFρ′,ρ1″,ρ2″,X20:ifIt>Imaxthen21:Imax=It22:ρbest″={ρ1″,ρ2″}23:end if24:end for25:returnρbest″and ImaxAlgorithm 3Optimal number of splits of all region in ρ.1:Inputs:ρ and broad phonetic labels X of multiple rtMRI frames2:ρsplt=∅3:fori=1 to D★do4:Imax=0;ρS★=∅;5:forj=1 to |ρ|do6:(ρS, I) = OPTIMALSPLITREGION({ρ, ρsplt}, 1, X)7:ifI>Imaxthen8:Imax=I9:ρS★=ρS10:end if11:end for12:ρsplt=ρsplt⋃ρS★13:ρ=ρ∖ρ114:end for15:returnρspltand Imax