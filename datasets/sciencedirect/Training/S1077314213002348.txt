@&#MAIN-TITLE@&#
A texton-based kernel density estimation approach for background modeling under extreme conditions

@&#HIGHLIGHTS@&#
Background and foreground modeling method able to run seamlessly under extreme conditions.Modeling structural variations of pixels’ neighbors via joint domain-range approach integrating textons into the model.Exhaustive testing of state-of-the-art approaches on real-life scenarios.

@&#KEYPHRASES@&#
Background and foreground modeling,Non-parametric kernel density estimation,Texture analysis,Video analysis,

@&#ABSTRACT@&#
Background modeling is a well-know approach to detect moving objects in video sequences. In recent years, background modeling methods that adopt spatial and texture information have been developed for dealing with complex scenarios. However, none of the investigated approaches have been tested under extreme conditions, such as the underwater domain, on which effects compromising the video quality affect negatively the performance of the background modeling process. In order to overcome such difficulties, more significant features and more robust methods must be found. In this paper, we present a kernel density estimation method which models background and foreground by exploiting textons to describe textures within small and low contrasted regions. Comparison with other texture descriptors, namely, local binary pattern (LBP) and scale invariant local ternary pattern (SILTP) shown improved performance. Besides, quantitative and qualitative performance evaluation carried out on three standard datasets showing very complex conditions revealed that our method outperformed state-of-the-art methods that use different features and modeling techniques and, most importantly, it is able to generalize over different scenarios and targets.

@&#INTRODUCTION@&#
Detecting moving objects in videos is a fundamental task for many machine vision systems such as object tracking, behavior understanding, and event detection. One of the most common approach to identify moving object is resorting to background modeling approaches, which aim at building an estimated image of the scene without objects of interest; this model is then compared to each new video frame for identifying foreground objects. The most popular approaches to achieve this goal are the density-based ones, where the distribution of each background pixel is modeled by either a probability density function (e.g. Gaussian) [1] or non-parametric kernel density estimation [2]. In early years, these approaches exploited only historical variations of pixel colors; recently, we have witnessed a trend towards background modeling methods that, instead, employ spatial and texture information, i.e. methods that build the background scene by taking into account textures computed on neighboring pixels. In addition, approaches that estimate the background and the foreground separately have been favored to the ones relying only on the background model, since the latter do not account for the spatial and temporal changes that may happen in the foreground. For example, let us consider the case of a dark red object moving from a blue background to a light red one; if only the background is modeled, when the object enters the red zone it could be missed, depending on the chosen difference threshold. Instead, this could be avoided if the foreground is also modeled, due to the closer resemblance of the object to the foreground model than to the background one. This example also proves the necessity of resorting to textures, because color similarities may not suffice to discern accurately background from foreground.While excluding foreground modeling and spatial/texture information might work and make the algorithms run effectively in simple scenarios, the tides turn against them when more complex scenes need to be modeled, as in the case of underwater video surveillance [3] (see the Fish4Knowledge project.1http://www.fish4knowledge.eu1) The underwater case, in particular, is a rather complex scenario, as it may show a combination of features at the same time that affects the performance of the majority of the existing background modeling approaches. For instance, dynamic or multi-modal backgrounds, abrupt lighting changes, and radical and instant water turbidity changes can all be found in the same scene. To complicate even more the situation, the underwater environment shows two almost exclusive characteristics with respect to other domains: three degrees of freedom and erratic movements of objects (i.e. fish). This makes fish less predictable than people or vehicles, as fish may move in all three directions changing their size and their shape in the video. Moreover, because of light propagation in water, fish may also change rapidly appearance. Background modeling approaches should be able to operate effectively also in these cases without the need to re-design them according to the features of the domain at hand.The main scientific contributions of this paper are (1) to provide a robust and general background and foreground modeling method able to run seamlessly under extreme conditions (with different targets), (2) to demonstrate how textons outperform other texture descriptors in the background modeling process, and (3) to carry out, for the first time, an exhaustive testing of state-of-the-art approaches on different real-life scenarios.The remainder of the paper is organized as follows: Sections 2 and 3, discuss, respectively, the current state of the art in background modeling, highlighting current trends and limitations, and how we address the current challenges in the research field. Section 4 shows a performance evaluation of the proposed method, compared to state-of-the-art methods, in three different real-life scenarios dealing, respectively, with people, fish and vehicles. Finally, in the last section, some conclusions are drawn.

@&#CONCLUSIONS@&#
In this paper we have introduced a joint domain-range kernel estimation approach which builds the background and foreground models by integrating color and texture features. We have, in particular, demonstrated the utility of textures in the background modeling process and how the methods relying on textons and/or local binary patterns perform the best since they are able to extract significant texture also from small regions (e.g. in5×5pixel areas). Moreover, we have pointed out that, although dependent on the lighting conditions, the RGB color space is more suitable to model background color variations than the HSV or the Lab spaces, as opposed to what was stated in [28].The application of our approach on different domains and the comparison with state-of-the-art methods confirmed that background modeling methods which allow spatial influence from neighboring pixels and employ textures robust to illumination changes perform the best over different domains.As future work, we are focusing mainly to reduce the processing times (as at the method cannot be used for real-time purposes) by exploiting hashing techniques and GPU programming for speeding up the feature extraction and the model update processes.