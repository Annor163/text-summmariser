@&#MAIN-TITLE@&#
TCBR-HMM: An HMM-based text classifier with a CBR system

@&#HIGHLIGHTS@&#
The paper presents an innovative solution to model distributed adaptive systems in biomedical environments.A Case Based Reasoning system with an original Hidden Markov Model for biomedical text classification is proposed.The model classifies scientific documents by their content, taking into account the relevance of words.The model is able to adapt to new documents in an iterative learning frame.The model is tested with the SVM and k-NN classifiers using the Ohsumed scientific collection.Empirical and statistical results show the method outperforms other efficient text classifiers.

@&#KEYPHRASES@&#
Hidden Markov Model,Case Based Reasoning system,Text classification,

@&#ABSTRACT@&#
This paper presents an innovative solution to model distributed adaptive systems in biomedical environments. We present an original TCBR-HMM (Text Case Based Reasoning-Hidden Markov Model) for biomedical text classification based on document content. The main goal is to propose a more effective classifier than current methods in this environment where the model needs to be adapted to new documents in an iterative learning frame. To demonstrate its achievement, we include a set of experiments, which have been performed on OSHUMED corpus. Our classifier is compared with Naive Bayes and SVM techniques, commonly used in text classification tasks. The results suggest that the TCBR-HMM Model is indeed more suitable for document classification. The model is empirically and statistically comparable to the SVM classifier and outperforms it in terms of time efficiency.

@&#INTRODUCTION@&#
Currently there is a lot of information related to biomedicine, usually stored in public resources commonly used by the healthcare community. For example, Medline, the most requested biomedical bibliographic database, stores references to magazine articles from 1950 to the present, containing over 20 million citations.The high volume of items makes it impossible for any expert to handle it manually. Therefore there is great interest to automate the classification process and produce relevant documents to a topic. A large number of techniques have been developed for text classification, including Naive Bayes, k-Nearest Neighbours (kNN), Neural Networks, and Support Vector Machines (SVMs). Among them, SVM has been recognized as one of the most powerful and promising machine learning methods [1–5].In other studies, Hidden Markov Models (HMM) have been used to describe the statistical properties of a sequential random process. They are known for their application in language problems like speech recognition [6]. However, their application has been extended to fields of text processing such as information extraction [7–9], information retrieval [10], text categorization [11–13], and text classification [14,15].One of the common tasks that can be addressed by deploying an HMM in text analysis is to infer the internal structure of the documents. This task is related to the decoding problem for HMMs (see Section 2.2). For instance, Frasconi et al. [11] use HMM to categorize pages in a multi-page document, exploiting the internal organization of the document to provide the learner with better information. The HMM is trained with page sequences to subsequently find the most probable structure for a new given document.Barros et al. [7] propose a hybrid machine learning approach to Information Extraction by combining conventional text classification techniques and Hidden Markov Models. The machine learning algorithm generates an initial classification of the input fragments that is refined by an HMM. Experiments performed on two different case studies show that the use of an HMM compensated the low performance of less adequate classifiers and feature sets chosen to implement the text classifier.In [16], Zhou and He propose an information extraction system based on the hidden vector state (HVS) model, for biomedical events extraction, and investigate its capability in extracting complex events. The results suggest that the HVS model with the hierarchical hidden state structure is indeed more suitable for complex event extraction, since it could naturally model structural context embedded in sentences.Another text processing task that is solved with the use of HMMs is to perform text sequence classification, in which the HMM is commonly treated as a word generator and receives as input a sequence of units belonging to a document. This task is related to the evaluation problem for HMMs (see Section 2.2). For example, Kairong Li et al. [12] research the text categorization process based on Hidden Markov Model. The main idea of the article lies in setting up an HMM classifier, combining χ2 and an improved TF-IDF (term frequency-inverse document frequency) method, and reflecting the semantic relationship in the different categories. The process shows the semantic character in different documents to make the text categorization process more stable and accurate.Miller et al. [10] use HMM in an information retrieval model. Given a document set and a query Q, the system searches a document D relevant to the query Q. It computes the probability that D is the relevant document in the user's mind, i.e. P(D is R|Q), and ranks the documents based on this measure. The proposed model generates a user query with a discrete Hidden Markov process, depending on the document the user has in mind. The HMM is viewed as a generator of the query, and is used to estimate the probability that each document is produced in the corpus.Kwan Yi et al. [14] use the previous idea in a similar approach. They describe the text classification as the process of finding a relevant category c for a given document d. They implement a Hidden Markov Model to represent each category. Thus, given a document d, the probability that a document d belongs to category c is computed on the specific HMM model c. In their system, a document is treated as a wordlist, and the HMM for each category is viewed as a generator of a word sequence. This last idea, which is also used in our study, also appears in probabilistic topic models [17].Probabilistic topic models, like Latent Dirichlet Allocation (LDA) [18], are generative models that specify a probabilistic process for generating documents. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. However, the focus of our paper is to categorize a document into a certain binary class (relevant or non-relevant) rather than infer the topic distribution in the entire corpus. In this case, topics cannot be assigned to classes in a direct manner and the goal of the used classifier is to capture the semantic information in a class. This is why an HMM is created for each category.In general, HMMs cannot be used directly in a text classification process. Previous HMM related techniques provide a way of applying these models to a text processing task; however, they are specifically suited for information retrieval problems [10] or need to use prior knowledge to achieve significant results [14].In order to obtain a simpler and more effective classifier system than existing methods, we propose a model based on the Hidden Markov Model, called T-HMM. This model aims to classify documents according to their content. The classifier is focused on distinguishing relevant and non-relevant documents from a dataset, and deals with the problem, common among search systems, of whether a document is relevant or not given the type of user query. T-HMM offers a simple and parametrizable structure, based on word relevance, which allows the model to be adjusted to future documents. Experimental results show that the application of this model appears to be promising. Our model outperforms commonly used text classification techniques like Naive Bayes, and achieves comparable results to the SVM approach, which is recognized as one of the most effective text classification methods presently used. Moreover, T-HMM is clearly more efficient in terms of running time than the other two approaches.In addition, in order to extend the usefulness of the T-HMM model, this article proposes an improved version called TCBR-HMM, based on a Case Based Reasoning (CBR) system. The purpose of this hybrid model is to provide a solution to another kind of text classification problem where a previously created model needs to be adapted to each new document in an iterative learning frame.The CBR system [19–23] solves new problems by recalling and adapting previous solutions. Consequently, CBR is useful for tasks using predicates that are poorly defined, lead to inconsistent outcomes, and have incomplete rules to apply [24,23]. CBR is a knowledge based problem solving technique based on the assumption that similar problems can possess similar solutions. As such, it is perfectly suited for solving the problem at hand. The TCBR-HMM is evaluated and compared with the results obtained by Naive Bayes, SVM and the basic T-HMM approach.The remainder of this paper is organized as follows: Section 2 presents the CBR and the Hidden Markov models concepts. Section 3 explains the process of adapting the HMM model to the text classification process that results in T-HMM; Section 4 details the proposed TCBR-HMM solution; Section 5 discusses the experiments and results obtained in the case study; Section 6 shows the conclusions and future directions.The purpose of case-based reasoning systems is to solve new problems by adapting solutions that were used to solve similar problems in the past. The fundamental concept when working with case-based reasoning is the concept of case. A case can be defined as a past experience, and is composed of two elements: A problem description, and the solution. The case memory holds a number of problems with their corresponding solutions. Once a new problem arises, the solution is obtained by retrieving similar cases from the case memory and studying the similarity between them. A CBR system is a dynamic system in which new problems are added to the case memory, redundant are eliminated, and others are created by combining existing ones.The way in which cases are managed is known as the case-based reasoning cycle. A typical CBR cycle is composed of four sequential phases (Fig. 1) that are recalled every time a problem needs to be solved [21,25,26]:1.Retrieve the most relevant case(s)Reuse the case(s) to attempt to solve the problemRevise the proposed solution if necessaryRetain the new solution as a part of a new caseThe Retrieve phase starts when a new problem description is received. Similarity algorithms are applied in order to retrieve from the case memory the cases with a problem description most similar to the current one. Once the most similar cases have been retrieved, the Reuse phase begins. In this phase the solutions of the retrieved cases are adapted to obtain the best solution for the current case. The Revise phase consists of an expert or automatic procedure revision of the proposed solution. Finally, the Retain phase allows the system to learn from the experiences obtained in the three previous phases, and then updates the case memory. Each of these steps of the CBR life cycle requires a model or method in order to perform its mission.Case-based reasoning can be used by itself, or as part of another conventional or intelligent system [27]. Although there are many successful applications based on CBR methods alone, CBR systems can be improved by combining them with other technologies [28–30]. Their suitability for integration with other technologies, creating a global hybrid reasoning system, stems from the fact that CBR systems are very flexible [31]. Therefore, they are capable of absorbing the beneficial properties of other technologies, such as HMM.A Hidden Markov Model is a statistical tool used to model generative sequences that can be characterized by an underlying hidden process [32]. It can be seen as a state diagram that consists of a set of states and transitions between them, where each state can also emit an output observation with a certain probability. Thus, two processes take place when generating sequences in an HMM. The first process describes the unobservable state sequence, i.e. the hidden layer represented by the state transition probability. The second process links each state to observations creating the observable layer represented by the output observation probability [12].The formal definition of an HMM is as follows:λ={N,V,A,B,π}1.N is the number of states in the HMM model. The state set is denoted by:S={s0,s1,…,sN}V is the set of possible observationsV={v0,v1,…,vM}, where M is the number of observations.We define Q as a fixed state sequence of length T, and its corresponding observation sequence as O:Q={q0,q1,…,qT},O={o0,o1,…,oT}A is the transition probability matrix of dimension N×N. It stores the probability of state j following state i in the aijcell:aij=P(qt=sj|qt−1=si)B is the observation output probability matrix of dimension N×M. We define bi(k) as the probability of observation k being produced at state i, which is independent (the probability) of time instant t.bi(k)=P(ot=vk|qt=si)π is the initial state probability array.π=(π0,π1,…,πN)πi=P(q0=si)Text classification is the task of automatically assigning a document set to a predefined set of classes or topics [33].In our context, given a training set T={(d1, c1), (d2, c2), …, (dn, cn)}, which consists of a set of preclassified documents in categories, we want to build a classifier using HMM to model the implicit relation between the characteristics of the document and its class, in order to be able to accurately classify new unknown documents.Each document dihas a binary class attribute ciwhich can have a value of Relevant or Non-relevant. Our work is therefore focused on building a classifier based on the training set that can classify new documents as relevant or non-relevant without previously knowing their class information.Following the idea proposed by Kwan Yi et al. [14], we use HMM as a document generator. Fig. 2shows the proposed framework. According to the structure, an HMM is implemented for each category: Relevant and Non-Relevant. Each model is then trained with documents belonging to the class that it represents. When a new document needs to be classified, the system evaluates the probability of this document being generated by each of the Hidden Markov models. As a result, the class with the maximum probability value is selected and considered as the output class for the document.The proposed model (T-HMM) aims to classify documents according to their content. To achieve that, input data need to be expressed in a format that HMM algorithms can handle.The most common approach in document classification tasks is the bag-of-words approach [34]. In this case, every document is represented by a vector where elements describe the word frequency (number of occurrences), as shown in Fig. 3(a). Words with a higher number of occurrences are more relevant because they are considered the best representation of the document semantic. For training purposes, words are placed in descending order according to their ranking to represent each document (see Fig. 3(b)).The complexity of text classification in terms of time and space depends on the size of these vectors. In order to reduce their dimension, a text preprocessing step is required, where rare words and those which do not provide any useful information (such as prepositions, determiners or conjunctions) are removed. This step is further explained in Section 5, where some adjustments such as TF-IDF are also made in the word frequency value. The final selected words to represent the documents are called feature words.When building an HMM model, it is important to reflect what “hidden states” signify. In Kwan Yi et al. [14], states are designed to represent different sources of information (e.g. Title, abstract or MeSH); other related works use hidden states to represent internal sections of the document [11,7]. However, as stated before, T-HMM is focused on classifying documents by their content rather than their structure. In addition, having multiple sources of information is difficult and occasionally there is only one part of the document available, e.g. when classifying according only to the abstract, which is the case of the experiments presented in this paper. Consequently, T-HMM aims to model a deeper semantic analysis in its states regardless of where the text comes from.Based on the assumption that words do not have the same importance, hidden states in T-HMM reflect the difference in relevance (ranking) among words within a document. Each state represents a relevance level for words appearing in the corpus. That is, the most probable observations for the first state are the most relevant words in the corpus. The most probable observations for the second state are the words holding the second level of relevance in the corpus, and so on. The number of states N is a modifiable parameter that depends on the training corpus and how much flexibility we want to add to the model.Considering that each document is ultimately represented by a vector or a wordlist ranked in decreasing order, and ignoring words with zero value, a Hidden Markov model is used to represent a predefined category c as follows:1.The union of words from the training corpus is taken as the set of observation symbols V. For each word, there is a symbolvk. The set of possible observations is the same for every HMM, taking into account all words in the corpus, regardless of their category.As mentioned above, states represent ranking positions. Therefore, states are ordered from the first rank to the last one. The state transitions are ordered sequentially in the same way, forming a linear HMM [6] without self-state loops, in which the probability of state Si+1 behind state Siis 1. The transition probability matrix A is then defined as:aij=1ifj=i+10otherwiseThe observation output probability distribution of each state is defined according to the training corpus and category c. A word/observationvkwill have a higher output probability at a given state siif the word appears frequently with the same ranking position that sirepresents. In addition, all states, regardless of the rank they represent, will also have a probability of emitting words appearing in documents with c category that HMM was built for. The weight (importance) of these two separate probabilities is controlled by a f-parameter.Given a category c and a dataset Dcof documents that belong to that category, the output probability matrix B for an HMM that represents category c is defined as shown in Eq. (1):(1)bi(vk)=f·∑d∈DcRd(vk,i)∑d∈DcEd(i)+(1−f)·∑d∈DcAd(vk)∑j=0|V|∑d∈DcAd(vj)where(i)bi(vk)stands for the probability of the word/observationvkbeing emitted at state sif∈[0, 1]Rd(vk,i)=1if wordvkappears atith rank position in document;d0otherwiseEd(i)=1if there is any word withith rank position in documentd0otherwiseThis factor is necessary because documents have a different number of feature words. If the number of states is too high, some documents may not have enough feature words to complete all position ranks.Ad(vk)=1if wordvkappears at least one time in documentd0otherwise|V| is the number of feature words.The initial probability distribution π is defined by giving probability 1 to the first state s0.Fig. 3 shows the training process of an HMM with three states for category R (Relevant documents). Once the dataset matrix has been obtained, in which every document is represented by a vector with adjusted word frequency values (a), relevant documents are then selected and formatted into a list of distinct words ordered according to their ranking (b). The HMM is then created and a probability distribution matrix is assigned to each state following the previous mentioned equation (c).In this example, it is important to note that, as the HMM has only three states, the 4th rank of words is ignored. Therefore, the number of states n is a cut-off parameter. It should be high enough to represent multiple relevance levels without overfitting the model to the training data. A good starting point is to consider n the average number of words with non-zero value in a document.The first part of Eq. (1) represents the relevance of the ranking order. The more weight this part has, the more restrictive the model is when classifying a new document, as it takes into account the exact order of words in relevance from the document training set. Although it can increase the precision of the categorization process, this can lead to an overfit if the f-value is too high.The second part maintains the same value for all states and provides the model with a better generalization to classify documents. This is why the number of states and the f variable must be adapted to the corpus.Finally, taking all the different possibilities for word ranks into account, if a specific document has the same relevance for two or more feature words (which is very improbable after applying TFIDF), then these words are considered to appear in every rank they belong to.Once the two Hidden Markov models are created and trained (one for each category), a new document d can be classified by, first of all, formatting it into an ordered wordlist Ldin the same way as in the training process. Then, as words are considered observations in T-HMM, we calculate the probability (likelihood) of the word sequence Ldbeing produced by the two HMMs. That is, P(Ld|λR) and P(Ld|λN) need to be computed, where λRis the model for relevant documents and λNthe model for non-relevant documents. The final output class for document d will be the class represented by the HMM with the highest calculated probability.The calculation of the likelihood measures is made by applying the forward-backward algorithm explained in the Rabiner article [6]. In this case, since there are no loops in the proposed model and the state transition is fixed, the likelihood P(Ld|λh) for the HMM λhwith n states is calculated as shown in Eq. (2):(2)P(Ld|λh)=∏i=1min(|Ld|−1,n−1)bi(Ldi)In order to extend the usefulness of the previous T-HMM model, this section presents an improved version called TCBR-HMM, based on a Case Based Reasoning system. When a CBR system is used in text classification, the basic idea is to integrate a classifier model into the Reuse phase, where decisions and classifying processes take place. Fig. 4shows how this general approach works. The selected model (which can be any classifier compatible with the corpus data) has to be trained with the similar cases each time a new document is classified. In the Retain phase, the new document is added to the case memory so that it can be retrieved if a new similar document arrives.The TCBR-HMM system proposed in this paper is significantly different from a typical approach (see Fig. 5). In this case, the T-HMM classifier is considered the knowledge base. The initial case memory of the CBR is used as an initial corpus to train the T-HMM classifier in order to represent the cases. Once the classifier is trained, obtaining an HMM for each class (Relevant and non-relevant) the CBR cycle can begin. When a new document d arrives, the four CBR sequential phases are recalled:1.In the Retrieve phase the T-HMM classifier, which represents the entire case memory, is retrieved to solve the new problem.The Reuse phase uses the T-HMM classifier to classify the new document, giving the output class of the classifier as the solution proposed by the system. It is important to note that the classifier does not need to be trained in this phase (unlike the typical approach), which reduces the execution time.In the Revise phase, a human expert or any automatic procedure revises the solution to obtain the real category/class c.The Retain phase directly modifies the HMM that represents c adapting the T-HMM classifier to d. Consequently, whenever a new document arrives, d has already been incorporated into the knowledge base (T-HMM classifier).In addition, in the Retain phase a decision must be made, depending on the corpus and how similar the documents are:•Total learning: Adjust T-HMM classifier with each new document. This will lead to better results if, in general, documents are very similar or there is a high number of topics in the corpus, making it necessary to gather multiple documents to represent each topic.Learning with errors: Adjust T-HMM classifier only with the new documents that were incorrectly classified. This would be preferable if a class could be well represented with few documents, unlike the previous approach.In any case, the T-HMM classifier has to be modified and adapted to new documents, and only the HMM representing the class which truly belongs to that document has to be modified. The proposed adaptation step is defined below.Given a trained HMM λcthat represents a category c (Relevant or Non-relevant) and a new document d categorized as c, in order to adapt the trained model to take d into account and give it a higher probability of being generated by λc, the following process is proposed:•The learning/memorization process of the new document is controlled by an “adjust weight” w. This weight depends on the number of documents that the HMM was trained with:w=Lnc1.nccorresponds with the number of actual cases in the knowledge base: initial case memory and learned cases.0≤L≤ncis a parametrizable value that we call “learning factor”. If L=0, then there is no learning/adjustment process. If L is increased, the observation probabilities of λcchange to increase the probability of d being generated. More specifically, the probability of producing words appearing in d is increased, while the probability of producing the remaining words decreases proportionally.The higher L is, the higher the adjustment will be. If L is too high, it will overfit the HMM with that document, as the other probabilities will be considerably diminished.The new document d is represented by a wordlist of length t ranked in decreasing order of word relevance, in the same way as in the classifying process. This t value is equal to the number of states of the HMM λc, so the adjustment/learning process proposed is similar to the training step to build the T-HMM classifier.For each state in λc, the output probability for a word is modified as:Adjustedbi(vk)=bi(vk)·(1−w)+f+(1−f)·1t·w(1)bi(vk)·(1−w)+(1−f)·1t·w(2)bi(vk)·(1−w)(3)(1)if the wordvkappears in the document d at the ith position rankif the wordvkappears in the document d but not at the ith position rankif the wordvkdoes not appear in the document d–w∈[0,1]bi(vk)stands for the probability of the word/observationvkbeing emitted at the state sif is the “generalization” factor of the T-HMM classifierIt is important to note that at a given state, there is no guarantee that the new word probability will be higher unless that word has the same rank as the state. In this case, the value of this increase is determined by f. While some words that appear in the document d may have their probabilities decreased in this state, they will increase in the state that represents their rank.The main idea behind this learning process is to adjust the output probabilities as if the new document d had been part of the initial training corpus in the first place. This implies reducing the probabilities of the words that are not in d, as seen in (3), in order to increase those words appearing in it: (2) and (3).

@&#CONCLUSIONS@&#
