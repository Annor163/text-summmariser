@&#MAIN-TITLE@&#
Multi-response non-parametric profiling using Taguchi's qualimetric engineering and neurocomputing methods: Screening a foaming process in a solar collector assembly

@&#HIGHLIGHTS@&#
Profiling of seven controlling factors for a solar-panel foaming operation.Three foaming characteristics are considered.Multi-factorial multi-responses are ranked and fused to a single master response with non-parametric transformations.Concurrent distribution-free optimization is effected through ANN linear and perceptron methods.Foam polyol temperature is found to be the predominant active effect.

@&#KEYPHRASES@&#
Taguchi methods,Artificial neural networks,Robust screening,Design of experiments,Orthogonal array,Solar collectors,

@&#ABSTRACT@&#
Taguchi's data programming techniques in synergy with data analysis tactics based on artificial neural networks have been fruitful in illuminating intricate manufacturing phenomena. We present a non-parametric approach to treat multi-response multi-factorial datasets created with Taguchi's orthogonal-array samplers. Replicated response datasets are compressed utilizing the signal to noise ratio and then they are homogenized by simple rank-ordering. The multiple response layout is reduced to the more tractable uni-response arrangement by using the super-ranking concept to enact the fusing of the individual responses. The ‘ranked-and-fused’ dataset is subjected to conversion by linear and three-layer perceptrons. The performance of a group of examined effects is assessed according to the perceptrons’ sensitivity analysis output. Using Wilcoxon's one-sample test, statistical significance is assigned to the accumulated ranking scores obtainable from a series of independent perceptron runs. We discuss the efficiency status for each of the two engaged perceptron options on affecting prediction accuracy as well as the influence of data fusion on the SNR-compressed datasets. Our robust neurocomputing solver is elucidated in a concurrent screening of three foam characteristics which are encountered in popular solar-collector assembly operations. Seven controlling factors were profiled and it was found that the temperature of the polyol additive is the sole statistically predominant effect. Finally, through our industrial paradigm, we illustrate the superiority of the fusing principle for downsizing stochastically multiple characteristics and thus gaining faster and more accurate perceptron predictions. We show that the proposed method outperforms the outcomes obtained by the desirability analysis. We identify the points of superiority to the crisper resolution in locating effect dominance accompanied with recovered stochastic significance.

@&#INTRODUCTION@&#
Modern manufacturing operations rely on feature enhancement through continuous improvement projects in order to remain globally competitive [1]. One promising strategy to ameliorate product and process traits has been credited to analyzing structured datasets with artificial neural network (ANN) methods [2,3]. Taguchi methods are usually implemented to streamline the data-intensive endeavors. Such methods primarily seek to resolve cause-and-effect relationships between product/process controls and their key responses [4–6]. Organizing a process improvement effort requires planning and collecting structured datasets by integrating fractional factorial designs (FFDs) in the sampling scheme [7]. FFDs are ‘ready-to-use’ experimental plans that assist in engineering highly dense datasets. A particularly popular version of FFD plans for programming a dataset cycle has been the orthogonal arrays (OAs) which comprise a substantial part of Taguchi's Design of Experiments (DOE) tool suite [5]. DOE is tailored to provide quick and economic information for making confident decisions by robustifying product/process performance [8–11].Profiling robustly OA-datasets for dominant effects demands a versatile converter that is susceptible to as fewer assumptions as possible [12–15]. Robustness remains an undisputable priority in quality engineering when multi-response ANN training is ensued for stochastic parameter profiling [16]. Therefore, finding opportunities to robustify a complex modeling procedure with novel estimators continues to being of interminable value [17]. ANNs have demonstrated that they may overcome several weaknesses that regular statistical processors such as the analysis of variance (ANOVA) and the general linear model (GLM) may experience during OA-data processing [18]. The combination of ANNs and OAs as boundary-based intelligent samplers for solving intractable optimization problems have been reviewed recently [19]. An outstanding feature of generic ANN processors is that they convey information without taking for granted an explicit reference distribution [20,21]. Equally worthwhile, ANN algorithms preclude the gauging of the individual effect strength against its remaining dataset error. That last attribute permits ANN routines to be keenly adapted for extracting information from daunting saturated OA schemes [22–24]. Saturation in OA designs occurs when the total degrees of freedom associated with the settings of an examined cohort of effects exactly match the prescribed number of the experimental combinations [5,7]. This condition when present prohibits the quantification of the unexplainable error. Therefore, mainstream multi-factorial treatments, such as the ANOVA and the GLM, which depend on rounding up a remaining error, are prone to breakdown when fed with saturated data [15]. Nevertheless, it should be pointed that saturation in OAs is a tactic that is welcomed in efficient trial planning because it is the optimum condition where the maximum allowable number of parameters is probed synchronously. In other words, only in saturated mode an experimenter achieves efficient use of the expended resources. Another issue that appears to be concomitant to saturation is that ANN-based solvers are not overwhelmed by the sparsity assumption [24]. The sparsity assumption conjectures a priori that some of the investigated parameters must be abeyant in a saturated experimental plan. Specialized multi-factorial methods developed to circumvent saturation were predominately plagued with the sparsity restriction [23]. Thereby, ANNs may be implemented in any saturated OA scheme even when all investigated effects are anticipated to be prevalently cogent.The usefulness of improving products and processes with Taguchi methods interlacing ANNs as the prime data converter option have been highlighted in a great number of technical publications for multi-factorial comparison studies that implicated either single or multiple characteristics. Chang and Chen [25] combined a three-stage approach that intertwined neural networks, Taguchi methods with desirability functions and genetic algorithms to a simulated example using control factors with continuous values. Antony et al. [26] combined Taguchi methods and a neuro-fuzzy approach to map multiple SNR-translated characteristics for a surface mount technology to improve performance in an electronic assembly. Neural networks in conjunction with Taguchi methods have shown to confront difficult multi-response optimization problems from the semiconductor and textile industries that otherwise were impossible to tackle with ordinary regression methods [27]. Moreover, integrating Taguchi methods with ANNs through non-linear saturated orthogonal arrays have identified important parameters for silicon thin film manufacturing [28]. Non-linear aluminum removal rate data have been collected through a large Taguchi orthogonal array by controlling four factors which regulated particulate metal-matrix composites and which were fitted with a multilayer artificial neural network and the back-propagation technique allowing small prediction errors [29]. The output voltage of a proton exchange membrane fuel cell has been parametrically optimized using a back-propagation neural network model and the Taguchi method [30]. The benefits of implementing Taguchi methods for programming organized experiments against merely gathering and analyzing unstructured data has also been discussed in that last work. Metamodeling cost-efficient and optimal solutions for improving the power density performance of PEMFCs have shown promise when Radial-Basis-Function Neural Networks are integrated with Taguchi-type orthogonal arrays [31]. For electric power systems, fault identification prior to reclosing is critical. Taguchi methods assisted in fine-tuning prony analysis of a transmission line with an intelligent autoreclosure method which was based on ANN and resilient backpropagation algorithms [32]. Two-stage progressive Taguchi neural networks have been developed to heighten the predictive ability in advanced laser cutting processes [33]. A micro-zoom optical design that attains the adjustment of several traits at the same time has been well coordinated using the fuzzy method [34]. The pursuit for enhanced robust and intelligent ANN methods that manipulate effectively data obtainable from Taguchi experimental plans is clearly unabated [35]. The combination of DOE samplers and ANNs may become more flexible by selecting fitness functions drawn from desirability scores as in the case of resolving the behavior of a sputtering process [36]. Among the most crucial machining operations is surface roughness minimization for a large spectrum of metals and composite materials which are intended to components manufacturing and assembly works. Mechano-tribo-performance of complex yet low-cost glass-epoxy composites have been found to be improved with filler addition after screening the erosion rate with Taguchi analysis and ANNs [37]. The leading role of Taguchi methods and ANN techniques in deciphering the multi-parameter optimization of surface roughness in a wealth of production environments has been reviewed exhaustively by Pontes et al. [38]. This last group inferred that neurocomputing algorithms when supplemented with the proper statistical engineering tools will gain momentum in quest for intelligent and robust processing. Furthermore, Taguchi methods may be even used internally within the ANN framework to optimize the selection of a deployed radial basis function network before attempting to predict surface roughness [39]. Turning processes in modern CNC centers become quality and cost effective by resorting to multi-factorial hybrid tools that seamlessly mingle intelligent processors with structured samplers [40]. A new hybrid harmony search approach surpasses several techniques in Taguchi-feature optimization problems when it is compared with a hybrid genetic algorithm, a scatter search algorithm, a genetic algorithm, and a feasible direction method [41]. Best solutions for a four-factor four-level welded-beam Taguchi-optimization for a single characteristic have been reached with a hybrid particle swarm algorithm [42]. A two-feature shape improvement of a vehicle component has been predicted using a four-factor four-level Taguchi-immune algorithm [43]. A hybrid Taguchi method that is activated by a robust differential evolution with improved searching abilities has been recommended by Yildiz [44]. A hybrid differential evolutionary-based algorithm has been shown to outplay as many as six population-based competing schemes on a Taguchi-type volume minimization of a vehicle component [45]. A hybrid artificial bee colony algorithm has outperformed on solution quality and computational effort several competing schemes tested on Taguchi optimization setups for a vehicle component and in multi-tool milling process [46]. Hybridized back-propagation neural networks with multi-response Taguchi analysis have elucidated the puzzle of optimizing the behavior of SGF and PTFE reinforced composites in a modern injection molding process [47]. Orfanakos and Besseris [48] reported on the multi-response multi-factorial problem for controlling the efficacy of spam filters using three-level saturated OAs and commercial ANN solvers. The applications of DOE in synergy with backpropagation ANN engines may be well extended beyond the manufacturing and engineering fields to embrace highly volatile transactions such as predicting stock price variations [49]. The diverse options that may be at the disposal of an analytics/data-mining practitioner who employs expert systems have been accumulated in two essential reviews by Liao et al. (2007) and Liao et al. [50].An intriguing topic arises when manipulating compressed OA-datasets with ANNs. Taguchi's DOE methods promote the Signal-to-Noise ratio (SNR) as the principal condenser for reducing replicated experiments while filtering uncontrolled background noise simultaneously [5]. The SNR compresses multiple replicates down to a single vector. Thus, each replicated OA recipe is eventually connected to a unique single-entry value. On the other hand, OAs are compact samplers that dictate that all prescribed recipes be executed and correspondingly mapped to an output if the analysis of the experimental plan is to maintain its balance and effectiveness [5,7]. For the conversion process to be successfully enacted, the complete SNR-transformed vector is fed to the chosen profiler. However, it is well known that standard ANN techniques proceed by requiring the segmentation of the available data in three distinct processing phases. This diversified subsetting procedure engages: (1) the training of the network, (2) the testing of the network and (3) the validating of the network. This means that the OA output should be divided in three separate batches of data to serve the three respective processing stages. Black-box commercial ANN algorithms usually select out randomly those strands of OA data. Due to the uniqueness of each trial's input combination, training a network is impacted by the particular collection of the partaking trial recipes. Clearly, at this point, the ANN training phase is coerced by design to march on incomplete (partitioned) OA data. Nevertheless, DOE screening outcomes are summarized in an effect hierarchy format which is derived from an ANN sensitivity analysis [51]. Sensitivity analysis represents the input pruning mechanism in ANN after carrying out the profiling effort. We immediately recognize a possible drawback in the fact that different permutations of groups of partitioned OA-recipes which enter the three ANN processing stages might produce disparate rankings. Therefore, harvesting information from SNR-compressed OA-data with typical ANN methods should be amenable to a stochastic quantification of the significance of their outcomes. Inasmuch as there is no previous attempt to deal with the deficiency we just mentioned, one major contribution of this work is to recommend a practical methodology to circumvent it.In this work, we will concentrate on confronting with ANNs the more general problem of multi-response multi-factorial concurrent OA-screening. Insofar as the maximum utilization of any given OA sampler remains challenging in all production environments, the OA-saturation will be maintained as a desirable condition in our developments. In the profiling process we explicate the incentive to size-up the strength of the investigated effects by exploiting the ANN-solver versatility to advance without being restrained by the sparsity assumption [23]. Additionally, we are strongly motivated to curtail the computational complexity by reducing the multiple response case to a single master response problem by using simple-to-deploy nonparametric fusion statistics [15,24]. Non-parametrics are advantageous in codifying the SNR-compressed data vectors emanating from various origins – independently of any knowledge about the distributional blueprint of the participating datasets. We statistically substantiate the efficiency of adopting ranking transformations against the alternative which is to proceed directly with the uncoupled multi-response screening. Our technique will be showcased by screening a polyurethane foaming process that is used in solar collector manufacturing. We demonstrate through our paradigm how to organize screening trials for difficult multi-response multi-factorial screenings by restricting any assumptions about effect trends and stochastic behaviors to a minimum. Our novel ANN-based proposition is simple to comprehend, implement and interpret. It utilizes exclusively mainstream packaged routines and thus it is also easily verifiable in its technical aspects. Since our illustrated production problem has not been previously resolved in the literature, we deploy both linear and non-linear ANNs to assess the validity of our solution. Specifically, the non-linear version of the ANN solver will be a three-layer perceptron which will be used to intelligently prioritize the sensitivities of the examined effects for the concurrent multi-response multi-factorial screening problem in fused and unfused versions.Taguchi methods include a versatile suite for structured information harvesting which includes tools primed for data compression and filtering as well as data analysis routines [1,5]. Taguchi methods originally unfolded through product or process improvement projects targeting mainly manufacturing operations. Taguchi philosophy espouses quick-minded experimentation while striving to filter out any uncontrolled noise. The goal is to unveil those significant mechanisms and their controls that stochastically influence: the improved functioning of a product through process maturity. To perform an improvement study, a researcher ought to decide on: (a) a suitable trial planner, (b) the data compressor/filter, and (c) the data convertor.Taguchi's trial planning fosters the implementation of a particular breed of OAs, which provide the backbone for structured dataset generation. OAs may be used collectively for either screening or/and parameter design studies. In this article, we will concentrate on screening a group of effects by deploying the L8(27) OA planner. Nevertheless, the generalization of our developments is easily extended to any type of OAs. The generic form for the L8(27) OA is given in Table 1for any arbitrary set of seven factors labeled respectively A–G, each modulated at two preselected operating endpoints (levels 1 and 2). Since structured experiments may not prevent the habitation of unknown and unknowable intrusions, screening trials should be benefited from considering replication of the planned trial runs. For economical testing, the minimum number of replicates which are necessitated for estimating measures of central tendency and variability is at least two. Replicates may be extended up to any number N depending on the achieved output variation in concert with trial costs and logistics. The Taguchi approach delineates the influence of unwanted noise by setting up replicates in either of two possible forms, i.e. in (a) controlled or (b) uncontrolled layout. In lack of any coherent knowledge about the ‘noise spectrum’ manifested in our screening study (see “Modeling with artificial neural networks” section), we will program trials in the more arbitrary framework for ‘uncontrolled-noise’. This simply means that the L8(27) OA trial runs will be scheduled to be replicated at random times in our examined production cycle.The signal-to-noise ratio (SNR) is the preferred compressor which is employed in replicated experiments to enforce data reduction and data filtration in a single step. SNR may be viewed as a ‘defogger device’ when applied in processing raw data which have been obnubilated by ambient noise. The concept of the SNR in stochastic screening and optimization problems containing loss function information about an investigated response has been greatly elucidated for generic industrial applications in the past [1,5]. The acceptance of the Taguchi-type SNR as a viable data reducer in soft computing studies is also well documented in modern literature [26,28,36,39,44,46]. The usefulness of the SNR as a performance measure for data compression research has been studied extensively with regards to robust design in the past [52]. However, to utilize the SNR concept, the direction of the optimal path for an examined response must firstly be identified. To assist with this task, testing product traits or process responses have been categorized by Taguchi to three distinct types [1]:(1)‘Nominal-is-best’: for characteristics that need to be drawn as close as possible to a nominal value.‘Smaller-the-better’: for characteristics that need to have their response minimized.‘Larger-the-better’: for characteristics that need to have their response maximized.Based on the above three goal types, Taguchi recommended three distinctly different SNR operators to respectively facilitate the data reduction process. If a sample of N observations, yi, have been collected for a given characteristic Y such that {yi|i=1, 2, …, N}, then the three possible options for computing the SNR quantities are:(1)‘Nominal-is-best’ case:(1a)SNR=10Logy¯s¯2wherey¯ands¯are the sample mean and standard deviation, respectively‘Smaller-the-better’ case:(1b)SNR=−10Log∑i=1Nyi2N‘Larger-the-better’ case:(1c)SNR=−10Log∑i=1N1/yi2NOne convenient and economical way to simplify the data-processing of multi-response OA schemes is to effectuate rank-ordering transformations. Rank-ordering homogenizes and standardizes automatically the various implicated outputs [53,15,24]. Then, a straightforward merging procedure such as the master-ranking operation [15] may be applied to fuse all different responses together to a single ‘master’ response that contains information from all concurrently tracked characteristics. By inducing the master-ranking concept, the data processing step is converted to the more manageable uni-response case, thus simplifying immensely the subsequent information harvesting phase. This last feature should be more advantageous as the number of investigated response increases. To review briefly the master-ranking concept, we assume that a set of replicated runs have been gathered previously in data columns arranged according to a selected OA scheme. Next, the resulted stacks of data columns are down-sized to single SNR-responses for each inspected characteristic separately. Then, each SNR-vector is rank-ordered individually by initiating a rank assignment, i.e. allocating the rank of 1 to the column entry with the maximum SNR value and so forth. The rank allotment process concludes with the column entry that possesses the smallest SNR value. SNR entries possessing equal magnitudes are permitted to share tied ranks. This process is schematically recreated in Fig. 1, elucidating the steps after the SNR transformations have been completed for each of the m participating characteristics. If n is the number of executed trial combinations, then the SNR entries are denoted as {SNRij|i=1, 2, …, n; j=1, 2, …, m}. After rank-ordering, the newly transformed columns are symbolized by {rij|i=1, 2, …, n; j=1, 2, …, m}. Since the m responses have been now non-parametrically standardized they may be added to a single response by summing over the squares of their ranks (SSR) to commingle the multiple ranked-vectors to a single aggregate (‘master’) response: {SSRi|i=1, 2, …, n}.What has been accomplished here is that by the end of this procedure, the original multi-response replicated-OA dataset has been mapped into a single distribution-free response.The paradigm that will be showcased in this work reflects a foam production improvement effort in a manufacturer of solar collectors. Solar panel systems transform sun radiation into thermal energy [54,55]. The specific model regards a solar water heater model which is installed either as a floor- or roof-mounted close-coupled thermosiphoned unit [56]. Flat-plate solar thermal collectors in the thermosiphon configuration are very popular around the world because they match overall available panel area to their corresponding absorber area (∼2m2). Thermosiphoned thermal collectors deliver maximum efficiency (0.74) when compared to other competing designs. To attest to the increasing demand for using solar water heaters for domestic and industrial needs, the Renewables Global Status Report [76] announced that the worldwide consumption owing to such sources has topped 20GW (thermal), recently.Polyurethane foams (PFs) constitute the key insulating material in the mass production of solar water heaters. PFs effectively support the copper tubing network in the solar panel assembly against sidewall energy losses. Equally important is the insulation of the assorted water tank and its associated connecting tubing surfaces with PF layers in order to minimize heat leaks to the environment. Polyurethanes are thermosetting polymers that resist melting at increasing temperatures [57]. Thus, as mainstream insulators, polyurethanes remain rigid providing a high-resilient foam seating. A particularly attractive feature in modern polyurethane formulations is that such mixtures are comprised of low cost, yet environmentally friendly, polyol compounds [58]. Additionally, green-engineered blowing agents are also introduced to facilitate foam-density minimization and thermal-insulation maximization [59].The case study focuses on an effort put forth by a large solar collector manufacturer to improve its insulation materials by enhancing the performance of their foaming processes [60]. There was no previous systematic experience in the plant about how to organize a process/product improvement project. After a brainstorming session that involved production, quality and research-and-development managers, it was conferred that it was of an immediate need to firstly discover the relative strength of a group of relevant PF manufacturing factors. Therefore, it was crucial that the experiments would be conducted in pragmatic conditions on host production machinery in order for the results to impact profitably a follow-up decision-making. However, the required number of trials would necessitate interrupting normal production scheduling, thus incurring losses in machinery availability while imposing extra operational costs. In turn, it was instructed to truncate the number of trial runs to an absolute minimum. To program the expected trials in an efficient and effective manner while suppressing trial costs, it was concluded that: (1) structured data will be collected through a proper fractional factorial plan, and (2) repetitions will be restricted to merely duplicate runs.Additional savings would be realized by screening multiple foam quality characteristics simultaneously. The operational gain from such a tactic was thought to be two-fold: (1) it reduces multiple trial cycles to a single one, and (2) it allows searching and locating optimal solutions for adjusting synchronously vital product characteristics. Indeed, if the number of investigated responses is m, then concurrent experimentation curtails trial expenses at least by a factor of 1/m in comparison to executing sequential cycles for each characteristic separately. Thus, it became practically meaningless to profile multiple responses in a ‘one-at-a-time’ mode when it is a consensus adjustment among the examined controls that it is sought. Therefore, the vital question that would be posed was how to potentially compromise among the various responses simultaneously such that to harvest the most dominating effects of all.Being a start-up project, it was decided to screen a group of seven controlling factors. The nominated factors are listed in Table 2in terms of their coded abbreviations and their respective measuring units. In the same table, there are tabulated the two operating endpoints where the screening was planned to be carried out. The measured PF responses were: (1) free-rise density (DFR) in kg/m3, (2) creaming time (CT) in s, and (3) gelling time (GT) in s. The responses DFR and GT are sought to be maximized. CT is targeted toward a manufacturer's specification range of 15–20s. A detailed account regarding the description of the industrial process arrangement as well as the measurement requirements and techniques that were implemented are found in Barkas [60] and they will be published elsewhere due to their extensive length. The experiments were scheduled according to Taguchi's L8(27) orthogonal array (Table 3). Duplicate runs were judged to be adequate such that to contain materials consumption and machinery availability given that the experimental work was planned on full-scale working conditions.Neural networks have demonstrated to be a prolific analysis toolset in data-mining studies [61,62,21]. Artificial neural networks are quantitative algorithmic-based techniques that learn to associate adaptively patterns of inputs and outputs by mimicking a circuit of biological neurons (Bishop, 1996; [20,63]). An extensive account about the growth and the potential of artificial neural networks have been well-organized by several fundamental reviews spanning a developmental era that encircles the last decade [64,65]. In particular, categorizing and analyzing vast compilations of production measurements through modern data mining techniques have been greatly assisted by neural networks [66–68]. Ergo, the inclusion of neural networks in designing quality in products and robustness in functional performance has been profitable [12,2,69]. In this work, we will contrast both linear ANNs and non-linear perceptrons to investigate in a novel fashion ANNs’ predictability in explaining stochastically fused multi-response multi-factorial structured screening data. The treatment of dense multi-trait datasets which have been condensed with the aid of an OA trial planner and further compressed with the master-rank transformer has never been explored previously in neural networks. The technique as presented herein exploits neural network capability to abstract away the unexplainable error contribution in sieving through dense datasets. Thus, neural networks are tested as a data converter in one of the most demanding situations in DOE, i.e. to treat an OA-generated dataset that has been saturated.The overall concept of profiling medium-sized OAs with ANNs is challenging on its own right because of the inherent limited data available for processing. For instance, the eight-run L8(27) OA when studied in a single-response layout form is confined to engage only four data entries for network training. The rest of the four entries are compromised for subsetting in the selection and test phases. Under such circumstances, working with small data, it is justifiable to advance the data conversion process using at least two models for inspecting the internal consistency of the outcomes. To enable verifying the solution, a linear neural network is employed where singular value decomposition provides the solver machinery. Additionally, to bolster consistency, a three-layer perceptron is trained to offer a competing prediction. The aim of profiling is to obtain the rankings of the investigated effects based on sensitivity analysis results from both aforementioned neural network models. Using repetitive runs for either of the two neural network layouts, ranking statistics are collected and non-parametric significance of their outcomes is estimated.Linear neural networks (LNN) are among the simplest forms of ANN models [20,3,21]. LNNs are associators of input and output signals that are shaped into information in a single node (axon). In lack of any prior knowledge about the behavior of investigated responses, LNNs constitute a logical starting point according to Occam's razor principle in exploring input–output relationships. The basic LNN model describes the affine mapping (ℜn→ℜk) of an input layer of n incoming signals X∈{xi|i=1, 2, …, n} to k outgoing signals Y∈{yj|j=1, 2, …, k} according to their weighted contribution W∈{wij|i=1, 2, …, n; j=1, 2, …, k} and the bias (resting state) of the k nodes, b∈{bj|i=1, 2, …, k}. For all xi, wiand b∈ℜ, the output is the sum of all weighted signals compiled on the bias at the node:(2)yj=∑i=1nwijxi+bjA popular solver for LNN problems is the singular value decomposition (SVD) method [62]. For carrying out the fused solution of the seven-input and three-output screening problem, which is presented in the preceding subsection (see “Foaming production for solar collectors” section), a single-node LNN layout will be utilized. Fig. 2A depicts the seven-effect input allotment due to L8(27) OA arrangement to accommodate the one-response output representing the ‘ranked-and-fused’ vector SSR, as defined in section “Joint ranking compressors for multiple response screening”. In section “Discussion”, we will compare the benefits of our transformation methods against the option of dealing directly with the ordinary three-response SNR output. In such case, we will test the LNN problem for the three-node layout as depicted in Fig. 2B, where each node correspondingly accommodates each of the three SNR-translated responses.The multi-layer perceptron (MLP) is an ANN model that maps in a feedforward topology groups of input controls to groups of output responses [70,71]. The MLP is assembled by an input and an output layer of node units where one or more hidden layers interject the input–output relationship to allow configuring a meaningful network topology.Feedforward signals, X∈{xi|i=1, 2, …, n}, are modulated on the kth node by “synapse” weights, W∈{wik|i=1, 2, …, n; k=1, 2, …, m}. Those weights assign inhibitory or excitatory characteristics on the “synapse” by virtue of their sign being either negative or positive, respectively. A compounded impulse, I∈{Ik|k=1, 2, …, m}, received at the kth node is simply then the weighted sum of all inputs which may or may not include a bias term, b∈{bk|k=1, 2, …, m}, i.e.:(3)Ik=∑k=1mwikxi+bkA transfer function manipulates the compounded impulse Ik, thus, essentially regulating the activation of the computational synapse. Often used is the logistic function (log-sigmoid), S(Ik), which animates more realistically the input–output relationship of biological neurons with a slope parameter β, i.e. at the kth node output, yk(Y∈{yk|k=1, 2, …, m}):(4)yk=S(Ik)=11+e−βIkAn MLP algorithm monitors the training of a network which is paced by a supervised learning technique known as backpropagation [72–74]. Backpropagation drives the weights associated with the incoming signals across all hidden and output units toward optimum values such that the predicted output approximates the experimental response dataset as close as possible. If Tk(T∈{Tk|k=1, 2, …, m}) is the target output which is sought to be closely approximated by yk, then, the error function is defined as:(5)E=∑k=1mEk=12∑k=1m(Tk−yk)2The recurrence relation that updates the weight change Δwikfrom the current iteration t to the advancing iteration (t+1), in its common form, is written as:(6)Δwik(t+1)=−η∇Ek+αΔwik(t)where η is the learning rate and α is the momentum coefficient which in turn may or may not be iteration dependent. The local error gradient, ▿Ek, for the sigmoid in Eq. (6) (β=1) is defined as:(7)∇Ek=−(Tk−yk)yk(1−yk)xiIn this work, we will employ a three-layer perceptron with three hidden-layer units to complement the requirements of the case study in the previous sub-section (see “Modeling with artificial neural networks” section).The perceptron profiler generates diagnostics through a standard sensitivity analysis which is portrayed in terms of an effect hierarchy [51]. To accomplish the conversion of the (fused-response SSR) seven-factor OA-dataset, the three-layer perceptron schematic which we deploy is depicted in Fig. 3A. Furthermore, we discuss the profiler performance for the proposed ‘ranked-and-fused’ response method against the typical three-response (unfused) problem, in Discussion. Therefore, the layout for the topology of the three-layer perceptron with the seven inputs and the three distinct SNR-outputs, which is computed at a three-node hidden layer, is sketched in Fig. 3B. During preliminary testing for selecting an efficient topology for the non-linear perceptron, it was found that utilizing four- to seven-nodes for the hidden layer would produce predictions at about the same accuracy as a three-node network which nevertheless demanded larger computation times. Instead, a two-node hidden layer was a fairly unstable choice inciting the need for larger samples to be gathered.To facilitate in planning and converting the experiments in signal-to-noise units, the statistical software package MINITAB 16.1 was utilized. From the ‘DOE’ menu, the ‘Create Taguchi Design’ module was chosen to automatically program the corresponding trial recipes. The responses were collected in duplicate datasets. Therefore, the ‘Analyze Taguchi Design’ module was used to compress each duplicate set of columns to a single SNR-transformed column, for each characteristic separately. The entire L8(27) OA factor-setting arrangement assorted with the three SNR-reduced responses which were obtained from MINITAB 16.1 were next loaded in a worksheet in the statistical software Statistica 7.0 for ANN processing. The ‘Intelligent Problem Solver’ (IPS, Neural Networks module, Statistica 7.0) was selected as the platform to execute the required analysis for both modes. In the first mode, the single non-parametric response (SSR) was formed from the sum of squared ranks of the entries of the three PF SNR-reduced characteristics. In the second mode, the three SNR-reduced responses were treated as a concurrent three-response problem. The IPS provided results for both indicated network types, i.e.: (a) the LNN and (b) the three-layer perceptron (MLP). For the MLP executed runs, the selected number of hidden units was fixed at three while the regression output encoding was linear. For all runs in our study, the number of tested networks was constant at 10,000. Sampling subset sizes were set for all attempts at 4, 2 and 2 associated to training, validation and testing, respectively.Recorded data were in terms of model summaries that listed the three relevant errors for: (a) training, (b) validation, and (c) testing. The accompanying sensitivity analysis provided the input network error in ratio form, as well as rank-ordered, gathered from multiple test cases. However, no prior information exists regarding the sampling demands for linear and perceptron runs with respect to the case study of section “Foaming production for solar collectors”. To generate minimum ‘start-up’ information, we evoke the ‘sample-size-of-30′ rule for all tested layouts [75]. Then, after inspecting the sensitivity analysis output regarding the 30 runs, we ascertain the adequacy of the sample size by making use of the ‘Power and Sample size’ module of MINITAB 16.1. If it is found that the prediction is less than 30 samples, then, we merely analyze the existing dataset. However, if the sample size prediction requires a larger sample, we simply conduct the additional experiments to reach the predicted limit. In the network illustrations, the unit activation levels are displayed automatically and they are differentiated in color by the Statistica 7.0 software package. The red or green hue is associated with either positive or negative activation levels, respectively. Triangle shapes indicate input neurons performing no processing but initializing a variable while squares symbolize dot product synaptic function units.All main effects plots (response graphs) were produced through MINITAB 16.1. Moreover, to furnish a statistical comparison of the individual effects according to their frequency during consecutive collections of their relative sensitivity analysis ranking, their ranks were compounded for all trials forming an aggregate rank score. The one-sample Wilcoxon test was used to check if all effects could belong distributionally in the same family utilizing the corresponding MINITAB 16.1 application. The relative output that is monitored is the estimated median value, the confidence interval and the estimated achieved confidence.The proposed methodology resolves the effect screening of a multi-factorial input through constrained stochastic optimization. Screening is a unidirectional optimization task since it always seeks to minimize the number of initially nominated effects by filtering out statistically weak influences. Diagram 1offers the steps for the novel methodology that resolves the ‘triple-fuzzy’ problem of dealing with:(1)The induced unreplication-saturation on the multi-response OA-output by the SNR data reduction.The downgrading of the original stochastic problem to a virtual missing value problem on the multi-response OA layout which is encountered in the training phase due to the inherent data segmentation by the perceptron.The tactical elimination of the sparsity assumption from the solver algorithm.Unreplication-saturation intensifies the fuzziness in the input–output relationship by eliciting the blurring of the magnitude of the uncertainty in the experiments. This puzzling phenomenon arises because of the natural incompatibility of OA-data structures to harmonize with variance-based statistical methods, i.e. ANOVA and GLM. Maximum efficiency for an OA sampler occurs at the saturation point which unfortunately permits no degrees of freedom for error variance. Furthermore, the implementation of ANNs in order to convert OA-datasets ostensibly destroys the rigid OA-sampling structure. By partitioning the trial-run data into the three distinct subsets, the ANN training phase is deprived of the full information content which the uniquely prescribed recipes of an OA scheme provide. This causes a virtual missing-value problem to manifest in the training phase since portion of the original OA-dataset is segmented away to be rendered available for the testing and validation phases instead. Consequently, the degrees of freedom to be awarded to the examined effects for training the perceptron amount in fact to a total value below the required minimum threshold. This last aspect also magnifies the incurred fuzziness in the input–output relationship. The algorithm is called upon to reconstruct a stochastic solution by directing the perceptron to extract information from three different subsets that had been segmented in such unorthodox manner that each subset offers a different side view about the behavior of the effects. Lastly, by removing the sparsity assumption, the algorithm becomes judicious which naively marches on without prior knowledge regarding the possibility that a subgroup of the effects might be weak. This places towering demands on the resolving power of our approach thereby contributing additional fuzziness. The pseudo-algorithm that decomposes the OA-dataset producing each time a sensitivity analysis report for the constrained (missing-value) optimization problem is outlined in Diagram 2. We point at the novelty of using ANNs to convert the original ‘triple-fuzzified’ dataset to a meta-dataset. The repeated perceptron runs facilitate the tracking of the variability in the effect hierarchy. The trivial ANN pseudo algorithm actually reshapes and molds obfuscated information well-camouflaged within the original input-output relationship. Thus, our new perceptron deployment approach promotes the development of a new stochastic entity to be easily interpreted from a meta-analysis with simple robust tools. The perceptron transforms incoming signals to a new virtual yet simplified output by protecting the de-fuzzification process such that to prevent the dataset to shed its stochastic nature.

@&#CONCLUSIONS@&#
In this work, we proposed a method for profiling effects sampled with fractional factorial designs. The technique involves the statistical processing of factor rankings furnished by ANN sensitivity analysis. Our developments are suitable for explaining the effect hierarchy for multi-response multi-factorial datasets which have been designed by implementing orthogonal arrays. The dataset may be collected in replicated form which is then reduced with the regular signal-to-noise ratio compressor. At the core of our proposition, is the non-parametric transformation of the SNR-translated characteristics which subsequently are fused to a single master response for more convenient data handling. Thus, the multi-response problem may be treated with a homogenized and fused data structure which is simpler and faster to process. More efficient processing is anticipated for non-linear models. We showed that the overall concept may be useful for linear and non-linear perceptron modeling alike with regards to obtaining more accurate results. We illustrated our novel methodology with a case study which is drawn from foam production intended for assembling solar collector panels. We investigated the influence of seven controlling factors with respect to three foam characteristics. By employing linear and three-layer perceptron models, we conducted screenings attempting two different procedures. In the first procedure, the three-response problem was directly optimized in synchronous fashion. In the second procedure, the fused vector derived from incorporating the individual rank-orderings from each of the responses was analyzed as a single response. In all four cases, it was found that the polyol temperature was the only factor that adjusted in a statistically significant manner the optimal behavior of free-rise density, creaming and gelling times. However, the linear ANN with the fused vector converged to the correct answer faster with better statistical performance. Comparing our findings with those obtained by the desirability analysis demonstrated that the proposed method defuzzyfies the screening outcome more efficiently while recovering the hidden stochastic significance.