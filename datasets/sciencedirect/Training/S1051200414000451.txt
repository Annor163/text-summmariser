@&#MAIN-TITLE@&#
Maximum similarity thresholding

@&#HIGHLIGHTS@&#
This research provides a maximum-image-similarity view of Otsu method.It constructs a flexible maximum similarity thresholding (MST) framework.It proposes a new thresholding method based on the MST framework.New method can deal with images with strongly unbalanced class sizes.It is robust to thresholding images with Gaussian or non-Gaussian mixture distributions.

@&#KEYPHRASES@&#
Image thresholding,Image similarity,Pearson correlation coefficient,Otsu thresholding,Minimum error thresholding,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Image segmentation is a critical preprocessing step in computer vision and image understanding. After reviewing various methods for gray level image segmentation, Pal and Pal [1] state that image thresholding is a popular segmentation method because of its simplicity and ease of implementation. There are two types of thresholding methods, i.e., global thresholding and local thresholding. Generally, a local thresholding method better suits poor and unevenly illuminated images [2,3]. However, a global thresholding approach is a more appropriate choice for images in which the object and background can be separated with an optimal threshold [4–6]. Although there are many algorithms to select the segmentation threshold of global thresholding, automatic selection of a robust, optimum threshold remains an interesting and challenging task [7,8]. In this paper, we will focus on global thresholding methods.Global thresholding methods compare each pixel in a gray level image with a calculated threshold. Thus, how to determine an optimal threshold becomes a core problem of global thresholding segmentation [5]. Many methods have been proposed to automatically determine the segmentation threshold over the past several decades [6–20]. An early review of thresholding approaches was reported in [21]. A comparative performance study of global thresholding techniques was reported by Lee et al. [22]. Another comparative analysis of the performance of eleven histogram-based thresholding methods was carried out by Glasbey [23]. Specially, an exhaustive survey of image thresholding methods was conducted recently by Sezgin and Sankur [24], where the readers can discover more comprehensive information about image thresholding methods.Among these global thresholding approaches, the two most popular ones are a minimum error thresholding (MET) method [12] and an Otsu method [13]. The two methods are widely used in practice and highly cited in scientific publications [25]. MET method finds the optimum threshold by optimizing the average pixel classification error rate. This method assumes that an image is characterized by a mixture Gaussian distribution. Otsu method utilizes the discriminant analysis to find the maximum separability of two classes. For every possible gray level, this method evaluates the goodness of this value if it is used as a threshold. Recently, several interesting views of Otsu method have been reported, which enhances understanding of the properties and thresholding performance of Otsu method [15,25,26]. From the perspective of maximum likelihood estimation, Kurita et al. [15], Xue and Zhang [25] show that Otsu method can be viewed as a special case of MET method, in which case equal class sizes, equal class variances, and image histogram composed of two Gaussian distribution are assumed. In addition, Xue and Titterington [26] provide a statistical-hypothesis-testing view of Otsu method by revealing the relationships between t-tests, F-tests, and Otsu method.MET method is based on the Gaussian assumption for modeling the class distributions in the gray level image. When the assumption of Gaussian distribution is strictly satisfied, MET method will succeed to determine a reasonable threshold. However, its effectiveness is strongly reduced when the prior probabilities of object and background classes are far away from the Gaussian distribution [4]. Otsu method seems not to depend on any special assumption about the class distributions. However, after analyzing the relationship between Student's t-test and Otsu method, Xue and Titterington [26] recently reported that two assumptions should be satisfied if Otsu method wants to obtain an optimal threshold. One assumption is that the prior probabilities of object and background classes follow Gaussian distribution with equal class variances. The other assumption is that object and background classes are of equal size. When the assumptions of equal class sizes and equal class variances are apparently violated, Otsu threshold tends to split the class with a larger size, and to bias towards the class with a larger variance [14,25,26].The histograms of gray level images show various patterns, such as mixture Gaussian modal (see Figs. 2(d)–(e) and Figs. 4(c)–(d)), mixture Rayleigh modal (see Fig. 6(g)), mixture extreme value modal (see Fig. 7(g)), mixture Beta modal (see Fig. 8(g)), mixture uniform modal (see Fig. 9(g)), comb-like modal (see Fig. 10(g)). Although the segmentation results of MET and Otsu methods are in general acceptable for the gray level images with mixture Gaussian or approximate Gaussian modal, these assumptions mentioned above will limit their segmentation accuracy and their application areas, since these assumptions often do not hold in real world images. Some improved methods have been proposed, such as recursive Otsu method [9], 2-dimensional Otsu method [16], two-stage Otsu method [17], expectation-maximization method [4], valley-emphasis method [18]. However, it is still difficult for these improved methods to determine the reliable thresholds for the images with mixture non-Gaussian modal.In order to determine automatically the robust and optimum thresholds for the images with various histogram patterns, this paper proposes a new global thresholding method based on a maximum-image-similarity idea. The idea is inspired by analyzing the relationship between Otsu method and Pearson correlation coefficient (PCC) [27,28], which provides a novel interpretation of Otsu method from the perspective of maximizing image similarity, i.e. Otsu method is to search for a binary image as the output that is most similar to the input gray level image, and PCC is used for the similarity measure. It is then natural to construct a maximum similarity thresholding (MST) framework by generalizing Otsu method with the maximum-image-similarity concept. The MST framework transforms the optimum threshold selection issue to the computation of image similarity, which facilitates us to develop new image thresholding methods with the image similarity theories. According to this framework, a novel thresholding method called MST is directly designed, and its robustness and effectiveness are confirmed by the experimental results on the synthetic and real world images.The rest of the paper is organized as follows. Section 2 describes some symbol specifications. Section 3 analyzes the relationship between PCC measure and Otsu method in detail. Section 4 proposes the MST framework, and Section 5 introduces the proposed MST method. In Section 6, 41 synthetic images and 86 real world images are used to verify the robustness and effectiveness of the proposed MST method. Section 7 discusses the multilevel thresholding extension of MST method. Finally, some conclusions and future works are described in Section 8.Let χ denote a gray level image with N pixels,xirepresent the gray level of the ith pixel, and L is the largest gray level in this image. A threshold t classifies the image χ into the background classC1(t)and the object classC2(t), whereC1(t)={i|0⩽xi⩽t,1⩽i⩽N}andC2(t)={i|t<xi⩽L,1⩽i⩽N}. In addition, let the gray level histogram of the image χ be denoted withh(x)that subjects to the following constraint:(1)∑x=0Lh(x)=1A binary image obtained by thresholding the image χ with t is denoted withγ(t), which is defined as:(2)yi(t)={0ifi∈C1(t)1ifi∈C2(t)whereyi(t)represents the ith pixel value in the binary imageγ(t).Letω1(t)andω2(t)denote the proportions of pixels representing classesC1(t)andC2(t), respectively.ω1(t)andω2(t)are defined as:(3)ω1(t)=∑x=0th(x)(4)ω2(t)=∑x=t+1Lh(x)Letμ1(t)andμ2(t)denote the average gray levels of pixels representing classesC1(t)andC2(t), respectively.μ1(t)andμ2(t)are given by:(5)μ1(t)=∑x=0txh(x)/ω1(t)(6)μ2(t)=∑x=t+1Lxh(x)/ω2(t)Letμχandσχ2denote the mean and the variance of the image χ, respectively.μχandσχ2are defined as:(7)μχ=∑x=0Lxh(x)(8)σχ2=∑x=0L(x−μχ)2h(x)=∑i=1N(xi−μχ)2/NPCC measure is widely used in statistical analysis, pattern recognition, and image processing. For two gray level images χ and ψ with the same size, PCC measure is defined as [27,28]:(9)r=∑i=1N(xi−μχ)(wi−μψ)∑i=1N(xi−μχ)2∑i=1N(wi−μψ)2wherexiandwiare the gray levels of the ith pixel in the images χ and ψ, respectively.μχandμψdenote the means of the images χ and ψ, respectively. Eq. (9) indicates that PCC measures the similarity between two images. In fact, PCC has the valuer=1if the two images are absolutely identical,r=0if they are completely uncorrelated.In Eq. (9), if the gray level image ψ is replaced with a binary imageγ(t), the corresponding PCC for a gray level images χ and a binary imageγ(t)can be rewritten as:(10)rb=∑i=1N(xi−μχ)(yi−μγ)∑i=1N(xi−μχ)2∑i=1N(yi−μγ)2Further, Eq. (10) can be simplified as (see Appendix A for the proof):(11)rb=ω1(t)ω2(t)(μ2(t)−μ1(t))σχOn the other hand, Otsu method describes the following criterion for selecting the optimum thresholdt⁎[13]:(12)t⁎=argmaxt{ω1(t)ω2(t)(μ2(t)−μ1(t))2}For a given gray level image χ,σχis a constant, andμ2(t)>μ1(t). Thus, maximizingrbis essentially identical to maximizingω1(t)ω2(t)(μ2(t)−μ1(t))2.The above analysis provides a novel interpretation of Otsu method from the perspective of maximizing image similarity: Otsu method is to search for a binary imageγ(t⁎)that is most similar to the image χ, and PCC is used for the similarity measure.Although Otsu method is proposed from the viewpoint of discriminant analysis (or class separability) [13], Section 3 shows that the selection criterion of optimum threshold of Otsu method follows the following maximum-image-similarity principle: searching for an optimum binary image as the output that is most similar to the original gray level image χ. Thus, from the perspective of maximizing image similarity, Otsu thresholding criterion can be rewritten as:(13)t⁎=argmaxt{S(χ,γ(t))}whereS(⋅,⋅)denotes the estimation of the similarity between χ andγ(t)with PCC measure.In fact, there are many other similarity measures besides PCC. In addition, the similarity estimation can be implemented not only between the images χ andγ(t), but also between their transformation results. Thus, Eq. (13) can be generalized as:(14)t⁎=argmaxt{S(T(χ),H(γ(t)))}whereT(⋅)andH(⋅)denote a transformation function, respectively, andS(⋅,⋅)still denotes the estimation of the similarity, but not limits to PCC measure.Eq. (14) is called an MST framework here, since it describes a thresholding framework from the perspective of maximizing image similarity.As an application of the MST framework, here we will introduce a new thresholding method by designingS(⋅,⋅),H(⋅), andT(⋅)directly. This new thresholding method is called MST hereinafter.In the MST method,S(⋅,⋅)still adopts the PCC measure.H(γ(t))is defined as removing the interior pixels of the objects in the binary imageγ(t), that is, setting a pixel to 0 if all its 4-connected neighbors are 1, thus leaving only the boundary pixels (see Figs. 1(b) and (d) for the graphic illustration ofH(γ(t))). Considering the real world images are often degraded by noise during the acquisition process,T(χ)is defined as a multiscale-gradient-multiplication (MGM) transformation [19] on the image χ (see Figs. 1(a) and (c) for the graphic illustration ofT(χ)):(15)T(χ)=∏i=1k‖∇G(x,y;σi)⁎χ‖whereG(x,y;σi)=12πσie−(x2+y2)2σi2, and the two symbols ∇ and ⁎ denote the derivation and convolution, respectively. The k space scales keep dyadic relationship (that isσk=2k−1σ1, k is a positive integer), as it will incorporate sufficiently edge information of different scales while reduce the correlation degree of the noise across scales [29–31].Mallat [32] illustrates mathematically that signal and noise have different singularities and edge structures present observable magnitudes along the scales, while noise decreases rapidly. Further, Zou et al. [19] show that the average gradient magnitude of the image will first decrease gradually then increase gradually along the scales due to the different singularities of signal and noise. Therefore, the maximal scaleσkin Eq. (15) will adopt the scale that the corresponding average gradient magnitude is minimal, in which the noise is suppressed to a great extent, and the average gradient magnitude of the edge pixels is greater than that of the noise. In addition, the minimal scaleσ1is assigned the value 0.25, as the half-width of the minimal discretized filters is 1 when the Gaussian function is truncated at the position of the 4 standard deviation [33]. The otherk−2filter scalesσ2,…,σk−1can also be directly obtained withσk=2k−1σ1.The proposed MST method is compared with MET method [12], Otsu method [13], Sezgin method [20], Valley-Emphasis method [18], Tsallis-Entropy method [34], MBT-Otsu method [35], and MBT-MET method [35]. MET method is reported to outperform 39 thresholding methods in the case of nondestructive testing images [24], whereas Sezgin method is reported to surpass MET method [20]. Sezgin method is an improved version of Otsu method, and it redefines the within-class variance of Otsu method by weighting the class means and the gray level probability [20].As shown in Section 6.2, on average, Tsallis-Entropy method [34], MBT-Otsu method [35], and MBT-MET method [35] are worst among all 8 compared methods in segmentation quality. Furthermore, both of Valley-Emphasis method [18] and Sezgin method [20] are the improved versions of Otsu method. However, Valley-Emphasis method [18] is inferior to Sezgin method [20]. Therefore, the experimental results of Tsallis-Entropy method [34], MBT-Otsu method [35], MBT-MET method [35], and Valley-Emphasis method [18] will not be shown in detail, when the detailed comparisons are implemented in the synthetic and real world images. Their results are only shown in Table 3 that reports the average segmentation quality of each method for all 86 real world images, and in Fig. 14 that illustrates the segmentation accuracy of each method on each real world image.The test images include not only 41 synthetic images, but also 86 real world images from different application areas, such as light microscope imaging, thermal imaging, laser imaging, etc. The 86 real world images contain small or big objects, and are noisy or smooth. The histograms of these images show various patterns, including not only simple bimodal shapes, but also more complex unimodal, comb-like, and some other irregular shapes. To verify the performance of all compared thresholding methods, the optimal thresholded image is manually created using visual inspection and used as a ground-truth image.Misclassification error (ME) measure [24] is adopted for the quantitative evaluation of the segmentation accuracy. The ME measure is widely used for the performance evaluation of image thresholding segmentation [24,36]. The ME measure reflects the percentage of background pixels wrongly classified into object, and conversely, object pixels wrongly assigned to background. For a two class segmentation problem, the ME measure can be simply expressed as:(16)ME=1−|Bgt∩Bt|+|Ogt∩Ot||Bgt|+|Ogt|whereBgtandOgtrepresent the background and object of the ground-truth image, respectively;BtandOtdenote the background and object in the segmented image by threshold t, and|⋅|denotes the cardinality of a set. The ME varies from 0 for a perfectly thresholded image to 1 for a totally wrongly thresholded image.Three experiments are implemented to test the robustness and effectiveness of the proposed MST method to three critical issues: (1) the sizes and the variances of the background and object; (2) the overlapped degree between the histograms of the background and object; and (3) the statistical model that the histograms of the background and object follow.In the first experiment, to estimate the influence of the sizes and the variances of the background and object on the proposed MST method, 18 test images representing Gaussian mixtures are produced by varying the proportion of background pixels and the gray level variances of the background and object classes. As examples, Fig. 2(a) shows 6 synthetic images, in which the proportions of the background pixels take value 0.5, 0.6, 0.7, 0.8, 0.9, and 0.99 from left to right, respectively. Figs. 2(b)–(c) show 6 noisy images corrupted by Gaussian white noise, and Figs. 2(d)–(e) show their gray level histograms, respectively. The quantitative results for the first experiment are reported in Fig. 3. In Fig. 3, the manual thresholding results are obtained by selecting interactively a threshold that minimizes the ME between the binary image and the ground-truth image. As can be seen, in all 18 test cases the ME values obtained by MST and MET methods are very close to the manual results. By contrast, Otsu and Sezgin methods are inferior to MST method, since their results are comparable to the manual ones only when the background and object are with comparable size and equal variance (see Figs. 3(a)–(b)). Otsu method performs worst for the case that the background class with larger size has a larger variance (see Figs. 3(c)–(d)); whereas, Sezgin method becomes worst for the case that the object class with smaller size has a larger variance (see Figs. 3(e)–(f)).As mentioned above, the second experiment aimed at assessing the sensitivity of MST method to the overlapped degree between the histograms of the background and object classes. Also in this case, 18 test images were generated to simulate Gaussian mixtures characterized by varying the gray level means of the object class and the gray level variances of the background and object classes. As examples, Fig. 4(a) shows 6 synthetic images, in which the gray levels of the objects take value 150, 160, 170, 180, 190, and 200 from left to right, respectively. Fig. 4(b) shows 6 noisy images by adding Gaussian white noise with mean 0 and normal variance 0.005 to the 6 images in Fig. 4(a), respectively. Figs. 4(c)–(d) show the gray level histograms of the first and last three noisy images in Fig. 4(b), respectively. The quantitative results for the second experiment are shown in Fig. 5. In general, the results point out that the proposed MST method and MET method perform better than Otsu and Sezgin methods (see Figs. 5(a), (c), and (e)). Specially, the proposed MST method is still robust even if the overlapped degree becomes strong (see Figs. 5(b), (d), and (f)), as its ME values are close to the manual results. In addition, when the overlapped degree is strong, for the case that the background and object classes have same variances, Otsu method performs worst (see Fig. 5(b)); for the case that the background class has a larger variance, the ME value of MET method is 0.89978, which indicates MET method fails to determine the reasonable threshold for this case (see Fig. 5(d)); and for the case that the object class has a larger variance, Sezgin method becomes worst (see Fig. 5(f)).The third experiment assesses the robustness of MST method to non-Gaussian statistical models that the histograms of the background and object follow. To carry out such a task, we produce 5 test images by simulating mixtures of 5 representative distribution models, including Rayleigh, extreme value, Beta, uniform, and comb-like distributions. The 5 test images are shown in Figs. 6–10(a), respectively, and their histograms are shown in Figs. 6–10(g). The thresholds and the corresponding ME values obtained by the different methods on the 5 test images are shown in Table 1. Again in this experiment, the results suggest that the proposed MST method performs best, since for all 5 test images, the ME values obtained by MST method are close to the manual results. Among the reference thresholding methods, Otsu method performs poorly for all 5 test images because of unbalanced class sizes and unequal class variances. MET method cannot perform as well as in the previous experiments, because the assumptions of Gaussian distributions of the background and object classes are not satisfied in the 5 test images. Specially, for the Beta, uniform, and comb-like distributions, MET method performs worst among all reference methods.The robustness of MST method to real world images is verified in this section by comparing with the reference methods on 86 test images. Due to the limited space, only the thresholding results of 3 representative images are analyzed in detail, and the other results can be found in the online supplementary material “Experimental results for 86 real world images”.Figs. 11(a)–(b) show a light microscope image (of size176×176pixels) containing 21 bacteria and the corresponding ground-truth image, respectively. The histogram composed of the bacteria looks like the left part of an extreme value distribution, whereas the histogram composed of the background appears a Gamma or Rayleigh distribution. The results in terms of thresholds and ME values obtained by 4 thresholding methods are reported in Table 2. In this case, MST method achieves a better segmentation result than other 3 reference methods. The threshold determined by MST method is 80, and its ME value is only 0.0002583. This promising result is confirmed by a visual comparison between the binary image obtained by MST method and the ground-truth image (see Figs. 11(b)–(c)). The thresholds selected by 3 reference thresholding methods bias severely toward the objects or background classes (see Fig. 11(g)). Specially, the worst result is provided by MET method, which totally failed to determine a reasonable threshold (see Figs. 11(d) and (g)), because the assumptions of Gaussian distributions of the background and object classes are not satisfied in this test image. In addition, the threshold determined by Otsu method biases toward the background class, because the background with larger size has a larger variance (see Figs. 11(e) and (g)).Figs. 12(a)–(b) show an infrared image (of size240×240pixels) and the corresponding ground-truth image, respectively. In Fig. 12(a), the interesting target is tiny, its proportion is less than 0.0004, and its gray levels take the values between 163 and 246. The results obtained by 4 thresholding methods are reported in Table 2. Also in this case, the best segmentation result is achieved by MST method. The threshold determined by MST method is 163, and its ME value is 0.000069. This promising result is confirmed by a visual comparison between the binary image obtained by MST method and the ground-truth image (see Figs. 12(b)–(c)). The thresholds selected by 3 reference thresholding methods bias severely toward the background class (see Fig. 12(g)), which result in the severe misclassification errors.Fig. 13(a) shows an image (of size128×128pixels) containing 5 coins on the dark background, and Fig. 13(b) is the corresponding ground-truth image. The histogram composed of the 5 coins looks like a uniform distribution, whereas the histogram of the background appears a comb-like distribution. The results obtained by 4 thresholding methods are reported in Table 2. Similarly to what is observed in the third synthetic images experiments (see previous Section 6.1), MET method performs worst among all reference methods (see Fig. 13(d)), its threshold is equal to 14, corresponding to ME value 0.3425. By contrast, the ME value of MST method is 0.000061, which suggests that MST method produces few misclassified pixels. This judgment can also be verified by a visual comparison between Figs. 13(b) and (c).The quantification comparisons of the segmentation results for 8 thresholding methods on all 86 real world images are shown in Fig. 14. As can be seen, the numbers of the ME value greater than 0.1 are 13, 28, 10, 19, 33, 55, and 53 for MET, Otsu, Sezgin, Valley-Emphasis, Tsallis-Entropy, MBT-Otsu, and MBT-MET methods, respectively. By contrast, the maximal ME value of the proposed MST method is less than 0.035, which suggests that MST method is more robust than the other 7 reference methods for the 86 real world images. This conclusion can be further confirmed by comparing the mean and standard deviation of the ME values of each method for all 86 real world images. The smaller the mean, the better the segmentation quality, and the smaller the standard deviation, the more stable the thresholding method. In Table 3, the average ME value of MST method is only 0.0046, which is far less than the average ME values of the other 7 reference methods. Furthermore, the smallest standard deviation also manifests that MST method has the more robust capability for all test images.As mentioned in Section 2, the symbol N denotes the total number of pixels of an input gray level image χ, and the symbol L is the total number of gray levels in the image χ. N and L define the input size of the image thresholding problem. In addition, the basic operation set includes arithmetic operations and comparisons.We now analyze the time complexity of the proposed MST method. The MST method mainly involves the computations of MGM transformationT(χ), boundary extraction transformationH(γ(t)), and similarity estimationS(⋅,⋅).T(χ)mainly involves the computations of the gradient magnitude images with different space scales. For space scaleσi, the window size of discretized first derivative of Gaussian (FDoG) filter is(8σi+1)×(8σi+1). Performing an(8σi+1)×(8σi+1)convolution operation on a pixel requires(8σi+1)×(8σi+1)multiplications and(8σi+1)×(8σi+1)−1additions. Thus, the time complexity of computing∇G(x,y;σi)⁎χisO(2((8σi+1)2+(8σi+1)2−1)N), where the reason for multiplying 2 lies in that each pixel requires performing convolution operation with x and y direction FDoG convolution kernel, respectively. Further, the gradient magnitude‖⋅‖of the whole image can be computed inO(4N). Thus, MGM transformationT(χ)=∏i=1k‖∇G(x,y;σi)⁎χ‖can be computed inO([∑i=1k(4(8σi+1)2N+2N)]+(k−1)N), whereO((k−1)N)is the computational complexity of multiplying the gradient magnitude images with different space scalesσieach other.The computations ofH(γ(t))andS(⋅,⋅)can be implemented synchronously. With given gray level t,H(γ(t))mainly involves 5 comparisons for each pixel in the image χ, i.e., comparing the current pixel and its 4-connected neighbors with the variable t in gray level. For each pixel in the image χ,S(⋅,⋅)mainly involves 2 additions, where one addition is used for computingω1(t)orω2(t), the other is used for computingμ1(t)orμ2(t). Then it is easy to understand that the computation complexities of the boundary extraction transformation and the similarity estimation of the whole image areO(5LN)andO(2LN), respectively.According to the above analysis, the time complexity of the proposed MST method isO([∑i=1k(4(8σi+1)2N+2N)]+(k−1)N+7LN). For MET method, Otsu method, Sezgin method, Valley-Emphasis method, Tsallis-Entropy method, MBT-Otsu method, and MBT-MET method, their time complexity can be obtained according to Refs. [12], [13], [20], [18], [34], and [35] respectively. Table 4summarizes the time complexities of 8 methods in the case of bilevel thresholding. According to Table 4, Otsu method runs fastest since its time complexity is lowest. Tsallis-Entropy method, MBT-Otsu method, MBT-MET method, MET method, Sezgin method have approximately equal execution efficiency, as they have the same order ofL2and N. The proposed MST method has higher computational complexity compared to the reference methods. Its main computational cost arises from the convolution operation, boundary extraction transformation, and similarity estimation.The proposed MST method can be easily generalized to the multilevel thresholding case. Its multilevel version can be formalized as:(17)tset⁎=argLocmaxt{S(T(χ),H(γ(t)))}where “Locmax” denotes a local maximum function, andtset⁎is a set including all possible thresholds. For each elementti⁎intset⁎,S(T(χ),H(γ(ti⁎)))should be a local maximum. Figs. 15(g) and 16(f)show two examples that are helpful to understand the meanings of expression (17).To demonstrate the robustness and effectiveness of multilevel MST method, the experiments have been implemented on many test images, including synthetic images and real world images from different application areas. Due to the limited space, only the thresholding results of 2 representative images are analyzed in detail. In addition, 3 recent multilevel thresholding methods, i.e. fast statistical recursive (FSR) method [37], semi-automated statistical type-one (SAS1) method [38], and semi-automated statistical type-two (SAS2) method [38], are compared with the proposed method.Fig. 15 shows the segmentation results of 4 compared methods on a synthetic image with 4 classes. The corresponding ground-truth image is easily obtained and shown in Fig. 15(b). Compared with ground-truth image, the segmentation result of MST method is superior to 3 reference algorithms (see Figs. 15(b)–(f)). Furthermore, Fig. 15(g) shows the similarity curve produced by the MST method, and 3 vertical lines indicate 3 thresholds obtained by the MST method. It is observed that each threshold almost locates at the histogram valley between two classes, which illustrates the reason why the segmentation quality of the MST method is close to the ground-truth image.In view of parameter characteristics of SAS1 and SAS2 [38], the in-depth comparisons are implemented by adjusting their parameters with manual intervention. Some more detailed results are shown in Table 5. In Table 5, 3 thresholds obtained by the MST method are 63, 137, and 213, respectively. They are close to 3 thresholds, i.e. 64, 140, and 219, obtained by the manual method. All 3 thresholds of FSR method are less than 64. As a result, 3 different objects, i.e. leaf, rabbit, and human, are misclassified as same class (see Fig. 15(d)). SAS1 and SAS2 seem to be promising methods when they are used for object separation in color images [38]. However, for this noisy test image, it seems to be difficult for them to obtain 3 thresholds approaching the 3 reference thresholds obtained by the manual method (see Table 5).A real world image is also used for testing the performance of 4 compared methods (see Fig. 16). Although the ground-truth image is not provided, it is still easy to judge the segmentation qualities of different methods with the visual estimation. In Fig. 16(b), each of possible objects is reasonably detected. The good result obtained by the MST method can be illustrated with Fig. 16(f). In Fig. 16(f), 5 vertical lines indicate 5 thresholds obtained by the MST method. It is observed that each threshold almost locates at the flat valley between two peaks. However, for FSR, SAS1 and SAS2 methods, there are often two or four objects misclassified as other classes. Some more detailed results are shown in Table 6. In Table 6, FSR and SAS1 methods show obvious instability, that is,ti−1is greater thanti(see boldface letter in Table 6).Two main contributions are made in this paper. Firstly, this paper reveals the relationship between PCC measure and Otsu method, which provides a novel interpretation of Otsu method from the perspective of maximizing image similarity. Secondly, as a natural result of the maximum-image-similarity concept, this paper constructs a flexible MST framework that facilitates the investigation of new image thresholding methods with the image similarity theories. Specially, a novel MST method is directly designed according to this framework.To assess the robustness and effectiveness of the proposed MST method, 41 synthetic images and 86 real world images are considered in the bilevel thresholding experiments. In addition, 7 thresholding methods widely used in the literature are adopted for the purpose of bilevel thresholding comparison. The experimental results show the following several main promising properties of MST method: (1) it produces the high-accuracy thresholding results (in average, it is more effective than the other 7 reference thresholding methods, and it obtains results that are very close to those yielded by the manual supervised thresholding); (2) it can deal with the special cases characterized by strongly unbalanced class sizes; (3) it is robust to thresholding the images characterized by the Gaussian as well as non-Gaussian mixture distributions. Except for the above advantages, our analysis results show that, in the case of bilevel thresholding, the proposed method has a higher time complexity than the compared approaches. In the future, as a development of this work, we will optimize the proposed method to reduce its time complexity.The MST framework provides a flexible mechanism to investigate new image thresholding methods, since we can directly design the three functionsS(⋅,⋅),H(⋅), andT(⋅)of this framework. Besides PCC measure, there are many other image similarity (or dissimilarity) measures, such as weighted distance [39], Hausdorff distance [40], Kullback–Leibler divergence [41], and mutual information [42], etc. Their applications to MST framework for developing new image thresholding methods will be considered in our future works. In addition,T(⋅)andH(⋅)may be designed to other functions with the feature-oriented transformation methodology.Recently, some active investigations have explored to combine respective advantages of global thresholding and local thresholding methods. For example, locally adaptive block thresholding (LABT) method proposed by S. Hemachander and P.K. Panigrahi acts as a hybrid between known local and global methods [43]. LABT utilizes the thresholds of neighboring sub-images to calculate a range of values. The sub-image is thresholded with the pixel values within the above range, which ensure that the transitions between sub-windows are maintained continuously. In the future, we will explore the possible strategies that integrate effectively MST's robustness to various histogram patterns and LABT's capability of maintaining image continuity, and finally extend the proposed MST method to the local thresholding case.

@&#CONCLUSIONS@&#
