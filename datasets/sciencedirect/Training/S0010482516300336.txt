@&#MAIN-TITLE@&#
A neural algorithm for the non-uniform and adaptive sampling of biomedical data

@&#HIGHLIGHTS@&#
Algorithm to under-sample data with a frequency lower than Nyquist limit.An adaptive neural predictor selects the subsequent samples to be measured.Example applications: EMG, ECG, EEG and acceleration data.The method outperforms uniform sampling and compressive sensing.Many potential applications to save energy and to reduce the memory storage.

@&#KEYPHRASES@&#
Electromyography (EMG),Electrocardiography (ECG),Electroencephalography (EEG),Accelerometer,Non-uniform sampling,Compressive sensing,Nyquist limit,

@&#ABSTRACT@&#
Graphical abstractThe paper proposes an innovative real time method to under-sample non-stationary data, providing applications to four different biomedical signals: electromyogram (EMG), electrocardiogram (ECG), electroencephalogram (EEG) and body acceleration. An example of application is shown in Figure 1, where an EMG is sampled under the Nyquist limit, adapting the sampling schedule to the data. The sampling rate is automatically increased during bursts of activity of the muscle and lowered when when the EMG has small amplitude (reflecting cross-talk from nearby muscles or noise). The Power Spectral Density (PSD) is fairly well represented by the adaptive under-sampling till the highest frequency components, even under a drastic reduction of the number of samples.The method outperforms both a uniform under-sampling and compressive sensing (CS) applied to the same data with the same compression ratio. The algorithm opens interesting perspectives for potential applications in body sensor networks (which are finding increasing applications, e.g. in self-monitoring and for the surveillance of sensitive people). Specifically, it could allow to lower the number of wireless communications (saving sensor power) and to reduce the occupation of memory.fx1

@&#INTRODUCTION@&#
There are applications in which sampling at the Nyquist frequency is not efficient. For example, sparse signals are decomposed using a few significant components in compressive sensing (CS) [1]. Reducing the sampling rate is useful when many body sensors are used to monitor continuously the lifestyle of a healthy person or the condition of sensitive people [2]. Sensors are lightweight, non-invasive, wearable or embedded in cloth and they include a wireless communication with a storage and decision making system. Many physiological data can be sensed, e.g., acceleration, bioelectric activity, blood pressure, galvanic skin response, and breathing [3]. Monitoring these data supports the individual self-assessment which allows to develop a personalized health care that helps healthy people to maintain their well-being [4]. Moreover, many diseases can benefit from a continuous monitoring, like cardiovascular problems, diabetes, Alzheimer׳s and Parkinson׳s diseases, renal failure, chronic obstructive pulmonary disease, post-operative conditions, stress or sudden infant death syndrome [5]. The remote patient monitoring could allow a rapid intervention when needed, developing an individualized care [6], with positive effects on the management of clinical services and on the quality of life of patients [7]. Many additional applications of body sensors are found, e.g., in military monitoring, interactive gaming [8], recognition of dietary activity [9], rehabilitation [10], personal information sharing and secure authentication [11].Some recorded signals can show burst activity, reflecting the alternation of periods in which the investigated physiological system is either silent or active: for example, surface electromyogram (EMG) during cyclic tasks [12] or in pathological conditions (e.g., motor tremor induced by epileptic seizures [13]); electrocardiogram (ECG), with the QRS complex including most of the high frequency content [14] (unless pathological behaviors arise [15]); electroencephalogram (EEG), when the brain is performing different tasks or in pathological conditions (e.g., seizure in epileptic patients [16]); body acceleration during different activities [18] or in pathological conditions [19].When data contain bursts, a high sampling rate is required if a uniform sampling is adopted, even if there are portions of the signal which could be down-sampled without loss of information. A non-uniform sampling, using a high sample rate only during the bursts, would allow both memory and energy saving (very important for wireless sensors, which are supplied with scarce resources [20]).A predetermined non-uniform sampling schedule cannot be established for biomedical applications, but the sampling should be adaptively defined on the basis of the data. Adaptive techniques, like AZTEC, CORTES, SLOPE, or Fan [21], have been developed specifically for ECG data compression. They can reduce considerably the number of acquired samples by a non-uniform sampling schedule, but they do not allow a real time modulation of the sampling frequency, as they require to measure each sample in order to decide if to keep it. A real time adaptive solution was proposed in [14], increasing the sampling rate when the signal curvature was high. The method showed higher performances than uniform sampling, but a-priori knowledge on the signal was required, limiting versatility [22]. In this respect, CS is considered more generally applicable [22], showing low compression ratio (CR, ratio between number of acquired versus original samples) and good accuracy [23,24]. However, it recovers the signal by an offline procedure applied on time epochs, introducing a delay. On the other hand, a real time adaptive sampling schedule could allow to save energy (to sample and transmit data) and to implement simple decision making control even on the sensor (e.g., a fall detector based on a threshold on the body acceleration). Thus, a versatile adaptive sampling algorithm could provide an important contribution.The method proposed in [20] increased automatically the sampling rate when the investigated signal became unpredictable or provided high frequency contributions. A conceptual framework was discussed in [20], showing different general applications, but without optimizing the algorithm on specific data and the under-sampling was not measured relative to Nyquist frequency (problem affecting also the literature on CS applied to biomedical data [1], where approximation errors at specific CR are often provided without caring about the possible over-sampling of the original signal; see Discussion for details). The present work investigates these open issues. The algorithm proposed in [20] is improved, by an automatic tuning on the data, based on an offline analysis of a training set. The adaptive algorithm is compared with uniform sampling and CS with the same CR, when applied to different biomedical signals sampled at the Nyquist frequency: EMG, ECG, EEG and body acceleration.The adaptive sampling is based on a prediction algorithm and on its application to estimate the uncertainty of a predicted sample. It can be split into three parts:1.selection of optimal predictors (based on the theory of time series embedding, Section 2.1.1);training of an adaptive algorithm to predict the next sample (a multi-layer perceptron, MLP, was used, Section 2.1.2);real time schedule of the sampling rate (based on the uncertainty of the prediction, Section 2.1.3).Embedding theory was applied to the data [25–27]. Specifically, the time series were supposed to be extracted from a deterministic physiological system described by a set of unknown deterministic rules(1)dx→dt=F→(x→)wherex→is the vector of state variables of the system andF→is a set of functions called the vector field (defining the evolution of the state variables), which was assumed not to be an explicit function of time (i.e., the system was assumed to be autonomous). The recorded time seriesy(t)(wheretfrom now on is a discrete time variable) was assumed to be extracted from the system through a measurement process described by an unknown functiong(⋅)of the state variables:(2)y(t)=g(x→(t))Given a single measurement, a vector of time delayed versions (delayed coordinates) was built [25](3)Y→(t)=[y(t)y(t−τ)⋮y(t−(m−1)τ)]where the time delayτwas chosen so that two delayed coordinates provided different information and the number m of elements of the vector is called the embedding dimension (as it is the dimension of the so called phase space in which the trajectoryY→(t)is embedded) [25–27].The time delayτand the embedding dimension m were computed as follows.•Time delay. The mutual information of the original and delayed data was computed:(4)MI(τ)=∫A∫BPAB(a,b)ln(PAB(a,b)PA(a)PB(b))dadbwhere the time seriesy(t)andy(t−τ)are considered as random variables A and B, respectively, with joint probability densityPAB(a,b)and marginal probabilitiesPA(a)andPB(b), respectively. The minimum between the delays corresponding to the first local minimum or to a 90% decrease ofMI(τ)was selected as the time delayτof the delayed coordinates.Embedding dimension. Cao׳s method was used [27,28]. It is based on the number of points of the trajectory, described by the vector in (3), which are neighbors of other points of the trajectory itself. When increasing the embedding dimension by adding one element to the vector (3), neighboring points which were close only due to the projection of the trajectory in a low dimensional space (false near neighbors) may turn away. Thus, the number of neighboring points decreases by increasing the embedding dimension, till false neighbors are present. The minimum phase space dimension allowing to remove the false near neighbors was selected as the embedding dimension m: it allows to identify uniquely the dynamics of the trajectory and possibly to predict it. Specifically, Cao׳s method investigates the following function of the embedding dimension [28](5)E1(m)=E(m+1)E(m),whereE(m)=1N−mτ∑i=1N−mτ‖Ym+1(i)−Ym+1(n(i,m))‖‖Ym(i)−Ym(n(i,m))‖where N is the number of considered samples of the time series,‖⋅‖is the absolute distance norm,Ym(i)is the ith sample of the reconstructed vector with embedding dimension m andn(i,m)indicates its nearest neighbor in the m-dimensional reconstructed phase space. The functionE1(m)saturates when all false near neighbors are removed. Thus, such a function has a point of maximum curvature (corresponding to the correct embedding dimension m), separating a region of increase from a plateau. Such a point was estimated automatically, considering the best approximation ofE1(m)by 2 lines [27].The delayτand the embedding dimension m were selected considering only a portion of data (the training set defined in Section 2.1.2).An MLP forecasted the subsequent sample of the time series with a constant interval using the delayed coordinates as inputs. Thus, given the time delayτand the embedding dimension m (estimated as described in Section 2.1.1), in order to predict the value of the sample number (i+1) of the time series, the MLP used as inputs the m valuesy(i),y(i−τ),…,y(i−(m−1)τ).Different MLPs were investigated, from which the optimal one was chosen as that with best generalization performances. The following procedure was adopted to select such an optimal MLP. The data were split into training, validation and test sets (50%, 25% and 25% of data, respectively). MLPs with a single hidden layer were used (notice that a single hidden layer is sufficient to approximate any nonlinear function [29]). Sigmoidal activation functions were used for the hidden neurons. Their number was chosen in the range 10−50. The output neuron had a linear activation function. The MLPs were trained on the training data, applying the quasi-Newton algorithm [30] for a number of iterations in the range of 25−350. The optimal MLP was selected choosing the topology (i.e., the number of hidden neurons) and the parameters (i.e., the synaptic weights and bias, after a specific number of iterations of the optimization algorithm) with best performances on the validation set (measured in terms of the mean square error).The sampling schedule was defined by selecting which sample to measure next, on the basis of the uncertainty of its prediction obtained by the optimal MLP described in Section 2.1.2. The algorithm was applied on the test set. The prediction and an estimate of its uncertainty were performed for each time sample, using the available (sampled or predicted) data [20]. All delayed coordinates (Section 2.1.1) used as inputs of the neural predictor (i.e., the optimal MLP, Section 2.1.2) were characterized by their (predicted or measured) values and uncertainties.The prediction of a new value was obtained as follows: 1000 random input data were simulated, using for each data a uniform distribution centered around its value and with a range given by its uncertainty; the neural predictor was run for each of these 1000 inputs; the prediction was the median of the distribution of the obtained estimates.The uncertainty of a data was chosen distinguishing between two cases, i.e., when the data was measured or predicted. The uncertainty of a measurement was defined by the user, considering the accuracy of the sensor and the expected noise level. The uncertainty of a predicted sample was defined as the mean of two terms (see [20] for details):1.the range of predicted values obtained from the 1000 random tests indicated above (excluding 10 possible outliers);the estimated prediction error obtained integrating in time the rate of prediction error, defined as the ratio between the estimation error (available when a new sample was measured and defined as the absolute difference between the measured and the predicted values) divided by the time delay from the last acquired sample; a memory term was also included, by computing the weighted average of the last two rates of prediction error (with weights 65% and 35%, respectively); finally, a saturation was considered, by imposing a minimum CR.An additional measurement was required from a sensor when the associated uncertainty overcame a threshold, chosen by the user. This free parameter and the minimum CR mentioned above allow the user to tune the level of under-sampling to the application at hand.Different experimental signals were used to test the algorithm. The first 3 signals were provided by different institutions, acknowledged at the end of the paper, whereas the last one was acquired by the author (in accordance with the Declaration of Helsinki).1.Surface EMG was recorded from the tibialis anterior muscle of a healthy subject walking on a treadmill at a self-selected speed (bipolar electrodes placed on the muscle belly; ground electrode on the right patella; sampling rate 2048Hz). The experiment lasted about 20s. The signal had a bandwidth of about 400Hz. It was low-pass filtered at 400Hz offline (4th order anti-causal, anti-aliasing Butterworth filter) and resampled at 800Hz, in order to be at the Nyquist limit.ECG was recorded from the wrists of a healthy subject [31]. The experiment lasted about 75s. The signal was sampled at 2kHz, but its bandwidth was about 30Hz. Thus, it was resampled at 60Hz (as the EMG described above).A scalp EEG was recorded from an epileptic patient for about 50min. The channel O2 was selected (with reference on the earlobe). The EEG was sampled at 1024Hz (Brain Explorer, EB-Neuro© amplifier), but the signal (after removing the frequency components under 0.5Hz) had more than 95% of energy under 14Hz: it was down-sampled to 32Hz (notice that with this preliminary down-sampling, only theta, delta and alpha rhythms are available; however, for this representative application, the main objective is identifying the epileptic seizures).Acceleration data were recorded for 5min from a subject keeping fixed in a bag a Tablet embedding the 3 axial accelerometer LSM330DLC. Quite standing, walking and jumping were the investigated activities. The signal was sampled at 50Hz. Its bandwidth was about 15Hz. It was resampled at 32Hz.The algorithm was applied to the test data (i.e., a portion of the signals described in Section 2.2, as indicated in Section 2.1.2). Different thresholds were used, obtaining different CRs. Then, data were uniformly down-sampled with the same CRs, by cubic interpolation. From the two time series, the samples of the original signal were estimated by cubic interpolation. The fit was assessed in terms of the averaged rectified error (ARE) and the percentage root mean squared difference (PRD)(6)ARE=100∑i|xi−x^i|∑i|xi|(%)PRD=100∑i|xi−x^i|2∑i|xi|2(%)wherexiandx^iare the original and approximated data, respectively. The power spectral densities (PSD) of the three time series (original and estimated from either of the two under-sampled signals) were also compared.Some tests were also performed using CS, with the principal components of the data as basis functions [32]. Specifically, principal components were computed from all adjacent epochs extracted from the training and validation sets. The duration of the epochs was chosen as that providing maximal accuracy on the test set (1s for all considered signals except for EMG, for which epochs of 50 ms were used).The down-sampling was written as the multiplication of the data with a sensing matrix. Such a matrix was chosen using one of the following criteria: 1) a random selection of a percentage of the rows of the identity matrix (tests were performed also choosing a sensing matrix with entries equal to either 1 or 0, drawn from a binomial distribution with probability of generating 1 equal to 0.6, as suggested in [23]; equivalent results were obtained, so that this possibility is not further considered in the following); 2) selecting the rows of the identity matrix corresponding to the samples chosen by the adaptive algorithm. The coefficients were estimated minimizing their L1 norm, imposing their sparseness [33]. The original signal was finally recovered as a sum of the basis functions weighted by the estimated coefficients.

@&#CONCLUSIONS@&#
This paper discusses a real time algorithm that schedules a non-uniform sampling adapted to the measured data, that reduces the number of acquired samples, but still preserving high frequency information, going beyond the Nyquist limit. The tests on experiments are promising (Figs. 2 and 3), showing that the adaptive algorithm outperforms both uniform under-sampling and CS using the same CRs.If applied in wireless sensor networks, the method lowers the required data transmissions, saving energy and allowing an efficient management of the resources. Moreover, it could be integrated with other techniques, for a further compression or a more accurate recovery of the original data. Many potential future investigations are expected, e.g., for the individual self-assessment of healthy people or the remote monitoring of patients.The author states no conflict of interest.There was no source of funding for this research. Ethical approval was given by the local committee for the acquisition of acceleration data.