@&#MAIN-TITLE@&#
Genetic algorithms for condition-based maintenance optimization under uncertainty

@&#HIGHLIGHTS@&#
We want to optimize a CBM policy when the maintenance model parameters are uncertain.The uncertainty is Monte-Carlo propagated onto the performance indicators (unavailability and cost).The objective functions are Cumulative Distribution Functions.An extension of Multi-Objective Genetic Algorithms is proposed to optimize maintenance.The technique is compared to a CVar based MOGA approach.

@&#KEYPHRASES@&#
Maintenance optimization,Genetic algorithms,Uncertain fitness,Ranking,Pareto dominance,

@&#ABSTRACT@&#
This paper proposes and compares different techniques for maintenance optimization based on Genetic Algorithms (GAs), when the parameters of the maintenance model are affected by uncertainty and the fitness values are represented by Cumulative Distribution Functions (CDFs). The main issues addressed to tackle this problem are the development of a method to rank the uncertain fitness values, and the definition of a novel Pareto dominance concept. The GA-based methods are applied to a practical case study concerning the setting of a condition-based maintenance policy on the degrading nozzles of a gas turbine operated in an energy production plant.

@&#INTRODUCTION@&#
Multi-state degradation modeling has recently received considerable attention in the domain of reliability and maintenance engineering (Baraldi et al., 2013; Baraldi, Compare, & Zio, 2013a, 2013b, 2013c; Lisnianski, Frenkel, & Ding, 2014; Moghaddass & Zuo, 2011, 2012), as it pragmatically allows setting advanced maintenance paradigms such as Condition-Based Maintenance (CBM) and predictive maintenance (Zio & Compare, 2013a, 2013b). In practice, the parameters governing the stochastic transitions among the states of these models are first estimated, based on the available data; then, the degradation model is embedded into the maintenance model to estimate the performance indicators of interest (e.g., unavailability (Lins & Droguett, 2011; Moura, Lins, Droguett, Soares, & Pascual, 2015), profitability (Alsyouf, 2007), quality in production (Ben-Daya & Duffaa, 1995), total costs, risk (Furtado et al., 2012), etc.): this model is at the basis of the optimization algorithm that identifies the set of optimal maintenance settings among which the decision maker selects the preferred solution (e.g., Kolowrocki & Soszynska-Budny, 2013; Lisnianski & Levitin, 2003).On the other side, the correct processing of the uncertainty in the maintenance models is emerging to be a crucial issue for the proper decision on the preferred maintenance solution to apply in practice without ‘surprises’. This importance is witnessed by the large amount of literature produced on this topic (e.g., Baraldi, Compare, & Zio, 2012, 2014, in press; Baraldi, Compare, & Zio, 2013a, 2013b, 2013c; Giorgio, Guida, & Pulcini, 2011; Moghaddass & Zuo, 2011, 2012). In other words, the maintenance models have to take into account the uncertainties affecting their parameters, which usually come from limited evidence available in the field. Such epistemic uncertainty (i.e., due to insufficient knowledge) needs to be propagated together with the aleatory uncertainty (i.e., due to the inherent stochastic nature of the degradation and failure phenomena), through the maintenance model, onto the considered (maintenance) performance indicators.Several theoretical frameworks and computational methods have been developed to incorporate imprecise parameters into Markov or semi-Markov multi-state degradation models, when the imprecision is represented by interval probabilities (De Cooman, Hermans, & Quaeghebeur, 2009; Kozine & Utkin, 2002; Rocco, 2011; Škulj, 2009), fuzzy stets (Ge & Asgarpoor, 2010), possibility distributions (Baraldi, Compare, & Zio, 2013a, 2013b, 2013c, 2014) and probability assignments (Baraldi et al., 2012; Baraldi, Zio, & Compare, 2009). However, the problem (undoubtedly difficult: Eskandari, Geiger, & Bird, 2007) of how to optimize maintenance in the setting where the epistemic and aleatory uncertainties in the embedded degradation model lead to uncertain objective functions (e.g., unavailability, cost, etc.), has not received the necessary attention. In fact, as pointed out in Eskandari et al. (2007) and in Petrone, Axerio-Cilies, Quagliarella, & Iaccarino (2013), few approaches have been propounded in the literature to effectively tackle such multi-objective optimization problems in the presence of uncertain objective functions. These works consider different frameworks for uncertainty representation: probability distributions in Eskandari et al. (2007), Hughes (2001), Martorell, Sanchez, and Carlos (2007), Petrone et al. (2013), Sanchez, Carlos, Martorell, and Villanueva (2009), Villanueva, Sanchez, Carlos, and Martorell (2008) fuzzy sets in Li and Kwan (2003) and Trebi-Ollennu and White (1997), and plausibility and belief functions in Compare and Zio (2015) and Limbourg (2005).In this context, the authors have proposed methodologies in the framework of Possibility Theory (PT, Baraldi, Compare, & Zio, 2013a, 2013b, 2013c, 2014) and Dempster–Shafer Theory of Evidence (DSTE, Baraldi et al., 2012; Eskandari et al., 2007) to represent and propagate the uncertainty in multi-state degradation maintenance models, and also to optimize the CBM policy based on the model outputs, which are pairs of plausibility and belief functions. Now, the aim of the present work is to propose an extension of Multi-Objective Genetic Algorithms (MOGA, Compare & Zio, 2015; Konak, Coit, & Smith, 2006; Lisnianski & Levitin, 2003; Marseguerra, Zio, & Martorell, 2006; Mitchell, 1996) to tackle the maintenance optimization issue when the epistemic uncertainty in the degradation model is represented in the probability theory framework. Namely, the parameters of the stochastic model of the degradation mechanisms are supposed to be Maximum Likelihood (ML)-estimated and the uncertainties in these estimations are represented by probability distributions (Baraldi, Compare, Despujols, & Zio, 2011; Johnson & Wichern, 2007; Papoulis & Pillai, 2002). A double-loop Monte-Carlo approach (Zio, 2013) is used to propagate the uncertainties from the model parameters onto the considered performance indicators (i.e., the objective functions of the optimization, which represent fitness values of the solutions), which turn out to be probability distributions.The development of a technique to rank the uncertain fitness values and the generalization of the Pareto dominance concept are the two fundamental issues to address in order to extend the application of MOGA to the case of uncertain objective functions, being the evolutionary schemes well-established frameworks in which these innovations have to be embedded. In this work, we propose the pairwise ranking technique propounded in Baraldi et al. (2009) and Compare and Zio (2015), and a Pareto dominance concept based on the ranks of the solutions. These concepts are combined with the NSGA-II elitist technique (Deb et al., 2002), which relies on the Pareto dominance to effectively divide the evolving populations into non-dominated fronts of different ranks, thus allowing for a significant reduction in the computational times.Finally, the results provided by this technique are compared to those of the CVaR approach (Mena, Hennebel, Fu, Ruiz, & Zio, 2014; Sarykalin, Fraud, Serraino, & Uryasev, 2008), which condenses the uncertainties in the objective functions into crisp values, representative of the associated risk. The comparison of these techniques and of their performances constitutes an additional original contribution of the paper.The remainder of the paper is organized as follows. Section 2 describes the ranking criterion and the Pareto dominance definition; Section 3 takes a glance at the GA advancements considered in this work (named extended NSGA-II, hybrid NSGA-II and CVaR measure). A practical case study is introduced in Section 4, which concerns the optimization of a CBM policy on the nozzle system of gas turbines. The results of the application of the proposed methodologies to the case study are shown and discussed in Section 5. Concluding remarks are given in Section 6.In the uncertainty setting considered in this work, finding the optimal set that minimizes the objective functions (e.g., unavailability and cost) requires developing a method to establish a relation order among two probability distributions. To this aim, we consider the algorithm proposed in Baraldi et al. (2009), which is briefly recalled in this section.Let us consider two generic random variables A and B. To establish which is the largest, we consider the random variable ΔAB= A − B. Then, the probability that A is larger than B, referred to as “exceedance measure”, is given byrAB=1−FΔAB(0), whereFΔABis the Cumulative Distribution Function (CDF) of ΔAB.The relationship between A and B is obtained by comparing rABto a threshold range [Tl, 1 − Tl], symmetric around 0.5, and considering the following criteria:•If rAB≥ 1 − Tl, then A is larger than B.If rAB≤ Tl, then B is larger than A.If Tl≤ rAB≤ 1 − Tl, then A is equal to B.This method is similar to that proposed in Fan, Liu, and Feng (2010), which also relies on the pairwise comparisons of the probability distributions. However, the solution proposed to solve multi-criteria decision problems is completely different from that proposed in this work.For example, Fig. 1 shows the Probability Density Functions (PDFs) of two variables A and B, (left) and the corresponding CDFs (center). The results of the application of the sorting algorithm are shown in Fig. 1 (right): the CDF of ΔABindicates that the exceedance measurerAB= P(A > B) = 1 − FAB(0) = 0.8; this allows concluding that A > B.Fig. 1 also highlights the drawback of the procedures to sort the probability distributions, which rely on points summarizing the distributions (e.g., the approach proposed in Coit, Jin, & Wattanapongsakorn, 2004). Namely, the expected value of A, (i.e., the middle point of the distribution) E[A], is larger than that of B, E[B]. On the other hand, the 90th percentile of A is smaller than the 90th percentile of B. This entails that if one were to perform the ranking based on the 90th percentile values the conclusion would be that B is larger than A, contrarily to what would be happened if the ranking were based on the expected values.Fig. 1 allows us also underlining the difference between the proposed ranking method and the classical definition of stochastic ordering (the so called ‘usual’ stochastic order, e.g., Lehmann, 1955; Shaked & Shanthikumar, 1994): B is smaller than A ifFA(x)≤FB(x),∀x∈R. Namely, the CDFs plotted in Fig. 1 (center) intersect, and the condition for having one curve dominating the other is not fulfilled. Thus, A and B need to be considered as ‘equal’, although P(A > B) = 0.8. On the contrary, Fig. 2shows the situation where A ≈ N(μ = 2.1, σ = 1) is larger than B ≈ N(μ = 2, σ = 1) in the usual stochastic order, although the probability that A > B is very poor (A−B≈N(μ=0.1,σ=22), and P(A > B) = 0.53).The same considerations hold when the third degree stochastic dominance is applied (Whitmore, 1970): one can establish an order relation between A and B even if the evidence that A is larger than B is very small. From these considerations, it seems fair to say that the ranking methodology proposed in Baraldi et al. (2009) is more capable of capturing the information contained in the CDFs.Finally notice that a ranking technique has been proposed in Petrone et al. (2013) in support to GA, which establishes the order relation based on the comparison of the solutions with an ideal, pre-fixed Dirac delta distribution. It is worth noticing that when the reference Dirac delta is positioned at zero, then the method proposed in Petrone et al. (2013) reduces to sorting the distributions based on their mean values. Then, also in this case the CDF sorting is based on points summarizing the CDFs, thus losing the information they encode.Baraldi et al. (2009) also pointed out that there may be cases in which the pairwise comparisons of three generic random variables A, B and C lead to A > B and B > C, but C > A. This is a ‘contradictory’ ranking, as the transitive property does not hold. However, it has been proven in Baraldi et al. (2013a, 2013b, 2013c) that by setting Tlsmaller than 1/3, such contradictory ranking is avoided and, at most, it can happen that A > B, B > C, and C = A. In this case, the three uncertain variables are considered equivalent.Notice that the loss of the transitive property for similarity measures is a well-known issue, whose raising dates back to the 1950s of the last century (Menger, 1951): Poincaré emphasized that in the observable physical continuum, ``equal'' means ``indistinguishable,'' and A = B and B = C do not imply A = C (i.e., physical equality is a non-transitive relation). These considerations have been formally modeled in a number of works (e.g., Greco, Matarazzo, & Slowinski, 2001; Luce, 1956). Nonetheless, the problem of the ‘contradictory’ ranking did not emerge in the works of the literature that propose extensions of GA to treat noisy fitness values (e.g., Eskandari et al., 2007; Hughes, 2001).When sorting of elements concerns multiple attributes, the concept of Pareto dominance is introduced. Assume that there are Q objectives Θ1, …, ΘQto be minimized; then, a feasible solution, say Xs, is dominated by another feasible solution, say Xj, if for all q = 1, …, Q the corresponding values of the objectives,ΘsqandΘjq, are such that (Konak et al., 2006; Marseguerra et al., 2006):1.Θsq≤Θjq,∀q=1,…,Q,andΘsq<Θjqat least for one q = 1, …, Q.A solution is said to be Pareto optimal if it is not dominated by any other solution in the solution space. This means that a Pareto optimal solution cannot be improved with respect to any objective without worsening at least one other objective. The set of all feasible non-dominated solutions is referred to as the Pareto optimal set; the objective function values corresponding to a given Pareto optimal set form the Pareto front in the objective space.When the exceedance-based sorting method described above is used to establish ifΘsq≤Θjq, for any q = 1, …, Q, the given definition of Pareto dominance needs to be modified (Compare & Zio, 2015). To prove this, for the sake of simplicity we can refer to the case ofQ = 2. On one side, one may have that the exceedance measurersj1=P(Θs1>Θj1)≥1−TlandTl≤rsj2=P(Θs2>Θj2)≤1−Tl, which leads to conclude thatXs≻Xj; on the other side, one may have also thatrsj1≥1−Tl,rjg1≥1−Tland1−Tl≥rsg1≥Tl(i.e., Xs, Xjand Xgare of the same rank with respect to the second objective): this means thatΘs1is equivalent toΘj1, with respect to objective 1, and thus Xsdoes not dominateXj.To overcome this issue, the sorting algorithm shown in Baraldi et al. (2013a, 2013b, 2013c) is first applied to every objectiveq = 1, …, Q. This algorithm exploits the ranking criterion described above and assigns the same ranking position to all the solutions that are equivalent with respect to objective q. For example, if we have H = 4 solutions and the second and third solutions are equivalent with respect to objective q, then the final ranking is 1, 2, 2, 4.Then, the following definition of Pareto dominance is introduced to identify the Pareto front:Xs≻Xjifρsq≤ρjqforallq=1,…,Qand,ρsq<ρjqforatleastoneq=1,…,Q.whereρsqandρjqare the ranking positions of the solutions Xsand Xjwith reference to objective q, respectively.Finally, notice that the approach proposed in Petrone et al. (2013) to implement the NSGA-II algorithm also relies on the ranking positions of the solutions in a population. However, the definition of the rank in Petrone et al. (2013) is different from that given in this work. Rather, it resembles the ranking method proposed in Modarres (2006), which applies the Monte Carlo sampling method to estimate, for every solution Xh, the probabilities of occupying the H positions in the ranking. The final ranking of the solution Xhis the average value of its ranking positions.GA is the most commonly known evolutionary algorithm for optimization, which uses techniques inspired by natural evolution to allow a population of solutions (also called, individuals), candidates to solve the (multiobjective) optimization problem, to evolve, i.e., move toward the best solution.The evolution usually starts from a population of randomly generated individuals, which change at each iteration, called a generation. In each generation, the fitness (the values of the objective functions) of every individual in the population is evaluated, and the most fitting individuals (those with largest or smallest objective functions values, depending on whether the aim is maximization or minimization, respectively) are selected. Each individual is modified by mating and a new, more evolved, generation of candidate solutions is formed.Commonly, the algorithm terminates when either a pre-set maximum number of generations has been produced, or a satisfactory fitness level has been reached in the population.In this section, the general procedure of the MOGA developed in our work is given as follows (Konak et al., 2006; Marseguerra et al., 2006) (see Fig. 3):Step 1 (initialization)Set t = 1. Randomly generate H solutions to form the first population Xt = 1 = {X1, …, XH}.In the specific case of the maintenance optimization problem, a solution is generally a vector of decision variables such as the time interval between two successive inspections, the type of maintenance action to be performed, etc.Step 2 (fitness evaluation)Evaluate the fitnesses of the solutions inXtfor every objective Θ1, …, ΘQ, and assign the corresponding rank value by applying the sorting algorithm shown in Baraldi et al. (2013a, 2013b, 2013c). In the uncertainty setting considered in this work, performing this step requires running the double loop Monte Carlo (MC) approach for propagating the uncertainty related to every solution Xh∈ Xt, h = 1, …, H. That is, the following procedure is implemented:•An external loop samples the values of the parameters of the multistate model from a normal distribution centered in the MLE values, with the estimated covariance matrix (Johnson & Wichern, 2007; Papoulis & Pillai, 2002).For every set of sampled parameters, an internal MC loop propagates the aleatory uncertainty into the maintenance model, and estimates the values of all the objectives Θ1, …, ΘQ. These values are collected to infer the CDFs of the fitnesses at Step 3.At the end of the double loop procedure, all the collected estimations are used to infer the empirical CDFs of the objective functions Θ1, …, ΘQ.The algorithm proposed in Baraldi et al. (2013a, 2013b, 2013c) is applied to get the rank positionρhqof the solution Xh∈ Xt, with respect to the objective q, ∀h = 1, …, H, ∀q = 1, …, Q. Finally, the definition of Pareto dominance given in Section 2 is used to identify the set Ptof non-dominated solutions in the population Xt.Step 3 (breeding)Generate an offspring population Wt= {W1, …, WH} as follows:I.SelectionChoose two solutions Xsand XlfromXt. This choice is usually based on the ranking values, and heavily influences the performance of the GA, which is typically evaluated in terms of effectiveness and efficiency. In this work, we apply the dominance depth criteria characterizing the NSGA-II (Deb et al., 2002), which is one of the most efficient evolutionary algorithms to solve multi-objective optimization problems (Mena et al., 2014). The main characteristic of NSGA-II is the Fast Non Dominated Sorting (FNDS) function, used for the selection phase, which allows grouping a population into different non-domination levels. That is, all non-dominated individuals in the current population are identified. These solutions are assigned the rank 1. Then, they are virtually removed from the population and the next set of non-dominated individuals are identified and assigned rank 2. This process continues until every solution in the population has been ranked. The selection procedure is then based on this ranking: individuals are randomly selected from the same rank class (Fig. 3). The rationale of this choice is that every individual belonging to the same rank class can be considered equivalent to any other of the class, i.e., it has the same probability of the others to be selected as a parent and survive the replacement.CrossoverUsing a crossover operator, generate offsprings and add them toWt.MutationMutate each solution {W1, …, WH} with a predefined mutation rate. This means that the genomes of the individuals are randomly changed, to favor the genetic diversity.Fitness assignmentFor every solution Whin Wt= {W1, …, WH}, estimate the values of the objective functionsΘ1, …, ΘQand the rankings of the solutions with respect to the objectives, by applying to Wtthe procedure described at Step 2.ReplacementAn archive of vectors is introduced, which contains the non-dominated solutions and the corresponding fitness values. This archive represents the current Pareto optimal set, which is dynamically updated at the end of each generation. That is, the solutions in Ptunite those already stored in the archive At, where A1 = ∅. This means thatAt+1=At∪Pt. Then, the solutions in At + 1 are again sorted with respect to every objective to get the dominance relationships. The following archival rules are implemented:•The dominated members are removed from At + 1;otherwise:•if the archive is not full, At + 1 is stored as it is, and it will be used at the next iteration.if the archive is full, the solutions most similar to solutions already existing in the archive are removed from At + 1. In this respect, an appropriate concept of distance is that of the Euclidean distance based on the values of the fitness of the chromosomes normalized to the respective mean values in the archive.Step 5 (stopping criterion)If the stopping criterion is satisfied, terminate the search and return to the current population, else, set t = t + 1 and go back to Step 3. In this respect, notice that there are many stopping criteria (e.g., Marseguerra et al., 2006). In this work, the algorithm terminates when the number of simulation reaches a pre-fixed threshold.Notice that along with convergence to the Pareto-optimal set, it is also desired that an evolutionary algorithm maintains a good spread of solutions in the obtained set of solutions. In NSGA-II, this is achieved through the crowding-distance computation (Deb et al., 2002), which guides the selection process at the various stages of the algorithm toward a uniformly spread-out Pareto optimal front. The crowding-distance computation requires sorting the population according to each objective function value in ascending order of magnitude. This is done by applying the sorting algorithm summarized in Section 2.A final consideration concerns the loss of the transitive property introduced by the sorting methods that rely on similarity measures: the problem of the ‘contradictory’ ranking did not emerge in the works of the literature that propose extensions of GA to treat noisy fitness values (e.g., Eskandari et al., 2007; Hughes, 2001). This situation is due to the fact that assigning different ranking positions to solutions with equal fitness values does not significantly affect the effectiveness of the Single Objective GA search of the optimal solution; rather, the GA efficiency (i.e., speed of convergence) may be weakened. For example, assume that the fit-fit approach is considered in the reproduction phase (Marseguerra et al., 2006), and that there are n solutions with equal fitness values. When we sort them in the corresponding ranking positions i, i + 1, …, i + n − 1, each solution occupies a rank, which depends on the sorting algorithm, or even on the particular run of the algorithm (e.g., the Quicksort algorithm may randomly choose the pivot element (Knuth, 1998)). Now, the fit-fit algorithm selects and mates members of these n solutions. This is a locally hybrid reproduction approach, which is between the fit–fit and random selection approaches, in the sense that, for those n positions, and at most the two neighborhoods in positions i − 1 and i + n, there is a random facet behavior entering the selection of the parents. This may be even beneficial for GA, as it combines the speed of the fit–fit technique with the capability of preserving genetic diversity, typical of the random selection method (see Marseguerra et al., 2006 for references). However, the systematic study to assess the impact that such local-hybridization of the selection algorithm has on efficiency and effectiveness is outside the scope of this work.In this work, the NSGA-II paradigm is also combined with the CVaRαmethod (Mena et al., 2014). CVaRαis a coherent measure of the risk associated to an uncertain function of interest, which has been broadly used in financial portfolio optimization to either reduce or minimize the probability of incurring in large losses (Melnikov & Smirnov, 2012).The definition of the CVaRαrisk measure is derived from that of value at risk (VaRα). Namely, let X be a random variable representing the uncertain losses in a given time horizon, and let its CDF beFX(x) = P(X ≤ x); then, VaRα(X) is the α-percentile of the random variableX. That is, VaRα(X) represents the smallest value of losses such that the probability of having losses exceeding VaRα(X) is smaller than 1 − α (Fig. 4).VaRαis commonly used in many engineering areas involving uncertainties, such as military, nuclear, material, aerospace, finance, etc. (Sarykalin et al., 2008). In the case addressed in this work, the losses concern the unavailability and cost. That is,Varα1andVarα2are the α-percentile of the unavailability and costs, respectively, associated to the CBM policy in a given time horizonT. These values have a clear interpretation by the maintenance decision maker: the probability of having losses in availability larger thanVarα1is smaller than 1 − α; the same holds for money losses.The CVaRα(Sarykalin et al., 2008; Rockafellar & Uryasev, 2002) of X with confidence level αε(0, 1) is the mean of the generalized α-tail distribution:CVaRα(X)=∫−∞+∞xdFXα(x)whereFXα(x)={0whenx<VaRα(X)FX(x)−α1−αwhenx≥VaRα(X)In turn, we consider the 1 − α-tail CDF, which represents the risk beyond the VaRα. Then, CVaRαrepresents the mean value of this tail. Fig. 4 shows the representation of both VaRαand CVaRαrisk measures.Also, CVaRαhas a clear engineering interpretation. For example,CVarα1(U)≤U−ensures that the average of (1 − α) percentage highest losses in availability does not exceed U−.From these definitions, it comes out that the VaRαand CVaRαrisk measures have different meanings and, then, mathematical properties. The problem of the choice between VaRαand CVaRαhas received increasing attention, especially in financial risk management (Sarykalin et al., 2008). Anyway, in our case we consider the CVaRαrisk measure, because it is more conservative than VaRα, which is particularly important when safety is a concern in the application. In particular, following (Mena et al., 2014), we modify the definition of the objective functions and take a convex combination of the average values of the objectives and their CVaRα. That is, we introduce two new objective functions:(1)-Θ1=β×E[Θ1]+(1−β)×CVarα1(2)-Θ2=β×E[Θ2]+(1−β)×CVarα2This allows considering different scenarios; that is:-In case we set β = 0, then we conservatively aim at minimizing the risk of having large unavailability and costs.In case of β = 1, the objectives we want to minimize are the mean cost and mean unavailability of the nozzle system.In case of β = 0.5, the objective functions are an equal compromise between the corresponding two values above.To conclude this section, it is worthy stressing that also the CVar method summarizes the information bring by a CDF in a point, as other works of the literature (e.g., Petrone, 2013). Nonetheless, the CVar point has a clear engineering interpretation, which directly relates to the final decisions.In this paper, we consider a real practical case study concerning the maintenance optimization of a gas turbine nozzle system affected by different degradation mechanisms. The degradation of the nozzles is modeled as a four-state degradation model, in which the transitions can occur from one degradation state to the next degraded state only, and the stochastic transition times between the states obey Weibull distributions.Recall that the scope of the case study is to investigate the potential of the proposed techniques in optimizing the Condition Based Maintenance (CBM) policy applied to the nozzle system, while giving due account to the uncertainty in the parameter estimates. For this, we consider a case study derived from a real industrial application faced by the authors, but with arbitrarily chosen values of the maintenance model parameters.The data available to estimate the parameters of the stochastic model are the outcomes of opportunistic, non-periodic, visual inspections of the turbine nozzles. That is, upon inspection, the maintenance experts disassemble the turbine and check the nozzles health states to qualitatively classify them into ‘Good’, ‘Light’, ‘Medium’ and ‘Heavy’ (Fig. 5). In particular, every nozzle system is made up of N = 22 nozzles.These data are used to estimate the parameters αk, βkof the Weibull distribution of the transition Tkfrom state Skto state Sk + 1, k = 1, …, 3, together with the corresponding uncertainties. For simplicity, we assume that such epistemic uncertainty in the parameter values is represented by normal distributions centered on the parameter ML estimates and with a given standard deviations. The general methodology to estimate these quantities can be found in Compare, Martini, and Zio (2015).In particular, to test the potential of the proposed GA advancements in treating uncertain fitnesses, we consider two numerical settings, with different amount of uncertainty affecting the estimates (Table 1). Namely, for every parameter in Table 1, the first column reports the MLE, whereas two different values of the standard deviations of the corresponding ML estimators are reported in the rows of the second column.The CBM approach applied to the nozzle system under study is based on the continuous monitoring of the turbine efficiency by processing the information provided by sensors which trace physical variables such as pressure, temperature, etc. When the efficiency value drops below a given threshold TE, then the nozzle system is replaced. Replacement makes the system unavailable for UR= 2 days. The cost of the consequent business interruption is given by the product of the duration of the unavailability period times the annual income I, which is defined as the income corresponding to one year of turbine continuous, full capacity operation. In this paper I = 20M€. Thus, the total cost CRassociated to a replacement action upon the achievement of TEis the sum of the business interruption cost due to system unavailability and the cost CS= 3M€ of replacing the nozzle system.The nozzle system is also periodically inspected, with period Π. Every inspection is performed by one maintenance operator, who takes Tinsp = 8 days for carrying out the machine disassembling and re-assembling operations necessary to check the health state of the nozzles. Obviously, larger values of Π steer the policy toward a full exploitation of the components and avoid ineffective machine stops. On the contrary, smaller values of Π allow the machine operation in better health conditions with larger efficiency values. For this reason, Π is an influential decision variable to optimize the maintenance policy.The duration t(Sγ) of the preventive maintenance action performed on component γ depends on the degradation state Sγin which it is found (Table 2). More precisely, fixing nozzles in degradation state S2 requires one operator working for 0.5 day; 1 day is needed for a maintenance operator to repair nozzles in degradation state S3. Finally, if a nozzle is heavily degraded (i.e., in degradation state S4), then a maintenance operator takes 3 days to repair it.Notice that a simplifying assumption is made in this study: independently on the degradation state where the nozzles are found (light, medium, heavy), these are always repaired upon inspection (i.e., not replaced), and their conditions after maintenance are always considered as good as new (AGAN).From this, it appears that the preventive maintenance time TMrequired for repairing all the N nozzles is given by:TM=Tinsp+∑γ=1Nt(Sγ)Obviously, the system is unavailable during inspections and repairs. This causes a business interruption, whose cost is given by the part of the annual income I that the maintenance actions prevent from being gained. Thus, reducing the amount of time spent in repairing the nozzle system has a beneficial effect on the maintenance costs. In this respect, a larger number of maintenance operators Nmo can be involved in repairing actions.The effect on the time reduction is given by:TM=Tinsp+∑γ=1Nt(Sγ)NmoOn the other side, reducing maintenance time has its own cost, as maintenance operators must be paid for their work. We assume that their daily cost is CO = 2000 €/man.Then, Nmo is another decision variable that enters the optimization of the CBM policy.Generally speaking, nozzle degradation entails loss in turbine efficiency, whose magnitude depends on the degradation state. In this work, we assume that when the generic nozzle γ enters degradation state S2, it causes a loss lE(Sγ= S2) = 0.2% in turbine efficiency. An additional drop of 0.3 percent is associated to each component in degradation state S3 (i.e., lE(Sγ= S3) = 0.3%), whereas each nozzle in state S4 brings about a further, large loss of 0.5 percent (i.e., lE(Sγ= S4) = 0.5%).Thus, the loss LEin turbine efficiency in a cycle (i.e., the time between two maintenance actions) is given by:LE=∑γ=1N∑k=13lE(Sγ=Sk+1)(Tstop−Tk+1γ)where Tstop is the end of the cycle (i.e., the inspection time at the end of the interval Π or the time in which the turbine efficiency reaches the threshold TE, whichever comes first),Tkγis the stochastic transition time Tkof component γ, from state Skto state Sk + 1Notice that the simplified scheme considered in this work entails that the worst condition (i.e., the N = 22 nozzles are all in degradation state S4) determines a total loss in turbine efficiency of at most 22(0.2 + 0.3 + 0.5) percent = 22 percent.Turbine inefficiency entails a cost, which is due to the production loss with respect to the full capacity production conditions. This is given by the part of the annual income that inefficiency prevents from being gained. That is, IC= LE· I is the inefficiency cost in a cycle.To sum up, the maintenance model described allows estimating the values of cost and unavailability corresponding to a triplet of decision variables TE,Nmo and Π.As mentioned before, a double Monte Carlo algorithm has been implemented to propagate the uncertainty from the degradation model parameters to the maintenance performance indicators, where the internal Monte Carlo loop simulates the life process of the turbine nozzles over a fixed time horizon (i.e., the aleatory uncertainty), whereas the external loop is used to sample the parameters of the degradation model (i.e., epistemic uncertainty).Finally, for clarity, all the parameters and variables of the case study, with relevant explanations, values and formulas, are summarized in Table 4.

@&#CONCLUSIONS@&#
