@&#MAIN-TITLE@&#
The bipartite quadratic assignment problem and extensions

@&#HIGHLIGHTS@&#
We introduced a new model that subsumes the well-known quadratic assignment problem.We developed meta-heuristic algorithms using three neighborhoods and their unions.Experimental analysis disclosed the superiority of hybrid algorithms.Complexity results and polynomially solvable special cases are also established.

@&#KEYPHRASES@&#
Quadratic assignment,Bilinear programs,Complexity,Metaheuristics,Local search,

@&#ABSTRACT@&#
We introduce a new binary quadratic program that subsumes the well known quadratic assignment problem. This problem is shown to be NP-hard when some associated parameters are restricted to simple values but solvable in polynomial time when some other parameter values are restricted. Three different neighborhood structures are introduced. A best solution in all these neighborhoods can be identified in polynomial time even though two of these neighborhoods are of exponential size. Different local search algorithms and their enhancements using tabu search are developed using these neighborhoods. Experimental analysis show that two hybrid algorithms obtained by combining these neighborhoods in specific ways yield results that are superior to the algorithms that use these neighborhoods separately. Extensive computational results and statistical analysis of the resulting data are presented.

@&#INTRODUCTION@&#
The quadratic assignment problem (QAP) (Burkard, Dell’Amico, & Martello, 2009; Cela, 1998; Koopmans & Beckmann, 1957; Lawler, 1963; Pardalos, Rendl, & Wolkowicz, 1994; Pitsoulis & Pardalos, 2008) is well studied in the combinatorial optimization literature. The original application that initiated the study of this versatile model comes from the facility location literature. Since then the problem has been used to model optimization problems in diverse application areas. An integer programming formulation of QAP can be given as follows (Lawler, 1963):(1)Maximize∑i=1n∑j=1n∑k=1n∑l=1nqijkl′xijxkl+∑i=1n∑j=1ncij′xij(2)Subjectto∑i=1nxij=1forj=1,2,…,n(3)∑j=1nxij=1fori=1,2,…,n(4)xij∈{0,1}fori,j=1,2,…,n,whereqijkl′andcij′are real numbers. QAP has been studied extensively in the literature and formulated in various alternative formats. The most popular variation of QAP is the Koopmans–Beckman case (Cela, 1998) where the quadratic cost matrixQ′=(qijkl′)n2×n2is defined using two n × n matricesA=(aij)andB=(bkl)such thatqijkl′=aijbkl. In the definition of QAP, if we omit constraints (3) we get the quadratic semi-assignment problem (QSAP) (Domschke, Forst, & Voß, 1992; Drwal, 2014; Greenberg, 1969; Malucelli, 1996; Pitsoulis, 2009; Saito, Fujie, Matsui, & Matuura, 2009) which itself has numerous applications.Let us now discuss a new model, closely related to QAP, called the bipartite quadratic assignment problem (BQAP). The problem can be stated mathematically as follows:(5)Maximize∑i=1m∑j=1m∑k=1n∑l=1nqijklxijykl+∑i=1m∑j=1mcijxij+∑k=1n∑l=1ndklykl+c0(6)Subjectto∑i=1mxij=1forj=1,2,…,m(7)∑l=1nykl=1fork=1,2,…,n(8)xij∈{0,1}fori,j=1,2,…,m(9)ykl∈{0,1}fork,l=1,2,…,n,where qijkl, cij, dkland c0 are real numbers.Whenm=nandxij=yijis forced for alli=1,2,…,n,j=1,2,…,m,BQAP reduces to the QAP. In fact,xij=yijcan be forced by modifying the cost matrices without explicitly introducing these constraints. Choosem=nand define,(10)qijkl={qijkl′ifij≠klqijkl′+2Mifij=klAlso, setcij=dij=−M+12cij′andc0=0. Then it can be verified that in an optimal solution to the resulting BQAP,xij=yijholds fori,j=1,2,…,n(=m)and this solution is optimal for the QAP. This follows by taking the constraintsxij=yijfor all ij into the objective function as a penalty term−M(xij−yij)2and simplifying it usingxij2=xijandyij2=yij. Thus BQAP is a proper generalization of QAP.BQAP can also be viewed as a special case of QSAP. Thus, algorithms developed for the QSAP can be used to solve BQAP. However, exploiting the special structure of BQAP, efficient algorithms can be obtained and development of such algorithms is the primary focus of this paper.Before discussing motivating applications of the BQAP model, to simplify the notations and to allow additional modeling flexibility, we consider a slightly more general version of BQAP which is called the binary quadratic programming problem with partitioned variables (BQP-PV). LetI={1,2,…,m}andS1,S2,…,Spbe a partition of I. Similarly, letJ={1,2,…,n}andT1,T2,…,Tkbe a partition of J. For any (i, j) ∈ I × J a cost qijis given. Further, costs cifor i ∈ I and djfor j ∈ J are also given along with a constant c0. Then BQP-PV is to(11)Maximize∑i=1m∑j=1nqijxiyj+∑i=1mcixi+∑j=1ndjyj+c0(12)Subjectto∑i∈Srxi=1,forr=1,2,…,p(13)∑j∈Tuyj=1,foru=1,2,…,k(14)xi,yj∈{0,1},fori∈I,j∈J.Note that BQP-PV is equivalent to its continuous version obtained by replacing constraints (14) by(15)0≤xi≤1fori∈I,0≤yj≤1forj∈J.The validity of this follows from the fact that the objective function is bilinear (Altman, 1968; Bloemhof-Ruwaard & Hendrix, 1996; Kono, 1976) which guarantees an extreme point optimal solution and constraints (12), (13) and (15) define a totally unimodular coefficient matrix which guarantees integer extreme points. This allows us to use bilinear programming algorithms (Altman, 1968; Konno, Kuno, & Yajima, 1992) to solve BQP-PV. However, general purpose bilinear programming algorithms are inefficient to solve BQP-PV and to the best of our knowledge, no special purpose bilinear programming algorithms are available that exploit the structure of BQP-PV. In this regard, our work is also a contribution to the bilinear programming literature.Supposem=n=(α2,say)and|Sr|=|Tu|=αfor all r and u. Then it follows thatp=k=α. ThusSr,r=1,…,αandTu,u=1,…,αcan be viewed as partitions of the same set1,2,…,α2. Assume that elements in each Sris given some ordering. For eachu=1,2,…,α,construct Tuas the set containing the uth element from eachSr,r=1,2,…,α. It is easy to see that the resulting BQP-PV is precisely a BQAP. Thus BQP-PV is a proper generalization of BQAP which in turn generalizes the QAP. The problem BQP-PV without the constraints (12) and (13) is the bipartite unconstrained 0–1 quadratic programming problem (BQP01) studied by various authors (Duarte, Laguna, Martí, & Sánchez-Oroa, 2014; Glover, Ye, Punnen, & Kochenberger, 2015; Karapetyan & Punnen, 2012; Punnen, Sripratak, & Karapetyan, 2013; 2015a; 2015b).A graph theoretic interpretation of BQP-PV can be given as follows. LetG=(I,J,E)be a bipartite graph andS1,S2,…,Sp,andT1,T2,…,Tkbe partitions of I and J respectively. A subgraphG′=(I′,J′,E′)is said to be a representative subgraph of G if I′ contains exactly one element from each Sr,r=1,2,…,pand J′ contains exactly one element from each Tu,u=1,2…,k. Further, for each (i, j) ∈ E a cost qijis prescribed. Then the maximum weight representative subgraph problem on G is to find a representative subgraph of G where the sum of the edge-weights is maximized. Clearly, the representative subgraph problem can be modeled as BQP-PV. A selected node in I′ or J′ represents an elected representative of the subset it belongs. The weight qijof the edge (i, j) represents the potential interaction measure between representatives i and j. Then the maximum weight representative subgraph chooses representatives with total interaction potential being maximized. If the selected subgraph is required to be a clique, then we call the resulting maximum representative subgraph problem a maximum weight representative clique problem and this problem can also be formulated as BQP-PV. It is possible that some representatives selected may be more important than others. In such a case, we have the node weighted version of the representative subgraph problem, where ciis the weight of node i ∈ I and djis the weight of node j ∈ J. This problem can also be modeled as BQP-PV.The representative subgraph model is applicable in scenarios where the collaborative output is maximized. For example selection of negotiating teams, selection of component manufacturers and assembly units. The BQP-PV (and BQAP) model can also be viewed as a robust optimization problem (Aissi, Bazgan, & Vanderpooten, 2009; Yu & Kouvelis, 1996) where the y-values represent specific weights (or probabilities) for a given set of scenarios and the robustness is measured in terms of weighted averages rather than the worst case as in the case of minmax and minmax regret problems. The constraints set on the y-variables provides allowable distribution of the weights. For example, whenk=1it defines a simplex and the resulting problem reduces to optimizing individual scenarios and choosing the overall best. This simplicity disappears as k increases and thereby allowing complex weight distributions. In particular, BQAP becomes a robust semi-assignment problem that focuses on weighted averages of the scenario functions guided by the specific weight distributions defined.Other applications of BQP-PV include problems in clustering and bioinformatics obtained by extending corresponding applications of BQP01 in those areas (Shen, Ji, & Ye, 2009; Tanay, Sharan, & Shamir, 2002). Also, because of the unifying nature, applications of QAP and QSAP can also be viewed as potential applications of BQP-PV. In particular, our results are directly applicable to solve low-rank QSAP by following the approach suggested in Torki, Yajima, and Enkawa (1996) to solve low-rank QAP.In this paper we first identify some structural properties of BQP-PV. It is shown that BQP-PV is strongly NP-hard even if|Sr|=|Tu|=2for allr=1,2,…,pandu=1,2,…k. Further, forp=O(logmlogSmax)ork=O(lognlogTmax),whereTmax=max1≤u≤k|Tu|andSmax=max1≤r≤p|Sr|,we show that BQP-PV is solvable in polynomial time. BQP-PV can be formulated as a BQP01 by introducing large penalties for violating constraints (12) and (13) using the ideas discussed in Alidaee, Kochenberger, Lewis, Lewis, and Wang (2008); Kochenberger, Glover, Alidaee, and Rego (2004); Lewis, Kochenberger, and Alidaee (2008) with minor modifications. However, the penalty factor imposed in the reformulation limits computational flexibility, especially for large scale problems. When|Sr|=|Tu|=2for allr=1,2,…,p,u=1,2,…k,we observe that the resulting BQP-PV can be formulated as a BQP01 without using large penalty factors and with half as many variables as in the original model. Thus such instances can be solved efficiently using existing algorithms for BQP01 (Duarte et al., 2014; Glover et al., 2015; Karapetyan & Punnen, 2012).For the general version of BQP-PV, specialized algorithms that exploit problem specific structures are required to achieve computational efficiency. For this purpose, we first introduce three neighborhood structures - swap, concurrent swap, and optimized swap to develop such algorithms. The swap neighborhood has a simple structure and the best improving solution in this neighborhood can be identified with linear time complexity. The basic idea of a swap operation is applied in developing heuristics for many 0–1 programming problems. Our contribution here is its application in the context of BQP-PV and development of very fast evaluation schemes for the neighborhood. The concurrent swap and optimized swap neighborhoods are new and could be of exponential size (Ahuja, Ergun, Orlin, & Punnen, 2002, 2007; Deneko and Woeginger, 2010). However, exploiting the properties of the neighborhoods we show that a best solution in these neighborhoods can be identified by investing only polynomial effort. We then computationally analyze these neighborhoods within a local search paradigm and investigate the efficacy of various combinations of these neighborhoods. Our computational results disclose that the resulting algorithms are enhanced when applying the neighborhoods in combination, instead of using them in isolation. In addition, we present a tabu search framework encapsulating these local search schemes to achieve superior outcomes.The paper is organized as follows. In Section 2, we discuss the computational complexity of BQP-PV and some polynomially solvable special cases of the problem. The new neighborhood structures and evaluation schemes to identify a best solution in these neighborhoods are discussed in Section 3. Experimental design, development of benchmark instances, and results of computational analysis are discussed in Section 4 followed by concluding remarks in Section 5.If|Sr|=1for any r or|Tu|=1for any u, we can easily reduce the problem size. SupposeSr={xt}for some r. Clearly xtmust be 1 in all feasible solutions. Thus we can eliminate the variable xtby deleting Qt*, the tth row of Q, from Q, deleting the component ctfrom c, replacing the vector d byd+Qt*and adding ctto c0. The case|Tu|=1can be handled in an analogous way. Thus without loss of generality we assume that |Sr| ≥ 2 for all r and |Tu| ≥ 2 for all u.Theorem 1BQP-PV is strongly NP-hard even if|Sr|=2for allr=1,2,…,pand|Tu|=2for allu=1,2,…,k.The proof of the theorem follows by a simple reduction from the bipartite unconstrained 0–1 quadratic programming problem (BQP01) (Punnen et al., 2013; 2015a; 2015b). An instance of BQP01 with decision variablesx1,x2,…,xmandy1,y2,…,ynis completely defined by an m × n matrixA=(aij),an m-vector b1 and an n-vector b2. From this instance, construct an instance of BQP-PV as follows. Define(16)Q=[AOm×nOm×nOm×n],c=[b10m]andd=[b20n],where Om × n, is a zero matrix, and 0m, 0nare vectors of size m and n respectively with entries 0. Consider the decision variablesx1,x2,…,x2mandy1,y2,…,y2nand defineSr={xr,xm+r},r=1,2,…,mandTu={yu,yn+u},u=1,2,…,n. It is easy to verify that an optimal solution to BQP-PV with data Q, c, d, Sr, Tuas defined above gives an optimal solution to BQP01. Since BQP01 is strongly NP-hard (Punnen, Sripratak, & Karapetyan, 2015b), the result follows.□It may be noted that the reduction from BQP01 to BQP-PV discussed above preserves the objective function value. Thus any ϵ-optimal solution to the reduced instance of BQP-PV is an ϵ-optimal solution to BQP01. Thus all non-approximability results for BQP01 (Punnen et al., 2013; 2015a; 2015b) translate into non-approximability results for BQP-PV as well, in particular to the special cases considered in Theorem 1.Interestingly, we now observe that a BQP-PV satisfying the condition that|Sr|=|Tu|=2for all r and u can be formulated as a BQP01 with the number of variables reduced by half. For definiteness, we denote such a BQP-PV by BQP-PV2. Without loss of generality we assume that BQP-PV2 has 2m x-variables and 2n y-variables. Thus the corresponding quadratic cost matrixQ=(qij)have dimension 2m × 2n and the vectors c and d have respective dimensions 2m and 2n. Further, without loss of generality we assume that in BQP-PV2,Sr={r,m+r}andTu={u,n+u}. Consider the m × n matrixQ¯=(q¯ij),vectorsc¯=(c¯1,c¯2,…,c¯m),d¯=(d¯1,d¯2,…,d¯n),and constantc¯0where,(17)q¯ij=qij−qm+i,j−qi,n+j+qm+i,n+j(18)d¯j=dj−dn+j+∑i=1m(qm+i,j−qm+i,n+j)(19)c¯i=ci−cm+i−∑j=1n(qi,n+j−qm+i,n+j)(20)c¯0=c0+∑i=1mcm+i+∑j=1ndn+j+∑i=1m∑j=1nqm+i,n+j.LetBQP01¯denote the bipartite unconstrained0−1quadratic programming problem(21)Maximize∑i=1m∑j=1nq¯ijxiyj+∑i=1mc¯ixi+∑j=1nd¯jyj+c¯0(22)Subjecttoxi,yj∈{0,1},fori=1,2,…,m,j=1,2,…,n.Theorem 2The optimal objective function values of BQP-PV2 andBQP01¯are the same. Further, from an optimal solution toBQP01¯,an optimal solution to BQP-PV2 can be constructed.In BQP-PV2, the constraintsxm+i∈{0,1}fori=1,2,…mandyn+j∈{0,1}forj=1,2,…nare redundant and hence can be discarded. Using the constraintxi+xm+i=1,the variablexm+ican be eliminated by substitutingxm+i=1−xi. Similarly, using the constraintyj+yn+j=1,the variableyn+jcan be eliminated by substitutingyn+j=1−yj. Repeating this fori=1,2,…,mandj=1,2,…,n,the equivalence of the objective function values of BQP-PV2 andBQP01¯can be established by simple algebraic calculations. Further, supposex10,x20,…xm0,y10,y20,…,yn0be an optimal solution toBQP01¯. Definexm+i0=1−xi0fori=1,2,…,mandyn+j0=1−yj0forj=1,2,…,n. Then it can be verified thatx10,x20,…x2m0,y10,y20,…,y2n0is an optimal solution to BQP-PV2.□We now observe that if p or k (the number of partitions of the x variables or the number of partitions of the y variables) is “small”, then BQP-PV can be solved in polynomial time. Letx0=(x10,x20,…,xm0)be a given solution to BQP-PV. For eachu=1,2,…,k,choose the index jusuch that(23)∑i=1mqijuxi0+dju=maxj∈Tu∑i=1mqijxi0+djNow, define(24)yj0={1ifj=ju,u=1,2,…,k,0otherwise.Clearly, if x is fixed at x0, then y0 is an optimal choice for y. Likewise, if y is fixed at some values, sayy*=(y1*,y2*,…,yn*),then a corresponding optimal value x* of x is given by(25)xi*={1ifi=ir,r=1,2,…p,0otherwise.where the index iris chosen such that(26)∑j=1nqirjyj*+cir=maxi∈Sr∑j=1nqijyj*+ci.BQP-PV has(∏r=1p|Sr|)(∏u=1k|Tu|)feasible solutions. If p is small, we can enumerate all possible choices of x and for each such x, an optimal y can be identified using Eq. (24). Comparing such solutions and choosing the best overall solution yield an optimal solution to BQP-PV. LetSmax=max{|Sr|:r=1,2,…,p}andTmax=max{|Tu|:u=1,2,…,k}. Ifp=O(logmlogSmax)then the number of possible choices of x is polynomial in m and hence we can solve BQP-PV in polynomial time. Similarly, ifk=O(lognlogTmax)then we can solve BQP-PV in polynomial time by enumerating all possible choices of y and computing the corresponding optimal x using Eq. (25).Let us now define some neighborhoods for solutions of BQP-PV to design various local search algorithms and extensions. More specifically we introduce three neighborhoods: the swap neighborhood, the concurrent swap neighborhood, and the optimized swap neighborhood. The structure of the swap neighborhood is very simple and the underlying ideas are used in many combinatorial optimization problems. The size of this neighborhood ism+n−p−kand a best solution in this neighborhood can be obtained inO(m+n)time. This very fast neighborhood evaluation scheme makes it attractive to be used within metaheuristic algorithms. The concurrent swap neighborhood employs several independent swap moves simultaneously. The size of this neighborhood can be exponential in m and/or n and a best solution in the neighborhood can be identified inO(mk+np)time. The optimized swap neighborhood is yet another exponential neighborhood that can be searched in polynomial time to find a best solution. Each of these neighborhoods have their distinct features and studying the empirical behavior of these neighborhoods within the context of local search and tabu search, when used in isolation or in unison, is the major focus of our experimental analysis. Thus, in addition to the novelty of the model, our work discloses the importance of combining exponential neighborhoods and simple neighborhoods to achieve superior outcomes that balances solution quality and computational time for metaheuristic algorithms. Also, unlike the application of swap operations in traditional binary quadratic problems such as QSAP, the bipartite structure of BQP-PV opens up possibilities of mixing ‘independent’ and ‘dependent’ moves controlled by choices of x or y variables as swap candidates.Let (x0, y0) be a solution to BQP-PV, wherex0=(x10,x20,…,xm0)andy0=(y10,y20,…,yn0). Since each set Srcontains exactly one index say i such thatxi0=1and each set Tucontains exactly one index, say j, such thatyj0=1,we represent these special indices as αrand βu, respectively. Thus for any solution (x, y),xαr=1forr=1,2,…,pandyβu=1,foru=1,2,…,k. Define(27)ρi(y0)=ci+∑j=1nqijyj0,i=1,2,…,m(28)γj(x0)=dj+∑i=1mqijxi0,j=1,2,…,nThe values ρi(y0) and γj(x0) are respectively called the row and column potentials. Then the objective function value f(x0, y0) of (x0, y0) can be represented as(29)f(x0,y0)=∑i=1mρi(y0)xi0+∑j=1ndjyj0+c0=∑j=1nγj(x0)yj0+∑i=1mcixi0+c0Recall thatxαr0=1andxi0=0for all i ∈ Sr∖{αr},r=1,2…,p. A swap operation on x-variables, denoted by swap(αr, i), replaces the componentsxαr0andxi0by their complements1−xαr0and1−xi0,respectively to generates a new solution, say (x1, y0). Note thatf(x1,y0)=f(x0,y0)−ραr(y0)+ρi(y0). Choose indices r* andt∈Sr*such that(30)−ραr*(y0)+ρt(y0)=max1≤r≤p{−ραr(y0)+maxi∈Sr∖{αr}ρi(y0)}Then the best swap move for x-variables is obtained by exchanging the values ofxt0andxαr*0.A swap operation on y variables, denoted by swap(βu, s), replaces componentsyβu0andys0by their complements1−yβu0and1−ys0respectively to generate a new solution (x0, y1). Note thatf(x0,y1)=f(x0,y0)−γβu(x0)+ρs(y0). Choose indices u* andh∈Tu*such that(31)−γβu*(x0)+γh(x0)=max1≤u≤k{−γβu(x0)+maxj∈Tu∖{βu}γj(x0)}Then, the best swap move for y-variables is obtained by exchanging the values ofyh0andyβu*0. Ifmax{−ραr*(y0)+ρt(y0),−γβu*(x0)+γh(x0)}≤0then (x0, y0) is a local maximum with respect to the swap neighborhood. Among the best swap moves for x-variables and y-variables, an overall best swap move can be identified. Thus, given the values of row and column potentials, Eqs. (30) and (31) show that the best swap move can be identified inO(m+n)time.Let(x¯,y¯)be the solution obtained after the best swap move. If the best move corresponds to x-variables, then we update the row and column potentials as(32)γj(x¯)=γj(x0)+qtj−qαr*j,forj=1,2,…,n(33)ρi(y¯)=ρi(y0),fori=1,2,…,mSimilarly, if the best move corresponds to y-variables, then we update the row and column potentials as(34)ρi(y¯)=ρi(y0)+qih−qiβu*,fori=1,2,…,m(35)γj(x¯)=γj(x0),forj=1,2,…,nThus, after a move to a new solution, the row and column potentials and other attributes of the solution such as the indices αr, βuetc., can be updated in O(m) or O(n) time, depending on whether the swap operation corresponds to an x variable or a y variable.Let (x0, y0) be a given solution. Note that two swap operations on y variables, say swap(βu, ℓ) and swap(βv, t) for u ≠ v, are independent in the sense that they can be applied concurrently or one after the other yielding the same solution from (x0, y0). The concurrent-swap neighborhood for y variables, denoted by cswap(y), keeps x variables fixed at x0 and apply swap moves swap(βu, ℓ), for some ℓ ∈ Tu,u∈J*⊆{1,2,…,k}to generate a candidate solution in the neighborhood. Ifℓ=βuthe swap operation is redundant. Note that any feasible value of y can be generated in this way, for fixedx=x0. There are∏u=1k|Tu|choices for y and hence the size of this neighborhood is∏u=1k|Tu|. Given the row and column potentials corresponding to (x0, y0), a best solution in the cswap(y) neighborhood, sayy1=(y1,y22,…,yn1),can be identified in O(n) time using the closed form expression(36)yj1={1ifj=βu,u=1,2,…k,0otherwise.where the index βuis given by(37)γβu(x0)=maxj∈Tuγj(x0),u=1,2,…,k.After moving to the solution (x0, y1), we need to calculate the row potentials ρi(y1) corresponding to this solution. LetΔ+(y1)={j:yj0=0,yj1=1}andΔ−(y1)={j:yj0=1,yj1=0}.Note that|Δ+(y1)|=|Δ−(y1)|≤k. Then the column potentials remain unchanged and the row potentials with respect to y1 can be updated as(38)ρi(y1)=ρi(y0)−∑j∈Δ−(y1)qij+∑j∈Δ+(y1)qij,i=1,2,…,m.The setsΔ−(y1)andΔ+(y1)can be constructed in O(k) time. Thus ρi(y1) fori=1,2,…,mcan be identified inO(k+|Δ+(y1)|m)time.Similarly, the concurrent-swap neighborhood for x variables, denoted by cswap(x), keeps y variables fixed at y0 and apply the swap(αr, t) move for some t ∈ Sr, for eachr∈I*⊆{1,2,…,p}to generate a candidate solution in the neighborhood. Ifαr=tfor any r, the corresponding swap is redundant. Note that any feasible value of x can be generated in this way, for fixedy=y0. There are∏r=1p|Sr|choices for x and hence the size of the cswap(x) neighborhood is∏r=1p|Sr|. Given the row and column potentials corresponding to (x0, y0), the best solution in cswap(x), say (x1, y0), among these possibly exponential number of choices can be identified in O(m) time using the closed form expression(39)xi1={1ifi=αr,r=1,2,…p,0otherwise.where the index iris given by(40)ραr(y1)=maxi∈Srρi(y1),r=1,2,…,p.After moving to the solution (x1, y0), we need to compute the column potentials corresponding to this solution. LetΛ+(x1)={i:xi0=0,xi1=1}andΛ−(x1)={i:xi0=1,xi1=0}.Note that|Λ+(x1)|=|Λ−(x1)|≤p. Then the column potentials with respect to x1 can be updated as(41)γj(x1)=γj(x0)−∑i∈Λ−(x1)qij+∑i∈Λ+(x1)qij,j=1,2,…,n.The setsΛ−(x1)andΛ+(x1)can be constructed in O(p) time. Thus γj(x1) forj=1,2,…,ncan be identified inO(p+|Λ+(x1)|n)time.The concurrent-swap neighborhood is the compound neighborhood given by cswap(x)∪cswap(y). A best solution in this neighborhood is either (x0, y1) or (x1, y0), whichever has the largest objective function value. Such a solution will be no worse than∏u=1k|Tu|+∏r=1p|Sr|alternative solutions. Thus based on the foregoing discussions, a best solution in this neighborhood can be identified inO(|Λ+(x1)|n+|Δ+(y1)|m)time.The neighborhood graph of the concurrent-swap neighborhood is connected and has diameter 2. i.e. transition from one solution to another solution in the solution space of BQP-PV can be achieved by at most two concurrent-swap moves: one cswap(x) move followed by a cswap(y) move or viceversa. The swap neighborhood and concurrent swap neighborhood are equivalent in the sense that a solution (x0, y0) is locally optimal with respect to the swap neighborhood if and only if it is locally optimal with respect to the concurrent-swap neighborhood. However we treat these neighborhood differently because of the possibility of mixing moves based on x variables and y variables that alters the search path and hence diversify the search.As noted earlier, given a feasible value x0 of x, we can compute the best value of the y variables in linear time when x is fixed at x0 and viceversa. Thus it is enough to maintain the values of the x variables and assume that the corresponding y values are ‘optimized’ for the given x. Such a representation is called the optimized representation of x variables. Likewise, we can maintain y values only with the assumption that the corresponding x values are optimized for the given y. We call this the optimized representation of y variables. A swap operation on an optimized representation of x is denoted by oswapx(αr, i) which interchanges the value ofxαrwith that of xifor some i ∈ Srto obtain the solutionx¯and update the corresponding y as the optimal selection when x is fixed atx¯. Unlike the swap neighborhood, two optimized swap operations, oswapx(αr, i) and oswapx(αs, t), s ≠ r, are not independent since they indirectly affects the y variables as well. Similarly, a swap operation on an optimized representation of y, denoted by oswapy(βu, j), interchanges the value ofyβuwith that of yjfor some j ∈ Tuto obtain the solutiony¯and update the corresponding x as the optimal selection when y is fixed aty¯. Again, unlike the swap neighborhood for y variables, two optimized swap operations on these variables, oswapy(βu, j1) and oswapy(βv, j2), u ≠ v, are not independent since they indirectly affect the values of x variables as well.The best oswapx(., .) and oswapy(., .) moves can be identified efficiently by combining the ideas and data representation discussed in the case of swap neighborhoods and concurrent swap neighborhoods. The details are omitted. Since for each trial oswapx(., .) move (oswapy(., .) move) examined, optimized y values ((x) values) need to be constructed, to evaluate the corresponding gain. This results in the worst case complexity ofO(min{mn,(mk+np)kp})per iteration. Thus optimized swap moves are expensive compared to the swap move and concurrent swap move. In practice however, the average complexity is significantly less since most of the time, the number of components in which an optimized y variables (optimized x variables) differ from their previous values is considerably small and this can be exploited by careful implementation.It can be verified that the size of the optimized swap neighborhood is at least(m−p)∏u=1k|Tu|+(n−k)∏r=1p|Sr|.Let us now discuss our heuristic algorithms using the neighborhoods discussed above. All our algorithms use the standard framework of local search and tabu search with multi-start to diversify the search path. Such ideas have been tested extensively in the literature for various combinatorial optimization problems. However, our contribution here is the novelty of our two exponential neighborhoods and the way in which the neighborhoods are used in developing local search algorithms that exploit the problem structure. Extensive experimental analysis has been carried out to compare specific implementations that we have developed. This also provided additional insights into the structure of the problem. Since this is the first experimental study on this problem, we believe that our analysis will inspire other researchers for follow up works on the topic.To develop a test bed for our experimental analysis, we used the data set of Karapetyan and Punnen (2012) developed for unconstrained bipartite binary quadratic programs and modified it by generating random partitions of the decision variables to get BQP-PV instances. The specific parameter settings for the instances we used are given below:•Random Problems: In this category, we set qij, ciand djfori=1,2,…,m,j=1,2,…,nare normally distributed random integers with meanμ=0and standard deviationσ=100.Max Biclique instances: These instances are generated using a random bipartite graphG=(V1,V2,E)withV1={1,2,…,m}andV2={1,2,…,n}. For each edge (i, j) in G, a weight wijis prescribed. The wijvalues are drawn from a normal distribution with meanμ=100and standard deviationσ=100. If (i, j) is an edge in G, then we setqij=wij,otherwise qijis set to−Mwhere M is a large positive number. The vectors c and d are chosen as zero vectors.Max Induced Subgraph instances: These instances are also generated using a random bipartite graphG=(V1,V2,E)withV1={1,2,…,m}andV2={1,2,…,n}. The edge weights wijare chosen exactly as in the case of Max Biclique instances. If (i, j) is an edge in G, then we setqij=wij,otherwise qijis set to 0.Max Cut instances: These instances are also generated using a random bipartite graphG=(V1,V2,E)withV1={1,2,…,m}andV2={1,2,…,n}. The edge weights wijare chosen exactly as in the case of Max Biclique instances. If (i, j) is an edge in G, then we setqij=−2wij,otherwise qijis set to 0. Also, we setci=Σj=1nwijanddj=Σi=1mwij.Matrix Factorization Problems: These instances are also generated using a random binary matrixH=(hij)where each entry has probability 0.5 to receive a value 1 or 0. Then, we setqij=1−2hij,and c and d are chosen as zero vectors.For each class of the benchmark problems discussed above, we generated 5 medium size instances withm=200,400,600,800,1000andn=1000and 5 large size instances withm=1000,2000,3000,4000,5000andn=5000. Then we introduced random partitions ofI={1,2,…,m}andJ={1,2,…,n}to generate the setsS1,S2,…,SpandT1,T2,…,Tp,respectively to obtain instances of BQP-PV. More precisely, for each (m, n) pair selected, p is set to 20, 40, 60, 80, 100 and k is set to 100. Then, the size of the subsetsSr,r=1,2,…,p−1are selected as a random integer in the range [0.8 m/p, 1.2 m/p]. The first |S1| elements of I are assigned to S1, the next |S2| elements are assigned to S2 and so on. After buildingp−1sets, we choose Spto include all the remaining elements of I. The setsT1,T2,…,Tkare constructed in an analogous way, where |Tu| is chosen as a random integer in the range [0.8 n/k, 1.2 n/k]. All the algorithms in our experiments were coded in C++ and compiled using the GNU g++ compiler on a Linux workstation with an Intel Xeon E5440 2.83GHz CPU and 8GB RAM.To assess the relative power of the swap neighborhood, concurrent swap neighborhood, and the optimized swap neighborhood, we performed preliminary tests using random instances using the best improving local search framework. The search path is diversified using 10 multi starts with random starting solutions. The results are summarized in Tables 1, 2, and 3.The quality of the local maximum obtained by optimized swap is better than that of the swap neighborhood or the concurrent swap neighborhood. However the computational time for our local search algorithm with the optimized swap neighborhood (OSLS) is much larger. This prompted us to the possibility of using the optimized swap neighborhood as a “cutting neighborhood” within a variable neighborhood search (Hansen & Mladenović, 2001; Mladenović & Hansen, 1997) framework. More precisely, when the local search algorithm with swap or concurrent swap neighborhood reaches a local optimum, we employ the optimized swap neighborhood to move out of the local optimum. If it finds an improved solution, we go back to the original neighborhood and local search is continued. If the optimized swap neighborhood cannot improve the current local optimum, the local search is terminated and we restart the search with another random starting solution. This type of local search that combines swap and optimized swap neighborhoods is called swap-optimized swap variable neighborhood search (S-OS-VNS). Similarly, the local search algorithm that combines concurrent swap and optimized swap neighborhoods as discussed above is called concurrent swap-optimized swap variable neighborhood search (CS-OS-VNS). We observed that for these hybrid algorithms computational time is reduced by a factor of approximately 5 for large instances in comparison to OSLS while retaining the quality of the optimized swap solutions. In Tables 4and 5we present experimental results for the algorithms OSLS, S-OS-VNS, and CS-OS-VNS by fixing their computational time to 10 minutes for medium instances and 60 minutes for large instances. For each problem size, 10 different problems are generated and the average, minimum, and maximum values of the objective function over 20 runs and number of iterations are noted. The focus of this experiment is to study the relative solution quality of the algorithms when running time is fixed. In the tables, the column ’time’ refers to the CPU time taken to reach the best solution.We applied the Wilcoxon nonparametric statistical test on the data generated by the algorithms S-OS-VNS, CS-OS-VNS and OSLS to verify the observed differences in terms of solution quality are statistically significant. All three variants performed well for the medium instances. However, for the large instances, the p-values obtained by Wilcoxon tests for S-OS-VNS versus OSLS, and CS-OS-VNS versus OSLS are 0.003504 and 0.01647, respectively. This indicates that both S-OS-VNS and CS-OS-VNS are significantly better than OSLS in terms of the average solution quality and demonstrates the effectiveness of the neighborhood combination for the challenging instances. In addition, no significant difference have been noted comparing S-OS-VNS versus CS-OS-VNS.Multi-start provides reasonable diversification of the search path for local search algorithms and it is one of the mechanisms used by researchers to explore various regions of the solution space. It is well known that examining the neighboring region of a local optimum could lead to good quality solutions. To achieve this, we have implemented tabu search (Benlic & Hao, 2013; Glover & Hao, 2010; Glover, Kochenberger, & Alidaee, 1998; Glover, Laguna, & Publishers, 1997; Lü, Glover, & Hao, 2010; Wang, Lu, Glover, & Hao, 2012) enhancements to our local search algorithms. Our swap tabu search algorithm (STS) implements a simple tabu search framework using the swap neighborhood. Tabu conditions are enforced by prohibiting reversal of a swap operation for a fixed time-period (tabu tenure). Specifically, once a swap move (xαr,xi) is performed (resulting inxi=1andxαr=0), the variable xiis prohibited from being assigned with value 0 for t0 ∈ [25, 40] iterations, andxαris prohibited from being assigned with value 1 fort1=t0*((m+n)/(K+L)−1)iterations. The asymmetry of tabu tenure values t0 and t1 is attributed to the significant differences in the number of 0-values versus the number of 1-values in a feasible solution. A move is considered tabu if both variables involved in the swap move have tabu status. We also used the aspiration condition (Glover et al., 1997) that a move which is “better than best found solution so far” overrides tabu conditions, if any. Further, we set an upper limit α on the number of consecutive moves that does not improve the best solution found so far. Once this limit is reached, the algorithm restarts with another random starting solution.To further enhance this basic tabu search, we used the optimized swap neighborhood as follows: In STS, if the best known solution x1 is not improved for α consecutive iterations, we start a local search using the optimized swap neighborhood with the best solution x1 as the starting solution, leading to a new local optimum x2. If the objective function value of x2 is better than that of x1, then we switch to STS with x2 being the starting solution. Otherwise, we restart STS with a new random solution. We call this the STS-OSLS hybrid algorithm (STS-OSLS). If we perform one iteration of OSLS instead of the full OSLS local search as in STS-OSLS, we get another variation of the tabu search algorithm which we call the STS-OS hybrid algorithm (STS-OS). Since all the algorithms we evaluate are already shown, we summarize the main ingredients of them in Table 6.The Wilcoxon nonparametric statistical test was used for results on Tables 7 and 8 to quantify the relative differences between the performances of STS, STS-OSLS and STS-OS algorithms in terms of solution quality. All the three algorithms performed well for the medium size instances and showed little differences in solution quality. However, for the large size instances, the p-values obtained by Wilcoxon tests for STS versus STS-OSLS and STS versus STS-OS are 0.01174 and 0.04189, respectively. This indicates that both STS-OSLS and STS-OS are significantly better than STS in terms of the average solution quality and demonstrates the efficacy of the hybrid algorithms using different neighborhoods. In addition, no significant difference in performance was observed in testing STS-OSLS against STS-OS.Further, the Wilcoxon test comparing S-OS-VNS versus STS-OSLS and S-OS-VNS versus STS-OS in terms of the average solution quality results in p-values of 0.002653 and 0.00207, respectively for large size instances and 0.0007068 and 0.0005847, respectively for medium size instances. This demonstrates the superiority of the tabu search enhancement.Tables 9and 10provide comparative performance of the six winning algorithms emerged from our study. The tables summarize the instance ID and the best found objective function value. For the medium instances, both STS-OSLS and STS-OS are able to reach the best objective function values for all the 25 instances. STS performs slightly worse, failing to reach the best objective value for 1 instance. The algorithms S-OS-VNS, CS-OS-VNS and CSLS that exclude the tabu search enhancement, obtain the best objective values for 16, 14 and 16 instances, respectively. For the large size instances, STS-OSLS delivers the best performance by reaching the best objective values for 14 instances.

@&#CONCLUSIONS@&#
