@&#MAIN-TITLE@&#
GSVM: An SVM for handling imbalanced accuracy between classes inbi-classification problems

@&#HIGHLIGHTS@&#
A new support vector machine has been designed.This model provides a more balanced accuracy between classes in a classification problem.This model is the most appropriate in order to manage on imbalanced accuracy.The results are very promising.

@&#KEYPHRASES@&#
Support vector machines,Imbalanced accuracy,Cost-sensitive schemes,Classification,Imbalanced databases,

@&#ABSTRACT@&#
A new support vector machine, SVM, is introduced, called GSVM, which is specially designed for bi-classification problems where balanced accuracy between classes is the objective. Starting from a standard SVM, the GSVM is obtained from a low-cost post-processing strategy by modifying the initial bias. Thus, the bias for GSVM is calculated by moving the original bias in the SVM to improve the geometric mean between the true positive rate and the true negative rate. The proposed solution neither modifies the original optimization problem for SVM training, nor introduces new hyper-parameters. Experimentation carried out on a high number of databases (23) shows GSVM obtaining the desired balanced accuracy between classes. Furthermore, its performance improves well-known cost-sensitive schemes for SVM, without adding complexity or computational cost.

@&#INTRODUCTION@&#
Accuracy is the usual measure to be maximized when a bi-classification problem is solved. Nevertheless, there exist problems where this solution is obtained with imbalanced accuracy: one class reaches much higher accuracy than the other class. This situation is common when learning from imbalanced datasets [26,34]. In bi-classification, it occurs when datasets provided for the learning algorithms contain many examples for one class (majority class), but very few for the other class (minority class). It is assumed that negative training instances are the majority and positive training instances are the minority. In this context, a model of high and balanced accuracy is difficult to be generated using traditional classification techniques [24]. Objective functions employed for the determination of the classifiers typically tend to favour the larger class [20,30], that is, the negative training examples overwhelmingly outnumber the positive ones making the classifier training to be imbalanced.It is worth noting that the degree of imbalance between classes is not the only factor hindering learning. Other factors contributing to classification deterioration are the presence of small disjuncts, the lack of density in the training data, the overlapping between classes, the identification of noisy data, the significance of the border-line instances, and the dataset shift between the training and the test distributions [30]. As it turns out, dataset complexity is the primary determining factor of classification deterioration, which, in turn, is amplified by the addition of a relative imbalance [20].Henceforth, let us consider that the problem of imbalance between the true positive rate and the true negative rate is addressed on imbalanced datasets, however keeping in mind that the problem of imbalanced accuracy is not exclusive of these datasets.The use of SVMs is an attractive option for handling these situations since their learning mechanism usually considers a small subset of patterns to build the classification model. SVMs are learning machines which implement the structural risk-minimization inductive principle to obtain generalization on a limited number of learning patterns [36]. This theory was developed on the basis of a separable binary classification problem where the optimization criterion is the width of the margin between the positive and negative examples which provides a good generalization performance, as shown in a number of applications [9]. However, like other learning machines that build classification models, SVMs aim to minimize the error on the entire dataset, and hence they are inherently biased towards the majority class when classifying imbalanced datasets [35].In order to improve the performance of SVMs on imbalanced datasets, a number of solutions have been proposed. Some of them, such as techniques for dataset sampling balance in a pre-processing stage, are of general application; other, more specific solutions, take into account particular features, for instance those based in cost-sensitive learning.The proposed approach takes into account that it has been empirically shown [21,29,43] that the hyperplane learned by an SVM in the presence of imbalanced sets has approximately the same orientation as the ideal hyperplane. Hence, reduced generalization on the minority class would be indeed associated with the bias, as positive instances lie far from this ideal limit, i.e., the SVM learns a boundary that is much too close to this class. Modifications should be based on the distribution of classes in the dataset, which also do not directly affect standard SVM training.Thus, a post-processing strategy, based on the consideration that the bias is a parameter to be tuned, is proposed in this paper. The function learned by a standard SVM will be adjusted through the bias to improve its performance, measured in terms of the geometric mean of recall and specificity. The proposed solution introduces no new parameters. Furthermore, it neither requires modifying the standard optimization problem for SVM training, nor additional steps, nor re-training.The remainder of this paper is organized as follows: Section 2 introduces several metrics commonly employed in classification problems. The standard SVM approach is outlined and an overview of strategies to improve performance in problems with imbalanced datasets is provided in Section 3. In Section 4, a family of classifiers which depend on the bias is examined and certain results are obtained on the metrics considered. Furthermore, a new SVM, called GSVM, is introduced based on the geometric mean. Experimentation is carried out in Section 5 in order to compare the performance of the GSVM with that of the standard SVM. Its performance is also compared with well-known cost-sensitive schemes. Finally, conclusions are drawn.LetZ={(x1,y1),…,(xN,yN)}be a training set, withxi∈Xas the input space,yi∈Y={+1,−1}the output space, and zi=(xi, yi). Let f(x) be a binary classifier such that outputs are obtained as h(x)=sign(f(x)), where sign(·) is the signed function.LetZpos={zi∈Z|yi=+1}≠∅andZneg={zi∈Z|yi=−1}≠∅be the sets of training patterns for the positive and the negative class, respectively. LetNpos=#ZposandNneg=#Znegbe the number of positive and negative instances, respectively, hence N=Npos+Nneg.The most commonly used metric for the evaluation of the generalization performance of the classifier f(x) on a test setD⊂X×Yis the Accuracy which is defined, from the confusion matrix (see Table 1), as follows:Accuracy(f,D)=tpos+tnegNpos+NnegThe Accuracy is inappropriate when prior probabilities of classes differ greatly, since metrics fail to consider costs for wrong classifications, and thus remain very sensitive to the bias between classes [20].Hence, other measures of assessment must be considered when the prior probabilities of classes differ so much. By considering accuracy rates onZposandZnegseparately, the Recall (or true positive rate) and Specificity (or true negative rate) metrics are defined as follows:(1)Recall(f,D)≡Rc(f,D)=tposNpos(2)Specificity(f,D)≡Sp(f,D)=tnegNnegFrom these metrics, an expression of Accuracy can be given:(3)Accuracy(f,D)=Npos·Rc(f,D)+Nneg·Sp(f,D)Npos+NnegHence, for imbalanced classes, Npos≪Nneg, Eq. (3) shows that Accuracy and Specificity are very similar, that is, the Recall metric barely alters the value of the Accuracy metric.A different metric should be considered when a more balanced ratio between Recall and Specificity metrics is desired. A natural option would be to consider the Arithmetic Mean,Am(f,D)=12(Recall(f,D)+Specificity(f,D))however it is well-known that the higher values in this measure compensate for the lower values.A measure which is more sensitive to the low positive values and that combines Recall and Specificity is the Geometric Mean[3], denoted by Gm, that is,(4)Gm(f,D)=Rc(f,D)·Sp(f,D)=tpos·tnegNpos·Nnegwhere both classes, positive and negative ones, have the same relevance for the purpose of classification.LetZ={(x1,y1),…,(xN,yN)}be a training set, withxi∈Xas the input space,yi∈Y={+1,−1}the output space, and zi=(xi, yi). Letϕ:X→F, x=ϕ(x), be a feature mapping with a dot product denoted by 〈·, ·〉. A binary linear classifierf:X→ℝ, defined as f(x)=〈x, w〉−b, is sought wherew∈F,b∈ℝ. Outputs are obtained as h(x)=sign(f(x)).The standard 2-norm C-SVM primal formulation leads to the optimization problem [36]:(5)minw∈F,b∈ℝ12∥w∥2+C∑i=1Nξis.t.yi(〈xi,w〉−b)+ξi≥1,ξi≥0,i=1…Nwhere w is the vector perpendicular to the hyperplane, which defines the orientation of the classifier, and bias b determines its position. Slack variables ξimeasure the error on the instances which violate constraints yi·(〈xi, w〉−b)≥1. The user-defined regularization parameter C determines the tradeoff between maximizing the margin and minimizing error, i.e. the higher the value of C, the more the SVM is focused on minimizing errors.In dual form, optimization problem (5) can be expressed as [17]:(6)maxαi∈ℝ∑iαi−12∑i,kαiyiαkyk〈xi,xk〉s.t.0≤αi≤C,∑iαiyi=0,i=1…Nleading to a solution w0=∑iαiyixi, where αiare Lagrange multipliers for the dual formulation (6). Term b is calculated a posteriori [18], which is denoted by [19] b0 (standard bias). Finally, the classifier is written asf(x)=∑i=1NαiyiK(xi,x)−b0=〈x,w0〉−b0whereK:X×X→ℝ, defined as K(x, y)=〈x, y〉=〈ϕ(x), ϕ(y)〉, is called the kernel function.For moderately imbalanced datasets, empirical results show that, unlike other machine learning algorithms, SVMs can produce a good hypothesis without any modification [1,43,23]. One explanation for this phenomenon is that SVMs use only the set of support vectors to construct classification models, and hence the negative instances far from the decision border are not be taken into account, and SVMs remain inaffected.However, SVMs cannot overcome the problem of imbalance between classes when the bias in the data distribution is significant. In these cases, it has been observed that the separation hyperplane learned by the SVM is very close to the minority class [43,29,21], resulting in a low performance or no generalization at all for examples from this class, in comparison with those from the majority class.Several strategies have been proposed to improve the performance of SVMs on imbalanced datasets: the most promising of which are described in this section. These strategies are introduced in accordance with the stage at which that they can be applied in the learning process. A deep study can be found in [12].They are primarily based on resampling techniques to balance the dataset, in order to build a new learning set where the minority class is better represented. Since the objective of these algorithms is to balance the dataset before training, they are general purpose procedures, not targeted at a particular machine learning [13,14].SMOTE is a widely used pre-processing strategy [7], which is based on the technique of k-nearest neighbours for over-sampling the minority class [38]. Other strategies can also be used, for example, by applying clustering algorithms for sub-sampling the majority class [47,28].Taking into account that the function learned by an SVM is built from the support vectors, certain proposals seek to increase representation of the minority class by considering only the margin area between the two classes. The information on the border can be obtained by applying heuristics based on the k-neighbour technique, as proposed in [6]. Other studies are based on the use of SVMs for obtaining the positive support vectors, and over-sample from this data [31,41]. This feature has also been exploited to build sub-sampling algorithms, as presented in [35], where an SVM is used to build a new dataset composed of only the negative support vectors and the positive data.Other solutions have been proposed which use sampling methods with combined classifiers. In this case, several SVMs are trained with different sets of data that have been balanced as a result of applying sub-sampling and/or over-sampling [25,29,42,45].These strategies include those proposals which modify the standard optimization problem for SVM training in order to incorporate information related to the proportions of classes in the dataset. One approach is through cost-sensitive learning which, to determine the classification model, incorporates into the learning problem information related with the costs of penalties associated with wrong predictions for each class. For imbalanced scenarios, a similar strategy is used, but with greater weight assigned to errors in the minority class.Hence, cost information about the two types of errors can be introduced into the formulation of the learning problem (5), using two regularization parameters [37,8], as follows:(7)minw∈F,b∈ℝ12∥w∥2+C(+)∑i|yi=+1ξi+C(−)∑i|yi=−1ξis.t.yi·〈xi,w〉−b+ξi≥1,ξi≥0,i=1…Nwhere C(+) and C(−) are the costs associated with errors in the positive and negative classes, respectively.Certain studies have also added new restrictions on the slack variables ξi, in order to control the margin of separation between the two classes [44,21]. A different approach is presented in [4], where only one regularization parameter C is used, although information about the cost of errors is incorporated by assigning a different weight to each slack variable ξi. As a result, each instance in the data is associated with a cost according to its importance in the classification task.Other proposed solutions combine cost-sensitive learning with other techniques. For example, the SMOTE algorithm is used in [1] to over-sample the minority class before training SVM with different costs. This strategy has been also used to build combinations of classifiers [39]. A last proposal, the KBA algorithm, is included in this strategic line as described in [43], which modifies the kernel matrix in accordance with to the observed imbalance in the distribution of data.In general, work following this strategy is oriented either towards modifying the weight vector w in the decision function or towards determining a new bias or threshold, in order to adjust the decision boundary learned by the SVM so that a good margin of separation for the positive class is provided.Hence, the z-SVM method is proposed in [23], which determines the value of a new parameter z, by solving an added optimization problem. This optimal parameter weights the contribution of support vectors of the minority class in the vector w of the decision function obtained after training.In [27], the bias of the decision function is modified by calculating an offset θ from the average of the unsigned values generated by f(x) for the support vectors. A similar strategy is used in [32], however the new threshold or offset θoptis calculated by applying the Beta-Gamma algorithm [46].Other studies suggest replacing the standard classification scheme (based on the sign function), with other schemes which enable another interpretation of the outputs of the SVM in unbalanced scenarios. For example, a fuzzy decision function is employed in [27] whose parameters are estimated from the observed distribution in the data set. In [40], the decision process incorporates a post-processing module, whose construction is based on methods of information theory, to define a new threshold for classification.The strategies proposed to improve the performance of SVMs on imbalanced datasets generally require tuning new parameters, such as the sample rate, and the number k of selected neighbours. Other methods can be computationally expensive since they consider the construction of several classifiers (such as methods based on ensembles), or they are based on iterative algorithms (such as those which modify the kernel matrix, and certain sampling techniques that require several overtraining steps). In cost-sensitive approaches, the standard SVM optimization problem must be modified and costs of errors on the classes must be known. Furthermore, these approaches can produce over-fitted models [39].On the other hand, as mentioned earlier, it has been also empirically shown [21,29,43] that the hyperplane learned by an SVM in the presence of imbalanced sets has approximately the same orientation as the ideal hyperplane. Reduced generalization on the minority class would be indeed associated with the bias, b, as positive instances lie far from this ideal limit, i.e., the SVM learns a boundary that is much too close to this class. Other studies, such as those presented in [33] in the domain of text classification, suggest increasing research on strategies which determine new thresholds for the SVM decision function. Modifications should be based on the distribution of classes in the dataset, which also do not directly affect standard SVM training.Thus, a post-processing strategy, based on the consideration that the bias is a parameter to be tuned, is introduced in the next section. The proposed approach inserts no new parameters, neither requires modifying the standard optimization problem for SVM training, nor additional steps, nor re-training.Let us now consider the set of classifiers fb(x)=〈x, w0〉−b, where w0 has been obtained by solving (5), andb∈ℝis a free parameter to be tuned. A mapΘb:X→−1,+1can be defined from classifiers fb(x) such that, given an input vector x, it assigns a label as follows:Θb(x)=sign(fb(x))=+1if〈x,w0〉≥b−1otherwiseProposition 1The Recall function,Rc:ℝ→[0,1], defined from(1)asRc(b)=Rc(fb,Z)is a decreasing function. Furthermore,maxb∈ℝRc(b)=Rc(β)=1andminb∈ℝRc(b)=0,∀b>β*, whereβ=minzi∈Zpos〈xi,w0〉, andβ*=maxzi∈Zpos〈xi,w0〉.ProofLetb,b′∈ℝ, with b≤b′. Givenzi∈Zpos, it holdsfb(xi)≥fb′(xi). Thus, ifΘb′(xi)=+1, then Θb(xi)=+1, and therefore Rc(b)≥Rc(b′). Hence, Rc(·) is a decreasing function.(i)Ifβ=minzi∈Zpos〈xi,w0〉, then〈xi,w0〉≥β,∀zi∈Zpos. HenceΘβ(xi)=+1,∀zi∈Zpos, and therefore tpos=Nposand Rc(β)=1.Ifβ*=maxzi∈Zpos〈xi,w0〉, then〈xi,w0〉≤β*,∀zi∈Zpos. Hence, if b>β*, thenΘb(xi)=−1,∀zi∈Zpos, and therefore tpos=0 and Rc(b)=0.Proposition 2The Specificity function,Sp:ℝ→[0,1], defined from(2)asSp(b)=Sp(fb,Z)is an increasing function. Furthermore,minb∈ℝSp(b)=Sp(α*)=0, andmaxb∈ℝSp(b)=1,∀b>α, whereα*=minzi∈Zneg〈xi,w0〉andα=maxzi∈Zneg〈xi,w0〉.ProofLetb,b′∈ℝ, with b≤b′. Givenzi∈Zneg, it holdsfb(xi)≥fb′(xi). Hence, if Θb(xi)=−1, thenΘb′(xi)=−1, and therefore Sp(b)≤Sp(b′). Hence, Sp(·) is an increasing function.(i)Ifα*=minzi∈Zneg〈xi,w0〉, then〈xi,w0〉≥α*,∀zi∈Zneg. HenceΘα*(xi)=+1,∀zi∈Zneg, and therefore tneg=0 and Sp(α*)=0.Ifα=maxzi∈Zneg〈xi,w0〉, then〈xi,w0〉≤α,∀zi∈Zneg. Hence, if b>α, thenΘb(xi)=−1,∀zi∈Zneg, and therefore tneg=Nnegand Sp(b)=1.An example of these properties for the two considered metrics is depicted in Fig. 1.The α*, α, β, and β* values are key elements in the development of the SVM, since the optimum in the usual metrics based on the confusion matrix can be found in intervals determined from these elements. Clearly, α*≤α and β≤β*, however there is also a relationship between α* and β*.Lemma 1Using definitions from Propositions1 and 2, it can be concluded that α*≤β*.ProofFor α≤β, the proof is trivial.For the case β<α, thenZis a non-linearly separable set [16]. Let us suppose α*>β*, then b*=−(α*+β*)/2 is such thatβ*<−b*<α*,and∀zi∈Zposand∀zj∈Zneg:〈xi,w0〉≤maxzi∈Zpos〈xi,w0〉=β*<−b*→〈xi,−w0〉>b*,〈xj,w0〉≥minzj∈Zneg〈xj,w0〉=α*>−b*→〈xj,−w0〉<b*.Thus, a vector (−w0) and a value b* exist such that the positive and negative instances ofZare separated linearly, that is,Zis a linearly separable set. This is a contradiction, therefore α*≤β* holds.   □From the results above, Proposition 3 can be enunciated.Proposition 3The geometric mean function,Gm:ℝ→[0,1], defined from(4)asGm(b)=Gm(fb,Z)verifies that(8)maxb∈ℝGm(b)=maxb∈[λ,β*]Gm(b)whereλ=maxα*,β.ProofIf α<β, then from (4), Propositions 1 and 2, it can be affirmed that Gm(b)=1 for any α<b≤β. Therefore, the maximum of Gm is attained in the interval [λ, β*], since λ=β.For the case β≤α, then the maximum of Gm is attained in the interval [λ, β*], since•Ifb≤λ=maxα*,β, then–ifmaxα*,β=α*, then Sp(b)=0, hence Gm(b)=0≤Gm(α*)=Gm(λ).ifmaxα*,β=β, then Rc(b)=1. Since Sp(b) is an increasing function, it is concluded that Gm(b)≤Gm(β)=Gm(λ).If β*<b, then Rc(b)=0 and Gm(b)=0≤Gm(β*).LetZpos={(x1,+1),…,,(xNpos,+1)}={(p1,+1),…,,(pNpos,+1)}be the set of training patterns with positive labels, withpi=xσ*(i), where σ* is a permutation of Nposelements such that(9)β=〈p1,w〉≤⋯〈pi,w〉⋯≤〈pNpos,w〉=β*In this form, biases bi=〈pi, w〉, i=1, …, Nposare defined in increasing order. The following result shows a more explicit way to find the solution for the maximum of the geometric mean.Proposition 4The Gm function,Gm:ℝ→[0,1], verifies thatmaxb∈ℝGm(b)=maxi=1,…,NposGm(bi)ProofFrom (9),−∞<β=b1≤⋯≤bNpos=β*<+∞. Hence,•If b≤b1, then Rc(b)=Rc(b1)=1, Sp(b)≤Sp(b1), and therefore Gm(b)≤Gm(b1).Ifb>bNpos, then0=Rc(b)<Rc(bNpos), and therefore0=Gm(b)≤Gm(bNpos).Ifb1<b≤bNpos, then an index i exists, 1<i≤Npos, such that bi−1<b≤bi. Hence, Rc(b)=Rc(bi), Sp(b)≤Sp(bi), and therefore Gm(b)≤Gm(bi).Furthermore, from the proof of Proposition 4, it is trivial to demonstrate the following result, which is stricter than (8).Proposition 5Letbm=minbi|λ≤biandbM=minbi|θ≤bi,whereθ=maxα,β*. Thereforemaxb∈ℝGm(b)=maxbm≤bi≤bMGm(bi)One way to maximize Gm is given in Proposition 5 by using the properties of the Recall and Specificity metrics. This is a key result for the development of the GSVM.Starting from w0, the solution vector of the optimization problem (5), the GSVM classifier is defined by modifying the standard bias b0,ΘGSVM(x)=+1if〈x,w0〉≥bGSVM−1if〈x,w0〉<bGSVMwhere the bias bGSVMis given as follows:•If α<β, then bGSVM=(α+β)/2. In this case, the problem (5) is linearly separable and the GSVM bias is equal to the standard bias b0[18]. Hence, if the problem is linearly separable, then the performance of the SVM and the GSVM is the same.If α≥β, thenbGSVM=argmaxm≤i≤MGm(bi)(from Proposition 5).According to the previous theoretical results, the classification performance will be always higher for GSVM than for SVM on the training set, when the Gm metric is employed. This implies that a more balanced performance between the true positive rate and the true negative rate should be attained.Experimentation is performed on the following standard UCI datasets [5]: Australian Credit Approval, Bupa liver disorders, Pima Indians diabetes, Iris Plants, Teaching assistant evaluation, Japanese Credit Screening, German Credit data, Wine Recognition, Lung Cancer, Haberman's Survival, Blood Transfusion Service Center, Vehicle, Vertebral Column, Hepatitis, Thyroid Disease, Vowel Recognition, Balance Scale Weight & Distance Database, Dermatology, Glass Identification, and Protein Localization Sites (Ecoli).Datasets are split into the form of one class, which is indicated in parentheses in the first column of Table 2, versus the rest of the classes. The positive class chosen for each dataset is that which contains the fewest instances. Additionally, another class have also been selected as positive for the Glass, Dermatology, and Ecoli datasets. Hence, up to 23 datasets have been used in the experimentation.A summary of the characteristics for these datasets is shown in Table 2. The imbalance ratio (IR) [11], defined as the ratio between the number of instances of the majority and the minority class, is provide to rank the different datasets according to this measure [15].There is no agreement in the research community on what threshold must be set up for a given dataset to suffer from the imbalance problem [12]. Nevertheless, the problem of imbalanced dataset is considered in our study only as a baseline in order to study the problem of imbalanced accuracy. Hence, let us define the imbalance accuracy ratio, denoted by IAR, as follows:IAR=maxRecall,SpecificityminRecall,SpecificityIn this form, the higher the value of IAR, the more imbalanced is the Accuracy. Experimentation is carried out by following a similar experimental framework to that suggested in [22,2]. So, training data is normalized in order to prevent problems with outliers, and test data is normalized accordingly. Performance is evaluated on models using the Radial Basis Function (RBF) kernel, with σ (RBF width) and C (regularization term) explored on a two-dimensional grid: σ=[2−4, 2−3, …, 25, 26] and C=[2−2, 2−1, …, 26, 27]. The criterion employed to estimate performance is the 3-fold cross-validation on the whole set of test data and this procedure is repeated 25 times in order to ensure good statistical behaviour. Performance is measured according to the Accuracy, Recall, Specificity and Gm metrics for the standard SVM, as well as for GSVM.The results can be checked in Table 3, where σ and C are the values of the parameters that provide the best Accuracy for the standard SVM and the best Gm for the GSVM, respectively. The proposed IAR measure is also provided.Some conclusions can be drawn from the empirical experimentation carried out:•The value of the Gm in the GSVM is always greater than the value of the Gm in the Standard SVM for any dataset, except in the Dermat (6) and Vowel datasets where no difference exists. Therefore, the main aim of the GSVM is achieved, that is, a more balanced performance between the true positive rate and true negative rate is attained in the test set (IARGSVM≤IARSVM).Dermat (6) and Vowel datasets can be considered as highly imbalanced datasets since IR≥10, however not imbalanced accuracy exists when a standard SVM is employed (IAR≤1.03). Therefore, no special strategies are needed in both datasets related with the imbalace between classes. The reason for this so good classification behaviour is that the SVM problem is linearly separable for both datasets.The value of the Accuracy in the GSVM is higher than the value of the Accuracy in the Standard SVM in six datasets (Australian, Bupa, Iris, Wine, Dermat (2), and Thyroid). Therefore, the GSVM also enables improvement in the Accuracy metric in several cases in addition to improvement in the Gm.In the Bupa, Tae, Japanese, German, Lung, Haberman, Transfusion, Hepatitis, Balance, Glass (4), Glass (5), and Ecoli (5) datasets, the value of Recall is low for SVM, although a significant improvement is attained with the GSVM. The value of Specificity is improved in the Australian, Pima, Wine, and Vehicle datasets.The Bupa dataset cannot be considered as a imbalanced dataset (IR=1.38), however there exists a high imbalance accuracy ratio (IARSVM=1.81), that is, Specificity is 81% higher than Recall. A similar conclusion can be drawn for Pima, Tae, Japanese, German, Lung, Haberman, Transfusion, and Hepatitis datasets.In order to compare the performance of this post-processing strategy with other methods reported in the literature, the cost-sensitive scheme provided by the Matlab Bioinformatic's toolbox is implemented to train SVM on the listed UCI datasets. Results obtained using the same evaluation metrics, as well as the same cross-validation structure are shown in Table 4, with σ and C being the values for the parameters providing the best Accuracy for the cost-sensitive SVM and the best Gm for the GSVM, respectively. Let us indicate that given a C value, the values for C(+) and C(−) in (7) used in the Matlab toolbox are (C·N)/(2·Npos) and (C·N)/(2·Nneg), respectively, that is, the ratio between them is the imbalance ratio,C(+)C(−)=NnegNpos=IRBased on the empirical experimentation carried out, a list of conclusions can be drawn:•GSVM obtains better results than cost-sensitive SVM in nine datasets (Australian, Bupa, Pima, Wine, Lung, Transfusion, Thyroid, Balance, and Dermatology) for all the metrics. However, cost-sensitive SVM approach behaves better than GSVM in two datasets (Ecoli (6) and Vowel) for all the metrics.With respect to the Accuracy, Specificity and Gm metrics, GSVM obtains better results than cost-sensitive SVM in 17 out of 23 datasets. The same result is obtained for the imbalance accuracy rate, IAR.With respect to the Recall metrics, GSVM is experimentally better than cost-sensitive SVM in 14 out of 23 datasets.The Wilcoxon Signed-Rank Test [10], a non-parametric statistical procedure, has been employed to analyse whether differences are significant for the results obtained when comparing the GSVM and the standard SVM approaches, as well as the GSVM and the cost-sensitive SVM approaches on the different datasets for each considered metric. In this way, Table 5has been obtained, where the p-value is displayed, which represents the lowest level of significance of a hypothesis resulting in a rejection.From this Table, for a confidence level fixed at 1%, it can be concluded that differences are significant for the results obtained between GSVM and standard SVM for all the considered metrics.When comparing GSVM and cost-sensitive SVM, for a 5% confidence level, then it can be concluded that the performance of GSVM is better than that of cost-sensitive SVM for the Gm metric, Accuracy metric and IAR. In terms of the Specificity metric, both methods can be considered equivalent. Furthermore, for a confidence level fixed at 9%, it can be also concluded that the performance of GSVM for the Recall metric is better than that of cost-sensitive SVM.

@&#CONCLUSIONS@&#
A new SVM has been designed whose main aim is to improve the geometric mean metric in order to obtain a more balanced performance between the true positive rate and the true negative rate.The proposed approach does not involve tuning new parameters and, furthermore, it neither requires modifying the standard optimization problem for training the SVM, nor does it require additional steps of re-training.Experimentation on 23 standard UCI datasets shows this approach to be a good alternative to standard SVM since the property to attain a more balanced performance between the positive and the true negative ratio is generalized to the test set.It is worth noting that the GSVM can be used with any variation for SVM; moreover this strategy can be applied to any classifier that incorporates a bias in its definition.