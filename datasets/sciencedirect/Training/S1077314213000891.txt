@&#MAIN-TITLE@&#
Structure preserving non-negative matrix factorization for dimensionality reduction

@&#HIGHLIGHTS@&#
We consider structure preservation and basis compactness in the NMF framework.We formulate our proposal as an optimization problem and iteratively solve it.The convergence of the solution algorithm is proved and experimentally validated.We qualitatively analyze the computational complexities of the proposed method.Experiments on several datasets show the superior performance of our work.

@&#KEYPHRASES@&#
Dimensionality reduction,Non-negative matrix factorization,Structure preserving,Basis compactness,Multiplicative update algorithm,

@&#ABSTRACT@&#
The problem of dimensionality reduction is to map data from high dimensional spaces to low dimensional spaces. In the process of dimensionality reduction, the data structure, which is helpful to discover the latent semantics and simultaneously respect the intrinsic geometric structure, should be preserved. In this paper, to discover a low-dimensional embedding space with the nature of structure preservation and basis compactness, we propose a novel dimensionality reduction algorithm, called Structure Preserving Non-negative Matrix Factorization (SPNMF). In SPNMF, three kinds of constraints, namely local affinity, distant repulsion, and embedding basis redundancy elimination, are incorporated into the NMF framework. SPNMF is formulated as an optimization problem and solved by an effective iterative multiplicative update algorithm. The convergence of the proposed update solutions is proved. Extensive experiments on both synthetic data and six real world data sets demonstrate the encouraging performance of the proposed algorithm in comparison to the state-of-the-art algorithms, especially some related works based on NMF. Moreover, the convergence of the proposed updating rules is experimentally validated.

@&#INTRODUCTION@&#
Recent years have witnessed large volumes of high-dimensional data in information retrieval, computer vision and pattern recognition, which have strongly motivated the development of technologies on dimensionality reduction [1]. Subspace learning, which dominates dimensionality reduction, has been widely exploited in image understanding and computer vision researches. It is helpful to reveal low dimensional structure embedded in high dimensional space. Namely, given an observed high dimensional data setX=[x1,…,xn]∈Rd×n, we explore the corresponding low dimensional (latent) pointsV=[v1,…,vn]∈Rp×n, where p≪d. Principal Component Analysis (PCA) [2–4] and Linear Discriminant Analysis (LDA) [5,4,6,7] have been the two most popular linear algorithms because of their relative simplicity and effectiveness. Optimal Component Analysis (OCA) [8,9] is proposed to search for an optimal space. To detect the underlying lower dimensional manifold structure, some manifold learning methods have been proposed, such as ISOMAP [10], Locally Linear Embedding (LLE) [11], Laplacian Eigenmap [12], Locality Preserving Projections (LPP) [13] and Neighborhood Preserving Embedding (NPE) [14]. Recently, the algorithms based on factor analysis [15–18] have become popular for data representation. Non-negative Matrix Factorization (NMF) [19] is an effective and attractive factor analysis method due to its theoretical interpretation and desired performance. NMF can support the parts-based representation of objects, which accords with the cognitive process of human brain from the psychological and physiological studies [20–22].NMF aims to find two non-negative matrices whose product provides a good approximation to the original matrix. It is optimal for learning the parts of objects because the non-negative constraints allow only additive combinations. Whereas, NMF assumes that data points are sampled from a Euclidean space and does not exploit the geometric structure of the data. Besides, there are no extra constraints on the embedding spaces corresponding to the non-negative matrices.In this paper, we propose a novel subspace learning method, named Structure Preserving Non-negative Matrix Factorization (SPNMF), to properly preserve the local affinity structure without the distortion of the distant repulsion property. The local affinity structure indicates the local neighborhood correlation, that is, if point xjis a neighbor point of xi, vjshould also be close to vi. The distant repulsion property is inspired by the observation that points which are far apart are generally semantically different. That is, the distant data points in the original space should be kept distant. Additionally, to learn better parts-based representation, we also require that a basis vector should not be further decomposed into more components and different bases should be as orthogonal as possible to reduce redundancy [23,24]. Therefore, we add the constraint of embedding basis redundancy elimination on NMF. We can optimize SPNMF via an iterative multiplicative updating algorithm and theoretically guarantee its convergence. We conduct extensive experiments on synthetic data and six real data sets to evaluate the performance of our method on clustering and classification tasks. The experimental results demonstrate that SPNMF outperforms NMF and its variants as well as other state-of-the-art dimensionality reduction methods, which reveals that it can indeed discover the low dimensional spaces embedded in high dimensional spaces.Our main contributions are summarized as follows:•To discover the intrinsic geometric and discriminating structure of the data space, we incorporate three kinds of explicit constraints: local affinity, distant repulsion, and basis redundancy elimination, into the standard NMF framework.We formulate our proposal as an optimization problem and provide an iterative multiplicative updating algorithm to solve it. A proof of the convergence of the proposed algorithm is also provided.We conduct extensive experiments to demonstrate the effectiveness of our proposed algorithm. Compared to other dimensionality reduction techniques, SPNMF achieves the best performance.We qualitatively analyze the computational complexities of the proposed algorithm and experimentally validate the convergence rate to demonstrate the algorithm efficiency quantitatively.The remainder of this paper is organized as follows. We present a brief review of some related work in Section 2. In Section 3, we propose our SPNMF algorithm and its learning procedure. The convergence of our method is also proved. Experimental results on synthetic data and six real data sets (widely used as benchmarks) are illustrated in Section 4. Finally, Section 5 discusses our conclusion with future work.

@&#CONCLUSIONS@&#
