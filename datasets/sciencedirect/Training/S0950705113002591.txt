@&#MAIN-TITLE@&#
Missing value imputation using decision trees and decision forests by splitting and merging records: Two novel techniques

@&#HIGHLIGHTS@&#
Two novel missing value imputation techniques.Justification of the basic concepts of the techniques through some empirical analyses.Experimentation on nine data sets, four evaluation criteria.Comparison with two existing techniques.A complexity analysis of all techniques.

@&#KEYPHRASES@&#
Data pre-processing,Data cleansing,Missing value imputation,Decision tree algorithm,Decision forest algorithm,EM algorithm,

@&#ABSTRACT@&#
We present two novel techniques for the imputation of both categorical and numerical missing values. The techniques use decision trees and forests to identify horizontal segments of a data set where the records belonging to a segment have higher similarity and attribute correlations. Using the similarity and correlations, missing values are then imputed. To achieve a higher quality of imputation some segments are merged together using a novel approach. We use nine publicly available data sets to experimentally compare our techniques with a few existing ones in terms of four commonly used evaluation criteria. The experimental results indicate a clear superiority of our techniques based on statistical analyses such as confidence interval.

@&#INTRODUCTION@&#
Organisations are extremely dependent nowadays on data collection, storage and analysis for various decision-making purposes. Collected data often have incorrect and missing values [1–3]. For imputing a missing value rij(i.e. the jth attribute value of the ith record ri) in a data set DF, an existing technique kNNI [4] first finds the k-most similar records of rifrom DFby using the Euclidean distance measure. If Aj∈A is a categorical attribute the technique imputes rijby using the most frequent value of Aj(the jth attribute) within the k-Nearest Neighbor (k-NN) records. If Ajis numerical the technique then utilizes the mean value of Ajfor the k-NN records in order to impute rij. While simplicity is an advantage of kNNI, a major disadvantage of the technique is the fact that for each record having missing value/s it needs to search the whole data set in order to find the k-nearest neighbors. Therefore, the technique can be expensive for a large data set [4]. Moreover, kNNI requires user input for k as it does not find the best k value automatically. It can be difficult for a user to estimate a suitable k value.Therefore, a suitable k value is often estimated automatically in some existing techniques such as Local Weighted Linear Approximation Imputation (LWLA) [5]. In order to estimate k automatically, LWLA first artificially creates a missing value rilfor a record rithat has a real missing value rij. Note that the original value of rilis known. The technique uses all possible k values, and for each k value it finds the set of k-NN records of ri. For each set of k-NN records, it then imputes the missing value rilusing the mean of the lth attribute of all records belonging to the set of k-NN records. Based on the imputed value and the actual value of ril, LWLA calculates the normalized Root Mean Squared Error (RMSE). RMSE is calculated for all sets of k-NN records. The best k value is estimated from the set of k-NN records that produces the minimum RMSE value. Finally LWLA imputes a real missing value rij, by using each record rmbelonging to the set of k-NN records (using the best k) of ri, the similarity between riand rm, and the jth attribute value of rm.LWLA uses the most relevant horizontal segment of a data set through the use of the best k-NN records. However, it does not pay attention to the relevance of the attributes and thus it does not divide a data set vertically. A recent technique called “Iterative Bi-Cluster based Local Least Square Imputation” (IBLLS) [6] divides a data set in both horizontal and vertical segments for imputing a numerical missing value, rij, of a record ri.Similar to LWLA, it first automatically identifies the k-most similar records for ri. Based on the k-most similar records it then calculates the correlation matrix R for the attributes with available values in riand the attributes whose values are missing in ri. It next re-calculates a new set of k number of records (for the same k that was automatically identified) using R, and a weighted Euclidean distance (WED) [6]. For the WED calculation the attributes having high correlation with the jth attribute (i.e. the attribute having a missing value for rij) are taken more seriously than the attributes having low correlations.IBLLS further divides the k-NN records vertically by considering only the attributes having high correlations with the jth attribute. Within this partition it then applies an imputation technique called Local Least Square framework [7] in order to impute rij. This procedure is repeated for imputing each missing value of ri. Similarly, all other records of the data set having missing values are imputed.Another group of techniques [8–10] uses classifiers such as a neural network for imputation. For example, a technique [10] uses a three layered perceptron network in which the number of neurons in both input and output layers are equal to the number of attributes of the data set DF. The method first generates a perturbed data set, Dpfrom DFby considering some available values as missing. Using the values of Dpinto the input layer and the values of DFinto the output layer, it then trains the network. It finally imputes the missing values of DFusing the trained neural network. The performance of a neural network can be improved by using a Genetic Algorithm (GA) in the training process of the network [8].Genetic Algorithm (GA) can also be used to estimate a suitable set of parameters of a Fuzzy C-means algorithm (FCM) which can then be used to impute missing values [11]. In an existing technique [11], a missing value is first imputed separately using a Support Vector Regression (SVR) and an FCM with user defined parameters. The imputed values are then compared to test their mutual agreement. If the imputed values are not similar then a GA technique is applied to re-estimate the parameters of FCM. The new parameters are used in FCM to impute the missing value again. The process continues until the FCM and SVR imputed values are similar to each other. GA can also be used to estimate suitable parameters for FCM by first artificially creating missing values, and then imputing the artificial missing values by FCM. Once the artificially created missing values are imputed then the difference between the original and imputed values can be used in GA as a fitness measure for further improvement of imputation [11]. Often FCM based imputation techniques are more tolerant of imprecision and uncertainty [12].Unlike the family of k-NN imputation techniques, EMI [13] takes a global approach in the sense that it imputes missing values using the whole data set, instead of a horizontal segment of it. For imputing a missing value EMI relies on the correlations among the attributes. The techniques using a localized approach, such as the family of k-NN imputation methods, have an advantage of imputing a missing value based on the k nearest neighbors (i.e. k most similar records) instead of all records. However, k-NN techniques find the k nearest neighbors based only on similarity and without considering correlations of attributes within the k nearest neighbors. Since EMI relies on correlations it is more likely to perform better on a data set having high correlations for the attributes. Correlations of the attributes are natural properties of a data set and they cannot be improved or modified for the data set. However, we realize that it is often possible to have horizontal segments (within a data set) where there are higher correlations than the correlations over the whole data set. Hence, the identification of the horizontal segments having high correlations and application of EMI algorithm within the segments is expected to produce a better imputation result.In this paper, we propose two novel imputation techniques called DMI [14] and SiMI. DMI makes use of any existing decision tree algorithm such as C4.5 [15], and an Expectation Maximisation (EM) based imputation technique called EMI [13]. SiMI is an extension of DMI. It uses any existing decision forest algorithm (such as SysFor [16]) and EMI, along with our novel splitting and merging approach. DMI and SiMI can impute both numerical and categorical attributes. We evaluate our techniques on nine data sets based on four evaluation criteria, namely co-efficient of determination (R2), Index of agreement (d2), root mean squared error (RMSE) and mean absolute error (MAE) [17,18]. Our experimental results indicate that DMI and SiMI perform significantly better than EMI [13] and IBLLS [6].The organization of the paper is as follows. Section 2 presents our techniques (DMI and SiMI). Experimental results are presented in Section 3. Section 4 gives concluding remarks.EMI based imputation techniques [13] rely on the correlations of the attributes of a data set. We realize/argue that their imputation accuracies can be high for a data set having high correlations for the attributes. We also realize that perhaps correlations among the attributes can be higher within some horizontal partitions of a data set than within the whole data set. Therefore, we focus on exploring such partitions for higher imputation accuracy. We use a decision tree or a decision forest to identify such natural partitions. A decision tree divides a data set into a number of leaves having sets of mutually exclusive records. A decision forest builds a number of decision trees.Let us assume that we build a forest having two trees Txand Ty(Fig. 1b and c) from a sample data set (Fig. 1a). Suppose a leaf L1 belonging to Txhas five records, L1={R1,R2,R3,R4,R5} (Fig. 1b and d) and a leaf L4 belonging to Tyhas five records, L4={R1,R2,R3,R6,R7} (Fig. 1c and e). Records belonging to any leaf are considered to be similar to each other since they share the same or very similar values for the test attributes [19,20]. For example, records belonging to the leaf L1 (in Tx) can be considered to be similar to each other since they share similar values for Aiand Aj. Similarly, records belonging to L4 (in Ty) are also similar to each other. If we take the intersection of L1 and L4 then we get their common records in the intersection, i.e. L1∩L4={R1,R2,R3} (Fig. 1f). Here, R1, R2 and R3 share the same or similar values for the test attributes (Ai, Aj, Aland Am) in both Txand Ty, whereas records belonging to the leaf L1 only share the same or similar values for the test attributes (Ai, and Aj) in the single tree Tx. Therefore, we expect that records belonging to a leaf have a strong similarity with high correlations for the attributes, and records belonging to an intersection have even stronger similarity and higher correlations.We now empirically analyse the concepts on a real data set. Applying C4.5 algorithm [15] on the Credit Approval (CA) data set [21] we build a decision tree, which happens to have seven leaves. We then compute the correlations among the attributes for all records of Credit Approval, and for the records belonging to each leaf separately. That is, first we compute the correlationCijWbetween two attributes, say Aiand Ajfor the whole data set (i.e. for all records). We then compute the correlationCijLbetween the same attributes Aiand Ajusing only the records within a leaf L, instead of all records in the data set. If the absolute value ofCijLis greater than the absolute value ofCijWthen we considerCijLas a better correlation thanCijW.For the six numerical attributes of the CA data set there are 15 possible pairs (6 choose 2) among the attributes, and therefore 15 possible correlations. Column B shows the number of pairs of attributes Aiand Ajfor a leaf L (shown in Column A) where we getCijL>CijW. For example, for Leaf 1 (as shown in Column A) we get 11 pairs of attributes (out of 15 pairs) that have higher correlations within the leaf than the whole data set. The first three columns, of Table 1, show that for all leaves (except Leaf 2 and Leaf 3), at least 11 out of 15 pairs have higher correlations within a leaf. Leaf 2 and Leaf 3 have a small number of records, as shown in Column C.A decision forest can be used for identifying records with even better similarity (among the records) and higher correlations (among the attributes). To investigate the concept, we apply an existing forest algorithm called SysFor [16] on the CA data set. For the purpose of demonstration of basic concepts, we build a forest of only two trees from CA and then take the intersections of the leaves. Note that the leaves of a single tree do not contain any common records. Therefore, we only take the intersection of two leaves Liand Ljbelonging to two different trees Txand Ty. Columns D, E and F of Table 1 are derived from the intersections of leaves from two different trees that are not shown due to space limitations. They show higher correlations within the intersections. For example, for Intersection 1 (shown in Column D) there are 13 pairs of attributes (shown in Column E), out of 15 pairs, having higher correlations within the intersection than the correlations within the whole data set. A number of intersections such as Intersection 2 (not shown in the table) have only 1 or no records in them, for which correlation calculation is not possible.We also calculate similarity among all records of the CA data set, all records within each leaf separately and all records within each intersection separately. Fig. 2indicates that the average similarity of the records within an intersection is higher than the average similarity of the records within a leaf. Additionally, the records within a leaf have higher average similarity than the similarity of the records within the whole data set. Similarity calculation steps are introduced in Step 4 of Section 2.3.Often the intersections can have a small number of records. We realize that the EMI based imputation can be less effective on such a small sized intersection. In order to explore the impact of an intersection size on the imputation accuracy, we apply EMI on different numbers of records of the Credit Approval and CMC data sets [21]. We first artificially create a missing value in a record, and then create a sub-data set of size τ by taking τ-nearest neighbors (τ-NNs) of the record having a missing value. We then apply EMI on each sub-data set to impute the missing value. Fig. 3shows that both CMC and Credit Approval achieve high accuracy for τ=25 in terms of RMSE evaluation criteria. The accuracy then does not increase drastically with the increase of size any more.In order to avoid applying EMI on a small sized intersection, another basic concept of our technique is to merge a small sized intersection with a suitable target intersection in such a way so that the merged intersection produces the best possible correlations and average similarity among the records. The merging process is shown in Fig. 1f and g, where the record R10 is merged with the records R8 and R9 in order to increase the similarity and correlations. The merging process is presented in details in Step 4 of Section 2.3.We argue that the accuracy of EMI may depend on three factors: (a) size of an intersection, (b) similarity (i.e. inverse distance) among the records of the intersection and (c) correlations among the attributes within the intersection. A suitable size for an intersection can be 25 records as shown in Fig. 3. While merging a small sized intersection with a target intersection we aim to increase both similarity and correlations. In order to explore a balance between similarity and correlations we carry out another initial test, where we first introduce a Similarity–Correlation threshold λ, the value of which varies between 0 and 1. Now, λ=1 indicates that the target intersection is selected based on only the influence of similarity and λ=0 indicates that it is selected by considering only the correlations. We informally test the impact of various λ values on Credit Approval [21] by imputing missing values using our algorithm called SiMI, which we introduce in detail in Section 2.3, with different λ values. Fig. 4shows that we generally get the best imputation accuracy at λ=0.7 for R2, d2, RMSE and MAE evaluation criteria. Since λ is a proportion between similarity and correlation we call it the Similarity–Correlation threshold.Please note that in the above example, we use a forest of two trees only to demonstrate the basic concepts. However, as shown in Algorithm 2 by default we use 5 trees unless a user defines it otherwise. In all our experiments forests of 5 trees are used. If we use more trees then they will produce a bigger number of intersections where many of them will have a very low number of records resulting in a higher number of intersections requiring merging. Both the creation of a bigger number of intersections and merging many intersections will cause higher computational complexity. Additionally, since many intersections will be merged there is no point in having the huge number of intersections. We also carry out empirical analysis on the CA and CMC data sets to evaluate the impact of the number of trees, k in terms of accuracy of imputation (RMSE), number of intersections (NOI) and execution time (in ms). Fig. 5shows that k=5 gives an overall good result on Credit Approval (CA) and CMC data sets.Generally τ=25, k=5, λ=0.7 and l=100 can be a good option. However, these values can differ from data set to data set. A possible solution for a user to make a sensible assumption on these values can be to try different values and compare the imputation accuracies. For example, if a user has a data set DFwith some missing values then he/she can first produce a pure data set DCby removing the records having missing values. The user can then randomly select attribute values of DCin order to artificially create missing values where the original values are known to him/her. He/she can then use different values of τ, k, λ and l and impute the artificially created missing values. The values of τ, k, λ and l that produce the best imputation accuracy can then be used in imputing the real missing values in DF. A user may run the evaluation many times. For example, he/she can run it 30 times to create 30 data sets with missing values. A similar approach was taken in the literature to find a suitable k value for k-NN based imputation [22].We first mention the main steps of DMI as follows and then explain each of them in detail.Step-1:Divide a full data set DFinto two sub data sets DCand DI.Build a set of decision trees on DC.Assign each record of DIto the leaf where it falls in. Impute categorical missing values.Impute numerical missing values using EMI algorithm within the leaves.Combine records to form a completed data setDF′without any missing values.DMIStep-1:Divide a full data setDFinto two sub data setsDCandDI.To impute missing values in a data set, we first divide the data set DFinto two sub data sets DCand DI, where DCcontains records having no missing values and DIcontains records having missing values (see Step-1 of Algorithm 1).Build a set of decision trees onDC.In this step we first identify attributes Ai(1⩽i⩽M), where M is the total number of attributes in DI, having missing values. We make a temporary copy of DCintoDC′. For each attribute Aiwe build a decision tree (considering Aias the class attribute) from the sub data setDC′, using any existing algorithm (such as C4.5 [15]). If Aiis a numerical attribute, we first generalize AiofDC′into NCcategories, where NCis the squared root of the domain size of Ai(see Step-2 of Algorithm 1).In Step 2 of Algorithm 1, we also introduce a boolean variable Ijwith initial value “FALSE” for a data set djbelonging to a leaf, for each leaf of all trees. The boolean variable is used in Step 4 to avoid unnecessary multiple imputation on a data set.Assign each record ofDIto the leaf where it falls in. Impute categorical missing values.Each record rkfrom the sub data set DIhas missing value/s. If rkhas a missing value for the attribute Aithen we use the tree Ti(that considers Aias the class attribute) in order to identify the leaf Rj, where rkfalls in. Note that rkfalls in Rjif the test attribute values of Rjmatch the attribute values of rk. We add rkin the segment djrepresenting Rjof Ti(see Step 3 of Algorithm 1). If Aiis a categorical attribute then the missing value of Aiin rkis imputed by the majority class value of Aiin dj. Otherwise, we use the Step 4 to impute numerical missing values.While identifying the leaf of Tiwhere rkfalls in, if we encounter a test attribute the value of which is also missing in rkthen we will not be able to identify the exact leaf in Tifor rk. In that case we consider any leaf that belongs to the sub-tree of the test attribute the value of which is missing in rk.Impute numerical missing values using EMI algorithm within the leaves.We now impute numerical missing values for all records in DIone by one. For a record rkwe identify a numerical attribute Aihaving a missing value. We also identify the data set djwhere rkhas been added (in Step 3) for the imputation of a missing value in the numerical attribute Ai. If djhas not been imputed before (i.e. if Ijis FALSE) then we apply the EMI algorithm [13] for imputation and thereby impute the values of Aiin dj(see Step-4 of Algorithm 1).There are two problematic cases where the EMI algorithm needs to be used carefully. First, the EMI algorithm does not work if all records have the same value for a numerical attribute. Second, the EMI algorithm is also not useful when, for a record, all numerical values are missing. DMI initially ignores the attribute having the same value for all records. It also ignores the records having all numerical values missing. DMI then imputes all others values as usual. Finally, it uses the mean value of an attribute to impute a numerical missing value for a record having all numerical values missing. It also uses the mean value of an attribute to impute a missing value belonging to an attribute having the same value for all records.Combine records to form a completed data setDF′.We finally combine DCand DIin order to formDF′, which is the imputed data set.We first introduce the main steps of SiMI as follows, and then explain each of them in detail.Step-1:Divide a full data set (DF) into two sub data sets DCand DI.Build a decision forest using DC.Find the intersections of the records belonging to the leaves of the forest.Merge a small sized intersection with the most suitable intersection.Impute numerical missing values using EMI algorithm and categorical missing values using the most frequent values.Combine records to form a completed data setDF′without any missing values.Step-1:Divide a full data set (DF) into two sub data setsDCandDI.Similar to DMI, we first divide a data set DFinto two sub data sets DCand DI(see Step-1 of Algorithm 2). In this step, we also use an existing algorithm [23] to estimate the similarity between a pair of categorical values belonging to an attribute, for all pairs and for all categorical attributes. The similarity of a pair of values can be anything between 0 and 1, where 0 means no similarity and 1 means a perfect similarity.Build a decision forest onDC.We build a set of k number of decision trees T={T1,T2,⋯,Tk} on DCusing a decision forest algorithm such as SysFor [16], as shown in Step-2 of Algorithm 2. k is a user defined number.Find the intersections of the records belonging to the leaves of the forest.Let the number of leaves for T1,T2,⋯,Tkbe L1,L2,⋯,Lk. The total number of possible intersections of all trees is L1×L2×⋯×Lk. We then ignore the intersections that do not have any records in it. See Algorithm 2 and Procedure Splitting (see Algorithm 3).SiMIProcedure Splitting ()Procedure Merging ()Step-4:Merge a small sized intersection with the most suitable intersection.We first normalize the numerical values of the whole data set D into D′, and thereby convert the domain of each numerical attribute to [0,1]. The CreateIntersections() function (see Algorithm 4) re-creates all intersectionsYi′;∀ifrom D′. If the size of the smallest intersection is smaller than a user defined intersection size threshold τ then we merge the smallest intersection with the most suitable intersection. The most suitable intersection is the one that maximizes a metric Sj(see Algorithm 4) based on the similarity among the records and correlations among the attributes within the merged intersection.Similarity Wjbetween two intersections is calculated using the average distance of the pairs of records, where the records of a pair belong to two intersections. Distance between two records is calculated using the Euclidean distance for numerical attributes and a similarity based distance for categorical attributes. If similarity between two categorical values i and j is sijthen their distance dij=1−sij. Similarity of categorical values belonging to an attribute varies between 0 and 1, and is estimated based on an existing technique [23].Once two intersections are merged, we repeat the same process if the number of records in the smallest intersection is smaller than τ. L2 norm [24] of the correlation matrix of a merged intersection is calculated by∑i=1r∑j=1cxi,j2|A|×|A|, where r is the number of rows of the correlation matrix, j is the number of columns, xi,jis the value of the ith row and jth column of the correlation matrix, and ∣A∣ is the number of attributes of the data set.Impute numerical missing values using EMI algorithm and categorical missing values using the most frequent values.Missing values of all records in DIare imputed one by one. For a record rk∈DI, we first identify the intersection Yj∈Y, where rkbelongs to. If a missing attribute Aiis numerical then we apply the EMI algorithm [13] for imputing all numerical missing values in Yj. If the attribute Aiis categorical, we use the most frequent value of Aiin Yjas the imputed value of Ai.We finally combine DCand DIin order to form a completed data setDF′without any missing values.We now analyse complexity for DMI, SiMI, EMI and IBLLS. We consider that we have a data set with n records, and m attributes. We also consider that there are m′ attributes with missing values over the whole data set, nIrecords with one or more missing values, and ncrecords (nc=n−nI) with no missing values. DMI and SiMI uses the C4.5 algorithm [25,15] to build decision trees, which requires a user input on the minimum number of records in a leaf. We consider this to be l.First we analyse the complexity of DMI. Step 2 of the DMI algorithm uses the C4.5 algorithm which has a complexity O(ncm2) [26], if it is applied on ncrecords and m attributes. Step 4 uses the EMI algorithm which has the complexity O(nm2+m3), if EMI is applied on n records and m attributes. However, in Step 4 EMI is applied repeatedly on different data segments representing the leaves or logic rules. In the worst case scenario for the ith tree we can have at mostnclleaves where the minimum number of records for a leaf is l. IfnI⩽nclthen EMI is applied at most nItimes, else ifnI>nclthen it is applied at mostncltimes. Therefore, the maximum times EMI can be applied isnclresulting in the maximum complexity for EMI to beOncl(lm2+m3). Considering that there are m′ attributes with missing values resulting in m′ trees (following the for loops of Step 4) the complexity of the step isOm′ncl(lm2+m3).Generally, l is chosen to be a small number which is significantly smaller than nc. Therefore, we can ignore l from the complexity analysis. Moreover, we assume that nI≪nc, and nc≈n. Therefore the overall complexity for Algorithm 1 can be estimated as O(n2mm′+nm3m′). For typical data sets (such as those used in the experiments of this study) having n≫m the complexity is O(n2). However, for the data sets having m⩾n the complexity is O(m3) if m′≪m or O(m4) if m′≈m.We now analyse the complexity of SiMI algorithm (see Algorithm 2). Complexity of Step 1 for preparing DIand DCis O(nm). However, the complexity of similarity calculation [23] in Step 1 is O(nm2d2+m3d5), where we consider that the domain size of each attribute is d.The overall complexity of Step 2 isOncm2k+nc2mkl. Step 3 (see Algorithm 3) finds the non-empty intersections of the leaves for k trees. However, note that the number of non-empty intersections can only be at most n in the worst case scenario, and therefore L (in the algorithm) can be at most equal to n. We consider that each leaf has the minimum number of records (i.e. l) in order to allow us to consider the worst case scenario where the number of leaves is the maximum i.e.nl. Therefore, the complexity of this step is O(n2lk).Step 4 (see Algorithm 4) can have two extreme situations where initially we can have n number of intersections, each having 1 record. After merging the records we may eventually end up in another extreme scenario where we have only two intersections; one having τ records and the other one having (n−τ) records. Considering both situations in two iterations of the while loop we find the worst case complexity of the step is O(n2m+nm2).In Step 5 we can have at mostnτnumber of intersections altogether. Therefore, EMI algorithm may need to be applied at mostnτtimes resulting in the complexity of the step to beOnτ(τm2+m3). Considering τ to be very small the complexity of the Step is O(nm3). The complexity of Step 6 is O(nm).Typically, d, k, l and τ values are very small, especially compared to n. Besides, we can also consider nIto be very small and therefore nc≈n. Hence, the overall complexity of SiMI is O(n2m+nm3). Moreover, for low dimensional data sets such as those used in this study the complexity is O(n2). We estimate the complexities of EMI and IBLLS (i.e. the techniques that we use in the experiments of this study) as O(nm2+m3) and O(n3m2+nm4), respectively. This is also reflected in the execution time complexity analysis in the next section (see Table 6).We implement our imputation techniques i.e. DMI and SiMI and two existing techniques, namely EMI [13] and IBLLS [6]. The existing techniques were shown in the literature to be better than many other techniques including Bayesian principal component analysis (BPCA) [27], LLSI [7], and ILLSI [28].We apply the techniques on nine real life data sets as shown in Table 2. The data sets are publicly available in UCI Machine Learning Repository [21]. There are some data sets that already contain missing values as indicated by the column called “Missing” in Table 2. For the purpose of our experiments, we first need a data set without any missing values to start with. Therefore, we remove the records having any missing values in order to prepare a data set without any missing values. We then artificially create missing values, in the data set, which are imputed by the different techniques. Since the original values of the artificially created missing data are known to us, we can easily evaluate the accuracy/performance of the imputation techniques.The performance of an imputation technique typically depends on both the characteristics/patterns of missing data and the amount of missing data [17]. Therefore, we use various types of missing values [14,17,29] and different amount of missing values as explained below.We consider the simple, medium, complex and blended missing patterns. In a simple pattern we consider that a record can have at most one missing value, whereas in a medium pattern if a record has any missing values then it has minimum 2 attributes with missing values and can have up to 50% of the attributes with missing values. Similarly, a record having missing values in a complex pattern has minimum 50% and maximum 80% attributes with missing values. In a blended pattern we have a mixture of records from all three other patterns. A blended pattern contains 25%, 50%, and 25% records having missing values in the simple pattern, medium pattern and complex pattern, respectively [17]. Additionally, for each of the missing patterns, we use different missing ratios (1%, 3%, 5% and 10%) where x% missing ratio means x% of the total attribute values (not records) of a data set are missing.We also use two types of missing models namely Overall and Uniformly Distributed (UD) [14]. In the overall model, missing values are not necessarily equally spread out among the attributes, and in the worst case scenario all missing values of a data set can even belong to a single attribute. However, in the UD model each attribute has equal number of missing values.Note that there are 32 combinations (4 missing patterns×4 missing ratios×2 missing models) of the types of missing values. For each combination, missing values are created randomly. Due to the randomness, every time we create missing values in a data set we are likely to have a different set of missing values. Therefore, for each combination we create 10 data sets with missing values, i.e. we create all together 320 data sets (i.e. 32 combinations×10 data sets per combination) with missing values for each natural data set.Due to the space limitation we present summarized results in Tables 3 and 4. For each data set, we aggregate the results based on missing ratios, missing models, and missing patterns. Aggregated result for 1% missing ratio means the average value for all combinations having 1% missing ratio, any missing pattern and any missing model. Bold values represent the best results and the values with italic font represent the second best results. It is clear from the tables that SiMI performs significantly better than other three techniques. SiMI and DMI never lose to EMI and IBLLS even for a single case for R2, d2 and RMSE. They only lose twice for MAE.Since due to space limitations we cannot present the detailed non-aggregated result, we present Table 5that displays the number of times a technique performs the best for an evaluation criteria. There are nine data sets and 32 combinations per data set, resulting in 288 combinations altogether. SiMI performs the best in 243 out of 288 combinations for R2. DMI performs the best in the remaining 45 combinations.In Fig. 6we present line graphs for average d2 values and 95% confidence intervals for all four techniques and all combinations with the Simple pattern. There are only three combinations that are encircled, where SiMI has either “lower averge”, or “higher average, but overlapping confidence intervals” with DMI. Obviously, DMI performs the second best. Higher average d2 values and non-overlapping confidence intervals indicate a statistically significant superiority of SiMI and DMI over existing techniques. Confidence interval analysis for all other patterns (not presented here) also demonstrates a similar trend. We also perform a t-test analysis, at p=0.0005, suggesting a significantly better result by SiMI and DMI over EMI and IBLLS. For the comparison between SiMI and IBLLS, the t values are higher than the t-ref value for all evaluation criteria on all data sets. For the comparison between SiMI and EMI, the t values are also higher than the t-ref value for all evaluation criteria, except some evaluation criteria on the CMC and CA data sets.Fig. 7presents the average performance indicators for 320 data sets (i.e. 32 missing combinations×10 data sets/combinations) for all techniques. SiMI and DMI clearly perform better than the existing techniques.Another advantage of SiMI and DMI is that unlike EMI and IBLLS, both of them can impute categorical missing values in addition to numerical missing values. Therefore, we also compare the imputation accuracy of SiMI and DMI based on RMSE and MAE for categorical missing values as shown in Fig. 8. In the figure we present the overall average (for all 32 combinations) of RMSE and MAE values for SiMI and DMI on all nine data sets.We now present the average execution time (in milliseconds) for 320 data sets (32 combinations×10 data sets per combination) for each real data set in Table 6. We carry out the experiments using two different machines. However, for one data set we use the same machine for all techniques. The configuration of Machine 1 is 4×8 core Intel E7-8837 Xeon processors, 256GB RAM. The configuration of Machine 2 is Intel Core i5 processor with 2.67GHz speed and 4GB RAM. Both SiMI and DMI take less time than IBLLS, whereas they take more time than EMI to pay the cost of a significantly better quality imputation.

@&#CONCLUSIONS@&#
