@&#MAIN-TITLE@&#
Retail store scheduling for profit

@&#HIGHLIGHTS@&#
Optimizing retail schedules under a profit-maximization criterion.A stochastic model of retail store sales in terms of a revenue decomposition.A CP and MIP model to solve a mixed integer non-linear problem (MINLP).A case-study with a retail chain which project net profit increases on the order of 2–3%.

@&#KEYPHRASES@&#
Shift scheduling,Constraint programming,Mixed integer programming,Statistical forecasting,Retail,

@&#ABSTRACT@&#
In spite of its tremendous economic significance, the problem of sales staff schedule optimization for retail stores has received relatively scant attention. Current approaches typically attempt to minimize payroll costs by closely fitting a staffing curve derived from exogenous sales forecasts, oblivious to the ability of additional staff to (sometimes) positively impact sales. In contrast, this paper frames the retail scheduling problem in terms of operating profit maximization, explicitly recognizing the dual role of sales employees as sources of revenues as well as generators of operating costs. We introduce a flexible stochastic model of retail store sales, estimated from store-specific historical data, that can account for the impact of all known sales drivers, including the number of scheduled staff, and provide an accurate sales forecast at a high intra-day resolution. We also present solution techniques based on mixed-integer (MIP) and constraint programming (CP) to efficiently solve the complex mixed integer non-linear scheduling (MINLP) problem with a profit-maximization objective. The proposed approach allows solving full weekly schedules to optimality, or near-optimality with a very small gap. On a case-study with a medium-sized retail chain, this integrated forecasting–scheduling methodology yields significant projected net profit increases on the order of 2–3% compared to baseline schedules.

@&#INTRODUCTION@&#
The retail sector accounts for a major fraction of the world’s developed economies. In the United States, retail sales represented about $3.9trillion in 2010, over 25% of GDP, employing more than 14M people at some $300B annual payroll costs (U.S. Census Bureau, 2011). Given these figures, it stands to reason that effective sales staff scheduling should be of critical importance to the profitable operations of a retail store, since staffing costs typically represent the second largest expense after the cost of goods sold (Ton, 2009): as a result, all efficiencies coming from better workforce deployment translate into an implicit margin expansion for the retailer, which immediately accrues to the bottom line.The best retailers today rely on a staff schedule construction process that involves a decomposition into three steps (Netessine, Fisher, & Krishnan, 2010). First, the future sales over the planning horizon are forecasted, usually a few weeks to one month ahead at a 15- to 60-minutes resolution. Second, this forecast is converted into labor requirements using so-called “labor standards” established by the business (e.g., every $100 in predicted sales during a given 15-minutes period requires an additional salesperson that should be working during that period). Finally, work schedules are optimized in a way that attempts to match those labor requirements as snugly as possible, while meeting other business and regulatory constraints (e.g., one may have that an employee cannot be asked to come to work for less than three consecutive hours or for more than eight hours in total, must have breaks that follow certain rules, etc.).It may be somewhat unsettling that nowhere does this process acknowledge, explicitly or tacitly, that in many retail circumstances, salespeople actively contribute to revenue by doing their job well—advising an extra belt with those trousers, or these lovely earrings with that necklace—and not only represent a salary cost item to store operations. In other words, the presence of an additional staff working at the right time drives up expected sales, a crucial dynamics ignored when sales forecast tranquilly descends “from above”.This paper, in contrast, formalizes the retail staff scheduling problem by formulating it as one of expected net operating profit maximization. It introduces a modeling decomposition that allows representing the expected sales during a time period as a function of the number of salespersons working, thereby capturing the impact of varying staffing hypotheses on the expected sales. The profit-maximizing labor requirements are obtained, for a given time period, as the number of employees beyond which the marginal payroll cost exceed the marginal revenue gain from having an additional staff working. Finally, it introduces new solution techniques that are able to capture the profit maximization aspects of the problem. On a case-study with a medium-sized Canadian clothing and apparel chain, this integrated forecasting–scheduling methodology yields significant projected net profit increases on the order of 2–3% with respect to baseline schedules.The literature on schedule optimization is quite vast; see Ernst, Jiang, Krishnamoorthy, Owens, and Sier (2004); Ernst, Jiang, Krishnamoorthy, Sier (2004) and Van den Bergh, Beliën, De Bruecker, Demeulemeester, and De Boeck (2013) for comprehensive reviews. Given the retail industry’s overall economic significance, the impact of staffing decisions has received some attention in the literature, albeit perhaps not any commensurate with the gains that are to be expected from improved planning. Thompson (1995) proposed a scheduling model that takes into accounts a linear estimate of the marginal benefit of having additional employees, but not accounting for understaffing costs. Lam, Vandenbosch, and Pearce (1998) and Perdikaki, Kesavan, and Swaminathan (2012) have shown that store revenues are causally and positively related to staffing levels, opening the door to staffing rules based on sales forecasts. Moreover, using data from a large retailer, Ton (2009) finds that increasing the amount of labor at a store is associated with an increase in profitability through its impact on conformance quality but, surprisingly, not its impact on service quality. Mani, Kesavan, and Swaminathan (2011) find systematic understaffing during peak hours in a study with a large retail chain, using a structural estimation technique to estimate the contribution of labor to sales. Additionally, in what is probably the most potent case to date for improving scheduling practices, Netessine et al. (2010) find—with another large retailer—strong cross-sectional association between labor practices at different stores and basket values, and observe in their examples that small improvements in employee scheduling and schedule execution can result in a 3% sales increase for a moderate (or zero) cost.Perhaps the contribution closest in flavor to what we suggest in this paper comes from the work of Kabak, Ülengin, Aktaş, Önsel, and Topcu (2008): similarly to the present paper, these authors introduce a two-stage approach to retail workforce scheduling based on a statistical forecasting model of sales incorporating an explicit labor effect. The profit-maximizing number of salespeople under this model is then computed to obtain the desired hourly staffing levels in the store, assuming workforce cost homogeneity. This is followed by a optimization phase, which assigns individual employees to a set of pre-existing shifts. The approach is then validated through simulation to demonstrate its effectiveness on a Turkish apparel store.As we show later, our paper extends this approach by suggesting the use of complete sales curves within the schedule optimization stage, better reflecting the uncertainty surrounding the sales response model (which often exhibits a rather flat shape around the optimal number of salespeople), and easily allows optimizing schedules with non-homogenous staff costs. To achieve this goal, we first propose a stochastic model of retail store sales in terms of a revenue decomposition that is particularly amenable to robust and modular modeling. This decomposition allows to obtain an accurate 15-minutes-resolution forecast of sales, conditional on any desired determinant, opening the path to operating profit maximization.Furthermore, in contrast to Kabak et al. (2008), the proposed schedule optimization does not use a set of predefined shifts, but rather builds those that, while meeting labor and union regulations, allow to maximize profit. The main difficulty is to efficiently combine two distinct sets of objectives and constraints: those of the retailer, defined in terms of the number of working employees in each time period, and those of the workers, defined in terms of the “friendliness” of the working shifts assigned to them. We formulate the profit-maximization problem as a mixed integer non-linear problem (MINLP) where the decision variables specify the work status (working, break, lunch, rest) of each employee, and we examine two ways of solving this problem. The first approach linearizes the problem and turns it into a mixed integer program (MIP), which is then solved either directly or approximately using piecewise linear functions. The second approach expresses the original MINLP as a constraint program (CP), which is solved directly. It provides better performance, especially on larger problems. The proposed problem formulations are based on the use of a regular language to specify the admissibility of shifts with respect to union agreements. Schedules are constructed both over a day and a week.The rest is organized as follows. In Section 2 we formulate the problem, setting forth overall schedule construction objectives. In Section 3, we explain the stochastic models we developed for sub-hour sales forecasts, and we evaluate the forecasting ability of these models on out-of-sample sales at several locations of a mid-sized clothing and apparel chain of retail stores. We continue in Section 4 with two scheduling problem formulations, one as a MIP and the other as a CP, whose solutions are employee schedules that maximize expected profits, over a day and over a week (as shown in Section 5). Section 6 concludes. A preliminary and summarized version of this work was previously presented in Chapados, Joliveau, and Rousseau (2011).Here we present an overview of the proposed approaches. The global schedule construction process has two main steps, summarized in Fig. 1:1.Analyzing the historical workforce sales performance to construct a stochastic model of store behavior and to build sales curves, which give the expected sales at each store for each time period, as a function of the number of staffs assigned to sales during that period.Using these sales curves to construct admissible work schedules, for the employees, that maximize the expected profit.Each sales curve provides a functional characterization of how the expected sales at a given store and time period are impacted by varying the staffing. A classical staffing demand curve, in contrast, indicates the desired number of employees at a given store, for each time period.As outlined earlier and in Fig. 1, we model store sales through a decomposition into two simpler quantities: (i) the number of items sold during a time period and (ii) the average price per item. Of course, sales could be forecast without such a decomposition, by applying standard statistical time series approaches directly. But at the high intra-day resolutions (15-minutes periods) that we are considering, we found forecasts based on the decomposition to be markedly more accurate than standard time series models having similar parametric complexity, not only in expectation but with respect to the whole distribution as well.An important driving factor affecting the number of items sold is the in-store traffic, of which we require a precise forecast at a high intra-day resolution over the entire planning horizon, accounting for all known explanatory variables and capable of yielding sensible forecasts for stores for which little amounts of historical data are available. To this end, we introduce a traffic forecasting methodology that allows sharing statistical power across multiple locations (for a chain of stores), yet allowing store-specific effects to be accurately handled.To forecast sales, a key aspect of our contribution is the ability to parsimoniously model the joint distribution between the two factors in the decomposition; to achieve this, we require a model of the full posterior distribution of the number of items sold during a time period, conditional on the known sales drivers. This distribution exhibits significantly higher dispersion than a conditional Poisson, and a standard Poisson Generalized Linear Model (McCullagh & Nelder, 1989)—which linearly relates the intensity of a conditional Poisson random variable to a set of covariates—proves insufficient for the purpose. We adapt the statistical technique of ordinal regression to obtain an accurate estimator of the posterior distribution of the number of items sold.The aforementioned decomposition is related to one often used by retail practitioners (e.g., Lam, Vandenbosch, Hulland, & Pearce, 2001) wherein sales during a given period are written as the product of three factors: the number of shoppers in the store during the period, the conversion rate (how many shoppers become buyers during that period), and the average basket (given that a shopper buys, how much does s/he buy on average). However, this conventional decomposition is very sensitive to having a precise measure of the number of shoppers in the store at any time—an information that many retailers cannot measure precisely, or do not have at all.A “demand curve” can be obtained from the sales curve by subtracting from the latter the wage cost incurred at each staffing level—yielding the profit curve—and finding, on the profit curve for each store/time-period pair, the staffing that maximizes profit. This demand curve is often useful for management purposes, providing a concise and interpretable summary of how many employees “should” be working at any particular time. As mentioned previously this type of optimal staffing for profit maximization is studied by Kabak et al. (2008) and also by Koole and Pot (2011) in the context of call centers where successful calls can bring revenue. However, as we show in Section 4, it is generally preferable to incorporate the sales curves directly into the mathematical program that constructs the schedules, rather than trying to fit the demand curve. This overcomes two limitations of traditional demand curves. Firstly, it allows true staff costs to be accounted for, at the individual employee level, allowing to dispense with a cost homogeneity assumption. Secondly, it frequently happens that the profit curve is fairly flat around its maximum, in which case there is a large confidence band around the “truly optimal” number of employees; this allows for more flexibility during the scheduling process to produce better schedules with only minor impact on the cost.In the retail store context, finding the optimal number of salespersons for each time period (the optimal staffing) is not sufficient, because there is generally no way to match these optimal numbers exactly given the various constraints on the admissible shifts of employees. We could think of a two-stage approach that computes the optimal staffing in a first stage and then finds a set of admissible shifts that cover these required staffings, say by having a number of salespersons larger or equal to the optimal number in each time period, at minimal cost. But such a procedure is generally suboptimal, as illustrated for example in Avramidis, Chan, Gendreau, L’Ecuyer, and Pisacane (2010) in the context of work schedule construction for telephone call centers.Here we solve the optimization problem in a single stage. The decision variables select a set of admissible working shifts, one for each employee, over the considered time horizon (one day or one week, for example). The objective is to maximize the total expected profit, rather than merely minimizing salary costs under staffing constraints as usually done. That is, we recognize explicitly the dual role played by employees, who generate costs from their salaries but also revenues by driving up sales. This gives a novel problem formulation.In the objective, the expected revenue as a function of the number of working employees, for each time period of each day, is given by the sales curves, which are generally nonlinear, and these expected revenues are added over the considered time horizon. The cost of each shift is assumed to be decomposable as a sum of costs over the time periods it covers, where the cost over a time period depends on the day, the period, and the activity performed over that period. These costs are added up over all selected shifts, and subtracted from the total expected revenues.This gives an integer programming problem with a nonlinear objective. The nonlinearity can be handled by various techniques, such as extending the formulation by using boolean variables for every possible number of working employees in each period, approximating the sales curves with piecewise linear functions, and modeling the nonlinear objective directly with a constraint program (see Section 4 for the details).The constraints are of two kinds. Some address operational issues of employers, such as imposing a minimal number of working employees at any given time, constraints on the number of full-time and part-time employees, and operating hours of the store. Other constraints deal with union regulations or labor law, such as minimum and maximum number of hours of work per day for an employee, minimum and maximum number of hours of work per week, break and meal placement regulations, minimum rest time (delay) between successive shifts for an employee, etc. Each shift specifies a sequence of activities over the entire time horizon (e.g., a day or a week). In our formulation, employees can have different availabilities, but are assumed homogenous in terms of selling efficiency.The data used for our case study was provided by a medium-sized chain of upscale clothing and apparel retail stores. It comes from 15 stores, located in major Canadian cities, which obey normal retail opening hours and sales cycle, including increased holiday activity and occasional promotional events, such as a mid-year sale and an after-Christmas sale. Stores are of two types: regular “signature” stores in standard shopping locations such as malls, and discount outlets, which exhibit substantially higher volume.All stores are open seven days a week, with varying hours per day and per store; they are closed only a small number of days per year on major holidays (Christmas, New Year, Easter). A total of 20months of historical data was used, covering the January 2009–August-2010 period. For the purpose of estimating and evaluating the statistical forecasting models, a total of 8038 store–days and 167,725 store–intra-day (30-minutes) intervals were used. The available traffic data was aggregated by 30-minutes intervals, so we were able to construct sales curves for those intervals. On the other hand, the work schedules are defined based on 15-minutes time intervals (the breaks last 15minutes, for example). So for the scheduling, each 30-minutes period was split in two 15-minutes intervals with the same sales curve. To evaluate the schedule optimization methods, the 5089 store–days covering the latter portion of the data were used. Summary statistics of the average traffic, sales, items sold, and historical staffing are shown in Table 1. Confidentiality agreements preclude from giving additional details.In this section, we develop stochastic models to construct the sales curves. For this, we decompose the expected sales in any given time period as the expectation of the product of the number of items sold by the expected average price per item conditional on the first number. Then we develop a stochastic model for the cumulative distribution function (cdf) of the number of items sold as a function of the number of working employees on the floor, the estimated in-store traffic (or traffic proxy), and certain explanatory variables that account for seasonal effects and special events. This model uses ordinal regression with cubic spline basis functions. The estimated daily in-store traffic is modeled in log scale by a mixed-effects linear model, whose fixed effects are indicators that account for the month of the year, day of the week, interactions, and special events, and whose residuals follow a seasonal autoregressive moving-average (SARMA) process (Brockwell & Davis, 1991; Box, Jenkins, & Reinsel, 2008). This estimated traffic is assumed to be distributed across opening hours of the day according to a multinomial distribution, estimated by multinomial regression. The overall model is summarized on the left of Fig. 1. Sections 3.3, 3.4 and 3.5 detail the submodels for the traffic proxy, the sales volume, and the conditional average item price, respectively. Reports on experimental results and validation are given at the same time.Simple ways of measuring the in-store traffic in a given time period could be by counting the number of people entering the store during that period, or perhaps by the average number of people in the store during that period. However, in practical settings, this type of information is often unavailable, and one must rely on some indirect measure of store activity which we shall call a traffic proxy. In our case study, each store was equipped with people counters that register a “tick” each time a customer enters or leaves the store (the device does not distinguish the direction), and simply count the number of ticks in each period. This recorded data does not tell precisely how many people are present in the store at any given time, or how many entered in a given time period, but nonetheless correlates well with store activity and was the best available information in our case. Our traffic proxy was thus the “tick count” within each given time period.We considerj∗stores indexed by j, overd∗days indexed by d. Each day is divided int∗(d,j)time periods indexed by t (the number of time periods generally depends on the day of the week and on the store). For period t of day d in store j, letyd,t,jwbe the number of employees assigned to sales on the floor (the selling employees),T∼d,t,jdenote the traffic proxy,Xd,t,ja vector of selected explanatory variables such as calendar regressors (time of the day, day of the week, month of the year) and indicators for special events,Vd,t,jthe number of items sold (the sales volume),Pd,t,jthe average price per item, andRd,t,j=Vd,t,jPd,t,jthe total amount of sales. The numberyd,t,jwof selling employees excludes the employees that (we assume) have no direct impact in turning shoppers into buyers; for instance cashiers that do not directly assist customers on the floor and employees tasked with replenishing shelves, as well as those on break. In this paper, when scheduling the employees, we assume that there is only one type of work activity: each considered employee is assigned to sales. That is, we do not consider the situation where an employee works on the floor for some periods and as a cashier or something else on other periods. To cover this type of situation we could extend our framework to a multi-activity setting, which would make the resolution more complicated (but still feasible).The sales curves express the expected salesrd,t,j(y)=ERd,t,jyd,t,jw=yfor period t of day d in store j as a function of the number y of selling employees in the store for that period. To estimate the sales curves, we decompose the expectation as follows (the dependence of each term on the deterministic explanatory variablesXd,t,jis implicit):(1)rd,t,j(y)=EERd,t,jT∼d,t,j,yd,t,jw=y=EEVd,t,jE[Pd,t,j∣Vd,t,j]T∼d,t,j,yd,t,jw=y=∑v=0∞vEPVd,t,j=vT∼d,t,j,yd,t,jw=yE[Pd,t,j∣Vd,t,j=v].We shall construct separate models for the two expectations that appear in (1): the expected conditional probability (the conditional distribution of the volume) and the expected average price per item as a function of the volume. The first expectation is with respect toT∼d,t,j, so we need a model for this random variable, and also for the conditional probability inside the expectation. The reason for introducing such a decomposition is that it reduces significantly the estimation error in comparison with a direct model of the expected sales (see Section 3.6). With the decomposition, our model forE[Pd,t,j∣Vd,t,j=v]can use a single parameter vector estimated from all the data, which makes the model much more accurate. This idea of decomposing the sales in terms of more fundamental quantities is traditional in retail sales modeling. Different decompositions have been proposed; e.g., a popular one in retail expresses the sales as the product of the traffic, conversion rate, and average basket value (Lam et al., 2001). But with the traffic proxy measurements available in our case, we found the latter decomposition to be less robust to statistical estimation errors than the one we have adopted.To speed up computations, we will in fact replaceEPVd,t,j=vT∼d,t,j,yd,t,jw=yin (1) by the approximationPVd,t,j=vET∼d,t,j,yd,t,jw=y, in whichT∼d,t,jis replaced by its expectationET∼d,t,j. This introduces an approximation error due to the nonlinearity of the conditional probability as a function ofT∼d,t,jin (1), resulting in slightly less probability mass in the tails and more mass near the mean for the random variableVd,t,j. The accurate (but more expensive) option would be to integrate numerically with respect to the density ofT∼d,t,jto computeEPVd,t,j=v∣T∼d,t,j,yd,t,jw=y. In our case study, we found that using the cruder approximation did not bring significant changes on the sales curves and scheduling solutions.In this subsection, we explain how we will test the goodness of fit of our proposed models. To account for the sequential nature of the data and possible non-stationarities, our testing experiments are conducted in a sequential validation framework (also known as “simulated out-of-sample”): the models are initially trained on the 2009/01–2010/02 time interval, and tested out-of-sample on the one-month period of 2010/03. This month is then added to the training set, the model is retrained and tested on the subsequent month; this whole process is repeated until 2010/08, the last month in our data set. The reported performance results are the average across all such training/testing runs.The following measures are used to evaluate the statistical accuracy of model forecasts. We consider a generic time series of random variables{Yi,i=1,…,n}with observed realizations (“targets”){yi,i=1,…,n}, and corresponding model responses giving the forecasts{yˆi,i=1,…,n}. For example, when predicting the daily trafficT∼d,jat store j, we havei=dandYi=T∼d,j.The mean squared error (MSE) is defined asMSE=1n∑i=1nyi-yˆi2,the root mean squared error (RMSE) is defined asMSE, and the mean absolute percentage error (MAPE) is defined asMAPE=100n∑i=1nyi-yˆiyi.The MAPE is appropriate when we are interested in relative errors for strictly positive target quantities. To avoid numerical instability in the results, we eliminate from the error computation the very small targets in absolute value(∣yi∣<10-4).ForYi=Vd,t,j(the sales volume), which has a discrete distribution conditional onT∼d,t,j,yd,t,jw, we also evaluate the negative log likelihood (NLL), defined asNLL=-∑i=1nlogP^Yi=yi∣T∼d,t,j,yd,t,jw,whereP^denotes the conditional probability under the considered model. This NLL measures the fit of the entire distribution; it evaluates the ability of a model to put conditional probability mass in the “right places”, as opposed to just getting the conditional expectation right (the latter being evaluated by the MSE, since the conditional expectation minimizes the MSE; see Lehmann & Casella, 1998).Recall that the q-quantile of a random variable Y is defined asyq=inf{y∈R:P[Y⩽y]⩾q}. For continuous-response models that yield prediction intervals (traffic and item-price models), we evaluate interval coverage by measuring at which actual quantiles (in the test data) are mapped nominal quantiles given by the prediction intervals. For example, when we compute a 95% prediction interval, we estimate the 0.025 and 0.975 quantiles, and we would like to know how good are these estimates. We estimate a q-quantileyqby the q-quantileyˆqof the model’s predictive distribution, and we test the accuracy of this estimate by computing the fractionfqof observationsy1,…,ynthat fall belowyˆq. Ideally,fqshould be very close to q. We will report the values offqforq=0.025, 0.1, 0.9, and 0.975. To a certain extent, these values also measure the goodness of our models in terms of distributional forecasts.When we compare models against each other and we want to test the significance of the difference in predictive ability between any two models, we proceed as follows. Let{yˆ1,i,i=1,…,n}and{yˆ2,i,i=1,…,n}be the forecast time series of two models to be compared, and letg(yi,yˆi)be a measure of error between the realizationyiand the forecastyˆi; for exampleg(yi,yˆi)=(yi-yˆi)2if we use the square error. We define the sequence of error differences between models 1 and 2 bydi=g(yi,yˆ1,i)-g(yi,yˆ2,i),i=1,…,n. Letd¯=1n∑i=1ndibe the empirical mean difference. It is important to note that thesediare generally dependent, so to test if there is a difference in forecasting performance, one cannot just apply a standard t-test as if thediwere independent. For this purpose, to test the statistical significance ofd¯, we use the Diebold–Mariano (D–M) test (Diebold & Mariano, 1995), which estimates the variance ofd¯by estimating the correlations as follows. If{di,i⩾1}is assumed to be a stationary time series with lag-k autocovarianceγkfork⩾0, thenVar[d¯]=1n2∑i=1n∑j=1nCov(di,dj)=1n2∑k=-n+1n-1(n-k)γk.Theγk’s are assumed negligible outside a certain window; that is, one assumes thatγk=0for∣k∣>k0for some window lengthk0. Ifγˆkis a consistent estimator ofγkfor∣k∣⩽k0, and n is large compared withk0, then we can estimateVar[d¯]byvˆDM=1n∑k=-k0k0γˆk. Under (standard) mild conditions, the D–M statisticd¯/vˆDMis asymptotically distributed as aN(0,1)random variable under the null hypothesis that there is no predictive difference between the two models, and a classical test can be carried out based on this. In this paper we usek0=5; this was determined from the observed autocorrelation structure in the results. In our results, we will report the p-value p(D–M) for each D–M (two-side) test that we perform.The traffic at retail stores is characterized by yearly, weekly, and intraday seasonalities, as illustrated in Fig. 2, which shows the evolution of the traffic proxy for one clothing and apparel store in our case study over 8months (left) and over 5days (right). On the left, one can observe a significant traffic increase during the end-of-year shopping season (including after-Christmas sales). Traffic is also much higher over the weekends, which correspond to the regular “spikes” in the traffic pattern. On the right, one can observe the patterns at the individual-day level This plot is based on counts per 30-minutes period while the store is open and these counts have a large relative variance.Although not a traffic characterization per se, one endemic problem with store traffic data is the prevalence of missing values (shown as gaps in the solid traffic curve in the left panel of Fig. 2). These may be due to a number of factors: sensor failure, downtime of the recording computer, staff changing store mannequins and obstructing sensors for a long time, etc. It is important that the forecasting methodology be robust to the regular occurrence of such missing data.The goal of traffic modeling is to characterize the customer arrival process to a store and to make forecasts about future traffic (or its proxy) over the planning horizon. For scheduling purposes, we need forecasts at a high intraday resolution, such as 15- or 30-minutes periods. These forecasts should account for all known characteristics of retail traffic patterns, such as seasonalities, autocorrelations, impact of special events and other covariates. We also want to share statistical power across stores, namely use historical data from other stores to help predictions made about any given store. This is particularly valuable when new stores can open on a regular basis, having little historical data of their own.Our strategy is to first model the total traffic (proxy) at any given store during the whole day using a mixed-effects log-linear model, whose covariates have a multiplicative impact on the response variable. This first model is static and only captures contemporaneous effects. As a second step, in order to capture traffic time-series effects (persistence across successive days at a given store), a store-specific SARMA time series model is fitted to the residuals of the log-linear daily model. Finally, an intraday distribution model spreads the daily traffic across the intraday periods that correspond to the store-specific opening hours for that day, according to a multinomial distribution. These three model pieces are summarized in Fig. 3and detailed in SubSections 3.3.2, 3.3.3 and 3.3.4.We model the total daily trafficT∼d,jon day d at store j, forj=1,…,j∗, with a log-linear mixed-effects model (McCulloch, Searle, & Neuhaus, 2008; Pinheiro & Bates, 2000) of the general form(2)logT∼d,j=β0,j+β′xd,j+∊d,j,whereβ0=β0,1,…,β0,j∗is a vector of random store-specific intercepts,βis a vector of regression coefficients shared among all stores,xd,jis a vector of store-specific indicator variables for day d, and∊d,jis a zero-mean residual with varianceσd,j2. These residuals are discussed in SubSection 3.3.3.For our dataset, we ended up with the specific formulation:(3)logT∼d,j=β0,j+βD′Dd+βM′Md+βDL′DLd,j+βE′Ed,j+∊d,j,whereDd,Md,DLd,j, andEd,jare vectors of 0–1 indicators (dummy variables) for the day of the week, month of the year, interactions between the day of week and the store, and special events occurring at store, for store j on day d. For example, eachDdis a six-dimensional unit vector with a 1 at the position that corresponds to the day type of day d (the effect of the first day is absorbed in the interceptβ0,j, so we have six remaining daily effects instead of seven), and zeros elsewhere, whileβDis a six-dimensional vector of coefficients. The vectorβis the concatenation ofβD′,βM′,βDL′, andβE′. The coefficients in{βD,βM,βE}are shared across stores, which allows sensible forecasts to be carried out even for stores which have relatively little data. Obviously, for store chains with greater cross-sectional variability than in our sample, additional interaction terms could be incorporated to better capture store specificities; in our case, such refinements did not help. Although not used for the results presented in this paper, additional variables can be added, such as meteorological conditions (or forecasts thereof), significant sports events, macroeconomic variables or a continuous time variable for a long-term trend. Preliminary investigations suggest that weather-related variables can provide material improvements in traffic forecast quality in a retail context.The model parametersβ0andβare estimated by ordinary least squares (OLS) regression. For the purposes of estimating the model parameters, the residual varianceσd,j2is assumed to be a common constantσ2, but when using the model for forecasts, we take the variance obtained from the residual model introduced in Section 3.3.3. Under this model, the predictive distribution ofT∼d,jis lognormal, so the traffic cannot take negative values. If we denoteμd,j≡ElogT∼d,j∣xd,j, the conditional expected traffic can be written as (Aitchison & Brown, 1957),(4)ET∼d,j∣xd,j=expμd,j+σd,j2/2.Note that this model is reasonable only in situations where the traffic levelT∼d,jis distributed over sufficiently large values, i.e., not concentrated over just a few non-negative integers close to 0, and with no significant point mass at zero. Were this not the case, an alternative is to use a Poisson regression model (McCullagh & Nelder, 1989).The residual model aims at extracting the dynamic structure present in the process of daily residuals∊d,jthat remains unexplained by (3). This model operates at the individual store level, each store yielding a univariate time series of daily residuals∊d,j—where the store j is kept constant and the day d varies—modeled by a seasonal autoregressive moving averageSARMA(p,q)×(P,Q)Sspecification (see, e.g., Brockwell & Davis, 1991), specified as(5)ΦP(LS)ϕp(L)∊d,j=ΘQ(LS)θq(L)νd,j,jheld fixed,whereϕpandθqare polynomials of orders p and q in the lag operatorL,ΦPandΘQare polynomials of orders P and Q in the seasonal lag operatorLS, respectively, and{νd,j,d=1,…,d∗}, is an i.i.d. zero-mean innovations process with finite varianceσν,j2, for each j. In our context, the seasonality S is taken to be 7days. (coefficients of the polynomialsΦP,ϕp,ΘQ, andθq) are estimated by maximum likelihood. The choice of model order (values ofp,q,P,Q) is carried out automatically using an AIC criterion; see, e.g., Makridakis, Wheelwright, and Hyndman (1997) and Hyndman, Koehler, Ord, and Snyder (2008). This procedure is implemented with the help of the forecast package in the R statistical language (Hyndman, Razbash, & Schmidt, 2012). We emphasize that since a distinct model is estimated for each store, the model order across stores can be completely different.We compare three variants of our daily traffic model: (a) the static model in (3) with the indicator covariates only, with the residuals assumed to be independent (termed covariates only); (b) the model for the SARMA residuals only, without the covariates (termed SARMA residuals only); and (c) the model that combines the previous two, with the covariates at the highest level and a SARMA model for its residuals (termed covariates+SARMA residuals).When reporting the results and test the goodness of fit, we convert all model responses from the logarithmic scale back to their original scale in “daily persons count.” For the conditional expectation, this involves accounting for the conditional variance due to Jensen’s inequality, namely for a random variableX∼N(μ,σ2), we haveE[eX]=eμ+σ2/2.Fig. 4shows the training fit (in-sample) for one of the 15 stores, under the model with covariates only (top) and the model with covariates+SARMA residuals (bottom). The latter gives a much better fit.Table 2shows the performance results of the three models. The combined model exhibits the best performance, both in-sample (the Train column) and out-of-sample (the Test column), and on both the RMSE and MAPE measures. This is corroborated by the D–M statistic, which compares each model with the combined one (although the difference is statistically significant, in bold, only for the SARMA model). The combined model also exhibits the most accurate prediction interval coverage.Our intraday traffic model takes a forecast for the total traffic during the day at a given store (obtained from the daily model), and distributes it across the store opening hours, given the explanatory variables. An appropriate model for this is the multinomial regression model (McCullagh & Nelder, 1989), which assumes that a set of random variables is jointly multinomially distributed given explanatory variables. For time period t of a given day, letyd,t,j=βt′zd,j, whereβtis a set of regression coefficients andzd,jis a vector of day- and store-specific explanatory variables. The intraday probability attributed to interval t isP(t∣zd,j)=expyd,t,j∑t′expyd,t′,j.The coefficientsβtcan be estimated by maximum likelihood. We then haveET∼d,t,j∣xd,j,zd,j=P(t∣zd,j)ET∼d,j∣xd,j,withE[T∼d,j∣xd,j]given in (4).Goodness-of-fit results for the intraday traffic model (30-minutes periods) are given in Table 3. The only difference between the tested models lies in the choice of daily model. We observe that the best daily model yields the best intraday forecasts, both in- and out-of-sample. The significance of the performance advantage is confirmed by the D–M test, whose results are more significant than for the daily models, largely because there are considerably more intraday test observations.The interval coverage is narrower than the nominal intervals since none of the intraday models account for intraday autocorrelations in traffic patterns; these autocorrelations are indeed significant, but they do not impact significantly the ex ante traffic expectations for any given time period, which are the relevant quantities for making schedules.At a typical small- to medium-sized retail store, the number of items sold during an intraday period (e.g., 30minutes) will be a small integer. Due to the decomposition in (1), one needs to estimate the entire distribution of sales volume, not only the expected value. Empirically, we find that standard parametric forms, such as a conditional Poisson distribution, generally provides imperfect fits to the data distribution: the latter is significantly more dispersed and exhibits richer shapes than the former generally admit. We propose to adapt a more flexible framework, that of ordinal regression to estimate the volume distribution.Ordinal regression models (McCullagh, 1980; McCullagh & Nelder, 1989) attempt to fit measurements that are observed on a categorical ordinal scale. LetZ∈Rbe an unobserved variable andV∈{1,…,K}be defined by discretizing Z according to ordered cutoff points-∞=ζ0<ζ1<⋯<ζK=∞.We observeV=kif and only ifζk-1<Z⩽ζk,k=1,…,K. The proportional odds ordinal regression model assumes that the cumulative distribution of V on the logistic scale is modeled by a linear combination of explanatory variablesx, i.e.logitP(V⩽k∣x)=logitP(Z⩽ζk∣x)=ζk-θ′x,whereθare regression coefficients andlogit(p)≡log(p/(1-p)). Model parameters{ζi,θ}can be estimated by maximum likelihood (e.g., McCullagh, 1980). Although ordinal regression is usually employed in contexts where discrete-valued observations only have ordering constraints (without explicit magnitude information), it is important to emphasize that in this work the ordered values are the non-negative integers themselves (up to K): hence magnitude information is preserved, and ordinal regression can be used as a bona fide discrete distributional model, with observations spanning a finite range. Within this range, it provides more flexible distributions than typical discrete regression models (e.g. Poisson generalized linear models), while remaining reasonably easy to fit.In our application,V=Vd,t,j, and we use the following explanatory (input) variables:•Indicators for the store, the month, and day of week.Cubic B-spline basis (e.g. Hastie, Tibshirani, & Friedman, 2009) of the trafficTd,t,j, with 6 degrees of freedom, whose knots are placed automatically for a uniform coverage of the quantiles of the training data up to the specified number of degrees of freedom.Product (interaction) between the store indicators and the traffic B-spline traffic basis.Cubic B-spline basis of the number of employees working during the time period, with 6 degrees of freedom.This provides a flexible distributional forecast of the sales volume. As mentioned earlier, to reduce the computations in the construction of the sales curves, we replaceTd,t,jinxby its expectation. But to estimate the models parameters, we use the observed traffic.Fig. 5illustrates two different operation points for a store that was part of our case study. We see that for low traffic and a single employee on the floor, most of the probability mass is concentrated towards selling zero items, whereas when the traffic or the number employees increases, the mass shifts rightwards, reflecting an increase in sales volume.The prediction errors for this ordinal regression volume model are in Table 4. For comparison, we report the corresponding results for a Poisson generalized linear model (GLM; e.g., McCullagh & Nelder, 1989) and a negative binomial (NB) GLM (Hilbe, 2011) with the same input variables. These models are frequently used in integer-valued observations models, with the negative binomial distribution allowing to represent “overdispersion” (variance greater than the mean) of the response with respect to the Poisson. For these models, the predictive distribution is respectively a Poisson or NB with the log-expectation given by a linear mapping between the input variables and the regression coefficients.From Table 4, the Poisson and NB models produce better estimates of the expected number of items sold than the ordinal regression model. However, the latter—owing to its greater flexibility—is better at assigning probability mass in the ‘right places’, as ascertained by the NLL criterion. The D–M test statistic confirms the significance of the difference. This is of consequence in practice inasmuch as the complete probability distribution for the number of items sold is necessary in (1) to compute the conditional expectation of sales.Our item-price model takes the simple log-linear form,logPd,j=βp′wd,j+ηd,j, whereβpis a vector of regression coefficients (a single vector for all the data),wd,jis a vector of item-price explanatory variables at day d for store j, andηd,jis a residual.Two specifications of this model are compared: a linear and a log-linear form. They both use of the following explanatory variables:•Indicators for the store, the month of the year, and interactions thereof.Cubic B-spline basis of the number of items sold, with 8 knots placed on a power scale between 1 and 30 (so that small integers are more finely covered than larger ones), impacting the average item price non-linearly.The number of working employees was not found to be a significant determinant of the average item price.Results are given in Table 5. The log-linear model performs slightly but significantly better than the purely linear one. It also has the advantage of never allowing negative prices to occur. If product returns occur frequently, it would make sense to handle them separately, perhaps by allowing negative quantities in the volume distribution of the previous section. In our data, product returns were very infrequent and were ignored.We compare in Table 6the sale curves model based on our proposed decomposition (denoted Decomposition (full expectation)) to two alternatives:1.A model based on the same decomposition, but which treats the volume and average item price as independent quantities. This amounts to making the following approximation in (1):rd,t,j(y)=EEVd,t,jPd,t,jET∼d,t,j,yd,t,jw=y≈EEVd,t,jET∼d,t,j,yd,t,jw=yEEPd,t,jET∼d,t,j,yd,t,jw=y.This is referred to as Decomposition (independence assumption) in the results table.A model that attempts to directly model intraday sales in log scale. It is based on the same daily/intraday break-down that is used for traffic modeling, but using the log of sales as the target quantity instead of traffic. In addition, the intraday model makes use of the following variables:•Indicators for the store, month, day of the week, and time period.Cubic B-spline basis for the traffic, with 6 degrees of freedom.Interaction terms between the store and the traffic B-spline basis.Cubic B-spline basis for the numberyd,t,jwof working employees during the time period, with 6 degrees of freedom.Overall, the direct model has about the same number of free parameters as all the models that are part of the decomposition. It is named Direct in the table.On an out-of-sample basis, the model based on the decomposition, with the full expectation computation, is seen to outperform both the expectation approximation, and much more significantly the direct model. Our subsequent results are based on this model.The traditional cost-driven interpretation of shift scheduling problems led to a variety of classical models and formulations. Recent innovative formulations based on concepts from language theory enable one to efficiently represent the shifts that can be assigned to employees. In this section, we first review the principles of these problem formulations, before extending them to our specific profit-based formulation. This extension is achieved by linearizing the objective function and formulating the problem of scheduling for profit as a MIP. We also propose another formulation of the same problem as a CP, which has the advantage of handling the non-linearity in a more natural way. We consider a single store, so we can drop the index j.We are given a set E of employees, a set T of time periods of equal length, a set of activitiesA={w,r,m,b}(which stand for{work,meal,break,rest}), and a setΩof feasible shifts, each one defined by a finite sequence of activities from A.In the classical, cost-driven, shift scheduling problem, a minimal numberbtwof employees that perform activityw∈Aduring period t is specified for eacht∈T, and the goal is to select a multiset of shifts inΩ(with multiple selection of the same shift allowed) that cover the personnel requirement at a minimum cost. A set covering formulation of this problem was already formulated by Dantzig (1954) for the toll-booth operator shift scheduling model of Edie (1954). In this model, each shifto∈Ωhas a costCoand is defined by listing explicitly the activities assigned for each time period. The costCois often assumed to be additive, i.e., it can be written as a sum of costs over the periods that it covers:Co=∑a∈A∑t∈Tαt,oaCta,whereαt,oa=1if shift o assigns activity a in periodt,αt,oa=0otherwise, andCtais a cost for doing activity a in period t. In real-life situations, the setΩis typically very large and the corresponding MIP becomes very hard to solve directly, even via column generation techniques as in Mehrotra, Murthy, and Trick (2000); Vatri (2001), Bouchard (2004), Demassey, Pesant, and Rousseau (2006).Alternative mathematical reformulations have also been proposed to reduce the number of variables. The most popular ones are so-called implicit formulations (Bechtolds & Jacobs, 1990; Aykin, 1996; Rekik, Cordeau, & Soumis, 2004; Rekik, Cordeau, & Soumis, 2010), which provide an interesting alternative to the model of Dantzig (1954). The qualifier implicit comes from the fact that these models do not assign breaks to shift a priori. They rather introduce the notion of shift types, which are characterized only by starting and ending times and do not contain any details about what happens in between. Typically, these models capture the number of employees assigned to each shift type and to each break with different sets of variables. From an optimal solution to such an implicit model, one can retrieve the number of employees assigned to each shift type and each break, and construct an optimal set of shifts through a polynomial-time procedure.More recently, Côté, Gendron, Quimper, and Rousseau (2011) and Côté, Gendron, and Rousseau (2011) showed how to exploit the expressiveness of automata and context-free grammars to formulate MIP models in which any feasible shift is represented by a word in a regular language, and the set of admissible staffing solutions (which correspond to an admissible collection of shifts) is defined implicitly by an integral polyhedron that can be constructed automatically even for complex work regulations such as those mentioned earlier. In what follows, we will adapt this approach to our profit-maximizing problem. Our use of automata together with CP for this type of scheduling problem is novel.Following Côté, Gendron, Quimper et al. (2011); Côté, Gendron, and Rousseau (2011), we will represent each admissible shift by a word defined over the alphabetA={w,r,m,b}, where the ith letter in the word indicates the activity of the employee during the ith time period of its shift. In our case study, those time periods have 15minutes. As an illustration, the word “wwbwwwmmwwww” symbolizes a 3-hours shift where the employee works for 30minutes, takes a 15minutes break, gets back to work for 45minutes, takes a lunch break of 30minutes, and goes back to work for the remaining hour.To recognize the valid words, i.e., those that correspond to admissible shifts, we define a deterministic finite automaton (DFA) that specifies the rules that regulate the transitions in the sequence of activities that can be assigned to an employee in a shift. The DFA is defined as a 5-tupleΠ=〈Q,A,τ,q0,F〉, where•Q is the finite set of states;A is the alphabet;τ:Q×A→Qis the transition function;q0∈Qis the initial state;andF⊆Qis a set of final states.A word is recognized by a DFA if, by processing its symbols one by one from the initial state and following the transitions, we end up in a final state after processing the last symbol.Fig. 6illustrates a DFAΠthat recognizes a language defined over the alphabet A. It hasQ={0,1,…,6},q0=0andF={5,6}. This language contains, for instance, the words “wwbwwmww”, “rr” and “r”, but it does not include the words “wwr” or “wwbm” as they are not recognized byΠ.From a given DFA that specifies the set of admissible shifts, one can obtain a MIP by reformulating the constraints expressed in the DFA as a set of network flow constraints which, together with the profit-maximizing objective, define a network flow optimization problem. This is similar to what was done by Côté, Gendron, Quimper et al. (2011). For this, we first introduce for each word (each admissible shift) of lengthℓthe binary decision variablesxta=1if the word has activitya∈Ain position (time period)t,xta=0otherwise, fort=1,…,ℓ. This representation permits one to directly formulate complex rules that regulate the sequence of activities (e.g., an employee must work at least an hour before taking a break), which are usually complex to handle by traditional approaches, in a much more efficient way by expressing them as network flow constraints.To control the length of words that can be accepted by the DFA, we derive fromΠanother automatonΠℓthat recognizes only the sequences of lengthℓthat are accepted byΠ(Pesant, 2004). The automatonΠℓis represented by an acyclic directed layered graph withℓ+1layers, whose set of states in layer t has the form{q.t:q∈Nt}, whereNt⊆Q, fort=0,…,ℓ. One hasN0={q0}andNℓ⊆F. Fig. 7illustrates the automatonΠ7that represents all the words of lengthℓ=7included in the language defined by the automatonΠof Fig. 6. Each word is obtained by taking a path from node 0.0 to one of the two shaded nodes, and collecting the symbols on the arcs taken along the road. The lower branch corresponds to an off day (no work). The automatonΠℓ, withℓequal to the number of periods in the day, can be used to construct a graph G as in Fig. 7, from which the network flow formulation is obtained. Fort=0,…,ℓand eachq∈Nt, each pairq.tis a node of G and each transition inΠℓis an arc of G. A flow variablesq,a,q′tis assigned to each arc of G, indicating that the symbol a is assigned to position t in the sequence by transiting from state q to stateq′ofΠ(and stateq.(t-1)to stateq′.t)(ofΠℓ). The unique elementq0ofN0is identified as the source node, and the nodesq.ℓinNℓcorrespond to the set of final states F ofΠ. Note that the actual length of the shift can be less thanℓ; this can be handled by assigning the activityrto some of the periods, usually at the beginning or at the end of the shift.Letπq,q′a=1,if the transition functionτallows a transition from stateq∈Qto stateq′∈Qwith labela∈Ain the automatonΠ,0,otherwise.A word of lengthℓis included in the language defined byΠif and only if its corresponding flow variables satisfy the following constraints:(6)∑a∈A∑q∈Qsq0,a,q1=1(7)∑a∈A∑q′∈Qsq′,a,qt-1=∑a∈A∑q′∈Qsq,a,q′t∀t∈{2,…,ℓ},q∈Q(8)∑a∈A∑q∈Q∑f∈Fsq,a,fℓ=1(9)sq,a,q′t⩽πq,q′a∀t∈{1,…,ℓ},a∈A,(q,q′)∈Q2(10)∑(q,q′)∈Q2sq,a,q′t=xta∀t∈{1,…,ℓ},a∈A(11)sq,a,q′t∈{0,1}∀t∈{1,…,ℓ},a∈A,(q,q′)∈Q2.Constraints (6)–(8) ensure that one unit of flow enters the state, and that the amount of flow entering and leaving each other node in the graph is the same. Constraints (9) guarantee that the transition functionτofΠis respected, whereas constraints (10) link the decision variablesxtaand the flow variables. Finally, constraints (11) define the domain of the flow variables.We are now ready to formulate our profit-driven workforce scheduling problem as a MIP by building over the network flow formulation of constraints on feasible shifts as given in the previous subsection. We consider a single store j at a time, so we drop the index j. We use E to denote the set of employees, D for the set of days for which we want to construct the schedule (e.g., Monday to Sunday if it is over a week, or a single element if it is over a single day), andTdfor the set of time periods when the store is open on day d. LetM={0,1,…,∣E∣}(the possible number of employees that can perform a given task during a given period) andA‾={w,b,m}(the non-rest activities, which are the only ones considered to determine the real length of a shift). We introduce two sets of binary variables:xd,te,aindicates if employee e performs task a during period t on day d, whileyd,tm,aindicates if exactly m employees perform task a during period t on the day d. Each flow variablesq,a,q′e,d,tindicates whether employee e is transiting from state q to stateq′(ofΠ) during period t of day d by performing task a. Our MIP formulation is:(12)max∑m∈M∑d∈D∑t∈Tdrd,t(m)yd,tm,w-∑a∈AmCd,tayd,tm,a(13)subject to∑a∈A∑q∈Qsq0,a,qe,d,1=1∀e∈E,d∈D(14)∑a∈A∑q′∈Qsq′,a,qe,d,t-1=∑a∈A∑q′∈Qsq,a,q′e,d,t∀e∈E,d∈D,t∈{2,…,∣Td∣},q∈Q(15)∑a∈A∑q∈Q∑f∈Fsq,a,fe,d,∣Td∣=1∀e∈E,d∈D(16)sq,a,q′e,d,t⩽πq,q′a∀e∈E,d∈D,t∈Td,a∈A,(q,q′)∈Q2(17)∑(q,q′)∈Q2sq,a,q′e,d,t=xd,te,a∀e∈E,d∈D,t∈Td,a∈A(18)∑e∈Exd,te,a=∑m∈Mmyd,tm,a∀d∈D,t∈Td,a∈A(19)∑a∈Axd,te,a=1∀e∈E,d∈D,t∈Td(20)∑m∈Myd,tm,a=1∀d∈D,t∈Td,a∈A(21)δe-⩽∑t∈Td∑a∈A‾xd,te,a⩽δe+∀e∈E,d∈D(22)Δe-⩽∑d∈D∑t∈Td∑a∈A‾xd,te,a⩽Δe+∀e∈E(23)sq,a,q′e,d,t∈{0,1}∀e∈E,d∈D,t∈Td,a∈A,(q,q′)∈Q2(24)xd,te,a∈{0,1}∀e∈E,d∈D,t∈Td,a∈A(25)yd,tm,a∈{0,1}∀m∈M,d∈D,t∈Td,a∈AIn the objective function, we find the sales curvesrd,t(·)defined in (1) (where we have dropped the store index j), which give the expected revenue from sales in each time period as a function of the number of working employees, and the total salary expenses, whereCd,tais the per-employee cost for performing activity a during period t on day d.We want to maximize the total expected profit.We have used binary variablesyd,tm,a∈{0,1}instead of integer variablesyd,ta∈Mto represent the number of employees performing each activity in each time period because it provides a linear MIP formulation of the problem, despite the nonlinear sales curves in the objective function.Constraints (13)–(17) guarantee that all shifts assigned to employees on day d correspond to words of length∣Td∣in the language defined byΠ, by deriving a flow problem over the variablessq′,a,qe,d,tand linking them to the variablesxd,te,a(as in subSection 4.2).Constraints (18) link the variablesxd,te,aandyd,tm,a, whereas constraints (19) guarantee that a single task a is assigned to each employee e for every period t of a day d, and constraints (20) ensure thatyd,tm,acaptures the number of employees performing task a in period t of day d.Constraints (21), (22) impose a minimum duration ofδe-and a maximum duration ofδe+to the shift length of each employee (excluding theractivity) over any given day d, as well as a minimum duration ofΔe-and maximum duration ofΔe+for the total shift duration over all days covered by the schedule (e.g., over the week).Finally, constraints (23)–(25) define the domain of all variables.The linearization of the objective function in our MIP formulation has introduced a huge number of binary variablesyd,tm,a, which increases significantly the size of the problem. One way to reduce this size is to approximate the sales curves by piecewise linear functions. Ifp⩾1denotes the number of linear pieces in the approximation, the PLA formulation, which we denote by MIP-PLAp, use p binary variables to indicate the piece in which we are, and an integer variableyd,ta⩾0to indicate the number of employees performing activity a in period t of day d. Typically, the sales curves are concave, in which case the PLA will underestimate the expected revenue (see Fig. 8). Whenp=∣E∣-1, MIP-PLApis equivalent to the original MIP formulation, whereas ifp<∣E∣-1, MIP-PLApis an estimation of the MIP, so the optimal solution of MIP-PLApmight not be optimal for MIP. However, any feasible solution to MIP-PLApis feasible for MIP, because the PLA does not modify the constraints.We now consider another efficient approach, based on a problem formulation as a CP. This formulation handles the non-linearity naturally without altering the model by using approximations and without adding extra variables. Our CP formulation is based on three sets of variables:xd,tespecifies the activitya∈Aperformed by employee e in period t of dayd,yd,tarepresents the number of employees that perform activity a in period t of day d, andsd,te∈Qdescribes the state of the automaton that led to the assignment of the activity performed by employee e in period t of day d. We represent a transition from state q to stateq′labeled by a by the triplet〈q,a,q′〉, and we defineΘas the set of triplets〈q,a,q′〉that correspond to transitions allowed under the DFAΠ. Before stating the CP model, we recall to the reader that in Constraint Programming the expression “(variable=value)”, when used within a constraint, is automatically transformed (known as reification) into a boolean variable that takes value 1 when the expression holds and 0 otherwise. The proposed CP model is then:(26)max∑d∈D∑t∈Tdrd,t(yd,tw)-∑a∈ACd,tayd,ta(27)subject tosd,0e=q0∀e∈E,d∈D(28)〈sd,t-1e,xd,te,sd,te〉∈Θ∀e∈E,d∈D,t∈{1,…,∣Td∣}(29)sd,∣Td∣e∈F∀e∈E,d∈D(30)∑e∈Exd,te=a=yd,ta∀d∈D,t∈Td,a∈A(31)δe-⩽∑t∈Td(xd,te≠r)⩽δe+∀e∈E,d∈D(32)Δe-⩽∑d∈D∑t∈Td(xd,te≠r)⩽Δe+∀e∈E(33)sd,te∈Q∀e∈E,d∈D,t∈Td(34)xd,te∈A∀e∈E,d∈D,t∈Td(35)yd,ta∈M∀a∈A,d∈D,t∈TdThe objective function in (26) is the same as in the MIP, but is expressed more concisely, as it can use directly the integer variablesyd,tw. The constraints (27)–(29) ensure that the shift performed by any employee e over a day d is recognized as a word of length∣Td∣in the language defined byΠ. Constraints (27) and (29) ensure that the exploration of the automaton begins at its initial state and ends in a final state (in F), whereas constraints (28) guarantee that the transition functionτof the DFA is respected when moving from state to state. Constraints (30) link thexd,teandyd,tavariables. Constraints (31), (32) guarantee that the length of each employee’s shift respects the minimum and maximum durations defined over a day and over a week, respectively. Finally, constraints (33)–(35) define the domain of all variables. In constraint programming terminology, constraints (28) are known as table constraints and constraints (30) as global cardinality constraints. Most constraint programming solvers can handle these families of constraints using special-purpose algorithms. This CP is exactly equivalent to the MIP introduced earlier. However, the two formulations give rise to different solutions methods, whose performance can be very different. In particular, the large number of binary variables in the MIP can have a significant impact on the speed of resolution.We have implemented the MIP and CP formulations of Section 4 for our case study, and solved them using the ILOG CPLEX optimization tool on 2.1gigahertz dual core processors with 3.5gigabytes of memory. In this section, we demonstrate and compare the accuracy and the efficiency of the proposed approaches to construct schedules for one day and one week, while considering the work regulation rules of the retailer. Note that a new labor regulation impacting scheduling was put in place by the retailer in all its stores after the period for which we have data. Although the regulations defined over a day did not change, the new regulations impose a new set of rules common to all stores, over a week. Before the change, each store manager was constructing weekly schedules according to his own set of rules, which could potentially change between weeks. In this sense, we do not have full access to the exact weekly scheduling rules for the period studied. Therefore, to perform a fair comparison between our schedules and the schedules historically used in the stores, we compare them in terms of expected profit over a day, given past information, based on the profit curves estimated with our models. We will measure the expected profit increase/decrease from our new schedules asΔp=profitnew-profitold/profitold,whereprofitnewis the objective function value associated with the output of our optimization models, andprofitoldis the expected profit that would have been achieved with the schedules that were actually operated in the stored (the baseline schedules), using our profit curves.The MIP and CP problems are usually too large to be solved to full optimality in a reasonable amount of time. CPLEX is given a computing budget (CPU time) and we retain the best solution found within that time. An upper bound on the true optimal value is given by the optimal value of the LP relaxation, returned by the MIP solver when it stops. We take the lowest upper bound we can find, obtained when solving the MIP with the largest computing budget. The relative differenceΓbetween this best upper bound and the value of the returned solution (by MIP or CP) is a bound on the relative optimality gap. We report this valueΓ, expressed in percentage, in our results.In addition to the generic constraints given in our MIP or CP formulation, there are other daily and weekly constraints specific to our retailer. In their formulation, two types of employees are distinguished: part-time and full-time.•Daily regulations:–Work and rest distribution: each day, the employee can have a day off or work for one shift.Shift duration: a shift over a day lasts from 3 to 8hours.Break and meal distribution: non-working stretches (break, meal, rest) have to be separated by at least one hour of work.Break and meal authorization: an employee working a shift lasting less than 5hours must take a single 15-minutes break, whereas an employee working a shift of 5hours or more must take two 15-minutes breaks, separated by a 30-minutes meal break.Minimum presence:*Stores with external back stores must have a minimum of two employees at all times during opening hours, else a minimum of one employee is required.Part-time employees must never work alone, a full-time employee must always be present.Weekly regulations:–Work and rest distribution*An employee cannot work more than 5 shifts in a week.An employee must have two consecutive days off in a week or one week-end day off.Shift duration*A part-time employee must work between 10hours and 30hours over a week.A full-time employee must work between 25hours and 40hours over a week.We used data coming from 15 stores, for 5089days in total (the same portion of data that was used to evaluate the forecasting models). In every store, half the workforce are full-time employees, and we must have at least three employees with a full-time contract. These stores hire between 4 and 9 employees, and their opening hours vary from 9hours to 12hours per day. Table 7gives additional details on these values. Only stores 14 and 15 have an external back store. Each day is split into 15-minutes intervals. All employees earn the same hourly salary, from which the costsCtaare extracted. Breaks and meals are not paid. Note that, although the number of employees of each kind (partial time vs full time) is fixed, the amount of time spent working on the floor over a day and a week by each of them is only constrained to be inside a specific range, namelyδe-,δe+for a day andΔe-,Δe+for a week for every employeee∈E, so the total cost of employees over a day or a week has to be determined during the optimization process.Fig. 9illustrates the automatonΠthat defines the formal language for our model. This automaton is used to construct a word for each employee, that represents his admissible daily shift, in which the sequence of activities he performs obeys all the sequencing rules (e.g., an employee must work at least one hour before his first break, he must work for a shift of at least five hours to be allowed to take two breaks, etc.). The other regulation rules are incorporated using traditional MIP and CP constraints based on the number of occurrences of each activity in the word of each employee or the states of the automaton explored by an employee.To formulate these extra constraints, we useE‾to denote the set of full-time employees,D‾for the set of the first four days of the week (Monday to Thursday),D˚for the set of weekend days (Saturday and Sunday), andq1to denote state 1 of the automaton of Fig. 9. For the extra constraints in the MIP formulation, we introduce two new sets of binary variables:vdeindicates if employee e is off for both day d and the following day, whilezeindicates if employee e has a day off over the weekend. The extra constraints for the MIP are:(36)δe-∑t∈Tdsq0,w,q1e,d,t⩽∑t∈Td∑a∈A‾xd,te,a⩽δe+∑t∈Tdsq0,w,q1e,d,t∀e∈E,d∈D(37)∑t∈Tdyd,t0,w=0∀d∈D(38)∑e∈E‾xd,te,r⩽∣E‾∣-1∀d∈D,t∈Td(39)∑d∈Dsq0,r,q0e,d,∣Td∣⩾2∀e∈E,d∈D(40)2vde⩽sq0,r,q0e,d,∣Td∣+sq0,r,q0e,d+1,∣Td+1∣∀e∈E,d∈D‾(41)ze⩽∑d∈D˚sq0,r,q0e,d,∣Td∣∀e∈E(42)ze+∑d∈D‾vde⩾1∀e∈E(43)vde∈{0,1}∀e∈E,d∈D‾(44)ze∈{0,1}∀e∈Ewhile the extra constraints to be added to the CP are:(45)δe-∑t∈Tdsd,te=q1⩽∑t∈Tdxd,te≠r⩽δe+∑t∈Tdsd,te=q1∀e∈E,d∈D(46)yd,tw⩾1∀d∈D,t∈Td(47)∑e∈E‾xd,te≠r⩾1∀d∈D,t∈Td(48)∑d∈Dsd,∣Td∣e=q0⩾2∀e∈E,d∈D(49)∑d∈D‾∑t∈Tdxd,te=w=0∧∑t∈Td+1xd+1,te=w=0⩾1∨∑d∈D˚∑t∈Tdxd,te=w=0⩾1∀e∈E.The constraints (36) and (45) are reformulation of the generic constraints (21) and (31) that still guarantee that the length of each employee’s shift respects the minimum and maximum durations defined over a day, while adding the possibility that an employee does not work over a day (i.e., the employee has a day off). The constraints (37) and (46) ensure that there is at least one employee working on the floor at any time of the day. Note that if the store have an external back store, these constraints must be replaced by the constraints (50) and (51) given by:(50)∑t∈Tdyd,t0,w+yd,t1,w=0∀d∈Dand(51)yd,tw⩾2∀d∈D,t∈Td,respectively. The constraints (38) and (47) guarantee that there is at least one full-time employee working at any time. Constraints (39) and (48) ensure that an employee cannot work more than five shifts (days) over a week. The days off are managed differently in the MIP and CP; the MIP requires the introduction of new variables. For the MIP, constraints (40) and (41) are added to link the new variablesvdeandzeto the variablessq,a,q′e,d,t, respectively, so the constraints (42) can guarantee that an employee has two consecutive days off during the week or one weekend day off. Constraints (43) and (44) must also be added to define the domain of variablesvdeandze. For CP, only the constraints (49) must be added, as they directly ensure that an employee has two consecutive days off in a week or one weekend day off.By taking the set D as a single element, the proposed models can be used to construct work schedules over a day. In this context, the weekly regulations are ignored.Table 7 compares the expected profit increases provided by solving our models, over the scheduling solutions used by the retailer, over a single day. These numbers are averages over the 5089days of our data. The table gives two types of informations: (i) the estimated relative increaseΔpin expected profit with the schedules constructed by MIP after 300seconds of CPU time, and by CP after 30seconds and 300seconds of CPU time, compared with the schedules used by the retailer and (ii) the relative gapΓbetween those MIP and CP solutions and the best upper bound extracted from the MIP solution after 300seconds. As the solutions provided by solving the MIP with PLA generate less profit than the solutions of the full MIP while using the same amount of CPU time to construct feasible schedules, we always used the full MIP (no PLA) in this section. For many instances, the MIP failed to return a feasible solution after 30seconds; for this reason we do not report the MIP results for 30seconds. The solutions returned by either MIP or CP after 300seconds are very accurate. They are nearly optimal with a respective average gapΓof only 0.3% and 0.1%, and correspond to estimated profit increasesΔpof 2.6% and 2.8%, respectively. CP seems to provide a slightly larger profit growth than MIP, and also provides good solutions already after 30seconds. On the other hand, CP does not provide an upper bound on the optimal value during the resolution. In practice, the CP approach seems to be the most efficient method overall.For the scheduling problem over a week, CP was able to construct feasible weekly schedules consistently in less than 2hours of CPU time, whereas for the MIP, even the combination with PLA with a single linear piece (MIP-PLA1) failed to provide feasible solutions for over 30% of the weeks. Table 8shows the evolution of the gapΓbetween the solutions identified by solving the CP model after 30minutes, 60minutes, and 120minutes of CPU time, and the best upper bound extracted after 24hours of CPU time with the MIP model. Given that the weekly schedules used historically in the stores do not satisfy the new rules imposed by the retailer, we cannot make a fair comparison with our solutions. This is why we only compare with an upper bound. The quality of the schedules constructed by CP is demonstrated by the small average relative gap (less thanΓ≈3%after 60minutes) between the value of these solutions and the corresponding upper bounds (which are likely to be larger than the optimal values).

@&#CONCLUSIONS@&#
