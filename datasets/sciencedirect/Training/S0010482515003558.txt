@&#MAIN-TITLE@&#
Unsupervised learning assisted robust prediction of bioluminescent proteins

@&#HIGHLIGHTS@&#
Combination of unsupervised learning with SMOTE for imbalance learning problems.Effective handling of between class and within class imbalance.Diversification of the training set with optimal class distribution.Does not require evolutionary information for prediction.

@&#KEYPHRASES@&#
Class imbalance,Training set diversity,Optimal class distribution,K-Means,SMOTE,

@&#ABSTRACT@&#
Bioluminescence plays an important role in nature, for example, it is used for intracellular chemical signalling in bacteria. It is also used as a useful reagent for various analytical research methods ranging from cellular imaging to gene expression analysis. However, identification and annotation of bioluminescent proteins is a difficult task as they share poor sequence similarities among them. In this paper, we present a novel approach for within-class and between-class balancing as well as diversifying of a training dataset by effectively combining unsupervised K-Means algorithm with Synthetic Minority Oversampling Technique (SMOTE) in order to achieve the true performance of the prediction model. Further, we experimented by varying different levels of balancing ratio of positive data to negative data in the training dataset in order to probe for an optimal class distribution which produces the best prediction accuracy. The appropriately balanced and diversified training set resulted in near complete learning with greater generalization on the blind test datasets. The obtained results strongly justify the fact that optimal class distribution with a high degree of diversity is an essential factor to achieve near perfect learning. Using random forest as the weak learners in boosting and training it on the optimally balanced and diversified training dataset, we achieved an overall accuracy of 95.3% on a tenfold cross validation test, and an accuracy of 91.7%, sensitivity of 89. 3% and specificity of 91.8% on a holdout test set. It is quite possible that the general framework discussed in the current work can be successfully applied to other biological datasets to deal with imbalance and incomplete learning problems effectively.

@&#INTRODUCTION@&#
There are mainly two phenomena, bioluminescence and bioflourescence which are responsible for the emission of visible light from the living organisms. The mechanisms of these two processes are distinct as the former involves a chemical reaction, and the latter involves absorption of light from external sources and its emission after transformation. Bioluminescence is observed in both terrestrial and marine habitats. The chemical reaction, which is responsible for bioluminescence, generates very less heat and can be categorized into oxygen dependent (luciferin-luciferase system) and oxygen independent types (ex. Photoproteins). The colour of the emission is governed by the amino acid sequence, and by accessory proteins like yellow fluorescent proteins (YFP) and green fluorescent proteins (GFP) [1]. Diverse systems for bioluminescence exist in nature, for ex. in Dinoflagellates, specialized organelles known as Scintillons [2,3] exhibit bioluminescence. Bioluminescence plays an important role in bacterial intracellular chemical signalling and in symbiosis: a common example of which is shown by Epryme scolopes and Vibrio fishcri[4,5], in attracting for a mate and repelling the predators. The independent evolution of bioluminescence in different organisms has been discussed in Hastings et al. [1]. In some organisms, the usefulness of bioluminescence is still unknown.In silico prediction of bioluminescent proteins (BLP) was first carried out by Kandaswamy et al. [6]. They developed Blprot, which is an SVM based method. Their prediction model was trained by using 544 amino acid physicochemical properties. The prediction of bioluminescent proteins was further improved by Zhao et al. (BLPre) [7] using evolutionary information in the form of PSSM (Position Specific Scoring Matrices) obtained from PSI-BLAST. Fan et al. [8] used a balanced dataset (equal number of positive and negative samples for training) with average chemical shift and modified pseudo amino acid composition for prediction of bioluminescent proteins. Recently, Huang [9] proposed a scoring card method (SCBM) for their prediction.The imbalanced class ratios are often encountered in the protein family classification problems. This causes the overrepresentation of instances belonging to majority class and under representation of instances belonging to minority class in the training set. The machine learning models trained with the imbalance training dataset have classification bias towards majority class and behave like a majority class classifier. This issue of imbalanced dataset has not been given the required attention in the bioinformatics community as it deserves.In the current prediction problem, bioluminescent proteins (BLPs) are the positive minority class (which is the class of interest) and the majority class consists of all the non-bioluminescent proteins (NBLPs) belonging to different other protein families. The negative class is naturally very large as compared to the number of BLPs. So, the bioluminescent prediction problem training dataset is one of the classic examples of imbalanced dataset. This imbalance in class distribution greatly affects the accuracy in predicting the positive class instances (as the prediction models tends to act as a majority class classifier) and it is also quite evident from the previous studies [6–9].When we use any machine learning algorithm to build a prediction model, the major motive is to maximize the generalization ability of the model. This insures that the trained predictive model will yield good prediction accuracy on the future unseen data. Ideally the training dataset that is presented to the learning algorithm should be properly diversified by covering the representatives from the entire input instance space to achieve the maximum possible generalization ability. If the training data are composed of a large number of very similar instances, it may get biased towards those instances. This notion holds true in the cases of both between-class (inter-class) and within-class (intra-class) instances. So the diversification of the training set is essential to gain enhanced generalization. Both between-class imbalance and within-class imbalance have a negative influence on the performance of machine learning algorithms [10].In the present study, we have created a diversified and balanced training dataset by using unsupervised K-Means clustering algorithm (to deal with the within class imbalance where each class contains subgroups of similar instances of varying numbers) and then using SMOTE [11] (to selectively amplify the representative minority class sequences for balancing the between-class imbalance). The boosted random forest algorithm which has performed considerably better than the other machine learning algorithms was used to create our prediction model.As the next part of this study, we have investigated the effect on prediction performances by varying the balancing ratio from ideal ratio (that is 1:1) to the original imbalance ratio. Analyzing the experimental results has revealed that the best prediction performance can be achieved at an optimal balancing ratio rather than at ideal balancing ratio. It was found that another performance factor (diversity) gets affected at the ideal balancing ratio of 1:1. This has motivated us to probe for optimal class distribution which is required to achieve superior accuracy (provides the best trade-off between inter-class balancing ratio and the diversity). The optimal class distribution is seldom explored in bioinformatics.Finally individual features are ranked using the Relieff feature ranking algorithm and investigated the performance of the classifier by varying the number of features starting from 5 most discriminating features up to 40 (according to their rank) and recorded the calculated performance evaluation metrics obtained for RARF. The prediction performance increases with the increasing number of features (according to their ranks). This has authenticated the presence of large diversity among BLPs and there is a need for finding the optimal class distribution in order to achieve the best prediction performance. The superiority of the proposed framework as compared to random sampling is also discussed.We used the dataset of Kandaswamy et al. [6] which consists of 441 positive class sequences (bioluminescent proteins) having less than 40% sequence identity and 18202 negative class sequences (non-bioluminescent protein NBLP) having more than 40% sequence identity. The redundant sequences in the dataset may result in bias and overestimation of model evaluation parameters. So we have used CD-HIT [12] to reduce the redundancy by removing sequences having more than 40% sequence identity, which resulted in 13,446 negative sequences. The final dataset consisted of approximately 1:30 positive to negative instances ratio. The data imbalance is intrinsically present in most of the protein family classification problems and affects the accuracy of predicting the members of a particular protein family. So the datasets are needed to be appropriately balanced to achieve the true performance of the classifiers.The input vectors were created by extracting the following three types of features from every protein sequence.(i)Amino acid composition: We used the percentage composition of amino acid residues (aa) as one of the feature vectors. This feature was selected on the assumption that there are some specific avoidances and preferences of certain amino acids in the formation of a protein family to perform a common functionality, which resulted in distinguishable frequency compositions (fres).(1)fres=Nres,iNtotal_res,i×100whereresstands for one of the 20 different amino acid residuesfresdenotes the amino acid percentage frequency of the specific residue in ith Sequence.Nres,idenotes the total count of amino acid of the specific type in the ith sequence.Ntotal_res,idenotes the total count of all residues in the ith sequence (i.e. sequence length).Amino acid property group composition: The percentage frequency counts of amino acid property groups were used as the second component in the feature vector. The different amino acid property groups [13] that are selected for this study are given inTable 1. This is a refinement over amino acid frequency composition where specific property group count is computed instead of the individual amino acid count.wherepgdenotes one of the 11 different amino acid property groupsfpgdenotes the percentage frequency of the specific amino acid property group in the ith sequence.Npg,idenotes the total count of the specific amino acid property group in the ith sequence.Ntotal_res,idenotes the total count of all residues in the ith sequence.Physicochemical n-grams: Physicochemical properties of amino acid residues play an important role in determining the protein’s function. There are many effective ways to incorporate the physicochemical properties of amino acid residues for representing protein sequences which can be used for discrimination of proteins of interest from other proteins. For calculating physicochemical n-grams, we used a technique of sliding window of length n (where n is an integer). If all the amino acid residues inside the sliding window share the same physicochemical group then the frequency of that physicochemical group is counted. If the amino acid residues are having more than one similar physicochemical group then corresponding counts are made for all those physicochemical groups. The physicochemical groups mentioned in table 1 were retained for calculation of physicochemical n-grams.(3)Physicochemical2−gram:Small=∑i=1N−1C(i,i+1)Ndenotes the length of the protein sequenceIdenotes the position of the amino acid residue along the protein sequenceC(i,i+1) if the condition(aai∈S*,aai+1∈S*)is satisfied then C(i,i+1)= 1 otherwise C(i,i+1)=0. The set of small-amino-acids S⁎={Ala,Cys,Asp,Gly,Asn,Pro,Ser,Thr,Val}In the similar way the physicochemical 2-grams for the other ten physicochemical property groups were calculated. This feature captures the most important positional related information. So our input vector is a judicious combination of amino acid frequency compositions, physiochemical properties and positional features that are extracted from every sequence from the dataset.For any supervised learning algorithm, the presentation of a diverse set of labelled data from the entire input space belonging to different classes is very important for its proper learning of all the concepts and the sub-concepts. Moreover, a dataset is said to be imbalanced when there is a large difference between the numbers of examples belonging to different classes. Normally every classifier tends to be the majority class classifier. The Bioluminescent protein classification problem is one of the classical examples of the class imbalance problem. In this present classification problem, the minority examples are the bioluminescent proteins and majority examples belongs to the non-bioluminescent proteins.Most of the learning algorithms are designed to optimize the accuracy as the evaluation metric during the process of learning the concepts and the sub-concepts from the dataset. When accuracy is taken as the evaluation metric, it gets too strongly biased towards the majority class by predicting correctly most of the majority class instances as compared to minority class instances. So the accuracy does not prove to be the true indicator for better performance as it is a weighted average of accuracies in predicting both the majority and minority classes. Often in cases, when there is imbalanced data and the class of interest is the minority class, it is very important to gain better predictive accuracy and generalization ability for the minority class than for the majority class. When the imbalance ratio is high between the majority and minority class instances, the accuracy in predicting the minority class is low, as the learning algorithm will have less opportunity to learn all the minority class sub-concepts as compared to majority class sub-concepts due to the overwhelming number of instances from the dominant class. This may also result in misclassification of some of the minority class instances into the majority class.Apart from between-class imbalance, the within-class imbalance in the training data may also result in a lower generalization of the learned models. Presence of rare cases or less common cases results in the within-class imbalance. If the common cases are present in more numbers in the training data, then learning algorithm will have less opportunity to learn the rare case sub-concepts. Ideally, there should be an adequate representation of common as well as rare cases from both majority and minority sub-classes in the training data. Also the class distribution of the training and testing samples should be similar otherwise the model may not generalize well on the unseen test set examples. Here we propose a hybrid sampling method using K-Means clustering and SMOTE for creating a balanced and diversified training dataset. Diversification of the training set is important as it includes as many distinct training samples as possible to maximize the generalization ability.One of the popular methods to handle class imbalanced data is by using a sampling of the dataset. Sampling can be done in a random manner like random downsampling; random oversampling; and in an intelligent manner by using SMOTE or its variants [14]. There is a good chance of losing important instances in random downsampling of the majority class. While in random oversampling there is a good chance that some instances of the minority class may get overrepresented. Both these situations results in an incomplete learning. Random sampling results in the reduction of data variation in the training set and consequently results in a low generalization of the prediction model. Jo et al. [15] in their work addressed the problem of small disjuncts (disjuncts can be defined as those regions in the input space that covers only a few training examples). These small disjuncts are difficult to learn and ideally there should be an adequate representation of them in the training data.This clustering technique aims to find homogeneous groups that occur naturally in a dataset. It is an unsupervised method of clustering, where with a given similarity measure (clustering criterion); it tries to find hidden patterns in the dataset and groups together the more similar entries. We have applied K-Means clustering separately on positive and negative class instances. The objective function, which is to be minimized during the each iteration of K-Means, is given as follows:(4)SSE=∑j=1K∑i=1nj||Pij−Cj||whereCjdenotes the centroid of the jth cluster.Pijdenotes the ith pattern of the jth cluster.njdenotes the number of objects in the jth cluster.K denotes predetermined number of clusters|| || denotes matching distance metric usedThe entire feature vectors were normalized before clustering with euclidean distance as the distance metric. To avoid sticking to the local minima, we have repeated the clustering for 10 times with a new set of centroid positions for the initial clusters. To determine the optimal value for K, (i) we have calculated the ratio of within cluster variance to between cluster variance (distortion ratio) for every value of K that varies from 1 to 441(that is the total number bioluminescent proteins in the dataset) using the K-Means algorithm for the positive dataset, (ii) a graph was plotted for K versus distortion ratio (Fig.1) and (iii) the value of K after which there is no significant decrease in the corresponding value of the distortion ratio is taken as its optimal value (375). We used the optimal K value obtained from the positive dataset as the initial value of K for the negative dataset (the majority class) due to the constraints of very high time and space complexity in finding its K‐optimal as well as the need for dealing with an within-class imbalance. The experiments were carried out with the K‐optimal as well as with integral multiple of K‐optimal (from 2×K‐optimal to 7× K‐optimal) for the negative dataset to find out the best balancing factor. The rationale for using the higher value for K for the negative dataset is that the K-optimal for the positive dataset need not be optimal for the relatively larger negative dataset. Moreover, clustering with the same K-optimal for negative dataset causes tight clustering of the negative instances and may result in merging two or more distinct clusters or redistribution of unique cluster elements to different clusters. Accordingly, the higher values of K for the negative class were used to have a relax clustering so as to include vast diversity from the negative instances.For the given K-optimal for positive dataset =375, the possible training datasets were created, such as Training Set I: 375 positive instances and 375 negative instances, Training Set II: 375 positive and 750 negative instances, Training Set III: 375 positive, 1125 negative instances and so forth.The main purpose of clustering is to select representative samples from both the positive and negative classes to achieve diversification of the training set, so as to minimize the within-class imbalance. One instance from each cluster is selected from both the positive class clusters and negative class clusters for the training set and the rest of the cluster members are retained in the testing set.In SMOTE, minority class is oversampled by inducing artificial instances. It is a nearest neighbour based method. It selects randomly a minority class instance and its N nearest minority class neighbours (the default value of N =5). Distance is calculated between the sample and one of the randomly chosen nearest neighbour in the feature space and then it creates a synthetic instance along the line segment between the minority sample and it’s selected nearest neighbour.In cases where there are an unequal K (for ex. in 2×K, 3×K, etc.), we used SMOTE to selectively oversample the positive class representative instances equal to the number of negative class instances. We experimented with different % of SMOTE sampling and have examined the effect of a balanced and imbalanced dataset on the prediction evaluation metrics by creating different datasets with different proportions of positive and negative instances. The properties of different training and testing sets are presented inTable 2.Boosting [16,17] combines many weak base learners linearly to construct a strong classifier with improved accuracy. It is an iterative procedure. During the each iteration, the incorrectly classified instances from both the positive and negative classes are given more weights so that the learning is concentrated on the hard and difficult to classify instances in the training set. It is a sequential ensemble method where the subsequent learners are evolved from the previous learners.Random forest [18] is an ensemble learning method consisting of many individual decision trees. Classifier ensembles promote an optimal trade-off between diversity and accuracy. Ensemble classifiers usually outperform single classifiers and they are robust to the presence of noise in the data and to over fitting of inputs [19]. Different base classifiers making errors in different parts of the hypothesis space give better accuracy when properly combined together.The concept of bagging [20] is implemented in the random forest classification algorithm. In random forest bootstrap samples from the training set with randomly selected feature subsets were evaluated at each node of the decision tree. The final decision is made by decision fusion of all the trees by majority voting. Random forests have been successfully applied to many classification and prediction tasks [21,22]. Major steps of random forest are summarized as follows: (1) a bagged sample is drawn from the training data. (2) A decision tree is grown without pruning on the bagged sample, where at each node a randomly selected subset of features from the full feature subset is evaluated. (3) Fusing the decisions from all the individual trees.We have used random forest as weak learners for Boosting algorithm. Recently, some of the authors have also successfully applied boosted random forest for classification and prediction [23,24]. Real Adaboost is one of the popular modifications of the Adaboost algorithm. The major steps are same except that it involves the calculation of real valued class probability estimates. We experimented with both discrete and real Adaboosting algorithms. The schematic representation of the proposed methodology is shown inFig.2.The performance of the machine learning methods is evaluated by using threshold-dependent and threshold-independent parameters and these parameters are calculated from the values of true positives (TP), false negatives (FN), true negatives (TN) and false positives (FP).Sensitivity: expresses the percentage of correctly predicted BLPs.(5)Sensitivity=TP(TP+FN)×100Specificity: expresses the percentage of correctly predicted NBLPs.(6)Specificity=TN(TN+FP)×100Accuracy: expresses the percentage of correctly predicted BLPs and NBLPs.(7)Accuracy=TP+TN(TP+FP+TN+FN)AUC: the area under the receiver operating characteristic curve can be summarized by a single numerical quantity known as area under the curve (AUC). An AUC value close to 1 is considered good.g-means: It is the geometric mean of sensitivity and specificity and is calculated by(8)g−means=(Sensitivity×Specificity)Youden׳s index (Y): this parameter measures the models ability to avoid failures and is calculated by(9)Y=Sensitivity−(1−Specificity)

@&#CONCLUSIONS@&#
