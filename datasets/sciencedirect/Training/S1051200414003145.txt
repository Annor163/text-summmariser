@&#MAIN-TITLE@&#
Recursive least squares parameter identification algorithms for systems with colored noise using the filtering technique and the auxilary model

@&#HIGHLIGHTS@&#
The parameter estimation problems of an output error type system are discussed.An auxiliary model based recursive generalized least squares algorithm is presented.An input–output data filtering based recursive least squares algorithm is presented.The data filtering based algorithms can achieve more accurate parameter estimates.

@&#KEYPHRASES@&#
Filtering technique,Parameter estimation,Recursive identification,Least squares,Box–Jenkins system,

@&#ABSTRACT@&#
This paper focuses on the parameter estimation problems of output error autoregressive systems and output error autoregressive moving average systems (i.e., the Box–Jenkins systems). Two recursive least squares parameter estimation algorithms are proposed by using the data filtering technique and the auxiliary model identification idea. The key is to use a linear filter to filter the input–output data. The proposed algorithms can identify the parameters of the system models and the noise models interactively and can generate more accurate parameter estimates than the auxiliary model based recursive least squares algorithms. Two examples are given to test the proposed algorithms.

@&#INTRODUCTION@&#
The development of information and communication technology has had a tremendous impact on our lives, e.g., the information filtering, optimization and estimation techniques [1–4]. In the areas of signal processing and system identification, the observed output signals always contain disturbances from process environments [5–8]. The disturbances are of different forms (white noise or colored noise). It is well known that the conventional recursive least squares (RLS) method generates biased parameter estimates due to correlated noise or colored noise [9]. Thus the identification of output error models with colored noise has attracted many research interests [10]. The bias correction methods have been considered very effective to deal with the output error models with colored noise [11,12]. However, the bias correction methods ignore the estimation of the noise models [13]. In this paper, we propose new identification methods for estimating the parameters of the system model and the noise model.Since the noise in real life can be fitted by the autoregressive (AR) models, the moving average (MA) models [14,15] or the autoregressive moving average (ARMA) models [16], this paper considers the output error (OE) model with AR noise as shown in Fig. 1(the OEAR model for short), which can be expressed as(1)y(t)=B(z)A(z)u(t)+1C(z)v(t),where{u(t)}and{y(t)}are the system input and output sequences, respectively,{v(t)}is a white noise sequence with zero mean and varianceσ2, andA(z),B(z)andC(z)are polynomials in the unit backward shift operatorz−1[z−1y(t)=y(t−1)]:A(z):=1+a1z−1+a2z−2+…+anaz−na,B(z):=b1z−1+b2z−2+…+bnbz−nb,C(z):=1+c1z−1+c2z−2+…+cncz−nc.Assume that the ordersna,nbandncare known, andu(t)=0,y(t)=0andv(t)=0fort⩽0. The coefficientsai,biandciare the parameters to be estimated from the input–output data{u(t),y(t)}.The model in (1) can be transformed into a new controlled autoregressive moving average (CARMA) form,A(z)C(z)y(t)=B(z)C(z)u(t)+A(z)v(t),orA′(z)y(t)=B′(z)u(t)+D(z)v(t),A′(z):=A(z)C(z),B′(z):=B(z)C(z),D(z):=A(z).This CARMA model can be identified using the recursive extended least squares algorithm [9,17]. However, the model in (1) only contains(na+nb+nc)unknown parameters, while this new model contains(2na+nb+2nc)parameters, resulting in the increment of the computation load of identification algorithms. Moreover, some extra computation is required to compute the estimates of the parametersbiandci.In practical industries, there exist many unmeasurable variables in systems, such as the state variables [18] and the inner variables or the noise-free outputs. In general, one can use the outputs of an appropriate auxiliary model to replace the unmeasurable variables for identification. The auxiliary model identification idea can be applied to linear systems containing the unknown variables in the information vectors [19,20], nonlinear systems, dual-rate/multirate systems [21,22], and missing-data systems or systems with scarce measurements [23,24]. Recently, Chen et al. presented a data filtering based least squares iterative algorithm for parameter identification of output error autoregressive systems [25].On the basis of the work in [25–27], this paper investigates the recursive identification problems of the OEAR models and the Box–Jenkins models using the filtering technique. Two-stage recursive least squares algorithms are proposed through filtering the input–output data. Since the OEAR models and the Box–Jenkins models involve the system models (the OE part) and the noise models (the AR or ARMA part), the proposed algorithms can generate the parameter estimates of the system models and the noise models.The rest of this paper is organized as follows. Section 2 gives the auxiliary model identification algorithm for OEAR systems. Section 3 analyzes the convergence analysis of the auxiliary model based recursive generalized least squares algorithm. Section 4 derives a parameter estimation algorithm based on the data filtering technique. Section 5 gives simply a filtering based identification algorithm for Box–Jenkins systems. Section 6 provides two examples to show the effectiveness of the proposed algorithms. Finally, some concluding remarks are given in Section 7.Define the noise-free outputx(t)and the noise termw(t)as(2)x(t):=B(z)A(z)u(t),w(t):=1C(z)v(t),and the parameter vectorϑand the information vectorϕ(t)asϑ:=[θc]∈Rn,n:=na+nb+nc,θ:=[a1,a2,…,ana,b1,b2,…,bnb]T∈Rna+nb,c:=[c1,c2,…,cnc]T∈Rnc,ϕ(t):=[φ(t)ψ(t)]∈Rn,φ(t):=[−x(t−1),−x(t−2),…,−x(t−na),u(t−1),u(t−2),…,u(t−nb)]T∈Rna+nb,ψ(t):=[−w(t−1),−w(t−2),…,−w(t−nc)]T∈Rnc.Eqs. (2) and (1) can be written as(3)x(t)=[1−A(z)]x(t)+B(z)u(t)=φT(t)θ,(4)w(t)=[1−C(z)]w(t)+v(t)=ψT(t)c+v(t),(5)y(t)=x(t)+w(t)=ϕT(t)ϑ+v(t).A difficulty of identification is thatϕ(t)contains the unknown inner termx(t−i)and the unmeasurable noise termw(t−i). An effective method of estimating the parameter vectorϑis to employ the auxiliary model identification idea in [19,28] as shown in Fig. 2, wherexa(t):=Ba(z)Aa(z)u(t)is the output of the auxiliary model. The unknown termx(t−i)is replaced with the outputxa(t−i)of the auxiliary model and the unknown noise termw(t−i)is replaced with its estimatewˆ(t−i)for parameter estimation. Defineϕˆ(t):=[φa(t)ψˆ(t)]∈Rn,φa(t):=[−xa(t−1),−xa(t−2),…,−xa(t−na),u(t−1),u(t−2),…,u(t−nb)]T∈Rna+nb,ψˆ(t):=[−wˆ(t−1),−wˆ(t−2),…,−wˆ(t−nc)]T∈Rnc.Referring to the methods in [19,28], we can obtain the auxiliary model based recursive generalized least squares (AM-RGLS) algorithm for generating the estimateϑˆ(t)ofϑ:(6)ϑˆ(t)=ϑˆ(t−1)+L(t)[y(t)−ϕˆT(t)ϑˆ(t−1)],(7)L(t)=P(t)ϕˆ(t)=P(t−1)ϕˆ(t)[1+ϕˆT(t)P(t−1)ϕˆ(t)]−1,(8)P(t)=P(t−1)−L(t)LT(t)[1+ϕˆT(t)P(t−1)ϕˆ(t)],P(0)=p0I,(9)ϕˆ(t)=[φa(t)ψˆ(t)],(10)φa(t)=[−xa(t−1),−xa(t−2),…,−xa(t−na),u(t−1),u(t−2),…,u(t−nb)]T,(11)ψˆ(t)=[−wˆ(t−1),−wˆ(t−2),…,−wˆ(t−nc)]T,(12)xa(t)=φaT(t)θˆ(t),(13)wˆ(t)=y(t)−xa(t),(14)ϑˆ(t)=[θˆT(t),cˆ1(t),cˆ2(t),…,cˆnc(t)]T,(15)θˆ(t)=[aˆ1(t),aˆ2(t),…,aˆna(t),bˆ1(t),bˆ2(t),…,bˆnb(t)]T.To initialize the AM-RGLS algorithm, we takeϑˆ(0)to be a small real vector, e.g.,ϑˆ(0)=1n/p0with1nbeing an n-dimensional column vector whose elements are 1,p0to be a large positive number, e.g.,p0=106, andIto be an identity matrix of appropriate dimensions.The martingale convergence theorem is one of the main tools of studying the convergence of recursive identification algorithms [17,30–32]. The basic idea is to establish a recursive equation about the parameter estimation errorϑ˜(t):=ϑˆ(t)−ϑ, to formulate a Lyapunov function in the estimation errorϑ˜(t)and to prove the convergence of the algorithms by using the martingale convergence theorem.Assume that{v(t),Ft}is a martingale difference sequence defined on the probability space{Ω,F,P}, where{Ft}is the σ algebra sequence generated by the observations up to and including time t. The noise sequence{v(t)}satisfies the following assumptions [17]:(A1)E[v(t)|Ft−1]=0,a.s.,(A2)E[v2(t)|Ft−1]=σ2,a.s.Theorem 1For the system in(5)and the AM-RGLS algorithm in(6)–(15), assume that (A1)–(A2) hold and that there exist positive constants α, β, γ andt0such that the following generalized persistent excitation condition (unbounded condition number) holds[28]:(A3)αI⩽1t∑j=1tϕˆ(t)ϕˆT(t)⩽βtγI,a.s.,t⩾t0.Then for anyc>1, we have‖ϑˆ(t)−ϑ‖2=O([ln⁡t]ct)→0,a.s.This means that the parameter estimation error‖ϑˆ(t)−ϑ‖converges to zero with the increasing of t.ProofDefine the parameter estimation error vectorϑ˜(t):=ϑˆ(t)−ϑ.Note thatP(t)is a symmetric matrix:P(t)=PT(t). Using (6) and (5) gives(16)ϑ˜(t)=ϑˆ(t−1)+P(t)ϕˆ(t)[y(t)−ϕˆT(t)ϑˆ(t−1)]−ϑ=ϑ˜(t−1)+P(t)ϕˆ(t)[ϕT(t)ϑ+v(t)−ϕˆT(t)ϑˆ(t−1)]=ϑ˜(t−1)+P(t)ϕˆ(t){−ϕˆT(t)ϑ˜(t−1)+[ϕ(t)−ϕˆ(t)]Tϑ+v(t)}=ϑ˜(t−1)+P(t)ϕˆ(t)[−y˜(t)+Δ(t)+v(t)],where(17)y˜(t):=ϕˆT(t)ϑ˜(t−1)∈R,(18)Δ(t):=[ϕ(t)−ϕˆ(t)]Tϑ∈R.Applying the matrix inversion formula(A+BC)−1=A−1−A−1B(I+CA−1B)−1CA−1to (8) gives(19)P−1(t)=P−1(t−1)+ϕˆ(t)ϕˆT(t).Define a Lyapunov functionW(t):=ϑ˜T(t)P−1(t)ϑ˜(t).Note thaty˜(t)=ϕT(t)ϑ˜(t−1)=ϑ˜T(t−1)ϕ(t)is scalar valued. Using (16) and (19), we have(20)W(t)={ϑ˜(t−1)+P(t)ϕˆ(t)[−y˜(t)+Δ(t)+v(t)]}TP−1(t){ϑ˜(t−1)+P(t)ϕˆ(t)[−y˜(t)+Δ(t)+v(t)]}=W(t−1)+y˜2(t)−2y˜2(t)+2y˜(t)[Δ(t)+v(t)]+ϕˆT(t)P(t)ϕˆ(t){y˜2(t)+v2(t)+Δ2(t)−2y˜(t)[Δ(t)+v(t)]+2Δ(t)v(t)}=W(t−1)−[1−ϕˆT(t)P(t)ϕˆ(t)]y˜2(t)+2[1−ϕˆT(t)P(t)ϕˆ(t)]y˜(t)[Δ(t)+v(t)]+ϕˆT(t)P(t)ϕˆ(t)[v2(t)+Δ2(t)+2Δ(t)v(t)].Referring to the proof of Lemma 3 in [28], we have1−ϕˆT(t)P(t)ϕˆ(t)=[1+ϕˆT(t)P(t−1)ϕˆ(t)]−1⩾0.Refer to the method in [30,31] and assume thatΔ(t)is bounded withΔ2(t)⩽ε. Sincev(t)is a white noise with zero mean and varianceσ2, andy˜(t),ϕˆT(t)P(t)ϕˆ(t)andΔ(t)are uncorrelated withv(t), taking the conditional expectation of both sides of (20) with respect toFt−1and using (A1) and (A2) giveE[W(t)|Ft−1]=W(t−1)−[1−ϕˆT(t)P(t)ϕˆ(t)]y˜2(t)+ϕˆT(t)P(t)ϕˆ(t)[v2(t)+Δ2(t)]⩽W(t−1)+ϕˆT(t)P(t)ϕˆ(t)[σ2+ε],a.s.Letr(t):=tr[P−1(t)]. Referring to the proof of Theorem 1 in [28], applying the martingale convergence theorem (Lemma D.5.3 in [17]) to the above inequality, we can conclude that‖ϑˆ(t)−ϑ‖2=O([ln⁡r(t)]cλmin[P−1(t)]),a.s.,c>1.Furthermore, using (A3), we can conclude that the parameter estimation error‖ϑˆ(t)−ϑ‖converges to zero as t goes to infinity.  □By introducing a linear filter, sayC(z), to filter the system input and output data, the OEAR system model in (1) can be transformed into an OE model with white noise. Define the filtered inputuf(t)and outputyf(t)as(21)uf(t):=C(z)u(t)=u(t)+c1u(t−1)+c2u(t−2)+…+cncu(t−nc),(22)yf(t):=C(z)y(t)=y(t)+c1y(t−1)+c2y(t−2)+…+cncy(t−nc).It is easy to see thatyf(t)=0anduf(t)=0fort⩽0fromy(t)=0andu(t)=0fort⩽0.Multiplying both sides of (1) byC(z)yieldsC(z)y(t)=B(z)A(z)C(z)u(t)+v(t).Then we have the following filtered OE model,(23)yf(t)=B(z)A(z)uf(t)+v(t):=xf(t)+v(t),wherexf(t):=B(z)A(z)uf(t).Define(24)φf(t):=[−xf(t−1),−xf(t−2),…,−xf(t−na),uf(t−1),uf(t−2),…,uf(t−nb)]T∈Rna+nb.Then we have(25)xf(t)=[1−A(z)]xf(t)+B(z)uf(t)=−a1xf(t−1)−a2xf(t−2)−…−anaxf(t−na)+b1uf(t−1)+b2uf(t−2)+…+bnbuf(t−nb)=φfT(t)θ,(26)yf(t)=xf(t)+v(t)=φfT(t)θ+v(t).Based on the identification model in (26), we have the following recursive least squares algorithm for computing the estimateθˆ(t)ofθ:(27)θˆ(t)=θˆ(t−1)+Pf(t)φf(t)[yf(t)−φfT(t)θˆ(t−1)],(28)Pf(t)=Pf(t−1)−Pf(t−1)φf(t)φfT(t)Pf(t−1)1+φfT(t)Pf(t−1)φf(t).Here, we can see that the algorithm in (27)–(28) cannot be applied to estimateθˆ(t)directly, because the filtered inputuf(t), the filtered outputyf(t), and the inner variablesxf(t−i)'s in the information vectorφf(t)are unknown. To overcome this problem, the inner variablesxf(t−i)'s inφf(t)are replaced with the outputsxˆf(t−i)'s of an auxiliary model according to the auxiliary model identification idea. The auxiliary model can be taken to be(29)xˆf(t)=φˆfT(t)θˆ(t),(30)φˆf(t):=[−xˆf(t−1),−xˆf(t−2),…,−xˆf(t−na),uˆf(t−1),uˆf(t−2),…,uˆf(t−nb)]T∈Rna+nb,whereuˆf(t−i)is the estimate ofuf(t−i)andφˆf(t)is obtained by replacing the unknown filtered inputuf(t−i)and filtered outputyf(t−i)inφf(t)with their estimatesuˆf(t−i)andyˆf(t−i). From (21) and (22), we can see that the estimates of the filtered inputuf(t−i)and filtered outputyf(t−i)rely on the estimates of the noise part parametersci's. The following discusses the estimation of the noise model.Letcˆ(t)be the estimate ofcat time t. From the identification model in (4), we can obtain the estimation algorithm of computingcˆ(t):(31)cˆ(t)=cˆ(t−1)+Pn(t)ψ(t)[w(t)−ψT(t)cˆ(t−1)],(32)Pn(t)=Pn(t−1)−Pn(t−1)ψ(t)ψT(t)Pn(t−1)1+ψT(t)Pn(t−1)ψ(t).Notice thatw(t−i)in the above algorithm is unmeasurable. From (1) to (3), we havew(t)=y(t)−x(t)=y(t)−φT(t)θ.Replacingφ(t)andθwithφa(t)andθˆ(t−1), respectively, yields the estimate ofw(t):(33)wˆ(t)=y(t)−φaT(t)θˆ(t−1).Replace the unmeasurable noise termsw(t−i)'s inψ(t)with its estimateswˆ(t−i)'s and define(34)ψˆ(t):=[−wˆ(t−1),−wˆ(t−2),…,−wˆ(t−nc)]T∈Rnc.Thus, replacingw(t)andψ(t)in (31)–(32) withwˆ(t)andψˆ(t)gives(35)cˆ(t)=cˆ(t−1)+Pn(t)ψˆ(t)[wˆ(t)−ψˆT(t)cˆ(t−1)],(36)Pn(t)=Pn(t−1)−Pn(t−1)ψˆ(t)ψˆT(t)Pn(t−1)1+ψˆT(t)Pn(t−1)ψˆ(t).Use the estimatecˆ(t):=[cˆ1(t),cˆ2(t),…,cˆnc(t)]T∈Rncto form the estimate ofC(z)as follows:Cˆ(t,z):=1+cˆ1(t)z−1+cˆ2(t)z−2+…+cˆnc(t)z−nc.From (21) and (22), the estimates of the filtered inputuf(t)and the filtered outputyf(t)can be computed throughuˆf(t)=Cˆ(t,z)u(t)=u(t)+cˆ1(t)u(t−1)+cˆ2(t)u(t−2)+…+cˆnc(t)u(t−nc),yˆf(t)=Cˆ(t,z)y(t)=y(t)+cˆ1(t)y(t−1)+cˆ2(t)y(t−2)+…+cˆnc(t)y(t−nc).Replacingφf(t)in (27)–(28) withφˆf(t), andyf(t)withyˆf(t), we obtain the recursive least squares estimation algorithm for the OE part parameters,(37)θˆ(t)=θˆ(t−1)+Pf(t)φˆf(t)[yˆf(t)−φˆfT(t)θˆ(t−1)],(38)Pf(t)=Pf(t−1)−Pf(t−1)φˆf(t)φˆfT(t)Pf(t−1)1+φˆfT(t)Pf(t−1)φˆf(t).Define the gain vectorsLf(t):=Pf(t)φˆf(t)∈Rna+nbandLn(t):=Pn(t)ψˆf(t)∈Rnc. From (33)–(38), we can derive the filtering based recursive least squares (F-RLS) algorithm for the OEAR model [26]:(39)θˆ(t)=θˆ(t−1)+Lf(t)[yˆf(t)−φˆfT(t)θˆ(t−1)],(40)Lf(t)=Pf(t−1)φˆf(t)[1+φˆfT(t)Pf(t−1)φˆf(t)]−1,(41)Pf(t)=[I−Lf(t)φˆfT(t)]Pf(t−1),(42)xˆf(t)=φˆfT(t)θˆ(t),(43)φˆf(t)=[−xˆf(t−1),−xˆf(t−2),…,−xˆf(t−na),uˆf(t−1),uˆf(t−2),…,uˆf(t−nb)]T,(44)uˆf(t)=u(t)+cˆ1(t)u(t−1)+cˆ2(t)u(t−2)+…+cˆnc(t)u(t−nc),(45)yˆf(t)=y(t)+cˆ1(t)y(t−1)+cˆ2(t)y(t−2)+…+cˆnc(t)y(t−nc),(46)cˆ(t)=cˆ(t−1)+Ln(t)[wˆ(t)−ψˆT(t)cˆ(t−1)],(47)Ln(t)=Pn(t−1)ψˆ(t)[1+ψˆT(t)Pn(t−1)ψˆ(t)]−1,(48)Pn(t)=[I−Ln(t)ψˆT(t)]Pn(t−1),(49)wˆ(t)=y(t)−φaT(t)θˆ(t−1),(50)xa(t)=φaT(t)θˆ(t),(51)φa(t)=[−xa(t−1),−xa(t−2),…,−xa(t−na),u(t−1),u(t−2),…,u(t−nb)]T,(52)ψˆ(t)=[−wˆ(t−1),−wˆ(t−2),…,−wˆ(t−nc)]T,(53)cˆ(t)=[cˆ1(t),cˆ2(t),…,cˆnc(t)]T,(54)θˆ(t)=[aˆ1(t),aˆ2(t),…,aˆna(t),bˆ1(t),bˆ2(t),…,bˆnb(t)]T.The F-RLS estimation algorithm involves two stages: the parameter identification of the system model (see (39)–(45)) and the parameter identification of the noise model (see (46)–(52)). The two estimation stages are coupled: the identification of the noise model relies on the identification of the system model, vice versa, see (39) and (49). The procedure of the proposed F-RLS algorithm is as follows.1.Initialization: setu(t)=0,y(t)=0,yˆf(t)=0,uˆf(t)=0,wˆ(t)=0,xa(t)=1/p0andxˆf(t)=1/p0fort⩽0, andp0=106.Lett=1,θˆ(0)=1na+nb/p0,cˆ(0)=1nc/p0,Pf(0)=p0Ina+nb,Pn(0)=p0Inc.Collect the input–output datau(t)andy(t), and formφa(t)using (51) andψˆ(t)using (52).Computewˆ(t)using (49), the gain vectorLn(t)using (47), and the covariance matrixPn(t)using (48).Update the parameter estimation vectorcˆ(t)using (46).Computeyˆf(t)using (44) anduˆf(t)using (45), and formφˆf(t)using (43).Compute the gain vectorLf(t)using (40) and the covariance matrixPf(t)using (41).Update the parameter estimation vectorθˆ(t)using (39), and computexˆf(t)using (42) andxa(t)using (50).Increase t by 1, and go to Step 3.Theorem 2For the identification models in(26)and(4)and the F-RLS algorithm in(39)–(54), assume that (A1)–(A2) hold and that there exist positive constants α, β, γ andt0such that the following generalized persistent excitation condition (unbounded condition number) holds[28]:(A4)αI⩽1t∑j=1tφˆf(t)φˆfT(t)⩽βtγI,a.s.,t⩾t0,(A5)αI⩽1t∑j=1tψˆ(t)ψˆT(t)⩽βtγI,a.s.,t⩾t0.Then for anyc>1, the parameter estimation error vectors‖ϑˆ(t)−ϑ‖converges to zero‖θˆ(t)−θ‖2=O([ln⁡t]ct)→0,a.s.,‖cˆ(t)−c‖2=O([ln⁡t]ct)→0,a.s.The proof can be made by using a similar way in the previous section.More generally, consider the following Box–Jenkins systems(55)y(t)=x(t)+w(t)=B(z)A(z)u(t)+D(z)C(z)v(t),whereD(z)=1+d1z−1+d2z−2+…+dndz−nd.Note that the noisew(t):=1C(z)v(t)in (1) is an autoregressive process and the noisew(t):=D(z)C(z)v(t)in (55) is an autoregressive moving average process.Defineθs:=[a1,a2,…,ana,b1,b2,…,bnb]T∈Rna+nb,θn:=[c1,c2,…,cnc,d1,d2,…,dnd]T∈Rnc+nd,φn(t):=[−w(t−1),−w(t−2),…,−w(t−nc),v(t−1),v(t−2),…,v(t−nd)]T∈Rnc+nd.Employing the data filtering technique, we can derive the data filtering based recursive least squares estimation algorithm for the Box–Jenkins systems as follows:(56)θˆs(t)=θˆs(t−1)+Lf(t)[yˆf(t)−φˆfT(t)θˆs(t−1)],θˆs(0)=1na+nb/p0,(57)Lf(t)=Pf(t−1)φˆf(t)[1+φˆfT(t)Pf(t−1)φˆf(t)]−1,(58)Pf(t)=[I−Lf(t)φˆfT(t)]Pf(t−1),Pf(0)=p0Ina+nb,(59)xˆf(t)=φˆfT(t)θˆ(t),(60)φˆf(t)=[−xˆf(t−1),−xˆf(t−2),…,−xˆf(t−na),uˆf(t−1),uˆf(t−2),…,uˆf(t−nb)]T,(61)uˆf(t)=−dˆ1(t)uˆf(t−1)−dˆ2(t)uˆf(t−2)−…−dˆnd(t)uˆf(t−nd)+u(t)+cˆ1(t)u(t−1)+cˆ2(t)u(t−2)+…+cˆnc(t)u(t−nc),(62)yˆf(t)=−dˆ1(t)yˆf(t−1)−dˆ2(t)yˆf(t−2)−…−dˆnd(t)yˆf(t−nd)+y(t)+cˆ1(t)y(t−1)+cˆ2(t)y(t−2)+…+cˆnc(t)y(t−nc),(63)θˆn(t)=θˆn(t−1)+Ln(t)[wˆ(t)−φˆnT(t)θˆn(t−1)],θˆn(0)=1nc+nd/p0,(64)Ln(t)=Pn(t−1)φˆn(t)[1+φˆnT(t)Pn(t−1)φˆn(t)]−1,(65)Pn(t)=[I−Ln(t)φˆnT(t)]Pn(t−1),Pn(0)=p0Inc+nd,(66)wˆ(t)=y(t)−φaT(t)θˆs(t−1),(67)xa(t)=φaT(t)θˆs(t),(68)φa(t)=[−xa(t−1),−xa(t−2),…,−xa(t−na),u(t−1),u(t−2),…,u(t−nb)]T,(69)vˆ(t)=wˆ(t)−φˆnT(t)θˆn(t),(70)φˆn(t)=[−wˆ(t−1),−wˆ(t−2),…,−wˆ(t−nc),vˆ(t−1),vˆ(t−2),…,vˆ(t−nd)]T,(71)θˆs(t)=[aˆ1(t),aˆ2(t),…,aˆna(t),bˆ1(t),bˆ2(t),…,bˆnb(t)]T,(72)θˆn(t)=[cˆ1(t),cˆ2(t),…,cˆnc(t),dˆ1(t),dˆ2(t),…,dˆnd(t)]T.The initialization of the above algorithm is similar to the F-RLS algorithm.Example 1Consider the following OEAR modely(t)=B(z)A(z)u(t)+1C(z)v(t),A(z)=1+a1z−1+a2z−2=1+0.35z−1−0.28z−2,B(z)=b1z−1+b2z−2=0.50z−1−0.60z−2,C(z)=1+c1z−1=1+0.82z−1,θ=[a1,a2,b1,b2,c1]T.The input{u(t)}is taken as a pseudo-random binary sequence with zero mean and unit variance, and{v(t)}as a white noise sequence with zero mean and varianceσ2=0.302, the corresponding noise-to-signal ratio isδns=46.30%. The noise-to-signal ratioδnsof a system is defined as the square root of the ratio of the varianceσw2of the disturbancew(t)and the varianceσx2of the noise-free outputx(t)(namely, the outputy(t)whenv(t)≡0) [29,30] – see Fig. 1:δns=var[w(t)]var[x(t)]×100%=σwσx×100%.By applying the AM-RGLS and the F-RLS algorithms to estimate the parameters of this example system, the parameter estimates and their estimation errors are shown in Tables 1–2, the parameter estimation errorsδ:=‖θˆ(t)−θ‖/‖θ‖versus t are shown in Fig. 3.Furthermore, using the Monte Carlo simulations with 15 sets of noise realizations, the parameter estimates and their estimation biases of two algorithms are shown in Table 3withσ2=0.302.From Tables 1–3 and Fig. 3, we can draw the following conclusions.•The parameter estimation errors of the AM-RGLS and F-RLS algorithms become generally smaller with the data length t increasing – see the estimation errors of the last columns in Tables 1–2 and the estimation error curves in Fig. 3.For the same data length, the parameter estimation accuracy of the F-RLS algorithm is higher than that of the AM-RGLS algorithm – see the estimation error curves in Fig. 3.The average values of the parameter estimates are very close to the true parameters and the variances are small for large t – see Table 3.Example 2Consider the following Box–Jenkins model,y(t)=B(z)A(z)u(t)+D(z)C(z)v(t),whereA(z)=1+a1z−1+a2z−2=1+1.00z−1+0.40z−2,B(z)=b1z−1+b2z−2=0.35z−1+0.62z−2,C(z)=1+c1z−1=1+0.20z−1,D(z)=1+d1z−1=1−0.20z−1,θ=[a1,a2,b1,b2,c1,d1]T.The simulation conditions are similar to those of Example 1 but the noise varianceσ2=0.802andσ2=1.002, respectively, the corresponding noise-to-signal ratios areδns=124.91%andδns=156.13%. By applying the F-RLS algorithm to estimate the parameters of this example system, the parameter estimates and errors are shown in Table 4and Fig. 4. For 15 sets of noise realizations, the Monte Carlo simulation results are shown in Tables 5–6.From Tables 4–6 and Fig. 4, we can see that the proposed F-RLS algorithm is effective for estimating the parameters of the Box–Jenkins system. With the noise-to-signal ratios decreasing, the convergence rate of the parameter estimates given by the proposed algorithm becomes faster – see the estimation error curves in Fig. 4. With the data length t increasing, the estimation errors become generally smaller – see the estimation errors in the last column in Table 4. The Monte Carlo simulation results show the effectiveness of the proposed algorithms.

@&#CONCLUSIONS@&#
This paper investigates parameter identification methods for output error autoregressive models and Box–Jenkins (i.e., OEARMA) models. Based on the data filtering technique, two recursive least squares algorithms are derived by filtering the input–output data. The proposed algorithms have the following properties.•The parameter estimation errors given by the proposed algorithms become generally smaller with the increasing of the data length.The proposed algorithms interactively estimate the parameters of the system models and the noise models.The proposed filtering based estimation algorithm requires lower computational load and achieves higher estimation accuracies than the auxiliary model based recursive generalized least squares algorithm.The proposed algorithms can effectively estimate the parameters of the OEAR and Box–Jenkins systems.The proposed methods can be extended to study identification problems of other linear (time-varying) systems [33–35], linear-in-parameters systems [36,37] and nonlinear systems with colored noise [38–42].