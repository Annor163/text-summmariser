@&#MAIN-TITLE@&#
A hybrid algorithm for feature subset selection in high-dimensional datasets using FICA and IWSSr algorithm

@&#HIGHLIGHTS@&#
A hybrid method is proposed for efficient subset selection in high-dimensional datasets. The symmetrical uncertainty (SU) criterion is exploited to weight features in filter phase.In wrapper phase, both fuzzy imperialist competitive algorithm (FICA) and Incremental Wrapper Subset Selection with replacement (IWSSr) in weighted feature space are executed to search and find relevant attributes.The proposed method has been assessed by applying on 10 standard high-dimensional datasets. We compared our proposed algorithm with other five hybrid algorithms (LFS, IWSS, IWSSr, BARS, and Grasp) and two filter methods (FCBF and PCA). The comparison between the results of our method and others confirms that our method has the best accuracy.The average number of attributes selected by proposed algorithm is considerably less than the other methods.The diagrams show low convergence time and low number of iterations with regard to other methods.

@&#KEYPHRASES@&#
Feature subset selection,FICA,IWSSr algorithm,High dimensional classification problems,

@&#ABSTRACT@&#
Feature subset selection is a substantial problem in the field of data classification tasks. The purpose of feature subset selection is a mechanism to find efficient subset retrieved from original datasets to increase both efficiency and accuracy rate and reduce the costs of data classification. Working on high-dimensional datasets with a very large number of predictive attributes while the number of instances is presented in a low volume needs to be employed techniques to select an optimal feature subset. In this paper, a hybrid method is proposed for efficient subset selection in high-dimensional datasets. The proposed algorithm runs filter-wrapper algorithms in two phases. The symmetrical uncertainty (SU) criterion is exploited to weight features in filter phase for discriminating the classes. In wrapper phase, both FICA (fuzzy imperialist competitive algorithm) and IWSSr (Incremental Wrapper Subset Selection with replacement) in weighted feature space are executed to find relevant attributes. The new scheme is successfully applied on 10 standard high-dimensional datasets, especially within the field of biosciences and medicine, where the number of features compared to the number of samples is large, inducing a severe curse of dimensionality problem. The comparison between the results of our method and other algorithms confirms that our method has the most accuracy rate and it is also able to achieve to the efficient compact subset.

@&#INTRODUCTION@&#
One of the greatest challenges faced by scientists is the low number of instances while there is a large number of features describing each instance occurred sometimes in applicable fields such as data mining, pattern recognition, image processing, bioinformatics [1,2]. The purpose of effective feature subset selection (FSS) would be improving both the accuracy rate and classification efficiency, reducing both computational and spatial costs in constructing classifier, avoiding over-processing, and increasing generalizability power of classifier [3â€“5]. In high-dimensional classification tasks, the attributes are typically categorized into three types: relevant, irrelevant, and redundant features [6,7]. Relevant features are attributes within a dataset which are strongly related to the class labels and they consist of significant information about given dataset. Irrelevant features are not predominant attributes; they are put into dataset due to some reasons (e.g., when the source of initial dataset is noisy, lots of this kind of features are unintentionally added to dataset). Not only they increase the computational complexity, but also they decrease the rate of classifying accuracy when the model is constructed. Finally, whenever the characteristics of two features are interdependent within datasets, it is called that there is redundancy between features. The task of feature subset scheme is choosing variables that have a relevance greater than the other ones and variables that are less relevant (according to criteria applied on their weights) should be discarded [8].Feature selection algorithms are generally divided into following categories: filter, wrapper, embedded and hybrid methods. Filter approaches utilize intrinsic and statistical characteristics of attributes to select optimal subset of features and are independent of learning algorithms. In these algorithms, each feature is weighted based on whether correlation between feature and labels or relationships within features, which is rooted in information theory and correlation criteria. The advantage of this technique is being fast and general. On the contrary, the accuracy rate decreases during performance [9,10]. A learning model and a classifier are exploited to probe desired feature subset in wrapper techniques. Learning model has the task of searching original feature space to find the most efficient subset. Classifier is required to undertake evaluation of candidate efficiency. In comparison with filter approaches, its computational cost makes it almost impossible in high dimensional datasets; on the other hand, the process is successful in finding efficient subset and this method has the advantages of high accuracy [9]. In embedded methods, the classification model specifies which attribute is relevant with high classification efficiency and accuracy. In the first phase of hybrid methods (made with the cooperation of filter and wrapper methods), filter approaches reduce the number of desired attributes and then in reduced feature space, a wrapper method is employed to select efficient features [11,12].The main challenge in high dimensional data classification is the low number of instances while the number of attributes describing instances is very high. Nowadays, since advances in science and technology leads to data stream processing, we need to employ some ways to reduce the dimension of data and some features which are not vital and important should be omitted. Irrelevant and redundant features not only increase both the computational cost and speed of constructing classification model, but also decrease the accuracy of classification. Different kinds of approaches are adopted to select the most efficient feature subset; but every one of them has disadvantages of instability and increase in time convergence rates. Because of these problems, they are not able to extract efficient attributes and so choosing semi-optimal solution is always the best answer.Here, we propose a hybrid method for feature selection of high dimensional dataset. In this method, the SU criteria, which is based on nonlinear information theory [13], are employed to weight attributes done in filter phase. Then, by using both the FICA [14] and IWSSr [15] algorithms in wrapper phase, the search process is fulfilled to detect the most effective subset of attributes. According to the results of our experiments, the comparison between efficiency of our method and other similar methods concerning the accuracy rate and achieving to the impact subset of attributes confirms that proposed method would be able to overcome the weaknesses of previous methods.This paper is organized as follows: first, an overview of previous works is discussed in Section 2. Detailed descriptions of FICA and IWSSr are introduced in Section 3. The proposed FICA-based feature selection algorithm is explained in Section 4. The experimental results are given in Section 5. The conclusions and future work are presented in Section 6.

@&#CONCLUSIONS@&#
In this paper, the principal goal is to present the FSS approach in high dimensional classification problems. We proposed two-phase hybrid algorithm using the advantages of both filter and wrapper. In filter phase, the SU criterion is utilized to weight original features existed in datasets. Then, in wrapper phase, FICA and IWSSr are exploited in weighted space to search and find the efficient and relevant features. The robust FICA is employed to find the global optimal points and select random attributes (based on their weights) by performing sufficient iterations in high dimensional feature space for constructing countries and then updating colonies. In the some other methods such as IWSS, IWSSr, and SFS, the order of arrangements of initial attributes must be considered as a basic input; but this weak point has been removed in our method. Since the feature subset selected in the wrapper phase consists of the accuracy rate of subsets generated in previous steps, it can be said that the accuracy rate of next steps will be guaranteed. By exerting the concept of fuzziness in association with countries (emperors and colonies), and inserting the distribution of attributes into feature vector of colonies are advantages of this approach lead to high rate of accuracy and proficiency in FSS. At last, the IWSSr algorithm has been involved in the context of implementation to remove the redundant features to fulfill the procedure and achieve an optimal compact feature subset.For evaluating of the proposed method, we applied it on 10 standard datasets. The comparison between the results of our method and other ones confirms that our method has the best accuracy. The average number of attributes selected by proposed algorithm is considerably less than the other methods. The diagrams show low convergence time and low number of iterations with regard to other methods. It is also able to achieve to the efficient compact subset in the minimum convergence time concerning other methods. In addition, applying Friedman test on various kinds of methods confirms that our code not only has the high value of accuracy rate but also selects the optimized subset of features which is representative of its excellent proficiency and robustness.In the future, we want to revise this algorithm to extract optimal number of features in multi-label classification problems. In other words, we aim to convert multi-label datasets to datasets with single labels and then, inspect features to find an optimal subset for each dataset. Finally, the aggregate of subsets can be lead to form configuration of final compact subset for each multi-label dataset. In the next steps of our work, we would like to compare the robustness, misclassification, and efficiency of this method with other ones that can be obtained by combining any other type of genetic-based approach (PSO, ACO, etc.).