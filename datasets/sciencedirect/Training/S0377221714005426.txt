@&#MAIN-TITLE@&#
Robustness analysis in Multi-Objective Mathematical Programming using Monte Carlo simulation

@&#HIGHLIGHTS@&#
Robustness analysis of Pareto optimal solutions with perturbations on weights.Systematic Monte Carlo simulation–optimization approach.Introduction of Robustness Index.Revelation of the most robust solutions in a weight neighborhood.Effectiveness of Augmented Weighted Tchebycheff method vs weighted sum.

@&#KEYPHRASES@&#
Multi-objective programming,Robustness analysis,Monte Carlo simulation,

@&#ABSTRACT@&#
In most multi-objective optimization problems we aim at selecting the most preferred among the generated Pareto optimal solutions (a subjective selection among objectively determined solutions). In this paper we consider the robustness of the selected Pareto optimal solution in relation to perturbations within weights of the objective functions. For this task we design an integrated approach that can be used in multi-objective discrete and continuous problems using a combination of Monte Carlo simulation and optimization. In the proposed method we introduce measures of robustness for Pareto optimal solutions. In this way we can compare them according to their robustness, introducing one more characteristic for the Pareto optimal solution quality. In addition, especially in multi-objective discrete problems, we can detect the most robust Pareto optimal solution among neighboring ones. A computational experiment is designed in order to illustrate the method and its advantages. It is noteworthy that the Augmented Weighted Tchebycheff proved to be much more reliable than the conventional weighted sum method in discrete problems, due to the existence of unsupported Pareto optimal solutions.

@&#INTRODUCTION@&#
Optimization affected by parameter imprecision has been a focus of the mathematical programming community during the last twenty years. Solutions to optimization problems can exhibit remarkable sensitivity to perturbations in parameters of the problem, thus often rendering a computed solution infeasible, or significantly suboptimal, or both (Bertsimas, Brown, & Caramanis, 2011). Therefore the concept of robustness in mathematical programming has drawn attention of the scientific community in this field and it is usually under the umbrella of “robust optimization”. By using the term “robustness” we actually mean that there is some kind of uncertainty (or imprecision) in the model and we want to be “at the safe side”. Uncertainty can be present in various forms (uncertain data linked with future outcomes, imprecise model parameters, etc.).Robustness can be defined as a degree to which a solution is insensitive to underlying assumptions within a model. Key elements of robust optimization are volatility and flexibility. The former asks for a solution that is relatively stable to data variations and hedges against bad outcomes while the latter is concerned with keeping options open in a sequential decision process having recourses for the effects of earlier decisions (Greenberg & Morisson, 2008).The concept of “robust optimization” in operational research was introduced by Soyster in 1973 but it received significant attention after the work of Mulvey, Vanderbei, and Zenios (1995) introducing the robust model (“almost feasible”) and robust solution (“close to optimal”). Ben-Tal and Nemirovski (1998, 2000) proposed a robust optimization approach to formulate continuous uncertain parameters and Bertsimas and Sim (2003, 2004) proposed robust optimization models preserving their linear structure and make them more tractable. See also Bertsimas et al. (2011) for a recent review.Despite the vast work that has been done in last two decades on single objective mathematical programming problems the concept of robustness is not so extensively examined in multi-objective programming. Kouvelis and Yu in their seminal textbook devote a section to robustness and efficiency (Kouvelis & Yu, 1997; p. 59). Deb and Gupta (2006) introduce the concept of robustness in multi-objective optimization using meta-heuristics. Liesiö, Mild, and Salo (2007, 2008) focus on robustness in project selection using mathematical programming. Wang and Zionts (2006) provide robustness analysis for the Aspiration-level Interactive Method (AIM). Some recent works also deal with robustness and multi-objective optimization like Zhen and Chang (2012) where robustness is quantified and is used as a second objective function in a berth allocation problem which is heuristically solved. Roland, De Smet, and Figueira (2012) provide a stability radius for the efficient solutions in multi-objective combinatorial problems. In a recent paper Roy (2010) discusses the “multi-faceted” issue of robustness in the general context of operational research and not only in optimization. He claims that the “robustness concern” should be present regarding the choice of model parameters as they are imperfectly defined (p. 630). A concept of robustness in multi-objective optimization was also introduced by Figueira, Greco, Mousseau, and Słowiński (2008) especially regarding interactive multi-objective optimization. Finally, Lahdelma, Hokkanen, and Salminen (1998) introduced method SMAA (Stochastic Multiobjective Acceptability Analysis) in order to deal with uncertainties in multi-criteria problems with discrete alternatives. SMAA can be considered as a robustness (or stability) analysis in multi-criteria decision making methods (Tervonen, Figueira, Lahdelma, Almeida-Dias, & Salminen, 2009).In the present paper we study the concept of robustness in multi-objective programming. In such problems we aim at selection of the most preferred among Pareto optimal solutions. Multi-objective programming combines two aspects: optimization and decision support as we have to deal with a subjective selection among objectively determined solutions. The question that we try to answer in this paper is “how sensitive is our choice on the preference parameters?”. This means that the concept of robustness has to do only with the decision maker’s preference and not on other model’s data. We assume that the decision maker’s preference corresponds to a set of weights for objective functions and we want to examine how sensitive is the obtained solution to the imposed weights. In Mulvey et al’s. (1995) terminology we deal with a “solution robust” situation. We want to measure the robustness of Pareto optimal solutions having as source of uncertainty the precise definition of weights. For this task we design an integrated method that can be used in multi-objective discrete and continuous problems using a combination of Monte Carlo simulation and optimization (Vose, 1996).In the proposed method we gradually increase the sampling space for the objective function weights and we observe how many times we obtain the selected most preferred solution. In order to quantify the “persistence” of the selected solution we introduce appropriate measures of robustness exploiting the results of the Monte Carlo simulation and optimization. In this way, we can compare Pareto optimal solutions according to their robustness, introducing one more characteristic for the solution quality. In addition, especially in multi-objective discrete problems, we can go one step further detecting new, hopefully most robust Pareto optimal solutions in the selected solution neighborhood. In this case our approach is not only descriptive but also prescriptive as it does not only measure the robustness of the Pareto optimal solutions but also proposes new Pareto optimal solutions in the neighborhood that may be more robust. The aim of the present paper is to examine the robustness of one Pareto optimal solution (the selected or most preferred) and not the robustness of the entire Pareto set.It must be noted that under the term “sensitivity analysis” we change one parameter at a time. On the contrary, using Monte Carlo simulation (like e.g. in SMAA) we can simultaneously change the required parameters (the weights in our case) in a systematic way. The proposed approach is also different from the weight stability intervals used in multi-criteria methods (see e.g. Mareschal, 1988 for the PROMETHEE method) where the weights vary one at a time.In order to examine the applicability of the proposed method, a computational experiment is designed using three cases: (1) a Multi-Objective Integer Linear Programming (MOILP) model, (2) a Multi-Objective Mixed Integer Linear Programming (MOMILP) model and (3) a Multi-Objective Linear Programming (MOLP) model. The particular characteristics of each kind of problems and the differences in their behavior are also discussed.The remainder of the paper is as follows: In Section 2 we describe the methodological approach, in Section 3 we describe the computational experiment. In Section 4 we discuss the results. Finally, in Section 5 we present the main concluding remarks.The starting point of our methodology is that there is a Pareto optimal solution (POS) that emerged as “the most preferred” using a multi-objective programming method. We refer to this solution as the reference POS and we denote it using an asterisk (POS*). Usually, the parameters that express the preferences of a decision maker are the weights that are attributed to the objective function. In most multi-objective interactive approaches these weights are provided directly by the decision maker or they can be indirectly determined. It must be mentioned that the objective functions should be first brought in the same scale (normalization) in order the weights to be meaningful. In the present paper we use the following normalization scheme:(1)zk(x)=fk(x)-fkminfkmax-fkminfor maximization criteria(2)zk(x)=fkmax-fk(x)fkmax-fkminfor minimization criteriawhere fkmin and fkmax are the minimum and the maximum of the kth objective function fk(x) as obtained from the payoff table. We must also keep in mind that although the meaning of the weights may be different from method to method, they always represent the preference parameters imposed by the decision maker (Steuer, 1986). Eventually, the reference POS* corresponds to a specific combination of weights for objective functions (either using a weighted sum approach, or weighted Tchebycheff, or goal programming, or aspiration reservation methods), (see e.g. Ehrgott & Gandibleux, 2002; Miettinen, 1999; Steuer, 1986; Wierzbicki & Granat, 1999).The basic idea behind the proposed method is the following: we “relax” the weights that correspond to POS* by allowing them to take values in their “neighborhood”. The “neighborhood” is determined by the decision maker and it is expressed by an interval around the so called reference weights and they are symbolized with wp* (p is the index of objective functions). These intervals are defined by the neighborhood parameter α which is actually a percentage of the initial weight. For example, a 10%-neighborhood means that corresponding weights (wp*) take values in the interval [w×(1−0.1), w×(1+0.1)]. For example, if we have three objective functions and the selected Pareto optimal solution corresponds to the weights 0.1, 0.3 and 0.6 then in Fig. 1the 10%, 25% and 50% neighborhood is illustrated.In order to better examine the weight neighborhood, the decision maker divides these weight intervals using a number G of grid points (usually G is set between 5 and 10). The higher the number G of grid points, the greater the accuracy in evaluation of robustness, but computation time also increases. Starting from the reference weights wp* grid points help to gradually expand in the weight neighborhood. If there are G grid points the whole process is completed in G steps. When we reach gth grid point the weight interval becomes:(3)wp∗×1-g/G×a,wp∗×(1+g/G×a)where wp* is the reference weight for objective function p and a is the neighborhood parameter i.e. the percentage of reference weight that defines the border of neighborhood around that weight. Therefore the maximum weight interval for wp* is:(4)wp∗×(1-a),wp∗×(1+a)Subsequently, for each grid point g a Monte Carlo simulation is performed in order to sample weights from the corresponding intervals using a joint uniform distribution (i.e. not one weight at a time, but all the weights simultaneously). The random sampling within weight intervals is performed using the sample and rejection technique. After a vector of uniformly distributed normalized weights has been generated, the weights are tested against their bounds. If any of the constraints is not satisfied, the entire set is rejected and the weight generation is repeated (see e.g. Tervonen & Lahdelma, 2007; p. 507). This simple technique proved sufficient for our case as the rejection rate is rather small (from 2% to 12% which means that for 1000 valid samples we need from 1020 to 1136 Monte Carlo iterations). Probably this is due to the nature of the weight space in our case. Specifically, the initial weights sum to 1 and subsequently we symmetrically extend their intervals around the initial value. Therefore, the resulting weight space can be easily represented by uniformly distributed vectors. However, in more complicated cases of weight space, more sophisticated algorithms for random weight generation need to be considered like e.g. the hit and run algorithm from Tervonen, van Valkenhoef, Basturk, and Postmus (2013).After weight sampling, we use these weights in the scalarization function of the multi-objective programming model. It must be noted that the scalarization function may be of any kind that uses weights e.g. weighted sum, augmented Tchebycheff (or Chebyshev), goal programming, etc. (we shall return to this later on providing some conclusion regarding their sufficiency in specific kind of models). The results of optimization (i.e. the values of objective functions and the values of decision variables) are stored and we continue with the next Monte Carlo iteration. When we finish the N Monte Carlo iterations we obtain a set of N Pareto optimal solutions designated as Sg. We then count how many times in Sgwe have produced the reference POS* which corresponds to the weights wp*. This frequency is an indication of the robustness of the most preferred solution due to small perturbations within weights. As we move to the next grid point we actually expand the weight interval of random sampling. Therefore we expect that as we move to wider intervals the frequency of POS* to drop. The degree of resistance to this drop is considered as a measure of robustness for the specific POS*.In order to measure the degree of robustness according to this drop we introduce the Robustness Index (RI) which is calculated as follows: We calculate the frequency of POS* among the solutions obtained from Monte Carlo simulation for the specific grid point g. We draw the chart of frequency as a function of the width of sampling interval as indicated by the grid points. The ordinate of each point λgis the frequency of POS* in the set of solutions Sgand the abscissa is the range around wp* of the sampling interval. This is definitely a decreasing function and the so called Robustness Chart depicts the robustness of POS* as sampling intervals around wp* are getting wider. An illustrative Robustness Chart is shown in Fig. 2.From the Robustness Chart we can calculate the Robustness Index as a measure of robustness for a specific POS*. The Robustness Index is calculated as the area under Robustness Curve created by λgpoints divided by the full robustness area which is actually the case where for the whole interval the frequency remains at 100% (i.e. a rectangular area 100%×10% in Fig. 2). The formula for calculating Robustness Index is the following:(5)RI=λ0+λ12+λ1+λ22+⋯+λG-1+λG2×aGaRI=λ02+∑g=1G-1λg+λG2GRI=12+∑g=1G-1λg+λG2GFrom a mathematical point of view, the Robustness Index can be seen as the integral of the function that expresses the frequency of POS* in relation to the width of the sampling interval x (x is real and x⩽a) divided by the maximum robustness (=a×100%). In other words, the function fq(x) that expresses the frequency of POS* as a function of x, is integrated from 0 to a in order to provide the Robustness Index of POS*, as follows:(5a)RI=∫0afq(x)dxαEq. (5) is actually the arithmetic calculation of the integral of Eq. (5a) using the corresponding grid points for discretization.RI is a measure of robustness that can be used to compare different POS* on how much robust they are on small perturbations of weights. The higher the RI, the more robust the corresponding POS*. RI quantifies the concept of robustness and can provide useful information to the decision maker enriching his perception about candidate solutions before reaching his final choice. It must be noted that RIs are relevant for POS* of the same problem. The integrated flowchart of the robustness analysis algorithm is shown in Fig. 3.It must be noted that the computational effort is almost insensitive to the involved number of objective functions. This can be attributed to the fact that we work with a scalarization of the objective functions which always leads to a solution of a single objective problem. Regarding the representation of the weight space and how it may explode with more objective functions (and therefore weights), we must keep in mind that we talk about a neighborhood around the original weights. This neighborhood is fairly restricted in comparison to the full weight space (the neighborhood is defined as a range of maximum ±20% around the original weights). Therefore, even for 5 or 6 objective functions 1000 Monte Carlo iterations can produce an adequate representation of the weight space. Moreover, if we want to increase the number of Monte Carlo iterations we can always reduce the number of grid points of the formulation in order to keep the computational effort to tractable limits (more iterations per round, but less rounds). Conclusively, the proposed approach escalates smoothly with the number of objective functions.Some clarification is required on how we can conclude that one solution from Sgis the same with POS*. For this comparison we rely on the value of decision variables and not on the value of objective functions as there may be alternative optima. In Multi-Objective Integer Programming (MOILP) problems this is an easy task as the decision variables are integer (and very often binary) and therefore the comparison of two solutions is straightforward. In Multi-Objective Linear Programming (MOLP) and Multi-Objective Mixed Integer Linear Programming (MOMILP) problems we use the following approach: For every solution in Sgwe calculate the percentage absolute deviation (relative deviation) of every decision variable’s value from the value of the same decision variable in POS* according to the following equation:(6a)rdi=|xi-xiPOS∗|xiPOS∗where xiis the value of ith variable and xiPOS* the value of ith variable in POS*. It must be noted that if xiPOS*=0 and xi=0 then rdiis defined to be 0 otherwise if xiPOS*=0 and xi≠0 then rdiis defined to be 1.The percentage absolute deviation is calculated also for the objective function values according to:(6b)rzp=|zp-zpPOS∗|zpPOS∗where zpis the value of pth objective function and zpPOS* the value of pth objective function in POS*. Again we define that if zpPOS*=0 and zp=0 then rzpis defined to be 0 otherwise if zpPOS*=0 and zp≠0 then rzpis defined to be 1. In this way we obtain the deviations in the decision variable space as well as in the objective space.Subsequently we find the greatest relative deviation across decision variables and objective functions. If the greatest deviation is less than a tolerance (designated as tol) then the two solutions are considered to be equal. It must be noted that tolerances for the decision space and the objective space may be different. The method is illustrated with the following example: Assume that we have a bi-objective problem with 6 decision variables and 5 solutions to compare with POS* as shown in Table 1.If we assume a tolerance limit of 10% for decision variables (the maximum deviation among all variables should be less than 10% in order to be considered “the same as POS*”) and 3% for objective functions, we see that only solution S2 can be considered as “same as POS*”). However if we raise the tolerance limit to 20% and 6% for decision variables and objective functions respectively, then solution S5 is also added. Usually a tolerance limit of 5% or 10% is appropriate, but it depends on the problem. The greater the number of decision variables the lower the tolerance limit should be.We test our method for robustness analysis in multi-objective programming using three cases: (1) A MOILP problem dealing with project selection (2) a MOMILP problem dealing with stock portfolio optimization with cardinality constraints and (3) A MOLP problem dealing with stock portfolio optimization. In all three types of problems we tested two models, namely, (1) the simple weighted sum of objective functions and (2) the Augmented Weighted Tchebycheff (AWT) method (Steuer, 1986). The reason for using AWT is its capability to generate unsupported POS in MOILP problems (Steuer, 1986; p. 420). As we will see this is of crucial importance in the robustness analysis of MOILP problems. Although the meaning of the weights varies from method to method, we assume that in both methods these preference parameters capture criteria importance. Objective functions are normalized to a common scale in order to have meaningful weights. As we will see, the results from two methods differ significantly according to the type of problem. For each case three reference POS* are generated (corresponding to a specific combination of weights) in order to calculate and compare their robustness measures.The first model is a capital budgeting problem concerning 133 renewable energy source (RES) projects. The objective is to find the most preferred project portfolio. There are 5 objective functions namely:(1) regional development, (2) CO2 emissions reduction, (3) economic performance measured with the internal rate of return (IRR) of the investment, (4) employment positions, (5) land use. The complete data are available in Makrivelios (2011).The decision variables of the model are binary, indicating acceptance (Xi=1) or rejection (Xi=0) of the ith project in the final portfolio. The constraints of the model are:•Available budget is 200M€ (total cost of the 133 projects is 659M€).Cost of projects in Central Greece should be less than 30% of the total cost.Cost of projects in Peloponnese should be less than 15% of the total cost.Cost of projects in East & West Macedonia, Northern & Southern Aegean, Epirus should be greater than 10% of the total cost.Number of projects from each technology should be between 20% and 60% of selected projects.Total capacity of selected projects should be greater than 300megawatt.The objective function of the weighted sum model is the following:(7)maxZ=∑k=15wk×zkwhere zkare the normalized expressions calculated from Eqs. (1) and (2) for the five objective functions with:(8)f1(x)=∑i=1133regi×Xi,f2(x)=∑i=1133co2i×Xi,f3(x)=∑i=1133irri×Xi,f4(x)=∑i=1133empi×Xi,f5(x)=∑i=1133lui×Xiwith regi, co2i, irri, empi, luibeing the performance of the ith project in regional development, CO2 emission reduction, IRR of investment, employment positions and land use respectively.The AWT model differs from the weighted sum model in the objective function. It has also some additional constraints, decision variables and parameters. The objective function is:(9)minZAWT=a+ρ×∑k=15fkmax∗-fk(x)and the additional constraints are:(10)a⩾wk×fkmax∗-fk(x)fkmax∗-fkminfork=1,…,5where α is an auxiliary variable of minimax type, ρ a small parameter (=0.001) and fkmax* is the maximum obtained from the payoff table increased by one (corresponding to z** used in Steuer, 1986; p. 420). The fkmin is the constant definition point for objective k (Steuer, 1986, p. 426). We use a normalized distance from the ideal in order to make the weights of the two approaches (weighted sum and AWT) having similar meaning.We consider three combinations of weights in order to examine the robustness of corresponding reference POS* as shown in Table 2.The neighborhood parameter α is set to 20% which means that the weights will eventually vary to [wp*×(1−0.2), wp*×(1+0.2)]. For example, the weight intervals presented in Table 3correspond to first weight combination from Table 2. The number of grid points G is set to 10 and the number of Monte Carlo iterations is set to 1000.In the second case we have a portfolio selection problem with 50 stocks from Eurostoxx 50 market. Objective functions are (1) the portfolio’s return (2) the portfolio’s risk as quantified by Mean Absolute Deviation (MAD) and (3) the portfolio’s Dividend’s Yield. It is a MOMILP model which it is described in detail in Xidonas and Mavrotas (2012). The resulting model is a MOMILP with 3 objective functions, 100 binary variables, 410 continuous variables and 520 constraints. The formulation of the weighted sum and the AWT model follows the guidelines of Section 3.1.We test three combinations of weights in order to examine the robustness of corresponding POS* (see Table 4).The neighborhood parameter α is set to 10% which means that weights will eventually vary within [wp*×(1−0.1), wp*×(1+0.1)]. The number of grid points G is set to 10 and the number of Monte Carlo iterations is set to 1000. Given that we have also continuous variables along with 0–1 variables, we follow the approach of Section 2.2 for comparison of solutions in Sgwith POS*. In this case we consider a 10% tolerance limit for decision variables and 3% tolerance limit for objective functions (i.e. for one POS in Sgin order to be considered as “the same” with POS* the maximum relative deviation across the values of decision variables should not be greater than 10% and across the objective function values should not be greater than 3%).This is the same problem as in Section 3.2 but we have removed cardinality constraints, regulatory constraints and entrance thresholds that need binary variables for their formulation. Therefore it is eventually a MOLP problem with 3 objective functions, 260 continuous variables and 317 constraints. The same three combinations of weights as in Section 3.2 are used also in the MOLP case as well as the same neighborhood parameters and tolerance limit for the comparison of solutions.

@&#CONCLUSIONS@&#
Robustness analysis in Multi-Objective Mathematical Programming can provide useful insight to the decision makers. In the present paper robustness analysis deals with weights of objective functions which are usually the most important preference parameters in this decision making context. The question is how small perturbations on weights may affect the final decision. We developed a methodology for measuring this kind of robustness based on Monte Carlo simulation. In contrast to sensitivity analysis where we change one parameter at a time, by using Monte Carlo simulation we can simultaneously alter the required parameters (in our case – the weights) and directly examine their effect on obtained solutions. Two new concepts of that are introduced with this paper, namely, the Robustness Chart and the Robustness Index may convey very useful information to the decision maker regarding the robustness of Pareto optimal solutions. Measuring robustness of candidate solution can lead to more robust decisions.The comparison of reference solutions (POS*) regarding their robustness can be considered as a post optimality phase. However we can use robustness analysis in order to find new Pareto optimal solutions that may be more robust than the initial reference solution. For this reason, a systematic way of examining the neighborhood of reference solution is also proposed. In this way, we can discover additional Pareto optimal solutions in reference solution’s neighborhood that may be more robust.From the computational experiment it was confirmed that the weighted sum method in multi-objective problems with discrete variables leave several Pareto optimal solutions undetected. In this kind of problems the weighted sum method is insufficient as it leads to a significant underestimation of the size of Pareto set and provides misleading results concerning the robustness analysis of reference solutions. On the other hand, AWT proved to be much more appropriate for this kind of problems.There are a lot of new features that can be considered for future research. One thing is the integration of all these characteristics of robustness analysis in multi-objective programming in an integrated platform where the decision maker provides just robustness analysis parameters and receives robustness analysis results. A second thing is to test the proposed method with more multi-objective programming methods that use weights. In addition, we can move one step further and adjust the method to Goal Programming where beyond the weights also the value of goal can be subject to small perturbations, following the same methodology (Monte Carlo simulation–optimization, comparison of solutions with POS*, etc.). A challenging task is to produce an approximation of the Pareto front using an adequate number of weight vectors and then apply the proposed method for each one of them, in order to identify robust regions of the Pareto front. Finally we can test the incorporation of more probability distributions in addition to the currently used uniform distribution following the guidelines of Steuer (1986) that uses a 50–50% Uniform – Weibull distribution for similar purposes.