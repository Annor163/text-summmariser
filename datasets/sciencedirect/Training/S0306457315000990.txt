@&#MAIN-TITLE@&#
Optimization and label propagation in bipartite heterogeneous networks to improve transductive classification of texts

@&#HIGHLIGHTS@&#
Scalable algorithm based on bipartite networks to perform transduction.Unlabeled data effectively employed to improve classification performance.Better performance than algorithms based on vector space model or networks.Rigorous evaluation to show the drawbacks of the existing transductive algorithms.Trade-off analysis between inductive supervised and transductive classification.

@&#KEYPHRASES@&#
Text classification,Transductive learning,Graph-based learning,Text mining,Label propagation,Bipartite heterogeneous network,

@&#ABSTRACT@&#
Transductive classification is a useful way to classify texts when labeled training examples are insufficient. Several algorithms to perform transductive classification considering text collections represented in a vector space model have been proposed. However, the use of these algorithms is unfeasible in practical applications due to the independence assumption among instances or terms and the drawbacks of these algorithms. Network-based algorithms come up to avoid the drawbacks of the algorithms based on vector space model and to improve transductive classification. Networks are mostly used for label propagation, in which some labeled objects propagate their labels to other objects through the network connections. Bipartite networks are useful to represent text collections as networks and perform label propagation. The generation of this type of network avoids requirements such as collections with hyperlinks or citations, computation of similarities among all texts in the collection, as well as the setup of a number of parameters. In a bipartite heterogeneous network, objects correspond to documents and terms, and the connections are given by the occurrences of terms in documents. The label propagation is performed from documents to terms and then from terms to documents iteratively. Nevertheless, instead of using terms just as means of label propagation, in this article we propose the use of the bipartite network structure to define the relevance scores of terms for classes through an optimization process and then propagate these relevance scores to define labels for unlabeled documents. The new document labels are used to redefine the relevance scores of terms which consequently redefine the labels of unlabeled documents in an iterative process. We demonstrated that the proposed approach surpasses the algorithms for transductive classification based on vector space model or networks. Moreover, we demonstrated that the proposed algorithm effectively makes use of unlabeled documents to improve classification and it is faster than other transductive algorithms.

@&#INTRODUCTION@&#
Text automatic classification (TAC) is one of the most important tasks to manage, retrieve and extract knowledge from a huge number of textual documents (Manning, Raghavan, & Schütze, 2008; Nedjah, Mourelle, Kacprzyk, Frana, & de Souza, 2008; Berry & Castellanos, 2008; Li, Zhu, & Ogihara, 2008; He & Zhou, 2011; Uysal & Gunal, 2014). TAC automatically assigns a predefined category to a textual document.Generally TAC is carried out by using inductive leaning algorithms (Weiss, Indurkhya, & Zhang, 2012; Sebastiani, 2002), which induce classification models to classify new or unseen texts. Usually a large number of labeled documents are necessary to induce an accurate classification model. Nevertheless, labeling texts is usually expensive and time consuming. Thus, a more practical approach is to employ methods which make use of the plenty of unlabeled texts available to perform and improve TAC.Transductive approaches are widely used when labeled training data are insufficient. In this case, they make use of unlabeled data to improve classification performance (Kong, Ng, & Zhou, 2013; Chapelle, Schölkopf, & Zien, 2006; Belkin, Niyogi, & Sindhwani, 2006; Joachims,1999). Transductive classification directly estimates the labels of unlabeled instances without creating a model to classify new texts. Several algorithms considering texts represented in a vector space model have been developed to perform transductive classification such as Self-Training (Yarowsky,1995), Co-Training (Blum & Mitchell, 1998), Expectation Maximization (EM) (Nigam, McCallum, Thrun, & Mitchell, 2000), and Transductive Support Vector Machines (TSVM) (Joachims,1999). However, the use of these algorithms is unfeasible in practical applications due to the assumptions of these algorithms about the data distribution and computational cost. Moreover, the assumption that instances or terms are independent also impairs their classification performances.Network-based algorithms came up to avoid the drawbacks of the algorithms based on vector space model and to improve transductive classification. Networks are mostly used for label propagation, in which some labeled objects propagate their labels to other objects through the network connections to perform transductive classification (Zhu & Goldberg, 2009; Rossi, Lopes, & Rezende, 2014; Subramanya & Bilmes, 2008; Zhou, Bousquet, Lal, Weston, & Schölkopf, 2004). Label propagation using just few labeled examples can obtain higher classification performance than inductive classification using a large number of labeled examples for TAC (Rossi, Lopes, & Rezende, 2014). Moreover, the use of networks to model text collections allows extracting patterns which are not extracted by algorithms based on vector-space model (VSM) (Breve, Zhao, Quiles, Pedrycz, & Liu, 2012).Text collections are modeled as networks using homogeneous or heterogeneous networks. Homogeneous networks contain objects of a single type and heterogeneous networks are compounded by objects of different types. Document homogeneous networks have been used to model text collections as networks for label propagation (Jebara, Wang, & Chang, 2009; Kim, Pantel, Duan, & Gaffney, 2009; Subramanya & Bilmes, 2008; Wang & Zhang, 2006; Castillo, Donato, Gionis, Murdock, & Silvestri, 2007; Zhou et al., 2004; Zhu, Ghahramani, & Lafferty, 2003). In such networks, documents propagate their labels directly to other documents. Documents are connected according to hyperlinks, citations or similarities. The use of just hyperlinks and citations to build document networks reduces the quality of classification (Angelova & Weikum, 2006) and limits the application domains. On the other hand, documents wired considering similarity have been applied since they model any type of text collections and improve the classification quality (Angelova & Weikum, 2006). However, computing similarities poses a high computational cost, and the parameters such as minimum similarity or number of neighbors, significantly impact the classification accuracy (de Sousa, Rezende, & Batista, 2013).Bipartite networks have come up as an alternative to model text collections as networks (Rossi, Faleiros, Lopes, & Rezende, 2012; Rossi, Lopes, Faleiros, & Rezende, 2014; Rossi, Lopes, & Rezende, 2014), in which objects correspond to documents and terms. Terms are linked to documents in which they are present. This network is easily generated, since there is no need to set parameters or compute similarities. Moreover, it has provided promising results for text classification (Rossi et al., 2012; Rossi, Lopes, Faleiros, et al., 2014: Rossi, Lopes, & Rezende, 2014). In such networks, documents propagate their labels to terms and then the terms propagate their labels to documents.Instead of using the bipartite network structure just as means to propagate labels, this structure can be used to set the relevance scores of terms for classes, i.e., how much the presence of a term in a document increases or decreases the probability of a document belonging to a class. In (Rossi et al., 2012; Rossi, Lopes, Faleiros, et al., 2014) the relevance scores of terms for classes are induced using the bipartite network structure. These relevance scores were used to classify new/unseen documents, providing accuracies higher than state-of-the-art algorithms. However, scenarios with only few labeled documents impair the induction of term scores and consequently the classification accuracy.In this paper we propose an algorithm to set the relevance scores of terms for classes considering labeled and unlabeled documents represented in a bipartite heterogeneous network. The relevance scores are obtained through an optimization process considering the current labels of the documents. The obtained relevance scores are propagated to define the new labels to unlabeled documents. Optimization and label propagation are repeated iteratively until converge, i.e., until the labels assigned to unlabeled documents do not change. The proposed algorithm, named TCBHN (Transductive Classification based on Bipartite Heterogeneous Network) obtains better classification performance and is faster than transductive algorithms based on vector space model or networks.The main contributions of this article are fivefold:•We propose a transductive classification algorithm which effectively makes use of unlabeled data to improve text classification.We propose a scalable transductive classification algorithm which makes use of bipartite networks to perform transductive classification.We propose an algorithm which surpasses the classification performance of state-of-the-art algorithms based on vector space model or networks.We conduct a rigorous comparative evaluation of the proposed classification algorithm with traditional and state-of-the-art algorithms based on vector space model and networks. The evaluation carried out in this article allows to highlight the drawbacks of the existing transductive classification algorithms and to highlight the advantages of the proposed algorithm. We also present the behavior of the algorithms for a different range of labeled documents.We present a trade-off between inductive classification and transductive classification. We analyse the differences between inductive classification and transductive classification considering classification evaluation measures and classification time.The remainder of this paper is organized as follows. Section 2 presents background and related works about transductive classification, texts represented by networks, and the use of bipartite networks to induce relevance scores of terms for classes. Section 3 presents details on the proposed algorithm for transductive classification of texts using bipartite networks. Section 4 presents details of the experimental evaluation and the results. Finally, Section 5 presents the conclusions and points to future work.In this section we present the notations and computational structures to perform transductive learning. We standardize both of them to be used in transductive learning on vector space model and networks. Next, we detail the algorithms which perform transductive learning on vector space model or networks, present their pseudocodes, drawbacks, and how to generate networks from text collections to be used as input to network-based algorithms. We also present how to induce the class information of terms in text collections in a scenario with all labeled documents.LetC={c1,c2,⋯,cl}represent the set of class labels, letT={t1,t2,⋯,tm}be the set of terms, and letD={d1,d2,⋯,dn}be the set of documents of a text collection. In a transductive learning scenario,D=DL∪DU, in whichDLrepresents the set of labeled documents andDUrepresent the set of unlabeled documents. Finally, letY={y1,y2,⋯,y|D|}be the labels of the labeled documents and the labels assigned during the classification process, i.e.,Y=YL∪YU.Differently from supervised inductive classification, which aims to create a classification model to approximate a real category assignment functionR:DL→YL, the goal of transductive learning is to find an admissible functionF:DL+U→YL+U, in which the unlabeled data are used to improve classification performance. In practice, transductive learning assigns weights or relevance scores to documents for each one of the classes and the documents are classified considering these weights. In order to do so, letfdi={fc1,fc2,⋯,fc|C|}be the weight vector of a document diwhich stores the weights of a document difor all classes inC. Hence it is also referred to as class information vector. LetF(D)={fd1,fd2,⋯,fd|D|}Tbe a matrix which stores all the weight vectors of the documents. The values of the vector f can be binary, probabilities or real values.The predefined labels for a documentdi∈DLare stored in a weight vectorydi={y1,y2,⋯,y|C|}, which has the value 1 in the position corresponding to the class of the document diand 0 to the others. The predefined labels of all labeled documents are stored in a matrixY(DL)={yd1,yd2,⋯,yd|DL|.}T. Most of the transductive classification algorithms restrict thatF(DL)=Y, i.e., there is no change in the class information of labeled documents (Yarowsky, 1995; Blum & Mitchell, 1998; Joachims, 1999; Nigam et al., 2000; Zhu et al., 2003). However, some of them relax this restriction and allow to change the class information of labeled examples during classification (Zhou et al., 2004; Yin, Li, Mei, & Han, 2009, Ji et al., 2010).Some algorithms need to obtain the weights of terms for classes to infer the class information of unlabeled documents. Thus, a structure to store class information from terms are also required. We usedftj={fc1,fc2,⋯,fc|C|}to represent the class information of terms for classes and the matrixF(T)={ft1,ft2,⋯,ft|T|}Tto store all class information of the terms.The weights of the links among network objects are stored in a matrix W. Link weights among a document diand other documents are represented by a vectorwdi={wd1,wd2,⋯,wd|D|}. The same vectorwdiis used to denote the link weights among a document diand its terms. In this case,wdi={wt1,wt2,⋯,wt|T|}. This vector is also used to represent a document diin a vector space model and store the frequency, or other frequency-based measure, of terms in the document di.The first researches about transductive learning for text classification consider text collections represented in vector space model (Yarowsky, 1995; Blum & Mitchell, 1998; Joachims, 1999; Nigam et al., 2000). Usually a bag-of-words is used to represent the text collection, in which each document is represented by a vector and each dimension of the vector corresponds to a single word of the collection. The values in the vectors are based on the frequency of a term in a document, such as binary weights, term frequency (tf) or term frequency – inverse document frequency (tf–idf) (Salton, 1989).Traditional and state-of-the-art transductive algorithms based on vector space model are (Zhu & Goldberg, 2009; Chapelle et al., 2006): Self-Training, Co-Training, Expectation Maximization, and Transductive Support Vector Machines. There are also some combinations or variations of these algorithms to perform transductive learning. In the next subsections we detail the algorithms based on vector space model mentioned above.Considering texts represented in a vector space model, perhaps the most natural way to perform transductive learning is through Self-Training (Culp & Michailidis, 2008; Haffari & Sarkar, 2007; Yarowsky,1995). In this approach, initially the labeled documents are used to induce a classification model through supervised inductive learning. Any inductive algorithm, such as Multinomial Naive Bayes, Transductive Support Vector Machine, or k-Nearest Neighbor can be used to induce the classification model. This model is used to classify the unlabeled documents and the X most confident classified documents are added to the set of labeled documents. Then, the classification model is reinduced considering the new set of labeled documents. This process is repeated until all unlabeled documents were added to the set of labeled documents. Algorithm 1 presents the pseudocode of the Self-Training approach.Algorithm 1Self-Training.The assumption of Self-Training is that the most confident classifications are correct. However, this is difficult to hold in practice and is true just when the classes are well separable in vector space model (Zhu & Goldberg, 2009). Moreover, mistakes in the most confident classifications degrade the classification performance in the next iterations.Self-Training have to induce|DU|/Xclassification models and may not scale well for small values of X. On the other hand, high values of X will practically perform an inductive supervised learning, since most of the unlabeled documents will be classified through an model induced considering only labeled documents.Despite generating classification confidences to sort unlabeled documents, the most confident documents are inserted into the set of labeled documents considering the value 1 for the class with the highest confidence and 0 for others, i.e., Self-Training performs a hard classification of unlabeled data.Co-Training is an extension of the Self-Training approach for text collections with two views (Blum & Mitchell, 1998). In a simplified version of Co-Training presented in (Zhu & Goldberg, 2009), the X most confident classifications in one view are added as labeled examples in the other view. Similarly to Self-Training, this process is repeated until all unlabeled documents be added to the set of labeled documents.The assumptions of Co-Training are: (i) there are two views to represent the text collections; (ii) the views are independent to each other; and (iii) each view is able to induce an accurate classification model by itself. Nevertheless, these assumptions usually do not hold in practice. Moreover, Co-Training presents the same drawbacks of Self-Training such as degradation in classification performance due to error propagation (Laguna & de Andrade Lopes, 2010), and the cost to induce a classification model and sort documents by classification confidence repeatedly.For collections which do not have two views, Co-Training is performed by splitting the feature space into two disjunct sets (Xu, Tao, & Xu, 2013; Li, Meng, Cao, & Sun, 2009; Bickel & Scheffer, 2004), which usually does not significantly improves results, or as performed in (Laguna & de Andrade Lopes, 2010). In the latter, the two views are produced by using two different k-Nearest Neighbor classifiers with different biases. These distinct biases allow the effective cooperation between the two classifiers in the Co-Training learning phase. In Algorithm 2 we present the pseudocode of the simplified version of the Co-Training approach (Zhu & Goldberg, 2009) for a bag-of-words splitted into two views.Algorithm 2Co-Training.The Expectation Maximization (EM) approach allows to assign class information to unlabeled documents iteratively without inducing classification models repeatedly. EM performs a soft classification on unlabeled data, i.e., classification confidences are assigned to unlabeled documents and these confidences are used in the next iteration of the approach. EM performs hill climbing search to estimate maximum a posteriori probability in problems with incomplete data (Dempster, Laird, & Rubin, 1977).Nigam et al. (2000) presents an instantiation of the EM approach for transductive classifications of textual documents. In this work, the authors consider that each text is generated by one or more mixture components and each mixture component is associated with one class. A component for a class cjcorresponds to the class information of terms for the class cj(F(T)). Thus the goal of EM is to obtain the mixture components for each class using labeled and unlabeled documents.Nigam et al. (2000) combines EM with Multinomial Naive Bayes to perform transductive classification. The class information of a document difor a class cjaccording to Bayes rule is:(1)fdi,cj=P(cj|di)=P(cj)P(di|cj)P(di).P(cj) is the probability of occurrence of the class cjconsidering the class information of the documents. Therefore, P(cj) using Laplace estimator is:(2)P(cj)=1+∑di∈D0.35em0exP(cj|di)|C|+|D|.P(di|cj) is the probability of occurrence of a document digiven a class cj. The naive Bayes assumption states that the words of a document are generated independently of each other and of its position in the document. Therefore, the probability of occurrence of a document digiven a class cjis given by the probability of occurrence of its terms for a class cj, i.e.:(3)P(di|cj)=∏tk∈T,wdi,tk>0P(tk|cj).The probability of occurrence of a term tiin a class cjgiven the current class information of the documents using Laplace estimator is:(4)fti,cj=P(ti|cj)=1+∑dk∈D0.35em0exwti,dkP(cj|di)|T|+∑tl∈T0.35em0ex∑dm∈D0.35em0exwtl,dmP(cj|di).P(di) is the normalization term, i.e.,(5)P(di)=∑cj∈CP(cj)P(di|cj).Algorithm 3 presents the pseudocode of the EM approach for transductive text classification (Nigam et al., 2000). The computation of the class information of terms (line 3) is called E-step and the estimation of the class information of terms (line 4) is called M-step. E-Step and M-step are repeated until convergence, i.e., until the class information of labeled documents do not change too much in consecutive iterations, or until a fixed number of iterations.Algorithm 3Expectation Maximization.The EM approach presented above can be extended or improved considering (Nigam et al., 2000): (i) weighting the importance of unlabeled documents in the computation ofF(T)and (ii) considering multiples components for each class. In the first case a function Λ(di), presented in Eq. (6), is used to weightP(cj|di;0.12em0exF(T))in Eqs. (2) and (4).(6)Λ(di)={λif0.35em0exdi∈DU(parametersetbytheuser)1if0.35em0exdi∈DL.The use of multiple components for each class implies the use of EM to estimate the parameters of each component. Then, the class information of a document difor a class cjconsiders the sum of the probability to occur diin each one of the components from class cj.EM classification is not accurate if the generative assumption is violated. A mix of Co-Training and EM is also found in literature to perform transductive classification in text collections with two views (Ghani, 2002; Nigam & Ghani, 2000).Transductive Support Vector Machines (TSVM) (Vapnik, 1998; Joachims,1999) considers labeled and unlabeled documents to obtain a maximal margin hyperplane. It is a transductive version of the well-known Support Vector Machines. The coefficients of a hyperplane correspond to the class information of terms. Considering a binary classification problem, i.e.,fdiandydiare variables (or vectors with one dimension), in whichydiorfdi={−1,1}, the class information of a document diin TSVM is:(7)fdi={+1,if0.35em0ex∑tk∈T0.12em0exftk·wdi,tk+b>0;−1if0.35em0ex∑tk∈T0.12em0exftk·wdi,tk+b<0LetHbe the set of possible hyperplanes. LetHi∈Hbe a hyperplane and letHi+be the parallel hyperplane closest to Hiand a positive document dilocated on the hyperplaneHi+, i.e.,(8)∑tk∈Tftk·wdi,tk+b=0,0.35em0exfdi=+1.LetHi−be the parallel hyperplane closest to Hiand a negative document djlocated on the decision boundaryHi−, i.e.,(9)∑tk∈Tftk·wdj,tk+b=0,0.35em0exfdj=−1.Rescaling the hyperplane coefficients and b to make the optimization problem easier,Hi+andHi−are:(10)Hi+:∑tk∈Tftk·wdi,tk+b=1,0.35em0exfdi=+1(11)Hi−:∑tk∈Tftk·wdj,tk+b=−1,0.35em0exfdj=−1.Therefore the goal of TSVM is to induce the hyperplane coefficients and b to satisfy(12)∑tk∈Tftk·wdi,tk+b≥10.35em0exif0.35em0exfdi=+1,and(13)∑tk∈Tftk·wdj,tk+b≤−10.35em0exif0.35em0exfdj=−1.The Eqs. (12) and (13) can be summarized by:(14)fdi(∑tk∈Tftk·wdj,tk+b)≥1,0.35em0ex∀di∈D.Therefore, scoring correctly the hyperplane coefficients and b depends on assigning values tofdicorrectly.The documents located on the margins are called support vectors. The Euclidean distance between the hyperplanesHi+andHi−is referred to as margin. Let dist(di, dj) the distance between two points in which diis located onHi+and djis onHi−. Then the margin is computed subtracting Eq. (9) from Eq. (8), i.e. (Tan, Steinbach, & Kumar, 2005)(15)f·(di−dj)=2∥f∥×dist(di,dj)=2∴0.35em0exdist(di,dj)=2∥f∥The optimization carried out by TSVM aims to maximize the margin or minimize(17)SubjecttoSubjectto:APTARANORMALminf,b12∥f∥2Subjectto:1em0exydi(∑tk∈Tftk·wdj,tk+b≥1),0.35em0ex∀di∈DL1em0exfdj(∑tk∈Tftk·wdj,tk+b≥1),0.35em0ex∀dj∈DU1em0exydi,0.35em0exfdj∈{−1,+1},0.35em0ex∀di∈DL,0.35em0ex∀dj∈DUAs in SVM, TSVM also allows the documents to be in the wrong side of the hyperplane to obtain hyperplanes with higher margins. To do so, the slack variables ξ are used in the TSVM optimization. Then, the function to be minimized with slack variables is(18)SubjecttoSubjectto:APTARANORMALminf,b12∥f∥2+C∑di∈DLξdi+C′∑dj∈DUξdjSubject0.35em0exto:1em0exydi(∑tk∈Tftk·wdj,tk+b≥1−ξdi),0.35em0ex∀di∈DL1em0exfdj(∑tk∈Tftk·wdj,tk+b≥1−ξdj),0.35em0ex∀dj∈DU1em0exydi,0.35em0exfdj∈{−1,+1},0.35em0ex∀di∈DL,0.35em0ex∀dj∈DU1em0exξdi≥0,0.35em0ex∀di∈DL1em0exξdj′≥0,0.35em0ex∀dj∈DU,where the parameters C and C′ allow a trade-off between classification error and margin size considering labeled and unlabeled documents respectively.The restriction thatfdicontains only integer values makes the TSVM a non-convex optimization problem, and the global optimum solution is feasible only for few unlabeled documents (Joachims, 1999; Chapelle et al., 2006). Local search or a relaxation in the optimization problem is needed to make TSVM feasible for a large number of unlabeled documents. In Algorithm 4 we present the local search solution for TVSM used for transductive classification of texts proposed by Joachims (1999). In this algorithm, initially a hyperplane is induced considering only labeled examples using SVM. The unlabeled documents are classified according to the induced hyperplane, and two documents which falls on the wrong side of the hyperplane has their labels changed. Then a hyperplane is induced considering labeled documents and the labels assigned to unlabeled documents. The hyperplane induction and the change of labels among misclassified document are repeated until convergence, i.e., until the labels of unlabeled documents do not change too much, or a fixed number of iterations. Changing the labels of misclassified documents strictly improves the classification in the next step. Moreover, the algorithm starts with a small value of C′ and rises it during the iterations. This allows more documents to be on the wrong side of the hyperplane in the first iterations and less documents in the last iterations.Algorithm 4Transductive Support Vector Machines (Binary Classification).TSVM has the assumption that the classes are well-separated, so that the hyperplane with maximal margin falls into a low density region. When this assumption does not hold, the transductive classification obtained by TSVM is not accurate.Despite the fact that to obtain local minima solutions be fast, TSVM is still not scalable for large datasets since it needs to build classification models repeatedly. Moreover, TSVM may present unbalanced solutions, i.e., most of the documents are assigned to a single class. In this case, a function is used to maintain the class distribution of the unlabeled documents the same as in the labeled documents (Chapelle et al., 2006).Network-based representation is a natural and direct way to represent textual data for different tasks. The networks presented in this article are defined byN=〈O,E,W〉, in whichOrepresents the set of objects (also called vertices or nodes),Erepresents the set of connections (also called relations or links) among the objects, andWrepresents the weights of the connections. The objects correspond to entities of a problem. In case of text collections, examples of objects are documents, terms, sentences or authors. WhenOis compounded by a single type of object, the network is called homogeneous network. WhenOis compounded by h different types of objects, i.e,O=O1∪⋯∪OhandOirepresents the objects of the i-th type, the network is called heterogeneous network (Sun & Han, 2012).Different types of objects and different relations can be used to generate a network-based representation. We can extract objects representing the entire collection, as documents and terms, or representing pieces of a text, as terms, sentences or paragraphs. Documents are connected according to (i) “explicit relations” as hyperlinks or citations (Oh, Myaeng, & Lee, 2000; Page, Brin, Motwani, & Winograd, 1998; Mei, Cai, Zhang, & Zhai, 2008; Sun, Han, Gao, & Yu, 2009) or (ii) considering similarity (Angelova & Weikum, 2006; de Sousa et al., 2013). Terms are connected (i) if they precede or succeed each other in a text (Aggarwal & Zhao, 2013; Markov & Last, 2006), (ii) if they co-occur in pieces of texts as sentences/windows (Solé, Corominas-Murtra, Valverde, & Steels, 2010; Palshikar, 2007; Mihalcea & Tarau, 2004) or in the text collection (Wang, Do, & Lin, 2005; Tseng, Ho, Yang, & Chen, 2012; Matsuo, Sakaki, Uchiyama, & Ishizuka, 2006), or (iii) if they present syntactic/semantic relationship (Solé et al., 2010; Steyvers & Tenenbaum, 2005). Sentences or paragraphs are connected considering (i) similarities (Salton, Singhal, Mitra, & Buckley, 1997; Yang & Soo, 2012) or (ii) considering semantic similarity, co-reference resolution and discourse relations (Ferreira et al., 2013).A combination of different types of objects is also used. Dhillon (2001), Rossi et al. (2012), and Rossi, Lopes, Faleiros, et al. (2014) use documents and terms to generate a bipartite network. In these cases, terms are connected to documents in which they occur. Wan, Yang, and Xiao (2007) uses sentences and words as network objects. In this case, terms are connected to sentences in which they occur, sentences are connected to each other considering the number of shared words, and terms are connected to each other considering the number of shared sentences.The main algorithms for transductive classification in data represented as networks are based on regularization (Zhu & Goldberg, 2009), which have to satisfy two assumptions: (i) the class information of neighbors must be close and (ii) the class information assigned during the classification process must be close to the real class information. These two assumptions are satisfied through the minimization of the following regularization framework:(20)Q(F)=12∑oi,oj∈Owoi,ojΩ(foi,foj)+μ∑oi∈OLΩ′(foi,yoi),where the first term corresponds to the first assumption, the second term corresponds to the second assumption, μ is a parameter to control how much the labeled objects must keep their class information in the transductive classification, andΩ(⋯)andΩ′(⋯)are distance functions. The differences among regularization-based algorithms are in theΩ(⋯)andΩ′(⋯)functions and how μ is set.Eq. (20) can be minimized by closed solutions. However, this might be computationally expensive for large networks. Iterative solutions are preferable in this case, since they are less expensive and allow to set a maximum number of iterations, which speed up the transductive classification. The iterative solutions are called label propagation, since they propagate the labels among the network objects through the network connections in a way to minimize Eq. (20). The labels correspond to the class information vectors.Performing transductive classification on networks requires modeling text collections in a way that documents are able to propagate their labels to other documents. In order to do so we can model text collections as document networks or bipartite heterogeneous networks (Rossi, Lopes, Faleiros, et al., 2014). In the next subsections we present regularization algorithms based on document and bipartite networks and the corresponding label propagation solutions to perform transductive classification. We also present how to generate the networks used as input for these algorithms.In a document network,O=D, i.e., the network objects represent the documents of a text collection. Therefore, this is a homogeneous network. Documents connected considering their similarity improve the classification accuracy (Angelova & Weikum, 2006) and do not limit the application domains such as the use of hyperlinks or citations. Thus we focus on similarity-based document networks in this article for comparison with the proposed approach.Similarity based network generally is undirected, i.e., if there is an edge between a document diand a document dj, there is also an edge between djand di. Both edges have the same weight. Usually two approaches are used to generate similarity-based document networks (Zhu & Goldberg, 2009): (i) fully connected-network or (ii) nearest neighbor network. In this paper we consider the most representative type of each approach: (i) Exp network and (ii) Mutual k Nearest Neighbors (MkNN) network. In an Exp network, the weight of the relation between a document diand a document dj(wdi,dj)is given by a Gaussian function aswdi,dj=APTARANORMALexp(−Ω(di,dj)2/σ2), in which Ω(di, dj) is the distance between the documents diand dj, and σ controls the bandwidth of the Gaussian function. In the MkNN network, an object diand an object djare connected if djis one of the k nearest neighbors of diand diis one of the k nearest neighbors of dj.The two main regularization based algorithms to perform transductive classification on homogeneous networks are: (i) Gaussian Fields and Harmonic Functions (GFHF) and (ii) Learning with Local and Global Consistency (LLGC).GFHF (Zhu et al., 2003) algorithm performs the transductive classification minimizing the following function:(21)Q(F(D))=12∑di,dj∈Dwdi,dj∥fdi−fdj∥2+APTARANORMALlimμ→∞μ∑di∈DL∥fdi−ydi∥2There is a restriction thatfdi(DL)≡ydi(DL), so the second term of Eq. (21) has a value tending to infinity. Class Mass Normalization (CMN), presented in Eq. (22), is used to classify the objects (Zhu et al., 2003) considering the final values offdivectors fordi∈DU. The label propagation solution to minimize Eq. (21) is presented in Algorithm 5.(22)class(di)=argAPTARANORMALmax1≤l≤|C|Pr[cl]·fdi,cl∑dj∈D0.12em0exfdj,clAlgorithm 5Gaussian Fields and Harmonic Functions.LLGC (Zhou et al., 2004) decreases the influence of objects with a high degree in the definition of class information of neighboring objects. Besides, the class information of labeled documents can be changed during the classification process. The function to be minimized by LLGC is:(23)Q(F(D))=12∑di,dj∈D0.35em0exwdi,dj∥fdi∑dk∈D0.12em0exwdi,dk−fdj∑dk0.12em0ex∈Dwdj,dk∥2+μ∑di∈DL∥fdi−ydi∥2.The documents are classified considering the arg-max of the final values offdivectors fordi∈DU. The label propagation solution to minimize Eq. (23) is presented in Algorithm 6.Algorithm 6Learning with Local and Global Consistency.In a bipartite heterogeneous network,O=D∪T, i.e., the network objects correspond to documents and terms of a text collection.di∈Dandtj∈Tare linked if tjoccurs in di(Rossi et al., 2012). The weight of the relation between diand tj(wdi,tj)is based on the frequency of tjin di. Thus, just the terms and their frequencies in the documents are necessary to generate a bipartite network. The bipartite networks used in this article are undirected. Therefore,wdi,tjis equal towtj,di.Bipartite networks are generated faster than document networks, since they do not need to compute similarities among all documents of a text collection, and there are no parameters, which drastically change the classification performance in document networks (de Sousa et al., 2013; Rossi, Lopes, & Rezende, 2014). Moreover, bipartite networks do not need explicit links such as citations of hyperlinks, which allows them to model any type of text collection.The general idea of regularization in bipartite networks is to minimize the differences among the class information of documents and their connected terms. In the case of label propagation, documents propagate their labels to terms and the terms propagate their labels to the documents. Therefore, terms are used as “bridges” to propagate the labels among documents. The two main regularization-based algorithms to perform transductive classification on bipartite networks are: (i) Tag-based Model (TM) and (ii) GNetMine (GM).TM (Yin et al.,2009,2009) algorithm minimizes the differences between the (i) real class information of labeled documents(DL)or previous class information of unlabeled(DU)and the class information assigned to both of them, (ii) the real class information and the class information assigned to objects from other domains that aid the classification process(A), like authors and conferences, and (iii) the class information among terms(T)and objects in(D)or(A). The function to be minimized by TM is:(24)Q(F)=α∑ai∈A∥fai−yai∥2+β∑di∈DL∥fdi−ydi∥2+γ∑di∈DU∥fdi−ydi∥2+∑oi∈D∪A∑tj∈Twoi,tj∥foi−ftj∥2.The parametersα,0.35em0exβand γ control the importance given to the assumptions of TM. Documents are classified using class mass normalization (Eq. (22)). The label propagation solution to minimize Eq. (24) is presented in Algorithm 7.Algorithm 7Tag-Based Model.GM (Ji, Sun, Danilevsky, Han, & Gao, 2010) is a general framework for classification in heterogeneous network based on LLGC algorithm. The difference between the algorithms is that GM considers the different types of relations among the different types of objects. For the problem of texts modeled as a bipartite network, GM minimizes the following functions:(25)Q(F)=∑di∈D∑tj∈Twdi,tj∥fdi∑tk∈T0.35em0exwdi,tk−ftj∑dk∈D0.35em0exwtj,dk∥2+∑di∈DLα∥fdi−ydi∥2,where 0 < α < 1. An unlabeled document diis classified according to the arg-max value offdi. The label propagation solution to minimize Eq. (25) is presented in Algorithm 8.Algorithm 8GNetMine.Instead of using terms just as bridges to propagate labels in a bipartite network, the bipartite network structure can be used to induce relevance scores (class information) of terms for classes, i.e., how much a term increases (positive values) or decreases (negative values) the probability of a document belonging to a class. Rossi et al. (2012) and Rossi, Lopes, Faleiros, et al. (2014) present IMBHN (Inductive Model based on Bipartite Heterogeneous Network) algorithm, which induces the relevance scores of terms for classes through the minimization of the following function:(26)Q(F(T))=12(∑cj∈C0.35em0ex∑dk∈DLλ(∑ti∈Twdk,tifti,cj)−ydk,cj)2,where the functionλ(⋯)returns the value 1 for the class cjwith the highest value for∑ti∈T0.12em0exwdk,0.35em0extifti,0.35em0excj. The relevance scores of terms are induced to minimize the squared sum of the differences between the predicted and real classes of the training documents. The Least-Mean-Square method (Widrow & Hoff, 1960) is used to induce the relevance scores of terms. The iterative solution to minimize Eq. (26) is presented in Algorithm 9.Algorithm 9Inductive Model based on Bipartite Heterogeneous Networks.The classification performance obtained by IMBHN is higher than that by Rossi, Lopes, Faleiros, et al. (2014). However, IMBHN just consider labeled documents to induce the relevance scores of terms for classes, which can decrease the classification performance when just few labeled documents are available. As labeling documents is time consuming, an algorithm to induce the relevance scores of terms also considering unlabeled data is interesting in scenarios with few labeled documents and plenty of unlabeled documents.In this article we propose an algorithm which performs transductive classification of texts through the induction of the relevance scores of terms for classes using labeled and unlabeled documents. The proposed algorithm, named TCBHN (Transductive Classification based on Bipartite Heterogeneous Network) uses the bipartite network structure to induce the relevance scores of terms for classes and to define the labels of unlabeled documents. The relevance scores of terms are induced through an optimization process considering the current labels of the documents. The induced relevance scores are propagated through the bipartite network to define new labels to unlabeled documents which consequently refine the relevance scores of terms for classes. This process is repeated until convergence, i.e., until the labels of unlabeled documents remain the same. In the next subsections we present details of the algorithm, time complexity analysis, and an example using a toy and a real text collection to illustrate the TCBHN functioning.The assumption of TCBHN is that the class information of documents inDLand inDUare useful to induce the class information of the terms, and the induced class information for the terms aids the improvement of the class information of documents inDU. Thus, the objective of TCBHN is to minimize the function(27)Q(F)=12(∑cj∈C∑dk∈DLydk,cj−∑ti∈T(wdk,tifti,cj))2+12(∑cj∈C∑dk∈DUfdk,cj−∑ti∈T(wdk,tifti,cj))2,i.e., the objective is to induce a matrixF(T), which contains the relevance scores of terms for classes, in a way that the frequencies of terms weighted by these relevance scores corresponds to the real class information of labeled documents and the class information assigned to unlabeled documents. The class information of labeled documents remain the same during transductive classification, i.e., we changefdionly fordi∈DU. This is equivalent to allow changing the class information of labeled document and adding a termAPTARANORMALlimμ→∞μ∑di∈DL(fdi−ydi)2in Eq. (27) as performed in GFHF (Eq. (21)).The induction of the relevance scores of matrixF(T)is performed using the Lest-Mean-Square (LMS) method (Widrow & Hoff, 1960). LMS makes successive corrections in the relevance scores of terms in the direction of the negative gradient vector, which will lead to the minimum mean squared error. The relevance score updating equation using LMS is presented in Eq. (28). The direction of the gradient can be estimated by the derivative of Q(F), as presented in Eq. (29).(28)f(n+1)=f(n)+η[−∇(Q(F))](29)∇(Q(F))=∂Q(F)∂F=∑cj∈C0.35em0ex∑dk∈DL0.35em0exydk,cj−(∑ti∈Twdk,tifti,cj)∑cj∈C∑dk∈DL∑ti∈Twdk,ti+∑cj∈C∑dk∈DUfdk,cj−(∑ti∈Twdk,tifti,cj)∑cj∈C∑dk∈DU∑ti∈Twdk,tiWe call the differenceydk,cj−(∑ti∈T0.12em0exwdk,tifti,cj)orfdk,cj−(∑ti∈T0.12em0exwdk,tifti,cj)aserrordk,cj. Considering Eqs. (28) and (29), the scorefi,j(s+1)(T)of a term tifor the class cjin time(s+1)is given by the following equation:(30)fti,cj(T)(s+1)=fti,cj(T)(s)+η(∑dk∈DLwdk,tierrordk,cj(n)+∑dk∈DUwdk,tierrordk,cj(n)),where η is the error correction rate, i.e., the rate in which the error will be considered in the relevance score updating.If we directly apply Eq. (30) iteratively, the relevance scores of terms are obtained considering just the labeled documents since there is no class information assigned to unlabeled documents at the beginning of the classification process. Thus, we propose a three-step approach to induce the relevance scores of terms considering labeled and unlabeled documents: (i) inducing the relevance scores of terms considering just labeled documents, (ii) propagating the relevance scores of terms to unlabeled documents, and (iii) refining the relevance scores of terms considering unlabeled documents. Hence, Eq. (30) is divided into two equations: one considering labeled documents (Eq. (31)), which is applied in the first step, and one considering unlabeled documents (Eq. (32)), which is applied in the third step.(31)fti,cj(T)(s+1)=fti,cj(T)(s)+η(∑dk∈DL0.12em0exwdk,tierrordk,cj(n))(32)fti,cj(T)(s+1)=fti,cj(T)(s)+η(∑dk∈DU0.12em0exwdk,tierrordk,cj(n))Eqs. (31) and (32) are applied until a stopping criterion is reached. We adopted as stopping criteria the maximum number of iterations and the minimum mean squared error ( ∈ ), i.e., until the mean squared error considering labeled (for Eq. (31)) or unlabeled (for Eq. (32)) is lower than a user’s threshold. We call the iterations carried out to optimize the relevance scores of terms using Eqs. (31) or (32) as local iterations.The relevance scores of terms are propagated to unlabeled documents using the weighted linear function presented in Eq. (33) (second step).(33)fdi,cj(DU)=∑tk∈Twdi,tkftk,cjSince the labeled documents have the sum of the class information equal to 1, we standardize the class information propagated to unlabeled documents using Eq. (34) to also make the sum equal to 1. In case of negative value in a class information vector of a document, we sum the module of the most negative value in all class information values before applying Eq. (34).(34)fdi,cj(DU)=fdi,cj(DU)∑ck∈C0.12em0exfdi,ck(DU)We apply the 3 steps iteratively until a maximum number of iterations is reached or until the class information of labeled documents remains the same in two successive iterations. An iteration containing the three steps is called global iteration. The pseudocode of TCBHN algorithm is presented in Algorithm 10. Lines 2–12 induces the relevance scores of terms using labeled documents, lines 13–15 assign class information to unlabeled documents, and lines 16–26 induces the relevance scores of terms using the class information assigned to unlabeled documents. The functionClassifyInstance(di,F(T))applies the Eq. (33) to define the class information of documents using the current relevance scores of terms and Eq. (34) to make the sum of values of a class information vector equal to 1.Algorithm 10Transductive Categorization based on Bipartite Heterogeneous Network.The relation weights for terms in a document are normalized to improve the optimization (Valin & Collings, 2007). Thus, the weight of a relation between a term tjand a document diis(35)wdi,tj=wdi,tj∑tk∈T0.12em0exwdi,tk.The cost to induce the relevance scores of terms through optimization isO(nLocal0.25em0ex*0.25em0ex|D|0.25em0ex*0.25em0ex|T|¯0.25em0ex*0.25em0ex|C|), since the relevance scores of terms from each document are updated up to nLocaltimes for each class. The cost to propagate the labels isO(|D|0.25em0ex*0.25em0ex|T|¯0.25em0ex*0.25em0ex|C|), since each document receives the relevance scores from their terms for all classes. Optimization and label propagation are repeated up to nGlobaltimes until convergence. Thus, the total cost for transductive classification using TCBHN isO(nGlobal0.25em0ex*0.25em0ex((nLocal0.25em0ex*0.25em0ex|D|0.25em0ex*0.25em0ex|T|¯0.25em0ex*0.25em0ex|C|)+(|D|0.25em0ex*0.25em0ex|T|¯0.25em0ex*0.25em0ex|C|))≡O(nGlobal0.25em0ex*0.25em0ex(nLocal0.25em0ex*0.25em0ex|D|0.25em0ex*0.25em0ex|T|¯0.25em0ex*0.25em0ex|C|)).To illustrate the functioning of the proposed algorithm and show the impact of the unlabeled documents in the relevance scores of terms for classes, we ran TCBHN in a toy and in a real text collection. For the toy collection we generated a network with 8 documents and 9 terms, as presented in Fig. 1. In this figure, Documents 1, 2, 3, and 4 belong to Class 1 and documents 5, 6, 7, and 8 belong to Class 2. Terms 1, 2 and 3 belong exclusively to the documents of Class 1, terms 7, 8, and 9 belong exclusively to the documents of Class 2, and Terms 4, 5 and 6 belong to the document of both classes. We set term frequencies equal 1 andη=0.5to facilitate the understanding.We started the transductive classification with just one labeled document for each class, Doc 1 for Class 1 and Doc 8 for Class 2, as presented in Fig. 1(a) (first step). Firstly, the relevance scores of terms for the classes were induced considering just the labeled documents, as presented in Fig. 1(b) (second step). These relevance scores were propagated to unlabeled documents to set their class information and normalized, as presented in Fig. 1(c) (third step). The new class information assigned to documents were used to optimize the relevance scores of terms for the classes. The illustrations presented in Fig. 1(a)–(c) are the first global iteration.The new relevance scores of terms at the second global iteration and the class information of the documents after propagating these relevance scores are presented in Fig. 1(d). We notice that terms with no induced relevance score in the first global iteration have their relevance scores induced in the second iteration. Besides, the relevance scores of terms are changed for those terms with previous induced relevance scores.Even if all the documents have class information, the relevance scores of terms for classes may change and consequently the class information of unlabeled documents, as presented in Fig. 1(e). At the end, labels are assigned to unlabeled documents according to the arg-max of their class information vectors. In this illustrative example we correctly classify all the documents. We notice that if we just use labeled documents (Fig. 1(b)), Doc 4 and Doc 5 will not receive any class information and will not be classified.We also present the impact of the unlabeled documents in the relevance scores of terms for classes in a real collection. In order to do so, we used CSTR (Computer Science Technical Reports) collection (Rossi, Marcacini, & Rezende, 2013) which contains documents from 4 areas: Artificial Intelligence (AI)/Natural Language Processing (NLP), Robotics/Vision, Systems, and Theory. We selected 10 documents from each class randomly to be the labeled objects of the network. The maximum number of global iterations was 10, the maximum number of local iterations was 100, and we setη=0.05. We used stemmed single words as terms. The words were stemmed using Porter’s algorithm (Porter, 1980). We also run the algorithm IMBHN, which induces the relevance scores of terms just considering labeled documents, i.e., using just the first term of Eq. (27), to obtain the relevance scores of terms.Table 1 presents the ranking of terms for each class without and with unlabeled documents respectively. We notice that important terms for AI/NLP such as “learn”, “label”, and “language” are not in the top ranked terms when just labeled documents are used. Other important terms as “knowledge” and “class” are in both rankings. However, terms as “class” and “knowledge” presents a lower relevance score for IA/NLP using just labeled documents than using labeled and unlabeled documents. Besides, we can notice that important terms for IA/NLP and not important for other areas of the CSTR collection, such as “learn”, “word”, “label”, “knowledge”, “gener”, and “discours” present negative relevance scores for other classes, i.e., these terms decrease the class information of the documents belonging to these classes when labeled and unlabeled documents are considered. The differences in the relevance scores of terms provided by the use of unlabeled documents also impact the classification performance, as we present in Section 4.In the experimental evaluation we compared TCBHN with algorithms presented in Section 2, which consider text collections represented in a vector space model, document and bipartite networks. We also considered inductive supervised learning algorithms to demonstrate if and how much unlabeled documents are useful to improve classification performance. Moreover, our goal is to demonstrate that (i) our proposal surpass the classification performance obtained by state-of-the-art algorithms for transductive classification of texts and (ii) our proposal is scalable for large text collections. In next sections we present the text collections used in the experimental evaluation, experiment configuration, evaluation criteria, results and discussion. Due to reasons concerning reproducibily, all source codes and text collections used in our experimental evaluation are freely available.11http://sites.labic.icmc.usp.br/ragero/jipm_2015/.We used 30 textual document collections from different domains: e-mails (EM), medical documents (MD), news articles (NA), scientific documents (SD), sentiment analysis (SA), TREC (Text Retrieval Conference) documents (TD), and web pages (WP). Below we will give a brief description of the collections:E-mailsSpamAssassin: collection for testing spam filtering systems. This collection is composed by spams and non-spam (ham) e-mails (Apache, 2013).Trec7-3000: composed by spam and non-spam e-mails (Cormack & Lynam, 2007). We selected 3000 spam and 3000 non-spam e-mails.Oh0, Oh5, Oh10, Oh15, Ohscal: these collections are subsets of the OHSUMED collection (Forman, 2006; Hersh, Buckley, Leone, & Hickam, 1994).Foreign Broadcast Information Service (FBIS): this collection is composed by newspaper articles from around the world (Forman, 2006). FBIS is a subcollection of the TREC collection (TREC, 2013).Hitech: composed by news about computers, electronics, health, medical, research, and technology from the San Jose Mercury newspaper (Karypis, 2013). This collection is part of the TREC collection (TREC (2013)).La1, La2: these collections are composed by Los Angeles Times news articles extracted from TREC-5 (Forman, 2006; TREC, 2013).Reuters Corpus Volume 1 – v2 – Top Four Categories (RCV1 Top-4): newswire stories available by Reuters22http://www.reuters.com/.(Lewis, 2014). We consider just the top 4 categories of the category hierarchy (Economics, Government/Social, Corporate/Industrial, Equity Markets) and documents from a single category.Re0, Re8: Re0 (Forman, 2006) and Re8 (Pang, 2014) are composed by articles from Reuters-21578 collection (Lewis, 2013).Reviews: derived from the San Jose Mercury newspaper.33http://www.mercurynews.com/.The news is about food, movies, music, radio, and restaurants (Karypis, 2013).Classic4: composed by the collections CACM (titles and abstracts from the journal Communications of the ACM), CISI (information retrieval papers), CRANFIELD (aeronautical system papers), and MEDLINE (medical journals) (D.M. Research, 2013).Irish Sentiment: composed by articles labeled by volunteers as positive, negative or irrelevant. The articles were extracted from Irish online sources: RTE News, The Irish Times, and the Irish Independent (Group, 2012).Multi Domaim Sentiment: contains product reviews taken from Amazon44http://www.amazon.com/.from different product types (Blitzer, Dredze, & Pereira, 2013).Review Polarity: composed by 1000 positive and 1000 negative reviews about movies (Pang & Lee, 2013).Dmoz-Health-500: composed by web pages from the subcategories of Health extracted from DMOZ – Open Directory Project (Netscape, 2013). We consider 500 document from each subcategory.Dmoz-Science-500: composed by web pages from the subcategories of Science extracted from DMOZ (Netscape, 2013). We consider 500 documents from each subcategory.Dmoz-Sports-500: composed by web pages from the subcategories of Sports extracted from DMOZ (Netscape, 2013). We consider 500 documents from each subcategory.Industry Sector: composed by web pages of companies from various economic sectors (Nigam, 2012).Syskill & Webert: composed by web pages about bands, sheep, goats, and biomedical (Pazzani, 2013).WebACE Project (WAP): composed by web pages from de WebACE Project (Han et al., 1998; Forman, 2006). The web pages belong to the subject hierarchy of Yahoo.55http://dir.yahoo.com/.World Wide Knowledge Base (WebKB): composed by web pages collected from computer science departments of various universities in January 1997 by the CMU Text Learning Group66http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-4/text-learning/www/index.html.(Group, 2013).Tr11, Tr31, Tr41, Tr45: these collections (Forman, 2006) are derived from Trec-5, Trec-6 and Trec-7 collections (TREC, 2013). The documents are labeled with queries which they were considered relevant.The collections have different characteristics. The number of documents (|D|) ranges from 334 to 685,071, the number of terms (|T|) from 2001 to 153,458, the average number of terms per document (|T¯|) from 11.52 to 281.66, the number of classes (|C|) from 2 to 20, the standard deviation considering the class percentages in each collection (σ(C)) from 0 to 34.45, and the percentage of the majority class (max(C)) from 5.31 to 74.36. Table 2presents the characteristics of the collections.For the collections La1s, La2s, Oh0, Oh10, Oh15, Oh5, Re0, Tr11, Tr31, Tr41, Tr45, and WAP (Forman, 2006) no preprocessing was performed since these collections were already preprocessed. For the other collections, single words were considered as terms, stopwords were removed, terms were stemmed using the Porter’s algorithm (Porter, 1980), HTML tags were removed, and only terms with document frequency ≥ 2 were considered. We used term frequency to weight terms in documents. More details about the collections are presented in Rossi et al. (2013).We compared TCBHN with traditional and state-of-the-art transductive algorithms based on vector space model, and based on document and bipartite networks. We also ran inductive supervised learning algorithms to verify if and how much unlabeled documents improve classification performance and the cost to move from inductive supervised learning to transductive learning. All algorithms used for comparison were presented in Section 2. The parameter values used in the experimental evaluation were based on the values found in the proposal of the algorithms or empirically.The transductive learning algorithms based on vector space model, considerations, and parameters are:•Self-Training (MNB-Se): we considered Multinomial Nave Bayes (MNB) as inductive learning algorithm for Self-Training since it presents the best trade-off between classification performance and time (Rossi et al., 2012; Rossi, Lopes, Faleiros, et al., 2014). We usedX={5,10,15,20}.Co-Training (MNB-Co): we considered MNB as inductive supervised learning algorithm for Co-Training. We usedX={5,10,15,20}. We randomly split the feature set into two disjunct sets to allow running Co-Training (Xu et al., 2013; Li et al., 2009; Bickel & Scheffer, 2004). Other ways to generate views or other inductive supervised learning algorithms would make impracticable the execution of Co-Training. The classification performance of Co-Training is an average of the classification performances obtained by 10 different random splits in feature set, since different splits can generate different classification performances.Expectation Maximization (EM): we considered the EM instantiation for text classification presented in Nigam et al. (2000). We usedλ={0.1,0.3,0.5,0.7,0.9}and 1, 2, 5, 10 components for each class.Transductive Support Vector Machines (TSVM): we considered the iterative solution for text classification proposed by Joachims (1999). We usedC=1.0to induce a maximal margin hyperplane considering labeled documents since this is one or the best parameters for text classification (Rossi et al., 2012). We vary C′ by a factor of ten from10−5to 101. We run TSVM with and without the function proposed in Joachims (1999) to maintain the same class proportion of labeled documents in the classification of unlabeled documents.For the transductive algorithms based on networks we used the iterative solutions presented on Section 2 (algorithms used for comparison) and Section 3 (our proposal). We set 1000 as the maximum number of iterations. For algorithms based on document networks we generated Mutual k-Nearest Neighbor (MkNN) and Exp networks. To build MkNN networks we usedk={7,17,37,57}, and to build Exp networks we usedσ={0.05,0.2,0.35,0.5}.The algorithms based on document networks, considerations, and parameters are:•Gaussian Fields and Harmonic Functions (GFHF): There are no parameters for GFHF.Learning with Local and Global Consistency (LLGC): We usedα={0.1,0.3,0.5,0.7,0.9}.The algorithms based on bipartite networks, considerations, and parameters are:•Tag-based Model)(TM): we usedα=0, since there are no objects from different domains,β={0.1,1,10,100,1000}, andγ={0.1,1,10,100,1000}.GNetMine: we usedα={0.1,0.3,0.5,0.7,0.9}.Transductive Categorization based on Bipartite Heterogeneous Networks (TCBHN): we usedη={0.01,0.05,0.1,0.5},0.35em0ex∈=0.01, 10 as maximum number of global iterations and 100 as maximum number of local iterations, which gives a total of 1000 iterations.We also run inductive supervised learning algorithms to analyse the trade-off between computational cost and classification performance. This also allows us to analyse if the use of unlabeled documents actually improve classification performance. The algorithms, parameters, and considerations of the inductive supervised learning algorithms are:•Multinomial Nave Bayes (MNB): we considered MNB since it is the learning algorithm used in Self-Training, Co-Training and Expectation Maximization. This allowed us to measure the difference in classification performance and time to move from inductive supervised learning to transductive learning for algorithms based on vector space model. There are no parameters for MNB.Inductive Model based on Bipartite Heterogeneous Network (IMBHNR): we induced the relevance scores of terms using just labeled documents, i.e., the first term of Eq. (27). This corresponds to IMBHN’s equation (Rossi et al., 2012; Rossi, Lopes, Faleiros, et al., 2014) without the class function. We call this algorithm by IMBHNR, since it performs regression (R) on the information class of labeled documents to induce the relevance scores of terms. The use of IMBHNR allowed us to measure the differences in classification performance and time when considering unlabeled data to define the relevance scores of terms. We usedη={0.01,0.05,0.1,0.5},0.35em0ex∈=0.01, and 1000 as maximum number of iterations. These were the same parameters of TCBHN.Only for RCV1 Top-4 collection we did not run LLGC and GFGF algorithms, since computing, storing, and building document networks for such amount of documents is impracticable. We also did not run Self-Training and Co-Training since the parameters used for these algorithms would make to reinduce classification models about 34,000 times for each configuration, which is also impracticable. TSVM also demonstrated to be impracticable for this collection.We used the F1 measure to compare the classification results. F1 is the harmonic mean of precision and recall measures, in which both measures have the same weight, i.e.(36)F1=20.25em0ex*0.25em0exPrecision0.25em0ex*0.25em0exRecallPrecision+Recall.Precision and recall were computed for each class in multiclass evaluation. The precision and recall of a class ciare:(37)Precisionci=TPciTPci+FPci,(38)Recallci=TPciTPci+FNci,where TP (True Positive) means the number of test documents correctly assigned to classci,0.35em0exFP(False Positive) means the number of test documents from class cj(cj0.25em0ex≠0.25em0exci) but assigned to class ci, and FN (False Negative) is the number of test documents from class cibut assigned to class cj(cj0.25em0ex≠0.25em0exci). Precision returns the percentage of documents correctly classified as ciconsidering all documents classified as ci, and recall returns the percentage of documents correctly classified as ciconsidering all documents which actually belong to class ci.Two strategies to summarize the results of precision and recall computed for each class of a text collection are: (i) micro-averaging and macro-averaging (Sokolova & Lapalme, 2009; Manning et al., 2008; Sebastiani, 2002). The micro-averaging strategy performs a sum of the terms of the evaluation measures. Therefore, the precision and recall using the micro-averaging strategy are:(39)PrecisionMicro=∑ci∈C0.12em0exTPci∑ci∈C0.12em0ex(TPci+FPci),(40)RecallMicro=∑ci∈C0.12em0exTPci∑ci∈C0.12em0ex(TPci+FNci).The macro-averaging strategy performs and average over the evaluations measures for each class. Therefore, the precision and recall using macro-averaging strategy are:(41)PrecisionMacro=∑ci∈C0.12em0exPrecisionci|C|,(42)RecallMacro=∑ci∈C0.12em0exRecallci|C|.Micro-averaging scores are dominated by the number of TP. Therefore, large classes dominate small classes in micro-averaging scores. On the other hand, macro-averaging gives equal weight to each class. In this case, the number of TP in small classes are emphasized in macro-averaging scores. These two strategies give different scores and are complementary to each other. We denote F1 computed through micro-averaging of precision and recall by Micro-F1, and through macro-averaging by Macro-F1.Micro-F1 and Macro-F1 scores were obtained considering the average from 10 runs. In each run we randomly selected N documents from each class as labeled documents. We carried out experiments usingN={1,10,20,30,40,50}. We started with the minimum number of labeled document per class and varied by a factor of ten from 10 to 50. This variation in the number of labeled documents allowed us to better demonstrate the behavior of the algorithms for different number of labeled documents, a trade-off between the number of labeled documents and classification performance, and the differences among inductive supervised learning algorithms and transductive learning algorithms as we increase the number of labeled documents. The remaining|D|−(N0.25em0ex*0.25em0ex|C|)documents were used to evaluate the classification.

@&#CONCLUSIONS@&#
In this articlewe presented an algorithm which uses the structure of a bipartite heterogeneous network to perform transductive classification of texts. The proposed algorithm, named TCBHN (Transductive Classification based on Bipartite Heterogeneous Networks) obtains the relevance scores of terms for classes through an optimization process considering the labels of labeled documents and the labels assigned to unlabeled documents. The induced relevance scores are propagated and employed to assign new labels to the unlabeled documents. Optimization and label propagation are repeated until there is no change in the labels assigned for unlabeled documents.We demonstrated that optimization plus label propagation lead to better classification performance than just label propagation in document or bipartite networks. The proposed algorithm also obtained a better classification performance than algorithms based on vector space model and proved to make better use on unlabeled documents to improve classification than other algorithms. TCBHN also presented better results with statistically significant differences than other algorithms used for comparisons when using more than 10 labeled documents per class. Moreover, TCBHN presents a lower classification time than other algorithms, which makes it useful for classification in large text collections in which all documents are known.As future work we intend to: (i) incorporate other types of relations as document-document or term-term with document-term relations and analyse the impact in the classification performance and (ii) use the relevance scores of terms induced by TCBHN to classify unseen/new documents.