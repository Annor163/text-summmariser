@&#MAIN-TITLE@&#
A two-phased and Ensemble scheme integrated Backpropagation algorithm

@&#HIGHLIGHTS@&#
A novel two-phased and Ensemble scheme integrated Backpropagation (TP-ES-BP) algorithm.Greatly alleviate the problem of local minima of SBP algorithm.Overcome the limitations of individual components in classification performance by the integration of Ensemble.Empirical and t-test results on ORL face image database and UCI database show its effectiveness.

@&#KEYPHRASES@&#
Standard backpropagation (SBP),Local minima,Ensemble,Two-phased and Ensemble scheme integrated Backpropagation (TP-ES-BP),

@&#ABSTRACT@&#
This paper presents a novel two-phased and Ensemble scheme integrated Backpropagation (TP-ES-BP) algorithm, which could greatly alleviate the problem of local minima of standard BP (SBP) algorithm, and overcome the limitations of individual component BPs in classification performance by the integration of Ensemble method. Empirical and t-test results of three groups of simulation experiments, including the face recognition task on ORL face image database and four benchmark classification tasks on datasets drawn from the UCI repository of machine learning databases, show that TP-ES-BP algorithm achieves significantly better recognition results and higher generalization performance compared to SBP and the state-of-the-art emotional backpropagation (EmBP) learning algorithm.

@&#INTRODUCTION@&#
Artificial neural networks (ANNs) provide a general, practical method for learning real-valued, discrete-valued, and vector-valued functions from training examples [1]. The relationship between training set and test set consists in that, a NN is given and trained based on a training dataset, and is expected to generalize well for the test dataset. There might exist an optimal solution, however, it is usually extremely difficult to achieve. Algorithms such as Back-propagation (BP) utilize gradient descent technique to adjust network parameters to best fit a training set of input–output pairs [1]. It has proven surprisingly successful in many practical problems such as learning to recognize handwritten characters [2], learning to recognize spoken words [3], learning to realize document categorization [4], and learning to recognize faces [5].Although BP algorithm [6] is a powerful supervised learning algorithm for training multi-layer feed-forward neural networks (FNNs) [7], this gradient descent-based algorithm often suffers from problems of local minima and slow convergence. Aiming at ameliorating the convergence properties of BP [8], many proposals have been made by researchers in the last twenty years. From simple approaches which utilize random disturbances [9] to more principled evolutionary algorithms, including simulated annealing [10] and genetic algorithms [11], a variety of proposals have been presented in the literature, most without a striking success [8].An interesting research area which arouses our attention is about the diversified initialization methods of the modified BP algorithms. Among which, Husken and Goerick proposed to use evolutionary algorithms to make choices of a proper initialization weights set for a neural network from a set of optimal weights solutions acquired from similar applications transcendently [12]. They focused on finding initial weights not only for learning one particular problem, but for a class of related problems. Their method was applied to the problem class of learning the solution of a differential equation with variable boundary conditions. The resultant initial weights were substantiated as being better shaped, compared with other initialization methods, to learning particular problems of the problem class as well as tracking changes of the problem [12].Thimm and Fiesler studied the influence of different distributions of initialization on the performance of BP training [13]. They aimed at determining the optimal range for the initial weights and biases, which is the principal parameter of random initialization methods for multilayer perceptrons and for high-order perceptrons. Their conclusions on the methods of initializing the two types of networks are validated by sufficiently small confidence intervals of the mean convergence times [13].Nguyen and Widrow proposed to initialize each hidden processing element (PE) within an approximated portion of the range of the desired response [14]. In their paper they described how a two-layer neural network can approximate any nonlinear function by forming a union of piece-wise linear segments. They gave a method for choosing initial weights for the network to reduce training time. Training examples were presented and the learning curve for these examples illustrated the decrease in necessary training time [14].Drago and Ridella proposed a statistically controlled activation weights initialization method, which attempted to prevent PEs from saturation by figuring out the maximum initial values of weights [15]. Their main discovery from computer simulation was the existence of an amplitude window in the maximum magnitude of the initial weights. Small values of the previous quantity result in local minima, whereas large ones cause neurons to a saturated state, which prevents the network from attaining the desired solution with high possibility [15].Deniz Erdogmus et al. proposed a principled novel method, i.e. back-propagating the desired response through the layers of a multi-layer perceptron (MLP), which accurately initialize neural networks in the minimum mean squared error sense, utilizing the analytic linear least square solutions [8]. However, simulation results demonstrate that, in most cases, the performance achieved through the proposed initialization method leaves little room for further improvement which could be achieved by training [8]. Besides, the proposed initialization method might induce overfitting problem to the neural networks trained with BP algorithm afterwards.In one of our previously published paper [16], we proposed a modified BP algorithm which could remarkably alleviate the problem of local minima confronted with the standard BP (SBP). As an output of the modified training procedure, a bucket of all the possible solutions of weights matrices found during training is acquired, among which the best solution is chosen competitively based upon their performances on a validation dataset. Simulations conducted on four benchmark classification tasks verified the effectiveness of the proposed modified BP.And in another previous paper of ours [17], we proposed the Back-propagation with diversive curiosity (DCPROP) algorithm, for solving the “flat spot” problem and for escaping from local minima. An internal indicator is designed to represent the diversive curiosity in BP algorithm, which detects the phenomenon of being trapped in local minima and the occurrence of premature convergence. Upon such detection, the neural network is activated again to explore optimal solution in search space and escape from local minima by means of stochastic disturbance. The proposed DCPROP algorithm was implemented and applied to two well-known face recognition problems, and the results were compared with SBP.While in this work, a novel two-phased and neural network ensemble (NNE) scheme [18] integrated Backpropagation (TP-ES-BP) algorithm is proposed, which also can be categorized into the modified BP algorithm utilizing improved initialization methods. Its novel improved initialization method is substantially about the automatic and adaptive initialization of its integrated NNE.The main idea is presented as the following: (1) Phase one, the training phase to explore proper potential solutions: The regular training process of SBP is implemented along with some reformative operations. During training, every time when the sum squared error E(i) of the ith training iteration is smaller than an error threshold ɛ1, which is set to be a sufficiently small positive value in advance, it is regarded that a local minimum is found, thus the network weights matrices trained till that time are stored up. After doing these, random perturbation terms generated with white Gaussian noise are added to the network weights matrices to force it cast off the control of the local minimum befallen and resume its learning once again. So when the training concludes, those potential solutions of network weights matrices and the number of local minima are obtained, with each potential solution corresponding to one local minimum. Therefore, a bucket of potential solutions matrices is received at the end of phase one. (2) Phase two, member networks generation and predictions combination phase of the integrated neural network ensemble (NNE): A NNE system is created, with the size of which equals to the number of local minima and with the component networks being initialized based on those potential solutions of network weights matrices. Afterwards, the regular training process of one NNE is carried out, followed by the final testing process, for which, actually, any state-of-the-art NNE generation and combination techniques can be employed [18–23].The main contribution, benefit and originality of the proposed two-phased and Ensemble scheme integrated BP (TP-ES-BP) algorithm is its essential alleviation and amelioration to the local minima problem of SBP. The rationales of this amelioration are as follows: First, since all of the local minima points detected during the training in phase one are saved up and treated as potential solutions, and since every time when one local minimum is barged up against, the network is managed to cast off its control and restart to learn once again, a markedly broader weights solution space is explored by the proposed algorithm in comparison with SBP.Second, due to the same reason, the network is less prone to falling into a local minimum from the early beginning of training until the end. Thereby the circs that the learning of network gets into stagnation since a very early time, i.e. premature convergences, will be avoided mostly.Third, the motive of integrating the popular NNE scheme into the proposed algorithm is to improve classification performance of the system further. NNE is popular for its success in relatively high prediction performance [18]. It is generally ratified that by combining multiple different network components, as what is done in an NNE, the limitations of individual networks in classification performance could be broken through [18].Fourth, the NNE scheme is integrated into phase two of the proposed algorithm, where the size of NNE and the initialized condition of each component network in the NNE are decided automatically based upon the number of local minima and those potential solutions gained from the first phase of the algorithm. While, in fact, both the determination of the proper size and the appropriate initializations of component networks of one classical NNE are rather difficult and key issues for its successful constitution.Fifth, the probability of divergence is relatively small for the proposed algorithm, as the training process of phase one will always conclude when the maximum iteration cycle is reached. And when it concludes, it will at least find one potential solution, which is the network weights matrix learned and updated until the final training cycle. This circumstance takes place when no local minimum point is met during the learning process in phase one, and the network trained from the beginning until the end is kept as the only potential solution. For this case, the NNE constituted in phase two has only one member network, being one of its extreme cases. It has been observed from experiments that, this single member NNE possesses rather good classification and generalization capability. In fact, the final performance of this single network member is very close to the global optimal solution of the specific problem. Therefore, it could be concluded that the local minima problem of SBP is partially resolve or at least alleviated by the proposed algorithm. Besides, the classification performance of the whole system is further enhanced by integration of the NNE scheme.In short, the originality of the work consists in presenting a novel neural network ensemble (NNE) creation method, and alleviating the local minima problem of SBP with the integration of this NNE scheme. There certainly exist many other NNE algorithms, while the major difference between the TP-ES-BP algorithm and them lies in the unique initialization method of its member networks.Although the TP-ES-BP algorithm is very simple, its design philosophy is rather reasonable. Just as what was considered by Perrone and Cooper [24], although the fact that, some commonly used neural network models are prone to be trapped in local minima during their learning procedure, is generally regarded as one of the main flaws of neural networks, this very characteristic plays an important role in the improvement to the generalization capability of NNE. The reason is because, if the individual neural networks are unrelated with each other, they might possibly be trapped in different local minima, thus causing big variance within the NNE, thereby reducing its generalization error. In other words, the negative effects of each local minimum would cancel each other out. This might be the most fundamental principle behind the design of TP-ES-BP algorithm. Because in TP-ES-BP, those member networks of ensemble system generated in phase two are directly initialized as those local minima confronted with and saved as potential solutions in phase one.The rest of this paper is organized as follows. Section 2 describes the TP-ES-BP algorithm in detail. Section 3 presents results of the experimental study. From these experimental results, we draw conclusions in Section 4.Supervised learning of multi-layered neural networks with conventional learning algorithms is faced with the local minima problem. Gradient descent-type learning algorithms, including SBP, adjust the network weights based on a training set, dispensing with any prior knowledge, which is a distinct merit of this kind of learning algorithms. However, the gradient descent-type learning algorithms adjust network weights following a local slope of the error surface, which may result in some undesirable points, i.e. the local minima [25]. Convergence of an algorithm to a local minimum forecloses a network learning of the entire training set and results in an inferior performance network or probably premature convergence.Intuitively, the existence of local minima is due to the fact that the error function is the superposition of nonlinear activation functions that may have minima at different points, which sometimes results in a non-convex error function [26]. The inadequate number of hidden nodes as well as improper initial weight settings can cause convergence to a local minimum. It can be said that gradient descent BP algorithms are local minimization methods and have no mechanism that could make themselves to break away from the control of a local minimum [27].There exist some state-of-the-art modified algorithms in the literature which try to avoid the local minima problem of gradient descent BP algorithms. In [28], Rubio et al. proposed a theorem to assure the uniform stability of the general discrete time systems. They proved that the BP algorithm with a new time varying rate is uniformly stable for online identification; the identification error converges to a small zone bounded by an uncertainty. They also proved that the weights error is bounded by the initial weights error, i.e. the overfitting is not presented in the proposed algorithm. They applied their proposed BP algorithm to predict the distribution of loads that a transelevator receives from a trailer and place in the deposits each hour in a warehouse, and confirmed through experiments that the deposits in the warehouse can be reserved in advance using the prediction results.In [29], Rubio et al. proposed a new method which uses an intelligent algorithm for online pattern recognition to provide a better diagnosis for patients related with the abnormal eye movements. The difference between their proposed method and the other previous works mainly lies in that, in other works, both up and down behaviors are trained with an intelligent algorithm, whereas in their proposed method, up behavior is trained with an intelligent algorithm and down behavior is trained with another intelligent algorithm. Their rationale is simply because any multi-output system can always be decomposed into a set of single-output systems, possessing the advantage of using different parameters for each single-output system while it is necessary.In [30], J.H. Perez-Cruz et al. address the issue of trajectory tracking for a wide category of uncertain nonlinear systems with multiple inputs, each one subject to an unknown symmetric dead zone. They directly employ a continuous-time recurrent neural network based upon a model of the dead zone to identify the uncertain dynamics. They demonstrate with Lyapunov analysis that the identification error exponentially converge to a bounded zone. Moreover, they designed a proper control law in such a way that the singularity problem is conveniently avoided, so that the state of the neural network is compelled to follow a bounded reference trajectory. Their proposed strategy has one prominent advantage that the controller can work satisfactorily without any prior knowledge of an upper bound for the unmodeled dynamics or the disturbance term.In [31], J.J. Rubio proposed two novel modified optimal controllers based on neural networks to solve the trajectory tracking problem of robotic arms. By means of a Lyapunov-like analysis, they guaranteed the uniform stability of both the tracking error and approximation error for the aforementioned controllers. And they verified the effectiveness of their proposed controllers by simulation experiments.In [32], an online self-organizing fuzzy modified least-square network is proposed by J.J. Rubio. The network possesses the capability to reorganize and adapt itself to a dynamically changing environment where both the architecture and learning parameters are adapted simultaneously. In their work, they proposed a new network, which uses unidimensional membership functions, and employs only two parameters for each rule, thus reducing the number of parameters. Besides, they proposed a new pruning algorithm based on the density. The density refers to the number of times each rule is employed in the algorithm. The rule with the smallest density in a specific number of iterations is pruned if its density is smaller than a preset threshold.In [33], J.J. Rubio et al. proposed an evolving intelligent system to solve the modeling problem of nonlinear systems with dead-zone input. The uniform stability of the modeling error obtained by their system is guaranteed by means of a Lyapunov-like analysis. And they verified the effectiveness of their proposed system by simulation experiments.Aiming at alleviating the problem of local minima of SBP algorithm, and overcome the limitations of individual component networks in classification performance by the integration of ensemble method, the two-phased and Ensemble scheme integrated Backpropagation (TP-ES-BP) algorithm is proposed. As can be seen from Fig. 1, the main idea of the proposed two-phased and Ensemble scheme integrated Backpropagation (TP-ES-BP) algorithm is as follows: (1) Phase one, the modified BP training phase to detect proper potential solutions: During training, each time when the sum squared error E(i) of the ith training iteration is smaller than an error threshold ɛ1, which is set to be a sufficiently small positive value in advance, it is thought that one local minimum is met, therefore the network weights matrices trained till then are stored up. After doing these, random perturbation terms generated with white Gaussian noise are added to the network weights matrices to make it escape the control of the local minimum encountered and restart its learning again. Consequently, when the modified training procedure is finished, those potential solutions of network weights matrices are gained, with each potential solution of the network weights matrices corresponding to one local minimum. Therefore, a bucket of several potential solutions is achieved from the exploration in phase one. (2) Phase two, member networks generation and classification decisions combination phase for the integrated NNE: A neural network ensemble (NNE) system [18] is created, with the size of which equals to the number of local minima and with the components of which being initialized based on those potential solutions, which are just the two results from phase one. Then, the regular training process of one NNE is carried out, followed by the final testing process, for which, actually, any state-of-the-art NNE generation and combination techniques can be utilized [18–23].A material improvement of the local minima problem of SBP is profit from the proposed TP-ES-BP algorithm. It may be derived from the following rationales. First, as all the local minima points are saved up as potential solutions, and moreover, as every time when one local minimum is met, the network is managed to get away and restart learning once again, an evidently broader solution space is explored by TP-ES-BP compared with SBP.Second, for the same reason as the first one, the network is less likely to be trapped into one local minimum from an early time of training until the end. So the circumstances that the network ceases to learn since a very early time, i.e. search stagnations or premature convergences, will be avoided mostly.Third, since the desirable NNE scheme is integrated into the proposed algorithm, it is expected that a greater enhancement to the classification performance could be achieved, as one particular merit of NNE is its accomplishment in the melioration to classification performance. It is generally accepted that, in most cases, combined multiple network components in an ensemble could surpass the limitations of individual networks in classification performance.Fourth, the NNE scheme is integrated into phase two of TP-ES-BP, where its size and the initialized state of its every network member are settled independently on the basis of those calculation results from phase one. While, in practice, both the determination of the proper size of one NNE and the appropriate initializations of its components are quite difficult and crucial tasks for its successful constitution.Fifth, in the normal BP algorithm, the divergence being known as overfitting is presented [28,32–35]; thus, by selecting the appropriate learning rate using the trial and error technique, it helps to avoid the divergence of the TP-ES-BP algorithm. As a whole, the likelihood of divergence is relatively small for TP-ES-BP, because the training procedure in phase one will always terminate when the maximum iteration cycle is arrived at. And when it terminates, it will at least find one potential solution, which is the network trained from the beginning until the final cycle of training. This happens when no local minimum is met with during the learning process in phase one, and the network weights matrix trained from the beginning until the end is saved up as the only potential solution. In this case, the NNE constructed in the latter phase possesses only one network member, being one of its extreme cases. The experimental results show that, this one member NNE has very good classification and generalization capability. Actually, the final performance of this single network member is very close to the global optimal solution of the specific problem.Finally, the most fundamental principle behind the design of TP-ES-BP algorithm consists in: although the phenomena that, some commonly used neural network models are prone to be trapped in local minima in their learning process, is generally regarded as one of the main drawbacks of neural networks, this property plays an important role in the improvement to the generalization capability of NNE [24]. The reason is because, if the individual network models are irrelevant with each other, they would possibly be trapped into different local minima, thus causing big variance within the NNE, thereby reducing its generalization error. In other words, the negative effects of each local minimum would cancel each other out [24]. In TP-ES-BP algorithm, accordingly, those local minima confronted with and saved up as potential solutions in phase one are utilized to initialize those network components of ensemble system generated in phase two directly.Therefore, it can be concluded that the local minima problem of BP is partly solved or at least exceedingly alleviated by the proposed method. And through the integration of NNE method, a further improvement to the classification performance of the whole system is realized.Moreover, it is necessary to mention that, there exist mainly two differences between our proposed TP-ES-BP algorithm and those algorithms proposed in references [28–33]: (1) TP-ES-BP algorithm is based on the two phase scheme and in the relation between the sum of squared error and an error threshold, while those algorithms proposed in [28–33] are based on a Lyapunov stability analysis; (2) TP-ES-BP algorithm is thought in the pattern recognition issue while those algorithms proposed in [28–33] are thought in the identification issue.Consider a three-layered feed-forward neural network (FNN), with the basic structure of which shown in Fig. 2. The network possesses N input nodes, H hidden nodes, and M output nodes, and adopts sigmoid function as its activation function [36]. Its associated detailed training procedure with the proposed two-phased and Ensemble scheme integrated BP (TP-ES-BP) algorithm is shown as below:Phase one (the training phase to find proper potential solutions):Input:A learning rate η, a momentum factor α, error thresholds ɛ1 and ɛ2, and a maximum iteration number I. The number of input nodes N, the number of hidden nodes H and the number of output nodes M.Output:The two buckets of Potential Solution Weights Matrices PSWM1∈RN,H,NLMand PSWM2∈RH,M,NLM, and the Number of Local Minima NLM obtained through the algorithm.{Comments:PSWM1∈RN,H,NLMdenotes the bucket of Potential Solution Weights Matrices between input and hidden layer of corresponding component network. PSWM2∈RH,M,NLMdenotes the bucket of Potential Solution Weights Matrices between hidden and output layer of corresponding component network.}01:Begin02:Initialize network weights vectorsw˜andw. Set the iteration index i=0. Set the local minima counter NLM=0. Initialize the two buckets of Potential Solution Weights Matrices PSWM1∈RN,H,MNLMand PSWM2∈RH,M,MNLM. {Comments: MNLM denotes the Maximum value of the Number of Local Minima NLM.}03:Fori=1 toIDo04:For each training samplexp=(xp1, …, xpN), p=1, …, PDo05:Input a training samplexp=(xp1, …, xpN) to the network, and compute the output vectoro˜pandoplayer by layer using the following equations:(1)o˜ph(i)=f∑n=1Nw˜nh(i)xpn(2)opm(i)=f∑h=1Hwhm(i)o˜ph(i)where f(·) is adopted as the sigmoid function:(3)f(x)=11+e−x06:End For07:Calculate the sum squared error E(i) for the ith training iteration as follows:(4)E(i)=12∑p=1P∑m=1M[tpm−opm(i)]2wheretp=(tp1, …, tpM) is the target vector corresponding to the pth training samplexp.08:IfE(i)<ɛ1Then(5)NLM=NLM+1(6)PSWM1(:,:,NLM)=w˜(7)PSWM2(:,:,NLM)=w(8)RD_dW1=wgn(H,N,PW)(9)RD_dW2=wgn(M,H,PW)(10)w˜=w˜+RD_dW1(11)w=w+RD_dW2where RD_dW1 and RD_dW2 refer to the random disturbance terms forw˜andw, respectively. Function wgn(M, N, PW) generates an M-by-N matrix of white Gaussian noise, and PW specifies the power of the output noise in dBW.{Comments:NLM denotes the Number of Local Minima, or termed as local minima counter. Accordingly, PSWM1(:,:,NLM) denotes the Potential Solution Weights Matrices between input and hidden layer of the NLMth component network, and PSWM2(:,:,NLM) denotes the Potential Solution Weights Matrices between hidden and output layer of the NLMth component network.}09:Else calculate the network weights changes for the next iterationΔw˜nh(i+1)and Δwhm(i+1) as follows:(12)Δwhm(i+1)=−η∂E(i)∂whm(i)+αΔwhm(i)=η∑p=1Pδpm(i)o˜ph(i)+αΔwhm(i)(13)Δw˜nh(i+1)=−η∂E(i)∂w˜nh(i)+αΔw˜nh(i)=η∑p=1Pδ˜ph(i)xpn+αΔw˜nh(i)where(14)δpm(i)=(tpm−opm(i))opm(i)(1−opm(i))(15)δ˜ph(i)=o˜ph(i)(1−o˜ph(i))∑m=1Mδpm(i)whm(i)10:Update the network weights vectors for the (i+1)th iteration as follows:(16)w(i+1)=w(i)+Δw(i+1)and(17)w˜(i+1)=w˜(i)+Δw˜(i+1)11:i=i+1;12:End If;13:End For;14:IfE(I)>ɛ2Then return Failed;15:Else(18)NLM=NLM+1(19)PSWM1(:,:,NLM)=w˜(20)PSWM2(:,:,NLM)=w16:End If;17:EndPhase two (the component networks generation and decisions combination phase of the integrated NNE):01:Begin02:Size of the integrated NNE=NLM;03For Index-Of-Local-Minima=1 toNLM04Initialize the weights matrix of each component network in the NNE based on the corresponding values of PSWM1(:,:,NLM) and PSWM2(:,:,NLM).05Perform the training task of each component network employing any state-of-the-art neural network training algorithm.06End For07Perform the testing task of the generated NNE based on the test dataset, utilizing any state-of-the-art ensemble combination method.08EndThere are many more sophisticated BP training methods, such as Levenberg–Marquardt algorithm and Conjugate Gradient algorithm. However, this work employs BP with momentum algorithm. The reason is because, our focus of this work is to present a novel two-phased and Ensemble scheme integrated BP (TP-ES-BP) algorithm, and to investigate its effectiveness and validity, while not to study the absolute superiority of those BP algorithms. The investigation results here of this paper could be expanded to other state-of-the-art BP algorithms easily and directly.In recent years, human face recognition has drawn considerable attention from researchers, and face recognition technology has found a variety of applications in areas such as human–computer interfaces, model-based video coding, information security, law enforcement and surveillance, smart cards, access control, etc. [37–40].To compare the classification performances of TP-ES-BP algorithm with SBP and with the emotional backpropagation (EmBP) learning algorithm presented by A. Khashman [41], so as to verify its effectiveness, experiments are carried out for investigating the pattern recognition performance of TP-ES-BP for the facial recognition task based on the ORL face database [42] as shown in Fig. 3. It possesses 40 persons and 10 different images for each person, 92×112 pixels with slightly varying lighting conditions, pose, scale, face expression and presence or absence of glasses [43].In our experiments, all the images have been scaled down by factor 2 to speed up learning, since it is known from D. Bryliuk and V. Starovoitov's research work that such scaling do not greatly influence recognition rate on the ORL database [42,43]. The number of input units of feed-forward neural network (FNN) is set to be the number of image pixels, i.e. 46×56. Gray level of each pixel has been linearly scaled from range [0; 255] to [0; 0.1] for avoiding paralysis or surfeit of the network. The number of output units is set to be equal to the number of classes, i.e. 40, the number of persons in the ORL database. Each output unit corresponds to one unique class among the 40 classes.Three groups of experiments have been carried out in this work: for the first group, five images among the total ten images of each person are used for training, and the remaining five are used for testing; for the second group, four images of each person are used for training, and the remaining six for testing; and finally, for the third group, three images of each person are used for training, and the remaining seven for testing. Obviously, the less the ratio of training patterns number to testing ones is, the more challenging the task will be. Fig. 4illustrates the three different training/testing set partition strategies for one of the 40 persons in ORL database.As the proposed algorithm here is a two-phased and Ensemble scheme integrated BP (TP-ES-BP) algorithm, therefore the experiments conducted for this research naturally consist of two phases: for the first one, those proper potential solutions are explored by training; for the second one, the network components of the integrated NNE are generated and their classification decisions are combined.Specifically, for the classifications combination of the integrated NNE, two popular methods are employed. One method is to select the best single model (BSM) according to the performance of the models on the validation set, and accordingly, the whole algorithm is denoted as “TP-ES-BP (BSM)”. While the other one adopted is the simple majority voting (MV) method, and the whole algorithm is accordingly denoted as “TP-ES-BP (MV)”. The first group of comparative experiments is conducted between the proposed TP-ES-BP (MV), TP-ES-BP (BSM) and SBP. All the testing results given in this group of comparative experiments are the average performance after 100 runs.The second group of comparative experiments is carried out between the proposed TP-ES-BP (MV), TP-ES-BP (BSM) and the state-of-the-art emotional backpropagation (EmBP) learning algorithm, which was presented by A. Khashman in one of their published works [41]. Their new EmBP algorithm has additional emotional weights that are updated using two additional emotional parameters: anxiety and confidence. Their EmBP neural network was implemented to the facial recognition task on the ORL face database [42]. And experimental results have shown that the addition of the two novel emotional parameters improves the performance of the neural network yielding higher recognition rates and faster recognition speed [41]. All the testing results given in this second group of comparative experiments are the average performance after 50 runs.

@&#CONCLUSIONS@&#
This paper presents a novel two-phased and Ensemble scheme integrated BP (TP-ES-BP) algorithm, which could observably mitigate the local minima problem of SBP algorithm, and overcome the limitations of individual component networks in classification performance through the integration of Ensemble method. The proposed TP-ES-BP algorithm has been compared with SBP and EmBP, and its effectiveness and validity have been empirically verified by the experiments that are conducted on the ORL face image database, as well as on the four benchmark classification datasets (i.e. the Car Evaluation dataset, the Diabetes dataset, the Image Segmentation dataset, and the Thyroid dataset), which are all drawn from the UCI repository of machine learning databases.The novel algorithm represents the following characteristics: (1) An evidently wider weights solution space is explored by TP-ES-BP in comparison with SBP. (2) The circumstance that the learning of network gets into stagnation since a very early time of training, viz. premature convergence, could be abstained basically. (3) By integrating the ensemble scheme into TP-ES-BP, the limitations of individual networks in prediction performance could be overcome. (4) The difficult issues about the determination of NNE size and initialization of its component networks are settled automatically and adaptively in TP-ES-BP. (5) The occurrence of divergence could be avoided mostly by TP-ES-BP.