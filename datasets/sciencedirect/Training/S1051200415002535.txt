@&#MAIN-TITLE@&#
Adaptive recursive algorithm with logarithmic transformation for nonlinear system identification in α-stable noise

@&#HIGHLIGHTS@&#
An enhanced recursive least mean pth power algorithm with logarithmic transformation is proposed.The new cost function with the p-norm logarithmic transformation of the error signal is presented.The convexity of the proposed cost function is demonstrated.The convergence performance of the proposed RLogLMP algorithm is analyzed.

@&#KEYPHRASES@&#
α-stable noise,Nonlinear system identification,Logarithmic transformation,Volterra filter,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
System identification, which is to establish a mathematical model for an unknown system through the input–output relationship, plays a significant role in many practical problems. Over the past decades, linear system identification technique has been extensively used because of its simple implementation. In many actual systems, however, the unknown system exhibits nonlinear characteristic. Linear system identification technique performs poorly, and in some cases even fails. As a result, nonlinear system identification technique must be considered in the identification problem of these cases [1].On the other hand, adaptive nonlinear filtering techniques, are effective methods to deal with the modeling problems under the non-stationary and non-Gaussian signals environment [2]. In recent years, various kinds of nonlinear filtering techniques were developed to solve nonlinear modeling problems, such as multilayer perceptron (MLP) [3], recurrent neural networks (RNNs) [4,5], wavelet neural network [6], and Volterra filters [7,8]. Among these nonlinear filtering techniques, the Volterra filter was one of the most popular models due to its causal and time-invariant nonlinear operation [7,8]. It could deal with a general class of nonlinear systems, while its output was still linear with respect to various higher order kernels or impulse responses. The main drawback of using Volterra filter is that a large number of coefficients are required to accurately model a nonlinear system. It coefficients increase geometrically as the delays and orders increasing, which makes the infinite series form of Volterra filter computationally intensive and complex. Hence, the infinite series form of Volterra filter is prohibited for many practical applications. To solve the computational complexity problem, some truncated Volterra filters were proposed, such as second-order Volterra (SOV) [9] and third-order Volterra (TOV) [10]. The truncated Volterra filters have been widely applied to estimate and identify many nonlinear dynamic systems [11–14], equalization and compensation of channel systems [15], image processing [16] and speech processing [17,18].In system identification problem, the ideal Gaussian model was reasonable in most cases [19]. Kalluri et al. proposed a class of nonlinear normalized least mean square algorithm (NNLMS) for nonlinear system identification. The NNLMS algorithm was a stochastic gradient algorithm based on mean square error (MSE) criterion [12]. Compared with the least mean square (LMS), this algorithm converged faster in general. However, for the past decades, researches indicated that many practical signals are impulsive in nature. Many methods were presented to describe the probability density function (PDF) of impulsive noise, such as Gaussian mixture model (GMM) [20], contaminated Gaussian (CG) model [21–24], etc. By using robust mixed-norm [21], robust M-estimate method [20,22–24] and so on, it solves the performance deterioration of adaptive filter in these impulsive model environments. Alternatively, the α-stable distribution also provides a valid model for this type of signals [25,26]. The density functions of the α-stable distribution decays in the tails less rapidly than the Gaussian density function, and the noises in this distribution are more likely to exhibit sharp spikes. Therefore, the distribution well agrees with the actual signals which are heavy-tails. Because of the characteristic of the above-mentioned noise, the MSE criterion is unsuitable to estimate error due to the lack of finite second-order statistics of the noise. To address this issue, Shao et al. [27] introduced a stochastic gradient descent algorithm, called the least mean pth power (LMP), which was based on the minimum of the p-norm power of the error signal. As the NLMS algorithm was derived from the LMS algorithm, a normalized least mean pth power (NLMP) algorithm and normalized least mean absolute deviation (NLMAD) were proposed by Arikan et al. [28]. Later, a new method based on the iteratively reweighted least-squares (IRLS) algorithm [29] was presented by Kuruoglu et al. [30], namely nonlinear iteratively reweighted least squares (NIRLS) algorithm. The NIRLS algorithm was valid for impulsive noise, but it is not adaptive [31]. Following this work, Kuruoglu developed a class of polynomial algorithms based on Volterra prediction filters to address this problem, such as polynomial LMP (PLMP) algorithm and polynomial least mean absolute deviation (PLMAD) algorithm [31]. In [32], Weng and Barner replace the data matrix of LMP with the Volterra expansion to identify the nonlinear system under the impulse interference. Compared with the LMS algorithm, the LMP algorithm exhibits better performance under high impulse interference. However, all of the algorithms have high misadjustment in the presence of α-stable noise. Moreover, when the input signal was highly correlated, these algorithms exhibited slow convergence rate.In 2006, a concept of zero-order statistics (ZOS) was introduced, a statistical framework that is sound and consistent for all processes with finite logarithmic moments [33]. This “logarithmic-order” class includes α-stable noise process, which gives an increased number of outliers. More recently, Wu et al. developed a filtered-x least-mean square with logarithmic transformation (FxLogLMS) algorithm which is more robust than existing algorithms for active noise control (ANC) in the presence of α-stable noise [34]. In this paper, motivated by this framework and the FxLogLMS algorithm, a novel enhanced recursive least mean pth power algorithm with logarithmic transformation (RLogLMP) is proposed to improve the filtering performance of adaptive Volterra filters. The proposed RLogLMP algorithm of adaptive Volterra filters is derived by recursively minimizing the pth norm power of the error signal with logarithmic transformation. The logarithmic transformation, which availably compresses the magnitude of the impulsive noise, can suppress the impact on identification performance under the impulsive noise environment. Therefore, the proposed algorithm offers a more stable and robust solution than those of LMP and NLMP algorithms in the context of nonlinear system identification under α-stable noise environment. In particular, contributions of the paper can be outlined as: (1) development of RLogLMP algorithm with logarithmic transformation for nonlinear system identification in the presence of α-stable noise, (2) analyze some characteristics of logarithmic transformation, (3) discuss the optimality of the proposed filter inH∞norm, (4) analyze the computational complexity of the existing Voletrra algorithms.This paper is organized in the following manner. Section 2 presents a brief description to α-stable noise and nonlinear system identification problem. The existing p-norm algorithms for nonlinear system identification are also described. In Section 3, the derivation and analysis of the proposed algorithm are provided in detail. In Section 4, computer simulation results are provided to show the efficiency of the proposed algorithm. Finally, some conclusions are given in Section 5.The impulsive noises are often due to the occurrence of noise disturbance with low probability but large amplitude in some circumstances, such as underwater acoustics and communications. This noise, which can be generated by some natural and man-made electromagnetic sources, exhibits the characteristics of impulse noise. Usually, the impulsive noise processes can be modeled as α-stable noise process. Its PDF has no closed expression, but its distribution can be described by the characteristic function as follows:(1)φSαS(t)=exp⁡{−γ|t|α}Such a distribution is called the symmetric α-stable (SαS) distribution. The shape parameter α(0<α<2)is a characteristic exponent. Whenα<2, the SαS distribution has no variance. The PDFs for various values of α are plotted in Fig. 1. It can be clearly found that a small value of α indicates a peaky and heavy tailed distribution and likely more impulsive noise. Consider two different tail index characterizing,α=1.25corresponds to the more impulsive context,α=1.9is a slightly impulsive case, andα=2corresponding to the Gaussian distribution.It is well-known that the Volterra filter can provide an important general representation of unknown nonlinear systems. In Fig. 2, the structure of nonlinear identification system based on the adaptive Volterra filter is depicted. The input signal of adaptive Volterra filter is denoted asx(n),z(n)is the nonlinear system output,d(n)denotes the desired signal,ε(n)is the impulsive noise,y(n)is the output of the Volterra filter ande(n)is the error signal (posteriori error) betweeny(n)andd(n). The output signaly(n)of adaptive Volterra filter is expressed by(2)y(n)=∑m1=0N−1h1(m1)x(n−m1)+∑m1=0N−1∑m2=m1N−1h2(m1,m2)x(n−m2)x(n−m1)+⋯+∑m1=0N−1⋯∑mr=mr−1N−1hr(m1,…,mr)x(n−m1),…,x(n−mr)where N denotes the memory length of the adaptive Volterra filter, andhr(m1,…,mr),r=1,…,Rrepresents the rth order kernel.For the sake of simplificity, (2) can be rewritten in a vector form as follows:(3)y(n)=wT(n)u(n)where the input vectoru(n)and the weight vectorw(n)are defined by, respectively,(4)u(n)=[x(n),x(n−1),…,x(n−N+1),x2(n),x(n)x(n−1),…,xr(n−N+1)]T(5)w(n)=[h1(0),h1(1),…,h1(N−1),h2(0,0),h2(0,1),…,hr(N−1,…,N−1)]TIn particular, whenr=2, the Volterra series expansion corresponding to SOV, which is described by(6)y(n)=∑m1=0N−1h1(m1)x(n−m1)+∑m1=0N−1∑m2=m1N−1h2(m1,m2)x(n−m2)x(n−m1)=wT(n)u(n)and(7)u(n)=[x(n),…,x(n−N+1),x2(n),…,x2(n−N+1)]T(8)w(n)=[h1,h2]T=[h1(0),h1(1),…,h1(N−1),h2(0,0),…,h2(N−1,…,N−1)]TThe symbolsh1=[h1(0),h1(1),…,h1(N−1)]Tandh2=[h2(0,0),…,h2(N−1,…,N−1)]Tdenote the linear and quadratic kernels of w, respectively.To enhance the robustness against the α-stable noise for nonlinear system identification, some algorithms of adaptive Volterra filters were developed [27,28] based on minimizing a specified cost function by using the p-norm power as follows:(9)J(n)=|e(n)|p1) LMP algorithm [27]For some0<p<α, to minimize the pth moment, the stochastic gradient method to update tap weightw(n)is described as follows [27].(10)w(n+1)=w(n)+μ|e(n)|p−1sign{e(n)}u(n)where μ is the step-size, and(11)sign(t)={1ift>00ift=0−1ift<02) NLMP algorithm [28]To further improve the filtering performance, a normalized LMP (NLMP) algorithm was proposed as follows in [28].(12)w(n+1)=w(n)+μ|e(n)|p−1sign{e(n)}‖u(n)‖pp+τu(n)Note that τ is a positive number used to avoid division by zero,‖⋅‖pis the p-norm of a vector. Forp=2, the solution to (9) can be adjusted at every iteration by using 2-norm algorithms, such as LMS and NLMS algorithms. Forp=1, it reduces to the NLMAD algorithm. The adaptation rule of NLMAD algorithm can be expressed as follows(13)w(n+1)=w(n)+μsign{e(n)}‖u(n)‖1+τu(n)where‖⋅‖1is the 1-norm of a vector. The LMP and NLMP algorithms have been applied to identification of nonlinear systems in α-stable noise environment [27,28]. However, it is found that the LMP and NLMP algorithms can become unstable in the presence of high impulse interference [31].In order to overcome these limitations and improve the robustness for the α-stable noise, a novel RLogLMP algorithm is proposed. The recursive scheme is used to accelerate the convergence property in the presence of α-stable noise.To derive the proposed algorithm with adaptive Volterra filters, the novel cost function using the p-norm of the error with logarithmic transformation is firstly defined by(14)Jp(n)=∑i=1nλn−i⋅logp⁡(1+|e(i)|)=∑i=1nλn−i⋅logp⁡(1+|d(i)−wT(n)u(i)|)where the forgetting factorλ∈(0,1]controls the convergence performance of the algorithm,w(0)=0. The newly defined cost function can avoid the situation that the value of the cost function goes to ∞ when|e(n)|tends to zero.Similar to the LMP and NLMP algorithms of adaptive Volterra filters, the objective of the RLogLMP algorithm is to minimize the cost functionJp(n). Then, taking the gradient ofJp(n)with respect tow(n), we get(15)∇Jp[w(n)]=p∑i=1nλn−ilogp−1⁡(1+|e(i)|)1+|e(i)|sign{e(i)}u(i)Setting (15) to zero, gives(16)∑i=1nλn−ilogp−1⁡(1+|e(i)|)1+|e(i)|sign{e(i)}u(i)=0According tosign{e(n)}=e(n)/|e(n)|, (16) is rewritten as(17)∑i=1nλn−ilogp−1⁡(1+|e(i)|)1+|e(i)|e(i)|e(i)|u(i)=0Substitutinge(i)=d(i)−wT(n)u(i)into (17), we have(18)∑i=1nλn−iν(i)u(i)uT(i)w(n)=∑i=1nλn−iν(i)u(i)d(i)whereν(n)=logp−1⁡(1+|e(n)|)(1+|e(n)|)⋅|e(n)|is different from the standard solution for 2-norm. Note that the terms of posteriori errore(n)cannot be obtained in practical. For this reason, the algorithm forces an iterative estimation to the solution [35]. By usingw(n−1)to calculateν(n), thenw(n)is gained by the value ofν(n).Furthermore, the expression ofw(n)is obtained from (18) as follows:(19)w(n)=P(n)r(n)where(20)P(n)=R−1(n),R(n)=∑i=1nλn−iν(i)u(i)uT(i)and(21)r(n)=∑i=1nλn−iν(i)u(i)d(i)Ifν(n)≈1, the RLogLMP algorithm reduces to recursive least square (RLS) algorithm. Whenν(n)≠1,P(n)andr(n)are themselves functions of the optimal weights viaν(n). It has to recalculate (20) and (21) in each iteration. To further derive the truly online algorithm,R(n)must be updated by recursive expression. According to the approach proposed in [36], the following formulations are obtained:(22)R(n)=∑i=1nλn−iν(i)u(i)uT(i)=λR(n−1)+ν(n)u(n)uT(n)and(23)r(n)=∑i=1nλn−iν(i)u(i)d(i)=λr(n−1)+ν(n)u(n)d(n)To avoid the inverse matrix operation,P(n)can be updated by using Woodbury's matrix inversion lemma [37](24)P(n)=λ−1P(n−1)−λ−2ν(n)P(n−1)u(n)uT(n)P(n−1)1+λ−1ν(n)uT(n)P(n−1)u(n)For simplicity, (24) can be rewritten as(25)P(n)=λ−1P(n−1)−λ−1k(n)uT(n)P(n−1)Assume thatP(0)=(1/σ)I=δIwhich is related to a soft initialization [37], where I represents an identity matrix, σ and δ are small positive constants, respectively. Thus, the gain vectork(n)is defined by(26)k(n)=ν(n)P(n−1)u(n)λ+ν(n)uT(n)P(n−1)u(n)Rearrange (26) as(27)k(n)=λ−1ν(n)P(n−1)u(n)−λ−1k(n)ν(n)uT(n)P(n−1)u(n)=[λ−1P(n−1)−λ−1k(n)uT(n)P(n−1)]ν(n)u(n)According to (25), (27) can be simplified to(28)k(n)=P(n)ν(n)u(n)Then, inserting (23) into (19), we get(29)w(n)=P(n)(λr(n−1)+ν(n)u(n)d(n))=λP(n)r(n−1)+P(n)ν(n)u(n)d(n)The inverse autocorrelation matrixP(n)of the first term in (29) can be replaced by (25)(30)w(n)=P(n−1)r(n−1)−k(n)uT(n)P(n−1)r(n−1)+P(n)ν(n)u(n)d(n)=w(n−1)−k(n)uT(n)w(n−1)+P(n)ν(n)u(n)d(n)Introducing (28) to (30), we obtain the tap-weight update equation of the proposed RLogLMP algorithm as:(31)w(n)=w(n−1)+k(n)[d(n)−uT(n)w(n−1)].Note thatd(n)−uT(n)w(n−1)is the prior error of the filter. As a consequence, the RLogLMP algorithm is summarized as Table 1.The cost function in (14) is convex. To prove it, the following definitions are provided. The symbolsw1andw2denote the weight vectors which appear to be the same dimension as w.Assuming thatw1,w2∈RM, we define the following equation(32)w=ξw1+(1−ξ)w2where ξ is a positive constant in[0,1], and M is a memory length of the filter.Considering the following inequality(33)E{logp⁡(1+|e(n)|)}<E{|e(n)|p}and(34)E{|d(n)−wT(n)u(n)|p}=E{|d(n)−[ξw1+(1−ξ)w2]Tu(n)|p}=E{|ξ[d(n)−w1Tu(n)]+(1−ξ)[d(n)−w2Tu(n)]|p}≤E{ξp|d(n)−w1Tu(n)|p}+E{(1−ξ)p|d(n)−w2Tu(n)|p}≤ξE(|d(n)−w1Tu(n)|p)+(1−ξ)E(|d(n)−w2Tu(n)|p)we have(35)E{logp⁡(1+|e(n)|)}<ξE(|d(n)−w1Tu(n)|p)+(1−ξ)E(|d(n)−w2Tu(n)|p)which holds forp≥1, i.e.,Jp(ξw1+(1−ξ)w2)<ξJp(w1)+(1−ξ)Jp(w2)forp≥1. The proof is completed. Consequently, it can be concluded that the cost function (14) is convex. The convexity of the cost function for the proposed algorithm is worth investigating. If the proposed cost function of filter coefficients is convexity, every minimum of the performance function is a global minimum, so it has no local minima [38]. In other word, it is easy to ensure stability during adaption in the convex error surfaces. Moreover, the results of convex function can be used to verify that every minimum of performance functionJp(n)is a global minimum.Gonzalez et al. introduced the concept of ZOS and referred that the geometric power of α-stable process is a logarithmic-order [33]. Keeping the view of the logarithmic-order, the proposed RLogLMP algorithm utilizes the logarithmic transformation to availably compress the magnitude of the α-stable noise. Fig. 3depicts that the α-stable noise is a non-Gaussian process, but its logarithmic transformation has lower magnitude than α-stable noise. The logarithmic transformation can be regarded as an effect approach to reduce the impact of the outliers in the presence of α-stable noise. Therefore, the influence of high impulsive noise for RLogLMP algorithm is less than those of the LMP and NLMP algorithms.To verify the stability of the RLogLMP algorithm relied on logarithmic transformation, the convergence analysis of the proposed algorithm is provided in the following. First, the two assumptions are given as1) The desired response is produced by(36)d(n)=woTu(n)+eα(n)wherewoTis a vector containing the optimal coefficient values, andeα(n)is a α-stable noise; The noise signaleα(n)and the input sequenceu(n)are mutually independent.2) The input sequenceu(n)is generated at random, the autocorrelation function is ergodic, which can be expressed as(37)D≈1nR(n)ifn>MwhereR(n)is a time average correlation matrix of the input sequenceu(n).Assumption 2 implies that the time average can substitute for the ensemble average. Adding a regularization factorδλnItoR(n)(38)R(n)=∑i=1nλn−iν(i)u(i)uT(i)+δλnIwhenλ=1, we have(39)R(n)=∑i=1nν(i)u(i)uT(i)+R(0)Next, introducing (36) into (21), the vectorr(n)can be written as(40)r(n)=∑i=1nν(i)u(i)uT(i)wo(i)+∑i=1nν(i)u(i)eα(i)Combining (39) and (40), we get(41)r(n)=R(n)wo−R(0)wo+∑i=1nν(i)u(i)eα(i)Thus, (19) is rewritten by(42)w(n)=R−1(n)R(n)wo−R−1(n)R(0)wo+R−1(n)∑i=1nν(i)u(i)uT(i)eα(i)Taking expectations of both sides of (42) and using assumptions 1 and 2, we arrive at(43)E{w(n)}≈wo−1nD−1R(0)wo=wo−δnD−1wo=wo−δnK(ifn>M)whereK=D−1wois the average of cross-correlation vector between the desired signald(n)and input sequenceu(n). Consequently, according to the aforementioned analyses, the RLogLMP algorithm is convergent and stable. Due to usingP(0)=(1/σ)I=δIto initialize the algorithm, the estimated value ofw(n)is biased forn>M. However, whenn≫M(i.e.n→∞), the estimator is unbiased and the estimation error tends to zero.To further verify the stability of the RLogLMP algorithm, the mean weight behavior analysis is conducted in this section. The weight deviation vector is defined as follows:(44)Ω(n)=wo−w(n)Combining (26) and (31), the tap-weight update formulation of the proposed algorithm can be rewritten as(45)w(n)=w(n−1)+ν(n)P(n−1)u(n)λ+ν(n)uT(n)P(n−1)u(n)×[d(n)−uT(n)w(n−1)]Then, the update formulation of the weight deviation vector of the proposed algorithm can be expressed by using (44)(46)Ω(n)=Ω(n−1)−ν(n)P(n−1)u(n)λ+ν(n)uT(n)P(n−1)u(n)×[d(n)−uT(n)w(n−1)]Taking expectation of both sides of (46), the mean convergence behavior of the coefficient vector can now be expressed by(47)E[Ω(n)]=E[Ω(n−1)]−E[ν(n)P(n−1)u(n)λ+ν(n)uT(n)P(n−1)u(n)×[d(n)−uT(n)w(n−1)]]The prior errorζ(n)=d(n)−uT(n)w(n−1)can be approximately calculated by(48)ζ(n)≈uT(n)Ω(n−1)Introducing (48) to (47), and supposing the independence between the prior errord(n)−uT(n)w(n−1)andν(n)P(n−1)u(n)λ+ν(n)uT(n)P(n−1)u(n), then, (47) can be given as(49)E[Ω(n)]=E[Ω(n−1)]−E[ν(n)P(n−1)u(n)uT(n)λ+ν(n)uT(n)P(n−1)u(n)Ω(n−1)]≈E[Ω(n−1)]−E[ν(n)P(n−1)u(n)uT(n)λ+ν(n)uT(n)P(n−1)u(n)]E[Ω(n−1)]whereE[ν(n)P(n−1)u(n)uT(n)λ+ν(n)uT(n)P(n−1)u(n)]≈0. Therefore, the weight vector in RLogLMP converges if and only if(50)0<λmax{E[ν(n)P(n−1)u(n)uT(n)λ+ν(n)uT(n)P(n−1)u(n)]}<2whereλmax{⋅}represents the largest eigenvalue of a matrix. According to the fact thatλmax(AB)<Tr(AB)in (50), we obtain(51)λmax{E[ν(n)P(n−1)u(n)uT(n)λ+ν(n)uT(n)P(n−1)u(n)]}<E[Tr(u(n)Λ(n)uT(n))λ+ν(n)uT(n)P(n−1)u(n)]<1where,Λ(n)=ν(n)P(n−1). Therefore, the mean error weight vector of the proposed RLogLMP algorithm is convergent if the input signal is persistently exciting [39, Chapter 14].In this section, we obtain upper and lower bounds for theH∞norm of proposed RLogLMP algorithm. In other word, the proposed algorithm isH∞optimal in α-stable noise. TheH∞norm is defined as a transfer operator, which is related to the worst case behavior of a system. In this situation, the resulting minimax algorithms will be more robust and less sensitive to parameter variations, to model uncertainties, and to the lack of statistical information on the exogenous signals. Rely on theH∞norm for LMS and RLS algorithm [37,40–42], theH∞norm for RLogLMP algorithm is derived. Considered the tap-weight update equation for RLogLMP (31), the forgetting factorλ=1, and supposing the time average correlation matrixR(n)is an invertible matrix. Then, we obtain (the method to derive (52) is presented in Appendix A)(52)ΩT(n)R(n)Ω(n)+ν2(n)|ζ(n)|2χ(n)=ΩT(n−1)R(n−1)Ω(n−1)+ν(n)|ε(n)|2+[ν2(n)−ν(n)]|ζ(n)|2whereχ(n)is defined as(53)χ(n)=1+uT(n)P(n−1)u(n)Taking summation in both side form range of1≤n≤T, where T denotes the iteration of the filtering.(54)ΩT(T)R(T)Ω(T)+∑n=1Tν2(n)|ζ(n)|2χ(n)=ΩT(0)R(0)Ω(0)+∑n=1Tν(n)|ε(n)|2+∑n=1T[ν2(n)−ν(n)]|ζ(n)|2By using the initial condition, (54) can be rewritten as(55)ΩT(T)R(T)Ω(T)+∑n=1Tν2(n)|ζ(n)|2χ(n)=δ‖Ω(0)‖22+∑n=1Tν(n)|ε(n)|2+∑n=1T[ν2(n)−ν(n)]|ζ(n)|2The notation‖⋅‖2is used for Euclidean norm of a vector. For1<p<2,ν(n)is limited in the range of(0,1), rewriting (55) in the form of an inequality, we obtain(56)ΩT(T)R(T)Ω(T)+1χ¯∑n=1T|ζ(n)|2≤ΩT(T)R(T)Ω(T)+∑n=1Tν2(n)|ζ(n)|2χ(n)=δ‖Ω(0)‖22+∑n=1Tν(n)|ε(n)|2+∑n=1T[ν2(n)−ν(n)]|ζ(n)|2whereχ¯=max⁡{χ(n)ν2(n)}. The third term in the above inequality is less than zero becauseν2(n)−ν(n)<0. Other terms can be rewritten as(57)ΩT(T)R(T)Ω(T)+1χ¯∑n=1T|ζ(n)|2≤δ‖Ω(0)‖22+∑n=1Tν(n)|ε(n)|2+∑n=1T[ν2(n)−ν(n)]|ζ(n)|2<δ‖Ω(0)‖22+∑n=1T|ε(n)|2In fact, the equation (57) is the estimating equation for RLogLMP algorithm, which has the similar expression as RLS [37,42]. The description is useful, since the upper bound forH∞norm of proposed algorithm can be easily developed from it.After some straightforward simplification [37,42], we can derive the upper bound forH∞norm of RLogLMP algorithm:(58)supw,ε∈H2⁡(∑n=1T|ζu(n)|2δ‖Ω(0)‖22+∑n=1T|ε(n)|2)<(1+χ¯)2whereζu(n)is expressed as(59)ζu(n)=ζ(n)−ε(n)Similarly, by establishing an appropriate interference signal and computing the its energy gain, a lower bound ofH∞norm of proposed algorithm can be calculated as [37,42](60)(χ¯−1)2<supw,ε∈H2⁡(∑n=1T|ζu(n)|2δ‖Ω(0)‖22+∑n=1T|ε(n)|2)Thus, theH∞bound for proposed algorithm in α-stable noise can be expressed as(61)(χ¯−1)2<supw,ε∈H2⁡(∑n=1T|ζu(n)|2δ‖Ω(0)‖22+∑n=1T|ε(n)|2)<(χ¯+1)2It can be found from (61) that the bound of RLogLMP is relatively tight (especially for large values ofχ¯). In effect, the upper and lower bounds differ only by two, owing to(χ¯+1)−(χ¯−1)=2. And, the energy gain of RLogLMP algorithm is relate to the input signal, that is, the robustness of proposed algorithm is depend on its own input data. In general, we can conclude that the propose RLogLMP algorithm is anH∞optimal filter.The LMP algorithm and NLMP algorithm are used to address the problem of nonlinear system identification under impulsive noise, but these algorithms require to choosing an appropriate value for p. When the prior knowledge for impulsive is known, we set the value of p close to α in general. The performance of these algorithms is the best in terms of transient and steady-state behavior. Conversely, an improper setting for p can lead to degraded performance. In addition, when the forgetting factor λ is very close to one, the algorithm achieves good stability and low misadjustment, but its convergence rate is reduced. Therefore, the proper selection of the forgetting factor λ and p also ensures fast convergence and low misadjustment of the RLogLMP algorithm. Here, the best value of p and λ is considered in two examples for SαS process: Gaussian sequence input with two cases (α=1.25,γ=1/15;α=1.9,γ=1/10) and uniform input with two cases (α=1.25,γ=1/15;α=1.9,γ=1/20), whereα=1.25relates to the highly impulsive noise, andα=1.9corresponds to a slightly impulsive case. Furthermore, to evaluate the overall performance, the mean square deviation (MSD)10log10⁡‖w(n)−w0‖22(dB) is used as a measure of performance. Learning curves forp=1,1.2,1.4,1.8and2forα=1.25are shown in Figs. 4 and 7, learning curves forp=1,1.2,1.6,1.89and2forα=1.9are shown in Figs. 5 and 8, respectively. It is clearly seen from simulation results that the values of p is not very sensitive to the change of α. Moreover, for an overall consideration of steady-state performance and convergence rate, the best choice isp=1.2in the two examples, and the proper selection of the forgetting factor isλ=0.995forp=1.2. We have tested the different input signal and α for p (second-order autoregressive (AR(2)) input withα=1.25and 1.6, Gaussian input withα=1.6, uniform input withα=1.6), which obtain similar results. Therefore, the value of p and λ for RLogLMP algorithm can be settled asλ=0.995,p=1.2. To further illustrate the improved performance of RLogLMP, we display a comparison of different p of LMP ((10)–(11)) and RLogLMP in Figs. 6 and 9. As can be seem, the performance of LMP algorithm is highly dependent on the nature of the input signal and p, it has large fluctuations in some cases. The proposed algorithm, in contrast, has good stability during the adaptation.The computational complexity of the RLogLMP is compared with that of LMP, NLMP, NLMAD and NIRLS algorithms in terms of the total number of additions, multiplications, p-norm operations, p-power operations, logarithmic computation and computation time. Here, the computation complexity of the p-norm operations and the p power operations are defined as the number of the power-series, and the logarithmic computation denotes the times of the logarithmic computation operations. Table 2summarizes the computational complexity of existing nonlinear algorithm with Volterra filter. As can be seen from the Table 2, it is easily observed that the NLMAD algorithm needs fewer computations than those of the other algorithms. It needs 2M multiplications,2M+1additions and has no p-norm operations. The LMP algorithm needs2M+1products, 2M additions and has no p-norm operations. Compared to the LMP algorithm, the NLMP algorithm slightly increases the computational complexity. The3M−1additions and(Mp)padditional operations of p-norm is required per iteration. Both of the LMP and NLMP algorithms needp−1more operations to compute the p power. Compared to other algorithms, the RLS Volterra filter increases the computational burden with improving performance. It needs2M2+4Mmultiplications and2M2+2Madditions. Since RLogLMP is a recursive-type filter with a logarithmic transformation, it needs2M2+4M+2multiplications,2M2+2M+2additions,p−1power operation, and a logarithmic computation. The RLogLMP has more complexity at each iteration, but it can be compromised by it robustness against α-stable noise. To better quantify the computational burden, we have measured the average run execution time of the algorithm on a 2.1-GHz AMD processor with 2 GB of RAM, running Matlab R2013a on Windows 7. Obviously, the fastest method is the one based on 1-norm, NLMAD algorithm. On the contrary, the NIRLS algorithm is badly subjected to the intensive computation owing to block update method and inverse operation, and is not practical for real-time applications. The proposed RLogLMP algorithm requires moderate computational cost and provides good performance, but the best final performance is obtained withλ=0.995,p=1.2, still with an affordable computation timeTo evaluate the performance of the RLogLMP algorithm, three examples of nonlinear system identification are carried out in this simulation.In all these experiments, the results presented here are obtained from average over 200 independent trials. The signal-to-noise ratio (SNR) is defined as [32](62)SNR=σx2γwhereσx2is the power of the input signal. Here, the normalized kernel error (NKE) [32] is adopted to quantify the estimation performance for linear kernel and quadratic kernel, respectively:(63)NKE1=‖h1−Ψ1‖22‖Ψ1‖22(64)NKE2=‖h2−Ψ2‖F2‖Ψ2‖F2where‖⋅‖Fis the matrix Frobenius norm for misadjustment of quadratic kernel,Ψ1andΨ2denote the linear and quadratic kernels of the system, respectively. To demonstrate the NKE learning curve clearly, a logarithmic scale is employed to y-axis by using the semilogy function from Matlab.Example 1The unknown plant which is given in [32] is modeled as a second-order nonlinear system as follows:(65)z(n)=x(n)−0.8x(n−1)+1.9x(n−3)+0.95x2(n)+1.1x(n)x(n−2)−0.63x(n−2)x(n−3)The Gaussian signalN(0,1)with zero mean and varianceσx2=1is employed as the exciting input. The characteristic exponent of the α-stable noise is set to beα=1.25. The SNR is 15 dB, γ equals to 1/15. Figs. 10(a) and (b) depict the trajectories of some of the filter weights in different environments, respectively. The best choices of four algorithms are summarized in Table 3. In fact, outputs of all the algorithms are rather close to the actual weight vectors, but the proposed algorithm converges faster than other algorithms. Moreover, at the steady-state stage, the RLogLMP algorithm is more stable than other algorithms. From the Fig. 11, it is clearly observed that the RLogLMP algorithm outperforms the LMP, NLMP, NLMAD, RLS and NIRLS algorithms in terms of convergence rate and steady-state kernel error. The parameter settings of NIRLS algorithm is the same as [30] (see Table 3). After the about iteration 500, the NIRLS algorithm meets the requirement and stop. In addition, it is obviously shown from these curves that the proposed algorithm reaches low misadjustment (10−3and10−4) in about iteration 100, whereas the LMP and NLMP algorithms start to converge in about iteration 800, NLMAD algorithm starts to converge at about 500 points. The RLS-Volterra filter, in contrast, has large fluctuations during adaptation. The RLogLMP algorithm finally settles at an NKE value of10−4and10−5, NLMAD and NIRLS algorithms achieve NKE value of10−2and10−3, while the NKE of the LMP, and NLMP algorithms eventually reach10−3and10−4, respectively.Example 2AR(2) input: In this example, the input signalx(n)is an AR(2) signal [31] which is generated as follows:(66)x(n)=0.5x(n−1)−0.4x(n−2)−0.03x(n−1)x(n−2)+0.03x2(n−2)+ϑ(n)whereϑ(n)is a zero mean white Gaussian signal. The additive noiseε(n)and unknown system are the same as the Example 1. The proper selection of parameters for compared algorithms are summarized in Table 3. The trajectories of some of the filter weights are depicted in Fig. 12. As can be seen from these figures, the proposed algorithm has faster convergence rate than that of the LMP, NLMP, NLMAD and RLS algorithms. Fig. 13describes the learning curves of AR(2). Clearly, it is seen that the proposed algorithm has advantages over LMP, NLMP NLMAD, RLS and NIRLS algorithms. When the input signal is highly correlated, the LMP, NLMP and NLMAD algorithms suffer from slow convergence rate. The RLS algorithm has poor convergence property at iteration. Owing to the block update rule, the NIRLS algorithm achieves the similar initial convergence rate as compared to RLogLMP algorithm, it stop update after iteration 763. But it has relative high kernel misadjustment for nonlinear system identification. The proposed algorithm, which adopts the recursive method, speeds up the convergence rate when the AR(2) is selected as the input signal. Meanwhile, the low steady-state kernel error is obtained due to the logarithmic transformation. Furthermore, it can be obviously observed from these figures that the estimation error for the linear kernel is less than that of the quadratic kernel because of the eigenvalue spread of the correlation matrix for quadratic terms larger than that of linear terms.Example 3Uniform input: In this example, the nonlinear system in [12] is identified by the algorithms in α-stable noise. The unknown system is a 20-tap system as follows:(67)z(n)=0.3x(n)−0.5x(n−1)+0.7x(n−2)−0.6x(n−3)+0.2x(n−4)+0.1x2(n)+0.25x(n)x(n−1)−0.2x(n)x(n−2)+0.08x(n)x(n−3)−0.03x(n)x(n−4)+0.4x2(n−1)−0.3x(n−1)x(n−2)+0.5x(n−1)x(n−3)+0.06x(n−1)x(n−4)+0.65x2(n−2)−0.35x(n−2)x(n−3)−0.3x(n−2)x(n−4)+0.45x2(n−3)+0.2x(n−3)x(n−4)+0.15x2(n−4)The input signal is the uniform input [32]. It is well-known that the uniform input leads to the large eigenvalue spread of the correlation matrix. This phenomenon can reduce the convergence rate in Volterra filter case. The characteristic exponent ofα=1.25,γ=1/15andSNR=15dB. The parameters for LMP, NLMP, NLMAD, RLS, NIRLS and RLogLMP algorithms are chosen to guarantee the transient and steady-state behavior, and are summarized in Table 3. In order to clearly demonstrate the identification performance, the ensemble-averaged tap weight trajectoriesψi(n)for adaptive Volterra system identification using LMP, NLMP, NLMAD, RLS and proposed algorithms are shown in Fig. 14. To further evaluate the convergence performance and kernel misadjustment of proposed algorithm, Fig. 15depicts the curves of the steady-state kernel error. The convergence characteristic shown in Fig. 15 shows the effectiveness of the RLogLMP algorithm over the LMP, NLMP, NLMAD and RLS algorithm. Based on block update rule, the NIRLS algorithm also obtains fast convergence property and stop updating after iteration 525. However, it still has relative high kernel misadjustment, especially for quadratic kernel. The proposed RLogLMP algorithm, inherits the advantages of the recursive algorithm in p-norm, and achieves best final misadjustment owing to logarithmic transformation.From the experiment results of the above three examples, it is demonstrated that the proposed algorithm is superior over the LMP, NLMP and NLMAD algorithms for both convergence rate and steady-state kernel error, and has stability convergence property as compared to RLS and NIRLS algorithms. Moreover, the robustness of the proposed algorithm is confirmed by different unknown systems. The improved performance of the proposed algorithms is attributed to the recursive method and logarithmic transformation utilized in RLogLMP algorithm. As a consequence, it is concluded that the nonlinear system identification with the RLogLMP algorithm can provide a satisfied result in the presence of α-stable noise.

@&#CONCLUSIONS@&#
