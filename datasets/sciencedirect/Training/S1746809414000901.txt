@&#MAIN-TITLE@&#
An implementation of independent component analysis for 3D statistical shape analysis

@&#HIGHLIGHTS@&#
A unique ICA formulation for complex multi-dimensional continuous signals is shown.A novel framework to incorporate ICA for statistical shape analysis is shown.The ICA algorithm is validated for decomposition of 3D surfaces.ICA is compared to PCA for human right ventricle decomposition.

@&#KEYPHRASES@&#
Right ventricle,Statistical shape analysis,Function assessment,Computer-aided diagnosis,

@&#ABSTRACT@&#
An implementation of the independent component analysis (ICA) technique for three-dimensional (3D) statistical shape analysis is presented. The capabilities of the ICA approach to uncover inherent shape features are first demonstrated through analysis of sets of artificially generated surfaces, and the nature of these features is compared to a more traditional proper orthogonal decomposition (POD) technique. For the surfaces generated, the ICA approach is shown to consistently extract surface features that closely resembled the original basis surfaces used to generate the artificial dataset, while the POD approach produces features that clearly mix the original basis. The details of an implementation of the ICA approach within a statistical shape analysis framework are then presented. Results are shown for the ICA decomposition of a collection of clinically obtained human right ventricle endocardial surfaces (RVES) segmented from cardiac computed tomography imaging, and these results are again compared with an analogous statistical shape analysis framework utilizing POD in lieu of ICA. The ICA approach is shown to produce shape features for the RVES that capture more localized variations in the shape across the set compared to the POD approach, and overall, the ICA approach produces features that represent the RVES variation throughout the set in a considerably different manner than the more traditional POD approach, providing a potentially useful alternate to statistically analyze such a set of shapes.

@&#INTRODUCTION@&#
Statistical shape analysis has been shown to provide a powerful means to efficiently represent a large variety of shapes for various applications, and especially for applications in medical image analysis. In particular, statistical shape analysis has been shown to be capable of providing shape-based features that can be used to build effective metrics for classification and diagnostic purposes in medicine [1–6]. In general, the key components of statistical shape analysis frameworks for classification purposes involve building a correspondence between the given set of shapes (as could typically be obtained by medical image segmentation from a population), statistically decomposing the shape set into fundamental shape components, and then building features from the shape components that are suitable to cluster the shape set into various groupings and/or build a classifier associated with the application (e.g., pathological state) of interest. There have been a wide variety of techniques developed and employed within statistical shape analysis frameworks depending on the specific features, restrictions, and/or objectives of the particular applications. Furthermore, the statistical shape analysis work to-date has been largely focused on developing the mathematical representation of the shapes in a given set along with the preprocessing methods necessary to build a correspondence between these shapes, including aspects of topological mapping, shape alignment/registration, and parameterization, while much less consideration has been given to the method of statistically decomposing the shape set once this correspondence is set. The vast majority of statistical shape analysis work thus far has used some form of principal component analysis (PCA) (interchangeably referred to as proper orthogonal decomposition (POD) or by other names depending upon the specific formulation and/or application) to decompose the shape sets into fundamental components (i.e., modes or basis functions). PCA can be viewed as providing the orthogonal basis of the specific order that is optimal in an average sense for representing the given dataset. PCA has been shown to be useful in several of the examples referenced above for extracting significant shape features, and yet, PCA provides only one perspective to view the components of the shape set of interest and is subject to the constraints of its formulation (e.g., orthogonality and average L2 optimality), which may or may not be optimally suitable for the given application.An alternative statistical decomposition technique known as independent component analysis (ICA) was established in [7] that utilizes a considerably different approach compared with PCA. Generally, ICA seeks to uncover the inherent patterns in a given signal dataset by identifying the fundamental components that can represent the dataset in a linear combination and are maximally statistically independent from one another [8–11]. The derivation of ICA has been approached with several different concepts and tools, such as information maximization [12], maximum likelihood estimation [13], and utilization of artificial neural networks [14]. Furthermore, ICA has been applied to extract independent features in a diverse range of research fields, including image processing [15,16], electroencephalogram (EEG) signal analysis [17,18], and audio signal processing [19], among others. A large portion of the prior work has been focused on processing one-dimensional signals, yet, formulations have also been presented to consider multi-dimensional signals, namely the multilinear ICA, which were formulated to assess multiple “modes” of discrete signals within a dataset [20–23]. ICA has shown promising capabilities in terms of extracting substantially distinct features and utilizing these features for further classification purposes compared with PCA, thus leading to ICA being considered an alternative to PCA that may be preferable in some instances depending upon the application. ICA and PCA have been directly compared in several works, including applications in face recognition [24,25] and EEG signal processing [17,18], with ICA being preferred in someinstances and PCA in others.ICA has seen minimal application in the area of statistical shape analysis to-date, with the current contributions including [26,27], which both sought to identify patterns related to the shape variation of cardiac structures (e.g., left and right ventricles) with two-dimensional cross-sectional analysis (i.e., only analyzing line segments from the cardiac structures). There has yet to be work considering three-dimensional (3D) statistical shape analysis with ICA. A likely contributor to the lack of work integrating ICA into 3D statistical shape analysis is the lack of the necessary ICA formulation to generally accommodate multi-dimensional and/or continuously distributed shapes (i.e., signals), with the applications thus far only considering single-dimension uniformly distributed (i.e., sampled) discretized signals.This work presents a formulation of ICA that is generally applicable for analyzing multi-dimensional continuous signals, and that is appropriate for 3D statistical shape analysis. The formulation presented is focused on one particular and popular implementation of ICA known as FastICA [28,29], but the extensions presented could easily be similarly applied to other existing ICA methods as well. Sets of artificially generated shapes were analyzed to verify the ICA algorithm and the results are presented and compared with a standard PCA (i.e., POD) algorithm. The ICA approach was then implemented within a statistical shape analysis framework (as previously developed by the authors and reported in [30]). The shape analysis framework with ICA was applied to analyze a clinically obtained set of 3D right ventricle endocardial surfaces (RVES), and the results are again presented in comparison to the shape analysis approach with POD as the statistical decomposition strategy to show the substantial differences in the two techniques and their outcomes as related to statistical shape analysis. As such, the core contributions of this work are: (1) a uniquely formulated and universally applicable ICA derivation and algorithm that can be directly applied to 3D statistical shape analysis, and (2) the presentation and examination of an ICA-based statistical shape analysis workflow applied to analyze human RVES in comparison to the previously built more standard PCA/POD-based approach [30]. The work presented in this paper broadens the state of the art of ICA-based signal processing to analyze not only vectorized discrete signals, but also more complex continuous and/or non-uniformly sampled signals, and, in particular, provides a useful alternate path to statistical shape analysis for future disease diagnostic purposes.Section 2 presents the details of the ICA algorithm developed. Section 3 shows the analysis of artificially generated shapes with both ICA and POD, and outlines the statistical shape analysis framework incorporating ICA along with the results of analyzing the clinically obtained set of right ventricle endocardial surfaces with both ICA and POD, which is followed by the concluding remarks in Section 4.The following formulation assumes that a given collection of n shapes to be analyzed have already been parameterized and a correspondence has been built (see [4] or similar works for examples of how such a correspondence can be built), such thatu→i(x→)defines the three-dimensional (3D) cartesian coordinates of the surface of the ith shape in terms of the coordinatesx→in the common domain of the parameterization Ω, such thatx→∈Ω. The fundamental assumption of ICA is that each shape function is a linear combination of m hidden (latent) statistically independent basis functions (i.e., independent components) as(1)u→i(x→)=∑j=1maijs→j(x→),fori=1,2,...,n,wheres→j(x→)j=1mis the set of m independent components and aijis the modal coefficient corresponding to the ith shape and jth independent component. Assuming each shape is arranged as a column vector, the entire collection of the vector-valued functions can be defined as(2)u→1T(x→)⋮u→nT(x→)=a11⋯a1m⋮⋱⋮an1⋯anms→1T(x→)⋮s→mT(x→),or(3)[U]=[A][S].Therefore, to determine an estimate of the set of independent components, [S], it is only necessary to estimate the left pseudoinverse of the mixing matrix, [W]≈[A]−1, leading to(4)[S˜]=[W][U],where[S˜]is used to denote the set of estimated independent components.The first challenge of any ICA algorithm is to find an effective way of quantitatively measuring statistical independence. Non-Gaussianity is commonly considered as an equivalent means to measure the independence under the Central Limit Theorem [11,9]. In brief, the Central Limit Theorem states that the distribution of the summation of any independent random variables tends towards a Gaussian distribution as the number of random variables being summed goes to infinity. For example, the sum of two independent random variables will be more “Gaussian” than the individual random variables. Therefore, if the estimated independent components (Eq. (4)) are obtained under the condition of maximizing the non-Gaussianity of each component, this would theoretically lead to maximizing the separation of the independent components. There have been several methods proposed for quantitatively measuring non-Gaussianity based on various principles, including information maximization [12] and maximum likelihood estimation [13], among others. Once the measure of non-Gaussianity is defined, an optimization-based approach is typically utilized to obtain the independent components, with several algorithms having been developed for this purpose, such as the FastICA [28], JADE [31], and Infomax [32] algorithms. The FastICA algorithm is one of the most widely implemented approaches, and is therefore the approach selected herein to be extended for direct multi-dimensional continuous signal analysis. The main details of this formulation are provided in the following, and additional details of the existing FastICA algorithm for one-dimensional discrete signal analysis can be found in [9] and the references therein.The first step in the FastICA algorithm is to preprocess the dataset through centering and whitening. In the centering step the mean (i.e., expected value) is removed from the dataset, [U]. Defining the three components of the ith shape asu→i(x→)=[pi(x→),qi(x→),ri(x→)]T, the expected value of each 3D shape function can be defined as(5)E[u→i(x→)]=13A∫Ω(pi(x→)+qi(x→)+ri(x→))dx→,where A is the area of the domain of the parameterization (i.e.,A=∫Ωdx→). Thus, each centered 3D shape function can be calculated as(6)u→ic(x→)=u→i(x→)−[E[u→i(x→)],E[u→i(x→)],E[u→i(x→)]]T.The objective of the whitening step is to linearly transform the centered dataset, [Uc], so that each shape is uncorrelated with all other shapes in the transformed dataset and the variance is equal to one. To perform the whitening a standard whitening transformation algorithm, shown in [33], was implemented as follows. First, the components of the covariance matrix are defined as(7)Σij=1A∫Ωu→ic(x→)·u→jc(x→)dx→.Eigen decomposition is then applied to the covariance matrix to obtain the corresponding set of eigenvalues and eigenvectors, defining [λ] as the diagonal matrix of eigenvalues and [C] as the matrix of eigenvectors arranged columnwise. Finally, the set of whitened, centered shape functions can be calculated as(8)[U˜]=[λ]−12[C]T[Uc].Commonly to reduce noise and dimensionality, which may also prevent overlearning in ICA, only a subset of the whitened shape functions that correspond to the highest energy components (i.e., those corresponding to the eigenvalues with the highest relative magnitude) are retained for the following steps in the analysis [9]. One approach is to ensure that the subset (of size k) of shape functions retained has a cumulative energy percentage that exceeds a chosen threshold, as in(9)∑i=1kλi∑j=1nλj>threshold.To then determine the independent components for the centered and whitened functions the FastICA algorithm seeks solutions to the following optimization problem to maximize the non-Gaussianity of the components(10)maxw→iJ(w→i),fori=1,2,...,m,where(11)J(w→i)=EG([U˜]Tw→i)−E[G(v→)]2.As such,J(w→i)is defined as the measure of non-Gaussianity for the ith independent component as generated byw→i, which are the approximations of the rows of the pseudoinverse of the mixing matrix ([W] in Eq. (4)).v→is a three-dimensional column vector (to match the dimension of the shape functions) of Gaussian random variables with zero mean and unit variance for each component and G(x) is a nonspecific non-quadratic function that contributes to the measure of non-Gaussianity. Since the second term in Eq. (11) is independent ofw→i, the maxima ofJ(w→i)will correspond to either the maxima or minima of the first term in Eq. (11), as in(12)maxw→iJ(w→i)=maxw→iorminw→iEG([U˜]Tw→i).Then, constraining the magnitude ofw→ito be unity and applying the necessary condition for extrema of a function, a solution to the optimization problem can be obtained from(13)∂∂w→iEG([U˜]Tw→i)±λ‖w→i‖2−1=0→,where λ is a Lagrange multiplier and ||.|| is the standard l2-norm. Differentiating, applying the definition of the expected value (Eq. (5)), and simplifying, the gradient can be rewritten as(14)13A∫Ω[U˜]g([U˜]Tw→i)dx→±2λw→i=0→,where g(x)=dG(x)/dx. An arbitrary constant β can then be defined such that −β=±2λ, and the gradient equation becomes(15)13A∫Ω[U˜]g([U˜]Tw→i)dx→−βw→i=0→,Lastly, β can be explicitly calculated by taking the dot product of the gradient equation withw→ito produce(16)13A∫Ωw→iT[U˜]g([U˜]Tw→i)dx→−βw→iTw→i=0,and noting thatw→iis constrained to be a unit vector, β can be calculated as(17)β=13A∫Ωw→iT[U˜]g([U˜]Tw→i)dx→,Examples of g(x) that have been used in the literature [34] include(18)g(x)=x3,(19)g(x)=tanh(x),or(20)g(x)=xexp−x22.Eq. (19) is known to be a good general-purpose function, Eq. (20) may be better when robustness is very important or when the independent components are highly super-Gaussian, and Eq. (18) is appropriate for estimating sub-Gaussian independent components when there are no outliers [29].Finally, Eq. (15) can be solved approximately using Newton's method by providing an initial guess for the unknown vectorw→i(often randomly generated) and then iteratively updating the solution vector as(21)w→i+=w→i−−[Q(w→i−)]−1b→(w→i−),where(22)b→(w→i−)=13A∫Ω[U˜]g([U˜]Tw→i−)dx→−βw→i−,w→i−is the previous iteration's estimate for the solution vector andw→i+is the updated estimate, and[Q(w→i−)]is the Jacobian matrix. As is standard, the Jacobian matrix is defined as the gradient of the residual vector (Eq. (22)) with respect to the vector to be updated for the approximation (w→) as(23)[Q(w→)]=∂∂w→13A∫Ω[U˜]g([U˜]Tw→)dx→−βw→.The definition of the Jacobian matrix can be expanded and shown in indicial form as(24)Qjk=13A∫Ω∑l=13g′∑i=1nU˜ilwiU˜jlU˜kldx→−βδjk,where g′(x)=dg(x)/dx and δjkis the standard Kronecker delta. Then, by assuming that the g′ terms are all reasonably approximated by the constant mean of the three components over the integral domain (and returning to the matrix-vector form), the Jacobian matrix can be approximated with the simplified form(25)[Q(w→)]=Eg′([U˜]Tw→)13A∫Ω[U˜][U˜]Tdx→−β[I],and since the data ([U˜]) has already been whitened based on Eq. (7), the Jacobian matrix can be simplified even further to(26)[Q(w→)]=13Eg′([U˜]Tw→)−β[I],where [I] is the n×n identity matrix. Substituting the approximation for the Jacobian, the update equation can be implemented in a simplified form as(27)w→i+=z→‖z→‖with(28)z→=Eg′([U˜]Tw→i−)w→i−−1A∫Ω[U˜]g([U˜]Tw→i−)dx→.Various standard measures of convergence can be implemented, and for the present work, iteration was ended when the update of the vectorw→ireached a prescribed tolerance, ϵ, as in(29)‖w→i+−w→i−‖<ϵ.Thus, this iterative procedure can be applied to approximate several different vectorsw→ii=1mto estimate the independent components as(30)s→i(x→)=[U˜]Tw→i.In addition, to ensure that unique vectorsw→iare obtained for all m independent components (i.e.,w→i≠w→jfor i≠j), a Gram-Schmidt technique [35] can be applied to further update the estimates at each Newton iteration as(31)w→i++=w→i+−∑j=1i−1w→i+Tw→jw→j,fori=2,...,m.Lastly, there are several approaches that have been employed to determine the number of independent components to generate, with the approach used herein being to simply generate an equivalent number of components to the size of the data subset retained after whitening [34,10].

@&#CONCLUSIONS@&#
An approach was presented to extend the FastICA algorithm for applications to multi-dimensional continuous functions, particularly relating to incorporation within statistical shape analysis frameworks. The capabilities of this ICA approach were shown through two sets of example analyses, a set of artificially generated shapes and a set of clinically obtained human RVES, and both analysis sets were compared to the same analysis procedure, but with the ICA portion replaced with the more traditional POD/PCA approach. The analysis of the artificially generated shape sets showed that for certain signal types (such as signals of the type considered herein, which had non-orthogonal inherent components), the ICA approach may be better suited for extracting inherent shape features in comparison with the POD approach. Similarly, when applied to analyze the set of human RVES, the ICA approach was shown to extract considerably different shape components than the POD approach, particularly in terms of the localizations. More significantly, the shape components produced by ICA were shown to provide features that can lead to substantially different representations of the RVES set in comparison to the analogous features produced by POD. By providing an alternate means to represent changes in the function of a biological structure throughout a clinical dataset, such as that considered here, the ICA approach is expected to provide a worthwhile alternate for consideration in applications of statistical shape analysis in medicine, among other potential applications.