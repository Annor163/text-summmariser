@&#MAIN-TITLE@&#
Incremental Semi-Supervised classification of data streams via self-representative selection

@&#HIGHLIGHTS@&#
We advance an Incremental Semi-Supervised classification (ISSC) approach via Self-Representative Selection (IS3RS).We develop an incremental self-representative data selection strategy.Most representative exemplars from the sequential data chunk are incrementally labeled to expand the training set.

@&#KEYPHRASES@&#
Incremental learning,Semi-supervised classification,Self-representative selection,Data streams,Big data,

@&#ABSTRACT@&#
Incremental learning has been developed for supervised classification, where knowledge is accumulated incrementally and represented in the learning process. However, labeling sufficient samples in each data chunk is of high cost, and incremental technologies are seldom discussed in the semi-supervised paradigm. In this paper we advance an Incremental Semi-Supervised classification approach via Self-Representative Selection (IS3RS) for data streams classification, by exploring both the labeled and unlabeled dynamic samples. An incremental self-representative data selection strategy is proposed to find the most representative exemplars from the sequential data chunk. These exemplars are incrementally labeled to expand the training set, and accumulate knowledge over time to benefit future prediction. Extensive experimental evaluations on some benchmarks have demonstrated the effectiveness of the proposed framework.

@&#INTRODUCTION@&#
Data today is more deeply woven into the fabric of our daily lives than ever before due to the rapid improvement of digital technology of storage and information processing. Very recent few years have witnessed an explosive growth of data, where continuously collected data streams accounts for a large and important part [1,2]. From the perspectives of computation and machine intelligence, one should establish a data-driven machine that is capable of incrementally analyzing large-scale dynamic data stream, and accumulating knowledge incrementally over time to benefit future learning and decision-making process [3–11]. Consequently, a machine learning paradigm, Incremental Learning (InLe), is developed where the learning process takes place according to the newly emerged examples [12–21]. Compared with traditional supervised learning, InLe is capable of learning new information from sequential examples to facility the decision-making process. It is very suitable for applications where examples do not always arrive simultaneously, and the newly arriving data may bring a new perspective, may even change the statistical distribution of data. Moreover, from the biological viewpoint, InLe is more consistent with human learning where human beings already use possessed knowledge along with the experiences for learning and decision making.Nowadays many incremental learning architectures [22,23] and algorithms [12–15,20,21,33] have been developed to deal with data streams, which can be categorized as Absolute Incremental Learning (AInLe) and Selective Incremental Learning (SInLe). In AInLe, new data are analyzed separately, and new features are formed and combined with the existing ones. In SInLe, the selected training set based on the proximity and impact of new data and new information are retrained in light of new information. Most of available InLe approaches are SInLe, which do not assume the availability of a sufficient labeled dataset before the learning, but the training examples appear over time. However, in real-life scenarios, new examples are not always labeled timely. In practical, massive amounts of data are collected dynamically in very rapid mode, resulting in the difficulty of offering labeled samples over time. For example, labeling examples from surveillance and mobile sensor network data streams is infeasible both in time and resource. On the other hand, preparing a sufficiently large number of labeled training samples at the very beginning is practically impossible, for the changing environment where new characteristic of samples or even new kind of samples are generated over time. Consequently, it is necessary to automatically update an existing training set in an incremental fashion to accommodate new information, by adding newly emerged samples to the training set.Although the classification of data streams are characteristics of scarce labeled examples, enormous number of sequentially incoming samples are available. Because learning from labeled as well as unlabeled data is very useful for incremental learning, semi-supervised learning technologies can be developed by exploiting unlabeled data to modify and refine the classifier or discriminate criteria to improve classification accuracy [24–26]. Different with AInLe and SInLe, Semisupervised Incremental Learning (SSInLe) first builds knowledge base incrementally from the available labeled data. Then with the unlabeled data, SSInLe updates and restructures the knowledge incrementally. Finally it makes decisions about the new instance on the basis of the knowledge base and update the training set.SSInLe is very important from various real-time learning perspectives, but few works have done on it. In order to explore both the labeled and dynamic unlabeled samples for a more accurate prediction of data streams, in this paper we advance an Incremental Semi-Supervised classification approach via Self-Representative Selection (IS3RS), for data streams classification. In the SSInLe, an important issue is to identify relevant unlabeled data that can be added to the existing training set. In our method, an incremental self-representative data selection strategy is proposed to find the representative exemplars from the sequential data chunk. These exemplars are incrementally labeled to expand the training set, to accumulate knowledge over time to benefit future prediction. Inspired by the representation learning theory [27], we aim to find a subset of data that efficiently describe the entire data set. It assumes that each data in a dataset can be represented as a linear combination of a limited number of exemplars, which is regarded as a compact representation of data set. By adding some initial exemplars to the labeled set, a new training set can be obtained. Then we can acquire the labels of exemplars by co-training technique [28] via self-representation of each data chunk. The most confidently recovered testing data is added into training set to facilitate the learning.The remained of this paper is organized as follows: In Section 2, the incrementally semi-supervised framework and self-representation are detailed. In Section 3, some experiments are taken on several datasets to validate the efficiency of our proposed method. The configurations, results and discussions of experiments are given. Conclusions and discusses are presented in Section 4.The proposed IS3RS approach is illustrated in Fig. 1, which consists of three phases: self-representative selection, co-training, and finial decision. First each data chunk is self-represented to determine its exemplars. Under the framework of co-training, labels of these exemplars are predicted by the K-nearest neighbor (KNN) classifier. Then the training set is expanded by adding the most confident exemplars together with their predicted labels. Finally, the final classification is performed based on the expanded training set. In the following we describe each step in detail.As described in [27,34], the representative training data plays a key role in deciding the performance of learning algorithm. Therefore, learning representative data from vast amount of data is of great importance when building effective classifier or other prediction for data streams. In the data chunk classification, a key factor is whether the learning machine can take advantage of the representative testing data to construct a compact training set. Among various kinds of representative selection methods, sparsity inspired representation learning attracts a lot of interests because of its simple principle and feasibility. Moreover, it does not need to cast any distribution prior on data and present convincing performance. In this paper, we learn exemplars by a self-representation of data, under the assumption that there exist some exemplars, and each data in the dataset can be described as a linear combination of those exemplars. Mathematically, given a data set X∈ℜD×Nwith some D-dimensional data xi, where D is the dimensionality of data and N is the number of samples in the data set. We would like to select an informative data subset that can represent the whole dataset. Selecting exemplars can be reduced to the following optimization problem,(1)minSX−XSF2s.t.Srow,0≤kwhere S∈ℜN×Nis the coefficient matrix andSrow,0counts the number of nonzero rows of S. In other words, we expect to select at most k(k≪N) samples in X that can best represent X. These k informative samples are called as exemplars. This is a self-representation model, where the dictionary is the data set itself. The property makes the obtained exemplars coincide with the actual data point which can be well revealed the whole data set. By minimizing the reconstruction error of each data point as a linear combination of the examples in the dataset and enforcingS0,q≤k,(0,qnorm is defined asS0,q=∑i=1NI(siq>0), and siis the i-th row of coefficient matrix S and I() denotes the indicator function, we can determine the indices of nonzero rows correspond to the exemplars. Thus the above optimization problem can be represented as:(2)minSX−XSF2s.t.S0,q≤kThis is an NP-hard problem as it requires searching over every subset greedily. A standard relaxation of this problem is obtained as:(3)minSX−XSF2s.t.S1,q≤τwhere1,qnorm isS1,q=∑i=1Nsiq, which is the sum of q-norm of the each rows in S and τ>0 is a positive threshold parameter. In this paper, we experimentally assign q as 2.Using Lagarange multipliers, we rewrite the above optimization problem in (3) as:(4)minSλS1,q+12X−XSF2As the method of Multipliers, we introduce an auxiliary equivalent variable J for S, that is J=S, which allows the optimization problem to be more easily solved. Thus the augmented Lagarange form of Eq. (4) can be formulated as:(5)minS,J,Y1λJ1,q+12X−XSF2+〈Y1,J−S〉+μ2J−SF2where 〈〉 denotes the trace operator, Y1 denotes the Lagarange multiplier and μ is penalty parameter. The added last term in (5) is used to make the intermediate variable J equal to the variable S, and the added third term in (5) is an augmented Lagarange regularizer term. Then we can use ADMM technique to alternately optimize these variables iteratively. The above optimization problem can be easily implemented in an alternating manner by using Alternating Directing Method of Multipliers (ADMM) optimization algorithm [28]. As soon as the sparse coefficients S are obtained, the exemplars can be determined as to the indices of nonzero row of S. This self-representative selection can select some representative samples to reduce the redundancy of the data set.As mentioned above, the classification performance largely depends on the training set, when the initial labeled set is limited and unlabeled samples are increasing chunk by chunk. In our work, we aim to construct a representative and informative training set during all the learning process. We attempt to find some informative samples to enhance the learning results. In this section, the exemplars are labeled by means of co-training techniques introduced in [29,30]. As discussed in [30], we simply split the features of each sample into two dependent parts (two views) randomly and use KNN classifiers to estimate the labels of these exemplars. The most confident exemplars that are classified into the same class by different classifiers, are added to the training set.Specifically, an initial training set L0 is given before the learning. Denote Xjas the j-th data chunk received between time tj−1 and tj,XjRpbe the preliminary representative exemplars of data chunk Xjobtained via self-representation learning, Lj−1 be the labeled set at time tj−1. Then we perform a collaboration co-learning onXjRpand the training set Tj−1, and add the most confident exemplars together with their labels into Lj−1 to form a new labeled set Lj. Mathematically,Li=YiRp∪Li−1.Note that exemplars that are predicted as belonging to two or more classes will be excluded from the recovered exemplars. Finally, the recovered exemplars with their estimated labels are combined to formulate the representative training set. Based on this training set, we classify the testing data using KNN classifier. The objective of IS3RS algorithm is to design an effective training set by exploiting the useful information from the testing data to improve the classification accuracy. The main procedure of IS3RS is summarized in Table 1. By means of co-training technique, the exemplars that are selected by unsupervised representation learning, are prone to be confidently labeled to form an informative and representative training set. Since data come chunk by chunk, one can accumulate knowledge by a small number of exemplars with low storage and computation cost. Moreover, the selection can be extended to a distribution algorithm and taken on a parallel platform, if a large scale of data need to be processed.In our experiments, we use Synthetic dataset, USPS digital dataset and some UCI datasets (http://archive.ics.uci.edu/ml/) to evaluate the proposed IS3RS method. Some aspects are investigated in our experiments, including: (1) an investigation on the efficiency of the proposed self-representative selection strategy; (2) an investigation on the performance of the proposed ISSC approach; (3) an investigation on the classification results of IS3RS algorithm, and a comparison of IS3RS with some related incremental approaches, including: ADAIN.MLP [5], ADAIN.SVR [5], Learn++[31] and IMORL [32]; (4) an investigation on the computational complexity of IS3RS algorithm. All experimental simulations are performed with MATLAB R2013a on a personal computer with 3.2GHz Intel Core i5-3470 CPU and 4.0GB RAM.In this experiment, we use two datasets (one synthetic data set and one USPS digital data set) to demonstrate the efficiency of the proposed representative selection strategy.1)Synthetic dataset:We first construct 3 independent subspaces whose bases{Ui}i=13are computed by Ui+1=TUi, where T is a random rotation and U1 is a random orthogonal matrix of dimension 100×4. Then we generate 100×120 data matrix X=[X1X2X3] by randomly sample 40 data points from each subspace by Si=UiCi, 1≤i≤3 being a 4×40 with Cibeing a i.i.d.N(0, 1) matrix.USPS digital dataset:The USPS digital data set contains 10 classes of hand draft characters. Each sample is a digital gray scale image with size 16×16.In this test, we first use the self-representation learning to find the exemplars in the Synthetic data set. Fig. 2illustrates the exemplars. The samples are represented as points in a 2D feature space after a dimensionality reduction operator by Principle Component Analysis (PCA). Some data points are randomly chosen to be corrupt, the observed data are formulated by adding Gaussian noise with zero mean and0.2x2variance respectively (xmost range from 0.1 to 0.4 in this experiment).From Fig. 2, we can find that the self-representation learning selects the samples near the boundary which can well represent the corresponding class. Even for the noisy data, the representation learning can learn the noisy data, which indicated that seriously corrupted data are often incoherent with clean data, and they prone to be classified to a new class that does not appear in the previous training set. Because a sample belongs to a new class can be regarded as an outlier or serious noisy data, when compared with the other samples in the training set, it is expected to be selected. Consequently, all the classes in the dataset can be found. Fig. 3shows two USPS digital number exemplars obtained by self-representation, from which we can see that the exemplars are informative.An important question still remains for the proposed algorithm, i.e. to what extent or under what assumption that the proposed method can benefit the finial decision-making process? In this simulation, we discussion it and take an experiment to demonstrate how the proposed algorithm generates the representative labeled set. We first randomly choose 50 samples from the whole dataset for each class and then divided them into five chunks with identical size of 100 samples per chunk, to form a subset named SubUSPS. Each chunk is enforced to contain at most four classes of digital characters. The detailed description of each chunk is given in Table 2. The size of the chunk and the class of each chunk can be randomly initialized. In our simulation, we set the number of samples in each chunk as 100, and the classes as 3 or 4.Firstly, self-representation learning is performed for each data chunk to obtain its exemplars. (The second and third columns in Table 3show the class indexes and the corresponding number of exemplars in each chunk). For the Chunk 1, we query the labels of initial exemplars by means of initial labeled set. The remaining 4 exemplars of chunks are sequentially fed to the labeled set to update the training set. Meanwhile, we predict the subsequent exemplars that are selected from the training set until 10 classes are found.From the Table 3, we can find that the query classes and their corresponding number. (class – #. No.): (0/3/5 – 4/5/7)→(0/3/2/7 – 1/1/2/3)→(2/7/1/4 – 3/2/2/2)→(1/4/6/9 – 2/2/3/2)→(6/9/8 – 3/2/5). The bold class number in the sixth column denotes the new classes that are selected in the previous active annotation. The results demonstrate that new patterns are easily chosen as representative exemplars since it cannot be classified as the other classes. Thus a complete training set can be built, and we can use the learned labeled set to classify the SubUSPS. Finally a classification accuracy 99.20% can be obtained with SVM classifier.To validate the performance of the proposed incremental algorithm, four real-world data sets with varied size and number of classes from UCI machine learning repository [http://archive.ics.uci.edu/ml/] are employed for empirical study in the following test [5]. A detail description of the four data sets can be found in Table 4. In this simulation, each data set is sliced into chunks with size between 150 and 300. At each run, one chunk is selected to be added to the training set according to its arriving order, and the subsequent chunks are fed to the classifier according to its arriving order.In this experiment, we have included some of state-of-art incremental learning algorithms including: ADAIN.MLP [5], ADAIN.SVR [5], Learn++[31] and IMORL [32]. Our major focus here is to demonstrate that the proposed IS3RS algorithm can learn the informative and representative training set and labeled set, by predicting the most informative exemplars. By using the accumulated knowledge over time, we can subsequently add the most confident samples to update the label. For KNN classifier, we use Euclid distance and L1-Norm as a distance measure, and the number of neighbors is set as 1 and 3 respectively. Table 5gives the numerical results of these data sets, including the Overall Accuracy (OA, the total classification accuracy that is defined as the ratio of the number of correctly classified examples to total examples), Average Accuracy (AA, the average value of classification accuracies for each class) as well as Kappa Coefficient (KC, an accuracy assessment that is defined as the ratio of (Po−Pe) to 1−Pe, where Po and Pe are the observed label agreement and expected label agreement respectively). In essence, for classification tasks, the Kappa Coefficient measures the association between the ground truth labels to the labels that acquired by classifiers and helps to evaluate the predicted labels.It can be observed from Table 5 that, in most cases, the proposed algorithm obtained the best numerical results compared to other methods. But in some cases, the classification result is not the best. This is perhaps due to the fact that not all the informative exemplars are included in the labeled set.In this experiment, we test the Magic data set to analyze the computational complexity of the proposed framework, when different size of chunk is used. The running time of the initial exemplars selection procedure for all chunks is shown in Fig. 4with different predefined chunk size. The number in the box indicates the number of samples in the chunk. Though the number of chunks decreases with respect to the increase of the chunks size, the time is mainly decided by the chunk size not the number of chunks, which can be seen from Fig. 4. The reason lies in the fact that the optimization needs to update variables by using a singular value decomposition of a matrix, whose computational complexity relies on the size of chunks. Therefore, if the computational time is limited, we should limit the chunk size.As mentioned at the beginning, we are now in the era of big data, since the data are large both in the dimensionality and volume. When dealing with large scale data, we can extend the proposed algorithm to a distributed version and realize it on a parallel platform. For each data chunk, we can find its representative individually on a slave machine, and then synthesize the exemplars to a master machine to update the training set by adding the most confident exemplars.

@&#CONCLUSIONS@&#
