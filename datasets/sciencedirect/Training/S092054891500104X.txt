@&#MAIN-TITLE@&#
Empirical evaluation of the link and content-based focused Treasure-Crawler

@&#HIGHLIGHTS@&#
We present the experimental results of a focused Web crawler that combines link-based and content-based approaches to predict the topical focus of an unvisited page.We present a custom method using Dewey decimal classification system to best classify the subject of an unvisited page into standard human knowledge categories.To prioritize an unvisited URL, we use a dynamic, flexible and updating hierarchical data structure called T-Graph, which helps find the shortest path to get to on-topic pages on the Web.For the background review, the experimental results from several crawlers are presented.We compare our results against other significant focused Web crawlers.

@&#KEYPHRASES@&#
Focused Web crawler,T-Graph,HTML data,Information retrieval,Search engine,

@&#ABSTRACT@&#
Indexing the Web is becoming a laborious task for search engines as the Web exponentially grows in size and distribution. Presently, the most effective known approach to overcome this problem is the use of focused crawlers. A focused crawler employs a significant and unique algorithm in order to detect the pages on the Web that relate to its topic of interest. For this purpose we proposed a custom method that uses specific HTML elements of a page to predict the topical focus of all the pages that have an unvisited link within the current page. These recognized on-topic pages have to be sorted later based on their relevance to the main topic of the crawler for further actual downloads. In the Treasure-Crawler, we use a hierarchical structure called T-Graph which is an exemplary guide to assign appropriate priority score to each unvisited link. These URLs will later be downloaded based on this priority. This paper embodies the implementation, test results and performance evaluation of the Treasure-Crawler system. The Treasure-Crawler is evaluated in terms of specific information retrieval criteria such as recall and precision, both with values close to 50%. Gaining such outcome asserts the significance of the proposed approach.

@&#INTRODUCTION@&#
To improve the quality of searching and indexing the Web, our proposed focused crawler depends on two main objectives, namely, to predict the topic of an unvisited page, and to prioritize the unvisited URLs within the current page by using a data structure called a T-Graph. We elaborated the architecture of the Treasure-Crawler in a preceding paper [1], where a thorough review on the background of this subject field was discussed by briefly introducing significant Web crawlers. Also, the requirements of a focused crawler were elicited and the evaluation criteria were outlined.Based on the current enormous size of the Web, which has passed sixty trillion Web pages [1], the need for the distribution of resources to harvest, download, store and index the Web has become a clear necessity. In this context, focused crawlers are inevitable tools, aimed for detecting pages based on the diverse topics of human knowledge, and indexing the relevant pages into distributed repositories while keeping a merged index for further faster retrievals.This task of indexing must be capable enough to keep up with the changes of dynamic contents on the Web, since keeping an old version of the Web is obsolete. That is the reason for employment of machine learning or reinforcement learning strategies within the body of some search engines; to diligently watch the Web. Section 2 includes a succinct background study of focused crawlers. Section 3 is the introduction to our proposed prediction and prioritization methods. Then, the implementation and experimental setup details and results are discussed in Section 4. This paper concludes with the evaluation of the results and the future directions for interested researchers.To cover the maximum possible number of Web pages, the first generation of crawlers recursively followed the links between Web documents. Therefore, these exhaustive crawlers pay less attention to page content as opposed to the interlinked structure of the Web. The exponential growth of the Internet motivated scientists to design focused crawlers that restrict their concentration to specific topics and, as a necessity, value the page content in addition to the link structure between pages. In this section, we introduce the most appropriate of these focused crawlers as state of the art. Then, in Section 6, we present a high-level comparison between their approaches and our proposed system in terms of used algorithms, modules and concepts. In terms of performance, one of these crawlers is compared to the Treasure-Crawler.Context Focused Crawler (2000) has been the basis for many crawlers during the past decade. An exemplary multi-level tree is built out of sample documents for each target Web page. These graphs are then merged into one and help prioritizing any downloaded page by finding its content’s similarity to the nodes in the graph [2].Meta Search Crawler (2004) relies on the results from other search engines, where several topic-specific queries are sent to multiple search engines and the results are merged and indexed. This approach has a good accuracy in remaining on topic, while having the precision value of 50.23%, although its dependence on other search engines is questionable [3].LSCrawler (2006) uses the corresponding ontology of its topical focus, and semantically assesses the anchor text and the text around any unvisited URL. This crawler reportedly has a recall value of 0.6, which is very close to that of the Treasure-Crawler, though it is based on semantic technology [4].Relevancy Context Focused Crawler (2006) addresses the high classification load and low accuracy issues of the original context graph algorithm [2] and uses a novel structure that reaches 22% in precision. This approach uses the general and on-topic feature words to enhance the prioritizing process [5].Hybrid Focused Crawler (2006) is based on both content similarity and link structure (links-to and links-from). The content similarity is calculated using vocabulary vectors and simple keyword matching. This crawler demonstrates a high harvest ratio in the retrieval of on-topic pages since it is intended to first download and then determine the relevancy of a Web page [6].HAWK (2008) is another known link- and content-based combinatory method to focused crawling of the Web. HAWK has a harvest ratio of 60%, which significantly decreases after traversing 5000 pages as reported [7].Modified Naïve Bayes (2010) is the root classification technique for a crawler that uses reinforcement learning to be trained on the topic taxonomy. It also uses a modified form of TF-IDF for its assessments of textual data. These modifications bring about a relatively high harvest ratio and performance [8].Intelligent Focused Crawler (2011) carries out its URL scoring task based on a threshold-restricted modified form of Naïve Bayes classifier. The modifications resulted in a significantly improved harvest ratio of 80%, although the number of crawled pages in the experiment (1000 pages) is not a reliable size for test dataset [9].OntoCrawler (2011) employs a fuzzy technique to weigh the content of a page and is considered as a semantic crawler, as it relies on ontologies for knowledge classification. OntoCrawler has empirically shown a precision rate of 90% on the topic of football, which is a significant performance [10].Clickstream-Based Parallel Crawler (2012) follows the history clickstream parameter as opposed to the interlinked structure of Web pages for the URL prioritization task. Therefore, an authorized access to server log is required as this approach relies on previous users’ activity. We consider this technique as a potential module to be added to our proposed system [11].BDS for SIS Crawlers (2014) is a novel technique called bridge-driven search and addresses the social internetworking systems. In this flexible technique, the problem of loose connections between thick populations of on-topic regions on the Web is targeted. In our proposed system, we solved this problem with appending off-topic pages to the download queue with the lowest priority score, while making certain that their priority score is incremented gradually [12].Beam Classifier (2014) is based on decision trees for the classification of lexicons. However, this lexicographic classifier shows more accuracy as compared to the greedy approach of DTs. Also a fitness function constantly watches the accuracy and F-measure to evaluate the performance. We are now studying the possibilities of incorporating a similar solution into a future version of our Treasure-Crawler, with respect to the fact that using the trie data structure will result in a better performance because of its linear time complexity [13].While most of the research in this area has been carried out in the scope of designing better classification and link scoring functions, some scientists set out their software engineering viewpoint as the basis of their research, taking into account the programming, implemental and applicative details of such crawling systems [14,15]. Some of these considerations are specific features of Web pages such as DOM, Internet applications’ automatic crawling, guided and example-based approaches to crawling, cyber-security threats and system vulnerabilities and Web application development methods such as component-based techniques [16]. It is noteworthy that based on software engineering standards, we studied the essential requirements of a Web crawler system [1]. These requirements are grouped into functional requirements, such as relevance calculation, priority calculation, and data storage and indexing for later search. The non-functional requirements are more abstract and include flexibility, robustness, standardization, manageability, modularity, etc.In our preceding paper [1], we presented the architecture of a Treasure-Crawler–based search engine and its modules. Predicting the topical focus of an unvisited page and assigning appropriate priority score to its URL are the main functions of a focused crawler.For the topic prediction task, we designed a custom method to detect the main subject(s) of the target page. In order to best classify the main subject of a Web page into known and standard human knowledge categories, we use the Dewey decimal classification (DDC) system [17,18] as the human knowledge classification reference. Fig. 1shows an example of finding the topic ‘butterfly’ in DDC.A dataset is created from the vocabulary and corresponding codes from DDC (see Table 1in the next section). Basically, the word tokens from specific HTML elements of the parent document are extracted, preprocessed, cleaned and then compared to the entries of this dataset in consecutive phases, each phase corresponding to a digit position of the code, i.e. 1st digit, 2nd digit, and so on. In each phase of the prediction process, the acquired topical focus becomes more precise and detailed. If the final selected codes (indicating the predicted topic) are parts of the desired DDC list of codes, then the unvisited page is considered as on-topic [1].For the prioritization objective and in addition to the above procedure, the T-Graph structure as an exemplary guide carries out the task of priority association by providing a conceptual route for the crawler to follow and find on-topic regions. T-Graph is a tree-like hierarchical structure where each node has possibly multiple parents in multiple upper levels, and evidently each node has multiple children in any lower levels, while the target (most relevant) documents are in the lowest level (leaves). The tokens of HTML elements of the parent page along with the anchor text of the unvisited link are compared to all nodes of the T-Graph, the node with highest similarity is found and its level number in the tree indicates the priority score of the unvisited link, calculated as (1/level_number). The URL of the unvisited page is then inserted into the download queue (i.e. a priority queue) along with this priority score for later download.Based on these objectives, we designed and implemented our system as a prototype. From the evaluated results of our experiment, we can assert that the framework, architecture, design, and implementation of our prototype were effective and satisfied the two objectives of this research. The rest of this paper elaborates the experimental setup and results.The Treasure-Crawler is designed and implemented based on the modularity and object oriented concepts and techniques; hence all the modules can simply be plugged and played while requiring the minimum change of other modules and at interface level only. The prototype experiment is conducted to test the capability of the crawler to download Web pages related to a predefined topic/domain, by best using the T-Graph structure and other methods that are defined or used in this research. With this overview, the Treasure-Crawler system was set up as elaborated in Fig. 2, showing the main components as well as the data that travel within the system, with the following explanation:The crawler robot contains fetcher and response queues. The seed URLs are initially inserted into the fetcher queue with the highest priority. When an item (URL) with a high priority is fetched (1), the robot downloads the textual content of the page from the Internet (2). The HTML data of the page, or if unsuccessful, server’s HTTP response relating to temporary unavailability of the link, is then transferred to the structural HTML parser (3) where specific HTML elements are extracted and given to the crawler (4). These elements are properly segregated and then given to the relevance calculator (5) to algorithmically predict the topical focus of all unvisited links within the page. For this task, the relevance calculator is supplied with the complete set of DDC entries (6), as well as the set of DDC entries that specify the crawler’s topic of specialty (7). If the relevance calculator determines an unvisited link as on-topic, the data from specific HTML elements are compared to T-Graph nodes (8) and the priority score of the link is calculated (9). If the unvisited link is determined as off-topic, it receives the lowest priority score in order to push the crawler into harvesting unconnected on-topic regions of the Web. The URL along with its priority score is then inserted into the fetcher queue (10), where the scores of items are cyclically incremented to prevent starvation. Also, the actual HTML data of the current page is stored in the repository (11) with other measurements such as the priority scores given to the links. The rest of this section is on the description of how the test is carried out.The system has been manually supplied with DDC codes and corresponding word tokens relevant to the topic of English language and grammar, in which the system specializes. It must be mentioned that DDC is a proprietary dataset of codes and classes, organizing human knowledge into distinct categories. All the textual data is lowercased and stemmed using the Porter stemming algorithm before taking part in any calculation or comparison. This way, the acronyms of each word will be taken as similar to the base word. Table 1 lists the DDC data entries indicating the topical focus of the system, i.e. English language and grammar.Seeds are the designated starting points of the system for crawling. These points, in terms of URLs, are initially handed to the crawler to start populating its document collection. These seed URLs as well as the topics of interest constitute the essential parameters of a Web crawler. In our experiments, two different sets of seeds were supplied to the system. The first set consisted of seventeen on-topic URLs, while the second set contained seven generic URLs, as listed in Table 2.As described before, the T-Graph has the possibility of being constructed either top-down or bottom-up. According to the requirements of our implementation, the construction of T-Graph starts from a set of target documents that are the most topically relevant Web pages, and from that point onwards, their parents (in terms of Web hyperlinks) are retrieved. Therefore, the utilized approach for T-Graph construction is bottom-up. The target documents in our experiment are:•http://en.wikipedia.org/wiki/Languagehttp://en.wikipedia.org/wiki/Grammarhttp://www.reference.com/browse/languagehttp://www.reference.com/browse/grammarUsing a backlink checker tool, the Open Site Explorer [19], the parents of these four target documents are retrieved to form the nodes of level 1. In the same fashion, the nodes of levels 2 and 3 are retrieved and, for each node, the constituent components are filled with the necessary data. In total, the graph has fifty nodes, made out of twenty-eight URLs. These nodes constitute the graph in four levels (0–3). It must be stated that several edges are discarded in this implementation of the graph. Table 3lists the URLs that are used in the T-Graph, as well as number of nodes made out of each.The system is run with different sets of seeds, configurations and conditions in order to be well evaluated. All the runs are carried out on commonly configured machines with permanent Internet ADSL connection. As described before, our experimental prototype system is specialized in the topic of English language and grammar. The April 2014 version of DMOZ directory [20] is employed as the dataset, with the metadata shown in Table 4. It must be stated that the DMOZ dataset is originally an ontology. For our system, the converted version of the DMOZ dumps has been used, which is in MySQL database format.We tested the system on a development set of data (a portion of the dataset that we use for trial runs to find best initial values for system parameters) to come up with a set of tuned parameters that produce highest functionality/performance. Table 5lists these parameters along with their tuned values.The 1st parameter is the depth of T-Graph that ultimately affects the priority score assigned to URLs. If the textual similarity of the selected HTML elements to some node(s) of T-Graph is above a threshold (2nd parameter) then the node at the lowest level is selected and its level number helps calculate the priority score of the URL as (1/level_number). It is noteworthy that a deeper T-Graph results in more accuracy in assignment of priority score. However, in the scope of our experimental prototype, a depth of 3 will reasonably help distinguish highly topically related pages. The 3rd item indicates that anchor text of a link has 40% more impact on the topic prediction of the unvisited page, as opposed to the text around the link. The 4th item specifies the number of digits of DDC codes (D-numbers) that the system processes to determine the topical focus. Obviously, as the number of processed digits becomes greater, the accuracy of the topic detection increases. The 5th item is the default assigned priority value to an unvisited link when it has no corresponding node in the T-Graph with OSM value of more than the threshold or when the topical focus of the unvisited link is not what the system specializes in. The 6th item is the value that is periodically added to the priority score of each item in the fetcher queue until it is less than 1 to prevent aging of the queue items (also called starvation). This value is added after a specific number of insertions, which is the 7th item in the table.To evaluate the performance of the Treasure-Crawler, in addition to the default tuned parameters, we alternatively tested the system with changes in three individual parameters to observe and compare their impact on performance. Table 6lists these alternative conditions under which the system has been run and tested. For each set of parameters’ values, we run the system with both generic and on-topic seed URLs, listed in Table 2.

@&#CONCLUSIONS@&#
