@&#MAIN-TITLE@&#
Fusion of a panoramic camera and 2D laser scanner data for constrained bundle adjustment in GPS-denied environments

@&#HIGHLIGHTS@&#
Accurate scale is estimated with laser data and features extracted from camera.Many more loop closures are detected by sensor than that of camera-only methods.Scale and loop closure geometric constraints are enforced on Constrained Bundle Adjustment, which produces a robust pose estimate.Experiments show that the proposed method is practicable and more accurate than vision-only methods.

@&#KEYPHRASES@&#
Panoramic camera,Laser,Sensor fusion,Constrained bundle adjustment,

@&#ABSTRACT@&#
Pose estimation is a key concern in 3D urban surveying, mapping, and navigation. Although Global Positioning System (GPS) technologies can be used to estimate a robot's or vehicle's pose, there are many urban environments in which GPS functions poorly or not at all. For these situations, we offer a novel approach based on a careful fusion of panoramic camera data and 2D laser scanner input. First, a Constrained Bundle Adjustment (CBA) is introduced to handle scale and loop closure constraints. The fusion of a panoramic image series and laser data then enables an accurate scale to be estimated and loop closures detected. Finally, the two geometric constraints are enforced on the global CBA solution, which in turn produces a robust pose estimate. Experiments show that the proposed method is practicable and more accurate than vision-only methods, with an average error of just 0.2m in the horizontal plane over a 580m trajectory.Localization and mapping, and especially their integration in SLAM (Simultaneous Localization and Mapping) and SFM (Structure from Motion), represent a core challenge for many applications in robotics and computer vision. Related research dates back to the early photogrammetry work of the last century [1]. In recent years, the rapid growth in computing power and algorithmic sophistication has yielded great progress, as well as a number of successful systems [2,3]. At heart, the challenge consists of an estimation problem that can be solved using two distinct approaches [4]: filtering methods, which marginalize past poses and summarize information gained over time via a probability distribution [3,5,6], and bundle adjustment (BA) methods, which retain a global optimization of the entire vision sequence [7–11], and emphasize local optimization with respect to both accuracy and efficiency [12,13].Whereas filtering methods have generally been used to estimate robot motion in real time, BA methods have been used for the global or local optimization of 3D bundles extracted from a sequence of images. Comparing the accuracy of the two methods, [4] concluded that BA outperforms filtering. In this paper, we focus on accurate localization and mapping (full SLAM) rather than autonomous navigation. We adopt BA methods to improve the performance of constrained bundle adjustment (CBA) with a scale factor and loop closure, thus achieving more accurate localization based on fused imagery and laser data.In monocular SLAM/SFM applications, the translation between two frames is only recoverable up to a scale factor. Thus, a more accurate estimate of the scale will yield better spatial alignment of consecutive data frames with visual odometry [14]. Note that the scale factor can be observed/estimated for any two frames captured by a stereoscopic system; in monoscopic systems, there is no direct way to measure feature depths or odometry, resulting in compounded error drift [5]. To address this, if we start from a physical object of known size, we can assign a precise scale to the estimated map and motion, rather than considering the scale as a completely unknown degree of freedom [15]. If the target sizes are not available (as in many real-world applications, such as vehicle-borne urban mobile mapping), GPS can be substituted as a constraint on the error drift, as it scales directly to the real world [14,16].Although the spatial alignment of consecutive images can be calculated using scaled monoscopic odometry, camera pose error (and especially attitude error) will still compound the drift in monoscopic systems. In GPS-denied systems, a globally consistent alignment of the complete data sequence under a closed loop constraint becomes a convenient and efficient solution to prevent drift [17]. However, the loop closure detection mechanism must be very robust, because false-positive loop closures will cause corruption in most mapping systems [3].The state-of-the-art detection algorithm is FAB-MAP [18], a rigorous probabilistic approach to image matching based on a ‘visual bag-of-words’ model. Unfortunately, as purely appearance-based methods, they are unable to distinguish between different locations or landmarks that have a similar appearance [3]. A local spatial geometric constraint can be added to FAB-MAP to give FAB-MAP 3D, which uses 3D Delaunay tessellation to accelerate the comparison of spatial layouts modeled as random graphs [19]. The combination of a geometric comparison and appearance matching reduces the likelihood of perceptual aliasing, though some (especially urban) environments may exhibit repetitive locations that are identical in both structure and appearance. Using camera motion sequences to calculate the probability of loop closure through the location prior, the CAT-SLAM [3] algorithm determines more correct loop closures than the first generation of FAB-MAP. Although such combination methods significantly reduce the ambiguity of loop closure detection, CAT-SLAM must revisit a previously traversed trajectory with the same orientation to detect loops, and can seldom deal with cases where the orientation is different [20]. Although FAB-MAP can recognize loop closures in a new direction, the core of its appearance-based matching system (the state-of-the-art SIFT descriptor) suffers a rapid decrease in matching rate owing to larger viewpoint angles [21]. This may result in a low loop detection rate.Laser scanning offers another means of improving loop closure detection. In [17], point clouds from each end of the loop closure were captured from short segments of a vehicle's motion. These were used to determine the loop closure through iterative closest point (ICP) approximation. Unfortunately, iterative approximation based only on laser point clouds is not guaranteed to converge to a solution. There are also practical limitations of using laser points for loop closure detection, such as when dynamic objects such as moving people cause incorrect point clouds to appear in the scene [19]. To solve the spatial aliasing problem of laser-only methods and detect more robust loop closures, the image texture can be used to compensate the laser data.In this paper, we propose a new method that fuses panoramic images and laser point cloud sensor input. Our approach uses motion sequences for scale estimation and loop closure detection, and these are in turn used by CBA for global optimization. Similar studies on sensor fusion have been involved in visual-SLAM and aerial triangulation. Using a local BA strategy, [22] compared three weighting methods for real-time visual-SLAM combined with an odometer or gyroscope. In photogrammetry, GPS, inertial measurement unit (IMU), and camera data are often integrated under the unified global BA framework to obtain accurate georeferencing [23]. However, to the best of our knowledge, there is no efficient scale estimation and loop closure detection method that fuses panoramic image sequences and laser point clouds. In our system, a panoramic camera and slope-trawling laser scanner are integrated as shown in Fig. 1(a); the estimated scale and the detected loop closures then constrain the BA global optimization according to the flowchart in Fig. 1(b). Note that this proposal extends a previous work [16] involving data association and the spatial alignment of consecutive image frames.The remainder of this paper is structured as follows. Section 2 outlines the fundamental theory of BA, and then presents the CBA algorithm for use with the estimated scale factor and detected loop closure. Section 3 describes our algorithm for scale estimation based on sensor fusion, and discusses the advantages of this method over camera-only methods. Our loop closure detection mechanism is introduced in Section 4, emphasizing its insensitivity to changes in camera view angle. Section 5 details our use of high-accuracy GPS/IMU positioning to obtain the ground-truth for the experimental data. Using this as a basis for comparison, we demonstrate that our sensor fusion method is significantly more accurate than techniques that use only a panoramic camera, and gives results that are very close to the ground-truth.In this section, we briefly review a classic BA solution for nonlinear global pose estimation. Global BA is typically conditioned by all ray observations in an offline procedure that emphasizes accuracy over efficiency. In this form, BA has been applied to many full-SLAM problems.The following notation and the graph shown in Fig. 2are used to represent the classical BA problem described in [24,25]. Here, the estimate of the sequence of 6D poses is denoted by X=[x1; … ; xn], where xi is a 6×1 vector indicating the i-th pose. The vertical concatenation of these vectors forms the pose vector X6n×1. Estimates of 3D landmarks are given by M=[m1; … ; mq], where M3q×1 is the landmark vector defined in a similar way. When the i-th landmark is observed from the j-th pose, the observation is denoted as zij. The residual of observations is then defined as(1)lij=zij−hxjmiwhere h(∙) denotes the observation model. In this paper, h(∙) represents the rigorous sensor model of a panoramic camera described in [16]. In general, the optimal value of the unknown parameters [X; M] can be achieved by minimizing the weighted sum of residuals, defined as follows:(2)minX,M12LTPLwhere the residual vector Lr×1 is the vertical concatenation of all available lijand P is a symmetric positive-definite weight matrix.It is challenging to directly solve the nonlinear problem in Eq. (1). Hence, BA often exploits numerical optimization such as the Gauss–Newton method. In this way, the problem in Eq. (1) can be approximately solved by iteratively calculating the increment in X and M as follows:(3)minδx,δm12Aδx+Bδm−LTPAδx+Bδm−Lwhere δx, δm are the incremental updates of X, M, and A, B are the corresponding Jacobian matrices, respectively.The explicit solution is easily obtained by calculating the derivatives, written as:(4)ATPAATPBBTPABTPBδxδm=ATPLBTPL.In Eq. (4), the coefficient matrix on the left-hand side is the information matrix (see Fig. 2). This contains two unknowns, δx and δm, and is of very large dimension, (6n+3q)×(6n+3q). Fortunately, δm can be eliminated by applying the Schur complement trick:(5)ATPA−ATPBBTPB−1BTPAδx=ATPL−ATPBBTPB−1BTPL.The solution for δx can be obtained using a classic sparse Cholesky solver [20], and δm is then given by substituting δx into Eq. (4).CBA is a modified form of the classic BA. Additional constraints, such as scales from an odometer, locations from a GPS receiver, or attitudes from an IMU measurement, can be introduced to further improve the performance of BA.In robotics and computer vision, scale usually means the ratio of a ground-truth distance to a corresponding distance in camera-centered space. For simplicity, we treat the distance between two camera poses in the absolute world coordinate system as the scale observation, because the relative distance in camera-centered space is known. In this paper, we obtain the scale observation dijbased on our sensor fusion method (see Section 3). Ideally, it should be equal to the 3D Euclidian distance between the i-th pose xi and j-th pose xj. Therefore, an additional constraint can be set in BA as follows:(6)minX,M12LTPL+12LdTPdLdwhere Ld is the residual vector of scale constraints and Pd is the corresponding weight matrix. The problem can be converted as follows:(7)minδx,δm12Aδx+Bδm−LTPAδx+Bδm−L+12Dδx−LdTPdDδx−Ldwhere D is the Jacobian matrix with respect to X. Note that M is not involved in this constraint, and the scale observation is considered independent of the ray observations.In the information matrix form illustrated in Fig. 3, the measured distance d23 (red) between poses x2 and x3 only impacts on the red boxes, whose values are directly added to the original values represented by the corresponding grey boxes. Thus, the new information matrix can be constructed as:(8)ATPA+DTPdDATPBBTPABTPBδxδm=ATPL+DTPdLdBTPLwhich can be reformulated as:(9)ATPA+DTPdD−ATPBBTPB‐1BTPAδx=ATPL+CTPdLd−ATPBBTPB‐1BTPL.A loop closure indicates that the same scene is observable from two corresponding poses, xiand xj. This implies that a geometric constraint between xiand xjcan be constructed:(10)xi=xj·Δxijwhere Δxijrepresents the relative pose between xiand xj, which can be calculated using a common 3D transformation based on the i-th and j-th poses. In this paper, the observation of relative poses can be obtained using our sensor fusion method (see Section 4). This forms an additional constraint in our CBA model. Considering that the loop closure constraint is independent of the ray observations, the new CBA can be written as:(11)minX,M12LTPL+12LdTPdLd+12LcTPcLcwhere Lc is the residual vector of loop closure constraints and Pc is the corresponding weight matrix. This loop closure constraint can be incorporated into the information matrix. For example, Fig. 4shows the loop closure between x1 and x5 (sharing the same 3D landmark m5). This closure contributes to the original information matrix through the additive values represented by red boxes.Finally, considering both scale and loop closure constraints, the information matrix and corresponding constant terms can be constructed as:(12)ATPA+DTPdD+CTPcCATPBBTPABTPBδxδm=ATPL+DTPdLd+CTPcLcBTPLwhich can be reformulated as:(13)ATPA+DTPdD+CTPcC−ATPBBTPB‐1BTPAδx=ATPL+DTPdLd+CTPcLc−ATPBBTPB‐1BTPL.It is clear that accurate scale constraints are very important to the robustness and convergence of the least-squares adjustment for the information matrix, especially those elements on the diagonal (see Fig. 3). In this section, we present a method that improves on the common method of scale estimation by fusing panoramic image data and laser point cloud data.Data association [26–30] enables all relative pose components except the scale to be determined for an entire trajectory. This means that, for a given scale, the otherwise correct pose B may lie anywhere along the line AB (with A corresponding to the correct pose, at the correct scale), as shown in Fig. 5(a). There is a simple but non-rigorous method for estimating scale using only a panoramic camera. If we assume that a road surface is perfectly flat, and that a camera moves across it from point A to point B, the scale can be estimated by the known camera height above the road surface. In Fig. 5(a), pose B is estimated with the correct scale using the relative height of road surface point a with respect to A, which should be equal to the known camera height:(14)Za=−Hwhere H is the camera height. If (Xa Ya Za) are the coordinates of 3D object a on the road surface, Zacan be expressed as:(15)Za=λtxZ2−tzX2x1Z2−z1X2where λ is a scale parameter, T=[tx ty tz]T is the translation vector between the two poses; (x1, y1, z1) and (x2, y2, z2) are the camera coordinates of point a with respect to A and B, respectively; and R is a rotation matrix with (X2, Y2, Z2)T=R[x2, y2, z2]T.Eq. (14) exhibits small model errors if the road surface is bumpy. Real road surfaces are always irregular, and a given place (usually described by a feature) not on the road surface may be misused in a scale calculation, particularly in complex dynamic environments. This will cause the real road point a to be mistaken as a′, and pose B as B′, leading to big discrepancies in scale estimation.Our proposed method of scale estimation uses a laser point cloud to calculate the height of point a relative to position A. Even when the road surface is uneven, this is a rigorous computation, and a dense point cloud further reduces the risk of taking the measurement from a false road point a′. In Fig. 5(b), the road surface is represented by the red 3D laser points, whose coordinates are based on camera pose A. The intersection of this surface with ray Aa′ determines the correct road surface point a, and thus the correct scale for correct pose B.(15)Da=dIn Eq. (15), d is the distance observation measured by laser; Dais the distance between A and a, and is the square root of (Xa^2+Ya^2+Za^2), where (Xa Ya Za) are functions of the scale parameter λ and can be obtained by 3D intersection. Clearly, d is an absolute observation, and is unaffected by the uneven surface of the road.According to Eq. (15), each road surface point determines one value of λ. And redundant observations provided by many surface points can contribute a more robust estimate. A histogram voting method is introduced: every observation votes once for a certain scale, and the histogram bin with maximum number of votes indicate the optimal estimate.Using scaled monoscopic odometry, accurate camera pose estimates can be achieved with GPS-supported BA [16]. In GPS-denied environments, however, pose estimation errors will become compounded and drift away from the accurate state. To address this, it is possible to form a globally consistent alignment (i.e., global optimization) of the entire data sequence under a closed loop constraint [17].Although recovering the relative pose of a loop from images alone can yield excellent results, in certain loop closure cases—particularly those involving cameras with very different view direction angles—proper recovery may become infeasible. Variations in viewing angles cause deformations that cannot be reflected by a single planar similarity transformation, as the depth of landmarks in the scene is unknown and many have strong 3D structures. Conventional visual-SLAM systems, such as FAB-MAP and CAT-SLAM, both employ SIFT descriptors for loop detection, resulting in noticeable performance degradations as the viewing angle increases in the 2D scene [21]. The left-hand image in Fig. 6was taken while traversing east-to-west through a region, whereas the right-hand image was captured while traversing west-to-east. SIFT performs poorly in this case, because landmarks at different distances from different viewing angles cause more complex geometric deformations.In [17], point clouds (trawled by slope lasers) were generated from short segments of the vehicle's motion around each end of the loop closure. These could be used in an ICP algorithm to determine the loop closure. However, this iterative method based only on the point cloud is not guaranteed to converge to a solution. For more robust and efficient loop closure detection, we propose a fusion method that integrates an image sequence with point cloud data. The core concept of our approach is to detect loop closure in images that have been orthographized for the fused data, as opposed to the original, direction-sensitive images. The main advantage of our “ortho-images” is that they are all top-down views, thus obviating the complex and/or uncertain geometric transformations introduced by landmarks of unknown depth, and allowing for simple alignment of directions in the corresponding images. This simplification is especially helpful in complicated scenes, for which the proposed method can detect many more loops than purely appearance-based methods.The local ortho-images are generated as follows. First, local motion sequences from data associations of panoramic images (i.e., local odometry results) are used to generate the 3D laser point cloud (shown as green dots in Fig. 7). Second, the 3D point cloud is used as road surface elevation data to generate a local digital elevation model (DEM) grid. Third, the local ortho-image is generated by mapping color information from the panoramic images to the local DEM grid, pixel by pixel (producing the images on the right in Fig. 7). These local ortho-images with the same scale make loop closure detection much more straightforward, and the detected features are then matched using a rotation-invariant intensity correlation method [31] to determine whether the current observed image is new or simply a loop closure.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
In this paper, we proposed a new method for urban mapping and localization based on the fusion of image and depth data combined with motion sequences. The resulting improvements in scale estimation and loop closure detection mean that we can constrain the bundle adjustment for the global optimization of sensor pose estimation in GPS-denied urban environments. The resulting robust and accurate localization results are close to the ground-truth. Experiments demonstrated that our method is practicable and feasible for most urban localization or mapping projects, especially where elevation accuracy is not a strict demand. The main innovation of our method is the fusion of panoramic camera images and 2D laser scanner input to constrain the bundle adjustment.In future work, we will focus on achieving greater accuracy and robustness in elevation accuracy. To accomplish this, additional geo-referenced data set, such as aerial images or digital maps, will be integrated into the current panoramic/laser data, acting as adequate inputs for an accurate 6-D CBA solution.