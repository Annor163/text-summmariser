@&#MAIN-TITLE@&#
Robust regional bounding spherical descriptor for 3D face recognition and emotion analysis

@&#HIGHLIGHTS@&#
A regional bounding spherical descriptor is used for 3D face and emotion analysis.Region segmentation is based on shape index and spherical band on 3D face.The regional descriptors are obtained by projecting regional bounding spheres.A regional and global regression is proposed for the weighted regional descriptor.

@&#KEYPHRASES@&#
3D face recognition,Emotion analysis,Regional bounding spherical descriptor,Regional and global regression,Kullback–Leiber divergence (KLD),

@&#ABSTRACT@&#
3D face recognition and emotion analysis play important roles in many fields of communication and edutainment. An effective facial descriptor, with higher discriminating capability for face recognition and higher descriptiveness for facial emotion analysis, is a challenging issue. However, in the practical applications, the descriptiveness and discrimination are independent and contradictory to each other. 3D facial data provide a promising way to balance these two aspects. In this paper, a robust regional bounding spherical descriptor (RBSR) is proposed to facilitate 3D face recognition and emotion analysis. In our framework, we first segment a group of regions on each 3D facial point cloud by shape index and spherical bands on the human face. Then the corresponding facial areas are projected to regional bounding spheres to obtain our regional descriptor. Finally, a regional and global regression mapping (RGRM) technique is employed to the weighted regional descriptor for boosting the classification accuracy. Three largest available databases, FRGC v2, CASIA and BU-3DFE, are contributed to the performance comparison and the experimental results show a consistently better performance for 3D face recognition and emotion analysis.

@&#INTRODUCTION@&#
Face recognition and emotion analysis are two important branches in biometric systems in remote communication, medical rescue, intelligent monitoring and so on. A large number of demands of face recognition and expression control are emerged due to the rapid development of 3D movies and entertainment [1–3]. More and more practice requirements are no longer satisfied by facial recognition or emotion analysis separately. The emerging industry needs to find an effective facial representation, which not only achieves a higher-quality discriminative power for the large size individuals but also provides a better expression description for control.Our previous bounding sphere descriptor demonstrated superior performance in 3D face recognition, especially where there were large pose variations. For practical applications, the proposed feature descriptor should be able to simultaneously perform face recognition and expression analysis. Expression variations imply variations in specific facial regions. Therefore, in this paper, we extend the global bounding sphere descriptor to regional bounding sphere descriptors. Regional weighted selection is used to form a general feature descriptor, which can be used for face recognition and expression analysis simultaneously. The novel low-dimensional features of this approach facilitate both theoretical innovations and practical applications.Sufficient broad investigations of 3D face processing have been achieved in the literature [4,5]. Although most of them independently analyze the issue of 3D face recognition and 3D facial expression classification, the literature illustrate that facial regional descriptor analysis can provide better accuracy for the two aspects simultaneously. For 3D face recognition, Faltemier et al. [6] divided a face into a group of regions and led to a better recognition performance. A region-based registration was employed to establish correspondence and 3D shape descriptor was used for statistical feature extraction [7]. Passalis et al. [8] used facial symmetry to overcome the challenges of large pose variations and the wavelet-based biometrics signature was used to evaluate the real-world applications. Local shape difference boosting [9] selected optimal local features for assembling three collective strong classifiers and found the most discriminative feature for 3D face recognition. Ioannis Marras [10] introduced novel subspace-based methods for learning the azimuth angle of subspace normal, which were well-suited for all types of 3D facial data for recognition. However, note that these algorithms focus on 3D face recognition. They cannot perform facial expression recognition.Some scholars are concerned with the descriptions of the different facial expressions, without the discrimination of the different individuals. For example, the Facial Action Coding System (FACS) [11], as a human facial expression representation, has been found over 25years ago. Prominent regions can describe the variant facial characteristics on the different individuals [12]. However, practical application requirements, i.e. high recognition rate, low computational complexity, and easy implementation, are the issues of wide concern.With the rapid development of science and technology, more and more systems need to distinguish the individual and his/her expression simultaneously, and then provide personalized services, as ours developed the system [2]. One barrier is how to precisely segment the facial regions on the different facial images, which is highly influenced on the accuracy of 3D face recognition and expression classification. In this paper, we first develop a facial segmentation mechanism, and then principally concentrate on how to find an effective facial descriptor with the capability of individuals' identification and expression description. The lower dimensional feature vectors can be used to compensate for computationally expensive.Empirical study shows that facial shape has a significant variance in terms of different regions on the facial surface. In order to better reflect anatomical structure and describe the expression variations, we introduce the segmenting scheme to address the major challenges and present a new recognition framework for 3D processing. Our research consists of three important procedures: facial region segmentation, feature representation and feature extraction as shown in Fig. 1.1.Facial region segmentation: By combining the facial shape characteristics and shape index information, a committee of facial local regions can be coarsely located. Shape band [13] algorithm is introduced to detect the contours of facial regions and refine the localization to highlight the discriminative and descriptive regions.Feature representation: Exploring our previous research [14], we further develop the robust regional bounding sphere descriptor (RBSR) on the facial surface, which are relatively consistent in the presence of expressions and poses. Combined with facial prior assumption, the region descriptors are weighted and projected into the spherical domain based on the contribution of the descriptiveness and discrimination.Feature extraction: Facial poses, occlusions and corruptions, as the major challenging issues for facial recognition and expression classification, have varying degrees at different facial regions. Regional and global regression mapping (RGRM) is introduced to grasp the intrinsic manifold structure of our feature representation for weighted RBSR descriptors, which better reflect the geometrical properties of the different facial regions with low redundancy.In our framework, a novel mechanism is proposed to overcome the unsolved challenging issues encountered for facial processing, called Regional Bounding Sphere Representation (RBSR). The main contributions of our scheme can be summarized in the following items:1.Robustness: Facial segmentation scheme based on shape index and region selection can robustly extract facial shape regions. The regional bounding sphere representation can effectively reduce the impact of non-discriminant areas and provide a promising way to describe facial regional properties. RGRM algorithm can extract the low-dimensional information from RBSR descriptor, which contains the most discriminative face and descriptive expressions.Efficiency: 3D data provides reliable data to support synergetic analysis for face recognition and expression classification. Large-capacity data analysis demands for high-performance and multi-task concurrency. The proposed framework can find a fast and general descriptor to learn different visual tasks simultaneously and compresses redundant data. Efficiency of our proposed framework is more suited to practical applications.Synergy: The current mainstream of visual study focuses on the single visual task. Research is generally conducted on specific tasks, for a specific application, using a specific method to solve certain vision related problems. We propose a generic descriptor for facial information. By means of effective feature learning, the results of face recognition and expression classification can be output simultaneously. The proposed framework lays the foundation for synergetic analysis for multi-visual tasks and face real-time intelligent interactive systems.The rest of this paper is organized as follows. Facial regional segmentation is described in Section 2. Section 3 presents the regional bounding sphere representation. In Section 4, regional and global regression mapping is used with RBSR for low dimensional feature extraction. The detailed experimental comparison with 3D face recognition and expression classification is analyzed in Section 5. Finally, we conclude the paper in Section 6.The original 3D facial images in the databases usually contain some non-facial areas, spikes and holes as shown in Fig. 2(a)[15]. Exploiting our previous research [14], facial profile can be extracted by the robust ASM algorithm [12] on the 2D texture image. The main facial area can be coarsely extracted by the “and” logic processing between the 2D profile and its corresponding 3D valid matrix in Fig. 2(b).In order to further refine the facial area, the position of the nose tip needs to be detected. The frontal face model is chosen as the reference model Fr. Then, the mirror face Fr′ is obtained by the coordinate plane Σxz. The TPS-RPM algorithm [16] is used to register Fr' to Frand generates the registered image Fr″. The facial symmetry plane Σris fitted using the point set:(1)pir=pi+pi″/2,1≤i≤N,where pi=(xi, yi, zi)Tis the point cloud of 3D reference face model and pi″=(xi″,yi″,zi″)Tis the point cloud of 3D registered face model. The cross curve of Frand Σrwith 1mm bandwidth is treated as the objective surface OSrand the highest point on OSris determined as nose tip Nrin Fig. 2(d). When there are nose occlusions, according to the proportion of prior knowledge of facial organs, we select the cross point on the fitted surface OSrbetween two-thirds on the vertical direction and one-half on the horizontal direction, as the nose tip as illustrated in Fig. 3. Nose tip refined detection under occlusion is the next stage of our research focus.The following procedure is used to normalize the size of the various 3D input images. Firstly, we estimate the horizontal and vertical projection curves for the extracted facial area by calculating row and column sums of the valid point matrix. Next, we select the maximum value of the horizontal and vertical projection curves as the location of the maximum horizontal and vertical distance respectively on the facial area. The ratios of the longest horizontal and longest vertical facial locations in the input and reference images are then used to normalize the width and length of the 3D input images as follows:(2)ratioX=distX/distXone,ratioY=distY/distYone,where distX,distY are respectively the values of horizontal and vertical lengths on the input 3D face image. distXone,distYone are the values of horizontal and vertical lengths on the reference face model. Finally, the normalization is calculated using the ratio calculated above.Then, the distance of two eyes' centers can be computed by locating the eyes' centers on 2D image by ASM [17] and project them to the corresponding 3D images. The ratio of eye center's distances between the input and reference models is introduced to refine the normalization results in Fig. 2(e). Empirical study shows that the results with facial normalization can significantly boost the accuracy for face recognition and expression classification.Axis-angle representation can align the input with the reference model in Fig. 2(f) as follows:(3)pireg=picosθ+r×pisinθ+rr⋅pi1−cosθ,where piis the input points of the 3D images and piregis the registered face points. The rotation axis r can be obtained via the cross product of the unit normal nrand n of the reference and input images. The angle θ between the nrand n describes the rotation magnitude. Unlike ICP, we just match the point clouds between input and reference models. As a result, our method can save a huge number of computational costs.A 3D facial image contains more facial discriminative and expression descriptive information than a 2D image and the degree is different from the different facial regions and curvature. The derived shape index serves as an important descriptor of intrinsic surface properties and is used for segmenting the different facial regions.We can segment that the points like cone shape as the upper nose. To find the inner eye pit locations, shape band [13] of eyes area extracted from the reference model is used as the search window on an input face and Gaussian curvature values, which is close to zero, is used to determine the local minimum inside the search window as the positions of eye pits. Then, we introduce the shape index values for searching nose borders as saddle rut-like shapes [18]. The nose border outline extracted by the shape band of the reference model is developed for searching the rightmost and leftmost points along the band, which can be treated as the left and right nose borders [7]. The shape band search window, based on the sign and values of curvature, is used to coarsely segment the mouth area. Considering facial prior information, the region, above the eyes, can be treated as forehead and the areas, between left/right eyes and mouth, can be determined as left/right cheek as shown in the bottom part of Fig. 2.In order to balance the facial discriminative power and expression descriptiveness, extended with our previous research [14], a novel descriptor is proposed, denoted as regional bounding sphere representation (RBSR). For each region of a 3D facial image, the descriptor is formed from the projection of the facial point cloud into regional bounding spheres which are centered at the center points of the regions as shown in Fig. 4. The distance between points in the region and the corresponding center point divided by the radius of regional bounding sphere produces the values of the spherical points. The value of points on the regional bounding sphere can be defined as follows:(4)RBSRcj=pix−cxj2+piy−cyj2+piz−czj2/Rbsj,where piis the coordinate value of each aligned 3D facial region point, Rbsjdenotes the maximum difference among the three different directions X, Y, Z on the Cartesian coordinates and cjis the centroid point of the j-th region. Then, the RBSR descriptor of each region j is denoted as RBSRj={RBSR1j,⋅⋅⋅,RBSRNj},1≤i≤N, where N is the number of points on the each facial region. The RBSR has successfully reduced computation complexity based on the discussion in our previous work.In the following, we estimate a weight for each regional representation. Mutual information is introduced to describe the relativity and redundancy of the different facial regions [19]. We define the average value of all mutual information between the regional descriptor rbsriand the region r as the relevance of the whole descriptor RBSR:(5)DRBSRr=1RBSR∑rbsri∈RBSRIrbsri;r.The redundancy of the descriptor RBSR is summed by the average value of all mutual information between the regional descriptor rbsriand the regional descriptor rbsrj:(6)RRBSR=1RBSR2∑rbsri,rbsrj∈RBSRIrbsri;rbsrj.Suppose region pairs are conditionally independent, we can approximately calculate the probability of a descriptor set RBSR given a region r and select the r that maximizes this probability. We can treat the maximum probability as the regional weight for specified application in the following equation. To build the connection between a specific individual and the corresponding facial regions, the object label information is provided for each facial region:(7)PRBSR|r=∏rbsri∈RBSRPli|r∏rbsrj∈RBSRPlj|li,rPD,R|r,li,ljwhere li,lj∈L is the discrete quantized labels associated with descriptor rbsri∈RBSR, and D, R are the values of relevance and redundancy computed by Eqs. (5) and (6), respectively. The detailed numerical solution method can be seen in Appendix A and the regional weights of RBSR descriptor can be calculated as follows:(8)ωr=logPRBSR|r=∑l∈LlogPbl|rwherelogPbl|r=∑rbsri∈RBSR∑rbsrj∈RBSRlogPD,R|r,l,ljis the bin probability and ωris the weight of RBSR descriptor for the region r.Thus, the descriptor of the whole 3D facial images can be expressed for the specified applications:(9)xk=ω1rbsr1+⋯+ωirbsri+⋯+ω7rbsr7where k is used for the specified 3D facial processing, which can be referred to face recognition and verification, expression analysis, and pose estimation. i ranges from 1 to 7 for the different facial regions, including nose, left/right eyes, left/right cheeks, forehead and mouth. The processing can further improve 3D facial region segmentation and selection, which not only details local properties but also enhances the shape information of 3D facial images.In this section, we have two learning tasks for face recognition and expression classification when the same input feature descriptor is used. Here, we assume X⊂RDis the feature descriptor computed by Section 3. Our proposal is to learn two regression functions a1 and a2 and obtain their corresponding lower dimensional feature vectors z1 and z2. Robust regional and global regression mapping algorithm (RGRM) is proposed to dimension reduction and overcome some remaining artifacts left from the region segmentation, such as some stretched or misaligned images, hair occlusions, large data noises and corruptions.In order to better cope with the unseen data and reflect the facial global properties, we introduce a nonparametric approach [20] for out-of-sample extrapolation. Since the regional structure of manifold in 3D face data is linear, the regression expression can be defined as a linear regression model, i.e., Y=WTX+E, where W is the local projection matrix, and E is the noise matrix. We map the weighted facial RBSR descriptor into a Hilbert space H and Rd, i.e., yi=ϕ(W)Tϕ(xi)+ei, where ϕ(W) is the regional regression matrix from H to Rdand ei∈Rdis noise term.The objective function can be rewritten to simultaneously learn the low dimensional embedding Y and the mapping matrix as(10)minωi,Y∑i=1nXiTWi+1jeiT−YiF2+γωiF2+μΦXTΦW+1nET−YF2+γΦWF2,s.t.YTY=Iwhere 1j∈Rjand 1n∈Rnare two vectors of all ones. According to the literature [20], y can be computed by(11)y=YTHKH+γI−1HKx+1nYT1n−1nYTHKH+γI−1HK1nwhereH=I−1n1n1nTdenotes as the global centering matrix, Kx∈Rndenotes as a vector with its i-th element Kxi=ϕ(x)Tϕ(xi)=exp(−‖x−xi‖2/σ2), and xi∈χ is the i-th RBSR feature descriptor in the training set. In order to avoid overfitting, we perform local PCA to reduce the dimension of each facial RBSR descriptor as preprocessing.In multi-task learning, given an extended training set of the RBSR descriptorsxikyiki=1mkcomputed by Eq. (11), xik∈Rndenotes the RBSR descriptor of the i-th training sample for the k-th application, yikdenotes the corresponding output, mkis the number of facial points. LetXk=x1k,⋅⋅⋅,xmkk∈Rmk×ndenote the data matrix for the k-th application, andYk=y1k,⋅⋅⋅,ymkk∈Rmkdenote the lower dimensional embedding as the corresponding label of the whole image. The regression vector for all regions from the regression matrix A=[a1,⋅⋅⋅,ak]∈Rn×kneeds to be estimated by the discussion below . Then, we can obtain the maximum posterior estimation for face recognition and expression classification by the following equation:(12)Ak*=argminAkPy|Ak,μ1Py|μ2∏j=1NPajk|δj∏k=12∏i=1miPyik|Ak,xik,σkwhere μ1 and μ2 are the parameters for specified application. We assume the corresponding target yk∈R for the k-th application has a Gaussian distribution with mean yk∈R and precision σk>0:(13)pyik|Ak,xik,σk=σk2πexp−σkyk−AkTxk2.Assume the prior probability p(Ak|δk) is generated according to the exponential prior: p(Ak|δk)∞exp(−‖Ak‖δk). Here, pairwise data x, y is drawn independently from the distribution (Eq. (12)) and A1,⋅⋅⋅,Anare drawn independently from the prior in Eq. (13). Then, the likelihood function and prior can be expressed as follows and the classification result is obtained based on the maximum probability:(14)py|A,X,δ=∏k=12∏i=1mkpyik|Ak,xik,σk,pA|δ=∏i=1npAk|δk.Then, we can obtain the optimal estimation of the regression matrix A⁎ as follows:(15)A*=argminApy|A,μ1py|μ2py|A,X,δpA|δ.Finally, the high-dimensional input RBSR descriptor X can be embedded into an intrinsic low-dimension feature vector by(16)Z=AX.Our method can effectively balance the discriminant power of face recognition and the descriptiveness of expression analysis and the extracted features can better reflect facial surface structure, which is immune to the distorted artifacts and noise corruptions.

@&#CONCLUSIONS@&#
In this paper, we have presented a novel framework for 3D face recognition and emotion analysis at the same time. We propose a multistage preprocessing method to segment a group of facial regions. Then, the RBSR descriptor is proposed to describe the different facial regions with different discrimination and descriptiveness. The RGRM model developed by local and global regression scheme is used to extract facial intrinsic manifold and realize the ability of facial discrimination and emotional descriptiveness. We present experimental results on the three largest standard 3D face databases, FRGC v2, BU-3DFE and CASIA. The performance of our proposed framework is with the properties of effectiveness, robustness and generality for 3D face recognition and emotion analysis.In practice, log probability is employed to avoid extremely small values with numerical precision [34] and the above equation can be expressed as follows:(A.1)logPRBSR|r=∑rbsri∈RBSRlogPli|r+∑rbsrj∈RBSRlogPlj|li,r+logPD,R|r,li,lj.To decrease the computational complexity, we assume P(li|r) and P(lj|li,r) as the uniform probabilities. Nonuniform label probability can be computed by concatenating the individual label histogram to the pairwise regional descriptor vector in the probability expression:(A.2)logPRBSR|r=∑rbsri∈RBSR∑rbsrj∈RBSRlogPD,R|r,li,lj+C.Since the uniform probabilities for labels approximately estimated by a constant C are not depended on the region r, we can omit for clarify. The above equation can be rewritten as to bin probability based on the individual descriptor label:(A.3)ωr=logPRBSR|r=∑l∈LlogPbl|rwherelogPbl|r=∑rbsri∈RBSR∑rbsrj∈RBSRlogPD,R|r,l,ljis the bin probability and ωris the weight of RBSR descriptor for the region r.