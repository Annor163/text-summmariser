@&#MAIN-TITLE@&#
Multi-objective design of state feedback controllers using reinforced quantum-behaved particle swarm optimization

@&#HIGHLIGHTS@&#
A multi-objective design approach for tuning LQR controllers is proposed.A new reinforced quantum-behaved PSO algorithm is introduced.Comparative studies among nine various techniques are conducted.

@&#KEYPHRASES@&#
Linear quadratic regulator (LQR),Quantum-behaved particle swarm optimization (QPSO),Multi-objective optimization (MOO),Optimal control,Swarm intelligence,

@&#ABSTRACT@&#
A novel and generic multi-objective design paradigm is proposed which utilizes quantum-behaved PSO (QPSO) for deciding the optimal configuration of the LQR controller for a given problem considering a set of competing objectives. There are three main contributions introduced in this paper as follows. (1) The standard QPSO algorithm is reinforced with an informed initialization scheme based on the simulated annealing algorithm and Gaussian neighborhood selection mechanism. (2) It is also augmented with a local search strategy which integrates the advantages of memetic algorithm into conventional QPSO. (3) An aggregated dynamic weighting criterion is introduced that dynamically combines the soft and hard constraints with control objectives to provide the designer with a set of Pareto optimal solutions and lets her to decide the target solution based on practical preferences. The proposed method is compared against a gradient-based method, seven meta-heuristics, and the trial-and-error method on two control benchmarks using sensitivity analysis and full factorial parameter selection and the results are validated using one-tailed T-test. The experimental results suggest that the proposed method outperforms opponent methods in terms of controller effort, measures associated with transient response and criteria related to steady-state.

@&#INTRODUCTION@&#
Optimal control theory refers to the controller design patterns that simultaneously satisfy the physical constraints of the controlled process and optimize some predetermined performance criteria. The evolution of optimal control theory has led to the emergence of linear quadratic regulators (LQR) – an optimal multivariable feedback control approach that improves the stability and minimizes the excursion in state trajectories of a system while requiring minimum controller effort. Applying LQR technique to a controllable linear time-invariant (LTI) system results in a set of optimal feedback gains that minimizes a quadratic criterion and stabilizes the system [1]. The LQR approach has been utilized in vast variety of real world engineering applications such as but not limited to missile guidance [2,3], flight control [4], multiple spacecraft formation [5], controlling unmanned vehicles [6], active car suspension [7], ABS break system [8], power converters [9,10], active power filter [11], and tuning PID controllers [12]. Essentially, LQR controllers minimize a quadratic cost function also known as performance index that consists of two penalty matrices including state (Q) and control (R) weighting matrices. These two parameters are main design parameters to be selected by the designer and greatly influence the behavior of the controller. It is worth noting that it is not a trivial task to decide these two matrices. In general, deciding the LQR parameters for a given process is a continuous, multimodal and multi-objective optimization problem. For the simplicity purposes, in most of the applications the problem is modeled as a single-objective which limits the designer to only one configuration. In the proposed approach, the problem is considered as a multi-objective problem and as a result the designer is provided with a set of Pareto optimal solutions which lets her to decide the target solution based on some practical preferences.Traditionally, weighting matrices are determined by the trial-and-error method in which a domain expert adjusts the weighting matrices intuitively and then refines them iteratively to obtain a satisfactory performance. This method is not feasible for high-dimensional systems and even for simpler systems is labor-intensive and time-consuming. Bryson's method [13] is another iterative method in which initial state and feedback variables are normalized with respect to their largest permissible and then are utilized to initialize the weighting matrices. Then, similar to trial-and-error method, the weighting matrices are gradually refined to approach the minimum index value. Pole placement [14] is another popular technique for determining the weighting matrices in which the matrices are decided based on the given poles. This approach neither guarantees a good performance nor the satisfaction of the constraints. Other approaches have been proposed as well such as utilizing asymptotic modal properties [15] and expressing the system as an explicit function of the weighting matrix elements [16]. Yet they suffer from the similar deficiencies. These classic approaches are labor-intensive, time-consuming and do not guarantee the expected performance. They only aim to minimize the quadratic performance index and ignore the other competing or incommensurable control objectives such as minimizing the overshoot, rise-time, settling-time, and the steady-state error.In order to tackle these problems, some studies have utilized soft computing techniques including but not limited to particle swarm optimization (PSO) [17], artificial bee colony (ABC) [18], ant colony optimization (ACO) [19], genetic algorithm (GA) [20], differential evolution (DE) [21], memetic algorithm (MA) [22], artificial immune systems (AIS) [23], imperialist competitive algorithm (ICA) [24], neural networks [25], and fuzzy systems [26]. These methods can explore the search hyperspace in an informed manner and converge to the optimal solutions in a few iterations using a combination of knowledge sharing and individual explorations. The main problem with most of the computational intelligence techniques is that they are prone to premature convergence which causes them to get trapped within the local optima.In this paper, a novel and generic multi-objective design paradigm is proposed which utilizes a global convergence guaranteed variation of particle swarm optimization (PSO) called quantum-behaved PSO (QPSO) for deciding the optimal configuration of the LQR controller for a given problem considering a set of competing objectives including the quadratic performance index, overshoot, rise-time, settling-time, steady-state error, and integrated absolute error. The proposed method is called reinforced multi-objective quantum-behaves PSO (RMO-QPSO). The rationale behind the selection of QPSO as the core optimizer is as follows. (1) The experimental studies suggest that PSO as the predecessor of QPSO outperforms GA, DE, MA, ACO, and ABC in terms of success rate, solution quality and processing time [27,28]. It is also experimentally shown that PSO is scalable, requires less computational resources, and its processing time grows at a linear rate with respect to the size of the problem [29]. (2) It is theoretically guaranteed that the QPSO converges to the global optimum; (3) it is less sensitive to the bias problem as it has only one parameter, and (3) it outperforms other PSO variations in finding the optimal solutions [30].There are three main contributions introduced in this paper as follows. (1) The standard QPSO algorithm is reinforced with an informed initialization based on the simulated annealing and Gaussian neighborhood selection mechanism. (2) It is also augmented with a local search strategy which integrates the advantages of memetic algorithm into conventional QPSO. (3) Finally, an aggregated dynamic weighting criterion is introduced that dynamically combines the soft and hard constraints with control objectives to provide the designer with a set of Pareto optimal solutions and lets her to decide the target solution based on practical preferences. As far as the authors’ knowledge is concerned, this is the first time that a multi-objective derivation of PSO is applied for deciding the configuration of LQR controllers.Without loss of generality, RMO-QPSO is utilized to decide the configuration of LQR controllers for two control benchmarks including: (1) stabilizing an inverted pendulum system, and (2) controlling a flight landing system. In order to have comparative studies, nine different techniques (i.e. one trial-and-error based method, one gradient based technique, and seven stochastic meta-heuristics) including the trail-and-error method, Levenberg–Marquardt optimization (LM), GA, DE, ABC, PSO, QPSO, chaotic PSO (CPSO), and adaptive inertia weighted PSO (AIWPSO) are utilized to decide the LQR parameters of the same benchmarks. In order to mitigate the bias problem and have a fair comparison among the different techniques, full factorial parameter selection and sensitivity analysis [31] are exploited for all of the applied techniques to find the best parameter setting including population size, iteration number, etc. The comparative results are also validated using one-tailed T-test to investigate whether the experimental results are statistically significant.The paper is organized as follows. Section 2 provides the mathematical foundation of LQR controllers. In Section 3 an overview of related works is presented. In Section 4, we investigate the concept of multi-objective QPSO. In Section 5, we present the proposed technique for optimal tuning of LQR controllers and in Section 6 we discuss the experimental results. Section 7 concludes the paper.The LQR is an optimal multivariable feedback control approach that improves the stability and minimizes the excursion in the state trajectories of a system while requiring minimum controller effort. From a Mathematical point of view, for a controllable LTI system with a state-space model shown in Eq. (1), the LQR approach constructs a linear state feedback law as depicted in Eq. (2):(1)x˙(t)=Ax(t)+Bu(t)y(t)=Cx(t)+Du(t)(2)u(t)=−Kx(t)In these equations, x(t) denotes an n-dimensional state vector, y(t) presents an r-dimensional output vector, and u(t) is an m-dimensional control vector. K∈ℜn×mis the optimal state-feedback gain matrix. The control law in Eq. (2) minimizes the quadratic performance index shown in Eq. (3) which integrates the state and control energies through the time. In other words, it minimizes the distance between the process outputs and the desired outputs with minimum control energy.(3)J=∫0∞(xTQx+uTRu)dtIn Eq. (3), Q∈ℜn×ndenotes a symmetric positive semi-definite state weighting (state penalty) matrix and R∈ℜm×mdenotes a symmetric positive definite control weighting (control penalty) matrix. The control gain matrix K is given by Eq. (4).(4)K=R−1BTPwhere P is a unique symmetric positive semi-definite solution to the algebraic Riccati equation shown in Eq. (5).(5)PA+ATP+Q−PBR−1BTP=0

@&#CONCLUSIONS@&#
