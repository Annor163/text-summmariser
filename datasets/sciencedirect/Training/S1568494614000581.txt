@&#MAIN-TITLE@&#
Differential evolution based on covariance matrix learning and bimodal distribution parameter setting

@&#HIGHLIGHTS@&#
Point out the drawbacks of the crossover operator and the parameter settings of differential evolution (DE).Propose a novel DE variant based on covariance matrix learning and bimodal distribution parameter setting, named CoBiDE.Verify the effectiveness of CoBiDE by many experiments.

@&#KEYPHRASES@&#
Differential evolution,Global numerical and engineering optimization,Covariance matrix learning,Bimodal distribution parameter setting,

@&#ABSTRACT@&#
Differential evolution (DE) is an efficient and robust evolutionary algorithm, which has been widely applied to solve global optimization problems. As we know, crossover operator plays a very important role on the performance of DE. However, the commonly used crossover operators of DE are dependent mainly on the coordinate system and are not rotation-invariant processes. In this paper, covariance matrix learning is presented to establish an appropriate coordinate system for the crossover operator. By doing this, the dependence of DE on the coordinate system has been relieved to a certain extent, and the capability of DE to solve problems with high variable correlation has been enhanced. Moreover, bimodal distribution parameter setting is proposed for the control parameters of the mutation and crossover operators in this paper, with the aim of balancing the exploration and exploitation abilities of DE. By incorporating the covariance matrix learning and the bimodal distribution parameter setting into DE, this paper presents a novel DE variant, called CoBiDE. CoBiDE has been tested on 25 benchmark test functions, as well as a variety of real-world optimization problems taken from diverse fields including radar system, power systems, hydrothermal scheduling, spacecraft trajectory optimization, etc. The experimental results demonstrate the effectiveness of CoBiDE for global numerical and engineering optimization. Compared with other DE variants and other state-of-the-art evolutionary algorithms, CoBiDE shows overall better performance.

@&#INTRODUCTION@&#
Differential evolution (DE), proposed by Storn and Price [1,2] in 1995, has become a hotspot in the community of evolutionary computation. Similar to other evolutionary algorithms (EAs), DE is a population-based optimization algorithm. In DE, each individual in the population is called a target vector. DE produces a mutant vector by making use of the mutation operator, which perturbs a target vector using the difference vector of other individuals in the population. Afterward, the crossover operator is applied to the target vector and the mutant vector to generate a trial vector. Finally, the trial vector competes with its target vector for survival according to their objective function values. Due to some advantages, e.g., simple structure, ease of implementation, and fast convergence speed, DE has been widely applied to some fields of science and engineering, such as cluster analysis [3], robot control [4], controller design [5], and graph theory [6].It is noteworthy that in DE, the crossover operator depends mainly on the coordinate system and the distribution information of the population is usually unreasonably ignored. Moreover, the crossover operator of DE can be considered as a discrete recombination [7], and thus, the interactions among variables have not been systematically studied. As a result, DE often loses its effectiveness and advantages when solving problems with high variable correlation.In addition, DE is sensitive to its two main control parameters: the scaling factor F and the crossover control parameter CR. These two control parameters have a significant impact on the performance of DE. Moreover, different control parameter settings show different characteristics [8]. For example, a larger F is effective for global search; however, a smaller F can accelerate the convergence. On the other hand, a larger CR results in higher diversity of the population, since the trial vector will inherit more information from the mutant vector. However, a smaller CR focuses on local exploitation since the target vector will contribute more information to the trial vector. Indeed, it is still an open issue to choose suitable settings of F and CR to balance the exploration and exploitation of DE during the evolution.Based on the above considerations, in this paper, we present a novel DE, referred as CoBiDE, including two main components: covariance matrix learning and bimodal distribution parameter setting. In CoBiDE, the covariance matrix learning establishes a coordinate system according to the current population distribution, and then the crossover operator is applied according to the coordinate system thus built to generate the trial vector. Furthermore, in CoBiDE, both F and CR are produced according to a bimodal distribution composed of two Cauchy distributions, the aim of which is to balance the global exploration and the local exploitation during the evolution. CoBiDE has been tested on 25 benchmark test functions developed for the 2005 IEEE Congress on Evolutionary Computation (IEEE CEC2005) [9], as well as a variety of real-world application problems [10]. The experimental results suggest that the performance of CoBiDE is better than that of four other DE variants and three other state-of-the-art EAs.The remainder of this paper is organized as follows. Section 2 briefly introduces DE and its operators. Section 3 reviews the related work and four main research directions of DE. Then, CoBiDE is presented in Section 4. The experimental results are given in Section 5. Section 6 concludes this paper.DE is a population-based heuristic search algorithm. Similar to other EAs, DE contains three basic operators: mutation, crossover, and selection. Firstly, DE produces an initial population by randomly sampling several points (each point is called a target vector) from the search space:(1)P0={x→i,0=(xi,1,0,xi,2,0,…,xi,D,0),i=1,2,…,NP}where NP denotes the population size and D denotes the number of variables.At each generation G, a mutant vectorv→i,G=(vi,1,G,vi,2,G,…,vi,D,G)(i∈1,2,…,NP) is produced by the mutation operator for each target vectorx→i,G. Afterward, the crossover operator is implemented on the mutant vector and the target vector to generate a trial vectoru→i,G=(ui,1,G,ui,2,G,…,ui,D,G)(i∈1,2,…,NP). The crossover operator and the mutation operator together are called trial vector generation strategy. The selection operator of DE is based on a one-to-one competition between the target vector and the trial vector.Next, the mutation, crossover, and selection operators are introduced.The commonly used mutation operator can be formulated as follows:(2)v→i,G=x→r1,G+F⋅(x→r2,G−x→r3,G)where r1, r2, and r3 are mutually different integers randomly chosen from [1, NP] and also different from i, and F is the scaling factor.The crossover operator combines the mutant vectorv→i,Gwith the target vectorx→i,Gto generate a trial vectoru→i,G:(3)ui,j,G=vi,j,G,ifrandj(0,1)≤CRorj=jrandxi,j,G,otherwisewhere jrandis a random integer between 1 and D, resulting in the trial vector being different from the target vector by at least one dimension, randj(0, 1) is a uniformly distributed random number between 0 and 1, and CR is the crossover control parameter.Based on Eq. (3), it is clear that the trial vector is a vertex of the hyper-rectangle defined by the mutant and target vectors [11]. Moreover, since the information of the trial vector is provided by the mutant vector or the target vector, the crossover operator is dependent on the coordinate system.The selection operator of DE adopts a one-to-one competition between the target vectorx→i,Gand the trial vectoru→i,G. If the objective function value of the trial vector is less than or equal to that of the target vector, then the trial vector will survive into the next generation, otherwise, the target vector will enter the next generation:(4)x→i,G+1=u→i,G,iff(u→i,G)≤f(x→i,G)x→i,G,otherwiseDuring the past fifteen years, DE has attracted much attention by the researchers [12]. The current studies of DE mainly focus on the following four aspects: (1) improving the trial vector generation strategy, (2) adapting the control parameter setting, (3) hybridizing with other techniques, and (4) integrating multiple trial vector generation strategies with multiple control parameter settings.Fan and Lampinen [13] proposed a trigonometric mutation operator and embedded it into DE to design a new method called TDE. In TDE, a probability parameter Mtis utilized to balance the trigonometric mutation operator and the original mutation operator of DE. The trigonometric mutation can be considered as a local search operator, which is able to enhance the convergence velocity of DE. The performance of TDE has been evaluated on two test functions and two practical problems.Zhang and Sanderson [14] presented an improved current-to-best/1 operator, called current-to-pbest/1, which can be formulated as follows:(5)v→i,G=x→i,G+Fi⋅(x→best,Gp−x→i,G)+Fi⋅(x→r1,G−x→′r2,G),i∈{1,2,…,NP}wherex→best,Gpis randomly chosen from the best 100p% individuals in the current population, and p is chosen from (0, 1]. Moreover, the previously generated offspring, which cannot survive into the next population, have been stored into a predefined archive. The individualx→′r2,Gin Eq. (5) is randomly chosen from the union of the archive and the current population. As analyzed in [14], the advantages of the current-to-pbest/1 operator are twofold: (1) the information of multiple best individuals can balance the greediness of the mutation and the diversity of the population, and (2) the difference between the recently explored inferior individuals and the current population may represent promising directions toward the global optimum.Das et al. [15] proposed a neighborhood-based mutation operator, which contains two parts: global neighborhood-based mutation and local neighborhood-based mutation. In the method proposed by Das et al. [15], two trial vectors are produced by the global and local neighborhood-based mutation. Moreover, these two trial vectors are combined to form the actual trial vector by using a weight factor. Clearly, the main aim of the neighborhood-based mutation operator is to balance the exploration and exploitation abilities of DE. This mutation operator has been tested on 24 benchmark test functions and two real-world problems and shown very competitive results.After recognizing that the trial vector generated by the crossover operator is just a vertex of the hyper-rectangle defined by the mutant and target vectors, Wang et al. [11] employed orthogonal crossover [16] to make a systematic and rational search in the hyper-rectangle defined by the mutant and target vectors, and proposed a generic framework to enhance the search ability of DE. The experimental results have demonstrated that this framework can be used to improve the performance of different variants of DE.Liu and Lampinen [17] designed a fuzzy adaptive DE (FADE) based on fuzzy logic controller. In FADE, the mean square roots of differences of the objective function values and the population members during the successive generations are treated as the inputs of the fuzzy logic controller, and the outputs are the values of F and CR. The experimental results have shown that FADE outperforms the classic DE on problems with high dimensionality. The main weakness of FADE lies in its complicated implementation due to fuzzy adapting.Brest [18] proposed a DE with self-adaptive parameter control (jDE). In jDE, the control parameters F and CR are encoded into the chromosome and participate in the evolution. Each individual in the population is assigned an initial control parameter setting: Fi=0.5 and CRi=0.9 (i=1,2,…,NP). During the evolution, jDE regenerates Fiand CRiaccording to the uniform random distributions U(0.1, 0.9) and U(0, 1) with probabilitiesτ1andτ2, respectively. One of the main advantages of jDE is that its implementation is very simple. In [18], 21 test functions have been used to assess the performance of jDE.In JADE proposed by Zhang and Sanderson [14], for each target vector, the scaling factor F is generated by the Cauchy distribution C(μF, 0.1), and the crossover control parameter CR obeys the normal distribution N(μCR, 0.1). In addition, JADE uses the following equations to update μFand μCR:(6)μF=(1−c)⋅μF+c⋅meanL(SF)(7)μCR=(1−c)⋅μCR+c⋅meanA(SCR)where c controls the rate of parameter adaptation, SFand SCRare the sets of all successful scaling factor F and crossover control parameter CR at each generation, respectively, and meanA(·) and meanL(·) are the usual arithmetic mean and the Lehmer mean, respectively. The above parameter adaptation has the capability to adapt parameters to appropriate values, and thus, improves the robustness of DE.Noman and Iba [7] proposed a crossover-based adaptive local search operator to enhance the convergence rate of DE. In this method, simplex crossover [19] is applied to the best individual and two other individuals of the population at each generation before implementing DE. This method does not add any additional complexity or any additional parameter. Moreover, it exhibits a higher convergence velocity compared with the original DE.Opposition-based DE (ODE) is proposed by Rahnamayan et al. [20], which employs opposition-based learning to generate the initial population and new solutions. The experimental results suggest that opposition-based learning is a very effective way to speed up the convergence of DE. Concretely, ODE is on average 44% faster than the original DE on 58 test functions.Sun et al. [21] combined DE with estimation of distribution algorithm (EDA), and proposed DE/EDA. In DE/EDA, one part of the trial vector is generated in the DE way, and the other part of the trial vector is sampled from the constructed probability distribution model. As a result, DE/EDA can not only utilize the global statistical information derived from EDA, but also use the differential information provided by DE.Recently, some researchers investigated the idea of integrating multiple trial vector generation strategies with multiple control parameter settings in DE. The main motivation is that different strategies along with different parameter settings may be suitable to different problems [8].Qing et al. [8] proposed a self-adaptive DE (SaDE), in which both trial vector generation strategies and control parameter settings are self-adapted according to the previous information. SaDE establishes a strategy candidate pool which contains four trial vector generation strategies. At each generation, one trial vector generation strategy is chosen for one individual. In addition, SaDE assigns different control parameter settings for different individuals. SaDE has been used to solve a suite of 26 test functions and the experimental results are very promising.Mallipeddi et al. [22] proposed a DE with ensemble of control parameter settings and trial vector generation strategies (EPSDE). EPSDE involves a pool of distinct trial vector generation strategies and a pool of values for each control parameter. During the evolution, a trial vector generation strategy and a control parameter setting are chosen based on their success experience in the past generations to create a trial vector. As a result, the successful combination of strategy and parameter setting has a higher probability to produce the trial vector. Since the strategies and the parameter settings in a pool have distinct properties, EPSDE exhibit distinct performance characteristics during different stages of the evolution.During the past fifteen years, DE researchers have obtained some important experiences about choosing trial vector generation strategies and control parameter settings, which will be very useful for designing more effective DE. Motivated by the above consideration, Wang et al. [23] investigated whether the performance of DE can be improved by combining several trial vector generation strategies with several different control parameter settings, which exhibit different characterizes, and proposed a composite DE, named CoDE. CoDE combines three trial vector generation strategies with three control parameter settings in a random way to produce the trial vectors. The performance of CoDE has been evaluated on 25 benchmark test functions developed for IEEE CEC2005 [9].Gong et al. [24] used four trial vector generation strategies proposed in [14] to form the strategy candidate pool and designed two adaptive methods to choose a suitable trial vector generation strategy for a problem at hand. In addition, the parameter adaptation mechanism proposed by Gong et al. [24] is similar to that proposed in [14]. The experimental results on 20 test functions and two real-world problems have verified that the method proposed in [24] is able to adaptively determine a more suitable strategy for a specific problem.In this section, we propose a novel DE, named CoBiDE. CoBiDE contains two main components: covariance matrix learning and bimodal distribution parameter setting. Next, the implementation of the above two main components will be introduced in detail.As mentioned previously, the crossover operator of DE is dependent mainly on the coordinate system, and the distribution information of the population, which could reflect the landscape of the problem to a certain extent [12], is usually ignored during the evolution. Indeed, the statistical properties of the population (such as mean value, variance, and covariance) can be utilized to represent the distribution of the population. In particular, the covariance matrix composed of variance and covariance reflects the diversity of the population and the interactions among the variables. Hence, systemically utilizing the covariance matrix should be very useful for relaxing the dependence of DE on the coordinate system and loosing the interactions among the variables.Based on the above analysis, covariance matrix learning is proposed in this paper, the aim of which is to establish an Eigen coordinate system with loose variable correlation for the crossover operator. Fig. 1shows the differences between the crossover operator in the original coordinate system (Fig. 1(a)) and the crossover operator in the Eigen coordinate system (Fig. 1(b)) for a problem with variable correlation. Suppose that the Eigen coordinate system (i.e.,ox′1x′2) is obtained after analyzing the distribution of the population. From Fig. 1, it is clear that crossover in the Eigen coordinate system is more promising to find the global optimum, since the trial vectors generated by the crossover in the Eigen coordinate system may be more close to the global optimum than the trial vectors created by the crossover in the original coordinate system.In this paper, the covariance matrix learning includes two core techniques: Eigen decomposition of the covariance matrix and the coordinate transformation. The purpose of the former is to obtain Eigen vectors which can serve as the axial orientations of the Eigen coordinate system. In addition, the latter transforms the trial vectors into the original coordinate system, after implementing the crossover operator according to the Eigen coordinate system. The procedure of the covariance matrix learning is introduced as follows.Step 1. Compute the covariance matrix C of the top ps·NP individuals in the current population, and apply Eigen decomposition to C as follows:(8)C=BD2BTwhere B and BTare orthogonal matrices and D is a diagonal matrix composed of Eigen values. Note that each column of B is an Eigen vector of the covariance matrix C.Step 2. Update the target vector and the mutant vector in the Eigen coordinate system by making use of BT:(9)x→′i,G=B−1x→i,G=BTx→i,G(10)v→′i,G=B−1v→i,G=BTv→i,GStep 3. Apply the crossover operator tox→′i,Gandv→′i,G, and create a trial vectoru→′i,Gin the Eigen coordinate system:(11)u′i,j,G=v′i,j,G,ifrandj(0,1)≤CRorj=jrandx′i,j,G,otherwiseStep 4. Transformu→′i,Ginto the original coordinate system by taking advantage of B:(12)u→i,G=Bu→′i,Gwhereu→i,Gis the trial vector in the original coordinate system.During the evolution, due to the randomness of the distribution of the population, if all the individuals in the population are used to compute the covariance matrix, the covariance matrix will be disturbed by such randomness and, as a result, the Eigen coordinate system constructed may not be quite reasonable. Therefore, in Step 1, we use the ps·NP individuals with the minimum objective function values in the population to compute the covariance matrix, where ps is in the interval [0, 1].Remark 1CMA-ES [25], BLXPCA [26], and BLXICA [26] have a similar motivation to use statistical information based on the covariance matrix. However, there are some differences between CoBiDE and them. In CoBiDE, the parameter ps is introduced to compute the covariance matrix of the top ps·NP individuals in the current population, while CMA-ES adopts a weighted method to compute the covariance matrix. In addition, all the individuals in the population are used to compute the covariance matrix in BLXPCA and BLXICA. On the other hand, CMA-ES, BLXPCA, and BLXICA use Eigen values and Eigen vectors obtained by the Eigen decomposition of the covariance matrix simultaneously. However, because of the properties of crossover in DE, the proposed approach only uses Eigen vectors to construct an appropriate coordinate system, and Eigen values are not used.Recently, several methods which hybridize DE with CMA-ES [25] have been proposed. For example, DE performs the global exploration and CMA-ES is used as a local search engine in [27]. In [28], CMA-ES and a hybrid DE are executed serially. Moreover, two populations are utilized, one for CMA-ES and the other for the hybrid DE. LaTorre et al. [29] presented a multiple offspring sampling framework to combine a restart CMA-ES [30] with DE. In this framework, the average fitness increment is adopted as a quality function to update the participation ratios of the restart CMA-ES and DE. There are two major differences between CoBiDE and the above three methods. Firstly, DE is not coupled with CMA-ES in CoBiDE. Indeed, CoBiDE only exploits the statistical information provided by the covariance matrix of the population. Secondly, in CoBiDE the statistical information provided by the covariance matrix is embedded into DE to strengthen the crossover operator. However, in the above three methods, CMA-ES is independent of DE.In this subsection, bimodal distribution parameter setting is proposed for the scaling factor F and the crossover control parameter CR. It is necessary to emphasize that the proposed bimodal distribution parameter setting is inspired by [14] and [23]. In addition, like [18], the parameters F and CR are encoded into each target vectorx→i,G, i.e.,Fi,GandCRi,Gcorrespond to eachx→i,G. Moreover, if the trial vectoru→i,Gcan successfully enter the next population, thenFi,G+1=Fi,GandCRi,G+1=CRi,G;otherwise,Fi,G+1andCRi,G+1are generated for the next generation according to the bimodal distribution parameter setting.The bimodal distribution forFi,G(i∈{1,…,NP}) is composed of two Cauchy distributions as follows:(13)Fi,G=randci(0.65,0.1),ifrand(0,1)<0.5randci(1.0,0.1),otherwisewhere rand(0, 1) is a uniformly distributed random number between 0 and 1, and randci(a, b) is a random number obeying a Cauchy distribution with location parameter a and scale parameter b. If the value ofFi,Gis larger than 1.0, then is truncated to 1.0; and if the value ofFi,Gis less than 0.0, then is regenerated according to Eq. (13).The crossover control parameterCRi,G(i∈{1,…,NP}) is generated using the bimodal distribution composed of two Cauchy distributions as follows:(14)CRi=randci(0.1,0.1),ifrand(0,1)<0.5randci(0.95,0.1),otherwisewhere rand(0, 1) is a uniformly distributed random number between 0 and 1, and randci(a,b) is a random number obeying a Cauchy distribution with location parameter a and scale parameter b. If the value ofCRi,Gis larger than 1.0, then is truncated to 1.0; and if the value ofCRi,Gis less than 0.0, then is truncated to 0.0.The scaling factor F has the capability to control the search range of the mutation operator. In CoBiDE, two Cauchy distributions with the same probability (i.e., 0.5) are used for the setting of F. It is necessary to note that, Cauchy distribution with a higher location parameter (i.e., 1.0) tends to produce a bigger value for F which emphasizes the global exploration; however, Cauchy distribution with a relatively lower location parameter (i.e., 0.65) aims at producing a slightly smaller value for F, which focuses on the local exploitation.On the other hand, two Cauchy distributions with the same probability (i.e., 0.5) are designed for the setting of CR. The Cauchy distribution with a bigger location parameter (i.e., 0.95) means that the trial vector may inherit more information from the mutant vector, which encourages the diversity of the population and the exploration. On the contrary, the Cauchy distribution with a smaller location parameter (i.e., 0.1) signifies that the trial vector may be quite similar to the target vector. In this case, the search will put emphasis on the neighbor of the parent population, which can accelerate the convergence.Based on the above analysis, the use of Eqs. (13) and (14) is able to achieve an effective tradeoff between the exploration and exploitation. In addition, the scale parameter in both Eqs. (13) and (14) is set to 0.1, which results in the values of F and CR being located in the relatively small neighborhood of the location parameter with a higher probability.By combining the covariance matrix learning with the bimodal distribution parameter setting, CoBiDE is presented. The pseudocode of CoBiDE has been shown in Fig. 2.At each generation, for each target vectorx→i,G, a mutant vectorv→i,Gis generated by making use of the mutation operator (i.e., Eq. (2)). Afterward, if the predefined parameter pb is larger than a random number between 0 and 1, the crossover operator according to the covariance matrix learning is utilized to produce a trial vectoru→i,G, otherwise, the crossover operator according to the original coordinate system is exploited to produce a trial vectoru→i,G. Moreover, during the evolution, each target vector has its control parameter setting and the control parameter setting is dynamically adapted based on Eqs. (13) and (14).In the above procedure, we use the parameter pb to adjust the effect of the covariance matrix learning on the performance. The main reason is the following. Although the covariance matrix learning is an effective way to alleviate the dependence of DE on the coordinate system and the interactions among the variables, it is a relatively deterministic and greedy mechanism due to the use of some best individuals of the population to compute the covariance matrix, and as a result, the performance of the algorithm might degrade for some complex problems. Note that the crossover operator according to the original coordinate system has no bias to any special search directions. Consequently, the crossover operator is implemented in the original coordinate system with a probability (1-pb) to encourage the diversity of the population. With respect to CoBiDE, combining these two kinds of crossover can achieve a good tradeoff between diversity and convergence.CoBiDE was tested on 25 benchmark test functions developed for IEEE CEC2005 [9]. These 25 benchmark test functions can be divided into four classes:(1)Unimodal functions F1–F5.Basic multimodal functions F6–F12.Expanded multimodal functions F13–F14.Hybrid composition functions F15–F25.Among the above test functions, F1 and F9 are separable functions and the others are non-separable functions. Some test functions are rotated using orthogonal matrices to make variables correlated with each other, and the global optima of some test functions are shifted so as to not at the center of the search space. Moreover, F4 and F17 are used to test the robustness of the algorithm on noise. F15–F25 are hybrid composition functions which are composed of 10 sub- functions. The details of these 25 benchmark test functions have been given in [9].In our experiments, the dimension (D) of each test function was set to 30 and each test function was independently run 25 times with 300,000 function evaluations (FES) as the termination criterion. All the experiments are performed on a computer with 2.4GHz Dual-core Processor and 4.0 GB of RAM in Windows XP. The population size NP in CoBiDE was set to 60, pb=0.4, and ps=0.5.In this section, the mean and standard deviation of the function error value (f(x→)−f(x→*)) were calculated over 25 independent runs for each test function, wherex→is the best solution in the population when the algorithm terminates andx→*is the global optimal solution. Wilcoxon's rank sum test at a 0.05 significance level was performed to test the statistical significance of the experimental results between two algorithms.CoBiDE was compared with four other DE variants: JADE [14], jDE [18], SaDE [8], and CoDE [23]. These four algorithms have been briefly introduced in Section 3. JADE and jDE adopt self-adaptive parameter setting, and SaDE uses the normal distribution N(0.5, 0.3) to produce the scaling factor F and adjusts the crossover control parameter CR in a self-adaptive way. For the above four algorithms, we used the same parameter settings as given in their original papers. The experimental results of CoBiDE and other four algorithms are summarized in Table 1. It is necessary to emphasize that the experimental results of JADE, jDE, SaDE, and CoDE were directly taken from [23] to ensure the comparison fair.Table 1 also records the Cohen's d effect size [31] (within parentheses), which is a simple measure for quantifying the difference between two groups of data. The Cohen's d effect size is independent of the sample size. In general, we call a “small” effect if an effect size is between 0.2 and 0.3, a “medium” effect if an effect size is around 0.5, and a “large” effect if an effect size is from 0.8 to infinity [31]. Concretely, for F3 the effect size is equal to 1.63 when comparing JADE with CoBiDE, which means that the performance difference between JADE and CoBiDE is large and that JADE exhibits performance improvement. In contrast, for F11 the effect size is equal to −10.36 when comparing JADE with CoBiDE, which means that the performance difference between JADE and CoBiDE is also large and that JADE shows performance deterioration. It is necessary to note that for some test functions, the differences between both the mean and the standard deviation are equal to 0 when comparing CoBiDE with another algorithm and, as a result, the corresponding effect size is denoted as NaN in Table 1.The last three lines of Table 1 summarize the experimental results:(1)Unimodal functions F1–F5: JADE exhibits the best performance on five unimodal functions among the five algorithms. Evidently, the greedy mutation operator, i.e., current-to-pbest/1, results in the fast convergence speed and high convergence precision of JADE under these conditions. CoBiDE is outperformed by JADE on four test functions and surpasses jDE, SaDE, and CoDE on four, four, and three test functions, respectively. jDE and SaDE cannot show better performance than CoBiDE on any test functions and CoDE performs better than CoBiDE on only one test function. Therefore, the performance of CoBiDE is the second best in terms of these five unimodal functions.Basic multimodal functions F6–F12: Clearly, CoBiDE has the best performance on this kind of test functions. CoBiDE has an edge over JADE, jDE, SaDE, and CoDE on five, six, five, and three test functions, respectively. JADE and CoDE are statistically better than CoBiDE on one test function, and jDE and SaDE cannot outperform CoBiDE on any test functions. The outstanding performance of CoBiDE can be attributed to its capability to balance the exploration and exploitation.Expanded multimodal functions F13–F14: The mean function error values of all the algorithms are of the same order of magnitude on F13 and F14. JADE and CoDE are statistically better than CoBiDE. CoBiDE exhibits the similar performance with jDE. In addition, CoBiDE outperforms SaDE on these two test functions.Hybrid composition functions F15–F25: The solution of these 11 test functions is much more difficult than that of other test functions. For these 11 test functions, the results provided by the five algorithms are far way from the global optima. However, from Table 1, we can still observe that the performance of CoBiDE is superior to that of the other four algorithms according to the Wilcoxon's rank sum test.According to the last three lines of Table 1, overall CoBiDE is the best among the five algorithms. For five unimodal functions, CoBiDE is ranked the second, and for basic multimodal functions and hybrid composition functions, CoBiDE is more reliable than others. The superior performance of CoBiDE stems from two aspects: (1) the bimodal distribution parameter setting is capable of motivating the population toward promising directions, and (2) the covariance matrix learning is able to accelerate the convergence by exploiting the information provided by some potential individuals.In addition, we also performed the multiple-problem Wilcoxon's test [32] to check the behaviors of the above five algorithms. It is necessary to emphasize that the multiple-problem Wilcoxon's test was accomplished in this paper by using the KEEL software [33]. Table 2summarizes the statistical analysis results. From Table 2, we can see that CoBiDE provides higher R+ values than R− values in all the cases. According to the Wilcoxon's test at α=0.05, the significant differences can be observed in two cases (i.e., CoBiDE vs jDE and CoBiDE vs SaDE). When α=0.1, the significant differences can be observed in three cases (i.e., CoBiDE vs jDE, CoBiDE vs SaDE, and CoBiDE vs CoDE), which means that CoBiDE is significantly better than jDE, SaDE, and CoDE on 25 test functions at α=0.1.To further detect the significant differences between CoBiDE and the four competitors, the Friedman's test was carried out, in which Bonferroni–Dunn's procedure was used as a post hoc procedure. Again, the Friedman's test was implemented based on the KEEL software [33]. Table 3summarizes the ranking of the five algorithms obtained by the Friedman's test. As shown in Table 3, CoBiDE has the best ranking among the five algorithms on 25 test functions.Since all the five compared algorithms are the DE variants, one may be interested in the execution time of them on different test functions. To this end, we recorded the average runtime of each algorithm on each test function over 25 independent runs in Table 4. In order to compare the average runtime, we used the acceleration rate (AR). For each test function, AR is equal to the average runtime of CoBiDE divided by the average runtime of another algorithm. AR>1 and AR<1 mean that CoBiDE is faster and slower than another corresponding algorithm, respectively. The last row of Table 4 gives the average AR values. According to the average AR values, it is evident that JADE and jDE are faster than CoBiDE. In contrast, SaDE and CoDE are slower than CoBiDE. Moreover, based on our observation, the average AR values are 0.69 and 0.53 for 12 test functions (i.e., F1–F10, F13, and F14) when comparing CoBiDE with JADE and jDE, respectively. For these 12 test functions, the computational cost of the function evaluation is relatively cheap, and thus, the computing of the covariance matrix leads to the additional burden of the runtime of CoBiDE. However, for the other 13 test functions (i.e., F11–F12 and F15–F25), the average AR values are 0.89 and 0.88 when comparing CoBiDE with JADE and jDE, respectively. For these 13 test functions, since the function evaluation is time-consuming, the overhead of computing the covariance matrix in CoBiDE seems to be trivial. Under these conditions, CoBiDE, JADE, and jDE have the similar average runtime. It is necessary to point out that we directly run the codes of the other four algorithms provided by the developers and the programming techniques of the developers also have a significant effect on the runtime.The evolution of the mean function error values of the five algorithms in some typical test functions has been shown in Figs. 3 and 4.CoBiDE was also compared with three other EAs: CLPSO [34], CMA-ES [25], and GL-25 [35]. CLSPO, proposed by Liang et al., is an improved version of particle swarm optimization (PSO). In CLPSO, a novel learning strategy is proposed, in which all other particles’ historical best information is used to update a particle's velocity. CMA-ES, proposed by Hansen and Ostermeier, is an evolution strategy (ES) based on completely derandomized self-adaptation. GL-25, proposed by Garcia-Martinez et al., is a global and local real-coded genetic algorithm (GA) based on parent-centric crossover operators. The reasons of the selection of these three algorithms in comparison are twofold: (1) CLPSO, CMA-ES, and GL-25 represent the state-of-the-art in PSO, ES, and GA, respectively. According to the Google Scholar Citation, as of December 20, 2013, the number of citations of CLPSO, CMA-ES, and GL-25 is 1011, 1332, and 81 respectively, and (2) their performance is very competitive. Table 5summarizes the experimental results of CoBiDE and the above three algorithms. The parameter settings of CLPSO, CMA-ES, and CLPSO were the same as in their original papers and the experimental result of them were directly taken from [23] to make the comparison fair.From Table 5, it is evident that, overall, CoBiDE is the best among the four compared algorithms in a statistically significant fashion. Specifically, CoBiDE outperforms CLPSO on 19 test functions and is worse than CLPSO on three test functions. CMA-ES surpasses CoBiDE on three unimodal functions; however, CoBiDE is significantly better than CMA-ES on three other types of test functions. Compared with GL-25, CoBiDE shows better and worse performance on 23 test functions and one test function, respectively.In addition, some interesting phenomena can be observed according to the experimental results in Table 5. For separable functions (i.e., F1 and F9), the performance of CLPSO is significantly better than that of the other algorithms except for CoBiDE. Moreover, CLPSO also outperforms the other algorithms on F15 which is separable near the global optimum [9]. The superiority of CLPSO in separable functions is mainly due to the dimension-wise updating rules for velocity and position in PSO. CMA-ES performs quite well on some unimodal functions, which means the convergence speed of CMA-ES is very fast. Moreover, CMA-ES outperforms the other algorithms on test functions with high condition numbers, i.e., F3 and F22. It is because CMA-ES has the capability to adapt the population distribution according to the landscape of test functions. However, the performance of CMA-ES is not good when solving some multimodal functions, especially for test functions with noise, i.e., F4 and F17. Therefore, we can conclude that CMA-ES is sensitive to the noise. In contrast, CoBiDE shows the best performance on test functions with noise.Tables 6 and 7also present the statistical analysis results according to the multiple-problem Wilcoxon's test and the Friedman's test, respectively. It can be seen from Table 6 that CoBiDE obtains higher R+ values than R− values in all the cases. Furthermore, the p values of all the cases are less than 0.05. On the other hand, the experimental results in Table 7 indicate that CoBiDE has the best ranking among the four compared algorithms. In summary, the above comparison clearly demonstrates that CoBiDE is significantly better than the three competitors.As mentioned previously, CoBiDE includes two main components: the covariance matrix learning and the bimodal distribution parameter setting. The aim of this subsection is to verify the effectiveness of the above two components. To this end, two additional experiments were executed for 25 benchmark test functions. In the first experiment, CoBiDE only adopts the covariance matrix learning and the bimodal distribution parameter setting is not used (denoted as CoBiDE-1). In this case, like [18,20], F and CR were fixed to 0.5 and 0.9 during the evolution, respectively. In addition, in the second experiment, CoBiDE only adopts the bimodal distribution parameter setting and the covariance matrix learning is ignored (denoted as CoBiDE-2). It is necessary to note that for CoBiDE-2, Steps 11–15 and Step 19 in Fig. 3 can be eliminated and only the crossover operator of the original DE (i.e., Eq. (3)) is employed.For each test function, 25 independent runs were implemented and the maximum number of FES was set to 300,000. The experimental results of CoBiDE-1, CoBiDE-2, and CoBiDE have been shown in Table 8.From Table 8, CoBiDE surpasses CoBiDE-1 on 18 test functions. We attribute the above phenomenon to the fact that the fixed parameter setting could not adjust the search behavior to suit different landscapes, and that the bimodal distribution parameter setting is more effective to balance the exploration and exploitation during the evolution. In addition, CoBiDE-1 outperforms CoBiDE on three test functions (i.e., F4, F13, and F19). According to our further experiments, we found out that the parameter setting of F=0.5 and CR=0.9 provides best or near-best performance for these three test functions, which means that the above parameter setting happens to be very suitable for these three test functions.In addition, compared with CoBiDE, CoBiDE-2 shows worse performance on 13 test functions, and cannot show better performance on any test functions. It is not difficult to understand, since the covariance matrix learning is not dependent on the coordinate system when implementing the crossover operator. As a result, it has the capability to adapt the search according to different landscapes. Moreover, once some potential regions have been located, it can accelerate the convergence speed and enhance the convergence accuracy of the population for different kinds of test functions, due to the use of the population information to construct more suitable coordinate system. It is also interesting to note that for 12 test functions (F1, F8–F10, F12–F13, F15, F20–F23, and F25), the performance differences between CoBiDE and CoBiDE-2 are marginal. These 12 test functions can be divided into two categories: F1 and F9 belong to the first category, and the remaining 10 test functions belong to the second category. For F1 and F9, both CoBiDE and CoBiDE-2 can consistently reach the global optimum, and thus, the performance differences between CoBiDE and CoBiDE-2 are not significant. In addition, since CoBiDE might be easily trapped into a local optimum and the covariance matrix learning cannot help the population jump out of the local optimum, the insignificant performance differences occur for CoBiDE and CoBiDE-2 on the remaining 10 test functions.From Table 8, we can conclude that the above two components can benefit each other to enhance the performance of DE. Indeed, the bimodal distribution parameter setting achieves high reliability and the covariance matrix learning results in fast convergence.CoBiDE contains two parameters pb and ps. The former controls the computational resource assigned to the covariance matrix learning and the latter controls the number of individuals for computing the covariance matrix.In order to investigate the sensitivity of the above two parameters, we tested CoBiDE with different pb: 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, and 1, and different ps: 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, and 1.0. Seven test functions (i.e., F3, F4, F6, F9, F10, F13, and F14) were selected to test the performance of CoBiDE with different combinations of pb and ps. These test functions involve shifted problems, problems with noise, rotated problems, and high conditioned problems. The dimension was set to 30 for all the test functions and the maximum number of FES was set to 300,000. Fig. 5shows the average function error values of CoBiDE with different combinations of pb and ps.Generally speaking, a larger value of pb may discourage the diversity of the population, however, if the value of pb is too small, the covariance matrix learning cannot play its role in solving problems with high variable correlation. On the other hand, if ps is set to a larger value, the randomness of the population may cause side effect on the computation of the covariance matrix. However, if the value of ps is too small, the chosen individuals cannot reflect the statistical information of the population. Therefore, moderate values should be chosen for these two parameters in order to achieve competitive performance.From Fig. 5, we can observe that, actually, CoBiDE is not sensitive to these two parameters, and that pb and ps can be chosen from a relatively large range to achieve competitive performance for CoBiDE. In general, the value of pb is recommended in the interval [0.2, 0.7] and the value of ps is recommended in the interval [0.3, 0.7].Besides the above 25 benchmark test functions, eight real-world engineering optimization problems chosen from different fields including radar system, power systems, hydrothermal scheduling, spacecraft trajectory optimization, etc., were used to evaluate the performance of CoBiDE in this subsection. These eight real-world engineering optimization problems (denoted as P1–P8 in this paper) are problems T01, T06, T08, T10.1, T11.4, T12.1, T12.2, and T13 collected for the 2011 IEEE Congress on Evolutionary Computation (IEEE CEC2011) [10], respectively, which exhibit different complex characteristics and are very difficult to solve. For each problem, 25 independently runs were implemented with 150,000 FES as the termination criterion. The parameter settings of CoBiDE were the same with those for the 25 benchmark test functions, i.e., NP=60, pb=0.4, and ps=0.5. In addition, the parameter settings of JADE, jDE, SaDE, and CoDE were the same as in their original papers.Table 9summarizes the mean and standard deviation of the objective function values over 25 independent runs for each problem. In order to have statistically sound conclusions, Wilcoxon's rank sum test at a 0.05 significance level was conducted on the experimental results. From the experimental results shown in Table 9, we can see that JADE, jDE, SaDE, and CoDE cannot outperform CoBiDE on any problems, and that CoBiDE surpasses JADE, jDE, SaDE, and CoDE on four, eight, six, and five problems, respectively, which indicates that overall, CoBiDE performs significantly better than the four competitors on eight complex real-world engineering optimization problems.By making use of the KEEL software [33], the multiple- problem Wilcoxon's test and the Friedman's test have been implemented. The experimental results have been summarized in Tables 10 and 11. As shown in Table 10, CoBiDE shows higher R+ values than R− values in all the cases. Moreover, the p values less than 0.05 and 0.1 in three cases (i.e., CoBiDE vs JADE, CoBiDE vs jDE, and CoBiDE vs SaDE). In addition, CoBiDE has the best ranking according to Table 11.Therefore, the above experimental results verify the potential of CoBiDE in the real-world applications.

@&#CONCLUSIONS@&#
