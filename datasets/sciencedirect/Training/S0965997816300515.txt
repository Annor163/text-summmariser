@&#MAIN-TITLE@&#
A self-tuning system for dam behavior modeling based on evolving artificial neural networks

@&#HIGHLIGHTS@&#
HighlightWe proposed a self-tuning system for a dam behavior modeling.The system performs near real-time generation of the optimized ANN dam model.Optimized model is adapted to currently available measurements and input parameters.The system is based on artificial neural networks and genetic algorithm.Case study showed advantages and disadvantages of this system compared to MLR/GA.

@&#KEYPHRASES@&#
Hybrid dam models,Artificial neural networks,Evolving networks,Genetic algorithms,Multiple linear regression,Dam stability,

@&#ABSTRACT@&#
Most of the existing methods for dam behavior modeling presuppose temporal immutability of the modeled structure and require a persistent set of input parameters. In real-world applications, permanent structural changes and failures of measuring equipment can lead to a situation in which a selected model becomes unusable. Hence, the development of a system capable to automatically generate the most adequate dam model for a given situation is a necessity. In this paper, we present a self-tuning system for dam behavior modeling based on artificial neural networks (ANN) optimized for given conditions using genetic algorithms (GA). Throughout an evolutionary process, the system performs near real-time adjustment of ANN architecture according to currently active sensors and a present measurement dataset. The model was validated using the Grancarevo dam case study (at the Trebisnjica river located in the Republic of Srpska), where radial displacements of a point inside the dam structure have been modeled as a function of headwater, temperature, and ageing. The performance of the system was compared to the performance of an equivalent hybrid model based on multiple linear regression (MLR) and GA. The results of the analysis have shown that the ANN/GA hybrid can give rather better accuracy compared to the MLR/GA hybrid. On the other hand, the ANN/GA has shown higher computational demands and noticeable sensitivity to the temperature phase offset present at different geographical locations.

@&#INTRODUCTION@&#
To describe and predict the structural behavior of dams, a number of statistical, deterministic and hybrid mathematical models have been developed over the past decades. Statistical models based on multiple linear regression (MLR) and their advanced forms such as stepwise regression, robust regression, ridge regression and partial least squares regression have been shown to be more or less successful in dam modeling [1–3]. In contrast to statistical modeling, deterministic models require the solving of differential equations, for which closed form solutions could be difficult or impossible to obtain [4]. Therefore, many models that are based on numerical methods, such as the finite element method (FE), have also been developed [5]. Recently, numerical and statistical methods have been enriched with various heuristics from the artificial intelligence (AI) domain, creating hybrid models that combine their advantages.Some of these artificial intelligence techniques and heuristic algorithms are artificial neural networks (ANN) [6–10], genetic algorithms (GA) [11–13] and particle swarm optimization (PSO). In his paper [8], Mata presented a comparison between MLR and ANN models for the characterization of dam behavior under environmental loads for the Alto Rabagao arch dam. Gholizadeh et al. [10] used a hybrid methodology with a combination of metaheuristics (GA and PSO) and neural networks to propose an efficient soft computing approach to achieve optimal shape design of arch dams that were subjected to natural frequency constraints. Gomes et al. [14] employed PSO for structural truss mass optimization on size and shape, considering frequency constraints. The results showed that the PSO algorithm performed similarly to other methods and even better in some cases. Several recent studies have also described the application of artificial immune algorithm (AIA) techniques, which imitate the function of a natural immune system [15,16]. Xi et al. [15] proposed an immune statistical model to resolve the data analysis problems of dam horizontal crest upstream-downstream displacements.In a number of papers [17–22] researchers have made a significant effort to find optimal structures of neural networks for various problems. Majdi et al. [17] combined a neural network and genetic algorithm for predicting the deformation modulus of rock masses. GA is utilized to find the optimal number of neurons in a hidden layer, and the learning rates and momentum coefficients of hidden and output layers of the network. Using a standard backpropagation gradient descent algorithm, they tested networks with linear and sigmoid activation functions. Zhou et al. [19] presented a combined procedure of the orthogonal design (OD), FE analysis, ANN and GA for inverse modeling of the seepage/leakage problems. The chosen neural network used the sigmoid transfer function and had a fixed number of layers: one input layer, two hidden layers and one output layer. The number of neurons at the hidden layers was determined by minimizing an error function on a test dataset using a trial-and-error method. To obtain a quick training time and high generalization accuracy, the Levenberg–Marquardt backpropagation algorithm (LM) combined with Bayesian regularization is used for training of the network. In the study [18], a hybrid finite element–boundary element analysis (FE–BE) in conjunction with an ANN procedure is proposed for the prediction of dynamic characteristics of an existing concrete gravity dam. The conjugate gradient algorithm (CGA) and the LM algorithm are implemented for fast training of the ANNs. The authors tested neural networks with one hidden layer where the number of neurons was determined by a trial-and-error method. Hooshyaripor et al. [21] showed that a three-layer ANN model is appropriate to deal with a dam breach problem which has two inputs: the height of water behind the dam and the volume of water behind the dam at the failure time, and one output: the peak outflow discharge. In their study a feed-forward neural network model with a single hidden layer is used. Applying Hecht–Nielsen criterion [22], it was found that an ANN with four neurons in the hidden layer has higher performance. The LM algorithm was employed to train the ANN model. As a transfer function, tan-sigmoid and linear functions were employed in the hidden and output layers, respectively.In our previous work we developed an adaptive system for dam behavior modeling based on a linear regression model optimized for given conditions using genetic algorithms [23]. Throughout the evolutionary process, the system performs near real-time adjustment of regressors in the MLR model according to currently active sensors. Following this idea, we developed a system for dam behavior modeling based on artificial neural networks, capable of adapting to persistent changes in the measuring system and measurement database. In order to achieve a full adaptability of the ANN model in real-world conditions, the system should be able to optimize all significant network parameters according to available input sensors and historical data. To the best of our knowledge, the existing solutions optimize a limited set of ANN parameters only, while the other parameters are chosen arbitrarily, based on experience, literature or trial-error methods. In this paper, we present a novel methodology and a system for automatic generation of an ANN dam model, which optimizes all significant elements of the ANN architecture. Guided by a variant set of input variables, the system permanently optimizes network topology, activation functions and learning algorithms in order to fit the growing measurement database. Optimization of the parameters is performed using genetic algorithms. The quality of the proposed ANN/GA hybrid dam models has been tested on a real-world case study and compared to equivalent dam models based on multiple linear regression and GA (MLR/GA).The ANN is a simplified mathematical model of a natural neural network. It is a computing system made up of a number of simple, interconnected processing elements or neurons, which process information by a dynamic state response to external inputs [17]. Processing elements are grouped in layers: an input layer, one or more hidden layers and an output layer. The neurons (nodes) are interconnected by weighted links. A special class of ANN are feed-forward networks, which propagate a signal from the input to output layer [24]. A schematic view of a feed-forward network is given in Fig. 1, where X denotes a vector of predictor variables, Y is a vector of response variables, w(i, h)is a column-matrix of weighting coefficients between neurons in the input layer and the first hidden layer, and w(h, o)is a column-matrix of weighting coefficients between neurons in the last hidden layer and the output layer. The termwij(q−1,q)denotes the weight between ith neuron from(q−1)th hidden layer and jth neuron from (q)th hidden layer. In order to improve performance of the neural network, there is an extra neuron assigned to each hidden layer and the output layer, with the role of sending a constant signal x0 (bias) to all neurons in these layers.According to the sigma rule, the total inputαj(q)into processing element j in qth layer is a weighted sum of all outputsxi(q−1)from the previous layer. In the same manner, the input signals into neurons of the output layer are calculated as a function of outputs from the last hidden layer. When input signalαj(q)passes through a neuron, it is processed and transformed to the output signalxj(q)using an activation function. The activation function (AF) is necessary to transform the weighted sum of all signals hitting on a neuron so as to determine its firing intensity [17]. Some of the frequently used activation functions are: Gaussian, log, sigmoid, bipolar sigmoid, sine, hyperbolic tangent (TANH). Characteristics of various AFs are given in details in [25].The aim of the learning process is to set weights to the values for which the difference between desired and calculated values (error function) will be minimal. One of the most popular learning algorithms that minimizes error function is a backpropagation algorithm. According to the backpropagation algorithm, the error is propagated backward through the network and the weights are adjusted during a number of iterations. The procedure progresses until it reaches convergence of the calculated and expected outputs [17]. There are many variations of the backpropagation learning algorithm. In this work we used some of the best known: The Backpropagation gradient descent algorithm (BPGD) [24] and the Resilient propagation algorithm (RPROP) [26–29].Design of an ANN is specified by network architecture (such as the number of hidden layers and neurons, type of AFs, etc.) and learning rules. Both the architecture and learning rules are very important, thus good selection of these will give better performance of the network [17]. This task is still an unsolved issue and most researchers use a trial-and-error method to find a suitable number of hidden layers and nodes [22,30–32]. Different ideas have been stated about the required number of training data. The number of training samples has an influence on the quality of the model obtained. Researchers propose different sizes of training datasets, ranging from 60% to 80% of the available data [33–35]. The learning rules specify an initial set of weights, as well as the learning rate η and the momentum μ in the BPGD algorithm orη+andη−parameters in the RPROP algorithm. The range of initial weights commonly used in literature is[−0.5,0.5][36]. In order to reduce the impact of the stochastic nature of initial weights on the quality of the ANN model, Messer and Kittler proposed that the learning process should be repeated at least 5–10 times with different initial weights [17,37].Genetic algorithms are search techniques that are inspired by the theory of natural selection, in which strong species have a greater opportunity to survive and pass their genes on to future generations via reproduction [38,39]. GAs are probabilistic algorithms that maintain a population of individuals,P(t)=x1(t),…,xn(t), for the iteration (generation) t, where each individual represents a potential solution to a problem. Each solution xi(t) is evaluated to quantify its fitness. Subsequently, a new population (iterationt+1) is formed by selecting better individuals from those of generation t. In GA terminology, a solution vector x ∈ X is called an individual or chromosome and corresponds to a unique solution x in the solution space. The chromosomes are made of discrete units called genes. Each gene controls one or more features of the chromosome. In this paper, genes are assumed to be binary digits, according to the original implementation of GAs by Holland [38].GAs use two operators to generate new solutions from existing solutions: crossover and mutation. In crossover, two chromosomes, called parents, are combined together to form new chromosomes, called offspring. The mutation operator introduces random changes into the characteristics of chromosomes, for the purpose of reintroducing genetic diversity back into the population and assisting the search to escape from local optima.Reproduction involves selecting a set of chromosomes that will survive into the next generation. The procedure of a generic GA is given in Fig. 2.After the random generation and evaluation of initial solutions, the population is subjected to the iterative process of selection, mating (crossover), mutation and evaluation for the next iteration (generation). The iterative process is terminated when there is a satisfactory quality of solutions or when the maximum number of iterations is reached.A full adaptability of a real-world dam model requires the ability of the model to deal with the persistent increase of a measurement dataset and a variant set of predictor variables induced by permanent changes and malfunctions in the measuring system. In order to provide an ANN dam model that is fully adaptive to a variant dataset and predictors, in this paper we present a methodology for near real-time optimization of a structural dam model based on artificial neural networks. This concept implies optimization of all elements of the network with the aim to generating a model that best describes structural dam behavior under given conditions. The elements subjected to optimization are the number of hidden layers, the number of neurons per layer, activation functions, learning algorithms and learning rules.Let Ω denote the set of possible ANN models that contain at most Nmax  hidden layers with a maximum of nmax  neurons per layer, where all the neurons use one of the available AFs. In addition, every single ANN model uses one of the offered learning algorithms tuned with the parameters from the recommended ranges.LetRMSETest(ψ)denote the root mean squared error that the trained network ANN(ψ) from Ω produces using the testing dataset. Then, a mathematical programming model to find the best ANN topology and the best choice of AF, learning algorithm, and learning rules, can be written in the following form:MinimizeRMSETest(ψ),ANN(ψ)∈ΩSubjected to(1)N(ψ)≤Nmax(2)n(ψ)≤nmax(3)AF(ψ)∈{Gaussian,sine,TAHN,...}(4)LA(ψ)∈{BPGradientDescent,RPROP,...}(5)a∈[amin(ψ),amax(ψ)]where N(ψ), n(ψ), AF(ψ), LA(ψ) are the number of hidden layers, the number of nodes in hidden layers, activation function, and learning algorithm of neural network ANN(ψ), respectively. In Eq. (5), the variable a stands for any of the parametersη,μ,η+orη−. In addition, for the sake of computational efficiency, the same AF is used for all hidden and output neurons.Regarding the complexity of the optimization problem, we used a genetic algorithm as an optimization technique, due to its inherent generality and robustness. The developed optimization methodology (ANN/GA hybrid) is based on the iterative strategy of the GA shown in Fig. 2, where individuals represent neural networks from Ω. Every single individual (neural network ANN(ψ)) from the initial population is evaluated in order to determine its fitness. In our case, the networks with lowerRMSETest(ψ)are considered to be better, thus the fitness is calculated as1/RMSETest(ψ). According to the chosen selection criterion and the elitism concept, networks with the lower fitness are discarded. Through the processes of crossover and mutation, a new generation of neural networks with different topologies, learning algorithms, activation functions, and learning rules is generated. Once the convergence criterion is achieved (defined time, required accuracy...), a population of optimized artificial neural networks is obtained. Sincethere is only one optimization criterion (RMSE), the network with the best fitness within the final population is chosen as the most accurate one.According to Holland's original approach, individuals in the population pool are in the form of binary chromosomes. The proposed genetic structure of the individuals in a population pool is shown in Fig. 3.The first two groups of genes represent the number of hidden layers N(ψ) and the number of neurons n(ψ) in these layers. Note that the number of neurons is assumed to be the same in all hidden layers (n1(ψ)=⋯=nNmax(ψ)=n(ψ)), in order to keep the chromosome size constant over the population. The next group of genes represents a type of activation function AF(ψ). The LA(ψ) genes in the chromosome define the learning algorithm, while LP1(ψ) and LP2(ψ) genes include the parameters of the learning algorithm (η and μ for the BPGD, orη+andη−for the RPROP algorithm). In order to avoid constraint violations, each of integer variables (including enumerated activation functions and learning algorithms) should be in the range[1−2l], where l is the number of bits in the gene representing that variable. On the other hand, the range for each real variable is set according to recommendations, but its resolution will depend on the number of bits in that gene. Coding variables in this way guarantees that their values will remain inside the prescribed ranges during crossover and mutation processes.Fig. 4 shows the example of two ANN individuals, where both networks have 3 predictors and one response variable. Binary chromosome ANN(ψ) in Fig. 4a represents an ANN with 2 hidden layers, each with 5 neurons and the TANH activation function. For the sake of brevity, the genes that represent the learning algorithm and learning rules are not shown in the Figure. The ANN(γ) chromosome, shown in Fig. 4b, represents a coded ANN with 3 hidden layers, each with 3 neurons and a sinusoidal activation function. Here we used a notation of the form I/N(n)/O, where I, N, n and O are the number of predictors, the number of hidden layers, the number of neurons per layer, and the number of response variables, respectively. Accordingly, the topology of ANN(ψ) and ANN(γ) individuals can be written in forms 3/2(5)/1 and 3/3(3)/1.In this study, the single point crossover operator has been used. Once the crossover point is randomly selected, two mating chromosomes are cut at corresponding points, and the sections after the cuts are exchanged (Fig. 5a). To avoid the local minima problem, each bit is independently flipped with a probability of p, using a bit-flip mutation operator (Fig. 5b) [23].Evaluation of each ANN(ψ) individual is realized as shown in Fig. 6. The content of the chromosome defines the activation function, learning algorithm and learning rules used in the neural network represented by that individual. According to the activation function, the data in the learning dataset (LDS) and test dataset (TDS) are normalized. After the generation of random initial values of weighting coefficients wij, the learning algorithm with defined learning rules is performed as described above.The learning process is aborted in the overlearning zone [31]. In order to reduce the influence of the stochastic generation of the initial weight matrix (W0(ψ,r)) on the quality of the ANN(ψ) model, the whole procedure is repeated several times with different initial weights. Finally, the fitness of the ANN(ψ) individual is calculated as1/RMSETest(ψ)=1/min(RMSETest(ψ,W0(ψ,r))),r=1,R, where R is a number of repetitions, arbitrarily chosen by a researcher.According to the optimization criterion that is defined in the presented mathematical programming model, all of the individuals (models) are ranked based onRMSETest(ψ),ψ∈Ω. In this study, we employed binary tournament selection, which performs a tournament between pairs of individuals and selects the winners that pass to the next generation. To assure that the best individuals always survive to the next generation, an elitism strategy has also been used [23].In order to prove the proposed ANN/GA concept, we have developed a self-tuning software system for dam behavior modeling, named DEVONNA (Dam Evolving Neural Networks), (Fig. 7).The structural dam behavior (1) can be considered as a black box that transforms the vector of input variables X (2) into the vector of output variables Y. Measurements of input variables, such as headwater H, air temperature Taand time t (ageing), as well as the output variables of interest, such as displacements and stresses, are stored in the database (3). Nevertheless, if temperature measurements are incomplete or unavailable, trigonometric functions as temperature cyclic (seasonal) loadings are commonly used in dam modeling to describe deformation patterns as follows [11]:(6)sin(2·k·π·d/365),k=1,2,3…cos(2·k·π·d/365),k=1,2,3…where d is the day of the year.The ANN/GA hybrid randomly generates an initial population of individuals (binary chromosomes), each representing one ANN (4). In order to find the network that best represents the historical behavior of the dam, the population of the networks is optimized throughout an evolutionary process (5, 6, 7, 8, 9) described in Section 3.1. The optimization procedure is repeated a few times in order to reduce the effects of the inherent stochastic nature of the GA (usually 5 times, [40,41]).The final product of the DEVONNA system is the optimized mathematical model (10) based on the ANN, which can predict the structural dam behaviorY^given the known input variables. This model can be further used for dam behavior prediction, analysis, and comparison with measurements and possible actions.The presented DEVONNA system has been developed in a Microsoft .NET software environment and comprises a data acquisition service, an MS SQL database and a module for the optimization of ANN dam models. The special component of the system is the interface to the Encog software library [25], which is employed for the processes of learning the ANN.The following sections present validation of the proposed ANN/GA hybrid and the developed DEVONNA system using the case study of modeling Grancarevo dam displacements.The Grancarevo dam is the first step of the hydropower system of the Trebisnjica river in the Republic of Srpska. The dam is located 18km from the river source and 17km upstream from the city of Trebinje, creating a reservoir of 1278 · 106 m3 (Fig. 8).The dam body is made of 377000 m3 of concrete in the form of 31 cantilever blocks, numerated from the right to the left bank (Fig. 9). This dam is a double curvature arch dam (R = 185.48m), which is 123m high and has a 439.3m long crest, with a thickness of 4.6m at the top and 26.9m at the bottom of the dam. It is equipped with a monitoring system to measure parameters such as concrete conditions, water and air temperatures, the reservoir water level, horizontal and vertical displacements, rotation, movements of joints, strain, stress, uplift pressure, foundation displacements and seepage, at a total of 465 measuring points.Three pendulums were installed to measure radial and tangential deformations. In this paper we modeled the radial displacements of point P1 at the dam crest (elevation 403m.a.s.l.) (Fig. 9a), block 17 (Fig. 9b). Displacements of the point are measured using coordinometer V17-1. In fact, the dam monitoring system usually provides many outputs, which can be used in the assessment of structural behavior. Although artificial neural networks have the ability to model more than one output variable simultaneously, for the sake of clarity, in this work we present the principle using only one output.The dam was built in 1967 and up to now about 14500 measurements of point P1 displacements have been made. However, there are a lot of missing values in the period between 1967 and 1984, thus the period from January 1984 to the end of August 2011 was chosen for modeling. For the sake of computational efficiency we used every second data, so the dataset consisted of 5042 data samples in total. According to a cross-validation strategy, this dataset is divided into LDS and TDS subsets with a given ratio [42–44].The effect of hydrostatic pressure on the dam displacements was taken explicitly into account through the headwater H. The range of the input variable H was from 331.05 to 401.28m. The thermal effect was accounted by the mean daily air temperature Ta(–7.10 to 32.10°C). Thermal effects are also represented by trigonometric functions as temperature cyclic (seasonal) loadings which can be used to describe deformation patterns through input variable d. The variable d represents the time elapsed from the beginning of the year, ranging from 1 to 365 days. However, at different geographical locations, temperature oscillations can have a phase offset from the beginning of the year. In order to test stability of the models regarding temperature phase offset, we used dummy input variables d20 and d50 that represent phase offsets of 20 (d20=d+20) or 50 (d50=d+50) days, respectively. The ageing effect, as a function of time t, includes the influence of degradation of the material properties during the structural lifetime on measured values. As mentioned before, the radial displacement of point P1, represented by variable y17, is chosen to be the output (response) variable. The range of the displacements in the analyzed period was from +13.26 to +64.4mm, where radial displacements in downstream direction are considered positive.An example of the learning dataset, which represents 60% of the available data, is given in Fig. 10. As an effect of hydrostatic pressure, radial displacements are increased with an increase of water level H, and vice versa. In contrast, due to thermal expansion, radial displacements are decreased with a raise of temperature Ta, andvice versa.

@&#CONCLUSIONS@&#
