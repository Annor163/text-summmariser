@&#MAIN-TITLE@&#
A structure-based region detector for high-resolution retinal fundus image registration

@&#HIGHLIGHTS@&#
The core of the proposed registration method is an effective region detector to determine correspondences.The registration method is invariant against rotation and small-scale changes.The registration method can deal with the registration of different viewpoint images when there are common regions in the overlapping areas.The registration method is computationally efficient in the presence of high-resolution retinal fundus images.The efficiency and accuracy of the proposed method make it suitable to be applied for further processes like change analysis.

@&#KEYPHRASES@&#


@&#ABSTRACT@&#
A fundamental problem of retinal fundus image registration is the determination of corresponding points. The scale-invariant feature transform (SIFT) is a well-known algorithm in this regard. However, SIFT suffers from the problems in the quantity and quality of the detected points when facing with high-resolution and low-contrast retinal fundus images. On the other hand, the attention of human visual systems directs to regions instead of points for feature matching. Being aware of these issues, this paper presents a new structure-based region detector, which identifies stable and distinctive regions, to find correspondences. Meanwhile, it describes a robust retinal fundus image registration framework. The region detector is based on a robust watershed segmentation that obtains closed-boundary regions within a clean vascular structure map. Since vascular structure maps are relatively stable in partially overlapping and temporal image pairs, the regions are unaffected by viewpoint, content and illumination variations of retinal images. The regions are approximated by convex polygons, so that robust boundary descriptors are achieved to match them. Finally, correspondences determine the parameters of geometric transformation between input images. Experimental results on four datasets including temporal and partially overlapping image pairs show that our approach is comparable or superior to SIFT-based methods in terms of efficiency, accuracy and speed. The proposed method successfully registered 92.30% of 130 temporal image pairs and 91.42% of 70 different field of view image pairs.

@&#INTRODUCTION@&#
Retinal image registration is the process of finding the geometric transformation between two or more retinal images from different times, viewpoints and sources [1,2]. This paper focuses on registration of images from different times (i.e., temporal registration) and viewpoints in the presence of high-resolution retinal fundus images. Temporal image registration facilitates estimating the diseases or therapeutic progress by better measuring of changes both on retinal vascular tree and on the color contents of the eye fundus [3]. Moreover, registration of different viewpoint images generates a wider view of the retina which is helpful for retinal tracking [4]. Accurate and real-time retinal fundus image registration is still challenging in the presence high-resolution, small-overlapping regions, time-varying intensities and contents and low quality of retinal images. High-resolution images have made it more convenient for ophthalmologists to detect very fine structures, such as hemorrhages and microaneurysms in diabetic eye screening [5,6]. However, existing retinal fundus image registration algorithms have high computational cost in the presence of high-resolution retinal fundus images. Since reducing image resolutions produces artifacts, the registration results of resized images will be inapplicable to change detection and eye-screening [5,6]. For example, rescaling of images may produce blocking artifacts, which are liable to mistake the detection of some lesions like microaneurysms, whereas a needed resolution to detect the smallest microaneurysms is 1360×1000 pixels [6]. Based on this reason, the need for an image registration approach that will resolve the problem is urgent.Retinal fundus image registration methods include area-, feature- and hybrid-based methods [2]. Area-based methods optimize intensity-based similarity measures to find parameters of geometric transformation between image pairs [7–9]. The similarity measures include cross-correlation, sum of absolute values of differences [10], mutual information [7,9,11] and phase correlation [12]. These intensity-based similarity measures are sensitive to temporal image registration since large morphological changes (e.g., optic disc topographic changes in a glaucoma patient [7]) lead to significant changes between pixel intensity values in the input images. Therefore, many local maxima in the similarity measure function mislead the optimization methods [7]. Moreover, similarity measures may mislead by non-overlapping areas of viewpoint images [13]. Area-based methods suffer from computational burden since they employ the entire image content, whereas feature-based algorithms detect suitable locations in the images to establish feature correspondences. Then, correspondences determine the transformation parameters. Good locations are stable under content and geometric variations between image pairs. Moreover, they should generate distinctive descriptors to find correct matches [14]. In retinal fundus images, vessel bifurcations are invariant to intensity variations. However, bifurcation extraction requires reliable centerline detection of vessels. Besides, angle-based invariant descriptors, which used around bifurcations to find correspondences, may be similar and points will not be distinguishable from each other [15,16]. Hybrid methods employ the advantages of both area-based and feature-based methods. In this view, Chanwimaluang et al. [11] extracted bifurcations, and established correspondences using mutual information in regions around the bifurcations.Nowadays, scale-invariant feature transform (SIFT) [17] is the most stable and distinctive local feature which is independent of vascular extraction. It consists of the scale-space extreme point detection, filtering spurious detections via a single global threshold and invariant descriptor generation. Existing feature-based retinal fundus image registration methods, General Dual bootstrap iterative closest point (GDB-ICP) [18] and SIFT-GTM [19] (Graph Transformation Matching), find initial matches by SIFT. The performance of those methods degrades in the presence of contrast variability within high-resolution retinal fundus images since SIFT detector suffers from the quantity, quality, and distribution of the detected points [20]. A global threshold of intensity-based SIFT detector is unable to overcome intensity variations within the retinal images and leads to detecting inadequate points in the presence of high-resolution and low-contrast retinal images [20]. Although a small global threshold produces lots of points, it hampers feature matching performance since the feature points are mainly unstable and redundant. In addition, processing of many feature points to find correspondences leads to high computational complexity and memory requirements.The human visual system seeks similar regions in the images instead of points to establish correspondences [21,22]. In the recent years, efforts have been made to imitate such behavior, because it allows optimization of the computational resources as they can be focused on the processing of a set of selected regions only. Recently, Zheng et al. [23] employed a region saliency measure based on local entropy and variance values of image pixels to extract informative and salient feature regions on multimodal retinal images. Regions were matched by gradient based descriptors. Soon after, Gharabaghi et al. [24] extracted closed-boundray regions on segmented retinal fundus images. Then, regions were matched by the moment invariant descriptors. Both existing retinal region detectors [23,24] and SIFT found stable areas or points by analysis of local intensity cues. They focuses on the appearance of region interiors and ignored structural cues of region boundaries which are more robust to intensity variations [25,26]. Conversely, this paper proposes a structure-based region detector. It is based on a watershed segmentation of a clean vascular structure map to detect polygonal enclosed regions by vessels reliably. Descriptions of polygonal boundaries with distinct shapes are more effective than the appearance of region interiors by SIFT since retinal images contain many vessels similar to each other. In comparison to SIFT-based approaches, our retinal fundus image registration method is fast since it only focuses on the processing of regions which the number of them are fewer than thousands of SIFT feature points, but they are selected as non-redundant and highly stable regions.The organization of this paper is as follows: Section 2 explains our methodology, including region detection, region description, region matching and mismatch elimination, and transformation parameter estimation. Section 3 presents experimental results. Conclusion and suggestions for further study are given in Section 4.The proposed registration framework includes five stages as follows:•Region detectionRegion descriptionRegion matching and mismatch eliminationIncreasing the number of correspondencesTransformation parameter estimationFig. 1shows the flowchart of the registration framework including the proposed region detector. The proposed registration method first extracts polygonal regions of retinal fundus images. Next, it provides correct matches by computing rotation and scale invariant boundary-based descriptors, cross-matching the descriptors, and eliminating the mismatches. Since our approach may lead to few matches in some challenging cases and they are inadequate to estimate the parameters of geometric transformation between images, it is essential to increase the number of matches. Finally, the matched points are employed to find the transformation parameters between the two input images. The following subsections provide a detailed description of the main components.We develop a fast process that locates the enclosed regions by the vessels using the watershed segmentation [27] of a vascular structure map. Watershed transformation leads to over-segmentation due to nonhomogeneous intensity variations in high-resolution unhealthy retinal backgrounds [25]. Hence, computing a clean and reliable vascular structure map is a starting point to detect robust watershed regions. The following subsections provide a detailed description of region detection stages.Finding vessel structure map consists of three steps. First, the color saliency boosting algorithm [28,29] converts an RGB color space to an RGB color-boosted space which is analogous to the color channels in human visual processing. Then, gray-scale morphological operations enhance the gray-value version of the new RGB image. Finally, the Frangi vesselness measure (FVM) [30] intensifies the vesselness areas.Consider an RGB image f=(R, G, B) and its local color-derivativesfx=Rx,Gx,Bx. Studies show that the luminance variations in color-derivative distribution of RGB images are more probable rather than chromatic changes [28]. Hence, RGB channels are highly correlated and retinal fundus images have dull appearances. Furthermore, the gray-value versions of the RGB retinal images contain low contrast and interpretability [31]. Here, we apply the color saliency boosting algorithm [28] to transform an RGB color space f=(R, G, B) to a new uncorrelated color spacefboost=Rboost,Gboost,Bboostusing a boosting matrix Mboostfboost=Mboostf,(1)RboostGboostBboost=m11m12m13m21m22m23m31m32m33RGBwhere, mijis the element of matrix Mboost in ith row and jth column.In this view, a color saliency boosting function g, based on information theory, transforms the color derivative distribution to a white distribution, so that luminance and chromatic attributes contain equal contributions for image understanding. Hence, retinal image features become much more clearly visible. The decorrelation function g(fx) is based on the pre-processing steps of independent component analysis [29]. The color derivative distribution is centered at the origin of coordinates, i.e., E[fx]=0 and the covariance matrix is ∑x=E(fxfxT) By singular value decomposition (i.e., USVT) into eigenvector matrix U and eigenvalue matrix S, the principle axes of the new color derivative distribution and their corresponding squared relative half-lengths of eigenvalues λ1, λ2, λ3 are determined:(2)ΣxUλ1000λ2000λ3VTHence, the whitening functiong(fx)=∑x−1/2fx=Mboostfxnormalizes RGB color image components with respect to luminosity, contrast and chromatic distribution. Regardless of the chromatic attribute which makes images to look more natural [32], the gray-scale contrast of an RGB color-boosted image fboost is higher than that of the RGB color image f (Fig. 1(a)).Once a gray-scale image Ig of an RGB color-boosted image fboost is obtained, two consecutive gray-scale morphological opening and closing operations are applied to obtain connective vessels and smoothed backgrounds not only in healthy retinal images, but also in poor quality images of unhealthy retinas. Fig. 1(b) shows image IM after morphological operations. The opening is defined asIG∘b=IG⊖b⊕b, where, b is the disk shape structuring element with radius 5. The closing is defined asIG∘b=IG⊕b⊙bwhere b is a 3×3 disk shape structuring element. The gray-scale morphological erosion and dilation are shown by ⊖ and ⊕, respectively. The opening operation widens dark structures, corresponding to the vessels and capillaries. It fills the gaps inside the vessels [33] and connects branching points of the vessels. The closing operation erases spurious vessels and isolated structures which are smaller than the structuring element. The optimum size of structuring elements are determined in an experimental analysis of all image pairs of our datasets (refer to Section 3.1) with a wide variety of pathologies, and illumination and content changes. The selected values are based on vessel widths of retinal fundus images, so that they lead to maximum repeatable regions.Once the vessels are enhanced, a filter process, FVM, is applied on multiple scales to detect different size of vessels in image IM. According to Frangi et al. [30], the vesselness measure at pixel X=(x, y) and scale S is computed by(3)V(X,s)=0ifλ2(X,s)>0e−Rβ22β2VX,s=0ifλ2X,s>0,e−Rβ22β2(1−e)−S22c2where, Rβ=λ1(X, s)/λ2(X, s) captures only the geometric information of image and is based on eigenvalues of the Hessian matrix of image I at scale s. Vessel pixels are characterized by a small λ1 value and a higher positive λ2 value. β=0.25 is chosen empirically and c is half of the max Hessian norm.S=∑i=1,2λi2(X,s)is a measure of structureness. It is obvious that the measure is small in the backgrounds where there is no structure and the contrast is low.The vesselness measure, Vout, at every pixel x is estimated by a maximum vesselness measure across all scales:(4)Vout(X)=maxsmin<s<smaxV(X,s)where, smin and smax are minimum and maximum scales at which the vessels are expected to be found. Hence, they can be chosen based on vessel widths. On the other words, the maximum ofVX,sis at a scale that approximately matches the size of the vessel to be detected. Hence, we start s from 1 with increasing step 2. smax is determined based on the input image size. smax=13 for first and second datasets (refer to Section 3.1) and smax=10 for third and fourth datasets (refer to Section 3.1) are adequate to ensure proper working of the proposed method. Proper tune of these parameters is necessary for achieving adequate stable regions and consequently success of registration. Here, the optimum values of the parameters are determined in an experimental analysis of all temporal and partially overlapped image pairs of our datasets. These values led to maximum number of repeatable and corresponding regions. At the end, the FVM algorithm produces a gray-scale image IFVM including the highlighted retinal vessels (Fig. 1(c)).The watershed transform [27] is applied on the vascular structure map IFVM. Vessel structure map can be considered as a topographic surface. If we flood this surface from its minima and, if we prevent the merging of the waters coming from different sources, we partition the image into two different sets. Background pixels and vessels become catchment basins and watershed lines respectively to produce enclosed regions by vessels. Fig. 1(d) shows watershed regions with different colors. We drop very small regions by the number of pixels (i.e., areas) since they are unstable and nondistinctive (Fig. 1(e)). Our approach is insensitive to this parameter since it produces almost the same results with areas of 30–70 pixels. Thus, in the present study we set this parameter to a fixed 70 pixels. Fig. 2shows detected regions, so that each one is enclosed by an ellipse that has the same normalized second order central moments as the region [21].To match regions, we characterize them by a boundary-based region descriptor, normalized shape signature [34], which is invariant to rotation and scale changes. However, the shape signature is sensitive to noise, and slight changes in the boundary can cause large errors in matching [34]. Hence, we derive a convex polygon for each region by the QuickHull method [35], so that a convex hull decomposes a region's boundary into segments (Fig. 1(f)). This reduces the boundary's complexity and increases the robustness of matching. Then, we estimate the centroid of each region by(5)μ=1|φ|∫φXdφwhere, |φ| is the area of the region. Next, the distance r from the centroid to the boundary is plotted as a function of angles θ (Fig. 1(g)).The descriptor generated by the approach depends on rotation and scaling. Thus, the descriptor is normalized with respect to rotation by rotating the histogram of the descriptor, so that the peak of the histogram becomes the first bin. In addition, since changes in the regions’ sizes lead to different magnitudes ofrθ, division of each sample of descriptor by its variance leads to scale invariance [34].In this section, we compute the similarity between regions using their region descriptors. The region descriptors are introduced to a cross-matching process [20] to avoid one to many matches [13]. A descriptor Piof the first image P is matched to a descriptor qjof the second image Q if it has the minimum Euclidean distance between all other descriptors in Q and qjhas also a minimum Euclidean distance to pibetween all descriptors in p. Before cross-matching in the reverse direction, to obtain certainty, the ratio between the first and second minimum distances is also checked to be less than a threshold, Tc=0.8, which is determined empirically. If we increase or decrease this value, it influences on the number of initial matches [13]. We achieved the best registration results by choosing the value of the threshold to 0.8.The cross-matching may result in some mismatches which are eliminated in the next stage. There are n matched regions asp1,q1,p2,q2,…,pn,qn, where p and q are centroids of regions in the first and second image, respectively. We assume that the geometric changes between retinal images can be approximately described by a similarity transformation. Hence, all ratios rij=d(pi, pj)/d(qi, qj) of Euclidian distances (d) between any two matches(pi,qj),(pj,qj)will remain the same [13]. Consequently, a centroid will be eliminated if the Euclidian distance between it and other centroids in P is not equal to the distance of corresponding centroids in Q.In challenging cases, few matches may be achieved. Hence, in addition to centroids of matched regions, the extremal points of the convex hulls are considered as matches. On the other hand, a higher number of correspondences increases registration accuracy, especially since matched extremal points are uniformly distributed [20]. Each extremal point has an extremal coordinate value in either its row or column coordinate position. There can be as many as eight distinct pixels to a region: topmost right, rightmost top, rightmost bottom, bottommost right, bottommost left, leftmost bottom, leftmost top, and topmost left [36]. Two different extremal points may be coincident as Fig. 1(g). Since the order of extremal points changes by rotation, we consider a vector for each region as {Xi, li}, i=1..8 where liis the Euclidean distance between an extremal point Xiand the centroid. Then, Xiis arranged, so that the largest distance is taken as the first element. Then, corresponding extremal elements of two vectors in previous matched regions are new matched points which are determined invariant to rotation. Fig. 3(a,b) shows an example of all matches for a temporal image pair of Fig. 1 with a resolution of 3504×2346 pixels. There are 15 matched region pairs i.e., 15 corresponding centroids and 120 corresponding extremal pixels in the input images.Corresponding points are used to estimate the transformation parameters by the least square method [37,38]. In the literature, different transformation models such as similarity, affine, and polynomial functions corrected geometric deformation between retinal images. On the one hand, the second-order polynomial transformation function has more flexible mapping in the presence of non-linear retinal curvature [18,39]. It has less alignment average error than the affine and similarity models in the case of high-resolution temporal image pairs with large overlapping areas [18]. On the other hand, selecting a model type depends on the number of matched points in the case of small overlapped image pairs. In challenging cases, few matches may be found. Based on the above explanation, the polynomial function is selected when the number of correspondences is more than 30, and the affine transformation is selected when the number of aligned point pairs is fewer than 30 and more than 8. Consequently, when the number of matched points is fewer than 8, the similarity function is chosen. Fig. 3(c,d) shows an example of the registration results for a temporal image pair of Fig. 1, using the checkerboard overlaying method [2].We evaluated the proposed approach on temporal and different fields of view image pairs, and compared the results to GDB-ICP [18,39] and SIFT-GTM [19]. The proposed method and GTM were implemented in MATLAB. The GDB-ICP algorithm and Vedaldi's SIFT [40] were downloaded as a C+ program. The parameters to compute the SIFT algorithm were determined as suggested by Lowe et al. [17]. The experiments were performed on a personal computer with Intel® Core™ (Intel, Santa Clara, CA, USA i5 2.53GHz and 6 GB of RAM).There is no publicly available dataset and ground truth to evaluate the registration results of different techniques. We collected four datasets to evaluate the proposed method. Table 1shows the properties of four retinal fundus image datasets in details. The images of first and second datasets were taken by digital camera Canon EOS 40D attached to Canon CR-1 fundus camera with 45° angle of view [41,42]. The images of third and fourth datasets were acquired with a Topcon 50 DX fundus camera (TRC-50IA, Tokyo, Japan) [43,44]. Some of input image pairs were taken at the same time with different viewpoints, while temporal images were taken months or even years apart. All images are more or less affected by retinopathies such as atrophy and hemorrhages. The minimum overlapping area of viewpoint image pairs is approximately 15%.This part evaluates the robustness of the proposed technique under rotation and scale variations, and different percentage of overlaps between images.To avoid memory shortage, we only select 10 test image pairs from third and fourth datasets which are smaller than the images of the first and second datasets. The image pairs have 60% overlapping areas and little vascular damage so that we can find at least one corresponding region to register images successfully. The first image is held fix and the second image is rotated by 30°, 45°, 60°, 90° and 180° respectively. Then, we applied the proposed method on the first image and the rotating second image. The proposed method successfully registered 43 rotated pairs out of 50 image pairs with a success rate (refer to Section 3.3) of 86%. The results of this test show that our method successfully registers image pairs regardless of the rotation angle, since the regions are enclosed by the same vascular structures before and after rotation, and region boundary descriptors are rotation invariant. This demonstrates that our method is rotation invariant. However, retinal image pairs in clinic are of very small rotation difference. Two samples of this test are shown in Fig. 4. In the first row, the yellow pathological areas were reduced in the second image which was taken two months later than the first image.Scale differences of retinal fundus image pairs are due to changing either the resolution of the lens in a camera or the distance between the camera and the head. It is rare to capture retinal fundus images with both large scale changes and small overlapping areas in clinical applications. Moreover, rescaling high-resolution images of the first and second datasets result in memory shortage. Therefore, to test the scale insensitivity, we selected 30 image pairs with 60% of overlapped areas and without large vascular disease from third and fourth datasets and rescaled them. The first image was held fixed and the second image rescaled with a scaling factor from 1 to 3 with a step of 0.1. Then, the proposed method was applied. Fig. 5shows the number of successful registration relative to scale change by the proposed method. Since the scale factor between image pairs becomes above 1.6, our method usually fails to provide a reliable vascular structure map for the stable region detection. Therefore, the low repeatability rate of the region detector leads to lack of corresponding points and failure of registration in the presence of images with large scale changes. Most retinal fundus images in clinical practice usually have scale difference of less than 1.3 [13]. Thus, our framework is still suitable for retinal image registration.To test the performance of our method on small overlapping area image registration, we selected 40 images out of 70 viewpoint image pairs of fourth dataset which their overlapping areas are equal or less than 50%. The images were classified in five groups so that each group had 8 viewpoint image pairs. The overlapping percentage between image pairs of five groups is from 10% to 50% with an increasing step of 10%. Fig. 6shows the percentage of successful registration (refer to Section 3.3) by proposed method relative to the overlapping area. The results show that our method will successfully register small-overlapping retinal image pairs if there are common polygonal regions between images. Fig. 7shows registration results for four viewpoint image pairs. The overlapping areas are approximately below 25% for images in the first and second rows, and 50% for images in the third and fourth rows. The third pair is severely affected by retinopathies. GDB-ICP fails to register these images except the last one, and SIFT-GTM is unable to register the images of the fourth row.We evaluated the overall performance of the proposed registration method by average and standard deviation of registration accuracy, its success rate and the running time (see Table 2).We manually select 6–20 matched points (for example bifurcations and corner points) on images with different percentage of overlaps by MATLAB R2011b, and generate ground truth to evaluate the proposed method over all selected correspondences. Points have to be distributed uniformly with an accurate localization. In practice, there are differences between the coordinates of the manual correspondences by automatic methods. Therefore, accuracy of registration can be evaluated by the mean error and standard deviation of manually selected points on input images after automatic registration.There are n selected corresponding points asp1,q1,p2,q2,…,pn,qn, where p and q are selected points in the first and second image, respectively. The parameters θ of the transformation function T are estimated by the proposed method and the corresponding points of q are transformed into the coordinate system of first image asqt=Tq,θ. Then, the mean distance of p and qt(i.e., meanpi−Tqi,θ;i=1...n)is computed as the registration error. Finally, we compute average of mean error for all images of our dataset. Table 2 shows accuracy of registration as mean error and standard deviation in comparison to GDB-ICP and SIFT-GTM. The mean error and standard deviation values of the proposed method are 2.2 and 1.6, respectively.The success rate [20,23] is the ratio of the number of image pairs with successful registration to the number of all image pairs. Successful registration is determined with regard to the mean error value of matched points. A mean error below 5 pixels is acceptable for clinical purposes [20]. Our proposed method registered 70 different fields of view image pairs with the success rate of 91.42% and 130 temporal image pairs with the success rate of 92.30% (Table 2). GDB-ICP has satisfactory results since the overlapping area between images is above 40%. SIFT-GTM is unable to complete the process of image registration for the images of the first dataset, such as the image pair of fourth row of the Fig. 7 due to memory shortage.The comparative results of this section indicate that our proposed method is more robust in registering high- resolution image pairs. However, the proposed algorithm may fail in the presence of low-quality images. Fig. 8shows such image pair. The second retinal image is blurred due to the lens problems of the eyes. In this situation, the proposed method is unable to provide a reliable vascular structure map for the stable region detection. Existing algorithms are unable to register the image pairs of Fig. 8, too.The greatest advantage of the proposed method is its running time especially in the case of high-resolution temporal images where SIFT detects thousands of features. For example, SIFT extracts more than 40,000 and 10,000 features in temporal retinal image pairs of first and second, and third datasets, respectively. The proposed method processes only a small number of distinctive regions to find corresponding points and therefore, it is fast in comparison to standard SIFT. GDB-ICP first finds initial matches by SIFT. Then, it expands the region around the initial correspondence iteratively until two images can be fully mapped. The approach has high computational cost due to the running of the process on every initial match, separately. Since no iterative and optimization schemes are used in the proposed method, it is fast in comparison to GDB-ICP. The proposed method takes about 116.4 and 86.43s on average to register a temporal image pairs of first and second datasets, respectively. It also takes about 30.11s on average to register an image pair of third and fourth datasets (Table 2).

@&#CONCLUSIONS@&#
