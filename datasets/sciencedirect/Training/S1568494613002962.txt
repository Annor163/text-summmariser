@&#MAIN-TITLE@&#
The L1/2 regularization method for variable selection in the Cox model

@&#HIGHLIGHTS@&#
We propose a L1/2 penalized Cox model.We develop a coordinate descent algorithm with a new univariate half thresholding operator.The L1/2 penalized Cox model is able to reduce the size of the predictor even further at moderate costs for the prediction accuracy.The L1/2 penalized Cox model is suitable for survival analysis with the high dimensional biological data.

@&#KEYPHRASES@&#
Survival analysis,Regularization,Variable selection,Cox model.,

@&#ABSTRACT@&#
In this paper, we investigate to use the L1/2 regularization method for variable selection based on the Cox's proportional hazards model. The L1/2 regularization can be taken as a representative of Lq(0<q<1) regularizations and has been demonstrated many attractive properties. To solve the L1/2 penalized Cox model, we propose a coordinate descent algorithm with a new univariate half thresholding operator which is applicable to high-dimensional biological data. Simulation results based on standard artificial data show that the L1/2 regularization method can be more accurate for variable selection than Lasso and SCAD methods. The results from real DNA microarray datasets indicate the L1/2 regularization method performs competitively.

@&#INTRODUCTION@&#
Censored survival data analysis of DNA microarray data is an increasing interest in relating gene expression profiles to survival phenotypes such as time to cancer recurrence or death. The identification of gene signatures related to survival may provide new information and tools for clinical decision making, prognosis, diagnosis, choice of therapy, and may further aid in the search for new targets for drug design. The Cox proportional hazards model [1,2] is the most popular method for the censored survival data. However, due to the very high dimensional space of the predictors, i.e., the number of genes typically far exceeds sample size in the microarray experiments, the standard maximum Cox partial likelihood method cannot be applied directly to obtain the parameter estimates. To deal with the problem of high dimensionality, various approaches have proposed, for example, semi-supervised learning methods [3], supervised principal components [4], and the residual finesse approach [5].Besides the high-dimensionality, the genes expression levels of some genes are often highly correlated, which creates the problem of high co-linearity. To deal with the problem of co-linearity, the most popular approach is to use the penalized partial likelihood, including both the L2 penalized estimation [6], which is often called the ridge regression, and the L1 penalized estimation [7], which is called the least absolute shrinkage and selection operator (Lasso) estimation. Comparing to the L2 penalized procedure with constraints on the sum of the square of the coefficients, the Lasso procedure provides method for variable selection. However, the Lasso estimator does not possess the oracle properties, and several approaches beyond the lasso have been developed in recent years. Fan and Li [8] adopted the smoothly-clipped-absolute-deviation (SCAD) penalty which has better theoretical properties than the Lasso. Zhang and Lu [9] developed the adaptive Lasso method based on a penalized partial likelihood with adaptively weighted L1 penalties on regression coefficients. Engler and Li [10] proposed the penalized Cox method with Elastic net penalty which is a linear combination of L1 and L2 penalties.Inspired by the aforementioned methods, we proposed a new, efficient version of penalized Cox model, which based on a L1/2 penalty. The L1/2 penalty can be taken as a representative of Lq(0<q<1) penalties in both sparsity and computational efficiency, and has demonstrated many attractive properties, such as unbiasedness, and oracle properties [11]. In this paper, we developed a coordinate descent algorithm with the L1/2 regularization in the Cox model. The approach is applicable to high-dimensional data, such as DNA microarray datasets.The rest of the paper is organized as follows. In Section 2, we proposed a new version of the penalized Cox model with the L1/2 regularization. In Section 3, we developed the coordinate descent algorithm for the L1/2 penalized Cox model. In Section 4, we evaluated the performance of our proposed approach on the simulated and real DNA microarray datasets respectively. We concluded the article with Section 5.Suppose the dataset has a sample size of n to study survival time T on covariate X, we use the data form of(ti,δi,Xi)i=1nto represent the individual's sample, where t is the survival time, δ is the censoring indicator, if δi=0 indicates right censoring time and δi=1 indicates no censoring, Xi=(xi1, …, xip) represents the p-dimension covariates. By the Cox's proportional hazards model, the hazard function can be defined as: h(t|β)=h0(t)exp(βTX), where baseline hazard function h0(t) is unspecified or unknown and β=(β1, β2, …, βp) is the regression coefficient vector of p variables. Let ordered risk set at timeP(β)=∑βqbe denoted by Rr={j∈1, …, n:tj>ti}, based on the available sample data, to estimate the Cox model is to through minimizing the Cox's partial log likelihood function:(1)l(β)=∑i=1nδixiTβ−log∑j∈Riexp(xjTβ)In practice, not all the n covariates may contribute to the prediction of survival outcomes: some components of βj(j=1, 2, …, p) may be zero in the true model. When the sample size goes to infinity, an ideal model selection and estimation procedure should be able to identify the true model with probability one, and provide consistent and efficient estimators for the relevant regression coefficients. Therefore, the regularization methods are applied to address this problem. When adding a regularization term (also call the penalty function) to Eq. (1), the penalized Cox model can be modeled as:(2)β=argminl(β)+λ∑j=1pP(βj)where λ>0 is a tuning parameter and P(β) is the regularization term. Recently a series of regularization methods were proposed for the Cox's proportional hazards model, the popular regularization technique is Lasso (L1) [12] which has the regularization termP(β)=∑β. Many others L1 type regularization terms also have been proposed to solve the Cox model, such as SCAD [8], adaptive Lasso [9]. By shrinking some regression coefficients to zero, these methods select important variables and estimate the regression model simultaneously.The aforementioned penalized Cox model methods were based on the L1 type penalties. Theoretically, the Lq(0<q<1) type regularizationP(β)=∑βqwith the lower value of q would lead to better solutions with more sparsity. However when q is very close to zero, difficulties with convergence arise. Therefore, Xu et al. [13] further explored the properties of the Lq(0<q<1) regularizations and reveals the extreme importance and special role of the L1/2 regularization. They proposed that when 1/2<q<1, the L1/2 regularization can yield most sparse results and its difficulty with convergence not very high compared with that of the L1 regularization, while when 0<q<1/2, the performance of the Lqpenalties makes no significant difference and solving the L1/2 regularization is much simpler than solving the L0 regularization. Therefore, the L1/2 regularization can be taken as a representative of the Lq(0<q<1) regularizations. In this paper, we proposed an efficient version of the L1/2 penalized Cox model as follows:(3)β1/2=argminl(β)+λ∑j=1pβj1/2The L1/2 regularization has been demonstrated many attractive properties, such as unbiasedness, and oracle properties. The theoretical and experimental analyses show that the L1/2 regularization is a competitive approach. Motivated by the fact, the penalized Cox model with the L1/2 regularization is naturally expected.The coordinate descent algorithm [14,15] is a “one-at-a-time” approach, which can apply to the L1 type penalized regression. Its basic procedure of the coordinate descent algorithm is as follow: for each coefficient, to partially optimize the target function with respect to βj(j=1, 2, …, p), the remaining elements of β fixed at their most recently updated values, iteratively cycling through all coefficients until converged. Before introducing the coordinate descent algorithm for the L1/2 penalized Cox model, we first consider the linear regression case. Suppose the dataset D has n samples,D=(Xi,yi)i=1n, where Xi=(xi1, xi2, …, xip) is ith input variables with dimensionality p and yiis response variable for the ith observation. Assume the variables are standardized, so that∑i=1nxij=0,∑i=1nxij2=1and∑i=1nyi=0.The linear regression with the regularization term can be modeled as:(4)L(β)=argmin1n∑i=1n(yi−X′β)2+λ∑j=1pP(βj)where P(β) is the regularization term. The coordinate descent algorithm solves βjand other βk≠jare fixed (k≠j notes the parameters remained after jth element was removed). Eq. (4) can be rewritten as:(5)L(β)=argmin1nyi−∑k≠jxikβk+xijβj2+λ∑k≠jP(βk)+λP(βj)The first order derivative at βjcan be estimated as:(6)∂L∂βj=∑i=1n−xijyi−∑k≠jxikβk−xijβj+λPβj′=0Definey˜i(j)=∑k≠jxikβkas the partial residual for fitting βjandωj=∑i=1nxij(yi−y˜i(j)), the coordinate descent algorithm for the L1 regularization (Lasso) can be solved by using the univariate soft thresholding operator:(7)βj=S(ωj,λ)=ωj+λifωj<−λωj−λifωj>λ0ifωj<λSimilarly, for the L0 penalty, the thresholding operator of the coordinate descent algorithm can be defined as:(8)βj=H(ωj,λ)=ωI(|ωj|>λ)H(ωj, λ) is the univariate approximate solution to the L0 regularization. It is equivalent to a hard thresholding operator.According to Eqs. (7) and (8), we can know that the different penalties are associated with the different thresholding operators. Therefore, Xu et al. [13] proposed a half thresholding operator to solve the L1/2 regularization for the linear regression model. It is an iterative algorithm and can be seen as a multivariate half thresholding approach. In this paper, a novel univariate half thresholding operator of the coordinate descent algorithm for the L1/2 regularization is proposed, and can be expressed as:(9)βj=Half(ωj,λ)=23ωj1+cos2(π−φλ(ωj))3ifωj>34(λ)2/30otherwiseSince the aforementioned coordinate descent algorithms cannot directly be applied for the nonlinear Cox model to obtain parameter estimates, Tibshirani [7] proposed a modified Newton–Raphson iterative procedure to reformulate the partial likelihood function of the penalized Cox model. Specifically, let η=Xβ, μ=−∂l(β)/∂η, A=−∂2l/∂ηηTand z=η+A−μ, by a one term Taylor expansion, l(β) can then be represented as (z−η)TA(z−η). According to the approach of Gui and Li [16], this approximation can be rewritten as(Y−Xˆβ)T(Y−Xˆβ), wherezˆ=(A1/2)⋅zandXˆ=(A1/2)⋅X. Thus, we can directly apply the coordinate descent algorithm to the L1/2 penalized Cox model with the Newton–Raphson iterative procedure, and the details are given follows:The coordinate descent algorithm for the L1/2 penalized Cox modelStep 1: Initial all βj=0 (j=1,2,…,p) and λ, set m=0.Step 2: Compute η(m), μ(m), A(m),Xˆ(m),zˆ(m)based on current β(m)Step 3: Solve(zˆ(m)−Xˆβ(m))T(zˆ(m)−Xˆβ(m))+λ∑j=1pβj(m)1/2repeat the following:Cycle over j=1, …, p, until β(m) not change:Calculateωj(m)=∑i=1nxˆij(zˆi(m)−zˆi(j)(m)), wherexˆijis a element ofXˆ(m)=(A1/2(m))⋅Xandzˆi(j)(m)=∑k≠jxˆikβk(m). Then update βj(m)=Haft(ωj, λ)Step 4: Let m=m+1, repeat Steps 2 and 3 until β(m) convergenceNote that updating formulas by calculating weights samples (Xˆ(m)=(A1/2(m))⋅X) and pseudo-responses (zˆ(m)) based on the current β(m) for each Step 2. The coordinate descent algorithm with the L1/2 penalty works well in the sparsity problems, because the procedure does not need to change many irrelevant parameters and to recalculate partial residuals for each update step.In order to assess performance of the L1/2 penalized Cox model, several simulation studies were conducted under different data scenarios. The selection of the tuning parameters and the measures for comparing the performances of prediction models are introduced in Section 4.1. The comparison results of the L1/2 penalized Cox model with other existing methods on the simulated and real DNA microarray datasets are reported in Sections 4.2 and 4.3, respectively.To select the tuning optimal parameter by cross-validation in the Cox model, we use a special approach proposed by van Houwelingen et al. [6], which is defined as:(10)CV(λ)=∑i=1k{l(β(−i)(λ))−l(−i)(β(−i)(λ))}where β(−i)(λ) represents the estimation of β that is obtained by the penalized method when the kth fold is left out. The term l(β) is the log partial likelihood, and l(−i)(β) is the log partial likelihood without the ith fold. The optimal tuning parameter λ is obtained by maximizing CV(λ). Note that the choice of k will depend on the size of the dataset. In our experiments, we used 5-fold cross validation (k=5).The predictive performance measures of censored survival data are more complicated: these measures can only be computed if the case is not right censoring. Thus, several specially designed measure methods have been proposed in the literatures. In this paper, we employ the integrated Brier-Score (IBS) [17] and the concordance index (CI) [18] to evaluate the prediction ability of the survival data analysis methods.The Brier Score (BS) is defined as a function of time t>0 by:(11)BS(t)=1n∑i=1nSˆ(t|Xi)21(ti≤t∧δi=1)Gˆ(ti)+(1−Sˆ(t|Xi))21(ti>t)Gˆ(t)whereGˆ(⋅)denotes the Kaplan–Meier estimation of the censoring distribution andSˆ(⋅|Xi)stands to estimate survival for the patient i. Note that the BS(t) is dependent on the time t, and its values are between 0 and 1. The good predictions at the time t result in small values of BS. The integrated Brier Score (IBS) is given by:(12)IBS=1max(ti)∫0max(ti)BS(t)dtThe IBS is used to assess the goodness of the predicted survival functions of all observations at every time between 0 and max(ti).The Concordance Index (CI) can be interpreted as the fraction of all pairs of subjects which predicted survival times are correctly ordered among all subjects that can actually be ordered. By the CI definition, we can determine ti>tjwhen fi>fjand δj=1 where f(·) is survival function. The pairs for which neither ti>tjnor ti<tjcan be determined are excluded from the calculation of CI. Thus, the CI is defined as:(13)CI=∑i∑j1(fi<fj∧δi=1)∑i∑j1(ti<tj∧δi=1)Note that the values of CI are between 0 and 1, the perfect predictions of the building model would lead to 1 while have a CI of 0.5 at random.We carried out a simulation study to evaluate the parameter estimation, model selection and prediction capability performance of the three penalized Cox model methods, Lasso, SCAD and L1/2 penalties. We adopted the Cox model simulation scheme in Bender's work [19]. The data generation procedure is as follows:Step 1:We generate γi0, γi1, …, γip(i=1,…,n) independently from standard normal distribution and set [20]:Xij=γij1−ρ+γi0ρ(j=1,…, p) where ρ is the correlation coefficient.The survival time Ti(i=1,…,n) is constructed from a uniformly distributed variable U by Ti=(1/α)log(1−(α×log(U)/ωexp(βX))), where ω is the scale parameter, α is the shape parameter, β is the ground-true regression coefficients.Censoring time pointT′i(i=1,…,n, n indicates sample size) is obtained from an exponential distribution E(θ), where θ is determined by specify censoring rate.Here we defineti=min(Ti,T′i)andδi=I(Ti≤T′i), the observed data represented as (ti, δi, Xi) for the Cox model (Eq. (1)) are generated.For our experiments, we generate high-dimensional and low sample size datasets. In every simulation, the dimension of the predictor genes is p=500, the coefficients of the three prognostic genes are: β1=3, β4=−2, β7=1, and the coefficients β of the remaining 497 genes are zeros. About 25% of the data are right censored. We consider the cases with the training sample size n=50, 100, 200 and the correlation coefficients ρ=0 and 0.3, respectively. To assess the variability of the experiment, each method is evaluated on a test set including 50 samples, and replicated over 50 random training and test partitions.Table 1shows the average number of genes and prognostic genes selected by three methods in 50 runs. The recovery rate is defined as the ratio of the average number of the prognostic genes to the average number of genes selected [20]. Overall the Lasso method selected the largest number of genes and the L1/2 penalized method selected the least. When the training sample size is related small (n=50), all the methods selected the prognostic genes difficultly. However, as n is increased, they can select almost all the three prognostic genes. The recovery rate of the L1/2 penalized method was the highest, the SCAD was the second, and the Lasso was the lowest. All three methods perform better at the correlation coefficients ρ=0 than at the correlation coefficients ρ=0.3 as expected.Fig. 1displays the solution paths of the three methods in one selected simulation run. Here the x-axis displays the number of running steps, the y-axis is the coefficients. The optimal results obtained by the three methods are shown as vertical dotted lines. From Fig. 1, it clearly shows that the computational results of the L1/2 penalized Cox model are more sparse than those of the Lasso and SCAD penalized methods. In summary, the L1/2 penalized method can select almost all prognostic genes while discarding the vast of irrelevant genes.To evaluate prediction performance of the three penalized Cox models, we presented their average IBC and CI values on the simulated datasets among 50 times in Table 2. In terms of IBC and CI, the three penalized methods performed similar for different parameter settings, and their differences seem to be marginal. Combined with the results reported in Table 1, we concluded that the L1/2 penalized method showed similar predictive performance as the other methods with a more succinct model.In this section, we evaluated the performance of the prediction methods on the real survival gene expression datasets. To further evaluate the performance of the L1/2 penalized Cox model, we added three other type prediction methods which have been applied to high-dimensional biological data in this comparison: principal components regression (PCR) [21], supervised principal components regression (SPCR) [3,4], and partial least squares (PLS) [22].Five publicly available datasets are used in this part. A brief description of these datasets is given below and summarized in Table 3.Diffuse large B-cell lymphoma dataset (DLBCL)2002: This dataset published by Rosenwald et al. [23]. The dataset consists of 240 samples from patients. For each sample, 7399 gene expression measurements were obtained. The clinical outcome was survival time, either observed or censored.Diffuse large B-cell lymphoma dataset(DLBCL)2003: This dataset is from Rosenwald et al. [24]. It consists of 92 lymphoma patients, and each patient contain 8810 genes.Dutch breast cancer dataset(DBC): The Dutch breast cancer dataset is from van Houwelingen et al. [6], and consists of survival times and gene expression measurements from 295 women with breast cancer. The expression levels of p=4919 genes were available.Lung cancer dataset: The lung cancer dataset is from Beer et al. [25]. It consists of gene expressions of 4966 genes for 83 patients. The survival time as well as the censoring status is available.AML dataset: The AML dataset is from Bullinger et al. [26]. It contains the expression profiles of 6283 genes for 116 patients, and the number of censored cases is 49.We evaluated the prediction accuracy of the six estimated models using random partition: the training set of about 2/3 of the patients used for estimation and the test set of about 1/3 of the patients used for testing of the prediction capability. For estimating λ, we employed the 5-fold cross validation scheme using the training set. We repeated each procedure over 50 times.Table 4shows the average number of genes selected by each approach on the five real datasets. We can see that the genes selected by the penalized type methods (Lasso, SCAD and L1/2) are significantly less than those of the non-penalized type methods (SPCR, PCR and PLS). The PLS and PCR methods selected all genes. Their performances are achieved at the expense of employing a quite large number of genes in the model, and are not suitable for knowledge discovery tasks. Among of the three penalized methods, the number of genes selected by the L1/2 penalized method was the smallest, the SCAD was the second, and the Lasso was the largest.With respect to prediction accuracy, Tables 5 and 6summarize the results of IBS and CI obtained by the six methods, respectively. In terms of IBS, the SPCR method performed a bit better than the other methods on the DLBCL(2002) dataset, the PCR method got the bit better performance on the DLBCL(2003) dataset, the Lasso method performed best on the AML dataset, the SCAD method performed best on the Lung cancer and AML datasets, the L1/2 method performed best on the Dutch breast cancer dataset respectively. However, the results of all the methods were not much different. On the other hand, from terms of CI in Table 6, different methods yielded slightly larger CI value than the others on different datasets. For example, the SPCR method performed best with an CI of 0.5942 on the DLBCL(2002) dataset, and the L1/2 method performed best on the DLBCL(2003) and Dutch breast cancer datasets, respectively. The predictive performances of those methods were similar and no one approach dominates the others. Combined with the results reported in Tables 4–6, we concluded that the prediction accuracies are comparable between all methods, and the L1/2 penalized method selected the smaller subset of the prognostic genes, especially compared with the non-penalized methods. This is an important consideration for clinical applications, where the goal is often to develop an accurate predicting test using as few genes as possible in order to control cost. This indicates that if an analyst wants an extremely small subset of prognostic genes, the L1/2 penalized method could be the best approach to use among these discussed methods.

@&#CONCLUSIONS@&#
