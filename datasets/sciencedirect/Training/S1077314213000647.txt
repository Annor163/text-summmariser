@&#MAIN-TITLE@&#
2D/3D image registration using regression learning

@&#HIGHLIGHTS@&#
A novel rigid or deformable 2D/3D image registration method using regression learning.The method learns a shape space of geometric transformation from the 3D image set.The method uses multi-scale linear regressions on the 2D image intensity residues.Target problem is registration between planning time and treatment time in radiotherapy.

@&#KEYPHRASES@&#
2D/3D registration,Regression,Machine learning,IGRT,Radiation therapy,

@&#ABSTRACT@&#
In computer vision and image analysis, image registration between 2D projections and a 3D image that achieves high accuracy and near real-time computation is challenging. In this paper, we propose a novel method that can rapidly detect an object’s 3D rigid motion or deformation from a 2D projection image or a small set thereof. The method is called CLARET (Correction via Limited-Angle Residues in External Beam Therapy) and consists of two stages: registration preceded by shape space and regression learning. In the registration stage, linear operators are used to iteratively estimate the motion/deformation parameters based on the current intensity residue between the target projection(s) and the digitally reconstructed radiograph(s) (DRRs) of the estimated 3D image. The method determines the linear operators via a two-step learning process. First, it builds a low-order parametric model of the image region’s motion/deformation shape space from its prior 3D images. Second, using learning-time samples produced from the 3D images, it formulates the relationships between the model parameters and the co-varying 2D projection intensity residues by multi-scale linear regressions. The calculated multi-scale regression matrices yield the coarse-to-fine linear operators used in estimating the model parameters from the 2D projection intensity residues in the registration. The method’s application to Image-guided Radiation Therapy (IGRT) requires only a few seconds and yields good results in localizing a tumor under rigid motion in the head and neck and under respiratory deformation in the lung, using one treatment-time imaging 2D projection or a small set thereof.

@&#INTRODUCTION@&#
In a variety of situations of image-guided therapy in medicine, a 2D/3D geometric transformation is required to relate a 3D image of the patient used in planning the treatment with a set of 2D images acquired at treatment time [1]. Current 2D/3D registration methods [2–5] find the transformation that optimizes an objective function consisting of an image matching term and a regularization term. As a fast optimization often requires many evaluations of the function’s Jacobian, optimization-based registration methods without further parallelization are structurally slow unless often unrealistically accurate initialization is provided. With GPU parallelization recent optimization-based 2D/3D registration methods are able to localize the tumor within 1s assuming rigid patient motion [6,7] or non-rigid motion [8,9]. However, the mismatch in the registration dimensionality often introduces a non-convex objective function which is prone to optimization solutions that are caught in local minima (i.e., non-global solutions) with normally available initializations. In order to avoid local minima and to reduce the registration time, Li et al. [8,9] adopted a bootstrap-like approach where optimizations were initialized by registration results from previous time points. Their optimizations were fully-implemented on high-end GPUs and obtained sub-second speed. Other methods have used neural networks to model rigid [10–12], or non-rigid transformations [13] and to achieve efficient computation at registration time. However, to the best of our knowledge, there is no general framework that supports both rigid and non-rigid 2D/3D registration. We have sought a learning-based framework that is fast, general to both types of registration, robust to normally available initializations, and not based on optimization.In this paper, we describe the methodology of our general learning-based framework that was initially presented in Chou et al. [14] for rigid registration and Chou et al. [15] for non-rigid registration, respectively. Steininger et al. [16] subsequently presented a similar approach for rigid registration. In a way similar to the face alignment algorithm AAM (Active Appearance Model) by Cootes et al. [17] and the efficient tracking scheme by Jurie and Dhome [18], we seek a linear operator M, calculated by linear regression, that when iteratively applied to intensity differences (residue) R between digitally-reconstructed radiographs (DRRs), i.e., projections, of the currently estimated 3D image and the measured 2D images, yields the update of the estimated transformation parametersΔC^that reduce the residue.(1.1)ΔC^=M·RThe registration process in Eq. (1.1) requires no optimizations; therefore it can support efficient registration. Different from the AAM, our linear operator M estimates the 3D transformation parameters from 2D projection intensity residues R for the 2D/3D registration.The paper is organized as follows. First, we describe our 2D/3D registration framework and our efficient approximation of the shape parameters C in Section 2. In Section 3, we describe how we obtain low-order parameterization for rigid motion and for a deformation shape space. In Section 4, we describe our regression learning to calculate the linear operator M and an efficient multi-scale learning scheme. In Section 5, we describe how we generate commensurate projection intensities to support our regression estimation. In Section 6, we describe the experimental setup and clinical context of our medical application. In Section 7, we present our registration results and compare them to those of an optimization-based method. In Section 8, we discuss our rigid and non-rigid registration results.We first describe the general framework of our 2D/3D image registration method. Second, we describe our approach for efficient registration within this framework.The goal of the 2D/3D registration is to match a transformed 3D grey-scale source image to a set of target 2D projections Ψ. We denote the projection intensity at pixel location x=(x1,x2) and projection angle θ as Ψ(x;θ). The registration is formulated as an iterative process. Let I denote the 3D source image and I(t) denote the 3D image at iteration t. The estimated 3D image region’s motion/deformation parametersC^(t)define a geometric transformationT(C^(t))in a shape space determined from the 3D images. TheC^(t)are calculated by the estimated parameter updatesΔC^(t)(Eq. (2.1)) obtained from the projection intensity residues R between the target 2D projections Ψ(x;θ) and the computed projections P(x,I(t−1);θ) of the transformed 3D source image at iteration t−1 (Eq. (2.2)). After parameter estimation in each iteration, an image transformation (Eq. (2.3)) is required in order to produce updated computed projections for the parameter estimation in the next iteration.(2.1)C^(0)=0C^(t)=C^(t-1)+ΔC^(t)(2.2)R[Ψ(x;θ),P(x,I(t-1);θ)]=Ψ(x;θ)-P(x,I(t-1);θ)(2.3)I(t)=I(0)∘T(C^(t))I(0)=IT(0)=IdId is the identity transformation. The projection operator P is formulated by a simulation of the imaging process. For example, in the medical literature, to simulate a 3D image’s X-ray projections from its 3D volume (DRRs), we use ray-casting to compute the photon attenuation through a given imaging geometry (Fig. 2.1). We note that although Eq. (2.2) indicates a simple subtraction of the projection of the 3D image from the target projection, in actual clinical application one must apply additional processing to account for X-ray scatter in the target projection. This will be explained further in Section 5.One way to obtain the estimated parameter updatesΔC^(t)is by optimizing a measure ρ of the concatenated intensity residue R† with respect to the parameter updates ΔC. The concatenated intensity residues R†, defined as the concatenation over all of the projection angles θ of the residuesRθ:R†=(Rθ1,Rθ2,…,RθΓ).(2.4)ΔC^(t)=argminΔC‖R†[Ψ(x),P(x,I(0)∘T(C^(t-1)+ΔC))]‖ρWithout parallelization, iterative computations to carry out this optimization are structurally slow. Moreover, the optimization may easily converge to a local minimum since the energy functional in Eq. (2.4) is not convex. See Section 7.2.3 for the detailed evaluation of the optimization-based approach.We propose an alternative method to calculate ΔC using multi-scale linear operators M. At each iteration of the registration, our method estimates the motion/deformation parameter updatesΔC^(t)by applying a linear operator Msof scale s to the current concatenated intensity residue R†. That is,(2.5)ΔC^(t)=R†[Ψ(x),P(x,I(t-1))]·Ms,wheres=1,2,…,S;t=1,2,…,tmaxTypically, S=4 and tmax⩽10 are satisfactory. The computation in Eq. (2.5) involves only matrix multiplications by Ms, computation of the projections P, and subtractions (Eq. (2.2)). This makes the registration structurally fast. The calculation of the multi-scale linear operators M involves a machine learning process described in detail in Section 4. Due to the leveragable advantage of the machine learning process and the fast linear operation, our proposed method shows a more robust and faster registration than the optimization-based approach. See Section 7.2.3 for the comparisons.Our method limits the motion/deformation to a shape space. To allow M to be accurately learned, we require a low-order parametrization C of this shape space. We describe the shape space calculation for rigid motions and for non-rigid deformations in Sections 3.1 and 3.2 respectively.Rigid motions are modeled explicitly as the variation in the Euler’s six dimensional rigid space:(3.1)C=(tx,ty,tz,rx,ry,rz)where tx, ty, tzare the translation amounts in cm along the world’s coordinate axes x, y, z, respectively; and rx, ry, rzare the rotations in degrees (°) about the image center, around the world coordinate axes x, y, and z, in succession.Like others [19,9], we model deformations as a linear combination of a set of basis deformations calculated through principal component analysis (PCA). In our target problem, a cyclically varying set of 3D images {Jτover time τ} are available at pre-registration learning time. From these a mean imageJ¯and a set of deformations ϕτbetween JτandJ¯can be computed. The basis deformations are chosen to be the primary eigenmodes of the PCA of the ϕτ. The computed mean imageJ¯will be used as the reference mean image I throughout this paper.In order to model the deformation space realistically, our method computes a Fréchet mean imageJ¯via an LDDMM (Large Deformation Diffeomorphic Metric Mapping) framework [20] from the cyclically varying set of 3D images {Jτover time τ}. The Fréchet mean, as well as the diffeomorphic deformations ϕ from the mean to each image Jτ, are computed using a fluid-flow distance metric dfluid[21]:(3.2)J¯=argminJ∑τ=1Ndfluid(J,Jτ)2(3.3)=argminJ∑τ=1N∫01∫Ω‖vτ,γ(x)‖2dxdγ+1α2∫Ω‖J(ϕτ-1(x))-Jτ(x)‖2dxwhere Jτ(x) is the intensity of the pixel at position x in the image Jτ, vτ,γis the fluid-flow velocity field for the image Jτin flow time γ, α is the weighting variable on the image dissimilarity, and ϕτ(x) describes the deformation at the pixel locationx:ϕτ(x)=x+∫01vτ,γ(x)dγ.The mean imageJ¯and the deformations ϕτare calculated by gradient descent optimization. The set {ϕτover τ} can be used to generate the deformation shape space by the following statistical analysis.Starting with the diffeomorphic deformation set {ϕτ}, our method uses PCA to find a set of linear deformation basis functionsϕpci. The scoresλτi(basis function weights) for eachϕpciyield ϕτin terms of these basis functions.(3.4)ϕτ=ϕ¯+∑i=1Nλτi·ϕpciWe choose a subset of n eigenmodes that capture 95% of the total variation. Then we let the n basis function weights λiform the n-dimensional parameterization C.(3.5)C=(c1,c2,⋯,cn)(3.6)=(λ1,λ2,⋯,λn)From the motion/deformation shape space we calculate linear operators M that correlate coarse-to-fine sampled model parameters C with the corresponding projection intensity residue vectors R. We describe our regression learning to calculate the linear operators M in Section 4.1 and an efficient multi-scale learning strategy in Section 4.2.As detailed in Section 4.2 we select a collection of model parameters {Cκover cases κ} for learning. Each case is formed by a selection of parameter settings. The training uses deviations from the reference image, such that ΔC=Cκ. Linear regression is used to correlate the selected modeled parameters Cκin the κth case with the co-varying projection intensity residue set {Rκ,θover the projection angles θ}. Rκ,θ(x) is computed as the intensity difference at pixel location x=(x1,x2) between the projection at angle θ of the mean image I (or an untransformed 3D image for the rigid case) and the projection of the image I∘T(Cκ) transformed with the sampled model parameter Cκ:(4.1)Rκ,θ(x)=P(x,I∘T(Cκ);θ)-P(x,I;θ)We concatenate the residues at each projection angle to formulate a residue set in a vectorRκ†=(Rκ,θ1,Rκ,θ2,…,Rκ,θΓ)and build a linear regression for all cases κ=1,2,…,K:(4.2)C1C2⋮CK≈R1†R2†⋮RK†·MThe regression matrix M that gives the best estimation of the linear operators per parameter scale is computed via a pseudo-inverse:(4.3)M=(R†R†)-1R†CTo provide adequate regression learning, C must be sufficiently sampled to capture all the shape variations. However, the direct implementation requires an exponential time computation. Instead, we have designed an efficient scheme that learns the model parameters from large to small scales, 1 to S, to yield S scale-related regression matrices M1,M2,…,MS. At the sth scale of learning, each model parameter ciis collected from the combinations of ±3σi·(S−s+1)/S and 0 where σiis the standard deviation of the basis function weights λiobserved at pre-registration time. In the registration stage the calculated multi-scale linear operators are applied sequentially, from M1 to MS, to give new estimations of the model parameters from large to small scale. After evaluating the estimation accuracy for target examples of both the rigid and non-rigid types, we found that four scales of learning (S=4) produced sufficiently dense samples in C to achieve the required registration accuracy.X-ray scatter is a significant contribution to the cone-beam CT projections. However, the regression estimators M are not invariant to the projection intensity variations caused by X-ray scatter. Therefore, our method uses a normalization filter (Section 5.1) and a subsequent histogram matching scheme (Section 5.2) to generate commensurate intensities between learning-time computed projections and registration-time target projections.To account for variations caused by X-ray scatter, we perform a 2D Gaussian-weighted normalization for each pixel in the learning projections (Fig. 5.1d) and the target projections (Fig. 5.1b). To calculate the normalized value Ψ′(x;θ) at pixel location x=(x1,x2) and projection angle θ, we subtract a Gaussian-weighted spatial mean μ′(x1,x2) from the raw pixel value Ψ(x1,x2) and divide it by a Gaussian-weighted standard deviation σ′(x1,x2).(5.1)Ψ′(x1,x2)=Ψ(x1,x2)-μ′(x1,x2)σ′(x1,x2)(5.2)μ′(x1,x2)=∑ξ=x1-Ax1+A∑η=x2-Bx2+BG(ξ,η;0,w)·Ψ(ξ,η)(2A+1)×(2B+1)(5.3)σ′(x1,x2)=∑ξ=x1-Ax1+A∑η=x2-Bx2+BG(ξ,η;0,w)·Ψ(ξ,η)-μ′(x1,x2)2(2A+1)×(2B+1)12where 2A+1 and 2B+1, respectively, are the number of columns and rows in the averaging window centered at (x1,x2); the function G is a zero mean Gaussian distribution with a standard deviation w. We choose A, B, and w to be a few pixels to perform a local Gaussian-weighted normalization for our target problem (see Section 6).In order to correct the intensity spectrum differences between the normalized learning projectionΨlearning′and the normalized target projectionΨtarget′, a function Fωof intensity to achieve non-linear cumulative histogram matching within a region of interest ω is applied. To avoid having background pixels in the histogram, the region ω is determined as that pixel set whose intensity values are larger than the mean value in the projection. That is, Fωis defined by(5.4)FωHfΨtarget′≈HfΨlearning′where Hfis the cumulative histogram profiling function. The histogram matched intensitiesΨtarget☆(Fig. 5.1c) are calculated through the mapping:(5.5)Ψtarget☆=Ψtarget′∘FωWe describe the experimental setups for evaluating the method and provide some clinical context. Our target problem is IGRT (Image-guided Radiation Therapy). There the 3D image I is the planning CT (Computed Tomography), and the target projection images Ψ are treatment-time imaging kV projections. In particular, the kV projections are produced by (1) a rotational CBCT (Cone-beam CT) imager or (2) a stationary NST (Nanotube Stationary Tomosynthesis) imager specified in Maltz et al. [22]. Our method’s application to IGRT, referred to as Correction via Limited-Angle Residues in External Beam Therapy, or CLARET [14,15], has shown promise in registering the planning CT to the treatment-time imaging projections. We describe the two treatment imaging geometries in Section 6.1 and CLARET’s application to head-and-neck IGRT and lung IGRT in Sections 6.2 and 6.3, respectively.A CBCT is a rotational imaging system with a single radiation source and a planar detector, which are mounted on a medical linear accelerator. This pair rotates by an angle of up to 2π during IGRT, taking projection images Ψ during traversal (Fig. 6.1a). A limited-angle rotation provides a shortened imaging time and lowered imaging dose. For example, a 5° rotation takes ∼1s. In our application, CBCT projections were acquired in a half-fan mode. Half-fan mode means that the imaging panel (40cm width by 30cm height, source-to-panel distance 150cm) is laterally offset 16cm to increase the CBCT reconstruction diameter to 46cm. The method’s linear operators are trained for projection angles over 360° at 1° intervals beforehand at planning time. At treatment time the method chooses the linear operator that is closest to the current projection angle.An NST is a stationary imaging system mounted on a medical linear accelerator that can perform imaging without interfering with treatment delivery. As illustrated in Fig. 6.1b, it consists of an arrangement of radiation sources arrayed around the treatment portal, together with a planar detector. The geometry thus is fixed and known beforehand. Firing the sources in sequence produces a sequence of projection images at different orientations. Each projection image requires ∼200ms.In head-and-neck IGRT, the geometric differences of the skull between planning time and treatment time can be represented by a rigid transformation. Therefore, in the pre-registration learning, CLARET samples clinically feasible variations (±2cm,±5°) in the Euler’s 6-space C to capture the treatment-time patient’s motions. With a single planning CT I of the patient, the computed learning projections P(x,I∘T(C);θ) are generated by transformation of the feasible variations T(C) and projection from a given angle θ of the transformed 3D volume I∘T(C).In the registration, CLARET iteratively applies S multi-scale linear operators M1 to MSto estimate the rigid transformation from the 2D intensity residues formed by the difference between the normalized target projections Ψ☆ and the normalized projections computed from the currently estimated rigid transformation applied to the planning-time 3D image.A consideration in lung IGRT is that respiratory motion introduces non-rigid transformations. In the pre-registration learning stage, a set of 10-phase RCCTs (Respiratory-correlated CTs) collected at planning time serve as the cyclically varying 3D images {Jτover the phase τ}. This image set is used to generate the deformation shape space C. From these RCCTs, a Fréchet mean imageJ¯and its deformations ϕτto the corresponding images Jτare calculated via an LDDMM framework. Fig. 6.2c shows an example respiratory Fréchet mean image. The deformation basis functions ϕpcare then generated by PCA on the deformation set {ϕτover phase τ}. Liu et al. [19] have shown that a shape space with three eigenmodes adequately captures 95% respiratory variations experienced at treatment time. Fig. 6.3shows the first two principal deformation basis functions.To generate feasible variations in the deformation space C for learning the linear operator M, CLARET samples the largest scale of parameters by three standard deviations of the basis function weights derived from the RCCT image set. From the Fréchet mean image the computed projections P(x,I∘T(C);θ) are generated by (1) transformation based on the feasible variations T(C) and (2) projection from a given angle θ to the transformed 3D volume I∘T(C).Just prior to treatment, the Fréchet mean image obtained at planning time is rigidly registered to the CBCT for correcting patient position. During treatment with planar imaging, CLARET iteratively applies S multi-scale linear operators, from M1 to MSto estimate the weights C on the basis functions ϕpcfrom current 2D intensity residues. The residues are formed by the difference between the normalized and histogram matched target projections Ψ☆ (Fig. 5.1c) and the normalized projections (Fig. 5.1d) computed from the presently estimated deformation applied to the Fréchet mean image.

@&#CONCLUSIONS@&#
