@&#MAIN-TITLE@&#
A new hybrid PSO algorithm based on a stochastic Markov chain model

@&#HIGHLIGHTS@&#
Development of a new hybrid PSO algorithm.Parallelism with a Markov chain model.Testing of the newly developed algorithm on classic benchmark functions.

@&#KEYPHRASES@&#
Particle Swarm Optimization,PageRank,Optimization,Markov chains,Population topology,Algorithms,

@&#ABSTRACT@&#
Based on the recent research concerning the PageRank Algorithm used in the famous search engine Google [1], a new Inverse-PageRank-Particle Swarm Optimizer (I-PR-PSO) is presented in order to improve the performances of classic PSO. The resulted algorithm uses a stochastic Markov chain model to define an intelligent topological structure of the swarm’s population, in which the better particles have an important influence on the others. In the presented experiments, calculations on some benchmark functions classically used to test optimization methods are performed, and the results are compared to different versions of the standard PSO, that is using different topological structures of the population. The experimental results show that I-PR-PSO can converge quicker on the tested functions, and can find better results in the solution domain than its tested peers.For decades, the field of optimization has been explored as an active research area. An unconstrained optimization problem can be formulated as D-dimensional minimization problems as follow(1)Minf(x→)x→=(x1,x2,x3,…,xD)where D is the number of design variables to be optimized, that is the dimension of the problem, and f is the objective function to minimize. The past few years saw the development of many different optimization techniques. The population-based metaheuristic methods have been demonstrated and defined as very useful and efficient, even though there is not any mathematical evidence of their convergence to the global optimum. In fact, those methods consider a population of solutions instead of a single one. Using some stochastic parameters, they can converge efficiently to the global optimum. They are generally inspired by physical or biological phenomena, such as the Ant Colony Optimization [2] which draws its inspiration from the foraging behavior of some ant species, the evolutionary algorithms [3–5] which mimic the process of natural evolution, using processes such as inheritance, mutation, selection and crossover. The Particle Swarm Optimization Algorithm (PSO) comes from the observation of some flocks of birds by Reynolds [6] in 1987, and has been developed by Kennedy and Eberhart [7] in 1995. Understanding how the birds can achieve their complex and optimal movement, a new optimization method which uses a swarm of potential solutions has been proposed. Because of stochastic parameters, these solutions can follow the best particles and converge together to the global optimum of the considered objective function. Lately, lots of different improvements of PSO concerning the population’s topology have been presented in the literature [8–18]. The population’s topology defines how the particles are structured, thus defines the influence they have on each others. The first population’s topology proposed in the literature is statical. Therefore, each particle is always influenced by the same other particles all along the calculation. For example, Eberhart and Kennedy [9] have developed the well known LBEST11Local Best particle’s topologyand GBEST22Global Best particle’s topologytopologies. In the classical GBEST population topology, the entire population is treated as the individual’s neighborhood [19]. Eventually, the particles are influenced by the global best one, as one can see in Fig. 1. In the local LBEST version, the particles are linked with two of the other particles. The population topology is then a ring, as one can see in Fig. 2, and the best performance of each particular neighborhood is chosen between the two particles of its neighborhood.In their work, Mendes et al. [19] proposed different statical population topologies, such as the pyramid, which is a three-dimensional wire-frame triangle, the Von Neumann, which is a square lattice whose extremities connect as a torus, and the four clusters one, in which four clusters of particles are completely interconnected, connected among themselves by a few short-cuts, as one can see in Fig. 3. An approach in which the quality of the solution is considered in a weighted definition of the particles’ moving has also been presented in [8]. In fact, it has been noted by Mendes et al. [20] that all the neighbors of a particle can be a source of influence.The second type of population topology is the evolutive one. This type defines a population topology which is able to change through the iterations of the optimization algorithm. Akbari and Ziarati have applied the concept of ranking to the Particle Swarm Optimization [8]. In this work, at each iteration, the particles are sorted on the basis of their fitness value. Then, the γ best particles are used to influence the moving of the other particles. γ decreases during the iterations, thus the particles are less linked to the others during the optimization process: this algorithm starts with a GBEST topology, and finally the particles are only influenced by the global best one. This type of evolutionary topology has been also used in the work of Suganthan [13] in which the swarm starts linked as a LBEST topology. Then the number of links between the particles are extended during the PSO iterations, to finish with a GBEST topology. In a simpler way, Pasupuleti and Battiti [14] proposed to use only the best particle of the swarm to influence the others in his Gregarious Particle Swarm Optimizer. Janson and Middendorf [15] suggested a hierarchical Particle Swarm Optimizer in which all particles are arranged in a hierarchy tree that defines the neighborhood structure. The particle which achieves the global best fitness is the tree’s root. If a particle finds a better solution than the one found by its direct hierarchical superior in the tree, the particles switch their places. Then, an evolutive topology of the population is provided, and the results proposed are globally better than the classical versions of PSO, that is with the GBEST and LBEST topologies. Jiang et al. [10], Lovbjerg et al. [16] and Blackwell and Branke [18] have proposed to partition the population into sub-swarms to improve the ability of exploration and exploitation. Angeline [11] have proposed a selective mechanism which ranks the particles as a function of the obtained fitness. Then, the worst half part of the swarm is teleported in the area of the best half part, but keep in memory its own best performances. Then, the moving of the swarm can be compared to the evolutionary algorithms, because of the sudden teleportation of the particles in the solution domain. In their work, Mohais et al. [12] generated a random oriented graph, defining the influences of the particles on the others. The topology of the swarm can be redefined randomly, with the static probability prdefined at the beginning of the calculation. In conclusion of this article, it has been shown that evolutionary topology can exhibit better results than algorithms using a statical topology.In this paper, a new efficient population topology based on a stochastic Markov chain model, used as in the inverse PageRank ®algorithm, is proposed. The population topology has the ability to evolve, and the calculations of the particles’ motions are smartly weighted considering the quality of the solution. The linked particles are then considered as a Markov chain, and the quality of the solutions defines the probability transition of the chain, which determines the influence of the particles on the others. Section 2 gives the mathematical bases concerning the classical PSO. In Section 3, the mathematical background concerning the Markov chains and the PageRank algorithm, as well as the inverse PageRank methodology are depicted. Then, based upon the previous mathematical theory, the newly developed I-PR-PSO is proposed. Section 4 describes the simulations performed to test and validate the new optimization process, and the obtained results. Finally, Section 5 concludes the work.Such as the Genetic Algorithms [21], or the Ant Colony Optimization [2], the Particle Swarm Algorithm (PSO) [7] is a population-based metaheuristic optimization method. In PSO, the potential solutions of the optimization problem, called particles, move in the solution domain with a velocity33Called “the velocity” in the literature, this parameter is actually the displacement of the particle in the solution domain., which is adjusted as a function of the position of other particles. All the particles follow the best one during the iterations and converge together to the global optimum of the considered objective function. Then, in the linear version of PSO which considers the neighborhood of the particles, the position of a particle noi at iterationt+1,notedXit+1is given as a function of•Xit: The position of the particle i at the iteration t,Vit: The velocity of the particle i at the iteration t,Gi,bestt: The position of the best particle in the neighborhood of the particle, at iteration t,Pi,bestt: The position of the best personal performance of the particle found at the iteration t.The position change of each particle of the swarm is given in the following manner [22](2){Vit+1=ω×Vit+c1×rand1×(Pi,bestt+1−Xit)+c2×rand2×(Gi,bestt+1−Xit)Xit+1=Xit+Vit+1where c1 and c2 are acceleration factors, ω is the inertia weight defined to control the influence of the previous velocity on the next one [23], and rand1 and rand2 are some random real numbers distributed in [0, 1].The speed of the particles has to be constrained for the calculation to converge. The speed of the particles in then defined in[−Vmax;Vmax]where Vmaxdepends on the solution domain, such asVmax=Xmaxwhere Xmaxis the maximum position of the particles in the domain.A discrete-time Markov chain is a mathematical system that describes the transitions from one state to another, both given in a state space. This stochastic mathematical process is characterized as memoryless, which means that the future and the past are independent from the present state. Formally, a Markov chain is a sequence of Xnrandom variables in a state space E, where Xnis the state of the process at discrete time n. Then, the Markov process is defined such as follow.(3)∀n≥0,∀(p0,p1,…,pn−1,k,l)∈En+2,wehaveP(Xn+1=l|Xn=k,Xn−1=pn−1,…,X0=p0)=P(Xn+1=l|Xn=k)Markov chains can also be described by a sequence of oriented graphs, in which the edges of graph are weighted by the probabilities of going from one state at time n to the following state at timen+1. The process can then be written in a simpler way using the transition probability matrix. If the probability of moving from state k to state l in one time step is notedP(l|k)=Ck,l,then the stochastic transition matrix, also called the stochastic connectivity matrix of the Markov chain, is given by Ck,l, where k is the row number, and l the column number. Since the probability of transitioning from state k to the others is 1, this matrix is a right stochastic matrix and we have(4)∑lCk,l=1Generally speaking, the probability transition of going from one state to another one in m discrete time steps is given byCm. Thus, a stationary probability vectorπis defined as the steady state of the Markov chain model and does not change under application of the transition matrixCover the iterations.πis thus defined by a left eigenvector of the probability matrix associated with eigenvalue 1, and we have(5)πC=πFor a matrix with strictly positive entries, which is the case for the matrixCof a Markov chain, this vector is unique, and can be computed by observing that(6)∀klimm→∞(Cm)k,l=πlNamed after Larry Page, one of the founders of Google®, the PageRank algorithm is a powerfull method to rank the web pages. Actually, a page is important if it is pointed to by other important pages [24]. The web is then considered as an oriented graph, in which the nodes represent the webpages, and the links are weighted by the probability to click on. Thus, in the PageRank model, the web is considered as a Markov chain. The PageRank algorithm is detailed in Appendix.In PSO, the population of the swarm can be seen as an oriented graph. The nodes represent the particles, and the transition probabilities can be seen as the influences of the particles on the others. In this paper, a new PSO algorithm based on the inverse PageRank algorithm is proposed. In the PageRank algorithm, the stochastic connectivity matrix between the nodes of the graph is known, and the PageRank vector is searched. In this work, the exact opposite is done. As said by Newton, talking about the work presented in [25]: “Basically, we are doing the inverse of what Google does. They know the transition probabilities and compute the steady-state, we know the steady-state and compute the transition probabilities.”44http://www.slate.com/blogs/future_tense/2013/03/27/google_pagerank_algorithm_markov_chains_and_cancer.html Last connexion: June, 30, 2014.While the basic calculation of Google is presented in Fig. 4, the calculation proposed in this paper, that is an inverse PageRank calculation, is presented in Fig. 5.The using of the connectivity matrixCin inverse Markov chains calculations has already been studied in the literature. The solution to this linear inverse problem in not unique, and has been addressed in the works of Gzyl and Velásquez [26,27] and Csiszar [28]. In those papers, the solution to this constrained linear inverse problem is obtained by identifying the transition matrix that satisfies a certain maximum entropy condition, satisfying a least-squares condition.In Inverse-Page-Rank PSO (I-PR-PSO), to define the PageRank vector, that is the steady-state of the Markov chain, the relative success of each particle of the swarm is used. Then, at each iteration, the relative success of each particle k regarding the best one Gbestis calculated as given in Eq. (7). The vector containing all the relative successes respectively to each swarm’s particle is then normalized as given in Eq. (8).(7)πtargetT(1,k)=|fitness(Gbest)×100fitness(Gbest)−fitness(Pk)+ϵ|∀k∈[1,n]where fitness(X) represents the value of the objective function for the particleX, and n is the number of particles in the swarm. The parameter ϵ (10−7or10−15depending on the precision of the computer) is used in order to avoid a division by zero whenfitness(Gbest)=fitness(Pk). Eq. (7) represents a classification of the particles based not on their ranks in the population but with respect to the distance from the global best particles Gbest. If a particle Pkis close to the Gbest, its value inπtargetis big.The advantage of the newly developed I-PR-PSO algorithm is to take into account not only the fitness of the individuals at current position but especially the history of the iterations. The memory of previous iterations is stored and used in the Gbestvariable in order to avoid premature convergence.(8)πtargetT(1,k)←πtargetT(1,k)∑k=1D(πtargetT(1,k))∀k∈[1,n]where k is the kthcomponent of the vectorπtargetT.AsπtargetTis a probability vector, it is then normalized so thatπtargetT(1,k)∈[0,1]∀kand the sum of all its components is 1. This mathematical expression is effective only in the case of a minimization optimization problem. The vector defined by Eq. (7) is then considered as the target vector of the connectivity calculation. In fact, as it can be seen in Fig. 4, the PageRank vector is calculated knowing the stochastic connectivity matrixC. Then, the purpose of our inverse PageRank algorithm is to find the stochastic connectivity matrixC(also called the “help matrix” in [29]) which fits with the previously defined target vectorπtargetT(also called the “reputation vector” in [29]).In I-PR-PSO, this target vectorπtargetTdefines the influence of each particle in the swarm according to their personal fitness.πtargetTcan be seen as the steady state of the Markov chain defined by the graph of the PSO population topology. Its dimensions are (1 × n), where n is the number of nodes in the considered graph, that is the number of particles in the swarm. It can be seen that the sum of all of its components is equal to 1. Then, the goal of this work is to find the (n × n) connectivity matrixCdefining the transition probabilities between the nodes of the considered graph, that is the influence of all the particles on the others, corresponding to this target vector. The constraints are 0 ≤ Ckl≤ 1 and∑l=1nCkl=1.In this way, the best particles will be the most influent among the swarm, and the worst ones will not have an important influence on the others. This calculation is then an inverse PageRank process, in which the steady-state of the Markov chain is known and given in Eq. (7), and the transition probabilities are searched.As it has been done in [25], the algorithm to compute the Markov transition matrix, that is in our case the stochastic connectivity matrix defining the influence of all the particles on the others, is given by the following stepsStep 1:The choice of an initial matrixC0. In our case, the initial matrix is random, but each line is then normalized, because the sum of all the terms in each line has to be 1.An iterative process is performed to adjust the entries ofC0 in order to find a final transition matrixCf. The steady-state vector ofCfis the previously defined target vectorπtargetT. Let us defineCmthe stochastic connectivity matrix during the step m of the iteration process, with the corresponding steady-stateπmT. Then the Markov process at time m can be described asAs said previously, the purpose of this calculation is to find the entries ofCmso that we have(10)πtargetT(Cm−I)=0that is∥πmT−πtargetT∥2=0. Then, a residualrmcan be defined at the iteration m, which is(11)rm≡(πtargetT−πm)(Cm−I)Finally, the goal of this work is to find the components ofCmso that ||rm||2 ≤ ϵPR≪ 1, where ϵPRis the convergence threshold of the calculation. To do this, the components ofCmare adjusted at each iteration m by the factor δ, according to Algorithm 1[25].In the final converged connectivity matrixCf, the nonzero elements of row k are relative to the links going out of the page k, whereas the nonzero elements of column k are relative to the links coming in the page k. Then, in our case, the nonzero elements of column k show how the particle k influences the others, whereas the nonzero elements of row k show how the particle k is influenced by the others. As the sum of all the terms in each line ofCis , one can note that the total influence of all the particles on one of them is always 1.It is important to note that the changing parameters δ is defined as a function of the target vectorπtargetT. In fact, δ is the order of magnitude of the minimum component ofπtargetT. Then, δ is calculated according to Algorithm 2.Because there are lots of random parameters in the algorithm, the final matrix Cfcan be slightly different, from one calculation to another, even though the initial matrixC0 is the same [25]. Indeed, the final matrix depends on the randomly chosen row to be modified. Newton et al. [25] performed a statistical study to show the differences of the final matricesCm, which are all conditioned by the same initial matrixC0. It has been shown that the sensitivity of the final converged connectivity matrixCfwith respect to the initial connectivity matrixC0 could be neglected (the order of magnitude of the standard deviation is 10 at the outside).Finally, this calculation allows us to find a stochastic connectivity between all the particles of the swarm. The weighted influence between the particles, corresponding to the normalized target PageRank vectorπtargetTis defined in Eq. (8).Some examples have been performed to show how the connectivity matrixCis calculated by Algorithm 1. The first example is the following : the target vectorπtargetTis given byπtargetT=[1234]and then the normalized vector isπtargetT=[0.06670.13330.20000.6000]. The first initial matrixC0 is random, and each line is normalized so that the sum of all the terms in each line is 1. Using Algorithm 1, the final PageRank vector isπmT=[0.07370.14350.20990.5729]and we have∥πtargetT∥−∥πmT∥=0.0314. One can see here that the two vectors are quiet similar. The final population connectivity is given by Eq. (12), and the convergence of the residual rmduring the iterations is given in Fig. 6.(12)[0.00240.09810.16480.73470.00920.00980.26330.71780.00310.14430.27730.84700.12500.18260.27730.4151]One can note that the particle no4 is the most influent in the swarm, which is coherent because its target value inπtargetTis the upmost.It is important to note that if the values in the target vectorπtargetTare too far from each other (about some powers of ten), the calculation does not converge. Actually, in that case, the factor δ is too small to change efficiently the connectivity matrix. For example, if the target vector is given byπtargetT=[11.E−101.E−101.E−10]before normalization, the final connectivity matrix is the same as the initial one, because δ is1E−10and can not change the connectivity matrix components sufficiently to converge toπtargetT.In the same way, if the components ofπtargetTare the same, the final topology should be a GBEST topology, in which all the particles influence the others with the same weight. The connectivity matrix should be full of non-zero components which would be slightly the same. Nevertheless, in that case, the calculation does not converge at all, becauseπtargetT=π0T,and the first residual is then 0. Though, Algorithm 1 does not activate the loop because∥rm+1∥is directly inferior to ϵPR.In I-PR-PSO, all the particles are used to influence each others, but their respective influences are weighted by the components of the previously seen stochastic connectivity matrixC. Then, the position change of each swarm’s particle is then given in the following way(13){Vit+1=ω×Vit+c1×rand1×(Pi,bestt+1−Xit)+c2×rand2×∑j=1nCij×[Pj,bestt+1−Xit]Xit+1=Xit+Vit+1Aswe have previously seen, the particle i is influenced by all the particles of the swarm, and their respective influence are given by the components of the ith line ofC, that isCij∀j.The global I-PR-PSO algorithm is described in Algorithm 3in which itPSO,MAXis the maximum number of PSO iterations, and itPR,MAXis the maximum number of PageRank iterations, that is the iterations needed to calculate the stochastic connectivity matrixC.Concerningthe issues previously presented in part (3.2.2), one can note that when the particles have the same fitness values (that is when the components ofπtargetTare slightly the same), or when the particles have fitness values far from each other in the solution domain (that is when the components ofπtargetTare very different (about some powers of ten)), the population topology is then given by the first random connectivity matrixC0. This strategy corresponds to the one proposed by Mohais et al. in [12], in which it has been suggested that random topologies can be competitive to predefined ones [30]. Moreover, it has been shown in the literature that the proximity of individuals could cause premature convergence problems, because of the loss of diversity. This random reactualization of the population topology is a solution to this loss of diversity.I-PR-PSO has been tested on the different benchmark functions given in Table 2 in which D represents the dimension of the problem.A statistical study has been performed to obtain sufficient results to prove the efficiency of I-PR-PSO. On each function, 100 runs have been performed in the dimensions 10, 20, 30 and 50, with 50 particles. As it has been shown in the literature that PSO can be more efficient than other metaheuristic methods in large dimensions [31,32], this research focuses especially on the comparison between different PSO variants. Then, I-PR-PSO has been compared to three different versions of classic PSO, that is with the previously presented GBEST topology, the LBEST topology, and the 4-clusters topology, with the same calculation parameters given in Table 1.In this paper, the best value of the objective function reached after 600 PSO iterations is investigated, for all the twelve objective functions, with all the 4 different PSO variants. The results, that is the best values of the objective function found so far, are presented in a log scale in Figs. 7–10. The values obtained are given in Tables 3 and 4 in which the mean and the standard deviation of all the 100 runs are presented.Basedon the results given in Figs. 7–10, we conclude that our proposed I-PR-PSO is more efficient than the tested peers on the tested objective functions in dimensions10, 20, 30 and 50.Finally, to have a visual aspect of the convergence of the different algorithms on the considered objective functions, the mean of the best fitness values found over the iterations, for the 100 different calculations performed, is presented. The convergence curves are presented for the dimension 50 in Figs. 11–22. Some of these graphs are presented in a log scale, so that the results are readable. As one can see in these figures, the results presented are coherent with those previously presented. The Inverse-PageRank-PSO algorithm converges closer to the global optimum than its tested peers, in all dimensions. Moreover, I-PR-PSO is quicker to converge than the tested peers. Actually, the algorithm has a better global research ability, while its local research ability is not better than the other algorithms: once the swarm is close to the global optimum of the tested objective function, Inverse-PageRank-PSO needs lots of iterations to finally stabilize the swarm. I-PR-PSO is also quicker to converge and converges closer to the global optimum than its tested peers in dimensions 10, 20 and 30, but the convergence curves are not presented here.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
In this paper, I-PR-PSO is proposed to solve unconstrained minimization optimization problems defined in continuous solution domains. In I-PR-PSO, the population topology evolves during the iterations. The population developed is a weighted GBEST topology, in which the weights are defined using the Markov chains theory, and the Inverse PageRank algorithm in particular. Indeed, Inverse-PageRank-PSO provides a general adaptive algorithm that updates the population topology of the swarm, without any additional parameter compared to classical PSO. Its social behavior is then enhanced based on the actual evolution of the population.The obtained numerical results show that I-PR-PSO has the ability to find the global optimum of the considered objective function than its peers. So I-PR-PSO achieves a better balance between the exploration and exploitation phases needed by the particles to find the global optimum in large dimensions. Nevertheless, the algorithm has a better global research ability than its peers, while its local research ability is not better than the other algorithms: once the swarm is close to the global optimum of the tested objective function, I-PR-PSO needs lots of iterations to finally stabilize the swarm.Moreover, I-PR-PSO is quicker to converge than the tested peers in terms of number of objective function evaluations needed to converge. Comparing I-PR-PSO with the work of Lim and Isa [33], we can show that I-PR-PSO has a better ability to push the swarm close to the global optimum (I-PR-PSO needs approximately 1000 function evaluations while its peers need approximately 5000 function evaluations on the same objective functions). However, an additional iterative process is needed to calculate the connectivity matrixC. I-PR-PSO is then much more longer than its peers to converge in terms of CPU time. Thus, I-PR-PSO is very efficient in mechanical applications when the Finite Element Method is used because, in this context, the evaluations of the cost function are very expensive. Then, reducing the number of calls to the objective function could also reduce efficiently the CPU time. On the contrary, if the objective function evaluations are not very expensive in terms of CPU time, I-PR-PSO could be more expensive than its peers, but could find better results, as it has been seen in Figs. 7–10 and 17–22.Obviously, the No Free Lunch theorem has shown that no algorithm can perform better than any other, on all possible objective function [19,34]. Then, testing I-PR-PSO on different benchmark functions that have been identified as hard problems can show that this newly developed algorithm could be more efficient on lots of different objective functions. This algorithm has been tested on engineering structural optimization problems [35], and has been shown to be very efficient on constrained optimization problems.