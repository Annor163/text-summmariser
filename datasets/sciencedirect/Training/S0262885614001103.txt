@&#MAIN-TITLE@&#
Motion boundary based sampling and 3D co-occurrence descriptors for action recognition

@&#HIGHLIGHTS@&#
A motion boundary based sampling strategy is proposed for dense trajectory.A set of 3D co-occurrence descriptors is developed to describe cuboids.Two decomposition strategies are presented to further improve performance.We achieve state-of-the-art results on several human action datasets.

@&#KEYPHRASES@&#
Dense trajectory,Action recognition,3D co-occurrence descriptors,Motion boundary,Bag of Features,

@&#ABSTRACT@&#
Recent studies witness the success of Bag-of-Features (BoF) frameworks for video based human action recognition. The detection and description of local interest regions are two fundamental problems in BoF framework. In this paper, we propose a motion boundary based sampling strategy and spatial-temporal (3D) co-occurrence descriptors for action video representation and recognition. Our sampling strategy is partly inspired by the recent success of dense trajectory (DT) based features [Wang et al., 2013] for action recognition. Compared with DT, we densely sample spatial-temporal cuboids along a motion boundary which can greatly reduce the number of valid trajectories and preserve the discriminative power. Moreover, we develop a set of 3D co-occurrence descriptors which take account of the spatial-temporal context within local cuboids and deliver rich information for recognition. Furthermore, we decompose each 3D co-occurrence descriptor at pixel level and bin level and integrate the decomposed components with a multi-channel framework, which can improve the performance significantly. To evaluate the proposed methods, we conduct extensive experiments on three benchmarks including KTH, YouTube and HMDB51. The results show that our sampling strategy significantly reduces the computational cost of point tracking without degrading performance. Meanwhile, we achieve superior performance than the state-of-the-art methods. We report 95.6% on KTH, 87.6% on YouTube and 51.8% on HMDB51.

@&#INTRODUCTION@&#
Automatic recognition of human action in videos has been an active research area in recent years due to its wide range of potential applications, such as smart video surveillance, video indexing, and human–computer interface. Though various approaches have been proposed and significant progresses have been made, action recognition still remains a challenging task due to the high dimension and complexity of video data, the large intra-class variations, clutter, occlusion and other fundamental difficulties [2].A fundamental problem in action recognition is how to represent an action video. The approaches for action video representation can be roughly divided into five categories: (1) dynamic model based approaches which apply statistical sequential models such as HMM and Bayesian network to describe the temporal states of actions [3,4]; (2) human pose based approaches which utilize pose structure information [5,6]; (3) global action template based approaches which construct global templates to capture appearance and motion information of the whole motion body [7–9]; (4) local feature based approaches which mainly extract spatial-temporal cuboids [10–17] or motion parts [18,19]; and (5) supervised feature learning based methods which learn the representation by hierarchical networks or other models [20–23].Among the state-of-the-art methods, the representation of local spatial-temporal feature with Bag-of-Features (BoF) framework [24] is perhaps the most popular and successful one for action recognition. Local features are usually obtained by cuboid detectors and descriptors. Laptev [25] developed space-time interest points (STIP) detector by extending the Harris detector to 3D domain. Dollar et al. [10] detected space-time salient points by applying 2D spatial Gaussian and 1D temporal Gabor filters. Willems et al. [26] utilized Hessian matrix to extract scale-invariant spatial-temporal interest points in videos. Wang et al. [14] densely sampled cuboids at regular positions and scales. For descriptors, well-known approaches include HOG/HOF [11], Cuboids [10], HOG3D [13], 3D-SIFT [27], and so on.Recently, Wang et al. [15] proposed dense trajectory for sampling spatial-temporal interest points and introduced a novel descriptor named motion boundary histogram (MBH) for action recognition. The motion boundary is defined by the gradient magnitude of optical flow which is initially introduced in the context of human detection [28]. Extensive experiments on nine popular human action datasets have demonstrated the excellent performance of this approach [1]. Despite its great power, the DT based representation is expensive in memory storage and computation due to the large number of densely sampled trajectories.In this paper, we first develop a motion boundary based sampling strategy named DT-MB to reduce the computation and storage consumption of the previous DT based method. We start from densely sampled patches with grids in a frame. Meanwhile, motion boundary (Fig. 1) is derived from optical flow and a binary mask is estimated from motion boundary. Then we remove those sampled regions that have very few overlaps with foreground in the mask. Central points of the rest patches are refined by averaging the location of occupied foregrounds within the patches. Our DT-MB is partly motivated by the fact that those trajectories on motion boundary are the most meaningful ones. This is also implied by the superior performance of the MBH descriptor [1]. Using our sampling method, the number of DTs can be sharply reduced without hurting the performance.In addition, to further enhance the discriminative power of DT based representation, we propose a set of spatial-temporal (3D) co-occurrence descriptors to describe the local appearance and motion information along trajectories. This is partly inspired by the success of co-occurrence feature in image domain [29–31]. In [30], a descriptor based on co-occurrence HOG (CoHOG) is presented for human detection. In [29], gray-level co-occurrence matrix (GLCM) is introduced to extract textural features for image classification. Our motivation is that the spatial-temporal co-occurrence features, which depict the local tiny context of motion and appearance in videos, can provide important cues for action recognition. The novel descriptors are composed of 3D-CoHOG, 3D-CoHOF and 3D-CoMBH. We find that (1) 3D-CoHOG depicts more complex structure of spatial patch and the appearance changes along with time; (2) 3D-CoHOF conveys complex motion structure and motion direction changes; and (3) 3D-CoMBH captures the complex gradient structure of optical flow and the changes of gradient orientations of flow. Furthermore, we thoroughly exploit two types of multi-channel pipelines for these descriptors, namely the pixel level pipeline and the bin level pipeline. Considering 3D-CoHOG in a given cuboid aligned by trajectory, we set several offsets at horizontal, vertical and temporal axes for each point, and the co-occurrence matrices in all the offsets are vectorized and concatenated to form 3D co-occurrence descriptors. For pixel level multi-channels of 3D-CoHOG, we vectorize the co-occurrence matrices for each offset and model them by the BoF pipeline individually, and then combine all the BoF pipelines by a multi-channel kernel SVM. For the bin level, we split the co-occurrence matrices into several channels by their co-occurrence bins for each offset. The idea of using multi-channels for 3D co-occurrence is partly inspired by the fact that MBHx and MBHy perform differently [1] and the complementarities can be better investigated in a multi-channel way as shown in [32].To evaluate our sampling strategy and the proposed descriptors, we perform action classification with a standard BoF framework and a kernel SVM classifier [11] on three widely-used datasets, namely KTH [33], YouTube [34] and HMDB51 [35]. Our framework is illustrated in Fig. 1. We investigate our DT-MB sampling strategy over that of the original dense trajectory [1] in the view of computation and memory cost. We evaluate the improvement of our new descriptors over original HOG, HOF and MBH [1]. Furthermore, we provide a theoretical analysis on the advantages of using co-occurrence feature.The main contributions of this paper are summarized as follows:1)we develop a motion boundary based sampling strategy to reduce the number of dense trajectories which can save memory and computation without degrading performance;we propose a set of 3D co-occurrence descriptors, namely 3D-CoHOG, 3D-CoHOF and 3D-CoMBH, which can depict the spatial-temporal contextual information within local cuboids;we present two decomposition strategies for 3D co-occurrence descriptors (pixel level and bin level) and integrate the decomposed components with a multi-channel framework, which can further improve the performance;we achieve state-of-the-art results on several widely-used human action datasets.It's worth noting that our new descriptors are independent of the spatial-temporal cuboid detectors (e.g., DT [1], STIP [25], dense cuboids [14]). Though we mainly discuss our novel descriptors with dense trajectory, one can easily extend them with other detectors as well. The analysis and results presented here extend our preliminary work in BMVC 2013 [36]. Here, we develop more general spatial-temporal co-occurrence descriptors and further improve the performance by exploiting their multi-channel versions. We also provide an information theory analysis to validate the advantages of using co-occurrence descriptors.The rest of this paper is organized as follows. In Section 2, we give a brief review of dense trajectory based method and present our DT-MB method in detail. In Section 3, we present our 3D co-occurrence descriptors. The two decomposition strategies for 3D co-occurrence descriptors are presented in Section 4. Section 5 shows the experimental results and gives a comprehensive comparison for each individual descriptor. We conclude our work in Section 6.In this section, we first give a brief review of dense trajectory method [1] and explain the advantage of DT from a view of human visual fixation system. Then, we present our new sampling strategy based on motion boundary in details.Dense sampling strategy has been widely used in extracting local image features and achieved great success in image classification. This fact inspires researchers to develop dense sampling approaches for video based action recognition, which can yield richer description of action than spare interesting points. Two successful examples are dense cuboid [14] and dense trajectory [1]. Dense trajectory based method mainly consists of the following steps.Feature points are sampled in the current frame on a grid by a step size w at S spatial scales. To track successfully, points in homogeneous image areas are filtered out by examining the eigenvalues of their auto-correlation matrices.Dense points are tracked by median-filtered optical flow on each spatial scale separately. Tracked points in successive frames at scale s are concatenated to form trajectories: (Pts, Pt+1s,…), where Pts=(xts,yts) represents the spatial position. To prevent the trajectories from drifting, the length is limited to L frames. Once a trajectory's length reaches L, its mean position drift and variation will be checked. Trajectories with tiny or large mean drift and variation will be pruned since they usually correspond to static or erroneous trajectories.There are four types of descriptors for each cuboid aligned by trajectories [1]. The trajectory shape is described by a sequence (ΔPts,…, ΔPt+Ls) of displacement vectors ΔPts=(xt+1s−xts,yt+1s−yt). Usually, this vector is normalized by the ℓ1-norm. Therefore, we obtain a 2L-dimensional shape descriptor. To catch the motion and structure information, HOG, HOF and MBH are extracted within a space-time cuboid whose size is N×N×L aligned with the trajectory. HOG and HOF are among popular descriptors which yield excellent results on many datasets [11]. The MBH is derived from the gradients of optical flow which is originally introduced for human detection [28]. To embed more structure information, we usually subdivide the cuboid into a spatial-temporal grid of size nσ×nσ×nτ. Assuming nbinis the number of quantized bins for HOG and MBH, and nbin+1 (1 for static) for HOF, then we can obtain a nσ×nσ×nτ×nbinfeature vector for HOG, nσ×nσ×nτ×(nbin+1) for HOF and nσ×nσ×nτ×nbinfor MBHx and MBHy, respectively.Dense trajectory based approaches whose features are extracted along with trajectories are consistent with human visual fixation system as illustrated in the 3rd row of Fig. 2. Visual fixation refers to the maintaining of the visual gaze on a single location, also known as smooth pursuit or temporal slowness [37]. A number of species, including humans, other primates, cats and rabbits can perform this mechanism by three categories of eye movements: micro-saccade, ocular drift, and ocular micro-tremor. There are two basic properties for smooth pursuit from a view of video representation. On the one hand, it makes the feature robust to velocity change. Obviously, the appearance features in a cuboid with smooth pursuit can be very similar despite the difference of motion velocity. Motion features also remain similar after normalization. On the other hand, as shown in the 4th row of Fig. 2, more meaningful appearance and motion information are captured with temporal slowness. For these reasons, DT can always outperform dense cuboids with identical parameters [1] in theory.A limitation of DT is that too many points need to be tracked in the original dense sampling criterion [1]. However, only a few of them may lead to valid trajectories. We notice that those points on the motion boundary are the most discriminative ones. This is indeed partly implied by MBH descriptor [1] and motion boundary contour system (BCS) in neural dynamics of motion perception [38]. In this paper, we introduce motion boundary based sampling strategy which constrains the sampled points to the sharp regions of motion boundary.The implementation of DT-MB is straightforward. Different from the original DT, it needs two successive frames to sample points. A comparative example is illustrated in Fig. 3. Fig. 3(d) shows an example of motion boundary image. We calculate the gradient magnitudes for both the horizontal and the vertical components of optical flow, and set the maximum of them as motion boundary image. After a thresholding operation on the motion boundary image, a mask is generalized to refine the original DT sampled points. Particularly, we estimate the mask by Otsu's algorithm [39] empirically. Those regions outside the foreground of mask will be removed. Central points of the remaining patches will be refined by the average location of foregrounds. It's worth noting that the motion boundary image is a middle result of DT, so we do not need to add complexity. As shown in the 2nd column of Fig. 3, our approach removes a large number of points which are not on the motion foreground. The 3rd column of Fig. 3 exhibits the trajectories from historical points by DT and DT-MB. The red marks are the end points of trajectories. Note that we do not force all the points of trajectories on the motion boundaries in case of inaccurate tracking. Our DT-MB can be viewed as an effective strategy to reduce the influence of camera motion in the early stage. The detailed analysis of complexity and performance are given in Section 5.Generally, there always exist strong correlations among spatial-temporal neighborhoods of pixels. Traditional HOG, HOF and MBH descriptors are statistical histograms counted pixel-wise which ignore the correlation of pairwise pixels. To jointly encode the spatial-temporal correlations of pixels, we present 3D co-occurrence descriptors which consist of 3D-CoHOG, 3D-CoHOF and 3D-CoMBH.The spatial CoHOG (2D-CoHOG) is initially introduced in the context of pedestrian detection [30]. Specially, it uses pairs of gradient orientations as units and employs the co-occurrence matrix for image representation. As for 3D-CoHOG in video domain, offsets in time domain are taken into account, which is an extension from 2D. The co-occurrence matrix expresses the joint distribution of gradient orientations between anchor points and offset points over a cuboid as illustrated in Fig. 4, and it can jointly depict more complex structure of spatial patch and the appearance changes along with time. A straightforward co-occurrence strategy is to obtain the statistics of all the possibilities of joint occurrences. This results in anbinnoffset+1co-occurrence matrix where noffsetis the number of offset points and nbinis the quantized bins for HOG. This strategy leads an excessively redundant matrix whose high dimensionality not only increases the computation and storage cost but also makes the classification expensive. To overcome this problem, we use pairwise co-occurrence representation. Considering three offsets shown as the red points in Fig. 4, each pair of which will vote for one co-occurrence matrix. After voting, we vectorize all co-occurrence matrices and concatenate them into a vector for each cell of grid, and then concatenate these vectors cell-wise to yield the final descriptor for a cuboid. Given a cuboid with grid size nσ×nσ×nτ, the final 3D-CoHOG descriptor is a nbin×nbin×noffset×nσ×nσ×nτdimensional vector.We can also apply the above 3D co-occurrence strategy to HOF and MBH descriptors. The implementations of these are very similar with 3D-CoHOG except for the inputs. 3D-CoHOF applies spatial and temporal pairs of optical flow orientations as units, and 3D-CoMBH utilizes spatial and temporal pairs of the gradient orientations in the horizontal and vertical flow components, separately. So there will be two 3D-CoMBH components, namely 3D-CoMBHx and 3D-CoMBHy.In our case, considering the computation and discriminative ability, we use three offsets (i.e., (2,0) and (0,2) for spatial offsets, and Δt=2 for temporal offset) and process the trajectory-aligned cuboids pixel-wise with a grid of size nσ×nσ×nτ. Specially, a co-occurrence matrix C over a M×N×T cell I, parameterized by an offset (x,y), is defined as:(1)Cx,ypq=∑t=1T∑i=1M∑j=1NGtij+Gti+x,j+y2,ifOtij=p,Oti+x,j+y=q;0,otherwisewhere p and q are the quantization bins, Gt(i,j) is the gradient magnitude and Ot(i,j) is the assigned bin (e.g., gradient orientation, flow orientation) at position (i,j) of the t-th frame. As shown in Eq. (1), the average gradient magnitude is used to weight co-occurrence matrices. To reduce the boundary effects, we also apply linear interpolation for voting the co-occurrence matrix. For example, given a pre-quantized gradient bin 1.4, we would vote this for both bin 1 and bin 2 with weights 0.6 and 0.4, respectively.One can imagine that it needs Δt+1 frames at least to calculate the co-occurrence matrices for 3D-CoHOG and Δt+2 for 3D-HOF and 3D-MBH. The offsets in time domain we used are aligned by trajectories. It is worth noting that tracking is a necessary step in dense trajectory based approach, so our descriptors can benefit from the computational process of DT.Here, we give an information theory analysis for co-occurrence descriptors and explain why co-occurrence can yield extra information for classification and how to select the offsets.Suppose we have K categories denoted by set C={c1,c2,…, cK}. The prior probabilities of C can be denoted as p(C)={p(c1),p(c2),…, p(cK)}. We utilize the mutual information to analyze the different discrimination between the distributions yielded by individual and co-occurrence (pairwise) units or pixels.Suppose we have N bins for individual unit distribution H={h1,…, hN} (e.g., HOG) and the probability of the ith element hiis denoted by p(hi). For histogram features, each bin is assumed to be independent. Thus, for an individual unit H, its contribution to the classification can be defined as the mutual information:(2)IHC=ICH=∑n=1N∑k=1Kphnpck|hnlogpck|hnpck.The joint distribution between H and another individual unit distribution H′={h1′,h2′,…, hM′} can be denoted as p(F)={p(f1,1),p(f1,2),…, p(fN,M)} where p(fn,m)=p(hn,hm′). Then, the information gain of co-occurrence featureFwith respect to H is given by,(3)IFC−IHC=∑n,m∑kpfn,mpck|fn,mlogpck|fn,mpck−∑n∑kphnpck|hnlogpck|hnpck.Recall,(4)phn=∑mpfn,m=∑mphnhm′,(5)pck|hn=1phn∑mpfn,mpck|fn,m.Then, we can rewrite Eq. (3) to,(6)IFC−IHC=∑k∑n,mpfn,mpck|fn,mlogpck|fn,mpck|hn=∑n,mpfn,mKLp(ck|fn,m,pck|hnwhere KL represents the Kullback–Leibler divergence [40] between two distributions. We conclude two important properties from Eq. (6):1)Since KL(;)≥0, we have I(F;C)−I(H;C)≥0. This suggests the co-occurrence feature can provide extra information for classification. In fact, this property can be interpreted by the so-called Data Processing Inequality (DPI) [41] as well. DPI states that post-processing cannot increase information [41]. In particular, if Z=f(Y), then I(X;Y)≥I(X;f(Y)), where X, Y, and Z are random variables and f is a (probabilistic) function. Eq. (4) shows that H can be seen as a function ofF, then we have I(F;C)≥I(H;C) from the view of DPI.One key parameter to design our co-occurrence descriptor is spatial and temporal offsets. Considering Eq. (6), p(ck|hn) denotes the probability distribution by a given special bin. It is an approximate uniform distribution in our investigation, which means an individual bin has very limited discriminative power. Thus, the information gain in Eq. (6) is mainly dependent on the distribution p(ck|fn,m). The gain is zero when p(ck|fn,m) tends to be uniform. Intuitively, p(ck|fn,m) will tend to be uniform in two cases: very small offset and large offset. The extreme case for small offset is zero offset, which yields uniform distribution obviously. Large offset tends to be independent with the anchor point, then the joint distribution can be rewritten as p(ck|hn)p(ck|hm′), which is near to a uniform distribution as well. We evaluate the offset experimentally in Subsection 5.3.3.In this section, we first present the multi-channel scheme at pixel level for 3D co-occurrence descriptors. Then, we revisit our previous spatial-temporal context descriptors. Finally, we give the details of bin level multi-channel scheme.Inspired by the fact that MBHx and MBHy perform differently and their combination leads to better results [1], we also split all the 3D co-occurrence descriptors according to the offsets and integrate them in a multi-channel scheme with BoF model. In our case, we get four types of descriptors to split, namely 3D-CoHOG, 3D-CoHOF, 3D-CoMBHx and 3D-CoMBHy. As shown in Fig. 5, we split each 3D co-occurrence descriptor along with offsetx, offsetyand offsett. After voting in the spatial-temporal grids, we apply BoF model for each vectorized co-occurrence matrix, and leverage multi-channel kernel SVM to combine channels for each 3D co-occurrence descriptor. The complementarity among each co-occurrence pair can be effectively exploited by multi-channel SVM as shown in [32].In a previous work [36], we have developed another type of spatial and temporal context descriptors which employ spatial co-occurrence and temporal co-occurrence, separately. They are composed of spatial co-occurrence HOG (S-CoHOG), S-CoHOF, S-CoMBH, temporal co-occurrence HOG (T-CoHOG), T-CoHOF and T-CoMBH. Each S-Co descriptor is yielded by concatenating the co-occurrence matrices from offsetxand offsety. As the previous case in Fig. 4, only the blue and red co-occurrence matrices will be vectorized and concatenated for S-Co descriptors. T-Co descriptors are designed to depict the appearance and motion changes from successive patches as illustrated in Fig. 6. These spatial and temporal context descriptors can be seen as spatial channel and temporal channel of the 3D co-occurrence descriptors.The diagram of bin level decomposition is illustrated in Fig. 7. The co-occurrence matrices can be seen as the interchanges of gradient orientations or flow orientations between anchor points and their offsets. Inspired by the multi-channels of Motion Interchange Pattern [42], we also decompose each co-occurrence matrix according to the relative angle of pairwise bins. As shown in Fig. 7, the relative angles are 0°, 90°, 180° and 270° from channel ℓ1 to channel ℓ4, respectively. Although the use of angle channels in [42] is aimed at computing conveniently, we find this strategy to significantly improve the performance in our case, especially by the channels from offsett. We explain that different angle channels can reflect the difference of action categories. Take the offsettof 3D-CoHOF for example, high value can occur in the ℓ3 channel for action boxing, while strong values happen in the ℓ2 and ℓ4 channels for action waving.

@&#CONCLUSIONS@&#
