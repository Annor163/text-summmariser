@&#MAIN-TITLE@&#
Estimation of physical, mechanical and hydrological properties of permeable concrete using computational intelligence approach

@&#HIGHLIGHTS@&#
Four different models were developed for estimating properties of PC using SVR.The developed models were optimized using test-set-cross validation technique.The developed models are characterized with high CC, low RMSE and low MAE.The values of the estimated properties agreed well with the experimental values.

@&#KEYPHRASES@&#
Permeable concrete,Support vector regression,Density,Compressive strength,Tensile strength and porosity,

@&#ABSTRACT@&#
Permeable concrete (PC) has gained a wide range of applications as a result of its unique properties which result into highly connected macro-porosity and large pore sizes. However, experimental determination of these properties is intensive and time consuming which necessitates the need for modeling technique that has a capability to estimate the properties of PC with high degree of accuracy. This present work estimates the physical, mechanical and hydrological properties of PC using computational intelligent technique on the platform of support vector regression (SVR) due to excellent generalization and predictive ability of SVR in the presences of few descriptive features. Four different models were built using twenty-four data-points characterized with four descriptive features. The estimated properties of PC agree well with experimental values. Excellent generalization and predictive ability recorded in the developed models indicate their high potentials for enhancing the performance of PC through quick and accurate estimation of its properties which are experimentally demanding and time consuming.

@&#INTRODUCTION@&#
Permeable concrete (PC) is a special type of concrete that has high permeability rate due to their highly connected macro-porosity and large pore sizes. It has gained a wide range of applications as sustainable construction materials which can be used for parking lots, walkways, sidewalks, construction of secondary roads, in-situ aerobic bioreactor, and plants purification. It has great advantages because of its physical (such as density), mechanical (such as tensile and compressive strength) and hydrological properties (such as porosity) which give rise to high water infiltration capacity, storm water retention ability, ground water recharge capacity, hydrocarbon pollution control and suspended solid removal [1,2]. The desired properties of PC are majorly obtained by controlling the amount of nominal coarse aggregate size of the concrete, cement, water to cement ratio and coarse aggregate. This present work utilizes these descriptive features to develop models which allow accurate estimates of the properties of PC without the use of intensive conventional methods.The compressive strength and tensile strength of PC are key parameters that determine its suitability as a pavement structural material. These mechanical properties are affected by pore connectivity, diameter of the pore, surface roughness of the pore and the volume fraction of the pore. However, porosity has been identified to be the primary determinant of the magnitude of the compressive strength and hydraulic conductivity [2]. Porosity also affects water permeability, the higher the porosity ratio, the higher the water permeability, but an inverse relationship exists between the porosity and compressive strength [3]. The proper usage of PC is attributed to accurate determination of its physical, mechanical and hydrological properties which will guarantee its effectiveness when use to reduce the noise generated on the road, to remove the suspended solid in the water, to prevent stagnation of water on the road and thereby ensuring the safety of the road users [4]. The laboratory determination of the PC properties according to the ASTM standard is time consuming and expensive. Given the importance of this material and the research interest it has generated over the years, it is a worthwhile endeavor to develop models that can accurately estimate the properties of PC.Quite a lot of experimental and theoretical work has been conducted on PC [5]. Most of these works have focused more on the experimental design of PC with a view to derive a comprehensive understanding of its properties and at the same time seek means to enhance the properties of the material [6,7]. A typical example is the work of Cheng [8] on the design of a pervious concrete from recycled aggregate. The objective of the work is to study the mechanical properties and performance of pervious concrete. It is a fact that the research in PC design and construction is a laborious endeavor, most times, consuming time and limited resources. As such, it is extremely important to perform a theoretical estimation of the fundamental properties of PC in order to minimize wastage of materials during fabrication and testing of samples. To this regards, several approaches have been deployed to estimate these properties. For instance, Atici's [9] estimated compressive strength of concrete that contains various amount of blast furnace slag and fly ash using a combination of multiple regression analysis and artificial neural network. In a similar effort, Ahmet [10] utilized neural network to predict the compressive strength and slump of high strength concrete. Marai [11] also employed neural network for predicting compressive strength of structural light weight concrete. Although, ANN has performed well in all cases mentioned above. However, it has been demonstrated that SVR gives a better performance than ANN in many instances due to its unique properties such as non-convergence to local minimal and superior ability to generalize well in the presence of small dataset [12–16]. Among the uniqueness of this present work is that its develops models for estimating properties of special type of concrete (PC) as a result of the important roles play by these properties on the application of PC.SVR is a robust machine learning algorithms based on statistical learning theory [17]. It has been successfully applied to regression problems in wide range of science and engineering applications. The algorithm was developed by Vapnik in 1995 and it has been attracting much attention in recent time due to its many attractive features and improved performance [18]. Unlike the traditional neural network which generalizes as a result of the optimization algorithms utilized in the selection of the parameters and the statistical approach used in selecting the best model, SVR employs structural risk minimization principle that minimizes the upper bound on the expected risk thereby increasing SVR ability to generalize well in the presence of few data-point and descriptive features. The choice SVR in this work is due to the limited data-points that characterized the experimental results in determining the properties of PC. SVR stands a good chance in modeling properties of PC using few experimental data because of its sound mathematical foundation and non-convergence to local minimal [19,20].Generalization performance evaluation conducted on our developed models shows excellent results as deduced from high coefficient of correlations, low root mean square error, low mean absolute error, low mean absolute percentage error and low value of scatter index.The main underlying principle in SVR is the mapping of input data into a high-dimensional feature space by nonlinear transformation mapping function that is defined by inner product function which allows a linear regression to be performed in the high dimensional space.SVR algorithm selects a function that estimates the actual value of target as close as possible to the reference value with a precision ϵ defined as the insensitive loss function which measures the flatness of generalized pattern and the maximum permitted deviations of the targets from the estimated values for all the given training dataset [21,22]. Consider a decision function represented by Eq. (1), in order to ensure the flatness of the equation, it is required that w is made small through minimization of the Euclidean normw2.(1)f(x,α)=〈w,x〉+bwherew∈R′and b∈R for a set of training samplesMinimization of the Euclidean norm results in Eq. (2).(2)minimize12w2subject toyi−〈w,xi〉−b≤ε〈w,xi〉+b−yi≤εEq. (2) holds on the assumption that there exists a function that is capable of providing error which is less than ɛ for all training pairs of the dataset. The slack variables(ξiandξi∗)are introduced in order to create room for another kind of error that may arise while dealing with real life problems. Therefore, Eq. (3) is modified and presented as Eq. (4).(3)minimise12w2+C∑i=1k(ξi+ξi∗)subject toyi−〈w,xi〉−b≤ε+ξi〈w,xi〉+b−yi≤ε+ξi∗ξi,ξi∗≥0for alli=1,2,…,kThe optimization problem in Eq. (3) can be solved using Lagrangian multipliers (ηi,ηi∗,λiandλi∗) to transform the problem into dual space representation. Hence, the Lagrangian for the Eq. (3) result in the Eq. (4).(4)L=12w2+C∑i=1k(ξi+ξi∗)−∑i=1kλi(ε+ξi−yi+〈w,xi〉+b)−∑i=1kλi∗(ε+ξi∗+yi−〈w,xi〉−b)−∑i=1k(ηiξi+ηi∗ξi∗)By equating the partial derivatives of the Lagrangian(withrespecttow,b,ξiandξi∗)to zero, the saddle point of the Langrangian can be found.Hence, this results in the following equations:(5)w=∑i=1k(λi∗−λi)⋅xi(6)ηi=C−λi(7)ηi∗=C−λi∗By substituting Eq. (5)–(7) in (4), the optimization equation is maximized and gives rise Eq. (8).(8)12∑i=1k∑j=1k(λi∗−λi)(λj∗−λj)(xj⋅xi)−ε∑i=1k(λi∗+λi)+∑i=1kyi(λi∗−λi)=0subject to∑i=1k(λi∗−λi)=0,0≤λi∗andλi≤CBy solving Eq. (8), the solution ofλi∗andλiobtained are substituted in Eq. (1) to give Eq. (9)(9)f(x,α)=∑i=1k(λi∗−λi)K〈xi,x〉+bwhere K〈xi, x〉 is the kernel function. Any function that satisfies Merce's condition can be used as a kernel function [21]. The variables of kernel function determine the structure of high dimensional feature space which controls the complexity of the final solution. The kernel functions that are available in the literature include linear, polynomial, Gaussian, sigmoid and others [20].The generalization performance of the developed models were evaluated using coefficient of correlation (CC), root mean square error (RMSE), mean absolute error (MAE), mean absolute percentage error (MAPE) and scatter index (SI) [23,24]. The relations are presented in Eq. (10)–(12).(15)CC=∑i=1n(Yi(exp)−Y′(exp))(Yi(est)−Y′(est))∑i=1n(Yi(exp)−Y′(exp))2∑i=1n(Yi(est)−Y′(est))2(16)RMSE=1n∑i=1nei2(17)MAE=1n∑i=1nei(18)MAPE=1n∑i=1nei∑i=1nYi(exp)×100(19)SI=RMSE1n∑i=1nYi(exp)where eiand n represent error (difference between the experimental and estimated value of the target) and number of data point respectively. Yi(exp) and Yi(est) respectively represent the experimental and the estimated target whileY′i(exp)andY′i(est)represent their mean respectively.The targets are compressive strength, tensile strength, density and porosity for model I, II, III and IV respectively.The training and testing stages of the developed models (model I, model II, model III and model IV) were conducted using a dataset contained in Table 1. Each of the developed models were built by training and testing SVR using twenty-four data-points with four descriptors which include nominal coarse aggregate size of the concrete, cement, water to cement ratio and coarse aggregate. The descriptors and targets are drawn from the literature [3]. These four descriptors were utilized in building model I (that estimates the compressive strength of the PC), model II (that estimates the tensile strength of PC), model III (that estimates the density of PC) and model IV (that estimates the porosity of PC).Statistical analysis of the dataset was carried out and the results of the analysis are presented in Table 2. The values of the mean, median, maximum, minimum and standard deviation presented in the table indicate consistency in the dataset and further suggest its suitability for the estimation task. Correlation coefficients between each of the descriptor and the target are also presented in Table 3. The amount of cement in each of the prepared mixtures is well correlated with each of the target and water to cement ratio also show adequate degree of correlations with the targets.Development of model I, II, III and IV was carried out using MATLAB computing environment. Normalization of the dataset contained in Table 1 was carried out before the commencement of modeling in order to improve the prediction strength of the model as well as to ensure efficient computations. The dataset was further randomized and separated into training and testing set in the ratio of eighty to twenty respectively. The training set of data was used for training using SVR algorithm while the remaining twenty percent was used to validate the developed models. Each of the models was optimized using test-set-cross validation technique which is further discussed in Section 3.3. The experiment was conducted for model I (in which the target is the compressive strength), model II (in which the target is the tensile strength), model III (in which the target is the density) and model IV (in which the target is the porosity). Algorithm 1 shows step by step procedures used in developing each of the models.Algorithm 1Computational methodology of the developed models.Test-set-cross validation optimization technique was used in optimizing all the developed models. It involves monitoring the values of correlation coefficient, root mean square error and mean absolute error for a group of model parameters such as regularization factor C (bound on the Langrangian multiplier), λ (conditioning parameter for QP methods), ɛ (epsilon) and kernel option (η). The optimum performance measure was noted with the corresponding parameter values. All the available kernel functions were used to repeat the experiment followed by an incremental step of parameter values. The optimum values of the model parameters were presented in Table 4and further used to train the final SVR algorithm. The procedures for this technique are illustrated in Algorithm 2. Each of the model parameters is to be varied and altered during optimization stage. The model parameters include the regularization factor (C), hyper-parameter, epsilon, kernel option and the corresponding kernel function. The regularization factor or penalty factor controls the trade-off between the complexity of the model and the maximum allowed deviation of the estimated value of the target from the experimental or actual value of the target. The choice of hyper-plane that ensures minimum error is controlled by the hyper-parameter while the epsilon (ɛ) represents the maximum allowed deviation of the estimated from the actual target. Kernel function transforms the data into high dimensional feature space and this mapping is characterized with selected value kernel option.Algorithm 2Optimization strategy employed for developing the models.

@&#CONCLUSIONS@&#
