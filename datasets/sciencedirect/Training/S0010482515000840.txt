@&#MAIN-TITLE@&#


@&#HIGHLIGHTS@&#
Solving the problem of setting entropy descriptors by varying the pattern size instead of the tolerance.Improving the discrimination of signals of different complexity such as fBm and fetal heart rate signal (normal-IUGR).Linking intrinsic features of time series with the optimal pattern size that maximizes the similarity entropy.Improving the discrimination by developing a new paradigm as the n-order fuzzy similarity entropy.

@&#KEYPHRASES@&#
Maximum similarity,n-Order,Fetal heart rate,Entropy,Fuzzy,Complexity,Fetal distress,

@&#ABSTRACT@&#
This paper presents two new concepts for discrimination of signals of different complexity. The first focused initially on solving the problem of setting entropy descriptors by varying the pattern size instead of the tolerance. This led to the search for the optimal pattern size that maximized the similarity entropy. The second paradigm was based on the n-order similarity entropy that encompasses the 1-order similarity entropy. To improve the statistical stability, n-order fuzzy similarity entropy was proposed. Fractional Brownian motion was simulated to validate the different methods proposed, and fetal heart rate signals were used to discriminate normal from abnormal fetuses. In all cases, it was found that it was possible to discriminate time series of different complexity such as fractional Brownian motion and fetal heart rate signals. The best levels of performance in terms of sensitivity (90%) and specificity (90%) were obtained with the n-order fuzzy similarity entropy. However, it was shown that the optimal pattern size and the maximum similarity measurement were related to intrinsic features of the time series.

@&#INTRODUCTION@&#
Detecting transitions in nonlinear dynamic systems is an important and challenging issue for which there is no definitive answer. This is particularly the case, for instance, in medicine when detecting the precise instant in time when a stroke occurs, i.e. when a patient goes from a normal to an abnormal state [1], or when discriminating between normal and abnormal fetuses on fetal heart rate recordings [2,3].One of the most promising ways for detecting such states lies in quantifying the complexity of time series. Among the possible complexity descriptors, fractal [4–7], multifractal [8–12], recurrent [1,13] and entropy [14–18] indicators are undoubtedly the most effective.However, entropy descriptors are highly dependent on the setting parameters (m,r) [19], and the choice of these parameters is critical, especially for moderately sized signal lengths [20]. Chon et al. [21] claimed that entropy descriptors such as approximate entropy and sample entropy are not accurate in assessing signal complexity using the recommended values in the literature [22].Though it has been suggested by [23] to set the tolerance r empirically between 0.1 and 0.2 times the standard deviation of the time series, recent studies focusing on improving the detection of transitions [24,19] have shown the interest of setting it at another value. This suggested value, adapted for the detection of transitions, has been proposed to maximize entropyE(m,r):{E⁎=E(r⁎,m),r⁎=argmaxr(E(r,m)),where r is the tolerance and m is the pattern size.On the other hand, for the setting of pattern size m, few studies have focused on finding the optimal value of m, and the only studies published focused on the reconstruction of the original phase space11The principle of reconstruction consists of finding the minimum embedding dimension m that corresponds to a sudden change in the nearest false neighbors.[25,26] rather than the detection of transitions or the discrimination of time series differing in complexity. However, the recent study by Restrepo et al. [19] tested several sizes of pattern m. Basically, the most commonly used values of m were set at 1, 2 or 3 [27,3]. These low values of m were proposed because too high a value of m leads to a poor estimation of the entropyE(m,r)[19] or to a poor reconstruction of the system dynamics.Because entropy descriptorsE(m,r)suffer from setting problems, and as there are no satisfactory solutions, the main purpose of this study was to extend the studies recently undertaken by [24,21,19] based on the search for parameter settings leading to maximum entropy.The first challenge to resolving the setting problems was investigating the value of m⁎ to maximize entropyE(m,r):{E⁎=E(r,m⁎),m⁎=argmaxm(E(r,m)).As this study was devoted only to searching for the optimal value m⁎ with a constant value r, the latter was fixed at r=0.2 for the remainder of the study. The variable r was therefore deliberately omitted from the subsequent equations. Moreover, the search for the optimal set of parameters(r⁎,m⁎)to optimize the entropyE⁎⁎=E(r⁎,m⁎)will be the subject of a future study. The study reported here showed that examining the role of the pattern size m could provide important insights into quantifying the complexity of time series, thus leading to improved understanding of nonlinear dynamic systems. Furthermore, it was shown that the combined use of m⁎ andE⁎can provide a more consistent method to distinguish between different dynamics.In view of the lack of a satisfactory method, the need to find a new transition detector or a discriminator is greatly needed. The second aim of this study was to establish a new paradigm to provide important insights into quantifying the complexity of time series. This new concept, for which, as suggested by [18], a membership function was introduced, encompassed the standard definition of entropy descriptors. Indeed, the general framework of “n-order fuzzy entropy” on which the new paradigm is based depends on the following equation:(1)E(n,m)=ϕ(m+n)−ϕ(m)where n is the order, and ϕ is the average of the natural logarithm of the probability of finding similar patterns of size m. By setting n=1 in Eq. (1), we recognized the definition of approximate entropy [14].The concepts of maximum entropy and of n-order entropy were then introduced in the following work to achieve the above aims. The two new paradigms were validated through simulations and recordings of fetal heart rate.The aim of this section is to show that the concept of maximum entropyE⁎=E(m⁎)is really valuable for discrimination purposes. For this purpose, and within a more general framework, the similarity entropy was used as the foundation since its superiority was recently shown with regard to approximate entropy [18]. Definition of the similarity entropy is written as follows:(2)Ep(1,m)=ϕp(m+1)−ϕp(m),where(3)ϕp(m)=1N−(m−1)∑i=1N−(m−1)log(1N−(m−1)∑j=1N−(m−1)Dp(i,j,m)).The degree of similarityDp(i,j,m)between theX(i,m)andX(j,m)vectors within a fixed tolerance r can be calculated through an exponential fuzzy function as follows:(4)Dp(i,m,r)=e−(d(i,j,m)/r)p.When p=2 the membership function is a Gaussian function, whereas whenp=∞the membership function is a Heaviside function. The distanced(i,j,m)between the two m-patternsX(i,m)andX(j,m)is defined as follows:(5)d(i,j,m)=d(X(i,m),X(j,m))=maxk∈(0,m−1)|x(i+k)−x(j+k)|,wherex=x−x¯andx¯stand for the mean of x. Thus a centered vector sequenceX(m,i)is formed from a time series vector composed of N points:(6)X(m,i)={x(i),x(i+1),…,x(i+m−1)}−X¯(i,m),withX¯(i,m)=1m∑l=0m−1x(i+l).To guarantee that the functionϕp(m)varies between 0 and 1, a normalized version is proposed as follows:(7)Φp(m)=1+ϕp(m)log(N),N being the length of the time series.Finally, 1-order entropy, that corresponds to the first discrete derivative of the functionΦp(m), was reduced to(8)Ep(1,m)=Φp(m+1)−Φp(m),and the search for m⁎ maximizing entropyEp(1,m)and the maximum entropyEp⁎led to the following equations:{m⁎=argmaxm(Ep(1,m)),Ep⁎=Ep(1,m⁎),withp=∞for non-fuzzy estimations and p=2 for fuzzy estimations.Once theoretical aspects were complete it was then important to demonstrate the value of determining the maximum entropyEp⁎through simulations. These simulations were focused on fractional Brownian motion (fBm) because (i) they are good archetypes of biomedical signals and (ii) their degrees of irregularity can easily be varied through the Hurst exponentH.It should be recalled that an fBm is a time seriesx(t)governed by the self-affinity equation:x(rt)=rHx(t), where r is a scaling parameter andHthe self-affine invariant called the “Hurst exponent”. The autocorrelation function of self-affine time series is a decreasing function whose decreasing speed depends on the Hurst exponent: the higher the Hurst exponent, the more regular the time series and the longer the correlation length.In order to show thatEp⁎and m⁎ can be related to the intrinsic features of fractional Brownian motion (fBm) and can be used as discriminating descriptors, 100 normalized fBms of unitary energy composed of N=1024 samples were simulated with Hurst exponents ranging fromH=0.0to 0.9. To guarantee that the maximum entropy was reached, the size of the pattern m was varied from 1 to 80 and r=0.2. The results are set out in Figs. 1 and 2and Table 1.Fig. 1a and c represents the mean ofΦ∞(m)andΦ2(m). From these graphs, it can be seen that theΦp(m)functions that quantified the probability of finding patterns of size m were monotonic, decreasing from 1 to 0 as m increased. These functions that represented a cumulative effect possessed slopes depending on the Hurst exponent: the higher the Hurst exponent, the lower the slope. It can also be seen thatΦ∞(m)andΦ2(m)decreased as the size of the pattern increased. Indeed, the greater the sizes of the patterns, the lower the probability of finding large patterns. Fig. 1b and d represents the mean ofE∞(1,m)andE2(1,m). From these graphs, it can be seen that theEp(1,m)functions, that represented discrete derivatives ofΦp(m)functions (cumulative effect removed), were non-monotonic, reaching a maximumEp(1,m⁎)and a location m⁎ that both depended on the value of the Hurst exponent. These curves started from 0 and rose to a maximum and then decreased asHincreased. As fBms are self-affine time series, their autocorrelations are decreasing functions whose correlation length depends on the Hurst exponent value: the lower the Hurst exponent value, the shorter the correlation length and the lower the pattern size m. Fig. 1b and d shows m values that are particularly interesting, the most visible are those maximizing the 1-order entropy. These values that depended on the Hurst exponent represented inflection points in theΦp(m)functions that were highlighted in theEp(1,m)functions representing the discrete derivatives ofΦp(m). As inflection points can still be seen in the 1-order entropyEp(1,m), then calculating high order derivatives of the functionsΦp(m)should provide additional insight in the study of fBm. This will be developed in the next section.In order to advance in the examination of fBm, new simulations were undertaken with Hurst exponents ranging fromH=0.0to 1 by steps ofδH=0.01.Ep⁎and m⁎ were evaluated from 1-order entropiesEp(1,m)from each fBm withp=∞for non-fuzzy approaches and p=2 for fuzzy approaches. Fig. 2a and c represents the maximum of 1-order entropiesE∞⁎(H)andE2⁎(H). The blue line corresponds to the mean estimation and green lines to 95% confidence interval obtained with the traditional normal-based approach. It can be seen from these graphs thatE∞⁎(H)andE2⁎(H)were monotonic functions, decreasing from≈0.3to 0 asHincreased, the value of≈0.3being similar to that obtained for a Gaussian white noise, i.e. without any correlation in the time series. It can also be seen from these graphs that the higher the Hurst exponentH, the lower the maximum 1-order entropy; indeed, the greater the regularity (degree of correlation), the lower the complexity. These graphs show clearly a link between the maximum 1-order entropyEp⁎(H)and the intrinsic parameterH. Fig. 2b and d represents the abscissa of the maximum 1-order entropiesm⁎(H)(non-fuzzy and fuzzy). The blue line corresponds to the mean estimation and green lines to 95% confidence interval. It can be seen from these graphs thatm⁎(H)were discrete monotonic functions, increasing from 2 to≈50for non-fuzzy estimation and≈40for fuzzy estimation asHincreased: the higher the Hurst exponentH, the greater the optimal size of pattern m⁎. The latter corroborates the fact that the higher the Hurst exponent, the longer the correlation length and the greater the optimal pattern size. These graphs show clearly a link between the size of patternm⁎(H)and the intrinsic parameterH.As suggested in [3] Kruskal–Wallis tests with p-value<5%were performed to validate thatEp⁎and m⁎ can be used for discrimination purposes. Kruskal–Wallis tests were performed betweenEp⁎(1)=Ep⁎(H)andEp⁎(2)=Ep⁎(H+δH), and betweenm⁎(1)=m⁎(H)andm⁎(2)=m⁎(H+δH). These tests were obtained from fBm withHranging from 0.0 to 1 and withδHranging from 0.01 to 0.09. The minimumδHE⁎†andδHm⁎†values above which the discrimination was significant are reported in Table 1. The results set out in Table 1 show that minimumδHE⁎†andδHm⁎†deviations were dependent on the value of the Hurst exponent and varied from 0.01 to 0.03. It is obvious from Table 1 thatEp⁎is a good discriminating parameter since it permits significant differentiation of two fBms differing by less than 10% of the Hurst exponent value. This outcome augurs well for discriminating two time series of different complexity. Findings regarding the other parameter m⁎ were not so clear and suggest that it should be used with caution. Finally, it should be noted that the outcomes derived fromEp⁎were very similar between non-fuzzy and fuzzy approaches, and this was not the case for m⁎.To sum up this section, it can be confirmed that the maximum amplitudeEp⁎of the 1-order entropy was a good descriptor, directly related to intrinsic features of the time series under study, and was also a good indicator to discriminate different levels of complexity while keeping tolerance fixed at r=0.2. Although the maximum entropy augured well for the complexity analysis, not all features of theΦp(m)functions were used profitably. This latter point is the subject of the next section.It was shown in the previous section thatΦp(m)׳s were monotonic decreasing functions whose slope and inflection points depended on the Hurst exponent. This feature that was not used profitably is the starting point of this section. To benefit from the slope-dependence ofΦp(H), the discrimination function was defined by(9)ΔEp(n)=|Φp(1)(1+n)−Φp(2)(1+n)|,and was based onΦp(i)(1+n)with i=1 forHand i=2 forH+δH. By slightly modifying the definition ofΔEp(n)as(10)ΔEp(n)=|(Φp(1)(1+n)−Φp(1)(1))−(Φp(2)(1+n)−Φp(2)(1))|,and by imposingΦp(1)(1)=Φp(2)(1)(made possible by the normalization outlined in Eq. (7)) the definition of the n-order similarity entropy previously presented in Eq. (1) was recognized with m=1:Ep(i)(n,1)=Φp(i)(1+n)−Φp(i)(1), leading to(11)ΔEp(n)=ΔEp(n,1)=|Ep(1)(n,1)−Ep(2)(n,1)|.Although it is probable that the following equation does not always guaranteeΦp(1)(m)=Φp(2)(m), Eq. (11) can be generalized∀mas follows:(12)ΔEp(n,m)=|Ep(1)(n,m)−Ep(2)(n,m)|,based on the n-order entropyEp(i)(n,m)defined by(13)Ep(i)(n,m)=Φp(i)(m+n)−Φp(i)(m),with i=1 forHand i=2 forH+δH.p=∞corresponded to non-fuzzy estimations and p=2 to fuzzy estimations.All possible values ofEp(i)(n,m)were gathered in the symmetrical matrixMp(i)defined by(14)whereMp(i)(k,l)=|Φp(i)(k)−Φp(i)(l)|. This matrix was symmetrical since|Φp(i)(k)−Φp(i)(l)|was equal to|Φp(i)(l)−Φp(i)(k)|. The structure of this matrix was specific since its main diagonal was null (Mp(i)(k,k)=0). The main diagonal corresponded to the 0-order entropies and the diagonals just above and below the main diagonal corresponded to the 1-order entropies (in blue), 2-order entropies (in green), 3-order entropies (in red), etc.Finally, the discrimination functionΔMpbased on the matrixMp(i)was defined by(15)ΔMp=|Mp(1)−Mp(2)|.For discrimination purposes, 200 normalized fBms composed of 1024 samples were simulated: 100 fBms withH=0.07and 100 withH+δH=0.3were selected because it had been shown in [12] that such fBms could coarsely model fetal heart rate signals for abnormal and normal fetuses, respectively. The size of the pattern m varied from 1 to 30 to guarantee that the maximum was reached, and thereforeΦp(m)andEp(1,m), as well as a discrimination functionΔEp(n), were considered for these kinds of fBm. The feasibility of this new concept was validated by the simulation results reported in Figs. 3–5.Fig. 3a–d representsΦ∞(1)(m),Φ∞(2)(m),E∞(1)(n,1),E∞(2)(n,1),E∞(1)(1,m),E∞(2)(1,m),ΔE(1,m)=E(1)(1,m)−E(2)(1,m)andΔE(n,1)=E(1)(n,1)−E(2)(n,1). Figs. 3e–4h represent the latter functions with fuzzy estimations. It can be seen from Fig. 3c–d and g–h that the location of the maximumΔEp⁎occurred at positions that were different from those obtained forEp⁎(i). Note thatΔEp(n⁎,1)was defined by Fig. 3d and h:{ΔEp⁎=max(ΔEp(n,1)),n⁎=argmaxnΔEp(n,1).MaximumΔEp(1,m⁎)was defined by Fig. 3d and h:{ΔEp⁎=max(ΔEp(1,m)),m⁎=argmaxmΔEp(1,m).It can be seen from these graphs thatΔEp(n⁎,1)>ΔEp(1,m⁎).Indeed,ΔE∞(5⁎,1)>ΔE∞(1,2⁎)andΔE2(4⁎,1)>ΔE2(1,2⁎)were obtained forp=∞and for p=2, respectively. This suggested that it was more advantageous to use the maximum n-order entropy rather than the maximum 1-order entropy.Several values derived from Fig. 3 are reported in Tables 2 and 3.The results derived from Table 2 showed the following:•The lowest “relative accuracy” was obtained forE∞(5⁎,1)since it represented on average(91/14)/2=52.5%of the mean value while it represented(95+22)/2=58.5%and(71+109)/2=90.0%forE∞(1,2⁎)andΦ(5), respectively. This is advantageous for discrimination purposes.The lowest “relative accuracy” was obtained for fuzzy estimations compared to non-fuzzy estimations. Reductions of(90−72)=18.0%,(52.5−32.5)=20%and(58.5−31)=27.5%were obtained, respectively. This indicates that the fuzzy approach fully satisfies its role.By applying Kruskal–Wallis tests with a p-value<5%to the complexity parameters obtained withH=0.07and withH+δH=0.3, the complexity parameters were significantly different.The results shown in Table 3 indicated the following:•n-Order entropy was always higher than 1-order entropy since:ΔE∞⁎(5,1)>ΔE∞⁎(1,2)for non-fuzzy estimations andΔE2⁎(4,1)>ΔE2⁎(1,1)for fuzzy estimations.The lowest “relative accuracy” was obtained for fuzzy estimations since it represented 98% of the mean value ofΔE∞⁎(5,1)and86%ofΔE∞⁎(1,2)and97%ofΔE2⁎(4,1)and 92% ofΔE2⁎(1,1). Once again, it was advantageous to use fuzzy estimations since the standard deviations were the smallest. However, it can be seen that the mean values obtained from fuzzy estimations were lower than those obtained from non-fuzzy estimations. This latter point was not in favor of fuzzy estimations when fBms were considered.By applying a Kruskal–Wallis tests with a p-value<5%to complexity parameters obtained withH=0.07and withH+δH=0.3, the complexity parameters were significantly different.Moreover, if the idea was to discriminate two fBms (with for instance22These values were used since the difference was significant whatever the values of n and m when consideringH=0.07andH+δH=0.3.H=0.3andH+δH=0.325) from any values of m and n except for m⁎ and n⁎, then it was more advantageous to use n-order entropy than 1-order entropy (Fig. 4) since the difference between two fBms was significant for n-order entropy whatever the value n, whereas the difference for 1-order entropy was significant whatever the value of m except for m=7, 8, 9.Mp(1)andMp(2)are reported in Fig. 5b and c for non-fuzzy estimations, and in Fig. 5f and g for fuzzy estimations. It can be seen from Fig. 5b, c, f and e that the more interesting values were located on the border of the matrix, i.e. for small values of n when m was fixed and vice versa. The matrix differenceΔMpreported in Fig. 5d for non-fuzzy estimations and in Fig. 5h for fuzzy estimations showed that the best values were between n=4 and 8 when m=1 and between n=25 and 30 when m=6. These results indicate that it is worthwhile using n-order entropy for discriminating fBms of different levels of complexity.To sum up this section, it was shown that it was more advantageous to use maximum n-order entropy rather than maximum 1-order entropy. This was confirmed for high values of entropy and for the lowest relative accuracy. However, these results were obtained for fBms that were hypothesized as a coarse model of fetal heart rate signals. In order to confirm that n-order entropy still works on biomedical signals, examination of fetal heart rate signals is reported in the following section.The aim of this section is to demonstrate the validation that the n-order fuzzy entropy provides discrimination between abnormal and normal fetuses. Data collection is introduced first and then the methods are compared.Fetal heart rate (FHR) signals were measured using a homemade pulse Doppler system co-developed with Altaïs Technologies (Tours, France). This system, which transmits ultrasound waves of 2.25MHz for an acoustic power limited to 1mW/cm2 (for more details see [28,29]), was developed to measure both the FHR and fetal movements (pseudo-breathing and limb movements).After locating the fetal heart with an echographic scanner, 80 Doppler recordings of 30min each were acquired at CHRU Bretonneau Tours, France. In order to constitute homogeneous groups without spurious data, gestations complicated by other kinds of disorders (hypertension, diabetes) were discarded. Two groups of fetuses were selected: normal and those with severe Intra-Uterine Growth Retardation (IUGR). The normal group included 40 fetuses without disorders, delivered at term by spontaneous labor. The severe IUGR group included 40 fetuses delivered prematurely by cesarean section. For this clinical protocol, the gestational ages of fetuses ranged from 26 to 34 weeks. The consent of each parent was obtained and the study was approved by the Ethics Committee of the Clinical Investigation Centre for Innovative Technology of Tours (CIC-IT 1415 CHRU of Tours). All parents were over eighteen years of age and pregnancies were single.

@&#CONCLUSIONS@&#
