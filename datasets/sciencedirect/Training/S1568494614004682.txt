@&#MAIN-TITLE@&#
Granular modeling and computing approaches for intelligent analysis of non-geometric data

@&#HIGHLIGHTS@&#
We present a review paper on the importance of data granulation in the wide contexts of soft computing and pattern recognition.We focus on the analysis of the so-called non-geometric data.We provide the motivations and highlight the founding concepts.We finally elaborate on the proposed data analysis framework.

@&#KEYPHRASES@&#
Granular modeling and computing,Analysis of non-geometric data,Dissimilarity measure,Soft computing,Pattern recognition,

@&#ABSTRACT@&#
Data analysis techniques have been traditionally conceived to cope with data described in terms of numeric vectors. The reason behind this fact is that numeric vectors have a well-defined and clear geometric interpretation, which facilitates the analysis from the mathematical viewpoint. However, the state-of-the-art research on current topics of fundamental importance, such as smart grids, networks of dynamical systems, biochemical and biophysical systems, intelligent trading systems, multimedia content-based retrieval systems, and social networks analysis, deal with structured and non-conventional information characterizing the data, providing richer and hence more complex patterns to be analyzed. As a consequence, representing patterns by complex (relational) structures and defining suitable, usually non-metric, dissimilarity measures is becoming a consolidated practice in related fields. However, as the data sources become more complex, the capability of judging over the data quality (or reliability) and related interpretability issues can be seriously compromised. For this purpose, automated methods able to synthesize relevant information, and at the same time rigorously describe the uncertainty in the available datasets, are very important: information granulation is the key aspect in the analysis of complex data. In this paper, we discuss our general viewpoint on the adoption of information granulation techniques in the general context of soft computing and pattern recognition, conceived as a fundamental approach towards the challenging problem of automatic modeling of complex systems. We focus on the specific setting of processing the so-called non-geometric data, which diverges significantly from what has been done so far in the related literature. We highlight the motivations, the founding concepts, and finally we provide the high-level conceptualization of the proposed data analysis framework.Information granularity is a concept pioneered by Zadeh (see e.g., [84,85]), which has been conceived with the aim of developing the so-called human-centered computation. According to Pedrycz [60]: “Humans perceive and process complex information by organizing existing knowledge along with available experimental evidence and structuring them in a form of some meaningful, semantically sound entities, which are central to all ensuing processes of describing the world, reasoning about the environment, and supporting decision-making activities”. An aggregate of such entities results in what is commonly termed Information Granule (IG), which constitutes the key formal element of the discipline that is nowadays called Granular Computing (GrC) [2,3,57,61]. To quote again Pedrycz [60]: “Granular Computing has emerged as a framework in which information granules are represented and manipulated by intelligent systems. The two-way communication of such intelligent systems with the users becomes substantially facilitated because of the usage of information granules.”GrC can be understood as a general framework (or a paradigm) for the analysis of complex data and of the generating systems through IGs, in which the human interpretability, of both the data and the mechanism driving the analysis itself, is of primary importance. A founding prerequisite of an IG is the capability of handling the uncertainty of the entities it aggregates. As claimed by Klir [29] “The nature of uncertainty depends on the mathematical theory within which problem situations are formalized”. This intuitive fact suggests that the mathematical description of the data uncertainty pertaining a specific situation/process is not absolute, although it should be possible a reasonable and consistent mapping among the various theories. As a consequence, the specific setting in which IGs are defined affects in turn the way the data uncertainty is handled and therefore used in practice by an intelligent system operating through data granulation. Modeling IGs with formal mathematical settings has been object of study for over a decade, which resulted in a wide collection of frameworks, such as interval analysis, fuzzy sets, shadowed sets, and rough sets, whose choice and effectiveness-in-practice depend on the nature of the problem at hand [2,62]. The integration of GrC constructs and methods for the design of intelligent systems, such as pattern recognition, control, and decision making systems, is nowadays a well-established and effective possibility [7,11,12,25,36,38,42,43,46,47,65,68,70,80].According to our personal research viewpoint, modeling data through IGs and computing by means of GrC-based techniques assumes great importance in the design of intelligent systems for two intriguing operative perspectives:1.for the possibility to extract from the available dataset suitable IGs with a well-defined, problem-dependent, semantic value, allowing an effective description of the process to be modeled at the “right” description level;for employing IGs in the design of automatic and effective data-driven inference engines.In the first case, data aggregates (i.e., the IGs extracted from the dataset) can be considered as (i) synthetic yet semantically and formally sound knowledge to be analyzed by field experts, or as (ii) completely new (complex) data types to be considered as input of a suitable intelligent system. The second point of the list, instead, elaborates toward the possibility of using GrC-based computational techniques (i.e., algorithms) inside the data-driven inference engine of the intelligent system (e.g., a classifier or a control system). In this case, we stress the need to conceive computational procedures able to extract/synthesize IGs from the raw input data (that is not necessarily of numeric type). Besides, we underline also the even more pressing need to allow a “comparison” among IGs, and among an IG and a non-granulated data type. The design of such techniques for comparing IGs assumes a great importance at all levels of system design process, especially when dealing with hierarchical representations of complex systems.Of course, a procedure for information granulation, which generates output IGs, is highly dependent on the particular ultimate objective at hand. In fact, the granularity level, i.e., the level of specificity/generality of an IG, is rather a problem-dependent issue. Suitable optimization procedures are usually employed for the purpose of finding the “correct level” of data granulation [10,58,63]. System–data models can be expressed (and synthesized) at different granulation levels. In fact, complex input–output relations can be difficult to discover with a “wrong” information aggregation level, while they can be more easily expressed by relying on an appropriate collection of IGs. For instance, let us consider a data mining problem in the bioinformatic context, where the aim is to discover the causal relation between some complex functions in cell membranes with respect to its structure. Depending on the nature of the underlying process to be modeled by a data-driven procedure, we can identify at least three different levels of granulation. The first one relies on single atoms, which can be considered as the basic “row data” level. The second one considers chemical groups, such as the methylene (CH2), as the fundamental elements to be used in system description and modeling. The third one defines cell membrane structure description in terms of macromolecules, such as phospholipids, glycolipids, and cholesterols (see Fig. 1). The availability of an automatic procedure able to determine the best granulation level for the problem at hand is essential in data-driven modeling of complex systems.Many mathematical models of IGs have been defined so far [62]. Among the many settings, we can cite ordinary sets, interval analysis, (higher-order) fuzzy sets, shadowed sets, and rough sets [2,15,47,53,60,62]. Hybrid models of IGs have also emerged more recently, such as fuzzy rough sets and rough fuzzy sets [18,64] and shadowed fuzzy sets [74–76]. While the theoretical analysis of each specific IG model can be performed with notable interest in an abstract sense (e.g., by studying operations, characteristic descriptors, and associated complexities), the effectiveness-in-practice of the model is mostly an engineering problem, which depends on the specific task at hand and on the ultimate needs of the (target) user.The remainder of the paper is structured as follows. In Section 2 we introduce the pattern recognition and soft computing contexts, toward pointing to the direction pertaining the analysis of particular types of data: non-geometric patterns. Section 2.1 highlights the importance of the definition of the so-called dissimilarity (and similarity) measures. In Section 3 we discuss the high-level conceptualization of the granular computing approach for dealing with problems of data-driven inductive inference by means of dissimilarity measures. Section 4 conveys the concluding remarks.According to Theodoridis and Koutroumbas [78]: “Pattern recognition is the scientific discipline whose goal is the classification of objects into a number of categories or classes. Depending on the application, these objects can be images or signal waveforms or any type of measurements that need to be classified. We will refer to these objects using the generic term patterns.”. Patterns are essentially ubiquitous also in our everyday experiences. In fact, it is possible to identify recurring or characterizing activities even in the sequence of basic actions of most of us in a typical day (e.g., wake-up time, breakfast, day activities, and so forth). Such a pattern could be conceptualized as a behavioral pattern, which in fact provides the description of an instance of a typical day schedule. Patterns are everywhere in Nature and society, such as in atmospheric physics, complex biochemical processes in living beings and ecosystems, clinical diagnostics, financial markets and economical trends, social networks, organization and behavior of animals, and so on. Many more intuitive examples could be provided. Human knowledge and reasoning is founded on searching for such patterns and on their effective aggregation to define meaningful concepts and rules. However, formally speaking, a pattern is essentially an experimental instance of the so-called data generating process, P. A (data generating) processP:X→Yis the idealization of a system (either abstract or physical) which generates outputs according to inputs.Xis referred to as the input space (or domain, representation space), where the patterns are effectively represented according to some suitable formalism.Yis instead the output space, which intuitively is the space that allows the “recognition” – e.g., the classification – of input pattern instances. In pattern recognition, the closed-form expression describing P is unknown, in the sense that we do not know the details pertaining the process P. Instead, we are able to observe such a process through a finite sample, or datasetS, containing the aforementioned pattern instances, that is, a sampling of P. The ultimate goal of pattern recognition techniques can be roughly summarized as the aim of reconstructing a model of P, say M, by analyzingSonly, in order to optimize a suitable, problem dependent, performance measure of M on “unseen” patterns (the so-called generalization capability). According to this operative, but yet vague definition, it is possible to distinguish three problem types in pattern recognition: (i) classification, (ii) clustering, and (iii) function approximation. Such problems are intimately related to each other, while however they induce different possible applications and require different mathematical approaches. In classification,Yis finite set of classes (a finite nominal set of data) that is completely – or in some variations, partially – known a priori andXcontains pattern instances that are (correctly) labeled with respect toY; in clustering,Xcontains unlabeled pattern instances, and therefore the definition ofYis not available a priori (there is no supervision); finally, in function approximation (also termed regression)Yis a normed space, usually a subset ofℝd. Assuming to have established the mathematical model M used to describe P through some formalism, the other notable aspect of pattern recognition is the so-called learning or synthesis algorithm of the model. In practice, learning or synthesizing a model M from the available finite dataSconsists in optimizing some criteria, i.e., a performance measure semantically related to the problem, which allows to fit the model to the data instance at hand. Of course, it is possible to conceive different learning algorithms for the same problem (or model), while instead they usually differ when considering different problems (or models). Pattern recognition is therefore rooted in the reasoning framework of inductive inference logic, which is essentially a data-driven mechanism that implements generalization (i.e., inductive inference) starting from the available experimental evidence.Researches on pattern recognition have produced several theoretical and experimental results–applications in the past 50 years. It is beyond our scope to provide a detailed description of pattern recognition models and learning algorithms; the interested reader can refer to [19,72,78]. Pattern recognition techniques rely on two mainstream approaches: discriminative and generative. The former include modeling and computational techniques that are aimed at a direct synthesis of the decision regions/surfaces. As instance, the well-known perceptron model [78], key component of neural networks, in essence defines a hyperplane in the input space,X, as a decision surface inℝd. Such a geometrical object (i.e., the hyperplane) is used to discriminate among the data pattern instances (e.g., to associate the input pattern with a class). The generative settings instead originates in the statistical context, and basically it aims to characterize the data generating process, P, using statistical analysis (e.g., through random variables and stochastic processes). Usually, the baseline objective is to approximate the data distribution through the analysis ofS. Of course, as in any nowadays scientific context, many hybridized formulations exist.The soft computing discipline emerged almost in parallel with pattern recognition. Soft computing is a toolbox of techniques with the aim of providing solutions to modeling and computational problems in an intrinsically approximate or uncertain setting. Bonissone [9] provides the boundary for defining the soft computing context as the interaction of different areas: “The term Soft Computing (SC) represents the combination of emerging problem-solving technologies such as Fuzzy Logic (FL), Probabilistic Reasoning (PR), Neural Networks (NNs), and Genetic Algorithms (GAs). Each of these technologies provide us with complementary reasoning and searching methods to solve complex, real-world problems.” Pattern recognition and soft computing are nowadays interacting and hybridized disciplines, whose techniques are usually grouped under the recently coined term “Computational Intelligence”. Systems pertaining to such domains can be generally termed as data-driven intelligent systems, or data-driven inductive inference systems; a universally accepted terminology is not yet established in the current literature. Data-driven inductive inference systems can be implemented in the soft computing context by departing from the assumption of Boolean truth values in the logic inference (like, if A is true then B is true). In fact, inductive inference implemented in soft computing systems can be conveniently rooted in Zadeh's fuzzy logic [82,83], which is a many-valued logic with truth values in [0, 1]. Different many-valued logics (and corresponding set-theoretic frameworks) have been implemented so far, such as rough sets [53], Atanassov [1] intuitionistic fuzzy sets, and the three-valued logic underlying the shadowed sets construct of Pedrycz [55]. Well-known applications of fuzzy logic are the so-called rule-based (adaptive) fuzzy inference systems [26,48], and modern evolutions of fuzzy neural networks and higher-order fuzzy inference systems [8,22,27,28,31–33,45,52,81,87].Currently, there is a steadily increasing interest in processing particular types of complex data, by using the well-established pattern recognition and soft computing settings. Data-driven systems are historically designed according to the convention that the input spaceXis essentially a subset ofℝd, where it is convenient to use geometric, algebraic, and measure-theoretic concepts. When departing from theℝdvector-based pattern representation, many theoretical and practical problems arise, which are mostly due to the absence of an intuitive geometric interpretation of the data domain,X. Notwithstanding, recent availability of large and complex datasets motivated the development of pattern recognition and soft computing techniques on such non-geometric domains [34,44,54,71]. A particularly interesting instance of such non-geometric data are structured patterns. A structured pattern is conceived to convey in the same data structure the information regarding the relations and the characterizing attributes describing the pattern itself. A labeled graph is the most general structured pattern that is conceivable, since it allows to characterize a pattern by describing the topological structure of its constituting elements (the vertices) through their relations (the edges). Both vertices and edges can be equipped with suitable labels, i.e., the specific attributes characterizing the elements and their relations. Sequences of generic objects, trees, and automata, for instance, are (or can be though as) particular instances of labeled graphs. Moreover, many conventional data types, such as images and audio sequences can be conveniently interpreted as particular graphs. For instance, a gray-scale digital image can be represented by a lattice (i.e., a graph) where vertices represents image pixels and are labeled by the respective brightness value, while edges represent the neighborhood relations between adjacent pixels. Labeled graphs are used to represent data and systems in many different technical contexts, such as electrical circuits [17,49], dynamical and complex networks [6,50,86], biochemical networks [4,16,21,79], and segmented images [14,51,66,73].Other instances of non-geometric patterns are usually referred to as “unconventional” or “non standard” [42,43,77], in the sense that their usage or effectiveness is not yet well-established. Non-geometric patterns include data patterns which are characterized by pairwise “dissimilarities” that are not metric; therefore they cannot be straightforwardly represented through a geometric interpretation [23,54,71] (e.g., they cannot be drawn on an Euclidean space). As we will formalize in Section 2.1, the pairwise input data dissimilarities can be considered as the actual input of a data-driven system, since in fact there is a strong relationship among the dissimilarity and the data patterns that ideally generated such dissimilarity values.As stressed before, IGs extracted from the input datasetScan be used for the purpose of intepretability through the analysis of field experts. It is interesting to look at IGs as “new” patterns to be considered as the input of a suitable data-driven inference system (such as a classifier). Many theoretical and practical problems arise in doing so. We have recently addressed such issues in [39,42,43,69], where we have focused on the problem of adapting and evaluating well-known classifiers in the domain of type-2 fuzzy sets, such as the k-NN, support vector machine (SVM), and a neuro-fuzzy min-max network [67]. It is worth stressing the general validity of interpreting an IG as a data pattern, which can be straightforwardly extended to virtually any IG model.The analysis of patterns has been traditionally conceived for data represented in a suitable vector space, which is normally (a subset of)ℝd; this representation is commonly termed as feature-based representation. The feature-based representation automatically provides a way to define the positions of patterns in the input space, which allows to suitably implement the concepts of similarity and distance (see Fig. 2). The similarity (degree of nearness) of two patternsx_,z_∈ℝdcan be favorably elaborated by exploiting the inner product,(1)〈x_,z_〉=x_·z_=∑i=1dxi×zi.The concept of distance (degree of separation) is the dual concept of the similarity. In fact, in an inner product space the Euclidean (metric) distance among patterns can be obtained directly from their similarity:(2)d2(x_,z_)=〈x_−z_,x_−z_〉=〈x_,x_〉+〈z_,z_〉−2〈x_,z_〉.The converse is also true, since position, similarity, and distance are intimately related concepts.Let Xn×dbe the data matrix (i.e., a matrix with n feature vectors arranged as d-dimensional row-vectors), and letK∈ℝn×nbe the similarity matrix containing the pairwise inner products:Kij=〈x_i,x_j〉. K, called Gram matrix, is effectively defined in terms of X, since: K=XXT. The positioning inℝd(i.e., the feature vectors representing the patterns) of the data can be retrieved from the similarity matrix K through its eigendecomposition [54]:(3)K=UΛUT;X=UΛ1/2.In (3), U is the square orthogonal matrix containing the leading eigenvectors (as column vectors) of K, and Λ1/2 is the diagonal matrix containing the roots of corresponding non-zero eigenvalues (also called singular values). D and K are related by the following expression,(4)K=−12CD2C,which allows to obtain the positions from the distances via Eq. (3). Note that D2 is the matrix containing the squared distances, i.e.,Dij2=d2(x_i,x_j)2, and C=I−1/n (called centering matrix), where I is the identity matrix, 1 is the all-ones matrix, and finally n is the input data size.The algebraic relations herein introduced can be generalized to virtually any input space (i.e.,X≠ℝd) by means of the positive semi-definite (PD) kernel functions. A PD kernel functionk:X×X→ℝis a symmetric function that satisfies:(5)∑i,j∈{1,…,n}cicjk(xi,xj)≥0,∀n∈ℕ,x1,…,xn∈X,c1,…,cn∈ℝ.Stated in terms of matrix analysis, let us define Kij=k(xi, xj). Then, the kernel is PD if the similarity (Gram) matrix K is PD, that is, if it has nonnegative eigenvalues only. Such kernel functions are Mercer's kernels, and therefore their evaluation on the input spaceXcorresponds to an inner product evaluation on an implicit, and usually unknown, high-dimensional Hilbert spaceH. This property is referred to as the kernel trick[72], and can be formally summarized in the following well-known expression:(6)k(xi,xj)=〈ϕ(xi),ϕ(xj)〉H,whereϕ:X→His the implicit map. Examples of popular PD kernel functions are the Radial Basis Function (RBF), the exponential, the polynomial, and the linear kernel functions. Kernel functions are similarity functions of particular importance in pattern recognition and data mining, since they allow to include non-linear and adaptable components into the so-called kernel machines, such as the well-known (kernelized) SVM [72].In many practical situations, however, the information about the input data is available in terms of a weaker distance matrix D, which is called dissimilarity matrix (DM) in the literature, whereas accordingly the function d(·, ·) used to construct such a matrix is called dissimilarity measure[37,40,41,54,71]. A dissimilarity measure, d(·, ·), is in practice the generalization of a metric distance, which possibly satisfies only partially the four axioms of metric distances [54]. Nonetheless the information contained in the corresponding D can be greatly exploited for the design of pattern recognition systems for non-geometric input spaces [40,54]. A similar argument holds in the case of so-called indefinite similarity matrices/kernel functions, which can be still used, with appropriate observations, along with “conventional” kernel machines [13].A very interesting perspective in this scenario is the (weighted) combination of multiple (dis)similarity measures. In fact, when dealing with complex (structured) input objects, such as labeled graphs, usually the (dis)similarity measure is in turn defined as a combination of different (dis)similarity measures, which are tailored for specific tasks – e.g., in the graph scenario, it is common to use dedicated functions for the vertex and for the edge labels, which are combined to define the (dis)similarity measure for a pair of graphs as a whole [34]. Data normalization is a particularly important aspect of any real-world data analysis application. Notwithstanding the normalization of structured (and unconventional) patterns could be inapplicable at the pattern level (while it may be still performed for the specific descriptive attributes), normalization of related (dis)similarity measures output is instead always possible. Notably, it assumes paramount importance when combining, according to a specific weighting scheme, different and possibly heterogeneous (dis)similarity measures. This approach allows to easily define a (dis)similarity measure over very complex data structures, by combining several measures in a nested schema. Moreover, (dis)similarity values are easier to handle in practice and more interpretable when confined to a specific interval (e.g., [0, 1]).Modeling of granulated data-information is implemented by means of IGs. The extraction/synthesis of IGs from a finite input datasetS⊂Xis achieved by establishing the so-called indistinguishability rule. Such a rule (or strategy) is defined with the aim of aggregating the elements ofSaccording to their (dis)similarity, which may be expressed in terms of functional (dis)similarity, spatio-temporal (dis)similarity, or simply the closeness in a suitable representation space. Fig. 3provides an illustration describing the interplay among the dissimilarity measure and the indistinguishability rule.IGs extraction can be performed, for instance, by a clustering algorithm [56,59], once a proper similarity or dissimilarity measure onXhas been established. When performing clustering on the datasetSat hand, indistinguishable elements are grouped into the same cluster and each cluster model yields an IG. Obviously, for a fixed clustering algorithm and for a given dataset to be processed, changing the (dis)similarity measure would change the resulting collection of IGs. It is therefore fair to claim that the definition of a (dis)similarity measure is the most important component of any granular modeling system.Another very important aspect of utmost importance in the design of GrC-based inductive inference systems is the definition of (dis)similarity measures operating in the space of IGs. For example, when dealing with hierarchical agglomerative clustering algorithms [20], a dissimilarity measure among a pair of clusters must be defined. Besides, the possibility to process IGs as an effective data type in their own input space [42] gives rise to a recursive data granulation procedure, hence building a hierarchy of representations of the original dataset,S, where each layer of the hierarchy corresponds to a certain granulation (i.e., semantic) level.Granules of information are semantically sound constructs elaborated from the input datasetS⊂X. Such data aggregates are extracted, for the purpose of pattern recognition and data mining, according with two main objectives: (i) they can be used to characterize the input dataset and (ii) they can be considered as completely new data types, where in turn it is possible to establish new, semantically different, problems.In the first perspective, we identify two main additional possibilities. A collection of IGs,A={A1,A2,…,Ad}, elaborated fromSaccording to a suitable dissimilarity measure d(·, ·) and an algorithmic implementation of the indistinguishability rule (e.g., a clustering or a compression procedure), can be interpreted as a synthetic description ofS. Field experts may attempt to interpret such models with the aim of interpretability. On the other hand,Acan be used to develop a different representation of the input data inS, according to a suitable “embedding procedure”,ϕA(·), which describes each elementx∈Sin terms of the IGs ofA. Such a last possibility is particularly useful when dealing with non-geometric spaces, where a convenient strategy is to map the problem defined on the non-geometric domain,X, to a subset ofℝd, or other geometric space types (e.g., manifolds) – see Fig. 4and Section 2.1 for a brief introduction to the topic. However, the mapping is still useful also when dealing with “conventional” input spaces, especially for the purpose of data dimensionality reduction [60], since usuallyd≪|S|. It is therefore easy to understand that a dissimilarity measure able to compare pairs of IGs, and raw input elements with IGs, is of paramount importance in this case.The second aforementioned perspective suggests to consider the collectionAof IGs extracted fromS, as completely new data patterns. To be more specific, each IGAj∈Acan be considered as a new pattern instance, which can be assigned to a given class (or more generically, labeled) according to an automatic or human-based categorization. For instance, a new classification problem could be defined in the data domain underlyingA(i.e., the domain of IGs having a specific model) with the common aim of automatic learning and recognition. This possibility is also the fundamental conceptual procedure to design automatic modeling systems able to define a “hierarchy” of IGs, representing in this way the same datasetSin different semantic levels.The dissimilarity measure and the algorithmic implementation of the indistinguishability rule are the two underlying pillars of data granulation. The specific formal model used to describe an IG, which is an a priori design choice, may induce variations on the practical implementation of the indistinguishability rule, since different IG models usually require different information for their synthesis and/or representation. A very intuitive example of such a concept can be provided through the setting of clustering. IfS⊂X=[0,1]d, and d2(·, ·) is the Euclidean metric, then it is possible to granulate the data by means of the well-known k-means algorithm, for instance, which in its standard implementation (see as instance [78]) produces as output a hard partition. The resulting clusters are the IGs that are usually modeled as (standard) sets, and they are usually synthetically represented by centroids (although other cluster models exist inℝd). The well-known fuzzy k-means [5,24] (also called fuzzy C-Means) is a direct variation of the classical k-means, which generates a fuzzy sets based partition; in practice, the generated clusters are (type-1) fuzzy sets. A cluster in this case can be described by a point-wise membership function or, more conveniently, by means of a standard model (e.g., triangular, trapezoidal, Gaussian, etc.). The algorithmic implementation of the two k-means variations is almost identical (although they are guided by a slightly different optimization problem).The point that we want to stress here is that there is not necessarily a one-to-one mapping among the generated IG instances and the particular dissimilarity measure and indistinguishability rule used to construct them (although there is an obvious causality). In fact, different choices of dissimilarity measure and indistinguishability rule could lead to the same data granulation result.The so-called principle of justifiable granularity [58,60,63] has been developed as a guideline to the construction of IGs from the available (experimental) input data. IGs generated following such a principle have to adhere to the two conflicting requirements of (i) justifiability and (ii) specificity. The first requirement is designed to assure the definition of IGs in order to model sufficient experimental evidence. This means that a well-formed data aggregate (an IG) must be sufficiently populated. The minimum allowed cardinality is closely dependent to the problem/data at hand. The second requirement, on the other hand, ensures that IGs are not too dispersive, in the sense that an IG should come with a well-defined semantics. In other words, from the cluster validity perspective, a well-formed IG must be grounded on a cluster whose elements are well-described by the representation model of the IG itself.Modeling through IGs and computing by means of GrC-based techniques assumes great importance in the design of intelligent systems for two intriguing operative perspectives: for the purpose of a formal modeling of sound data aggregates elaborated from the available datasets, and for their use in the design of data-driven inference engines of intelligent systems. Many different and very important research topics (smart grids modeling and control, intelligent transportation systems optimization, biochemical and biophysical networks modeling, just to cite a few) are facing the same challenge: model, understand, and control complex systems. A complex system, by definition, cannot be described by a single model able to explain the dynamic and emergent behaviors. Rather, it must be characterized by a hierarchy of descriptions (i.e., models), where a given semantic level is defined considering the (possibly bi-directional) causal relations with the lower (upper) semantic level. To this aim, we claim that GrC techniques should be considered among the most promising tools to face complex systems modeling, allowing to build effective intelligent systems able to perform complex computational tasks.IGs can be used for the offline analysis of suitable field experts, aiming at the interpretation of such constructs. Moreover, IGs extracted from datasets can be considered (i) as new data patterns and therefore used to feed a suitable data-driven modeling system or (ii) to construct a new data description as the starting point for a higher level granulation procedure. The definition of inductive modeling problems on such data types opens new perspectives which are not yet substantially explored in the literature. However, in both cases the problem of IGs extraction is central. The definition of a universal and well-justified method for this purpose is certainly a major undertaking, since IGs are by definition problem-dependent data aggregates. Different general methods/approaches have been proposed in the literature to generate IGs (e.g., we cited the principle of justifiable granularity); however, fundamental issues remain open. In fact, while the principle of justifiable granularity itself is general enough, its implementation as an optimization problem has been successfully applied to generate hyperbox and type-1 fuzzy set based IGs, but assuming input data of numeric type. When (raw) input data is not numeric in nature, such a technique may not be (straightforwardly) applicable – while however studies investigating this particular aspect are not available in the literature.An important requirement when constructing IG models of input data is the preservation of the uncertainty [35]. Accepting such a high-level guideline for the design of IGs allows to conceive a general principle for the IGs formation based on “uncertainty preservation”. In fact, the uncertainty is a system/process mesoscopic quantity that is context-independent and it can be formally defined according to different mathematical settings (inducing hence different, although related, semantic interpretations) [29,30]. The problem of IGs generation could be defined as an optimization problem with the aim of minimizing the input–output uncertainty difference, where the input is the original input data space and the output is the space of IGs. Future research studies will be devoted also to such problems.Finally, another important aspect when dealing with granular modeling is the scalability property of the data aggregation procedure adopted for extracting IGs. In fact, especially when facing massive databases (the so-called Big Data) of complex patterns, the granulation procedures are in charge of computing a huge number of (dis)similarity measures, which are most of the times characterized by a high computational cost (e.g., when dealing with labeled graphs). For this reason it is important to focus on distributed and parallel computing systems, in order to design and implement scalable information granulation procedures. A step towards this direction is constituted by the multi-agent setting. In this framework, an IG is no longer a “static” data aggregate. Instead, it would be interpreted as an intelligent agent playing at the same time (i) the role of an active computational element belonging to a wider information processing system and (ii) the role of IG, discovered at a given granulation layer, possibly associated to a specific concept (a symbol) in the respective semantic level. An interesting example of this approach is offered by the hierarchical-granularity holonic modeling system recently proposed by Calabrese et al. [11]. In this agent-based scenario, a fundamental role towards effective and automatic complex systems modelling approaches will be played by swarm intelligence and evolutionary computation, towards the final revolution in systems engineering grounded on the concept of evolutionary design. To this aim massive parallel computing systems will be probably based on a new, more effective and flexible physical layer for information processing. When we will get the final breakthrough in this last engineering frontier, we will design by evolution, since evolution is the most intelligent design.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
