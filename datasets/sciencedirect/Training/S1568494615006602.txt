@&#MAIN-TITLE@&#
A memory based differential evolution algorithm for unconstrained optimization

@&#HIGHLIGHTS@&#
A novel “Memory Based DE” algorithm proposed for unconstrained optimization.The algorithm relies on “swarm mutation” and “swarm crossover”.Its robustness increased vastly with the help of the “Use of memory” mechanism.It obtains competitive performance with state-of-the-art methods.It has better convergence rate and better efficiency.

@&#KEYPHRASES@&#
Differential Evolution,Mutation,Crossover,Elitism,Unconstrained optimization,

@&#ABSTRACT@&#
In optimization, the performance of differential evolution (DE) and their hybrid versions exist in the literature is highly affected by the inappropriate choice of its operators like mutation and crossover. In general practice, during simulation DE does not employ any strategy of memorizing the so-far-best results obtained in the initial part of the previous generation. In this paper, a new “Memory based DE (MBDE)” presented where two “swarm operators” have been introduced. These operators based on the pBEST and gBEST mechanism of particle swarm optimization. The proposed MBDE is employed to solve 12 basic, 25 CEC 2005, and 30 CEC 2014 unconstrained benchmark functions. In order to further test its efficacy, five different test system of model order reduction (MOR) problem for single-input and single-output system are solved by MBDE. The results of MBDE are compared with state-of-the-art algorithms that also solved those problems. Numerical, statistical, and graphical analysis reveals the competency of the proposed MBDE.

@&#INTRODUCTION@&#
Optimization is a ubiquitous and spontaneous process and frequently appears in the real world problems. In the world of optimization evolutionary algorithms (EAs) have been treated as the successful alternatives, since last few decades. Among all EAs, differential evolution (DE) [1] is an efficient, formidable, and popular ingredient [2]. Some reasons for the popularity of DE are highlighted as follows:i.The main body of the classical DE requires 4–5 lines in any programming language. Therefore it has easy implementation, faster convergence, and stronger stability [3].It requires only a very few control parameters like CR (crossover rate), F (mutant factor) and NP (population size) to be tuned.The space complexity of DE is also low as compared to some of the most competitive real parameter optimizers [3]. In general, it has efficient global search ability and hence considered as global optimization algorithm [3].As evidenced by the recent studies on DE [4,5], it exhibits much better performance in comparison with several other EAs.So far, DE has received extensive attention and applied to many engineering optimization problems, such as mechanical engineering design problem [6], fuzzy clustering of image pixel [7], economic load dispatch [8] and many others [3]. However, most of the time, the solution gets stuck in some local optima. As a result it leads to a premature convergence. It is because DE have some individual shortcomings such as follows:i.The local exploitation ability and convergence rate of DE is too low [3].It loses maintaining the diversity in the population during simulation [3].The performance of DE decreases as the dimension of the problem increases [3].As other EAs it does not guarantee to find a global optimal solution in a finite time interval [3].Therefore, in order to improve the performance of basic DE, a number of attempts are made in the literature [3–16]. A detailed survey on the variants of DE can be found in [4,5]. Moreover, in order to improve the robustness of DE, a number of mutation strategies of DE have been proposed in [3,10–12]. Basically, DE is much sensitive to choice of the mutation strategy. On the other hand, inappropriate choice of mutation strategy may lead to premature convergence, stagnation, or wastage of computational time [3]. Also, it is very difficult to recommend a fixed set of parameters for different problems [3].Similarly, researchers mainly used two types of crossover schemes in DE, namely binomial crossover and exponential crossover [1]. In [17], Price recommended that the use of binomial crossover is better. But later, it is observed that there are no significant differences between these crossovers [18].Unfortunately, according to “No Free Lunch Theorem [19])”, no single optimization method exist, which is able to solve consistently to all global optimization problems. In spite of quite a high number of DE variants exist in the literature; DE further yields improved results while hybridizing with particle swarm optimization (PSO) [20]. Each of them is capable of dominating the shortcoming of the other to add the robustness in the resultant hybrid algorithm. The magical synergy of DE and PSO has been well established and has crossed many success milestones in recent past. The year-wise applications of DE-PSO hybrid techniques and their variants are summarized as follows:AuthorYearTechnique usedApplicationHendtlass [21]2001SDEAUnconstrained global optimizationZhang and Xie [22]2003DEPSOUnconstrained global optimizationKannan et al. [23]2004CPSOGeneration expansion planningTalbi and Batouche [24]2004DEPSOMedical image processingDas et al. [25]2005PSO-DVUnconstrained global optimizationMoore and Venayagamoorthy [26]2006DEPSO-MVMulti-objective optimizationHao et al. [27]2007DEPSOUnconstrained global optimizationOmran et al. [28]2008BBDEUnconstrained optimization problems and image processingDas et al. [29]2008PSO-DVEngineering designJose et al. [30]2009DEPSONoisy functionsZhang et al. [31]2009DE-PSOUnconstrained global optimizationCaponio et al. [32]2009SFMDEUnconstrained and engineering design optimizationXu and Gu [33]2009PSOPDEUnconstrained global optimizationWang and Cai [34]2009HMPSOConstrained optimization problemsKhamsawang et al. [35]2010PSO-DEPower systemsLiu et al. [36]2010PSO-DEConstrained and engineering optimizationWang et al. [37]2010DEDEPSOUnconstrained global optimizationNiknam et al. [38]2011FAPSO–VDEPower systemsPant and Thangaraj [39]2011DE-PSOUnconstrained and real life problems optimizationThangaraj et al. [40]2011DE-PSO, AMPSO, GA-PSOUnconstrained global optimizationEpitropakis et al. [41]2012A family of DE and PSO based hybridsUnconstrained global optimizationDor et al. [42]2012DEPSO-2SUnconstrained real life problemsXin et al. [43]2012−--Review and taxonomy of hybrid DE and PSONwankwor et al. [44]2013HPSDEWell placement optimizationAraújo and Uturbey [45]2013PSO–DEPower systemsSayah and Hamouda [46]2013DEPSOPower systemsKordestani et al. [47]2014CDEPSODynamic optimization problemsYu et al. [48]2014HPSO-DEUnconstrained global optimizationZuo and Xiao [49]2014Multi-DEPSODynamic optimization problemsParouha and Das [50]2015DPDConstrained and engineering optimizationThough many variants of DE and its hybrid algorithms have been suggested in the literature to solve optimization problems, they are unable to provide satisfactory result. The reason behind this is DE has no mechanism to memorize the so-far best solution, but it uses only the global information about the search space [37,51]. Therefore, in spite of the increased convergence rate of DE, the algorithm mostly loses its computing power and eventually leads to premature convergence [3].An attempt is made in this paper to employ the memory-based mechanism in DE algorithm under the PSO environment. The rest of the paper is organized as follows. Section 2 presents the traditional DE. Section 3 presents a detailed description of the proposed algorithm. Section 4 presents result and discussion. Application of proposed algorithm is presented in Section 5. Finally, Section 6 draws the conclusion with some future scopes.DE is simple yet powerful optimization algorithm introduced by Storn and Price in 1995 [1]. It uses three operators, mutation, crossover, and selection to evolve from the randomly generated initial population to the final individual solution. In the initialization a population of NP target vectors (parents) Xi=(x1i, x2i, …, xDi), i=1, 2, …, NP is randomly generated within user-defined bounds, where D is the dimension of the problem. This population undergoes with the cyclic processes of mutation, crossover, and selection, which are briefly explained below. In this paper, only the minimization problems are considered. However, maximization problem can easily be converted to minimization problem.Mutation: Let Xi(t)=(x1i(t), x2i(t), ……, xDi(t)) be the ‘ith’ individuals at ‘tth’ generation. A mutant vectorVi(t+1)=(v1i(t+1),v2i(t+1),……,vDi(t+1))is generated as follows:(1)Vi(t+1)=xr1(t)+F∗(xr2(t)−xr3(t)),r1≠r2≠r3≠i,i=1,2,…,NP,wherer1,r2,r3∈1,2,…,NPare randomly chosen integers, different from each other and also different from the running index i. F∈[0, 1] is a scaling factor which controls the amplification of the difference vector.Crossover: According to the target vector Xi(t) and the mutant vector Vi(t+1), a new trial vector (offspring) Ui(t+1)=(u1i(t+1), u2i(t+1), …, uDi(t+1)) is created as follows:(2)Uji(t+1)=Vji(t+1)if(rand(0,1)≤CR)orj=rand(i)Xji(t)if(rand(0,1)>CR)andj≠rand(i)where j∈{1, 2, …, D}, CR∈[0, 1] is the crossover constant, rand(i)∈[1, 2, …, D] is a randomly chosen index, which ensures that Ui(t+1) gets at least one parameter from Vi(t+1) [1].Selection: The generated trial vector Ui(t+1) from the crossover operation will be compared with the target vector Xi(t) based on better fitness values. The fittest between these two will survive for the next generation. Therefore the selection criteria in DE are defined as follows:(3)Xi(t+1)=Ui(t+1)iff(Ui(t+1))≤f(Xi(t))Xi(t)otherwise.The cyclic implementation of mutation, crossover, and selection is continued till it meets with the predefined stopping criterion.

@&#CONCLUSIONS@&#
