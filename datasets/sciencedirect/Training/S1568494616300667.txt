@&#MAIN-TITLE@&#
Integrated dimensionality reduction technique for mixed-type data involving categorical values

@&#HIGHLIGHTS@&#
An integrated approach of dimensionality-reduction technique is proposed.The approach can handle mixed-type datasets involving categorical attributes.Semantic similarity embedded in categorical values is taken into account.The proposed approach improves topological order of the high-dimensional, mixed data in the low-dimensional space.

@&#KEYPHRASES@&#
Information technology,Dimensionality reduction,Categorical data,Mixed-type data,Distance hierarchy,t-SNE,

@&#ABSTRACT@&#
Dimensionality reduction is a useful technique to cope with high dimensionality of the real-world data. However, traditional methods were studied in the context of datasets with only numeric attributes. With the demand of analyzing datasets involving categorical attributes, an extension to the recent dimensionality-reduction technique t-SNE is proposed. The extension facilitates t-SNE to handle mixed-type datasets. Each attribute of the data is associated with a distance hierarchy which allows the distance between numeric values and between categorical values be measured in a unified manner. More importantly, domain knowledge regarding distance considering semantics embedded in categorical values can be specified via the hierarchy. Consequently, the extended t-SNE can project the high-dimensional, mixed data to a low-dimensional space with topological order which reflects user's intuition.

@&#INTRODUCTION@&#
Large amount of data usually contain hidden patterns which may be useful to decision makers. There are many successful applications of data mining on real-world business [1–3]. In addition to huge volume of the data in the company's data repository, real-world data are usually high-dimensional. For instance, in the field of fraud detection, network robustness analysis, and intrusion detection, most such applications are high dimensional domains in which the data can contain hundreds of dimensions [4]. In the field of pattern recognition, image data [5] and genomic microarray data [6] could have thousands of features. High dimensionality of real-world data suffers several issues, including increased computational cost and curse of dimensionality which causes the notion of density and the distance between data points become less meaningful [7]. The minimum and the maximum distance become indiscernible as the difference between the minimum and maximum compared to the minimum distance converges to zero [8]. In other words, distance functions lose their usefulness in high dimensionality. A solution to cope with the problems resulting from high dimensionality of the data is dimensionality reduction which seeks techniques to reduce the number of data features.Many dimensionality reduction techniques have been proposed. One type of the techniques is based on feature selection which selects a subset of the original features according to some criteria. Qiu et al. proposed an algorithm combining support vector machine and recursive feature elimination to select relevant features, and then higher-order singular value decomposition was further applied to extract features [9]. Danubianu et al. presented a combined method for feature selection, where a filter based on correlation is applied on whole features set to find the relevant ones, and then, on these features a wrapper is applied to find the best features subset for a specified predictor [10]. Tsai et al. used feature selection methods to detect and extract notable oncogenes which were then used as the input for an SVM-based classifier for ovarian carcinoma classification [11].Another category of dimensionality reduction techniques is based on feature transformation which maps high-dimensional data to a low-dimensional space by some designated transformation function. Different from the previous type of techniques, the resultant set of features is not a subset of the original ones but newly created based on the original features. Many such methods of the type in the literature have been proposed, to name a few, principal component analysis (PCA) [12], classical multidimensional scaling (MDS) [13], Sammon mapping [14], locally linear embedding (LLE) [15], Laplacian eigenmaps [16], Kernel PCA (KPCA) [17], Maximum Variance Unfolding (MVU) [18], Local Tangent Space Alignment (LTSA) [19] and t-distributed stochastic neighbor embedding (t-SNE) [20], etc. The techniques have been applied to tackle many problems. Jamieson et al. applied Laplacian eigenmaps and t-SNE to computer-extracted breast lesion feature spaces for malignancy classification and visual inspection of medical image mappings [21]. Garces et al. applied t-SNE for visualizing the diverse styles of clip art [22]. Liu et al. applied locally linear embedding algorithm to reduce dimensionality of cancer gene expression data [23]. Thomas et al. proposed a data-driven bandwidth selection criterion for KPCA applied to DNA microarrays data. Their method optimizes the bandwidth and the number of components by maximizing the projected variance of KPCA [24].This study focuses on the transformation-based dimensionality reduction techniques. A shortcoming of the previous studies is that the investigations were all conducted in the context of numeric data.Most of real-world datasets consist of categorical and numeric attributes at the same time. For example, the data of credit card applications include numeric attributes such as annual salary, age, and the amount of various savings, and categorical attributes such as education, job, position, and marital status. Nevertheless, many knowledge discovery algorithms do not process mixed-type data. The algorithms analyze either only numeric or categorical data and tackle the deficiency by transforming one type of the data to the other. For instance, decision tree algorithms discretize numeric values to several ranges.For the algorithms which handle only numeric data, 1-of-k coding is a popularly adopted technique which converts each categorical value to a vector of binary values. However, the method suffers several drawbacks. First, the transformed data increase its dimensionality and so does its computational cost. Second, 1-of-k transforms a categorical value to a vector of binary values in which semantics embedded in the categorical value is lost. As a result, the new data do not retain its original topological structure. Moreover, the conversion could affect classification accuracy or clustering performance of algorithms such as k-NN classifier, k-mean clustering, SOM, etc.In this study, a method of dimensionality reduction integrated with a data structure, distance hierarchy or DH, for dissimilarity calculation between categorical values is proposed. The aim of the integration is to facilitate the recent dimensionality reduction technique t-SNE [20] to handle mixed-type datasets in the way of preserving semantics in categorical values. We verified the proposed approach by synthetic datasets to see whether topological order can be reflected in a two-dimensional plane. In addition, we investigated whether classification accuracy by processing categorical values with distance hierarchy coding scheme is better than that with 1-of-k coding scheme. Furthermore, we want to explore whether data processing that is based on DH and 1-of-k will affect data analysis in a low-dimensional data space resulted by dimensionality reduction.It shall be noted that the integration is not limited to t-SNE only. In fact, as long as the reduction techniques take distance matrix as input, such as Sammon mapping [14], the proposed integration is applicable. The advantage of the integrated model is enabling the capability of performing dimensionality reduction on datasets involving categorical attributes with consideration of semantic similarity between categorical values. Consequently, the projection of mixed-type datasets to a low-dimensional space by the new model match user's intuition better than that by the traditional approach.In the remaining of the paper, we introduce some background knowledge relevant to this study including 1-of-k coding scheme, dimensionality reduction and k-NN classifier in Section 2. In Section 3, the approach of integrating distance hierarchy with t-SNE dimensionality reduction is described. Experimental results on synthetic and real datasets are reported in Section 4. Section 5 gives conclusions.The 1-of-k coding is a commonly used method for converting a categorical value to numeric values by transformation of a categorical attribute to a vector of binary attributes. Specifically, a categorical attribute with a domain of k distinct values is transformed to a set of k binary attributes such that every binary attribute corresponds to one of the k categorical values. A categorical value in a data point is thus converted to a vector in the new data point in which the value of the corresponding binary attribute is set to one and the others zero. For instance, Drink in Fig. 1is a categorical attribute with a domain, say, {GreenTea, OolongTea, Cappuccino, Latte}. Drink is converted to four binary attributes accordingly. In the transformed data, the value GreenTea is represented by the binary vector 〈1000〉.As seen in Fig. 1, the dimension is increased from four to seven in this toy dataset. Furthermore, for the part of binary vectors, any two of the vectors from the transformed dataset yield the same Euclidean distance, namely,2. In other words, the semantic similarity embedded in the categorical values that GreenTea is more similar to OolongTea than to coffee Latte is lost in the transformed vectors in terms of the Euclidean distance. Consequently, topological order and classification accuracy based on distance functions can be affected.Transformation-based dimensionality reduction methods map data in a high-dimensional space to a low-dimensional space. Many dimensionality reduction methods have been proposed such as principal component analysis (PCA) [12], Sammon mapping [14], classical multidimensional scaling (MDS) [13], Locally Linear Embedding [15], t-Distributed stochastic neighbor embedding (t-SNE) [20], etc. The dataset in the high-dimensional space is defined as X={x1, x2, x3, …, xn}, xi∈Rd1 and in the low-dimensional space is defined as Y={y1, y2, y3, …, yn}, yi∈Rd2 and d2<d1. The transformation of the high-dimensional space to the low-dimensional space is denoted byf:X→Y.A general principle of dimensionality reduction includes three components which are characteristics of the data, characteristics of projection and error measure [25]. First, the distance or similarity between data points in the original data space is represented asdxij=fdx(xi,xj)The functionfdxcan be Euclidean distance for MDS, or joint probability for t-SNE. Second, the distance or similarity in the low-dimensional space is defined asdyij=fdy(yi,yj)Similarly, functionfdycan be Euclidean distance for MDS, or joint probability for t-SNE. Finally, the error of projection in the low-dimensional space is referred to as cost function and is defined asε=fε(dx,dy)The function fɛis minimized by weighted least squared error for MDS, and Kullback–Leibler divergences for t-SNE.t-Distributed stochastic neighbor embedding or t-SNE was proposed recently by Maaten and Hinton [20]. The performance of t-SNE is better than that of other dimensionality reduction methods including Sammon mapping [14], Isomap [26] and Locally Linear Embedding [15].The idea behind t-SNE is to construct a probability distribution in the low-dimensional space which is similar to the probability distribution constructed in the high-dimensional space. In the probability distribution, similar data points have a high probability of being picked, while dissimilar points have a low probability of being picked. t-SNE includes four main components [20]. The conditional probability of point j with respect to point i in the high-dimensional space is defined bypj|i=exp−xi−xj22σ2∑k≠iexp−xk−xi22σ2.The joint probabilities pijin the high-dimensional space is set to be the symmetrized conditional probabilities, i.e.,(1)pij=pj|i+pi|j2nwhere n is the number of data points.The probability distribution of data points in the low-dimensional space is defined by(2)qij=1+yi−yj2−1∑k≠l1+yk−yl2−1.The goal is to minimize the Kullback–Leibler divergence between the probability distribution P in the high-dimensional space and Q in the low-dimensional space. The cost function is defined byC=KL(P||Q)=∑i∑jpijlogpijqij.The cost function can be minimized by using gradient descent. The gradient of the Kullback–Leibler divergence is defined as(3)δCδyi=4∑j(pij+qij)(yi−yj)1+yi−yj2−1.k-Nearest neighbors or k-NN algorithm [27] is one of the most popular methods for classification. k-NN is a type of supervised learning that has been employed in various domains such as data mining, image recognition, patterns recognition, etc. To classify an unknown data point, the algorithm uses class labels of the k nearest neighbors to determine the class label of the unknown point. The unknown point is assigned to the class label which occurs most frequently among the set of nearest training neighbors. The classifier is sensitive to noise points if k is too small while the neighborhood may include points from other classes if k is too large. Moreover, the weighting with respect to the distances between the unknown point and the neighbors may affect classification result as well [28]. In principle, the nearer the training point is closer to the unknown point, the larger weight the training point shall have.

@&#CONCLUSIONS@&#
