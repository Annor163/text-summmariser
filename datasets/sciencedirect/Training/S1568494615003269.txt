@&#MAIN-TITLE@&#
Spectral clustering and semi-supervised learning using evolving similarity graphs

@&#HIGHLIGHTS@&#
We describe a spectral graph clustering method that aims to optimise a graph structure.The initial population is constructed using nearest neighbour graphs, defining a new way to convert a matrix to a one dimensional chromosome.We focus on evolving similarity graphs by applying fitness functions, based on clustering criteria.The proposed method is generic as it can be applied to all problems that can be modeled as graphs.

@&#KEYPHRASES@&#
Spectral clustering,Similarity graphs,Evolutionary algorithms,

@&#ABSTRACT@&#
Spectral graph clustering has become very popular in recent years, due to the simplicity of its implementation as well as the performance of the method, in comparison with other popular ones. In this article, we propose a novel spectral graph clustering method that makes use of genetic algorithms, in order to optimise the structure of a graph and achieve better clustering results. We focus on evolving the constructed similarity graphs, by applying a fitness function (also called objective function), based on some of the most commonly used clustering criteria. The construction of the initial population is based on nearest neighbour graphs, some variants of them and some arbitrary ones, represented as matrices. Each one of these matrices is transformed properly in order to form a chromosome and be used in the evolutionary process. The algorithm's performance greatly depends on the way that the initial population is created, as suggested by the various techniques that have been examined for the purposes of this article. The most important advantage of the proposed method is its generic nature, as it can be applied to several problems, that can be modeled as graphs, including clustering, dimensionality reduction and classification problems. Experiments have been conducted on a traditional dances dataset and on other various multidimensional datasets, using evaluation methods based on both internal and external clustering criteria, in order to examine the performance of the proposed algorithm, providing promising results.

@&#INTRODUCTION@&#
Clustering is an unsupervised learning process that aims at discovering the natural grouping of a set of data, such that similar samples are placed in the same group, while dissimilar samples are placed into different ones. The problem of clustering is a very challenging problem due to the assumption that no labels are attached to the data. Clustering has been used in a wide variety of applications, including bioinformatics [1,2], data mining [3], image analysis [4], information retrieval [5,6], etc. A detailed survey on clustering applications can be found in [7] and a more recent study in [8]. In [9] the authors attempt to briefly review a few core concepts of unsupervised and semi-supervised clustering.Spectral graph clustering [10] refers to a class of graph techniques, that rely on eigenanalysis of the Laplacian matrix of a similarity graph, aiming to divide graph nodes in disjoint groups (or clusters). In spectral clustering, as in all clustering techniques, nodes that originate from the same cluster should have high similarity values, whereas nodes from different clusters should have low similarity values. Spectral analysis can be applied to a variety of practical problems, including face clustering [11,12], speech analysis [13,14] and dimensionality reduction [15], and, as a result, spectral clustering algorithms have received increasing interest. More clustering applications of spectral graph clustering are reviewed in [16].In the last years, evolutionary-based approaches have been extensively applied to clustering problems due to their ability to adapt to very different problems with only few changes [17]. In [18] the authors proposed a genetic algorithm in order to search for the cluster centers by minimising a clustering metric, while in [19] authors aim to find the optimal partition of the data, using a genetic algorithm, without searching all possible partitions. In another, more recent work [20] the authors propose a new grouping genetic algorithm applied to clustering, emphasising to the proposed encoding and different modifications of crossover and mutation operators. A more detailed survey of evolutionary algorithms for clustering is presented in [21].Evolutionary algorithms are probabilistic algorithms, which might not guarantee the optimal solution, but are likely to return a good one, in a reasonable period of time. The most important advantage of evolutionary algorithms is that they do not require any auxiliary knowledge, but only a fitness function. This makes them good candidates for optimising different kinds of criteria. Moreover, other advantages of evolutionary algorithms are the simplicity of the method, their adaptability and the fact that they can cope with multi-modal functions. They are also particularly well suited for difficult problems, where little is known about the underlying search space.In the proposed approach, spectral graph clustering has been employed and applied on evolving similarity graphs, which have been transformed in such a way so as to play the role of the chromosomes in the genetic algorithm [22]. The initial population, for the needs of the genetic algorithm, is constructed with the aid of k-nearest neighbour graphs, represented as matrices, which are, then, transformed to one-dimensional binary strings and undergo genetic operators. The evolutionary algorithm is performed based on the value of the employed fitness function, that uses some of the most common clustering criteria. In the proposed method, we make use of spectral graph clustering in order to find logical grouping of the dataset.The remainder of this paper is structured as follows. In Section 2, we state the problem that we are dealing with in detail, as well as some of the general aspects that concern the proposed algorithm. We also discuss the way that the similarity graph is created and some spectral clustering issues. In Section 3, the proposed evolutionary algorithm is presented in detail. In Section 4, experimental results of the algorithm are presented and described. Finally, in Section 5, conclusions are drawn and future work is discussed.Clustering is the process of partitioning a usually large dataset into groups (or clusters), according to a similarity (or dissimilarity) measure. It is an unsupervised learning process that no labels are provided and, also, no information of the number of clusters is given. The aim of any clustering algorithm is to place in the same cluster samples that have a small distance from each other, whereas samples that are placed in different clusters are at a large distance from each other. If we assume that we have a dataset X, defined as X=x1, x2, x3, …, which consists of all the data that we want to place into clusters, then we define a clustering of X in m clusters C1, …, Cm, in such a way that the following conditions apply:•Ci≠∅, i=1, …, m∪i=0mCi=XCi∩Cj=∅, i, j=1, …, m, i≠jSimilarities between data samples can be represented as a similarity graph G=(V, E), where V, E represent vertices (or nodes) and edges of the graph, respectively. Assuming that each vertex virepresents a single data sample, then two nodes vi, vjare connected if the similarity si,jbetween them is positive or larger than a threshold, and the edge is weighted by si,j. Thus, the problem of clustering can be reformulated as finding a partition of the graph such that the weights within a cluster have high values, whereas weights between different clusters have low values. In an ideal situation, all data that belong to the same cluster should be placed in a compact group, which lie away from other compact groups.Before constructing a similarity graph, we need to define a similarity function on the data. This means that we should take under consideration that points which are “similar” in the dataset should also be similar after applying the similarity function. The most common similarity function S is the Gaussian similarity function (heat kernel) [23]. Heat kernel between two graph nodes is defined as:(1)Si,j=h(vi,vj)=exp−vi−vj2σ2,where σ is a parameter that defines the width of the neighbourhood. The value of σ plays an important role to the performance of the algorithm, thus, there are several, arbitrary, rules concerning its value. One of them is to choose σ using variants of the data diameter, for example the exact or multiples of the diameter.The purpose of a similarity graph is to connect data points that belong to local neighbourhoods, in a way that each point corresponds to a node in the graph. Local neighbourhoods can be expressed in many different ways, leading to many types of similarity graphs (e.g. k-nearest neighbour graphs, ϵ-neighbourhood graphs, fully connected graphs) and node connections. In general, experience shows that constructing a similarity graph is not a trivial task [24], and little is known on the effect of the various constructions. Moreover, it affects, significantly, the performance of the algorithm; different choices of the similarity graph lead to entirely different results.One of the most common choices when constructing a similarity graph [24], is k-nearest neighbour graphs (to be called k-nn graphs) because of their simplicity as well as their sparsity. The aim of a k-nn graph A is to connect node viwith node vjif vjis among the k nearest neighbours of vi, which results in a directed graph [24]. In the proposed method, an undirected graph was used, obtained by simply ignoring the directions of the edges, meaning that we connect viand vjif viis among the k-nearest neighbours of vjor if vjis among the k-nearest neighbours of vi(symmetric k-nearest neighbour graph).It is well known that spectral clustering is very sensitive to the choice of the similarity graph that is used for constructing the Laplacian [24]. Indeed, selecting a fixed k parameter for the k-nn graph is very difficult and different values lead to dramatically different clusterings.Indeed, the standard approach so far was to deal only with k-nn graphs and the optimisation of the parameter k was dealt with trial and error or cross-validation approaches. However, there is no justification of the use of k-nn graphs when someone wants to optimise a generic clusterability criterion. In the most generic case any arbitrary graph could be the fittest solution to the specific optimisation problem. Moreover, when we consider distributions that are not unimodal or they are mixtures of distributions with different density from region to region, it is not reasonable to consider that a single parameter k can work appropriately for the entire data space.A second approach, which also restricts the solutions is to consider a threshold for building the neighbourhoods (ϵ-ball neighbourhood). However, in that case the problem still remains that the solution is sub-optimal and considers that a single parameter ϵ will fit to the entire dataset.Optimising the clustering results over the graph structure is not a trivial task, since the clustering criteria are not differentiable with respect to the graph structure. Thus, we propose in this paper to use evolutionary algorithms in order to optimise specific clustering criteria, that are considered as fitness functions, with respect to the underlying graph, which is transformed to a chromosome solution.The proposed approach is by far the most generic one since it allows for any arbitrary graph to become the fittest given that the initial population and the genetic transformations are appropriately designed. To this end, we use the intuitively good solutions of the k-nn graphs in the initial population and also random graphs that will give us the diversity in the population.Spectral graph clustering [10], refers to a class of graph techniques, which rely on the eigenanalysis of a matrix, in order to partition graph nodes in disjoint clusters and is commonly used, in recent years, in many clustering applications [16]. As in all clustering techniques, in spectral graph clustering nodes that belong to the same cluster should have high similarity values, whereas nodes from different clusters should have low similarity values. Before proceeding to spectral clustering algorithm, it is crucial to define the Laplacian matrix.Let D be a diagonal N×N matrix having the sum dii=∑jWi,jon its main diagonal. Then, the generalised eigenvalue problem is defined as:(2)(D−W)v=λDvwhere W is the adjacency matrix, and v, λ are the eigenvectors and eigenvalues, respectively.Although many variations of graph Laplacians exist [24], we focus on the normalised graph Laplacian L[25], which is a symmetric matrix, and can be defined as:(3)L=D−1/2LD−1/2(4)=D−1/2(D−W)D−1/2(5)=I−D−1/2WD−1/2where W is the adjacency matrix, withwi,j=wj,i≥0,Dis the degree matrix and I is the identity matrix. The smallest eigenvalue of L is 0, which corresponds to the eigenvector D−1/21. The L matrix is always positive semi-definite and has n non-negative real-valued eigenvalues λ1≤⋯≤λn. The computational cost of spectral clustering algorithms is quite low when matrices are sparse. Luckily, we make use of k-nn graphs which are in fact sparse.In the proposed method, we perform eigenanalysis on L matrix, where W is defined as:(6)W=S⊙A,where S represents the full similarity matrix obtained using (1) and A represents an undirected k-nn matrix, which is a sparse matrix. The ⊙ operator performs element-wise multiplication. This process results in a sparse matrix W, only containing non-zero elements in places where A matrix contains non-zero elements. An example of the ⊙ operator is illustrated in Fig. 1. Eigenvalues are always ordered increasingly, respecting multiplicities, and the first k eigenvectors correspond to the k smallest eigenvalues. Once the eigenanalysis has been performed and the new representation of the data has been obtained, the k-means algorithm is used in order to attach a cluster to every data sample.In order to partition a dataset into clusters, spectral graph clustering has been applied on evolving k-nn similarity graphs. In more detail, we evolve a number of k-nn similarity graphs with the aid of a genetic algorithm, in order to optimise the structure of the graph, by optimising a clustering criterion. In this paper, clustering criteria were employed as fitness functions. Moreover, k-nn similarity graphs are transformed properly into chromosome solutions, in order to be used in the genetic algorithm.Let J be a clustering criterion that depends on the similarity graph W. However, the optimisation problem is not convex and, moreover, the fitness function is not differentiable with respect to W. Since S is considered constant after selecting a specific similarity function and through the definition of W in (6), the optimisation problem is defined as:(7)optimiseAJ(A),where Ai,j∈0, 1 is a k-nn graph.In order to create the initial population, we do not make use of the full similarity matrix S, mainly for time and space efficiency reasons. Instead, we use the sparse matrices that originate from k-nn graphs, resulting in an initial population that consists of matrices with binary elements. The decision of employing of the k-nn graphs, for the construction of the initial population, was based on the observation that their structure was already good (also they are sparse graphs). The aim of the proposed algorithm is to find a new structure of these k-nn graphs, so as to obtain better clustering results. Also, efforts to use only random sparse matrices, as initial population, have been made in order to gain completely different structures of the graphs, which only led to worse results, thus, not presented here.In this method, a Gaussian function has been employed as a similarity measure, in order to obtain the similarity matrix S, which is calculated pairwise for all the data in a database of our choice, using (1). Our experiments showed that the value of σ has a decisive role to the performance of the algorithm. In the proposed method, we have used multiples of the data diameter.The process of construction the initial population begins with the calculation of k-nearest neighbour matrices A, with k=3, …, 8, which constitute the backbone of the initial population. Next step is to enrich the population with nearly k-nearest neighbour matrices. In order to achieve that, we alter the k-nearest neighbour matrices that have already been calculated, by converting a small proportion of 0's, from A matrices, to 1's and vice versa. In more detail, in order to choose which elements of the matrix are going to be inverted, we first select randomly, with probability of 1%, T elements to be inverted from ones to zeros, and then, using the exact same number of elements we invert randomly T zeros to ones.This process guarantees that the proportion of 1's and 0's will remain the same in the new matrix, thus the new chromosomes will have almost equal number of zeros and ones. It is important not to alter the k-nn graphs completely, so as to keep all the good properties. Finally, a small proportion of completely random matrices are added to the set of matrices, in order to increase the population diversity, in which the number of 1's are equal to the number of 1's that a 5-nn graph would have. Approximately 10% of the initial population is comprised of random graphs.From the various experiments conducted, we have concluded that the selection of the parameter k of the nearest neighbour graphs is crucial to the clustering results, as illustrated in Fig. 2. Fig. 2a presents a dataset that consists of two classes with each one having a different colour. Fig. 2b and c represents the clustering results when a 3 and a 5-nearest neighbour graph were used, respectively. We should highlight the difference between the clustering results, especially when the elements are close to both classes.Before proceeding to the proposed algorithm, we must define the way that the k-nn matrices, and variants of these matrices, in the initial population are transformed into chromosomes, thus, we need to define how a square matrix, like a similarity described earlier, becomes a one-dimensional vector. As the k-nn graphs A are constructed in such a way to be symmetrical, we may only keep the elements of the upper triangular matrix, with no loss of information. Then, the remaining elements are accessed in rows sequentially, forming the one-dimensional vector. The procedure of reforming a square matrix in order to obtain the one dimensional chromosome is illustrated in Fig. 3.The novelty of the proposed algorithm is based on the way that we select to optimise the solutions of the problem, by optimising a clustering criterion J, as previously defined in (7). Since clustering criteria are not differentiable we make use of genetic algorithms in order to optimise them. Before focusing on how this is achieved, we need to define clustering criteria.Clustering criteria are divided into two main categories, internal and external criteria. The calculation of internal criteria implies that we have no prior knowledge about the data and we can only rely on quantities and features inherent to the dataset, whereas calculation of external criteria implies that we have some knowledge about the dataset in advance. More specifically, in order to use an external criterion, we should already have a representation of human reference clustering, called ground truth. Usually, in real problems, we do not have any prior information about the dataset, thus it is difficult to use external criteria in such problems. Nevertheless, external criteria are more representative of the cluster structures.In the literature, many different criteria have been proposed [26,27], that can be used in order to measure the fitness of the clusters produced by clustering algorithms. Some of the most widely used internal criteria are Calinski–Harabasz index [28], Davies–Bouldin index [29] and Dunn's index [30], whereas some external criteria are F-measure [31], purity [32], normalised mutual information [33] and a measure based on hungarian algorithm [34]. All the aforementioned criteria have been used in the proposed algorithm, some of them both for optimisation and evaluating the performance of the algorithm and some only for evaluation. Starting with internal criteria:•Calinski–Harabasz index [28] can be defined as:(8)CH=traceSBtraceSW×N−kk−1,where N is the number of the elements in the dataset examined (that is if a dataset consists of 100 images, then N=100), k is the number of the disjoint clusters after the partition, SBand SWare the between-cluster scatter and within-cluster scatter matrices, respectively:(9)SB=∑i=1kNimi−mmi−mT,and(10)SW=∑i=1k∑p=1Nixi(p)−mixi(p)−miTwhere Niis the number of objects assigned to the ith cluster, xi(p) is the pth element assigned to the ith cluster, miis the vector of element means within the ith cluster, and m is the vector of overall element means. Generally, we expect compact and well separated clusters to have high values oftraceSBand low values oftraceSW. Therefore, the better the data clustering the higher the value of the ratio betweentraceSWandtraceSB.Davies–Bouldin index [29] is a criterion also based on within and between cluster similarities and is defined as:(11)DB=1k∑i=1kmaxi≠jd¯i+d¯jdi,j,where k denotes the number of the disjoint clusters after the partition, i, j are cluster labels,d¯iis the average within-group distance for the ith cluster and di,jis the inter-group distance between clusters i and j. If the Euclidean norm is used, thend¯i,di,jtake the form of Eqns. (9) and (10), respectively. The smaller the value of DB index, the better the partition.Another internal clustering criterion is Dunn's index [30], which is based on separability and compactness of the clusters, and can be defined as:(12)D=minp,q∈1,…,k,p≠qdp,qmaxi∈1,…,kδi,where δiis the diameter of the ith cluster and dp,qis the set distance between p and q clusters. The diameter δiof a cluster i is defined as the maximum distance between a pair of elements within that cluster. The set distance dp,qcan be defined as the minimum distance between a pair of elements between two different clusters p and q. Large values of D imply a better partition of the data.In the bibliography, there are also many variations of all the aforementioned criteria [26], which are based on different definitions of distance, diameter, etc.External criteria assume that we, somehow, have a representation of the human reference clustering.•F-measure is a widely used external criterion that makes use of precision and recall measures [31]. Precision, is defined as the ratio of correct instances to the total number of elements that actually belong to the cluster, whereas recall is defined as the ratio of the correct elements to the total number of instances that the algorithm returned to belong in the cluster. Then, precision of a cluster j with respect to cluster i can be defined as:(13)prec(i,j)=Cj∩Ci*Cj,while the recall of a cluster j with respect to cluster i is defined as:(14)rec(i,j)=Cj∩Ci*Ci*.where C=C1, …, Cnis a clustering of a set andC*=C1*,…,Cm*is a representation of human reference clustering of the same set (i.e. ground truth). Then, F-measure is defined as:(15)F(i,j)=2×prec(i,j)×rec(i,j)prec(i,j)+rec(i,j),while the overall F-measure is given by:(16)F=∑i=1mCi*S×maxj=1,…,nF(i,j).whereSis the cardinality of the elements in the set andCi*is the cardinality of the elements in the human reference clustering set.Generally, a large precision value implies that most of the elements in the set were correctly clustered, but we might not have derived all the elements that belong to this cluster. On the other hand, a large recall value implies that we have found almost all elements that belong to a cluster, but we might also have a lot of elements that do not belong to this cluster. It must be noted that, F-measure, punishes more clusters that contain samples from different ground truth classes, than splitting a ground truth class into two clusters which, nevertheless, contains samples of only one ground truth cluster. In order to have better partitioning results, we need to maximise the F-measure.Another external criterion that is commonly used is purity [32], which can be defined as:(17)Pur=∑i=1knin1nimaxj(nij),where niis the size of the cluster i, k is the number of clusters, n is the total number of elements in the dataset, and(nij)is the number of elements in cluster i with the label j. Higher values of Pur, imply better clustering results.Normalised mutual information [33] is also used as an external criterion for evaluating clustering algorithms, and is defined as:(18)NMI(X;Y)=H(X)+H(Y)H(X,Y).where H(X), H(Y) denote the entropy of X and Y probability distributions, respectively and H(X, Y) is the entropy:(19)H(X)=−∑p(x)logp(x),(20)H(X,Y)=−∑p(x,y)logp(x,y),where p(x) is the marginal probability of X and p(x, y) is the joint probability of X and Y. Generally, the higher the NMI value in (18), the better the partition.Another external criterion is the hungarian measure Hun[34]. The aim of this measure is to match every cluster, which the algorithm returned, to the best possible class, as it was defined by ground truth. In more detail, if we suppose to have k clusters to which we want to assign l classes, on a one-to-one basis, and we also know the cost of assigning a given cluster to a given class, then we want to know the optimal assignment, the one that minimises the total cost. The hungarian method is an algorithm that finds an optimal assignment for a given cost matrix C. The higher the Hun value the better the clustering result.As the value of such criteria cannot be optimised, without the use of derivatives, we have employed evolutionary techniques in order to solve this problem. The optimisation is performed by altering the chromosomes or, else, by altering the k-nn similarity matrices A as in (2).As we have already defined how the initial population is formed and how the chromosome evaluation is performed, we may now define the details of the genetic algorithm. Some of the main operators are single-point crossover, multi-point crossover and mutation operators which are illustrated in Fig. 4. In a single-point crossover operator a random crossover point is selected, in a corresponding place in both the chromosome-parents. The offsprings are produced by mutually exchanging the sub-sequences of the chromosome-parents, in the crossover point. The multi-point crossover operator is the general case, where multiple crossover points are chosen and are mutually being exchanged, sequentially. Mutation operator is the random alternation that happens in one or more points of the bit string. In all Fig. 4(a)–(c) the chromosome-parents are presented on the left, while the chromosome-children are presented on the right.Evolutionary algorithms solve problems based on operators inspired from biology. The first step of the genetic algorithm is to select the chromosomes which will undergo the crossover operator. For this purpose, a roulette wheel method has been employed [35], where a probability is associated with each chromosome, based on the value of the fitness function: the higher the value, the higher the probability to be selected. The probability piof the ith chromosome to be selected, if fiis its fitness value, is defined as:(21)pi=fiΣj=1Nfj.This technique is based on the notion that the fittest chromosomes are more likely to produce fitter offsprings when combined, than if some less fitter were combined. The number of the offspring-chromosomes, that is chosen to undergo crossover operator, is usually the same as the parents-chromosomes in the population. Nevertheless, it is very usual a chromosome with a high fitness value to be chosen for crossover more than once and, also, a chromosome with a low fitness value not to be chosen at all.Next, we combine the selected chromosomes, some of which, it is reasonable to expect that they will produce chromosomes with a higher fitness value. The crossover rate defines if chromosomes will finally undergo crossover operator. The crossover rate takes values between 0 and 1 but, usually, a value closer to 1 is used; in our experiments we have used a value of 0.7. In the proposed algorithm, a single crossover point is randomly selected for every set of chromosomes and the sub-sequences that are formed are exchanged respectively.Then, we randomly choose a proportion of the chromosomes to undergo mutation, that is the random change of some elements of a chromosome. This proportion is based on the mutation rate, which was set to 0.4 in all of our experiments, that takes values from 0 to 1, as the crossover rate did. In more detail, for every chromosome, a random number r between 0 and 1 is generated; if r is larger than the mutation rate, then this chromosome will undergo mutation, else it will remain unchanged. A small value of the mutation rate is usually preferable. Moreover, in order to guarantee that the newly produced chromosomes will not have been altered too much, after the mutation, mutation is performed by converting a number of 0's to 1's and vice versa. More precisely, we first select randomly, with probability of 1%, T elements to be inverted from ones to zeros, and then, using the exact same number of elements we invert randomly T zeros to ones.After the application of genetic operators to the chromosomes, the new generation has been formed. In order to perform spectral clustering (Section 2.2), we need to reconstruct the k-nearest neighbour matrix A, which will consist of binary digits, from the one-dimensional vector chromosome. Then we apply the similarity matrix S on A using the ⊙ operator, in order to obtain the W as illustrated in Fig. 1. Spectral clustering [25] may now be performed on L as in (5).The next step is to calculate the fitness values of all the newly produced chromosomes, and place them along with the parent-chromosomes. Then, elitism is performed: we sort all chromosomes, with the fittest being on the top, and we keep only those chromosomes with the highest fitness value, so as the number of the chromosomes kept to remain unchanged after every generation.The proposed algorithm terminates when a maximum of 50 generations has been reached, or when the optimised criterion has not been altered for 5 consecutive generations.To the best of our knowledge this is the first paper that tries to optimise the generic graph structure in spectral clustering. Indeed, the proposed approach is very promising since it gives a generic solution to several machine learning algorithms that use the data structure in terms of their adjacency matrices as regularizers. The application of the proposed approach to supervised learning is straightforward and it will lead to novel classifiers with improved performance. Laplacian least squares, Laplacian support vector machines, graph embedding, etc. are some of the examples of algorithms that can benefit from the proposed approach.Traditionally, learning has been approached either in an unsupervised manner, for example clustering, where all the data are unlabeled, or in a supervised manner, for example classification and regression, where all the data are labeled. Generally, it is time consuming, and thus expensive, to collect labeled data, while unlabeled ones are easier to gather. The aim of semi-supervised learning is to combine labeled and unlabeled data in such a way in order to change the learning behavior, and design algorithms that take advantage of such a combination [36].It is natural for many practical problems to consider that we only possess a proportion of labels in a dataset. Then, the problem of clustering can be transformed into how this small proportion of labels can be used in order to obtain a better clustering of the data. Semi-supervised learning [37], in machine learning, is a class of techniques which uses both labeled and unlabeled data, usually a small set of the former and a large set of the latter, in order to obtain clusters. Thus, both labeled and unlabeled data are used in the learning process.Semi-supervised learning has been widely used during the recent years in clustering. It has been applied on many practical problems, including image classification [38], person identification [39] and large scale text applications [40].Following the notion of semi-supervised learning, we assume that our data follow a specific probability distribution P. Then, according to this definition, we can redefine each data sample, to which has been attached a label, as a pair of (x, y), where x represents the data sample and y represents the labels, and which are created based on the probability P. Moreover, the data with no labels are represented as x, which are created based on the marginal probability Pxof P. Semi-supervised learning is based on the fact that this marginal probability Pxcan be used in order to better learn any function (e.g. in clustering or classification). In this paper, we aim to learn how to produce chromosomes that improve clustering after every generation, by optimising a clustering criterion. In other words, the clustering criterion plays the role of the function being optimised.Usually, we need to make some assumptions about the underlying structure of the dataset distribution. Often, semi-supervised algorithms make use at least one of the following assumptions [41]:•Smoothness assumption. This assumption is based on the notion that two points, close to each other, in a high density area, is probable to share the same label.Cluster assumption. It is based on the notion that, generally, data tend to create distinct clusters, and those that are in the same cluster have also a high probability to share the same label.Manifold assumption. This hypothesis is based on the notion that the data are of low dimensionality. With this assumption, we aim to avoid the problem of very high data dimensionality.Low density separation. We usually assume that the boundaries of a cluster are selected to be in low density areas.In this paper, semi-supervised learning has been used in clustering, in order to optimise an external criterion. In more detail, in terms of the proposed algorithm, for some of the experiments presented we have assumed that we possess a small proportion of labels l of the dataset, which are selected randomly once and, then, the same labeled data are used in every genetic cycle. The proportion of labels that we have used in our experiments ranged between 5% and 20% of the total labels of each dataset. Then, using only these l labels, we have computed the fitness value f of the population, by using one of the external criteria. The evaluation of the algorithm is performed using only the rest of the criteria (and not the one being optimised), which are also being calculated during every experiment. The overall value of a criterion is the value of an external criterion calculated as if we possessed the labels for the whole dataset. Thus, this technique uses both labeled and unlabeled data in order to obtain clusters. Essentially, only a small proportion of labels was used in this method for obtaining the fitness values of chromosomes, while the rest of the procedure remained unchanged.

@&#CONCLUSIONS@&#
