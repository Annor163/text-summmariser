@&#MAIN-TITLE@&#
Multiview feature distributions for object detection and continuous pose estimation

@&#HIGHLIGHTS@&#
Multi-view model of object categories.Suitable to any type of image features, e.g. edges and coarse-scale gradients here.Performs detection, localization and continuous pose estimation in unified manner.Encode appearance at discrete training viewpoints and in-between.Competitive with best task-specific methods, with framework generally applicable.

@&#KEYPHRASES@&#
Appearance-based object recognition,Object detection,Pose estimation,Hough voting,Edges and shape models,Viewpoint synthesis,

@&#ABSTRACT@&#
This paper presents a multiview model of object categories, generally applicable to virtually any type of image features, and methods to efficiently perform, in a unified manner, detection, localization and continuous pose estimation in novel scenes. We represent appearance as distributions of low-level, fine-grained image features. Multiview models encode the appearance of objects at discrete viewpoints, and, in addition, how these viewpoints deform into one another as the viewpoint continuously varies (as detected from optical flow between training examples). Using a measure of similarity between an arbitrary test image and such a model at chosen viewpoints, we perform all tasks mentioned above with a common method. We leverage the simplicity of low-level image features, such as points extracted along edges, or coarse-scale gradients extracted densely over the images, by building probabilistic templates, i.e. distributions of features, learned from one or several training examples. We efficiently handle these distributions with probabilistic techniques such as kernel density estimation, Monte Carlo integration and importance sampling. We provide an extensive evaluation on a wide variety of benchmark datasets. We demonstrate performance on the “ETHZ Shape” dataset, with single (hand-drawn) and multiple training examples, well above baseline methods, on par with a number of more task-specific methods. We obtain remarkable performance on the recognition of more complex objects, notably the cars of the “3D Object” dataset of Savarese et al. with detection rates of92.5%and an accuracy in pose estimation of91%. We perform better than the state-of-the-art on continuous pose estimation with the “rotating cars” dataset of Ozuysal et al. We also demonstrate particular capabilities with a novel dataset featuring non-textured objects of undistinctive shapes, the pose of which can only be determined from shading, captured here by coarse scale intensity gradients.This paper is concerned with the joint recognition and pose estimation of object categories in 2D images. Recognizing that these two tasks represent two sides of a same problem, we tackle them in a unified approach. In general, the pose (3D orientation) of objects cannot be inferred from just one type of image information, e.g. silhouette and edges, to cite a common example. Additional visual cues may be necessary, such as the shading onto the object surface. A key point of our contributions is thus to provide techniques generally applicable in this regard, even to low-level, dense and/or non-descriptive image features. To perform continuous pose estimation, our object model captures, in addition to the appearance at discrete training viewpoints, the deformations between these, detected from the optical flow between training examples. A measure of similarity between generated views of the object (possibly at an unseen viewpoint) and a test image allows us to perform detection, recognition, and pose estimation in a unified manner. The following paragraphs present the principal motivations and key points of the method, comparing them to existing related work. Parts of these contributions were introduced in earlier publications [1,2].The recognition of objects in 2D images encompasses a number of tasks, detailed below, which are often considered as separate research problems. They are however closely related, and we handle them all with the same model and methods. Notably, we do not train discriminative models, which is the usual approach for the classification tasks.Localization The goal is to identify the parts of the test image that belong to the object of interest, versus the parts of the image that correspond to background clutter. The result of localization is typically a set of bounding boxes, which encircle candidate objects in the image, each accompanied with a detection score. We handle this task with an algorithm similar to the generalized Hough voting scheme. The model of the object can be learned from one or several training examples: we handle both cases identically by modeling distributions of features through kernel density estimation (see Section 2).Detection One must decide whether the object of interest appears in the test image or not. This task can be performed alone, or by setting a threshold on scores of localizations to obtain binary detection results.Classification (among objects or among discrete poses) One must determine which object or which viewpoint among learned ones appears in the image. This traditionally involves learning discriminative classifiers. In our method however, we simply build generative models independently for each learned object or viewpoint, and determine the best match from the similarity measured between the test image and one of those models.Continuous viewpoint (pose) estimation This more challenging task is handled by extending our generative models to also synthesize unseen (untrained) viewpoints.The method for performing recognition of objects in 2D images depends heavily on the internal representation chosen to model the appearance of those objects. We are interested in building models of appearance for object categories (or “classes”) rather than specific instances, thus capable of recognizing, to some extent, unseen objects that are similar to a category learned from a few training examples. The goal is for example to train the system with a set of different cars, then to recognize the pose of a new, unseen car. The categories in such a scenario are defined implicitly by the training instances used as examples. In the proposed approach, the appearance of the object under a specific viewpoint is modeled as the distribution of low-level image features, represented, in a non-parametric manner, by the actual image features of one or several training images of the objects under that specific viewpoint. We therefore handle variability in appearance in a probabilistic way; this variability among the training examples can equally come from different objects of a same category, or from variations of appearance of a unique object, e.g. observed under different conditions of illuminations.We choose to model the 2D appearance of the objects without explicit knowledge of their underlying 3D shape. The motive for this choice is to handle more easily and naturally the variability within categories, both in appearance and shape. As a result, the model is thus trained with simple example images. Existing methods have used explicit, geometrical, 3D models of objects [3], but the modeling of variations in appearance is generally limited in regards with shape [4,5]. One exception is the work of Glasner et al. [6], which uses structure-from-motion to reconstruct accurate 3D models from the training images. They then account for within-category variability simply by merging multiple exemplars in their non-parametric model, in a fashion very similar to the one we use (with our 2D training examples). One drawback of their approach is the initial need for a large number of views to reconstruct accurate 3D models. In comparison, our exemplar-based model can use an arbitrary number of views, which do not need to overlap, and the model can be incrementally updated as more views become available.Object localization and detection among clutter is commonly achieved with variants of either the “sliding window” or the “Hough voting” approaches. The former (used e.g. in [7]) uses a binary classifier, which is evaluated on a uniform sample of image locations and scales. Such an exhaustive search may prove computationally expensive, and many heuristics have been proposed to alleviate this issue [8]: salient regions, coarse-to-fine-search, etc. Voting techniques based on the well-known generalized Hough transform [9] provides another way to alleviate the complexity issue. Probabilistic formulations of this voting technique have been proposed through the implicit shape models [10,11]. Our algorithm for detection uses this voting scheme, applied to low-level, dense image features. Hough voting was extended to discriminative framework by Maji and Malik [12], by computing optimal weights to the image features of the model. They obtained excellent results, further improved by a subsequent verification step, in which the initial detections are rescored by an SVM-based classifier. We reuse this idea of weighting parts of the learned model; the exact procedure is slightly different, and suited to our non-discriminative features. Although not a central element of our contributions, we will show that this weighting often brings substantial improvements.The type of image features used to encode the appearance of the objects is a crucial choice. Some methods historically used of the appearance of the object as a whole [13–15], but with the common downsides of poor robustness to occlusions and a need for large numbers of training views. At the opposite end, feature-based methods have relied on “interest points”, precisely located in the images, and characterized by hand-designed descriptors of local appearance, such as SIFT descriptors [16]. Those discrete points can then be matched between the test image and the training examples [3]. While this approach has proved to be highly successful and efficient in many cases, the extraction of such discriminant image features cannot be relied upon in general cases, as it often fails with non-textured objects. The basic approach also does not readily extend to variability within categories. A recent trend is to describe image contents with similar descriptors of appearance over a dense grid across the image, such as done by the successful histograms of oriented gradients (HOG) [7], also used within the state-of-the-art detector of Felzenszwalb et al. [17]. The idea behind those descriptors is to capture statistics or distributions of primitive characteristics (such as intensity gradients) over local image regions. We believe that this approach is indeed the most generally-applicable one, and is the central motivation for our technique. Similarly to, e.g. HOGs, our “distributions of features” capture local statistics densely over the images, but we do not depend on hand-designed descriptors, and we offer a unique formulation suitable to different types of image features. Another notable difference of our method with HOGs is to use gradients extracted at a coarse scale, intended to capture shape (rather than pure appearance) of smooth surfaces, whereas HOGs were most successful with gradients extracted at a much smaller scale, thus essentially capturing sharp transitions like edges.Most current, state-of-the-art methods for object recognition rely on the use of image edges (e.g. [18,19], among many others), seen as an efficient representation of the silhouette and shape of objects. The typical technique basically consists in building intermediate representations such as contour fragments, which can then be matched discriminatively between training and test images, and used e.g. in a Hough voting scheme. Our approach, which leverages the simplicity of low-level, fine-grained image features, can be applied to edges by considering all edge pixels of the image as features. At the cost of higher computational costs, this approach leads to excellent results as well, while satisfying our aim for a general and straightforward formulation.A large area of research has focused on the modeling and detection of deformable shapes (see [18] for a review). Interestingly, our simple approach proves competitive with some of those techniques, as demonstrated on the ETHZ shape dataset. Although we neither model continuous contours nor their variations explicitly, our low level features (edge points) can encode similar variations to some degree. Another advantage of our method is its ability to learn shape models similarly from a single or multiple examples, and from only loosely segmented images (with a bounding box). Such capabilities are not commonplace in the domain of shape matching, but were also offered by the work of Ferrari et al. [18].Finally, our capability of handling dense image features is demonstrated and used with great advantage with intensity gradients, extracted at a coarse-scale over the whole images. Using such gradients provides unique capabilities, as it allows one (1) to effectively handle non-textured objects (see Section 5.6), and, even more importantly, (2) to resolve cases where edges alone would only offer ambiguous information on the presence or the pose of an object in a scene. Indeed, the shading over homogeneous surfaces, captured by such gradients, may sometimes be the sole relevant clue, in particular to identify the exact pose of certain objects, or, for example, to differentiate between hollow versus full objects of similar shapes (see our experiments in Section 5.6).Object recognition with 2D training examples typically uses viewpoint-specific models, e.g. a model for cars seen from the front, and another for cars seen from the side. Recent contributions have included more and more techniques that handle multiple registered training viewpoints. The object in the test image is then matched against one of these viewpoints and allows performing a coarse estimation of its pose (or 3D orientation) also called pose classification. We refer to this basic approach as a “nearest-neighbor” pose estimation. Some applications (robotic interaction and grasping for example) require however a more precise estimation of the pose [20,21]. This capability was commonly reserved to recognition methods using 3D object models. As discussed above though, they do not cope well with object categories, which are clearly very challenging with regards to the task of pose estimation. Few appearance-based methods have been designed to provide this capability [14]. Most recent multiview models of appearance consider the different training viewpoints independently [21–25], while others try to match and link features across viewpoints [26–28]. Savarese and Fei-Fei [27], for example, model an object as a collection of planar parts that can appear in different views. We follow an intermediate approach, by storing independently the image features that make up the different views, but we also store, along with every each image feature, how its appearance varies with respect to the pose of the object. The multiview models mentioned above only performed localization and classification such as “frontal view” or “side view”, whereas we allow precise, continuous pose estimation.Simple techniques have been proposed to improve the precision of nearest-neighbor pose classification. They typically involve voting in the 3D pose space followed by averaging [21] or probabilistic smoothing schemes [1,25,29], leading to a precision beyond the resolution of viewpoints given as training examples. While those simple techniques have sometimes given very interesting results, we rather chose, in the work presented here, to explicitly detect, and include in the model, the changes of appearance between the discrete viewpoints seen during training (practically, how image features translate in the image, and thus how the appearance “deforms” between neighboring viewpoints). This information extends our generative model, which can now synthesize arbitrary, untrained viewpoints. We can then finely optimize the 3D pose, starting from the initial nearest-neighbor estimates. Let us mention the work of Torki and Elgammal [30]. In their radically different approach to appearance-based pose estimation, they learn a direct regression from local image features to the pose of the object. This original approach recovers a precise pose, but cannot handle significant clutter or occlusions, and the accurate pose estimation depends on the (supervised) enforcement of a one-dimensional manifold constraint (corresponding to the 1D rotation of the object in the training examples). It is not clear how that approach would extend to the estimation of the full 3D pose of an object. Other recent works such as [31] have looked further at manifold modeling for appearance-based pose estimation, but with an evaluation limited to fairly simple conditions, and the performance of such methods for detection in cluttered scenes is not obvious.During an off-line training phase, we use an optical flow algorithm between pairs of images to detect how the appearance of each training object varies between these viewpoints. The image features extracted from one of these images can then be deformed into the other, and the interpolation for intermediate viewpoints is straightforward. We thereby obtain a generative model that synthesizes the appearance of the object in any (possibly unseen) viewpoint. This procedure is related to the technique of morphing in computer graphics [32–34], with the difference that we are considering arbitrary numbers of input views, and we do not rely on established correspondences between specific landmarks of the input views. This similarly contrasts with the competing method of Savarese and Fei-Fei [35], which does use specific correspondences between nearby views. Our advantage is to handle non-textured objects with little detail. Although some global consistency in the detected deformations is enforced by the optical flow algorithm, each image feature independently stores its possible deformations. This does not limit the model to a particular class of transformations. In comparison, Savarese and Fei-Fei [35] specifically models affine transformations of object parts, assuming that objects are made of large planar parts. We also use a sparse set of training views (typically spaced about20°apart on the viewing sphere) and do not require videos or dense sequences of images to track features between frames, as opposed to Sun et al. [36].Our main contributions can be summarized in the following points.1.We present a general framework for modeling the appearance of objects and object categories, suitable to virtually any type of image features, applicable for detection and recognition without relying on hand-designed local visual descriptors, while still providing performance and efficiency on par with state-of-the-art – arguably more complex – methods.We show how to handle dense, unmatchable image features, such as coarse-scale intensity gradients. This ultimately enables the method to recognize objects without texture, and to handle cases where shading constitutes the sole source of unambiguous visual information.We provide a technique for identifying, and storing, within a multiview model of appearance, how the appearance varies between discrete training viewpoints. This ultimately allows performing continuous pose estimation of an object in a novel scene, without relying on an explicit 3D model of the object. This also readily applies to object categories, and not only to specific objects.This section presents our model of appearance with a bottom-up description. We start by turning a set of image features of a given image into a “distribution of features”, then use those representations to form our model that includes several viewpoints, and possibly several training examples for each viewpoint. We finally show how to detect and recognize those training views in a novel test image.Our approach is based on a representation of images as continuous probability distributions of image features. The motivation for representing images as distributions is twofold. First, this representation accounts for the inevitable uncertainty of the description of any single image, due to e.g. image noise, quantization errors, uncertainty during feature extraction, etc. Secondly, it also provides, as we will see in the next section, a way of modeling variability in appearance of an object or object category, e.g. given several different examples of this category. It will also give us a more abstract representation of the images that is convenient to manipulate with existing probabilistic techniques, and that generally applies to any type of image features. The approach is first applied and presented for a test image – in which we want to recognize the object of interest – while the next section will then apply it to the training examples.We start off by extracting, from a given test image, different types of features (detailed in Section 4.1), each type denoted by an indexf=1,…,F. These can be as simple as the pixels belonging to edges (which we call “edge points”), or to the value of the intensity gradients for all pixels of the image (“gradient points”). In general, each feature x is thus characterized by (1) its position in the image, notedx.pos(∈R2) and (2) some appearance attributes, notedx.app. In the case of edge points, we use, as an attribute, the local orientation of the edge (an angle inS1+=[0,π[); in the case of gradient points, we use the orientation and the magnitude of the gradient. The contents of a given test image form thus, for each type f of features, a settestf=xii, withxi∈Af, the domain of these features. For example with edge points,Aedges=R2×S1+(see Section 4.1 for details).We now show how to turn such a set of discrete image features (from a given test image) into a continuous probability distribution. We define and represent such distributions over the appearance space of image features (Af) in a non-parametric manner, through kernel density estimation (KDE). With this procedure, all image features are used as particles supporting simple kernels, the sum of which represents a continuous distribution. Formally, for each type of image features f, we use the set of featurestestfextracted from our test image to define the distribution(1)ϕtestff(x)=∑xi∈testfwt(xi)N(xi.pos;x.pos,σpos)Kf(xi.app;x.app),withx∈Af,Na Gaussian kernel for the position of the features,Kfa kernel for their appearance attributes (see Section 4.1), andwt(xi)the weight of the featurexi. Those weights are set uniformly for the features of a test image, i.e.wt(xi)=1|testf|∀xi∈testf. This representation with KDE will be reused for the training images, where the weights will then take a more complex form (Section 2.4). Practically, Eq. (1) gives us a probability density function that can be easily evaluated for any x. For example, in the case of edge points, we can evaluate the probability of observing a horizontal edge at a specific location in the image.We have represented our test image as continuous distributions of image features. We will now similarly apply that approach to the training images. Two differences are worth mentioning though.First, we may observe the object of interest under multiple viewpoints. Each training image t corresponds to a viewpointvt∈S2(a point on the viewing sphere), and gives, a set of featurestrainvtffor each type of feature f (defined similarly to the setstestfabove). Those multiple viewpoints are considered independently at this point, and they each define distributionsϕtrainvtffas in Eq. (1). Only in Section 3 will we consider multiple viewpoints together, in order to perform continuous pose estimation. As a first step though, we are only interested in recognizing (approximately at least) one of the discrete viewpoints provided as the training examples.Second, we may be provided with training images of several, different objects (object “instances”) representative of an object category. We assume that all training images are aligned and at the same scale, which can be practically done automatically as explained in Section 5. We now want our distributions of features to reflect statistics relevant to all the different training examples. This is straightforward within our formulation with a KDE: for each viewpointvt, we simply include, in the set of featurestrainvf, the features extracted from all training images corresponding to that viewpoint (Fig. 1). The resulting distributionsϕtrainvff, as defined earlier, are then representative of the occurrence of image features among all those training examples together, and they constitute our model of appearance of an object category. Consequently, the appearance of that category is thus defined implicitly by the instances provided as training examples.We now would like to detect, or recognize the learned object in the test image. The solution to this task consists in the optimal set of in-plane transformationsw∗(a translation, rotation and scaling in the image) and viewpoint (out-of-plane transformations)v∗(∈S2), which corresponds to the training viewpoint recognized in the test image. Let us mention, as a side note, that this result(v∗,w∗)presents 6 degrees of freedom (DoF), and that it can be equally described in the image space (as we do) or in the “world” space (as Euclidean coordinates for position and orientation). The latter is usually preferred in the field of robotics, and commonly called the 6-DoF pose of the object. Both representations are however equivalent and interchangeable, provided the calibration of the camera.We will first present how to measure the visual similarity between the test image and the learned object at a specific viewpoint and in-plane transformations. We will then provide an algorithm to identify the optimal set of such transformations, determining the local maxima of that similarity. At this point, we still consider the training viewpoints independently, and thus perform a “nearest-neighbor” classification of the viewpoint. This will serve as a starting pointer later, for a local optimization procedure to perform continuous pose estimation (Section 3).Let us consider a test image is represented by the distributions of featuresϕtestf, and a specific training view t represented byϕtrainvff. This training view may appear in the test image under any similarity transformations w (in-plane translation, rotation, scaling), trivially applied by a functiontransformw(x). Accounting for such transformations, we measure the similarity between the test and training views with the cross-correlation of the distributions(2)ϕtestff★ϕtrainwff(w)=∫Afϕtestffxϕtrainvfftransformw(x)dx.To efficiently obtain an approximate evaluation the integral of Eq. (2), we use Monte Carlo integration [37]. This involves drawing samplesxi(ℓ=1,…,L) from the distributionϕtestf(see Section 4.3), and computing the following sum:(3)ϕtestff★ϕtrainvff(w)≈1L∑iLϕtrainvfftransformw(xi).We can substitute the distributionϕtrainvffby its definition with KDE (as in Eq. (1)). Assuming this distribution is represented byL′particlesxj(either the original image features extracted from the training images, or a resampled set of those as will be discussed in Section 4), we have(4)ϕtestff★ϕtrainvff(w)≈1LL′∑iL∑jL′wt(xj)N(xi.pos;transformw(xj.pos),σpos)Kf(xi.app;xj.app).Now, taking into account several types f of image features (f=1,…,F), the full similarity measure between two images finally uses the product over f of the expression above, which gives(5)similaritytest,trainv(w)=∏fϕtestf★ϕtrainvf(w).We now have the core of the proposed method, with Eqs. (4) and (5): we can easily evaluate the likelihood of observing, in the test image, the object under the viewpoint v and in-plane transformations w. The solution to the problem of object localization corresponds the maxima of Eq. (5), i.e.(6)(v∗,w∗)=argmaxv,wsimilaritytest,trainv(w).Our algorithm to solve this maximization problem is detailed in Section 4.2. It efficiently computes the values of the objective function over all image locations (in-plane translations), with a method similar to a Hough voting using samples drawn from our distributions of features.We now present how to assign adequate weights to samples drawn from the trained model. The model of appearance presented in Sections 2.1 and 2.2 is merely a convenient way of representing the appearance of object categories. Since our goal is specifically to use this model to detect an object among clutter, and to determine its actual pose, we wish to give more weight its parts that are most informative to those tasks. As will be detailed in the Implementation section (Section 4), we choose to preselect samples offline from the trained model for efficiency. Therefore, the weights associated to these samples can also be computed in a pre-processing step, using the procedure described below.Weighting training data in the context of object recognition is common among many existing methods [12,38–40], where it has shown to increase performance significantly. In comparison to existing methods, our procedure is better suited to non-discriminative low-level image features, and does not rely on large amounts of training examples. It iteratively uses a validation test set to weight each feature relative to how informative it is to discriminate the appearance at a specific pose, versus other poses and against background clutter.The procedure is performed for each type f of image feature separately; we omit the superscripts f in the following paragraph to lighten the notations. We initially run the algorithm for detection and pose estimation (Section 2.3) with uniform weights on all image features of the training data. The idea is then to decrease the relative weight of those features that lead to incorrect results, from false positive detections (object identified in the background clutter) or from the recognition of incorrect poses (e.g. a car facing right identified as a car facing left). For each training view t (corresponding to a viewpointvt), we obtain some incorrect results(vn,wn)n(n=1,…,N) to be used as negative examples (typically a pose estimate off by20°or more, or an overlap of the detection bounding box less than 0.5 with the ground truth). We then update the weights of all image featuresxiof the training view t according to a three step rule:(7)wt′(xi)=1-1N∑nϕtrainvttransformwn-1(xi)wt(xi)←λwt′(xi)+(1-λ)wt(xi)wt(xi)←wt(xi)∑iwt(xi).The first of these steps evaluates the contribution of the image featurexito the negative examples (incorrect results), by simply measuring how well that feature “matches” with the training view superimposed onto the test view (according to the in-plane transformationswn). The weights are then updated (step 2, with learning rateλ=0.5, typically), and normalized as to always sum to 1 (step 3). The effect of these steps is thus to actually decrease the relative weight of the features that lead to misdetections of misclassifications of the pose. The whole procedure is then be repeated iteratively: detection is performed, again, on the same validation dataset, but with the new weights for the model, which gives different negative examples, that are used with the three step rule to update the weights. As shown through our experiments, stable weights are usually reached within the order of 4–5 iterations (Section 5.2, Fig. 9).Note that, if no validation test set is available, the weights can still be computed as described above by reusing, as validation test set, the training images themselves. When performing detection on the training images, the difficulty is then essentially to recognize the object in one viewpoint versus the other viewpoints (and not versus clutter). As a result, the weights then learned from negative results will help to differentiate each training viewpoint: higher weights are given to the image features that are very informative to a specific viewpoint (Fig. 2). This effect is similar to the one obtained in earlier work [1].Finally, let us remark that the weighting scheme proposed here could be compared to the classical “term frequency – inverse document frequency” approach used in text mining, where high weights are assigned to words (image features, in our application) specific to a class of documents to retrieve (a specific viewpoint, here), relative to their likelihood of occurrence in general (in background clutter, in our case) [41].The appearance model presented so far treats the different viewpoints provided in the training data independently, and performs a coarse pose estimation, or pose classification, by recognizing one of those discrete viewpoints. Our objective is now to provide a more accurate estimate of the pose, beyond the resolution of the training viewpoints. We first present a generative model capable of synthesizing the appearance of the learned object (or object category) at an arbitrary viewpoint, interpolating between the known views, then we show how to use it for a local optimization of initial (coarse) results.The goal of our generative model is basically to fill in the gaps between the discrete training viewpoints. Although it is sometimes possible to establish explicit correspondences between image features of nearby training views, this approach could not be relied upon in general, as it does not generalize to dense or non-discriminative image features. Therefore, we chose instead to identify dense deformations between pairs of adjacent training views, using an optical flow algorithm. Those deformations are then combined and linearly interpolated to deform the image features of the training images into any arbitrary viewpoint (Figs. 3 and 4). More precisely, we first define a functiondist(v,v′)that measures the angular distance between two viewpoints on the viewing sphere. We define the set of all pairs of neighboring training viewpointsV=(t,t′):dist(vt,vt′)<th(with a threshold ofth=20°typically). During an off-line training phase, an optical flow algorithm [42] is applied on all pairs of views(t,t′)∈V.1When building a model of an object category, the deformations are detected using pairs of views of a single object instance at a time, since the detection of optical flow requires fairly similar images to succeed.1Each pair produces a dense flow mapUVt→t′(x)that corresponds, in our case, to the local deformation (translation in the image plane) undergone at an image location x when moving from viewpointvttovt′. We can now define our generative model notedtrainv, which corresponds to the set of image features defining the appearance of the object at a novel viewpoint v, as the union of image features of nearby training views, translated appropriately using the precomputed deformations. Formally,(8)trainv=⋃vt:dist(vt,v)<thdeformvt→vtrainvt.The functiondeformvt→vadjusts the position of the image features of a training viewvtinto the novel viewpoint v. It uses a linear combination of two2The use of two precomputed deformations accounts for the two dimensions of the viewing sphere.2precomputed deformations, in order to translate each image feature adequately. We denote these two deformations by the indices of the two viewpoints between which we computed them, and call them(t,t′)and(t,t″). They are chosen fromVso that the novel viewpoint can be reached (on the viewing sphere) by a positive linear combination of them. Therefore,∃α,β∈R+:v=vt+α(vt′-vt)+β(vt″-vt). Practically, this means that the viewpointsvt,vt′andvt″cannot be collinear on the viewing sphere. In the simple case where training viewpoints spaced on a grid (as in the experiments of Section 5), we simply choosevt′andvt″respectively along the changes in azimuth and elevation. It is now straightforward to define the function that combines the two deformations:(9)deformvt→vtrainvt=xi′:xi′.pos=xi.pos+αUVt→t′(xi.pos)+βUVt→t″(xi.pos)andxi′.app=xi.app,∀xi∈trainvt.The appearance of the image features is thus left unchanged, but their position in the image is modified using a linear combination of the deformations detected with optical flow. Using a parameterization of the viewpoint with euler angles as we do in our implementation (Section 4.4), this linear interpolation of image location with respect to angles is a simplistic approximation of the underlying transformations (3D rotation and projection onto the image plane). This linear approximation however proved appropriate, since the deformations are detected between fairly close viewpoints (due to the limitations of the optical flow algorithm), and more complex interpolation schemes did not prove more effective in practice.We use the algorithm of Section 2.3 to obtain initial detections and recognitions of training poses. Those are then used as starting points to run a local optimization, using the generative model described above, in order to refine and obtain a precise pose estimate. The objective function to maximize during this optimization is still the same as described in Section 2.3 (Eq. (5)). The only difference now is that the similarity is measured between the test view and a generated view, at an arbitrary viewpoint. Since the appearance of a generated view varies smoothly across viewpoints, the value of the similarity measure (our objective function) is also guaranteed to be smooth in the neighborhood of the optimum we are seeking. However, no assumption can be made about its convexity, and its complex definition (parameterized on the 6 dimensions of the viewpoint and in-plane transformations) makes the evaluation of its gradient expensive. Fortunately, the initial estimates used as starting points can be assumed to be close approximations of the global optimum. All those conditions motivated the use of a simple hill-climbing algorithm. We iteratively optimize pairs of dimensions at a time, namely the 2 viewpoint angles, the image location, then the scale and in-plane rotation. We empirically observed that a close approximation of the global optimum can be reached in this way after only a few iterations [2].

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
We introduced a representation of 2D appearance as distributions of low-level, fine-grained image features. We used this representation to build multiview models of object categories. Those models encode the appearance of objects at a number of discrete viewpoints, and, in addition, how these viewpoints deform into one another as the viewpoint continuously varies. Those deformations between neighboring viewpoints are detected with an optical flow algorithm, and encoded as translations of individual image features with respect to viewpoint changes. We provide a way to measure the similarity between an arbitrary test image and an object model at a specific viewpoint. We use this measure of similarity to perform a number of tasks: detection and localization in cluttered images (identifying the local maxima of the similarity measure with respect to locations in the test image), discrete pose estimation (identifying the learned viewpoint with the highest similarity measure with the test image) and continuous pose estimation (identifying the maxima of the similarity measure as the viewpoint continuously varies). In contrast with common practice, we address and evaluate a number of related tasks with a single approach. This is reflected in our experimental evaluation, which includes extensive testing on a number of very different benchmark datasets, which are seldom considered together. We demonstrate performance on the “ETHZ Shape” dataset for shape matching and detection in clutter of categories well above baseline methods, on par with a number of more task-specific methods. We also obtain remarkable performance on the recognition of more complex objects, notably the cars of the “3D Object” dataset, with detection rates of92.5%and an accuracy in pose estimation of91%. For the task of continuous pose estimation, we obtain results superior to the state-of-the-art on the “rotating cars” dataset.The limitations of our appearance model lie mostly in the representation of object categories. The distribution of image features are representative of the occurrence of features among the training examples, but they do not encode the co-occurrence of these features. The resulting model can thus represent all combinations of variations present in the examples. A model learned from images of cars and giraffes would not only represent those two types of objects, but also anything looking partially like a car and partially like a giraffe (i.e. combining visual features from different training examples). This may be seen as a strength, as few examples can suffice to represent wide variations of overall appearance. However, this also means that the overall procedure will practically be most effective with training examples sharing strong visual characteristics, and not with categories defined semantically or including instances looking vastly different. This representation of appearance thus also assumes fairly rigid objects (although we still obtained good performance on shape matching of the ETHZ classes). Complex deformable objects would probably be better handled by part-based models (e.g. [17,64]). We believe that this limitation was probably masked by the relative simplicity of the objects in the available datasets. Let us note however that the proposed representation as distributions of features could serve as a building block of part-based models.The importance of shape and structure in the model leads to another limitation, in the context of object recognition in complex scenes. As opposed to, e.g. the classical “bag of visual words” approach, our model does not encode contextual clues of the scene. For example, blue color and clouds in the background of an image may be indicative of the presence of an airplane. Such information is however not encoded within our model, aimed at individual object recognition. This information could be taken into account at another, higher level, dealing for overall scene understanding.All limitations discussed above lead to potential avenues for further developments. In addition, on the task of continuous pose estimation, one could explore alternative optimization algorithms to use with our generative model. Improvements in efficiency at this level could render the model suitable for continuous pose tracking, thereby widening its range of applicability even further. The detections of the deformations between the trained viewpoints, which currently uses a standard algorithm to detect optical flow, could also be improved, be made applicable to more distant viewpoints and to other types of training data, e.g. videos of the object. Finally, one could evaluate other types of image features within the proposed approach. We demonstrated its particular applicability to low-level features, although more traditional, higher-level features could also be used, such as histogram-based descriptors [7,16] or region features [65].The research leading to these results has received funding from the European Community’s Seventh Framework Programme FP7/2007-2013 (Specific Programme Cooperation, Theme 3, Information and Communication Technologies) under Grant Agreement No. 270273, Xperience. Damien Teney is supported by a research fellowship of the Belgian National Fund for Scientific Research (FNRS).