@&#MAIN-TITLE@&#
Co-constructing intersubjectivity with artificial conversational agents: People are more likely to initiate repairs of misunderstandings with agents represented as human

@&#HIGHLIGHTS@&#
This article demonstrates a method for evaluating human-agent interaction against human–human benchmarks.An experiment assesses the effort people exert toward building common ground with a conversational agent.Believing an interlocutor is a person (vs. an agent) augments efforts to establish common ground.Interfacing with a human body (vs. a text screen) augments efforts to establish common ground.

@&#KEYPHRASES@&#
Common ground,Conversational repair,Echoborg,Human-agent interaction,Intersubjectivity,Psychological benchmarks,

@&#ABSTRACT@&#
This article explores whether people more frequently attempt to repair misunderstandings when speaking to an artificial conversational agent if it is represented as fully human. Interactants in dyadic conversations with an agent (the chat bot Cleverbot) spoke to either a text screen interface (agent's responses shown on a screen) or a human body interface (agent's responses vocalized by a human speech shadower via the echoborg method) and were either informed or not informed prior to interlocution that their interlocutor's responses would be agent-generated. Results show that an interactant is less likely to initiate repairs when an agent-interlocutor communicates via a text screen interface as well as when they explicitly know their interlocutor's words to be agent-generated. That is to say, people demonstrate the most “intersubjective effort” toward establishing common ground when they engage an agent under the same social psychological conditions as face-to-face human–human interaction (i.e., when they both encounter another human body and assume that they are speaking to an autonomously-communicating person). This article's methodology presents a novel means of benchmarking intersubjectivity and intersubjective effort in human-agent interaction.“Intersubjectivity has [ … ] to be taken for granted in order to be achieved.” –Rommetveit (1974, p. 56)

@&#INTRODUCTION@&#
Psychological research involving artificial agents designed to emulate human social capabilities (e.g., robots, androids, and conversational agents that interact using spoken language and/or nonverbal behavior) has largely focused on whether people self-report these agents to be humanlike. Arguably, however, what is more important is whether such agents elicit humanlike patterns of interaction. Cassell and Tartaro (2007) claim that “the goal of human-agent interaction [ … ] should not be a believable agent; it should be a believable interaction between a human and agent in a given context” (p. 407). Accordingly, it has been proposed that the appropriate means of benchmarking an agent is to evaluate the extent to which the agent and a human interactant can together demonstrate a quality of intersubjectivity similar to that displayed in human–human interaction (Cassell & Tartaro, 2007; Schönbrodt & Asendorpf, 2011), herein referred to as “benchmark intersubjectivity.” Intersubjectivity is a term that refers to the interactional relationship between perspectives within a dyad or larger group that becomes evident through each interactant's behavioral orientation to the other (Gillespie & Cornish, 2010; Linell, 2009; Trevarthen & Aitken, 2001). Intersubjectivity is co-constructed within social interaction (Jacoby & Ochs, 1995). When used as a criterion for evaluating human-agent interaction (HAI), emphasis is placed not on isolated characteristics of either party (e.g., how humanlike the agent appears), but rather on the specific communicative processes through which the human-agent pair's perspectives are coordinated.A key intersubjective process demonstrated by humans involves the use of spoken language to build and sustain common ground (i.e., a shared understanding of the semantics and frames of reference particular to a given interaction) via a linguistic toolkit that enables the diagnosing, signaling, and repair of misunderstandings (Clark & Brennan, 1991; Schegloff, 1992). Merely possessing this toolkit, however, is insufficient for establishing common ground; this accomplishment requires active facilitation by each party to an interaction by-way-of regular and appropriate use of this toolkit (Alterman, 2007; Clark & Schaefer, 1989). When a person facilitates common ground at a level indicative of benchmark intersubjectivity, the person can be said to be exerting “benchmark intersubjective effort.” With respect to HAI, exerting benchmark intersubjective effort toward an agent is necessary otherwise the interactant will deprive the agent of the communicative support necessary to ascend into the complex intersubjective world of humans.The current article tests the idea that absent the belief that they are engaging with an autonomously communicating person, human interactants will not exert benchmark intersubjective effort when in communication with an artificial agent, nor will they exert benchmark intersubjective effort if an agent communicates via a nonhuman interface (i.e., does not have a human body). This idea is explored via the “echoborg” method demonstrated by Corti and Gillespie (2015a). An echoborg is a hybrid entity composed of a human speech shadower who wears a concealed inner-ear audio receiver and vocalizes words they receive from a conversational agent. The technique enables social situations wherein people believe they are speaking to an autonomously communicating human (due to the fact that they engage with another human body face-to-face and in person) when in reality the words spoken by this human are entirely determined by an unseen agent. This method can elicit an approximation of benchmark intersubjective effort from interactants in a baseline condition (i.e., human body interface + no explicit knowledge of an interlocutor's words being agent-determined) that can be compared to the intersubjective effort demonstrated in conditions involving a nonhuman interface and/or explicit knowledge that an interlocutor's words are agent-generated.Intersubjectivity has been conceptualized as entailing the interactions among (minimally) three levels of perspectives: (1) direct-perspectives (each party's point-of-view), (2) meta-perspectives (what each party thinks the other party's point-of-view is), and (3) meta-meta-perspectives (what each party thinks the other party thinks their point-of-view is) (Gillespie & Cornish, 2010; Icheiser, 1943; Laing, Phillipson, & Lee, 1966). According to Gillespie and Cornish (2010), this framework can be used to understand social processes such as deception (i.e., the manipulation of meta-perspectives) as well as operationalize disagreements (i.e., misalignments between self's direct-perspectives and other's direct-perspectives) and misunderstandings (i.e., misalignments between self's meta-perspectives and other's direct-perspectives). This distinction between disagreement and misunderstanding is crucial: achieving common ground is not about parties agreeing with one another, but about parties forming accurate meta-perspectives in relation to the context of an interaction, and this is facilitated via empirically observable conversational processes that display and repair perspectives (see Clark & Brennan, 1991; Marková, 2003; Tirassa & Bosco, 2008).Consider the following vignette, in which Aaron (from London) and Bryan (from New York) have a conversation:Aaron:How did you get to work today?Bryan:I took the subway.Aaron:You took the subway?Bryan:Err, I mean I took the underground. I forgot that that's what you call it here in London.Aaron:Got it.Bryan formulates his initial response (“I took the subway”) on the assumption that Aaron's meta-perspective with regard to the semantics of the utterance will match his direct-perspective (i.e., Bryan “designs” his utterance based on expectations he holds about Aaron; see Arundale, 2010; Gillespie & Cornish, 2014). Aaron then signals to Bryan that, in fact, he does not understand the semantics of Bryan's initial response (“You took the subway?”), indicating that Aaron's meta-perspective of the phrase “I took the subway” does not align with Bryan's direct-perspective of the phrase. Bryan subsequently infers that Aaron is requesting an update to his meta-perspective and responds by clarifying the semantics of his initial response (“Err, I mean I took the underground. I forgot that that's what you call it here in London”). As evidenced by Aaron's final utterance (“Got it”), Bryan's clarification sufficiently resolves the misunderstanding. Aaron now understands what Bryan meant by the phrase “I took the subway” as there is now alignment between Aaron's meta-perspective and Bryan's direct-perspective.The intersubjective effort exerted by both Aaron and Bryan in pursuit of common ground is evidenced by the relationship between their various speech acts. Producing speech acts in support of establishing common ground is a process known as “grounding” (Clark & Brennan, 1991; Clark & Schaefer, 1987). At any fixed point in time prior to, during, and after a social interaction there exists a relationship between the various possible direct-, meta-, and meta-meta-perspectives held by each interactant. Behaviors arising from of intersubjective effort (e.g., grounding) cause these perspectives to act upon one another so as to make evident to each interactant loci of agreement/disagreement and understanding/misunderstanding, and it is through such processes that the contents of perspectives are negotiated and updated.Conversation Analysis (CA) provides a basis for evaluating the quality of intersubjectivity in dialog (Gillespie & Cornish, 2010). CA arose out of the sociological tradition of “ethnomethodology” developed by Garfinkel (1967) and seeks to interpret language usage within the micro-context experienced by parties to an interaction (i.e., “talk-in-interaction”) rather than in a context-free, idealized form (Goodwin & Heritage, 1990; Hutchby & Wooffitt, 2008). Originators of CA identified fundamental organizational elements of talk-in-interaction, including how speakers allocate turns at talk as well as manage errors and misunderstandings (Sacks, Schegloff, & Jefferson, 1974; Schegloff, Jefferson, & Sacks, 1977), and CA has since proved useful in interactionist approaches to evaluating human-computer dialog (e.g., Brennan, 1991; Frohlich, Drew, & Monk, 1994; Raudaskoski, 1990; Zdenek, 2001). The current article focuses exclusively on the repair of misunderstandings, the mechanisms of which tie most directly to the operationalization of intersubjectivity and intersubjective effort described herein.In the course of human dialog, interlocutors regularly produce utterances that are misunderstood. CA researchers refer to such utterances as “trouble sources” (Schegloff, 1992). Repair activity is a type of grounding interactants deploy in order to mutually manage the presence of trouble sources and consists of the speaker of the trouble source (“self”) and the recipient of the trouble source (“other”) structuring their turns at speech so as to produce common ground. Successful repair sequences can take one of four general turn-taking forms (Zahn, 1984): (1) self-initiated self-repair involves the speaker of a trouble source both signaling and self-correcting a trouble source; (2) other-initiated self-repair involves the speaker of a trouble source self-correcting the trouble source following it being signaled by an interlocutor; (3) self-initiated other-repair involves an interlocutor correcting a trouble source following it being signaled by the speaker of the trouble source; (4) other-initiated other-repair involves an interlocutor both signaling and correcting a trouble source following its production by another speaker. These repair formats function as “the self-righting mechanism[s] for the organization of language use in social interaction” (Schegloff et al., 1977, p. 381), and according to Sidnell (2010), play “a vital role in the maintenance of intersubjectivity” (p. 111).Nearly all repair initiations occur within a “limited space around their self-declared trouble-source,” while “virtually all repairs (i.e., solutions) occur within a very narrowly circumscribed space from their repair initiations” (Schegloff, 2000, p. 208, emphasis in original). There is a strong tendency for other-initiations of repair to occur in the turn following the utterance that contains the trouble source (the second position) and be immediately followed by a self-repair (Schegloff, 2000), creating a three-turn sequence known as “repair after next turn”: (1) trouble source (self) → (2) repair initiation (other) → (3) repair outcome (self). As the third position provides the speaker of a trouble source an opportunity to resolve a misunderstanding in the brief window of space opened by an other-initiation, repair after next turn has been described as “the last structurally provided defense of intersubjectivity in conversation” (Schegloff, 1992, p. 1295).In the terminology of Laing et al. (1966), three turns are the minimal unit required to establish mutual understanding: the first turn presents a direct-perspective, the second turn conveys a meta-perspective, and the third turn confirms or corrects the meta-perspective. Repair after next turn thus coordinates perspectives, providing an elemental three-turn stitch in the co-created fabric of intersubjectivity.Analysis of other-initiated self-repair can be further linked to intersubjectivity by considering how its mechanics involve bilateral joint attention, a prerequisite of complex intersubjectivity. When engaged in joint activities involving shared intentionality (i.e., the ability to understand joint activity not merely from multiple subjective points-of-view, but also from a “bird's eye” point-of view from where the perspectives of self and other are seen as integrated; Tomasello, Carpenter, Call, Behne, & Moll, 2005), humans can through a repertoire of behaviors (e.g., speech acts) direct the attention of other humans to aspects of their environment relevant to shared goals (e.g., the goal of establishing common ground). Kaplan and Hafner (2006) outline four skills that an actor (biological or otherwise) must possess in order to accomplish bilateral joint attention: (1) attention detection (i.e., the ability to track the attention of others), (2) attention manipulation (i.e., the ability to manipulate and influence the attention of other actors through verbal and/or nonverbal gestures), (3) social coordination (i.e., the ability to engage in coordinated interaction with others via techniques such as turn-taking and role switching), and (4) intentional understanding (i.e., the ability to understand the intentions of others and interpret and predict others' behaviors as they relate to goals).Consider once again the following vignette (“TS,” “RI,” and “R” indicate trouble source, repair initiation, and repair, respectively):Aaron:How did you get to work today?Bryan:TS →I took the subway.Aaron:RI →You took the subway?Bryan:R →Err, I mean I took the underground. I forgot that that's what you call it here in the U.K.Aaron:Got it.At work in this passage are each of the four requisite skills for bilateral joint attention outlined by Kaplan and Hafner (2006), thus the complexity of Aaron and Bryan's intersubjective relationship and the intersubjective effort exerted by both can be observed. Bryan's first-position utterance (“I took the subway”) is misunderstood by Aaron. Aaron's misunderstanding is signaled in the next turn in the form of a repair initiation (“You took the subway?”) that functions as an attempt to focus Bryan's attention on the previous utterance wherein lies the trouble source (attention manipulation). As a direct consequence of this repair initiation, Bryan becomes aware of the fact that Aaron's attention is turned backward toward a trouble source located in Bryan's first-position utterance (attention detection). Bryan infers that Aaron's intention in uttering the repair initiation is to elicit a third-position repair (intentional understanding), thus Bryan clarifies the trouble source in his next turn. The entire repair sequence occurs within a formal structure of turn-taking supported by both interlocutors (social coordination).Kaplan and Hafner's (2006) four requisite skills can be segmented into behavioral and non-behavioral varieties. Attention manipulation involves overtly producing a behavior intended to influence the perspective of an interactant (e.g., uttering an other-initiation), while social coordination encompasses synchronizing one's behavior in accordance with that of an interlocutor in a manner conducive for the communication of perspectives (e.g., turn-taking). Attention detection and intentional understanding, meanwhile, are principally cognitive skills that do not necessarily manifest in the form of observable motor or linguistic behaviors (i.e., one can understand the intentions of another without producing an associated behavior). Insofar as intersubjective effort is operationalized as a behavioral indicator of a commitment to shared understanding, evidence for it in dialog can be found in observable actions such as other-initiations of repair. Failing to manipulate the attention of an interlocutor so as to alert them to the presence of a misunderstanding when one otherwise could constitutes a lack of intersubjective effort. For instance, had Aaron for whatever reason not uttered a repair initiation despite misunderstanding Bryan's use of the word “subway,” then Bryan would have failed to recognize that his direct-perspective and Aaron's meta-perspective of the word “subway” were incongruent and the two interlocutors would thereby have failed to establish common ground.Why might an interactant fail to exert benchmark intersubjective effort when in communication with an agent when they otherwise could? Answering this question requires considering how the agent is represented in the mind of the interactant. Specifically, it requires considering the factors that influence how the interactant generates meta-perspectives of the agent's direct-perspectives and how these perspectives are interacted with (if at all). This article examines two such factors: (1) the nature of the agent's means of interfacing (i.e., its embodied means of participating in social communication), and (2) the framing of the agent's communicative agency (namely, whether or not the interactant holds the belief that they are talking to an agent as opposed to another human being).First, consider the role of interfaces in fostering intersubjectivity. The sense that an interlocutor possesses attention that can be manipulated so as to jointly manage misunderstandings provides to an interactant the impetus for intersubjective effort, and attributing attention to a potential interlocutor involves the supposition that said interlocutor has a subjective perspective of a shared social world (see Graziano, 2013). Detection of the subjective perspectives possibly held by another interlocutor involves inferring information signaled via the interlocutor's interface (e.g., its physical body), therefore the properties of an interlocutor's means of interfacing influence how an interactant perceives and orients to the interlocutor's perspectives (be they real or imagined).That an interface can exert such a powerful influence over intersubjectivity has long been of interest to psychologists and philosophers concerned with the embodied nature of perspective-taking (e.g., on this topic, the phenomenologist Husserl invoked the concept of “analogical apperception” – reflexively apperceiving other people's subjectivity based on their appearing to be similarly embodied and thereby becoming an “Other”; Husserl, 1931; also see De Preester, 2008; Hemberg, 2006). The connection between interfaces and the intersubjective relationship between two or more parties has been triangulated upon by numerous empirical research streams connected to social robotics and HAI. For example, in a neuroimaging study that involved humans interacting with a spectrum of entities ranging from extremely non-humanlike computers to humanlike androids to actual humans, Krach et al. (2008) demonstrated that “the tendency to build a model of another's mind linearly increases with its perceived human-likeness” (p. 1). Riek, Rabinowitch, Chakrabarti, and Robinson (2009), meanwhile, found that people self-report greater empathy for robots perceived to be humanlike than for non-humanlike robots. Furthermore, Saygin and Stadler (2012) showed that people are more accurate when processing and predicting the motor behavior of humanlike agents compared to non-humanlike agents, suggesting that the degree to which the motor activity of an agent “resonates” with a human observer corresponds with how humanlike the agent is perceived to be. These findings suggest that as an agent's means of interfacing becomes more humanlike, the degree to which interactants consciously and unconsciously form models of the agent's perspectives and attention increases (this is often referred to as “mentalizing,” or demonstrating “theory of mind”). This also implies that the more an interactant's awareness of an agent's perspectives is reduced as a result of the agent's particular means of interfacing, so to will be the interactant's impetus for exerting benchmark intersubjective effort.The notion that artificial agents with humanlike means of interfacing provide for more intersubjectively rich interactions has inspired the development of both embodied conversational interface agents (sometimes referred to simply as embodied agents, or intelligent virtual agents) and androids. Embodied agents are conversational agents that have been combined with anthropomorphic onscreen or immersive virtual interfaces. Many can respond to both verbal and non-verbal input, generate verbal and non-verbal output, engage in repairs of misunderstanding, and communicate about the communication they engage in (Bailenson & Yee, 2005; Cassell, 2000). Androids, meanwhile, are physical machine imitations of humans. The field of android science has used such machines to better understand principles of human psychology being that the similarities in morphology between androids and humans allow researchers to investigate whether people respond in an alike manner when interacting with human and humanlike stimuli (Ishiguro & Nishio, 2007; MacDorman & Ishiguro, 2006). Android science has shown that while humans do demand more sociality from actual humans than from androids, people expect more sociality from androids than from mechanical looking robots and lesser-looking agents (MacDorman, 2006). The echoborg was introduced to the field of android science by Corti and Gillespie (2015a) in order to leapfrog current bottlenecks concerning the imperfect appearance and motor behaviors of contemporary androids as an echoborg approximates an android that can “pass” as human in terms of physical appearance and motor behavior.An interactant's mental formulation of the potential perspectives held by an interlocutor is not solely a function of the interlocutor's means of interfacing, however. In fact, the meta-perspectives of an interlocutor's direct-perspectives held by an interactant can be manipulated simply by altering the interactant's beliefs about the interlocutor. Indeed, many experiments that assess the degree to which people engage with the real or imagined perspectives of other entities involve varying the ways in which an entity's communicative agency is framed. In HAI research this often entails either priming research participants to believe that they are engaging a fully-autonomous agent when in reality the agent is human-controlled (an approach referred to as the “Wizard of Oz” technique; Dahlbäck, Jönsson, & Ahrenberg, 1993) or priming them to believe that they are engaging a real person when they are in reality interacting with an agent. Studies have shown that people mentalize less about an entity when they believe the entity to be controlled by an artificial agent rather than an actual person (Chaminade et al., 2012; Gallagher, Jack, Roepstorff, & Frith, 2002; Kircher et al., 2009; also see Branigan, Pickering, Pearson, McLean, & Brown, 2011). Kennedy, Wilkes, Elder, and Murray (1988) found that in the context of text-based human-agent dialog, the primed belief that an agent-interlocutor was actually a real person led to an increase in interactants' use of anaphors (words that point back to earlier parts of a conversation), implying that people less often attempt to direct an interlocutor's attention backward toward prior utterances when they believe the interlocutor to be a nonhuman agent. These findings suggest that intersubjective effort can potentially be impacted by the mere belief that one is or isn't interacting with another human being.While with traditional HAI methods researchers can prime the belief that an agent is really a human, this approach is only possible when used in conjunction with a nonhuman interface (i.e., a researcher cannot convince a research participant that a robot is actually an autonomous human being, they can only prime the belief that the robot is controlled by a real person, be that true or false in reality). Although embodied agents and androids mimic human likeness in a manner that augments the complexity of intersubjectivity expected by interactants, these interfaces are not fully human, therefore they do not evoke the full spectrum of intersubjective expectations that color true human–human interaction (MacDorman, 2006). HAI research, therefore, has never to-date investigated HAI within a fully human–human social psychological frame wherein both the means of interfacing is fully human and the implied communicative agency of the agent-interlocutor is fully human. Since the echoborg method of HAI can achieve this, it presents a way to investigate the intersubjective processes that occur between an interactant and an artificial agent when the interactant both believes they are speaking to an autonomous human being and encounters a truly human interface (Corti & Gillespie, 2015a).

@&#CONCLUSIONS@&#
