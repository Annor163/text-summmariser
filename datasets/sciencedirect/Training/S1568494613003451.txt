@&#MAIN-TITLE@&#
An integrated approach to automated innovization for discovering useful design principles: Case studies from engineering

@&#HIGHLIGHTS@&#
This paper proposes a post-optimality task of analyzing multiple trade-off solutions obtained using a multi-objective optimizer for hidden solutions principles.The recently proposed “automated innovization” procedure is applied to three engineering design problems to show the usefulness of the procedure.On each of the three problems, new and innovative design principles (which were not known before) are achieved.

@&#KEYPHRASES@&#
Multi-objective optimization,Innovization,Design principles,Extrusion process,Noise barrier optimization,Friction stir welding,

@&#ABSTRACT@&#
Computational optimization methods are most often used to find a single or multiple optimal or near-optimal solutions to the underlying optimization problem describing the problem at hand. In this paper, we elevate the use of optimization to a higher level in arriving at useful problem knowledge associated with the optimal or near-optimal solutions to a problem. In the proposed innovization process, first a set of trade-off optimal or near-optimal solutions are found using an evolutionary algorithm. Thereafter, the trade-off solutions are analyzed to decipher useful relationships among problem entities automatically so as to provide a better understanding of the problem to a designer or a practitioner. We provide an integrated algorithm for the innovization process and demonstrate the usefulness of the procedure to three real-world engineering design problems. New and innovative design principles obtained in each case should clearly motivate engineers and practitioners for its further application to more complex problems and its further development as a more efficient data analysis procedure.

@&#INTRODUCTION@&#
Quest for new knowledge about problems of interest to an engineer or scientist has always been of utmost importance. However, due to constraints on time and other resources, practitioners are most interested in arriving at a single solution that will suffice the requirements for the instant. In a routine problem solving scenario such as in a design or a process operation activity, practitioners often need to solve an identical problem repeatedly but for different parameter settings. In such activities, instead of repeatedly executing similar tasks (which can be somewhat monotonous to an intelligent mind), a more wise approach would be to gather useful knowledge and problem properties that constitute a high-performing solution. Such knowledge will go a long way in providing insights about the problem and making the person an expert in solving the problem under consideration. In this paper, we suggest and discuss a computational approach for arriving at such useful knowledge thorough the use of an optimization process.The elicitation of knowledge can be different in different problems. Here, we are particularly interested in knowledge that may help a designer or a practitioner in understanding their problems better. Often such knowledge can be thumb-rules or other rules such as decision-trees or semantic nets involving a few decision variables and problem functionalities. The important constituent of our approach is that the knowledge being extracted must be true for, not any arbitrary solution set, but for high-performing solutions of the problem. High-performing solutions are the solutions that are optimal or near-optimal corresponding to one or more objectives of the problem. This is where the need for an optimization algorithm arises.When an optimization problem is formed for a single objective function, usually there is a single optimal solution that most optimization applications attempt to find. What we propose here is a multi-objective optimization study in which at least two conflicting objectives are considered. For example, cost of fabricating a product and its quality are two usual conflicting objectives of design. The advantage of considering multiple conflicting objectives is that the resulting optimization problem gives rise to a set of trade-off Pareto-optimal solutions [1–3]. Each of these solutions is optimal (and hence high-performing) with respect to certain trade-off among the objectives. Since all these solutions are optimal and high-performing, an analysis of them may reveal important properties that they share. Such properties then can be considered as knowledge that high-performing solutions possess. Often, such knowledge brings in new concepts and innovative ideas of solving the problem optimally. Due to these possibilities, the task of searching for multiple trade-off solutions and identifying properties commonly appearing to these solutions is called as an innovization process – creating innovation through optimization.There are some related studies in the data-mining and machine learning literature. However, most of these studies only provide information that can be perceived visually. For example, self-organizing maps have been used to project the multi-dimensional objective and design spaces onto a two-dimensional map, followed by hierarchical clustering to reveal clusters of similar design solutions [4]. Taboda and Coit [5] used k-means clustering on the trade-off solutions to simplify the task of analyzing them. Dendograms are used to depict strongly related decision variables in [6]. MODE or multi-objective design exploration [7] uses a combination of kriging and self-organizing maps to visualize the structure of the decision variables using the non-dominated solutions. Heatmap visualization inspired from biological micro-array analysis was proposed [8]. For many-objective problems, Walker et al. [9] proposed ‘Pareto shells’ and analyzed various methods for ordering the solutions. Oyama et al. [10] used proper orthogonal decomposition to decompose the design vector into the mean and fluctuation vectors.Other studies aim at representing knowledge in the form of ‘if-then’ type rules using association rule-mining [11] or rough-set theory [12]. Such information, though very helpful in specific cases, is not compact. A typical multi-dimensional dataset may give rise to many such rules since no clustering procedure is invoked to highlight only the most important set of rules. The use of decision trees for representing the knowledge contained in large datasets also suffers from similar drawbacks [12]. Methods based on functional analysis of variance (ANOVA) [13] are only useful for considering each feature of the dataset one at a time. Correlations between different features are difficult to identify using ANOVA. While neural networks are very effective in modeling non-linear correlations between multiple inputs, the black box nature of the obtained networks may not be attractive to a practitioner. In this study, our goal is to extract knowledge from the trade-off dataset of any given multi-objective optimization problem. More importantly, the extracted knowledge should be simple, compact and significant. The methodology should be capable of identifying inter-dependencies between different problem entities (variables and objective functions) of the dataset and representing them in the form of closed-form mathematical expressions, so that any practitioner can remember them as thumb-rules for creating a good design in future design scenarios having a similar underlying structure.It is important here to differentiate our method from data-modeling techniques such as regression, multivariate adaptive regression splines (MARS) [14], response surfaces and kriging [15] which also condense data in the form of closed-form mathematical expressions. Since the main purpose of these methods is to model the given data as closely as possible, they are not designed to look for abrupt changes in correlations. Being quite flexible, they are capable of adapting the resultant mathematical function to fit the differently correlated part of the data. An instance of this effect in MARS has been studied in [16]. On the other hand, our methodology has been tailored to weed out those parts of the input dataset that are either outliers or show an abrupt change in relationship.The remainder of this paper is organized as follows. We briefly describe the proposed innovization process in Section 2. In Section 3, we describe a clustering based optimization technique for knowledge extraction, termed as automated innovization. All computer algorithms required for the integrated innovization process are outlined in this section. Thereafter, we consider three different real-world engineering design problems and apply the proposed automated innovization process and reveal useful knowledge about each problem. In all cases, the extracted knowledge provided new concepts of design which were not known before.Designers and practitioners are often interested in solving their current problem at hand in order to meet deadlines and pre-specified targets. However, by virtue of their scientific bend of mind, they are always interested in gathering useful knowledge about their problem. The type and extent of knowledge can be different in different problems, but practitioners interested in engineering design problems would most likely be interested in knowing what design principles must a solution have in order for it be an optimal or high-performing solution. Such questions are vitally important to a designer as the answers to such questions provide deep insights among parameter interactions that would elevate a design to become optimal.In the past few years, the first author has proposed a two-step procedure for unveiling such important information about a problem. The first step involves finding a set of high-performing solutions and the second step involves analyzing the obtained solutions to reveal important design principles. We discuss each of these two steps in the following paragraphs.1Finding a set of high-performing solutions: A design task usually involves a number of design variables each of which needs to be determined in order for the design to be feasible to be used and to achieve a certain goal. The goal is often to minimize the cost of fabrication, weight of the product, operation time, amount of harmful gas etc. The feasibility of a design is often checked by investigating if the design satisfies a number of pre-defined constraints, such as maximum stress developed due to loading is smaller than or equal to the strength of the material used or natural frequency of vibration is set well above the applied forcing frequency. Clearly, achieving such a feasible and optimal solution is not possible by manual (or trial-and-error) setting of variables, rather a computer-aided optimization algorithm is called for. Because of vagaries of design variables, constraints and goal functions, it becomes important to design or customize a suitable optimization algorithm for a particular problem. However, if a single goal is considered in the optimization task, the outcome would be a single optimal solution (we refer here as a high-performing solution). In the context of discovering design principles, we would require not one, but multiple high-performing solutions. An important question then to ask is where from multiple high-performing solutions will come? One way to look at the problem is again to follow what designers usually do in practice.A designer in practice usually solves a similar problem repeatedly but with different parameter values. Let us take a typical scenario of an engineer who works in a pressure vessel design company. Today, the engineer may need to design a pressure vessel for a goal of minimum volume and for a particular internal pressure requirement for a refinery, tomorrow the same engineer may be designing another vessel for different internal pressure requirement for another petrochemical industry, and so on. By multiple solutions, we mean the optimal solution for each such scenario that the engineer is faced with every now and then in his/her work. One way to find multiple such solutions would be to treat the problem as a bi-objective optimization problem in which in addition to volume being a goal, we can include maximizing internal pressure as another conflicting goal. Theoretically, such a bi-objective consideration should find multiple trade-off solutions, each of which is an optimal solution.Analyzing solutions to reveal relationships: After a set of trade-off solutions are found, the next step is usually taken to choose a single preferred solution by using subjective considerations and multi-criterion decision making principles [2,17–22]. However, here we suggest analyzing these solutions to unveil common principles hidden in them. Such principles, being common to high-performing trade-off solution, will dictate properties that would ensure Pareto-optimality and hence will indicate valuable properties related to the problem.In the past few years, the first author has applied the above two steps of innovization to a number of engineering and other problem solving tasks [23–33]. In these studies, the first task was achieved using an evolutionary multi-objective optimization (EMO) technique and the second task was achieved by using manual regression fit of trade-off data. Certainly, such manual tasks are time consuming and are capable of bringing out relationships between two and at most three entities. An automated data mining procedure for the second task provides us with a hope of finding relationships involving multiple entities. However, the task is not trivial and far more complex than usual regression tasks for a number of different reasons:1First, we are interested in finding a mathematical relationship between variables, objective and constraint values of trade-off solutions obtained by an EMO and no information about the structure of such relationships are known a priori.Second and importantly, most EMO-obtained datasets are likely to be close to the true Pareto-optimal set and may not be on the Pareto-optimal set. Thus, the methodologies for deriving relationships from near-optimal dataset are expected to remove noisy data and must be more challenging than the deterministic regression analysis methods which use the whole data.Third, a relationship may exist parametrically to different subsets of the entire dataset. Thus, instead of one fixed relationship valid for the entire dataset, there can be multiple parametrically varying relationships that are valid to different subsets with different parameter values.Fourth, the algorithms are expected to find multiple relationships so as to have a clear picture of how all the entities are inter-related.Fifth, there are possibilities of lower and higher-level principles [34] that can be attempted to be learned from a given dataset.In the following sections, we describe how each task listed above is handled by an automated innovization algorithm, an early version of which has been suggested in the recent past by the authors [35,16].Automated innovization extends the mathematical structure of relationships obtained by manual innovization to higher dimensions using the following representation,(1)∏j=1Nϕj(x)ajbj=c.Here, ϕj's are the symbolic entities (variables, objectives, constraints or any other functions of x) which can combine in various ways to form parametric relationships common to all or most of the Pareto-optimal front. With N such ‘basis functions’ any combination can be generated by the Boolean variables aj. Additionally, these functions can have exponents bj's. The reasons for using such a mathematical structure are as follows: firstly, we have empirical evidence [23,24,26,27,29,30,32] that such a form is more common than others as far as design principles are concerned. Secondly, this form greatly simplifies the task. Thirdly, the form in Eq. (1) is scale-invariant which allows us to reduce the search space using appropriate transformations as shown in the next section. Most importantly, Eq. (1) falls into the category of ‘power laws’. There is evidence [36] that many natural, physical, biological and man made processes follow power laws, easily identified by a straight line on logarithmic plots. Here, we have simply extended power laws to higher dimensions and hope to find all relationships that fit into this structure. Other problem specific mathematical structures may also be used. However, in the absence of such information, the above form should be used due to its resemblance to power laws.The expression on the left hand side in Eq. (1) can be called a commonality principle, only if it gives the same value for all Pareto-optimal vectors x* or utmost changes only between subsets while remaining constant within them. Thus, c can be treated as a parameter, whose value is not as important as it remaining constant on the whole or subsets of the front.Now, let us consider the variables bj's. For generic problems, they can take any real value (−∞, ∞). However, since we are not concerned with the actual values of c, any set of bj's that are in proportion to the set which actually forms a commonality principle, will also exhibit the features of commonality. To obtain relationships in a standard form and to restrict the search space of b so as to remove such multi-modalities, we suggest the following transformations,(2)bj=bj{bp|p=(argmaxp|apbp|)},to have bj∈[−1, 1] ∀ j=1toN. Note that the c values also transform during this process in accordance with Eq. (1).Most multi-objective problem solving techniques are numerical algorithms and therefore the obtained trade-off fronts are never truly Pareto-optimal. This means that the c values evaluated above can never be exactly same. However, a low spread of c values definitely indicates that the evaluated expression is a valid principle. The lower the spread, the better the accuracy of that relationship. It is clear that this calls for an optimization task with the spread of c values as the objective and aj's and bj's as variables.As discussed before, c values can change parametrically over the Pareto-optimal front. Clustering of these c values is a viable method for identifying such subsets. Most clustering techniques require pre-specification of number of clusters [37,38]. It has been referred to as the “the fundamental problem of cluster validity” [39]. Although literature suggests different methods for determining the optimal number of clusters, in this paper we are able to circumvent this problem by integrating a special type of clustering called grid-based clustering[37] with the search process for design principles as will be shown later in Section 3.4.2. Grid-based clustering works by superimposing a grid on the input data points. For n-dimensional data, an n dimensional grid is required. Number of grid elements in each dimension are specified by n different parameters. Here, the c values have only one dimension. Let,d: be the number of grids (or divisions as called in this paper),m: be the number of Pareto-optimal points supplied, andPt: be the number of points in the t-th division.Sort all m c-values obtained by evaluating Eq. (1) for all m trade-off solutions in the dataset.Divide the range [cmin, cmax] into d divisions.Count the number of points in each division.Label those divisions that satisfy the following criterion as sub-clusters:(3)Clusteringcriterion:Pt≥md=Averagepointsperdivision.Label all points within any division that does not satisfy Eq. (3) as unclustered points.Merge adjacent (in the dimension of c values) sub-clusters to form clusters.Count the number of clustersCand the total number of unclustered pointsU.Note that 1≤d≤m is a requirement for correct clustering for avoiding zero-element clusters and zero number of clusters. OnceCclusters are identified, the spread of c values should be calculated in each cluster separately.The mixed variable nature of the optimization proposed above and the clustering procedure involved in the calculation of spread, limits the use of conventional optimization methods. The Boolean nature of aj's suggests the use of binary strings in genetic algorithms (GAs).The population based nature of GA also enables us to obtain multiple principles in the final population. Without niching only the best principle will survive, but by introducing the niched-tournament selection operator [40], selection tournaments can be restricted to solutions of the same kind (i.e. involving the same basis functions), thus promoting solutions with different sets of basis functions to coexist in the population. This can be implemented by comparing the binary strings of the population members involved.The c values are scaled for each population member due to the scaling of bj's. Since the objective function depends on the spread of these c values, it is important to ensure that all population members have a normalized measure of spread. In this work we use the percentage coefficient of variancecv=(σ/μ)×100%as this normalized measure. This value is calculated in all theCclusters and summed. For best clustering results, it is also necessary that clusters which are close yet disjoint (for example, due to gaps in the Pareto-optimal front) be combined. To achieve this we include the number of clusters in the objective function. By considering the parameter d as an additional variable in GA, we allow a very flexible clustering approach and at the same time alleviate the user from choosing this parameter.A binary+real GA that employs (i) the new niched-tournament selection operator for handling the constraints, (ii) one-point crossover and bit-wise mutation for the binary string of ajbits, (iii) simulated binary crossover (SBX) [41] and polynomial mutation [1] for bj's and, (iv) a discrete version of SBX and polynomial mutation for the variable d, is used to solve the following automated innovization problem for extracting multiple principles simultaneously,(4)MinimizeC+∑k=1Ccv(k)×100%,cv(k)=σcμc∀c∈k-thclusterSubjectto−1.0≤bj≤1.0∀j:aj=1,1≤d≤m,|bj|≥0.1∀j:aj=1,1≤∑jaj≤N,U=0,S=m−Um×100%≥Sreqd,UrecalculatedusingPt≥md+ϵ,aj′sareBoolean,bj′sarerealanddisaninteger.The objective function is a weighted combination of the number of clusters (a positive integer) and the sum of percentage coefficient of variation in these clusters (which lies in the range 0–100). The use of percentage values helps to bring both terms of the objective function to the same order of magnitude. This allows an approximately unbiased weighting. A multi-objective formulation is avoided in favor of using niching to obtain multiple design principles in a single optimization run.The first and second set of constraints are due to the imposed bounds. The third and fourth set of constraints are necessary to discourage trivial relationships. When all the exponents are close to zero, a case of low variance c values is obtained. This problem can be mitigated by restricting the exponents to a certain value. For the engineering examples considered in this paper and previous works [16,35,34], we have experimented with this limiting value and found that a minimum absolute exponent of 0.1 yields non-trivial relationships. Hence we have fixed this value to be 0.1 in this study. Recently however, the authors proposed an altogether different mathematical structure for the relationships [42] that completely eliminates the need to specify such a value. Another trivial case is a relationship with no basis functions, leading to a flat distribution of c values. This can be avoided by imposing a lower bound on ∑jaj. An upper boundNcan also be used to specify the maximum number of basis functions that a relationship can contain. The fifth constraintU=0, ensures that d is sufficiently high so that even low-element (1 or 2) clusters are possible. The advantage of having a finer grid is that the resulting relationships are more accurate since the variance within clusters will be small. However, too large a value will cause number of clusters to increase in general. A population member that is close to an actual design principle would converge to it so as to balance the two terms in the objective. The last constraint enables a user to set a minimum requirement (Sreqd) for the significance S of the commonality principles. For calculating S, clustering is again performed but with a stricter criterion, so that low-element clusters can be identified.In this section, we present the proposed integrated algorithm for automated innovization:1Algorithm 1 shows the basic structure of the multi-objective optimizer NSGA-II [43]. The final population matrix QNGENis directly read by Algorithm 2 to carry out the innovization procedure in an integrated way. The algorithms for nondominated-sort() and crowding-distance() can be found in [43].Algorithm 2 shows the proposed automated innovization approach in a GA framework. The output population matrix from NSGA-II is fed to this algorithm along with additional user defined parameters. For the basis functions we use the variables and objectives. The other routines are:create(): It uses the variable vectors x in QNGENand evaluates all the basis functions (N) for all the m trade-off solutions in QNGENto give a data matrix Dm×N.initialize(): Randomly initializes the GA population for automated innovization such that aij={0, 1} and bij∈[−1, 1].grid-cluster(): Perform grid-based clustering. See Algorithm 3.evaluate-obj-constr(): Evaluate the objective and all constraints in Eq. (4) except the significance constraint.niched-tournament(): Perform niched tournament selection operation.crossover(): Perform SBX crossover on bjand single point crossover on aj.mutation(): Perform polynomial mutation on bjand bitwise mutation on aj.Algorithm 3 takes the sorted c values in C and uses the clustering criterion ⌊(m/d)⌋+ϵ to calculate number of clustersCand the unclustered pointsU. It also returns the cluster matrix (CM) which stores which points belong to which clusters.•associate(): It associates the current set of Ptpoints to theC-th cluster and updates CM.Algorithm 1Elitist non-dominated sorting genetic algorithm (NSGA-II).Input: P0, POP, NGENOutput: QNGEN1:sett←0, Q0←Φ2:whilet<NGENdo3:Rt=Pt∪Qt4:F=nondominated-sort(Rt)5:Pt+1=Φ and i←16:while|Pt+1|+|Fi|≤POPdo7:crowding-distance(Fi)8:Pt+1=Pt+1∪Fi9:i←i+110:end while11:Sort(Fi,≺c)12:Pt+1=Pt+1∪Fi[1:(POP−|Pt+1|)]13:Qt+1= select-cross-mutate(Pt+1)14:t←t+115:end whileAutomated innovization.Input: QNGEN, m, N, maxgen, popsize, ϕj∀ j,N, ϵ, SreqdOutput: DPmaxgen1:Dm×N= create(QNGEN, ϕ1, ϕ2, …, ϕN)2:DP1= initialize(aij, bij∀ j=1:Nanddi) ∀ i=1:popsize3:gen←14:whilegen<maxgendo5:fori:=1topopsizedo6:bij←bij{bip|p=(argmaxp|aipbip|)}∀ j=1:N7:cik=∏j=1NDijaijbij∀ k=1:m8:C= Sort(ci1,ci2,…,cim)9:Ci,Ui, CMi= grid-cluster(C,m,di,0)10:DPi,gen= evaluate-obj-constr(DPi,gen,Ci,Ui,CMi)11:Ci,Ui, CMi= grid-cluster(C,m,di,ϵ)12:Evaluate significance constraint:Si=(m−Ui)m×100%≥Sreqd13:end for14:DPgen+1= niched-tournament(DPgen)15:DPgen+1= crossover(DPgen+1)16:DPgen+1= mutation(DPgen+1)17:gen←gen+118:end whilegrid-cluster(C,m,di,ϵ)Input: C, m, d, ϵOutput:C,U, CM1:setC←0,U←0, flag←1, CM←Φ2:fort:=1toddo3:ifPt<⌊(m/d)⌋+ϵthen4:U←U+Pt5:flag←16:else7:CM= associate(CM,C,C,Pt)8:ifflag=1 then9:C←C+110:end if11:flag←012:end if13:end forBeing an integrated approach that first solves the original design problem (using Algorithm 1) and uses its output to solve the automated innovization problem (using Algorithm 2), it would be beneficial to use the same optimization sub-routines (e.g. crossover, mutation, etc.) for both cases. For the three problems studied in this paper, the best results were obtained using GAs (as supported by the references given in each case study). Therefore the same GA sub-routines are also used for automated innovization.It is well-known that the computational complexity of Algorithm 1 (NSGA-II) is O(M·POP2) [43] where M is the number of objectives and POP is the population size. In this section we calculate the complexity of automated innovization shown in Algorithm 2 in terms of m the size of the dataset, N the number of basis functions used and d the number of divisions.Each generation of Algorithm 2 performs three major operations on all popsize population members:1Calculation of c values (Step 7): The evaluation of each row in the data matrix Dm×Nrequires N exponentiations and (2N−1) multiplications. Thus each c value requires (3N−1) basic operations. This operation altogether would require m(3N−1) basic operations. Thus the complexity is O(mN).Sorting of c values (Step 8):Algorithm 2 performs merge sort [44] on the m c-values calculated in the above operation. It has a worst case complexity of O(mlogm) [44].Grid-based clustering (Step 9): For d divisions, Algorithm 3 shows that the associate() routine can be called at most d times (when all points are clustered). This routine in turn uses at most m comparisons (when all points lie within the same cluster). Thus the complexity for grid-based clustering is O(dm). Note that the maximum possible value for d is m, which gives the worst case complexity for clustering over all generations as O(m2).Of the three operations, sorting is the most efficient and hence it does not govern the complexity. The overall worst case complexity of Algorithm 2 is therefore either O(mN) or O(m2). In general, we would expect m>N, thereby giving O(m2) as the theoretical upper limit.In the following sections, we use the above integrated innovization algorithm to solve three real-world design optimization problems. The obtained principles are evaluated by domain experts and important insights are drawn. Table 1shows the parameters used for solving the three problems.Noise pollution is a problem of concern in the majority of developed countries, leading to the formalization of legal codes, laws, or ordinances that establish certain limits on noise, particularly in urban areas. The problem of diminishing and controlling acoustic pollution can be addressed in several ways depending on the location of noise, i.e. at its source, during its propagation and at the receiver end. The placement of noise barriers between high traffic areas and residential ones is a common way of mitigating the environmental impact of noise. Among the methods for simulating open noise propagation around noise barriers, the use of the Boundary Element Method (BEM) has been established with the pioneering work of Seznec [45]. Later, in the nineties, some researchers developed the modeling of noise barriers with BEM and the evaluation of their ‘insertion loss’ [46–48]. Generalization to three-dimensional space was proposed in [49]. A further development in this active research field during the 2000s was achieved by Branco et al. [50] who use the BEM to study the effect of varying the shape of rigid acoustic barriers on the insertion loss. Recently BEM has been coupled with other methods. Hampel et al. [51] couples the BEM with the ray-tracing procedure; Tadeu et al. [52] introduces the Traction BEM (TBEM) and also couples BEM/TBEM with the method of fundamental solutions [53]. Some recent applications of traffic/highway noise barrier designs with BEM modeling are Oldham and Egan [54] and Grubesa et al. [55]. In the last lustrum, interest has been focused in obtaining optimum designs by coupling the BEM modeling with global automated optimization methods such as evolutionary computation. For example, Duhamel [56], Baulac et al. [57], etc. Other global meta-heuristic methods such as simulated annealing have also been tried [58]. Particularly, the optimum design of Y-shaped noise barriers has been considered in [59–62] by adjusting the insertion loss (IL) spectrum at different frequencies with respect to a reference curve to minimize the following fitness function,(5)Fitnessfunction(x)=∑iNFreqILi(x)−ILiR2,where x={y1, y2, y3, x2} are the barrier design variables shown in Fig. 1,ILi: insertion loss in the third octave band center frequency for the evaluated Y-barrier profile,ILiR: insertion loss reference in the third octave band center frequency,NFreq: Number of frequencies (=5 here).The reference ILRspectrum could be obtained either from improving the IL spectrum of a previous existing noise barrier by a certain percentage (as done in [59,60,62], where 15% and 30% values are used) or from the IL spectrum of another existing barrier (as done in [61], where simple straight barriers with higher effective heights than the searched design are used). Multi-objective optimization of noise barriers was introduced in [60–62], using the above fitness function, combined with the effective height of the barrier [60] and also with the barrier length [61,62]. In this paper, the approach taken in [60] has been followed, performing a multi-objective optimization which combines: (a) the minimization of difference between the IL spectrum of the searched Y-noise barrier design and a reference IL spectrum, with (b) the simultaneous minimization of the barrier's effective height Heff. The method searches for the barrier shape design which best fits ILRfor each effective height value. The five frequencies taken into account are: 63, 125, 250, 500 and 1000Hz. A schematic of the two-dimensional configuration for scalar wave propagation problems in the frequency domain is shown in Fig. 1.The required variables to construct the Y-shape noise barrier are the y-coordinates of points A, B and C, and the x-coordinate of point B, the complete search space taking a trapezoidal shape. Here H1=10m, H2=50m, the search space width = 1m and maximum effective height Heff=2.9891m.Twelve independent runs of the NSGA-II were executed. Differences with [60] include a population size of 200 individuals, a stopping criterion set to 1000 generations and a smaller variable range. Uniform crossover is used with gray coding and two different mutation rates of 1.5% (for first six runs) and 3% (for the other six runs). The non-dominated points accumulated from the final populations of the twelve runs are shown in Fig. 2along with the reference results from [60]. The shapes reconstructed using BEM on these non-dominated noise barrier designs are shown in Fig. 3.

@&#CONCLUSIONS@&#
Methods to gather useful knowledge about a problem always fascinated man. In engineering and scientific problem solving tasks, often users are engaged in finding a single solution for their problems at hand. In this paper, we have suggested a computational optimization based methodology for unveiling solution principles that are associated with most high-performing solutions of a problem. For this purpose, we have suggested a two-step procedure in which first a set of high-performing trade-off solutions are obtained using a multi-objective optimization technique and then these solutions are analyzed to reveal useful mathematical relationships among decision variables, constraint and objective functions that would guarantee a solution to be near-optimal. Although the usefulness of the proposed innovization task executed through manual means has been demonstrated amply in many engineering and scientific problem solving tasks in the past by the first author and his collaborators, in this paper, we have discussed recently proposed automated innovization procedures that use sophisticated clustering based optimization techniques to extract hidden solution principles.The main hallmark of this study is the suggestion of a combined optimization cum analysis procedure based on elitist non-dominated sorting genetic algorithm (or NSGA-II) and a clustering based niched evolutionary algorithm. The procedure is applied to three real-world engineering design problems. In all three cases, new and innovative information about the problems have been discovered. These information are simplistic and ‘thumb-rule’ like and importantly none of these information was available before. Based on these results, we suggest further and immediate application of the proposed integrated innovization procedure and its extensions (such as lower-level innovization, higher-level innovization [34] and temporal evolution [72]) to other and more complex engineering design problems.