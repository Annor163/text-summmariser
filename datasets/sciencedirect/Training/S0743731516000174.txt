@&#MAIN-TITLE@&#
Joint scheduling of MapReduce jobs with servers: Performance bounds and experiments

@&#HIGHLIGHTS@&#
We investigate a schedule problem for MapReduce-like frameworks by taking server assignment into consideration.We formulate the MapReduce server-job organizer problem (MSJO) and show that it is NP-complete.We propose a 3-approximation algorithm and a fast heuristic design to address the MSJO problem.We implement our algorithms and some state-of-the-art algorithms on Amazon EC2 with deploying schedulers in Hadoop.By comprehensive simulations and experiments, the results show that our algorithm outperforms other classical strategies.

@&#KEYPHRASES@&#
MapReduce,Scheduling,Server assignment,NP-complete,Fast heuristic,

@&#ABSTRACT@&#
MapReduce-like frameworks have achieved tremendous success for large-scale data processing in data centers. A key feature distinguishing MapReduce from previous parallel models is that it interleaves parallel and sequential computation. Past schemes, and especially their theoretical bounds, on general parallel models are therefore, unlikely to be applied to MapReduce directly. There are many recent studies on MapReduce job and task scheduling. These studies assume that the servers are assigned in advance. In current data centers, multiple MapReduce jobs of different importance levels run together. In this paper, we investigate a schedule problem for MapReduce taking server assignment into consideration as well. We formulate a MapReduce server-job organizer problem (MSJO) and show that it is NP-complete. We develop a 3-approximation algorithm and a fast heuristic design. Moreover, we further propose a novel fine-grained practical algorithm for general MapReduce-like task scheduling problem. Finally, we evaluate our algorithms through both simulations and experiments on Amazon EC2 with an implementation with Hadoop. The results confirm the superiority of our algorithms.

@&#INTRODUCTION@&#
Recently the amount of data of various applications has increased beyond the processing capability of single machines. To cope with such data, scale out parallel processing is widely accepted. MapReduce  [11], the de facto standard framework in parallel processing for big data applications, has become widely adopted. Nevertheless, MapReduce framework is also criticized for its inefficiency in performance and as “a major step backward”  [14]. This is partially because that, performance-wise, the MapReduce framework has not been deeply studied enough as compared to decades of study and fine-tune of other conventional systems. As a consequence, there are many recent studies in improving MapReduce performance.MapReduce breaks down a job into map tasks and reduce tasks. These tasks are parallelized across server clusters.11The server clusters here are meant to be general; it can either be data center servers or cloud virtual machines.Although map tasks and reduce tasks overlap partly in the real Hadoop scheduling mechanism, researchers  [18,19] generally assume that reduce tasks will not start until the accomplishment of map tasks because reduce tasks rely on intermediate data produced by the map tasks. This is a parallel–sequential structure. In current practice, multiple MapReduce jobs are scheduled simultaneously to efficiently utilize the computation resources in the data center servers. It is a non-trivial task to find a good schedule for multiple MapReduce jobs and tasks running on different servers. There are a great number of studies on general parallel processing scheduling in the past decades. Nevertheless, whether these techniques can be applied directly in the MapReduce framework is not clear; and especially, their results on theoretical bounds are unlikely to be translated.In this paper, we conduct research in this direction. There are recent studies on MapReduce scheduling  [7,8]. As an example, an algorithm is developed in  [8] for joint scheduling of processing and shuffle phases and it achieves an 8-approximation. To the best of our knowledge, previous studies commonly assume that the servers are assigned. That is, they assume that tasks in MapReduce jobs are first assigned to the servers, and their scheduling is conducted to manage the sequences of the map and reduce tasks in each job. It is not clear whether the scheduling on map and reduce tasks will be affected in a situation where the server assignment is “less good”. We illustrate this impact by a toy example in Fig. 1. There are three machines and two jobs. Job 1 has 4 map tasks and 2 reduce tasks. Job 2 has 1 map task and 1 reduce task. Assume the processing time to be 75 s for every single map task and 100 s for every single reduce task. If server assignment is not considered, it will result in Fig. 1(a), which follows the default FIFO strategy of Hadoop  [3]. However, if we jointly consider server assignment, we can achieve a schedule shown in Fig. 1(b). It is easy to see that the completion time of job 2 in Fig. 1(a) is 250 s and in Fig. 1(b) is 175 s, a 30% improvement.In this paper, we fill in this blank by jointly consider server assignments and MapReduce jobs (and the associated tasks). To systematically study this problem, we formulate a unique MapReduce server-job organizer problem (MSJO). Note that the MSJO we discuss is the general case where the jobs can have different weights. We show that MSJO is NP-complete and we develop a 3-approximation algorithm. This approximation algorithm, though polynomial, has certain complexity in solving an LP-subroutine. Therefore, we further develop a fast heuristic. We evaluate our algorithm through extensive simulations. Our algorithm can outperforms the state-of-the-art algorithms by 40% in terms of total weighted job completion time. We further implement our algorithm in Hadoop and evaluate our algorithm using experiments in Amazon EC2  [1]. The experiment results confirm the advantage of our algorithm.The rest of the paper is organized as follows. We discuss background in Section  2. We formulate the MSJO problem and analyze its complexity in Section  3. In Section  4, we present several algorithms. We evaluate our algorithms in Section  5. In Section  6, we show an implementation of our scheme in Hadoop and our evaluation in Amazon EC2. Finally, Section  7 summarizes the paper.

@&#CONCLUSIONS@&#
