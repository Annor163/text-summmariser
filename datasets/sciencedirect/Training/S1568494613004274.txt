@&#MAIN-TITLE@&#
Effects of type reduction algorithms on forecasting accuracy of IT2FLS models

@&#HIGHLIGHTS@&#
Forecasting using interval type-2 fuzzy logic system.Comprehensive evaluation of type reduction algorithms of IT2 FLSs.Identifying TR algorithms generating best forecasts.Obtaining best forecasts using IT2 FLS models.

@&#KEYPHRASES@&#
Electricity price,Neural networks,Prediction intervals,Delta,Bootstrap,

@&#ABSTRACT@&#
Type reduction (TR) is one of the key components of interval type-2 fuzzy logic systems (IT2FLSs). Minimizing the computational requirements has been one of the key design criteria for developing TR algorithms. Often researchers give more rewards to computationally less expensive TR algorithms. This paper evaluates and compares five frequently used TR algorithms based on their contribution to the forecasting performance of IT2FLS models. Algorithms are judged based on the generalization power of IT2FLS models developed using them. Synthetic and real world case studies with different levels of uncertainty are considered to examine effects of TR algorithms on forecasts’ accuracies. As per obtained results, Coupland–Jonh TR algorithm leads to models with a higher and more stable forecasting performance. However, there is no obvious and consistent relationship between the widths of the type reduced set and the TR algorithm.

@&#INTRODUCTION@&#
Interval type-2 fuzzy logic systems (IT2FLSs) have gained popularity amongst researcher due to their superior performance compared to traditional type-1 (T1) FLSs. IT2FLSs are characterized by secondary MFs that only take the value of one over their domain. Such a restriction greatly reduces the computational burden of performing inference compared with general T2 FLSs [1,2]. Applications of IT2FLSs have been already reported in many fields of science and engineering including but not limited to, energy systems [3], transportation [4], control [5–8], emotion recognition [9], finance [10], and healthcare [11,12]. A recent review of these applications can be found in [13]. Also, a review of design methods for IT2FLS using evolutionary and bio-inspired algorithms is provided in [14,15].IT2 and T1 FLSs are very similar in terms of structure. Both include fuzzifier, rulebase, inference engine, and the output processing block. While a defuzzifier forms the output processing block of T1 FLSs, IT2FLSs have two units in their output processing block: type-reducer and defuzzifier. A type reducer converts a T2 fuzzy set into a T1 fuzzy set. This type-reduced set is then defuzzified into a crisp value [16].The Karnik–Mendel (KM) algorithm is the most common TR algorithm applied in literature. It iteratively computes two left and right end points, which are later used for defuzzification. A variety of type reduction (TR) algorithms have been proposed to minimize the computational requirements of IT2FLSs caused by the original KM algorithm. As per [17], these algorithms can be divided into two categories:•enhancement to original KM algorithm,alternative TR algorithms.Algorithms in the first category improve the speed of KM algorithm by reducing the number of iterations for finding the left and right points. These are mainly related to initialization of switch points [18,19], stop conditions [20,21], and opposite direction searching [22]. Note the principle of these algorithms are all the same. They are just smart modifications of the original KM algorithm.Computational burden of the KM algorithm and its analytical difficulty due to an iterative nature have encouraged researchers to seek for alternative TR algorithms. Eleven alternative type reduction methods are comprehensively reviewed and compared in [21]. These TR algorithms have been developed from different perspectives offering close-form equations, minimizing the number of iterations, and retaining adaptiveness of IT2FLSs. Consequently, their characteristics and outputs generated by them may differ with those of KM algorithm.In existing literature about TR algorithms, the main focus is on developing methods that their computational burden is less than the original KM method. It has been argued that TR algorithms with massive computational requirements may hinder IT2FLSs from certain real world applications [23]. As virtual and more powerful computing resources become available, we believe concerns related to computational burden of TR algorithms will eventually disappear. So, it is more appropriate to examine and evaluate TR algorithms based on their performance and how they change outputs of an IT2FLS model.Fig. 1schematically displays effects of the type reducer on the final output of IT2FLS model. It is important to note the inference information (rule output calculation) fed into three hypothetical type reducers is the same. However, application of these TR algorithms leads to completely different type reduced sets. These have different spreads (related to how they treat uncertainties) and mean (the model output).In this paper, we employ IT2 Takagi–Sugeno–Kang (TSK) FLSs for forecasting and prediction purposes. Five frequently used TR algorithms are considered in the output processing block of models. Five synthetic and real world case studies are applied to examine the effects and contributions of TR algorithms. Training of IT2 TSK FLS models is performed using a heuristic optimization method. Comprehensive experiments are performed for each case study and TR algorithm before driving conclusions. Performance of TR algorithms is monitored and evaluated using error-based indices such as mean absolute percentage error (MAPE).The rest of this paper is organized as follows. Section 2 briefly introduces IT2 TSK FLSs. TR algorithms investigated in this study are described in Section 3. Simulation results and discussions are provided in Section 4. Finally, we draw conclusions in Section 5.TSK models are a systematic way to design and develop FLSs for input and output mappings [24,25]. IT2 TSK FLSs are TSK fuzzy models, where antecedent MFs are IT2 FSs or consequent parameters are intervals. In an IT2 TSK FLS with a rule base of N rules in which each rule has p antecedents, let the nth rule be denoted by Rn[26],(1)Rn:Ifx1isF˜1n,x2isF˜2n,…,andxpisF˜pn,thenYn=C0n+∑i=1pCinxiwhere n=1, …, N.F˜inis the ith IT2 fuzzy set (FS) (i=1, …, p) composed of a lower and upper bound membership function (MF),(2)μF˜in(xi)=[μ_F˜in(xi),μ¯F˜in(xi)]Cinis also an interval T1 FS, where its center and spread arecinandsin, respectively,(3)Cin=[cin−sin,cin+sin]where i=0, …, p. Given an input x=(x1, x2, …, xp), the result of the input and antecedent operations (firing strength) is an interval type-1 set,Fn=[f_n,f¯n], where,(4)f_n(x)=μ_F˜1n(x1)*μ_F˜2n(x2)*⋯*μ_F˜pn(xp)(5)f¯n(x)=μ¯F˜1n(x1)*μ¯F˜2n(x2)*⋯*μ¯F˜pn(xp)where * represents a t-norm. It is assumed that the singleton fuzzifier is used in obtaining (4) and (5).Ynin (1) is the output from the nth If-Then rule, which is a T1 FS,Yn=[y_n,y¯n].y_nandy¯nare evaluated as,(6)y_n=∑i=0pcinxi−∑i=0psin|xi|(7)y¯n=∑i=0pcinxi+∑i=0psin|xi|where x0=1. As per (6) and (7), the consequent of the nth rule is an interval set with mean∑i=0pcinxiand spread∑i=0psin|xi|.The final output of the IT2 TSK FLS model is obtained through combining the outcomes of N rules,(8)Y=[yl,yr]=∫y1∈[y_1,y¯1]⋯∫yN∈[y_N,y¯N]=∫fl∈[f_1,f¯1]⋯∫fN∈[f_N,f¯N]1(∑n=1Nfnyn)/(∑n=1Nfn)In a more compact version, (8) can be expressed as,(9)Y=[yl,yr]=∑n=1Nfny_n∑n=1Nfn,∑n=1Nfny¯n∑n=1Nfnwherey_nandy¯nare given in (6) and (7), respectively. yland yrin (8) are calculated through type reduction algorithms. Finally, the defuzzified crisp output from the IT2 TSK FLS is the mean of yland yr,(10)y=yl+yr2The Karnik–Mendel (KM) procedure [27] transfers a T2 FS into a T1 FS using the concept of center of sets.{y_n}and{y¯n}are first sorted in an ascending order. Then yland yrare calculated as below,(11)yl=∑n=1Ly_nf¯n+∑n=L+1Ny_nf_n∑n=1Lf¯n+∑n=L+1Nf_n(12)yr=∑n=1Ry¯nf_n+∑n=R+1Ny¯nf¯n∑n=1Rf_n+∑n=R+1Nf¯nwhere L and R are integer switch points in [1, N−1] and satisfy the following condition:(13)y_L≤yl≤y_L+1(14)y_R≤yr≤y_R+1Detailed discussion about calculating switch points (L and R) can be found [27,18]. The enhanced versions of the KM algorithm [18–22] are not investigated in this study, as they generate the same left and right points similar to the original KM algorithm. Therefore, the forecasting performance is the same as the original algorithm.In contrast to KM algorithm, the UB algorithm [28] provides a closed-form type reduction. Furthermore, it does not require{y_n}and{y¯n}to be sorted. It computes the left and right end points using the following equations:(15)yl=y_l+y¯l2(16)yr=y_r+y¯r2wherey_l,y¯l,y_r, andy¯rare calculated as follows:(17)y¯l=min{y_(0),y_(N)}(18)y_r=max{y¯(0),y¯(N)}(19)y_l=y¯l−∑n=1N(f¯n−f_n)∑n=1Nf¯n∑n=1Nf_n=∑n=1Nf_n(y_n−y_1)∑n=1Nf¯n(y_N−y_n)∑n=1Nf_n(y_n−y_1)+∑n=1Nf¯n(y_N−y_n)(20)y_r=y¯r−∑n=1N(f¯n−f_n)∑n=1Nf¯n∑n=1Nf_n=∑n=1Nf¯n(y¯n−y¯1)∑n=1Nf_n(y¯N−y¯n)∑n=1Nf¯n(y¯n−y¯1)+∑n=1Nf_n(y¯N−y¯n)(21)y_(0)=∑n=1Ny_nf_n∑n=1Nf_n(22)y¯(0)=∑n=1Ny¯nf_n∑n=1Nf_n(23)y_(N)=∑n=1Ny_nf¯n∑n=1Nf¯n(24)y¯(N)=∑n=1Ny¯nf¯n∑n=1Nf¯nType reduction and defuzzification are performed together in the Nie–Tan (NT) algorithm [29]. The final output of an IT2FLS is computed as follows:(25)y=∑n=1Nyn(f_n+f¯n)∑n=1N(f_n+f¯n)Again sorting {yn} is not required by the NT algorithm. As the crisp output is just obtained in one step (no iteration is required), the computational requirement is much less than the KM algorithm [29]. However, the method does not provide any indication of uncertainties encapsulated by the width of the type-reduced set.The closed-form TR and defuzzification algorithm proposed by Begian, Melek, and Mendel (BMM) is as follows [30],(26)y=α∑n=1Nf_nyn∑n=1Nf_n+β∑n=1Nf¯nyn∑n=1Nf¯nwhere α and β are two coefficients. BMM algorithm computes the crisp output value of IT2FLSs as a linear combination of two T1 fuzzy sets (upper and lower MFs). BMM algorithm does not require sorted {yn}. An extension of the BMM algorithm wheny_n≠y¯nis proposed in [31],(27)y=α∑n=1Nf_ny_n∑n=1Nf_n+β∑n=1Nf¯ny¯n∑n=1Nf¯nCoupland–John (CJ) algorithm [32] borrows ideas from the field of computational geometry. Rather than using the extension principle, Coupland and John apply and extend the geometric interpretation of the centroid to drive the defuzzified output. The algorithm constructs a polygon using firing strengths and sorted {yn}. Assuming there are N fired rules, the polygon has 2N points on its boundary, indicated by (yn, fn), n=1, …, 2N. The defuzzified output is the center of the polygon,(28)y=∑n=12N(yn+yn+1)(ynfn+1−yn+1fn)3∑n=12N(ynfn+1−yn+1fn)Note that (y2N+1, f2N+1)=(y1, f1). Again, the CJ algorithm is in closed-form, which means it is computationally less expensive than KM [17].Five synthetic and real case studies are considered in this section to examine effects of five TR algorithms. The data set for the first case study is generated using Mackey–Glass equation:(29)x˙(t)=0.2x(t−τ)1+x10(t−τ)−0.1x(t)Time series is chaotic (τ>17) and non-convergent. It has been widely used as a benchmark in prediction studies [33,34]. The parameter τ and x(0) are set to 17 and 1.2, respectively. 1000 samples are generated and four past values are used to predict x(t). The input–output vector is [x(t−4), x(t−3), x(t−2), x(t−1), x(t)].The second case study is a nonlinear plant, where its output nonlinearly depends on both its past output values and the input values [35,36]. The plant difference model is given by,(30)y(t+1)=y(t)1+y2(t)+u3(t)where the input signal is,(31)u(t)=sin2πt100y(1) is set to 0.05. 500 samples are generated and both y(t) and u(t) are used to predict y(t+1). The input–output vector is [u(t), y(t), y(t+1)].Another chaotic system is considered as the third case study in this paper [37,38],(32)y(t+1)=−1.4y2(t)+0.3y(t−1)+1Similar to case study two, 500 samples are generated from the initial set [y(0), y(1)]=[0.4, 0.4]. y(t−1) and y(t) are fed into IT2FLS models to predict y(t+1).Data set for the fourth case study is the hourly electricity load demand for January, 2011 in Victoria, Australia. The purpose is to forecast 24hrs ahead loads using historical data. Autocorrelation and partial correlation functions are applied for analysis and selection of appropriate lagged values for model development. Up to 96 lagged load demand values are considered in correlation studies, and the peak values are selected as the set of inputs for each model.The dataset for the fifth case study comes from the Woolnorth wind farm located in Tasmania, Australia. Hourly electricity generated and metred in January 2010 is here used to develop IT2FLS models and check their performance. Similar to case study four, correlation analysis is applied to select the set of lagged values as inputs of IT2FLS models. As per this, the input–output vector is [x(t−3), x(t−2), x(t−1), x(t)]. This dataset has been widely used as a bench mark case study for wind power forecasting [39,40].80% of samples are used for training. The remaining 20% are used for testing forecasting and prediction performance of IT2FLS models. A simulated annealing method [41] is applied for training of IT2 TSK FLS models through minimization of mean squared error cost functions. The initial temperature is set to 5 and it is decreased using a geometric cooling schedule, with a factor set to 0.9. All experiments are repeated five times to avoid any misleading results due to initialization of parameters or random numbers used by the optimization method.

@&#CONCLUSIONS@&#
