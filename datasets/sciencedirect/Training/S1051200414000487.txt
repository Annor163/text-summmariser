@&#MAIN-TITLE@&#
Self-adaptive sampling rate assignment and image reconstruction via combination of structured sparsity and non-local total variation priors

@&#HIGHLIGHTS@&#
We propose a novel framework of adaptive sensing and reconstruction.The blocks' compressibility is estimated from pre-sensed measurements.The sampling rate is assigned based on the estimated blocks' compressibility.Structured sparsity prior and non-local total variation prior are employed in reconstruction.

@&#KEYPHRASES@&#
Compressive sampling,Assignment of sampling rate,Compressibility,Structured sparsity,Non-local total variation,

@&#ABSTRACT@&#
Compressive sensing (CS) is an emerging approach for acquisition of sparse or compressible signals. For natural images, block compressive sensing (BCS) has been designed to reduce the size of sensing matrix and the complexity of sampling and reconstruction. On the other hand, image blocks with varying structures are too different to share the same sampling rate and sensing matrix. Motivated by this, a novel framework of adaptive acquisition and reconstruction is proposed to assign sampling rate adaptively. The framework contains three aspects. First, a small part of sampling rate is employed to pre-sense each block and a novel approach is proposed to estimate its compressibility only from pre-sensed measurements. Next, two assignment schemes are proposed to assign the other part of the sampling rate adaptively to each block based on its estimated compressibility. A higher sampling rate is assigned to incompressible blocks but a lower one to compressible ones. The sensing matrix is constructed based on the assigned sampling rates. The pre-sensed measurements and the adaptive ones are concatenated to form the final measurements. Finally, it is proposed that the reconstruction is modeled as a multi-objects optimization problem which involves the structured sparsity and the non-local total variation prior together. It is simplified into a 3-stage alternating optimization problem and is solved by an augmented Lagrangian method. Experiments on four categories of real natural images and medicine images demonstrate that the proposed framework captures local and nonlocal structures and outperforms the state-of-the-art methods.

@&#INTRODUCTION@&#
Conventional approaches to sampling a signal follow the Shannon's theorem: the sampling rate must be at least twice the maximum frequency of a signal (i.e. Nyquist rate). As is commonly known, since typical images have much redundant information, only a few of well-chosen observations suffice to reconstruct original image without much perceptual loss. Recently, a new theory of compressive sensing (CS) [1,2], built on those prominent works of Candès, Tao and Donoho [1,2] is a revolution to the traditional way for acquisition of signals and has been taken significant interested. A fundamental idea behind CS is sampling and compressing is synchronous instead of two independent processes. It means that data are directly sensed in a compressed form (i.e., at a lower sampling rate), instead of being sampled and compressed alternatively. A sparse or compressible signal x can be reconstructed with high probability from its linear projections y, named CS measurements(1)y=[y1,…,ym]T=[φ1x,…,φmx]T=ΦxwhereΦ=[φ1T,…,φmT]Tis anm×nsensing matrix and the sensing vectorφi∈Rnis a row vector of the matrix Φ which satisfies the restricted isometry property (RIP) [2]. The sampling rate is defined asSR=m/n. Compared with the conventional sampling, CS is a new method of acquiring, compressing and reconstructing signals in the field of signal processing. The former samples uniformly ambient data within local region as local features of image, while the latter can be considered as a process of information sensing, which operates on the structure information of image via the randomized projections in the form of linear combinations of the image.In a typical CS system in image processing, each measurement is the projection of an image onto an individual random vector. Computing these projections would result in a very huge and cumbersome system when the size of signal is large. Therefore classical CS is not directly suitable for those large scale applications such as high density signals and high resolution images. These application-specific limitations naturally point out that the practical sampling systems depend only on a portion of the entries of an acquired signal. In current literature [3–5], the idea of block compressive sensing (BCS) is proposed to reduce the size of the sensing matrix where the whole high resolution image is divided into many blocks and image sensing and reconstruction are conducted in a block-by-block manner. Therefore the computational complexity of sampling and reconstruction are greatly reduced.However, the assignment of sampling rate is fixed so that the sampling rate of each block is identical without consideration of the structures of the blocks. To overcome this drawback of the identical sampling rate, several adaptive assignment schemes are proposed. Stankovic [6] assigned the sampling rate of the current frame by predicting its sparsity based on the previous frames. However, this approach is difficult to be employed in single-image acquisition. Ying Yu et al. [7] proposed a saliency-based compressive sampling scheme for images, where more sensing resources are allocated to the more salient image regions. To extract saliency information embodied in a scene, a complementary optical sensor is employed to acquire a low-resolution optical image of the scene. However, this low-resolution image is discarded after saliency computation which is an obvious waste of resource. Another method of adaptive assignment of sampling rate is proposed in [8]. More sampling resources are assigned to low frequency coefficients and the process of sensing image is implemented in the wavelet transform domain. However, for general optical images, it is more difficult that sensing is implemented in transform domain than in spatial domain.One of the key issues in adaptive assignment framework is the estimation of compressibility of blocks. In current literature, to estimate compressibility of image blocks, an optical image always is required [7–9], and estimation is processed in image domain or transform domain. But before sensing there is no prior information about the compressibility of each block and it is difficult to estimate original image only with sensed measurement. Until now it is still an open problem how to estimate the compressibility from the CS measurements of image blocks. It is well known that the compressibility is related to the redundancy of images. Inspired by this, in this paper, a scheme is firstly proposed to estimate the compressibility only based on the local redundancy (here, local redundancy means the smoothness of the image block which is measured by statistics of the pre-sensed measurements). In this method, a relationship between an image block and its measurement is described by an equation. And then a two-stage compressive sensing framework is proposed and the overall architecture of the proposed self-adaptive CS scheme is shown in Fig. 1. The total sampling rate is divided into two parts: a pre-sampling rate (or fixed sampling rate) and an adaptive sampling rate. At the pre-sensing stage, a small part of the sampling rate is assigned to each block identically to obtain the pre-sensed measurements. They are not only used to estimate compressibility of each block, but also they can ensure the fundamental reconstructed quality of image blocks. At the adaptive sensing stage, the remaining sampling resource is assigned to each block according to the estimated compressibility. The final measurement is the combination of the pre-sensed measurement and the adaptively assigned measurement.The CS theory also indicates that if a signal is sparse or compressible, it can be exactly reconstructed from a small set of linear, non-adaptive measurements by solving an optimization problem. Generally, the reconstruction of x from y is an ill-posed problem. Without any prior information, there exist many different candidate signals x corresponding to the same measurement y. But there exists rich redundancy in natural images. This is a fundamental that an original high-dimensional image can be compressed without perceptual loss and the image can be reconstructed from a few CS measurements. In short, performance of CS reconstruction is mainly determined by two sub-problems, the one is how to exploit the priors in an image to reduce the feasible space, and the other one is how to carry out searching in this reduced space to find the reconstructed image. Recently, many efficient computational methods have been developed to solve the reconstruction problem. They include iterative shrinkage methods [10], Bregman iterative algorithms [11], fixed point continuation algorithms [12], iterative reweighted algorithms [13] and some hybrid algorithms [14]. More recently, sparse and total variation (TV) constraint are combined and an alternative minimization scheme is employed to accelerate the convergence [15]. However, most of them search the optimal solution under sparse and local smooth assumption. However for CS reconstruction problem, especially at relatively low sampling rate, only a local sparsity prior is insufficient to perform well. A multi-scale multi-hypothesis prediction (MS-MH-BCS) method [16] is proposed. In this method, a block is reconstructed by multiple hypothesis predictions which are made by spatially surrounding blocks and these predictions are generated at multiple scales. Then, these predictions are weighted to generate the final image. Structural sparsity is also a useful prior to model image compressibility and is employed in image restoration more recently. A reconstruction method based on structural group sparse representation (SGSR) [17] is proposed. Spatially surrounding blocks are employed to learn a dictionary which is employed to reconstruct the current block.In this paper, inspired by non-local means (NLM) [18,19] model, a new reconstruction algorithm is proposed by combining local and non-local priors, where the former is modeled by the structured sparsity of the wavelet coefficients and the latter is modeled by non-local total variation (NLTV) on the wavelet coefficients. This combination is a more general and more accurate regularity assumption than the classical one. Beside this, in the proposed adaptive sensing scheme, more resource is assigned to incompressible blocks and the reconstruction quality of compressible blocks may be reduced relative to the incompressible ones. But this combined prior can help make up for this loss of compressible blocks. The process of CS reconstruction can be modeled by a multi-objective optimization problem: fidelity, local prior on structured sparsity and non-local prior on NLTV. And it is converted into a 3-stage optimization problem. At the first stage, the projection of an image vector onto a hyper-plane is a simple and effective solution. At the second stage, the bivariate shrinkage is employed to encourage a local sparse solution with potential structure dependencies existing between the coefficients and their parents. Finally, designing of fast solvers for NLTV is much harder than for classical TV, since the first order derivative is no longer linear. Therefore the augmented Lagrangian method for TV is extended to solve the NLTV problem.In summary, the contributions of this paper are three folds. Firstly the relationship between the compressibility of an image block and the corresponding CS measurement is analyzed, based on which a novel algorithm is proposed to estimate the compressibility of image block only from the pre-sensed measurements. Secondly, a novel self-adaptively compressive sampling frame is proposed. The incompressible blocks are highlighted by assigned higher sampling rate, thus more measurements are acquired. Finally, in an attempt to take advantage of the high degree of redundancy of natural images, local prior and non-local prior knowledge are combined to define a reduced searching space. Then the augmented Lagrangian method (ALM) is applied to solve the corresponding optimization problem. Preliminary results show that the block CS strategies offer comparable performances to the available CS on higher reconstructed quality with admissible implementation cost. The rest of this paper is organized as follows. Section 2 gives a brief review of block CS. Section 3 introduces the self-adaptive sampling rate assignment scheme. Section 4 describes the procedure of reconstruction. Experimental results will be given in Section 5 and Section 6 is the conclusion.BCS has been proposed to reduce the size of the sensing matrix and the complexity of computation of sampling and reconstruction where the image sampling and reconstruction are conducted in a block-by-block manner. In this section, a brief review about BCS is given and then the motivation of this paper is described.Given an image X, it is divided into N image blocks with sizeB×B. Before sampling, the sub-blocks are concatenated into column vectorsxi∈Rn. The CS measurement vector isyi=ΦBxi, whereΦB∈Rm×n(m≪B2,n=B2) is the sensing matrix. The identicalΦBis employed in different image blocks [3]. The sensing matrix for the whole image X takes on the block-diagonal formΦ=diag{ΦB,ΦB,…,ΦB}. All the measurementsyiare concatenated together to obtain the measurement of the whole image Y(2)Y=ΦX=[y1T,y2T,…,yNT]T=[x1TΦBT,x2TΦBT,…,xNTΦBT]TEssentially, sampling operation is a process of dimensionality reduction which means that many different X correspond with the same measurement. Fortunately, if a sampling matrix satisfies the RIP condition, X can be reconstructed exactly by Y. However it is only a conclusion in theory.Definition 1Restricted isometry property(See [2].) The K-restricted isometry constant for a matrixΦ∈Rm×n, denoted byδK, is the minimal nonnegative number such that, for allx∈Rn, with‖θ‖0=K(1−δK)‖Ψθ‖2⩽‖ΦΨθ‖2⩽(1+δK)‖Ψθ‖2δKmeasures how a matrix Φ acts like an isometry when it is restricted to the K columns.In fact, three parametersδK, K (sparsity), and m (number of measurement) are related to each other. Therefore for signals with different sparsity, sensing matrices with different sampling rates should be employed to get the same reconstruction accuracy. In fact, image blocks should not share an identical sampling rate due to their different geometric structures and content.A comparison is done to demonstrate the above analysis. The two typical images (Lena, Barbara, size is 512 by 512) are respectively divided into non-overlapping blocks (block size is 32 by 32). And they are classified roughly into two groups with the same size: compressible groups and incompressible ones according to its standard deviation. To simplify the classification, all the blocks are sorted by standard deviation in ascending order. The former half is defined as compressible group, while the latter half is defined as incompressible group with plentiful detail or texture. The PSNR of compressible and incompressible image blocks at different sampling rates are shown in Fig. 2. It can be observed that there exists a significant difference of the reconstruction quality between compressible and incompressible image blocks.Comparison results demonstrate that at the same sampling rate, compressible blocks achieve a better quality than incompressible blocks. Therefore the reconstruction quality depends on the algorithm and the relationship between the compressibility and the number of measurement. It is natural that for the blocks with high compressibility, the relatively low sampling rate is enough to reconstruct the block accurately, while for the more incompressible block, a higher sampling rate should be assigned to preserve the details and obtain the same visual quality. In the next section, self-adaptive sensing framework will be introduced in more details.Compared with the classical optical imaging system, CS belongs to information sampling and each CS measurement contains the global information of image block. Thus the self-adaptive assignment of sampling resources is necessary and possible. A feasible self-adaptive compressive sampling framework should satisfy the following properties: (1) the compressibility is estimated only from partial CS measurements and the proportion of these measurements (called pre-sensed measurements) should be as few as possible. (2) The pre-sensed measurements should still be used to reconstruction and cannot be discarded. (3) Estimation approach should be simple and fast. Several self-adaptive compressive sensing schemes have been proposed to assign different sampling rates to image blocks. However, most schemes need the original image or a low resolution version in assignment. But in the real imaging system, the original image always is unknown. Therefore a two-stage self-adaptive CS framework is proposed in this section. In the proposed CS framework, the assignment is estimated only from a small amount of pre-sensed measurement. The total sampling rate SR consists of two parts. One part is the pre-sense sampling rate identical for each block and is denoted by FR. The other part is the self-adaptive sampling rate and is denoted by AR. They satisfySR=FR+AR. At the pre-sensing stage, the blocks are sensed at the fixed sampling rate FR and the sensed measurements are used to estimate the compressibility. At the self-adaptive sensing stage, the rest sampling rateN×ARis assigned according to the estimated compressibility. If a blockxiis compressible, then the adaptive sampling rateARiassigned toxisatisfiesARi>AR, otherwiseARi<AR.Generally, the compressibility of an image block largely depends on the amount of redundancy in an image. The so-called redundancy means the dimension of an image block is higher than its degree of freedom. Thus the same information can be represented with less data. In fact, there exit local and no-local redundancies and many types of metrics are employed to measure them. In this paper, to simplify the problem, only local redundancy is considered. Fundamentally, smoothness is one local characteristic imposed by redundancy. Therefore compressibility is measured indirectly by smoothness of the image in this paper.In the statistics and probability theory, standard deviation is a usual metric to measure the variability of data. For a digital signalx=(x1,x2,…,xn)T, its average is denoted byx¯and standard deviation is denoted byStd(x). A low standard deviation indicates that the samples tend to be very close to the mean or center, whereas a high standard deviation indicates that the samples are dispersed in a large range. It is reasonable that a non-smooth block with high standard deviation will contain significant detail and will possess low compressibility. Therefore, estimation of compressibility of block x can be approximately equivalent to the estimation ofStd(x). In this section, the relationship between CS measurements of the image block and its compressibility is represented via the following theorem.Approximation 1Letx∈Rnbe a compressible signal and sensing matrixΦ∈Rm×nbe a Gaussian random sampling matrix. The corresponding measurement vector can be denoted byy=Φx. Then, the averagex¯of the signal x can be roughly approximated byx¯≈Std(y)(m−1)/m, and the standard deviation of x can be approximated byStd(x)≈Std(yˆ),yˆ=Φ(x−x¯⋅ln), whereln=(1,1,…,1)Tis a vector of all ones with size n.This approximation will be justified in Appendix A. This theorem represents the fact that the standard deviation of an image block can be roughly estimated from a small amount of CS measurements.At the pre-sensing stage, the fixed part sampling rate FR is assigned to each block to obtain the pre-sensed measurements. Not only can they ensure the minimum quality of the reconstructed image but also the compressibility of the image block can be estimated from them. The adaptive part AR is assigned as follows:(3)ARi=Pi×AR×NHere,Piis a normalized assignment proportion coefficient related to the compressibility of image blocks andARiis the sampling rate assigned to blockxi. When a block is compressible, a lower sampling rate will be assigned or perhaps even no more sampling rate will be assigned. Otherwise a higher sampling rate will be assigned to preserve the detail contained in this block. It becomes one of the key issues how to compute thePiaccording to the estimated compressibility.Let Std denote the estimated standard deviation of all of the blocks in a vector formStd=[Std(x1),Std(x2),…,Std(xN)]TIt has been shown that the image reconstruction quality is largely influenced by the amounts of measurement and the compressibility ofxi. Since the compressibility is measured by the standard deviation, it is a reasonable assumption that the sampling rate is proportional to standard deviation. Under this assumption, the sampling rate can be assigned according to the distribution of the standard deviation of each block. Based on this idea, two schemes are proposed for the assignment of the sampling rate which fundamentally focus on the computation ofPiin (3).In the first scheme,Piis computed as the proportion of the standard deviation directly which can be described as follows:(4)Pi=Std(xi)∑j=1NStd(xj)+Cwhere C is a constant parameter to adjust the sampling rate difference between incompressible blocks and compressible blocks. A larger value of C implies a smaller difference in the sampling rates between them. When the total sampling rate SR is small, C should be assigned a big value. The human vision system is more sensitive to the distortion of compressible blocks than incompressible blocks. Therefore, the quality of compressible block should be guaranteed first. When the total sampling rate is big, C should be taken a small value, since the fixed sampling rate FR is large enough to reconstruct the compressible block accurately. Therefore the rest sampling rate AR can be assigned to incompressible blocks. More incompressible blocks should be assigned larger sampling rate.In the other one, let std_max denote the maximum of standard deviation of image blocks. Then the interval [0, std_max] is divided into K sub-intervals[si−1,si),i=1,…,Kands0=0,sK=std_max,sk−1<sk, as is shown in Fig. 3.For a certain image block x, if its standard deviationStd(x)belongs to interval [si−1,si), the proportion of sampling rateqishould be defined beforehand, satisfyingqi∈[0,1],qi−1⩽qi,∑i=1Kqi=1. Then thePican be computed as follows:(5)Pi=F(Std(xi))∑i=1NF(Std(xi))+C,F(x)={qiifx∈[si−1,si)qKifx>std_maxThe purpose of constant parameter C in (5) is the same as that in (4).In a word, the adaptive sampling rate is assigned according to the distribution of all the compressibility estimated from the measurementsΦxi. The above process of sampling is considered as pre-sensing.The whole adaptive compressive sensing system is shown in Fig. 1. It contains five main modules: sampling rate division, compressive sampling sensor, compressibility estimation, self-adaptive sampling rate assignment, and measurements concatenation. In the sampling rate division, the total sampling rateSR∈[0,1.0]is divided into two parts: one part is the fixed sampling rate FR which is equal and constant for all blocks. The remaining sampling rate is the self-adaptive sampling rate AR. In this paper, it is set thatFR=w×SRandAR=(1−w)×SR, where w is division parameter. When the whole sampling rate SR is big, parameter w is small, otherwise w is big to guarantee the basic quality of compressible blocks.The adaptive sensing matrix is constructed according to the assigned sampling rate. Before sampling, a full rank random sampling matrix will be constructed asΦ=[φ1T,…,φnT]T,φiT∈Rnis a vector on which an image blockxiwill be projected and n denotes the number of pixels inxi. The function of compressive sampling sensor is to acquire the measurement of the image blockxibased on a given sampling matrixΦi. In this framework, the whole sensing consists of two stages. The first stage is called as pre-sampling. All corresponding rows of sampling matrix are extracted from full sampling matrix Φ. The pre-sensing matrix can be easily obtained as the following matrixΦiF=[φ1T,…,φmT]T,m=⌈n×FR⌉. The second stage is adaptive sampling. The adaptive sampling matrix is constructed asΦiA=[φm+1T,…,φm+miT]T, wheremi=⌈n×ARi⌉. Therefore, the whole sample matrix of image blockxican be given by(6)Φi=[ΦiFΦiA]=[φ1T,…,φmT,φm+1T,…,φm+miT]TIn the module of the compressibility estimator, the average and the standard deviation of each blockxican be estimated from the pre-sensed measurementΦiFxi. The adaptive sampling rate is assigned based on (5) or (6). Finally, the pre-sensed measurement and the self-adaptive sensed measurement will be concatenated together to form the final measurement.(7)yi=[yiF,yiA]=[φ1xi,…,φmxi,φm+1xi,…,φm+mixi]It can be easily found that the pre-sensed measurement is a part of whole measurement and can be reused to reconstruct the original image. And both the pre-sampling and the self-adaptive sampling are included in compressive sampling systems, which can be easily implemented in a parallel-processing system with low computation complexity.Generally, the CS reconstruction aims to recover the x from the measurement y. Generally, it can be converted to a regularized problem since the equations in (1) is an ill-posed linear system. In this section, the original image is recovered from the measurements block-wise. This is usually achieved by minimizing a functionF(X)which includes a fidelity termFfidelity(X)=‖Y−ΦX‖22and a regularization termR(X)(8)X⁎=argminX[Ffidelity(X),R(X)]The regularization termR(X)which models some specific priors on the original image aims to select the optimal solution from the feasible space which can not only fit the measurements but also satisfy some priors.In this section, the structured sparsityGsparseand the non-local total variation (NLTV)JNLTV(X)are considered as the regularization terms which combine the local prior and the non-local prior on the image. Therefore the proposed compressive sampling reconstruction can be modeled as a multi-objective optimization problem(9)Xˆ⁎=argminXF(X)=argminX[Ffidelity(X),Gsparse(X),JNLTV(X)]Sparsity and compressibility are the most popular local priors assumption based on the dependency and similarity between adjacent pixels or regions. When an image X is assumed to be compressible with a given basis Ψ, the regularization term becomesR(X)=‖ΨX‖1. The minimization ofR(X)does not always lead to the desired sparse solution, since thel1-norm ignores the potential structural relationships (e.g. spatial, temporal or hierarchical) among the variables and does not distinguish the coefficients at different scales (e.g. when energy has been transferred from coarse to fine scale). In fact, the important variables are selected in groups rather than be independent and they are supposed to be structured into a predefined coefficient vector which is called as structured sparsity. The goal of using such a prior is to exploit the dependencies between the coefficients and their parents.In this section, the dependency of between two adjacent scale wavelet coefficients is considered as the structural prior. LetXΨ=[x1Ψ,…,xNΨ]denote the wavelet coefficient of an image X andYΨ=[y1Ψ,…,ynΨ]denote the parent coefficients of theXΨ(i.e.yiΨis the parent ofxiΨ). If the coefficientxiΨis at the coarsest level and has no parent, thenyiΨ=xiΨ. To sparsify the coefficientXΨand strengthen the dependencies betweenXΨandYΨ, these coefficients are regrouped asXF=[x1F,…,xNF],xiF=[x1,iF,y1,iF,…,xn,iF,yn,iF]T. The corresponding prior-inducing function can be defined as follows:(10)Gsparse(X)=(1−w1)∑i‖xiF‖2+w1∑i‖xiF‖1=(1−w1)∑i∑j(xi,jΨ)2+(yi,jΨ)2+w1∑i∑j(|xi,jΨ|+|yi,jΨ|)However, the sparsity of wavelet coefficients only involves the local prior. How to exploit the non-local prior still is still an open problem. Buades et al. [19] introduced a highly efficient denoising model called non-local means (NLM) by averaging pixels that can be arbitrarily far away. NLM defined a similarity measurement based on the distance between two patches. The above structured sparse model can exploit the dependency between coefficients and their parent and it can be improved by exploiting the other dependencies between any two coefficients, which can be learned from the image. Therefore in this paper, non-local total variation (NLTV) [18] is employed to CS reconstruction to enhance ability to exploiting the nonlocal redundancy. Compared with the classical local sparsity assumption, NLTV can make up for the deficiency of the structured sparsity by imposing constrain that the reconstructed image has a minimal non-local total variation.The NLTV is defined to be the l1 norm of the gradient∇NLxi(11)JNLTV(X)=∑i∈Ω‖∇NLxi‖1where∇NLxi=(∇wxi,j,j∈Ω)Tis a generalized weighted gradient at the site i for a given image X and∇wxi,j=(xi−xj)w′(i,j), and weightw′(i,j)is the similarity between two image patches centered at i and j. In fact the NLTV is the total-variation energy of the image according to the graph structure given by weightw′(i,j).To solve the above multi-objective optimization problem (10), it is converted into the following 3-step alternating optimization problem(12)P1:X¯k=argmin‖X−Xk−1‖22,s.t.Y=ΦX(13)P2:Xˆk=argminGsparse(X)+λ‖X−X¯k‖22(14)P3:X˜k=argminJNLTV(X)+μ‖X−X¯k‖22BothGsparse(X)andJNLTV(X)enforce the structured compressibility and the NLTV regularity on the reconstructed image. Due to their inherent similarity, the resulting solution at the k-th iterationXkcan be the convex combination ofXˆkandX˜k:Xk=w2Xˆk+(1−w2)X˜kwherew2∈[0,1].Optimization starts with ProblemP1. TheP1aims to search for a pointX¯kon a hyper-plane defined byY=ΦXwhich is closest to the solution at the last iterationXk−1. The closed solution toP1can be written as(15)X¯k=Xk−1+ΦT(ΦΦT)−1(Y−ΦXk−1)In fact there exist many candidate points on the hyper-plane and some priors on the image are needed to select a point as the optimal solution. Next,Gsparse(X)andJNLTV(X)are employed respectively to select the optimal solution from the candidates. The above projection operator finds a point on the hyper-plane which closest to the solution at the iterationXk−1.TheP2can be simplified to the following optimization problem under a wavelet basis(16)minx,y(x−x¯Ψk)2+(y−y¯Ψk)2+2λ((1−w1)x2+y2+w1(|x|+|y|)),x,y∈Rwherey¯Ψkis the parent wavelet coefficient ofx¯Ψk. It is easy to get the optimal solutionx⁎as(17)x⁎=[r−λ(1−w1)]+rsoft(x1,λw1)wherer=soft(x1,λw1)2+soft(y1,λw1)andsoft(x,δ)=sign(x)(|x|−δ)+. In fact this shrinkage is very similar with bivariate shrinkage [20], which can exploit the dependencies between coefficients and their parents.To convert the NLTV minimization problem to anl1-norm constraint minimization, an auxiliary matrixD=[d1,d2,…,dN]is introduced for the NL gradient of x, wherediis a column of D. ThenP3can be reformulated to the followingminX,D∑i‖di‖1+μ2‖X−X¯k‖22,s.t.di=∇NLxiInspired by augmented Lagrangian idea [21,23], the constrained optimization problem is converted into the following unconstrained optimization problem:(18)minX,DmaxλL(X,D,λ)=∑i‖di‖1+μ2‖X−X¯k‖22+λ⋅∑i(di−∇NLxi)+r2∑i‖di−∇NLxi‖22whereλis a vector Lagrangian multiplier and r is a positive constant. The augmented Lagrangian method employs an alternating iterative algorithm to solve the optimization problem (18). The iteration optimization form can be reduced to the following one:{(Xk+1,Dk+1)=argminX,dL(X,D,λk)λk+1=λk+r(dik+1−∇NLxik+1)the above optimization problem can be divided into the following two sub-problems(19){P4:Xk+1=argminXμ2‖X−X¯k‖22P4:Xk+1=−∑i(λk⋅∇NLxi+r2‖d−∇NLxi‖22)P5:Dk+1=argminD∑i‖di‖1+∑i(λk⋅diP5:Dk+1=+r2‖di−∇NLxi‖22)P4is solved by gradient decent and is equivalent to solving the following linear system:μ(xi−x¯ik)+∑jwi,j′(λi,jk−λj,ik)+r∑jwi,j′(di,jk−dj,ik)+2r(xi∑jwi,j′−∑jwj,i′⋅xj)=0where i and j are site of blocks in image andwi,j′is similarity measurement between two patches which is defined in the non-local means model [18,19].An approximated solution for above linear system can be obtained by a Gauss–Seidel algorithm [22] as followsxik+1,l+1=μx¯ik+2r∑jwi,j′xi,jl−∑jwi,j′(λi,jk−λj,ik)−r∑jwi,j′(di,jk−dj,ik)μ+2r.∑nwi,j′For the problemP5, it can be rewritten asargminD∑ir‖di‖1+12∑i(r⋅di−(r⋅∇NLxi−λik))2and can be easily solved by using a soft-thresholding formuladi,j={(1−1/|ωi,j|)ωi,j/r,if|ωi,j|>10otherwisewhereωi,j=rwi,j′(xi−xj)−λi,jk.In this paper, it is initialized asX0=ΦTYand when condition|Ek+1−Ek|<Tolis satisfied, the iteration will stop, whereEkmeasures the improvement of iteration and is defined asEk=‖Xk−Xk−1‖22/N.To conclude this section, it is summarized that the compressive sampling is based on an adaptive assignment of the sampling rate and CS reconstruction algorithm is based on the combination of the local sparsity and the NLTV prior. The adaptive sampling algorithm is shown in Table 1. The adaptive image sensing contains two stages: pre-sensing and self-adaptive sampling. The corresponding sensing matrix is constructed asΦi=[φ1T,…,φmT,φm+1T,…,φm+miT]T. The sensing matrixΦImof an image can be constructed in diagonal formΦIm=diag(Φ1,…,ΦN). The concrete process is as follows.At the stage of image reconstruction, the measurementY=[y1T,y2T,…,yNT]Tof the whole image is received. Then the measurement vector Y is divided into several segments to get the sub-measurement vectoryiof the image blockxi. Then the original image is reconstructed as in the algorithm in Table 2.In this section, the performance of the proposed CS framework (AS-SSNLTV) is tested in this section. Two innovations in the framework are as follows: (1) adaptive sensing (AS) with the assignment strategy of sampling rate; (2) combination of the structured sparsity (SS) and the non-local total variation (NLTV) priors at the stage of reconstruction. Several groups of the comparison experiments are designed to test and verify the benefits from these innovations respectively.As usually done in the literature, high resolution images are used to simulate the situation if the data are fully acquired. This serves as a reference of how the proposed CS framework would behave in a real-world situation. An image set is used for testing and includes four categories: people, animal, building and boat/plane. Each category contains seven images with size512×512as presented in Fig. 4. These are typical images fit for testing the performance of the compressive sampling and reconstruction system. Since all these images contain a wide variety of structures, which is a challenge to reconstruction from the sensed measurements.In these experiments, a whole image is divided into image blocks with size n (n=32×32). And they are classified roughly into two groups with the same size: compressible ones and incompressible ones based on their standard deviation. The full sensing matrix Φ is a random matrix with i.i.d. draws of a Gaussian distribution N (0,1). At the stage of sensing, division parameter w is taken as 0.5, i.e. a half of the whole sampling rate (SR) is used as the fixed sampling rate (i.e.FR=SR/2) and the other half is self-adaptive sampling rate (i.e.AR=SR/2). At the stage of reconstruction, parameterMax_Iter=250,Tol=0.0015,w1=0.5,w2=0.5. For the NLTV parameters, the decay parameterh=0.1and the size of search windows is13×13and the size of the comparison patch is5×5. It is experimentally found that the performance is not sensitive to the weight of NLTV. Therefore in all of the experiments the weight is updated one time per ten times of iteration, which reaches a good compromise between accuracy and efficiency.All of the experiments are run under MATLAB v7.8 (R2009a) on PCs with an Intel Pentium Dual Core CPU at 3.06 GHz and 2 GB memory. The performance is evaluated by the Peak Signal to Noise Ratio (PSNR). It is noted that all reconstruction quantity evaluations are averaged over five independent trials, since the performance of reconstruction varies significantly due to the randomness of the sampling matrix Φ.Estimation of the average and the standard deviation of an image block is a fundamental topic of self-adaptive assignment of sampling rate. For each image in dataset, average and standard are estimated for each block from its measurement. To evaluate the performance on the estimation algorithm, average and standard deviation of estimation error e (the error is defined ase=|x−x⁎|, where x is the real value,x⁎is the estimated value) are calculated over all images. Statistical indicators for each error at different sampling rates are plotted in Fig. 5together. It is shown that the estimation errors of Std are stable when sampling rate lager than 0.2 and others are in the reasonable range. This demonstrates that it is feasible that the compressibility of image blocks is estimated from the standard deviation of the measurements.In order to demonstrate the benefit from adaptive sensing (AS), the AS strategy is employed in the standard BCS with smooth projected Landweber (AS-SPL), where wavelet bases are employed in AS-SPL. The adaptive sampling scheme and reconstruction algorithm are applied to all 28 natural images at varying sampling rates from 0.1 to 0.5 with intervals of 0.1. At the same time, two assignment strategies of the sampling rate have been proposed, but in this experiment only the first strategy (as in (4) and (5)) is used in the AS-SPL due to its simplicity. The parameter C in (5) is set to zero which means that the number of CS measurements assigned to each block is proportional to standard deviation of the image blocks. A comparison of the aforementioned algorithms with varying CS sampling rates on the whole image of each category images is shown in Fig. 6respectively. It can be observed that AS-SPL performs better than DWT-SPL.Furthermore, to further evaluate influence of the adaptive assignment strategy on different kind of blocks, Lena and Barbara images are selected, since they are two typical images. One includes edges and compressible regions and the other one includes lots of textures. All the blocks of each image are classified into two groups: compressible group and incompressible group based on standard deviation. The performance is evaluated on incompressible group and whole image, which is listed in Table 3. In the table, the best performance is emphasized by bold-faced font. It can be observed that adaptive sensing makes an improvement at almost all the sampling rates. For Barbara image, in framework of SPL, the maximal gain is 0.83 dB and the average 0.37 dB. Specifically in incompressible groups, the maximal gain reaches 2.51 dB and the average 1.30 dB. For Lena image, in the framework of SPL, the maximal gain is 1.25 dB and the average one is 0.55 dB. Specifically in incompressible groups, the maximal gain reaches 2.72 dB and the average one is 1.46 dB. These comparisons highlight the role of adaptive sensing. It is natural that the improvement of Lena image is far better than Barbara. Since the average compressibility of Barbara image is lower and the distributions of the blocks' compressibility are different. For the Barbra image the compressibility distributes more average and in this condition the adaptive sensing is approximated to average sensing. On the contrary for Lena image, the difference among each patch is more obvious and higher sampling rate is assigned to the incompressible block. When the total sampling rate is rather low (e.g.SR=0.1) the advantage of the two algorithms with adaptive sensing is not very clear or performs worse than other algorithms. The reason, we argue, is in the adaptive sampling method relatively more sampling rate will be assigned to the incompressible block and less sampling rate assigned to the compressible block.On the contrary for the sampling method without bias, these limited sampling rates would be averagely assigned to all the blocks. To the compressible or smooth block it may be enough. An improved method is that parameter C in (4) can be adjusted bigger to reduce the difference among image patches and force more sampling rate to compressible block. Since the human visual system is more sensitive to the distortion of the compressible image patches. And just this factor builds a firm foundation for AS-SSNLTV algorithm to recover the information and improves the quality of image.To improve reconstruction performance furthermore, especially on compressible blocks and regular texture and edge, structural sparsity and non-local total variation priors are employed to reconstruct image from the CS measurement. To evaluate the benefit from the structure sparsity and the non-local variation, AS-SSNLTV algorithm is compared with the AS-SPL, AS-TV, MS-MH-BCS and SGSR algorithms, where the code for MS-MH-BCS and SGSR is kindly provided by the author. The comparisons of all the algorithms are demonstrated as average performance on the whole image. The average performances of all the algorithms on each category image are shown in Fig. 7. It can be observed that for all of the images, AS-SSNLTV makes a great advantage, especially when the sampling rate is rather low. Combining benefits from adaptive sensing, the AS-SSNLTV shows great advantages over other algorithms.Moreover, the Lena and the Barbara images are taken typical examples for further analysis again. Table 4lists the quantity evaluations on these two images for compressible group, incompressible group and whole image. It is obvious from the table that AS-SSNLTV makes a great improvement on the compressible group patches. As the reason mentioned in the last subsection, it is natural that the PSNR of compressible group is lower than that of DWT-SPL. Due to structural sparsity and NLTV priors in reconstruction, compressible groups can be reconstructed perfectly and the performance on whole image is greatly improved. For Lena image, AS-SSNLTV algorithm performs better than other algorithms, and especially in compressible group, it shows great advantage. For Barbara image, AS-SSNLTV algorithm performs better than other algorithms except SGSR which also employ non-local idea to reconstruct image. But in compressible group, AS-SSNLTV still performs better than SGSR. These two comparisons demonstrate our attempt to recovering compressible with structure sparsity and NLTV is successful especially in compressible patches. The evaluation results demonstrate after these two priors being introduced to reconstruction, the quality of both the compressible and the incompressible groups is improved compared with algorithm AS-SPL. For Lena image, lots of compressible patches are included and AS-SSNLTV shows great advantage to other algorithms. On the contrary for Barbara image, it includes lots of texture and incompressible patches. Although AS-SSNLTV performs better in compressible group, for whole image SGSR performs a little better.Finally, CPU times of all reconstruction algorithms are compared, which are listed in Table 5. Naturally the speed of AS-SSNLTV is slower than DWT-SPL, since the computing of NL weight is rather time-consuming. To improve performance of algorithm and save sampling resource, this additional cost and computation are worth and necessary. It is demonstrated from the comparisons, due to the adaptive assignment strategy, more sampling rate is assigned to the incompressible regions, while less sampling rate to the compressible regions. Therefore the PSNRs of the compressible regions are lower than non-adaptive assignment. But when the structured sparsity and NLTV prior are introduced during the reconstruction, the PSNR of the compressible regions are improved. But these priors cannot help restore the detail information and only more sampling rate is assigned to the incompressible regions, they can be restored well.In this paper, two assignment strategies have been proposed and they are motivated by the same idea that more sampling rate should be assigned to the incompressible block. In the first method (named as AS_1), it is assumed that the assignment percentage of sampling rate is proportional to standard deviation and can be calculated as (4). In the second one (named as AS_2), the incompressible block will also obtain more measurement, but the relationship between standard deviation and percentage of sampling rate is nonlinear which need to be pre-defined. To objectively evaluate effectiveness of the two assignment methods, two experiments are designed where the only difference between two CS frameworks is assignment stage of sampling rate and the reconstruction is based on the structured sparsity and non-local total variation prior. Table 6provides PSNR of the images recovered with different assignment stage at the same average sampling rate. It can be seen that their results are close. Here, the standard deviation interval is divided into 10 sub-intervals. For the AS_1 the recovered images provide a little better fidelity when the average sampling rate is low. On the contrary, the quality of the AS_2 is a little better. But that is not always true for the other image. How to choose the optimal percentage of sampling rate is a difficult problem, therefore the first assignment strategy is chosen.Finally, the proposed algorithm is employed in biomedical images for CS reconstruction and it is compared with AS-SPL, AS-TV and SGSR algorithms. Generally a biomedical image is required with whole-image acquisition, while in this experiment it is assumed that these images have been obtained and are considered as a kind of the acquiring data. The goal is to verify that the proposed scheme is valid for both natural images and biomedical image which are little different from natural images and the object is only a small part of image.There are seven nematodes images with size of512×512selected in this experiment. The comparisons are preceded with varying sampling rates from 0.1 to 0.5. Their performances are evaluated in the average PSNR, which are listed in Table 7. It is clear that the proposed algorithm performs best. It demonstrated that the proposed algorithm can successfully be employ varieties images.

@&#CONCLUSIONS@&#
