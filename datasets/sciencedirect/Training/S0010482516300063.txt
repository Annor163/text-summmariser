@&#MAIN-TITLE@&#
Symbolic features and classification via support vector machine for predicting death in patients with Chagas disease

@&#HIGHLIGHTS@&#
We predicted the risk of death with a good performance of classifier.We extracted features from symbolic series and time–frequency indices of HRV.Symbolic, time–frequency and clinical indices prove to be a good predictor of death.SVM was useful for accurately classifying two classes, survival and nonsurvival.Conventional autonomic indices have prognostic importance in Chagas disease.

@&#KEYPHRASES@&#
Symbolic dynamics,Ordinal pattern statistics,Heart rate variability,Support vector machine,Classification,Chagas disease,

@&#ABSTRACT@&#
This paper introduces a technique for predicting death in patients with Chagas disease using features extracted from symbolic series and time–frequency indices of heart rate variability (HRV). The study included 150 patients: 15 patients who died and 135 who did not. The HRV series were obtained from 24-h Holter monitoring. Sequences of symbols from 5-min epochs from series of RR intervals were generated using symbolic dynamics and ordinal pattern statistics. Fourteen features were extracted from symbolic series and four derived from clinical aspects of patients. For classification, the 18 features from each epoch were used as inputs in a support vector machine (SVM) with a radial basis function (RBF) kernel. The results showed that it is possible to distinguish between the two classes, patients with Chagas disease who did or did not die, with a 95% accuracy rate. Therefore, we suggest that the use of new features based on symbolic series, coupled with classic time–frequency and clinical indices, proves to be a good predictor of death in patients with Chagas disease.

@&#INTRODUCTION@&#
Sudden death, associated or not with advanced cardiomyopathy, is one of the characteristic phenomena of Chagas disease [1]. The electrophysiological mechanisms most frequently associated with sudden death in patients with Chagas disease are ventricular tachycardia and ventricular fibrillation. With the development of therapeutic strategies for preventing death by ventricular fibrillation, especially those using implantable cardioverter defibrillators (ICDs), the search for methods capable of predicting the progression to death, and particularly to sudden death, in Chagas disease [2–4] has become extremely important for proper patient selection. Therefore, a more accurate risk stratification is necessary to identify patients at high risk of sudden death. Furthermore, ICD implantation is an invasive procedure which has associated risks (inappropriate shocks, proarrhythmias, pre and postoperative complications, issues with electrodes, infection) and can cause depression, anxiety and worsen patient quality of life [5]. Many clinical features, such as age and functional class, have been assessed as possible risk factors for the development of fatal arrhythmias. However, the sensitivity and specificity of any single test for predicting these types of arrhythmias are limited [6].Heart rate variability (HRV) analysis is an important tool for detecting cardiac arrhythmias. The HRV reflects the complex interactions of the cardiovascular system. Cardiovascular diseases are often predicted by changes in HRV. Therefore, the ability to classify (or distinguish between) physiological and pathological HRV patterns can be useful for developing new diagnostic tools. The HRV analysis is performed by analysis of RR intervals series. The RR intervals are intervals between two R waves of the electrocardiogram (EKG). They correspond to the frequency of ventricular depolarization. The RR interval series represents the sympathetic-vagal balance of the autonomic nervous system (ANS) given by HRV. Thus, predicting the risk of arrhythmias from sequences of RR intervals requires extracting key features from HRV signals. However, there is still the issue of how to extract the appropriate features in order to effectively classify extremely similar classes with the lowest computational cost. Symbolic dynamics (SD) has been used in literature to analyze [7] and classify HRV [8].In addition to selecting relevant features, the performance of the pattern recognition system also depends on the choice of classifier. Many pattern recognition systems of cardiovascular signals employ artificial neural networks [9], k-nearest neighbor [10], hidden Markov models, independent component analysis and principal component analysis [9], among others. Recent studies have introduced an approach that uses support vector machines (SVMs) to classify electrocardiogram (ECG) signals [9,11]. However, results have shown that the classification error rate is still above 10% for some specific classes [11].This paper is intended to evaluate if HRV features obtained by traditional methods and via symbolic dynamics (SD) can predict death in patients with Chagas disease, and to use the SVM to classify signals.Patient recruitment was conducted at the Centro de Referncia Ambulatorial da Doena de Chagas (Outpatient Reference Center for Chagas Disease) of the Universidade Federal de Minas Gerais (Federal University of Minas Gerais), Brazil. Patients with consecutive ages between 20 and 70 years and a definite serologic status of Chagas disease were selected (≥two different positive reactions to Trypanosoma cruzi in patients at risk of infection). Those who agreed to participate and provided their informed consent underwent a standard protocol, including clinical laboratory tests, electrocardiograms and chest X-rays, echocardiograms, exercise testing and 24-h Holter monitoring. Protocol details can be found in published articles with the same cohort [12,13]. The exclusion criteria were: (1) atrial fibrillation or palpitation, artificial pacemaker rhythm or any other non-sinus rhythm; (2) no evidence of cardiovascular disease, diabetes, thyroid dysfunction, chronic obstructive pulmonary disease, kidney or liver failure, anemia or significant systemic disease; (3) alcoholism; (4) pregnancy; and (5) use of any drug with cardiovascular or metabolic effects. Patients were divided into two groups: one comprising 15 patients who died (nonsurvival) and another consisting of 135 who did not (survival).The experimental protocol was approved by the Ethics Committee of the Federal University of Minas Gerais. Twenty-four-hour Holter monitoring was performed using a three-channel portable cassette tape recorder (Dynamis, Cardios, So Paulo, Brazil). Patients were encouraged to continue with their normal daily activities during data recording, but to avoid physical exercise or using drugs that might interfere with autonomic function. The HRV analyses were carried out when at least 18h of good quality traces had been recorded and 85% or more sinus rhythm heartbeats were available.Recordings were analyzed on a Burdick/DMI Hospital Holter System (Spacelabs Burdick, Deerfield, Wisconsin, USA) using a semi-automatic technique with a sampling frequency of 200Hz. For each tape, the entire recording was carefully inspected and the QRS complexes were classified into normal beats, ectopic beats, or artifacts in order to create normal RR intervals time series. The RR intervals were extracted from each of the ECG recordings by means of an automatic QRS detection algorithm, using the Holter system [14]. For feature extraction, 5-min windows were randomly selected from the HRV series from 24-h traces.Following thorough investigation, 18 features were chosen for classifier training. These features were divided into three groups depending on the nature of the feature and the data by which it was estimated (Table 1). Four clinical features: patient age; NSVT, presence of nonsustained ventricular tachycardia during 24-h Holter monitoring or exercise testing; LVEF<0.5, left ventricular ejection fraction lower than 50% during electrocardiography; andQRS≥134, duration of the QRS complex greater or equal to 134ms during electrocardiography [13]. Nine time–frequency features: mNN, mean RR intervals; SDNN, standard deviation of RR intervals; RMSSD, square root of the mean of the sum of squares of differences between adjacent RR intervals; pNN50, percentage of RR interval differences exceeding 50ms; MSD, mean successive difference between adjacent RR intervals; TotPow, variance of RR intervals; LF, power in the low frequency range; HF, power in the high frequency range; and LF/HF, LF/HF ratio. Five symbolic features:Hsd, entropy of the sequence of symbols obtained by symbolic dynamics; nFW, number of forbidden words;Hpem1, entropy of the sequence of symbols obtained by ordinal patterns; andHpem2,Cindex, complexity index.The four features of the first group refer to the patients and remain constant during the 24h of data collection, but may vary between patients. The other two groups of features (Table 1) were obtained from the series of RR intervals for each epoch.The following section describes the methodologies employed in this study for feature extraction.Time–frequency feature extraction is based on classic indices calculated from the series of RR intervals. Time-domain indices are based on statistical methods derived from RR intervals and from the differences between them [15,16]. These parameters were calculated for 24-h traces [12]. Frequency-domain indices are based on periodic components of HRV series. Power spectral density (PSD) was estimated in high (0.15–0.4Hz) and low (0.04–0.15Hz) frequency ranges [15]. The PSD in high frequency (HF) ranges represents modulation of vagal activity, whereas the PSD in low frequency (LF) ranges represents both vagal and sympathetic activity. The ratio between high and low frequencies (HF/LF) represents the sympathovagal balance. In this study, we chose to analyze PSD in 5-min windows during the period of decreased heart rate during sleep, as previously reported [17].Features extracted using symbolic series were obtained from 5-min windows in the RR intervals time series from 24-h traces. The features were subdivided into three methods.Symbolic dynamics provides a class of features based on the transformation of a time series into a sequence of symbols that can be statistically characterized using symbol-sequence histograms and quantified using Shannon entropy.In order to analyze the dynamic behavior of HRV via SD, a partition of the first return map from tachograms must be defined. The first return map of a tachogram is formed by a cloud of points scattered along the line of identityx1(RRi+1=RRi). The dispersion of points perpendicular to this line reflects the variability of long-term signal and is quantified by SD1, defined as the standard deviation along the line x2 perpendicular to x1. Moreover, the dispersion along x1 reflects the short term variability and is quantified by the standard deviation along the line SD2 [7,16], as illustrated in Fig. 1(a).Several partitions are defined in literature [7,16]. In this study, a statistic partition was defined [16] so that the resulting symbols described variations in long-term HRV signal dynamics. This partition can be determined by:si={0,RRi≤b0,1,RRi≤b1,2,RRi≤b2,⋮⋮q−1,RRi≤bq−1,where the parameters bt,t=0,…,q−1are defined so that the resulting symbols are equiprobable [16]. The parameter q defines the number of partitions, i.e., the size of the alphabet. Fig. 1(b) illustrates this partition.Then the partition of the first return maps is defined, each point on the map is converted into its corresponding symbol, creating a series of discrete values. This series is used to generate sequences of symbols siof finite size n by means of a non-overlapping sliding windows algorithm.Therefore, a value for m is defined, it is possible to estimate the probability density function (PDF) by creating a relative frequency histogram of each of the sequences. To do so, the qmpossible sequences are converted into numbers in base q and arranged according to their natural order.The values of q=3 and m=6 were chosen in order to achieve well-defined statistics [16]. Therefore, we used 5-min epochs that represented Tpvalues with approximately 500 points, in which Tprepresents the total number of points in the first return map and was limited to the values of q and m, so that qmwas approximately 1% of Tp[16]. For each of the 150 series, size Tpwindows were separated using a sliding windows algorithm. All other features listed in Table 1 were also extracted from these epochs.Shannon entropy was calculated based on the symbol-sequence histograms and is defined as:(1)Hds=−∑i=0qm−1p(si)log2(p(si)),wherep(si)is the frequency of occurrence of size m words (or sequences) related to the value i in base q; and H assumes a null value when the frequency of occurrence of a word is equal to 1, and has a maximum value,log(1/qm), when all words are equiprobable.Fig. 2outlines the stages of the feature extraction process via SD.The forbidden words or forbidden sequences were subsequently selected. Forbidden words are words that did not occur (or were not observed) in the data record used. However, as the system is corrupted by noise, forbidden words are defined as those with a low frequency of occurrence (probability of occurrence lower than 0.1%) [7]. Both the entropy and the number of forbidden words (nFW) quantify the complexity of the signal, i.e., characterize its dynamic behavior.The relation between small segments of size q of a HRV time series can also be represented by ordinal pattern statistics (OPS), a tool proposed by Parlitz et al. [8] to encoding time series. Ordinal patterns have many applications, such as in analysis of the complexity of time series and in classification problems [8].Given a time seriesx1,x2,…,xk, the first step of the OPS approach consists in obtaining non-overlapping segments of size q. The samples of each segment are ordered according to their amplitudes, generating a vector with the indices of the sorted samples. The ordinal patterns are calculated from this vector as detailed in [8].Parlitz et al. [8] also work with subsequences of samplesxk,xk+τ,xk+2τ,…,xk+(q−1)τthat are separated in time by a delay τ, which is a multiple of the sampling time Ts. For q different words, there areq!possible ordinal patterns, also known as permutations [18]. The probabilities of occurrence of specific patterns for a given delay τ and length q are used to characterize RR intervals, i.e., the number of patterns π in the series represents a measure of the complexity of the signal. Therefore, Shannon entropy is calculated from the frequencies of occurrence of the symbols (patterns) generated by OPS. This entropy is also called permutation entropy and is defined by the following equation [18]:(2)Hperm=∑π=1k!p(π)log2p(π).For this study, we choose the second approach and generate two subsequences with empirical values k=4,τ=3and k=5,τ=4to get ordinal patterns for the 150 RR intervals series, and calculated their respective entropies, Hperm1 and Hperm2.The complexity index Cindexwas determined by encoding the series of RR intervals into symbols. The successive differences between adjacent RR intervals are used to generate a sequence of symbols ui, defined by the partition, which symbolizes the “acceleration” and “deceleration” of instantaneous heart rate [19]:ui={1,RRi+1−RRi≤0,0,RRi+1−RRi≥0.In order to measure the degree of complexity of the series based on the sequence of symbols, the complexity index is calculated according to the metric proposed in [20]. This index measures the amount of different patterns in the finite sequence of symbols and therefore characterizes the level of order or disorder of the series.The complete list of features is provided in Table 1.Classifiers were trained from data divided into 5-min epochs, which are supposedly stationary [15]. For feature extraction, we selected randomly 30 windows from each epoch of 5-min of each patient. All set of features of all patients make up the matrix of features of size:nump⁎numw×numfwhere numpis the number of patients, numwis the number of windows and numfis the number of features. The data of features matrix are normalized to be between 0 and 1.Each epoch has its own set of 18 features whose indices are displayed in Table 1, and each class (survival or nonsurvival) is assigned a class label by a cardiologist. Following training, the classifier automatically determines the corresponding class label for the set of 18 features associated with each epoch (Fig. 3). This procedure is based on an SVM with a Gaussian kernel (radial basis function, RBF), defined by:(3)f(x)=sign(∑i=1ℓαiexp{|x−xi|2σ2}),wherexiis the ith training vector,αi≥0are the Lagrangian multipliers and σ is a parameter to be found [21].The 18 features, extracted from the 5-min epochs from the 150 series (135 from class survival and 15 from class nonsurvival), were used as classifier inputs. A data balancing technique was subsequently used, whose details are as follows. Training and test groups were randomly chosen, with 20% of the samples being used for training and the remaining 80% for the test. Once a 5-min window is selected for training, this window is not replaced for selecting the testing data. Cross-validation was performed for choosing the C and σ SVM parameters. The experiment was repeated 10 times and yielded the mean accuracy rate, mean sensitivity, mean specificity and the receiver operating characteristic (ROC) curve. Details on SVM classification methods are described in others works [22,21].This method was described in detail in our previous work [21]. In brief, SVM is a model classifier, based on machine learning, which generates a nonlinear separation surface between two classes defined by instance-label pairs (xi, yi),i=1,…,ℓ, where ℓ is the number of epochs used for training. Vectorsxi∈Rnare composed of n features (Table 1) andyi∈{1,−1}is the class label (survival or nonsurvival) forxi. Input vectors can be represented in an N-dimensional space (N>n), using a vector function in which classification can be performed using a hyperplane. The support vectors defining this boundary surface. Therefore, the SVM performs a nonlinear transformation that can be defined by the functional:(4)Φ(w,ϵ)=12wTw+C∑i=1ℓϵi,in whichC>0is the regularization parameter andϵi≥0are the slack variables. The functional should be minimized with regard to the weight vector w.The main classifier parameters were chosen by cross-validation using a logarithmic search space whose values areC∈{100,101,102,103}andσ∈{10−2,10−1,100,101}). The classifier was trained with the best pair (C,σ) founded.The classification problem in Chagas disease is unbalance due to the distribution of the collected data. As there are 135 patients who survived and 15 who died, the number of epochs for one class is smaller than for the other. Such training with an unbalanced data set may generate biased classifiers. The chosen approach to solve this problem was to oversample the minority class, thus ensuring that each class (patients with Chagas disease who did or did not die) has the same number of occurrences in the training data. This was done for all training data sets used in this study. The chosen approach was random sampling by the Synthetic Minority Oversampling Technique (SMOTE). In SMOTE method, proposed by [23], oversampling of minority class is based on the generation of synthetic data by means of interpolation. For each positive examplexi, new artificial examples are created between the line segments that bindxiits k-nearest neighbors. The k-nearest neighbors are chosen randomly. We choose k=5. We compute the difference between the vector of features and the k-nearest neighbors of feature vectors. Then we multiply this vector of differences by a random number between 0 and 1 and add this to the original vector of features. The resulting vector is included into the matrix of features in order to increase its size. More technical details can be found in [23].Classifier performance was assessed by means of the classic indices: accuracy rate, sensitivity, specificity and area under the ROC curve.Classification via SVM was considered correct when it coincided with the clinical progression of the disease, as documented by the cardiologist. Therefore, for a given patient and a given RR series with M epochs, the percentage accuracy rate is defined by(5)Acc=naM×100%,in which naindicates the number of correctly classified epochs.Accis an estimate of the overall accuracy rate of the classifier. The mean and standard deviation were calculated for each sample.Sensitivity (Sen) is the ratio between the number of true-positive decisions and the actual number of positive cases. Specificity (Spe) is the ratio between the number of true-negative decisions and the actual number of negative cases.The ROC curve (receiver operating characteristic) is an essential instrument for evaluating the diagnostic test. The ROC curve is a two-dimensional representation of the performance measure of a binary classifier for different thresholds of the classifier output. The ROC curve provides the rate of true-positives as a function of the rate of false-positives (sensitivity versus 1 - specificity). Each point on the curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. The area under the ROC curve (AUC) is a measure of how well a parameter can distinguish between two classes diagnosis (in this case, survival and nonsurvival). The area under the ROC curve is the probability that a classifier will assign a higher classification to a randomly chosen positive example as opposed to a randomly chosen negative example. By convention, the values of the AUC are always≥0.5. Values equal to 1 represent a perfect separation between classes.A set of features used in this study is not standard in HRV series analysis. These features are associated with the complexity of RR intervals time series and are represented by series of symbols. In order to test this influence on classifier performance, a specific test was conducted in which these features were excluded to train a different classifier. Classifier performance was then compared with the performance achieved with the whole set of features (18 features). Then, three cases were studied: (i) clinical features were excluded (T(1)); (ii) symbolic features were excluded (T(2)) and (iii) time–frequency features were excluded (T(3)). In order to evaluate the impact of these changes, a non-parametric difference test (Wilcoxon test) was used to compare accuracy rates and other indices (classifier performance) before and after feature removal.

@&#CONCLUSIONS@&#
