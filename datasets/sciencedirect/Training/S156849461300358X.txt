@&#MAIN-TITLE@&#
A hybrid intelligent model of analyzing clinical breast cancer data using clustering techniques with feature selection

@&#HIGHLIGHTS@&#
Our hybrid intelligent model considers the use of filter- and wrapper-based feature selection methods.Three qualitative principles are highlighted.The usefulness of our model is demonstrated using relative cluster validities.Better use a subset of salient features for analyzing clinical diagnoses in performing clustering.

@&#KEYPHRASES@&#
Breast cancer diagnoses,Feature selection,Cluster analysis,Filter model,Wrapper model,

@&#ABSTRACT@&#
Models based on data mining and machine learning techniques have been developed to detect the disease early or assist in clinical breast cancer diagnoses. Feature selection is commonly applied to improve the performance of models. There are numerous studies on feature selection in the literature, and most of the studies focus on feature selection in supervised learning. When class labels are absent, feature selection methods in unsupervised learning are required. However, there are few studies on these methods in the literature. Our paper aims to present a hybrid intelligence model that uses the cluster analysis techniques with feature selection for analyzing clinical breast cancer diagnoses. Our model provides an option of selecting a subset of salient features for performing clustering and comprehensively considers the use of most existing models that use all the features to perform clustering. In particular, we study the methods by selecting salient features to identify clusters using a comparison of coincident quantitative measurements. When applied to benchmark breast cancer datasets, experimental results indicate that our method outperforms several benchmark filter- and wrapper-based methods in selecting features used to discover natural clusters, maximizing the between-cluster scatter and minimizing the within-cluster scatter toward a satisfactory clustering quality.

@&#INTRODUCTION@&#
Data mining and machine learning techniques have been used to analyze breast cancer diagnoses, and they have been used to create models to detect the disease early or assist the diagnosis understanding. Because many features are noisy and redundant, especially in high-dimensional data representations, the created models usually suffer from noisy features participating in the training process and then compromise a satisfactory performance. For example, in classification (or clustering) learning algorithms, biased classifiers (or clusters) obtained using noisy and redundant features in the forecasting (or partitioning) process are not reliable. In the literature, many studies on feature selection are found in the literature [1–4], and the methods are grouped for supervised or unsupervised learning. In supervised learning, a feature selection method usually selects a subset of features to build a classifier that is expected to perform better than other classifiers which are built using all of the features. For example, Chhatwal et al. [5] reported that a logistic regression model can discriminate difference between benign and malignant tumors when breast cancer is detected early, and the model can identify the most important features associated with breast cancer.However, when class labels are absent during training, supervised learning methods would be ineffective for analyzing breast cancer diagnoses. In general, a labeled breast cancer diagnosis must be labeled by an expert in the breast cancer domain, and thus, it is difficult and time-consuming to obtain, especially for new and clinical diagnoses. Furthermore, a testing record, e.g., with a cell mutation, would be less informative if it was different from training records. Most existing models assume that the class labels are well-harvested and are available for training features. However, when the class labels are absent during training, the unsupervised learning for the feature selection work are thus integral, but rarely studied in the literature.Our paper aims to present a hybrid intelligent model that uses the cluster analysis techniques with feature selection for analyzing clinical breast cancer diagnoses. Our model provides an option of selecting a subset of salient features (i.e., attributes) for performing clustering and comprehensively considers the use of most existing models that use all the features to perform clustering. In particular, we highlight three qualitative principles that help users, e.g., clinical doctors, to analyze clinical breast cancer diagnoses effectively. First, clusters built by a subset of salient features are more practical and interpretable than clusters built using all of the features, which include noise. Because most existing studies perform clustering using features that are noisy and redundant, the clustering results are usually difficult to understand and interpret, especially for high-dimensional data. Second, clustering results provide clinical doctors with an understanding of the context of breast cancer diagnoses. Similarities and differences between records within a cluster or among clusters are transparent because partitioning the dataset into clusters maximizes the between-cluster scatter and minimizes the within-cluster scatter [6]. The similarities and differences then help to diagnose whether a patient suffers from breast cancer. For example, SOM-based (self-organizing map) approaches [7] show the relationships between neurons (nodes) on the map. This visual representation allows clinical doctors to observe that the breast cancer diagnoses projected for near-neighbor nodes are similar to one another but different from nodes that are farther away on the map. Finally, an efficient search for relevant records can be performed when clusters are obtained without noisy features. If a doctor wants to find similar breast cancer diagnoses from a past history dataset, a search can start at a satisfactory cluster and be expanded to neighboring clusters [8]. Because the noisy features are ignored, the time complexity of the search is significantly reduced. These three principles rely on the use of salient features to discover natural clusters using clustering learning algorithms and are applicable only to unsupervised learning.Specifically, our presented model has three major components. The first component includes the implementation of our previous method [9] and many benchmark feature selection methods to assess their performance in the analysis of two breast cancer diagnosis datasets. Second, we use some well-known clustering learning algorithms to partition data into clusters using the identified subsets of features. The third component works on interpreting the cluster results and reporting the findings from the cluster analysis. For the purpose of performance comparison on coincident quantitative measurements, we analyze performance of our method compared to other methods in performing clustering using the identified subsets of features.The paper is organized as follows. Section 2 reviews studies related to filter- and wrapper-based feature selection methods in supervised and unsupervised learning. Section 3 explains our presented model. Section 4 gives the experimental results. A brief discussion is given in Section 5. Finally, Section 6 details the conclusions.For the last decade, feature selection has been prominent in the literature. This section gives a brief review of basic concepts of prior studies that relate to the development of feature selection methods in selecting salient features for the cluster analysis.In the literature, feature selection methods are generally grouped into two categories: filter [1] and wrapper [2] models. The wrapper model depends on the classification or clustering algorithm, whereas the filter model is independent of such algorithms. The purpose of filter- and wrapper-based feature selection methods for selecting a subset of salient features is to maximize an objective criterion. The functions as the criteria include saliency, scatter separability, entropy, smoothness, consensus, density and reliability. The mutual information criterion is one of the most robust and highest-order statistic to identify salient features [10]. With respect to consider that our presented model aims to use the cluster analysis techniques for analyzing breast cancer diagnoses, we briefly introduce some unsupervised filter- and wrapper- methods, which are available for the experiments.Several feature extraction techniques in unsupervised learning have been considered for dimensionality reduction. The methods identify a mapping from a high-dimensional space to a low-dimensional space with a minimal loss of information. Most existing technologies use the variance to identify the mapping. Principal component analysis (PCA) is one of the most well-known techniques for dimensionality reduction. It maps data in a high-dimensional data space to a lower-dimensional one with topological preservation. The similarity and relationship among data in the mapped space are then better understood rather than that in the original representation space. For finding this mapping, the core in PCA is a paradigm to maximize the variance. Intuitively, a larger variance of a random variable raises the between-cluster scatter [11,12]. The opposite example for this scatter is the uniform distribution of another random variable.The above review stimulates us study the variance (Var.) metric for the feature selection process because many feature extraction techniques are based on the use of the variance metric. We also use the Max-Relevance (Max-Rel), which was implemented in [13] and is based on mutual information. In addition, we study PCA, which is commonly used to represent data in experiments.To select features to discover natural clusters, wrapper-based methods in unsupervised learning require a pre-specified clustering learning algorithm to partition a dataset into subsets (clusters). Once the clustering process converges (e.g., the optimal clusters are obtained), features are learned from the clusters [14,15]. Nonparametric methods for wrapper feature selection have been widely developed in the literature [16,17]. Additionally, several studies used the internal or the relative cluster validity as the criterion to observe how a feature was informative to the clusters, when a dataset was partitioned into clusters. For example, Chow et al. [18] proposed a feature selection method based on the compactness of and separation from clusters. Huang et al. [19] proposed a feature co-selection for Web document clustering that clusters the results in one type of feature space to identify salient features in other types of feature spaces. Li et al. [20] proposed a new text clustering method with feature selection and extended the chi-square term-category independence test to measure whether the dependency between a term and a cluster was positive or negative. Sanguinetti [21] presented a latent variable model to perform dimensional reduction on a dataset that contained clusters. In his study, a variable was salient if it preserved clustered information from the original representation space when mapped to a latent space.We follow the literature studies and implement some wrapper-based methods that learn features from the clusters resulting from a particular clustering algorithm, e.g., A. “wrapper-A” represents a wrapper-based feature selection method. We use A to obtain M∈{2, 3,…,30} clusters for a dataset. We use the Max-Dependency criterion [13], based on mutual information, to evaluate the relevance of a feature to its labeled cluster and select salient features according to their relevance. Four clustering algorithms, K-means, self-organizing map (SOM), complete-link hierarchical clustering (HC) and partitioning around medoids (PAM), are applied for the wrapper model. We thus have four methods including wrapper-Kmeans, wrapper-SOM, wrapper-HC and wrapper-PAM. Because similar units can be grouped in SOM, we follow a previous study [22] to obtain the user-defined number of clusters, M, for the map units.We present a hybrid intelligent model that uses the cluster analysis techniques with feature selection for analyzing clinical breast cancer diagnoses. Our model provides an option of selecting a subset of salient features for performing clustering and comprehensively considers the use of most existing models that use all the features to perform clustering. This model starts with inputting diagnoses. Before performing clustering using clustering learning algorithms, we apply several feature selection methods, as briefly discussed in Section 2, to select salient features. The model is introduced in Fig. 1. The solid lines represent our clustering learning process upon using our presented feature selection method, whereas the dotted lines represent the existing clustering learning process. Considering the use of both processes provides a satisfactory comparison of coincident quantitative measurements for analyzing breast cancer diagnoses. Once the clusters are partitioned, we have reports with respect to visual analysis, topographic browser, interesting patters discovery and context summary. For example, SOM-based methods are very popular for yielding the clusters in a visualization format, resulting satisfactory reports.We utilize our proposed feature selection method, which was an Instance-Based learning to quantify features from the Nearest and Farthest neighbors (which we call IBNF [9]), to adaptively analyze breast cancer diagnoses. Consider a clustering characteristic in that (1) a data instance and its nearest neighbors are usually clustered in a cluster, and (2) this data instance and its farthest neighbors are usually clustered in different clusters. IBNF was then motivated by this clustering characteristic. In particular, a feature might be more salient if this feature helps to minimize distances between each instance and its nearest neighbors, and it helps to maximize distances between each instance and its farthest neighbors.IBNF denotes a datasetXconsisting of n data instances x1,…,xn, where xi=[x1,i,…,xj,i,…,xd,i]Tmeans the ith instance inXwith d dimensions. IBNF also defines a non-zero feature vector w(t)=[w1(t),…,wj(t),…,wd(t)]T, where the element wj(t) is a real-valued quantity at the tth iteration. The w(t) reflects the “weights” for the weighted Euclidean distance metric applied to calculate distances between data instances and is utilized to obtain the nearest and farthest neighbors. Each xihas its nearest and farthest neighbors while inputting w(t). IBNF then evaluates feature salience with respects these neighbors for xiand outputs a feature salience vector [uiw(t)=u1,iw(t),…,uj,iw(t),…,ud,iw(t)]Trecording feature salience for xiunder w(t). In particular, the criterion for evaluatinguj,iw(t)is defined using feature compactness and separability as follows (1).(1)uj,iw(t)=FSj;xi→l;w(t)ϕ+FCj;xi→k;w(t)Θ,fork=1,…,Kandl=1,…,Lwhere FS() evaluates feature separability and FC() evaluates feature compactness for the jth feature of xiunder w(t). Thexi→k;w(t)Θandxi→l;w(t)ϕrespectively represent the nearest and the farthest neighbor, wherexi→k;w(t)Θ=[x1,i→k;w(t)Θ,…,xj,i→k;w(t)Θ,…,xd,i→k;w(t)Θ]Tandxi→l;w(t)ϕ=[x1,i→l;w(t)ϕ,…,xj,i→l;w(t)ϕ,…,xd,i→l;w(t)ϕ]T. When the neighbors are obtained, we evaluate the salience for each feature using FS() and FC(), as expressed in (2) and (3).(2)FSj;xi→l;w(t)ϕ=1L∑l=1Lεxj,i,xj,i→l;w(t)ϕ(3)FCj;xi→k;w(t)Θ=−1K∑k=1Kεxj,i,xj,i→k;w(t)Θwhere ɛ() is an absolute operation (e.g.,ε([0.5,0.2]T,[0.1,0.3]T)=[0.4,0.1]T). The larger value ofuj,iw(t)indicates that the jth feature of xiunder w(t) is more salient, and the saliences of all the features are recorded inuiw(t).Because different w values inconsistently contribute salience u, we should locate the bestuiw(t)by deriving w(t) such that every instance consistently favors the salient features. If{uiw(t)}i=1nwere consistent for every instance, we would say w(t) is the best solution, and the features can be selected according the elements of w. The method to reduce the searching problem is to optimize the sum-of-squared error (i.e.,||uiw(t)–w(t)||2). Specifically, we make a decision of stopping criterion to obtain the best w(T) which is a local optimal solution. The procedure to output the best w(T) for the use of analyzing diagnoses is shown in Fig. 2.In this section, we consider the selected features for executing clustering learning algorithms. Because all the clustering learning algorithms depend on a distance metric which is used to calculate distances between data instances, we provide a justification on how the selected features are used for the distance metric adaptively used in a clustering learning algorithm (e.g., SOM).We illustrate a system with kernel based on SOM [23] with the use of the selected salient features (i.e., using w(T)). The SOM approach nonlinearly projects high-dimensional data onto a low-dimensional grid. The projection preserves the topological order from the input space; hence, similar data patterns in the input space would be assigned to the same map node or to nearby nodes on the trained map. Denote that we have an input patternxi=[x1,i,…,xj,i,…,xd,i]Tin the input space, where xj,iis the value of the jth attribute of xi; also assume a map unitmk=[m1,k,…,mj,k,…,md,k]T, where mj,kis the value of the jth attribute of the kth unit in the map. This system therefore has two major processes. The first process is to search for the best matching unit (BMU) from all of the map units for each input pattern, where the BMU is most similar to the input pattern. The method to search for this BMU thus requires a distance metric to compute distances between input patters and map units. The Euclidean metric is an option; specifically, we consider the outputs of the selected features by exploring w(T) for computing distance between xiand mk, namely,dis(xi,mk|w(T))written as follows.(4)dis(xi,mk|w(T))=∑j=1d(xj,i−mj,k)×wj(T)21/2The second process is to update the weights onto the map units, where we follow the study [23] for this update. When inputting an input pattern xifor training SOM, the update function for a neighborhood node mkis written as follows.(5)mk(t+1)=mk(t)+α(t)×hk(t)×[xi(t)−mk(t)]where 0<α(t)<1 is the learning-rate function, hk(t) is the neighborhood function, which calculates the lattice distance between the BMU and mk, and dis() is the weighted Euclidean distance. Both α(t) and the width of hk(t) decrease gradually with increasing step t.Our model has three major components according to feature selection method, clustering learning algorithm and interpretation of cluster analysis. To validate performance comparison on coincident quantitative measurements, we implement methods, algorithms and criteria for the experiments using Matlab with version R2013a. These feature selection methods, clustering learning algorithms and cluster validations are summarized in Table 1and are briefly introduced later.•Implementation of feature selection methods:We design four filter-based feature selection methods: IBNF, PCA, Var. and Max-Rel. Furthermore, we implement four wrapper-based methods: wrapper-Kmeans, wrapper-SOM, wrapper-HC and wrapper-PAM. These eight methods are briefly introduced in Section 2.Implementation of clustering learning algorithmsTo demonstrate performance of the selected features in identifying natural clusters, the selected features are tested in some well-known clustering learning algorithms to partition a dataset into clusters. In particular, we use K-means, SOM, HC and PAM algorithms, which are available in the study [29], to perform clustering. We followed a previous study [22] to obtain the user-defined number of clusters for the map units because similar units can be grouped in SOM.Implementation of cluster validations:To evaluate clustering performance, three cluster validations are implemented: the Davies-Bouldin index (DBI), the Calinski-Harabasz (CH) index [27] and the R-squared validity (R-squared) [28]. The DBI is chosen because it is popular in cluster analysis. A lower DBI value indicates higher compactness within a cluster or higher separability between clusters. We also use the CH index to measure between-cluster isolation and within-cluster coherence. A larger CH value represents better performance. Furthermore, we adaptively use R-squared validity, which has been used to measure interpretation capability in the statistical model.

@&#CONCLUSIONS@&#
Feature selection is one of the most effective methods to enhance data representation and improve performance in terms of specified criteria, e.g., generalization classification accuracy. In the literature, many studies select a subset of salient features using supervised learning rather than unsupervised learning. When the class labels are absent during training, feature selection in unsupervised learning is integral, but its extensible application is rarely studied in the literature.The objective of this study is to select salient features that can be used to identify interesting clusters in the analysis of breast cancer diagnoses. Specifically, we highlight three qualitative principles that help users to analyze clinical breast cancer diagnoses using clusters resulting from a subset of salient features. First, the clusters built by a subset of salient features are more practical and interpretable than those built by all of the features, which include noise. Second, the clustering results provide clinical doctors with an understanding of the context of clinical breast cancer diagnoses. Finally, a search for relevant records based on the clusters obtained when noisy features are ignored is more efficient. These three principles rely on the discovery of natural clusters using salient features and are applicable only to unsupervised learning.To demonstrate the usefulness of these three qualitative principles, we use coincident quantitative measurements to analyze the salient features for discovering clusters. Eight feature selections methods, four clustering learning algorithms and three cluster validations are implemented using Matlab software. The experiments on the Breast cancer Wisconsin (Diagnostic) and Breast cancer Wisconsin (Original) datasets demonstrate that the selected features are effective for selecting salient features to discover natural clusters. Based on a performance evaluation using well-known validations in statistical model and cluster analysis, our analysis provides an interesting aspect in feature selection for discovering clusters. Our method might significantly impact data mining and machine learning applications in the future.