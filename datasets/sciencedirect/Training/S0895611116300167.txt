@&#MAIN-TITLE@&#
Multi-modal vertebrae recognition using Transformed Deep Convolution Network

@&#HIGHLIGHTS@&#
First cross modality 2D vertebra recognition, efficient clinical tool.New Transformed Deep Convolution Network, great potential to many cross modality organ recognition.Above 90% sensitivity.

@&#KEYPHRASES@&#
Vertebra detection,Vertebra recognition,Deep learning,Convolution network,

@&#ABSTRACT@&#
Automatic vertebra recognition, including the identification of vertebra locations and naming in multiple image modalities, are highly demanded in spinal clinical diagnoses where large amount of imaging data from various of modalities are frequently and interchangeably used. However, the recognition is challenging due to the variations of MR/CT appearances or shape/pose of the vertebrae. In this paper, we propose a method for multi-modal vertebra recognition using a novel deep learning architecture called Transformed Deep Convolution Network (TDCN). This new architecture can unsupervisely fuse image features from different modalities and automatically rectify the pose of vertebra. The fusion of MR and CT image features improves the discriminativity of feature representation and enhances the invariance of the vertebra pattern, which allows us to automatically process images from different contrast, resolution, protocols, even with different sizes and orientations. The feature fusion and pose rectification are naturally incorporated in a multi-layer deep learning network. Experiment results show that our method outperforms existing detection methods and provides a fully automatic location+naming+pose recognition for routine clinical practice.

@&#INTRODUCTION@&#
Magnetic resonance imaging (MR) and computed tomography (CT) are two main imaging methods that are intensively and interchangeably used by spine physicians. The longitudinal/differential diagnoses today are often conducted in large MR/CT dataset which makes manual identification of vertebrae a tedious and time-consuming task. Automatic locate-and-name system of spine MR/CT images which supports quantitative measurement is thus highly demanded for orthopaedics, neurology, and oncology. Automatic vertebra recognition, particularly the identification of vertebra location, naming, and pose (orientation+scale), is a challenging problem in spine image analysis. The main difficulty arises from the high variability of image appearance due to image modalities or shape deformations of the vertebraes: (1) Vertebra is difficult to detect due to imaging modalities. The image resolution, contrast and appearance for the same spine structure could be very different when it is exposed to MR/CT, or T1/T2 weighted MR images. (2) Vertebra is difficult to automatically name. The vertebrae and intervertebral discs are lack of unique characteristic features that automatic naming could fail easily. (3) Vertebra pose is difficult to estimate. The poses of vertebrae are highly diverse and little stable features can be used for pose estimation. Except for the local pose and appearance problems, the global geometry of spine is often difficult to recover in some medical situations, i.e., spine deformity and scoliosis. The reconstruction of global spine geometry from limited CT/MR slices can be ill-posed and requires sophisticated learning algorithms.Most current spine detection methods focus on identification of vertebra locations or labels in particular one particular image modality [1–5], and vertebra pose information 9s seldom obtained in the same method. (1) For vertebra localization, learning-based detectors were employed for handling specified image modalities, they were proven to work on CT (generalized Hough) [2], MR (Adaboost) [3], or DXA images (random forest) [6]. Their training and testing were performed on the chosen image protocol only. Some detection methods claimed they can work on both MR and CT. Štern et al. [7] utilized the curved spinal geometric structure extracted from both modality. Kelm et al. and Lootus et al. [8,9] used boosting-trained Haar features and SVM-trained Histogram of Oriented Gradients (HOG) respectively. However, these cross-modality methods often required the separated training for MR and CT, and thus the separated testing for the two modalities too. (2) For vertebrae naming, [2–5] had successful labeling on fully or partially scanned image volumes. Their methods relied on the identification of some special landmarks detected from multiple image views, i.e., axial view templates [2], spinal canals [5] or anchor vertebrae [3], while the exact labels are inferred by a probability inference model, i.e., a graph model [10], Hidden Markov Model (HMM) [4], or hierarchical model [3,18]. (3) Besides the detection and naming, vertebral pose is critical information in orthopedics. Pose estimation was used [1,8,5] for extracting the 3D structure of the spines. These estimation methods exploited the multi-planar detector to match the correct vertebrae poses, but can not directly used in a single slice input. In addition, most of the training-based methods, as pointed out in [11], required dense manual annotations for ground truth labels, i.e., annotations of all the corners and the center for each vertebrae. This makes the training-based method not convenient to use.To overcome these limitations, we uniquely propose a unified framework using Transformed Deep Convolution Network (TDCN) to provide automatic cross modality vertebrae location, naming, and pose estimation. As presented in Fig. 1, our system is a learning-based recognition system which contains a multi-step training stage and an efficient testing stage. The example results on MR and CT are shown Fig. 2. The main ingredients of the system is a novel deep learning model [12] inspired by groupwise registration [13,14] and multi-modal feature fusion [15,16]. We have the following contributions in this paper:•Vertebra recognition. The location, name, and pose (scale+orientation) of each vertebra are identified simultaneously. Further spine shape analysis, i.e., spondylolysis analysis, is then possible basing on the recognition results.Multi-modal feature learning. The vertebra features are jointly learned and fused from both MR and CT. This enhances the features discrimination and improves the classification of vertebra/non-vertebra.Invariant representation. In the training and recognition stage, the sampled of detected vertebrae are automatically aligned, generating transform-invariant feature representations or rectified final poses respectively.Simple annotation. Thanks to the invariant representation, our method only requires single-clicking for each vertebrae in ground truth annotation while other methods [8,5,9] require four clicks or more.The Transformed Deep Convolution Network (TDCN) is a novel deep network structure, which can automatic extract the best representative and invariant features for MR/CT. It employs MR–CT feature fusion to enhance the feature discriminativity, and applies alignment transforms for input data to generate invariant representation. This resolves the modality and pose variation problems in vertebra recognition. The overall structure of TDCN presented in Fig. 3. The two major components in TDCN: the feature learning unit and the multi-modal transformed appearance learning unit are presented in details as follows.The low-level feature learning unit is for unsupervised learning adaptive features that best represent the training MR/CT samples. The feature learning is implemented by layers of Convolution Restricted Boltzmann Machines (CRBM) [12]. CRBM is a multi-output filtering system which can adaptively update its filter weights to obtain the best approximative feature maps for the training samples. The learned features can reveal some unique micro-structures in MR or CT, while manual features like SIFT or HOG cannot adapt to these micro-differences.As shown in Fig. 3, there are two CRBM layers for feature extraction over MR patches and two for the extraction over CT patches. Each CRBM will accept a set of visible layers v as input, generating binary hidden layers h. In the training stage, the CRBM continuously tunes the weight coefficients Wkfor of the k∈{1, …, K} filter, in order to minimize the following functional(1)E(v,h)=−∑k=1Khk•(W¯k*v)−∑k=1Khkbk∑i,jhi,jk−c∑i,jvi,jwhere bkand c are adjustable bias, i, j will range in [1, NV−NW+1] for a NV×NVinput layerv=[vi,j]and a NW×NWfilter Wk. Operator • is a element-wise product of two matrixes, while * is a convolution. The hidden layer generated by Wkis a NH×NHmaphk=[hi,jk]with NH=NV−NW.Two CRBMs are stacked together (levels 1 and 2 in Fig. 3), providing two level feature extraction. Each CRBM will first unsupervisely train itself until (1) is minimized, then connected its output(input) to the higher(lower) level CRBM. Let level 1 has K1 filters and level 2 has K2 filters, the output of the two-level feature learning will be K1×K2 feature maps. For a given 2D filter W and bias b, the output of the CRBM can be defined by:(2)f(v)=σ(W¯*v+b)where σ the element-wise logistic sigmoid function andvis in its vectorized form. Suppose f1 is the output of level 1 CRBM in Fig. 4, for MR–CRBM (or CT-CRBM), we let f1 be defined in (2) by substituting the corresponding W with the K1 filters’ coefficients, i.e.,{WMR1,…,WMRK1}(or{WCT1,…,WCTK1}). Similarly, the level 2 CRBM in Fig. 4 output mapping f2 is defined by substituting W in (2) with the corresponding K2 filters’ coefficients.Multi-modal feature fusion is important in classification as the missing of some features in one modality will be compensated by the others, i.e., the missing vertebra discs in CT can be compensated by its MR counterparts. Also, the features are expected to be pose invariant, as geometric invariance can enhance the discrimination of vertebra/non-vertebra. The multi-modal transformed appearance learning unit is a set of deep network layers specifically for fusing multi-modal feature and transforming input samples to achieve invariant representations. With the fusion of low-level features from different modalities, the feature representation of vertebra can be enhanced to obtain a better discrimination.MR/CT feature fusion using RBM. The feature maps extracted by CRBM are sent to two Restricted Boltzmann Machines (RBM) for multi-modal feature fusion. RBM is a simpler version of CRBM, which works with vectorized visible and hidden layers other than 2D layers in CRBM. It also can reduce the dimensionality of the input by setting a shorter hidden layer size. Suppose nowv=[vi]∈ℝNVis the visible layer, which can be either MR or CT features, andh=[hj]∈ℝNHis the hidden layer, the particular functional RBM minimizes is:(3)E(v,h)=−∑i,jviWijhj−∑jbjhj−∑iciviwhere Wijnow is a NV×NHmatrix and bj, ciare adjustable bias. As shown in Fig. 4, the MR/CT feature maps obtained from the previous low-level feature learning unit are vectorized and combined as the visible layer of the RBM, generating the MR–CT fused feature vector as the hidden layer of the RBM. The fusion process is performed specifically by the level 3 RBM in TDCN (see the level 3 layer in Fig. 3). The result of level 3 RBM is a dimension-reduced unified feature vector that jointly describe the vertebra patterns in MR and CT.Similar to (2), we can define the output mapping of RBM as nonlinear activation function:(4)f(v)=σ(WTv+b)wherev,bare vectors other than 2D maps in (2). The mapping of level 3 RBM in Fig. 3, particularly f3, is thus constructed by substituting (4) with the learned weight matrix and bias from training, i.e., W3 and b3. f4 for the level 4 RBM can be obtained in the same way.The multi-modal fusion has a number of advantages that cannot be provided by single-modal feature learning (i.e., features learned only from level 1, 2 while bypassing level 3 and 4 in TCDN): (1) MR (T1,T2) and CT features can be compensated for each other in the fused feature vectors; (2) Shape features, which are rich in both modalities, are enhanced in the fused feature vector; (3) The fused feature vector can be used for identifying vertebra structures across MR and CT using only one unified classifier.Pose rectification by congealing. Pose rectification is for unifying the different orientation and sizes (scales) of vertebrae in a spine scan, so that the resulting learned feature can be more representative for vertebra. The vertebra appearance can be considered as invariant such that any vertebra patch can be assumed as be generated by warping the standard invariant patch with certain rotation and rescaling. This warping uniquely describes the 2D geometric transform from a standard model to a sampled data, which we describe as the pose of the vertebra. The pose rectification is performed by congealing.Congealing is a groupwise registration for an image set. It is employed for aligning the 2D feature maps obtained from the level 3 RBM. Supposevis an arbitrary vertebra image patch (MR or CT) with free-form size/orientation, inverse warping G−1 deformsvto a m×m regular patch, denoted asv∘G−1. Let{v1,…,vn}be the collection patches, the corresponding warped versions are represented as vector{v1∘G1−1,…,vn∘Gn−1}. We require G=G(r, s) be a similarity transform described by the rotation r and scaling s. The congealing for the patch collection can be formulated as the minimization of energy:(5)C(r,s)=∑i,j=1,i≠jn||vi∘Gi−1(ri,si)−vj∘Gj−1||2.Asvcan be either a MR or a CT patch, we cannot directly align these two types of patches in (5) unless they have a unified representation. This representation can be obtained by the feature fusion of RBM (Fig. 4) where the fused feature mapping allows us to align images from different modalities in the same congealing model. By using the CRBM mapping f1, f2 and the RBM mapping f3 obtained by activation (2) and (4) respectively, (5) can be then revised as(6)C(r,s)=∑i,j=1,i≠jn||f3(f2(f1(vi∘Gi−1(ri,si)))→)−f3(f2(f1(vj∘Gj−1))→)||2,where·→is the vectorization. f1, f2 will extract the low-level MR/CT features of patchvand f3 will map them into a unified feature vector. The deep congealing (6) not only can align patches in either MR or CT as the original congealing does (5) but also can align the MR and CT patches in the same patch set.Fig. 5illustrate a toy example of congealing over CT+MR images by minimizing (6). The vertebra patches in the three images are first mapped to unified feature maps using f1, f2 and f3. The deep congealing (6) is then applied, obtaining the aligned patches. Note that the bounding boxes of the patches under congealing can reveal the correct poses of the corresponding vertebrae. Therefore, from the congealing operation, we can rectify the correct poses of the vertebrae.The training process of the TDCN system is presented in Fig. 6. The process starts with the annotation of sample patches in original scans. It then trains TCDN using the selected samples, generating invariant vertebra features. The features are applied in training a SVM, obtaining the desired vertebra classifier.One-click sample annotation. The training samples (positive/negative) are collected by simple clicking operations in image slices. For positive samples, the user only need to click the vertebra center in a training image slice regardless of the size and orientation of the vertebra. This significantly simply the sample collection process in other methods (i.e., [8,9]) where at least four clicks are needed for each vertebra. A coarse labeling of vertebra type: T/L or S vertebra is required after clicking but no further specific labeling is needed. This also simplifies the previous training-based system which detailed labeling is often required. For negative samples, the non-vertebra patches can be simply obtained by random sampling or clicking. The ratio of the number of vertebra to that of non-vertebra can be set around 1/10 to 1/8.TDCN feature learning. The selected training samples are served as the input of the TDCN (Fig. 3) according to their original modalities. The TDCN then starts the layer-wise learning via the optimization of CRBM energy function (1) and RBM energy (3). Particularly, the learning starts from the lowest layer, updating its parameters until optimized. The parameters of the layer are then freezed, generating output and triggering the learning of the upper CRBM/RBM layer. The layer-wise update continues until all layers are optimized.Vertebra classifier learning. Thanks to the unified feature representation of MR and CT patches, we can train the vertebra/non-vertebra classification for both modalities simultaneously in a SVM. In our system, a standard classification SVM is trained for three classes according to the TDCN trained features: T/L vertebra, S vertebra, and non-vertebra.After the complete training process, the trained TDCN+SVM can be directly applied to the blind recognition for both MR and CT patches. With a patch sampling process (i.e., sliding window), the trained system can then be used for vertebra recognition for arbitrary MR/CT slices.We directly apply the trained multi-modal recognition system for full automatic vertebra recognition on arbitrary spine image. The overall recognition process is shown in Fig. 7.Vertebra detection. As shown in the first step of Fig. 7, to simulate the various poses of the vertebrae, we first rotate or rescale the input MR/CT image, generating a set of articulately transformed images. Regular patches (i.e., 51×51) are then randomly sampled from the images, and are sent as input to the trained TDCN and SVM. The locations that generates positive SVM output are marked as new vertebrae positions. The detection result is represented as a set of bounding boxes as green boxes for L/T vertebrae and yellow for S vertebrae in Fig. 8(a). A vertebra pose is now described by the box size and orientation.Pose post-rectification. After obtaining the positive results from SVM (2nd-last part of Fig. 7), we can rectify the poses of the bounding boxes using the same mechanism of deep congealing (6). This can be done by the optimization (6) with the positive patch set substituted as the input variables. The resulting aligned bounding boxes not only recover the correct poses of the detected vertebrae, but can also join the duplicated bounding boxes near a location into one box. Fig. 8(b) illustrate the process post-rectification of vertebrae poses using deep congealing.Vertebra naming. The specific vertebrae names can be obtained using the pose-rectified bounding boxes (last part of Fig. 7). Since the vertebra orientation has been correctly recovered by congealing, one can easily link up the separated bounding boxes in pairwise to form a spine structure using their orientations. The orientation-based linking also help to avoid the connecting with unrelated outliers bounding boxes from falsely detected locations. As shown in Fig. 8(c), when the bounding boxes are linked, we can easily identify the connecting part of T/L and S vertebra along the linking directions. The vertebra L5 and S1 can then be identified from the connecting part. Starts with L5 and S1, by propagating the specific labels along the linked bounding boxes, we can obtain all vertebra labels for the detected vertebrae. The naming of MR and CT can be done simultaneously using the same work flow.

@&#CONCLUSIONS@&#
