@&#MAIN-TITLE@&#
A cost sensitive decision tree algorithm with two adaptive mechanisms

@&#HIGHLIGHTS@&#
An adaptive selecting cut point mechanism is designed to build a classifier.Adaptive removing attribute mechanism will remove the redundant attributes.We adopt two mechanisms to design algorithm which for classifier construction.Experimental results show the effectiveness and feasibility of our algorithm.

@&#KEYPHRASES@&#
Adaptive mechanisms,Cost sensitive,Decision tree,Granular computing,

@&#ABSTRACT@&#
Decision trees have been widely used in data mining and machine learning as a comprehensible knowledge representation. Minimal cost decision tree construction plays a crucial role in cost sensitive learning. Recently, many algorithms have been developed to tackle this problem. These algorithms choose an appropriate cut point of a numeric attribute by computing all possible cut points and assign a node through test all attributes. Therefore, the efficiency of these algorithms for large data sets is often unsatisfactory. To solve this issue, in this paper we propose a cost sensitive decision tree algorithm with two adaptive mechanisms to learn cost sensitive decision trees from training data sets based on C4.5 algorithm. The two adaptive mechanisms play an important role in cost sensitive decision tree construction. The first mechanism, adaptive selecting the cut point (ASCP) mechanism, selects the cut point adaptively to build a classifier rather than calculates each possible cut point of an attribute. It improves the efficiency of evaluating numeric attributes for cut point selection significantly. The second mechanism, adaptive removing attribute (ARA) mechanism, removes some redundant attributes in the process of selecting node. The effectiveness of the proposed algorithm is demonstrated on fourteen UCI data sets with representative test cost Normal distribution. Compared with the CS-C4.5 algorithm, the proposed algorithm significantly increases efficiency.

@&#INTRODUCTION@&#
Data mining is one of the most actively researched areas in information science with important real world applications [42]. Classification is one of the most important tasks in the data mining domain [19,23,24]. There are several techniques for classification, such as k-nearest neighbor algorithms [3], support vector machines [5,14,35], artificial neural networks [16,20], decision trees [1,8,46], rough set theory [25,32,41,45,50], and so on. Decision tree is a useful choice when the tasks are to classify or predict outcomes and to generate easy-to-interpret rules [6,15,39].The structure of decision tree is simple and easy to interpret. Typical algorithms on decision tree induction, such as ID3 [33], CART [21] and C4.5 [34], have been successfully applied to a broad range of tasks from learning to diagnose medical cases to learning to assess credit risk of loan applicants. Existing techniques aim at training classifiers to minimize the expected number of errors [4,17,26,43]. This implicitly assumes that all classification errors involve the same cost [22,38]. Actually, different misclassification errors often lead to different costs. This leads to a new and hot research topic, cost sensitive learning, which addresses classification problems with cost. It aims to reduce the average total cost involved in the learning process.Cost sensitive learning [9,27,49] is an extension of traditional non-cost sensitive data mining and machine learning [38,40]. Test cost and misclassification cost are two most important types in real world applications [37]. The test cost is money, time, or other resources while we obtain attribute values of an object [18]. The misclassification cost is the cost of assigning an object to class j when it actually belongs in class i[10,11,48]. Several researches focus on the test cost, but fail to take into account misclassification cost. However, it is important to consider both test cost and misclassification cost together [28] in many applications.Some algorithms such as IDX [31],λ-ID3 [29], CSGain [7] and CS-C4.5 [13] have been developed to obtain appropriate cost sensitive decision trees. Existing cost sensitive learning techniques work well on small data sets. However, there are still a lot of redundant computations in the process of decision tree construction. On the one hand, the existing techniques choose an appropriate cut point of a numeric attribute by computing all possible cut points. On the other hand, all attributes are tested in the process of assigning node. Therefore, these algorithms have a low efficiency on medium or large data sets. For example, the run time of the CS-C4.5 algorithm is nearly 10,000ms on Magic data set. This motivates us to propose a new approach for this issue.In this paper, based on C4.5, we put forward a cost sensitive decision tree algorithm with two adaptive mechanisms. It is an effective method for cost-sensitive decision tree construction. We simply refer to it as ACSDT algorithm. The major contributions of this method are twofold. On the one hand, an adaptive selecting cut point (ASCP) scheme for cut points selecting is assigned. It greatly reduces the amount of candidate cut points than the traditional mechanism of cut points selecting. On the other hand, we design an adaptive removing attribute (ARA) mechanism to remove some redundant attributes in nodes assigning procedure. This mechanism is adaptive to the data size involved rather than being fixed, so it is realistic. These two mechanisms are the key of the ACSDT algorithm. In a word, the new algorithm greatly improves the efficiency of cost sensitive decision tree construction.The proposed algorithm is implemented with Java in our open resource software COSER (Cost sensitive rough sets) [30]. A representative distribution, namely Normal, is employed to generate test costs from a statistical viewpoint. We undertake experiments on fourteen data sets from the UCI (University of California-Irvine) library [2]. Experimental results demonstrate the effectiveness of the ACSDT algorithm. The average total costs obtained by our algorithm are smaller than CS-C4.5 algorithm [13] on twelve data sets which are selected in this experiment. In addition, ACSDT algorithm is more efficient than the existing CS-C4.5 algorithm. In larger data sets, the improvement of efficiency tends to be quite significant. For example, it is more than 115,777ms improvement of the ACSDT algorithm on Clean data set.The rest of the paper is organized as follows. Section 2 reviews basic knowledge involved in this article, including the decision system with test costs and misclassification costs, and the calculation about average total cost of decision trees. In Section 3, we introduce the existing CS-C4.5 algorithm and give an analysis of the cut points selecting mechanism. Section 4 introduces the cost sensitive decision tree algorithm with two adaptive mechanisms. Section 5 adopts two examples to illustrate the cost sensitive decision tree construction and its average cost calculation procedure. Section 6 presents experiment schemes and provides a simple analysis of the results. Finally, Section 7 presents the conclusions and the future works.In this section, we review the basic knowledge including the decision system with test costs and misclassification costs, and the calculation about average total cost of decision trees.In data mining and machine learning, the decision system with test costs and misclassification costs is an important concept and defined as follows.Definition 1[47]A decision system with test costs and misclassification costs (DS-TM) is the 7-tuple:(1)S=(U,C,D,V={Va|a∈C∪D},I={Ia|a∈C∪D},tc,mc),where U is a nonempty finite set of objects called the universe, C is a nonempty finite set of condition attributes, D is a nonempty finite set of decision attributes,{Va}is a set of values for each attributea∈C∪D,Iais an information function for each attributea∈C∪D(i.e.Ia:U→Va), tc is a test cost function (i.e.C→R+∪{0}), and mc is a misclassification cost matrix (i.e.mc:k×k→R+∪{0}).Table 1presents a decision system of Bupa liver disorder (Liver for short). WhereU={x1,x2,x3,…,x345}, C = {Mcv, Alkphos, Sgpt, Sgot, Gammagt, Drinks}, and D = {Selector}.We adopt the test cost independent model [36] to define the cost of a test set. That is,tc(B)=∑a∈Btc(a)for anyB⊆C. The test cost function can be stored in a vector. An example of text cost vector is listed in Table 2. That is, the test costs of Mcv, Alkphos, Sgpt, Sgot, Gammagt, and Drinks are $5, $5, $4, $8, $3, and $5 respectively.We represent the binary class misclassification cost function by Table 3, wheremc(1,0)stands for cost associated with a minority class object assigned to the majority class andmc(0,1)represents the opposite misclassification scenario. Usually,mc(0,0)=mc(1,1)=0. A decision system with both test costs and misclassification costs is represented by the following example.Example 1Consider a Liver decision system listed in Table 1. Table 2 is the test cost vector of Liver decision system. The misclassification cost matrix ismc=0501000. The test cost is $5+$4+$3=$12 when the conditional attributes Mcv, Sgpt, and Gammagt are selected. In Liver data set, the Selector field is used to split the data into two sets. LetS1={xi|Selector(xi)=1,xi∈U},S2={xi|Selector(xi)=2,xi∈U}. The number of objects ofS1is|S1|=145, and the number of objects ofS2is|S2|=200. That is, if a patientxi∈S1(xi∈S2)is misclassified asS2(S1), a penalty of$100($50)is paid.Let T be a decision tree, U be the testing data set, andx∈U. x follows a path from the root of T to a leaf. Let the set of attributes on the path beSx. The test cost of x is(2)tc(x)=tc(Sx)=∑a∈Sxtc(a).We denote the real class label of x beRxand the prediction x is denoted asPxprojected by classification T. The misclassification cost of x ismc(x)=mc(Rx,Px). The total cost of x istc(x)+mc(x).Definition 2[29]The average total cost(ATOC)of decision tree T on U is:(3)ATOC(U)=∑x∈Utc(x)+mc(x)|U|.A numeric attribute can be used to classify once more in classify process. After an attribute is measured, we have known the values of this attribute, and it is no necessary for us to test it again. Hence, when attribute a is used again, we lettc(a)=0in Eq. (3) to avoid computing the test cost repeatedly.Cost sensitive learning is an extension of traditional inductive learning for minimizing classification total cost. Learning from data with both test cost and misclassification cost is especially interesting. Recently, the CS-C4.5 algorithm for cost sensitive decision trees, which works well in real applications, has been published. In this section, we introduce the CS-C4.5 algorithm [13] and give a simple analysis with an example.Attribute selection is a fundamental process in decision tree induction. In each induction step (i.e., generating a node) of building the decision tree, it is necessary to choose one attribute to split the remaining data. A lot of strategies, such as the information gain ratio function [34] and GINI index criterion [21], have been proposed for choosing the splitting attribute at each node. CS-C4.5 algorithm considers both information gain and test cost. The heuristic function in the CS-C4.5 algorithm is(4)f(a)=Gain(a,pa)/(tc(a)∗ϕa)ω,where a is a conditional attribute,tc(a)is the test cost of the attributea,pais the best threshold value of attribute a andGain(a,pa)is the information gain of the attribute a.ωis a user provided parameter to trade off the cost and the information gain. It is a positive number in [0, 1].ϕais a risk factor used to penalize a particular type of tests, known as delayed test, which are tests, such as blood tests, where there is a time lag between requesting and receiving the information [44].A numeric attribute is typically discretized during decision tree generation by partitioning its range into two intervals. LetPabe the set of the values of numeric attribute a, andVaxbe the value of numeric attribute a of an object x.pa∈Pais a threshold value of attribute a. To an object x, ifVax⩽pa, we assign it to the left branch, otherwise we assign it to the right branch. Letlb(a,pa)={xi|Vaxi⩽pa,xi∈U},rb(a,pa)={xi|Vaxi>pa,xi∈U}. We call such a threshold value,pa, a cut point [12]. The information gain of the partition induced bypais denoted byGain(a,pa). The cut pointpafor whichGain(a,pa)is maximal amongst all the candidate cut points is taken as the best cut point andGain(a)=Gain(a,pa). The following example helps explain this mechanism.Example 2To illustrate the cut points selecting mechanism, we randomly select twenty objectsU′from U which are listed in Table 4. In this table,|S1|=11and|S2|=9. LetSl1(a,pa)={xi|Selector(xi)=1,xi∈lb(a,pa)},Sl2(a,pa)={xi|Selector(xi)=2,xi∈lb(a,pa)},Sr1(a,pa)={xi|Selector(xi)=1,xi∈rb(a,pa)},Sr2(a,pa)={xi|Selector(xi)=2,xi∈rb(a,pa)}. Let E be the information entropy of this table,Elbbe the the information entropy of the left branch andErbbe the the information entropy of the right branch. We have that:E=-∑i=12(|Si|/|U′|×log2|Si|/|U′|)=11/20×log211/20+9/20×log29/20=0.9709. For simply, we show the cut points selecting mechanism of attribute Mcv.From Table 4 we have that:PMcv={82,84,85,86,87,88,89,90,91,92,93},lb(Mcv,82)={x18},rb(Mcv,82)=U′-lb(Mcv,82). The number of objects oflb(Mcv,82)is|lb(Mcv,82)|=1and the number of objects ofrb(Mcv,82)is|rb(Mcv,82)|=19.Elb=-∑i=12|Sli(Mcv,82)|/|lb(Mcv,82)|×log2|Sli(Mcv,82)|/|lb(Mcv,82)|=0.Erb=10/19×log210/19+9/19×log29/19=0.9980.Gain(Mcv,82)=E-(|lb(Mcv,82)|/|U′|×Elb+|rb(Mcv,82)|/|U′|×Erb)=0.0447.Similarly, we have that:Gain(Mcv,84)=8.060×10-4,Gain(Mcv,85)=0.0018,Gain(Mcv,86)=0.0591,Gain(Mcv,87)=0.0600,Gain(Mcv,88)=0.0667,Gain(Mcv,89)=0.0110,Gain(Mcv,90)=0.0435,Gain(Mcv,91)=0.0208,Gain(Mcv,92)=0.0242andGain(Mcv,93)=8.060×10-4.Gain(Mcv)=maxpMcv∈PMcv{Gain(Mcv,pMcv)}=Gain(Mcv,88)=0.0667. Therefore, the selected cut point of attribute Mcv is 88.On EEG Eve State (Eeg) data set, the cut points and information gain of each attribute obtained by the CS-C4.5 algorithm are recorded. For simply, we show the candidate cut points of eight attributes on Eeg data set in Fig. 1. The horizontal coordinate represents candidate cut points of attribute and the vertical coordinate represents the information gain. From Fig. 1 we can observe the following. Each attribute is evaluated nearly 400 times when selecting the best cut point on Eeg data set.From the above example and experiment analysis, we can observe that this scheme of selecting a cut point is dissatisfactory since there are usually many attributes and many objects on each data set. The huge number of candidate cut points makes the computation space too large to search. Therefore, we study an algorithm with a new scheme for cut point selections in Section 4.In this section, we introduce a heuristic function for attribute selection. An adaptive selecting cut point (ASCP) scheme is designed for cut point selections. We propose a cost sensitive decision tree algorithm with two adaptive mechanisms which is designed for minimal cost decision tree construction. This algorithm is called as ACSDT algorithm.Attribute selection is a fundamental process in decision tree induction. The C4.5 algorithm adopts the information gain ratio as heuristic function. The information gain ratio of attribute a is(5)GainRatio(a,pa)=Gain(a,pa)/Split-infor(a,pa),wherea,paandGain(a,pa)have the same meanings as in CS-C4.5 algorithm andSplit-infor(a,pa)is the split information entropy of the attribute a. Obviously, when gain ratio of an attribute is larger, it contains more information.The proposed cost sensitive decision tree algorithm employs the heuristic function based on C4.5 as below:(6)Quality(a,pa)=GainRatio(a,pa)×(1+tc(a))λ,wheretc(a)is the test cost of attribute a andλis a non-positive number. It is introduced to adjust the influence of the test cost. That is, an attribute with lower test cost and more information takes more advantage in the choice.LetPabe the set of all candidate cut points of numeric attribute a. We rewrite Eq. (6) as below:(7)Quality(a)=maxpa∈Pa{Quality(a,pa)}.Compared with the heuristic function of the CS-C4.5 algorithm, our heuristic function adopts information gain ratio to express the attribute’s classified ability rather than information gain. It can reduce biased towards attributes that have more values [34]. In addition, our heuristic function degrades to the heuristic function of the C4.5 algorithm when attribute a is used again.In this section, an adaptive selecting cut point (ASCP) scheme is introduced in Algorithm 1. It gives us a detailed description of ASCP scheme for cut point selections. And it contains two main steps.Algorithm 1Adaptive selecting cut point schemeInput:pai;stepMethod: ASCPOutput:Quality∗1:Quality∗=Quality(ai,pai);2:if (Quality(ai,pai+step)>Quality∗) then3:if (Quality(ai,pai+step)⩾Quality(ai,pai-step)) then4:Quality∗=Quality(ai,pai+step);5:pai=pai+step;6:step=step/2;7:ASCP(pai,step);8:end if9:end if10:if (Quality(ai,pai-step)>Quality∗) then11:if (Quality(ai,pai-step)⩾Quality(ai,pai+step)) then12:Quality∗=Quality(ai,pai-step);13:pai=pai-step;14:step=step/2;15:ASCP(pai,step);16:end if17:end if18:returnQuality∗In Step 1, we compute the values ofQuality(a,pa),Quality(a,pa+step)andQuality(a,pa-step). Meanwhile, the maximum among these three values is obtained.In Step 2, we selectQuality(a,pa)as the return value if it is the maximum among the three values. On the contrary, we will perform this algorithm again ifQuality(a,pa+step)orQuality(a,pa-step)is the maximum. Lines 4 to 7 and Lines 12 to 15 illustrate this process clearly.Example 3Consider an example of numeric decision system in Table 4. Letλ=-1. We denote bymaxVathe maximum value of attribute a, byminVathe minimum value of a, that ismaxVa=maxxi∈U{Vaxi},minVa=minxi∈U{Vaxi}. For simplicity, this example illustrates the calculation process of heuristic function value of attribute Mcv.Step 1.Sort the objects by the value of the attribute Mcv and find its maximum value(maxVMcv)and minimum value(minVMcv). From Table 4 we have that:maxVMcv=95,minVMcv=82.pMcv=(95+82)/2=88.5andstep=(95-82)/4=3.25.Compute the heuristic function value of Mcv when the cut point ispMcv(that is 88.5).The information gain of attribute Mcv isGain(Mcv,pMcv)=Gain(Mcv,88.5)=0.0667.The information gain ratio of attribute Mcv isGainRatio(Mcv,88.5)=Gain(Mcv,88.5)/Split-infor(Mcv,88.5)=0.0667.The value of heuristic function isQuality(Mcv,88.5)=GainRatio(Mcv,88.5)×(1+tc(Mcv))λ=0.0667×(1+5)-1=0.0111.Compute the heuristic function value of Mcv when the cut point ispMcv+step(that is 91.75).Gain(Mcv,pMcv+step)=Gain(Mcv,91.75)=0.0208.GainRatio(Mcv,91.75)=Gain(Mcv,91.75)/Split-infor(Mcv,91.75)=0.0263.Quality(Mcv,91.75)=0.0044.Compute the heuristic function value of Mcv when the cut point ispMcv-step(that is 85.25).Gain(Mcv,pMcv-step)=Gain(Mcv,85.25)=0.0018.GainRatio(Mcv,85.25)=Gain(Mcv,85.25)/Split-infor(Mcv,85.25)=0.0025.Quality(Mcv,85.25)=0.0004.According to Eq. (7), we obtain thatQuality(Mcv)=0.0111.We only need evaluate three times for attribute Mcv in Example 3 by ASCP scheme. It is evaluated eleven times in Example 2 by the traditional cut points selecting mechanism. The traditional cut points selecting mechanism must evaluateN-1times for each attribute if the N examples have distinct values. As networks connecting computational resources get faster, an increasing number unprecedented amounts of data occur in our life. N is typically very large. The ASCP scheme adopts an adaptive method for searching the best cut point without being influenced by N.Despite ASCP scheme looks simplicity, it has a good performance for selecting the cut point of attributes. A simple experiment illustrates it clearly. We compare the traditional scheme of selecting cut points with ASCP scheme on a number of data sets. The candidate cut points and information gain of each attribute are recorded. The candidate cut points and information gain of twelve attributes on Wdbc data set are shown in Fig. 2. The horizontal coordinate represents different candidate cut points of attribute and the vertical coordinate represents the gain ratio. With Fig. 2, we have the following observations.(1)The ASCP scheme is very effective. As can be seen from Fig. 2, the ASCP scheme can select the best cut point in most cases.From Fig. 2 (b), we can find that only for attributesa07anda28the ASCP scheme can not obtain the best cut point. For other ten attributes, the ASCP scheme can obtain the best cut point. Hence, the performance of ASCP scheme is acceptable.In addition, the calculation of the ASCP scheme is much smaller than the traditional scheme. It is easily find from Fig. 2, there are only less than 10 candidate cut points are evaluated by new scheme of each attribute.In this section, we provide a detailed description of the ACSDT algorithm which is listed in Algorithm 2. It contains six main steps. In the following, we detail each of the steps of the algorithm.Algorithm 2Cost sensitive decision tree algorithmInput: the training data set S; the set of attribute C, parameter∂Method: ACSDTOutput: tree, A decision tree1:Create a node tree;2:if (S is pure or C is empty) then3:return tree as a leaf node;4:end if5:maxQuality=0; The max value of the heuristic function6://Select attribute with the hightest value of heuristic function7:for(i=0;i<|C|;i++)do8:Compute the max value (denoted as maxValue) and the minimal value (denoted as minValue) of attributeai;9:cp=12(maxValue+minValue),step=14(maxValue-minValue);10:Quality(ai)= ASCP(cp,step);11:if(Quality(ai)>maxQuality)then12:A=ai;maxQuality=Quality(A);13:else14://Remove attribute15:if (|C|>∂andQuality(ai)<1∂∗maxQuality) then16:C=C-{ai};17:end if18:end if19:end for20:if (maxQuality=0) then21:return tree;22:end if23:tree=tree←A;tc(A)=0;24://Split S into two data sets:S1,S2.25:Put the object withVAxi⩽cp(VAxi>cp) intoS1(S2);26:for(i=1;i⩽2;i++)do27:ACSDT(Si,C);28:end forcorresponds to Line 1. The tree starts as a single node representing the training objects.contains Lines 2–4. If the objects are all of the same class or there are no remaining attributes on which the objects may be further partitioned, then the node becomes a leaf.corresponds to Lines 7–19 which contain the key code of Algorithm 2. In this step, we select the best attribute among these attributes as a node. First, the max value and the minimal value of attribute a are obtained in Line 8. Then, we compute the median value and set a step of the attribute a for obtaining the value of the heuristic functionQuality(a). Meanwhile, we can get the best cut point cp by ASCP scheme which is described in Section 4.2. Finally, Lines 11–13 is a description of comparison process. We select the best attribute A from these attributes according to the values ofQuality(a)obtained from ASCP scheme.contains Lines 15–17. An adaptive removing attribute (ARA) mechanism is described in this step.∂in Line 16 is a positive parameter to adjust how many attributes be removed in decision tree construction. That is, the bigger of the parameter the more attribute be removed in the process of nodes selections. To illustrate the ARA mechanism clearly, an example is presented in Example 4.corresponds to Line 25. A branch is created forVAxi⩽cporVAxi>cprespectively. If an object’s value of attribute A is less than or equal to cp, it is put into the left branch. Otherwise, it is put into the right branch. Then the data set is divided into two data sets.contains Lines 26–28. The algorithm recursive applies the same process to generate a decision tree for the data setsS1andS2.An important problem is to label a node (N) when it has difference classes examples. In our method, for each node, let MA denote the nodes for a majority class and let MI denote the nodes for a minority class, the criterion is designed as:•MA; ifmc(1,0)×mi⩽mc(0,1)×maMI; ifmc(1,0)×mi>mc(0,1)×maConsider German data set which contains 24 condition attributes and 1000 objects. Let∂=10. The values of heuristic function for different attributes of German data set on deciding the first node are listed in Table 5. Let bestNode denote the current best attribute on selecting the first node. The process of deciding first node is as follows.First, initialize bestNode(bestNode=Quality(a1)=9.5×10-3). Second, compare the value of heuristic function of next attribute(Quality(a2))with bestNode. IfQuality(a2)<110bestNode, we deletea2. That is,a2is not considered in the process of next node selecting. IfQuality(a2)>bestNodewe update bestNode. We neither deletea2nor update bestNode since110bestNode<Quality(a2)<bestNode.Quality(a3)>bestNode, so we update bestNode(bestNode=Quality(a3)). Attributesa4,a5,a6have the same situation asa2. Attributea7is deleted sinceQuality(a7)<110bestNode. Similarly, we examine each attribute, in turn, and judge whether it should be deleted. Finally, the attribute with the highest heuristic function value is taken as the first node.Decision trees are built using a set of data referred to as the training data set. A different set, is used to check the model, called the test data set. When we obtain a new object of the test data set, we can make a prediction on the state of the class variable by following the path in the tree from the root to a leaf node. The following example helps explain this process.Example 5Consider a Liver decision system illustrated in Example 1. The Liver data set are randomly split into two subsets: 60% for training(Utr)and 40% for testing(Ute). For simplicity, we setλ=-1,∂=10. Fig. 3illustrates a cost sensitive decision tree(T)obtained by the ACSDT algorithm on training data set of Liver data set. The circles denote non-leaf nodes of T and the leaf nodes of T are denoted by rectangles.LN={A,B,…,O}is the set of leaf node of T. As can be seen from Fig. 3, there are three kinds of information in every leaf node: “label(num1;num2)”. Where “label” is the prediction class of the objects in this leaf node, “num1” is the number of the objects in this leaf node, and “num2” is the number of objects whose real real class is not “label”. For instance, the information in J is “1(40;8)”, where “1” indicates the class of the objects in leaf node J, “40” is the number of objects in J, and “8” is the number of objects whose real class is not “1” but been misclassified as class “1”.We adopt average total cost to test the performance of cost sensitive tree. An example is given to illustrate the calculation of the average costs of the cost sensitive decision tree.Example 6A cost sensitive decision tree is obtained from the decision system in Example 5. LetXMbe the set of the objects in leaf node M.AtMis the set of the attributes to reach the leaf node M. To illustrate the calculation process, we randomly select twenty objects are listed in Table 4.Step 1.According to the data set(U)in Table 4 and cost sensitive decision tree(T), we can obtainXi(i∈A,B,…,O)as follows.XC={x5,x9,x13,x18},XD={x1,x3,x4},XH={x15},XJ={x8,x19},XK={x2,x6,x7,x14},XL={x10,x11,x12,x16,x17,x20}, and there are no object in other leaf nodes.From the structure of T we can know that:AtC= {Gammagt, Sgpt, Drinks, Sgot},AtD= {Gammagt, Sgpt, Drinks, Sgot},AtH= {Gammagt, Sgpt, Drinks, Alkphos},AtJ= {Gammagt, Sgpt}, andAtK= {Gammagt, Sgpt},AtL= {Gammagt, Sgpt, Drinks, Sgot}.The test cost of the leaf node C istc(XC)=|XC|×tc(AtC)=|XC|×∑a∈AtCtc(x)=4×($3+$4+$5+$8)=$80.Similarly,tc(XD)=|XD|×tc(AtD)=3×($3+$4+$5+$8)=$60;tc(XH)=|XH|×tc(AtH)=1×($3+$4+$5+$5)=$17;tc(XJ)=|XJ|×tc(AtJ)=2×($3+$4)=$14;tc(XK)=|XK|×tc(AtK)=4×($3+$4)=$28;tc(XL)=|XL|×tc(AtL)=6×($3+$4+$5+$8)=$120.The total test cost is∑x∈Utc(x)=∑y∈LNtc(Xy)=tc(XA)+tc(XB)+⋯+tc(XO)=$80+$60+$17+$14+$28+$120=$319.The total misclassification cost of the leaf node C ismc(C)=1×m(0,1)=1×$50=$50.Similarly,mc(D)=1×mc(1,0)=1×$100=$100;mc(H)=1×mc(1,0)=1×$100=$100;mc(J)=0×mc(0,1)=0×$50=$0;mc(K)=0×mc(1,0)=1×$100=$100;mc(L)=3×mc(0,1)=3×$50=$150.The total misclassification cost is∑x∈U′mc(x)=∑y∈LNmc(y)=mc(A)+mc(B)+⋯+mc(O)=$500.The average total cost isATOC(U′)=∑x∈U′tc(x)+mc(x)|U′|=$319+$50020=$40.95.

@&#CONCLUSIONS@&#
