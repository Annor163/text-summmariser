@&#MAIN-TITLE@&#
Query-focused multi-document summarization using hypergraph-based ranking

@&#HIGHLIGHTS@&#
We propose a novel hybrid method to capture group relation of sentences.We cluster sentences with a KL-divergence based on word-topic distribution.We proposed a vertex reinforcement random walk process in a hypergraph model.The process simultaneously consider the query similarity, the centrality and the diversity of sentences.We implement our framework and verify improvement over appropriate baselines.

@&#KEYPHRASES@&#
Multi-document summarization,Hypergraph-based ranking,HDP,

@&#ABSTRACT@&#
General graph random walk has been successfully applied in multi-document summarization, but it has some limitations to process documents by this way. In this paper, we propose a novel hypergraph based vertex-reinforced random walk framework for multi-document summarization. The framework first exploits the Hierarchical Dirichlet Process (HDP) topic model to learn a word-topic probability distribution in sentences. Then the hypergraph is used to capture both cluster relationship based on the word-topic probability distribution and pairwise similarity among sentences. Finally, a time-variant random walk algorithm for hypergraphs is developed to rank sentences which ensures sentence diversity by vertex-reinforcement in summaries. Experimental results on the public available dataset demonstrate the effectiveness of our framework.

@&#INTRODUCTION@&#
The task of query-focused multi-document summarization is to create a summary for a document set, which aims to provide an answer for a given query. Since the task has been initiated in DUC (Document Understanding Conferences), it has attracted more and more attention.The main method for query-focused multi-document summarization is based on sentence selection, and the selected sentences both summarize the documents and answer the query. In general, there are three steps to select sentences. First, the document is divided into sentences. Second, an abstractive or extractive summarizer is used to get some representative sentences for the query. The final step is to select the most informative ones as a summary. In this paper, we concentrate on extractive multi-document summarization.For extractive summarization, the main strategies fall into several categories. Feature-based approaches use features like term frequency, sentence position, length, etc., to rank sentences (Ouyang, Li, Lu, & Zhang, 2010). Graph-based approaches rank sentences by a random walk which explores the relations between sentences. The main graph-based ranking approaches used in summarization include LexRank (Erkan & Radev, 2004b), Manifold-Ranking (Wan, Yang, & Xiao, 2007), Hyperlink-Induced Topic Search (HITS) (Kleinberg, Kumar, Raghavan, Rajagopalan, & Tomkins, 1999) and DivRank (Mei, Guo, & Radev, 2010).For graph-based approaches, it has been demonstrated that a cluster-based method can effectively improve the quality of extractive summarization (Cai & Li, 2012; Li & Li, 2012; Wan & Yang, 2008; Zhang, Ge, & He, 2012). The method generally focuses on sentence-level or cluster-level similarity, as well as the cluster-level relationship of sentences. For example, Wang, Li, Li, Li, and Wei (2013) and Wang, Wei, Li, and Li (2009) used hypergraphs to model both pairwise similarity and sentence clusters simultaneously, and they employed a hypergraph-based significance score propagation process to rank sentences. A good query-focused summary is expected to meet two factors: (1) high query relevance, (2) high diversity or minimum redundancy. High query relevance indicates the summary accounts for the given query. High diversity or minimum redundancy indicates the summary is able to present information without any convoluted. For these two aspects, existing graph-based methods have two limitations: (1) they cluster sentences simply based on co-occurrence lexical similarity, it may put semantically similar sentences into different topics, if they share few common words. For example:•S1: A forest is a large area where trees grow close together.S2: Woodland is land with a lot of trees.Since S1 and S2 share very few words, the methods based on “word co-occurrence” may result in a decision that S1 and S2 are discussing different topics. However, it is clear for a human interpreter that S1 and S2 are both talking about “forest”. (2) The highest ranked sentences may be those ones with higher similarity to each other. In other words, it must provide an extra algorithm to remove redundancy.In order to address the above limitations, we attempt to integrate probabilistic topic cluster and lexical similarity of sentences and develop a sentence ranking approach for achieving both diversity and centrality on the hypergraph model. Recently, Yin, Pei, Zhang, and Huang (2012) and Hennig and Labor (2009) exploited Probabilistic Latent Semantic Analysis (PLSA) to represent documents as a mixture of topics and clustered sentences by their topic distribution. Mei et al. (2010) proposed a reinforced random walk algorithm in an information network. Inspired by these work, we propose a Hypergraph-based vertex-rEinforced Ranking Framework (denoted as HERF) for query-focused summarization. First, we get a word-topic distribution by the HDP model and then cluster sentences by a hybrid clustering method in which it measures similarity based on word-topic distribution. Second, we build a hypergraph to capture both topical cluster relationship and pairwise cosine similarity of sentences. Then a sentence ranking approach, which balances the centrality and the diversity of the sentences by a vertex-reinforced strategy, is developed for scoring sentences. In practice, enforcing diversity in summarization can effectively reduce redundancy among the sentences. i.e., two sentences providing similar information should not be both present in the summary. Finally, a naive sentence selection and redundancy removal strategy is used to generate a summary. The experiments showed that our HERF framework performs better than other baseline summarizers on widely used benchmark datasets.Two basic issues addressed in this paper are: (1) how to cluster semantically similar sentences into one topic even if they share few common words and (2) how to integrate redundancy removing policy into a hypergraph-based sentence ranking algorithm. Then, the main contributions of our work are summarized as follows.•We proposed the HERF framework in which an adaptive vertex reinforcement random walk process is used to model the query similarity, the centrality and the diversity of sentences in hypergraph based model.We propose a hybrid method to construct a hypergraph to integrate topic distribution and word co-occurrence of sentences.To verify the effectiveness of our framework, we implement and evaluate our proposed framework HERF over widely used benchmark datasets, empirically verifying improvement over similar methods and systems.The remainder of this paper is organized as follows. In Section 2, we describe our proposed summarization framework and details of constructing hypergraph, ranking sentences and generating summary. Section 3 gives the experiments and results. Section 4 briefly reviews the related work on graph/hypergraph based summarization. Finally, Section 5 concludes the paper.In this section, we discuss our summarization framework which consists of four crucial components, as shown in Fig. 1. In HERF, we first cluster sentences using a HDP-based approach based on sentence topic similarity. The topic similarity is calculated by the transformed radius (TR) based on the KL divergence, and KL divergence is based on word-topic probability distribution learned by the HDP topic model. We then construct a hypergraph based on sentence clusters as well as pairwise relationship between sentences. Then, we score sentences with a vertex-reinforced ranking approach which considers both the topic sensitivity and the diversity of sentences. Finally, the summary generation follows a greedy approach for selecting ranked sentences.In state-of-the-art methods, one usually estimates the topic distribution of sentence and cluster sentences into the topic which has the highest probability among its topic distribution. In our method, we use the whole topic distribution but not the main topic of sentence as a similarity measure to group sentences. And we argue that only using topic-based similarity is not enough for selecting important sentences. So, we simultaneously compute similarity of sentences by their cosine distance and integrate both of them into hypergraph. In our work, the topic model HDP (Teh, Jordan, Beal, & Blei, 2006) is employed to represent document with a mixture of topics. We choose HDP but not LDA (Blei, Ng, & Jordan, 2003) because HDP could automatically determine the number of topics and LDA requires choosing the number of topics with cross validation or held-out likelihood. As shown in Eq. (1), a global random probability measure G0 is distributed according to a Dirichlet process (DP) with γ the concentration parameter and H the base probability measure. A probability measure Gdin Eq. (2) is a set of random measures for each document group d, one for each group j. It is drawn from another DP with the concentration parameter α0 and the base probability measure G0.(1)G0|γ,H∼DP(γ,H)(2)Gd|α0,G0∼DP(α0,G0)In the HDP model, the observed words in each document are considered as a product of a document-specific mixture of latent topics in corpus wide. We define a word vectorW=w1,w2,w3,…,wl,l is the length of corpus. The vectorz=z1,z2,…,zldefines the hidden topic of each word in corpus and the HDP automatically decides the total number of topics. After being processed by the HDP model, each word is assigned a topic tag, but this topic tag could not be used to cluster sentences because each sentence is associated with more than one topic (word). Therefore, in our framework, each sentence is represented as a vectors=zw1,zw2,…,zwnon a latent topic set z, where n is the length of the sentence. Then, the probability that the sentence belongs to the topic ziis represented asp=mi/n,where miis the number of words in topic zi. If there are two sentences x and y, we denote sentence-topic probability distribution as vectorsPx=px(z1),px(z2),…,px(zk)andPy=py(z1),py(z2),…,py(zk),where k is the number of the topics. The Kullback–Liebler (KL) divergence is a better method to assess the similarity between two distributions, but KL divergence is not symmetric. So, like Yin et al. (2012), we use the transformed radius (TR) based on the KL divergence to measure the sentence similarity between Pxand Py:(3)TR(Px,Py)=DKL(Px∥Px+Py2)+DKL(Py∥Px+Py2)whereDKL(P∥Q)=∑ilog(P(i)/Q(i))P(i). Then the similarity is:(4)Sim(Px,Py)=10−TR(Px,Py)Over the similarity, we cluster sentences using a modified DBSCAN (Density Based Spatial Clustering of Applications with Noise) algorithm proposed by Wang et al. (2013). The DBSCAN (Ester, Kriegel, Sander, & Xu, 1996) automatically determines the cluster number and filters noise nodes, this is the reason why we choose DBSCAN in our framework. There are two parameters, Eps defines the search radius and MinPts defines the least nodes number in a cluster. In our experiments, we empirically set MinPts as 3 and Eps as 0.0005. As shown in Algorithm 1, the modified DBSCAN automatically tunes the Eps value to get a reasonable cluster set.A hypergraph (Berge, 1984) is a generalization of a simple graph where the hyperedges (called edges in simple graph) could contain any number of vertices. This property makes it represent a set of vertices in a hyperedge when all the members of the set have a group relationship. Let G(V, E) be a hypergraph where V denotes the vertex set and E denotes the hyperedge set. A hyperedge e is a subset of V where⋃e∈E=V. A weighted hypergraph is denoted by G(V, E, w) where w(e) called the weight of hyperedge e. A hyperedge e is said to be incident on v when v ∈ e. The incidence matrix of hypergraph is represented by a |V| × |E| matrix H with entries as follows:(5)h(v,e)={1ifv∈e0ifv∉e.The degree of vertex and hyperedge are defined as follows:(6)d(v)=∑e∈Ew(e)h(v,e)(7)δ(e)=∑v∈Vh(v,e)=|e|Let De and Dv denote the diagonal matrices which contain the degree of vertex and hyperedge respectively, and W represents the diagonal matrix with the hyperedge weights.We construct hypergraph using a new way which can put semantically similar sentences into the same group. Our method is different from Wang et al. (2013) and Wang et al. (2009). Comparing with the two work, our work has two main contributions: (1) A new hypergraph construction method. We cluster sentences based on the word-topic distribution while they cluster sentences based on the cosine similarity among sentences. Our method can group semantically similar sentences into one cluster if they share few common words. (2) When selecting sentences, we provide a vertex reinforcement random walk process to rank sentences while they use a scoring function to rank sentences. Our method is concentrated on both centrality and diversity of sentences simultaneous by using a modified time-variant random walk process while they focused on centrality of sentences (as discussed in Section 2.3). In Section 3, we have compared our method with them by experiment and the results show the efficient of our method.In our framework, we cluster sentences based on the word-topic distribution. After getting the clusters, the thing left to do is construct a sentence hypergraph. In our hypergraph model, each vertex represents a sentence, and the hyperedge captures two types of relationships among vertices which present sentences: (1) pairwise similarity; (2) topic cluster among some vertices. We still use cosine similarity between two sentences to denote the weight of hyperedge in the first type. For the second type, the hyperedge contains all the vertices in the same cluster and the weight of the hyperedge is the cosine similarity between a virtual documentC=s1,s2,…,siand the document D multiplying by a factor a which tunes the weight of a cluster-based hyperedge, where sidenotes the ith sentence in the cluster. The process of hypergraph construction is summarized as Algorithm 2.Most ranking algorithms are based on random walk on graph. A random walk is a transition process in which the walker moves from one vertex to another connected vertex after each discrete time step t. The process is assumed to be a Markov chain M over a state sets=s1,s2,…,sn. Each state sicorresponds to a vertex v in the graph G, and the transition probability is defined as p(u, v). As shown in Zhou, Huang, and Schölkopf (2007), after choosing a hyperedge e which incidents with the current vertex u, the surfer has to pick a vertex v ∈ e uniformly at random because hyperedges involve more than two vertices in hypergraph. For example, a specific path is shown with curved arrows in Fig. 1. For the beginning vertex SN, the surfer chooses cluster-based path hyperedge2 which connected three vertices SN, S4 and S5, and then the surfer picks S4. For the second vertex S4, the surfer chooses a normal pairwise hyperedge between S4 and S3, so the surfer moves to S3, and so on. The final path is {SN−hyperedge2−S4−e(S4,S3)−S3−hyperedge1−S1−e(S1,S6)−S6−e(S6,S2)−S2−e(S2,S7)−S7−e(S7,S5)−S5}. The transition probability is the key to the random walk algorithm. On hypergraph, the common transition probability is(8)p(u,v)=∑e∈ɛ(u)w(e)h(u,e)h(v,e)d(u)δ(e)where ε(u) is the set of hyperedge incident to u. Then, the transition probability matrix is denoted byP=Dv−1HWDe−1HT. Under a standard random walk process, the probability that the surfer at vertex u at time T is defined as(9)QT(v)=∑(u,v)∈Ep(u,v)QT−1(u)where E is all the hyperedges that contain the vertex v. The probability QT(v) stops changing after n steps when the random walk process reach the steady state. We use π(v) to denote the stationary distribution which measures the importance of vertices.In most classic random walk algorithms, the corresponding Markov chain is time-homogeneous, the transition probability does not vary over the time. The importance of vertex, which is calculated using those algorithms, is concentrated on centrality but ignores diversity. For example, if we use those algorithms to rank sentences for summarization task, the top n sentences might focus on a few topics but not a well overview of the document. For achieving diversity in a random walk, Mei et al. (2010) proposed a time-variant random walk process DivRank. It balances prestige and diversity simultaneously by vertex-reinforcement. The iteration process of DivRank is described as follows:(10)QT(v)=(1−λ)p*(v)+λ∑u∈Vp(u,v)·NT(v)DT(u)QT−1(u)where QT(v) denotes the DivRank score of vertex v at time step T and p*(v) is the prior value of vertex v. p(u, v) is the transition probability from vertex u to v and NT(v) represents the number of times the surfer has visited v up to time T andDT(u)=∑v∈Vp(u,v)NT(v).In our hypergraph model, we propose a modified algorithm to derive the importance of sentences. In Section 3, we compared our algorithm with the original DivRank in Mei et al. (2010), the experiment results verify the performance of our method. Specifically, our algorithm firstly realizes prior value as the query similarity which naturally fits the query-focused summarization task. Secondly, we useScoreT−1(sj),which is the ranking score of sentence siup to time T, as the reinforced coefficient but not visited times. The motivation is, in our model, those sentences which have a high similarity with query may get a larger ranking score although they have a less visited times. So our iteration process for sentences ranking is defined as Eq. (11),(11)ScoreT(si)=(1−λ)Sim(query,si)+λ∑j:j≠ip0(sj,si)·ScoreT−1(si)DT(sj)ScoreT−1(sj)where ScoreT(si) is the score of sentence siuntil time T, and Sim(query, si) is the cosine similarity between query and sentence si. The parameter λ is used to control the ratios of query similarity, and we will discuss its effect in Section 3.3. p0(sj, si) is the probability from sentence sjto siwhich is calculated as follows:(12)p0(u,v)={μ·p(u,v)ifu≠v1−μifu=v.The transition factor μ is a probability that the surfer moves to another vertex. So1−μis the probability of staying in the current vertex and the current vertex is reinforced. And(13)DT(sj)=∑si∈Vp0(u,v)ScoreT−1(si)ScoreT−1(si)is the approximate value of NT(si), the theoretical analysis refers to Mei et al. (2010) for details.In our method, we use the query similarity as the reinforcement for each vertex when random walking. As shown in Eq. (11), the second term is used to ensure the diversity, it denotes the weighted value transferred from other adjacent sentences and we use the score at last time as the weight coefficient. When some sentences express similar information, they will have much the same adjacent sentences in hypergraph as well as transferred value. This term embodied the thought that rich nodes get richer over time and ”absorb” the scores of its neighbors. At the beginning of random walk, some sentences have ranking scores with little difference. When the random walk process has converged to a stationary distribution, these sentences will have different ranking scores. Therefore, only the sentence with the highest score can be present in the summary.After our ranking process, the importance score of a sentence automatically balances diversity and centrality. However, there are some paragraphs and/or sentences in different documents may describe the same contents. So, there is a requirement that the similarity of any two sentences in summary does not exceed a threshold k, and we set k as 0.3 in our experiments. At last, the summary generation algorithm is shown in Algorithm 3.

@&#CONCLUSIONS@&#
In this study, we propose a Hypergraph-based vertex rEinforcement Ranking Framework (HERF) for multi-document summarization, which tries to integrate multiple important factors for sentence ranking. To do that, we propose a hypergraph construction method using HDP-based topic relatedness, and adapt a vertex reinforcement random walk process to model the query similarity, the centrality and the diversity of sentences when ranking sentences. Compared with other topic model, HDP can estimate the number of topics automatically. Experiments are conducted to verify the effectiveness of our framework.Under the assumption that a given document set usually covers several topic aspects, each sentence may describe one or more topics, in this paper we mainly consider the topic distribution relationship as well as the word co-occurrence relationship. If relaxing this assumption by considering more relations among sentences, we can achieve better performance. Our HDP-based sentences topic clustering only consider the topic distribution relationship among sentences. When more relations among sentences are discovered, it must provide a new clustering method to incorporate other relations. In our framework, we employ a naive sentence selection strategy to generate a summary. A sophisticated generation method based on our ranked sentences may be an effective complement. Our HDP-based sentences topic clustering is a semantical grouping, another semantic representation method, distributed representation, may be an alternative method for sentences clustering. In the future, we will attempt to do some work in the above aspects.