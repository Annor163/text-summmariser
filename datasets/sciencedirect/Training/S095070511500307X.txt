@&#MAIN-TITLE@&#
A priori trust inference with context-aware stereotypical deep learning

@&#HIGHLIGHTS@&#
We have proposed CAST, a new context-aware stereotypical trust model.We have considered a comprehensive set of seven context-aware stereotypes.We have applied a deep learning architecture to keep trust stereotyping robust.We have confirmed the effectiveness of CAST using a rich set of experiments.

@&#KEYPHRASES@&#
Multi-agent systems,Stereotypical trust model,Deep learning,

@&#ABSTRACT@&#
In multi-agent systems, stereotypical trust models are widely used to bootstrap a priori trust in case historical trust evidences are unavailable. These models can work well if and only if malicious agents share some common features (i.e., stereotypes) in their profiles and these features can be detected. However, this condition may not hold for all the adversarial scenarios. Smart attackers can show different trustworthiness to different agents and services (i.e., launching context-correlated attacks). In this paper, we propose CAST, a novel Context-Aware Stereotypical Trust deep learning framework. CAST coins a comprehensive set of seven context-aware stereotypes, each of which can capture a unique type of context-correlated attacks, as well as a deep learning architecture to keep the trust stereotyping robust (i.e., resist training errors). The basic idea is to construct a multi-layer perceptive structure to learn the latent correlations between context-aware stereotypes and the trustworthiness, and thus can estimate the new trust by taking into account the context information. We have evaluated CAST using a rich set of experiments over a simulated multi-agent system. The experimental results have successfully confirmed that, our CAST can achieve approximately tens of times higher trust inference accuracy in average than the competing algorithms in the presence of context-correlated attacks, and more importantly can maintain a much better trust inference robustness against stereotyping errors.

@&#INTRODUCTION@&#
In multi-agent systems, trust is a vital element and can be used to ensure the security and quality of service. Each agent (i.e., a trustor) can assign a trust value to another (i.e., trustees) based on the trustor’s direct experiences on the trustee in the past (i.e. direct historical trust evidences). The trust can be later used to evaluate the possibility whether the trustee will perform particular services as expected by the trustor in the future [1]. That is, trust is a subjective concept and is with respect to a trustor agent, a trustee agent and a particular service the trustor expects the trustee to perform. All of the three constitute a trust context. In general, the trust is established through historical evidences in one context and can only be reused to evaluate future behaviors within the same context [1–5].When direct historical evidences are not available in a context, bootstrapping a priori trust for such context is quite challenging. In open and dynamic multi-agent systems, new agents and services may join at any time [5]. These newcomers are necessarily suffering from evidence unavailability. To address this issue, a typical solution is to respect trust recommendations from other trustor agents (i.e., advisors) [6], which is known as a reputation method. However, this method cannot work well in case the trustee is a newcomer (i.e., all the agents do not have experiences on the newcomer and thus cannot do any recommendations). To bridge this gap, stereotypical trust modelling has been proposed [7,5]. The idea is to “borrows” the trustworthiness from similar trustee agents through their visible features (i.e., stereotypes), such as the agents’ geographical location/timezone [7] or their organization and performance indices [5].Although the stereotypical trust modelling can bootstrap a priori trust for the contexts with new trustees, they cannot cover all the initial cases (e.g., both the trustor and the trustee are newcomers or the service is new), and even worse cannot make an accurate and robust a priori trust inference in the presence of sophisticated adversarial scenarios. In particular, most of the existing stereotypical algorithms can only model stereotypes for the trustees [7,5,8–10]. They have not considered which services the trustees offer and which trustors the trustees serve, and thus fail to capture the malicious patterns when the attackers use different trustworthiness to serve different trustors with different services (i.e., performing context-correlated attacks). Moreover, existing stereotypical trust models have not well addressed the robustness problem (i.e., trust accuracy against stereotyping errors), hence being very difficult to be applied to practical scenarios (in practice, benign agents may set incorrect features due to ignorance or mistakes; adversarial agents may display false features for some malicious purpose, or attack the trust management system itself and thus mislead the system giving high level of trustworthiness to malicious agents [11]).We list the main research problems we have to conquer in this paper as follows.1.How to avoid context-correlated attacks when inferring trust using stereotypical models?How to maintain the robustness of stereotypical trust modelling in the presence of stereotype errors?To solve the two aforementioned problems, we propose CAST, a new Context-Aware Stereotypical Trust deep learning architecture to extend existing solutions to a complete framework. In contrast to existing stereotypical trust models which only consider stereotypes (visible features) for trustees, we consider stereotypes from context’s perspective. That is, given a target context, we generate context-aware stereotypes from other contexts where either the trustor or the trustee or the service or a group of them is different. We have seven this kind of stereotypes in total and list them in Fig. 1, where each circle (i.e.,ni=1,2,3,4) represents an agent, each rectanglesk=1,2is a service and each arrow line indicates a trust value. As can be seen, ifn2attempts to estimate a priori trust forn1with respect tos1, then2can consult the posteriori trustn2has already modelled for a different services2(case ①) or agentn3(case ②) or a different pair (case ④), or the posteriori trust another agentn3/4has modelled for the same target (case ③) or a different service (case ⑤) or agent (case ⑥) or the pair (case ⑦), through the visible features from different agents and services.CAST is novel in two aspects. First, CAST can be considered as a generalization and extension of previous research, and can cover a comprehensive set of initial cases. For example, existing trustee stereotype is just the case ②, while the reputation method could be covered by the case ③or ④(in case the stereotypical information is ignored). Second, each of the seven stereotypes in our CAST can capture a unique kind of malicious patterns that are correlated to the context. For example, if a malicious trustee treats different trustors differently (i.e., discrimination attack), such patterns can be recognized by the context-aware stereotype ③in Fig. 1. But if the malicious one provides different services with different qualities (i.e., service selection attack), the stereotype ①plays the key role. Moreover, if an attacker combines the two, we should rely on ⑤for detection.In the design of CAST, we apply a deep learning architecture [12] for trust stereotyping. The deep model deploys a multi-layer perceptive structure [13,14] to mimic a human’s perception and decision making process [15]. It can be trained by learning context-aware stereotypes from the contexts with enough direct historical evidences (i.e., evidence-dense contexts or labelled samples), and later used to estimate a priori trust for the contexts without enough direct historical evidences (i.e., evidence-sparse contexts or test cases). We choose the deep model here since such model can maintain sufficient robustness against training errors. Even if some evidence-dense contexts involve inaccurate or incorrect stereotype information, the deep model can still work well for trust stereotyping.We summarize the key research contributions we have made in this paper as follows.1.We have taken into account a comprehensive set of seven context-aware stereotypes for stereotypical trust modelling, and hence being able to avoid context-correlated attacks.We have applied a deep learning architecture to maintain robustness for stereotypical trust modelling.The remainder of this paper is organized as follows. We first review related works and point out the novelty of our work in Section 2. We then design CAST with context-aware stereotypes in Section 3 and the deep model in Section 4. After evaluating CAST in Section 5, we conclude the paper in Section 6.In this section, we review state-of-the-art a priori trust inference methods in the literature. We discuss the open problems of existing methods and thereby motivate the new design.In general, a priori trust inference is required when direct historical trust evidences are lacking (here, we do not consider the trust from social relationships [16,17] and the pre-trusted third parties [18]). One of the most popular solutions is the reputation methods [19–23], by which the trustor agent will respect the recommendations from some trustworthy advisors to build up initial trust for the same trustee agent with the same service. Although these reputation methods may have some minor differences in their design (e.g., the works [19,21] may consider the trust is transitive and some others [22] may not), they share the basic idea: trustor agents can infer the trustworthiness for a given target by consulting some trustworthy advisors who have direct experiences to the target.Although the reputation methods have dominated the research domain for more than two decades, they all cannot bootstrap the trust when the trustees are newcomers. This issue has not been resolved until stereotypical trust models [7,5,24,8–10] appeared. On the one hand, StereoTrust [7] is perhaps the first work that introduced stereotypical models for a priori trust inference (case ②in Fig. 1). It proposed a grouping method to build a membership function for trustees’ visible features and then used this function to calculate the trustworthiness for the new trustees. Fang et al. [8] generalized StereoTrust with fuzzy theory. On the other hand, Burnett et al. [5,9] applied a classification and regression tree (e.g., M5 tree) to learn from existing trustees through their visible features and then used the tree to infer a priori trust for the new ones (case ②in Fig. 1). Although Burnett et al. have realized that trustee stereotypical models cannot work if the trustors are newcomers, and have designed a stereotypical reputation method (partially case ⑥in Fig. 1)) to address this issue, they have still overlooked that the trustors and the advisors may be treated differently by the trustees even if the advisors are trustworthy enough. Moreover, the work [24] has discussed how to discover more stereotypical sources to model trust in multi-agent systems and pointed out the importance of stereotypes (i.e., visible features) from the trustors (case ③in Fig. 1). However, that work [24] has not designed any practical stereotyping algorithms for trustors. In the most recent research, Sensoy et al. [10] proposed a graph extraction algorithm to mine a rich set of available features for stereotypical trust modelling. But unfortunately, their mining method is still restricted to trustees’ profiles (case ②in Fig. 1).In contrast to state-of-the-art stereotypical trust models [7,5,24,8–10], our CAST goes beyond in two aspects. One is the use of context-aware stereotypes, and the other is learning the context-aware stereotypes using a deep model. More precisely, CAST extends the available sources of stereotypes from trustees to trust contexts which consist of the trustors, the trustees and the services. With this extension, our CAST can naturally generalize existing reputation methods and stereotypical models to a complete framework and thereby is able to detect context-correlated attacks (e.g., discrimination attack, service selection attack or both). CAST also applies a deep architecture [14] for stereotypical trust modelling. As one of the most promising artificial intelligent techniques, the deep model has been proved effective in a broad set of application fields, including but not limited to image and video processing [25], speech recognition [26] and natural language processing [27] and so on. Following these successful experiences, we introduce the deep model, specifically the deep belief network [12], to solve the stereotypical trust inference problem for the first time.The deep model is a good approximation of human’s neocortex [13,14], while the stereotypical trust inference is a typical human perception and decision making process [28,15]. The use of a deep model to solve the stereotypical trust inference problem is nature and meaningful. Moreover and more importantly, our deep model can outperform the grouping method [7] and the classification tree [5,9], because it can abstract the stereotypes in a perceptive manner layer by layer and thus can better represent the latent joint distribution between the stereotypes and trustworthiness, even if the training set (i.e., evidence-dense contexts) contain incorrect and/or inaccurate stereotypical trustworthiness samples. In another word, the deep model can achieve a more robust trust stereotyping than existing algorithms. Note that, we have confirmed this finding through carefully designed experiments in Section 5.4.In this paper, we model a multi-agent system as a double bipartite graphG=(N,N,S,N×N×S), where N is the set of agents in the system and S is the set of services. In this graph, each edge(ni,nj,sk)∈N×N×Sdenotes a unique context for trust modelling. It can be interpreted as an agentni∈Nmodels the trustworthiness of another agentnj∈Nwith respect to a servicesk∈S.Given a context(ni,nj,sk), we model the trust associated to this context using subjective logic [2], which enables agents to express the trustworthiness as degrees of a belieftj:ki, a disbeliefdj:ki, an uncertaintyuj:kiand a base rateaj:ki(a priori degree of belief) about the context. We write it as(tj:ki,dj:ki,uj:ki,aj:ki), wheretj:ki+dj:ki+uj:ki=1andaj:ki∈[0,1]. We define a direct trust evidence regarding to a context(ni,nj,sk)as a binary event of whether the agentnihas successfully run the serviceskin another agentnjas expected. If this is true, the evidence is positive. Otherwise, it is negative. A sequence of independent binary events can be modelled as a Bernoulli experiment, and the posterior distribution of this experiment can be approximated as a beta distributionBeta(α,β)[2]. Letrj:kibe the number of positive historical evidences observed in the context(ni,nj,sk)andsj:kibe the number of negative ones. We haveα=rj:ki+1,β=sj:ki+1and thus can calculate the belief (i.e., posterior trustworthiness), the disbelief and the uncertainty using Eq. (1).(1)tj:ki=rj:kirj:ki+sj:ki+2,dj:ki=sj:kirj:ki+sj:ki+2,uj:ki=2rj:ki+sj:ki+2.In Eq. (1), ifrj:ki+sj:kiis small, the uncertaintyuj:kicould be quite large and the resultingtj:ki(anddj:ki) are not with enough confidences. In this case, the trustworthiness can be corrected using the base rateaj:ki, which is a priori trust value purely inferred without any historical trust evidences. The correction function ist^j:ki=tj:ki+aj:ki·uj:ki[2]. Whenrj:ki+sj:kiis large enough, the trustworthiness can be mainly determined bytj:ki. But ifrj:ki+sj:kiis too small,aj:kibecomes the dominant factor. In this paper, our ultimate goal is to inferaj:kifor the context whererj:ki+sj:ki⩽thby learning the availabletj:kivalues from the context whererj:ki+sj:ki>th. In the following of this paper, we will regard the contexts withrj:ki+sj:ki⩽thas evidence-sparse contexts while the ones withrj:ki+sj:ki>thas evidence-dense contexts. The th parameter is a threshold we should choose to classify whether a context is evidence dense or sparse. Normally and simply, we chooseth=0for this paper.It is known that we cannot calculate the trustworthiness for evidence-sparse contexts using the Eq. (1) directly. Instead, we should infertj:ki≃aj:kiwith the help of other evidence-dense contexts. The basic idea is to learn a supervised model from evidence-dense contexts to capture the latent correlations between the trustworthiness and visible features, and then use this model to infer a priori trust for the evidence-sparse contexts. Since visible features play the role as a mould to convey trust information, we also call them stereotypes. In this paper, our innovative design is to consider stereotypes with respect to the trust contexts. We let Fn and Fs be the set of visible features (i.e., stereotypes) for the agents and services, respectively.Given an evidence-sparse context(ni,nj,sk), we can group available evidence-dense contexts into seven categories by considering that theni(trustor) ornj(trustee) orsk(service) or multiple of them are different. We define the set of available evidence-dense/sparse contexts asCd/Cs, respectively:(2)Cd≜{(ni,nj,sk)∈N×N×S,rj:ki+sj:ki>th}Cs≜{(ni,nj,sk)∈N×N×S,rj:ki+sj:ki⩽th}Obviously,Cd∩Cs=∅andCd∪Cs=N×N×S. Therefore, given a(ni,nj,sk)∈Cs, we can classifyCdinto seven subsetsCd=⋃l=17Cdl(ni,nj,sk)(see Fig. 1) as:(3)Cd1(ni,nj,sk)≜{(ni,nj,sk′)∈Cd,sk′≠sk}Cd2(ni,nj,sk)≜{(ni,nj′,sk)∈Cd,nj′≠nj}Cd3(ni,nj,sk)≜{(ni′,nj,sk)∈Cd,ni′≠ni}Cd4(ni,nj,sk)≜{(ni,nj′,sk′)∈Cd,nj′≠nj,sk′≠sk}Cd5(ni,nj,sk)≜{(ni′,nj,sk′)∈Cd,ni′≠ni,sk′≠sk}Cd6(ni,nj,sk)≜{(ni′,nj′,sk)∈Cd,ni′≠ni,nj′≠nj}Cd7(ni,nj,sk)≜{(ni′,nj′,sk′)∈Cd,ni′≠ni,nj′≠nj,sk′≠sk}If we infertj:kiusingCd1, the model input in the training phase should be(tj:k′i,V(Fsk′))wheretj:k′iis the label andV(Fsk′)is the value vector ofsk′’s features. But if we considerCd7, the model input can be extended to(tj′:k′i′,V(Fni′×Fnj′×Fsk′))wheretj′:k′i′is the label andV(Fni′×Fnj′×Fsk′)is the value vector of the combination of the features fromni′,nj′andsk′. We defineF7≜Fni′×Fnj′×Fsk′as a set of context-aware stereotypes forCd7. By this way, we can also define context-aware stereotype sets for the other sixCdls as follows:(4)F1≜Fsk′,F2≜Fnj′,F3≜Fni′F4≜Fnj′×Fsk′,F5≜Fni′×Fsk′,F6≜Fni′×Fnj′In this paper, we consider a homogeneous multi-agent system, in which each agent (and each service) has the same feature set, that isFni′=Fnj′=FnandFsk′=Fs.We differentiate the seven context-aware stereotypes in our design, because they can preserve more precise discriminative information to correlate the visible profiles of agents and services with the potential trustworthiness. By this way, our solution can detect not only the malicious patterns shown in trustee’s profile but also the smart adversaries who hide their patterns among trustors, trustees and the services (i.e., context-correlated attacks).It is worth noting that, if we infer a priori trust based on evidence-dense contexts fromCd3,Cd5,Cd6andCd7, the agentni′plays the role as an advisor. Our method cannot detect malicious advisors when they recommend honest agents as attackers or recommend attackers as honest ones. To cope with this challenge, we need a set of trustworthy advisors, denoted asR⊆N, in the system. We then restrictni′∈Rto refine the definition ofCd3,Cd5,Cd6andCd7. Many existing trust models, such as [20,23], can accurately model the trustworthiness of advisors from which R can be chosen.To infer a priori trust for a given evidence-sparse context, we can train seven individual inference models based on the seven different kinds of evidence-dense contexts. In each training phase, the trustworthiness (from evidence-dense contexts) is the label and the value vector of context-aware stereotypes are the model inputs. We then use the seven models one by one to infer the trustworthiness of the target evidence-sparse context. We select the minimal value of outputs as the final result, because the lower value reported from the model onCdlindicates that some malicious patterns are more likely being captured inCdl(in caseni′∈R). We list the details in Algorithm 1. As can be seen, the while loop (from line 3 to line 7) traversals all the seven kinds of evidence-dense contexts given a target evidence-sparse context. Inside the loop, the code in line 4 is used to train an inference model using one kind of context-aware stereotype samples, while the code in line 5 is to use the trained model to infer a new trust for the target, and meanwhile keep the smallest inferred trust as the final output. It is worth noting that, we implement the InferModel in the algorithm using a deep architecture [12] in this paper.Algorithm 1Context-aware stereotypical trust inference algorithmAs a key step, a priori trust inference algorithm should implement a function (i.e., the InferModel in Algorithm 1) to map context-aware stereotypes to trustworthiness. In this paper, we choose the deep learning model, particularly the deep belief network (DBN) [12], for this task due to two reasons. First, DBN is a probability generative model without any a priori assumption. It can learn the empirical samples layer by layer and eventually can discover more latent informative probabilities which can hardly be achieved by other learning algorithms (such as the group methods and the classification tree). Second, despite the DBN requires the labelled samples to fine tune the whole model, it has a pre-training phase which is aimed to reconstruct layer-wise connections using restricted Boltzmann machine (RBM). RBM is built with Gibbs sampling in order to utilize unlabeled samples for the training. The Gibbs sampling is a typical Markov chain Monte Carlo algorithm which can obtain a sequence of samples from a specified and practical sample set [29]. This algorithm is able to re-construct a target sample set (e.g., the training set) to approximate the real distribution, and can therefore lead the deep model to a much better robustness against sampling errors in the training set (i.e., the set of evidence-dense contexts).In general, a deep belief network (DBN) consists of an input layerH0in the bottom, several hidden layersHnin the middle and a label layer L on the top. Each of two adjacent layers are fully interconnected, while the units in the same layer are disconnected. By this design, the units in each layer can independently interact with the units in their adjacent layers. This architecture can well simulate the physical structure of the human’s neocortex.In this paper, we design a new stereotypical DBN model to suit our trust inference problem. In particular, we build a meaningful deep architecture with three hidden layers, each of which aims to take a particular task from initial impressions to the final decision-making. We choose three hidden layers, because such design can well reason about the human’s brain and has many successful applications in solving real-world problems [12,30]. We present the overview of our architecture in Fig. 2.In this model, we use the first hidden layerH1to simulate a human’s “initial impression” about the stereotypes. That is, given an object, a human will quickly skim all the visible features of this object in order to establish a first-hand impression. We deploy|Fl|, the number of available stereotypes inFl, hidden units in this layer. If we takeFl=7as an example, we have|F7|=x2·ywherex=|Fn|is the number of visible features associated to each agent andy=|Fs|is the number of visible features for each service.For the second layerH2, we use it to mimic a human selecting significant stereotypes for decision-making based on his/her logical thinking. Although there may exist a numerous stereotypes associated to a trust context, only a few are significant and effective to characterize the context. This phenomenon follows real-life scenarios, where humans can abstract significant features to make a rational decision. As a result, we setσ·|Fl|,0⩽σ<1hidden units in the layerH2. We can choose appropriate σ based on our evaluations in Section 5.2.We employ the last hidden layerH3to mimic a human making a decision to rate the context. It is well known that humans cannot provide fine-grained ratings. For example, many online rating systems (such as Epinions and Amazon) restrict their users to rate products in 10 levels (from 1 star to 5 stars with each step in 0.5 star). Moreover, to mimic humans’ divergent thinking to some extent, we use 10 hidden nodes to represent each trust level and thus involve 100 hidden nodes inH3.In multi-agent systems, the agents’ and services’ features (i.e., stereotypes) may have diverse value ranges and types. In order to input the values from different stereotypes, we should normalize them at first. As the deep model is designed to accept quantitative data only, we consider two typical quantitative data value types: discrete values and continuous values [31]. To normalize discrete-valued stereotype, we can directly convert them to a binary vector, where the size of the vector is the logarithm (based on 2) of possible values of this feature. For example, if a feature has 10 candidate values, the length of the vector is⌈log210⌉=4. In case the stereotype takes the 2nd candidate value, we can convert it to 0010. For the continuous-valued stereotypes, we can discrete them using a segmentation method. The segmentation criterion can be set according to the value distribution of each continuous-valued stereotype. To sum up, we formalize the input normalization process as follows. LetBfbe the binary vector of a context-aware stereotypef∈Flwhich is associated to a context(ni,nj,sk). When we build the deep model for a sample context(ni,nj,sk)∈Cdl, we can generate the inputB(Fl)for this context by concatenating the binary value vectorsBf,f∈Flof(ni,nj,sk)into one vector:B(Fl)=Concatf∈FlBf.For the label layer, we determine the number of units depending on how precise we expect the deep model can output. Given a context(ni,nj,sk)∈Cdl, the label is the trustworthiness (or belief)tj:ki∈[0,1]. As a result, if we expect the output precision is 0.1, we can use 10 units in the label layer. But if we attempt to get 0.01 precision, we need 100 units. In this paper, we choose the 100-unit opinion. Note that, sincetj:kiis a continuous value, we need to discrete it and convert it to a binary vector at first.We list the main learning steps of our stereotypical DBN model given aCdlas follows.Step 1We prepare the input binary vectorsB(Fl)for all the(ni,nj,sk)∈Cdl(or including(ni,nj,sk)∈Csto get more samples to rebuild the empirical distribution), and hence generate the empirical distribution of the input layer.After the structure of the lower layer is determined, we reconstruct the upper layer and the layer-wise connections between the two adjacent layers using a restricted Boltzmann machines.We repeat Step 2 until all the layer-wise connections are constructed.We fine-tune the whole deep model using the labelled samples(ni,nj,sk)withtj:kifromCdl.Given the input layerH0, our model can reconstruct the first hidden layerH1and the layer-wise connections betweenH0andH1using a restricted Boltzmann machine (RBM). We repeat this step to reconstructH2givenH1,H3givenH2, and L givenH3, and eventually build the whole deep model. The target of each layer-wise reconstruction is to find a set of optimal layer-wise parametersθ=(W,o,u)which can minimize the state energy for each layer pair in RBM [32]. We will show the details of RBM using the layer-wise reconstruction of the first hidden layerH1given the input layerH0as follows. The layer-wise reconstructions for other layer pairs are similar.Leth0=B(Fl)be the input binary vector ofH0andh1be the hidden unit vector ofH1. The layer-wise parameters of this layer pair isθ0=(W0,o0,u0), whereW0is the connection weight matrix between the layersH0andH1,o0is the biased vector forH0andu0is the biased vector forH1.Wij0∈W0represents the connection weight between the i-th unit inh0and the j-th unit inh1.oi0∈o0is the bias of the i-th unit inh0, anduj0∈u0is the bias of the j-th unit inh1.We can calculate the state energy of this layer pair as(5)E(h0,h1,θ0)=-(h1)′W0h0-(o0)′h0-(u0)′h1and then generate a layer-by-layer joint distribution for RBM as(6)P(h0,h1,θ0)=e-E(h0,h1,θ0)∑h0∑h1e-E(h0,h1,θ0)By factorizing the RBM’s joint distribution, we can get two RBM associated layer-to-layer conditionals:(7)P(hi0=1|h1)=s∑kWik0hk1+oi0P(hj1=1|h0)=s∑kWkj0hk0+uj0wheres(x)=11+exp(-x)and each unit value (i.e.,hi0andhj1) is a binary value. Using Eq. (7), we can eventually have the conditional probability distributionP(h0|h1)andP(h1|h0). With these two conditionals, we can apply Gibbs sampling method to sampleH1givenH0, and then sampleH0givenH1and then repeat the two steps t times. This process forms a Markov chain [33], which can generateh1(t)only depending onh1(t-1).h1(t)is the t-th state ofh1in the Markov chain and the train starts ath0(t=0).By using Gibbs sampling to reconstructH0andH1, the layer-wise parametersθ0should be optimized as well. The optimization goal is to minimize a so-called log-likelihoodlogP(h0). We haveP(h0)=∑h1P(h0,h1,θ0)and can write its log version (i.e., the log-likelihood ofP(h0)) as(8)logP(h0)=log∑h1e-E(h0,h1,θ0)-log∑h0∑h1e-E(h0,h1,θ0)We cannot minimizelogP(h0)by finding the optimalθ0directly, because the parameters depend on each other (i.e., the partial derivative oflogP(h0)with respect to one parameter contains other parameters). To tackle this challenge, we apply a gradient descent (GD) method with the line search. The GD method can calculate the derivative oflogP(h0)with respect to the parameterθ0=(W0,o0,u0)as(9)∂logP(h0(0))∂θ0=-∑h1(0)P(h1(0)|h0(0))∂E(h1(0)|h0(0))∂θ0+∑h1(t)∑h0(t)P(h1(t)|h0(t))∂E(h1(t)|h0(t))∂θ0To determine how long the Markov chain is deserved (i.e., determining the t), we refer to a contrastive divergence algorithm [34], which can take a small t (typically taket=1) to run the chain by referring a difference between two Kullback–Leibler divergences. The Kullback–Leibler divergence is a typical measure of the information loss when one probability distribution is used to approximate another [35]. The authors [34] chose the Kullback–Leibler divergence in their design in order to ensure the minimized information loss when the Markov chain is constructed. By doing so, we can calculate the derivative oflogP(h0)with respect to the parameterW0as(10)∂logP(h0(0))∂W0=〈h1(0)′h0(0)〉d-〈h1(1)′h0(1)〉mwhere〈·〉dis the expectation regarding to the data distribution, and〈·〉mis the expectation for the model distribution which is sampled by one step Gibbs sampling. We therefore can update the parameterW0as(11)W0(t=1)=εWW0(0)+ρW(〈h1(0)′h0(0)〉d-〈h1(1)′h0(1)〉m)whereεWis the momentum for smoothness and used here to mitigate overfitting.ρWis the learning rate ofW0(i.e., the step length of gradient descent forW0). Similarly, we can update the other two parameterso0andu0as(12)o0(t=1)=εoo0(0)+ρo(h0(0)-h0(1))u0(t=1)=εuu0(0)+ρu(h1(0)-h1(1))whereεoandεuare momentums.ρoandρuare the learning rate ofo0andu0respectively. Our model uses the same values for the momentums and learning rates as the typical deep learning method [32] used.As described in Section 4.2, the RBM reconstruction is an unsupervised learning progress and thus cannot utilize the labelled samples. To avoid this issue and make a full utilization of the labelled samples in our learning process, we implement a supervised mean-field posterior approximation algorithm through back-propagation [12]. This method can fine-tune the deep model with labelled samples (i.e., evidence-dense contexts). The tuning goal is to minimize the model error by further optimizing the parameterθ=(W,o,u)from the label layer back to the input layer (down-pass). The model error can be measured in terms of a cross-entropy between the probability distribution of the true trustworthiness and that of the inferred trustworthiness. The cross-entropy is a typical measure that can be used to calculate the average number of error bits when one distribution is used to approximate another [36]. In our CAST, we can calculate the cross-entropy as-∑tj:kilogt¯j:ki, wheretj:kiis the correct trustworthiness label of a context(ni,nj,sk)in the evidence-dense context training setCdl, andt¯j:kiis the output trustworthiness label when inputting the stereotypes of the evidence-dense context(ni,nj,sk)to the model.

@&#CONCLUSIONS@&#
