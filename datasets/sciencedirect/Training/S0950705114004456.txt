@&#MAIN-TITLE@&#
Support vector machine-based optimized decision threshold adjustment strategy for classifying imbalanced data

@&#HIGHLIGHTS@&#
We analyze the reason why SVM can be damaged by class imbalance in theory.We propose SVM-OTHR algorithm to find the optimal moving distance of hyperplane.We integrate SVM-OTHR into Bagging ensemble framework to promote its robustness.The time complexity of SVM-OTHR is merely a little higher than standard SVM.Two proposed algorithms often outperform some other bias correction algorithms.

@&#KEYPHRASES@&#
Class imbalance,Support vector machine,Decision threshold adjustment,Optimization search,Ensemble learning,

@&#ABSTRACT@&#
Class imbalance problem occurs when the number of training instances belonging to different classes are clearly different. In this scenario, many traditional classifiers often fail to provide excellent enough classification performance, i.e., the accuracy of the majority class is usually much higher than that of the minority class. In this article, we consider to deal with class imbalance problem by utilizing support vector machine (SVM) classifier with an optimized decision threshold adjustment strategy (SVM-OTHR), which answers a puzzled question: how far the classification hyperplane should be moved towards the majority class? Specifically, the proposed strategy is self-adapting and can find the optimal moving distance of the classification hyperplane according to the real distributions of training samples. Furthermore, we also extend the strategy to develop an ensemble version (EnSVM-OTHR) that can further improve the classification performance. Two proposed algorithms are both compared with many state-of-the-art classifiers on 30 skewed data sets acquired from Keel data set Repository by using two popular class imbalance evaluation metrics: F-measure and G-mean. The statistical results of the experiments indicate their superiority.

@&#INTRODUCTION@&#
In the past decade, the class imbalance problem has received considerable attention in several fields, such as artificial intelligence [1], machine learning [2] and data mining [3,4]. A data set is said to be imbalanced when and only when the instances of some classes are obviously much more than that in other classes. The problem is important due to it widely emerges in many real-world applications, including financial fraud detection [5], network intrusion detection [6], spam filtering [7], video monitoring [8], medical diagnosis [9], Bioinformatics [10], etc. Generally, in these applications, we are more interested in the pattern represented by the examples of the minority class. However, majority traditional classification algorithms pursuing the minimal training errors would heavily damage the recognition accuracy of the minority class, thus it is necessary to adopt some bias correction techniques before/after constructing a classifier.The bias correction techniques can be roughly divided into four major categories as follows:1.Resampling the original training set until all the classes are approximately equally represented. Resampling includes oversampling [11–13], undersampling [14,15] and hybrid sampling [16].Cost-sensitive learning, which is also called instances weighting method, assigns different weights for the training instances belonging to different classes so that the misclassification of the minority class can be highlighted [17–19].Moving the decision boundary (decision threshold adjustment) towards the majority class in order to remedy the bias caused by skewed sample distributions [20,21]. Unlike the other correction techniques, decision threshold adjustment strategy runs after modeling a classifier.Ensemble learning that provides a framework to incorporate resampling strategy, weighting strategy or decision threshold adjustment strategy, usually produces better and more balanced classification performance [22–29].Among those correction techniques mentioned above, decision threshold adjustment is regarded as a potential solution for dealing with class imbalance in recent studies [20,21]. However, the existing decision threshold adjustment approaches generally give the moving distance of classification boundary empirically, thus fail to answer a significant question: how far the classification hyperplane should be moved towards the majority class? This study solves this puzzle in the context of support vector machine (SVM) [30]. SVM is a robust classifier and is relatively insensitive to class imbalance in comparison with many other classification algorithms, because its classification hyperplane only associates with a few support vectors [31].In this paper, we first investigate the reason that the classification performance of SVM can be destroyed by skewed classification data in theory, and then we analyze the merits and drawbacks of some existing SVM-based bias correction techniques. Next, the computational formula of the moving distance in SVM-THR algorithm proposed by Lin and Chen [21] is intensively modified to lead to one optimized version (SVM-OTHR). Furthermore, we incorporate SVM-OTHR into Bagging ensemble learning framework and present a novel classification algorithm named EnSVM-OTHR. In particular, to avoid overfitting and to guarantee the diversity of different individuals, a small random perturbation term is inserted into each SVM-OTHR to disturb the final position of classification hyperplane. Finally, we compare the two proposed classification algorithms with many state-of-the-art imbalanced classifiers on 30 data sets acquired from Keel data set Repository via non-parametrical statistical testing [32,33], indicating their superiority.The rest of this paper is organized as follows. In Section 2, we introduce SVM theory and explain the reason that the performance of SVM can be damaged by imbalanced classification data. Section 3 briefly reviews some existing SVM-based class imbalance correction techniques and indicates their pros and cons. In Section 4, one optimized SVM decision threshold adjustment strategy (SVM-OTHR) and its extended version based on ensemble learning (EnSVM-OTHR) are described in detail. Experimental results and discussions are presented in Section 5. Finally in Section 6, the main contributions of this study are summarized.Support vector machine (SVM), which comes out of the theory of structure risk minimization, has several merits as follows: high generalization capability, absence of local minima and adaptation for high-dimension and small sample data [31,34].Given some training data D, a set of m points of the form:D={(xi,yi)|xi∈Rp,yi∈{-1,1}i=1m}, whereyiis either 1 or −1, indicating the class to which the pointxibelongs. Eachxiis one p-dimensional real vector. SVM is used to find the maximum margin hyperplane that divides the points havingyi=1from those havingyi=-1. The decision function of SVM is described as:(1)h(x)=〈w,ϕ(x)〉+bwhereϕ(x)represents a mapping of sample x from the input space to high-dimensional feature space,〈·,·〉denotes the dot product in the feature space,wdenotes the weight vector for learned decision hyperplane andbis the model bias. We can optimize the values of w and b by solving the following optimization problem:(2)minimize:g(w,ξ)=12‖w‖2+C∑i=1mξisubject to:yi(〈w,ϕ(xi)〉+b)⩾1-ξi,ξi⩾0whereξiis the ith slack variable and C is regularization parameter (penalty factor) which is used to regulate the relationship between training accuracy and generalization. Then the minimization problem in formula (2) can be transformed to a dual form and be rewritten as:(3)maximize:W(α)=∑i=1mαi-12∑i=1m∑j=1myiyjαiαjK(xi,xj)subject to:∑i=1myiαi=0,∀i:0⩽αi⩽Cwhereαiis the sample xi’s lagrange multiplier,K(·,·)is a kernel function that maps the input vectors into a suitable feature space:(4)K(xi,xj)=〈ϕ(xi),ϕ(xj)〉Some previous work has found that radial basis kernel function (RBF) generally provides better classification accuracy than many other kernel functions [31,34]. RBF kernel is presented as follows:(5)K(xi,xj)=exp-‖xi-xj‖22σ2whereσis the width of RBF kernel.Although previous work found that SVM is more robust to class imbalance than many other machine learning methods as its classification hyperplane only associates with a few support vectors, it can be still hurt by skewed class distributions to some extent. We try to analyze its reason in theory.After training an SVM classifier, lagrange multiplierαican be divided into three categories as follows:Case 1:αi=0, it means the instancexiis classified accurately.Case 2:0<αi<C, the corresponding instancexiis called a normal support vector which is exactly on one of the margin hyperplanes.Case 3:αi=C,xiis called a boundary support vector that lies between margins. The percentage of boundary support vectors reflects the error rate of SVM to some extent.SupposeN+andN-represent the number of instances belonging to the positive class (minority class) and the negative class (majority class), respectively.Nsv+andNsv-are the number of support vectors (including normal and boundary support vectors) in two classes, whileNboundary+andNboundary-represent the number of boundary support vectors in two classes, respectively. According to formula (3), we can get:(6)∑i=1mαi=∑yi=+1αi+∑yi=-1αi(7)∑yi=+1αi=∑yi=-1αiBecause αi’s value is C at most, it can deduce two inequalities as follows:(8)∑yi=+1αi⩾Nboundary+×C(9)∑yi=+1αi⩽Nsv+×CBy integrating formula (8) and (9), we get:(10)Nsv+×C⩾∑yi=+1αi⩾Nboundary+×CSimilarly, it is not difficult to get the following inequality:(11)Nsv-×C⩾∑yi=-1αi⩾Nboundary-×CSuppose∑yi=+1αi=∑yi=-1αi=M, if formula (10) and (11) respectively divide byN+×CandN-×C, we get:(12)Nsv+N+⩾MN+×C⩾Nboundary+N+(13)Nsv-N-⩾MN-×C⩾Nboundary-N-whereNboundary+N+andNboundary-N-approximately reflect the upper bounds of error rates in the minority class and the majority class, respectively. It is clear thatMN+×Cis larger thanMN-×CasN+is smaller thanN-. Therefore, it is easy to draw a conclusion that the error rate of the minority class is usually larger than that of the majority class. Based on the analysis above, we find that the minority class often sacrifices more in the process of modeling SVM classifier, thus the impartiality of SVM can be destroyed by class imbalance.In previous work, some class imbalance correction strategies for SVM classifier have been proposed, including resampling [11,13], weighting [17,19] and decision threshold adjustment [21].Resampling can be accomplished either by oversampling the minority class or undersampling the majority class. In fact, resampling repairs the difference betweenMN+×CandMN-×Cby either increasingN+or reducingN-. However, both techniques have their advantages and disadvantages. Oversampling makes the classifier overfitting and increases the time of modeling, while undersampling often causes information loss [35]. Random oversampling (ROS) and random undersampling (RUS) are the simplest resampling approaches [11]. Akbani et al. [13] found that combining SVM classifier and SMOTE oversampling method which proposed by Chawla et al. [12] can acquire better classification results than ROS and RUS.Different from resampling, the weighting strategy (CS-SVM) assigns different weights to the instances in different classes, where the weight reflects in the penalty factor C[17]. That means for the samples in the minority class, the penalty factor is assigned asC+, while the penalty factor in the majority class is assigned asC-. To guaranteeMN+×C+=MN-×C-, we should makeC+C-=N-N+, then the formula (2) can be rewritten as:(14)minimize:g(w,ξ)=12‖w‖2+C+∑yi=+1ξi+C-∑yi=-1ξisubject to:yi(〈w,ϕ(xi)〉+b)⩾1-ξi,ξi⩾0z-SVM can be regarded as another weighting strategy of SVM [19]. Unlike CS-SVM that assigns different penalty factors for different classes, z-SVM directly rewrites the final decision function (formula (1)) by assigning a larger weight for the positive class. According to Ref. [19], formula (1) can be rewritten as:(15)h(x)=z×〈w,ϕ(xp)〉+〈w,ϕ(xN)〉+bwhere xp and xN denote the positive and negative support vectors in one learned SVM model, and z is weighted factor for the positive class. In Ref. [19], G-mean is adopted as the evaluation measure to determine the optimal z value z∗. Meanwhile, as one univariate unconstrained optimization problem, golden section optimization technology is adopted into the optimization process of z∗.The third method is decision threshold adjustment that is used after classifier modeling. Actually, the decision threshold adjustment method improves the classification accuracy of the minority class by directly moving the classification hyperplane towards the majority class. Lin and Chen [21] suggest to use the following function to calculate the moving distance of decision threshold:(16)θ=N--N+N++N-+2For each test instance x, it can be divided into the minority class or the majority class by the adjusted decision function as follows:(17)h′(x)=h(x)+θwhere h(x) and h′(x) represent the original decision function and the adjusted decision function, respectively. They named this adjustment strategy as SVM-THR and found it often performs better than many other correction techniques by experiments on several Bioinformatics data sets. However, the formula (16) was given empirically and the authors failed to provide its theoretical foundation. Then a confusing question emerges: how to find the optimal moving distance for the decision threshold? In Section 4, we will exhibit an novel optimized method (SVM-OTHR) which may be a potential answer for the question above.Fig. 1gives the graphical representations of standard SVM, SVM-RUS, SVM-ROS, SVM-SMOTE, CS-SVM/z-SVM and SVM-THR, respectively. It can be seen that each correction technique can alleviate the affection of class imbalance more or less.To adjust decision threshold (classification boundary) to the optimal position, the concept of optimization should be primarily defined. Obviously, for balanced classification tasks, SVM can spontaneously find the optimal position of classification boundary which is a tradeoff between classification margin and classification accuracy. But when the classification task is skewed, classification accuracy generally gives bias evaluation, thus some other specific evaluation metrics, such as F-measure and G-mean, are needed to evaluate the classification performance of a learner. F-measure and G-mean can be regarded as functions of the confusion matrix as shown in Table 1.They are calculated as follows:(18)F-measure=2∗Precision∗RecallPrecision+Recall(19)G-mean=TPR×TNRwhere Precision, Recall, TPR and TNR are further defined as:(20)Precision=TPTP+FP(21)Recall=TPR=TPTP+FN(22)TNR=TNTN+FPFrom the formula (19), it obviously expresses that G-mean reflects the balance between the accuracy of the minority class and that of the majority class. Therefore, G-mean can be regarded as an excellent criterion to determine the optimal position of the classification hyperplane. Unfortunately, it is difficult to find this optimal hyperplane taking advantage of direct optimization technology by rewriting the formula (2). Here, we give an iterative algorithm that can be also seen as an exhaustive search algorithm, to search the optimal position for the classification hyperplane. Considering each misclassified minority class instance in the training set, undoubtedly, its decision function (see formula (1)) output value will be negative. Therefore, we can acquire all candidate positions for the optimal classification hyperplane according to the output values of misclassified instances. By comparing G-mean values of these candidate positions, it is easy to find the optimal adjusted position. Furthermore, to maintain the generalization ability of the classifier to some extent, we profit from the idea of SVM to adjust each candidate position to the middle between the corresponding misclassified minority class instance xiand its nearest neighbor instance belonging to the majority class that lies far from the original classification hyperplane, denoting as nxi. Then the adjusted distance θican be calculated with the following formula:(23)θi=-h(xi)-h(nxi)2where h is the original decision function. Then we compare G-mean values of all candidate positions to find the optimal position and its corresponding adjusted distance, representing as θoptimal. Finally, the decision function is adjusted as:(24)h′(x)=h(x)+θoptimalwhere h(x) and h′(x) are original and adjusted decision functions, respectively. Fig. 2gives the schematic diagram of the search process.According to the idea above, we also present a novel SVM decision threshold adjustment algorithm that is named SVM-OTHR. The pseudo code description of SVM-OTHR algorithm is provided in Fig. 3.Fig. 3 indicates that the time complexity of SVM-OTHR is merely correlated with the number of misclassified instances belonging to the minority class, thus it will not increase computational burden to a large extent. Of course, from Fig. 3, we can also observe one obvious drawback of SVM-OTHR that is the appearance of overfitting and/or the decrease of generalization ability. The algorithm can be only local optimization for the limited training samples, as well the adjusted classification hyperplane cannot guarantee to minimize the structural risk.To overcome the defects described above, an extended ensemble learning version of SVM-OTHR named as EnSVM-OTHR is proposed. As indicated in previous work [36,37], ensemble learning can help repair the bias of single classifier and improve the generalization ability of classification results. Here, we select Bagging ensemble learning framework [38] to develop EnSVM-OTHR algorithm. Bagging adopts Bootstrap strategy to generate multiple diverse training subsets and majority voting rule to make the final decision. EnSVM-OTHR simply inherits both Bootstrap strategy and majority voting rule from Bagging. To alleviate the impact of class imbalance, we carry out SVM-OTHR algorithm on each training subset. In addition, to further promote the generalization ability and guarantee the diversity of different base classifiers, a small random perturbation term is inserted into each SVM-OTHR to disturb the final position of classification hyperplane. It is notable that the value of random perturbation term should be moderate due to a too small value is helpless to increase the diversity among base classifiers and a too large value can destroy the performance of each base classifier. According to the feedback from a mass of experimental results, we empirically assign each random perturbation term in the range of [−0.1×θoptimal, 0.1×θoptimal], then the actual moving distance θ′ can be calculated as:(25)θ′=θoptimal×(1+0.1×rand)where rand denotes one random number whose value is in the range of [−1,1]. The schematic diagram and pseudo code description of EnSVM-OTHR algorithm are presented in Figs. 4 and 5, respectively.

@&#CONCLUSIONS@&#
