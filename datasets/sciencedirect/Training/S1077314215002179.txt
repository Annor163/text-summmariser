@&#MAIN-TITLE@&#
A novel double-layer sparse representation approach for unsupervised dictionary learning

@&#HIGHLIGHTS@&#
We propose a DLSR approach for dictionary learning.The DLSR formulation enhances reconstructive and discriminative abilities of dictionary.A DLSR-OMP algorithm is developed to solve the DLSR formulation.

@&#KEYPHRASES@&#
Sparse representation,Unsupervised learning,Texture segmentation,

@&#ABSTRACT@&#
This paper presents a novel double-layer sparse representation (DLSR) approach, for improving both reconstructive and discriminative capabilities of unsupervised dictionary learning. In supervised/unsupervised discriminative dictionary learning, classical approaches usually develop a discriminative term for learning multiple sub-dictionaries, each of which corresponds to one-class training image patches. As such, the image patches for different classes can be discriminated by coefficients of sparse representation, with respect to different sub-dictionaries. However, in the unsupervised scenario, some of the training patches for learning the sub-dictionaries of different clusters are related to more than one cluster. Thus, we propose a DLSR formulation in this paper to impose the first-layer sparsity on the coefficients and the second-layer sparsity on the clusters for each training patch, embedding both the reconstructive (via the first-layer) and discriminative (via the second-layer) capabilities in the learned dictionary. To address the proposed DLSR formulation, a simple yet effective algorithm, called DLSR-OMP, is developed on the basis of the conventional OMP algorithm. Finally, the experiments verify that our approach can improve reconstruction and clustering performance of the learned dictionaries of the conventional approaches. More importantly, the experimental results on texture segmentation show that our approach outperforms other state-of-the-art discriminative dictionary learning approaches in the clustering task.

@&#INTRODUCTION@&#
Recently, there has been a growing interest in the use of sparse representation in both image processing and computer vision communities [1,2]. The key idea of sparse representation is to approximate a natural signal (e.g., an image patch) via a linear combination of a few elements (or called atoms) from an over-complete dictionary. That is, a natural signal can be reconstructed by aggregating the elements of the over-complete dictionary with their corresponding sparse coefficients, obtained via ℓ1 relaxation [3,4] or matching pursuit algorithms [5,6]. The earlier works of sparse representation [7] mainly concentrated on the predefined parametric over-complete dictionary. Afterwards, Wright et al. [8] found out that the dictionary can be directly chosen from training examples and it is competent for face recognition. More commonly, the dictionary learned from training examples has shown more effectiveness in modeling images [9]. The past decade has witnessed the explosion of approaches on learning the non-parametric dictionary from a set of training examples. At the beginning, several approaches [10,11] were proposed for learning reconstructive dictionary, aiming at faithfully reconstructing signals. A representative approach is K-SVD [10]. In exploring the reconstructive capability of the learned dictionary, K-SVD approach has been successfully applied for many image reconstruction tasks, such as image denoising [12] and image compression [13].In reality, the image patches with similar patterns may be sparsely represented by similar elements in a same sub-dictionary. Therefore, discriminative dictionary learning [14] was proposed for image classification in the coefficient space, where the term “discriminative” refers to making the sparse coefficients of an image patch discriminant from patches of other classes. Most recently, the algorithms on learning discriminative dictionary have emerged as promising and effective approaches to deal with the tasks of classification or clustering, in computer vision area [14–18]. For supervised classification, the discriminative terms have been developed to embed the discriminative ability in the learned dictionary. Mairal et al. [14] proposed a softmax discriminative cost term in K-SVD approach for learning multiple sub-dictionaries, each of which corresponds to one-class training examples. Then, the classification, such as scene analysis and texture segmentation, can be achieved via seeking the minimal reconstruction error of these sub-dictionaries on the test image patches. Besides, Yang et al. [18] proposed the Fisher discrimination dictionary learning (FDDL) method to impose the Fisher discrimination criterion on both structured dictionary and sparse coefficients, via introducing a discriminative fidelity term for the dictionary and developing a discriminative coefficient term for the sparse coefficients of image patches.Dictionary learning is more challenging in the unsupervised scenario [19–22]. Gowreesunker et al. [19] proposed a subspace clustering approach for dictionary learning. Such an approach utilizes Orthogonal Subspace Pursuit (OSP) decomposition to identify the subspaces, followed by clustering the observed data that fall into the same subspace. Sprechmann and Sapiro [21] proposed to learn the sub-dictionaries, with each one fitting best to a cluster of data, upon sparse representation. Later, in order to increase the independence between the sub-dictionaries of each cluster, an incoherence promoting term [22] was developed, as the discriminative term, for unsupervised dictionary learning in [21]. However, most existing unsupervised dictionary learning approaches22In this paper, unsupervised dictionary learning refers to learning discriminative dictionaries in unsupervised scenarios for clustering tasks, whereas some papers such as [23] define the reconstructive dictionary learning as unsupervised dictionary learning.assume that there is only one cluster assigned to each training image patch. It is intuitive that some of the image patches used for learning dictionary, in unsupervised cases, belong to more than one cluster (or class). Fig. 1 reveals the possibility of an image patch belonging to one or more classes. In [24], Gramfort et al. have also shown that exploiting sparse pattern, over a few subsets of dictionaries, is able to improve the reconstructive performance of sparse representation, for functional brain imaging. However, their approach simply utilizes the fixed dictionaries, rather than learning dictionaries.Therefore, the basic philosophy of this paper is to enforce the sparsity not only on the coefficients of training patches but also on the clusters of their related sub-dictionaries. That is a training patch can be reconstructed by a few elements from a handful of (one or more) sub-dictionaries, each of which is associated with a specific cluster. Towards this philosophy, we propose a double-layer sparse representation (DLSR) approach for unsupervised dictionary learning, where the first layer follows the traditional way to impose sparsity on coefficients with minimized reconstruction error, and the second layer is dedicated to sparsity on clusters of sub-dictionaries used for sparse representation. This way, both reconstructive (via the first layer) and discriminative (via the second layer) capabilities can be ensured in the learned dictionary. In our approach, the discriminative capability of the dictionary means that the image patches for different clusters can be discriminated in the sparse coefficient space, according to sparse representation with respect to the sub-dictionaries of different clusters. By integrating these two layers together, our approach has the potential to handle both reconstruction and clustering tasks in computer vision. The experimental results show that our approach is able to slightly improve the reconstruction performance of the conventional K-SVD approach, in terms of denosing accuracy. Although the denoising accuracy of our approach is less than other state-of-the-art approaches, it at least verifies that the discriminative ability is useful in image reconstruction. More importantly, our approach is much superior to other state-of-the-art discriminative dictionary learning approaches in texture segmentation, revealing the significant improvement of clustering performance by our approach.From the perspective of reconstructive dictionary, both our approach and Learned Simultaneous Sparse Coding (LSSC) [25] are exploiting the self-similarities of natural images. However, our DLSR approach integrates reconstructive and discriminative powers in a unified formulation, whereas LSSC only focuses on reconstructive power of the learned dictionary. Hence, LSSC cannot be used for image classification/clustering tasks. Moreover, the training stage of our DLSR approach only depends on the local image, rather than the extensive training images required by LSSC. Beyond, another benefit of our approach is that it can easily shift from reconstructive learning to discriminative learning via simply setting the parameter of between-cluster scatter.From the perspective of discriminative dictionary, the closest literature to our work is that of hierarchical dictionary learning or group dictionary learning [26–28], in which the dictionary is learned in a hierarchy or in a group structure. For example, elements learned by [26] are hierarchically organized with a tree-structure. Thus, they have to construct a complicate structure before learning the dictionary, and develop sophisticated algorithm for learning such a dictionary. In group dictionary learning [28], group structure for sub-dictionaries and their weight vectors have to be given before dictionary learning. Therefore, the advantage of our approach is that it requires no prior knowledge on dictionary structure, as needed in [26–28]. In addition, our approach enjoys the simplicity of only modifying the sparse representation step of conventional reconstructive dictionary learning approaches, with no change in the dictionary update step.Now, we briefly review the basic ideas of reconstructive dictionary learning (Section 2.1) and discriminative dictionary learning (Section 2.2).Due to the large dimension of images, sparse representation for modeling an image normally works with the image patches, extracted from a whole image. Let x be one of such image patches33As in other sparse representation approaches, each image patch is preprocessed by subtracting their mean intensity values., in the form of column vector with n pixels, extracted from input image X. Then, the problem of sparse representation for each image patch can be modeled to find the corresponding sparse coefficients w:(1)minw∥x−Dw∥22s.t.∥w∥0≤L,where L is the sparsity level of coefficients, andD∈Rn×m(n ≪ m) is the over-complete dictionary (with m elements) to be learned. Note that each column of D is normalized to be 1, in order to eliminate the scaling ambiguity of coefficients. Also, note that || · ||0 is ℓ0 norm, indicating the number of nonzero values in the vector. However, the minimization problem of (1) is NP-hard. Thus, an extensive literature on solving (1) has been proposed, divided into two main categories: greedy approaches, e.g., orthogonal matching pursuit (OMP) [5], and ℓ1 norm relaxation, e.g., LASSO [3]. As greedy approaches are simple yet effective, this paper mainly focuses on OMP algorithm, summarized in Table 1.Then, dictionary D can be learned in general from training image patchesX*={x1*,…,xt*,…,xT*}with their corresponding sparse coefficients beingW*={w1*,…,wt*,…,wT*}. According to (1), learning D can be formulated as(2)minW*,D∑tR(xt*,wt*,D)s.t.∥wt*∥0≤L.Here, we define byR(xt*,wt*,D)=∥xt*−Dwt*∥22the reconstructive term representing the squared error of reconstructingxt*with dictionary D and sparse coefficientswt*. Fortunately, (2) is able to be relaxed to convex in terms of either D or W*with the other being fixed. As such, dictionary can be learned by iteratively updating D and W*, with the following two steps:(a)Sparse representation step:Eq. (1) is addressed to find sparse coefficients W*of all training examples in X*, with dictionary D being fixed.Dictionary update step:D is updated upon (2) using the sparse coefficients, obtained at the previous step, such that D is more suitable to represent the training examples.To be more specific, we overview the skeleton of K-SVD [10], a well-known reconstructive dictionary learning algorithm. At the first step, K-SVD applies OMP to solve (1), finding the sparse coefficients of all training examples. At the second step, each element of D is updated sequentially with other elements being fixed. For updating each element diin D, only the training examples using diat the first step are utilized to estimate diand its corresponding coefficients, with the least overall reconstruction error addressed by singular value decomposition (SVD). The updating of each diusing the relevant training examples is similar to K-means algorithm. So, such an algorithm is called K-SVD.Instead of concentrating on the reconstructive capability of dictionary, discriminative dictionary learning approaches embed the discriminative power in dictionary D, which is composed of multiple sub-dictionariesD=[D1,…Dc,…,DC]. Here, Dcis the class-specified sub-dictionary corresponding to class c, and C is the class number in total. Then, input patch x can be categorized according to the minimum reconstruction error among all sub-dictionaries:(3)minw,c∈{1,…,C}∥x−Dcw∥22s.t.∥w∥0≤L.From (3), one may see that it is rather important to make D discriminative such that Dcis capable of reconstructing the patches of class c with small errors, whereas reconstructing the patches of other classes with large errors. Towards this end, a discriminative term is introduced in the following for either supervised or unsupervised learning approaches on discriminative dictionary.Supervised dictionary learning:Assume that Λcis the subset of indices{1,…,t,…,T},corresponding to the training examples of class c. Then, the discriminative dictionary can be learned by adding discriminative termS(xk*,wk*,Dc)in (2) for each k ∈ Λc:(4)minW*,{Dc}c=1C∑c∑k∈ΛcR(xk*,wk*,Dc)+βS(xk*,wk*,Dc)s.t.∥wk*∥0≤L,where β indicates the tradeoff between the reconstructive and discriminative capabilities of dictionary. Similar to learning reconstructive dictionary, the dictionary can be obtained via iterating between the sparse representation and dictionary update steps until convergence.There are many variations of discriminative termS(xk*,wk*,Dc),and a classical one is proposed in [14]. Assuming thatwkc*is the sparse coefficients ofxk*with respect to sub-dictionary Dc, [14] defines discriminative termS(xk*,wk*,Dc)by(5)S(xk*,wk*,Dc)≡log(∑c′=1Ce−μ(R(xk*,wkc′*,Dc′)−R(xk*,wkc*,Dc))),where μ > 0 is the parameter controlling the discriminative capability of dictionary. Obviously, the minimum value ofS(xk*,wk*,Dc)can be achieved, onceR(xk*,wkc*,Dc)is smallest value among allR(xk*,wkc′*,Dc′),c′=1,…,C. In addition, (5) encourages the difference in the reconstruction errors of each sub-dictionary via minimizingS(xk*,wk*,Dc)in (4). Finally, given (5) the dictionary can be obtained based on the discriminative K-SVD algorithm [14].Unsupervised dictionary learning:Since Λcof the training examples is unknown in unsupervised scenario, an additional example assignment step[22] has to be included for classifying the training examples via(6)Λc={k|1≤k≤T,R(xk*,wkc*,Dc)≤R(xk*,wkc′*,Dc′),∀c′=1,…,C},wherewkc*andwkc′*,as the sparse coefficients ofxk*regarding DcandDc′,can be obtained at the sparse representation step.D=[D1,…Dc,…,DC]is from the dictionary update step of the last iteration. Afterwards, the current iteration updates the discriminative dictionary via (4), with∑c′∥DcTDc′∥[22] being the discriminative term to promote the incoherence between sub-dictionaries. In a word, we can learn the discriminative dictionary in unsupervised manner by iterating over sparse representation, example assignment and dictionary update steps.From now on, we provide the details of our DLSR approach on unsupervised dictionary learning. First, Section 3.1 proposes a DLSR formulation, in which the first-layer sparsity on coefficients minimizes the reconstruction error and the second-layer sparsity on the clusters of each patch yields more discriminative dictionary. Second, Section 3.2 presents a solution to the proposed DLSR formulation. Third, as the key part of our solution, the DLSR-OMP algorithm, for the sparse representation step of dictionary learning, is developed in Section 3.3. Finally, given the dictionary learned with our DLSR approach, reconstruction and clustering can be achieved by (1) and (3), respectively.As presented above, (2) has formulated the first-layer sparsity on coefficients for dictionary learning. To make the dictionary discriminative, the corresponding sub-dictionaries of nonzero coefficients from the first layer have to fall into a small number of clusters. As such, the sparsity on the clusters of each image patch can be imposed. We therefore develop the diagram of DLSR formulation in Fig. 2, in which the objective of the second-layer sparsity is to enforce nonzero coefficients (via their indices) belonging to the sub-dictionaries of “few” clusters. Consequently, only these sub-dictionaries with their corresponding nonzero coefficients are updated, given the training patches that correspond to those “few” clusters. As such, the discriminative capability is encoded in the learned dictionary.We start to describe the proposed DLSR formulation by introducingf(wt*)=[∥wt1*∥1,…,∥wtc*∥1,…,∥wtC*∥1]as the importance weights of sub-dictionaries[D1,…Dc,…DC]to training image patchxt*.wtc*is the sub-vector ofwt*,corresponding to sub-dictionary Dc. Since nonzero values of∥wtc*∥1indicate thatxt*falls into cluster c,∥f(wt*)∥0means the cluster number, to whichxt*is related. Then, we make discriminative termS(xt*,wt*,D)=∥f(wt*)∥0as sparsity constraint on the clusters of each image patch, thus having the following DLSR formulation via rewriting (4):(7)minW*,D∑tR(xt*,wt*,D)+β∥f(wt*)∥0s.t.∥wt*∥0≤L,where parameter β ≥ 0 is the penalty on between-cluster scatter, implying the tradeoff between the discriminative and reconstructive capabilities of dictionary. Note that in (7), we haveD=[D1,…Dc,…,DC].Our DLSR formulation, as presented in (7), has advantages over the conventional approaches in three aspects. First, the DLSR formulation does not require any prior of Λcthat indicates the classification on training examples. Our approach thereby enjoys the simplicity of prescinding from the additional step of example assignment step for obtaining Λc, which is included in conventional unsupervised approaches [21,22]. Second, the prior on the dictionary structure is not required in our DLSR formulation. As such, our approach can be easily applied without any sophisticated design for dictionary structure, which is required in [26–28]. Third, in our DLSR formulation the discriminative term is only dependent on sparse coefficients, while others, e.g., [14,18], take into account dictionary in their discriminative terms. Thus, with sparse coefficients estimated for the specific clusters, our DLSR formulation can be easily implemented by directly utilizing the same dictionary update step as the existing reconstructive dictionary learning approaches.Now, we consider the solution to the DLSR formulation. Recall that for reconstructive dictionary learning in (2), a dictionary update step needs to be run with sparse coefficientswt*being fixed. It is important to recognize that whenwt*is fixed, the derivative formulation of Lagrange multiplier for (7), in terms of D, is(8)∂∂D∑tR(xt*,wt*,D),which is the same as that of Lagrange multiplier for (2). Therefore, the dictionary update step of reconstructive dictionary learning algorithms, e.g., K-SVD, can be directly utilized for learning dictionary with the DLSR formulation.In fact, the sparse representation step of our approach imposes discrimination on the dictionary, through the ℓ0 norm constraint onf(wt*)(i.e., the second-layer sparsity). This results in updating the sub-dictionaries of the relevant clusters at the dictionary update step, since their corresponding nonzero-valued coefficients only fall into these clusters. For each individualxt*andwt*,the coefficients of the sparse representation step can be determined by(9)minwt*R(xt*,wt*,D)+β∥f(wt*)∥0s.t.∥wt*∥0≤L,with all sub-dictionaries being fixed.Generally speaking, the dictionary can be achieved by addressing the DLSR formulation of (7) through alternating between sparse representation step and traditional dictionary update step. However, (9) for sparse representation step is a non-convex problem, similar to (1). Since OMP is an effective algorithm to solve (1), the DLSR-OMP algorithm is developed in the next subsection for solving (9), on the basis of OMP. Here, our DLSR-OMP algorithm extends OMP, to pose a possible solution to DLSR formulation of (7). The optimal solution to (7), such as LASSO-based algorithm, is beyond the scope of this paper, and it is worthy to be investigated in future work.For fast convergence, the dictionary has to be initialized to contain several sub-dictionaries. Following [22], this paper refers to the similarity graph G of dictionary elements by {D, W*W*T}, in which the elements of D are the vertex set and W*W*Tis the matrix of edge weights. Given G, we use the spectral clustering algorithm [30] to partition D into several clusters, as the initial structured dictionary. Note that despite sharing the same initial structured dictionary as [22], our DLSR approach has better convergence performance as validated in the experimental results of Section 4.2.With the dictionary learned by the proposed DLSR formulation, conventional sparse representation approaches can be directly used to compute sparse coefficients and their corresponding reconstruction errors, for each image patch. Then, reconstruction of each image patch can be obtained given the sparse coefficients yielded by (1). Besides, Eq. (3) can be directly applied for clustering image patches.This subsection concentrates on solving (9) by proposing the DLSR-OMP algorithm. Table 2 summarizes the proposed DLSR-OMP algorithm. More details about steps for one iteration in the DLSR-OMP algorithm (Table 2) are presented in the following.Steps 1–4.These four steps mainly deal with the classification of nonzero coefficients, as∥f(wt*)∥0needs to be estimated and minimized in (9). As summarized in Table 1, each iteration of OMP selects a new element from the dictionary to estimate the sparse coefficients, thus yielding one more nonzero coefficient at each iteration. To solve (9), the selection of the new element in each iteration of OMP needs to take into account the minimization on∥f(wt*)∥0. In fact, the value of∥f(wt*)∥0can be counted by judging whether the selected element belongs to the same or different clusters, compared to the previous iterations. Therefore, given residual r from the previous iteration and sub-dictionaries Dcof each cluster, the newly yielded nonzero coefficient at iteration l of the DLSR-OMP algorithm can be classified and estimated, according to the following two cases.Case 1: Compared to the previous iterations, the newly yielded nonzero coefficient stays in the same cluster. Then, the cluster of the new nonzero coefficient at iteration l can be obtained with(10)c^1s=argmaxc∈L∥DcTr∥∞,whereL⊆{1,…,c,…,C}is the cluster subset, to which the nonzero coefficients of the previous iterations belong. Infinity norm || · ||∞ is the maximum absolute value of vector. Afterwards, similar to OMP, the largest coordinate λ1 ofDc^1sTrin absolute value identifies the index of the new nonzero coefficient at iteration l, with respect toDc^1sdenoted as the sub-dictionary of clusterc^1s. Let I1 ← I ∪ {Υ(λ1)} be the index set of this case at the current iteration. Here, I is the index set from the previous iterations and Υ(·) maps coordinates ofDc^1sTrinto coordinates of DTr. Finally, the same as OMP, we compute the sparse coefficients of this case byw1=DI1†x.DI1†is the pseudoinverse matrix ofDI1,which only keeps the columns of D corresponding to I1 and sets others to zero vector.Case 2: Compared to the previous iterations, the newly yielded nonzero coefficient belongs to a different cluster. Clusterc^2sof the new nonzero coefficient can be selected from subsetH={1,…,c,…,C}∖L:(11)c^2s=argmaxc∈H∥DcTr∥∞.Given sub-dictionaryDc^2sof clusterc^2s,the index of the new nonzero coefficient at iteration l can be obtained with the largest absolute coordinate λ2 ofDc^2sTr. Similar to Case 1, we denote by I2 ← I ∪ {Υ(λ2)} the index set of Case 2. At last, sparse coefficients of this case, denoted by w2, can be reached viaDI2†x,whereDI2†is the pseudoinverse matrix ofDI2. InDI2,only the columns of D corresponding to I2 are maintained and others are set to 0.Steps 5–6. These two steps aim at choosing between w1 and w2, to find the minimum of (9). Clearly, since there is one more cluster involved in Case 2 than Case 1, the above two cases deliver∥f(w2)∥0−∥f(w1)∥0=1. This yields(12)w=argminw1,w2{R(x,w1,DI1)+β∥f(w1)∥0,R(x,w2,DI2)+β∥f(w2)∥0}=argminw1,w2{R(x,w1,DI1),R(x,w2,DI2)+β},for finding the optimal solution to (9) at each iteration. As seen from (12), the selection of w1 and w2 relies on the comparison betweenR(x,w1,DI1)andR(x,w2,DI2)+β. For the convenience of such comparison, β can be represented byηR(x,w2,DI2),where the value of η is to be discussed in the next section. After obtaining w at each iteration, residual r can be updated:r=x−Dw,for the next iteration.Step 7. In this step, cluster sets L, H and index set I have to be updated in the end of each iteration. As such, the information on the number of clusters already selected for sparse representation can be simply stored in L and H. If w is equivalent to w2, then sub-dictionaryDc^2sis picked up to estimate w, so that L, H, and I need to be updated withL∪{c^2s},H∖{c^2s},and I2, for the next iteration. Otherwise, we have I ← I1 without any change in L and H.At last, let us look at the initialization and convergence of the DLSR-OMP algorithm. It simply initializesI=⌀andr=x,identical to the conventional OMP algorithm. Besides, w2 of Case 2 has to be chosen as the output coefficients w at the first iteration, since no cluster is available before the first iteration. Meanwhile, H and L are initialized to{1,…,c,…,C}and⌀,respectively. This implies that no cluster is selected before the first iteration. For the convergence of the DLSR-OMP algorithm, see [5], since our algorithm is similar to the conventional OMP in the way of computing residual r.

@&#CONCLUSIONS@&#
