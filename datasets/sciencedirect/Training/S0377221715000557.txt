@&#MAIN-TITLE@&#
Surrogate upper bound sets for bi-objective bi-dimensional binary knapsack problems

@&#HIGHLIGHTS@&#
We define an upper bound set for the bi-objective bi-dimensional knapsack problem.It is based on the surrogate relaxation and generalizes the dual surrogate problem.We propose two exact algorithms: an enumerative algorithm and its improved version.We analyze the surrogate multipliers and the dominance relations between bound sets.Based on the improved exact algorithm, an approximated version is derived.

@&#KEYPHRASES@&#
Combinatorial optimization,Multiple objective programming,Bi-dimensional binary knapsack problem,Surrogate relaxation,Bound sets,

@&#ABSTRACT@&#
The paper deals with the definition and the computation of surrogate upper bound sets for the bi-objective bi-dimensional binary knapsack problem. It introduces the Optimal Convex Surrogate Upper Bound set, which is the tightest possible definition based on the convex relaxation of the surrogate relaxation. Two exact algorithms are proposed: an enumerative algorithm and its improved version. This second algorithm results from an accurate analysis of the surrogate multipliers and the dominance relations between bound sets. Based on the improved exact algorithm, an approximated version is derived. The proposed algorithms are benchmarked using a dataset composed of three groups of numerical instances. The performances are assessed thanks to a comparative analysis where exact algorithms are compared between them, the approximated algorithm is confronted to an algorithm introduced in a recent research work.

@&#INTRODUCTION@&#
Given a set of items J = {1, …, n} and a set of dimensions I = {1, …, m}, we associate with each dimension i ∈ I a capacityωi∈N1and a set of weightswij∈N1for each item j ∈ J. Moreover, each item is available in a single copy. The multi-objective multi-dimensional binary knapsack problem consists in packing a subset of J into a container with limited capacity over the dimensions I. This must be done while maximizing a profit according to a set of objectives K = {1, …, p}. For this, a profitcjk∈N1is associated with each item j ∈ J according to each objective k ∈ K. The general formulation of this problem is the followingpOmDKPmax∑j=1ncjkxjk=1,⋯,ps.t.∑j=1nwijxj≤ωii=1,⋯,mxj∈{0,1}j=1,⋯,nThis problem has received a lot of attention, see e.g.Fréville (2004), Varnamkhasti (2012), Lust and Teghem (2012) for recent surveys. It is used for representing various practical problems as capital budgeting, allocating processors (Kellerer, Pferschy, and Pisinger, 2004; Martello and Toth, 1990; da Silva, Clímaco, and Figueira, 2004). For example, Clausen, Hjorth, Nielsen, and Pisinger (2010) consider a special case of two-dimensional knapsack for the problem of assigning seats in a train for a group of people traveling together.Many particular cases of this problem have been studied, and all of them areNP-hard. We will denote this problem by (pOmDKP), according to the number of objectives and dimensions. We will omit the number of dimensions and/or the number of objectives in this notation whenever p = 1 and/or m = 1. In the following, we will make a particular distinction between the single-dimensional (m = 1) and the multi-dimensional (m > 1) case. Indeed,(KP)problem can be considered as an “easy”NP-hard problem as several practically efficient methods have been proposed for its exact solution (Kellerer et al., 2004; Martello, Pisinger, and Toth, 1999; Pisinger, 1994). Their theoretical time-complexity is pseudo-polynomial.(2OKP)problem is also one of the most studied multi-objective combinatorial optimization problems. A number of methods have been proposed for its exact solution (Bazgan, Hugot, and Vanderpooten, 2009; Delort and Spanjaard, 2010; Jorge, 2010; Ulungu and Teghem, 1997; Visée, Teghem, Pirlot, and Ulungu, 1998). Bazgan et al. (2009); Jorge (2010) have also provided exact solution methods for(3OKP)problem.(mDKP)problem is practically far more difficult than(KP)problem as pointed out by Fréville (2004). In the multi-objective case, instances of this problem are often used as a benchmark to compare metaheuristics (Ishibuchi, Hitotsuyanagi, Tsukamoto, and Nojima, 2009; Jaszkiewicz, 2004; Tricoire, 2012; Zitzler and Thiele, 1999). However, this problem has received less attention in an exact context (Florios, Mavrotas, and Diakoulaki, 2010; Lust and Teghem, 2012; Mavrotas, Figueira, and Antoniadis, 2011). To our knowledge, only one exact method has been proposed for the specific(2O2DKP)case (Gandibleux and Perederieieva, 2011; Perederieiva, 2011).In this paper, we also consider the(2O2DKP)problem. The main purpose is to design and compute a tight upper bound set for this problem. Such a bound set is a crucial component when it is embedded within an algorithm aiming to enumerate a complete set of nondominated points of the(2O2DKP).The main definitions, properties, and notations of multi-objective combinatorial optimization are given now. A more complete introduction can be found in Ehrgott (2005).A multi-objective combinatorial optimization problem can be formulated asMOCOmax{(z1(x),…,zp(x))=Cx:x∈X},with a linear objective matrixC∈Rp×n,variablesx∈Rn,and the feasible set (in decision spaceRn)X:={x∈{0,1}n:Ax≦b,x≧0}.Matrix A is an m × n matrix of constraints andb∈Rmthe right hand side vector. The image of X under C, i.e.,Y:=CX:={y=Cx∈Rp:x∈X}is called the outcome set in objective spaceRp.We assume that no feasible solution optimizes all objectives simultaneously and use the following notations for componentwise orders inRp. Lety1,y2∈Rp. We write y1≧y2 (y1weakly dominates y2) ifyk1≥yk2for k = 1, …, p; y1 ≥ y2 (y1dominates y2) if y1≧y2 and y1 ≠ y2; and y1 > y2 (y1strictly dominates y2) ifyk1>yk2,k=1,…,p. We defineR≧p:={x∈Rp:x≧0}and analogouslyR≥pandR>p.A feasible solutionx^∈Xis called efficient (weakly efficient) if there does not exist x ∈ X such thatz(x)≥z(x^)(z(x)>z(x^)). Ifx^is (weakly) efficient, thenz(x^)is called (weakly) nondominated. The efficient set XE⊆X is defined asXE:={x∈X:∄x¯∈X:z(x¯)≥z(x)},and its image in objective space is referred to as the nondominated set YN≔ z(XE). Equivalently, YNcan be defined byYN:={y∈Y:(y+R≧p)∩Y={y}}. This concept is extended by definingSN:={s∈S:(s+R≧p)∩S={s}}for an arbitrary setS∈Rp. The exact solution of a multi-objective combinatorial optimization problem consists in determining a complete set of efficient solutions, i.e. to determine at least one efficient solution for each nondominated point.As we consider here multi-objective combinatorial optimization problems, several classes of efficient solutions need to be distinguished. Supported efficient solutions are optimal solutions of a weighted sum single objective problem (Geoffrion, 1968)MOCOλmax{λ1z1(x)+⋯+λpzp(x):x∈X}for someλ∈R>p. Their images in objective space are supported nondominated points. We use the notations XSE and YSN, respectively. In order to avoid a confusion with the weights of the items of(pOmDKP),the weight vectorλ∈R>pwill be called direction in the following. All supported nondominated points are located on the boundary of the convex hull of Y (convY), i.e., they are nondominated points of(convY)−R≧p.Nonsupported efficient solutions are efficient solutions that are not optimal solutions of(MOCOλ)for any directionλ∈R>p. Nonsupported nondominated points are located in the interior of the convex hull of Y. In the particular bi-objective case, nonsupported nondominated points are located in the interior of triangles the hypotenuse of which is defined by consecutive supported nondominated points with respect to z1. The set of nonsupported efficient solutions and nondominated points are denoted respectively by XNE and YNN.Finally, we can distinguish two classes of supported efficient solutions. The set of extremal supported efficient solutions XSE1 is a subset of XSE the corresponding point of which is an extreme point of convY. YSN1 ≔ z(XSE1) is the set of nondominated extreme points. XSE2 ≔ XSE∖XSE1 and YSN2 ≔ YSN∖YSN1 are respectively the sets of nonextremal supported efficient solutions and nondominated points. The computation of the set YSN1 can be easily done in the bi-objective context using the algorithm by Aneja and Nair (1979), under the assumption that the corresponding single-objective problem can be solved efficiently in practice.Bounds on the optimal value of a single-objective problem are crucial to design efficient solution methods in the single-objective case. Their generalization to the multi-objective case called bound sets(Ehrgott and Gandibleux, 2001) are used to bound YN. Several definitions of bound sets have been proposed, we use the definition proposed by Ehrgott and Gandibleux (2007). Contrary to the single-objective case, the definitions of upper and lower bound sets are not symmetric. A lower bound set for YNis generally given by a set of known feasible points filtered by dominance. Upper bound sets for YN(or for any subset of points inNp) can be obtained by far more various ways.We introduce some additional terminology before the next definition. S isR≧p-closed if the setS−R≧pis closed andR≧p-bounded if there existss0∈Rpsuch thatS⊂s0−R≧p.Definition 1Ehrgott and Gandibleux, 2007LetY¯⊂Rp. An upper bound set U forY¯is anR≧p-closed andR≧p-bounded setU⊂Rpsuch thatY¯⊂U−R≧pandU⊂(U−R≧p)N.In solution methods,Y¯denote generally the set of nondominated points of a subproblem of the considered problem (e.g. a relaxation). The same way as the single-objective context, bound sets forY¯must be as tight as possible for a reasonable computational cost. Ehrgott and Gandibleux (2007) have proposed the notion of dominance between bound sets.Definition 2Ehrgott and Gandibleux, 2007Given two upper bound sets U1 and U2 for a same setY¯,U1 dominates U2 ifU1⊂U2−R≧pandU1−R≧p≠U2−R≧p.The dominance relation between bound sets is transitive. Nevertheless, if we can always compare bound values in the single-objective case, dominance between bound sets does not necessarily occur (Fig. 1). Computing several upper bound values in the single-objective context, the smallest defines naturally the tightest bound. Proposition 1 provides a way to merge upper bound sets.Proposition 1Ehrgott and Gandibleux, 2007If U1 and U2 are upper bound sets for a same setY¯andU1−R≧p≠U2−R≧pthenU*:=[(U1−R≧p)∩(U2−R≧p)]Nis an upper bound set forY¯dominating U1 and U2.It is interesting to note that the simultaneous use of several upper bound sets can thus be a way to obtain a particularly tight upper bound set (Fig. 2). We provide some additional explanations based on Figs. 1 and 2. We consider here polyhedral upper bound sets U defined by a set of extreme points {y1, …, yk}, i.e.U=(conv{y1,…,yk}−R≧2)N. U1~ = ~(conv {y1, y2, y3})Nand U2 = (conv {y4, y5, y6})Nare incomparable bound sets, i.e. there is no dominance relation between them. The bound setU*:=[(U1−R≧2)∩(U2−R≧2)]N(which is (conv {y4, y7, y3})N) is obtained by merging U1 and U2. We can note that the point y7 is not an extreme point of any of these later bound sets. This later point may not be feasible for the used relaxation, and even if it is, its importance is highlighted only when considering U1 and U2 simultaneously.The surrogate relaxation(Glover, 1975) is a well known relaxation that can be used to compute an upper bound on the optimal value of(mDKP)problem. Its principle is to aggregate all constraints of the problem using non-negative multipliers. In this subsection, we present some of its properties. Given a vector of multipliers(u1,…,um)∈R≥m,the surrogate relaxation is the problemSP(u1,…,um)defined bySP(u1,…,um)max∑j=1ncjxjs.t.∑i=1mui∑j=1nwijxj≤∑i=1muiωixj∈{0,1}j=1,⋯,nIt is an instance of (KP) problem. Any solution method for this last problem can therefore be used to determine an optimal solution of(SP(u1,…,um)). Next, the associated optimal value defines an upper bound on the optimal value of the initial(mDKP). However, the quality of the obtained upper bound depends on the used multiplier vector. The problem consisting in obtaining the best possible upper bound using a surrogate relaxation is called dual surrogate problem DSP:DSPmin(u1,…,um)∈R≥mmax{∑j=1ncjxj∣∑i=1mui∑j=1nwijxj≤∑i=1muiωi,x∈{0,1}n}The dual surrogate problem isNP-hard (Boyer, 2007). An exact solution method for the bi-dimensional case has been provided by Fréville and Plateau (1993). Several approximation methods for this problem have also been provided (see Wilbaut, Hanafi, and Salhi (2008) for a recent survey, and Boyer, Elkihel, and Baz (2009); Fleszar and Hindi (2009); Puchinger, Raidl, and Pferschy (2010) for recent contributions on the 0–1 multi-dimensional knapsack problem).The surrogate relaxation can also be considered for the multi-objective case, we obtain thus an instance of(pOKP).pOSP(u1,…,um)max∑j=1ncjkxjk=1,…,ps.t.∑i=1mui∑j=1nwijxj≤∑i=1muiωixj∈{0,1}j=1,⋯,nA distinction between feasible solutions and points of the initial problem(pOmDKP)and its relaxation(pOSP(u1,…,um))will be necessary. We denote by YN(u1, …, um) the set of nondominated points of(pOSP(u1,…,um)),and simply YNthe set of nondominated points of(pOmDKP). YN(u1, …, um) defines an upper bound set for YN. Obviously, the computation of this upper bound set is practically expensive. The set of nondominated extreme points YSN1(u1, …, um) of(pOSP(u1,…,um))defines an upper bound set (convYSN(u1, …, um))N for YN(u1, …, um). By transitivity, it is thus also an upper bound set for YN. This last upper bound set has been used by da Silva et al. (2004) and Perederieiva (2011).In the following, we will consider upper bound sets of the kind (convYSN(u1, u2))N for the specific bi-objective bi-dimensional case(2O2DKP). This kind of upper bound set will be called convex surrogate upper bound set (CSUB). As multipliers take only non-negative values, we can divide both multipliers u1 and u2 by (u1 + u2), so that their sum becomes equal to 1. In other words, only one multiplier u becomes necessary in order to specify a particular CSUB, that will be denoted by CSUB(u) ≔ (convYSN(u))N. The same way, surrogate relaxations will be denoted(2OSP(u))and their sets of (extreme supported) efficient solutions and nondominated points will be denoted respectively XE(u) (XSE1(u)) and YN(u) (YSN1(u)).The purpose of this paper is to first to characterize and next to determine the tightest possible bound set for YNusing CSUBs for the specific(2O2DKP)problem. This upper bound set will be defined in Section 2.1. An attempt of adaptation of a classical dichomotic method will be proposed in Section 2.2, with some unexpected difficulties. In Section 3, theoretical foundations about the differentiation of CSUBs depending on the used multiplier are provided. Based on this, an original approach enumerating all CSUB is proposed in Section 4. Afterwards, an improved algorithm based on dominance relations between CSUBs is proposed in Section 5, and a derived heuristic algorithm in Section 6. Numerical experiments are finally provided in Section 7.The goal is to determine (efficiently) the tightest possible upper bound set that can be obtained for(2O2DKP)using CSUBs. In other words, we aim to propose a generalization of the dual surrogate problem for the bi-objective case.As stated in Section 1.2 two upper bound sets are not necessarily comparable, this is in particular true for CSUBs defined using different multipliers. Perederieiva (2011) has used Proposition 1 to merge a number h of incomparable CSUBs, and thus to obtain a tighter upper bound set.Proposition 2Perederieiva, 2011(∩i=1h(CSUB(ui)−R≧2))Nis a valid upper bound set and it dominates any of the bound sets (convYSN(ui))N used in the intersection.Obviously, considering a larger set of CSUBs in Proposition 2 allows to obtain a tighter upper bound set. Ideally, we would like to consider all possible CSUB. Since the number of feasible solutions of problems of the kind(2OSP(u))is finite, then the number of possible CSUBs is also finite. Therefore, Proposition 2 can be applied to merge all CSUBs. We can thus give a first generalization of the dual surrogate problem.Definition 3(∩u≥0(CSUB(u)−R≧2))Nis the tightest upper bound set based on the convex surrogate relaxation and the number of multipliers to consider in order to obtain it is finite. It is denoted the optimal convex surrogate upper bound set (OCSUB).Using the same arguments, another generalization of the dual surrogate problem is possible. Using Proposition 1 in order to merge bound sets of the kind YN(u), we can obtain the tightest upper bound set based on the surrogate relaxation.Definition 4(∩u≥0(YN(u)−R≧2))Nis the tightest upper bound set based on the surrogate relaxation and the number of multipliers to consider in order to obtain it is finite. It is denoted the optimal surrogate upper bound set (OSUB).The computation of the OSUB would be particularly expensive as it would require the solution of a number of(2OKP)problems. This is why our aim is restricted the computation of the OCSUB.From Definition 3, we can deduce that the upper bound set OCSUB verifies the following properties:(OCSUB−R≧2)is convex and(OCSUB−R≧2)is a polyhedron. A first natural idea for the computation of the upper bound set OCSUB is to use the existing algorithms for the solution of the single-objective case. Indeed, the following result can be used.Proposition 3Ehrgott and Gandibleux, 2007Consider a relaxation(P˜)of a MOCO problem (P),(P˜λ)is a relaxation of (Pλ) for allλ∈R≥2.Solving a surrogate relaxation (for any multiplier) of(2O2DKPλ)withλ∈R≥2,we obtain a solutionx˜the image of which in objective space is given byy˜=z(x˜). Proposition 3 implies thatYN⊂(λTy˜−R≧2). Using different directionsλ1,…,λk∈R≧2and denoting y1, …, ykthe image of the optimal solutions of surrogate relaxations of the corresponding weighted sum problems, we obtain thatYN⊂⋂i=1k(λiTyi−R≧2).Thus(⋂i=1k(λiTy˜i−R≧2))Nis an upper bound set for YN. Obviously, this upper bound set will be tighter if we solve the dual surrogate problem for each considered weighted sum problem.As (OCSUB−R≧2) is a polyhedron, there exists a finite number of direction λ1, …, λldefining the normal to each edge of OCSUB. Therefore we can compute OCSUB by solving exactly the dual surrogate problem associated with each of these weighted sum problems. However, we do not know these directions initially.This problem seems similar to the computation of the convex relaxation (convYSN)N of a bi-objective combinatorial optimization problem. Indeed(convYSN−R≧2)is a polyhedron and its computation can be done by a dichotomic algorithm (Aneja and Nair, 1979). It seems thus very natural to apply the same algorithm with a minor modification: the exact solution of the weighted sum single-objective problem(2DKP)would be replaced by the solution of the corresponding dual surrogate problem. Nevertheless, some difficulties will immediately appear.Example 1We consider the following instance of (2O2DKP).2O2DKP−1max10x1+7x2+20x3+7x4+8x5max15x1+17x2+7x3+4x4+10x5s.t.3x1+1x2+9x3+4x4+9x5≤1313x1+11x2+2x3+1x4+7x5≤17xj∈{0,1},j=1⋯5We initialize the dichotomic method with the directions λ1 = (1, 0) and λ2 = (0, 1). For λ1 = (1, 0), the optimal solution obtained solving the dual surrogate problem (Fréville and Plateau, 1993) is x1 = (1, 0, 1, 0, 0) with z(x1) = (30, 22). For λ2 = (0, 1), the optimal solution obtained solving the dual surrogate problem is x2 = (0, 1, 1, 1, 0) with z(x2) = (34, 28). We can immediately note that x2 dominates x1. Using these two points is thus inappropriate in order to define a direction to continue the application of the dichotomy. Thus, we can only conclude thatYN⊂{y∈R2:y1≤30}∩{y∈R2:y2≤28}.However by using the direction λ3 = (3, 7), we can improve the bound set (see Fig. 3). Indeed, the optimal solution obtained solving the dual surrogate is then again x1 = (1, 0, 1, 0, 0) with z(x1) = (30, 22). Finally, there is no immediate way to guess this direction using a dichotomic principle.The idea behind the attempt of dichotomic method is, knowing an appropriate direction defining an edge of the OCSUB, to deduce the associated multiplier. However, Example 1 shows that these directions are not straightforward to find. In the following, we will consider another approach based on the computation of CSUBs.Definition 3 shows that the computation of the OCSUB can be done by an enumeration of a finite set of CSUBs. A finite set of multipliers can therefore be used to compute these CSUBs. In this section, we study the association of multipliers to CSUBs. Ideally, we would like to obtain a multiplier-set decomposition with a one-to-one correspondence between subsets of multipliers and CSUBs. Such a decomposition would guarantee an exhaustive enumeration of all CSUB, without redundancy.Differences between feasible sets of surrogate relaxations(2OSP(u))for u ∈ [0, 1] are crucial in the differentiation of CSUBs. In this subsection, we consider individually solutions that are feasible for a problem(2OSP(u))where u ∈ [0, 1]. To shorten the notations, this kind of solution will be called u-surrogate feasible. An u-surrogate feasible solution x respects the constraint uw1(x) + (1 − u) w2(x) ≤ uω1 + (1 − u) ω2. Hence, two cases are possible: either x respects both constraints of(2O2DKP)or x respects only one of them (if x does not respect any of the constraints, it does not respect this constraint neither).In the first case, x is feasible for(2O2DKP)and is thus u-surrogate feasible for any u ∈ [0, 1].In the latter case, x is u-surrogate feasible for any u ∈ [0, 1] such that uw1(x) + (1 − u) w2(x) ≤ uω1 + (1 − u) ω2. This can be equivalently written u (w1(x) − w2(x) − ω1 + ω2) ≤ ω2 − w2(x). Next, we have necessarily w1(x) − w2(x) − ω1 + ω2 ≠ 0. Indeed, w1(x) − w2(x) − ω1 + ω2 = 0 would imply w1(x) − ω1 = w2(x) − ω2, i.e. either none or both constraints are satisfied (and we make the assumption that only one is satisfied). The constraint can thus be rewritten{u≤ω2−w2(x)w1(x)−w2(x)−ω1+ω2if(w1(x)−w2(x)−ω1+ω2)>0,u≥ω2−w2(x)w1(x)−w2(x)−ω1+ω2if(w1(x)−w2(x)−ω1+ω2)<0.In other words, x is u-surrogate feasible for any{u∈[0,v]if(w1(x)−w2(x)−ω1+ω2)>0,u∈[v,1]if(w1(x)−w2(x)−ω1+ω2)<0,wherev=ω2−w2(x)w1(x)−w2(x)−ω1+ω2.v is therefore a particular multiplier associated with x. Indeed, it defines one of the extremities of the interval of multipliers u such that x is u-surrogate feasible.Definition 5(i)A critical multiplier u is a multiplier such that there exists at least one solution x that is u-surrogate feasible but not u − ε-surrogate feasible or not u + ε-surrogate feasible, for any ε > 0. The solution x and the critical multiplier u are called associated.The stability interval of a solution x is the interval of all multipliers u such that x is u-surrogate feasible.Finally, there are three kinds of stability interval:•[0, 1] for a solution feasible for(2O2DKP);[0, v] for a solution feasible only for the second capacity constraint of(2O2DKP). The critical multiplier v is said to be of type 0M;[v, 1] for a solution feasible only for the first capacity constraint of(2O2DKP). The critical multiplier v is said to be of type M1.As a critical multiplier can be associated with several solutions, it can be simultaneously 0M and M1.Two CSUBs are different if their sets of extreme points are not identical. It comes obviously from particular differences between the feasible sets of corresponding problems(2OSP(u)). Lemma 1 provides a necessary condition for two CSUBs to be different.Lemma 1Let uiand ujbe two different multipliers, ifCSUB(ui)andCSUB(uj)are different then at least one of the extreme supported solutions of(2OSP(ui))and(2OSP(uj))is feasible for one problem and not for the other one.Let x1 ∈ XSE1(ui). Suppose that z(x1) is not an extreme point of CSUB(uj), thenx1∉XSE1(uj). This is obviously the case if x1 is not uj-surrogate feasible (Fig. 4a). Otherwise, two sub-cases need to be considered.•There is an uj-surrogate feasible solution x2 dominating x1 (Fig. 4b). Therefore x2 is not ui-surrogate feasible as it would contradict the efficiency of x1 for(2OSP(ui)).There are two solutions x2, x3 ∈ XSE1(uj) such that a point y ∈ (z(x2), z(x3)) weakly dominates z(x1) (Fig. 4c). Therefore, x2 and/or x3 are not ui-surrogate feasible as it would contradict the assumption that x1 ∈ XSE1(ui).□The converse of Lemma 1 is not true. Indeed, if the solution x1 used in the proof of this lemma is not uj-surrogate feasible, there may exist another solution x2 such that x2 is uj-surrogate feasible and z(x1) = z(x2).Consequently to Lemma 1, the differences between CSUBs come only from the (in)feasibility of extreme supported solutions of corresponding surrogate relaxations. Thus, only critical multipliers associated with these solutions will be of interest.Definition 6A critical multiplier associated with a solution x ∈ XSE1(u) is also called critical multiplier associated withCSUB(u), and x is called solution associated withCSUB(u).Given two multipliers u and u′, the proof of Lemma 1 provides an analysis of the reason why a solution x associated with CSUB(u), is not necessarily associated with CSUB(u′): either this solution is not u′-surrogate feasible orx∉XSE1(u′).Definition 7Let x be a solution associated with a CSUB, if x is u-surrogate feasible and x∉XSE1(u) then x is called u-masked. More precisely, if there is a u-surrogate feasible solution x′ such that x′ dominates x, we will say that x is u-masked by x′. If there are two u-surrogate feasible solutions x′ and x′′ such that z(x) is weakly dominated by a point in (z(x′), z(x′′)), we will say that x is u-masked by the pair of solutions x′ and x′′. More generally, a solution that is u-masked for all u in a set U will be called masked on U.Knowing the list of all multipliers associated with all possible CSUB, Proposition 4 allows to find a finite set of multipliers in order to compute the set of all CSUBs.Proposition 4Suppose we know the list of all multipliers associated with all CSUBs, then the list of all CSUBs is given by the following enumeration:(i)CSUB(0)andCSUB(1);For all interval defined by two consecutive critical multipliers:CSUB(u)for a multiplier u in the relative interior of this interval;CSUB(u)for all multiplier u that is simultaneously 0M and M1.Lemma 1 implies that CSUB(u) is identical for all u ∈ (v1, v2) where v1 and v2 are consecutive critical multipliers. It is thus useless to consider two multipliers in such an open interval (v1, v2). On the contrary, CSUB(u) and CSUB(u′) such that u and u′ belong to the interior of different intervals given by consecutive critical multipliers, implies that CSUB(u) and CSUB(u′) may be different (but are not necessarily as the converse of Lemma 1 is not true). Knowing CSUB(u) for u multiplier in the interior of all interval defined by consecutive critical multipliers is thus necessary to guarantee an exhaustive enumeration of all CSUBs (item (ii)). If 0 and 1 are not critical multiplier, one multiplier must also be considered in the interval [0, umin ) and (umax , 1] where umin  and umax  are respectively the smallest and the greatest critical multiplier, CSUB(0) and CSUB(1) are appropriate choices. Nevertheless, it is not sufficient to obtain an exhaustive enumeration of CSUBs. Indeed if we consider a multiplier v that is simultaneously 0M and M1, and a small number ε > 0 such that there is no other critical multiplier in [v − ε, v + ε]. As v is 0M then CSUB(v) may be different to CSUB(v + ε), and as v is M1 then CSUB(v) may be different to CSUB(v − ε). The computation of CSUB(v) is thus also necessary (item (iii)). For the same reason, the computation of CSUB(0) (respectively CSUB(1)) is also required if 0 is a 0M multiplier (respectively 1 is a M1 multiplier). Computing CSUB(0) and CSUB(1) is therefore required in all cases (item (i)).□The multiplier set-decomposition described by Proposition 4 does not exactly give a one-to-one correspondence between subsets of multipliers and CSUBs (as the converse of Lemma 1 is not true) but guarantees the determination of all CSUB. However, a way to obtain the set of all critical multipliers is necessary in order to use this last statement. Proposition 5 is based on an assumption that is easier to verify: only a subset of CSUBs with their associated critical multipliers are known. In this context, there may exist unknown solutions associated with unknown CSUBs, that are either u-masked or not u-surrogate feasible for each multiplier u considered so far. In order to determine these unknown solutions, we must find multipliers u such that these solutions are potentially u-surrogate feasible and not u-masked. A characterization of the intervals of multipliers S on which a solution is masked is provided by Proposition 5.Proposition 5Suppose we know some CSUBs, with their associated solutions and critical multipliers. Let x be an unknown solution associated with a CSUB, then x is u-masked for all u ∈ U where U is an union of intervals. The possible patterns for these intervals, relies on the stability interval of x.(1)If the stability interval of x is [0, 1], then these patterns are:(i)[0, v0] where v0 is a known 0M multiplier;[v1, 1] where v1 is a M1 multiplier;[v1, v0] where v0 and v1 are respectively known 0M and M1 multipliers.If the stability interval of x is [0, v], then these patterns are:(i)[0, v0] where v0 is a known 0M multiplier such that v0 < v;[v1, v] where v1 is a known 0M multiplier such that v1 ≤ v;[v1, v0] where v0 (such that v0 ≤ v) and v1 are respectively known 0M and M1 multipliers.If the stability interval of x is [v, 1]:(i)[v, v0] where v0 is a known 0M multiplier such that v ≤ v0;[v1, 1] where v1 is a known M1 multiplier such that v < v1;[v1, v0] where v0 and v1 (such that v ≤ v1) are respectively a 0M and M1 critical multipliers.(1)We assume that the stability interval of x is [0, 1].We suppose first that x is u′-masked by one solution x′ for a given multiplier u′, then x is u-masked for all multiplier u such that x′ is u-surrogate feasible. In other words, if we denote by S the stability interval of x′, then x is masked on S. The interval S is either of the kind [0, v0] (pattern (i)) or [v1, 1] (pattern (ii)) where v0 and v1 are respectively 0M and M1 multipliers. The case S = [0, 1] is not possible since x is a solution associated with a CSUB.We suppose next that x is u′-masked by the pair of solutions x′ and x′′ for a given multiplier u′, then x is u-masked for all multipliers u such that both solutions are u-surrogate feasible. In other words, if we denote the stability interval of both solutions by S1 and S2 then x is masked on S1∩S2. If both solutions are associated with 0M critical multipliers v01, v02 then x is masked on [0, min (v01, v02)] (pattern (i)). If both solutions are associated with M1 critical multipliers v11, v12, then x is masked on [max (v11, v12), 1] (pattern (ii)). If one solution is associated with a 0M critical multiplier v0 and the other one is associated with a M1 critical multiplier v1, then x is masked on [v1, v0] (pattern (iii)).We assume that the stability interval of x is [0, v] where u is a 0M critical multiplier. In order to find the possible pattern for this second case, we just need to compute the intersection of [0, v] with the pattern enumerated in the case (1). For the pattern (1)(i), we necessarily have [0, v0]∩[0, v] = [0, v0] with v > v0 (pattern (i)), otherwise x is u-masked for all u in its stability interval. For the pattern (1)(ii), we either have [v1, 1]~∩~[0, v]~ = ~[v1, v] with v ≥ v1 (pattern (ii)) or [v1, 1]∩[0, v] = ∅ if v < v1. For the pattern (iii), we have either [v1, v0]∩[0, v] = [v1, v0] (pattern (iii)) with v1 < v0 ≤ v, or [v1, v0]∩[0, v] = [v1, v] (pattern (ii)) with v1 ≤ v < v0 (we always have v1 ≤ v since solutions associated with v0 and v1 are a pair masking x).The proof is symmetric to the case (2).□Given a unknown solution x associated with a CSUB, the (disjoint) union of intervals U on which x is masked by known solutions, is composed of zero or one interval of type [0, v0] (or [v, v0] for the case (3)), zero or one interval of type [v1, 1] (or [v, v0] for the case (2)) and zero or several disjoint intervals of type [v1, v0].The exact computation of the union of intervals U would require to know the solution x. Nevertheless, Proposition 5 only gives the patterns of intervals on which an unknown x is masked. However, as the boundaries of these intervals (except v) are defined by known multipliers, it will be possible to deduce particular multipliers u such that potential unknown solutions associated with CSUBs are not u-masked by known solutions or pairs of solutions.In this section, we describe a first enumerative algorithm for the computation of the OCSUB. The main idea of this algorithm is to determine first the set of all multipliers associated with all possible CSUB (line 2 of Algorithm 1), and next to deduce the set of all CSUBs (line 3 of Algorithm 1). The OCSUB can finally be computed by an application of Definition 3.In order to initialize our enumeration, we start by computing CSUB(0) and CSUB(1) and deducing their associated critical multipliers (line 2 of Algorithm 2). The computation of these CSUBs is indeed required in all cases according to Proposition 4. Another interesting property about these two multipliers is given by Lemma 2.Lemma 2All solutions associated with any CSUB are either 0-surrogate feasible or 1-surrogate feasible or both.We already know that the stability interval of solutions associated with CSUBs are of three kinds: [0, u], [u, 1], [0, 1]. The statement follows.□Consequently to this lemma, the union of intervals on which an unknown solution x (associated with an unknown CSUB) is masked by a known solution cannot be empty after this initialization.According to Lemma 1, a way to enumerate new CSUBs is to consider multipliers u such that at least one solution associated with a known CSUB is no longer u-surrogate feasible. In other words, we need to consider multipliers u outside of the stability interval of known solutions. There will be two cases to consider, a solution is associated with a 0M multiplier or a M1 multiplier (there is nothing to do in the case of a solution the stability interval of which is [0, 1] as it is u-surrogate feasible for all u ∈ [0, 1]). In the case of a 0M multiplier v0, any multiplier u > v0 will guarantee that the solution(s) associated with v0 is (are) no longer u-surrogate feasible. Symmetrically, any multiplier u < v1 will guarantee that the solution(s) associated with a M1 multiplier v1 is (are) no longer u-surrogate feasible.In an iteration of the procedure (line 5–12 of Algorithm 2), a critical multiplier v is considered individually (a multiplier simultaneously 0M and M1 is here considered as two different multipliers) to define a multiplier u the value of which is as near as possible to v. Ideally, there must not be any critical multiplier between v and u. Thus, appropriate values u are defined by u ≔ v + ε if u is a 0M multiplier, and u ≔ v − ε if u is a M1 multiplier, where ε > 0 is a small value (see Appendix A for an upper bound on ε). CSUB(u) is next computed; and its associated multipliers are deduced.All CSUBs computed during the execution of this algorithm are merged using Proposition 2 (line 4,12 of Algorithm 2).Proposition 6Algorithm 2 finds all critical multipliers associated with all possible CSUB.Let u be a critical multiplier associated with a CSUB. We denote by x a solution associated with u. If u is associated with CSUB(0) or CSUB(1), then u is found during the initialization. If u is not associated with either CSUB(0) nor CSUB(1), then we show that v is found in an iteration of Algorithm 2. We consider thus an iteration of this algorithm and suppose that v has not been found in a preceding iteration. Proposition 5 states the pattern of intervals U the union on which x is masked by known solutions. According to Lemma 2, there is at least one interval U defined by Proposition 5 on which x is masked. The interval U has either a left-extremity defined by a M1 multiplier v1 or a right-extremity defined by a 0M multiplier v0 or both. Algorithm 2 considers the computation of all CSUBs of the following kind: CSUB(v0 + ε), CSUB(v1 − ε), with possible new solutions and critical multipliers found. There are two possible conclusions: u is one of these new critical multipliers or not (i.e. all solutions associated with u are still masked). In the latter case, new critical multipliers allow to perform new iterations. Again, u will be found or not. By repeated computations of CSUBs, u will be finally found in a finite number of computations of CSUB, as the number of critical multipliers is finite and as multipliers v such that x is not v-masked by known solution(s) can always be deduced.□After the first part, all critical multipliers associated with all CSUBs are known. Thus, Proposition 4 can be used in order to compute all CSUBs. We summarize the CSUBs computed in the part 1:(i)CSUB(0) and CSUB(1) in the initialization;CSUB(u + ε) for all u 0M multiplier;CSUB(u − ε) for all u M1 multiplier.Items (ii) and (iii) of the enumeration above imply that at least one multiplier has been used in the interior of intervals defined by consecutive critical multipliers such that the left hand side is 0M or the right hand side is M1. To complete Item (ii) of Proposition 4, it remains thus to use one multiplier in the interval the left side of which is M1 and the right side of which is 0M. If we denote by u the left hand side of this last kind of interval, the multiplier u + ε is an appropriate choice for the computation of a CSUB.In conclusion, to achieve the enumeration of all CSUBs, it remains to compute:•CSUB(u + ε) for all interval of consecutive critical multipliers [u, u0] such that u is M1 and u0 is 0M (Item (ii) of Proposition 4);CSUB(u) for all multiplier u that is 0M and M1 (Item (iii) of Proposition 4).Algorithm 3summarizes this final computation.Example 1 (continued) The TotalEnumerative algorithm is performed on the instance of (2O2DKP) referred as Example 1. To facilitate the understanding of the output, Table 1reports all solutions associated with at least one CSUB and their stability interval.The solutions are represented in objective space on Fig. 5a. The stability intervals of the solutions are represented on Fig. 5b. On the representation of the stability interval of a solution xi, the dashed parts correspond to the interval of multipliers on which xiis masked.The execution of the algorithm is presented in Table 2. The OCSUB is defined by the set of extreme points (rounded to 10−2) {(30,22); (27.43,28)}. In checking Fig. 5a and Table 2, a number of obtained CSUBs, outcomed of the TotalEnumerative algorithm, appears dominated by other CSUBs. Such a situation may appear in general, and these dominated CSUBs are therefore useless for the computation of the OCSUB. The next section gives an analysis of the dominance relation between the different CSUBs.The dominance between CSUBs is based on the following lemma, illustrated by Fig. 4 (in Section 3.1 Page 11).Lemma 3We considerCSUB(ui)andCSUB(uj)two different CSUBs, defined respectively by the set of extreme points Piand Pj:1.If Pj⊂PithenCSUB(uj)dominatesCSUB(ui)(Fig. 4a)IfPj=Pj1∪Pj2such thatPj1⊂Piand the points ofPj2are either dominated by points of Pi(Fig. 4b) or weakly dominated by interior points in the edges defined by two points of Pi(Fig. 4c), thenCSUB(uj)dominatesCSUB(ui).Dominated CSUBs do not contribute to the computation of the OCSUB. It is therefore useless to determine such CSUBs. However, it is difficult to know that a CSUB is dominated by another one before its computation. In the following, we provide a characterization allowing to know without additional computation that some CSUBs are dominated.Given a known critical multiplier u, we consider CSUB(u − ε) and CSUB(u + ε) with ε small enough such that u is the only critical multiplier associated with a CSUB in the interval [u − ε, u + ε]. Suppose u is 0M and not M1, then there exists at least one solution associated with CSUB(u − ε) which is not u + ε-surrogate feasible. Since u is not M1, there does not exist a solution associated with CSUB(u + ε) that is not u − ε-surrogate feasible. Thus, by application of Lemma 3, either CSUB(u + ε) dominates CSUB(u − ε), or CSUB(u + ε) = CSUB(u − ε) (case of equivalent solutions). In both cases, the computation of CSUB(u − ε) is useless. We will say that CSUB(u + ε) weakly dominates CSUB(u − ε).We can do the symmetric analysis in the case u is M1 but not 0M and a similar one if u is both 0M and M1. Proposition 7 summarizes this result.Proposition 71.If a multiplier u is 0M and not M1 thenCSUB(u+ϵ)weakly dominatesCSUB(u−ϵ)=CSUB(u).If a multiplier u is M1 but not 0M thenCSUB(u−ϵ)weakly dominatesCSUB(u+ϵ)=CSUB(u).If a multiplier u is 0M and M1 thenCSUB(u−ϵ)andCSUB(u+ϵ)may not be comparable. Both weakly dominateCSUB(u).If 0 is a critical multiplier, then 0 is 0M and Proposition 7 holds, except that we ignore the comparison with CSUB(0 − ε) which is not defined. Similarly if 1 is a critical multiplier, it is M1 and CSUB(1 + ε) is not defined.Proposition 7 is illustrated in Fig. 6, an arrow from CSUB(uj) to CSUB(ui) means that CSUB(uj) weakly dominates CSUB(ui).Proposition 7 allows only a local analysis around critical multipliers. We will say that we can know a priori that a CSUB is weakly dominated. In the following, we apply the result of this proposition on the set of all critical multipliers associated with any CSUB, to obtain a stronger result.Definition 8Let L be a list of critical multipliers sorted by increasing values. We call 0M–M1 interval in L any pair u0 − u1 of successive critical multipliers in L∪{0, 1}, such that u0 is 0M and u1 is M1 and u0 < u1. 0 and 1 are respectively considered as 0M and M1 multipliers.We denote by Lallthe list of all critical multipliers associated with all CSUBs. The only CSUBs that are not weakly dominated a priori areCSUB(u)where u0 < u < u1 for any 0M–M1 interval u0 − u1 in Lall.We consider all possible types for a pair of critical multipliers u1 and u2 and we analyze the dominance relation between the CSUBs around these two critical multipliers. We only present the case for which u1 is 0M (but not M1). The proof is similar if it is M1 or, 0M and M1. As u1 is 0M, Proposition 7 implies that CSUB(u1 + ε) weakly dominates CSUB(u1) = CSUB(u1 − ε). As there is no critical multiplier between u1 and u2, CSUB(u1 + ε) = CSUB(u2 − ε). The question is next whether CSUB(u1 + ε) is weakly dominated or not by CSUB(u2 + ε) or CSUB(u2). In all cases, the answer comes from the application Proposition 7 for the multiplier u2.1.If u2 is 0M and not M1, then CSUB(u1 + ε) is weakly dominated by CSUB(u2 + ε).If u2 is M1 and not 0M, then CSUB(u1 + ε) weakly dominates CSUB(u2 + ε).If u2 is 0M and M1, then CSUB(u1 + ε) weakly dominates CSUB(u2).The only cases for which CSUB(u1 + ε) = CSUB(u2 − ε) is not dominated a priori, happen when u2 is M1.Suppose 0 is not a critical multiplier, and we denote the smallest critical multiplier by umin. The same CSUB is therefore obtained for CSUB(u) for u ∈ [0, umin). Depending of the type of umin, CSUB(umin − ε) is weakly dominated (umin  is 0M) or not (umin  is M1). The case of the multiplier 1 is symmetric.□Fig. 7illustrates Propositions 7 and 8. Dominance between CSUBs is again represented by an arrow, and the CSUBs that are not dominated a priori are denoted in solid black.The 0M–M1 interval algorithm is based on Proposition 8 which states that the only multiplier u such that CSUB(u) is not dominated a priori are in the interior of the 0M–M1 intervals in Lall. Obviously, the direct use of this result requires to know Lall. Knowing only a subset of CSUBs with their associated solutions and critical multipliers, Proposition 5 defines an union of (patterns of) intervals U such that an unknown solution associated with a critical multiplier v is masked on U. To find such an unknown solution x, it is necessary to compute CSUB(u) for u ∈ S∖U where S is the stability interval of x. According to Proposition 5, U is composed of zero or one interval of type [0, v0] (or [v, v0] if v is a M1 multiplier), zero or one interval of type [v1, 1] (or [v1, v] if v is a 0M multiplier) and zero or several disjoint intervals of type [v1, v0] where the multipliers denoted v0 and v1 are known 0M and M1 multipliers. It is interesting to note that the set S∖U is an union of 0M–M1 intervals defined by a subset of known multipliers, with the exception of at most one interval for which the extremities are a known critical multiplier and v (unknown).The 0M–M1 algorithm (Algorithm 4) is based on these observations. An iteration of the algorithm is performed as follows: given a 0M–M1 interval u0 − u1 in the list of known multipliers, CSUB(u0 + ε) is computed and the new critical multipliers are stored. We call this operation the exploration of the 0M–M1 interval. Since the exploration of a 0M–M1 interval is based on the 0M multiplier, we store the 0M multiplier for each explored 0M–M1 interval, in order to avoid repetitive computations of a same CSUB. Indeed, new multipliers may be found with the computation of a new CSUB. If a new M1 multiplier is inserted between a pair of 0M and M1 multiplier, a new 0M–M1 interval is defined, but its exploration is done by computing the same CSUB.The initialization is reduced to the computation of CSUB(0). Indeed CSUB(1) is either dominated or redundant with the exploration of a 0M–M1 interval.In this part we prove the correctness of the 0M–M1 interval algorithm. In this proof, we denote by Lalg the list of known critical multipliers at the end of Algorithm 4. This proof is decomposed in two propositions, its main idea is to show that the list Lalg contains at least the minimum number of critical multipliers to guarantee that the OCSUB is obtained as an output of the algorithm. As Algorithm 4 computes CSUB(u0 + ε) for each 0M–M1 interval u0 − u1 in Lalg, it is necessary for Lalg to contain all 0M multiplier that defines a 0M–M1 in the list Lall(Proposition 9). Moreover to ensure that CSUB(u0 + ε) is computed for all u0 defining a 0M–M1 interval u0 − u1 in Lall, Lalg must also contain a M1 multiplier u2 > u0 such that there does not exist any 0M multiplier in (u0, u2) (Proposition 10).We can note that for a same u0 defining a 0M–M1 interval u0 − u1 in Lall, several M1 critical multipliers are appropriate to ensure the computation of CSUB(u0 + ε). To illustrate this, we consider the example of Fig. 7. The CSUBs to be computed to guarantee that the OCSUB is found, are CSUB(u + ε) with u = u2, u5, u6, u7, u10. Lalg must therefore contain these 0M critical multipliers. The M1 critical multiplier that may cause the exploration of the 0M–M1 interval u2 − u3 (in Lall) can be either u3 or u4. Indeed there does not exist any 0M critical multiplier in (u2, u3) or in (u2, u4), thus the explorations of u2 − u3 and u2 − u4 are the same: CSUB(u2 + ε). The other M1 multipliers to find are u6, u7 and, u8 or u9.Proposition 9Algorithm 4 finds all multiplier u0 such that there is a 0M–M1 interval u0 − u1 in Lall.Let u0 − u1 be a 0M–M1 interval in Lall, and x0 be a solution associated with u0. We show in the following that x0 (and thus u0) is found by Algorithm 4. If x0 is associated with CSUB(0), then x0 is found with the initialization. If x0 is not associated with CSUB(0) then it is 0-masked (since x0 is 0-surrogate feasible). By Proposition 5, the union of intervals on which x0 is masked is composed of zero or one interval [0, v0], zero or several intervals of the kind [v1, v0], and zero or one interval of the kind [v1, u0], where the multipliers denoted v1 and v0 are respectively M1 and 0M multipliers in Lalg. Since x0 is 0-masked, it is in particular masked on one interval [0, v0]. Consequently, x0 is surrogate feasible and not masked by known solutions on a nonempty union of zero or several (v0, v1) open intervals, and zero or one (v0, u0] interval. The computation of CSUB(u) for u in the interior of one of these intervals, performed in the context of Algorithm 4, necessarily guarantee to obtain x0. Otherwise new solutions masking x0 would be found with their associated critical multipliers, and this contradict the assumption that Lalg is the list of known critical multipliers at the end of Algorithm 4.•If the first kind of interval (v0, v1) exists, v0 and v1 are respectively a 0M and M1 multiplier in Lalg. Hence, there is a 0M–M1 interval (in Lalg) in [v0, v1] and this 0M–M1 interval is explored during the algorithm.If the second kind of interval (v0, u0] exists, we prove next that there is a 0M–M1 interval in Lalg such that the 0M part is in [v0, u0].–If there is no 0M multiplier bigger than u0 in Lalg, then since 1 is considered as a M1 multiplier, there is a 0M–M1 intervalu0′−1in Lalg whereu0′is the biggest 0M multiplier found.If there is at least one 0M multiplier greater than u0 in Lalg, we denote byu0SFthe smallest one. Since u0 − u1 is a 0M–M1 interval in the list Lall, then there exists at least one M1 multiplier in(v0,u0SF](at least u1). We prove next that at least one of these multipliers necessarily belong to Lalg.Suppose there is no M1 critical multiplier in(v0,u0SF]∩Lalg,we consider u2 a M1 multiplier in(v0,u0SF]∩Lalland x2 one of its associated solution. If there are several possible critical multipliers u2, we consider one such that x2 is not masked by any solutions or pair of solutions associated with any other M1 multipliers of the interval(v0,u0SF]. By Proposition 5, the union of intervals on which x2 is masked is composed of zero or one interval[u2,v0′],zero or several intervals of the kind[v1′,v0′],and zero or one interval of the kind[v1′,1],where the multipliers denotedv1′andv0′are respectively M1 and 0M multipliers in Lalg. Consequently, x2 is not masked on a nonempty union of zero or several(v0′,v1′)intervals, and zero or one[u2,v1′)interval. The computation of CSUB(u) for u in the interior of one of these intervals, performed in the context of Algorithm 4, necessarily guarantee to obtain x2.*If the former kind of interval(v0′,v1′)exists,v0′andv1′are respectively a 0M and M1 multiplier in Lalg. Hence, there is a 0M–M1 interval in[v0′,v1′]∩Lalgand this 0M–M1 interval is explored during the algorithm. Consequently, x2 and thus u2 is found.If the later kind of interval[u2,v1′)exists, we havev0<u0<u0SF<v1′. There exists a 0M–M1 interval in[v0,v1′]∩Lalg. If its 0M part is smaller than u0 its exploration is the computation of a CSUB in (v0, u0), and consequently x0 (and u0) is found. Otherwise, the 0M part is greater than u0 (and thus greater thanu0SF), consequently u2 and x2 are found.□If u0 − u1 is a 0M–M1 interval in Lall, then there is a M1 multiplier u2 ∈ Lalgsuch that no 0M critical multiplier exists in (u0, u2).Let u0 − u1 be a 0M–M1 interval in Lall. By Proposition 9, u0 ∈ Lalg.If u0 is the greatest 0M multiplier in Lall, then there is no 0M multiplier in (u0, 1) and 1 is (considered as) a M1 multiplier in Lalg.If u0 is not the greatest 0M multiplier in Lall, we denote byu0Sthe smallest 0M multiplier in Lall such thatu0<u0S. Since 1 is a M1 critical multiplier, there exists a 0M–M1 interval in Lall in[u0S,1]. According to Proposition 9 the 0M part of this interval is also found by the algorithm. Thus there is at least one 0M multiplier greater than u0 in Lalg, we denote byu0SFthe smallest one. Since there is no 0M multiplier inLalg∩(u0,u0SF),there does not exist any M1 critical multiplier inLall∩(u0S,u0SF)(otherwise according to Proposition 9 a 0M critical multiplier would be found in(u0,u0SF)). Thus the M1 multipliers in(u0,u0SF]and in(u0,u0S]are the same.We show in the following that there is a M1 multiplier inLalg∩(u0,u0SF). We consider u2 a M1 multiplier in Lall in(u0,u0SF]and an associated solution x2 that is not masked by any other solution (or pair of solutions) associated with a M1 multiplier in(u0,u0SF]. By Proposition 5, the union of intervals on which x2 is masked is an union of zero or one interval [u2, v0], zero or several intervals of the kind [v1, v0], and zero or one interval of the kind [v1, 1], where the multipliers denoted v1 and v0 are respectively M1 and 0M multipliers in Lalg. Consequently, x2 is not masked on a nonempty union of zero or several (v0, v1) intervals, and zero or one [u2, v1) interval.•If the former kind of interval (v0, v1) exists: there exists a 0M–M1 interval in [v0, v1] in Lalg and it is thus explored during the algorithm.If the latter kind of interval [u2, v1) exists: since there is no M1 critical multiplier found in(u0,u0SF]thenu0SF<v1. Thus there exists a 0M–M1 interval in[u0SF,v1]⊂[u2,v1]and it is thus explored during the algorithm.Example 1 (continued)Algorithm 4 is applied on the instance of (2O2DKP) introduced as Example 1. At the beginning multipliers contains the M1 multiplier 1. The first bound set to be computed is CSUB(0) = B1. x1 and x2 compose this bound set. Then14and34(of type 0M) are added to multipliers.The only 0M–M1 interval in multipliers is34−1. Then the next CSUB computed isCSUB(34+ϵ)=B2. This is composed of x7 and x8. Thus the multiplier813(of type M1) is added to multipliers (represented in Fig. 8).There is two 0M–M1 intervals in multipliers now:14−813and34−1. SinceCSUB(34+ϵ)has already been computed then we do not compute it again.CSUB(14+ϵ)has not been computed before. So we compute it, it is B3 composed of x2 and x5. The new 0M multiplier716is added to multipliers.716−813is the only new 0M–M1 interval. SinceCSUB(716+ϵ)=B4has not been computed yet, we compute it now. It is composed of x2 and x6. The new M1 multiplier716is added to multipliers. The content of multipliers is represented in Fig. 9.The three 0M–M1 intervals found are14−716,716−813and34−1. All of them have already been explored. So the algorithm ends.The list of critical multipliers of the 0M–M1 interval algorithm can be initialized using the dichotomic algorithm we presented in Section 2.2. We showed that this algorithm does generally not allow to determine the OCSUB and that it may stop prematurely, however it allows to determine surrogate-feasible solutions that defines edges of the OCSUB with their associated critical multipliers. The early knowledge of these critical multipliers allows next to avoid the enumeration of weakly dominated CSUBs. Finally, each step of the dichotomic method consists in solving the dual surrogate problem for2DKP,which is less time consuming than to compute a CSUB according to our observations.The next section presents a heuristic derived from the 0M–M1 interval algorithm, and a heuristic from the literature.We describe here a method to compute an approximation of the OCSUB, based on the 0M–M1 interval algorithm and called 0MM1H. This heuristic works the same way as the 0M–M1 interval algorithm, initialized by the dichotomic method, except that the number of iterations is limited to h. Thus all 0M–M1 interval will generally not be explored. Hence we have to define how we choose the 0M–M1 interval to explore at each iteration. If there are several 0M–M1 intervals u0 − u1 to explore, then we choose the one for which u1 was part of the most 0M–M1 intervals previously explored. In case of equality we choose the lowest u0. The idea is to explore improving CSUBs immediately.If there does not exist any 0M–M1 interval to explore, then the method stops even if the maximum number of iterations is not achieved. Indeed the OCSUB is found, any additional computation of CSUB is therefore useless.An alternative and more intuitive heuristic, with a lower running time per iteration since it does not analyze the critical multipliers, is now presented.The SurrogateFamily heuristic (SF) has been introduced in the master thesis of Perederieiva (2011). The author has developed a dynamic programming based algorithm to find all efficient solutions of a2O2DKP. The upper bound set defined in this algorithm is based on the linear and surrogate relaxations. The resulting upper bound set is obtained by application of Proposition 1, where the upper bound set associated with the linear relaxation and the one based on surrogate relaxations are merged.SurrogateFamily has not been designed with the aim to compute the OCSUB, its purpose is to compute an upper bound set in a short computational time. Thus, there is no guarantee to determine the OCSUB using this method, even with a large number of iterations.For our numerical experiments, the part related to the linear relaxation is disabled; only the part related to the surrogate relaxation is computed and compared with 0MM1H. The principle of SurrogateFamily is simple. It computes h CSUBs: {CSUB(u1), ~…, ~CSUB(uh)} where the multipliers {u1, …, uh} are equidistant in the interval [0, 1]. Obviously if we increase the value of h, a tighter upper bound set will be computed, but with a larger computational cost.In SurrogateFamily, the choice of the multipliers is completely static. No information is used to avoid the computation of dominated CSUBs. In particular, even if the OCSUB is obtained, SurrogateFamily will continue its execution until the last iteration.The numerical experiments aim (1) to show experimentally the performances of proposed algorithms, (2) to observe if an algorithm presents in general better performances than its competitor, (3) to examine the behavior of the algorithm when the number of variables increases, and (4) to note, if it exists, the influence of correlations between the values of the instances on the behavior of the algorithm. To discuss about those factors, four descriptors are elaborated. The two first concern the exact algorithms (TotalEnumerative and 0M–M1 interval),•Descriptor 1. The number of computed CSUBs. A low number shows a fast convergence to the OCSUB.Descriptor 2. CPU time to get the OCSUB.The two last are for the approximation algorithms (0MM1H and SurrogateFamily).•Descriptor 3. Quality of the approximation. The quality assessment of an approximated bound set will be stated in comparison with the OCSUB. Here, an area measure (calledA-metric) is proposed and reported in terms of percentage. A smallA-metric value indicates an approximation close to the OCSUB.Descriptor 4. CPU time to get the approximated OCSUB.All our implementations require to fix a value for ɛ (small positive number used in Algorithms 2, 3 and 4). The smallest value manageable by our solver’s implementation is ɛ = 10−9 (the recommended value returned using the computing method described in Appendix A is smaller). Next, the methods approximating the OCSUB require to fix a value for the parameter h. The following values have been used: 10, 50, 100.The computer used for the experiments is equipped with a Intel Core i7 2640M 2.8 Ghz processor with 8 GB of RAM, and runs under Mac OS X Lion 10.7.5. All algorithms have been implemented in C++. The binaries have been obtained using the compiler GCC version 4.2.1. with the optimizer option − O3. Combo from Martello et al. (1999) has been used to solve single-objective single-dimensional knapsack problems.All algorithms have been evaluated on a dataset composed of 28 instances available on the MCDMLib,11http://www.mcdmsociety.org/MCDMlib.html.and organized in three groups:•Group 1: 6 instances of various sizes, randomly generated with the same generator, without any specific correlation. ZTL100, ZTL250, ZTL500 and ZTL750 are 4 instances picked from the collection maintained by Zitzler and Laumanns.22http://www.tik.ee.ethz.ch/sop/download/supplementary/testProblemSuite/.ZTL28 and ZTL105 are 2 additional instances derived respectively from ZTL100 and ZTL250, whereωi≈0.5∑j=1nwij,i=1,⋯,m. The number following ZTL in the name gives the number of variables.Group 2: 15 correlated instances with 28 variables, introduced by Perederieiva (2011). A1, A2, A3, A4, D1, D2, D3, D4, kp28W-ZTL, kp28, kp28-2, kp28W, kp28W-Perm, kp28c1W-c2ZTL and kp28cW-WZTL are extention of2DKPinstances available on the OR-library33http://people.brunel.ac.uk/~mastjjb/jeb/orlib/mknapinfo.html.where a second objective has been added with respect to several definitions of correlation to obtain a2O2DKPinstance (see Perederieiva, 2011 for further details).Group 3: 7 additional correlated instances with 105 variables, obtained following the rules described for the Group 2. W7BI-rnd1-1800, W7BI-rnd1-3000, W7BI-tube1-1800, W7BI-tube1-3000, W7BI-tube1-asyn, W7BI-tube2-1800, Wcollage-tube are summarized in Table 3.In this section, we compare the two methods that determines the OCSUB. In order to measure the impact of its initialization, the 0M–M1 interval algorithm is tested with and without its initialization. All results are summarized in Fig. 10. In this figure, we denote by respectively TotEnum, 0MM1 and 0MM1 + init, the TotalEnumerative algorithm, the 0M–M1 interval without initialization, and the 0M–M1 interval algorithm using the initialization.In Fig. 10, the y-axis is reported on a logarithmic scale for all graphics. The left column reports the numbers of computed CSUBs (descriptor 1), and the computational times (descriptor 2) are on the right column. The line on the top, the middle and the bottom correspond respectively to instances of Groups 1, 2 and 3.For all three groups of instances, the two versions of 0M–M1 interval are always faster than TotalEnumerative. The profile of the curves of time are similar. The difference of execution time increases with the size of the instance. This especially allows 0M–M1 interval without initialization to determine the OCSUB for ZTL500 and ZTL750 instances with less than 700 seconds of CPU time, while TotalEnumerative is unable to compute it in less than 1 hour.The additional cost of the initialization in the 0M–M1 interval algorithm is largely balanced for most instances. The only exceptions concerns small instances which solution time is particularly low with or without initialization (less than 0.1s). Moreover, the benefit of the initialization increases significantly with instance size, as can be seen in Fig. 10(b).For Groups 2 and 3, an obvious correlation appears between the required computational time and the number of CSUBs to compute. On the other hand, there is no particular difficulty for the algorithms to deal with numerical instances presenting various correlations. For four particular instances (D2, D3, D4 and tube-asyn), TotalEnumerative algorithm computes a large number of CSUBs, while 0M–M1 interval only computes 1. This difference is of course reverberated on the computational time.In conclusion of the analysis, 0M–M1 interval outranks TotalEnumerative, and shows no particular difficulty to deal with instances with correlations. The initialization procedure allows to largely reduce the computational time for large instances. The recommended algorithm for computing the OCSUB is thus the 0M–M1 interval algorithm with initialization. However, the computational time of the OCSUB remains far to be negligible, in particular for big instances. A better trade-off between computational time and quality of the obtained upper bound set should be used, to be embedded as a component of an implicit enumeration algorithm aiming to generate a complete set of efficient solutions for a2O2DKP.In order to evaluate the quality of the heuristics, we define a quality indicator. It aims to measure the gap between a heuristic upper bound set and the OCSUB. According to Definition 1, an upper bound set B for YNshould verifyYN⊂(B−R≧2). If we consider an upper bound set H obtained by 0MM1H or SurrogateFamily heuristics, we necessarily have(OCSUB−R≧2)⊂(H−R≧2). We denote next byA(V)the area of the polyhedraV∩R≧2. A possible way to measure the gap between H and the OCSUB could be to compareA(H−R≧2)andA(OCSUB−R≧2).In order to have the same scale for all instances, we could consider the ratioA(H−R≧2)−A(OCSUB−R≧2)A(OCSUB−R≧2). However, such an indicator would be very sensitive to the range of data. In particular, for instances for which the objective values of nondominated points are very large, there would be a large intersection betweenA(H−R≧2)andA(OCSUB−R≧2),for all possible upper bound sets H. Consequently, an indicator defined this way would necessarily return small values.In order to highlight the differences between bound sets, we propose to define our quality indicator by putting aside the common part betweenA(H−R≧2)andA(OCSUB−R≧2),for all possible upper bound sets H. Fig. 11illustrates the definition of this indicator. The OCSUB is composed of b extreme points {p1, …, pb} lexicographically ordered. The search area at the bottom left of p1 is common to all possible upper bound set H and to the OCSUB, thus we do not need to consider it in the measure. The same statement holds for pb. ThusC:=(p1−R≧p)∪(pb−R≧p)is included into(H−R≧2)for all possible upper bound set H. In the following, we callA-metric (Figs. 12–14) the following measure of quality of a heuristic bound set H:A(H−R≧p)−A(OCSUB−R≧p)A(OCSUB−R≧p)−A(C)×100This quality indicator has the effect to highlight with large values the difference between bound sets.The results are summarized in Figs. 12–14 respectively for instances of Groups 1, 2, and 3. In each figure, the graphics (a), (b) and (c) compare both heurstics according to descriptor 3 for respectively h = 10, h = 50 and h = 100. The y-axis is logarithmic. For an instance and a heuristic, if a bar is not visible, it means that the obtained upper bound set is the OCSUB or so close to it that the difference is insignificant. The graphics (d) present the computational times in six curves (descriptor 4), the names of the curves are composed by the name of the heuristic and the parameter h. To read this graphics, it is important to notice that, for a same heuristic, the curve for a given value of h cannot be lower than the one for a smaller value of h (h is the maximum number of iterations allowed). Thus in graphics (d), for a same heuristic, the curve at the bottom corresponds to h = 10 and the one at the top to h = 100.For all groups of instances, both algorithms behave the same way in terms of computational time. Generally the computational time of 0MM1H is slightly larger than for SurrogateFamily, because of the initialization and the analysis of the critical multipliers. For some instances, essentially from Group 2, 0MM1H appears faster. This is due to the early obtention of the OCSUB by 0MM1H. Consequently, this heuristic stops its execution, while SurrogateFamily continue its execution until the final iteration.In 85 percent of the cases 0MM1H performs better than SurrogateFamily, according to descriptor 3. When 0MM1H is performed with h = 100, the OCSUB is obtained for 16 of the 28 instances, doing less iterations than h.The quality of the upper bound set found by 0MM1H does not seem to be related to the size of the instances. Indeed in Group 1, the performance of 0MM1H is outranked by SurrogateFamily for the instances of size 105 and 750 variables, however the opposite occurs for the instances of size 250 and 500. For the instances of Group 2, 0MM1H gives upper bound set of good quality in a smaller number of iterations than SurrogateFamily, except for two instances (A1 and A3). The difference between both heuristics is also important for several other instances (for example A2), in favor of 0MM1H. In Group 3, both heuristics are almost equivalent in terms of descriptor 4. Finally, the best heuristic for an instance is not necessarily the same for every value of h. The instance tube1-1800 is an example of this phenomenon: for h = 10 both heuristics are equivalent, for h = 50 SurrogateFamily performs better and the opposite occurs for h = 100.The difference of behavior can be explained by the use of uniformly distributed multipliers in SurrogateFamily. Therefore, the obtained bound set is constructed from various CSUBs. In 0MM1H, the values of the multipliers are determined with the purpose to improve locally the bound set. Thus, with a low number of computed CSUBs, we obtain a bound set that may be very tight locally but not globally. This explanation is confirmed by the results collected for Group 2. In conclusion of the analysis, 0MM1H is competitive for every size of instances and every tested h, except for some rare instances. A deep analysis of those instances may allow us to find a better order for the exploration of the 0M–M1 intervals. The heuristic using this order could provide better results.The optimal convex surrogate upper bound (OCSUB) is the tightest bound set that it is possible to define and to compute with the convex surrogate relaxation. This paper gives a formal definition of the OCSUB, introduces two exact algorithms for computing the OCSUB, and demonstrates their correctness. A heuristic version is also introduced for a use when the computational time is limited. The numerical experiments clearly demonstrate the effectiveness of the algorithm called 0M–M1 interval, using the initialization procedure, for the computation of the OCSUB, and show that the algorithm is not sensitive to the presence of correlation in the numerical instances.The OCSUB finds its usage naturally in the context of an implicit enumerative method for solving a bi-objective bi-dimensional binary knapsack problems (2O2DKP). For example, OCSUB is ready to be embedded in an evaluation component aiming to prune or not a node of a branch-and-bound algorithm, or a state in dynamic programming algorithm. However, it is not surprising to note that the computation of the OCSUB may require a significant CPU time, even for mid-size numerical instances. Due to this fact, and in the state of knowledge, it is currently not reasonable to consider such an usage of the OCSUB. The introduction of a heuristic variant for the computation of the OCSUB is therefore justified. The quality of the resulting measured approximation, even for a short CPU time, shows the practical interest of the heuristic algorithm to compute an approximation of the OCSUB. Thus, it appears as a powerful component to encapsulate in an implicit enumeration method, in particular to solve instances of 2O2DKP. 0MM1H seems to be a good choice to determine an approximation of the OCSUB, even if its efficiency drops for some rare instances. However there remain several possible improvements that may change favorably this situation.The following prospects allow to expect interesting improvements in terms of the OCSUB computation time, and on the use of the 0MM1H in an implicit enumeration method. It is linked to the use of these algorithms in an implicit enumeration method, following a re-optimization principle, e.g. between two levels of the enumeration tree. The idea aims to avoid to repeat computations between two successive OCSUBs (or approximations), while the two nodes in the tree vary only by a small subset of variables changing from a free status to fixed. These ideas are currently under research.

@&#CONCLUSIONS@&#
