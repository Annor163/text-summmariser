@&#MAIN-TITLE@&#
Group counseling optimization

@&#HIGHLIGHTS@&#
A new population-based optimizer group counseling optimizer (GCO) is developed.We emulate the human beings in life problem solving through counseling within a group.The algorithm is tested using benchmark functions and a real-world application.A comparison is made with the state-of-the-art algorithms.The results are all highly promising, showing the efficacy of the proposed approach.

@&#KEYPHRASES@&#
Optimization,Group counseling,Particle swarm optimizer,NBIPOP-aCMA-ES,Benchmark test functions,Real-world application,

@&#ABSTRACT@&#
In this paper, a new population-based optimization algorithm – which we call a group counseling optimizer (GCO) – is developed. Instead of mimicking the behavior of living organisms such as birds, fish, ants, and bees, we emulate the behavior of human beings in life problem solving through counseling within a group. This is motivated by the fact that the human's thinking is often predicted to be the most reasonable and influential. The inspiration radiates from the various striking points of analogy between group counseling and population-based optimization which we have discovered, as elucidated in Section 2. The algorithm is tested using seven unrotated benchmark functions and five rotated ones. Further, a comparison is made with the comprehensive learning particle swarm optimizer (CLPSO) which outperforms many other variants of the particle swarm optimizer. Using new eight composition benchmark functions, another comparison is made with the BI-population covariance matrix adaptation evolution strategy with alternative restart strategy (NBIPOP-aCMA-ES) which is the winner of the competition on real-parameter single objective optimization at IEEE CEC-2013. The results are all highly promising, demonstrating the soundness and efficacy of the proposed approach. GCO is applied to real-world application which is spacecraft trajectory design problem. Also, the results show that GCO outperforms well-known optimizers.

@&#INTRODUCTION@&#
Optimization is the computational discipline devoted to the study of the ‘best’ solution of a problem [1,2]. It, mathematically, means either minimization or maximization of a certain objective function. Losses and drawbacks are to be minimized, whereas profits and merits are to be maximized. All of us seek optimum or, at least suboptimum, solutions because we often aspire to a better way of life. It is no exaggeration to assert that looking for a solution of an optimization problem is as old as human history itself.A great many optimization approaches have been developed and consolidated over the decades. From 1960 onwards, attention has been particularly focused on the category of population-based optimizers [3]. A prominent property of the computational algorithms of this category is that an iterative policy is followed, which relies on a group, or population, of candidate solutions, not just one solution. During the iterations, a population of constant size is maintained, and the group of solutions is improved progressively. The adoption of successive solution groups is advantageous in that working in groups is generally more productive than individual efforts. Specifically, improving a solution in an iteration can benefit from other solutions in the group, in the sense that the new value of a solution (to be used in the next iteration) can be deduced through ‘interaction’ or ‘cooperation’ with other solutions in an algorithm-dependent manner. Having a group of solutions ‘working together’ is the key to the development of modern biology-inspired optimizers, in which the behavior of biological organisms is emulated. In other words, a biological ‘metaphor’ does stimulate the algorithm. In what follows, we refer to five famous example algorithms and their pertinent metaphors.The genetic algorithm (GA) [4,5] emulates genetic evolution in biological organisms according to the theory of the Charles Darwin. It depends on the construction of an evolutionary computation scheme, using models of evolutionary processes such as ‘natural selection’, ‘survival of the fittest’, and ‘reproduction’. In a world with limited resources, each individual enthusiastically competes with others for survival. Individuals having the best traits are more likely to survive and reproduce, and these traits will be passed on to their offspring. With time, desirable qualities are inherited from generation to generation, and they turn dominant in the population.The particle swarm optimization (PSO) [6–8] began as a computer simulation of the social behavior of biological organisms living in groups such as a flock of birds, and a school of fish, where no leader can be recognized. Within such social groups, individuals are not very knowledgeable about the overall behavior of the group, nor are they fully aware of their environment. But they do have the capability of gathering as well as travelling and managing together, without any collision or apparent conflict. In doing all this, a plain principle is obeyed: imitation of successful activities of neighboring individuals. Intrinsic local interaction among individuals brings about intricate, graceful behavior that characterizes bird flocking, fish schooling, collective foraging, and many other aspects of living. In analyzing group dynamics of bird social behavior, inter-individual distances play a major role; that is, the synchrony of flocking behavior is conceived to hinge on the effort exerted by the birds to preserve optimum separation between themselves and their neighbors, so that cooperation of individuals becomes feasible and effective.The ant colony optimization (ACO) [9] is based on the foraging behavior of ants. Such social insects are capable of finding the shortest path between their nest and a food source, with no visible, centralized coordinating mechanism. There exists an initial chaotic activity pattern in the search for food but, once a food source is located, activity patterns become well organized and ant groups come to go along the shortest path heading for the food source. In an infinitesimal time interval, all ants follow the same path. Here, through an orderly recruitment process, the ants that discovered a food source direct other ants toward it. Most ants indirectly communicate with each other by means of secreting a chemical scented substance called pheromone. When an ant locates a food source, it carries a food item to the nest and deposits pheromone along the trail. Forager ants decide which path to select on the basis of the concentration of pheromone on the various paths. The path with higher pheromone concentration has a greater probability of being selected. As more ants follow a specific path, the desirability of that path is strengthened by extra pheromonal secretion of the foragers, and thus more and more ants are attracted to it.The artificial bee colony (ABC) optimization [10–12] is based on the foraging behavior of honey bees. A forager bee leaves the hive looking for a rich food source, a patch of flowers, to gather nectar from. For multiple food sources, forager bees are allocated among different flower patches in such a way as to maximize total nectar intake. The bee stores the nectar in her honey stomach, and a honey-making process begins with secreting an enzyme on the nectar. On returning to the hive, the bee unloads the nectar into empty honeycomb cells, and some extra enzymes are added to avoid fermentation and bacterial attacks. Then, the forager bee that has found a food source performs attractive movements, visualized to as a ‘dance’, around the place of the comb. Through dancing, she announces her information about the food source, such as how plentiful it is and where it is located. Other bees touch her with their sensory antennas and learn, moreover, the scent and taste of the food of the source. In this way, groups of bees are recruited to exploit the same source. ABC was improved to be able to detect the global optima via a lot of research works. Some of newly improvements can be found in [13,14].The differential evolution algorithm (DE), which is considered an extension to GA, was originated by Storn and Price in 1997, for minimization problems in terms of a cost function [15]. It participated in the First International IEEE Competition on Evolutionary Optimization (ICEC’96), and proved to be the fastest evolutionary algorithm at the time (although it came third among deterministic methods). In this algorithm, use is made of concepts of mutation, crossover, and selection, but with specific mathematical definitions, generally different form those of the genetic algorithm. The guiding principle is that information from within a population of parameter vectors is utilized to produce a new vector population of the same size. The scheme of computations depends on using a weighted difference vector of two randomly chosen vectors so as to vary (perturb) some other third vector. The perturbation is done for every population vector, without resort to a predefined probability distribution function. This is the process of mutation in differential evolution. The vector resulting after perturbation is a mutant vector. Some of the components of the mutant vector are ‘mixed’ with some components of the target vector to form the so-called trial vector. This is the process of crossover. Over the past years, researchers enhanced the DE behavior through various ideas. Some of the recent enhancements exist in [16,17].The above-mentioned algorithms, and many others, are useful and the accompanying metaphors are interesting. Yet the field of computational optimization is extensive and still open for further research work and advanced ideas with no foreseeable end. In the present paper, we do not concern ourselves with improving or even overcoming shortcomings of any one of these algorithms. Our main aim is to introduce a new population-based optimization algorithm inspired by an utterly different metaphor. Instead of mimicking the behavior of living organisms such as birds, fish, ants, or bees, we emulate the behavior of human beings in life problem solving through counseling within a group[18–20]. This is motivated by the fact that the human's thinking is, or should be, the most reasonable and influential. The subject of counseling is well known in sciences like psychology and sociology, but may seem rather obscure in computational optimization. The contribution of this paper is therefore twofold. First, we investigate some of the basic counseling concepts and procedures in an attempt to make counseling emerge as a convincing and appealing metaphor for population-based computational optimization. In this conceptual framework, we identify twenty striking items of significant analogy. Second, we utilize these metaphoric items to develop what we call a group counseling optimizer (GCO). The proposed algorithm is compared with the comprehensive learning particle swarm optimizer (CLPSO) [21], which outperforms several variants of the particle swarm optimizer, through use of seven unrotated benchmark functions and five rotated ones. An additional comparison is made with the BI-population covariance matrix adaptation evolution strategy with alternative restart strategy (NBIPOP-aCMA-ES) [22,23], which is the winner of the competition on real-parameter single objective optimization at IEEE CEC-2013 [24], using new eight composition benchmark functions. Results, including error values and convergence characteristics, obtained for GCO are highly satisfactory, demonstrating that the link we have established between group counseling and computational optimization is healthy, authentic, and valuable. We point out that a preliminary version of GCO, with unrotated benchmark functions alone, has been published in [25]. A multi-objective version of GCO is recently published in [26], which gives promising results in solving multi-objective optimization problems. To test the applicability, GCO is applied to a real-world application. Also, the results show that GCO outperforms well-known optimizers.The remainder of the paper is organized as follows: Section 2 introduces the analogy items between group counseling and the population-based optimization. In Section 3, the proposed algorithm, based on group counseling is introduced and the steps of GCO algorithm are explained in details. In Section 4, algorithmic comparison between GCO and other optimizers is presented. In Sections 5–7 the results of the experiments conducted on seven unrotated and five rotated and eight composition benchmark functions are given. A real world application of GCO is presented in Section 8. The conclusions are finally discussed in Section 9.People with problems often seek out another person as a sounding board: someone with whom they can talk over their problems, experiment with various solutions and finally reach some resolution. Examples of this approach are seen when people have relationship difficulties or want to change jobs or places of residence. The person, for instance, who wants to change his or her job may be advised by another person who has experience of job opportunities that exist in related careers [18]. Hence, people start to seek help when they should make a decision or solve a problem.Counseling can be thought of as a process of problem solving [19]. Individual counseling is an activity in which one person is helping (counselor) and one is receiving help (counselee) and in which the emphasis of that help is on enabling the other person to find solutions to problems [18]. However, individuals function most of their lives within groups. So, instead of the individual counseling there is another kind of counseling called group counseling [20] that offers the unique advantages of providing group members with the opportunity to discover that their peers also have problems and to learn new ways of resolving problems by observing other members in the group deal with those problems. Unlike individual counseling relationships, a group provides each individual the opportunity to give as well as to receive help.In the group, the members can discover that they are capable of understanding, accepting, and helping their peers, and that they can contribute to another person's life. Thus, members gradually begin to understand and accept themselves. The emerging trust in self and others facilitates the sharing of ideas and behaviors in a safe testing ground before applying those ideas and behaviors in relationships outside the group.Group members come to function not merely as counselees, but they practically behave as counselees at certain times in the sessions and as counselors at other times. Unlike individual counseling, where information and care flow in a single direction, in a group, the flow of information and care is multi-directional, where each member participates in the giving and receiving of advice.Population-based computational optimization approaches are widely used in practical applications especially when the objective function is complex and the solution vector of parameters is of high dimensionality. Reflecting on the nature and procedural phases of counseling among people, we can infer that there exist many highly interesting aspects of analogy between a group counseling process and a population-based optimization process, in spite of being in completely different settings. Table 1lists prominent points of resemblance between the two processes that well justify the adoption of group counseling as a metaphor for population-based optimization. This is the first scientific contribution of this paper.In this section, we develop a population-based, gradient-free, optimization algorithm – which we call a group counseling optimizer and abbreviate it as GCO. This is the second scientific contribution of the present paper. Instead of mimicking the behavior of biological organisms such as birds, fish, ants, and bees, our development is inspired by the human social behavior in solving life problems through counseling within a group, as explained in some detail in Section 2. The various intriguing points of analogy, or resemblance, between a group counseling process and a population-based optimization process, consolidated and itemized in Table 1, pave the way for a novel optimization approach with group counseling being the metaphor. We will here concentrate on the computational scheme of the proposed approach and its characteristics from the optimization point of view. Reference, when significant, will be made to the metaphoric items of Table 1. When we say, for instance, cf. Item 1 (problem solving), we mean to refer to item 1, entitled ‘problem solving’, in Table 1, for comparison of a certain computational aspect with a resemblant group-counseling aspect regarding problem solving.The problem under investigation is a single-objective, multivariate, unconstrained, continuous optimization problem. Given is a scalar objective function f(X), where X is a set of D components (variables),(1)X=(x1,x2,…xD)which can also be expressed as a D-dimensional vector,(2)X=x1x2⋮xDThe form (1) of X will often be used because we will be interested in the individual components xd, d=1, 2, …, D, rather than the vector X as a whole, Eq. (2). It is required to optimize f(X) by means of a suitably selected set X*,(3)X*=(x1*,x2*,…,xD*)Each component xdhas its own prespecified interval of values in which the component is allowed to vary. This interval is considered the search range of xdand is expressed as [xdmin, xdmax], where xdmin and xdmax are the minimum and maximum allowable values of xd, respectively. This means that the length of the range of variation of xd, denoted by range_lengthd, is(4)range_lengthd=xdmax−xdminThe problem at hand is to be computationally solved in the conceptual framework of the solution of a life problem through group counseling; cf. Item 1 (problem solving). The optimizing vector X* and the corresponding global optimum f(X*) are obtained as the most appropriate way out of a difficulty is found in group counseling; cf. Item 2 (purpose of solution). In general, there is no one unique method for achieving optimality. The parameters of the approach taken are problem-dependent, in both optimization and group counseling; cf. Item 3 (method of solution).Like other heuristics-guided approaches, we begin with a population size, m; that is, a certain number m of initial candidate solution vectors Xiin the D-dimensional search space,(5)Xi=(x1i,x2i,…,xDi),i=1,2,…,mThis vector subspace corresponds to the minisociety of participating members (persons) in group counseling; cf. Item 5 (work-force). Such solution vectors are to be improved in increments through successive iterations, as counseling is incrementally effected through successive counseling sessions; cf. Item 4 (increments of solution). This means that an iteration with m solution vectors is visualized as a group counseling session with m participating members. Member i is represented by a vector Xi, which in turn accommodates D componentsxdi, d=1, 2, …, D, designating what we consider the best experiences (so far) gained by the member. In solution improvement, candidate vectors therefore use their best possible positional values as group members use their best possible experiences in doing counseling; cf. Item 7 (best possible participation). Also, a key idea is that candidate vectors contribute to the improvement of a solution (act as counselors) at times and receive improving contributions (act as counselees) at other times; cf. Item 6 (role of participants). All candidate vectors are indeed eligible to give and receive contributions; cf. Item 8 (equal chances).It is particularly noted that the representation of a specific member generally varies from iteration to iteration [experiences vary (generally improve) from session to session]. A candidate vector is modified (hopefully improve) component-wise. A new value of each component in a vector is produced either by invoking contributions (experiences) of the corresponding components in some (not necessarily all) other vectors, or by modifying (directly) the current value of the component itself. These are two different strategies, each having distinct behavior properties. The situation is again analogous to what happens in group counseling, where a person – in solving his problem – asks other people for help; cf. Item 15 (dependence on other members) or, sometimes, he depends on himself only after self-discovery; cf. Item 16 (self-dependence). In group counseling, life problems are first discussed in general terms, then fine details are dealt with. Similarly, the search technique in the optimization algorithm proceeds first in relatively large steps in an exploration stage, and then in small steps in an exploitation stage; cf. Item 11 (from general terms to details). The optimization process terminates at the end of iterations using a stopping criterion as the group counseling process terminates at the end of sessions following a termination policy; cf. Item 20 (termination). We choose to employ a certain maximum number of evaluations of the objective function to be the stopping criterion. This is the product of the population size m and the maximum number of iterations, itr_max.The proposed GCO algorithm requires a number of effective parameters to be set. We employ four algorithmic parameters:•Number of group members acting as counselors, c, generally c≤m−1.Counseling probability, cp, set in the range [0,1].Search range reduction coefficient, red, set in the range [0,1].Transition rate from the stage of exploration to that of exploitation, tr.The role of these parameters will become apparent as we proceed.In the following, the steps of the GCO algorithm are explained in details:The algorithm starts with m D-dimensional initial candidate solution vectors Xibeing placed randomly in the search space. We locate the values of the vector componentsxdi, d=1, 2, …, D, in accordance with a beta probability distribution [27–29],(6)β(x)=xa−1(1−x)b−1B(a,b)0<x<1where the function in the denominator is a beta function defined as(7)B(a,b)=∫01ta−1(1−t)b−1dtWe adopt the case in which the two shaping parameters a and b are equal and less than unity, a=b<1. In such a case, the density function has a symmetric U-shape. This has the consequence that, most probably, the candidate solutions lie near the boundaries of the search space and that the global optimum is well within this candidate solution set.The solution vectors Xiare substituted, respectively, into the objective function f(X), yielding m values for f(Xi), called fitness values.This is the first iterative step. For each solution Xi, we produce an alternative solution(8)X′i=(x′1i,x′2i,…,x′Di)The production process is carried out component-wise. Each componentx'diis obtained through one of two counseling strategies:(a)Other-members counseling.Self-counseling.Having set the counseling probability cp to some value in the range [0,1], we, for each componentxdi, generate a random number in the range [0,1] according to a uniform probability distribution. This number is here termed a counseling decisive coefficient (cdc). We choose to do other-members counseling when cdc is less than or equal to cp, and do self-counseling otherwise. In what follows, we explain how to calculatex'di, givenxdi, in each of these strategies.Step 3a: Other-members counseling (cdc≤cp)In this strategy, member i, or the vector Xi, is regarded as a counselee. It asks for counseling of c other members (counselors), chosen randomly out of the population, so that an alternative (hopefully better) componentx'diis obtained; cf. Item 9 (active members). The value ofx'diis calculated by summing weighted values of the corresponding components (best experiences) of the c counselors. These are the contributions of the relevant counselors, in a brainstorming process; cf. Item 10 (brainstorming).The weight, wq, of component d in counselor q (q=1, 2, …., c) is a random number in the range [0,1] with a uniform probability distribution. It should be obvious that counselor q is some member i. As indicated in Eq. (9), the c weights sum to unity,(9)∑q=1cwq=1The form ofx'diis expressed as(10)x′di=∑q=1cwqxdint_randqwhich is valid for d=1, 2, …, D and i=1, 2, …, m (that is, m.D components). The superscript int_randqis an integer random number in the range [1,m] with a uniform distribution. Note that, according to Eq. (10), we generate c such random numbers for each componentxdi, with a total of D.m.c random numbers for all vector components. The component denoted byxdint_randqis the value of component d of counselor q (which is member int_randq).It should be evident that the set of c counselors in general varies from component to component (as d varies from 1 to D). A little thought reveals that wqand int_randqare both dependent on the values of i and d; these symbols are not superimposed on Eq. (10) for notational simplicity.Step 3b: Self-counseling (cdc>cp)In this strategy, an alternative componentx'diis obtained in an iteration through modification of the current componentxdi. That is, the new componentx'didepends on the best experiencexdiof member i with a specific modification.In the problem statement, each component in the vector X is assigned an overall permissible range of variation. Let the length of this range for component d be denoted by range_lengthd. We choose to search for a modification value of the component in a reduced range with length red. range_lengthd, where red, as mentioned previously, is the search range reduction coefficient, set in the range [0,1]. The set value of red is kept unchanged for all components of all vectors in all iterations.Component d, in the various iterations, is allowed to be modified in a maximum range [−mdf_maxd, mdf_maxd] about the current component, where(11)mdf_maxd=0.5.red.range_lengthdEq. (11) implies that the maximum modification range is divided into two halves about the current component. This results from adding the current component to the modification range.The maximum modification value in a certain iteration,mdf_maxditr, is estimated from the relation(12)mdf_maxditr=mdf_maxd1−itritr_maxtrwhere itr is the iteration number, and itr_max, as defined previously, is the maximum number of iterations. The exponent tr in Eq. (12) refers to a transition rate at which the search method transfers from exploration to exploitation. Relation (12) is a rule of thumb, supported by the mathematical illustration of Fig. 1, which shows the variation ofmdf_maxditrfrommdf_maxd(at the very beginning of iterations) to zero (at itr_max) for different values of tr. It is seen that at a certain iteration, the value ofmdf_maxditrdecreases as tr increases. In other words, as tr increases, exploration tends to exploitation in a smaller number of iterations. Here the value ofmdf_maxditrplays a central role in whether the optimization algorithm, in a certain stage, performs exploration or exploitation. It is a well-accepted principle that all optimization algorithms have to compromise between exploration and exploitation so that the global optimum is eventually attained.For each componentxdi, we generate a random number in the range[−mdf_maxditr,mdf_maxditr]and this is added toxdito obtain the modified valuex'diof the form(13)x′di=xdi+randdi(−mdf_maxditr,mdf_maxditr)(a)if(x′di>xdmax)thenx′di=xdi+randdi(0,xdmax−xdi)(b)if(x′di<xdmin)thenx′di=xdi+randdi(xdmin−xdi,0)(c)Note that the modification value (the random number) varies from one vector to another in an iteration.Step 2 is repeated for X′i(instead of Xi) and the fitness value, f(X′i), is evaluated; cf. Item 19 (judgment). If f(X′i) is better (less in minimization or greater in maximization) than f(Xi), then X′ireplaces Xi; otherwise, X′iis ignored and Xiis kept unchanged for possible subsequent improvement; cf. Item 17 (remaining unchanged).Steps 3 and 4 are repeated until the stopping criterion is met.This is a decision-making step. The m solutions, resulting from the last repetition step, are compared with each other based on the fitness values of the objective function. The best solution is taken as the optimum solution X* (with acceptable error).Fig. 2shows a general flowchart for the proposed GCO (for the minimization problem).The algorithmic comparison between GCO and other optimizer can be summarized in the following points:1.Like GA, PSO, ACO, ABC, etc., GCO is a population-based optimization algorithm in which an iterative policy is used, depending on a group, or population, of candidate solutions, not just one solution.Like GA, PSO, ACO, ABC, The decision into GCO is made depending on a probability (counseling probability cp). GA uses crossover and mutation probability. ACO utilizes the probability by which the ants choose a route from a node to another node. ABC employs the probability of an onlooker bee to choose to go to the preferred food source at some location.Unlike ABC, the metaphor of GCO, counseling, has no subtle concepts or sophisticated procedures [18–20]. This facilitates the implementation process of the metaphor. Metaphors of ABC have several details, leading to some difficulty in the implementation process.Unlike PSO, GCO does not depend on the best solution of the iteration to enhance the candidate solutions. Thus GCO avoids trapping to local optima (premature convergence).Unlike GA, PSO, ACO, ABC, etc., GCO uses an initialization process according to beta distribution. To the best of our knowledge, there is no optimizer uses the beta distribution in its initialization process.Unlike PSO, GCO represents the candidate solution by only its position. PSO represents the candidate solution by two items: its position and velocity.Unlike GA, GCO does not require to encode the candidate solution to another representation such as binary encoding.Unlike GA, GCO employs one of two strategies to improve the candidate solutions: other-members counseling or self-counseling strategy. GA utilizes two strategies together in the same time to enhance the candidate solutions: crossover and mutation.The proposed GCO algorithm is tested in minimization problems using seven unrotated benchmark functions [30–33]; cf. Item 18 (standardization). Two of these functions are of the unimodal type:•Sphere function, f1(X)Rosenbrock function, f2(X)and five are of the multimodal type:Ackley function, f3(X)Griewank function, f4(X)Weierstrass function, f5(X)Rastrigin function, f6(X)Schwefel function, f7(X)The algorithm is then compared with the comprehensive learning particle swarm optimizer (CLPSO) presented by Liang et al. [21], which renders distinguished performance in comparison with many other variants of the particle swarm optimizer.The definitions of the unrotated benchmark functions used here are given in what follows. Note that the dimension D is left free to be chosen by the designer of the algorithm. Apart from minor exceptions, the function becomes more complicated from the minimization viewpoint as D gets higher.A unimodal function,(14)f1(X)=∑d=1Dxd2It is simple in form, with no interaction between the variables (components xdof the vector X).A unimode function,(15)f2(X)=∑d=1D−1[100(xd2−xd+1)2+(xd−1)2]It has strong interaction between some of the variables.A multimodal function(16)f3(X)=20+e−20e−0.21D∑d=1Dxd2−e1D∑d=1Dcos(2πxd)It has several local minima. The minimization process is facilitated to some extent because the local minima are shallow [21].A multimodal function,(17)f4(X)=1+∑d=1Dxd24000−∏d=1DcosxddIt has strong interaction between all the variables. However, it is interesting to note that as the dimension D is increased, this function becomes simpler because the local minima induced by the cosine product term decrease in number and complexity [34].A multimodal function,(18)f5(X)=∑d=1D(∑k=0kmax[α1kcos(2πα2k(xd+0.5))])−D∑k=0kmax[α1kcos(2πα2k.0.5)]α1=0.5,α2=3,kmax=20Despite being continuous, this function is differentiable only on a specific set of points [21].A multimodal version of the sphere function,(19)f6(X)=∑d=1D[10+xd2−10cos(2πxd)]It has a large number of local minima arranged as sinusoidal bumps. The variables are independent.A multimodal function,(20)f7(X)=418.9829D−∑d=1Dxdsin(xd)This function is complex and often difficult to deal with. Its complexity stems from the fact that it possesses deep local minima which are far from the global minimum. The global minimum is located near one corner of the search space [35].For each of the above unrotated functions, Table 2gives the search range for the components xdof the vector X (all components have the same search range), the minimizing vector X*, and the global minimum f(X*). Note that all global minima are at the zero value.In the test experiments on unrotated functions, we take:•Dimension, D=30.Population size, m=40.Maximum number of evaluations of the objective function, FEs=200,000; that is, the maximum number of iterations, itr_max=5000 (stopping criterion).Shaping parameters of the beta probability distribution, a=b=0.1.All experiments are run 30 times. The mean and standard deviation of the numerical results are recorded. The mean is a measure of the accuracy and the standard deviation is a measure of the robustness in quantifying the performance of an optimizer [36].The four parameters: c (number of group members acting as counselors), cp (counseling probability), red (search range reduction coefficient), and tr (transition rate from exploration to exploitation) defined specifically for the GCO algorithm are set at different values according to the test function in question, as indicated in Table 3.The results of the GCO algorithm when applied to the unrotated benchmark functions f1(X) through f7(X) are statistically represented in Table 4. The second column of this table gives the mean and standard deviation (mean±standard deviation) of the error, defined as the deviation from the zero-value global minimum, of the 30 runs of the algorithm on each function. It turns out that the error resulting from the minimization process of all functions is infinitesimally small; for f4(X), f5(X), and f6(X), a zero-value error is even arrived at. This means that the GCO has successfully converged to the global minima of all unrotated test functions. It has not fallen into any local minimum of multimodal functions f3(X) through f7(X), thus avoiding premature convergence; cf. Item 12 (thorough investigation). Such a success is attributed mainly to a proper choice of the parameter tr and the value ofmdf_maxditr, Eq. (12), so that a compromise is smartly made between exploration and exploitation in the course of algorithm running.The convergence characteristics of the GCO algorithm for the unrotated functions f1(X) through f7(X) are demonstrated in Figs. 3–9, respectively. The solid curves in these figures represent the variation of the best (least) function value with the number of function evaluations, FEs (up to 200,000), in the best run. By the ‘best run’, we refer to the run of the algorithm which yields the best (least) error value at the end of the (5000) iterations. It is seen how each of the test functions takes on successive fitness values that tend to the global minimum (zero value) in a smooth, monotonically decreasing, acceptably fast manner.It is of great importance to compare a new algorithm with a powerful, recent one. Our GCO is compared with the comprehensive learning particle swarm optimizer (CLPSO) [21]. For a significant comparison, the same parameter values specified for the CLPSO in [21] are adopted for the GCO: D=30, m=40, FEs=200,000 (itr_max=5000), and number of runs=30. The remaining parameters pertinent to the CLPSO, and having no counterparts in the GCO, are taken as in [21].Table 4 (the third column from left) gives the mean and standard deviation of the error of the 30 runs of the CLPSO on the unrotated benchmark functions f1(X) through f7(X). The result of the GCO is better (having less error) than that of the CLPSO for the five functions f1(X), f2(X), f4(X), f5(X), and f6(X), although the differences are slight. For the two functions f3(X) and f7(X), on the other hand, the error of the GCO is above that of the CLPSO, but the error difference is hardly noticeable. In general, anyhow, we do not expect a better performance of the new algorithm on all classes of problems. An enhanced performance on one class can be offset by performance degradation on another class. This is the no free lunch theorem [37]. Figs. 3–9 (the dotted curves) demonstrate the convergence characteristics of the CLPSO for the unrotated functions f1(X) through f7(X), respectively. The variation of the best function value with the number of function evaluations, FEs, in the best run, is represented. The GCO curves are all below the CLPSO curves, indicating that the convergence of the GCO to the global minimum is faster than that of the CLPSO. Fast convergence is a noteworthy merit in the assessment of optimization algorithms.In Section 5, we tested the proposed GCO algorithm using seven unrotated benchmark functions f1(X) through f7(X) defined in Eqs. (14)–(20), respectively. In this section, we test the algorithm using five rotated multimodal benchmark functions, f8(X) through f12(X), which are the respective rotation forms of f3(X) through f7(X); cf. Item 18 (standardization). That is, we have•Rotated Ackley function, f8(X).Rotated Griewank function, f9(X).Rotated Weierstrass function, f10(X).Rotated Rastrigin function, f11(X).Rotated Schwefel function, f12(X).The principle of rotated functions relies on the concept of vector rotation in space [38]. The D-dimensional vector X is left multiplied by an orthogonal D×D rotation matrix R to form a rotated vector Y as(21)Y=RXwhere ||Y||=||X||. The rotation matrix R is calculated by Salomon's method [39], which we incorporate in the GCO code; see the Appendix.The task of using rotated functions in testing an optimization algorithm is more difficult than using unrotated functions [21]. In other words, an algorithm may succeed with unrotated functions and fail with rotated ones. Even when the algorithm succeeds with rotated functions, it is well expected that the resulting error will be much greater than that with unrotated functions. Therefore, rotated-function testing is of special significance to demonstrate the effectiveness and superiority of the algorithm.The rotated benchmark functions are defined below.(22)f8(X)=20+e−20e−0.21D∑d=1Dyd2−e1D∑d=1Dcos(2πyd)where ydrepresents the components of the rotated vector Y in Eq. (21).(23)f9(X)=1+∑d=1Dyd24000−∏d=1Dcosydd(24)f10(X)=∑d=1D(∑k=0kmax[α1kcos(2πα2k(yd+0.5))])−D∑k=0kmax[α1kcos(2πα2k.0.5)]α1=0.5,α2=3,kmax=20(25)f11(X)=∑d=1D[10+yd2−10cos(2πyd)]Note that Eqs. (22)–(25) have the same form as Eqs. (16)–(19) except for writing the components ydof the rotated vector Y in place of the components xdof the original vector X.This is a highly complex function that needs special attention. It is written in the form(26)f12(X)=418.9829D−∑d=1Dzdin which(27)zd=ydsin(yd)ifyd≤500(a)−0.001(yd−500)2ifyd>500(b)and the rotated vector Y becomes(28)Y=R(X−X*)+X*where R is an orthogonal D×D matrix, as in Eq. (21), and X* is the minimizing D-dimensional vector(29)X*=420.96420.96⋮420.96as given in Table 2 for f7(X). Eq. (28) is used for rotating the original vector X into the vector Y instead of Eq. (21). We remark that Schwefel function has rather easier-to-find minimizing solutions outside the range [−500,500]. Therefore, as Eq. (27b) tells, when |yd| is greater than the upper range bound 500, zdis set in a portion of the squared distance (difference) between |yd| and the value 500.To the best of our knowledge, the optimization algorithms available in the literature are not capable of achieving very high accuracy with the rotated Schwefel function. The computed minimum is often not sufficiently close to the actual global minimum; that is, a relatively big error usually appears at the end of iterations. Therefore, apart from the CLPSO [21], this function is seldom used in algorithm testing.The search range for the components of the vector X, the minimizing vector X*, and the global minimum f(X*) for the rotated functions f8(X) through f12(X) are the same as those for the unrotated functions f3(X) through f7(X) specified in Table 2, respectively. That is, the optimum solution for these test functions is not affected by function rotation because the point of global minimum is considered the center of rotation of the function.It is to be noted that a number of unrotated benchmark functions have regular grids of minima, in which the global minimum together with a group of local minima lie along the same coordinate direction. This configuration can facilitate the task of escaping from a local minimum toward the global minimum, as followed in specific approaches [40,41]. Additionally, the separability of components in some functions reduces the computational steps. A reliable algorithm should not depend on such special properties. With these arguments in mind, function rotation does lead to ‘concealing’ any simplifying features of this type. This is reinforced by Eq. (21) which results in strongly interconnected components of the rotated vector Y, by virtue of the rotation matrix R.The test experiments on the rotated functions are conducted using the same values for the dimension (D=30), population size (m=40), maximum number of evaluations of the objective function (FEs=200,000), shaping parameters of the beta probability distribution (a=b=0.1), and number of runs (30) as those used for the unrotated functions in Section 5. The GCO algorithm parameters c, cp, red, and tr are set differently for the rotated functions as indicated in Table 5.The results of application of the GCO algorithm to the rotated benchmark functions f8(X) through f12(X) are statistically represented in Table 6(second column) by way of the mean and standard deviation of the error of the 30 runs of the algorithm on each function. It is seen that the error for all functions is very small; a zero-value error is even obtained for f10(X). Such results ascertain the success of the GCO with rotated functions also. All local-minimum traps are capably avoided.As expected, the resulting error with rotated functions is somehow greater, although quite acceptable, than that with unrotated functions. Table 7lists error values, extracted from Tables 4 and 6, for a quantitative comparison. Note particularly that no difference in error (due to function rotation) exists for Ackley and Griewank functions, while the relatively big error occurs with the difficult-to-minimize Schwefel function, where the error amounts to 212±292.Figs. 10–14illustrate the convergence characteristics of the GCO for the rotated functions f8(X) through f12(X), respectively. The solid curves in these figures represent the variation of the best function value with FEs in the best run. The general behavior of algorithm convergence for the rotated functions is similar to that for the unrotated functions in Figs. 3–9.The GCO is again compared with the CLPSO on the basis of equal values for common particulars. Table 6 (the third column from left) gives the mean and standard deviation of the error of the 30 runs of the CLPSO on the rotated benchmark functions f8(X) through f12(X). A comparison between the result of the GCO and that of the CLPSO reveals that the GCO is generally better than the CLPSO for all rotated functions. For Schwefel function, in particular, the error of the GCO is 212±292, as we noted in Section 6.6, whereas the error of the CLPSO is 1700±186; that is, the mean value of the GCO error reduces to 12.5% of that of the CLPSO error, although the associated standard deviation is about 57% higher.Figs. 10–14 (the dotted curves) demonstrate the convergence characteristics of the CLPSO for the rotated functions f8(X) through f12(X), respectively, where the variation of the best function value with FEs in the best run is represented. Like the case of unrotated functions, the GCO curves are all below the CLPSO curves, indicating faster convergence for the GCO.In this section, we test the algorithm using eight new proposed composition benchmark functions, f13(X) through f20(X), which are f21(X) through f28(X) in [24]; cf. Item 18 (standardization). Each composition function is a summation of a number (n) of basic unrotated or rotated functions with certain weights, heights, and biases. These new proposed composition functions combine the traits of the sub-functions better and retain continuous around the global/local optima. That is, we have•Composition function 1, f13(X), (n=5 rotated)Composition function 2, f14(X), (n=3 unrotated)Composition function 3, f15(X), (n=3 rotated)Composition function 4, f16(X), (n=3 rotated)Composition function 5, f17(X), (n=3 rotated)Composition function 6, f18(X), (n=5 rotated)Composition function 7, f19(X), (n=5 rotated)Composition function 8, f20(X), (n=5 rotated)The task of using composition functions in testing an optimization algorithm is extremely difficult than using unrotated and rotated ones. In other words, an algorithm may succeed with unrotated and rotated functions and fail with composition ones. Even when the algorithm succeeds with composition functions, it is well expected that the resulting error will be much greater than that with unrotated and rotated functions. Therefore, composition-function testing is of highly special significance to illustrate the robustness and superiority of the algorithm.The definitions of the composition functions f13(X) through f20(X) and their global minimum f(X*) are shown in [24]. The search ranges for all composition functions are [−100,100]D. The test experiments on the composition functions are conducted using the values for the dimension (D=30), population size (m=50), maximum number of evaluations of the objective function (FEs=300,000), shaping parameters of the beta probability distribution (a=b=0.1), and number of runs (51). The GCO algorithm parameters c, cp, red, and tr are set differently for the composition functions as indicated in Table 8.The results of application of the GCO algorithm to the composition benchmark functions f13(X) through f20(X) are statistically represented in Table 9(second column) by way of the mean and standard deviation of the error of the 51 runs of the algorithm on each function.As expected, the resulting error with composition functions is somehow greater, although quite acceptable, than that with unrotated and rotated functions because these composition functions are more difficult-to-minimize.The GCO is compared with a more advanced optimizer BI-population covariance matrix adaptation evolution strategy with alternative restart strategy (NBIPOP-aCMA-ES) [22,23], which is the winner of the competition on real-parameter single objective optimization at IEEE CEC-2013[24], on the basis of equal values for common particulars. Table 9 (the third column from left) gives the mean and standard deviation of the error of the 51 runs of the NBIPOP-aCMA-ES on the composition benchmark functions f13(X) through f20(X). A comparison between the result of the GCO and that of the NBIPOP-aCMA-ES reveals that the GCO is generally better than the NBIPOP-aCMA-ES for functions f13(X), f14(X), f19(X), and f20(X). For function f14(X), in particular, the error of the GCO is 68.4122±33.56426, as we noted in Section 7.1, whereas the error of the NBIPOP-aCMA-ES is 838.392±459.988; that is, the mean value of the GCO error reduces to 8% of that of the NBIPOP-aCMA-ES error, and the associated standard deviation reduces to 7%. Also for function f20(X), the error of the GCO is 154.9125±90.13492 whereas the error of the NBIPOP-aCMA-ES is 268.627±73.458; that is, the mean value of the GCO error reduces to 58% of that of the NBIPOP-aCMA-ES error, and the associated standard deviation is about 23% higher. For function f16(X) through f18(X), the error differences between GCO and NBIPOP-aCMA-ES are relatively small. For functions f15(X), on the other hand, the error of the GCO is above that of the NBIPOP-aCMA-ES. It is not expected a better performance of the infant algorithm on all classes of problems. An improved performance on one class leads to performance degradation on another class. This is the no free lunch theorem [37].In recent years, aerospace researchers have realized the benefits of approaching trajectory design problems from a global optimization point of view. Many interplanetary trajectory design problems can be formalized as optimization problems [42,43]. One of the main goals to be achieved is the maximization of the spacecraft mass available for the pay-load or, equivalently, the minimization of the propellant necessary on board. The multiple gravity assist (MGA) problem has been intensively investigated. For example, Hartmann et al. [44] as well as Abdelkhalik and Mortari [45] use genetic algorithms to describe optimum trajectories. Izzo et al. [46] introduce a deterministic search space pruning algorithm which is then applied in conjunction with some heuristic global solvers, resulting in a very efficient algorithm. In the assessment and comparison of different optimization approaches, it is crucial to specify the exact problem, the underlying mathematical model, variable constraints, and all details of the solution scheme. The annual global trajectory optimization competition (GTOC), launched by the ESA ACT (European Space Agency, Advanced Concepts Team) in 2005, plays a stimulating role in this respect. Information about past editions can be found on the website http://www.esa.int/gsp/ACT/mad/op/GTOC/index.htm.We particularly refer to the work of Vinkó and Izzo [43], where eight trajectory optimization methods, including particle swarm optimizer, genetic algorithm, simulated annealing, differential evolution, and COOP-1,2,3,4, are assessed and compared together using MGA models for a number of missions. Each problem is formulated with the aid of a black-box objective function accepting, as input, the adaptable solution vector (state vector of the model) and returning the objective function and constraint evaluation. The best known solutions (the optimizing vectors and the corresponding objective function values) are reported as well. These are obtained using DiGMO (distributed global multi-objective optimizer), a complicated cooperative optimization method which combines several population-based optimizers.The gravity assist maneuver is the utilization of the gravity field of a celestial body and the relative motion of a spacecraft with respect to the body in order to change the spacecraft velocity [42]. During an interplanetary mission, the spacecraft encounters the massive celestial body with a specific relative velocity. Due to this close passage, there will be an exchange of momentum between the spacecraft and the body with the result that the spacecraft increases or reduces the magnitude of its inertial velocity vector or rotates the direction of this vector. The body loses an extremely small proportion of its orbital momentum owing to its enormous mass, compared to the mass of the spacecraft. However, the change in spacecraft velocity can be considerably large. Such an interesting ‘interaction’ between a spacecraft and a celestial body seems like that of a bike rider holding a running bus.Multiple gravity assist (MGA) maneuvers are now extensively planned for multi-path missions among a group of planets, each planet having a certain gravity-field impact on the velocity change, denoted collectively by ΔV, of the spacecraft. If no gravity assists were made use of throughout the mission, the velocity changes needed in the various stages for the spacecraft to travel from the Earth orbit to the target orbit would be furnished by the propulsion system alone, which would be a highly sophisticated system requiring exceedingly huge budgets. MGA missions have been sent forth in space over the past forty years to reach targets having orbits of very high or very low energy levels, in comparison with Earth. They are even intended to appreciably alter the heliocentric orbit inclination. These types of orbits are all categorized as high-ΔV targets because of the high amount of ΔV necessary to reach them from Earth.We will be concerned here with MGA models, those used for MGA missions. Before discussing the main guiding ideas of such models, the following definitions are in order [42]:A leg is the trajectory followed by the spacecraft between two celestial bodies.The pericenter radius, rp, at a celestial body is the minimum distance between the spacecraft trajectory and the body.The orbital eccentricity, e, of a celestial body represents the amount by which its orbit deviates from a perfect circle, wheree=0 for circular orbits.0<e<1 for elliptic orbits.e=1 for parabolic trajectories.e>1 for hyperbolic trajectories.In a trajectory design problem, we have a sequence of n+1 celestial bodies B0, B1, …, Bn(B0 is Earth). The bodies are not necessarily distinct. It is required to visit the body sequence in such a way that the overall mass consumption of the spacecraft is minimized. The sequence is here assumed to be fixed, but we may also think of models where this sequence is part of the decision problem.An MGA model has the variables:•t0, the starting date of the mission.Ti, i=1, 2, …, n, the time of flight along leg i, joining body Bi−1 with body Bi.Given the values of these variables, we are able to identify the positions pi−1 and pi, respectively, of body Bi−1 at timet0+∑j=1i-1Tjand of body Biat timet0+∑j=1iTjIt is then possible to calculate the velocities at the end of each leg i and at the beginning of the next leg i+1, i=1, 2, …, n−1. In order to transfer from one leg to the next one, the spacecraft needs to provide a single impulse, a single change in velocity ΔVi. In the initial leg, the spacecraft provides a single impulse ΔV0 to leave the Earth orbit and attain the starting velocity at the initial leg. Similarly, in the last leg, the spacecraft provides a single impulse ΔVnto transfer from the final velocity in the last leg to the velocity of the target body Bn.Each impulse causes a mass consumption proportional to the modulus (absolute value of the magnitude) of the change in velocity, |ΔVi|, i=0, 1, …, n. In order to minimize the overall mass consumption, the following objective function is taken as a performance measure:(30)f(X)=∑i=0n|ΔVi|where X is the state vector. The quantities constituting the right-hand side of (30) depend implicitly on the components (state variables) of X. The optimization process consists in finding a vector X=X* which minimizes function (30).MGA models usually include constraints on the pericenter radius, rpi, at each intermediate body Bi, i=1, 2, …, n−1, which typically require rpinot to fall below a certain lower threshold. The reason is that the spacecraft must be far enough from body Biin order to be able to leave the planet orbit and avoid ‘forced’ landing due to the gravitational force.Objective functions, other than (30), do exist. Of particular interest is the problem of ‘asteroid deflection’, suggested in the GTOC competition to avoid possible dangerous effects of an asteroid on Earth [43]. The aim of the spacecraft mission in this case is to deliver the greatest permissible push so that the asteroid is compelled to change the shape of its orbit to the farthest extent. The objective function takes the form(31)f(X)=mfurelvastwhere mfis the final mass of the spacecraft, urel is the spacecraft velocity relative to the asteroid at arrival, and vast is the heliocentric velocity of the asteroid. Again X is the state vector, with components implicit in the quantities of the right-hand side of (31). In the optimization process, a vector X=X* is to be found such that function (31) is maximized.Objective functions, like (30) and (31), are often considered as black boxes, accepting vectors X as inputs and returning the objective function values as outputs.Our group counseling optimizer (GCO) will be applied to trajectory design problems of two spacecraft missions: Cassini1 and GTOC1. The models of both missions are of the MGA type. The solution of the Cassini1 problem requires a minimization process, while that of the GTOC1 problem requires a maximization process.This is an MGA problem in the framework of the Cassini spacecraft trajectory design [43]. The aim of the mission is to reach Saturn and be captured by its gravity field into an orbit having a pericenter radius(32)rp=108,950kmand an eccentricity(33)e=0.98(ellipticorbit)The planetary fly-by sequence is made up of six celestial bodiesB0,B1,B2,B3,B4,B5(n=5)which are, respectively, Earth, Venus, Venus, Earth, Jupiter, Saturn.That is, the initial body is Earth (as usual) and the target body is Saturn (as planned), and the mission has five legs, leg i, i=1, 2, …, 5, joining body Bi−1 with body Bi. Note that leg 2 joins Venus with itself (the spacecraft returns to the same body) and leg 3 joins Venus with Earth (the spacecraft visits Earth once after launching during the mission). The real Cassini mission was launched from Earth on October 15, 1997 and arrived at Saturn on July 1, 2004.The model of the Cassini1 mission is a simplified model of the MGA type. The objective function f(X) is of the form (30), where the total change ΔV accumulated during the mission is calculated, including the launch ΔV and subsequent ΔV's needed at the planets and upon eventual arrival at the target to accomplish the intended orbit injection. The objective function in this situation is to be minimized in an optimization process.The state vector, X, of the model has six components (state variables) xi, i=1, 2, …, 6, which are the times, in days, of the various stages of the mission. These are denoted by t0, T1, …, T5, and we can write(34)X=x1x2x3x4x5x6=t0T1T2T3T4T5Specifically, t0 is the starting date of the mission, expressed in the astronomical unit MJD2000 (Modified Julian Date: time in days measured from January 1, 2000), and Ti, i=1, 2, …, 5, is the flight time along leg i. The search ranges of these variables are given in Table 10.The four pericenter radii rp1, …, rp4 at the intermediate bodies B1, …, B4, respectively, are constrained not to be less than the lower thresholds indicated in Table 11.The best solution (X) known for the Cassini1 problem is [43](35)XCbn=−789.8158.3449.454.71024.74552.8where the superscript bn stands for ‘best known’ and the subscript C stands for Cassini1.The corresponding value of the objective function (the least known value) is(36)f(XCbn)=4.9307km/sThis is again an MGA problem, drawing inspiration from the first edition of the GTOC competition [43,47]. The problem is concerned with asteroid deflection, suggested to avoid expected danger of an asteroid to Earth. Asteroid TW229 is taken as the target of the GTOC1 spacecraft ‘intercept’ mission. Attempts to obtain generic solutions to the asteroid-deflection problems are important precautionary measures: should astronomers in the future discover a potentially dangerous asteroid, a timely and efficacious decision can be confidently taken.The mission in this case is required to deliver the greatest possible push so that the target asteroid is compelled to largely change the shape of its original orbit, thereby preventing possible catastrophic events.The GTOC1 planetary fly-by sequence is rather long, containing eight celestial bodies B0, B1, …, B7 which are, respectively, Earth, Venus, Earth, Venus, Earth, Jupiter, Saturn, Asteroid TW229 (n=7), and there are seven legs. Note that Earth appears three times (B0, B2, B4) in the sequence and Venus appears twice (B1, B3).In the MGA simplified model of the GTOC1, the objective function f(X), of the form (31), is to be maximized in an optimization process. This implies the maximization of the change in semi-major axis of the asteroid orbit following an elastic impact of the spacecraft with the asteroid.The state vector, X, has eight state variables xi, i=1, 2, …, 8, which are the times t0, T1, T2, …, T7 of the various stages of the mission. That is,(37)X=x1x2x3x4x5x6x7x8=t0T1T2T3T4T5T6T7The search ranges of the state variables are given in Table 12.The six pericenter radii rp1, …, rp6 at the intermediate bodies B1, …, B6, respectively, are constrained not to be less than the lower thresholds indicated in Table 13.The solution is also based on a launcher ΔV of 2.5km/s, a specific impulse Ispof duration 2500s, and a spacecraft initial mass m0 of 1500kg.The best solution known for the GTOC1 problem is [43](38)XGbn=6809.5169.61079.456.51044.03824.21042.93393.1where the subscript G stands for GTOC1.The corresponding value of the objective function (the highest known value) is(39)f(XGbn)=1,580,599kg km2/s2In this section, we apply the GCO algorithm to the Cassini1 trajectory design problem. Appropriate values for the six components of the model's state vector X of Eq. (34) are to be obtained so that the objective function f(X) of Eq. (30) is minimized. Throughout the steps of the minimization process, the objective function is regarded as a black box, and its value is calculated on an input–output mapping basis. A program code in C++ is downloaded from the ESA ACT website and is used for objective function evaluation.The resulting solution is characterized by:•XCGCO: minimizing state vector (X*).f(XCGCO): best (least) found value of the objective function.fC,avGCO: average value of the objective function on a certain number of independent runs.The solution is assessed in light of the previously published solutions using the eight optimizers in [43] and the best known solution given by Eqs. (35) and (36). Furthermore, a separate comparison is made with a solution provided by the CLPSO, the enhanced version of the PSO. The convergence characteristics of both GCO and CLPSO, when applied to the Cassini1 problem, are studied in terms of the variation of the best found value of the objective function with the number of function evaluations, FEs.For GCO parameters, we have the particulars:•Dimension, D=6 (This is the given dimension of the state vector).Population size, m=20.Maximum number of evaluations of the objective function, FEs=200,000, implying that the maximum number of iterations, itr_max=10,000 (stopping criterion).Shaping parameters of the beta probability distribution, a=b=0.1.The four parameters defined specifically for the GCO are:•Number of group members acting as counselors, c=1.Counseling probability, cp=0.1.Search range reduction coefficient, red=0.25.Transition rate from exploration to exploitation, tr=2.The experiments are run 20 times. This number of runs and the number 200,000 of FEs are the same as those in solutions in [43] so that a comparison can be made on a common basis.The GCO code and the Cassini1 code for objective function evaluation are employed together (the former calls the latter) to solve the minimization problem. It is to be noted that the astronomical relationships of temporal and physical quantities constituting the various ΔV's of the black-boxed objective function of Eq. (30) are outside the realm of an optimization approach, although they are incorporated in the ready-made code of objective function evaluation. We have mentioned in passing few such quantities, including the pericenter radius (Eq. (32)) and eccentricity (Eq. (33)) of the orbit of Saturn, the target body of the mission; the lower thresholds of the pericenter radii of the four intermediate bodies (Table 11). Remember that the components of the state vector to be determined are the starting date t0 of the mission and the respective flight times T1, …, T5 along the five legs. The search ranges of these components are different from one another as Table 10 indicates.The resulting state vector, at the end of iterations, turns out to be(40)XCGCO=−789.7158.6449.454.51027.24564.7(FEs=200,000)The best found value of the objective function is(41)f(XCGCO)=4.9466km/s(FEs=200,000)On the 20 runs, the average value of the objective function is(42)fC,avGCO=5.2983km/s(FEs=200,000)The convergence characteristics of the GCO for the Cassini1 problem, with FEs=200,000, are demonstrated in Fig. 15. The solid curve in the figure represents the variation of the best found objective function value with the number of FEs, in the best run. It is clear that the objective function takes on successive fitness values in a smooth, monotonically decreasing, acceptably fast manner.In Table 14, we compare the results (41) and (42) for the Cassini1 problem with the corresponding results of the eight optimizers in [43]. It is evident from this table that, among the nine optimizers considered, the GCO is the next-best regarding the best found value of the objective function of the Cassini1 problem, and is truly the best regarding the average value of this function.The COOP-1 optimizer is the only one that could produce the best known value (4.9307, Eq. (36)) of the objective function. The GCO (the next-best) produces a best found value (4.9466) which is above the best known value by only 0.32%, a very slight difference.The average value of the objective function (5.2983) obtained by the GCO (the best) is significantly lower (better) than those of all other eight optimizers. It reduces to 62.6% of that obtained by the simulated annealing optimizer (the next-best).Having assessed and compared the GCO among a group of optimizers, we further attempt to employ it for a hopefully better solution for the Cassini1 problem. To this end, the maximum number of function evaluations is increased to FEs=400,000 through increasing the maximum number of iterations to itr_max=20,000. The other particulars and parameters of the algorithm remain unchanged. The resulting solution, under these conditions, takes the form(43)XCGCO=−790.8159.9449.454.41026.84557.3(FEs=400,000)(44)f(XCGCO)=4.9383(FEs=400,000)(45)fC,avGCO=5.5510(FEs=400,000)Comparing (44) and (45) with (41) and (42), respectively, we realize that increasing FEs from 200,000 to 400,000 leads to a limited reduction (improvement) of the best found value of the objective function, from 4.9466 to 4.9383. This is only 0.17% of the best known value. However, the average value increases (worsens) from 5.2983 to 5.5510, which is 5.13% of the best known value. See Table 15, the upper two rows. The convergence characteristics of the GCO for the Cassini1 problem, with FEs=400,000, are shown in Fig. 16.The CLPSO [21] is applied to the Cassini1 problem for the sake of comparison. We consider two cases: the first of FEs=200,000 and the second of FEs=400,000.When FEs=200,000, the maximum number of iterations is 10,000 (population size m=20). The solution in this case is(46)XCCLPSO=−785.4153.6449.454.91020.04540.6(FEs=200,000)(47)f(XCCLPSO)=4.9654(FEs=200,000)(48)fC,avCLPSO=6.0900(FEs=200,000)When FEs=400,000, the maximum number of iterations is 20,000 (m=20). The solution then is(49)XCCLPSO=−788.3158.4449.453.61033.74567.3(FEs=400,000)(50)f(XCCLPSO)=4.9444(FEs=400,000)(51)fC,avCLPSO=5.9264(FEs=400,000)See Table 15, the lower two rows. It is evident from Table 15 that the objective function values obtained by the CLPSO for the Cassini1 problem are all inferior (though slightly) to those obtained by the GCO. The best found value is 4.9383 for the GCO with FEs=400,000, and the best average value is 5.2983 for the GCO also, but with FEs=200,000. Note, in addition, that the results of the CLPSO, like the GCO, improve when the FEs increase from 200,000 to 400,000. The convergence characteristics of the CLPSO for the Cassini1 problem are demonstrated by the dotted curves in Figs. 15 and 16 for FEs=200,000 and FEs=400,000, respectively; the comparison with the GCO is obvious.Here we apply the GCO algorithm to the GTOC1 trajectory design problem. The eight components of the model's state vector X of Eq. (37) are to be obtained for maximizing the objective function f(X) of Eq. (31). The objective function is again regarded as a black box with available input and output ports only. A program code in C++ is also downloaded from the ESA ACT website, for objective function evaluation.The solution of the problem, accommodating the maximizing state vector, the best (highest) found value of the objective function, and the average value of the objective function on a certain number of independent runs, will be designated asXGGCO,f(XGGCO),fG,avGCOThe best parameters values of GCO are as follows:•D=8 (The given dimension of the state vector).m=20.FEs=200,000 (itr_max=10,000).a=b=0.1.c=1.cp=0.4.red=0.75.tr=5.The experiments are run 20 times. The GCO code and the GTOC1 code for objective function evaluation are employed together for solving the maximization problem. We emphasize that the astronomical quantities and relationships constituting the expression of mf, urel, and vast of the objective function (31) are irrelevant from an optimization viewpoint. However, we casually referred to the lower thresholds of the pericenter radii of the six intermediate bodies of the mission (Table 13) and the values of the launcher ΔV, specific impulse Isp, and spacecraft initial mass m0. These quantities, among others, are incorporated in the encapsulated code of objective function evaluation. The components of the state vector to be determined are the starting date t0 of the mission and the respective flight times T1, …, T7 along the seven legs. The search ranges of these components are generally different from one another (Table 12).The resulting solution is(52)XGGCO=7385.7175.91083.664.71416.6526.66260.78144.9(FEs=200,000)(53)f(XGGCO)=1,480,000(FEs=200,000)(54)fG,avGCO=1,131,267(FEs=200,000)Fig. 17demonstrates the convergence characteristics of the GCO for the GTOC1 problem, with FEs=200,000. The solid curve in the figure shows the variation of the best found objective function value with the number of FEs, in the best run. The objective function is seen to take on successive fitness values in a smooth, monotonically increasing, acceptably fast manner.In Table 16, the results (53) and (54) of the GCO for the GTOC1 problem are compared with the corresponding results of the eight optimizers considered in [43]. This table tells us that no optimizer, among the nine ones, is able to generate the best known value (1,580,599, Eq. (39)) of the objective function of the GTOC1 problem. However, the GCO yields comparatively excellent results. It is the best with respect to the best found value and also the best with respect to the average value.The best found value (1,480,000) of the GCO amounts to 93.6% of the best known value. It is higher (better) than the best found value of the COOP-1 optimizer (the next-best) by 73,967 (4.7% of the best known value).The average value (1,131,267) of the GCO is higher than that of the simulated annealing optimizer (the next-best) by 370,592 (23.4% of the best known value).In the hope of obtaining a better solution for the GTOC1 problem, we increase the maximum number of function evaluations to FEs=400,000 by increasing the maximum number of iterations to itr_max=20,000. The remaining particulars and parameters of the algorithm are kept unchanged. The resulting solution then takes the form(55)XGGCO=5648.5176.21081.869.71043.34534.71185.93685.1(FEs=400,000)(56)f(XGGCO)=1,497,706(FEs=400,000)(57)fG,avGCO=1,206,410(FEs=400,000)Comparing (56) and (57) with (53) and (54), respectively, reveals that increasing FEs from 200,000 to 400,000 leads to an improvement of the best found value of the objective function from 1,480,000 to 1,497,706, which is 1.12% of the best known value. There is likewise an improvement in the average value from 1,131,267 to 1,206,410 (4.75% of the best known value). See Table 17, the upper two rows. The convergence characteristics of the GCO for the GTOC1 problem, with FEs=400,000, are shown by the solid curve in Fig. 18.As we did with the Cassini1 problem, the CLPSO is here applied to the GTOC1 problem in the two cases: FEs=200,000 (m=20) and FEs=400,000 (m=20). In the first case, the solution is(58)XGCLPSO=7409.3161.81055.940.91062.4597.16810.87920.0(FEs=200,000)(59)f(XGCLPSO)=1,262,100(FEs=200,000)(60)fG,avCLPSO=970,820(FEs=200,000)In the second case, the solution is(61)XGCLPSO=5640.1189.81076.368.21041.54501.21201.32195.6(FEs=400,000)(62)f(XGCLPSO)=1,496,200(FEs=400,000)(63)fG,avCLPSO=1,124,100(FEs=400,000)See the lower two rows of Table 17. This table indicates that the objective function values obtained by the CLPSO for the GTOC1 problem are somewhat inferior to those obtained by the GCO. The best found value is 1,497,706 (94.8% of the best known value) for the GCO with FEs=400,000, and the best average value is 1,206,410 (76.3% of the best known value) for FEs=400,000 also. The convergence characteristics of the CLPSO for the GTOC1 problem are shown by the dotted curves in Figs. 17 and 18 for FEs=200,000 and FEs=400,000, respectively; the comparison with the GCO is clear.The rankings of the GCO among the nine optimizers, for both Cassini1 and GTOC1 missions, are summarized in Table 18.

@&#CONCLUSIONS@&#
