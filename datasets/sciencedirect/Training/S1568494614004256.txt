@&#MAIN-TITLE@&#
Interval-valued fuzzy decision trees with optimal neighbourhood perimeter

@&#HIGHLIGHTS@&#
A novel concept of interval-valued fuzzy decision trees is proposed.A mechanism to determine the order of interval-valued membership values in fuzzy decision trees is formulated.A stable α parameter is proposed to determine the neighbourhood of instances in LAFDT.A procedure to establish interval-valued fuzzy decision trees is presented.A real world application case is demonstrated to show the feasibility of the proposed model.

@&#KEYPHRASES@&#
Look-ahead based fuzzy decision tree induction,Optimal perimeter,Interval-valued fuzzy decision trees,

@&#ABSTRACT@&#
This research proposes a new model for constructing decision trees using interval-valued fuzzy membership values. Most existing fuzzy decision trees do not consider the uncertainty associated with their membership values, however, precise values of fuzzy membership values are not always possible. In this paper, we represent fuzzy membership values as intervals to model uncertainty and employ the look-ahead based fuzzy decision tree induction method to construct decision trees. We also investigate the significance of different neighbourhood values and define a new parameter insensitive to specific data sets using fuzzy sets. Some examples are provided to demonstrate the effectiveness of the approach.

@&#INTRODUCTION@&#
Decision tree is a powerful induction method in data mining. However, real-world applications of decision trees exhibit uncertainty through imprecise data, vagueness, ambiguity, etc. [1–4,23–25]. To deal with these uncertainties, fuzzy decision trees (FDT) employing type-1 fuzzy sets have been extensively investigated [11,12,22,28–32]. However type-1 fuzzy sets, by their very nature, require precise values in their application [20,26,27] and determining the exact membership values in type-1 fuzzy sets is well known to be difficult [19]. In real world data, however, we may have information on the boundary of values but cannot determine where it is within the boundaries. In this case, interval values are usually adopted to represent the uncertainty. The present FDT cannot deal with these kinds of interval values. There have been applications of type-2 fuzzy sets in decision tree construction [21]. However, these methods are designed to work with the same data as type-1 models. Therefore, there is a need to investigate the right way to treat with intervals in data mining problems. As an extension of the type-1 fuzzy sets, interval-valued fuzzy sets [5–7] are specifically designed to deal with fuzzy sets with interval representation of values. It is obviously a natural choice to deal with interval values in FDT problems. Here, we apply interval-valued fuzzy sets to construct an interval-valued fuzzy decision tree. In this way, the interval values in a data mining problem does not need to be converted into an average to employ FDT, instead, they can be directly mapped into an interval-valued fuzzy set and then an interval-valued fuzzy decision tree can be established without any aggregation operation. It is well known that aggregation can lose information, and an interval-valued fuzzy decision tree will certainly convey more information than a traditional FDT, hence the ability to reveal more useful knowledge from the data than a FDT. Obviously, such an interval-valued fuzzy decision tree is more powerful in dealing with data mining problems involving interval representation.In addition to the novel interval-valued fuzzy decision trees, we present also a new way to determine the parameters of FDT in look-ahead based fuzzy decision trees (LAFDT). LAFDT was proposed by [11] for data represented with type-1 fuzzy sets [11]. It can evaluate the classifiability of instances, that are split along branches of a given node based on evaluating the texture of the class label surface. This particular method works by finding the instances that are within a distance threshold from a given instance. This distance threshold has significant influence on the results of the classification and depends on the data sets, however, there is no well developed methodology so far to determine its value [5]. In this paper, we investigate the significance of this threshold and define a new parameter which is not sensitive to a specific data set. In this way, the application of LAFDT is greatly simplified.Combining interval-valued fuzzy sets and the LAFDT with the proposed new parameter, we propose a new model to construct a decision tree using interval-valued fuzzy membership values based on LAFDT induction and interval-valued fuzzy sets with a new optimal parameter [34]. We call this the look-ahead based interval-valued fuzzy decision tree with optimal perimeter of the neighbourhood (LAIVFDT-OPN). This new model simplifies the application of LAFDT by employing a parameter insensitive to data set changes on one hand, and improves the classification quality on the other hand by means of the employment of the full information in interval representation through interval-valued fuzzy sets.To demonstrate the feasibility of our proposed model, two well known data sets on weather condition for play [11] and car evaluation [22] are employed to verify the effectiveness of the proposed new threshold parameter. Although they have different data sizes and require different threshold values defined in LAFDT, but our result shows that they share the same threshold parameter defined in our model. To validate our interval-valued fuzzy decision tree model, the weather data set is converted into an interval data set by expanding each value into an interval. Then LAFDT is applied to the original weather data set, and LAIVFDT-OPN is applied to the interval data set of weather data. The comparison shows that their results are different, and LAIVFDT-OPN obtains a smaller tree in comparison with LAFDT. It demonstrates that the reservation of interval values provide more information than otherwise. In the end, we applied the proposed model to a real world data set from a factory in Thailand to demonstrate its usage.Section 2 provides some necessary background material on LAFDT, and interval-valued fuzzy sets (IVFS). In Section 3 we propose an extension of LAFDT induction to LAIVFDT-OPN. Section 4 describes the effect of the neighbourhood parameter on decision tree construction and Section 5 demonstrates an example of the application of LAIVFDT to data with uncertain fuzzy membership values. In Section 5.3 we present results of the experiment on a real world case study of factory data. Section 6 summaries the results of the study.There are many different models of FDT [1,13]. LAFDT induction is one of the more recent models employed to evaluate the classifiability of the instances along each branch of the split by linear discriminant (also called a single step) [1,10,11]. In a FDT, the key is to find the appropriate attribute to split samples into different branches along the tree [1,15,16]. The LAFDT has a particular method of evaluating the classifiability of attributes along the branches of a node to split and produce a smaller decision tree. A nonparametric method in LAFDT is to characterise the classifiability of attributes using an occurrence matrix.The occurrence matrix is deployed to characterise the texture of the class label [11] (a data set consists of variables and one for the class label or (n+1) dimension). The usual approach of LAFDT is, for any instance x, to measure a distance r between two instances that are within a circular neighbourhood of radius r based on the distance in Eq. (1). The r distance assists in filtering the instances which exceed the radius r, and to reduce computing time. Considering the universe of objects described by n attributes, an attribute has values of fuzzy subsetsA1k,A2k,…,Amkk(for the list of symbols adopted in this paper, please refer to Appendix). The distance between two objects (or instance x and y) can be measured using their fuzzy memberships.Definition 1[11] Letμi(k)(x)(1≤k≤n,1≤i≤mk,1≤x≤N)denote the membership value of instance x for the ith value of the kth attribute. The distance between instance x and y is defined by(1)Dxy=∑k=1n∑i=1mk|μi(k)(x)−μi(k)(y)|.For any object x in the universe, we can restrict its circular neighbourhood to those objects within a radius r of x. Then local occurrence matrix P for object x is defined as follows:Definition 2[11,12] Let μj(x), 1≤j≤C denote the membership value of instance x for class j and let μj(x)=[μ1(x), …, μC(x)]. The local co-occurrence matrix of instance x is defined by(2)P(x)=∑y,Dxy≤rμ(x)T×μ(y).where μ(x)Tis a transpose matrix and r is the neighbourhood radius of x. With a local occurrence matrix, we can derive the co-occurrence matrix for each attribute.Definition 3[11,12] The local co-occurrence matrix after attribute k is selected.(3)W(k)=∑i=1mk∑xP(x).Then, the classifiability of attribute k is(4)L(k)=∑i=1CWii(k)−∑i=1C∑j=1,i≠jCWij(k).According to the values of L(k), we can identify the attribute with the highest classifiability in order to build a decision tree.In Definition 2, the membership values are required to be precise values. It is not always possible to have precise membership values, instead, we may restrict a membership value to an interval and replace a type-1 fuzzy set with an interval-valued fuzzy set.Definition 4[5,7,18] Let X denote a universe of discourse. An interval-valued fuzzy set is an expression A denoted by(5)A={(xi,μA(xi))|xi∈X;i=1,2,…,n}.where μA(xi):X→D([0, 1]), andxi→μA(xi)=[μ_A(xi),μ¯A(xi)]∈D([0,1]).If we represent the interval relationship withμ_A(xi)andνA(xi)=1−μ¯A(xi)then we get intuitionistic fuzzy sets [5–7,14]. The interval of intuitionistic fuzzy sets is denoted by [μA(xi), 1−νA(xi)]. In this paper, we transform the intuitionistic fuzzy sets into interval-valued fuzzy sets as follows.GivenμA(xi)=[μ_A(xi),μ¯A(xi)]=[μA(xi),1−νA(xi)]. Then we can represent the distance between set A and B in the form of interval-valued fuzzy sets:(6)d=12∑i=1n[|μ_A(xi)−μ_B(xi)|+|μ¯A(xi)−μ¯B(xi)|].As demonstrated in Eq. (2), a predefined parameter neighbourhood radius r is necessary for a given data set in a look-ahead algorithm. [12] comments that “r should be large enough such that each instance has a few instances in its neighbourhood r also should be small enough to keep the calculation of co-occurrence matrix local” [12]. Obviously, it is necessary to investigate the role of neighbourhood r values in constructing a decision tree [33].To identify the role of neighbourhood r values for different data sets, two data sets are adopted in our experiment here: weather [11] and car [22]. The weather data set is obtained from [11,33]. There are four attributes: “outlook”, “temperature”, “humidity”, “wind” and one classification attribute: “plan”. Their values are described as fuzzy subsets: the “outlook” can be sunny, cloudy or rain; “temperature” can be hot, mild or cool; “humidity” can be humid or dry; “wind” can be windy or calm and “plan” can be A, B or C. The car evaluation data set comes from [22]. It is comprised of six attributes: “buying”, “maint”, “doors”, “persons”, “lug_boot” and “safety” and one classification attribute: “car evaluation”. The concepts of car acceptability can be described by: “buying” for purchase price, “maint” for price of maintenance, “doors” for the number of doors, “Persons” for capacity in terms of persons to carry, “lug_boot” for the size of luggage and “safety” for estimated safety of the car. The attribute values of each attribute and classification attribute are described as fuzzy subsets: “buying” and “maint” can be very high, high, medium or low; “doors” can be 2, 3, 4 or 5-more; “persons” can be 2, 4 or more; “lug_boot” can be small, medium or big and “safety” can be low, medium and high and “car evaluation” can be unacceptable, acceptable, good or very good [22]. Both data sets are represented by their corresponding fuzzy membership values for the subsets of their attributes. There are 16 instances in the weather data set and 87 instances in the car evaluation data set. Based on these data sets, we can evaluate the effect of neighbourhood parameter on decision tree construction.Based on Eqs. (1)–(5), we got L(k) values for each attribute in each data set under different neighbourhood r values. The results are shown in Figs. 1 and 2.From Figs. 1 and 2, it is obvious that the dominance of attributes can be classified into two different regions with respect to r values. In Fig. 1, wind has the larger L(k) value when r<5, but temperature gets a larger value when ≥5. Similar situations happen in Fig. 2. Obviously, the selection of r value can change the preferred attribute for the same data set, and we have to know which r value is the ideal one.For the weather data set, the ideal r value is 3 according to [11]. In this sense, the preferred attribute to split the tree in the root node is wind. It is obvious that a less effective tree would be constructed if we selected a r value from the region where r≥5. This fact tells us that we have to know the right region before we determine the r value. In the weather data set, it is the region where 1<r<5. In this region, the only r value satisfying “large enough” and “small enough” in the same time as required by Ming Dong [12] is 3. In weather data set, the smallest tree is established when r=3.Similar to the weather data set, there are two different regions of r values in the car evaluation data set as shown in Fig. 8: one for r≤5 and the other for r>5. Persons has the larger L(k) values when r≤5. However, Lug_boot would be selected when r>5. We could not construct the tree by selecting a r value from the region where 0.5<r<1 due to the equal L(k) values for all attributes. It is obvious that any r value between 1 and 5 will give the same dominate attribute. As an example, we tested r=2 and r=8, and the result tree has 87 nodes for r=2, but 83 nodes for r=8. Thus, Lug_boot appears as a better choice to split the root node compared with persons. Obviously, the region where r>5 is preferred in the car data set. r=8 looks fine with the “large enough” and “small enough” principle in the second region. In this case, we tested only some r values between 0.5 and 10. There is no reason to stop at 10. Then the question is how to know which region is ideal and the size of each region.Obviously, the value of r has a significant impact on the structure of the tree. In fact, for each specific data set, there exists an ideal r value which helps to split the samples to construct a tree with a relatively small size. When the r value is too small, there are less samples in the neighbourbhood belonging to the same class, so the tree would be very big; however, when r value is too big, most samples will go to one class, which cannot differentiate different classes. Therefore, it is essential here to find a right value for r, which cannot be too small or too big. The r value depends on the size of the data, which makes it more difficult to know the right value of r for each data set. So far, there is no clear way to identify the r value, and it has become a bottleneck for the application of LAFDT. An alternative parameter insensitive to the change of data size would certainly be a significant help in this situation.Considering the role of r value, it defines the neighbourhood of a sample x. If we consider those samples within the r distance as a neighbour set of x, we can actually define a fuzzy set for x's neighbourhood, and an α cut of this fuzzy set could obviously determine the size of this neighbourhood.Let S be the universe of samples (objects) S={x1, x2, …, xn} and xc∈S. The neighbourhoods of xcis a subset A★(xc)⊆S:A★(xc)={neighbourhood of xc}. Obviously, A★(xc) can be described byA★(xc)={xi∣xi∈S,0≤μ¯c(xi)≤1}. Its fuzzy membership value could be derived according to the distance between xiand xc:i=1, 2, …, n. For a given distanceDxixc, we haveμ¯c(xi), which is a membership value for xi∈A★(xc).(7)μ¯c(xi)=D−DxixcD.Here D is the boundary value ofDxixc. For example, we could useDxixcmin(max), thenD=Dxixcmin(max).Dxixcmin(max)is the minimum of the maximum values ofDxixcfor all xiin A★(xc).It is clear that whenμc¯(xi)approaches to 1, xiis near to xc. In this sense, we can identify the neighbours of xcusingμ¯c(xi). For a set B★(xc)⊆A★(xc), if xi∈B★(xc), then xiis near to xcwith a membership value higher than a given value α. Clearly, this is α-cut of A★(xc). The set B★(xc) can be represented asB★(xc)={xi∣xi∈S,μ¯c(xi)≥α};i=1,…,n. We call B★(xc) as the neighbourhood set of xc.For a given optimal value of r, it is clear that we could find a corresponding value of α. We can get an α-cut equivalent to the neighbourhood set obtained according to neighbourhood radius r in LAFDT. The samples of set A★(xc) are associated with a family of crisp subsets and the restriction of samples is that their membership values are greater than or equal to some chosen value α in [0,1] [25]. Consequently, we obtain a crisp subset B★(xc). In this way, α can play the same role as r, and we can replace r with α in LAVIFDT-OPN. The r value depends on the data set. There is no bound to r value. However, α is bound within 0 and 1. Thus, we can replace the neighbourhood r with α in Eq. (2). Through the known r values for some data sets, we can find the corresponding α values, and if its change is much less than r values, then it is obviously more convenient to use α value instead of r value. There is no systematic method for the selection of r value at the moment, but there are some well known data sets with r values which could be identified through comparative study in experiment. So, we can find α values which are equivalent to those r values, and if the ideal α values do not change significantly with a significant change of the ideal r values, then we can set that α value as the optimised one.As discussed in Section 2, there are situations where a precise fuzzy membership value is not available but a restriction of the possible values into an interval is possible. Hence it is necessary to introduce interval-valued fuzzy sets into fuzzy decision trees. Such an extension will further complicate the identification of suitable neighbourhood parameter and a parameter insensitive to the change of data is required. Here, an interval-valued fuzzy decision tree with optimal parameter of neighbourhood (LAIVFDT-OPN) is developed to further enhance the existing LAFDT.The key step in LAFDT is the calculation of the distance between two samples or instances. In LAIVFDT-OPN, the two samples or instances involved are combined using two interval-valued fuzzy sets with elements of attributes. In LAFDT, the distance between two instances is calculated as the distance between two fuzzy sets, as shown in Eq. (1). Obviously, Eq. (1) is not applicable here, and the distance between two interval-valued fuzzy sets in Eq. (6) should be applied.Considering the same universe for attributes and instances in Section 2, we have the following definition for the distance between two instances:Definition 5Letμi(k)(x)=[μ_i(k)(x),μ¯i(k)(x)]andμi(k)(y)=[μ_i(k)(y),μ¯i(k)(y)]; (1≤k≤n, 1≤i≤mk) denote the interval-valued fuzzy membership value of instance x and y for the ith value of kth attribute. The distance between the two instances is(8)Dˆxy=12∑k=1n∑i=1mk[|μ_i(k)(x)−μ_i(k)(y)|+|μ¯i(k)(x)−μ¯i(k)(y)|].For any instance in the universe, we can restrict its circular neighbourhood to those objects within a radius r of x. Then a local co-occurrence matrix P for object x is defined.Definition 6Letμˆj(x),1≤j≤Cindicate the interval-valued fuzzy membership value of instance x for class j and letμˆj(x)=[[μ_1(x),μ¯1(x)],…,[μ_C(x),μ¯C(x)]]. The local co-occurrence matrix of instance x isFor look-ahead based interval-valued fuzzy decision tree (LAIVFDT), the local co-occurrence matrix of instance x is(10)Pˆ(x)=∑y,Dxy≤rμˆ(x)T×μˆ(y).where r is neighbourhood radius of x.From Eq. (4), letLr=c(k){k=1,…,n;c=1,…,m}represent the corresponding values of Lkfor different r values andLα=t(k){k=1,…,n;t=0,…,1}for different α values. TL(r,α) is an average difference betweenLr=ckandLα=tkfor all attributes. We have(11)TL(r,α)|r=c,α=t=1n∑k=1nLr=c(k)−Lα=t(k)where c represents r value, c≥0, 0≤t≤1 and n≥1. If the minimum value of TL(r,α) is reached at t=T, then we get α value: α=T. Based on TL(r,α) value we can identify the ideal α value.According to thePˆ(x)matrix, each matrix element is represented by an interval value. Schneider et al. [9] described an interval X as a closed, bounded set of real numbers, in which{x|X_≤x≤X¯,x∈X}can be denoted asX=[X_,X¯][9]. For all real numberX_,X¯,Y_andY¯. Such that0≤X_≤X¯≤1and0≤Y_≤Y¯≤1[8–10]. The rules of interval arithmetic are as follows:•Addition:[X_,X¯]+[Y_,Y¯]=[X_+Y_,X¯+Y¯].Subtraction:[X_,X¯]−[Y_,Y¯]=[X_−Y¯,X¯−Y_].Multiplication:[X_,X¯]*[Y_,Y¯]=[X_*Y_,X¯*Y¯].Division:[X_,X¯]/[Y_,Y¯]=[X_/Y¯,X¯/Y_]. assuming0<Y_Distribution law:min([X_,X¯],[Y_,Y¯])=[min(X_,Y_),min(X¯,Y¯)]andmax([X_,X¯],[Y_,Y¯])=[max(X_,Y_),max(X¯,Y¯)].Note that the operation[X_,X¯]/[Y_,Y¯]is undefined, ifY_=0,Y¯=0, or if bothY_=0andY¯=0[9].Therefore, the rules of interval arithmetic above are employed for calculating thePˆ(x)matrix in Eq. (9), theWˆ(k)matrix in Eq. (12) and theLˆ(k)matrix in Eq. (13). With the local occurrence matrix, we can derive the co-occurrence matrix for each attribute.Definition 7The local co-occurrence matrix for attribute k is selected as follows:(12)Wˆ(k)=∑i=1mk∑xPˆ(x).then, the classifiability of attribute k is(13)Lˆ(k)=∑i=1Cwii′(k)−∑i=1C∑j=1,i≠jwij′(k).wherewii′(k)andwij′(k)are elements ofWˆ(x). C is the number of fuzzy subsets of the classification attribute.For classification of L(k) values, we can identify the attribute k with the highest classifiability to build an interval-valued fuzzy decision tree. For LAIVFDT-OPN, it is worked out comparing two or more L(k) values and the highest single value is chosen. When the membership values are represented by intervals, theLˆ(k)value is an interval. The comparison ofLˆ(k)values is not as simple as other values. In this case, the values ofLˆ(k)are compared through their probability [9]. This probability is used to consider the chance of the occurrence of a sample x and y in the intervals.For example, letX=[x_,x¯]andY=[y_,y¯]denote the interval of X and Y, respectively. Suppose that the sample x is in the interval X and y is in the interval Y. The relationship between x and y can be either x<y, x=y or x≥y. For a complete discussion of the relationship between x and y, Schneider et al. [9] evaluated P(x≤y), P(x>y) and P(x≥y) as follows [9]. P(x≤y) is derived from P(x<y) or the probability of x=y within the interval of intersection X∩Y. It is denoted by P(x≤y)=P(x<y)+P(x=y) or P(x≤y)=P(x<y). For the probability of x>y and x≥y, P(x>y) is obtained by P(x>y)=1−P(x≤y) and P(x≥y) is derived from P(x≥y)=1−P(x<y). Only the probability of x<y is needed in this context. The probability of x<y is split into three types: P(x≤y)I, P(x≤y)Pand P(x≤y)F[9].1.P(x<y)I: x and y are in XI=X∩Y and x<y is within the intersection.P(x<y)P: x precedes the intersecting intervals.P(x<y)F: x is in the intersecting interval and y follows.According to [9], there are six possible cases of the probability P(x<y). The six possible situations could be represented using probability P(x<y) as follows [9]:1.If x precedes y entirely, there is no overlapping. (X∩Y)=Φ and X precedes Y then P(x<y)=1 (Fig. 1).If (X∩Y)≠Φ and (x<y) then P(x<y)=P(x<y)I+P(x<y)P+P(x<y)F(Fig. 2).If (x<y) and x precedes (X∩Y) then P(x<y)=P(x<y)I+P(x<y)P(Fig. 3).If (x<y) and x is in (X∩Y) and y follows then P(x<y)=P(x<y)I+P(x<y)F(Fig. 4).If (x<y) is in (X∩Y) then P(x<y) = P(x<y)I(Fig. 5).If x follows y entirely, there is no overlapping. (X∩Y)=Φ and X follows Y. Then P(x<y)=0 (Fig. 6).In general, the equation of P(x<y) is P(x<y) = P(x<y)I+ P(x<y)P+ P(x<y)F. Obviously, if P(x<y)> 0.5, it means that there is greater opportunity for x<y then x>y. In this case, we can consider x<y more possible than x>y. Then the attribute represented by y should be have priority over the attribute. The probability of P(x<y) is employed to find the maximumLˆ(k)value.Therefore, a fuzzy decision tree can be constructed by the following algorithm:Step 1Fuzzify the training data and testing data into interval-valued fuzzy sets.Compute the distanceDˆxybetween all instance x and y by Eq. (8).Calculate the local co-occurrence matrixPˆ(x)by comparison with the α value which is computed by Eq. (9). Compute the local co-occurrence matrixPˆ(x)by Eq. (9).Pˆ(x)is subject to the restriction of α.Select an attribute and sum the local co-occurrence matrix (Wˆ(k)) along each branch using Eq. (12).Normalise the matrix and calculateLˆ(k)using Eq. (13).Repeating step 4 to step 5 for all attributes.The attribute with the maximum probability for having greaterLˆ(k)than others is selected for the corresponding node to split the sample set into next layer branches.7.1For the root node, select the attribute with the highest possibility for having a greater value of the look-ahead termLˆ(k)than others.For each child node, the attribute with the highest possibility to have a greaterLˆ(k)value than that of the left attribute is selected to further split branches of the decision tree.The node is a leaf node if enough of the instances corresponds to the same class of classification.Note:1.L(k) value and TL(r,α) value are computed by Eqs. (4) and (11), respectively. The minimum value of TL(r,α) at α=t is selected for the optimal α value.The algorithm of LAIVFDT can be founded on [33].The proposed algorithm has potentially significant computation complexity. In Step 2, if the number of attributes and their associated value sets are considered as equivalent which can increase by the same size m, Eq. (8) has complexity of O(m2). However, if we fix the number of attributes and their associated value sets, the complexity of Eq. (8) is O(1). In Step 3, if we still refer the size of samples as m, Eq. (9) introduces a computation complexity of O(m). In Step 4, if we fix the sizes of the value subsets of attributes, the computational complexity of Eq. (12) is O(m2). If we consider the size of the value subsets a variable in the same level as m, then we have the complexity as O(m3). Step 5 involves all the complexity in Step 4, and is compounded by the number of classes. If we consider the number of class as a variable size, then the complexity is even higher than Step 4. Due to this computational complexity, we restrict our applications here to some small size data sets where the computational complexity does not pose serious problems. However, the computational complexity is a potential problem of the proposed algorithm, and we will address it in our future work.In this section, we verify the applicability of a single α parameter across two different data sets in Section 3 using the traditional LAFDT method firstly, and then demonstrate the feasibility of the proposed LAIVFDT together with the proposed α parameter using the weather data set. In the end, the proposed model is applied to a real world data set (a data set from real world factory). For the first two data sets, the primary aim is to compare their r values and identify their α value, so all samples in the data sets are applied to establish the trees to make comparison. For the real world application data set, we randomly divide the data set into two parts, 100 samples for training to establish the tree, and another 100 samples for testing (Figs. 7 and 8).Based on Eqs. (8)–(10) and our two data sets in Section 4, some experiments are carried out in this section to demonstrate the feasibility of the proposed parameter α. The results of TL(r,α) is shown in Figs. 9 and 10.In Figs. 9 and 10, “alpha” refers to α, and each curve represents a different α value.Fig. 9 gives the result of weather data set. It is clear that TL(r,α) reaches the minimum value when α=0.4 and r=3 or α=0.6 and r=2. It means that we can either select the corresponding α value as α=0.4 or α=0.6. For attribute selection in weather data set, there is no difference between r=2 and r=3, however, there is a different α value associated with each of them. To make a choice between these two values, we have to look at the decision trees constructed from each of them. Table 2 lists the corresponding experiment results for decision tree construction under different r and α values. Obviously, the decision tree for α=0.4 and α=0.6 are different: α=0.4 results in a smaller decision tree. Thus, the optimal α value is 0.40 for weather data set.In Fig. 10, both α=0.4 and α=0.2 reach the minimum when r>6. α values could be 0.2 or 0.4 in this case. We constructed decision trees by using r values at r=6, 7 and 8, and the results in Table 1show that there is no big difference for the decision trees constructed with these values. The r value corresponding to α=0.2 is very large, and include nearly all samples. In that sense, it is not a “local occurrence” anymore. Therefore, α=0.4 should be selected to satisfy the “local” methodology.Obviously, α=0.4 is valid in both data sets. Considering the fact that α is restricted in [0,1], then its value is stable even if the r value changes significantly. With different data, r could change significantly, but α is relatively stable and it provides an alternative to r in LAFDT.For LAFDT, the function of r value is to restrict a reasonable local area to construct a simple and effective tree. In this sense, the data size and data distribution should have a significant impact on its value. In comparison with r, α value is determined from a fuzzy set defined on the data set in relation to its size and data distribution. Therefore, for each ideal r value, there should be an ideal α corresponding to that value. Although the r value can change significantly from one data set to another, we expect a relative stable α value due to its bounded domain [0, 1]. The two experiments here have confirmed this expectation as the first step, but we should certainly verify it with more experiments. This paper has just started this comparison, and we expect more test experiments in the future to further confirm it.Mendel et al. [17] suggested that a membership degree μA(x) can be provided by an expert with an appropriate degreeμ˜A(x)and a bound △x describing his uncertainty [17]. For example, an interval of possible values of uncertainty can be expressed as[μ_A(x),μ¯A(x)]=[μ˜A(x)−△x,μ˜A(x)+△x]. Therefore, an interval-valued membership value is assigned as[μ_A(x),μ¯A(x)],0≤μ_A(x),μ¯A(x)≤1. In this way, the weather data set in Section 3 is converted into interval representation as shown in Table 2. The results are derived using the algorithm proposed in Section 4, as shown in Figs. 11–13.We used 16 instances of the interval valued data set and tested them with r = 0.5, 1, 2, 3, 4, 5 and 6. Figs. 11–13 illustrate the decision trees with different r values. The trees in Figs. 11 and 13 have 19 nodes and Fig. 12 has 21 nodes; the number of nodes in Figs. 11 and 13 is less than in Fig. 12. Thus, the decision trees in Figs. 11 and 13 are better than the tree in Fig. 12. The interval-valued fuzzy decision tree with r = 2 in Fig. 11 and r = 0.5 in Fig. 13 can be selected for a root node of the tree.For the given data set in Table 2, we constructed decision trees with r = 0.5, 1, 2, 3, 4, 5 and 6. When r = 0.5 and 2 we obtain the smallest tree with 19 nodes. when r = 1 the trees have 21 nodes. When r = 3, 4, 5 and 6 the trees could not be constructed, because there was not a dominant attribute for a root node. For example, if we select r = 0.5, we get the results ofLˆ(k)for each attribute at root node as follows (see Table 3):Lˆ(outlook) = [−0.89, 0.75]Lˆ(temperature) = [−1.54, 1.40]Lˆ(humidity) = [−0.97, 0.83]Lˆ(wind) = [−1.22, 1.01]Table 4illustrates the probability of x<y for each pair ofLˆ(k)with r = 0.5, 1, 2, 3, 4, 5 and 6. Using the algorithm in Section 4, each pair ofLˆ(k)is compared, e.g. the probability forLˆ(wind)≤Lˆ(humidity) with r = 0.5 is 0.516. As we can see, it has a confidence or about 51.6%. From Table 4, we can see all values in humidity column with r = 0.5 are greater or equal to 0.5. It indicates that the probability for any other attribute to have a lowerLˆ(x)value than humidity is greater than 0.5. Therefore, we can draw our conclusion that humidity should be selected as the root attribute to split the tree into branches.The probability forLˆ(wind)≤Lˆ(temperature) is 0.512. As we can see, it has a confidence or about 51.1%. From Table 4, we can see all values in the temperature column with r = 0.5 are greater or equal to 0.5. It indicates that the probability for any other attribute to have a lowerLˆ(x)value than temperature is greater than 0.5. Therefore, we can draw our conclusion that temperature should be selected as the root attribute to split the tree into branches.However, the probability for temperature to have a larger L(k) value than wind is 0.512, but the probability between humidity and wind is 0.516 and the probability of L(k) for humidity is greater than wind. Thus, we should select humidity.Similar comparisons can be done for r = 1 and r = 2. With r = 3, 4, 5 and 6, each column has both values greater than 0.5 and values less than 0.5. It indicates that we cannot find any dominant attribute to start as a root node.Obviously, with the proposed look-ahead based interval-valued fuzzy decision tree (LAIVFDT), data with uncertain fuzzy membership values could be adopted to construct a fuzzy decision tree. Therefore, a precise fuzzy membership is not a precondition to construct a decision tree anymore. Such relaxation can significantly benefit data mining where precise fuzzy membership values are difficult to get. In this paper, we tested with different r such as r = 0.5, 1, 2, 3, 4, 5 and 6, respectively. The difference of fuzzy decision trees using LAIVFDT and LAFDT are listed as follows:1.A smaller decision tree is obtained when r=0.5 or r=2 in LAIVFDT and r=3 in LAFDT, where the r value in LAIVFDT is less than the r value in LAFDT method.If the distance r changes then the dominant attribute is changed. Thus, r is significant in constructing the tree.LAIVFDT can construct a decision tree using interval-valued fuzzy membership values.The decision tree from uncertain membership values is different from that of precise membership values. We cannot take the average value of an interval-valued membership to construct the decision tree.The data in Table 2 shows the data of interval-valued weather data set for construction of LAIVFDT. Fig. 13 illustrates the LAIVFDT of weather data for r=0.5. Fig. 14denotes the LAIVFDT-OPN of weather data using restriction α=0.40. There are 19 and 17 nodes in Fig. 13 and Fig. 14, respectively. It demonstrates the superiority of the proposed LAIVFDT-OPN method over the traditional LAIVFDT method.In comparison with LAIVFDT, The LAIVFDT-OPN uses a simple α value which significantly simplifies the task of r value determination, and provides a better value in most cases than a trial value of r.To demonstrate the applicability of the proposed LAIVFDT model, we apply it to a real world machine maintenance example.The data set is a historical record of daily preventive maintenance injection machine check sheet, and it contains 200 instances gathered by Department of Machine Maintenance, DDK (Thailand) Ltd. For the specific machines in this research, 11 important positions must be checked every day. Their symbols and associated physical meaning are described as follows [35]:1.D1 is a position of safety door in non-operation side of an injection machine.D2 is a position of emergency stop button operation (rear) on the molding machine.D3 is a position of water jacket temperature setting.D4 is a position of an electric line and sensor thermocouple line of heater barrel.D5 is a position of purge cover interlock.D6 is a position of melt leakage at a nozzle.D7 is a position of emergency stop button operation (front) on the molding machine.D8 is instead of a position of a safety door in operation side of injection molding machine.D9 is a position of mold mounting bolts and bolts/nuts at fixed platen.D10 is a position mold mounting bolts and bolts/nuts at moved platen.D11 is mold die cleaning, e.g. Tie bar.All attributes are recorded as normal operation (NOP), partial failure (PAF) or completely failure (COF) and the classification attribute (status attribute) has its value as acceptable (ACT) or unacceptable (UAC). With the assistance of the technicians who recorded these data, we have converted these data into normalised interval values as shown in Tables 5 and 6.Using the proposed LAIVFDT-OPN method with α=0.40 and 100 instances randomly selected from the data set, a decision tree can be constructed from the DDK factory data set. According to the algorithm discussed in Section 4, the key is to choose the right attribute as the start node in root and each branch level. To identify the suitable attribute, the L(k) values have to be calculated firstly. Tables 7 and 8illustrate the L(k) values of each attribute under different levels of each branch. For example, the highest value of L(k) values for level 1 is [−0.55,1.02]. Thus, the root node is “D9”. For level 2, the first branch is “D9=NOP” where the highest value [−0.51,1.04] is for “D10” so that “D10” is selected for the node of that branch. The next branch is “D9=PAF” and its highest value [−1.61,2.29] is for “D4” so “D4” is selected. The final branch of “D9” node is “D9=COF” where its L(k) values for all attributes are equal and there is only one instance in this branch. Thus, we come to a leaf node where status=UAC. In this way, a decision tree is established as shown in Fig. 15. In this tree, the number of nodes is 40. Similarly, we can construct decision trees using LAFDT and F-ID3 as well. Together with our results from the weather data set in previous section, the results from LAIVFDT-OPN, LAIVFDT, LAFDT and F-ID3 is compared in Table 9. Obviously, for the weather data set, the LAIVFDT-OPN method is better than LAFDT method, LAIVFDT method and F-ID3 method due to the smallest amount of nodes and levels. There are 17 nodes in the resulted tree from LAIVFDT-OPN, but 18, 19 nodes and 20 nodes in the trees from LAFDT method, LAIVFDT method and F-ID3 respectively. For DDK factory data set, LAIVFDT-OPN method is also better than other methods due to the smallest amount of nodes and levels. There are 40 nodes for LAFDT and LAIVFDT methods and 52 nodes for F-ID3 method. There are 6 levels for the tree constructed using LAIVFDT-OPN method, 10 levels for the tree with LAFDT method and 11 levels for the tree from F-ID3 method.In Fig. 15, the fuzzy decision tree looks unbalanced. The data set in Tables 5 and 6 are not well balanced. The data of daily preventive maintenance injection machine check sheet contains more normal operation status, and only a small fraction of data for partial failure status and completely failure status. From Fig. 15, the best node for a root node is “D9” node. The decision tree in 15 is obtained by choosing “D9” as root node. To check the tree shape of trees rooted from other attributes, a different attribute can be selected as the root node. For example, we take the second best node “D4” as the root node, there are 3 possible branches:1.D4=NOP has 93 nodes.D4=PAF has 7 nodes.D4=COF has 0 node.Obviously, this is an unbalanced tree as well. In a similar way, we can show the trees rooted from other attributes are also unbalanced trees.As aforementioned, 100 samples randomly selected from the data set are taken to establish the decision trees, and the 100 samples are employed to test the performance of the established trees. Three performance criteria are measured: accuracy, sensitivity and specificity [36,37]. The performance evaluation are defined by(14)Accuracy=TP+TNTP+TN+FP+FN.(15)Sensitivity=TPTP+FN.(16)Specificity=TNTN+FP.where TP and TN are the number of true positives and true negatives, respectively. FP and FN are the number of false positives and false negatives, respectively [36,37].Table 10.In Table 11, the test results of LAIVFDT-OPN algorithm using Eqs. (14)–(16) are displayed to compare the correct and incorrect classification from each class. The number of acceptable status is higher than half of the testing samples at about 58 out of 100 instances. Table 11 illustrates the sensitivity and specificity by proportion of acceptable and unacceptable machine status. The accuracy of prediction by LAIVFDT-OPN method is approximately 79.0%. Sensitivity and specificity of “ACT” is 0.879 and 0.618 respectively and “UAC” has inverse values.The execution time of LAIVFDT-OPN model associates with the size of the generated rule base. Table 12compares the execution times of both the training data and the testing data by LAIVFDT-OPN model; the execution time in this table is not noticeable by the user on small data set.Table 13illustrates the outcome of F-ID3, LAFDT and LAIVFDT-OPN methods using the remaining 100 samples/instances for testing the models. The total percentages of predictive value for the positive test outcome are at about 72%, 76% and 79% for F-ID3, LAFDT and LAIVFDT-OPN, respectively. And the negative test outcomes are at about 28%, 24% and 21% for F-ID3, LAFDT and LAIVFDT-OPN, respectively. 71% of the acceptable statuses are LAFDT and LAIVFDT-OPN methods and F-ID3 method has 68%. 29% of the unacceptable statuses are LAFDT and LAIVFDT-OPN methods and F-ID3 method has 32%.Table 14illustrates the performance evaluation of F-ID3, LAFDT and LAIVFDT-OPN methods. In Table 14, LAIVFDT-OPN method has the highest amount of instances of acceptable status in the true positives at 58 instances and the F-ID3 has the lowest at 52 samples. F-ID3 method has the highest amount of instances of acceptable status in false positive at 16 instances and LAFDT and LAIVFDT-OPN have 15 and 13 instances, respectively. F-ID3 and LAFDT method have the lowest amount of instances of unacceptable status in true negative at 20 instances and LAIVFDT-OPN method has 21 instances. The lowest amount of instances of unacceptable status in false negative is LAIVFDT-OPN at 8 instances and the F-ID3 has the highest at 12 instances. LAIVFDT-OPN method has the highest percentage of accuracy at about 79.0%. The second highest percentage is LAFDT method at about 76.0% and the lowest percentage is F-ID3 method at about 72.0%. The most sensitivity to the least sensitivity are LAIVFDT-OPN, LAIVFDT and F-ID3, respectively.Comparing the results in Table 14, it is clear that LAIVFDT-OPN outperforms both LAIVFDT and F-ID3, and LAIVFDT gives better results than F-ID3. As our analysis in previous sections, the full employment of interval information in LAIVFDT-OPN and LAIVFDT enables them to produce better trees in terms of size and suitability to wider uncertainties. The LAIVFDT-OPN directly applies a α value which has been known to be a near optimised value for α, but LAIVFDT has to make many trails to find a suitable r value. Therefore, LAIVFDT-OPN has good chance to outperform LAIVFDT in most cases.

@&#CONCLUSIONS@&#
In this paper, we proposed LAIVFDT-OPN method to apply interval-valued fuzzy sets to construct an interval-valued fuzzy decision tree. In the proposed model, Hamming distance between two interval-valued fuzzy sets is applied to measure the distance between the two instances. A probability model is employed to compare intervals to determine the classifiability of each attribute. A systematic algorithm was established to construct a decision tree from data with uncertain membership values. Our examples demonstrate that the proposed method does construct an acceptable decision tree when interval-valued fuzzy membership values are involved in the data set. To determine the ideal distance restriction in LAIVFDT method, the significance of different distance restriction values was investigated and a new parameter α for restricting neighbourhood instances in LAIVFDT induction with optimal distance for fuzzy data was proposed and an optimal α value was identified for the experiment data sets. Our preliminary experiment results show that the ideal distance restriction changes with data set but α is much more stable. Our experiments are still limited in terms of the number of data sets and their scope, but it does demonstrate the potential of the proposed model.