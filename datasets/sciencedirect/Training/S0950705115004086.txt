@&#MAIN-TITLE@&#
Particle swarm optimization for time series motif discovery

@&#HIGHLIGHTS@&#
We consider the task of finding repeated segments or motifs in time series.We propose a new standpoint to the task: formulating it as an optimization problem.We apply particle swarm optimization to solve the problem.The proposed solution finds comparable motifs in substantially less time.The proposed standpoint brings in an unprecedented degree of flexibility to the task.

@&#KEYPHRASES@&#
Motifs,Time series,Anytime algorithms,Particle swarm optimization,Multimodal optimization,

@&#ABSTRACT@&#
Efficiently finding similar segments or motifs in time series data is a fundamental task that, due to the ubiquity of these data, is present in a wide range of domains and situations. Because of this, countless solutions have been devised but, to date, none of them seems to be fully satisfactory and flexible. In this article, we propose an innovative standpoint and present a solution coming from it: an anytime multimodal optimization algorithm for time series motif discovery based on particle swarms. By considering data from a variety of domains, we show that this solution is extremely competitive when compared to the state-of-the-art, obtaining comparable motifs in considerably less time using minimal memory. In addition, we show that it is robust to different implementation choices and see that it offers an unprecedented degree of flexibility with regard to the task. All these qualities make the presented solution stand out as one of the most prominent candidates for motif discovery in long time series streams. Besides, we believe the proposed standpoint can be exploited in further time series analysis and mining tasks, widening the scope of research and potentially yielding novel effective solutions.

@&#INTRODUCTION@&#
Time series are sequences of real numbers measured at successive, usually regular time intervals. Data in the form of time series pervade science, business, and society. Examples range from economics to medicine, from biology to physics, and from social to computer sciences. Repetitions or recurrences of similar phenomena are a fundamental characteristic of non-random natural and artificial systems and, as a measurement of the activity of such systems, time series often include pairs of segments of strikingly high similarity. These segment pairs are commonly called motifs [34], and their existence is unlikely to be due to chance alone. In fact, they usually carry important information about the underlying system [41]. Thus, motif discovery is fundamental for understanding, characterizing, modeling, and predicting the system behind the time series. Besides, motif discovery is a core part of several higher-level algorithms dealing with time series, in particular classification, clustering, summarization, compression, and rule-discovery algorithms [see, e.g., 40, 41].Identifying similar segment pairs or motifs typically implies examining all pairwise comparisons between all possible segments in a time series. This, specially when dealing with long time series streams, results in prohibitive time and space complexities. It is for this reason that the majority of motif discovery algorithms resort to some kind of data discretization or approximation that allows them to hash and retrieve segments efficiently. Following the works by Lin et al. [34] and Chiu et al. [13], many of such approaches employ the SAX representation [35] and/or a sparse collision matrix [9]. These allow them to achieve a theoretically low computational complexity, but sometimes at the expense of very high constant factors. In addition, approximate algorithms usually suffer from a number of data-dependent parameters that, in most situations, are not intuitive to set (e.g., time/amplitude resolutions, dissimilarity radius, segment length, minimum segment frequency, etc.).A few recent approaches overcome some of these limitations. For instance, Castro and Azevedo [11] propose an amplitude multi-resolution approach to detect frequent segments, Li and Lin [33] use a grammar inference algorithm for exploring motifs with lengths above a certain threshold, Wilson et al. [55] use concepts from immune memory to deal with different lengths, and Floratou et al. [18] combine suffix trees with segment models to find motifs of any length. Nevertheless, in general, these approaches still suffer from other data-dependent parameters whose correct tuning can require considerable time. In addition, approximate algorithms are restricted to a specific dissimilarity measure between segments (the one implicit in their discretization step) and do not allow easy access to preliminary results, which is commonly known as anytime algorithms [58]. Finally, to the best of our knowledge, only the authors in [51,52,56] consider the identification of motif pairs containing segments of different lengths. This can be considered a relevant feature, as it produces better results in a number of different domains [56].In contrast to approximate approaches, algorithms that do not discretize the data have been comparatively much less popular, with low efficiency generally. Exceptions to this statement achieved efficiency by sampling the data stream [12] or by identifying extreme points that constrained the search [38]. In fact, until the work of Mueen and Keogh [44], the exact identification of time series motifs was thought to be intractable for even time series of moderate length. In said work, a clever segment ordering was combined with a lower bound based on the triangular inequality to yield the true, exact, most similar motif. According to the authors, the proposed algorithm was more efficient than existing approaches, including all exact and many approximate ones [44]. After Mueen et al.’s work, a number of improvements have been proposed, the majority focusing on eliminating the need to set a fixed segment length [39,45,57].Mueen himself has recently published a variable-length motif discovery algorithm which clearly outperforms the iterative search for the optimal length using the algorithm in [44] and, from the reported numbers, also outperforms further approaches as in [39,45,57]. This algorithm, called MOEN [40], is essentially parameter-free, and is believed to be one of the most efficient motif discovery algorithms available nowadays. However, its execution time may still be unaffordable in a number of situations. Furthermore, MOEN is specifically designed to work with Euclidean distances after z-normalization. In general, exact motif discovery algorithms have important restrictions with regard to the dissimilarity measure, and many of them still suffer from being non-intuitive and tedious to tune parameters. Moreover, few of them allow for anytime versions and, to the best of our knowledge, not one of them is able to identify motif pairs containing segments of different lengths. With the approach we propose here we try to overcome all these shortcomings at the same time.In this article, we propose a new standpoint to time series motif discovery by treating the problem as an anytime multimodal optimization task. To the best of our knowledge, this standpoint is completely unseen in the literature. We first motivate such a standpoint and discuss its multiple advantages (Section 2). Next, we present SWARMMOTIF (Section 3), an anytime algorithm for time series motif discovery based on particle swarm optimization (PSO). We subsequently evaluate the performance of the proposed approach using 9 different real-world time series from distinct domains (Section 4). These include economics, car traffic, entomology, medical data, audio, climate, and power consumption. Our results show that SWARMMOTIF is extremely competitive when compared to the state-of-the-art, obtaining motif pairs of comparable similarity in considerably less time and with minimum storage requirements (Section 5). Moreover, we show that SWARMMOTIF is significantly robust against different implementation choices. These two aspects, together with its flexibility and extension capabilities, make SWARMMOTIF a unique novel solution for time series motif discovery. The latter implies that SWARMMOTIF can, for instance, deal with motifs of different lengths, apply uniform scaling, use any suitable dissimilarity measure, or incorporate notions of motif frequency. To conclude, we briefly comment on the application of multimodal optimization techniques to time series analysis and mining, which we believe has great potential (Section 6). The data and code used in our experiments are available online11http://www.iiia.csic.es/~jserra/swarmmotif.From the work by Mueen et al. [40,44], we can derive a formal, generic similarity-based definition [41] of time series motifs. Given a time serieszof length n,z=[z1,⋯zn],a normalized segment dissimilarity measure D, and a temporal window of interest between wmin and wmax samples, the top-k time series motifsM={m1,⋯mk}correspond to the k most similar segment pairszawa=[za,⋯za+wa−1]andzbwb=[zb,⋯zb+wb−1],for wa, wb∈ [wmin, wmax],a∈[1,n−wa+1],andb∈[1,n−wb+1]. Thus, we see that the ith motif can be fully described by the tuplemi={a,wa,b,wb}. To avoid so-called trivial matches [34], we can force that motifs are non-overlapping22Notice that, following [40], this definition can be trivially extended to different degrees of overlap., that is,a+wa<borb+wb<a. The motifs inMare ordered from lowest to highest dissimilarity such thatD(m1)≤D(m2)≤⋯≤D(mk)whereD(mi)=D({a,wa,b,wb})=D(zawa,zbwb). An example of a time series motif pair from a real data set is shown in Fig. 1.It is important to stress that D needs to normalize with respect to the lengths of the considered segments. Otherwise, we would not be able to compare motifs of different lengths. There are many ways to normalize with respect to the length of the considered segments. Ratanamahatana and Keogh [49] list a number of intuitive normalization mechanisms for dynamic time warping that can easily be applied to other measures. For instance, in the case of a dissimilarity measure based on the Lp norm, we can directly divide by the segment length33The only exception is with L∞, which could be considered as already being normalized., using brute-force upsampling to the largest length when wa≠ wb.From the definitions above, we can see that a brute-force search in the motif space for the most similar motifs is ofO(n2wΔ2),wherewΔ=wmax−wmin+1(for the final time complexity one needs to further multiply by the cost of calculating D). Hence, for instance, in a perfectly feasible case wheren=107andwΔ=103,we have 1020 possibilities. Magnitudes like this challenge the memory and speed of any optimization algorithm, specially if we have no clue to guide the search [23]. However, it is one of our main objectives to show here that time series generally provide some continuity to this search space, and that this continuity can be exploited by optimization algorithms.A fundamental property of time series is autocorrelation, implying that consecutive samples in a time series have some degree of resemblance and that, most of the time, we do not observe extremal differences between them44If a time series had no autocorrelation, we might better treat it as an independent random process.[29]. This property, together with the established ways of computing similarity between time series [50], is what gives continuity to our search space. Consider a typical dissimilarity measure like dynamic time warping between z-normalized segments and the time series of Fig. 1. If we fix the motif starting points a and b to some random values, we can computeD(zai,zbj)fori,j=wmin,⋯,wmax(Fig. 2A). We see that these two dimensions have a clear continuity, i.e., thatD(zai,zbj)∼D(zai+1,zbj)∼D(zai,zbj+1)∼D(zai+1,zbj+1),and so forth. Similarly, if we fix the motif lengths waand wbto some random values, we can computeD(ziwa,zjwb)fori=1,⋯n−waandj=1,⋯n−wb(Fig. 2B). We see that the remaining two dimensions of the problem also have some continuity, i.e.,D(ziwa,zjwb+j)∼D(zi+1wa,zjwb)∼D(ziwa,zj+1wb)∼D(zi+1wa,zj+1wb),and so forth. The result is a four-dimensional, multimodal, continuous but noisy55We use the term noisy here to stress that the continuity of the space may be altered at some points due to potential noise in the time series. It is not the case that we have a noisy, unreliable dissimilarity measurement D that could change in successive evaluations.motif space, where the dissimilarity D acts as the fitness measure (or objective function) and the top-k valley peaks (considering dissimilarity) correspond to the top-k motifs inM.Finding an optimization algorithm that can locate the global minima of the previous search spaces faster than existing motif discovery algorithms can be a difficult task. However, we have robust and established algorithms for efficiently locating prominent local minima in complex search spaces [4,6,27]. Hence, we can intuitively devise a simple strategy: if we keep the best found minima and randomly reinitialize the optimization algorithm every time it stagnates, we should, sooner or later, start locating the global minima. In the meantime, we could have obtained relatively good candidates. This corresponds to the basic paradigm of anytime algorithms [58].Anytime algorithms have recently been highlighted as “very beneficial for motif discovery in massive [time series] datasets” [57]. In an anytime algorithm for motif discovery,D(mi)improves over time, until it reaches the top-k dissimilarity valuesD(mi)*obtained by a brute-force search approach. Thus, we gradually improveMuntil we reach the true exact solutionM*. A good anytime algorithm will quickly find lowD(mi),ideally reachingD(mi)*earlier than its non-anytime competitors (Fig. 3).Note that a good but suboptimalMmay suffice in most situations, without the need thatM=M*. This is particularly true for more exploratory tasks, where one is typically interested in data understanding and visual inspection [see 41], and can also hold for other tasks, as top-k motifs can be very similar among themselves. In the latter situation, given a seed withinM*,we can easily and efficiently retrieve further repetitions via common established approaches [28,48]. Thus, only non-frequent or singular motifs may be missed. These can be valuable too, as the fact that they are non-frequent does not imply that they cannot carry important information (think for example of extreme events of interest that perhaps only happen twice in a measurement). For those singular motifs, we can wait longer if using an anytime algorithm, or we can resort to the state-of-the-art if that is able to provide its output within an affordable time limit.The continuity and anytime observations above relax the requirements for the optimization algorithm to be employed in the considered motif spaces (Sections 2.2 and 2.3). In fact, if we do not have to assess the global optimality of a solution, we have a number of approaches that can deal with large, multimodal, continuous but noisy search spaces [4,6,27]. Among them, we choose PSO [2,5,14,46,47]. PSO is a population-based stochastic approach for solving continuous and discrete optimization problems [5] which has been applied to multimodal problems [3]. It is a metaheuristic [4], meaning that it cannot guarantee whether the found solution corresponds to a global optimum. The original PSO algorithm cannot even guarantee the convergence to a local optimum, but adapted versions of it have been proven to solve this issue [53]. Other versions guarantee the convergence to the global optimum, but only with the number of iterations approaching infinity [53].PSO has gained increasing popularity among researchers and practitioners as a robust and efficient technique for solving difficult optimization problems. It makes few or no assumptions about the problem being optimized, does not require it to be differentiable, can search very large spaces of candidate solutions, and can be applied to problems that are irregular, incomplete, noisy, dynamic, etc. [see 5, [14,47,2,46], 3, and references therein]. PSO iteratively tries to improve a candidate solution with regard to a given measure of quality or fitness function. Hence, furthermore, it can be considered an anytime algorithm.Notice that treating time series motif discovery as an optimization problem naturally yields several advantages:1.We do not require much memory, as we can basically store only the stream time series and preprocess the required segments at every fitness evaluation.We are able to achieve a certain efficiency, as optimization algorithms do not usually explore the full solution space and perform few fitness evaluations [23].We can employ any dissimilarity measure D as our fitness or objective function. Its only requirements are segment length independence and a minimal search space continuity. Intuitively, this holds for the high majority of time series dissimilarity measures that are currently used (Sections 2.1 and 2.2). Additionally, we can straightforwardly incorporate notions of ‘interestingness’, hubness, or complexity [see 50]. This flexibility is very uncommon in current time series motif discovery algorithms (Section 1).We do not need to force the two segments of the motif to be of the same length. The dissimilarity function D can expressly handle segments of different lengths or we can simply upsample to the largest length [see 49]. Although considering different segment lengths has been highlighted as an objectively better approach, practically none of the current time series motif discovery algorithms contemplates this option (Section 1).Since we search for the optimal waand wb, together with a and b, we do not need to set the exact segment lengths as a parameter. Instead, we can use a more intuitive and easier to set range of lengths wa, wb∈ [wmin, wmax].We can easily modify our fitness criterion to work with different task settings. Thus, just by replacing D, we are able to work with multi-dimensional time series [22], detect sub-dimensional motifs [37], perform a constrained motif discovery task [38], etc.We can incorporate notions of motif frequency to our fitness function and hence expand our similarity-based definition of motif to incorporate both notions [41]. For instance, instead of optimizing for individual motifsmi,we can optimize sets of motifsMi′of size risuch that1ri∑mj∈Mi′D(mj)is minimal. We can choose rito be a minimum frequency of motif appearance or we can even decide to optimize it following any suitable criterion.In addition, using PSO has a number of interesting properties, some of which may be shared with other metaheurisics:1.We have a straightforward mapping to the problem at hand (Section 3.1).By construction, we have an anytime algorithm (Section 2.4).We can obtain accurate and much faster solutions, as compared to the state-of-the-art in time series motif discovery (Section 5.3).We have an essentially parameter-free algorithm [5]. As will be shown, all our parameter choices turn out to be non-critical to achieve the most competitive performances (Sections 5.1 and 5.2).We have an easily parallelizable algorithm. The agent-based nature of PSO naturally yields to parallel implementations [2].We still have the possibility to apply lower bounding techniques to D in order to reduce its computational cost [41,48]. Among others, we may exploit the particles’ best-so-far values or spatially close dissimilarities.All of these use a simple, easy to implement algorithm requiring low storage capabilities (Section 3.2).Our PSO approach to time series motif discovery is based on the combination of two well-known extensions to the canonical PSO [47]. On one hand, we employ multiple reinitializations of the swarm on stagnation [17]. On the other hand, we exploit the particles’ “local memories” with the intention of forming stable niches across different local minima [32]. The former emulates a parallel multi-swarm approach [3] without the need of having to define the number of swarms and their communication. The latter, when combined with the former, results in a low-complexity niching strategy [3] that does not require niching parameters [see the related discussion in 7, 8]. SWARMMOTIF, the implementation of the two extensions, is detailed in Algorithm 1. A schematic block diagram is shown in Fig. 4.SWARMMOTIF takes a time serieszof length n as input, together with a segment dissimilarity measure D, which will conform our objective function, and the range of segment lengths of interest, limited by wmin and wmax. The user also needs to specify k, the desired number of motifs, and tmax, the maximum time spent by the algorithm (in iterations66The number of iterations is easy to infer from the available time as, for the same input, the elapsed time will be roughly directly proportional to the number of iterations.). SWARMMOTIF outputs a set of k non-overlapping motifsM. We implementMas a priority queue, which typically stores more than k elements to ensure that it contains k non-overlapping motifs. This way, by sorting the motif candidates as soon as they are found, we allow potential queries toMat any time during the algorithm’s execution. In that case, we only need to dynamically check the candidates’ overlap (Section 3.2). Notice that n, D, wmin, wmax, k, and tmax are not parameters of the algorithm, but requirements of the task (they depend on the data, the problem, and the available time). The only parameters to be set, as specified in Algorithm 1’s requirements, are the number of particles κ, the topology θ, the constriction constant ϕ, and the maximum amount of iterations at stagnation τ. Nevertheless, we will show that practically none of the possible parameter choices introduces a significant variation in the reported performance (Section 5.1).Having clarified SWARMMOTIF’s input, output, and requirements, we now elaborate on its procedures. Algorithm 1 starts by computing the velocity update constants (line 1) following Clerc’s constriction method [15], i.e.,c0=2|2−ϕ−ϕ2−4ϕ|and(1)c1=c2=c0ϕ/2.Next, a swarm with κ particles is initialized (line 2). The swarm is formed by four data structures: a set of particle positionsX={x1,⋯xκ},a set of particle velocitiesV={v1,⋯vκ},a set of particle best scoresS={s1,⋯sκ},and a set of particle best positionsP={p1,⋯pκ}(the initialization of these four data structures is detailed in Algorithm 2). Particles’ positionsxiandpicompletely determine a motif candidate, and have a direct correspondence withmi(see Section 3.2). A further data structure Θ indicates the indices of the neighbors of each particle according to a given social topology θ (line 3). Apart from the swarm, we also initialize a global best score s* (line 4) and the priority queueM(line 5). We then enter the main loop (lines 6–26). In it, we perform three main actions. Firstly, we compute the particles’ fitness and perform the necessary updates (lines 7–16). Secondly, we modify the particles’ position and velocity using their personal and neighborhood best positions (lines 17–23). Thirdly, we control for stagnation and reinitialize the swarm if needed (lines 24–26). Finally, when we exit the loop, we return the first k non-overlapping motif candidates fromM(line 27).The particles’ fitness loop (lines 7–16) can be described as follows. For the particles that have a valid position within the ranges used for particle initializations (line 8; see also Algorithm 2 for initializations), we calculate their fitness D (line 9) and, if needed, update their personal bests siandpi(lines 10–12). As mentioned, D needs to be independent of the segments’ lengths, which is typically an easy condition for time series dissimilarity measures (Section 2.1). In the case that the particles find a new personal best, we save the motif dissimilarity d and its positionxiintoM(line 13). Next, we update tupdate, the last iteration when an improvement of the global best score s* has occurred (lines 14–16).The particles’ update loop (lines 17–23) is straightforward. We first select each particle’s best neighbor g using the neighborhood personal best scores sj(lines 18–21). Then, we use the positions of the best neighbor’s personal bestpgand the particle’s personal bestpito compute its new velocity and position (lines 22–23). We employ component-wise multiplication, denoted by ⊗, and two random vectorsu1andu2whose individual componentsui,j=U(0,1),being U(l, h) a uniform real random number generator such that l ≤ U(l, h) < h. Note that by considering the particles’ neighborhood personal bestspgwe follow the aforementioned local neighborhood niching strategy [32]. At the end of the loop we control for stagnation by counting the number of iterations since the last global best update and applying a threshold τ (line 24). Note that this is the mechanism responsible for the aforementioned multiple reinitialization strategy [17].The initialization of the swarm used in Algorithm 1 (lines 2 and 25) is further detailed in Algorithm 2. In it, for each particle, two random positionsxiandx′are drawn (lines 2–6) and the initial velocity is computed as the subtraction of the two (line 7). To obtainxiandx′,uniform real random numbersu=U(0,1)are subsequently generated. The personal best score siis set to infinite (line 8) andxiis taken as the current best positionpi(line 9). Note thatu(line 4) is used to ensure a uniform distribution of the particles across the triangular subspace formed by xi, 1 and xi, 3 (line 5; see also Section 2.1).

@&#CONCLUSIONS@&#
