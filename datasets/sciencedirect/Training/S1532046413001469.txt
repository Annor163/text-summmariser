@&#MAIN-TITLE@&#
FALCON or how to compute measures time efficiently on dynamically evolving dense complex networks?

@&#HIGHLIGHTS@&#
A new time efficient complex network analysis library for dense networks is proposed.It is suited for the analysis, e.g. of several thousands of distributed brain sources.High performance is achieved by combined hard- and software optimizations.Advantages arise for repetitive/successive analyses in dynamic time series.Considerable speedups are demonstrated by comparing to already available libraries.

@&#KEYPHRASES@&#
Complex network,Brain network,OpenCL,GPGPU,Code optimization,SSE,

@&#ABSTRACT@&#
A large number of topics in biology, medicine, neuroscience, psychology and sociology can be generally described via complex networks in order to investigate fundamental questions of structure, connectivity, information exchange and causality. Especially, research on biological networks like functional spatiotemporal brain activations and changes, caused by neuropsychiatric pathologies, is promising. Analyzing those so-called complex networks, the calculation of meaningful measures can be very long-winded depending on their size and structure. Even worse, in many labs only standard desktop computers are accessible to perform those calculations. Numerous investigations on complex networks regard huge but sparsely connected network structures, where most network nodes are connected to only a few others. Currently, there are several libraries available to tackle this kind of networks. A problem arises when not only a few big and sparse networks have to be analyzed, but hundreds or thousands of smaller and conceivably dense networks (e.g. in measuring brain activation over time). Then every minute per network is crucial. For these cases there several possibilities to use standard hardware more efficiently. It is not sufficient to apply just standard algorithms for dense graph characteristics. This article introduces the new library FALCON developed especially for the exploration of dense complex networks. Currently, it offers 12 different measures (like clustering coefficients), each for undirected-unweighted, undirected-weighted and directed-unweighted networks. It uses a multi-core approach in combination with comprehensive code and hardware optimizations. There is an alternative massively parallel GPU implementation for the most time-consuming measures, too. Finally, a comparing benchmark is integrated to support the choice of the most suitable library for a particular network issue.

@&#INTRODUCTION@&#
Many systems in nature, society and technology can be described by networks or, more mathematically, graphs. The study of those so-called complex networks is an interdisciplinary field which deals with general structural properties in biological, neural, physical, chemical, social or technical network structures [1–4]. Famous examples are the analysis of huge social networks consisting of millions of people connected by their acquaintance [5] and protein interaction networks [6]. Interestingly, networks of different fields show comparable characteristics like very sparse connectivity or the small-world property as there exist very short paths between arbitrarily chosen nodes [2].A complex network abstracts from a certain topic and provides uniform measures to analyze inherent network properties. As a common basis mathematical graph theory is used, which treats networks as a set of nodes connected by (un)directed and (un)weighted edges (also called links) regardless of the context they represent. For example, a weight can symbolize a strength, length or intensity in a specific context. In order to guarantee the applicability of some measure formulas, these complex networks are restricted to edge weights between 0 and 1, no self-connections of nodes and not more than one edge from one node to another.Although complex networks are inherently general, they reflect biological structures in nature on many scales. Researches investigated protein interaction networks [8], epidemic spreading [9,10], cellular networks [11], protein folding [12], metabolic networks [13] and many more. In medicine, psychology and neuroscience, complex networks are used to analyze normal and pathological brain structures and spatiotemporal dynamics of brain activation [14], learning processes [15], early human brain development [16], resting state networks [17,18], cognitive state changes [19], age and sex differences [20,21] and neuropsychological differences in Alzheimer’s disease [22], ADHD [23], schizophrenia [24], epilepsy [25] and other pathologies. An overview of the study of psychopathology with complex networks can be found in [26].A comprehensive overview about complex networks, their measures and formulas can be found in [27]. The formulas given there were the theoretical basis for this article. Whereas most measure formulas use sums and products over nodes and edges, some of them need search algorithms on graph data structures to find, e.g. shortest path lengths between node pairs. Technically seen, there are different classical data structures which represent graphs (see Fig. 1). Since there are also different standard search algorithms, the choice and optimization of a suitable algorithm as well as data structure is of a high importance.On the one hand, the popular social networks can get very large with several millions of nodes, but mostly the edges are distributed very sparsely between them [5], since not everybody knows thousands of other people. The edge density, the fraction of existing edges (en2-nwith n nodes and e directed edges ore/n2-n2with e undirected edges), is exceptionally low. An often observed characteristic is described as small-world property. In that case, most nodes are only sparsely connected, but every node has only a very short path to all the others [2,4]. In this social network context analysis can concentrate on one or a few of those networks. In that case, algorithms and data structures for sparse networks will be applied, like Dijkstra’s algorithm for weighted shortest path lengths on adjacency lists (see Fig. 1).On the other hand, one could for example investigate functional, causal and effective connections inside the human brain [27]. A representative example would be the exploration of EEG signals, acquired during brain stimulation or rest. Using methods for estimation of distributed brain sources and their temporal connectivity, a network of connected sources for each time step can be built. Such a source network would preferably represent a high resolution of several thousand sources (nodes). Then it is possible to investigate the spatiotemporal relation of integration and localization of information in the brain [28]. Since this is explorative research, it is not clear how much information (lightly weighted edges) can be left out, so that, in the computationally worst case, a network could be fully connected with weighted edges. In other words, the edge density would be near 100%. It is obvious, that this problem can occur in every approach, where connections between nodes are calculated statistically from measured data.If a lot of (maybe dense) networks have to be analyzed, calculation runtime becomes crucial. Let us assume, we want to examine the temporal course of brain network properties over time in a simple EEG experiment. Then for every time step a brain network has to be constructed and network measures have to be calculated. When for example investigating fine-grained, dense networks of 5000 nodes (e.g. distributed sources as nodes) with weighted edges (e.g. temporal source correlations), calculation of even fundamental measures can take a few minutes on a standard desktop PC. It depends on used hardware, algorithms, graph data structures, implementation and edge density. When we want to know all clustering coefficients and all shortest path lengths in those networks then this may take perhaps 20min per network.For one network, this may be just annoying to wait for. But if we analyze an experiment of only 10 s and create a network every 50ms for a good temporal resolution, than 2000 networks have to be investigated (each with 5000 nodes and at worst up to almost 25,000,000 weighted edges). Even ignoring each network creation, a successive measure calculation would take about 28days for one trial of this experiment.For those cases, the need for faster computational approaches for dense networks on standard hardware is obvious. Unfortunately, just the most fundamental measures, that serve as basis for higher measures, are the hardest to calculate, as shown in the next section.The hierarchy diagram (Fig. 2) shows computational dependencies between a few complex network measures, each computed for all nodes, node pairs or the network. The corresponding formulas can be found in [27]. Every measure needs its predecessor measures to be computed before itself can be retrieved. Thus, it inherits the computational complexity of its predecessors. Therefore it is the most important task to optimize the fundamental computations like the numbers of (undirected, weighted or directed) triangles around all nodes and the shortest (undirected, weighted or directed) path lengths between all node pairs. Unfortunately, both need the most time since they have a cubic time complexity in the worst-case when calculated for all nodes.The denser a network is, the more calculation actually becomes this worst-case scenario. Since there is a lack of special treatment for this kind of networks, this causes serious time problems as mentioned above. So, it became necessary to create a highly optimized, but general complex network analysis software for standard hardware to enable valuable time saving for current research.FALCON (FAst Library for COmplex Networks) was developed because of the lack of network analysis software specialized in dense complex network analysis. The kind of EEG experiments described in the last chapter demonstrates one of the primary reasons to develop a new optimized library. The goal of FALCON was to compute all measures shown in Fig. 2, each for all nodes, node pairs respectively the entire network.There are three edge types: undirected (unweighted), (undirected) weighted, and directed (unweighted) edges. Node-based measures for these three versions are betweenness centralities, numbers of triangles, average neighbor degrees, clustering coefficients, average distances, efficiencies and closeness centralities. Based on node pairs are shortest path lengths. For the whole network there are number of links respectively sum of weights, transitivity and modularity. At last, there are six kinds of degrees for all nodes: (undirected, unweighted) degrees, directed degrees, directed reciprocal degrees, directed in-degrees, directed out-degrees and weighted degrees.Measures for directed, weighted edges are issue of current research and not implemented yet. But there are many of them available in the Brain Connectivity Toolbox (BCT)[27].

@&#CONCLUSIONS@&#
