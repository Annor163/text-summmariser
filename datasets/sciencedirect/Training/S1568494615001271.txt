@&#MAIN-TITLE@&#
System reliability prediction by support vector regression with analytic selection and genetic algorithm parameters selection

@&#HIGHLIGHTS@&#
A novel ASGA–SVR method has been proposed and applied for reliability prediction.This method combines an analytic selection (AS) and GA search for SVR parameters.The combination uses the prior knowledge by AS for guiding GA to avoid local optima.ASGA is superior to GA in accuracy, convergence speed and robustness in experiments.

@&#KEYPHRASES@&#
Reliability prediction,Time series forecasting,Support vector regression,Parameter selection,Analytic selection,Genetic algorithms,

@&#ABSTRACT@&#
We address the problem of system reliability prediction, based on an available series of failure time data. We consider support vector regression (SVR) as solution approach, for its known performance on time series forecasting. However, SVR parameters selection is very critical for obtaining satisfactory forecasting. Currently, two different ways are followed to set the values of SVR parameters. One way is that of choosing parameters based on prior knowledge or experts experience on the problem at hand: this is a simple and quick, practical way but often not optimal in complex situations and for non-expert users. Another way is that of searching the values of the parameters via some intelligent methods of optimization of the SVR regression performance: for doing this efficiently, one must avoid problems like divergence, slow convergence, local optima, etc.In this paper, we propose the combination of an analytic selection (AS) method of prior selection followed by a genetic algorithm (GA) for intelligent optimization. The combination of these two methods allows utilizing the available prior knowledge by AS for guiding the GA optimization process so as to avoid divergence and local optima, and accelerate convergence. To show the effectiveness of the method, some simulation experiments are designed, based on artificial or real reliability datasets. The results show the superiority of our proposed ASGA method to the traditional GA method, in terms of prediction accuracy, convergence speed and robustness.

@&#INTRODUCTION@&#
Rapid technological development contributes to improving welfare in many ways but also leads to increasing complexity in modern processes, systems and plants. This, together with the usual necessities of business profitability, safety of human life and protection of the environment, requires accurate evaluation of the reliability of systems, components and plants throughout their lives. Therefore, reliability prediction has received an increasing attention in practice and become an active area of research.The aim of reliability prediction is to estimate the future occurrences of system failures for maintenance planning and replacement policies, etc. [1]. A number of methodologies for this purpose have been proposed, including principle-based methods [2], statistics-based methods [3] and machine learning methods [4]. The first type of methods stands on models based on the physics, chemistry, mechanical and structural principles of the processes of degradation. The second type of statistics-based methods is based on statistical models and is useful when it is difficult to describe the reliability behavior of the system based on poorly known principles. Reliability models are developed by statistically describing the degradation and failure processes based on some assumptions and the values of the models parameters are selected based on failure data. However, the prior assumptions upon which the models are based may not always be applicable in the actual system operation environment, so that the reliability prediction results may not be accurate. The third type of methods, based on machine learning, comprises empirical algorithms that are designed and trained based on failure data. Popular machine learning methods include, for example, artificial neural network (ANN), and support vector machine (SVM). ANNs have already been applied to system reliability estimation, with demonstrated advantages over principle-based models [5–7]. Yet, ANNs may suffer from problems like ‘over-ﬁtting’, slow convergence and local optima [8]. Support vector machine (SVM) is another powerful learning machine paradigm [9]. SVMs are based on statistics learning theory or VC theory (VC-Vapnik, Chervonenkis, from the last names of the original proposers) and embody the idea of minimizing an upper bound of the generalization error by structural risk minimization (SRM), rather than the empirical risk minimization (ERM) adopted by neural networks. Because the ERM principle is only appropriate when there is a large number of training examples that somewhat guarantees good generalization performance, SVMs outperform ANNs particularly when only a small dataset is available for training. SVMs have been applied to many areas spanning from pattern recognition to fault diagnosis [10]. With the introduction of the so called ɛ-insensitive loss function, it has been extended to support vector regression (SVR) [11] to solve non-linear regression/prediction problems [12,13], including those associated to system reliability prediction [14–18].When ANN or SVR are used for reliability prediction, reliability prediction problem is often framed as a kind of time series prediction problem, whose goal is to estimate future values based on current and past data samples. Thus, it is necessary to introduce the development of the time series prediction problem, especially methods which are baseline algorithms in our experiments. Besides autoregressive integrated moving average (ARIMA) method [19], Kalman filter [20], ANN and SVR, some approaches related to fuzzy inference have been proposed in recent years. For example, Melin [21] brings up a paradigm that integrates the ANFIS (adaptive network based fuzzy inference system) to an ensemble to predict the value of time series. This EANFIS method is concise, efficient, and effective when forecasting the Mackey–Glass, the Mexican exchange stock, and the Dow Jones time series. Another time series prediction approach is the optimized ensemble neural networks with fuzzy integration (EFNN) proposed by Pulido [22]. This method utilizes the fuzzy inference to integrate the neural network optimized by GA and also shows satisfactory performance in predicting chaotic time series. Though this kind of methods based on fuzzy inference may perform comparably to, or even better than SVR-based methods when addressing the general time series prediction problem, SVR-based algorithms show superiority in case that only small training dataset is available, which common exists in the practice of reliability prediction problems.However, to obtaining accurate regression/prediction result for SVR, parameters selection is very important. Existing methods of parameter selection for SVR can be divided into two classes. The first kind of methods is based on prior knowledge of the analyst on the problem at hand. For example, Cherkassky proposed an analytical selection (AS) method [23] to choose SVR parameters directly from the training data, based on some existing consensus that the SVR parameters are suitable relative to statistical properties of the training data. This type of methodology is simple and effective for determining the parameters, provided that the prior knowledge is sufficiently informative. Obviously, in complex problem settings (high dimensional spaces, nonlinear functions, few representative data, etc.), these methods are not suitable.The second type of methods searches for the values of the parameters within an optimization scheme defined on specific performance objectives of the algorithm. In general, there are three types of searching methods that can be used for SVR parameter selection:(1)First are the exhaustive methods for searching the best values of the parameters in the entire parameter space. A typical exhaustive method is the grid-searching method [24], which divides the parameter space and calculates the SVR generalization performance with the parameters set at the values of each grid point. This process is very time-consuming.The second class of searching methods comprises the traditional optimization methods including the gradient descent method [25], ellipsoid method [26] and simultaneous perturbation stochastic approximation method [27]. These methods are not easily generalizable and perform well only in specific situations.The third class comprises intelligent optimization methods which are powerful searching methods that have emerged rapidly in recent years and have attracted significant attention because of their good performances in various problem settings, even highly complicated. For example, simulated annealing (SA) [28], genetic algorithm (GA) [29,30] and particle swarm optimization (PSO) [31,32] have been proposed for optimal values of the SVR parameters applied to system reliability prediction. Of course, these optimization algorithms could also be combined with other methods, such as approaches based on the fuzzy theory, and applied in SVR parameter selection. In 2013, Melin used fuzzy logic to improve the convergence and diversity of the swarm in PSO [33]. Valdez also combined the optimization results of PSO and GA by Fuzzy logic. Because more than one optimization methods are integrated together, these two methods have superior searching ability, but consume much more computational resources. Among these methods, GA is perhaps the most frequently used because of its demonstrated global search efficacy.Even though different optimization methods have been integrated in some literatures, there is no paper that combines the prior knowledge with the intelligent optimization scheme to choose SVR parameters. However, this combination is very necessary because most of optimization schemes are designed to solve the general problems and prior knowledge is obtained from special applications. If the problem-specific prior knowledge could guide the search process, some problems such as local optima and slow convergence speed, existing in optimization scheme, could be avoided.The purpose of this paper is to propose a novel SVR-based reliability prediction algorithm which combines two methods belonging to different class, the AS method and the GA method, which actually combines the prior knowledge with the intelligent optimization. In this paper, we consider the reliability prediction as a time series prediction problem, SVR is used to forecast the reliability of a system and a novel hybrid optimization method is presented to optimize parameters of SVR for a special application. The whole algorithm is named ASGA–SVR. The novelty of our algorithm focuses on SVR parameter selection method that combines the prior knowledge as used by AS for guiding the optimization search of GA during SVR parameter estimation. That is, the parameters values selected by the AS method are exploited to adjust the values of chromosomes after each crossover operation of GA evolution process. This new idea takes simultaneously full advantage of prior knowledge on SVR parameter selection and of the search power by intelligent optimization methods. The advantages of our algorithm over other methods exhibit in following aspects:(1)Compared with SVR using AS to estimate parameters, our ASGA–SVR has more accurate prediction results because of the following optimization by GA.By guiding with prior knowledge about the SVR parameters, the GA searching procedure avoids falling into local optima or divergences, and actually the prior information accelerates its convergence. Compared with SVR choosing parameters by standard GA, our ASGA–SVR method has more accurate prediction results, higher convergence speed and stronger robustness.Making comparation between our ASGA–SVR and other SVR methods using different parameter optimization techniques such as FPSO and FGAPSO, we find that the accuracy of them are similar, but our method spends less running time.When there are plenty of training data, our ASGA–SVR has the same prediction accuracy as that of prediction methods based on fuzzy inference theory. But when the historical data are insufficient, the prediction accuracy of our method is higher than that of fuzzy inference based method.The remainder of the paper is organized as follows. Section 1 introduces the background knowledge about SVR and GA–SVR, necessary to render the paper self-contained. Section 2 gives an exhaustive description about our ASGA–SVR method. The experiments are presented in Section 4 and, then the results are analyzed therein. Section 5 provides the conclusions that can be drawn from the findings of our research.The reliability of a component or a system generally changes with time and reliability prediction is often framed as a kind of time series prediction problem whose objective is to find a function to describe this evolution in time. With respect to a discrete time seriesYof variable y, time series prediction could be described as:(1)yˆi=f(si)=f(yi−1,yi−2,…,yi−l)where the outputyˆiis the prediction value of the ith sample of the discrete time seriesY, input vectorsiis composed of the l-lagged last values ofY, and l is the number of lagged variables.In this paper, SVR is used to build the function f and a hybrid GA-based method is proposed to identify SVR's parameters. Then, a brief introduction is given to explain the principle of SVR and how GA to choose parameters for SVR.In brief, for a datasetD={(si,yi}1n, wheresi∈Rldenotes the l-dimension input vector, yidenotes the real-valued output and n is the number of data patterns, the regression task amounts to finding a function betweensiand yi, which in the linear case can be described as follows:(2)yi=f(si)=wTsi+bwherewand b are respectively the weight vector and intercept of the regression model, whose values need to be determined so that the linear function built indeed fits at best the linear relation between the input and the output, as represented by the available dataset.In the nonlinear case, a nonlinear mapping Φ: Rl→F, where F is the feature space of Φ is introduced to translate the complex nonlinear regression problem in Rlto a simple linear regression problem in F. The regression function after this transformation reads:(3)yi=f(si)=wTΦ(si)+bTo evaluate the goodness of the regression function, the ɛ-insensitive loss function is used [9]:(4)l=|yi−f(si)|ε=0,|yi−f(si)|≤ε|yi−f(si)|−ε,|otherwiseBy ignoring the error if the difference between the estimated value obtained by Eq. (3) and the real value is smaller than ɛ, the ɛ-insensitive loss function measures the empirical risk. The parameter ɛ is to be tuned. Then, a procedure is set up for minimizing the empirical risk by introducing the slack variables ξ, ξ* that represent the deviation of the training data outside the ɛ-insensitive zone.Besides minimizing the empirical error by the ɛ-insensitive loss function, we must also minimize the Euclidean norm of the linear weightw, i.e.w, which is related to the generalization ability of the trained SVR model. As a result, a compromised quadratic optimization problem to identify the regression model arises as follows:(5)minw,ξ,ξ*J(w,ξ,ξ*)=12||w||2+C∑i=1n(ξ+ξ*)s.t.yi−wTΦ(si)−b≤ε+ξiwTΦ(si)+b−yi≤ε+ξi*ξi,ξi*≥0i=1,…,nwhere C denotes the penalty coefficient that determines the trade-off between empirical and generalization errors, whose value needs to be properly set. Through a Lagrangian dual method, we can obtain the solution of this quadratic optimization problem and estimate the output value as:(6)f(si)=〈w⋅Φ(si)〉+b=∑j=1nαiK(si,sj)+bK(si,sj)=Φ(si)TΦ(sj)where K(si,sj) is a kernel function satisfying the Mercer condition [34]. If not mentioned specifically, the kernel function used in this paper is the radial basis function with width γ, a parameter that also needs to be set.From Section 2.1, it is seen that there are three free parameters, ɛ, C and γ, which are important for the performance of the SVR method and, thus, need to be optimally set. Among the optimization methods for selecting SVR parameters, the genetic algorithm (GA) is common used because it is effective in the global search of complex search spaces [29]. As one kind of evolutionary computational search algorithm, GA encodes each potential solution of the target optimization problem by a simple chromosome-like data structure, and then sifts the critical information via some recombination operators that imitate biological evolution processes such as survival of the fittest, crossover and mutation [35]. The SVR model optimized by the GA method is hereafter named GA–SVR and its implementation is briefly described below.First, a set of three-dimensional chromosome-like vectors {X1,X2,…,Xi,…}, each of which represents a combination of three parameters, i.e.,X=[C,ɛ,γ], are randomly generated within a given range and taken as the initial population. The fitness value of each Xiin the population is calculated according to the mean square error of the k-fold cross validation method [36] on the training data with SVR parametersXi. On the basis of the calculated fitness values, a standard roulette wheel method [35] is employed to select survival individuals from the current population. Finally, the new 2nd generation population is generated from these survival individuals by crossover and mutation operators. The process of population evolution is iteratively repeated until a predefined stop criterion is met. Fig. 1shows the flowchart of the GA–SVR.From the flowchart of the GA–SVR, it can be seen that the GA operates without prior knowledge about the target optimization problem. On the other hand, the use of some problem-specific knowledge a priori available may guide the GA search and improve its performance.To improve the GA–SVR method in terms of regression or prediction accuracy, avoiding local optima and convergence speed, we propose to guide the GA search in a narrower range of parameter values and along directions which seem more profitable for the optimization of the SVR objectives. To do this, we introduce prior knowledge on the specific optimization problem during the GA search. The prior knowledge is extracted from the training data available.In many works on SVR, it is mentioned that the selection of the optimal parameters of the SVR is closely related to the statistical characteristics of the training data. For example, Mattera suggested that a ‘good’ value for parameter C can be chosen equal to the maximum in the range of output values in the training dataset [37] and Cherkassky translated this in the following implementation [23]:(7)C=max(|y¯+3σy|,|y¯−3σy|)wherey¯and σyare the mean and the standard deviation of the output values in the training dataset.It is also broadly accepted that the value of ɛ should be proportional to the input noise level [11,38]. The choice of ɛ should also be related to the size of the training dataset: intuitively, for datasets of large size small ɛ-values should be taken, which led Cherkassky to propose the selection of ɛ as follows [23]:(8)ε=3σlnnnwhere σ is the noise level in the training dataset estimated by the k-nearest neighbors method.For the value of the width parameter γ of the kernel function, it is well accepted that it should be selected to reflect the variability range of the input in the training dataset. Considering univariate inputs, for simplicity of illustration, γ could be, for example, set to:(9)γ∼(0.1−0.5)×range(s)where range(s)=|max(s)−min(s)|.Eqs. (7)–(9) compose a method of analytic selection (AS) of the values of the SVR parameter triplet,X=[C,ɛ,γ], based on the characteristics of the training data and estimated noise level.We propose to combine the prior knowledge as used by AS for guiding the optimization search of GA. The parameters values selected by the AS method are used to adjust the GA evolution process. This modification is realized by embedding a simple drifting operation after each crossover operation. The main steps of our ASGA–SVR are presented below, with reference to the flowchart of Fig. 2:(1)Representation: ChromosomeXis directly represented as a SVR parameter vectorX=[C,ɛ,γ]. The population at the ith generation is represented asGi={X1i,X2i,…,Xni}, where n is the number of chromosomes in the population.AS method: By applying Eqs. (7)–(9) to the training data, the AS identifies the values of the parameters vectorXc.Initialization of GA: The initial population for the GA search is composed of n (in this study equal to [40]) chromosomes randomly generated within the given ranges of variability of the three parameters.Fitness calculation: For every individualXiof the population in the current generation, the fitness value is calculated analogously to what is done for the standard GA–SVR.Selection: The standard roulette wheel method is employed to select survival chromosomes from the current population, in proportion to their fitness values.Crossover and mutation: these operations play a fundamental role in the progression of the search for the best chromosomes. In our method, the simulated binary crossover [39] and polynomial mutation methods are used. The probabilities of crossover pcand of mutation pmare respectively set to 0.8 and 0.05.Drifting: After the crossover operation, the newly generated pairs of chromosomes are drifted based on AS reference vectorXcas follows.The pair of individualsXaandXbafter crossover are drifted as follows:(10)X′a=Xa+1/AS_factor⋅(Xc−Xa)X′b=Xb+1/AS_factor⋅(Xc−Xb)where AS_factor is a parameter that allows controlling the amount of modification introduced: the larger the value of the AS_factor, the weaker the influence of AS on the GA search.Elitist strategy: The chromosome with the best fitness will skip the crossover and mutation operations and directly survive in the next generation.Stopping criteria: Steps 4–8 are repeated for a predefined number of generations (in our application this is set to 100).SVR prediction: With the optimal parameters found at the end of the generations, we can regress/predict the future output values (reliability values, in our case) by the SVR model.In this section, some experiments based on artificial and real reliability data are analyzed to verify the performance of the proposed ASGA method. Firstly, the real reliability data used in the experiments are introduced. Then, our method is compared with GA–SVR in terms of prediction accuracy, convergence speed and robustness in the first group of experiments. However, the influence of prior knowledge on the GA search is demonstrated by an example before the comparation. In the second group of experiments, FPSO and FGAPSO are used to optimize parameters of SVR. The prediction accuracy and running time of FPSO–SVR, FGAPSO–SVR are analyzed to compare with those of ASGA–SVR. At last, EANFIS and EFNN are taken as benchmarks to compare how the forcasting accuracy varies with the number of training data for three methods.In this paper, three sets of reliability data, i.e. the turbocharger failure data, age of a submarine diesel engine and miles-to-failure data of car engines, are chosen as the test datasets. More detailed descriptions about these reliability data are listed in Table 1. The data in the first set show a strong linear relation and those in the second set follow a piecewise linear function approximately, while the data in the last set are nonlinear. The experiments presented in the following sections are based on these datasets.In this section, some experiments based on artificial and real reliability data are analyzed to verify the superiority of the proposed ASGA method to GA–SVR.In this experiment, we want to show how the prior knowledge adjusts the optimization search of GA. In this example, the evolution of the population during GA search is illustrated and these illustrations partly discover why our ASGA–SVR overperforms GA–SVR through following evolution of the population during the search. To simplify the explanation, only dataset 1 is considered in this experiment. Out of the set of 40 data, 30 samples are used as training data and the rest 10 samples as test data. The initial population is randomly generated for both GA and ASGA. For the latter, the AS_factor is set equal to 5, on trial and error basis. In this experiment, we look at how the populations which represent various sets of SVR parameter values evolve during the optimization searches of the ASGA and GA methods. The experiments are repeated 20 times and two typical results are reported below. The evolution of the populations for the two search methods and their corresponding predictive performance are illustrated in Figs. 3 and 4. The solution of the exhaustive grid-searching method is also obtained and labeled in the figures as a benchmark. The reason we choose the exhaustive grid-searching method as a benchmark method is that the results of this method do not depend on the starting point and could be regarded as an optimal solution when the scale of the grid is set approximately fine enough.Additionally, the solution of AS is also shown in these two figures. It can be seen that the solution found by AS on the basis of the training data characteristics is clearly not the optimal solution (found by the exhaustive grid search), although it is near to it.In Fig. 3, the simulation result shows that the final populations of both GA and ASGA methods are around the grid-searching solution. However, under the direction of AS solution, the ASGA population is on average closer to the grid-searching solution than that of the GA and also, ASGA has a slightly superior predictive performance. However, in Fig. 4(a), it is seen that the final population of GA is scattered far from the grid-searching solution while, under the guidance of AS solution, that of ASGA is still near to the optimal solution. Sub-figure (b) shows the bad predictive performance of the GA solution in this case. The SVR prediction accuracy of the above two cases is shown in Table 2, with the optimal parameter values found by the different methods. The metric we use here to measure the prediction accuracy is the root mean square error (RMSE) value defined as follows:(11)RMSE=∑k=1n(yk−yˆk)2nIn case 1, the final population of GA clusters around the grid-searching solution and the prediction accuracy of GA is comparable with that of ASGA, which is very close to the accuracy of the grid-searching solution. But when the evolution of GA diverges as shown in case 2, the GA method has poor prediction accuracy. On the contrary, with the drifting modification of AS, the prediction accuracy of the ASGA method keeps to a robust and satisfactory level. From Table 2, it is worth noting that the ASGA has improved the prediction accuracy significantly, even if the prediction accuracy of the AS method by itself is not very good. This is because the AS method selects the SVR parameters values based on information on the training data only, whereas our ASGA further explores the space of parameters values seeking to converge near to the optimal solution, steered also by the AS prior knowledge.We now further investigate the improvement of prediction accuracy obtained by our method. The proposed ASGA–SVR and GA–SVR are applied to predict the reliability datasets 1, 2, 3. The length of the training data for the three methods is 30, 60 and 90, respectively, and the rest samples are regarded as the test data. To account for the stochasticity inherent in the GA search, the optimization is repeated 20 times and the RMSE of the results are listed in Table 3.The initial population of GA–SVR and ASGA–SVR are randomly chosen in the solution space and the AS_factor is set as 3 for ASGA–SVR. The results show the superior prediction accuracy of the ASGA method with respect to the GA method, on average. Also, ASGA–SVR is more stable than GA–SVR if we look at the dispersion of the 20 RMSE results. For example, in the table we have highlighted in bold the RMSE values larger than 1, which are 14 for the first dataset, 3 for the second dataset and 5 for the third dataset, for GA, and 3, 1, 0, respectively, for ASGA. Note also that the RMSE values of the AS method for dataset 1, dataset 2 and dataset 3 are, respectively, 6.094319, 1.0416 and 4.8859, which are quite poor compared to the values obtained for the GA and ASGA methods.In some practical applications, also the running time of the algorithms also can be a critical factor to be considered. In this section, we investigate the differences in the speeds of convergence of the GA and ASGA methods.Firstly, the evolution of the best individuals’ fitness with generations is studied for the dataset 1. In this case, the AS_Factor is set to 3 and the optimization for both methods is repeated 4 times and the results is reported as Fig. 5. From these four figures, we can see that the fitnesses of ASGA are becoming stable faster than those of GA in most cases. This shows that ASGA converges to an optimal solution quicker than GA, in most situations.Then, we analyze the influence of parameter AS_Factor on convergence speed. For dataset 1, we change the value of AS-factor and count the iteration times when the average fitness of populations have reached the premise threshold (for example, 90% of the final optimal fitness). The optimization for every value is repeated five times. Experiments results are illustrated in Fig. 6. From this figure, it is seen that the population of the ASGA method converge more rapidly than those of the GA method in most cases when the parameter AS_Factor get smaller. This means that the GA method convergence process is accelerated under the guidance of the prior information and the more AS method influences the searching the faster population converges. The experiments for dataset 2 and 3 are also given in Figs. 7 and 8. The same conclusion can be drawn. In general, from the experiments conducted, we suggest to take a value of the AS_Factor in the range of [3,5], to balance the advantages of accelerated convergence speed and improved prediction accuracy.The robustness of the algorithm to the different types of data is also important to consider. In this section, two artificial datasets of literature, constructed from a target function contaminated with different levels of noise, are here considered to illustrate the robustness of our ASGA method. The detailed description of the data used is given as follows.Each artificial dataset is composed of 40 training data (si, yi)(i=1, …, 40), wheresiis sampled uniformly from the input space [−10, 10], and yiis generated according to the given statistical model, i.e. yi=f(si)+ni. The specific target functions f(s) used in this section are a linear function and a polynomial function. The y-values of the training data are corrupted by Gaussian additive noise niwith zero mean and standard deviation σ. The inputs of 200 test data are sampled randomly in [−10, 10] and the outputs are the corresponding function values f(s). The simulation is repeated 20 times and experiments results are reported in Table 4.As seen from the results of Table 4 with low noise levels, the proposed ASGA method and the GA method are comparable but for high noise levels the performance of the GA method degrades, whereas the ASGA keeps a comparatively more robust prediction performance than GA.In the preceding part of the text, the superiority of the ASGA method to the traditional GA method in selecting SVR parameters is detailedly demonstrated in the aspects of prediction accuracy, convergence speed and robustness. In this section, more SVR methods with different parameter tuning manners are considered. Two fuzzy based optimization methods, the FPSO and FGAPSO, are introduced to find the optimal SVR parameters and address the reliability prediction tasks, and their performances in different cases are compared with the proposed ASGA–SVR method. The models and parameters setting of FPSO and FGAPSO are in accordance with the literatures [33,41] and the AS_factor of ASGA–SVR is set to 3. Here, for the reliability datasets 1, 2 and 3, the training sample sizes are also 30, 60 and 90, respectively, and the test samples are also the rest data. The average prediction accuracy (measured by RMSE) of 20 experiments and the total running time (with experiment platform of Microsoft Windows 7, Matlab 7.9.0, Intel 2.4GHz) of 500 experiments are reported in Tables 5 and 6, respectively.For prediction accuracy, the three methods have almost equivalent performance and the FGAPSO performs a little better because it combines more optimization methods. But for running time, the proposed ASGA–SVR consumes much less computational resources than other two methods, due to the acceleration effect of AS solution.Beside the SVR based methods, two other time series prediction approaches, the EANFIS method and the EFNN method, are also applied to the reliability prediction in this section. The models and parameters setting of EANFIS and EFNN are in accordance with the literatures [21] and [41] and the AS_factor of ASGA–SVR is set to 3. Table 7gives the prediction accuracy of EANFIS and EFNN, as well as ASGA–SVR, for different datasets. Besides the different dataset, the training sample size is also altering to investigate the performance of these approaches when the size of training sample is small. The length of training data is listed in Table 7 and the test data are the next five samples after the last training sample. For each case, the experiment is repeated for 20 times and the average prediction RMSE is listed.It is worth noting that, as we have pointed out in Table 1, the dataset 1 and dataset 2 are of linearity and piecewise linearity while dataset 3 are unlinear time series. It is obvious to see that all these methods have competent average performance when historical data are sufficient for dataset 1 and dataset 2. However, as the training data decreases, i.e., the situation of small population of samples, the proposed ASGA–SVR method shows its obvious superiority than EANFIS and EFNN because of the ability of SVR in handling the small size of samples. For dataset 3, however, the EANFIS with linear inference model given in Ref. [20] performs much worse than ASGA–SVR even if the training samples are sufficient. This experiment verifies the sensitivity of EANFIS to the proper model selection, while the SVR methods rely little on the premise selection for the evolutionary optimization.From the three experiments above, we can conclude that SVR based methods keep satisfactory prediction ability in small sample case and rely little on the premise initialization at the cost of heavy computational burden. However, our ASGA–SVR possesses the advantage of SVR method and makes full use of the prior knowledge about SVR parameters at the same time to accelerate the execution and enhance the accuracy, which makes the proposed ASGA–SVR a relatively accurate, fast and stable approach.

@&#CONCLUSIONS@&#
