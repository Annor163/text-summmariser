@&#MAIN-TITLE@&#
Acceleration of boundary element method by explicit vectorization

@&#HIGHLIGHTS@&#
The in-core vectorization of the Galerkin BEM using the Vc library is proposed.Fully numerical and semi-analytical integration schemes are discussed.Numerical experiments show significant speedup of the BEM computation.

@&#KEYPHRASES@&#
Boundary element method,Sound scattering,Helmholtz equation,Vectorization,SIMD,OpenMP parallelization,

@&#ABSTRACT@&#
Although parallelization of computationally intensive algorithms has become a standard with the scientific community, the possibility of in-core vectorization is often overlooked. With the development of modern HPC architectures, however, neglecting such programming techniques may lead to inefficient code hardly utilizing the theoretical performance of nowadays CPUs. The presented paper reports on explicit vectorization for quadratures stemming from the Galerkin formulation of boundary integral equations in 3D. To deal with the singular integral kernels, two common approaches including the semi-analytic and fully numerical schemes are used. We exploit modern SIMD (Single Instruction Multiple Data) instruction sets to speed up the assembly of system matrices based on both of these regularization techniques. The efficiency of the code is further increased by standard shared-memory parallelization techniques and is demonstrated on a set of numerical experiments.

@&#INTRODUCTION@&#
The boundary element method (BEM) is a counterpart to the finite element method (FEM) suitable for the solution of partial differential equations which can be formulated in the form of boundary integral equations. Since BEM reduces the given problem to the boundary of a computational domain, it is especially suitable for problems stated in unbounded domains, such as acoustic or electromagnetic wave scattering, or shape optimization.System matrices arising from the classical BEM are dense and the method has quadratic computational and memory complexity with respect to the number of surface elements. Moreover, special quadrature methods are needed due to the singularities in the kernels of the boundary integrals [1,2], which further contribute to computational demands of the method. Several fast BEM approaches can be employed to reduce computational and memory requirements to almost linear. The common methods are based on the decomposition of the surface mesh into clusters and subsequent low rank approximation of matrix blocks corresponding to admissible pairs of clusters. Nonadmissible blocks are assembled in the standard way as full rank matrices. The fast multipole method (FMM) is based on the approximation of the system matrices by the multipole series expansion [3–5], whereas the adaptive cross approximation (ACA) assembles the low rank approximation from an algebraic point of view [6,2].Regardless the above mentioned approximation techniques, there is still a need for an efficient assembly of nonadmissible matrix blocks or a certain number of rows and columns of admissible blocks in the case of ACA. Since these blocks are usually too small to be distributed among computational nodes by MPI, an OpenMP parallelization of the assembly is an obvious choice. In this paper, we discuss further acceleration of the process by means of vectorization of the quadrature over pairs of surface elements.With new SIMD instruction sets available in modern processors the usage of vectorization becomes more important in scientific computation. Neglecting it may lead to inefficient code not capable of reaching the theoretical performance of current CPUs. The SSE instruction set introduced by Intel in 1999 provided eight 128-bit registers and enabled concurrent operations on four 32-bit single-precision floating point numbers. Its successors, SSE2–SSE4, extended this capability to support SIMD operations on two 64-bit double-precision floating point operands while incrementally adding more instructions. The AVX instruction set supported by Intel processors since 2011 extends the registers length from 128bits to 256bits and introduces a three-operand SIMD instruction format. Its capabilities are further extended by AVX2. The AVX-512 should provide registers with 512-bit length allowing for concurrent operation on eight 64-bit double precision numbers. Its support is announced for Intel’s Knights Landing processor available in 2015 and for Intel’s Skylake microprocessor architecture [7].To use the vector instructions the existing scalar code usually has to be modified. While the automatic loop vectorization provided by the compiler is not capable of vectorizing more complex loops often occurring in scientific codes, exploiting the supported intrinsic functions may lead to a confusing and hardly maintainable code. One of the possibilities avoiding these issues is to use a higher level library, such as VML from Intel’s Math Kernel Library [8], VDT [9], or the Vc library [10], which is the main focus of this paper. The library provides a high level wrapper on SIMD intrinsics and enables explicit vectorization of C++ code. It is portable among various compilers and SIMD instruction sets and enables easy vectorization without the need for a major redesign of the existing object oriented C++ code.The topic of the vectorization of the BEM computation has been presented in several publications. In [11] an example of automatic loop vectorization of Fortran boundary element computation is provided. The original routines are manually altered using techniques such as loop unrolling and loop reordering in order to enable the compiler to employ SIMD instructions. Although a reasonable speedup with respect to the non-vectorized version is obtained, modifications lead to a significantly more complex code. The interested reader may also consult [12] for a comprehensive presentation of BEM quadrature vectorization. The author provides a general overview of the SIMD parallelism, compares two approaches to handling data during the computation (inter- and intra-register operations), and presents results of numerical experiments with a code vectorized using intrinsic functions. However, the work does not discuss the treatment of singularities in the related surface integrals, which is one of the crucial tasks of BEM computations.The structure of the paper is as follows. In the next section we provide a model problem on which we demonstrate the boundary element workflow. In Section 3, a short overview of our BEM library is provided, Section 4 discusses the vectorization of the computationally most demanding parts of the code. Finally, we provide results of numerical experiments and conclude.In this section we present the model problem under consideration, derive the corresponding boundary integral equations and their Galerkin discretization.We consider a time-harmonic scattering problem with a sound-soft obstacle modelled by a bounded Lipschitz domainΩ⊂R3. The incident wave is of the formui≔eiκ〈x,d〉with the imaginary uniti, the wave numberκ∈R+, and a unit direction vectord. The waveusscattered from Ω satisfies the exterior Dirichlet boundary value problem for the Helmholtz equation [13,2](1)Δus+κ2us=0inΩext≔R3⧹Ω‾,us=gon∂Ω,∇us(x),x‖x‖-iκus(x)=O1‖x‖2for‖x‖→∞.The total wave field around the obstacle is given byut≔ui+us. The Dirichlet condition in (1) is given by the negative of the incident waveg≔-ui. Thus, the total wave fieldutvanishes on∂Ω. The Sommerfeld radiation condition ensures uniqueness of the solutionus∈HΔ,loc1(Ωext), which is equivalent tous|Ω∼∈HΔ1(Ω∼)≔u∈L2(Ω∼):∂u∂xi∈L2(Ω∼)∧Δu∈L2(Ω∼),for all bounded domainsΩ∼⊂Ωextwith the derivatives understood in the distributional sense.To solve the boundary value problem (1) we use the direct approach via the representation formula [13,14,2,1,15](2)us=Wκγ0,extus-V∼κγ1,extusinΩext,with the trace operatorsγ0,ext:HΔ,loc1(Ωext)→H1/2(∂Ω),γ0,extu=u|∂Ωforu∈C∞(Ωext‾),γ1,ext:HΔ,loc1(Ωext)→H-1/2(∂Ω),γ1,extu=∂u∂nforu∈C∞(Ωext‾),ndenoting the unit exterior normal vector to Ω, the single-layer and double-layer potentials(3)V∼κ:H-1/2(∂Ω)→HΔ,loc1(Ωext),(V∼κq)(x)≔∫∂Ωvκ(x,y)q(y)dsy,(4)Wκ:H1/2(∂Ω)→HΔ,loc1(Ωext),(Wκt)(x)≔∫∂Ω∂vκ∂ny(x,y)t(y)dsy,and the fundamental solution of the Helmholtz equation in 3Dvκ(x,y)≔14πeiκ‖x-y‖‖x-y‖.To evaluate the solutionusinx∈Ωextusing the formula (2) it is necessary to complete the Cauchy dataγ0,extus,γ1,extus. Since the Dirichlet traceγ0,extusis given byg=-γ0,extui, only the missing Neumann traceγ1,extusneeds to be computed. Applying the Dirichlet trace operatorγ0,extto the representation formula (2) and using the well-known properties of the potential operators (3) and (4) (see, e.g., [14,2,15,1]) we obtain the boundary integral equation(5)(Vκγ1,extus)(x)=-12g(x)+(Kκg)(x)forx∈∂Ω,with the single-layer and double-layer boundary integral operators(6)Vκ:H-1/2(∂Ω)→H1/2(∂Ω),(Vκq)(x)≔∫∂Ωvκ(x,y)q(y)dsy,(7)Kκ:H1/2(∂Ω)→H1/2(∂Ω),(Kκt)(x)≔∫∂Ω∂vκ∂ny(x,y)t(y)dsy.Note that contrary to the potentials (3) and (4), the functionsVκq,Kκtare only defined on∂Ω.The Galerkin formulation equivalent to (5) reads(8)〈Vκγ1,extus,s〉∂Ω=-12I+Kκg,s∂Ωforalls∈H-1/2(∂Ω).To discretize the Galerkin formulation (8) we triangulate the surface∂Ωinto E flat shape-regular open trianglesτi, i.e.,∂Ω≈⋃n=1Eτi‾.To approximate the Cauchy data we use piecewise linear ansatz for the Dirichlet dataγ0,extusand piecewise constant ansatz for the Neumann dataγ1,extusγ0,extus=g≈t≔∑i=1Ntjφj,γ1,extus≈s≔∑i=1Esiψi,where N denotes the total number of mesh nodes. Using piecewise constant testing functionsψℓthis results in the discrete system of linear equationsVκ,hs=-12Mh+Kκ,ht,with the matricesVκ,h[ℓ,j]≔14π∫τℓ∫τjeiκ‖x-y‖‖x-y‖dsydsx,Mh[ℓ,i]≔∫τℓφi(x)dsx,Kκ,h[ℓ,i]≔14π∫τℓ∫∂Ωφi(y)eiκ‖x-y‖‖x-y‖3(1-iκ‖x-y‖)〈x-y,n(y)〉dsydsx.To set up the matrices one has to deal with integration of singular kernels. There are two commonly used methods, one involving analytic integration of the inner integral and numerical quadrature of the outer one [2], or a fully numerical scheme described in [1] and references therein. Since the treatment of all operators is very similar, in the two following sections we only describe the set up of the single-layer operator matrixVκ,husing both approaches. In Section 5, however, we provide results of our experiments for the set up of both boundary element matricesVκ,h,Kκ,h. In addition, we also provide assembly times of the hypersingular operator matrixDκ,h[i,j]≔14π∫∂Ω∫∂Ωeiκ‖x-y‖‖x-y‖〈curl∂Ωφj(y),curl∂Ωφi(x)〉dsydsx-κ24π∫∂Ω∫∂Ωeiκ‖x-y‖‖x-y‖φj(y)φi(x)〈n(x),n(y)〉dsydsx,which would be involved, e.g., in a sound-hard scattering problem with the Neumann boundary condition∂us∂n=-∂ui∂n[2,1]. The semi-analytic approach for this matrix is described in [16].Let us first address the semi-analytic approach. To find the analytic formula the singular inner integral is split up as∫τj14πeiκ‖x-y‖‖x-y‖dsy=∫τj14πeiκ‖x-y‖-1‖x-y‖dsy+∫τj14π1‖x-y‖dsy.Let us denote(9)f(x)≔∫τj14πeiκ‖x-y‖-1‖x-y‖dsy,g(x)≔∫τj14π1‖x-y‖dsy.For the first integrand we havelimx→yeiκ‖x-y‖-1‖x-y‖=lima→0eiκa-1a=l′Hlima→0iκeiκa=iκ,and thus the integralf(x)can be computed numerically without further regularization asf(x)≈∑n=1kωn14πeiκ‖x-yn‖-1‖x-yn‖,with suitable quadrature weightsωnand pointsyn∈τj. For the quadrature overτjwe use the 7-point Gauss scheme described in [2]. The explicit analytic formula for g in local coordinates related toτjcan be found in Chapter C.2 in [2]. For the outer integration off+goverτℓwe use the Gauss integration(10)Vκ,h[ℓ,j]≈∑m=1kωm(f(xm)+g(xm)).Since the fully numerical approach is quite involved, we only summarize the basic concept and refer the interested reader to Chapter 5 of [1] and the references therein.The main aim is to regularize the integrand by means of integral substitutions. This leads to a formal formulation given by four one-dimensional integrals∫01∫01∫01∫01vκ(F(z1,z2,z3,z4))S(z1,z2,z3,z4)dz1dz2dz3dz4,with the substitutionFn∘⋯∘F1≕F:[0,1]4→τi×τj,F(z1,z2,z3,z4)=(x,y),dsydsx=S(z1,z2,z3,z4)dz1dz2dz3dz4,which is different forτi,τjbeing identical, sharing exactly one edge, sharing exactly one vertex, or having a positive distance. The functionSstems from the parametrization via a reference triangle and a series of transformations given byF. Since the integrandh(z1,z2,z3,z4)≔vκ(F(z1,z2,z3,z4))S(z1,z2,z3,z4),is analytic [1], a tensor-product Gaussian quadrature scheme(11)∑m=1k∑n=1k∑o=1k∑p=1kωmωnωoωph(zm,zn,zo,zp),with the weightsω•and sampling pointsz•can be used.The solver for the wave scattering problem based on BEM is implemented in the BEM4I library [17]. The library is written using C++ in an object-oriented way. It utilizes templates to support various indexing and scalar types. OpenMP is used for the parallelization in shared memory and some parts of the code are parallelized in distributed memory by MPI.The structure of the solver is depicted in Figs. 1 and 2. Three main types of classes are responsible for the assembly of the system matrices.1.BESpace The class and its descendants keep the information needed for the boundary element approximation of the continuous function spaces, such as the order of spatial basis and testing functions or data necessary for the FMM or ACA acceleration. The object of the computational mesh is stored in this class.BEBilinearForm The main purpose of the descendants of this class is to assemble appropriate system matrices (either full or sparsified by ACA or FMM). The assembly is performed elementwise using BEIntegrator classes. Local contributions from individual pairs of elements are combined to form a global system matrix. The assembly is parallelized using OpenMP at this level.BEIntegrator The group of BEIntegrator classes is responsible for the assembly of element system matrices and evaluation of the representation formula (2). The most computationally demanding part of the code is the Gaussian quadrature over the pairs of elements. Its vectorization by the Vc library will be described in the next section.Besides these main classes the library also implements various supportive classes representing surface meshes, full and sparse matrices, matrices approximated by FMM and ACA, direct and iterative solvers, etc. To solve the problem, the user can either directly manipulate with these classes or use the interface provided in the ScatteringProblem class.In the following section we demonstrate the applicability of the Vc library to the vectorization of the quadrature occurring in the boundary element matrices computation. The BEM4I library implements both the semi-analytic [2,16] and numerical [1] methods, therefore the vectorized code is provided for both approaches.The Vc library contains counterparts of most mathematical methods of the C++ Standard Library which makes it relatively easy to convert the quadrature code from a scalar to a vectorized version. The development of the vectorized code is faster since the structures of both non-vectorized and vectorized versions are similar, the object-oriented design is preserved and the maintenance of the code is easier. In some cases a more complex redesign of the code may be necessary to further increase its efficiency.A common header file interface using standard C++ data structures is provided for both vectorized and non-vectorized versions. Therefore, the user does not interact directly with the Vc library and the external code remains the same for both scalar and vector computations. The vectorization is activated using the #define INT_VC preprocessor directive in the settings file.For the sake of clarity the template parameters are omitted from the following source code samples. The int and double types are used for indexing and the default float type, respectively.The main idea of the vectorization of the semi-analytic matrix assembly is to parallelly evaluate the outer quadrature (10), which leads to concurrent evaluation of the functions f and g from (9) in multiple quadrature points. A common interface for both vectorized and scalar versions of the single-layer operator local matrix assembly is provided by the method computeElemMatrix1LayerP0P0.void computeElemMatrix1LayerP0P0(int outerElem,int innerElem,FullMatrix& matrix);The first part of the method consists mainly of data allocation, establishing the local element coordinate system, and computation of Gaussian quadrature nodes. Contrary to the scalar version, where this is done using the classical C++ data structures, the vectorized version stores the coordinates of quadrature nodes as well as quadrature weights in instances of the Vc::Memory<Vc::Vector> class. The class ensures correct memory alignment depending on the underlying data type and allows efficient scalar and vector access to data. The Vc::Vector class abstracts the SIMD registers and their instructions into types familiar to C++ developers. The capacity of the vector depends on the instruction set used. With AVX it can store and concurrently process four double precision and eight single precision operands.In the next part of the method the Gaussian quadrature over the outer element is performed. The original scalar loop over quadrature points.double entry=0.0;for (int i=0; i<number_of_quadrature_points; i++){double value=collocation1LayerP0(quadrature_point[i]);entry += quadrature_weights[i]∗value;}is replaced by the vectorized loop. The simplified version of the vectorized loop follows.Vc::Memory<Vc::Vector<double > > quadrature_weights;Vc::Memory<Vc::Vector<double > > x0;// first coordinatesVc::Memory<Vc::Vector<double > > x1;// second coordinatesVc::Memory<Vc::Vector<double > > x2;// third coordinates//…(fill the instances of Vc::Memory class with data)Vc::Vector<double>entry=Vc::Zero;for (int i=0; i<quadrature_weights.vectorsCount(); i++){Vc::Vector<double>value=collocation1LayerP0(x0.vector(i), x1.vector(i), x2.vector(i));entry += quadrature_weights.vector(i)∗value;}double totalEntry=entry.sum();The computationally most demanding part of the code is the evaluation of the function collocation1LayerP0 which returns the values of f and g from (10). The method involves evaluation of several transcendental functions, such as logarithms, trigonometric, or inverse trigonometric functions, as well as multiple conditional expressions. Its automatic vectorization is beyond the abilities of current compilers and its proper vectorization using the Vc library is crucial for the efficiency of the code. A snippet of the original method is provided in the following listing.double tmp1=s - x[0];double tmp2=alpha∗s - x[1];if (std::abs(tmp1) > eps){if (tmp2<0.0)f += tmp1∗std::log((x[2]∗x[2] +tmp1∗tmp1)/ (a - tmp2));elsef += tmp1∗std::log(tmp2+a);}Here, x is an array storing three spatial coordinates of a collocation point. Note the nested conditional expression, which is necessary to deal with possible singularities in the integral and complicates the evaluation of the variable f.The vectorized version processes multiple integration points concurrently. Their coordinates are separated into three Vc::Vector objects x0, x1, x2 containing first, second, and third components, respectively (see Fig. 3). Other variables are substituted with their vector counterparts as well. The if statement is replaced with conditional write based on a comparison of two or more vectors as demonstrated in Fig. 4and the listing below. The results of the right-hand side operations are only stored on those positions of the logArg vector, for which the given condition is satisfied.Vc::Vector<double>tmp1=s - x0;Vc::Vector<double>tmp2=alpha∗s - x1;Vc::Vector<double>logArg(Vc::Zero);logArg(Vc::abs(tmp1) > eps && tmp2<Vc::Zero) =(x2∗x2+tmp1∗tmp1) / (a - tmp2);logArg(Vc::abs(tmp1) > eps && tmp2 >= Vc::Zero) =tmp2+a;f(Vc::abs(tmp1) > eps) += tmp1∗Vc::log(logArg);The rest of the method collocation1LayerP0 is vectorized in the same manner. The method returns the Vc::Vector object which is further processed by the Gaussian quadrature mentioned above. The resulting code is structurally very similar to the original one, while the performance gain is significant.The scalar version of the fully numerical scheme consists of four for loops corresponding to the sums in (11). During the integration a proper technique has to be chosen based on the mutual position of elements. For the sake of simplicity we only provide a listing for the case of identical elements.for (int m=0; m<qSize1; m++)for (int n=0; n<qSize2; n++)for (int o=0; o<qSize3; o++)for (int p=0; p<qSize4; p++)switch (type) {case identicalElements:for (int simplex=0; simplex<6; simplex++) {getQuadratureNodes(m, n, o, p, simplex, type, x, y, jacobian);getQuadratureWeights(m, n, o, p, w);entry+=evalSingleLayerKernel(x, y)∗w[0]∗w[1]∗w[2]∗w[3]∗jacobian;}break;…}In the vectorized version the loops are unrolled and the appropriate function evaluating h in several combinations of quadrature points is provided. Moreover, the function getQuadratureNodes now returns coordinates of multiple quadrature pointsx,ysplit into three Vc::Vector objects.int qSize=qSize1∗qSize2∗qSize3∗qSize4;for (int i=0; i<qSize; i++)switch (type) {case identicalPanels:for (int simplex=0; simplex<6; simplex++) {getQuadratureNodes(i, simplex, type, x0, x1, x2, y0, y1, y2, jacobian);getQuadratureWeights(i, w0, w1, w2, w3);entry+=evalSingleLayerKernel(x0, x1, x2, y0, y1, y2)∗w0∗w1∗w2∗w3∗jacobian;}break;…}After the computation of the Cauchy data the representation formula (2) can be used to evaluate the scattered waveusfrom (1) in an arbitrary pointx∈Ωext. Since the formula is usually evaluated in a large number of nodes and each of these computations can be done independently, it is well suited for parallelization in both shared and distributed memory. In addition, the vectorized version of a function evaluating (2) in multiple nodes concurrently is provided. The method takes arrays of 3D coordinates as arguments and stores them in the Vc::Memory objects. Individual Vc::Vector objects are loaded from Vc::Memory and a numerical quadrature for (3) and (4) is performed for multiple values ofx. The potentials are evaluated using the analytic formulae similarly as in Section 4.1.The following numerical experiments were carried out using one node of the Anselm cluster located at the IT4Innovations National Supercomputing Centre, Ostrava, Czech Republic. The node is equipped with two 8-core Intel Xeon E-2665 2.4GHz processors and 64GB of RAM. The processor supports the SSE4.2 and AVX instruction set extensions. The tests were performed using the GCC 4.9.0 compiler. Since the 256-bit registers are only supported by a subset of the first generation AVX instructions [18], the performance gain of AVX compared to SSE is not as significant as desired in most cases. For this reason we concentrate on the results obtained with the SSE extension.In the first set of numerical experiments we compare the assembly times of full system matricesVκ,h,Kκ,h, andDκ,husing the scalar and vectorized version of the library. Moreover, we provide results of the scalability test of the OpenMP code in combination with the Vc vectorization. The used computational domain Ω represents a unit ball with boundary discretized into 11,520 surface elements.Let us begin with the semi-analytic approach. The comparison of the computational times of scalar and vectorized versions of the code is given in Fig. 5. In double precision arithmetic the speedup reaches 1.81 in the case of the hypersingular operator matrix and 1.58 and 1.23 in the cases of the single- and double-layer operator matrices, respectively. For single precision arithmetic we obtain the speedups of 3.98, 3.65, and 2.45 for the assembly of matricesDκ,h,Vκ,h,Kκ,h, respectively.To further accelerate the assembly of the system matrices, the BEM4I library provides its shared memory parallelization by OpenMP. The combined speedups of the OpenMP parallelization and vectorization relative to the non-vectorized sequential version are depicted in Figs. 6 and 7. In the case of double precision computations we reach the speedup of 27.44 on 16 cores, thus reducing the computational time for the assembly ofDκ,hfrom 6580s on a single core without vectorization to 250s on 16 cores with vectorization. The time is further reduced to 120s when using single precision arithmetic. Similar results are obtained for the matricesVκ,h,Kκ,h.The results for the fully numerical approach are depicted in Figs. 8–11. For the experiments shown in Figs. 8–10 three quadrature points in each dimension were used (34=81quadrature points in total), the AVX experiments in Fig. 11 were performed with44=256quadrature points. Although the computation is not accelerated as significantly as in the case of the semi-analytic approach, the gains of vectorization are still apparent especially when using single precision arithmetic. Moreover, contrary to the semi-analytic approach, the AVX instruction set further accelerates the computation especially when requiring a higher precision quadrature, which is usually necessary for more complicated geometries (see Fig. 11).In Fig. 12we provide computational times for the evaluation of the representation formula (2). The vectorization approach described above leads to a significant speedup, namely 1.62 in the case of double precision arithmetic and 4.00 in the case of single precision arithmetic. The formula was evaluated in 5,000 nodes.Finally, the benefits of vectorization for fast BEM are demonstrated in Fig. 13. The graphs provide the computational times necessary for the assembly of nonadmissible parts of system matricesVκ,h,Kκ,h. The assembly of these parts forms a major portion of the FMM matrices computation and takes an important amount of time when assembling ACA matrices. We observe significant speedup especially in single precision arithmetic. The tests were performed using a surface mesh consisting of 103,680 elements.

@&#CONCLUSIONS@&#
