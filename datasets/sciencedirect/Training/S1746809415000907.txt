@&#MAIN-TITLE@&#
Feature selection method based on mutual information and class separability for dimension reduction in multidimensional time series for clinical data

@&#HIGHLIGHTS@&#
We improve a MTS feature selection method based on MI and class separability.We perform experiments on EEG and NSCLC clinical datasets respectively.The improved method is compared with CLeVer, Corona and AGV.Experimental results show the effectiveness of our method for dimension reduction.

@&#KEYPHRASES@&#
Multidimensional time series,Dimension reduction,Feature selection,Mutual information,Class separability,

@&#ABSTRACT@&#
In clinical medicine, multidimensional time series data can be used to find the rules of disease progress by data mining technology, such as classification and prediction. However, in multidimensional time series data mining problems, the excessive data dimension causes the inaccuracy of probability density distribution to increase the computational complexity. Besides, information redundancy and irrelevant features may lead to high computational complexity and over-fitting problems. The combination of these two factors can reduce the classification performance. To reduce computational complexity and to eliminate information redundancies and irrelevant features, we improved upon a multidimensional time series feature selection method to achieve dimension reduction. The improved method selects features through the combination of the Kozachenko–Leonenko (K–L) information entropy estimation method for feature extraction based on mutual information and the feature selection algorithm based on class separability. We performed experiments on the Electroencephalogram (EEG) dataset for verification and the non-small cell lung cancer (NSCLC) clinical dataset for application. The results show that with the comparison of CLeVer, Corona and AGV, respectively, the improved method can effectively reduce the dimensions of multidimensional time series for clinical data.

@&#INTRODUCTION@&#
Time-series analysis is widely used in many application fields, including medical data, financial data, moving-object tracking, human-computer interaction interface [1,2], etc. Data mining for time series has very important value, such as research on the classification, clustering or prediction of data, which can assist in finding the potential rules of time series data and provide support. Currently, most researches focus on univariate time series processing. However, with the development of data-collection technology, more and more multidimensional time series data become available, which contain a considerable amount of potentially valuable information. For example, diabetes clinical data, as a kind of time series data, contain abundant information including food intake, drugs intake and daily activities. The EEG data11http://archive.ics.uci.edu/ml/datasets/EEG+Database.which contain plentiful information on brain waves reflect correlations with certain genetic predisposition and disease. In Tanawongsuwan and Bobick [3], 22 markers are spread over the human body to measure the movements of body parts while walking. In medicine, EEG data from 64 electrodes placed on the scalp are monitored to examine the correlation of genetic predisposition to alcoholism [4]. Therefore, in recent years, multidimensional time series classification, dimension reduction and similarity search technology have become common concerns for researchers in the field of data mining [5–7].A time series is a series of observations,(1)xi(t);i=1,…,d;t=1,…,nmade sequentially through time where i indexes the measurements made at each time point t. It is called a univariate time series when d is equal to 1 and a multidimensional time series (MTS) when d is equal to or greater than 2. Due to the mass production of MTS data and the growing demand for classification in various fields, MTS classification techniques have been applied in many fields, such as the classification of RNA in bioinformatics, handwriting recognition and electrocardiogram (ECG) pattern matching. As MTS data are typical high-dimensional data [8], many features are either irrelevant or redundant. Moreover, dimension disasters, which are caused by excessive dimensions, exist in multidimensional feature space. Therefore, how to effectively select useful features for classification from the raw MTS data has become a current research hotspot with a high degree of difficulty.Feature extraction and feature selection are the main methods of dimension reduction [9]. Not only can they reduce classification errors, but can also improve classification efficiency. Currently, feature selection methods are used widely in MTS including CLeVer [10] and AGV [11] based on PCA and Corona [12] based on a correlation coefficient method. However, they can only identify linear relationships among dimensions, and their calculations are more suited to dealing with equal length samples of MTS. However, unequal length data are indeed the norm in clinical follow-up because patients may die or otherwise be lost from the dataset. Mutual information (MI) is an important concept in information theory. MI can be applied to nonlinear transformation and extraction of high-order statistics. Therefore, we consider using MI for feature extraction to transform the different lengths of samples to equal length. Meanwhile, by the nonlinear relationship in multidimensional feature space, we can effectively reduce dimensions through feature selection. However, the probability density estimation method has a great influence on MI computation which implies whether the method can effectively and efficiently express the typical features to promote the accuracy of feature selection. Thus, it is significant to choose an applicable probability density estimation method for MI feature extraction in MTS. In addition, the feature subset evaluation criterion is the key issue in feature selection and its quality directly impacts the final result. The class separability criterion is one of the important evaluation criteria. Between-class distance criterion is one of the commonly used methods. We get better class separability by minimizing within-class distance and maximizing between-class distance simultaneously. The purpose of feature selection is to choose the feature subsets with larger class separability. However, since the redundant variables have an obvious effect on the result of classification, while the between-class distance criterion cannot eliminate the redundant variables, we consider that introduce a criterion with redundancy variable to eliminate redundancies and irrelevant features. We then introduce the improved method which can effectively choose the optimal features and reduce dimensions.This paper aims to break the limitation that correlation matrices in traditional MTS feature selection method can only measure the linear relationships between variables. We improve the feature selection method based on mutual information and class separability. We first compute the MI value by a probability density estimation method to extract the linear and nonlinear relationship between variables through MI matrices. By considering the existence of redundancies we next introduce the feature selection algorithm based on class separability to eliminate redundancies and make high correlation between the chosen feature subsets and the target class. We then use the improved method for dimension reduction processing on MTS as is shown in Fig. 1. Finally, we verify that if the improved method can effectively reduce dimensions through the contrast experiments based on classification accuracy with an SVM classifier.The remainder of this paper is organized as follows. Section 2 introduces the feature extraction method based on MI. Section 3 introduces the feature selection algorithm based on class separability. The experiment and result with the improved method is followed in Section 4, which is followed by conclusion in Section 5.This section introduces the MI feature extraction method, which involves some basic concepts of entropy and MI as are shown in Refs. [13–15].In general, a MTS can be expressed as a d×n matrix [xi,t]d×n. Each matrix expresses one sample. Assume that these research data include several samples and that two of the samples are [xi,t]d×n1and [xi,t]d×n2. Generally speaking, each variable of within-sample sampling time has the same length. However, the length of two samples of between-sample sampling time t as n1 and n2 is not always the same. Therefore, each MTS sample is expressed by a d×tjmatrix [xi,t]d×tj.(2)x11x12...x1,tj............xi1......xitjxd1xd2...xdtj,i=1,2…,d;t=1,2,…,tjwhere xi,tdenotes the sampling value of the variable xiwith the ith dimension at time point t. Mjsubstitutes for the jth sample matrix [xi,t]d×tjas is shown in Fig. 2. tjDenotes the sampling time length of the jth sample. Xishows the index sequence of the ith dimension. Because each sequence Xihas different degrees of importance to classification, Xiis expressed in different colors and that a deeper color means a higher degree of importance. However, under the initial condition, for degree of importance for each sequence is unknown, the colors are shown in random depth. Fig. 3shows a MTS dataset with n samples and each sample is a matrix with dimension d and sampling time length tj. For any given sample, the degree of importance of each sequence is initially unknown.By the definition of information entropy and MI, the probability density distribution of random variables must be approximately estimated before MI calculation. One kind of probability density estimation method based on nearest neighbor is introduced in [16], which has good effect used in [17,18] as well. The advantage of this method is that there is no need to estimate the probability density distribution function for any variables.Assume that X and Y are two random variables, whereX=xi,i=1…n,Y=yi,i=1…n,. In [19], the K–L nearest neighbor estimation entropy is defined as(3)HˆX=−ψk+ψn+logcd+dn∑i=1nlogεXi,kwhere k is the number of the nearest neighbor points; d is the dimension of data; cdis the unit-sphere volume of d;εXi,kis the distance between xiand the kth nearest neighbor point; and ψ is the double gamma function.Based on formula (3), to solve the regression problem, one kind of MI estimation methods is proposed by Kraskov in [16] as(4)IˆX;Y=ψn+ψk−1k−1n∑i=1nψτxi+ψτyiwhere τx(i) is the number of points on the distance which is no more thanεi,kbetween X and xi. τy(i) is similar to τx(i). Here,εi,k=maxεXi,k,εYi,k.This paper utilizes formula (4) on MI computation by sequentially computing Xiof each Mjwith all sequencesX1,X2,…Xd. Hence, each sequence is transformed into a MI vector Viand each matrix is transformed into a d×d MI matrix Ij. Thus, the feature extraction method may be described as follows:Input: a MTS dataset with size n and dimension d. (Assume d≥tj)Output: d×d MI matrix Ij.(5)Ij=IjX1,X1IjX1,X2⋯IjX1,XdIjX2,X1IjX2,X2⋯IjX2,Xd⋮⋮⋱⋮IjXd,X1IjXd,X2⋯IjXd,Xd,j=1,2,…,nThe ith variable of the jth sample is shown by MI vector as follows:Vji=IjXi,X1,IjXi,X2,…,IjXi,Xd,,i=1,2,…,d. Therefore, each variable in Ijcan be described with a vector Vji. The data change process for the MI feature extraction phase in the MTS dataset is shown in Fig. 4which transforms each sample into a square matrix sample with equal dimension.After the feature extraction based on MI processing, the combination form of feature space for the sample matrix has been converted into an MI matrix allowing these features to express the data characteristics more clearly and to achieve better effect in the feature selection method. First, we introduce the principle of class separability criterion in Section 3.1. Then, we reference a feature selection algorithm based on class separability to eliminate redundant variables and we convert the MI matrices into vectors as inputs of an SVM classifier in Section 3.2.The class separability criterion is often used as the basis in feature selection. There are several criteria that are commonly used, such as, the class separability criterion based on the geometric distance, the probability density function and the posterior probability. The latter two need to obtain the statistical characteristics of samples, while the between-class distance criterion is more commonly used in class separability based on the geometric distance. Although the definitions of the between-class criteria vary in the literature [20–22], they are essentially based on the concept of distance.Assume that there are c types. ωjis the jth class.xk(j)is the kth sample of ωj. Let njbe the sample number of ωj. n is the total sample number. mjis the sample mean vector of ωjand m is the mean vector of all samples.(6)mj=1nj∑k=1njxkj(7)m=1∑j=1cnj∑j=1c∑k=1njxkjwhereJwis the within-class total mean square distance.(8)Jw=∑j=1cPjJjwherePjj=1,2,…,cis the prior probability of ωjwhich can be estimated by njand n. Jjis the within-class mean square distance of ωj.(9)Jj=1n∑k=1njxkj−mjTxkj−mjJbis the between-class total mean square distance.(10)Jb=∑j=1cPjmj−mTmj−mIn order to make an effect that minimizes within-class distance and maximizes between-class distance simultaneously, the class separability criterion Jmis constituted intuitively [23] as follows:(11)Jm=JbJwIn view of the distance ratio between the between-class and within-class is the contribution for classification to the variable. Therefore, the idea of class separability is to choose the optimal feature subsets for classification. As MTS is a kind of multidimensional data, during the process of feature selection, the redundant variables have obvious effect upon the result of classification. However, the between-class criterion function cannot eliminate these redundant variables which reduce the classification accuracy. Thus, we introduce a function [24] with a redundancy evaluation variable Jfto promote the accuracy of feature selection:(12)Jf=1S∑i=1S∑j=1Cnjmj−mijmj−mijTwhereSindicates the variable number which has been chosen in the feature subsets S. mijshows the ith average of the jth sample in S. The bigger the Jfis, the smaller the between-class redundant variable is. Therefore, the criterion Jris as follows:(13)Jr=Jb+JfJwThe following process is referenced by the general concept of the feature selection algorithm in [24].Algorithm 1MTS feature selection algorithm based on class separability.Input: a MTS dataset (d×d MI matrix Ijwith j samples).Output: the optimal feature subset with K sequences.Step 1: Compute each Jbi and Jwi in Ij. Because of the results of both Jbi and Jwi are the product between a row and a column vector, their values are quantitative values. In this way, all variables can be sorted by the formula as follows:(14)Jmi=JbiJwi,i=1,2,…,NThe larger the Jmi is, the more important the ith variable is to the classification result.Step 2: Choose the largest variable of Jmi as the first element of S.Step 3: Consider the existence of redundancies between variables and introduce the redundancy evaluation variable Jfi to synthesize selection variables.(15)Jmi=Jbi+JfiJwi,i=1,2,…,NThe larger the Jri is, the more important the ith variable is to the classification result. The largest Jriis chosen as feature element.Step 4: IfSis K, then algorithm ends, else loop operation with Step 3.To find the highest attribute to the classification contribution rate, we first compute each dimension of MI matrices with the introduced class separability criterion. Next, we sort all attributes using the criterion and choose k sequences to reduce the dimensions of matrices. As is shown in Fig. 5, the dimensions with the deeper colors mean that the higher attributes to the classification contribution rate are in the front. Vsi is the ith sequence after sorting.In order to satisfy the input requirement of an SVM classifier, MTS matrices need to be transformed into feature vectors, which is called a process of vectorization. The specific algorithm of vectorization is as follows:Algorithm 2MTS vectorization algorithm based on MI.Input: MTS sample after feature selection.Output: MI vectorIv.Step 1: Compute the MI value between variables in MTS and get a MI matrix I;Step 2: Initialize a void vectorIv=;Step 3: For i=1:d;Step 4:Iv=IvIi,i+1:d;Step 5: End.Eventually we get n MI vectorsIvwith k feature subsets and each input vector has a classification labelI′vin Fig. 6.where Vsi is a vector with the length of d. After vectorization, the MI vector of the jth sample is Ivj.After feature extraction, we have completed the dimension selection for MTS feature matrices so far. Through using the chosen dimensions in feature matrices we create the feature vectors and finally getIvby vectorization as inputs to an SVM classifier. Overall, the original MTS dimension is further reduced by choosing the top k attributes.According to the description above, we improve a dimension reduction method for MTS termed as feature selection based on mutual information and class separability (FSMICS).In order to evaluate the effectiveness of FSMICS in terms of classification performance and overall processing time, we conducted a verification and an application experiment on EEG and NSCLC datasets, respectively. In addition, we compared the performance of FSMICS with those of the other three methods including CLeVer, Corona and AGV. Here, CLeVer and Corona utilize the transformation of correlation matrices for dimension reduction. AGV extracts the average and variance of each variable for dimension reduction using the method in Ref. [11].For all data, we performed dimension reduction with four feature selection methods, respectively, and set the same parameters of SVM for classification. Subsequently, we got the baseline classification accuracy and processing time of each method. However, to increase the precision of the experimental results, we performed the experiments on the same dataset with each method 10 times. After calculation we got the average classification accuracy and processing time for each one. Classification method can be used as a tool to test the effectiveness of feature selection method. There are several classification methods such as decision tree (DT), neural network (NN) and SVM, which have different benefits and limitations. The effectiveness of classification method depends largely on the characteristics of data. SVM is a popular classification tool, which originally presented by Vapnik and his co-workers. It is also capable of nonlinear classification and handling high-dimensional data well, thus applied in many fields such as bioinformatics, cancer diagnosis, image classification, text mining and feature selection [25,26]. Therefore, FSMICS is compared with CLeVer, Corona and AGV via SVM which is adopted with linear kernel. Here, SVM classification is completed with LIBSVM [27] by using MATLAB.This experiment utilizes EEG data as the research data of feature selection. The EEG data originate from a large study to examine EEG correlates of genetic predisposition to alcoholism. The EEG data contain measurements from 64 electrodes placed on the scalp which are sampled at the rate of 256Hz. There are two groups of subjects: alcoholic and control. We selected 200 samples from each group to perform the experiment. Each group has two datasets which are training and testing including 100 samples for each one.An SVM with linear kernel is adopted for the classifier to evaluate the classification performance of FSMICS. Thus, the parameter inertia factor c is set for 2. In order to guarantee the experimental precision, we performed 10 experiments with each of four methods and got the average of each one, respectively. We then got the comparison of performance in Fig. 7for the four methods on EEG data. The X axis shows the chosen number of feature subsets. The Y axis shows the classification accuracy of the SVM.As can be seen from Fig. 7, the classification accuracy of CLeVer has the fastest convergence rate with the increase of the chosen number of feature subsets. FSMICS and Corona are similar and get decent convergence rate. The slowest is with AGV. However, when the classification accuracy converges, the chosen number of feature subsets for FSMICS is minimum, and the average classification accuracy of FSMICS is larger than other three methods. Therefore, it is observed that the three methods, FSMICS, CLeVer and Corona have good stability after the convergence of classification accuracy. The poor stability for AGV method, it might be due to the characteristics of the EEG data and the design concept of AGV. AGV is a filter feature subset selection method based on the across group variance that considers group structure in the data. AGV is not originally design for MTS data, and it cannot perform feature selection directly using MTS data. The MTS items should be first transformed into feature matrices before feature selection [28]. We can verify whether or not this conclusion is applicable in our clinical data. In addition, as a verification experiment, we can find from the results that the classification performance of the improved method is better overall than the original feature selection algorithm [24].The research data in this section are the treatments and follow-up medical information from middle-late stage NSCLC patients in a certain hospital. The data collected from the medical history sheets include four parts: TCM (Traditional Chinese Medicine) clinical symptoms, TCM Syndrome, the physical and chemical examination of clinical significance and patients self-administered FACT-L score [29]. There are 68 medical indexes altogether for our experiment.We selected the features for the NSCLC dataset based on the improved feature selection method for MTS. There are n=205 samples in these data that 205 patients are followed up during 2–3 years. Each sample includes 68 variables Xias medical indexes in the clinical data. The average length of each sample tjis 10. The patients were divided into two major classes: Class 1 (Deceased) including the deceased patients, and Class 2 (Living) containing the alive ones. They were separated from their different situations of tumor progression and whether or not the patient died during the observation period in the data. Then, there are 94 patients in Class 1 and 111 patients in Class 2.We first utilized the KNN method [30] to fill the missing data and obtained a complete matrix Mj, j=1, …, n. Next, we computed the MI value between variables by the K–L estimation method to get a 68×68 MI matrix Ij, j=1, …, n. We then got the variable Vs1 of the greatest classification correlation by the class separability criterion. And we got the secondary correlation variable Vs2 and others by the introduced criterion. In the same way, we can get the sequences Vs1, Vs2, …, Vs68 according to the correlation. Finally, we chose the top k sequences as the result of feature selection and vectorized the feature subsets to get the vectorIv.In order to validate the classification accuracy, we introduce the corresponding confusion matrix as is shown in Table 1.According to the confusion matrix, the sample classification accuracy P1, P2 and the total sample classification accuracy P of each class are as follows:P1=P11/(P11+P12),P2=P22/(P21+P22)andP=(P11+P22)/∑i=1,j=12Pij. To avoid the deviation of experimental result, the data are divided into 10 groups. All samples are randomly divided into training and testing samples for 10-fold cross-validations (10-fold CVs).In this paper, according to the total sample classification accuracy as the performance evaluation index, we perform the experiment with SVM for the classification accuracy of samples whose variables are chosen from 1 to 68. The trend for the dataset classification accuracy with the number of feature subsets after feature selection is shown in Fig. 8. In order to guarantee experimental precision, these curves are plotted from the averages of 10 experiments using different training and testing datasets. When total classification accuracy of model converges, the average classification accuracy, classification efficiency and standard deviation of each class after 10-fold CVs are indicated in Table 2.As can be seen from Fig. 8, for one thing, the average classification accuracy of FSMICS is the maximum when its classification accuracy converges. CLeVer and Corona get the decent and similar average classification accuracy which is the minimum with AGV. For another thing, the convergence rate of FSMICS and CLeVer are close, but the initial classification accuracy of CLeVer is smaller than FSMICS. More information of classification such as stability and processing time cannot be precisely reflected from Fig. 8. Therefore, the classification performance of the four methods is further analyzed by combining the data in Table 2.In Table 2, the “chosen number” means that the chosen number from 68 variable Xiorderly for each method when its classification accuracy achieves the convergence. The smaller the chosen number is, the better convergence performance the method gets. And this item is a part of the reference standard of the classification performance. The standard deviation demonstrates the classification stability after convergence of each method and the last column shows the processing time of classification. All data from Table 2 are calculated into the averages of 10 times.According to the results in Fig. 8 and Table 2, some information can be concluded as follows:(1)When classification accuracy converges:(a)The average classification accuracy of dataset after FSMICS method processing is the maximum, which reaches to 82.4%.Choosing 39 feature subsets for FSMICS can make the classification accuracy converge, which is the minimum of the other three. The others are in descending order, AGV, Corona and CLeVer.After classification accuracy converges, the standard deviations of FSMICS and CLeVer are close, which are both relatively stable. Relatively speaking, the stability of Corona is a bit poor and AGV is not stable.The time of classification for dataset after four feature selection methods with SVM is approximately 1S. The fastest is with AGV, followed by CLeVer, FSMICS and Corona. Since the feature vectors after vectorization process still has a high dimension, the FSMICS is not the best on classification time among four methods.After the analysis of the clinical experiment, we can give the conclusion that AGV is not applicable to our MTS data type. However, AGV shows great classification efficiency, which means that AGV is a good dimension reduction method to some extent.As can be seen from the classification result of public and clinical datasets, by comparing to the other three MTS feature selection methods, FSMICS gets the maximum average classification accuracy and the decent convergence rate when classification accuracy converges. Moreover, it shows good stability after the convergence. Through the experiments, we can determine that the FSMICS yields the highest selection accuracy, with relatively acceptable classification efficiency.From the medical perspective of mathematical statistics, we can conclude that FSMICS can classify the patients into corresponding classes with relatively accuracy in the clinical data.

@&#CONCLUSIONS@&#
