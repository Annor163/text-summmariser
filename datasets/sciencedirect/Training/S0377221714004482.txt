@&#MAIN-TITLE@&#
A “reduce and solve” approach for the multiple-choice multidimensional knapsack problem

@&#HIGHLIGHTS@&#
The MMKP is a difficult combinatorial optimization problem.We use information from linear relaxation to fix some groups and variables.We use an ILP solver (CPLEX) to solve the reduced problem.We use additional strategies to explore the space of the reduced problems.We find improved best lower bounds for 18 out of the 37 benchmark instances.

@&#KEYPHRASES@&#
Knapsack,Fixing heuristics,Linear relaxation,Hybridization,

@&#ABSTRACT@&#
The multiple-choice multidimensional knapsack problem (MMKP) is a well-known NP-hard combinatorial optimization problem with a number of important applications. In this paper, we present a “reduce and solve” heuristic approach which combines problem reduction techniques with an Integer Linear Programming (ILP) solver (CPLEX). The key ingredient of the proposed approach is a set of group fixing and variable fixing rules. These fixing rules rely mainly on information from the linear relaxation of the given problem and aim to generate reduced critical subproblem to be solved by the ILP solver. Additional strategies are used to explore the space of the reduced problems. Extensive experimental studies over two sets of 37 MMKP benchmark instances in the literature show that our approach competes favorably with the most recent state-of-the-art algorithms. In particular, for the set of 27 conventional benchmarks, the proposed approach finds an improved best lower bound for 11 instances and as a by-product improves all the previous best upper bounds. For the 10 additional instances with irregular structures, the method improves 7 best known results.

@&#INTRODUCTION@&#
The multiple-choice multidimensional knapsack problem (MMKP) can be informally described as follows. We are given a set of items that are divided into several groups and different types of limited resources. Each item requires a certain amount of each resource and generates a profit. The purpose of the MMKP is to select exactly one item from each group such that the total profit of the selected items is maximized while the consumption of each resource does not exceed the given limit (knapsack constraints).Formally, givenG={G1,G2,…,Gn}the set of n disjoint groups (i.e.,Gi∩Gj=∅for eachi,j,1⩽i≠j⩽n). LetI={1,2,…,n}be the group index set,gi=∣Gi∣the number of items of groupGi∈G,mthe number of resource types,bkthe capacity of resource k (1⩽k⩽m),pij⩾0the profit of thejthitem ofGi,wijkthe consumption for resource k of thejthitem ofGi. Additionally, letxijbe the decision variable such thatxij=1if thejthitem of groupGiis selected;xij=0otherwise. Then the MMKP can be stated as follows:(1)max∑i∈I∑j∈{1,…,gi}pijxijsubject to:(2)∑i∈I∑j∈{1…gi}wijkxij⩽bk,k∈{1,…,m}(3)∑j∈{1…gi}xij=1,i∈I(4)xij∈{0,1},i∈I,j∈{1,…,gi}The MMKP is tightly related to the conventional multidimensional knapsack problem (MKP) (Boussier, Vasquez, Vimont, Hanafi, & Michelon, 2010; Kellerer, Pferschy, & Pisinger, 2004; Puchinger, Raidl, & Pferschy, 2010; Vasquez & Hao, 2001; Vasquez & Vimont, 2005; Wilbaut & Hanafi, 2009) since the MMKP can be reduced to the MKP by restricting each group to a single item and dropping constraint (3). Like the MKP, the MMKP is known to be NP-hard. In addition to its theoretical importance, the MMKP is notable for its capacity of modeling a number of practical applications such as logistics (Basnet & Wilson, 2005), resource allocation (Gavish & Pirkul, 1982), capital budgeting (Pisinger, 2001), and telecommunications (Watson, 2001).Compared with the conventional MKP, the MMKP is somewhat less studied until recently. Yet, given both its theoretical and practical relevance, the MMKP is receiving increasing attention in recent years and a number of effective solution approaches have been proposed in the literature. For instance, exact methods based on the branch and bound framework were reported in Ghasemi and Razzazi (2011), Khan (1998) and Sbihi (2007). These algorithms have the advantage of guaranteeing the optimality of the solution found. Unfortunately, due to the very high computational complexity of the MMKP, exact approaches apply only to instances of limited sizes (i.e., n=100 and m=10 for the instances we used). To handle larger instances, several heuristic approaches were developed to seek sub-optimal solutions (corresponding to lower bounds) in an acceptable computing time.For instance, Hifi, Michrafy, and Sbihi (2006) introduced a reactive local search algorithm and showed much better results than those reported in Moser, Jokanovic, and Shiratori (1997), which was the first paper dealing directly with the MMKP. Later, the authors of Cherfi and Hifi (2009) proposed an efficient hybrid heuristic combining local branching with column generation techniques, which improved the lower bounds for many benchmark instances. In Hanafi, Mansi, and Wilbaut (2009) an iterative relaxation-based heuristic was applied to solve the MMKP, where a series of small sub-problems are generated by exploiting information obtained from a series of relaxations. In Crévits, Hanafi, Mansi, and Wilbaut (2012), the authors employed a similar but more general approach called semi-continuous relaxation heuristic approach where variables are forced to take values close to 0 or 1. More recently, again based on the iterative relaxation-based heuristic framework, the authors of Mansi, Alves, Valerio de Carvalho, and Hanafi (2013) explored a new strategy, consisting of a family of new cuts and a reformulation procedure used at each iteration to improve the performance of the heuristic and to define the reduced problem. This method reported most of the current best known results over the set of conventional MMKP benchmark instances, which will be used as one of our references for performance assessment and comparison. In Shojaei, Basten, Geilen, and Davoodi (2013), the authors proposed an original parameterized compositional pareto-algebraic heuristic (CPH) which explores incremental problem solving and parallelization. They reported interesting results on the well-known MMKP benchmark instances and introduced a set of new instances that we will use in our work. Finally, there are several other recent and interesting studies based on general approaches like ant colony optimization combined with local search (Iqbal, Faizul Bari, & Sohel Rahman, 2010), strategic oscillation exploring surrogate constraint information (Htiouech & Bouamama, 2013), Lagrangian neighborhood search (Hifi & Wu, 2012) and tabu search (Hiremath & Hill, 2013).In this paper, we present a “reduce and solve” heuristic approach that jointly makes use of problem reduction techniques and the state-of-the-art CPLEX ILP solver. The basic idea of the proposed approach is to employ some dedicated heuristics to fix a number of groups and variables in order to obtain a reduced critical subproblem which is then solved by the ILP solver. The key issue is how to choose the groups and variables to fix. For this purpose, we first define general fixing rules based on information from linear relaxation. To better explore the space of the reduced problems and achieve improved results (lower bounds), we additionally introduce specific strategies to enlarge progressively the reduced subproblems which are to be solved by CPLEX. Notice that our group and variable fixing techniques are in connection with the notion of strongly determined and consistent variables (Glover, 1977, 2005). Similar strategies for temporary or definitive variable fixing are explored in other contexts like, for instance, 0–1 mixed integer programming and binary quadratic programming (Wang, Lu, Glover, & Hao, 2011, 2013; Wilbaut & Hanafi, 2009).To assess the merit and limit of the proposed approach, we carry out extensive computational experiments based on two sets of benchmark instances from the literature. These experiments show that the proposed approach competes favorably with the state-of-the-art methods and is able to discover 11 improved lower bounds and in passing to improve all the current best upper bounds reported in the literature for the set of 27 conventional benchmark instances. Moreover the proposed approach improves 7 best known results for the 10 additional benchmarks with irregular structures.The paper is organized as follows. In Section 2, we present in detail the proposed approach. We begin with the introduction of the group and variable fixing rules and then introduce two solution procedures for the exploration of different reduced problems. Section 3 is dedicated to an extensive computational assessment in comparison with the state-of-the-art approaches. We also show an analysis of the effect of the group and variable fixing techniques in Sections 4. Conclusions are given in the last section.The “reduce and solve” approach proposed in this paper can be summarized as a three-step method.1.Group fixing: This step aims to identify some variables which are highly likely to be part of the optimal solution and fixes them to the value of 1. Given the constraint (3), once a group has a variable assigned the value of 1, the remaining variables of the group must be assigned the value of 0. We remove then the group (the group is said fixed) from the initial problem P, leading to a first reduced problemP′. Let q be the number of fixed groups.Variable fixing: For each of then-qremaining groups of the problemP′, we identify some variables that are unlikely to be part of the optimal solution, fix these variables to 0 and remove them from problemP′, leading to a further reduced problemP″.ILP solving: We run CLPEX to solveP″.Given this general procedure, it is clear that the success of this approach depends on the methods used for group fixing (step 1) and variable fixing (step 2). We will explain in Sections 2.3 and 2.4 the heuristic fixing rules based on linear relaxation of the problem. However, whatever the method we use, it is possible that some variables are fixed to a wrong value. To mitigate this risk, we introduce additional strategies to decrease gradually the number of fixed variables. By doing so, we explore different and increasingly larger reduced problems which provides a means to achieve improved solutions. These strategies are presented in Sections 2.5.2 and 2.5.3.The following notations and definitions will be used in the presentation of the proposed approach.–LetG={G1,G2,…,Gn}be the given MMKP problem P with its index setI={1,2,…,n}and letx∗be an optimal solution of problem P.LP(P)andx¯denote respectively the linear relaxation of P and an optimal solution ofLP(P).v̲(P)andv¯(P)denote respectively a lower bound and an upper bound of P.(P∣c),LP(P∣c)andx¯cdenotes respectively the problem P with exactly one additional constraint c, the linear relaxation of(P∣c)and an optimal solution ofLP(P∣c).Integer group: Given the LP-relaxation optimal solutionx¯ofLP(P), a groupGiof G is called integer group inx¯if∃ji∈{1,2,…,gi}:x¯iji=1.Fractional group: Given the LP-relaxation optimal solutionx¯ofLP(P), a groupGiof G is called fractional group if∃J⊂{1,2,…,gi},∣J∣>1:∑j∈Jx¯ij=1,∀j∈J:0<x¯ij<1. Letnfbe the number of fractional groups inx¯.Forx∈[0,1]n, we defineI1(x)={i∈I:Giis an integer group,xiji=1}as the index set of integer groups.Forx∈[0,2]n, we defineI2(x)={i∈I,∃ji∈{1,2,…,gi}:xiji=2},I2∗(x)=I⧹I2(x),F(x)={(i,j):i∈I2∗(x),0<xij<2}.rijis the reduced cost ofxijfrom the LP-relaxation of P.For an ILP problem, it is interesting to consider the relationship between its (integer) optimumx∗and its LP-relaxation optimumx¯. When we studied several typical small MMKP instances, we observed that the binary optimum and the LP-relaxation optimum share some selected items (i.e., the corresponding variables receive the value of 1 in both the binary optimum and the LP optimum). It is thus expected that some groups could be fixed according to the LP-relaxation optimumx¯.In order to reduce the risk of fixing wrong groups, we try to identify the set of “strongly determined” groups. For this purpose, we use a double LP-relaxation strategy. We first solve the LP-relaxation of the given problem P to identify a set of integer groups. Then we add an effective cut c to the original problem P and solve again the relaxed linear programLP(P∣c). The purpose of this second LP-relaxation with the cut c is to identify some unpromising (or unstable) integer groups in the first LP optimumx¯such that their variable assignments change in the new optimumx¯c. Those integer groups inx¯that “survive” inx¯care considered to be stable and are likely to be part of the true optimumx∗. We then fix these groups by assigning their variables to their LP optimal values. Once these groups are fixed, they are removed from the problem and will not be further considered.Formally, our group fixing rule for the given problem P is summarized as follows.1.Solve its LP-relaxationLP(P)and letx¯be its optimal solution. LetI1(x¯)be the index set of the integer groups inx¯.Define the following cutckwith respect tox¯(where k is a parameter) and let(P∣ck)be the new problem with the added cutck.(5)∑i∈I1(x)(1-xiji)+∑j∈{1,…,gi},j≠jixij⩾2∗kSolve the LP-relaxationLP(P∣ck)and letx¯ckbe the optimal solution.Definex̃=x¯+x¯ck, i.e., the value in each position ofx̃is obtained by adding the values of the corresponding position of vectorx¯andx¯ck. According to the definition ofI2(x), it holds:∀i∈I2(x̃):x¯iji=1∧x¯ijick=1.For each groupGisuch thati∈I2(x̃), fix the variables ofGito theirx¯values. As such we fix the groups ofI2(x̃)and put aside all their variables.One notices that given the cut defined in Formula (5), the integer groups inx¯ckis a subset of the integer groups inx¯. Intuitively, cutckforces at least k integer groups in the first LP optimumx¯to change their status in the second LP optimumx¯ck. Such a changed group either becomes a fractional group or remains an integer group where the value of 1 is however assigned to a different variable. By varying k, we can control the number of groups to be fixed and as a consequence the size of the reduced subproblems. A large (resp. small) k fixes a small (resp. large) number of groups. Withk=n-nf(nfbeing the number of fractional groups inx¯), all the integer groups inx¯change their status inx¯ckand consequently, no group will be fixed. Thus,n-nfcan be considered as the maximum value for k. In Section 2.5, we will discuss how one can (partially) enumerate k in a potential interval, or set it to a promising value.After the group fixing step, though∣I2(x̃)∣groups of variables are fixed and removed from the problem, we still have∣I2∗(x̃)∣groups of variables whose values are not decided. The associated subproblem might be still too large to be efficiently solved by the ILP solver. To further reduce the search space, we need to further fix some variables in the∣I2∗(x̃)∣unfixed groups. Notice that contrary to group fixing where exactly one specific variable of each fixed groups is assigned the value of 1, the variable fixing step tries to identify, for each unfixed group, some variables that are to be assigned the value of 0 (i.e., the corresponding items will not be part of the optimal solutionx∗). For this purpose, we will make use of another type of information from LP-relaxation, i.e., the reduced costs.According to the LP theory, variables with small absolute reduced costs have more potential to be changed without affecting too much the objective value. It is then interesting to fix the variables with a large absolute reduced cost to their LP optimal values and leave the variables with a small absolute reduced cost to form the reduced problem to be solved by the ILP solver. In order to determine the number of the variables to fix (and thus the size of the reduced problem), we use an overall “threshold” of the absolute reduced costs.Precisely, the “threshold” is given by the highest absolute reduced cost among all the variables that receive different values in the previous double LP-relaxation optima:RCmax(x̃)=max(i,j)∈F(x̃){∣rij∣}whererijis the reduced cost of variablexijin the first linear relaxation andF(x̃)is the index set of variables whose values are greater than 0 and less than 2 inx̃.Obviously,RCmax(x̃)is related to the results of the two linear programsLP(P)andLP(P∣ck), and more precisely it is related to the parameter k (see cut (5), Section 2.3). Generally, a small k gives a smaller thresholdRCmax(x̃)which allows to fix more variables, leading thus to a smaller reduced problem. The reverse is also true.GivenRCmax(x̃), our variable fixing step can be summarized as follows. In each group, variables with an absolute reduced cost greater thanRCmax(x̃)are set to the value of the LP-relaxation optimal solution. The remaining variables form the reduced problem.Formally, letS={(i,j):i∈I,j∈{1,…,gi}}be the index set of all variables in the original problem P,Sfree(x̃)={(i,j):i∈I2∗(x̃),j∈{1,…,gi}:∣rij∣⩽RCmax(x̃)}be the index set of the unfixed variables in the unfixed groups, the reduced problem associated tox¯,x̃, S andSfree(x̃)is defined as follows:P(x¯,x̃,S,Sfree(x̃))={P∣(i,j)∈(S⧹Sfree(x̃)):xij=x¯ij}When k is small enough, the reduced problem can be solved to optimality within a very short time by means of the ILP solver. In practice, depending on the size of problem instances, the reduced problems are either solved to optimality or stopped due to a given CPU time limit.As group fixing and variable fixing are two relatively separated procedures, we propose two different ways of combining these two procedures to devise the MMKP algorithms. The first algorithm (named PEGF) partially enumerates the parameter k for group fixing and for each k usesRCmax(x̃)for variable fixing and constructing the reduced problem. For the second algorithm (named PERC), we set k to a promising value (identified in an empirical manner) and partially enumerate the absolute reduced cost threshold by increasingRCmax(x̃)progressively. Before presenting our PEGF and PERC algorithms in Sections 2.5.2 and 2.5.3, we first recall some known results which are used in our algorithms.The Mixed Integer Programming (MIP) relaxation has been used in Wilbaut and Hanafi (2009) to strengthen the bound of the LP relaxation, which was previously explored by the authors in Crévits et al. (2012) to design their algorithms for solving the MMKP. To introduce this technique, we consider a more general case of the MMKP. Let p and b be two vectors of coefficients, W be a matrix of coefficients, N be the set of binary variables. The MIP relaxation of problem P which is related to a subset J of the set N of items is expressed as follows:(6)MIP(P,J)maxpxs.t.Wx⩽bxj∈{0,1}j∈Jxj∈[0,1]j∈N⧹JThe following proposition shows the MIP relaxation of a problem provides stronger bounds than the linear relaxation (the proof can be found in Wilbaut & Hanafi (2009)). This proposition is used in both PEGF and PERC to obtain an upper bound for the problem P.Proposition 1Letv(P)be the optimal value ofP,JandJ′be two subsets of N withJ′⊆J⊆N, yields:Algorithm 1 shows the basic steps of the PEGF algorithm. In the preprocessing step, we solve the LP-relaxation of the original problem P, generating an optimal solutionx¯and the reduced costs of all the variables. At each iteration, the algorithm obtainsx¯ckby solving the LP-relaxation of the current problem noted(P∣ck).x̃can then be computed by summing upx¯andx¯ck.Algorithm 1Pseudo-code of the PEGF algorithm for the MMKP1:Input:P: an instance of theMMKP;kstart: first value of k;Tmax: limit on total computing time;tmax: time limit for solving the reduced problem;Δ: step length to increase k;2:Output:the best lower boundv̲(P)found so far;3: SolveLP(P); keep an optimal solutionx¯and the reduced costrij,∀(i,j)∈S;4: LetTIbe the temporary index set of fixed groups,tcurbe the current elapsed CPU time;5:v̲(P)=-∞,v¯(P)=pTx¯,nf=n-∣I1(x¯)∣,k=kstart,TI=ø;6:while(⌊v¯(P)-v̲(P)⌋⩾1)∧(k⩽n-nf)∧(tcur<Tmax)do7:SolveLP(P∣ck); keep an optimal solutionx¯ck;8:x̃=x¯+x¯ck;9:ifI2(x̃)!=TIthen10:TI=I2(x̃);11:else12:skip to 23;13:end if14:SolveMIP(P,Sfree(x̃)); keep an optimal solutionx¯mip;15:ifpTx¯mip<v¯(P)then16:v¯(P)=pTx¯mip;17:end if18:ComputeRCmax(x̃)and construct reduced problemP(x¯,x̃,S,Sfree(x̃));19:Run CPLEX withmin{tmax,Tmax-tcur}to solve the reduced problemP(x¯,x̃,S,Sfree(x̃)); keep a best solutionx0;20:ifpTx0>v̲(P)then21:v̲(P)=pTx0;22:end if23:k←k+Δ;24:end whileTo avoid redundant computations, at each iteration, we check whether the current set of fixed groups is the same as that of the last iteration; and if this is the case, we increase k by a step lengthΔand move to the next iteration.v¯(P)is updated each time a better upper bound is obtained by solvingMIP(P,Sfree(x̃)). By reference tox̃, the absolute reduced cost thresholdRCmax(x̃)can be decided by comparing the values of all the “shakable” variables. After that, the reduced problemP(x¯,x̃,S,Sfree(x̃))is constructed and solved with time limitmin{tmax,Tmax-tcur}by the ILP solver. The so far best lower boundv̲(P)is updated when a new best solution is encountered.The process stops when one of the following three stopping criteria is met: the upper bound equals the lower bound, or k exceeds the maximum valuen-nf, or the current execution time reaches the total time limit.Notice that the first value of k, namelykstart, could be set to 0 from a general algorithmic viewpoint. However in practice, when an overall time limit is imposed,kstartis not suggested to be too small since too many groups would be fixed wrongly and it is a waste of computational resources to solve the related reduced problems. According to our experimental experience, we setkstart=13for the instances we used, which can be regarded as a good start point for our PEGF algorithm.Another way of exploring different reduced problems is to fix k to a specific valuek0and then vary the absolute reduced cost threshold. Our PERC algorithm is based on this idea (see Algorithm 2).To specifyk0, we devise the following empirical formula which is based on some characteristics of the given instance under consideration.(10)k0=min{n-nf,kstart+⌈lg1.2n+0.5m⌉}To enumerate different thresholds for the absolute reduced costs, we rely again on information from LP-relaxation. Precisely, we solve firstLP(P)andLP(P∣ck0)in the preprocessing phase to get theRCmax(x̃)value (see Section 2.4). This value is then used as the first absolute reduced cost threshold rd. At each iteration of the PERC algorithm, the upper bound and the lower bound are updated respectively by solving theMIP(P,Sfree(x̃))and the reduced problemP(x¯,x̃,S,Sfree(x̃))(see Section 2.5.1). rd is then increased by a small valueΔto expand the reduced problem to be solved in the next iteration of the algorithm.Algorithm 2Pseudo-code of the PERC algorithm for the MMKP1:Input:P: an instance of theMMKP;k0: chosen value for k;Tmax: limit on total computing time;tmax: time limit for solving the reduced problem;δ: step length to increase reduced cost thresholdrd;2:Output:the best lower boundv̲(P)found so far;3: SolveLP(P); keep an optimal solutionx¯and the reduced costrij,∀(i,j)∈S;4: SolveLP(P∣ck0), keep an optimal solutionx¯ck0;5:x̃=x¯+x¯ck0;6: ComputeRCmax(x̃);7:rd=RCmax(x̃),v̲(P)=-∞,v¯(P)=pTx¯; lettcurbe the current elapsed CPU time;8:while(rd<v‾(P)-v̲(P))∧(tcur<Tmax)do9:SolveMIP(P,Sfree(x̃)); keep an optimal solutionx¯mip;10:ifpTx¯mip<v¯(P)then11:v¯(P)=px¯mip;12:end if13:Construct reduced problemP(x¯,x̃,S,Sfree(x̃));14:Run CPLEX withmin{tmax,Tmax-tcur}to solve the reduced problemP(x¯,x̃,S,Sfree(x̃)); keep a best solutionx0;15:ifpTx0>v̲(P)then16:v̲(P)=pTx0;17:end if18:rd←rd+δ;19:end whileThe algorithm stops either when rd becomes larger than or equal to the gap between the upper bound and the lower bound, or the execution time reaches the total time limit. The rationale of the first stopping criterion is that according to the reduced cost constraint (see Section 2.5.1), including nonbasic variables whose absolute reduced costs are higher than the gap to the reduced problem cannot further improve the lower bound. Note that variables with an absolute reduced cost larger thanRCmax(x̃)are all nonbasic variables, because basic variables having fractional values in the LP-relaxation optimal solution are those variables whose absolute reduced costs are smaller thanRCmax(x̃)and are already included in the reduced problem.To evaluate the efficiency of the proposed heuristics, we carry out extensive experiments on two sets of 37 benchmark instances. The first set of 27 instances1Available at: http://www.laria.u-picardie.fr/hifi/OR-Benchmark/.1(7 instances named I07-I13 and 20 instances named INST01-INST20) is very popular in the literature and frequently used to test recent MMKP algorithms like Mansi et al. (2013); Shojaei et al. (2013); Crévits et al. (2012); Cherfi and Hifi (2009); Hifi et al. (2006). The results on the instances of this set are reported in Sections 3.3 and 3.4. The second set (10 instances named INST21-INST30) is introduced very recently in Shojaei et al. (2013). The results on the second set of instances are provided in Sections 3.5.•Instances I07-I13 belong to the instance set generated and used in Khan (1998) (We do not consider the first 6 instances (I01-I06) because they can be easily solved by state-of-art MMKP exact algorithms and CPLEX). INST01-INST20 which are introduced in Hifi et al. (2006) according to the procedure proposed in Khan (1998), are large sized and more difficult. Thus in total, we consider 27 instances for our first experiments. These instances have a number of groups from 50 to 400, a number of items from 10 to 30 per group and 10 constraints. Except for the two smallest ones (INST01 and INST02), optimal solutions are still unknown for the remaining 25 instances. The best known results of the literature listed in different tables of this section are reported in Mansi et al. (2013) (algorithm MACH), Shojaei et al. (2013) (algorithm CPH) and Cherfi and Hifi (2009) (algorithm CH). Notice that in Mansi et al. (2013), the authors present results of three algorithm variants named MACH1, MACH2 and MACH3. And for MACH3, results corresponding to three different parameter settings are also provided. Here for each instance, we select the best results from all these MACH variants as the reference results. Also in Shojaei et al. (2013), results obtained by the serial version and parallel version (executed on 10 processors) of the proposed algorithm were provided and we select those results obtained by the serial version as reference for our comparative study.Instances INST21-INST30 are new benchmarks introduced in Shojaei et al. (2013). Compared to the previous instances, these new instances have irregular structures such that the number of items per group varies and an item does not necessarily has resource consumption in all the resource dimensions. These instances have a number of groups from 100 to 500, a maximum number of items from 10 to 20 per group and a number of constraints from 10 to 40. The optimal solutions are unknown and the best known results are reported in Shojaei et al. (2013) with the Compositional Pareto-algebraic Heuristic (CPH). We use these results as our reference for our experimental assessment.Our PEGF and PERC algorithms are coded in C++2The source code of our algorithms and the best solution certificates are available at http://www.info.univ-angers.fr/pub/hao/mmkp.html.2and compiled using GNU GCC on a PC running Windows XP with an Intel Xeon E5440 processor (2.83Gigahertz and 2G RAM). We use CPLEX 12.4 to solve both the LP-relaxation of the problem and the reduced problems.For the set of 27 conventional benchmark instances (I07-I13, INST01-INST20), we report our results respectively with a total time limit of 500seconds and 1hour. These stopping conditions were previously used by the state-of-the-art algorithms to report their computational results (Mansi et al., 2013; Crévits et al., 2012; Cherfi & Hifi, 2009; Hanafi et al., 2009). With the time limit of 500seconds which is considered to be relatively short in the literature, the time limit for a reduced problem (tmax) is set to be 75seconds for PEGF and 500seconds for PERC. With the time limit of 1hour, the CPU time allowed to solve a reduced problem is set to 400seconds for PEGF and 900seconds for PERC.For the set of 10 new benchmark instances (INST21-INST30), we adopt the time limit used in the reference paper (Shojaei et al., 2013), i.e., a total of 1200seconds and 1hour respectively. For the case of 1200seconds, the time limit for a reduced problem (tmax) is set to be 200seconds for PEGF and 600seconds for PERC. With the time limit up to 1hour, the CPU time allowed to solve a reduced problem is set to 400seconds for PEGF and 900seconds for PERC.In all the cases, the step length to increase k (Δ) in PEGF is set to 3, and the step length to increase the reduced cost threshold (δ) in PERC is set to 1.We mention that it is not a straight-forward task to make a full comparison between our results and those reported in the literature due to the differences in computing hardware, the version of CPLEX, programming language, etc. However, we hope the experimental studies shown in this section provide some useful indications about the performance of the proposed algorithms relative to the current best MMKP procedures.In this section, we report our results obtained by the PEGF and PERC algorithms with the time limit of 500seconds, and compare them with the best known results over the set of the 27 classical benchmark instances. We also show a comparative study with respect to the best state-of-art algorithms.Table 1shows the results of our two algorithms together with the best known results in the literature. In Table 1, the best known lower bounds (columnBKLB⩽1200seconds) are given by several MACH variants (Mansi et al., 2013) (with a time limit of more than 500seconds), CPH (Shojaei et al., 2013) (with a time limit of 1200seconds) and CH (Cherfi & Hifi, 2009) (with a time limit of 1200seconds), while the best known upper bounds (column BKUB) are only provided by MACH (Mansi et al., 2013). Columnv̲(P)andv¯(P)respectively indicate the lower bound and upper bound obtained by the corresponding algorithms, where the best known value is highlighted in bold while a ‘*’ symbol indicates an improved best result. cpu gives the time when the algorithm encounters the best solution for the first time.From Table 1, we observe that our PEGF and PERC algorithms attain respectively 9 and 8 best known results out of the 27 instances among which 4 corresponds to improved best lower bounds (1 from PEGF and 3 from PERC). The best solutions provided by both PEGF and PERC are slightly different according to the instances, showing some complementarity of the two algorithms. The two algorithms attain together 14 best known results. For those instances where they fail to reach the best known results, the gaps between our results and the best known ones are very small. The largest gap is 0.01% for I10 (calculated by (OurResult-BestKnown)/BestKnown×100).Moreover, the values reported in columnsv¯(P)show that our algorithms generate strong upper bounds. Compared to the best known upper bounds (column BKUB) provided by MACH, PEGF produces better results for all the instances. The same performance is observed for PERC with only one exception (i.e. INST07). Recall that the upper bounds are obtained by solving the MIP-relaxation of the original problem P (see Eq. (6)), where a subset of variables is forced to be binary. In our two algorithms, the variables forced to be binary are those in the reduced problem and they show to be “critical” ones as confirmed by both the strong upper bounds and the favorable lower bounds. In addition, column cpu shows that our algorithms visit most of their best solutions in less than 400seconds, far before reaching the given time limit.In this experiment, PEGF realizes on average 9 iterations across the instance set. The reduced problems can be solved to optimality within 37seconds for all the instances when k is small enough (i.e.,k=kstart). As k increases, the reduced problems become larger and thus harder to solve. For PERC, given the value of k calculated by Eq. (10), the induced reduced problems cannot be optimally solved for most of the instances even if we impose a time limit of 500seconds for each of them. So we observed only 1 iteration for all the instances except INST02 where there are two iterations. The encouraging results presented in Table 1 show that these reduced problems are promising and additionally confirm the merit of our fixing rules. Indeed, a decrease oftmax(the time limit for a reduced problem) allows the method to explore more different reduced problems and introduces some diversities on the final results. However, the peak performance of PERC is achieved by adopting the parameter settings introduced in Section 3.2.To further evaluate the performance of our algorithms, we show a comparison with CPLEX, and 5 recent algorithms that achieve state-of-art performances:•A Hybrid heuristic approach (Mansi et al., 2013). The best version MACH3 of the 5 algorithm variants is used for comparison (column MACH3). The reported results were performed on a Dell computer 2.4Gigahertz and CPLEX 11.2 with an overall CPU limit >500seconds.3The paper (Mansi et al., 2013) indicates a limit of 500seconds, however one observes that Table 2 of Mansi et al. (2013) includes computing times greater than 500 for several cases.3A compositional pareto-algebraic heuristic method (Shojaei et al., 2013). The best results obtained by the serial version of their approach (i.e., CPH+OptPP) are listed (column CPH). The tests were performed with an overall CPU limit of 1200seconds on a PC with an Intel processor (2.8Gigahertz and 12G RAM).A column generation method (Cherfi & Hifi, 2009). We list its best reported results (column CH). The tests were performed on an UltraSparc10 2.5Gigahertz with an overall CPU limit of 1200seconds.An iterative relaxation-based heuristic approach (Hanafi et al., 2009). The best version of the 3 algorithm variants is compared in this paper (column HMW). The evaluations were performed with a CPU limit of 300seconds on a Pentium IV 3.4Gigahertz, but the version of CPLEX is not indicated.An iterative semi-continuous relaxation heuristic approach (Crévits et al., 2012). We select the best version among the 4 algorithm variants for comparison (column CHMW). The tests were performed on a Pentium IV 3.4Gigahertz, using CPLEX 11.2 with a CPU limit of 400seconds.Table 2summarizes the results of our PEGF and PERC algorithms along with those reported by the 5 reference algorithms and those obtained by CPLEX with a time limit of 500seconds. The last two rows (♯Bests and Sum.) indicate respectively the number of best known results and the total objective value over the 27 instances reached by an algorithm. From Table 2, we can observe that our two algorithms attain respectively 9 (PEGF) and 8 (PERC) best known results (♯Bests) and their total objective values are higher (better) than those of the reference algorithms (Sum.) and CPLEX.Compared to MACH3 which is one of the current best performing algorithms for the MMKP, our algorithms remain very competitive even if the improvement of our algorithms over MACH3 is relatively small. Notice that the overall qualities of the best known solutions of the benchmark instances are already very high (see Table 1, the gaps between the upper bounds and the lower bounds for all the instances are small). As such, even a small improvement could be difficult to reach and thus can be considered to be valuable. It should be noted that MACH3 was executed with a time limit of more than 500seconds on a machine which is slightly faster than our computer according to the Standard Performance Evaluation Corporation (www.spec.org), though our CPLEX version is more recent.This experiment demonstrates that our approach outperforms CPLEX and most of the reference algorithms and competes well with the best performing MACH3 algorithm.In this section, we investigate the behaviors of our algorithms by extending the total time limit to 1hour and comparing our algorithms with both CPLEX 12.4 and the results reported in Crévits et al. (2012) (see Table 3).In Crévits et al. (2012), the authors proposed 4 different variants of a heuristic approach namely ILPH, IMIPH, IIRH and ISCRH, and they reported the results obtained by their approaches with a time limit of 1hour. Table 3 lists for each instance the best lower bound visited by our PEGF and PERC algorithms, the value of the best feasible solution of CPLEX 12.4 obtained within 1hour of computing time, and the best value of ILPH, IMIPH, IIRH and ISCRH from Crévits et al. (2012). A value in bold means that our algorithms reach the best known result reported in the literature and a starred value represents a new best lower bound (a record lower bound) for the given instance. In the column “CPLEX”, the values of two instances (INST01 and INST02) are marked with∘which means they are proved to be optimal by CPLEX. In the last three rows of Table 3, we indicate for each algorithm the number of best lower bounds obtained, the number of improved lower bounds (lower bounds starred) which can be obtained by the algorithm (♯RLB) and the sum of the best solution values over the instance set (Sum.).From Table 3, we observe that our algorithms visit the best known solution for 19 out of the 27 instances. Among these best solutions, 11 were never reported by previous approaches. PEGF and PERC respectively obtain 15 and 14 best known lower bounds and their average solution qualities are both very high, which shows both algorithms are very effective under this stopping condition. Compared to CPLEX, our algorithms obtain 24 better lower bounds, 1 equal result and 2 worse results. Compared to the 4 variants proposed in Crévits et al. (2012), our algorithms compete favorably by holding more record lower bounds (11 vs. 3). Finally, when we examine the total objective value attained by each algorithm over the whole set of the 27 instances, we observe that the proposed algorithms dominate both CPLEX and the four reference algorithms.The last experiment is dedicated to the set of 10 large irregular benchmark instances introduced in Shojaei et al. (2013). Like Shojaei et al. (2013), we adopt a total time limit of 1200seconds and 1hour, respectively. We also provide the results obtained by CPLEX for our comparative study.In Shojaei et al. (2013), the results of two variants of their proposed CPH approach were reported, namely CPH+OptPP with the default resource usage aggregation and with the scarcity-aware resource aggregation. We use the best solution values of their two algorithm variants as our reference. Table 4shows our results (PEGF and PERC) as well as those of CPH and CPLEX under the two time conditions. For each instance, a solution value is highlighted in bold if it is the best among those results obtained by the four listed algorithms. In the last two rows of Table 4, we indicate for each algorithm the number of best lower bounds obtained (♯Bests) and the sum of the best solution values over the instance set (Sum.).From Table 4, we observe that our algorithms compete very favorably with CPH and CPLEX on these instances. Our algorithms discovers improved best known solutions for 7 out of 10 instances. For the time limit of 1200seconds, each of our two algorithms attains 5 best solutions against 2 for CPH and 1 for CPLEX. Both PEGF and PERC give better results than CPH and CPLEX in 8 out of 10 cases, while there are only two cases (INST22 and INST24) where CPH performs better and one case where CPLEX performs better (INST28). Regarding the total solution value, our two algorithms outperform both CPH and CPLEX. A similar observation can be made for the case of 1hour execution. One exception is that the total solution value of PEGF is slightly worse than that of CPH though PEGF obtains better results for more cases than CPH does (6 vs. 3). Moreover, the results show that our two algorithms are able to achieve further improved best lower bounds for 4 instances when more time is available. This experiment confirms that our proposed algorithms perform well on these irregular instances.

@&#CONCLUSIONS@&#
The multiple-choice multidimensional knapsack problem (MMKP) is a highly useful model in practice and represents nevertheless a real challenge from a computational point of view. We have presented a “reduce and solve” approach for the MMKP which jointly uses problem reduction techniques and the state-of-the-art ILP technology. On the one hand, the proposed approach employs a two-stage fixing method (group fixing and variable fixing) based on LP-relaxation to generate reduced problems which are solved by the ILP solver. To better explore the space of the reduced problems, the proposed approach makes uses of two dedicated strategies to progressively expand the reduced problems, leading to two algorithm variants (PEGF and PERC).Using a set of 27 well-known classical MMKP benchmark instances and a set of 10 recent benchmarks with irregular structures from the literature, we have assessed the performance of the proposed approach and compared our results with some current best performing methods. The computational experiments have shown the proposed approach competes favorably with the state-of-the-art reference algorithms and dominates the well-known commercial CPLEX ILP solver. In particular, the proposed approach discovers 11 improved lower bounds and as a by-product provides 27 improved upper bounds for the set of classical MMKP instances. And for the set of 10 irregular benchmarks, the proposed approach is also able to discover 7 improved lower bounds. This study confirms the merit of a hybrid approach combining heuristics and a state of the art ILP solver.