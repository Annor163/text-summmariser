@&#MAIN-TITLE@&#
Evolutionary strategy to develop learning-based decision systems. Application to breast cancer and liver fibrosis stadialization

@&#HIGHLIGHTS@&#
An evolutionary-based strategy for building decision models is proposed.Five medical datasets (breast cancer and liver fibrosis) were used for assessment.The synergetic decision-making involved is easy to understand and apply.Statistical benchmark showed the effectiveness of the model.The model is expected to easily adapt to different medical decision-making issues.

@&#KEYPHRASES@&#
Decision support systems,Evolutionary computing,Machine learning algorithms,Weighted voting system,Breast cancer,Liver fibrosis stadialization,

@&#ABSTRACT@&#
The purpose of this paper is twofold: first, to propose an evolutionary-based method for building a decision model and, second, to assess and validate the model’s performance using five different real-world medical datasets (breast cancer and liver fibrosis) by comparing it with state-of-the-art machine learning techniques. The evolutionary-inspired approach has been used to develop the learning-based decision model in the following manner: the hybridization of algorithms has been considered as “crossover”, while the development of new variants which can be thought of as “mutation”. An appropriate hierarchy of the component algorithms was established based on a statistically built fitness measure. A synergetic decision-making process, based on a weighted voting system, involved the collaboration between the selected algorithms in making the final decision. Well-established statistical performance measures and comparison tests have been extensively used to design and implement the model. Finally, the proposed method has been tested on five medical datasets, out of which four publicly available, and contrasted with state-of-the-art techniques, showing its efficiency in supporting the medical decision-making process.

@&#INTRODUCTION@&#
A decision support system(DSS) is defined as a computer-basedinformation system assisting thedecision-makingprocess involved in solving a large variety of real-life problems [1]. Basically, DSSs are developed to support the solution of unstructured management issues in order to improve the decision-making process.Recent advances in information technology, artificial intelligence (AI) and statistical learning (SL) enhanced these systems, giving rise to intelligent DSS (IDSS). By incorporating different AI/SL techniques, DSS is likely to become “artificially” intelligent, ideally behaving like a human expert [2]. IDSSs are based either on standalone machine learning (ML) techniques, such as neural networks, genetic algorithms, support vector machines, cluster analysis, k-nearest neighbors, swarm intelligence, and random forests [3–5], or structured frameworks involving more than one algorithm. Usually, in the latter case, the contribution of the ML paradigms is either weighted or parallelized using more or less sophisticated techniques [6–10]. The IDSSs development was encouraged by their effectiveness proven in many different real-world decision problems, such as: medical decision-making, business and management and education.The use of IDSS in medical decision-making is nowadays a current practice. On the one hand, the use of single ML approaches is widespread due to their modeling relative simplicity. Thus, in [11] neural networks are applied to predict the severity of acute pancreatitis at admission to hospital. A support vector machines approach has been used for the seizure prediction with spectral power of EEG in epilepsy [12]. Both support vector machines with linear kernel and classification trees have been used for improving the accuracy of early diagnosis of Alzheimer-type dementia [13]. In [14] a competitive/collaborative neural computing system has been considered for early detection of pancreatic cancer. A hybrid neural network-genetic algorithm has been applied in [15] for breast cancer. On the other hand, even if the modeling process is more complex, there are many examples in the literature using combinations of ML techniques in medical studies. Thus, in [16] an ensemble of optimal numbers of pruned classification trees is proposed to both distinguish disease subtypes for optimal treatment in lymphoma and lung cancer patients, and to identify breast cancer patients. Two classifiers ensembles (rotation forest and random oracles) are experimented with in [17] for several medical datasets from the UCI Machine Learning Repository showing the improvement obtained using combination of several classifiers in computer-based medical systems. Rotation forest ensembles of classifiers of 30 machine learning algorithms have been constructed to evaluate their classification performances using Parkinson’s, diabetes and heart diseases datasets from literature [18]. In [19] a combination of multiple feature representations and AdaBoost ensemble learning have been used for tackling the high number of false-positive detections of masses on mammograms when using the computer-aided medical diagnosis. An ensemble of ML classifiers generated by a rotation forest technique is used in [20] to substantiate/justify the performance of such a method in the medical field, using Thyroid, Liver, Haberman, Wisconsin and Hepatitis datasets from UCI Machine Learning Repository.It is worth mentioning the existence of powerful research groups within well-established universities, who are working in developing IDSSs for medical decision-making, e.g., the project “Heart Disease Program” – a computer system assisting the physician in the task of differential diagnosis and anticipating the effects of therapy in the domain of cardiovascular disorders, developed by the Clinical Decision Making Group within the Computer Science and Artificial Intelligence Laboratory-CSAIL, Massachusetts Institute of Technology-MIT (http://groups.csail.mit.edu/medg/projects/hdp/).The current work proposes a flexible strategy to design and implement an IDSS, inspired by both swarm intelligence and the evolutionary metaphor. Thus, like in the swarm intelligence approach, typically considering a population of simple agents, interacting locally with one another and with the environment, and also based on the evolutionary paradigm, this strategy envisages integration in an evolutionary manner different well-performing ML algorithms, competing and collaborating with each other in a direct relation to the environment, hence making an “intelligent” global decision. The main contributions of the paper are twofold: first, the description of the evolutionary-inspired strategy followed by the design of IDSS, and, secondly, its validation in different real-life applications regarding medical diagnosis.The evolutionary-based concept underlying the design and functionality of IDSS is inspired by the meta-classification paradigm successfully used in different domains [21,22]. Thus, by combining several ML techniques into a single system, a committee of algorithms is firstly formed, and, secondly, the overall decision of the committee is achieved in a synergetic evolutionary way using a fitness-based weighted voting system (WVS).Firstly, the idea borrowed from the evolutionary paradigm is choosing efficient types of natural computing algorithms to form the initial population of solutions for a decision problem. Then, they will “evolve”: (a) new variants (obtained by ‘mutation’), and (b) hybrids (obtained by ‘crossover’) will be developed, forming the next population. Next, they will be tested based on their fitness given by the individual decision performance and the best of them will be retained, in order to replenish the new generation. After a certain number of generations, the most performing algorithms will be kept to form the decision system. Briefly, the population of potential solutions (i.e., algorithms) is subject to a problem-dependent selection process, followed by the creation of a fitness proportional hierarchy.Secondly, the synergism of this structure involves the collaboration between the selected algorithms in making the final decision, based on proportionally WVS and inspired again from the evolutionary metaphor. In this respect, the ‘crossover’ operator refers to the ‘recombination’ of the individual ‘decision genes’ using the fitness-based weights, and the ‘offspring’ represents the ‘joint’ final decision.Because the algorithms representing the ‘intelligent’ components of the decision ‘engine’ are mostly of stochastic nature, they have to be independently run a certain number of times to obtain a reliable result regarding their robustness and effectiveness. From a statistical point of view, the classification accuracy obtained during the multiple independent computer runs of each algorithm constitutes a sample of decision performance. To ensure the statistical tests will have adequate power, one must perform a power analysis prior to running the experiment. In this respect, a sample size estimation procedure (two-tailed type of null hypothesis, with default statistical power goal P⩾95%, and type I error α=0.05) has been proposed. The model validation has been achieved by using the standard 10-fold cross-validation. The average accuracy computed as the percentage of correctly classified cases represents the decision performance of each competitor.The corresponding fitness measure of selecting the best performing algorithms has been defined by the decision accuracy of each algorithm in the testing phase (Test), along with the corresponding standard deviation (SD).The fitness-based contrast between the algorithms performance was statistically analyzed using the well-known one-way ANOVA technique applied to the independent samples of computer runs, allowing thus the testing of nested performances [23]. The ANOVA output consisted of: sums of squares (SS), degrees of freedom (df), mean squares (MS), F-value, and p-level. Note that the underlying assumptions are fulfilled since the samples are independent, with equal size (balanced experiment), and fairly large. A follow-up test consisting of the two-sided z-test for comparing proportions of correctly classified cases was done, thus assessing the difference between algorithms performances.The algorithms were then grouped according to their appropriateness to the problem solving using the k-means clustering. Technically, the algorithm has been run for k ranging from 2 to N−1, where N is the number of competitors, using the decision performance of each competitor on each dataset. The initial centroids have been chosen to maximize the distances between-cluster. An analysis of variances comparing the within-cluster variability and between-cluster variability to estimate the near optimal number of clusters has been subsequently conducted. Finally, the clusters are sorted in descending order of mean performances, representing the first level of algorithms’ hierarchization.Based on the fitness measure used in conjunction with the benchmark methodology, the initial algorithms were statistically compared and the fittest were chosen to seed the next generation by applying the ‘variation’ approach. Inspired by the idea underlying the classical variation operators from the evolutionary computing field, we have naturally considered the hybridization of two algorithms as ‘recombination’, and the development of new variants as ‘mutation’.The survivor selection mechanism used in this approach is based on a steady-state model, in which the entire population is not replaced at once, but just a number of individuals are replaced using a rank-based selection.The end of the evolutionary process is determined by choosing different STOP conditions, such as: flat improvement curve, and limited population diversity.In the decision-making process, the IDSS components are involved in a weighted collaborative operating mode, seen as a variation of meta-classification techniques.Technically, once the benchmark process accomplished for the last generation, one chooses a number of clusters containing the best performing algorithms. The choice of the optimal number of clusters is problematic. If we carry out a decision-making process in a sensitive medical area like cancer detection, for instance, we have to take into account a larger number of algorithms although computational costs are higher. Conversely, if the decision process must be fast even if accuracy will be lower, the choice will target algorithms in the very first top clusters. Thus, the choice of a cut-off value for the number of algorithms (clusters) depends on the problem at hand.The overall decision-making process consists of a two-level hierarchization of algorithms according to their testing accuracy:(a)Hierarchization of clusters.Hierarchization of algorithms in each cluster.The best performing algorithm in each kept cluster will be weighted proportionally to both the normalized performance of the cluster and its own normalized performance. Mathematically speaking, the normalized performances of the chosen clusters form a convex combination of the corresponding mean performances, while the normalized performances of algorithms in a cluster also form a convex combination of their mean performances. Technically, if P(Ci) represents the mean performance of the cluster Ci, then αirepresents its normalized performance, and:(1)αi=P(Ci)/ΣP(Ci),Σαi=1.Next, if P(Aik) represents the mean testing performance of the algorithm Aikbelonging to the cluster Ci, then βikrepresents its normalized performance, and:(2)βik=P(Aik)/ΣP(Aik),Σβik=1.Finally, the weightαi*βikwill be assigned to algorithm Aik.The best algorithms thus selected to form IDSS will be applied to new data and an overall decision will be made based on WVS. Mathematically, WVS was represented as {w1,w2,…,wk}, where to algorithm Akis assigned the corresponding weight wk.In essence, the synergetic decision-making process has two steps:(1)The weights’ estimation for the selected algorithms. By default, a weight is directly proportional to the individual decision performance.Computation of the voting process output to establish the IDSS decision, and estimation of the corresponding confidence level representing the weighted mean of the best algorithms’ decisions; moreover, an upper bound of the IDSS decision can also be determined.The assessment of both the confidence level and the upper bound of the IDSS decision is based on a probabilistic approach described in [14], using the total probability formula and theinclusion–exclusion principle in probability.The proposed IDSS has been applied on five real-world medical datasets presented below:1.Breast Cancer Wisconsin (Original) – BCWO (UCI Machine Learning repository). BCWO consists of 683 cases with two decision classes: benign 444 (65%) instances and malign 239 (35%) instances. The database contains nine ordinal (categorical) attributes: clump thickness, uniformity of cell size, uniformity of cell shape, marginal adhesion, single epithelial cell size, bare nuclei, bland chromatin, normal nucleoli, mitoses (detailed description of the BCWO database at: http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29).Breast Cancer Wisconsin (Diagnostic) – BCWD (UCI Machine Learning repository). BCWD consist of 569 cases, with two decision classes: benign 357 (62.74%) instances and malign 212 (37.25%) instances. From the total of thirty-two attributes, ten numerical attributes have been considered as the most relevant from medical point of view: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension (detailed description of the BCWD database at: http://archive.ics.uci.edu/ml/datasets/Breast+Cancer Wisconsin+%28Diagnostic%29).Breast Cancer – BC (UCI Machine Learning repository). BC consists of 286 cases with two decision classes: non-recurrent-events 201 (70.27%) instances and recurrent-events 85 (29.72%) instances. The database contains nine mixed attributes, with three numerical attributes and six categorical attributes: age, tumor-size, inv-nodes, menopause, node-caps, deg-malig, breast, breast-quad,irradiat (detailed description of the BC database at: http://archive.ics.uci.edu/ml/datasets/Breast+Cancer).Breast Cancer Wisconsin (Prognostic) – BCWP (UCI Machine Learning repository). BCWP, consists of 198 cases with two decision classes: non-recurrent-events 151 (76.26%) instances and recurrent-events 47 (23.73%) instances. From the total number of thirty-four attributes contained by the database, ten numerical attributes have been considered to be the most relevant from medical point of view: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension (detailed description of the WRBC database at: http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Prognostic%29).Liver fibrosis – LF Liver fibrosis database (LF) consists of 722 patients with chronic HCV infection. They have been tested for the assessment of liver stiffness with the Fibroscan® (Echosens, Paris, France – http://www.echosens.com/). The dataset contains twenty-five attributes related to the five fibrosis stages (F0–F4), considered as decision classes. The database originates from the 3rd Medical Clinic, University of Medicine and Pharmacy Cluj-Napoca, Romania. Recall that, from medical point of view, liver fibrosis is evaluated semi-quantitatively according to the METAVIR F scoring system as follows: F0 – no fibrosis, F1 – portal fibrosis without septa, F2 – portal fibrosis and few septa, F3 – numerous septa without cirrhosis and F4 – cirrhosis, representing liver fibrosis stadialization.Altogether, the following ML algorithms have been considered in this study: neural networks (NNs), genetic algorithms (GAs), support vector machines (SVMs), k-nearest neighbor (kNN), and naïve Bayes (nB). In this context, four well-known NNs have been selected: four-layer perceptron (MLP), radial basis function (RBF), probabilistic neural network (PNN), and self-organizing map (SOM) [24,25].We considered in this study only the hybridization of MLP with GA in order to illustrate the methodology, although a lot of other combinations are possible.Although the combination of GAs with NNs is not new [26,27], we considered a recent developed variant of evolutionary-driven MLP, which has been thoroughly investigated and validated on several real-world medical datasets [15]. The proposed methodology unfolds in the following manner. The hybrid algorithm consists of:(a)The NN component – the classifier, designed as MLP.The GA component – the MLP’s weights optimizer, designed as GA.Concretely, MLP consisted of a number of inputs equaling the number of predictive attributes, one hidden layer with a number of processing units equaling the number of decision classes, and one output unit representing the resulting class attribute.For GA, the binary tournament selection has been used, while the blend crossover (BLX-α) and non-uniform mutation have been considered as variation operators.The ‘mutation’ considered in this paper consisted of an artificial replica of the way the human brain works, commonly known as partially connected neural network (PCNN). Thus, following the methodology presented in [28], we developed PCNN built in the following manner: after a certain number of training samples have been presented to the network, the weights that did not suffer major modifications (i.e., did not surpass a certain thresholdτthroughout the BP learning algorithm) are erased from the network’s architecture, being inhibited (i.e., set to 0). After heuristically investigating different MLP architectures, we have considered a model with two hidden layers. For the first hidden layer we experimentally chose 7 neurons, providing optimal performance. The second hidden layer contains a number of processing units equaling the number of decision classes, and there is one output unit representing the resulting class attribute. Different parameters have been investigated aiming to produce higher performance with less computational costs.1.For a standardization of the decision performance of MLP, RBF, PNN, SVM, kNN, and nB, and for statistical tests, we used the implementation within the Statistica 7 package – StatSoft. Inc., Tulsa, OK 74104, USA.MLP/GA, PCNN, and SOM have been implemented in Java by the authors.For the sake of simplicity, we considered in this study the evolutionary process based only on a crossover and a mutation. Without any further difficulty, the process can be extended over several generations. The significant optimization of the decision model, obtained ending the evolution after one generation, showed the efficacy of such an approach.A main open question that cannot be simply answered without a complex combinatorial optimization approach concerns the choice of the values to be set for the algorithms parameters, taking into account the diversity of datasets. For the sake of simplicity, in this study we heuristically chose them for each dataset separately.

@&#CONCLUSIONS@&#
An evolutionary-based strategy to build an intelligent medical decision model for breast cancer detection and recurrence, as well as for prediction of stages for liver fibrosis in chronic hepatitis C is proposed in this paper.Applied to five real-world medical databases, its design and functionality proved to be straightforward and efficient. Comparisons to results obtained by other techniques on the same datasets have also been made in order to objectively validate the results of this approach. The proposed IDSS outputs accuracy comparable to that of state-of-the-art ML classifiers applied on the same datasets.The design and functionality of the application of IDSS for breast cancer detection and recurrence, on the one hand, and for discrimination among stages of liver fibrosis, on the other hand, attained its planned goals. Its performance equaled or exceeded the results reported in literature.Future research may lie in:•The design and deploy of a management module, seen as the “engine control unit”, automatically running the decision system.More tuning of the system’s parameters attempting to enhance accuracy, as a combinatorial optimization approach.The enrichment of the initial population of candidate algorithms by considering other powerful ML algorithms.The enrichment of the next generations of algorithms by considering more ‘offspring’.