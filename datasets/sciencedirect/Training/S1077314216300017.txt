@&#MAIN-TITLE@&#
Probabilistic model for 3D interactive segmentation

@&#HIGHLIGHTS@&#
Interactive 3D medical image segmentation based on a Bayesian inference is suggested.User-machine “dialogue” is allowed by a few mouse clicks in regions of disagreement.User input is formulated as a probabilistic spatial term in a level-set functional.A generic method which accommodates different modalities, e.g. CT and multimodal MRI.GUI for clinical uses allows real-time, high performance with minimal user feedback.

@&#KEYPHRASES@&#
Image segmentation,User interaction,Probabilistic model,MR,&,CT Brain imaging,Modality fusion,

@&#ABSTRACT@&#
Fully-automated segmentation algorithms offer fast, objective, and reproducible results for large data collections. However, these techniques cannot handle tasks that require contextual knowledge not readily available in the images alone. Thus, the supervision of an expert is necessary.We present a generative model for image segmentation, based on a Bayesian inference. Not only does our approach support an intuitive and convenient user interaction subject to the bottom-up constraints introduced by the image intensities, it also circumvents the main limitations of a human observer—3D visualization and modality fusion. The user “dialogue” with the segmentation algorithm via several mouse clicks in regions of disagreement, is formulated as a continuous probability map, that represents the user’s certainty to whether the current segmentation should be modified. Considering this probability map as the voxel-vise Bernoulli priors on the image labels allows spatial encoding of the user-provided input. The method is exemplified for the segmentation of cerebral hemorrhages (CH) in human brain CT scans; ventricles in degenerative mice brain MRIs, and tumors in multi-modal human brain MRIs and is shown to outperform three interactive, state-of-the-art segmentation methods in terms of accuracy, efficiency and user-workload.

@&#INTRODUCTION@&#
Being fundamental to medical imaging analysis, image segmentation is actively studied, and numerous approaches exist. Recent trends focus on fully automatic segmentation frameworks, which is much faster than manual annotation, less biased, and repeatable. Usually, the required workload for processing and analyzing large datasets is far behind the ability of a human rater. Moreover, the computational advancements of the machine in cases that require modality fusion or 3D visualization cannot be achieved even by an expert. Nevertheless, as the outcome of the image analysis process might have critical implications on patient recuperation prospects, the expertise of a clinician must be considered.Interactive segmentation (IS) approaches can be classified based on the form and the type of input provided by the user as well as the underlying segmentation framework (see He et al. (2013) and Zhao and Xie (2013) and references therein). The pioneering IS work, which led to the development of the live wire technique or intelligent scissors, independently suggested by  Falcao et. al. (1998) and Mortensen and Barrett (1998), is based on the image edge map. The shortest paths between the user’s mouse clicks calculated by the Dijkstra algorithm form the contour of the region of interest (ROI). Here, as well as in the united snakes framework (Liang et al., 2006), which relies on a classical active contour framework known as snakes (Kass et al., 1988), the user ‘plants’ anchors or seed points along the desired boundary, providing guidance for the segmentation.Mouse scribbles seem to be the most common form of user interaction. The marked regions provide information about the ROI and the background intensity distributions. A well known IS approach is the GrabCut technique (Rother et al., 2004), which is based on the graph-cut method (Boykov et al., 2001). Representing the image pixels by nodes in a graph, the graph-cut addresses a foreground-background image segmentation by solving a min-cut, max-flow problem. The user’s annotated regions are assigned to either the source or the sink of the graph. In a recent paper by Nieuwenhuis and Cremers (2013), marked regions via mouse scribbles were used for gathering spatially varying color statistics for multi-label segmentation.While the techniques above are very effective when prior information on the image to segment is not available, they do not handle local ambiguities resulting from overlaps between the ROI and the background intensity distributions. Moreover, often, in medical image analysis problems the ROI is well specified and image statistics can be estimated based on similar examples. The user input is required only for resolving local discrepancies based on contextual information.In Ben-Zadok et al. (2009) and Cremers et al. (2007a) the level-set framework of Chan and Vese (2001), which implicitly models the foreground and the background intensities by two distinct normal distributions, was extended to include a spatial term provided via user interaction. This additional spatial information is provided in regions that (according to the user) are not compatible with the assumed intensity distribution model. A random walk algorithm is the foundation of the IS framework proposed in Yang et al. (2010). The method combines soft and hard input constraints to spatially guide the segmentation as well as foreground and background user strokes to learn the image statistics. Split and merge interactive operations were suggested in Paulhac et al. (2011) via region adjacency graph representation. More recently, Gao et al. proposed to use local robust statistics to describe the object features, which are learned adaptively from the strokes drawn by the user (Gao et al., 2012). Their framework allows the partition of the image into multiple regions via the simultaneous evolution of two active contours and using the ‘action–reaction’ principle to avoid overlaps. The idea of a concurrent evolution of a pair of level-set functions has been used in another interesting approach, based on concepts from control theory (Karasev et al., 2013). In that framework, the user’s accumulated input guides the evolution of one of the contours while the evolution of the other contour is based on image intensities and smoothness term, as in Chan and Vese (2001). The two level-set functions interact with each other, leading to a closed loop behavior.In this paper, we present a novel generative approach for interactive 3D segmentation of medical images. The proposed framework is also extended to address the extraction of a common ROI in a multi-modal image set. The key contribution is the probabilistic formulation of the user interaction. Specifically, the discrete set of 3D coordinates provided by mouse clicks in regions of disagreement is converted into a continuous probability map, that represents the user’s certainty to whether the current segmentation should be modified. This continuous representation defines the voxel-vise Bernoulli priors on the image labels. The maximum a posteriori (MAP) estimate of the segmentation is therefore based on a user-provided spatial information in addition to the conventional image likelihood term and a regularization term.The MAP problem is solved via calculus of variation, using level-set formulation, in the spirit of Riklin Raviv et al. (2010). Interaction with mouse clicks via level-set based segmentation has been suggested before by Ben-Zadok et al. (2009) and Cremers et al. (2007b). Nevertheless, there are essential differences. In  Ben-Zadok et al. (2009) and Cremers et al. (2007b) binary (hard) segmentation is considered where each pixel is assigned to either the ROI or to the background. The interaction that follows the automatic segmentation is represented by a map containing clouds of positive and negative values (based on the user provided clicks) directly affecting the level-set evolution by a simple summation. In contrast to these works and to others mentioned above, the proposed framework is entirely based on probabilistic principles. First, the segmentation is fuzzy (soft) such that the value assigned to each voxel represents the likelihood that this voxel belongs to the ROI. Therefore, voxels within or nearby the ROI boundaries have maximum labeling uncertainty. Second, the user interactive map is constructed such that it spatially reflects the user’s certainty that the soft labels of the current (automatic) segmentation estimate should be altered. Finally, the probability that each of the image voxels ‘flips’ its ROI-background assignment is determined by the user-certainty map by considering it as voxel-wise Bernoulli parameters.The suggested probabilistic framework leads to a flexible and tolerable interaction, taking into account occasional user mistakes. Note that the user does not edit the segmentation. Instead, our model provides an elegant framework to refine segmentation by resolving voxel annotation ambiguities, through a user-machine dialogue. The user term in the unified cost functional is constructed such that voxels that are not within the user’s regions of influence do not contribute to the evolution of the segmentation (due to the user). In other words, ‘neglecting’ a region is not interpreted as ‘supporting’ its segmentation. Therefore, regions that are incorrectly labeled by the current segmentation and are missed by the user, can be easily corrected in a subsequent interaction step. In contrast, regions that are assigned with high probability to either the ROI or the background by the current soft segmentation are less likely to alter their assignment, if marked by mistake by the user. It turns out that this mechanism, while requiring user persistency in high confidence regions (in which apparently intensity-based segmentation should work well) eventually leads to a reduction in user effort.Our user-interactive segmentation method is exemplified on the segmentation of three different datasets including cerebral hemorrhages (CH) in human brain CT scans; ventricles in degenerative mice brain MRIs and tumors in multi-modal human brain MRIs.We developed a GUI to allow a convenient interaction with the software. The tool was tested by our clinical collaborators who acknowledged its operating convenience and accuracy (measured subjectively by rating user satisfaction). Usually, not more than a couple of user interaction steps were needed in order to obtain almost a complete overlap with an independent, fully manual annotation.The accuracy of the results, measured via Dice scores (Dice, 1945), the method’s efficiency (which is inversely proportional to the total duration of the interactive segmentation), as well as the user’s workload (indicated by the average number of required interactions) were favorably compared to those obtained by three different state-of-the-art user interactive (UI) segmentation tools, namely the Grabcut (Rother et al., 2004), the TurtleSeg (Hamarneh et al., 2005; Poon et al., 2007, 2008; Top et al., 2010, 2011) and the ilastik (Sommer et al., 2011). Brain tumor segmentation results of multi-modal MRI scans were favorably compared to those reported in the BRATS paper (Menze et al., 2015) suggesting IS as a means to break the limits of fully automatic segmentation approaches, when dealing with extremely challenging data.The rest of the paper is organized as follows: In Section 2 we define the underlying probabilistic model followed by the introduction of the corresponding level-set formulation (Section 3). Implementation details are discussed in Section 4. Section 5 presents the experimental results. We summarize the paper and suggest future directions in Section 6. Finally, the GUI is described in the Appendix A.LetI1,…,IMbe an ensemble of gray-level images defined on the same 3D image domain Ω. Here, we assume thatI1,…,IMare aligned multi-modal scans of the same subject, acquired at the same time. Our goal is to extract the common ROI denoted by ω ∈ Ω. Let S: Ω → {0, 1} denote the corresponding, unknown binary voxel annotation of Ω. We assume that the observed imagesI1,…IMare generated by the segmentation S with probabilityp(I1,…IM|S;ψ),where ψ denotes the model parameters of the ensemble intensities. A fully automatic segmentation can be stated by the following MAP problem:(1)p(S|I1,…IM;ψ)∝p(I1,…,IM|S;ψ)p(S)=∏m=1Mp(Im|S,ψm)p(S),where the prior p(S) is used for regularization as we show below. Here, in accordance with Ashburner and Friston (2005), we further assume that the imagesI1,…,IMare conditionally independent and ψmare the univariate Gaussian mixture (GM) parameters modeling the intensity distribution of Im. For reasons that will be clarified in Section 4.3 and Section 5.3, ψ is not a multivariate GMM of the joint ensemble intensities.Incorporating the user input, the proposed segmentation framework is carried out iteratively, where at each interaction step k the user provides a set of seed points (using the mouse), after observing the previous segmentation estimate, i.e.Sk−1. We denote this set by Ukand use it to construct a continuous user map ηk: Ω → [0, 1]. We assume that Skis generated with probability p(Sk|ηk). Given the intensity distribution parameters of the observed images and the user input, i.e. ηk, we can define the posterior probability of Sk, using the Bayes theorem and the chain rule, as follows:(2)p(Sk|I1,…IM;ψ1,…,ψM,ηk)∝∏m=1Mp(Im|Sk,ψm)p(Sk;ηk).Fig. 1graphically illustrates the probabilistic model defined above.Following Cremers et al. (2007b) and others, we define a Gibbs distributionp(Sk)∝exp(−βE(Sk)),where β is a free parameter. This allows the derivation of a level-set cost functionalE(Sk)based on probabilistic principles. Using the logarithm of the expression in Eq. (2), we look forS^k=argmaxpSk(Sk|I1,…,IM;ψ1,…,ψM,ηk)by minimizing the following cost functional:(3)E(Sk)=−∑m=1Mlogp(Im|Sk;ψm)−logp(Sk;ηk).In the following, an explicit formulation of the image likelihood terms p(Im|Sk; ψm) and the user input term p(Sk; ηk) will be presented. We assume i.i.d. distribution of the image voxels. Therefore, the probability over the entire image domain will be presented by the product of probabilities of each voxel. A continuous form ofEbased on a level-set framework will be introduced next, followed by a gradient descent (GD) optimization process to estimate Sk.LetUk={xlk}l=1Lkdenote a collection of user seed points at a time step k, defined on the 3D image domain, wherexlk=(xlk,ylk,zlk)are the coordinates of a seed voxel and Lkis the number of seed points at the kth interaction step. The probabilistic UI framework described in the following, is a key contribution of the proposed framework. Letω^ink−1be the ROI estimate at stepk−1,and letω^outk−1≜Ω∖ω^ink−1define the background. We partition UkintoUink≜{xlk∈Uk|xlk∈ω^ink−1}andUoutk≜{xlk∈Uk|xlk∈ω^outk−1},which are the foreground and the background seed points, respectively. Let R define the extent of user influence and letDk(x,xlk)define the distance of a voxel x from a seed pointxlk,assuming that both voxels belong either to the foreground or to the background. The regions of user influence are all image voxels x ∈ Ω that belong to either of the following subgroups:Rink≜{x∈ω^ink−1|Dk(x,xlk)<R,wherexlk∈Uink−1},andRoutk≜{x∈ω^outk−1|Dk(x,xlk)<R,wherexlk∈Uoutk−1}. IfDk(x,xlk)is a Euclidean distance, thenRinkandRoutkare collections of Lkspheres with radius R. In the general case,Dk(x,xlk)is a geodesic distance and the sub-regions defined by the seed points may not be isotropic. For example, the distance of a voxel x to a user’s marked voxel can take into account image gradients and intensity differences, as in Arbelle et al. (2015). This extension is the subject of a future study. We note that the calculation ofDk(x,xlk)is performed for every seed pointxlkusing fast marching algorithm (Sethian, 1999), such that in case of an overlap the nearest point (minimal distance) is chosen, i.e.xmink=argminxlkDk(x,xlk).We now construct a continuous probability map ηk: Ω → [0, 1] that represents the user’s ‘belief’ about the appropriateness of the current segmentation. Since the user provides input in regions of disagreement, then according to the user,RinkandRoutkshould be reassigned to the background and to the ROI (respectively). Letf(d,λ)=exp−λd,where λ is set such thatlimd→Rf(d,λ)=0.Again, ifd=Dk(x,xlk)is an Euclidean distance, the operation of f is equivalent to smoothing the user seed points by a Gaussian kernel. We now define the user’s probability map as follows:(4)ηk(x)={(1+f(min(0,Dk(x,xlk)−R),λ))−1ifx∈ω^ink−1(1+f(max(0,R−Dk(x,xlk)),λ))−1otherwiseThe map ηk(x) represents the user’s certainty that a voxel x should be assigned to the ROI. It is built such that ηk(x) ∈ [0, 0.5] and ηk(x) ∈ [0.5, 1], in the foreground and the background regions, respectively. For example, if ηk(x) < p and p < 0.5, then x is by construction an ROI voxel and the user ‘believes’ it should be assigned to the ROI with probability p and with probabilityq=1−pto the background (q > 0.5). Note thatηk(x)=0.5for all image voxels that do not belong to the regions of influence. This can be interpreted as being ‘neutral’ about the current assignment or having maximum uncertainty. In contrast,η(x)=0if x is an ROI seed point andη(x)=1if x is a background seed point.Fig. 2illustrates the main concept. Fig. 2a presents an example image (a 2D slice of a 3D MRI brain scan of a low grade glioma patient) along with a segmentation estimate (green contour) of the ROI (tumor) and three user clicks in regions of disagreement. The red stars mark the centers of regions within the the ROI that according to the user should be excluded. The yellow star marks the center of a region in the background that should be part of the ROI according to the user. The respective probability map, η is visually demonstrated by the gray-level image shown in Fig. 2b. The region within the ROI contour (shown here in green as a reference) ranges between zero (for the ROI seed voxels) to 0.5, while the region outside the ROI ranges from 1 (for the background seed voxels) to 0.5.The user term is a function of the logarithm of the Bernoulli distribution(5)EUI=−logp(Sk;ηk)=−p(Sk)log(ηk)−(1−p(Sk))log(1−ηk).Note that while voxels x that are outside the regions of user influence (ηk(x)=0.5) do not contribute to the cost; the added cost of voxels within those regions (either inRinor inRout) grows and approaches infinity as the voxels are closer to the seed points.We use mixtures of Gaussians to model the intensity distribution within and outside the ROI of each image Im. For clarity, the subscript m is omitted from the distribution parameters. Letψmin={μiin,σiin,αiin}i=1Ninand letψmout={μiout,σiout,αiout}i=1Nout,where Ninand Noutare the respective numbers of Gaussians and αiare the weights. For each image voxel x with intensity Im(x) we define(6)Pin(Im(x);ψmin)∝∑i=1Ninαiinexp(−(Im(x)−μiin)22(σiin)2)Pout(Im(x);ψmout)∝∑i=1Noutαioutexp(−(Im(x)−μiout)22(σiout)2).The image likelihood cost, for a given ROI, could be defined as follows:(7)EIL=−∑m=1M[∑x∈ωklogPin(Im(x);ψmin)+∑x∈Ω∖ωklogPout(Im(x);ψmout)]=−∑m=1M∑x∈Ω[Sk(x)logPin(Im(x);ψmin)+(1−Sk(x))logPout(Im(x);ψmout)]Note, that both the ROI (or equivalently, the segmentation Sk) and the intensity distribution parameters {ψm} are unknown, and are therefore estimated via an alternating minimization scheme that is detailed in the following.We use a level-set framework to formulate the UI-segmentation problem (Osher and Sethian, 1988). Let ϕkdefine a level-set function, such that∂ωk={x|ϕk(x)=0}denotes the estimated common ROI boundaries inI1,…,IMat step k, and let ωkdenote the corresponding ROI domain. As in Chan and Vese (2001), the binary segmentation Skcan be represented by applying the Heaviside function to ϕk, i.e. H(ϕk). Adopting the probabilistic approach in Riklin Raviv et al. (2010), we use the logistic regression sigmoid, which can be used as a regularized form of the Heaviside function, to represent the soft segmentation p(Sk):(8)Hϵ(ϕk)=12(1+tanh(ϕk2ϵ))=11+e−ϕk/ϵ,where ϵ can be use to determine the fuzziness of the estimated ROI boundaries (Riklin Raviv et al., 2010). As in  Riklin Raviv et al. (2010), we now define the level-set function ϕk, as follows:(9)ϕk(x)≜ϵlogit(p)=ϵlogp(x∈ωk)1−p(x∈ωk)=ϵlogp(x∈ωk)p(x∈Ω∖ωk).It can be shown by substitution of Eq. (9) into Eq. (8) that Hϵ(ϕk) is equivalent to p(x ∈ ωk). This relation is fundamental in the proposed probabilistic level-set framework. The level-set function ϕkis developed in an iterative manner, using a gradient descent framework. Note that k counts the number of user interaction steps, as opposed to the iteration index of the gradient descent process, which will be denoted by τ. Usually, not more than2−5interaction steps are needed. We next use the equivalence between p(Sk) and Hϵ(ϕk), and the continuous forms of the equations above to resolve the segmentation problem via a level-set based gradient descent optimization.The proposed cost functional for a level-set based segmentation includes an image likelihood term -EIL,a regularization term -EREG,and a user interaction termEUI:(10)E(ϕk|I1,…IM,ψ1,…ψM,ηk)=EIL(ϕk|I1,…IM,ψ1,…ψM)+EUI(ϕk;ηk)+EREG(ϕk).Adopting a continuous formulation and a soft definition of the ROI, the sum over the image voxels, i.e. ∑x∈ Ω, is replaced with an integration ∫Ωdx, and the binary segmentation Sk(x) is replaced by the probability that a voxel x belongs to the ROI: p(Sk(x)) or, using the level-set formulation, by Hϵ(ϕk(x)). The explicit form of the energy functional in Eq. (10) is as follows:(11)E(ϕk)=−∫Ω{WUI[Hϵ(ϕk(x))logηk(x)+Hϵ(−ϕk(x))log(1−ηk(x))]+WIL∑m=1M[Hϵ(ϕk(x))logpin(Im;ψmin)+Hϵ(−ϕk(x))logpout(Im;ψmout)]+WLEN|∇Hϵ(ϕk(x))|}dx.The last term in Eq. (11), i.e. |∇Hϵ(ϕk(x))|, is known in the level-set literature as the smoothness or regularization term (Chan and Vese, 2001). The relation to−logp(Sk)where p(Sk) is the prior in Eq. (1) is shown in Riklin Raviv et al. (2010). The weights WUI, WIL, WLENare positive scalars that balance the contribution of each term. Setting these weights is discussed in Section 4.2.We look for the segmentation ϕkthat optimizes the energy functional (Eq. (11)) via a gradient descent process. The gradient descent step∂ϕk∂τis derived from the first variation of the functional above and determines the evolution of the segmentation(12)∂ϕk∂τ=δϵ(ϕk){WUI[log(ηk)−log(1−ηk)]+WIL∑m=1M[logpin(Im;ψmin)−logpout(Im;ψmout)]+WLENdiv(∇ϕk|∇ϕk|)},where div is the divergence operator and δϵ(ϕk) is the derivative of Hϵ(ϕk) with respect to ϕk:δϵ(ϕ)=dHϵ(ϕ)dϕ=14ϵsech2(ϕ2ϵ).The scalar mapspin(Sk−1,ηink),pout(Sk−1,ηoutk)are updated once after each UI step, and remain constant throughout the gradient descent iterations. The GMM of the image intensitiesψminandψmoutare re-estimated in correspondence toϕτk,which determines the ROI boundaries, using expectation maximization (EM) (Dempster et al., 1977). Therefore,pin(Im;ψmin),pout(Im;ψmout)are calculated at every iteration, based on the updated intensity distribution parameters.

@&#CONCLUSIONS@&#
