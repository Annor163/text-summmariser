@&#MAIN-TITLE@&#
Robustness in nonsmooth nonlinear multi-objective programming

@&#HIGHLIGHTS@&#
Defining robust efficiency for nonlinear multi-objective optimization problems.Showing that, robust efficient solutions are proper solutions (under some assumptions).Providing a problem for calculating a robustness radius.Comparing the newly-defined robustness notion with two existing ones.Characterizing robust solutions and studying alterations of objectives.

@&#KEYPHRASES@&#
Multiple objective programming,Robust solution,Nonsmooth optimization,Clarke generalized gradient,Proper efficiency,

@&#ABSTRACT@&#
Recently Georgiev, Luc, and Pardalos (2013), [Robust aspects of solutions in deterministic multiple objective linear programming, European Journal of Operational Research, 229(1), 29–36] introduced the notion of robust efficient solutions for linear multi-objective optimization problems. In this paper, we extend this notion to nonlinear case. It is shown that, under the compactness of the feasible set or convexity, each robust efficient solution is a proper efficient solution. Some necessary and sufficient conditions for robustness, with respect to the tangent cone and the non-ascent directions, are proved. An optimization problem for calculating a robustness radius followed by a comparison between the newly-defined robustness notion and two existing ones is presented. Moreover, some alterations of objective functions preserving weak/proper/robust efficiency are studied.

@&#INTRODUCTION@&#
Due to perturbations and partial knowledge, in most practical optimization problems we are faced with uncertainty. Popular approaches for dealing with uncertainty are stochastic optimization, robust optimization and sensitivity analysis. Each approach has its own advantages and disadvantages (Ben-Tal, Ghaoui, & Nemirovski, 2009; Bertsimas, Brown, & Caramanis, 2011). From the late 1990s, robust single-objective optimization has been being investigated. In robust single-objective optimization, we try to find a feasible point that optimizes the worst case counterpart of the objective function, see Ben-Tal et al. (2009) and Bertsimas et al. (2011) for more formal definition and more details about robustness in single-objective optimization.The robustness of single-objective optimization has received considerable attention, but the robustness of multi-objective optimization has not been frequently considered. See e.g. Deb and Gupta (2006), Ehrgott, Ide, and Schöbel (2014), Kuroiwa and Lee (2012) and Georgiev, Luc, and Pardalos (2013), where this notion is studied from different standpoints. Deb and Gupta (2006) focused on two definitions for robustness. In the first definition, they call an efficient solution to be robust if it optimizes the mean of all objective functions. In the second definition, the objectives do not change, but a constraint is added which restricts the absolute difference between the mean and original objective functions. There, the efficient solutions of the modified problem are called robust. Ehrgott et al. (2014) extended the worst case robustness notion from single-objective optimization to multi-objective programming. They further introduced some scalarization methods which are able to produce a robust solution in worst-case sense. Bokrantz and Fredriksson (2014) studied more methods of scalarization with respect to the definition given by Ehrgott et al. (2014). Moreover, Fliege and Werner (2014) utilized a special robustness concept in portfolio optimization.In a recent work, Georgiev et al. (2013) have defined the robustness for linear multi-objective programming problems from a different point of view. They have considered a perturbation standpoint, and have defined an efficient solution as a robust solution if it remains efficient for small perturbations in the coefficients of the objective functions. Georgiev et al. (2013) studied their definition considering different kinds of perturbations, including changing the objectives’ coefficients and adding a new objective function. They obtained necessary and sufficient conditions and presented various nice properties of the robust solutions in linear cases. Goberna, Jeyakumar, Li, and Vicente-Pérez (2015) extended Georgiev et al.’s definition for linear multi-objective optimization problems under perturbations of the coefficients of both objective functions and constraints.In this paper, we extend the definition given by Georgiev et al. (2013) to nonlinear multi-objective programming problems. We show that, under the compactness of the feasible set or convexity, the set of robust efficient solutions is a subset of the set of proper efficient solutions. Some necessary and sufficient conditions for robust solutions with respect to the tangent cone and non-ascent directions, under appropriate assumptions, are given. A robustness radius is calculated. The relationships between the robustness notion considered in the present paper and two worst case-based definitions, studied by Fliege and Werner (2014) and Ehrgott et al. (2014), are highlighted. Two kinds of modifications in the objective functions are dealt with and the relationships between the weak/proper/robust efficient solutions of the problems, before and after the perturbation, are established. Some examples, to clarify the theoretical results, are given.The rest of the paper unfolds as follows. In Section 2, some preliminaries are given. In Section 3, robustness is defined, its relationship with proper efficiency is established, and some necessary and sufficient conditions are proved. Section 4 is devoted to the robustness radius calculation. Section 5 contains some results about connections between the new and previously defined robustness definitions. In Section 6, we study some alterations of the objective functions that preserve weak/proper/robust efficiency.This section is devoted to some preliminaries. For a setΩ⊆Rn,we use the notations coΩ, intΩ and clΩ to denote the convex hull, the interior and the closure of Ω, respectively. For a vectord∈Rn,dTstands for transpose of d, and for two vectors x and y, xTy denotes the standard inner product inRn. ForΩ⊆Rn,Pos(Ω):={y∈Rn:∃m∈N;y=∑i=1mλiyi,λi≥0,yi∈Ω,i=1,2,…,m}.IfΩ1,…,Ωl⊆Rnare convex sets, it can be shown thatPos(⋃i=1lΩi)={∑i=1lλidi:di∈Ωi,λi≥0,i=1,2,…,l}.ForΩ⊆Rnand x ∈ clΩ, the tangent cone of Ω at x, denoted byTΩ(x¯),is defined byTΩ(x¯)={d∈Rn:∃({xi}⊆Ω,{ti}⊆R);ti↓0,xi−x¯ti⟶d}.Forx,y∈Rn,the vector inequality x < y means xi< yifor alli=1,2,…,n. Analogously, ≤, >, and ≥ are defined componentwise.Consider the following multi-objective optimization problem:(1)minf(x)s.t.x∈Ω,thatΩ⊆Rnis nonempty andf:Ω→Rpwith p ≥ 2. In fact,f(x)=(f1(x),f2(x)…,fp(x))for each x. In the whole of this paper, we assume that fiis locally Lipschitz for each i, though some of the results given in this paper are valid without this assumption.Definition 2.1The vectorx¯∈Ωis called an efficient solution of Problem (1) if there exists no x ∈ Ω such thatf(x)≤f(x¯)andf(x)≠f(x¯).The vectorx¯∈Ωis called a weak efficient solution of Problem (1) if there exists no x ∈ Ω such thatf(x)<f(x¯).In order to obtain efficient solutions with bounded trade-offs, Geoffrion (1968) suggested restricting attention to efficient solutions that are proper in the sense of the following definition.Definition 2.3(Geoffrion 1968): The vectorx¯∈Ωis called a proper efficient solution of Problem (1) if it is efficient and there is a real number M > 0 such that for all i and x ∈ Ω satisfyingfi(x)<fi(x¯)there exists an index j such thatfj(x¯)<fj(x)andfi(x¯)−fi(x)fj(x)−fj(x¯)≤M.Proper efficiency has been defined in different senses and has been studied in several publications, including Benson (1979), Borwein (1977), Geoffrion (1968), Henig (1982) and Kuhn and Tucker (1951). In this paper, we use the above definition.Our robustness notion in the present work is based on the matrix perturbations. Hence, we need a matrix norm. For an m × n matrixC=[cij],the Frobenius norm is as∥C∥=(∑i,j|cij|2)1/2.Although we use this norm, almost all of the provided results are valid with any matrix norm.In this paper, Clarke generalized gradient (Clarke (2013)) is used in the presence of nonsmooth data.Definition 2.4Letf:Rn→Rbe Lipschitz near a given pointx∈Rn. The generalized directional derivative of f at x in the direction v, denoted by f°(x; v), is defined asf∘(x;v):=lim supt↓0y→xf(y+tv)−f(y)t,where y is a vector inRnand t is a positive scalar.Letf:Rn→Rbe Lipschitz near a given pointx∈Rn. The generalized gradient of f at x, denoted by ∂f(x), is defined as∂f(x):={ζ∈Rn:f∘(x;v)≥ζTv,∀v∈Rn}.Iff:Rn→Ris convex, then ∂f(x) reduces to the subgradient set in classic convex analysis (Clarke, 2013):{ζ∈Rn:f(y)−f(x)≥ζT(y−x),∀y∈Rn}.Definition 2.6(Clarke 2013): A functionh:Rn→Ris called regular atx¯ifh∘(x¯;d)exists andh∘(x¯;d)=limt↓0h(x¯+td)−h(x¯)tfor eachd∈Rn.Each convex function is a regular function (Clarke, 2013).We start this section by introducing the concept of robust solution for nonlinear multi-objective optimization Problem (1). This definition extends Definition 3.1 in Georgiev et al. (2013).Definition 3.1Letx¯∈Ωbe an efficient solution of Problem (1).x¯is called a robust efficient solution if there existsϵ>0such that for any p × n matrix C with∥C∥<ϵ,the vectorx¯is an efficient solution for(2)minf(x)+Cxs.t.x∈Ω.The following theorem presents a nice property of the robust efficient solutions. It proves that the set of robust efficient solutions is a subset of properly efficient solutions under the compactness of the feasible set.Theorem 3.1Let Ω be compact. Ifx¯is a robust efficient solution of Problem (1), thenx¯is a proper efficient solution of Problem (1).Supposex¯is not a proper efficient solution. Then, there exist {xi}⊆Ω, increasing sequence {Mi} of positive real numbers, andk∈{1,…,p},such thatMi⟶+∞,(3)fk(xi)<fk(x¯)∀i,and(4)fk(x¯)−fk(xi)fj(xi)−fj(x¯)>Miforeachj∈{1,…,p}withfj(xi)>fj(x¯).Since Ω is compact, without loss of generality, we may assume that {xi} converges to somex^∈Ω. Also, we defineQi={j:fj(xi)>fj(x¯)}.This set is nonempty becausex¯is efficient. Without loss of generality, by choosing an appropriate subsequence, Qiis a constant set for all i indices. So, we denote it by Q. Two cases may occur forx^; either it is equal tox¯or not. We consider these two possible cases and we get a contradiction in each case.As robustness ofx¯,there exists someϵ>0such thatx¯is an efficient solution of Problem (2) for any matrix Cp × nwith∥C∥<ϵ. Letx^≠x¯. We can choose matrix∥C˜p×n∥<ϵsuch that(5)C˜j(x^−x¯)<−2δ,∀j∈Q,(6)C˜j=0,∀j∈{1,…,p}∖Q,for some δ > 0 (C˜jdenotes the jth row ofC˜). Since f is bounded on Ω, from (4), we havefj(xi)⟶fj(x¯)for each j ∈ Q asi⟶+∞. Therefore, for sufficiently large i values, we havefj(xi)−fj(x¯)−δ<0.Hence, by (5), for sufficiently large i values, we get(7)fj(xi)+C˜jxi<fj(x¯)+C˜jx¯−δ<fj(x¯)+C˜jx¯,∀j∈Q.Also, by (6) and the definition of Q, for sufficiently large i values, we have(8)fj(xi)+C˜jxi≤fj(x¯)+C˜jx¯,∀j∈{1,…,p}∖Q.Inequalities (7) and (8) contradict the robustness ofx¯.Now, we consider the latter case, i.e.x¯=x^. We assume that the sequence{xi−x¯∥xi−x¯∥}converges to some nonzero vector d. We chooseC˜p×nsatisfying∥C˜p×n∥<ϵand(9)C˜jd<−2δ,∀j∈Q,(10)C˜j=0,∀j∈{1,…,p}∖Q,for some δ > 0. Assume that Ljis the Lipschitz constant of fjon a neighbourhood ofx¯. By (4), for sufficiently large i values, we get(11)fj(xi)−fj(x¯)<Lk∥x¯−xi∥Mi<δ∥x¯−xi∥.Therefore, by (9)–(11) we get inequalities (7) and (8) in this case as well. These contradict the robustness ofx¯and the proof is complete.□The converse of the above theorem does not hold necessarily, even for the linear case; see Example 3.2 in Georgiev et al. (2013).The following example shows that the compactness assumption of Ω in Theorem 3.1 is essential.Example 3.1Consider the multi-objective optimization problemmin(−x,x3)s.t.x∈R.It is not difficult to see thatx¯=1is a robust efficient solution (considerϵ=0.1), while the problem does not have any properly efficient solution.Now we are going to provide a characterization for robust efficient solutions with respect to the non-ascent directions of the objective function and the tangent cone of the feasible set.Definition 3.2d∈Rnis called a non-ascent direction of f atx¯if dTξ ≤ 0, for eachξ∈∂fi(x¯)and eachi∈{1,2,⋯,p}. Hereafter,G(x¯)denotes the set of all non-ascent directions of f atx¯.The following theorem presents a necessary condition for robustness.Theorem 3.2Ifx¯is a robust efficient solution to Problem (1), thenTΩ(x¯)∩G(x¯)={0}.We prove it by contradiction. Suppose that0≠d∈G(x¯)∩TΩ(x¯). By robustness ofx¯,there exists anϵ>0such thatx¯is an efficient solution to problem (2) for any matrix Cp × n, with∥C∥<ϵ. We choose a matrixC˜p×nsuch that(12)∥C˜∥<ϵandC˜d<−2δefor some δ > 0 (e is a column vector with all components equal to one). Sinced∈TΩ(x¯),(13)∃({xi}⊆Ω,ti↓0);xi−x¯ti→d.Therefore, from (12) and (13), for sufficiently large i, we have(14)C˜(xi−x¯ti)<−δewhich impliesC˜xi+tiδe<C˜x¯.Using the mean value theorem (Theorem 10.17 in Clarke, 2013), for each i,(15)f(xi)=f(x¯)+ξiT(xi−x¯)where ξiis an n × p matrix whose jth column belongs to∂fj(x˜ij)for somex˜ij∈(x¯,xi). Thus,f(xi)+C˜xi+tiδe<f(x¯)+ξiT(xi−x¯)+C˜x¯⇒f(xi)+C˜xi+ti(δe−ξiT(xi−x¯ti))<f(x¯)+C˜x¯.Sincex˜ij⟶x¯asi⟶+∞and f is locally Lipschitz atx¯,by Proposition 10.2 in Clarke (2013), the sequence {ξi} is bounded. Hence,ξi⟶ξfor someξ∈∂f(x¯),because of Proposition 10.10 in Clarke (2013). Thus, ξTd ≤ 0. Therefore, for sufficiently large i values,ξiT(xi−x¯ti)<δe.Thus, we getf(xi)+C˜xi<f(x¯)+C˜x¯,which contradicts the robustness ofx¯,and completes the proof.□The condition given in the above theorem is necessary for robustness and it is not sufficient in general case. The following example clarifies this:Example 3.2Consider the multi-objective optimization problemmin(f1(x),f2(x))s.t.x∈R,in whichf1(x):=x,f2(x):={−x|x|<1,−x(13)|x|≥1.Letx¯=2. At this point we haveTΩ(x¯)=RandG(x¯)={0}. It is not difficult to see thatx¯=2is an efficient solution to the above problem, while for anyϵ>0it is not an efficient solution tomin(f1(x),f2(x)+ϵ2x)s.t.x∈R,because for eachϵ>0,by settingxϵ=min{−125,−1ϵ3},we havef1(xϵ)<f1(2)andf2(xϵ)≤f2(2).As shown by the above example, the necessary condition given in Theorem 3.2 may not be sufficient for robustness in general. Theorem 3.3 establishes that this condition is sufficient under the convexity assumption.Theorem 3.3Let Ω be a closed and convex set andfi(i=1,…,p)be convex. Assume thatx¯is an efficient solution to Problem (1).x¯is a robust efficient solution to Problem (1) if and only ifTΩ(x¯)∩G(x¯)={0}.The ``only if” part is derived from Theorem 3.2. For ``if” part, suppose thatx¯is not a robust efficient solution. Thus there exist a sequence {Ci} of p × n matrices and a sequence {xi}⊆Ω such that Ci→ 0,(16)f(xi)+Cixi≤f(x¯)+Cix¯,andf(xi)+Cixi≠f(x¯)+Cix¯.Set(17)di:=xi−x¯∥xi−x¯∥.Two cases may occur for the sequence {xi}. Either it has a subsequence convergent tox¯or it does not have any subsequence convergent tox¯. We consider these two possible cases and we get a contradiction in each case.In the first case, without loss of generality, we assume thatxi→x¯. From the convexity of f, for anyξ∈∂f(x¯)we have,(18)f(xi)≥f(x¯)+ξT(xi−x¯),that ξ is an n × p matrix whose jth column belongs to∂fj(x¯). Therefore, due to (16), we have(19)∥xi−x¯∥−1(ξT(xi−x¯)+Ci(xi−x¯))≤0.Without loss of generality, we can assume that, di→ d for somed∈Rpwith∥d∥=1and it is obvious thatd∈TΩ(x¯). Moreover, from (19) we conclude thatd∈G(x¯). Thusd∈G(x¯)∩TΩ(x¯). This makes a contradiction.Now, we consider the latter case: {xi} does not have any subsequence convergent tox¯. Therefore, without loss of generality, there exist an r > 0 such that∥xi−x¯∥>r. On the other hand, di→ d for some nonzerod∈TΩ(x¯). Since Ω is convex and closed, for each itdi+x¯∈Ω,∀t∈[0,r],td+x¯∈Ω,∀t∈[0,r].Thus,0≠d∈TΩ(x¯). Suppose that {ti} is a sequence of scalars in [0, r] that converges to zero. By convexity of f and due to (16) and (17), we getf(x¯+tidi)≤(1−ti∥xi−x¯∥)f(x¯)+ti∥xi−x¯∥f(xi)≤f(x¯)+ti∥xi−x¯∥Ci(x¯−xi).Since Ci→ 0, from the convexity of f and the above statement, we have ξTd ≤ 0 that ξ is an n × p matrix whose jth column belongs to∂fj(x¯). Therefore0≠d∈G(x¯)∩TΩ(x¯). This makes a contradiction and completes the proof.□In the rest of this section, we consider a multi-objective optimization problem whose feasible set is defined by some constraint functions. Consider(20)minf(x)s.t.gi(x)≤0,i=1,2,…,m,wheref:Rn→Rpis the objective function (i.e.f(x)=(f1(x),…,fp(x))and gjfunctions define the constraints. Hereafter, whenever we use the Clarke subdifferential for gjfunctions, we assume that these functions are locally Lipschitz.For a feasible pointx¯,the index setA(x¯)is defined byA(x¯)={j∈{1,2…,m}:gj(x¯)=0}.In the following, we are going to provide a characterization for robust efficient solutions of Problem (20). The following constraint qualification (CQ) helps us in the sequel.Definition 3.3We say that the constraint qualification (CQ) holds atx¯if0∉co{⋃j∈A(x¯)∂gj(x¯)}.Ifx¯is a robust efficient solution which satisfies (CQ), thenPos(⋃i=1p∂fi(x¯))+Pos(⋃i∈A(x¯)∂gi(x¯))=Rn.For simplicity, we setAx¯=Pos(⋃i=1p∂fi(x¯))+Pos(⋃i∈A(x¯)∂gi(x¯)).It can be seen that under the assumptions of the theorem,{d:gi∘(x¯;d)≤0,∀i∈A(x¯)}⊆TΩ(x¯);see Theorem 10.42 in Clarke (2013). Therefore, according to Theorem 3.2, the system below has no solutiond∈Rn:ξTd≤0,∀ξ∈∂fi(x¯),∀i∈{1,…,p}ξTd≤0,∀ξ∈∂gi(x¯),∀i∈A(x¯)d≠0.Hence, the following system has no solutiond∈Rn:ξTd≤0,∀ξ∈∂fi(x¯),∀i∈{1,…,p}ξTd≤0,∀ξ∈∂gi(x¯),∀i∈A(x¯)d1>0.Using the semi-infinite Farkas’ theorem (see Corollary 3.1.3 in Goberna and Lopez, 1998), we havee1∈cl(Ax¯).Similarly, it can be shown that±ei∈cl(Ax¯)for eachi∈{1,2,…,p}. Here, eidenotes the i-th unit vector. Therefore,cl(Ax¯)=Rn.SinceAx¯is a convex set whose closure is equal toRn,we haveAx¯=Rnand the proof is completed.□Assume thatfi(i=1,…,p)andgj(j=1,…,m)in Problem (20) are continuously differentiable. Ifx¯is a robust efficient solution of Problem (20) which satisfies (CQ), thenPos{∇f1(x¯),…,∇fp(x¯)}+Pos{∇gi(x¯):i∈A(x¯)}=Rn.Theorem 3.6 provides a converse version of Theorem 3.4. Theorems 3.4 and 3.6 extend Theorem 3.4 in Georgiev et al. (2013).Theorem 3.6Letfi(i=1,…,p)andgj(j=1,…,m)in Problem (20) be convex. Ifx¯is an efficient solution andPos(⋃i=1p∂fi(x¯))+Pos(⋃i∈A(x¯)∂gi(x¯))=Rn,thenx¯is a robust efficient solution to Problem (20).We prove the theorem by contradiction. Suppose thatx¯is not robust. Then, according to Theorem 3.3, there exists a nonzero vectord¯∈TΩ(x¯)∩G(x¯). From the convexity assumption, we getξTd¯≤0for eachξ∈Pos(∂gi(x¯))and eachi∈A(x¯).Also,ξTd¯≤0for eachξ∈Pos(∂fi(x¯))and eachi∈{1,2,…,p},because ofd¯∈G(x¯). On the other hand, by the assumption of the theorem,d¯=∑i=1puiξi+∑j∈A(x¯)vjζjfor some ui, vj≥ 0,ξi∈Pos(∂fi(x¯)),andζj∈Pos(∂gj(x¯)). Therefore,d¯Td¯≤0. Hence we getd¯=0which makes a contradiction.□Assume thatfi(i=1,…,p)andgj(j=1,…,m)in Problem (20) are differentiable and convex. Ifx¯is an efficient solution andPos{∇f1(x¯),…,∇fp(x¯)}+Pos{∇gi(x¯):i∈A(x¯)}=Rn,thenx¯is a robust efficient solution of Problem (20).Although the compactness assumption is essential in Theorem 3.1 (see Example 3.1), the following result shows that Theorem 3.1 remains valid without compactness of the feasible set for convex programming problems with an appropriate (CQ).Theorem 3.8Letfi(i=1,2,…,p)andgj(j=1,2,…,m)be convex in Problem (20). Ifx¯is a robust efficient solution of Problem (20) which satisfies (CQ), thenx¯is a proper efficient solution of Problem (20).Suppose thatx¯is not a proper efficient solution. Therefore, there exist {xi}⊆Ω, increasing sequence {Mi} of positive real numbers, andk∈{1,…,p},such thatMi⟶+∞,(21)fk(xi)<fk(x¯)∀i,and(22)fk(x¯)−fk(xi)fj(xi)−fj(x¯)>Miforeachj∈{1,…,p}withfj(xi)>fj(x¯).DefineQi={j:fj(xi)>fj(x¯)}.This set is nonempty becausex¯is efficient. Without loss of generality, by choosing an appropriate subsequence, Qiis a constant set for all i indices. So, we denote it by Q. Also, define the feasible set of Problem (20) asΩ={x∈Rn:gj(x)≤0,j=1,2,…,m}.Without loss of generality, we assume that the sequence{xi−x¯∥xi−x¯∥}converges to some nonzero vector d. Settingti=min{1i,∥xi−x¯∥}anddi=xi−x¯∥xi−x¯∥,we have ti↓0 andx¯+tidi∈Ω,according to the convexity assumptions. Henced∈TΩ(x¯).Due to the convexity assumption, we getfj(xi)≥fj(x¯)+ξT(xi−x¯),∀ξ∈∂fj(x¯),∀j∈{1,…,p}∖Q,⇒ξT(xi−x¯)≤0,∀ξ∈∂fj(x¯),∀j∈{1,…,p}∖Q,⇒ξTd≤0,∀ξ∈∂fj(x¯),∀j∈{1,…,p}∖Q.Moreover from (22) and the convexity of the objective functions, we haveξT(xi−x¯)≤fj(xi)−fj(x¯),∀ξ∈∂fj(x¯),∀j∈Q,<fk(x¯)−fk(xi)Mi,∀ξ∈∂fj(x¯),∀j∈Q,≤1MiηT(x¯−xi),∀η∈∂fk(x¯).ThusξTd≤0,∀ξ∈∂fj(x¯),∀j∈Q.Therefore,d∈TΩ(x¯)∩G(x¯). This is a contradiction because of Theorem 3.2, and the proof is complete.□This remark indicates that the robust solution studied in the present paper may not exist in some special cases, though these solutions (if exist) have nice properties as compared to non-robust points. An efficient point is robust if it stays efficient under small linear perturbations. Let us assume that fiand gjfunctions are differentiable here. Under some CQs and appropriate assumptions, the KKT/FJ condition∑i=1pλi∇fi(x¯)+∑j∈A(x¯)μj∇gj(x¯)=0for some nonnegative μj’s and some nonnegative λi’s (not all zero), is necessary for the efficiency ofx¯. When some objective function, say f1, is perturbed, then∇f1(x¯)is alerted and hence to preserve the KKT/FJ condition (efficiency ofx¯), the Lagrangian multiplier(s) of some other objective function(s) or some constraint function(s) should be changed. Hence, at least one other objective function or at least one constraint function is required for robustness, i.e.m+p≥2. Thus, there is not any robust solution for unconstrained single-objective problems. To show this analytically, letx¯be an arbitrary optimal solution tominx∈Rnh(x)whereh:Rn⟶R. Then∇h(x¯)=0which implies∇h(x¯)+C≠0for each C ≠ 0. Therefore,x¯is not optimal tominx∈Rnh(x)+Cxfor each C ≠ 0. Hence,x¯is not robust forminx∈Rnh(x). Thus this unconstrained problem does not have any robust solution.Now, consider an unconstrained multi-objective programming problemminx∈Rnf(x)withf:Rn⟶Rpand p ≥ 2. Here,m=0. Ifx¯is a robust solution, then by Corollary 3.5,Pos{∇f1(x¯),…,∇fp(x¯)}=Rn,and hencep≥n+1.For constrained problem (20) satisfying the assumptions of Corollary 3.5, ifx¯is a robust solution, thenp+m≥n+1. It is not restrictive for practical cases, because in practice the problem has at least 2n constraints due to the lower and upper bounds of variables.The necessary condition introduced in Theorem 3.2 provides a tie-in to the gradient-like descent methods existing in the literature for solving vector optimization problems; see Drummond and Iusem (2004) and Fliege and Svaiter (2000). Extending these numerical tools to generate robust solution(s) can be worth studying in future.In this short section, we compute a radius of robustness. For a given vectora∈Rp,the vectora+is obtained from a by substituting all negative components by zero. It is not difficult to show that∥a+∥is equal to the distance from a to−R+p={x∈Rp:x≤0}.Lemma 4.1Let Ω be a closed and convex set and fi (i=1,…,p) be convex. Letd∈TΩ(x¯)with∥d∥=1. Ifx¯is a robust solution of Problem (1), then∥(f′(x¯;d))+∥>0and it is equal to the optimal value of the following problem:sup{t:f′(x¯;d)+tCd∉−R+p,∀∥C∥≤1}.First we show thatf′(x¯;d)∉−R+p. Iff′(x¯;d)≤0,then due to the convexity of f, we haved∈G(x¯),which makes a contradiction according to Theorem 3.2. The proof of the second part is similar to that of Lemma 4.2 in Georgiev et al. (2013).□Under the assumptions of Lemma4.1, the optimal value of the following problem is positive and it is a robustness radius forx¯.min∥(f′(x¯;d))+∥s.t.d∈TΩ(x¯),∥d∥=1Let ρ be the optimal value of the given problem. Thus, by Lemma 4.1, ρ > 0. Now, we show that ρ is a robustness radius forx¯. If this is not a robustness radius, then there exist some xo∈ X and some matrix Cosuch that ‖Co‖ < ρ, and(23)f(xo)+Coxo≤f(x¯)+Cox¯,f(xo)+Coxo≠f(x¯)+Cox¯.Settingdo=xo−x¯∥xo−x¯∥,we have∥do∥=1anddo∈TΩ(x¯)due to the convexity of Ω. Furthermore, by convexity of f, we getf′(x¯;do)+Codo=f′(x¯;xo−x¯)∥xo−x¯∥+Codo≤f(xo)−f(x¯)∥xo−x¯∥+Co(xo−x¯)∥xo−x¯∥.Therefore, according to (23), we get(24)f′(x¯;do)+Codo∈−R+p.Defining(25)ρo=sup{t:f′(x¯;do)+tCdo∉−R+p,∀∥C∥≤1},we have ρ ≤ ρo. Furthermore, for each t ∈ (0, ρo) and each C with ‖C‖ ≤ 1, we havef′(x¯;do)+tCdo∉−R+p.This is in contradiction with (24) by settingt=∥Co∥andC=Co∥Co∥,and the proof is complete.□It can be seen that, the optimal value of the optimization problem considered in the above theorem is equal to the maximum robustness radius if one furthermore assumes the equality of the tangent cone and the cone of feasible directions.There are some definitions for robustness in the multi-objective programming literature that optimize the worst case of the objective functions. In the following, we highlight the relationships between the robustness notion considered in the present paper and two worst case-based definitions studied by Fliege and Werner (2014) (FW in brief) and Ehrgott et al. (2014) (EIS in brief). See also Georgiev et al. (2013) for some comparison.Let U be an uncertain set andΩ⊆Rnbe the set of feasible solutions. Also, letfi:Ω×U⟶Rfori=1,2,…,pbe objective functions. For a feasible decision variable vector x ∈ Ω and a u ∈ U, the value of objective function is denoted by f(x, u). DefineF:Ω→RpbyFi(x)=maxu∈Ufi(x,u),i=1,2,…,p.A feasible vectorx¯∈Ωis called a robust solution in the sense of FW if it is an efficient solution to the following multi-objective problemminF(x)s.t.x∈ΩThe following proposition provides a connection between the robustness notion studied in the present paper and that in the sense of FW.Proposition 5.1Letx¯be a robust solution of Problem (1), in the sense ofDefinition 3.1, with radiusϵ. Then considering anyϵ¯∈(0,ϵ),the vectorx¯is a robust solution in the sense of FW withU={Cp×n:∥Ci∥≤ϵ¯p,∀i=1,2,…,p}andf(x,C)=f(x)+Cx.By contradiction, assume that there exists some xo∈ Ω such thatF(xo)≤F(x¯)andF(xo)≠F(x¯). Ifx¯=0,thenF(x¯)=f(0)and hence by considering a p × n matrix C with∥Ci∥≤ϵ¯p,i=1,2,…,p,we get∥C∥=(∑i=1p∥Ci∥2)12<ϵ,andf(xo)+Cxo≤f(0),f(xo)+Cxo≠f(0).These relations contradict the robustness ofx¯(in the sense of Definition 3.1). Now, assume thatx¯≠0.ThenF(x¯)=f(x¯)+ϵ¯∥x¯∥pe,where e is a vector with all components equal to one. Now, we consider a p × n matrixC¯,withC¯i=ϵ¯p∥x¯∥x¯T. We getC¯∈Uandf(xo)+C¯xo≤F(xo)≤F(x¯)=f(x¯)+ϵ¯∥x¯∥pe=f(x¯)+C¯x¯Furthermore,∥C¯∥=∑i=1p∥C¯i∥2<ϵ. Hence,∥C¯∥<ϵ,andf(xo)+C¯xo≤f(x¯)+C¯x¯,f(xo)+C¯xo≠f(x¯)+C¯x¯.These relations contradict the robustness ofx¯(in the sense of Definition 3.1) and the proof is complete.□In a recently published paper, Ehrgott et al. (2014) (EIS in brief) have defined a feasible pointx¯∈Ωas a robust solution if there is no x ∈ Ω such thatfU(x)⊆fU(x¯)−(R≥p∖{0})thatfU(x)={f(x,u):u∈U}and U it is an uncertain set.Theorem 5.2Letx¯be a robust solution of (1), in the sense ofDefinition 3.1, with radiusϵ. Thenx¯is a robust solution in the sense of EIS withfU(x)={f(x)+Cx:∥C∥≤0.5ϵ}.By contradiction assume that(26)fU(xo)⊆fU(x¯)−(R≥p∖{0})for some xo∈ Ω. This implies that(27)∀C∈U∃C¯∈Us.t.f(xo)+Cxo≤f(x¯)+C¯x¯,f(xo)+Cxo≠f(x¯)+C¯x¯.Two vectors xoandx¯cannot be zero. Ifxo=0,then by (27),f(0)+C¯(0)≤f(x¯)+C¯x¯,f(xo)+C¯xo≠f(x¯)+C¯x¯for someC¯∈U. This contradicts the robustness assumption. Moreover, ifx¯=0,then by consideringC=0in (27), there exists someC¯with∥C¯∥≤0.5ϵsuch thatf(xo)≤f(x¯)andf(xo)≠f(x¯). This contradicts the efficiency ofx¯. Hence, x0 ≠ 0 andx¯≠0.DefineM={λ∈R≥p:∥λ∥≤1,∑j=1pλj≥1}.It is clear that, M is a nonempty compact convex set. Now, letF:M⇉Mbe a set-valued mapping defined byF(λ)={λ′∈M:f(xo)+ϵ∥xo∥2∥λ∥λ≤f(x¯)+ϵ∥x¯∥2∥λ′∥λ′}.We show that F(λ) is nonempty and convex for each λ ∈ M.Let λ ∈ M. Defining p × n matrix CobyCo:=ϵ2∥λ∥∥xo∥λxoT,we have∥Co∥≤0.5ϵ,and hence by (27), there exists some p × n matrixC¯such that∥C¯∥≤0.5ϵ,and(28)f(xo)+ϵ∥xo∥2∥λ∥λ≤f(x¯)+C¯x¯.Considerλ¯withλ¯i=∥C¯i∥. Defineλ′:=λ¯∥λ¯∥. By considering Cauchy–Schwarz inequality andϵ2∥λ¯∥≥1,we havef(xo)+ϵ∥xo∥2∥λ∥λ≤f(x¯)+C¯x¯≤f(x¯)+∥x¯∥λ¯≤f(x¯)+ϵ∥x¯∥2∥λ¯∥λ¯≤f(x¯)+ϵ∥x¯∥2∥λ′∥λ′Therefore, due toλ′∈M,we haveλ′∈F(λ),and hence F(λ) is nonempty.To prove the convexity, let λ1, λ2 ∈ F(λ) andυ∈(0,1). First we assume that∥λ1∥=∥λ2∥=1.Then, by definition of F(λ), we getf(xo)+ϵ∥xo∥2∥λ∥λ≤f(x¯)+ϵ∥x¯∥2(υλ1+(1−υ)λ2)Due to∥υλ1+(1−υ)λ2∥≤1andυλ1+(1−υ)λ2≥0,we can infer thatf(xo)+ϵ∥xo∥2∥λ∥λ≤f(x¯)+ϵ∥x¯∥2∥υλ1+(1−υ)λ2∥(υλ1+(1−υ)λ2).Hence,υλ1+(1−υ)λ2∈F(λ)when∥λ1∥=∥λ2∥=1.Now, considering two arbitrary vectors λ1, λ2 ∈ F(λ) andυ∈(0,1),there are γ > 0 and μ ∈ (0, 1) such that(29)υλ1+(1−υ)λ2=γ(μλ1∥λ1∥+(1−μ)λ2∥λ2∥)Notice that 0 < ‖λ1‖, ‖λ2‖ ≤ 1. By definition of F(λ), it is clear thatλ1∥λ1∥,λ2∥λ2∥∈F(λ). Furthermore, ifλ′∈F(λ)andγλ′∈Mfor some γ > 0, thenγλ′∈F(λ). Therefore, according to (29), we haveυλ1+(1−υ)λ2∈F(λ).Hence, F is a convex-valued mapping. It is clear that F is a closed mapping. Therefore, by Kakutani fixed-point theorem (see Franklin, 2003), there exists some λ* ∈ M such that(30)f(xo)+ϵ∥xo∥2∥λ*∥λ*≤f(x¯)+ϵ∥x¯∥2∥λ*∥λ*.The above inequality does hold as equality, otherwise due to (27) we havef(x¯)+ϵ∥x¯∥2∥λ*∥λ*=f(xo)+ϵ∥xo∥2∥λ*∥λ*≤f(x¯)+C˜x¯,f(xo)+ϵ∥xo∥2∥λ*∥λ*≠f(x¯)+C˜x¯,for someC˜with∥C˜∥≤0.5ϵ.Thenϵ∥x¯∥2∥λ*∥λ*≤C˜x¯andϵ∥x¯∥2∥λ*∥λ*≠C˜x¯.By Cauchy–Schwarz inequality, we getϵ∥x¯∥2∥λ*∥λ*≤∥x¯∥dandϵ∥x¯∥2∥λ*∥λ*≠∥x¯∥din whichd∈Rpwithdi=∥C˜i∥. Therefore,∥C˜∥=∥d∥>0.5ϵwhich is a contradiction. Thus, inequality (30) holds and it does not hold as equality. On the other hand, by Cauchy–Schwarz inequality,f(xo)+ϵx¯Txo2∥λ*∥∥x¯∥λ*≤f(xo)+ϵxoTxo2∥λ*∥∥xo∥λ*Hence, according to (30),f(xo)+ϵx¯Txo2∥λ*∥∥x¯∥λ*≤f(x¯)+ϵx¯Tx¯2∥λ*∥∥x¯∥λ*,f(xo)+ϵx¯Txo2∥λ*∥∥x¯∥λ*≠f(x¯)+ϵx¯Tx¯2∥λ*∥∥x¯∥λ*.Therefore, settingCo=ϵ2∥λ*∥∥x¯∥λ*x¯T,we have∥Co∥<ϵandf(xo)+Coxo≤f(x¯)+Cox¯,f(xo)+Coxo≠f(x¯)+Cox¯.Two last relations contradict the robustness ofx¯(in the sense of Definition 3.1) and the proof is complete.□It is not difficult to see that, Theorem 5.2 will be valid if one replaces0.5ϵ,in the considered uncertainty set, with someϵ¯∈(0,ϵ).In fact, the robustness notion considered in the present paper is sufficient for two above mentioned (worst case-based) notions. In definition studied in the present paper, the robustness is coming from a linear perturbation instead of an arbitrary perturbation and this leads to Proper efficiency as proved in Section 3. See also, Section 3 of Georgiev et al. (2013) for some comparison.In this section, we consider two robustness aspects of (weakly/properly) efficient solutions. In the first one, we consider a convex combination of the objective function of Problem (20) with a new special function. The second robustness aspect is due to adding a new objective function to the problem. In both cases, we examine preserving the weak/proper/robust efficiency.Consider the following problem for α ∈ [0, 1],(31)minf(x)+(1−α)h(x)qs.t.gi(x)≤0,i∈{1,2,…,m},whereh:Rn→Ris a convex function and0≠q∈R+pis a p-vector with nonnegative components. We denote this program by (MOP)α, and this program coincides with (20) whenα=1.Note: In the whole of this section, we assume that the functions h,fi(i=1,…,p)andgj(j=1,…,m)are convex.Theorem 6.1 presents a sufficient condition for properly efficient solutions of problems (20) and (MOP)0 to remain properly efficient for (MOP)α.Theorem 6.1Letx¯be a proper efficient solution to both problems (20) and (MOP)0. If (CQ) holds atx¯,thenx¯is a proper efficient solution of (MOP)αfor each α ∈ (0, 1).Sincex¯is a properly efficient solution to Problem (20), then there existsλ∈Rpandw∈Rmsuch that (see Clarke, 2013):0∈∑i=1pλi∂fi(x¯)+∑j=1mwj∂gj(x¯),wjgj(x¯)=0,j=1,…,m,λ>0,w≥0.Also, sincex¯is a proper efficient solution to Problem (MOP)0, there existμ∈Rpandv∈Rmsuch that μ > 0, v ≥ 0, and(32)0∈∑i=1pμi∂fi(x¯)+qTμ∂h(x¯)+∑j=1mvj∂gj(x¯),(33)vjgj(x¯)=0,j=1,…,m.Notice that the convexity of h is crucial in obtaining (32).Let α ∈ (0, 1). We define t and γ as followst:=αqTμαqTμ+(1−α)qTλ,γ:=tλ+(1−t)μ.It is clear that 0 < t < 1 and γ > 0. Also,(34)(1−t)qTμ=(1−α)qTγ.Thus,∑i=1pγi∂fi(x¯)+(1−α)qTγ∂h(x¯)=t∑i=1pλi∂fi(x¯)+(1−t)∑i=1pμi∂fi(x¯)+(1−t)qTμ∂h(x¯).Therefore, settingz=tw+(1−t)v,we get0∈∑i=1p(tλi+(1−t)μi)∂fi(x¯)+(1−t)qTμ∂h(x¯)+∑j=1m(twj+(1−t)vj)∂gj(x¯)=∑i=1pγi∂fi(x¯)+(1−α)qTγ∂h(x¯)+∑j=1mzj∂gj(x¯),where γ > 0 and z ≥ 0. Thereforex¯is a global minimum formin∑i=1pγifi(x)+(1−α)qTγh(x)s.t.gj(x)≤0,j=1,…,m.This implies thatx¯is a proper efficient solution of (MOP)α, according to Theorem 3.11 in Ehrgott (2005).□The following two results give sufficient conditions for efficient (resp. weakly efficient) solutions of problems (20) and (MOP)0 to remain efficient (resp. weakly efficient) for (MOP)α. These results extend Proposition 2.2 in Georgiev et al. (2013).Theorem 6.2Letx¯be an efficient solution to both Problems (20) and (MOP)0. Thenx¯is an efficient solution of (MOP)αfor each α ∈ (0, 1).Let α ∈ (0, 1). By contradiction assume that there exists a feasible point,x^,such thatf(x^)+(1−α)qh(x^)≤f(x¯)+(1−α)qh(x¯),f(x^)+(1−α)qh(x^)≠f(x¯)+(1−α)qh(x¯).Ifh(x¯)<h(x^),thenf(x^)−f(x¯)≤(1−α)q(h(x¯)−h(x^))≤(and≠)0.This contradicts the efficiency ofx¯for (20). Hence, we assume thath(x¯)≥h(x^). Due to the convexity assumption, we havef(12x^+12x¯)+qh(12x^+12x¯)≤12f(x^)+12f(x¯)+12qh(x^)+12qh(x¯)≤(and≠)f(x¯)+12(1−α)q(h(x¯)−h(x^))+12qh(x^)+12qh(x¯)=f(x¯)+q(h(x¯)+α2(h(x^)−h(x¯)))≤f(x¯)+qh(x¯).Hence, settingz=12x^+12x¯,the vector z is feasible andf(z)+qh(z)≤f(x¯)+qh(x¯)andf(z)+qh(z)≠f(x¯)+qh(x¯).This contradicts the efficiency ofx¯for (MOP)0, and the proof is complete.□Letx¯be a weak efficient solution to both Problems (20) and (MOP)0. Thenx¯is a weak efficient solution of (MOP)αfor each α ∈ (0, 1).The proof of this theorem is similar to that of Theorem 6.2 and is hence omitted.□Theorem 6.4 gives a sufficient condition for robust efficient solutions of Problems (20) and (MOP)0 to remain robust efficient for (MOP)α.Theorem 6.4Letx¯be a robust efficient solution for both Problems (20) and (MOP)0. If (CQ) holds atx¯,thenx¯is a robust efficient solution for (MOP)αfor each α ∈ [0, 1].Let α ∈ [0, 1]. By Theorem 6.2,x¯is efficient for Problem (MOP)α. Now, we show thatx¯is robust for (MOP)α. By Theorem 3.4,Pos(⋃i=1p∂fi(x¯))+Pos(⋃i∈A(x¯)∂gi(x¯))=RnandPos(⋃i=1p∂(fi+qih)(x¯))+Pos(⋃i∈A(x¯)∂gi(x¯))=Rn.By the above two equalities, and since all of the ∂-sets are convex here, we havePos(⋃i=1p∂(fi+(1−α)qih)(x¯))+Pos(⋃i∈A(x¯)∂gi(x¯))=Rn.Therefore,x¯is a robust efficient solution for (MOP)α, because of Theorem 3.6.□In the rest of this section, we examine adding a new objective function to Problem (20). Consider the following multi-objective optimization problem, denoted by (MOPh):(35)min(f(x)h(x))s.t.gi(x)≤0i=1,…,m,whereh:Rn→R. The following two theorems address some connections between the proper efficient solutions of two problems (20) and (MOPh). Recall that the functions h, fi, and gjare convex.Theorem 6.5Letx¯be a proper efficient solution to Problem (MOPh). If (CQ) holds atx¯,and∂h(x¯)⊆Pos(⋃i=1p∂fi(x¯))+Pos(⋃i∈A(x¯)∂gi(x¯)),thenx¯is a proper efficient solution to Problem (20).Sincex¯is a proper efficient solution of Problem (MOPh), then there existλ∈Rpandw∈Rmsuch that (see Clarke, 2013):0∈∑i=1pλi∂fi(x¯)+∂h(x¯)+∑j=1mwj∂gj(x¯),wjgj(x¯)=0,j=1,…,mλ>0,w≥0.Therefore, by assumption of the theorem,0∈∑i=1pλ¯i∂fi(x¯)+∑j=1mw¯i∂gj(x¯),for someλ¯>0,w¯≥0. This implies thatx¯is a proper efficient solution for Problem (20).□By a manner similar to the proof of the above theorem, it can be shown that this theorem is valid for weak efficient solutions as well. The following example shows that this result may not be valid for efficient solutions.Example 6.1Letg(x)=f(x)=xandh(x)=x2. It is clear thatx¯=0is an efficient solution of (MOPh) and {∇h(0)}⊆Pos(∇f(0)) butx¯=0is not an efficient solution of Problem (20).The following result gives more connections between the proper efficient solutions of two problems (20) and (MOPh) when fiand gjfunctions are continuously differentiable.Theorem 6.6(i)Letx¯be a proper efficient solution to both Problems (20) and (MOPh). If (CQ) holds atx¯,then there exist vectorsu∈Rpandv∈Rmsuch that u > 0, and(∑i=1pui∇fi(x¯)+∑j=1mvj∇gj(x¯))∈∂h(x¯).Letx¯be a proper efficient solution to Problem (20). If there exist vectorsu∈Rpandv∈R|A(x¯)|such that v ≥ 0 and(∑i=1pui∇fi(x¯)−∑j∈A(x¯)vj∇gj(x¯))∈∂h(x¯),thenx¯is a proper efficient solution for Problem (MOPh).(i)Sincex¯is a proper efficient solution of Problem (MOPh), then there exist(λ,λp+1)∈Rp×Randw∈Rmsuch that0∈∑i=1pλi∇fi(x¯)+λp+1∂h(x¯)+∑j=1mwj∇gj(x¯),wjgj(x¯)=0,j=1,…,m,λ>0,w≥0.Therefore, there exists somed∈∂h(x¯)such thatd=−∑i=1pλiλp+1∇fi(x¯)−∑j=1mwjλp+1∇gj(x¯).On other hand,x¯is a proper efficient solution to Problem (20). Therefore, there existλ′∈Rpandw′∈Rmsuch that(36)0=∑i=1pλi′∇fi(x¯)+∑j=1mwj′∇gj(x¯),(37)wj′gj(x¯)=0,j=1,…,mλ′>0,w′≥0.Lett>max1≤i≤p{λiλi′λp+1}. We haved=∑i=1p(tλi′−λiλp+1)∇fi(x¯)+∑j=1m(twj′−wjλp+1)∇gj(x¯).Settingui:=tλi′−λiλp+1andvj=twj′−wjλp+1,completes the proof of part (i).Settingμj=0for eachj∉A(x¯),by the assumption of the theorem, we have0=−∑i=1pui∇fi(x¯)+d+∑j=1mμj∇gj(x¯)for somed∈∂h(x¯).On other hand, sincex¯is a proper efficient solution to Problem (20), there existλ′∈Rpandw′∈Rmsatisfying (36) and (37). Fort>max1≤i≤p{uiλi′},we have0=∑i=1p(tλi′−ui)∇fi(x¯)+d+∑j=1m(twj′+μj)∇gj(x¯)∈∑i=1p(tλi′−ui)∇fi(x¯)+∂h(x¯)+∑j=1m(twj+μj)∇gj(x¯).Therefore,x¯is a proper efficient solution to (MOPh), and the proof is complete.□The following example shows that part (i) of the above theorem may not hold when some fior gjfunctions are nonsmooth. A similar example can be constructed for part (ii).Example 6.2Letf:R→Rbe defined byf(x)={x2−x1x2>0−x1x2≤0Consider the following optimization problemminf(x)s.t.g(x)=x1−x2≤0.The functions f and g are convex and∂f(00)={(−1α):α∈[0,1]},∂g(00)={(1−1)}.Withλ=α=μ=1,we have:(00)=λ(−1α)+μ(1−1).Therefore,x¯=(00)is an optimal solution to the above problem. Now consider the functionh(x)=x1and the following problem(38)min(f(x)h(x))s.t.g(x)=x1−x2≤0.We have∂h(x¯)={(10)}. Also, forλ1=λ2=1andα=μ=0,we get(00)=λ1(−1α)+λ2(10)+μ(1−1),Therefore,x¯=(00)is a proper efficient solution to Problem (38). Hence, in this example,x¯=(00)is a proper efficient solution to both problems (20) and (MOPh), while there is not any λ > 0 satisfying(10)=λ(−1α)+μ(1−1)for someμ∈Rand α ∈ [0, 1]. It shows that part (i) of Theorem 6.6 may not be valid in the presence of nonsmooth fior gjfunctions.The last theorem of this section provides a connection between the robust solutions of two problems (20) and (MOPh).Theorem 6.7Ifx¯be a robust efficient solution to Problem (20), thenx¯is a robust efficient solution of (MOPh). The converse holds if∂h(x¯)⊆Pos(⋃i=1p∂fi(x¯))+Pos(⋃i∈A(x¯)∂gi(x¯)).These are derived from Theorems 3.3 and 3.8.□

@&#CONCLUSIONS@&#
