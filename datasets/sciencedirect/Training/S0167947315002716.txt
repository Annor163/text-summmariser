@&#MAIN-TITLE@&#
Graph-theoretic multisample tests of equality in distribution for high dimensional data

@&#HIGHLIGHTS@&#
We test whether two or more samples have arisen from the same multivariate density.Test uses new graphs as well as the minimum spanning tree or nearest neighbors.Test performs well and is easy to perform for larged, smallndatasets.Power of the new tests is competitive or beats other general multivariate tests.Mean and variance of the asymptotically normal null distribution are easy to compute.

@&#KEYPHRASES@&#
Multisample problem,Perfect matching,Minimum spanning tree,Nearest neighbor,Energy,Orthogonal graph,

@&#ABSTRACT@&#
Testing whether two or more independent samples arise from a common distribution is a classic problem in statistics. Several multivariate two-sample tests of equality are based on graphs such as the minimum spanning tree, nearest neighbor, and optimal nonbipartite perfect matching. Here, the samples are pooled and the test statistic is the number of edges in the graph that connect points with different sample identities. These tests are typically unbiased and perform well when estimates of underlying probability densities are poor. However, these tests have not been thoroughly studied when data is very high dimensional or in the multisample case. We introduce the use of orthogonal perfect matchings for testing equality in distribution. A suite of Monte Carlo simulations on artificial and real data shows that orthogonal perfect matchings and spanning trees typically have higher power than other graphs and are also more effective at discerning when samples have differences in their covariance structure compared to other nonparametric tests such as the energy and triangle tests.

@&#INTRODUCTION@&#
Testing for the equality of independent samples is one of the classic problems in statistics: have two (or more) samples arisen from the same underlying probability density or are the samples different in some way? In other words, ifsindependent samples have (unknown) distribution functionsF1,F2,…,Fs, and each case is i.i.d., we wish to test the hypothesisH0:Fi(x)=Fj(x),for everyx∈Rd,i≠jagainst the general alternativeHa:Fi(x)≠Fj(x),for at least onex∈Rdandi≠j.Our motivating problem is inspired by a data collection issue that we encountered doing educational research. Students in ten sections of an introductory statistics class rated (on a Likert scale) their attitudes towards statistics on 36 questions and answered an additional 14 questions regarding demographic information. Responses were collected during the first and last weeks of class with the goal of quantifying any change in attitudes. Unfortunately, the initial surveys for two sections were lost. If the responses among the other eight classes did not exhibit any systematic differences, we could treat the initial attitudes of all classes as arising from some common distribution. This distribution could then be estimated and used as a baseline for the two lost sections. Thus, we needed to test whetherF1=F2=⋯=F8(the distributions of initial responses for the eight sections are identical) versus the alternative where at least two of the distributions are somehow different.Distributions can differ in more than their first two moments, so tests developed to look for differences in location and/or scale (e.g., see Rousson, 2002 and references therein) are not broad enough. Relatively few tests for equality in distribution for multivariate data have power against these general alternatives. Many compare estimates of the underlying probability densities of the two samples (Zimmerman, 1993; Justel et al., 1997; Kim and Foutz, 1987; Jing and Wang, 2006; Anderson et al., 1994; Song et al., 2007). Unfortunately, the curse of dimensionality (Scott and Wand, 1991) makes estimating densities very difficult without large sample sizes when the dimensionality is large. For many modern applications, e.g., microarray data analysis and customer transaction databases, the number of variables is so large (and can exceed the number of observations) that these tests are not viable.Alternative tests consider interpoint (typically Euclidean) distances. Liu and Modarres (2011) use properties of all triangles connecting 3-tuples for the two sample test. Szekely and Rizzo (2004) explore the use of an energy statistic (Baringhaus and Franz, 2004) for the multisample problem and later extend the analysis using more general distance metrics (Rizzo and Szekey, 2010).Graph-theoretic tests also show promise. Friedman and Rafsky (1979) use the minimum spanning tree (MST) and its orthogonal generalization (KMST) for the two-sample problem and mention possible multisample extensions. Nearest neighbor graphs for the two-sample problem are used in Bickel and Breiman (1983), Schilling (1986), Henze (1988) and for the multisample problem in Nettleton and Banerjee (2001). The dual of the Voronoi diagram (Dwyer and Squire, 1993) and optimal nonbipartite perfect matchings (Rosenbaum, 2005) have been used for the two sample problem. Fig. 1illustrates some of these graphs. In each case, the pooled sample is considered and the test statistic can be thought of as the number of edges in the graph that connect members with different sample identities. Fig. 2provides an intuitive motivation for the test using the MST and illustrates why the test is powerful against general alternatives.This work explores the choice of graph when testing the equality ofsindependent samples, focusing on examples where the number of observations is small and the number of variables is large. Regardless of the type of graph, the test statistic isR=  the number of edges that connect points with different sample identities. In general, the sampling distribution ofRunder the null hypothesis of equality is not distribution free (the exception being when Rosenbaum’s (2005) optimal perfect match graph is used). We instead consider the null distribution ofRover all permutations of the sample identities given the observed graph. By considering on the topology of the graph, the permutation distribution ofRunder the null does not depend on the (unspecified)Fin the null hypothesis and an exactp-value can be found. Further, there are explicit formulas for the mean and variance of the permutation null distribution ofR.For many graphs, this distribution is asymptotically normal. When this is the case, the unconditional test is asymptotically distribution free. We show how to employ a “continuity correction” to approximate thep-value from this asymptotic result.We introduce the orthogonal perfect matching (KMATCH) graph for the multisample problem. Through a suite of Monte Carlo simulations, we compare the power of the KMATCH test to the powers of theK-nearest neighbor (KNN) and orthogonal minimum spanning tree (KMST) tests. On independent, multivariate normal samples that differ purely in location, scale, or correlation, the KMATCH graph yields the highest power. On a selection of small but high dimensional real data, the KMST graph yields slightly more power. The power of these tests depend onK, a free parameter that controls how many edges appear in the graph. We show that a useful heuristic is to takeK=0.10Nfor KMST and KNN andK=0.15Nfor KMATCH.We also compare the powers of the graph-theoretic tests with those of the energy and triangle tests. The energy statistic provides the most powerful test for pure location alternatives (though its power is relatively low when samples have different covariance structures) while the triangle test is only competitive when testing whether two high-dimensional samples have different scales. Overall, we conclude that the KMATCH or KMST tests are the best all-around options since their powers are still typically high in scenarios when other tests do better and, more importantly, there were no alternatives studied where their powers approached zero (the energy, KNN, and triangle tests all have such examples).This article is organized as follows. Section  2 introduces the graphs used for the multisample test. Section  3 discusses the finite sample and asymptotic properties of the permutation sampling distribution of the test statistic under the null. Section  4 discusses how to chooseKfor the graph-theoretic tests and discusses the estimated powers of graph-theoretic, energy, and triangle tests for a variety of artificial and real data. Some insight as to why particular tests have higher power than others is offered. Section  5 summarizes the results.A graph is an ordered pairG=(V,E)whereVis the vertex set whose elements are the nodes of the graph, andEis the edge set whose elements give which pairs of nodes are connected in the graph. In the context of this paper, the data points are the nodes (we use the terms point and node interchangeably) and a graph is constructed on the data by choosing a certain set of edges to connect pairs of observations. The number of edges connected to nodeiis referred to as the node’s degreeDi. Throughout the paper we useNto denote the number of nodes (also the number of cases in the dataset when all samples all pooled together) andEto denote the number of edges in a graph.Edges are often given weights that are proportional to some symmetric measures of dissimilarity between the two nodes they connect. We define an edge’s weight to be the Euclidean distance between the two points it connects (implicitly assuming all variables are continuous), though one could use the Mahalanobis distance or a geodesic distance (calculated with respect to the sample’s intrinsic geometry). If categorical variables are present, one may define weights using the Hamming distance, quantifications of the set of categorical variables using a Gifi (1990) model, or even the proximity scores output by random forests in unsupervised mode (Svetnik et al., 2003).The pooled dataset hasNcases, so up toN(N−1)/2edges can potentially appear in a graph. The question becomes which edges should be selected. Various edge selection strategies result in different graphs, and we discuss the KMATCH, KNN, and KMST in this paper. All of these choose edges that tend to connect similar cases.A sufficient (but not necessary) condition for these graphs to be unique is that there are no ties in interpoint distances. However, uniqueness is not required since the test is conditional on the structure of the resulting graph, regardless of how it was made, how edge weights were assigned, or the manner in which ties are broken, etc.A nonbipartite perfect matching is a graph that contains exactlyN/2(assumingNis even) edges and where the degree of each node is one. In other words, each node is connected (“matched”) to exactly one other node. The optimal perfect matching is the one whose sum of edge weights is the smallest. Rosenbaum (2005) uses this graph for the two-sample test and also discusses the case whereNis odd.One can construct orthogonal perfect matchings as follows. First, the optimal perfect matching on the data is found and is called the “first match”. The “second match” is taken to be the optimal perfect matching provided that edges in the first match may not be selected. The “third match” is taken to be the optimal perfect matching provided that edges in the first and second matchings may not be selected, etc. We define the union of the first, second,…,Kth matchings to be the KMATCH graph. Fig. 1 shows an example of the first and second optimal perfect matchings and the 2MATCH. To our knowledge, the KMATCH graph has not previously been introduced in the literature.The number of edges in a KMATCH graph isE=KN/2and each node degree isDi=K. The choice ofKaffects the power of the test. Our simulations in Section  4 show that the value ofKthat yields the test with the highest power depends on the size of the pooled dataset as well as the specific way that samples differ. A useful heuristic for samples with up to a few hundred observations is to construct a KMATCH graph whereK≈0.15N.TheK-nearest neighbor graph connects each node to theKpoints with which it has the lowest dissimilarity (typically, theKpoints that are closest to it). In other words, KNN is the result of selecting theKedges with the smallest weights that are connected to nodeifori=1,…,N, discarding any redundant edges. The KNN can be thought of as the union of “orthogonal” nearest-neighbor graphs. The “first NN” is the standard nearest neighbor graph. The “second NN” connects each point to its second nearest neighbor (i.e., selects the edge with the smallest weight after edges in the first NN are removed from consideration), etc. The KNN is the union of these lower order graphs. Nearest neighbor graphs have been used in Bickel and Breiman (1983), Schilling (1986), Henze (1988), Nettleton and Banerjee (2001) to conduct the two-sample test.Unlike with KMATCH, the number of edges (E) and the node degrees (Di) in the KNN graph depend on the configuration of the observed data. In other words,EandDiare random variables whose values can vary from sample to sample. A useful heuristic for samples with up to a few hundred observations is to construct a KNN graph whereK≈0.10N.A spanning tree selectsN−1edges so that a unique path (alternative sequence of edges and nodes) exists between all pairs of theNpoints in the pooled dataset. The minimum spanning tree (MST) is the one whose sum of edge weights is the smallest.Friedman and Rafsky (1979) originally proposed the MST for the two-sample test. In a small simulation study, they found that orthogonal spanning trees (KMST) may be more powerful. To construct the KMST one first finds the MST (called the “first MST”). A “second MST” is the MST with the restriction that edges in the first MST are removed from consideration and may not be selected. The third MST is the MST with the restriction that edges in the first and second MSTs are removed from consideration and may not be selected, etc. The union of the first, second,…,Kth MST is the KMST. Fig. 1 shows an example of the MST, second MST, and 2MST. The KMST has been used as a tool to quantify multivariate associations and to test independence between samples (Friedman and Rafsky, 1983) but its power has not been experimentally studied for multisample tests of equality.The number of edges in the KMST is fixed atE=K(N−1). However, the node degrees (Di) depend on the exact configuration of the observed data. In other words, the node degree distribution of the KMST is a random quantity which can vary from sample to sample. A useful heuristic for samples with up to a few hundred observations is to construct a KMST graph whereK≈0.10N.There is one additional consideration regarding the KMST that does not arise with KMATCH or KNN graphs. While one can construct KMATCH and KNN for anyK=1,…,N−1, the KMST does not exist for values ofKgreater than some threshold. Simulations suggest this threshold is typically betweenN/3andN/2, but it depends on the observed data. At this value ofK, one node is connected to every other node. No further edges can be added to this node, so a higher order KMST cannot exist.Let us address the computational complexity of naive approaches for constructing these graphs since these are typically the ones implemented in existing software. Although theoretical algorithms may exist that give better running times (especially if only approximations to the graphs are used, e.g. see the discussion on exact and approximate nearest neighbors in Hinneburg et al., 0000 and Borodin et al., 1999), they are rarely implemented. Since graphs require a dissimilarity matrix as input, naive construction algorithms require at leastO(dN2)computational time, wheredis the number of variables in the data. The package igraph in R has a routine that calculates the MST using Prim’s algorithm and runs inO(N2logN)time. The package nbpMatching (Lu et al., 2011) can find optimal perfect matchings with an algorithm that runs inO(N3)time (Galil, 1986). The package FNN has an exact nearest neighbor searching algorithm which runs inO(NlogN)time given a fixed dimensionality.Our domain of interest is smallNand highd, so the polynomial complexity of these algorithms inNis not discouraging. Indeed, ifNis large enough for computational time to be a factor, the test will be “too sensitive”: small differences that lack practical significance will be highly statistically significant. We do omit the dual of the Voronoi diagram (a graph used in Dwyer and Squire, 1993 for the two-sample test) because our simulations suggest that the running time is exponential ind.Consider a graph withEedges that has been built on the pooled dataset containing allsindependent samples. Denote the individual sample sizes asn1,n2,…,nsand let the pooled sample size beN=n1+⋯+ns. LetRbe the total number of edges joining points with different sample identities. To motivate the use ofRfor a test of equality, recall Fig. 2. When two samples of the same size (n1=n2=N/2) come from the same distribution (e.g., the left panel), the sample identities of the points will be well-mixed andRwill be aboutN/2. The two samples in the middle panel are generated by distributions that differ in location, while the two samples in the right panel come from distributions that differ in scale. In both cases,Ris small since there are relatively few edges that connect members with different sample identities.Intuitively, a test based on the number of edges connecting different samples should be sensitive to both differences in location and scale and can serve as a test against general alternatives. IfRis “too small”, the null hypothesis of equal distributions can be rejected. The test can of course be made two-sided by also rejecting the null for large values ofR, but this may only have applications in specific areas. Large values ofRcorrespond to the samples being “unnaturally” close to each other. A test for fraud detection (imagine someone faking a dataset by taking a legitimate dataset and perturbing each number by a small, random amount) would be interested in this alternative.Except for specific graphs (e.g., 1MATCH), the distribution ofRunder the null hypothesis is not distribution free and depends on the (unknown) data-generating distributionF. Thus, we condition on the topology of the graph that has been constructed on the pooled data. The null distribution ofRover all permutations of the sample identities, conditioned on the observed graph, is then distribution free.The exact (conditional)p-value of the test is easily found by generating all possible permutations of the sample identities on the observed graph and calculating the fraction of them whose values ofRare no larger than the observed value on the original graph constructed on the pooled sample. For larger sample sizes, thep-value can be approximated by considering a large random sample of permutations instead.Friedman and Rafsky (1979) present the first two moments of the null distribution ofRunder permutation of the sample identities for the two-sample problem. If samples have sizen1andn2, andN=n1+n2, then for any graph withEedges:(1a)μR|E=2n1n2EN(N−1)(1b)σR|E,C2=μR|EE{E+C+2(n1−1)(n2−1)(N−2)(N−3)[E(E−1)−2C]−EμR|E}.The quantityCis a function of the topology of the graph, specifically the node degree distribution of the points. IfDiis the node degree of pointi, then:(2)C=12∑i=1NDi(Di−1).Rosenbaum (2005) rediscovers a special case of this formula for the optimal nonbipartite perfect matching (1MATCH in our notation) whereDi=1,C=0, andE=N/2. He also proves that the sampling distribution ofRover all permutations of the sample indices is also the unconditional sampling distribution ofR, making the 1MATCH test distribution free (it is the only graph in this paper for which this is the case). The nearest neighbor tests in Schilling (1986), Henze (1988) use the proportion and absolute number of edges that connect members of the same sample, respectively.Friedman and Rafsky’s exploration of generalized correlation coefficients (Friedman and Rafsky, 1983) can be used to extend the test to multiple samples. Here, the test statisticRcan be thought of as a measure of association between the graph constructed on the pooled samples and an unweighted graph that connects only points with different sample identities. A large value ofRimplies the samples are well mixed and a small value ofRis evidence that the samples are somehow different. Let us defineG1andG2(which representeyandCyin Friedman and Rafsky, 1983) as(3)G1=∑j=2s∑i=1j−1ninjG2=12∑i=1sni(N−ni)(N−ni−1)Friedman and Rafksy’s equations (9) and (10) in Friedman and Rafsky (1983) give the first two moments of the null distribution ofRunder all permutations of the sample identities, conditioned on the topology of the observed graph. Rewriting them in our notation (exandCxin their equations areEandCin our notation, respectively):(4a)μR|E=2EG1N(N−1)(4b)σR|E,C2=μR|E(1−μR|E)+4N(N−1)(N−2)(CG2+(E(E−1)−2C)(G1(G1−1)−2G2)N−3).We note that the multisample test is always conditional for finite samples, even when using the 1MATCH graph. An alternative derivation of these moments can be found in Petrie (2007).For many graphs, the null distribution ofRover all permutations of the sample identities is asymptotically normal. This can be shown by castingRin terms of a generalized correlation coefficient (Daniels, 1944). Letaij=1if an edge connects pointsiandjfori<j(wheni>j, letaij=−1) andaij=0if an edge does not. Similarly, letbij=1if pointsiandjhave the same sample identities fori<j(wheni>j, letbij=−1). Then let:(5)Γ=12∑i=1N∑j=1Naijbij.Since edges in the graphs used for multisample testing do not connect nodes to themselves,aii=0, rendering the value assigned tobiiirrelevant (we takebii=0for convenience). The permutation distribution ofΓrefers to the values arising from permuting the indices ofbij. In this context, this corresponds to the distribution ofΓover all permutations of the sample identities.In this setting,Γequals the number of edges in the graph that connect points with the same sample identity. Thus,R=E−Γ. Conditioned on the topology of the observed graph, the asymptotic null distribution ofRover all permutations of the sample identities will be normal when the asymptotic distribution ofΓis normal.Pham et al. (1989) (refuting the original conditions presented in Friedman and Rafsky, 1983) show that the following conditions are sufficient for the asymptotic normality ofΓ:(6a)∑i=1N∑j=1Naij=∑i=1N∑j=1Nbij=0(6b)maxi(∑j=1N|aij|)=O(maxi,j|ai,j|)(6c)lim inf(∑i=1N∑j=1Naij2)/(Nmaxi,jaij2)>0(6d)lim sup(∑i=1N(∑j=1Nbij)2)/(N∑i=1N∑j=1Nbij2)<1.The limits are taken asN→∞. The first condition is satisfied since by our definitionaij=−ajiandbij=−bji.The second condition provides a constraint on the topology of the graph. Since∑j|aij|=Diandmax|aij|=1, the condition can be rewritten as:(7)max(∑j=1N|aij|)=O(max|ai,j|)⟹maxDi=O(1).For a graph to satisfy this condition, the node degrees cannot have any dependence onN. KMATCH hasDi=K, and Friedman and Rafsky (1983) note that sphere-packing properties in Euclidean space bound the maximum node degree in the KMST and KNN by a constant that depends only on the dimensionality of the space andK, but not onN. Thus, the second condition is satisfied by these graphs whenKis chosen independently ofN.The third condition provides a constraint on the number of edges in the graph. Since∑aij2=2E, the condition is satisfied as long asEgrows at least linearly withN.(8)lim inf(∑aij2)/(Nmaxaij2)>0⟹limE/N>0.The number of edges in KMATCH and KMST isKN/2andK(N−1), respectively, while the number of edges in the KNN is proportional toN(the exact number is determined by the point configuration). Thus, the third condition is satisfied by these graphs. Taken together, the second and third conditions are satisfied when the graph is neither “too dense” nor “too sparse”, respectively.The final condition is satisfied for any graph. The double sum in the denominator,∑bij2, can be written as∑k=1snk(nk−1). For the set of indicesiandjwhich belong to members of samplek, the double sum in the numerator evaluates tonk(nk2−1)/3.(9)lim sup(∑i(∑jbij)2)/(N∑bij2)=lim sup∑k=1snk(nk2−1)3N∑k=1snk(nk−1).Letpkbe the fraction of cases with sample identitykin the limit asN→∞. The limit goes to:(10)lim(∑i(∑jbij)2)/(N∑bij2)=∑k=1spk33∑k=1spk2≤1/3.Since0≤pk≤1andpk3≤pk2, the largest limiting value of expression is always at most 1/3 and the final condition is satisfied.We note that the conditions in Pham et al. (1989), while sufficient, have not yet been shown to be necessary. Weaker conditions on the topology of the graph may exist that allow a choice ofKto be proportional toN.ChoosingKto be a fixed quantity (independent ofN) allows the permutation null distribution ofRfor the graphs under consideration in this paper to be asymptotically normal. However, our simulations suggest that there is no “best” value ofK, and pickingKto be proportional toN(K≈0.10Nfor KMST/KNN andK≈0.15Nfor KMATCH) typically yields a test with a higher power than lettingKbe a fixed constant. When using a graph constructed with this heuristic, thep-value of the test can still be estimated to arbitrary precision via simulation.When the permutation sampling distribution ofRunder the null is asymptotically normal for the KMATCH, KMST, and KNN (i.e.,Kis chosen independently ofN), an approximatep-value for the test can be found by calculating the relevant normal probability using the expected value and variance in Eq. (4). However, sinceRis discrete, a “continuity correction” can be made to improve the approximation, similar to when one approximates a binomial distribution by a normal distribution. Instead of finding the area to the left ofRunder the normal curve, we suggest finding the area to the left ofR+0.5since this better captures the area of the bar representingRin the probability histogram of the null sampling distribution for finite samples.Via simulation, we find that the continuity correction greatly improves the accuracy of normally-approximatedp-values, though the importance of the correction diminishes as the number of edges in the graph becomes large. Fig. 3shows an example of the improvement gained by using the correction forK=2,s=2,n=30,d=2. Each plot (one for each type of graph) shows the estimated percent difference between exact and approximatep-values, i.e.100(papprox−pexact)/pexactfrom 200 trials versus the “exact”p-value itself (using 999 permutations of the sample identities). For the important domain where the exactp-value≈0.05, the correctedp-values are closer to the exactp-values than the uncorrected ones. The correctedp-values are still relatively far from the exact values for KMATCH because it only has 30 edges in this example compared to approximately 50–60 for KNN and KMST (the approximation improves with more edges).Finally, the permutation distribution ofRunder the null is asymptotically distribution-free for the KMATCH and 1MST (the 1MATCH is distribution free whens=2for any sample size Rosenbaum, 2005). This follows because a normal distribution is fully specified by its first two moments, and these moments (Eq. (4)) can be written solely in terms ofKand the ratiosn1/N,n2/N, etc., whenN→∞. For KMATCH, we can replaceE=KN/2andC=NK(K−1)/2. For the 1MST,E=K(N−1)andCconverge to a constant that depends only on the dimensionality of the data (Henze and Penrose, 1999). We suspect thatCconverges to a constant for generalK, but the asymptotic behavior of the node degree distribution for orthogonal spanning trees remains unaddressed in the literature. This null distribution ofRfor the KNN test is never distribution free since the number of edges it contains is not fixed but varies from sample to sample and depends on the data generating distribution regardless ofN.The choice of the graph affects the power of the test. We first compare the powers of KMATCH, KMST, and KNN for different values ofKto develop a heuristic for choosingK. Then, we compare the powers of the graph-theoretic, energy and triangle tests on a variety of artificial and real data.In all experiments, we avoid using any asymptotic results and always condition on the observed graph in the pooled sample. By conditioning on the topology of the observed graph, the test is distribution free regardless of the graph used for the test. We approximatep-values using 50,000 random permutations of the sample identities unless otherwise noted.The value ofKgoverns how many edges are included in the graph and is a critical parameter. The value ofKhas a practical interpretation. Out of theN(N−1)/2possible edges that could be included in the graph, KMST usesK(N−1)of them while KMATCH usesKN/2. If we chooseKto be proportional toN(so thatK=pN) then the fraction of possible edges used by the KMST and KMATCH is2pandp, respectively.The optimal value ofKwill depend on the particular ways in which the samples are different, so we consider many scenarios (i.e., number of samples, sample sizes, and dimensionalities) to gauge what range ofKis typically useful. The upshot is that we suggest takingK≈0.1Nfor KMST and KNN andK≈0.15Nfor KMATCH, whereNis the sample size of the pooled data. The recommended value ofKfor KMATCH is larger than that for the KMST and KNN becauseKreally serves as a proxy for the optimal number of edges in a graph. For a givenK, the KMATCH graph contains fewer edges (E=NK/2) than the KMST (E=K(N−1)).Specifically, let us consider independent, equal-sized, multivariate normal samples which differ in only one set of parameters at a time. For each scenario, the parameters have been selected purely for convenience so that the powers of the tests are not too close to 0 or 1.•Location alternatives—the covariance matrix is the identity matrix and the mean vectors of each sample are chosen randomly, e.g., the mean of each marginal is uniformly distributed between 0 and 1 fors=2,n=30,d=2.Scale alternatives—each sample is centered at the origin and the diagonal covariance matrices have elements that are chosen randomly, e.g., uniformly between 0.4 and 3.0 fors=5,n=20,d=100.Correlation alternatives—each sample is centered at the origin and the covariance matrix is generated randomly according to the “onion” procedure genPositiveDefMat in R (diagonal elements are 2.0 and the generated off-diagonal elements are multiplied by 0.6, 0.7, and 0.4 for thes=2,s=5/d=2, ands=5/d=100scenarios, respectively).To meaningfully compare powers, the sizes of the critical regions of each test must be comparable. BecauseRis a discrete test statistic, there will typically be a difference between the nominalαlevel of the test and the size achieved by the test. Especially for very small sample sizes,Rwill take on a relatively small number of possible values. Thus, the test may reject the null hypothesis with a frequency less thanα. In our paper, the samples sizes are large enough that the difference between the achieved and nominal sizes appears negligible.Further, the empirical type I error rates for the KMATCH, KMST, or KNN tests (regardless ofK) are very close to each other, as evidenced by Monte Carlo simulations using independent samples arising from uncorrelated multivariate unit normal distributions. One exception arose whens=2,n=30,d=100where 1MATCH rejects the null hypothesis slightly more often than the other graphs. Thus, the reported powers of 1MATCH may thus be artificially high. However, this is only a slight complication since our heuristic only recommends using 1MATCH whenN≤10.Fig. 4shows how the estimated powers of the graph-theoretic tests vary withKfor the two combinationss=2,n=30,d=2or 100 and the two combinationss=5,n=20,d=2or 100. All powers are found from 10000 trials. The specific shapes of the power curves are unimportant since we are merely tuningK(quantitative comparisons are presented in Section  4), but some general observations are in order. First, let us note that the curve for KMST “stops” because there is a maximumKbeyond which this graph is not defined (one node becomes “saturated” and is connected to all the others).For pure location alternatives, the power rises sharply withK, plateaus, then drops off slowly as “too many” edges (connecting points which are progressively more unrelated to each other) are added to the graphs. Typically, relatively large values ofKare preferable (up to0.7Nfor KMATCH and KNN or until the KMST can no longer be built).For pure scale alternatives, the power rises withKand drops off more quickly than for location alternatives. Optimal values ofKare smaller than those for location alternatives. One interesting case iss=2andd=100for KNN—the power is 0 (no rejections of equality after 10000 trials) for allK. In such an empty, high dimensional space, the nearestN/2−1neighbors of nearly each point belong to the sample with the smallest dispersion. Since both samples have equal size, this makesRvery close to the expected value ofN/2.For pure correlation alternatives, the power behaves similarly to the scale alternative case. However, there is no failure of KNN in high dimensions.Based on our analysis, we recommendK=0.10Nfor KMST and KNN andK=0.15Nfor KMATCH. There is a tug-of-war between larger values ofKfor location alternatives and smaller values ofKfor scale/correlation alternatives (for the two sample test withn=30andd=100the optimalKfor KMST and KNN is 1). The heuristic provides a good compromise.Although not presented, we have also looked at scenarios whered=5,10,20, and 50. The shape of the power curves transitions smoothly between what is seen atd=2andd=100. We have also examined scenarios where both the location and full covariance matrix differ between samples. The graphs of the powers versusKlook similar to “linear combinations” of the plots presented, skewed towards whichever feature is most responsible for the difference in samples, e.g., if samples differ mostly in location and slightly in scale, the plot looks most like the one for the pure location alternative.Our recommended values ofKare found by assuming multivariate normality. For modern, real-world datasets this may not be the case. We thus examine the performance of the graph-theoretic tests on datasets whose distributions are far from multivariate normals. These datasets are available on the UCI Machine Learning Repository (Bache and Lichman, 2014).We choose classification datasets where each class is logically or physically different from each other. A priori, the null hypothesis of equality in distribution is false. To study the power of the tests, we take small random samples from each class (e.g., a class may have 100 cases, of which we randomly select 15) and calculate the fraction of trials where equality is rejected. Though this procedure induces some dependency between trials (so that standard errors for the power cannot readily be calculated), this is not much of an issue since we are concerned only with the general pattern of how power varies withK. We consider the following high-dimensional datasets. Class comparisons and sizes are hand-picked so that powers can be easily compared (i.e., they are not too close to zero or one).•Amazon commerce reviews—each instance is a 10000-dimensional vector representing usage of digits, punctuation, word and sentence lengths, word frequencies, etc., of reviews from one of 50 extremely active customers. There are a total of 50 classes with 30 instances each. For each instance, most variables equal zero while the remaining are integers ranging from one to a few dozen. We compare random samples of size 25 from reviewers 23 and 24 and random samples of size 13 from classes 4 and 8. For the five-sample test, we compare samples of size 10 from reviewers 8, 12, 13, 26, and 29.Semeion Handwritten Digits—each instance is a 256-dimensional vector that represents a written digit stretched into a 16×16 rectangular box. Each value is either a 0 or a 1 depending on its underlying grayscale value. We have added distractor features (randomly chosen to equal 0 or 1 while keeping the overall proportion of 1s in the dataset the same) to bring up the number of variables to 10000. We compare random samples of size 15 from digits 6 and 8.ISOLET—each instance is a 617-dimensional vector representing a spoken letter. Subjects spoke the name of each letter of the English alphabet twice and creators of the data have pre-processed each variable to lie between −1 and 1. There are a total of 26 classes with 60 instances each. We compare random samples of size 20 from classes 7 and 20 as well as samples of size six from classes 5, 7, and 16.Libras—each instance is a 90-dimensional vector representing the path of a hand during some movement like a curved swing, circle, or horizontal zigzag. There are a total of 15 classes with 24 instances each. The creators of the data have pre-processed each variable to fall between 0 and 1. We compare samples of size 15 from classes 11 and 13.LRS—Each instance is a 93-dimensional vector which describes the fluxes from astronomical objects. We have scaled the original data values down by a factor of 30000 so that variables typically range between 0 and 1. There are a total of 8 classes spread over 531 instances in unequal quantities. We compare random samples of size 15 from classes 2 and 3.Fig. 5shows how the powers (estimated from 10000 trials) of the graph-theoretic tests vary withKfor select two, three, and five sample tests. As expected, the behavior of the powers withKis much richer than the multivariate normal cases. However, our heuristic of takingK=0.10Nfor KMST and KNN andK=0.15Nfor KMATCH still looks reasonable since the true optimal values ofKare nearly evenly split between larger and smaller than the recommended value.A key point from the simulation is that no graph consistently has a higher power than the others, even if we knew the optimalKto use. For example, KMATCH has the highest power for Isolet (2-sample) and Amazon (reviewers 23/24) while KMST and KNN have higher powers for Libras and Amazon (reviewers 4/8). Another point is that it is typically advantageous to considerK>1, with exceptions such as Amazon (4/8 and 5 class) and Libras for KMST and KNN, where the power peaks atK=1.Let us use the recommend values ofKfor each graph and compare the powers of the graph-theoretic tests to the powers of the energy (Rizzo and Szekey, 2010) and triangle (Liu and Modarres, 2011) tests. The energy test is readily available in R in package energy, while we obtained code for the triangle test from its authors. Tests based on estimating underlying probability densities are not considered here because they function poorly for high-dimensional datasets.The energy statistic compares the interpoint distances between samples to interpoint distances within samples. IfXandYare two samples withn1andn2observations, respectively, the energy statistic is defined to be:(11)e(X,Y)=n1n2n1+n2(2n1n2∑i=1n1∑j=1n2‖xi−yj‖−1n12∑i=1n1∑j=1n1‖xi−xj‖−1n22∑i=1n2∑j=1n2‖yi−yj‖)where‖⋅‖is the Euclidean distance. When there aressamples, the energy statistic is the sum over thes(s−1)/2pairs. When conditioned on the observed interpoint distances, the sampling distribution ofe(X,Y)over all permutations of the sample identities under the null hypothesis is distribution free (Rizzo and Szekey, 2010). The energy statistic has an expected value of zero if and only if the distributions are equal, and large values are evidence that samples arise from different distributions.Liu’sQstatistic (Liu and Modarres, 2011) considers the joint distribution of the fraction of triangles (formed between two points with one sample identity and one with the other) whose longest, shortest, and middle edges connect members with the same sample identity. Under the null hypothesis, the expected value of each of these fractions is one third. The test statistic calculates the discrepancy between the observed fractions and one third. The test is asymptotically distribution free. Liu proposes other statistics, but we findQto be the most powerful.The top half of Table 1compares the powers of the graph-theoretic tests (using both the recommended value ofKandK=1) with the powers of the energy and triangle statistics for a set of multivariate normal alternatives. Powers are estimated using 10000 trials so that the maximum margin of error is 0.01. Thep-values of each test are calculated via 999 permutations of the sample identities.Again, to meaningfully compare powers, the size of each test must be comparable. We test whether the empirical sizes equal 5% when all samples arise from uncorrelated multivariate normal variables with the combinations ofs,n, anddpresented in Fig. 4. Of the 56 tests, only one empirical type I error rate was not consistent with 5% (KMST ats=5,n=20,d=2whose 95% confidence interval for the size is [5.1,8.1]%).For location alternatives, the energy statistic is by far the most powerful. KMATCH is more powerful than the KMST and KNN, while the triangle test has the least power. In all cases, the test using the recommend value ofKhas a much higher power than the test usingK=1.For correlation alternatives, KMATCH often yields the most powerful test (though all graph-based tests are equally as powerful for the case whereS=5,n=20, andd=2). Tests using the recommend value ofKalmost always have a higher power than the tests usingK=1. The energy and triangle tests have relatively low power.Scale alternatives are quite interesting. As previously discussed, KNN has no power against pure differences in scale forS=2andd=100. In contrast, the triangle test has 100% power in this case. There is an intuitive explanation. If X is the sample with the smaller dispersion, then the shortest edge of a triangle connecting points with identities X, X, Y will almost always connect both X’s and the longest edge of a triangle connecting points with identities X, Y, Y will almost always connect both Y’s. After triangle, the KMATCH test has the highest power. We note that energy performs quite poorly whenS=5.Our conclusion is that the energy test is the clear choice if one is most interested in detecting pure location differences while KMATCH is the best choice if one is interested in detecting differences in the covariance structure among samples. In the specific case where one is purely interested in detecting differences in scale for high-dimensional data, the triangle test is superior.An interesting question is why the energy test has high power for location alternatives but low power for scale/correlation alternatives. We believe that an (rather informal) answer is provided by analyzing the equation for the energy itself (Eq. (11)).Imagine X and Y are both spherical point clouds centered at the origin. Let us fix the relative locations of the points within X and Y but allow the samples to shift relative to each other. As the displacement between samples grows, the only term in the test statistic that changes is the sum of interpoint distances between X and Y, which also grows.Now keep both samples centered at the origin but let the scale of Y increase (e.g.,yi→ayi). As this occurs, the sum of interpoint distances between X and Y also increases. However, the corresponding increase in the energy statistic is somewhat offset by subtracting off the increase in the sum of interpoint distances between points in Y. Thus, the energy between two samples that differ in location is in some sense larger than the energy between two samples that differ in scale, so the null hypothesis should be more readily rejected for location alternatives.A similar argument may hold for correlation alternatives. Let X and Y be two identical elliptical points clouds centered at the origin and fix the locations of points within each ellipse. Now allow one ellipse to rotate, thereby inducing a difference in correlation structures between X and Y. The only term in the energy that changes is the sum of interpoint distances between X and Y. However, the increase in energy is relatively “slow” compared to when samples are displaced from each other. Further, the energy between the two samples is bounded during a complete rotation. Thus, the energy between two samples that differ in correlation structure should be smaller than the energy between two samples that differ in location, so correlation alternatives should be relatively hard to detect.Let us now compare the powers of the graph-theoretic tests (using the recommend values ofK) with the powers of the energy and triangle tests on real-world data. To estimate the power, we take 10000 small random samples from each class (e.g., if a class has 100 cases we may randomly select 15) and find the fraction of them where the null hypothesis of equal distributions is rejected. The bottom half of Table 1 summarizes our findings.Regarding the graph-theoretic statistics, KMATCH no longer has the highest powers in the majority of scenarios. In fact, KNN has the highest power in five of the scenarios while KMST has the highest power in the remaining three (though often there is little practical difference between KMATCH and the best graph). Further, there are a few cases whereK=1is the best choice (e.g., KMATCH has a power of 0.64 when using the recommendedKand a power of 0.71 forK=1on the five sample Amazon comparison). However, we continue to recommend using the suggested value ofKsince in most cases it does greatly improve the power compared to takingK=1(e.g., KMATCH has a power of 0.82 while 1MATCH has a power of 0.21 for the two sample Isolet).For the Amazon comparisons, the graph-theoretic tests are the most powerful, though no one graph consistently yields a higher power than the others. The author 23/24 comparison is unusual because the energy statistic never rejects the hypothesis of equality. The reason is elusive. The overall distribution of interpoint distances is unremarkable (there are no outliers) and the means and variances for each variable are quite similar between the two authors. One possible explanation is that the samples differ subtly in the covariance structure, where the energy test has relatively low power compared to the other tests.The graph-theoretic tests are also the most powerful for the Libras data and have equal power to the energy test for the LRS data. However, their powers are lower than the power of the energy test for the digits and ISOLET datasets (we suspect the samples here differ mainly in location). The triangle tests are not competitive with the exception of the digits data (even then its power is lower than the powers of the energy and best performing graph-theoretic test).Since the powers in Table 1 are for hand-picked, difficult class comparisons for the graph-theoretic statistics, we also explore how each test performs on a wider array of class comparisons. We randomly choose two (or more) classes from a dataset, then randomly generate small samples from each class and perform the test of equality. The fractions of 1000 iterations of this procedure that result in the rejection of the null hypothesis are presented in Table 2. The selection of classes is done at random so some class comparisons may be duplicates (though the samples being compared are unique). Among the graph-theoretic tests, there is little practical difference in power, though the power of the KMST test is typically slightly higher. The energy test typically has comparable power (if not slightly more) for all datasets except Amazon.Overall, the triangle test is not competitive with the other tests. We believe the reason is that the triangle statistic considers all triangles that do not contain three points with the same sample identity. Rosenbaum (2005, p. 516) has noted that some edges in the 1MATCH are “better than others” (he proposes a weighted version of the test statistic), and it is likely true that some triangles (those that connect nearby points) are more informative about differences between samples than others (those that connect points that are far from each other). Our guess is that including largely irrelevant triangles in the test statistic diminishes its power. The reason that the triangle test has 100% power for scale alternatives in high dimensions is due to the emptiness of high-dimensional space. As discussed previous, all triangles are relevant in this scenario.We return to the original problem of testing whether there is any difference in the distributions ofd=53seven-point Likert scale responses (regarding initial attitudes towards statistics) among students inS=8introductory sections of statistics. The samples sizes are mixed, withn1=38,n2=62,n3=151,n4=32,n5=166,n6=88,n7=66, andn8=65. With such large samples, any important or large differences in the distributions should be easy to detect. Using the recommended values ofK, we find that thep-values (using 10,000 permutations of the sample indices) of the KMATCH, KMST, KNN, and energy statistics are 0.0005, 0.0084, 0.0121, and 0.0000, respectively. The null hypothesis is rejected and we conclude that there are statistically significant differences between the sections (the energy test also rejects the null hypothesis).Interestingly, if we had usedK=1, thep-values of the graph-theoretic tests would have been 0.243, 0.456, and 0.936, respectively. Thus, using the orthogonal graphs and the recommend value ofKas critical. After using a Bonferroni correction and analyzing pairs of sections, we find four statistically significant differences. Nearly all of the statistically significant differences involved one particular instructor. We believe this is because this instructor wrote and administered the survey with the intention of using results for future research and this instructor also had the highest participation rate.

@&#CONCLUSIONS@&#
