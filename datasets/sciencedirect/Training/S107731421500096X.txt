@&#MAIN-TITLE@&#
Bundle adjustment using aerial images with two-stage geometric verification

@&#HIGHLIGHTS@&#
A new SfM pipeline that uses aerial images as external references is proposed.Good matches between ground and aerial images are found by two-stage verification.Consistency of orientation and scale from a feature descriptor is locally verified.Outliers are removed by global verification with sampling based bundle adjustment.

@&#KEYPHRASES@&#
Structure-from-motion,Bundle adjustment,Aerial image,RANSAC,

@&#ABSTRACT@&#
In this paper, a new pipeline of structure-from-motion for ground-view images is proposed that uses feature points on an aerial image as references for removing accumulative errors. The challenge here is to design a method for discriminating correct matches from unreliable matches between ground-view images and an aerial image. If we depend on only local image features, it is not possible in principle to remove all the incorrect matches, because there frequently exist repetitive and/or similar patterns, such as road signs. In order to overcome this difficulty, we employ geometric consistency-verification of matches using the RANSAC scheme that comprises two stages: (1) sampling-based local verification focusing on the orientation and scale information extracted by a feature descriptor, and (2) global verification using camera poses estimated by the bundle adjustment using sampled matches.

@&#INTRODUCTION@&#
Structure-from-motion (SfM) is one of the key techniques developed in the field of computer vision and has been used in many applications, such as three-dimensional reconstruction and image-based rendering. SfM became a widely used tool after implementations of the state-of-the-art SfM (Bundler [1], VisualSFM [2], etc.) were distributed by their authors. They are very useful for processing a short image sequence. However, one significant problem in SfM, that is, the accumulation of estimation errors in a long image sequence with km order camera movement, remains to be solved. In this paper, to reduce accumulative errors in SfM, we propose a sampling-based bundle adjustment (BA) scheme using the aerial images that are already available for most outdoor scenes as external references.Although many types of external references, e.g., 3D models [3–8], GPS [9,10], and road maps [11], have been used for reducing accumulative errors in SfM, we focus primarily on aerial images owing to their availability for outdoor environments. The existing methods using aerial images [12–18] are based on feature matching between given aerial images and ground-view images taken by standard cameras. Unfortunately, existing methods can handle only a short image sequence that does not include difficult situations. In this paper, we tackle more difficult situations where a large number of similar/repetitive patterns exist and/or only a few texture patterns are available in a long image sequence, e.g., uniformly tiled ground or a road environment, where most of the available feature points are on uniform road signs drawn on the ground surface. Even if we can approximately limit the search area of feature points by using GPS, which is also commonly used as an external reference in studies in the literature, if we depend on a local consistency check in the feature matching stage, in principle, it is not possible to remove all incorrect matches for a long image sequence because of the existence of repetitive and/or similar patterns.In order to overcome this problem, we remove incorrect matches caused by repetitive/similar patterns by introducing a RANSAC framework [19] into both the feature matching and BA stages that verifies the local and the global consistencies among estimated camera poses and matched features. Fig. 1shows the flow of the proposed method. For local feature matching, in this study, we assume that we can approximately limit the area used for feature matching, by using, e.g., GPS embedded in mobile devices. Fig. 2(a) shows an example of a conventional feature matching result obtained by using a common combination of SIFT [20] and RANSAC for an aerial image containing many repetitive patterns. Even after limiting the search area and rectifying the ground-view image to facilitate matching, incorrect matches (blue dashed lines in the figure) are often erroneously determined to be inliers. In this scene, SIFT finds 158 tentative matches of which only five are correct. In order to successfully determine the incorrect matches as outliers, we modify the verification process of RANSAC so that it additionally checks the consistencies of matches according to the scale and orientation information from feature detectors and descriptors.Although it is expected that most of the incorrect matches will be determined as outliers by this local verification method, some incorrect matches may simultaneously satisfy the consistency in the position, scale, and orientation, because the ground textures have similar structures. Fig. 2(b) shows an example in which incorrect matches (blue dashed lines in the figure) are found even when the consistency check for scale and orientation is used. To remove these remaining incorrect matches, in the BA stage, we verify the global consistency of matches and poses for all the images using the RANSAC framework. More precisely, camera poses are first estimated by the BA scheme using sampled matches as external references (red line and triangle in the figure), and the consistency between the estimated poses and each match is then checked. After iterating sampling and estimation, the best samples that maximize the number of consistent frames are selected as inlier frames in which it is expected that incorrect matches will be excluded. When good feature matches have been obtained through the two-stage verification, the camera poses are refined by the BA scheme using the feature matches as external references to remove accumulative errors.It should be noted that the proposed method assumes that an SfM result for ground-view images is given as an initial guess for the BA. The camera model for an aerial image can be approximated by the orthographic camera model, and its image plane is perpendicular to the gravity direction. In addition, approximate positions and gravity directions of the ground-view images are given by external sensors. An easy method for obtaining this information, which is employed in this study, is to use the GPS and gyroscope sensors embedded in most recent smartphones. It should be noted that this paper is an extended version of a previous conference paper [21]. We have added experiments using a roadway and an in-depth discussion in this version.To reduce accumulative errors in SfM, loop closing techniques [22–24] are sometimes employed. When the loops have been detected, the accumulative errors can be reduced in the BA stage. In an approach related to loop closing, Cohen et al. [25] exploited symmetries, which often exist in man-made structures, instead of loops. Although these techniques are effective for some applications, it is essentially difficult for these techniques to remove accumulative errors for a general image sequence without either loops or symmetry.To reduce accumulative errors in an image sequence captured by a moving camera, several kinds of external references have been used. These external references can be classified into 3D models [3–8], GPS [9,10], road maps [11], and aerial images [12]. In studies in the literature, many types of 3D models, including 3D points [3,4], wire-frame models [5], plane-based models [6,7], textured 3D models [8], and digital elevation models (DEM) [7] were employed as references. One disadvantage of 3D-model-based methods for large outdoor environments is that time consuming manual intervention is required to create the 3D models. Although some models are already available in the GIS database [6,7], the available areas are still limited to large cities. One method to create 3D models without much manual intervention is to use 3D reconstruction techniques, e.g., SfM and multi-view stereo [26]. However, the reconstructed models are also affected by accumulative errors caused by SfM itself.In contrast to 3D models, GPS, road maps, and aerial images are already available for most outdoor scenes around the world. Yokochi et al. [9] and Lhuillier [10] proposed extended-BA using GPS that minimizes the energy function defined as the sum of reprojection errors and a penalty term of GPS. This method can globally optimize camera poses and reduce accumulative errors by updating poses so as to minimize the energy function. However, the accuracy of this method is directly affected by errors in GPS positioning, which easily grow to several tens of meters in urban areas when using the GPS embedded in smartphones. Brubaker et al. [11] proposed a method that uses community developed road maps. This method can reduce accumulative errors by matching the trajectory from SfM to road maps, unless there are ambiguities in the matched trajectories (e.g., straight roads and Manhattan worlds). Pink et al. [12] fused sparsely obtained camera poses from aerial images into SfM by using the Kalman filter. However, unlike BA-based fusion, global optimization is difficult in the Kalman filter-based approaches.There exist several methods that use aerial images as one of input data of SfM [27] or RGBD-SLAM [28]. Shan et al. [27] employed oblique aerial images as additional inputs of SfM for reconstructing the regions that are not covered by ground-view images. One challenge in our case is the employment of top-view aerial images as an external reference in which common feature matching method used in [27] cannot give reasonable matches. For obtaining matches between top-view aerial images and ground-view images, Forster et al. [28] utilized dense depth maps obtained from perspective aerial images in the feature matching stage. Although feature matching methods from widely different viewpoints [29,30] also work in the case depth/3D information is available, unfortunately, the information is not always easy to be obtained from commonly available orthographic aerial images.On the other hand, some methods estimate camera poses directly from aerial images [13–18]. There are two types of aerial images: perspective and orthographic. Bansal et al. [13] proposed a method for estimating camera poses by matching façades in the ground-view input image with perspective aerial images. Although perspective aerial images are available on Google Maps and Microsoft Bing Maps, the available areas are still limited to large cities.Most methods using aerial images employ the orthographic aerial images. These methods can be classified into learning- [14] and feature-matching-based [15–18]. Lin et al. [14] proposed a method based on the relationship of the appearance between ground-view and aerial images learned through community photos with position information. Although this method estimates camera positions from large regions (1600 km2 in their experiments), camera positions can be estimated only approximately. Other methods match the building edges [15,16] or feature points [17,18] of ground-view and aerial images. One of the difficulties of this approach is finding good matches for all the images of a video sequence under severe conditions for feature matching. Toriya et al. [17] and Noda et al. [18] relaxed the problem by stitching multiple ground images for feature matching. Toriya et al. [17] also proposed a robust feature matching procedure that compares the orientation and scale of each match with the dominant orientation and scale. Unfortunately, existing feature matching-based methods, which are expected to achieve highly accurate pose estimation, do not have the capability to handle a long image sequence because of the strong dependence on matching information given only for a local region. To the best of our knowledge, no method exists that handles feature matches between an aerial image and ground-view images in the global optimization stage (BA stage).As mentioned in the previous section, we tackle difficult situations for feature matching, where a large number of similar patterns exist in a large-scale outdoor environment. The main contributions of this paper are summarized as follows:•BA-based global optimization that uses feature matches between ground-view and aerial images.Two-stage geometric verification for removing incorrect matches•Local verification that focuses on the transformation between aerial image and each ground-view image and considers in particular the orientation and the scale information extracted by a feature descriptor,Global verification that focuses on camera poses estimated using the BA scheme with sampled matches.This section describes a method to find good matches between an aerial image and each ground-view image with local verification. As shown in Fig. 3, the method is composed of three processes: (1) ground-view image rectification, (2) feature matching, and (3) local geometric verification by RANSAC.Before finding matches, as in existing methods [17,18], we rectify the ground-view images so that the texture patterns are similar to those of the aerial image. To achieve this, we use homography calculated from the gravity direction in the camera coordinate system. More precisely, we map the ground image to a plane that is perpendicular to the gravity direction. To estimate the gravity direction, the vanishing points of parallel lines [31] or a gyroscope sensor can be used. Since even a cheap gyroscope sensor provides an accurate gravity direction, we used a gyroscope embedded in a smartphone in the experiment described below. Even if the patterns cannot be perfectly rectified because of the irregularity of the ground plane, it is expected that the chance of obtaining correct matches will be increased by using this rectification process.A region in an aerial image for feature matching is then determined. We first determine a certain size of the region the center of which is the GPS position, which includes measurement errors. In the experiment, the size is set to 50 m × 50 m. Tentative matches are then found between the rectified ground image and the limited region in the aerial image using a feature detector and a descriptor. Although we employed SIFT [20] in the experiment because of its robustness, any feature operators that output scale and orientation information can be employed in our framework. It should also be noted that a large GPS error may result in correct feature points outside the limited region. Even in this case, incorrect matches are automatically excluded by applying two-stage RANSAC with a geometric consistency check.Tentative matches often include many incorrect matches. The rate of incorrect matches sometimes reached over 95% in our experiment, even if the search range for matching was correctly set. In order to decrease the number of incorrect matches included in the tentative matches, we apply local geometric verification by using RANSAC with a consistency check of the orientation and scale of texture patterns, as shown in Fig. 4.Although final camera poses are estimated in six-DOF with BA, to achieve stable matching between rectified ground-view images and the aerial image, we use a three-DOF similarity transform, which is composed of scale s, rotation θ, and translationτ, as the model in RANSAC. In a standard RANSAC procedure, tentative matches of minimum number required for estimating the similarity transform (s, θ,τ), which are two matches in this case, are randomly selected first. The number of inlier matches that satisfy the following condition is then counted.(1)|ak−(sR(θ)gk+τ)|<dth,whereakandgkare the 2D positions of the kth match in the aerial image and the rectified ground-view image, respectively. R(θ) is the 2D rotation matrix with rotation angle θ, and dth is a threshold. After iterating the random sampling process, the trial with the largest number of inlier matches is selected.The problem here is that the distance-based single criterion described above cannot successfully find correct matches when there exists a huge number of incorrect matches. In order to achieve more robust matching, we modify the criterion commonly used in RANSAC by adding a consistency check for orientation and scale information extracted from a feature descriptor. More precisely, we select the matches that simultaneously satisfy Eq. (1) and the following two conditions as inliers in RANSAC procedure.(2)max(sgk·ssak,saksgk·s)<sth,(3)aad(θgk+θ,θak)<θth,where (sak, sgk) and (θak, θgk) represent the scale and orientation of feature points for the kth match on the aerial image and the rectified ground-view image, respectively. The function ‘aad’ returns the absolute angle difference in the domain [0°, 180.0°]. sth and θth are the thresholds for scale and angle, respectively. By using the additional consistency check, the feature matches are strictly verified, and it is expected that most of the incorrect matches will be removed as outliers. It should be noted that even though we employ a three-DOF model in RANSAC in this stage, as shown in the experiment in Section 5, feature points on slanted ground that violate the three-DOF model are successfully matched, since we can relax each threshold by simultaneously checking three criteria in this stage.As shown in Fig. 2(b), some frames contain incorrect matches even after local geometric verification because of repetitive similar patterns. In this study, as a global verification stage, we propose a new sampling-based BA scheme to find the frames that contain incorrect matches.In order to use the matches between ground-view and aerial images in BA as external references, as shown in Fig. 5, the energy function E is newly defined for this problem as the sum of reprojection errors for both ground-view (perspective) images Φ and the aerial (orthographic) image Ψ:(4)E({Ri,ti}i=1I,{pj}j=1J)=Φ({Ri,ti}i=1I,{pj}j=1J)+ωΨ({pj}j=1J),whereRiandtirepresent 3D rotation and translation from the world coordinate system to the camera coordinate system for the ith frame, respectively.pjis a 3D position of the jth feature point, I and J are the number of frames and feature points, respectively, and ω is a weighting coefficient that balances Φ and Ψ. Since the energy function is non-linearly minimized in BA, good initial values of parameters are required to avoid local minima. Before minimizing the energy function, we fit the parameters estimated by SfM to the GPS positions using a 3D similarity transform. In the following, two energy terms associated with reprojection errors Φ and Ψ are given in detail.In our method, camera poses and 3D positions of the feature points estimated by BA dynamically move in the world coordinate system, which is set on the aerial image coordinate, because of the tension from the external references (matches on the aerial image). Because of this dynamic camera movement, the 3D positions of the reference points on an aerial image frequently go behind the camera. However, the commonly used reprojection errors for the pinhole camera model cannot deal with projections from behind the camera. In this study, instead of the commonly used squared distance errors on the image plane, we employ the following angular reprojection error that is employed in SfM for omnidirectional cameras.(5)Φ({Ri,ti}i=1I,{pj}j=1J)=1∑i=1I|Pi|∑i=1I∑j∈PiΦij,(6)Φij=∠((xijfi),(XijZij))2+∠((yijfi),(YijZij))2,(7)(Xij,Yij,Zij)T=Ripj+ti,wherePiis a set of feature points detected in the ith frame. Function ∠ returns an angle between two vectors, (xij, yij)T is a detected 2D position of the jth feature points in the ith frame, and fiis the focal length of the ith frame. By this definition, the energy becomes large when projections behind the camera occur.Here, as mentioned in [32], the convergence of energy is very poor with an angular reprojection errorΦ^ij=∠((xij,yij,fi)T,(Xij,Yij,Zij)T)2. We then split the angular reprojection error into xz and yz components in order to simplify the Jacobian matrix of E required by non-linear least squares methods, such as the Levenberg–Marquardt method. The first and second terms of Φijdo not depend on the y and x components oftiin this definition. We experimentally confirmed that this splitting largely affects the convergence performance.The reprojection errors for the aerial (orthographic) image are defined as(8)Ψ({pj}j=1J)=1∑i∈M|Ai|∑i∈M∑j∈Ai|aj−pr(pj)|2,whereMis a set of frames in which matches between ground-view and aerial images are found,Aiis a set of feature points that are matched to the aerial image in the ith frame, andajis the 2D position of the jth feature point in the aerial image. The function ‘pr’ projects a 3D point onto the xy plane (aerial image coordinate system). Although the height of the 3D points is not affected by this term, the remaining 2D positions are constrained to their positions on the aerial image. These constraints are effective for reducing the accumulative errors through simultaneously minimizing both the perspective and orthographic reprojection errors in the BA.This section describes a RANSAC scheme introduced into BA for global geometric verification. Since the matches remaining after local verification are consistent in each frame, we judge inliers in a frame-wise manner. First, we randomly sample n frames from the frames that passed local geometric verification and execute BA using the matches in the sampled frames, i.e., using a set of sampled framesM′ instead ofMin Eq. (8). We then check the consistency between the camera poses obtained by BA and each frame that includes feature matches. More precisely, we count the number of inlier frames that satisfy the condition(9)averagej∈Ai(αij)<αth,where αijis an angular reprojection error of the jth feature point on the aerial image coordinate system, as shown in Fig. 6, and αth is a threshold. Here, αijis computed as(10)αij=∠(aj−pr(−RiTti),pr(RiT(xij,yij,fi)T)).After iterating the random sampling process at given times, the trial that has the largest support is selected. Finally, camera poses are refined by executing BA again using the feature matches in the selected inlier frames.In the experiments described below, the threshold αth is experimentally determined. It should also be noted that biased sampling, where samples are close to each other, frequently yields an unstable result in RANSAC. Thus, we modify the random sampling process of frames so that the distances between the average positions of matches on an aerial image are larger than threshold lth.

@&#CONCLUSIONS@&#
