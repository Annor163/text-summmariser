@&#MAIN-TITLE@&#
Evaluation of system efficiency using the Monte Carlo DEA: The

@&#HIGHLIGHTS@&#
Monte Carlo-DEA assessed the Small Health Areas (SHA) efficiency.Expert knowledge assigned statistical distributions for the variables.Input/output combinations analysed different perspectives of SHA efficiency.Best combinations were used for benchmarking.k-means analysis validated a panel of experts’ classification.

@&#KEYPHRASES@&#
Data Envelopment Analysis,Simulation,Expert knowledge,Operations research in medicine,Efficiency of health care areas,

@&#ABSTRACT@&#
This paper uses Monte Carlo Data Envelopment Analysis (Monte Carlo DEA) to evaluate the relative technical efficiency of small health care areas in probabilistic terms with respect to both mental health care as well as the efficiency of the whole system. Taking into account that the number of areas did not permit maximum discrimination to be achieved, all the scenarios of non-correlated inputs and outputs of a specific size were designed using Monte Carlo Pearson to maximize the discrimination of Monte Carlo DEA and the information included in the models. A knowledge base was included in the simulation engine in order to guide the dynamic interpretation of non-standard inputs and outputs. Results show the probability that all DMU and the whole system have of being efficient, as well as the specific inputs and outputs that make the areas or the system efficient or inefficient, along with a classification of the areas into four groups according to their efficiency (k-means cluster analysis). This final classification was compared with an expert-based classification to validate both the knowledge base and the Monte Carlo DEA model. Both classifications showed results that were very similar although not exactly the same, basically due to the difficulty experts experience in recognizing “intermediately-inefficient” DMU. We propose this methodology as an instrument that could help health care managers to assess relative technical efficiency in complex systems under uncertainty.

@&#INTRODUCTION@&#
Introduced by Charnes, Cooper, and Rhodes (1978), Data Envelopment Analysis (DEA) is a non-parametric method that evaluates the relative technical efficiency of a set of comparable Decision Making Units (DMU), each using multiple inputs to produce multiple outputs. DEA has been used in many decisional situations related to health care (Brandeau, Sainfort, & Pierskalla,. 2004; Cheng & Zervopoulous, 2014; Du, Wang, & Chen, 2011; Kontodimopoulos, Bellai, Labiris, & Niakas, 2006; Ozcan, Lins, Da silva, Fiszman, & Pereira, 2010; Prior, 2006; Salvador-Carulla, Garcia-Alonso, Gonzalez-Caballero, & Garrido-Cumbrera, 2007). However, in these complex stochastic systems, the application of standard DEA has some relevant drawbacks, such as that: (i) the results obtained do not agree with previous and well established expert opinions (Salvador-Carulla et al., 2007) if inputs and outputs (I/O) are not correctly interpreted (non-standard I/O); (ii) input and output values (variables in DEA models) are stochastic and the selection of appropriate statistical distributions to fit them is not a trivial matter; and (iii) the number of observations to be evaluated in the system (relative technical efficiency) is usually low compared to the number of I/O, which compromises the discriminating power of DEA models. For (i), weight control using specific constraints can be useful (DEA can have feasibility problems if these constraints are very restrictive) but only when I/O are correctly interpreted. In (ii), the statistical distribution selection for I/O values (all of them random variables) depends on the availability of enough real data (Kolmogorov-Smirnov-Lillefors or maximum likelihood tests can be used) or, instead, on expert knowledge. This second option is very frequent and usually needs to check different statistical distributions before obtaining results. Finally, for (iii), other models like order-α, and order-m, can be used (Cazals, Florens, & Simar, 2002; Wheelock & Wilson, 2003, 2004a, 2004b), but DEA is also a nice option if discriminating enough scenarios (Dyson et al., 2001) are designed (combinations of non-correlated I/O). This approach (Salvador-Carulla et al., 2007) has another relevant advantage: each scenario offers a different perspective of the relative technical efficiency of the system under study, and the analysis becomes multi-dimensional for experts and decision makers.Recently, the integration of expert knowledge in operational models has been the object of significant scientific attention (Bose, 2003; Salvador-Carulla et al., 2007). Decision makers, who deal with the intrinsic risk of their decisions, need reliable and useful solutions from operational models like DEA, order-α, order-m, etc. Evaluating these solutions negatively can be because the initial formulation of the operational model does not match the real framework under analysis (it cannot represent the complexity of the environment). Incorporating a knowledge base in DEA models is a requirement to help them interpret I/O values and their relationships, especially when they are non-standard, undesirable or flexible I/O (Cook & Zhu, 2007; Gibert, García-Alonso, & Salvador-Carulla, 2010; Seiford & Zhu, 2002) that have to be managed.A knowledge base designed by information transfer between experts and analysts in any DEA model includes: (i) I/O types—standard or non-standard—with different ranges of values; (ii) their corresponding statistical distribution; and, finally, (iii) their relative relevance (weights) in the system. Results are, therefore, guided by explicit expert knowledge and, in addition, can also be used to improve experts’ knowledge in a circular pursuit of excellence.When DEA models evaluate complex systems, I/O may have flexible measures. Some variables can play I/O roles depending on the circumstances (Cook & Zhu, 2007) and can have different meanings for experts depending on their specific values. Non-standard, undesirable or flexible I/O are very frequent in health care systems and can be handled in DEA models using four different basic approaches (Liang, Yongjun, & Shibing, 2009): (i) the hyperbolic measure approach (Färe, Grosskopf, Lovell, & Pasurka, 1989); (ii) the transformation of non-standard outputs to inputs and vice-versa (Hailu & Veeman, 2001; Reinhard, Lovell, & Thijssen, 2000); (iii) the data transformation function approach (Athanassopoulos & Thanassoulis, 1995; Lovell, Pastor, & Turner, 1995; Scheel, 2001; Seiford & Zhu, 2002); and, finally, (iv) the directional distance-function approach (Chung, Färe, & Grosskopf, 1997; Färe & Grosskopf, 2004).In stochastic DEA, the I/O interpretation (non-standard or flexible I/O) has to be guided by the knowledge base to apply the appropriate mathematical transformation.Standard DEA models always assume that I/O values are known, but in real-life decisional situations (as in health care management), I/O are stochastic because they can be missing or based on expert judgment and/or predictions (Zhu, 2003). Stochastic DEA models explore data variation and there are many approaches to this operational problem (Dyson & Shale, 2010): sensitivity and stability analysis (Färe, Grosskopf, & Lovell, 1994; Neralic, 2004; Seiford & Zhu, 1998), stochastic frontiers (Banker, 1993; Banker & Natarajan, 2004; Ruggiero, 2004), chance-constrained DEA (Land, Lovell, & Thore, 1993; Olesen & Petersen, 1995), fuzzy DEA (León, Liern, & Ruiz, 2003; Lertworasirikul, Fang, Joines, & Nuttle, 2003), Imprecise DEA (Zhu, 2003; Cook & Zhu, 2006) and Monte Carlo DEA (Kao & Liu, 2009; Krüger, 2012; Perelman & Santín, 2009).According to Ingalls (2008), Monte Carlo simulation is a computational tool for modelling and analysing complex systems under uncertainty. This procedure lets operational modellers analyse complex and stochastic models with fewer assumptions (Kao & Liu, 2009) and therefore allows for the design of a more faithful representation of the real system. A Monte Carlo simulation engine can include almost any operational model made to design experiments to check DEA properties (Banker, Gadh, & Gorr, 1993; Perelman & Santín, 2009; Ruggiero, 1999), but has not yet been widely enough used to evaluate probabilistic relative technical efficiency.In Monte Carlo DEA, the I/O values of each DMU are simulated from their statistical distribution to determine the distribution of each DMU's relative technical efficiency. Therefore, Monte Carlo DEA understands that both I/O values and their resulting DMU relative efficiencies are stochastic. The selection of the statistical distribution for I/O in the knowledge base is always critical because simulation results are heavily dependent upon it.On the other hand, the selection of the I/O for a DEA model is usually forced by the data available and can be subjective because it includes expert assumptions about system behaviour (Allen, Athanassopoulos, Dyson, & Thanassoulis, 1997; Cook & Zhu, 2007). Specifically for DEA models, the number of the I/O selected needs to be small compared to the total number of DMU, to reach an effective discrimination of the latter. Dyson et al. (2001) suggested that the number of DMU n should be at least two times the product of the number of inputs s and number of outputs r (n ≥ 2sr). If this does not happen, some of the I/O should be removed from the DEA model, but this selection is rarely obvious. Statistical correlation offers DEA modellers additional information that can be used for this selection (Farzipoor Saen, Memariani, & Hosseinzadeh Lotfi, 2005; Jenkins & Anderson, 2003). When I/O are stochastic, Monte Carlo simulation can also be used to carry out the correlation analysis (i.e. Monte Carlo Pearson) to design non-correlated I/O combinations of a specific size (scenarios).The main objective of this paper is to demonstrate that it is possible to include random variables (inputs/outputs) in a relative technical efficiency-analysis to avoid problems associated with other approaches like Imprecise-DEA. For this, a Monte Carlo DEA model was applied to evaluate the relative technical efficiency of a system composed of 12 smallhealth-care areas. This includes the evaluation of technical efficiency in probabilistic terms in both the DMU and the system as a whole. Our model also includes a knowledge base for the algebraic interpretation of inputs/outputs. This evaluation could help health care managers to provide solutions for improving operational efficiency and to reduce the wasting of resources, while assuring a higher level of service quality.This paper is structured as follows: Section 2 describes the methodology; an application to assess the efficiency of mental health care areas is carried out in Section 3; and, finally, some illustrative comments and conclusions are drawn in Sections 4–6.The Monte Carlo DEA methodology can integrate any DEA model into a simulation engine. The former evaluates the relative technical efficiency of each DMU once the latter has determined the I/O values for a specific simulation. This procedure generates a hybrid (statistical and operational) model that can be generalized by integrating other operational models like order-α and order-m ones (Wheelock & Wilson, 2003, 2004a, 2004b). According to this strategy, the process has four sections (Table 1): (i) I/O values are determined at random according to their specific statistical distribution (steps 1, 2, 17 and 18); (ii) original I/O values are interpreted—mathematically transformed—according to the knowledge base (steps 3–5); (iii) DEA models are designed automatically and solved (steps 6–10) and the solutions—relative technical efficiency—are saved in a file; finally, (iv) the statistical error is calculated (Nakayama, 2008) and if it is low enough, the simulation process ends (steps 11–15).The number of simulations performed for each DMU and scenario depends on the statistical error that can make the simulation process terminate (Nakayama, 2008). Taking into account that our Monte Carlo DEA model is a steady-state process, the non-overlapping batch means the method is used to evaluate statistical error. The simulation process finishes once the error is lower than a small predefined value (the percentage over the mean of efficiency scores); once this situation is reached repetitively, the number of simulations does not greatly affect the efficiency mean (Kao & Liu, 2009).For a number d of DMU, the number of all possible I/O mathematical combinations without repetition (scenarios) c can be very high:(1)c=(ni)(pj)where n is the total number of inputs, p the total number of outputs, i the number of inputs selected for the DEA model and, finally, j the number of outputs selected for the same DEA model.Our decisional framework is composed of 12 DMU of mental health care, eight inputs and four outputs. This system cannot be analysed in a standard way (Dyson et al. 2001) because the rule 2 × n × p ≤ d is not fulfilled (2 × 8 × 4 > 12), and the resulting models will not be discriminating enough.If scenarios of three inputs by two outputs are designed, the number of all possible combinations (1) is 336, great enough to make the problem computationally unfeasible. The number of scenarios can be reduced taking into account those combinations which are composed of non-correlated sets of i, j I/O. The selection of non-correlated I/O avoids the problem of a discretionary selection of variables in DEA models. These scenarios offer different technical perspectives for the same decisional situations and can be easily interpreted by experts because their structures are simpler. On the other hand, some I/O can be over-weighted because they might block others in the design process. If something like this happens, an appropriate selection of feasible scenarios is required.Our decisional problem is stochastic and the correlation analysis cannot be standard. Monte Carlo correlation analysis (Pearson) is used to evaluate correlations between I/O according to their specific statistical distributions. This procedure generates thousands of 12 (DMU) by 12 (8I+4O) matrixes and evaluates the correlation coefficient between I/O (α = 0.05). If the number of simulations where two I/O correlate significantly is greater than an expert-based predefined value (for example, greater than 95 percent of the simulations), and the mean of the correlation coefficients is greater than or equal to another predefined value (for example, 0.9; Farzipoor et al., 2005), then the two variables are highly correlated.Once the scenarios are designed, Monte Carlo DEA systematically calculates the relative technical efficiency of each DMU in each scenario. Once the result sets are obtained, both the DMU probability of being efficient as well as its basic statistics can be calculated easily. For each DMU, in probabilistic terms, many different views of relative technical efficiency (one for each scenario) are obtained as well as the global behaviour of the system.Expert knowledge is absolutely necessary to guide the simulation process because of: (i) the selection of appropriate statistical distribution for all I/O, and (ii) the interpretation of non-standard I/O values in each simulation (Salvador-Carulla et al., 2007). These non-standard I/O need to be interpreted according to an expert-driven model because, if not, DEA will give incorrect results. For example, TR2, the available number of R2 type care units in a small health care area (Fig. 1), is a non-conventional variable because values close to 1 are considered very appropriate by health planners. Taking into account that this is an input, the best value that DEA can manage for it is ε, a non-Archimedean number smaller than any real number—almost zero—(no input consumption). Values close to 1 need to be reduced and DMU with these original input values will prove to be better from a relative technical efficiency point of view.The Monte Carlo DEA randomly generates a different DEA model for each DMU and simulation. The original I/O values (selected according to their statistical distributions) can instantiate corresponding rules in the knowledge base. These rules are structured in a rulebase—a standard relational database—included in the main knowledge base. The structure of the rulebase includes the information needed to transform the original I/O values algebraically (2) when specific conditions—I/O values—are fulfilled (Fig. 1).(2)xitr=xior⊗fiwherexitris the transformed I/O value,xioris the original value calculated by the simulation engine and, finally, fiis the function that transforms the original I/O value according to expert knowledge. This function is usually a linear function that increases or decreases the original value to a more appropriate one according to expert knowledge used. For example, in Fig. 1 the linear monotone transformation in TR2∈[1, 1.5) isxitr=3xior−3.The knowledge base was designed by a panel of assessment and management experts in mental health care systems (Salvador-Carulla et al., 2007). This previous effort identified 12 variables (eight inputs and four outputs, Table 2) according to the European Service Mapping Schedule model (Johnson & Kuhlmann, 2000). The I/O interpretation (Tables 3and 4) generated an Expert-driven Model of Community Care that was the core of the knowledge base in our Monte Carlo DEA model (Table 2).Following this structure, the knowledge base identifies: (i) the specific I/O to be interpreted; (ii) the ranges for each I/O where, according to expert opinion, a special interpretation is needed; and, finally, (iii) the specific transformation (usually linear monotone transformation, Fig. 1) of the original I/O value within the previous range (steps 3–5 in Table 1). In Fig. 1, an example for the input TR2 (the available number of R2 type care units in a small health care area) and three DMU are developed (D1, D2 and D6). For example, an input like TR2 in our model has three different interpretations depending on its value (Fig. 1): between [0,1), TR2 is a non-standard input (the greater the original value, the lower the transformed value); between [1,1.5), there is a strong penalization from almost zero up to 1.5 (the greater the original value, the greater the transformed value); and, finally, TR2 [1.5,20] is considered a standard input (it does not undergo any transformation).In a steady-state simulation model where the system evolves in an infinite horizon, the evaluation of error is delicate basically due to the calculation of the variance (Nakayama, 2008). The existence of independent and identically distributed (normal distribution) data cannot be assumed so the estimation of the output variance is not a trivial issue. The method of multiple replications (Nakayama, 2008) is used to deal with this drawback. This method is based on designing (sufficiently large) independent and identically distributed replications r, each of them with k simulations. Independence is achieved by generating non-overlapping series of random numbers in the simulation process. The replications are identically distributed if all of them commence under the same initial conditions. Taking this process into account, a sample variance of r simulated values can be calculated, as well as their error and confidence intervals.This error is critical because it determines when the simulation engine stops (it defines the number of simulations nsim, Table 1). When the error (in percentage over the mean) is repeatedly lower than a specific number (usually, 2.5 percent), the steady state model stops.Monte Carlo simulation models are completely blind, which implies that the validation of the results they give is one of the main challenges to be dealt with. In our case, the Monte Carlo DEA model gives us the probability that each DMU has of being efficient (or inefficient), taking into account their I/O statistical distribution and the expert-driven model. These probabilities are therefore biased by the expert knowledge integrated into our model and, without any additional information, cannot be validated at all. For validation purposes, the panel of experts that designed the B-MHCC model also classified the 12 DMU into four groups: efficient, nearly efficient, inefficient and doubtful. This expert-based classification is used to validate both the expert-driven model and the Monte Carlo DEA models. Once the probabilities that each DMU and scenario has of being efficient have been calculated, a k-means cluster analysis is carried out to classify the DMU into four groups (k = 4) in order to match it to expert-based classification. An intra-class correlation analysis is carried out to evaluate the degree of agreement among them (McGraw & Wong, 1996).The PSICOST-12 database used to check the Monte Carlo DEA model includes 12 widely different DMU in Spain, described by their health care structure (Table 2) classified according to the European Service Mapping Schedule model. All existing health and social services for mental health care were evaluated in every DMU by an external expert. The data for each catchment area were aggregated into residential use, structured day activities, continuous out-patient care and emergency out-patient care, as shown in Table 2. The variables chosen are standard according to the eDESDE-LTC instrument (Salvador-Carulla, Poole, Bendeck, Romero, & Salinas, 2009). This instrument was developed in a European Union 7th framework project (http://www.edesdeproject.eu/). The selection of specific variables in each scenario was done according to two criteria:•finding sufficiently discriminating DEA models (Dyson et al., 2001). The number of DMU (12) is very small: in this situation the problem of dimension must be taken into account (Alirezaee, Howland, Van de Panne, 1998; Staat, 2001) The Monte Carlo DEA model completely analyses the statistical distributions selected for inputs/outputs. This process shows if there are relevant changes in the relative technical efficiency distribution when input/output values vary. If this occurs, Monte Carlo DEA can identify the specific input/output ranges where the size of the example becomes critical.To avoid correlated I/O. All of the selected 3×2 combinations (scenarios) have non-correlated I/O.All of the variables have been described previously (Salvador-Carulla et al., 2007) and all have a specific meaning for decision makers (health planners).According to the expert driven model (Table 2), all DMU inputs and outputs were interpreted numerically (Tables 3 and 4) and their values were fitted to triangular statistical distributions (some examples in Table 5, data available on request) through modal estimators (Table 4 shows an example for D1, D6 and D9). The knowledge base includes 144 statistical distributions—12 × (8 + 4)—in addition to the rules generated by the expert-driven model.The DEA model selected (3) was the standard input-oriented BCC model—variable returns to scale (steps 7 and 8 in Table 1). The BCC model was selected because there is no evidence that inputs/outputs in small health areas can show constant returns to scale behaviour (Salvador-Carulla et al., 2007). In the assessment of health areas, input consumption is always the real problem once the outputs are known: this is why the input-oriented model was selected for the example.Once the simulation engine has generated I/O values, they can be interpreted according to the expert-oriented rulebase structure. This process randomly generates expert-interpreted BCC-DEA models. The rules in the rulebase are automatically instantiated once I/O values have been generated. All the rules are semantically expressed and formalised in a standard relational database.(3)Minθ−ɛ(∑h=1iSh−+∑r=1jSr+)s.t.∑m=1dxhmλm+Sh−=θxho;h=1,2,…,i∑m=1dyrmλm−Sr+=yro;r=1,2,…,j∑m=1dλm=1λm≥0;m=1,2,…,dWhere d is the number of DMU, i the number of inputs and j the number of outputs. The DMU m consumes xhmof input h and produces yrmof output r. θ is the efficiency score andSh−andSr+are the slacks.The existence of outliers is usually a big problem in DEA when the number of DMU is low. Other methods like order-α and order-m are less sensitive to this situation and can also be included in the simulation engine (Wheelock & Wilson, 2003, 2004a and 2004b).Accordingto Dyson et al. (2001), 12 DMU do not make the BCC-DEA model discriminating enough with eight inputs and four outputs. In this case, a scenario analysis was designed to improve the global design. These scenarios of non-correlated I/O were designed automatically once the Monte Carlo Pearson model (two-tailed value 0.708 with 10 degrees of freedom, α = 0.05 and number of simulations = 10,000) had shown the existing correlations between I/O. In order to reach a compromise between the maximum discrimination of BCC-DEA models and the maximum information to be included, scenarios with i = 3 inputs and j = 2 outputs were designed.As stated before, our simulation model is steady state. The number of simulations, nsim (Table 1), was determined by the statistical error; the simulation engine stopped when this error (percentage over the mean) was repeatedly (20 times) lower than 2.5 percent. The error was calculated using 10 batches (r = 10) after the first 500 simulations. For safety reasons, a maximum number of simulations of 10,000 was selected for stopping the process if the error did not converge (Table 1).

@&#CONCLUSIONS@&#
It is absolutely possible to include an operational model and a knowledge base in a Monte Carlo simulation engine. This procedure lets researchers and potential users analyse relative technical efficiency in complex systems where some or all variable values are statistical distributions.Health systems and health areas are complex phenomena which require data analysis methods that consider non-linearity (De Savigny & Adam, 2009). This study demonstrates that Monte Carlo DEA can be successfully used in complex probabilistic DEA models, whatever their structure might be, to determine the statistical distribution of technical efficiency of either any DMU—small health area—or the whole system. The main advantage of this method is that explicit expert knowledge can be included once it is formalized in a standard way (for example using a relational database). Expert knowledge guides operational analysis and, on the other hand, results obtained from the analysis can improve expert knowledge in a positive iterative process. Expert knowledge structures the knowledge-base that includes the statistical distribution selected for I/O, the Expert-driven Model of Community Care and the management of I/O weights.Relative technical efficiency depends upon the statistical distribution selected for each DMU and I/O. The existence of any kind of empirical evidence with respect to its behaviour is really critical for the design of the model. The design of the Expert-driven Model of Community Care needed a panel of management experts in health care systems who, in the end, could validate the results obtained by the Monte Carlo DEA model. Finally, I/O weights need to be carefully managed because they can control the relevance of specific I/O in all or some of the scenarios designed and can make many algebraic models unfeasible.The Expert-driven Model of Community Care is composed of rules that are instantiated once the I/O original values have been determined by the simulation engine. These rules can be more or less complicated, from linear monotone transformations (as in this paper) to fuzzy rules. The process of evaluating rule outputs is completely automatic and can be easily included in a Monte Carlo simulation engine.The design of non-correlated scenarios offers experts the opportunity to analyse various technical perspectives of the same problem. The aggregation of all scenarios to evaluate the efficiency of the whole system must be done very carefully to avoid an overweighting of some I/O that could dominate the scenario design process.The efficiency of each DMU is expressed in terms of probability and lets the operational designer evaluate potential improvements in its I/O. Taking into account the existence of multiple scenarios designed by sets of non-correlated I/O, I/O improvements in one scenario for a specific DMU can have unexpected results in other scenarios (positive or negative) and on the efficiency of the whole system. The study of these potential trade-offs is the next challenge facing Monte Carlo DEA.