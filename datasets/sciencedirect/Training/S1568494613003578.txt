@&#MAIN-TITLE@&#
Classification of silent speech using support vector machine and relevance vector machine

@&#HIGHLIGHTS@&#
We obtained EEG data and classified silent speech.We model system including CSP, SVM, and adaptive collection.We examine changes from SVM to RVM for feasibility study.We compare classification accuracies, numbers of vectors, and calculation costs.

@&#KEYPHRASES@&#
EEG,Silent speech,Brain computer interface (BCI),Support vector machine (SVM),Relevance vector machine (RVM),Adaptive collection,

@&#ABSTRACT@&#
To provide speech prostheses for individuals with severe communication impairments, we investigated a classification method for brain computer interfaces (BCIs) using silent speech. Event-related potentials (ERPs) were recorded using scalp electrodes when five subjects imagined the vocalization of Japanese vowels, /a/, /i/, /u/, /e/, and /o/ in order and in random order, while the subjects remained silent and immobilized.For actualization, we tried to apply relevance vector machine (RVM) and RVM with Gaussian kernel (RVM-G) instead of support vector machine with Gaussian kernel (SVM-G) to reduce the calculation cost in the use of 19 channels, common special patterns (CSPs) filtering, and adaptive collection (AC). Results show that using RVM-G instead of SVM-G reduced the ratio of the number of efficient vectors to the number of training data from 97% to 55%. At this time, the averaged classification accuracies (CAs) using SVM-G and RVM-G were, respectively, 77% and 79%, showing no degradation. However, the calculation cost was more than that using SVM-G because RVM-G necessitates high calculation costs for optimization. Furthermore, results show that CAs using RVM-G were weaker than SVM-G when the training data were few. Additionally, results showed that nonlinear classification was necessary for silent speech classification.This paper serves as a beginning of feasibility study for speech prostheses using an imagined voice. Although classification for silent speech presents great potential, many feasibility problems remain.

@&#INTRODUCTION@&#
Daily life demands that we use verbal and non-verbal communication. However, severely handicapped individuals such as people with advanced amyotrophic lateral sclerosis (ALS), locked-in syndrome, or nasopharyngeal cancer have difficulty expressing their thoughts. Their caregivers also face difficulties when caring for patients. A brainâ€“computer interface (BCI) has been developed to provide prosthetics for such individuals.The anticipated benefits of the BCI are not merely confined to those individuals. They are expected to be useful for entertainment, personal communication, and game devices, and with preventive medical treatments for healthy individuals. When both healthy individuals and those with a disability use the same core technology, the demand shown by healthy people is expected to contribute to the welfare of handicapped individuals through improved production and reduced costs of assistive equipment. We seek to develop devices that are attractive for both handicapped and healthy people. The objective technology requires portability, high classification accuracy, and usability.For these supporting prosthetics, many studies have been conducted using methods such as P300 speller [1], steady-state visual evoked potentials (SSVEP) speller [2], SSVEP cursor controller [3], and near infrared spectroscopy (NIRS) [4]. For the P300 and SSVEP spellers, users must gaze on the attempted word. With the SSVEP cursor controller, subjects must undergo training to move cursors using electroencephalography (EEG). In the method using hemodynamic response, e.g., NIRS, users must train in the mode of imagining calculations or imagining fast songs for detection. The methods described above necessitate training of skills that users have never developed in daily life.The classification of silent speech is a simple method that requires no special training. Many silent speech interface studies have used electromyographic (EMG) signals [5], electromagnetic field measurements with implanted magnets [6], and ECoG signals detected using invasive electrodes [7]. The method using EMG signals requires electrodes mounted on the user's face or neck. The system is uncomfortable and fragile. The method using magnet implantation around a patient's mouth is effective, but it requires surgical operations. Severely paralyzed patients might accept surgical operations, but healthy individuals would not accept them. Moreover, any method using invasive electrodes necessitates surgical operations. The detection of silent speech by EEG is a good method in terms of portability and user-friendliness.In the last paper, we classified silent speech of vowels using EEG, as measured using scalp electrodes, common spatial pattern (CSP) support vector machine with Gaussian kernel (SVM-G), and adaptive collection (AC) [8]. We demonstrated the potential of silent speech.Regarding feasibility, some problems arose: The classification accuracies were insufficient when using the small number of electrodes and it needs multi-class classification and online processing. The SVMs are well known as a pairwise classifier. For online processing, classification speed is important and large calculation cost is problem, but SVM with Gaussian kernel (SVM-G) has hyperparameters that must be optimized by cross validations.The relevance vector machine (RVM) was proposed as a method to reduce the number of relevance vectors and optimize the hyperparameters automatically [9,11].In this study, we recorded EEGs of five healthy subjects when they imagined the vocalization of the Japanese vowels, /a/, /i/, /u/, /e/, and /o/, while they remained silent and immobilized. EEG recordings were band-pass filtered and divided into epochs. Liner RVM, RVM with Gaussian kernel, and SVM with Gaussian kernel were used to classify 2 of 5 vowels.Our research started from vowels because Japanese syllables are based on five vowels, /a/, /i/, /u/, /e/, and /o/. Most Japanese syllables consist of one of the five vowels and a consonant. Therefore, we studied vowels at first.

@&#CONCLUSIONS@&#
