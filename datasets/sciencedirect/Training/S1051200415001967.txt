@&#MAIN-TITLE@&#
Verification of hidden speaker behind transformation disguised voices

@&#HIGHLIGHTS@&#
We investigate the effects of voice transformation disguise on ASV performance.We propose an algorithm to estimate transformation factor.We propose an algorithm to restore the genuine acoustic characteristics.We integrate the above algorithms into GMM–UBM based ASV system.The proposed approach achieves 3–4% EER.

@&#KEYPHRASES@&#
Voice transformation,Disguise,Automatic speaker verification,Security,

@&#ABSTRACT@&#
Voice transformation, which has been integrated in many audio (speech) processing tools, is a common operation to change a person's voice and to conceal his or her identity. It can deceive human beings and automatic speaker verification (ASV) systems easily, and thus it presents threats to security. Until now, few efforts have been reported on the recognition of hidden speakers from such disguised voices. In this paper, we propose concrete countermeasures to erase the disguise effects and verify the speaker's identity from voice transformation disguised voices. The proposed system is tested by commonly used audio editors and voice transformation algorithms. The experimental results show that the performances of baseline ASV system without our proposed countermeasures are entirely destroyed by voice transformation disguise with equal error rates (EERs) higher than 40%; while with our proposed countermeasures, the verification performances are improved significantly with EERs lowered to 3%–4%.

@&#INTRODUCTION@&#
Voice disguise can be classified into two categories: voice conversion (VC) and voice transformation (VT) [1,2]. VC intends to transform one's voice to imitate a target person provided with the target's acoustic information, while VT intends to change the sound without any target. It is apparent that VC is to change one's voice in order “to be recognized as another person” while VT is to change one's voice in order “not to be recognized”. Both present threats to human identification. However, since no target information is needed, VT is much easier to implement than VC, leading to the fact that VT has been incorporated in many prevailing audio editors while VC has never been.Besides electronic disguise by audio editors, VT can be performed by non-electronic means of a mechanic system [3–8] like a mask over the mouth, a pen in the mouth or pinching the nostril. However, by using sophisticated algorithms the output by editors generally sounds much more natural than the mechanic one [1], and thus it presents greater confusion as people tend to be deceived more by voices that sound natural.With high disguise quality and ease of implementation by abundant tools, digital VT disguise has been used in more and more criminal cases, and has presented threats to security. However, research efforts on this topic is still insufficient. In this paper we will examine automatic speaker verification (ASV) of digital VT disguised voices.Though few researches on ASV of VT disguise were reported, there have been some related efforts in the past 15 years.Tan [5], Zhang et al. [6], Perrot et al. [7] and Künzel et al. [8] investigated the effect of several mechanic VT disguises including hand over mouth, pinched nostril, high pitch and low pitch, and conducted experiments using Gaussian Mixture Model (GMM) [9], Vector Quantization (VQ) and Support Vector Machine (SVM) classifiers for ASV. The results indicated that the transformations make the recognition system fail totally. However, no detailed solution has been presented. Jin et al. [10,11], Bonastre et al. [12–14], Wu et al. [15] and Kinnunen et al. [16] studied the effect of VC on ASV systems to find that recognition performance was damaged. Unfortunately, no solution was proposed either. Alegre et al. [17] presented an algorithm based on the reduction in pair-wise distances between consecutive feature vectors, and integrated it into ASV systems. The experimental results showed that with this countermeasure, false acceptance rates (FARs) and equal error rates (EERs) are both significantly lowered compared to without it. However, the authors admitted that this is completely unrealistic of a practical spoofing scenario because the proposed countermeasure exploits prior knowledge of only one limited single spoofing attack. Kons et al. [21] reported an analysis of the vulnerability of text dependent ASV systems to simple VC spoofing attacks. They revealed that training using genuine voices as well as converted voices can improve the verification performance. Like the one in [17], this method also exploits prior full knowledge of only one spoofing attack which is unrealistic in practice; and the method aims at text dependent ASV, which is far more limited than text independent ASV.From the above reports, it can be concluded that most of the efforts are focused on VC and mechanic VT disguise, while digital VT disguise has received less attention. More importantly, the major problem of the current work is that most of the researchers only investigate the effects of disguise on ASV systems. They do not present concrete robust and universal solutions to erase the negative effects for revealing genuine speaker's identity, which is a far more challenging issue.Recently, Wang et al. [18] and Wu et al. [19,20] proposed algorithms to detect VT disguised voices from genuine voices. Cross-disguise-method and cross-corpus were included in the experiments. The results showed that detection rate can reach over 90% in all test situations. These works are significant contributions to VT disguise forensics. But again, a further research is needed on verification of hidden speaker VT disguised voices.Aiming at the above problems, we will investigate the effects of electronic VT disguise on ASV performances. More importantly, we will propose robust countermeasures for ASV to recognize speaker's identity from disguised voices. We propose a new algorithm for estimating VT parameter estimation by using fundamental frequencies [22,23], and a modified MFCC (Mel Frequency Cepstral Coefficient) extraction algorithm [9] which is effective to recover the original MFCCs from VT disguised voices. The proposed countermeasures are integrated into the GMM–UBM ASV system to test its efforts on the most prevailing and dominant audio editors and algorithms. The proposed system is demonstrated to outperform the baseline system by achieving a low error rate.The remainder of this paper is organized as follows. In Section 2, we introduce the principles of electronic voice transformations. In Section 3, we propose countermeasures to compensate the deformation of acoustic feature and to erase the disguise effects. Experimental results are given in Section 4. Finally, we summarize conclusions and future works in Section 5.VT can be divided into two categories: frequency-domain based and time-domain based techniques. In this section we discuss the principle of VT techniques and their deformation effects on acoustic features.The principle of frequency domain based VT is to raise or lower voice pitch by stretching or compressing the frequency spectrum [24]. Being related by Fourier transform, time and frequency characteristics of a signal are not independent but are of a duality relationship. It is essential to break this traditional tie between them to keep the tempo unchanged.The most frequently used tool for analysis in frequency-domain methods is based on the quasi-stationary sinusoidal model [25], in which speech signalx(t)is represented as a sum of sinusoids whose instantaneous frequency and amplitude vary slowly with time. In most applications, this model is represented by short-time Fourier transform (STFT), which starts with dividing a signal into short segments. Fast Fourier transform (FFT) is then applied to each segment and the resulting spectral components can be manipulated in a variety of ways. However, due to the resolution limitation, FFT bin frequencies generally do not represent true or instantaneous frequencies. For example using a window of size 2048 and a sampling rate of 44.1 kHz, the resolution in frequency domain is only 21.5 Hz, which is far too coarse in the lower frequency band.In order to solve this problem, a phase-vocoder is introduced which, by insight of relationship between phase and frequency, employs phase information that STFT ignores to improve frequency estimation. The kernel of the phase vocoder is to compute deviation from FFT bin frequencies to instantaneous frequencies by using phase information. Instantaneous frequency can then be computed by adding the deviation and the FFT bin frequency. Finally, three numbers obtained from the FFT analysis for each sinusoid, namely bin magnitude, bin frequency and bin phase, are reduced to only magnitude and transient frequency. We now present it in a simple form in Eqs. (1)–(3).Supposext(n)is a frame of length N from the input speech signal at time t. Firstly, it is windowed byw(n), and then an FFT is performed on the windowed signal, using Eq. (1), wherew(n)is a Hamming or Hanning window and k is the bin frequency index.(1)F(k)=∑n=0N−1xt(n)⋅w(n)e−i2πknN0⩽k<NThen, instantaneous magnitude|F(k)|and instantaneous frequencyω(k)are calculated by Eq. (2) and Eq. (3) respectively,(2)|F(k)|=|∑n=0N−1xt(n)⋅w(n)e−i2πknN|0⩽k<N(3)ω(k)=(k+Δ)⋅Fs/N0⩽k<NwhereFsis the sampling frequency and Δ is the deviation from the kth bin frequency.For voice transformation, transient frequencyω(k)is modified by Eq. (4), where α is the scale factor.(4)ω′(⌊k⋅α⌋)=ω(k)⋅α0⩽k,k⋅α<N/2There are several ways to modify the instantaneous magnitude. The commonest method is linear interpolation, as seen in Eq. (5)[24], where0⩽k,k′<N/2,k=⌈k′/α⌉, andμ=k′/α−k.(5)|F′(⌊k⋅α⌋)|=μ|F(k)|+(1−μ)|F(k+1)|Another commonly used method is energy-preserving modification by Eq. (6).(6)|F′(⌊k⋅α⌋)|=∑⌊k⋅α⌋⩽k⋅α<⌊k⋅α⌋+1|F(k)|For simplicity, we still use k as the index of the modified instantaneous frequencyω′and the instantaneous magnitudeF′.The instantaneous phaseϕ′(k)is then calculated via the instantaneous frequencyω′(k)and the transformed FFT coefficients is obtained by Eq. (7).(7)F′(k)=|F′(k)|eiϕ′(k)An inverse FFT (IFFT) is performed onF′(k)and the transformed signal can thus be obtained.By phase-vocoder based transformation, all frequency components are adjusted by the scaling factor that includes fundamental frequencies and formants [24]. Considering that fundamental frequencies are more stable and easier to extract than formants, we will use fundamental frequencies to estimate scaling factor.A phase-vocoder is not universal, however. In some applications, STFT phases are either lost or not applicable with STFT magnitude (STFTM) as the only available information. Hence, algorithms [26–30] have been explored to reconstruct time-domain signal from STFTM without phase information. Among them the latest and most effective algorithms are real-time iterative spectrogram inversion (RTISI) [26] and RTISI with look-ahead (RTISI-LA) [27,28]. In RTISI or RTISI-LA, real phases are not needed for signal reconstruction. Instead, they are estimated by previous and (or) future frames. The estimated phase and the given magnitude spectrum are combined to construct the current frame. RTISI and RTISI-LA can be used for voice transformation, illustrated in Fig. 1. If the normal analysis frame length is L, and transformation factor is α, then for each frame we use a block ofL′=αLsamples, as shown in Fig. 1(b). Resampling is done in the time-domain to generate a frame of length as shown in Fig. 1(c). The target STFTM is then computed on the resulting frame. RTISI or RTISI-LA is then run with the given STFTM and the estimated phases to reconstruct the transformed frame.From the above principles, we can learn that the scaling factor α is the key factor in VT. α must be estimated in order to retrieve the original features from disguised voices. In phonetics, transformation is always measured by a 12-semitones division [31], which leads the scaling factor α to the following form.(8)α(s)=2s/12Theoretically, VT can be froms=−12tos=12semitones. In practice, audio editors generally provide VT from −11 to 11 semitones.VT has been incorporated in many audio editing tools, including Adobe Audition (formerly Cool Edit Pro) [32], a commercial standard editor for over ten years, Audacity [33], the winner of SourceForge 2007 and 2009 Community Choice Award for Best Project for Multimedia with 76.5 millions downloads [34], and GoldWave [35], a popular commercial digital audio editing software product. The quasi-stationary sinusoidal model and phase vocoder are used by Audacity for VT. Adobe Audition and GoldWave use frequency domain based techniques for VT, but the detailed algorithms are not open to public. In this paper, we will test voices disguised by these three leading editors and RTISI-LA.Frequency-domain based VT can modify a voice to a large extent while preserving its tempo and natural audible quality, while time-domain based VT can do it only when the modification is small [24]. This leads to the fact that the latter has rarely been adopted by today's audio editors. But in some limited applications, time-domain VT with small scaling factors still proves useful for its simplicity.Most time-domain techniques for VT are based on the same fundamental method, best illustrated by the Pitch-Synchronous Overlap and Add method (PSOLA) [36,37]. There are several variants of PSOLA, among which time-domain PSOLA (TD-PSOLA) [36,37] is the most popular and commonly used one.In TD-PSOLA, a preliminary analysis stage is carried out to detect locations and contours of pitches. Then short-time signals are extracted at a pitch-synchronous rate (denoted asP(t)in Fig. 2) by use of a weighting window. The short-time signals are then overlap/added at a modified rateP′(t)=P(t)/α(t), and are repeated (or discarded) to compensate for the corresponding modification of duration, whereα(t)is the scaling factor. The periodicity of the glottal pulses is modified, which corresponds to an alteration of the fundamental frequency. Hence fundamental frequencies can be used to estimate scaling factor.In our proposed ASV approach, we use MFCC, which incorporates psychoacoustics mode of human auditory system and has become the dominant acoustic feature in today's ASV systems. The normal MFCC extraction algorithm is described as follows.Supposext(n)is a frame of length N from the input speech signal at time t. Firstly, it is windowed by a Hanning or Hamming windoww(n), and then an FFT is applied on the windowed signal as shown in Eq. (9).(9)F(k)=∑n=0N−1xt(n)⋅w(n)e−i2πknN0⩽k<NF(k)is then divided into M subbands according to the mel scale to simulate the human auditory system (HAS) critical subbands [38]. Energy of each FFT line within a subband is then weighted by a windowHmto combine the total contribution of this subband to the HAS, and then a log operation is performed by Eq. (10).(10)Y(m)=log⁡[∑k=0N−1|F(k)|2⋅Hm(k)]1⩽m⩽MFinally inverse discrete cosine transform (IDCT) is performed and a D-dimension MFCC vector at time t,vt, is obtained by Eq. (11).(11)vt(d)=1M∑m=1MY(m)cos⁡(d(m−0.5)πM)0⩽d⩽Dwherevt(d)is the dth MFCC component. Derived coefficients (ΔMFCC) reflecting dynamic cepstral features are computed from the MFCC vector. In this paper, 12-dimension MFCC and 12-dimension ΔMFCC are combined to formed the feature vector. For simplicity, we still usevtto represent the feature vector at time t, in which the first 12 coefficients are of MFCC components, and the next 12 coefficients are of ΔMFCC components.From the computation of MFCC and frequency-domain based VT principles, we can learn that MFCCs are modified by frequency-domain based VT because STFTMs|F(k)|are used to compute MFCCs and STFTMs are changed by VT. We have conducted extensive experiments to study the effect of frequency-domain based VT on MFCCs. As an example, we present 3 24-dimension feature vectors from a 1024-sample frame of an original speech signal and from its VT disguised versions respectively in Fig. 3. We can see that deformation is introduced to the features by VT which damages ASV performances in their entirety. Please refer to Section 4 for the details. Hence, countermeasures are needed to erase the deformation for ASV.TD-PSOLA is different from the frequency based VT. In TD-PSOLA, because the short-time signals are not re-sampled or altered but are merely copied back, the FFT spectrums of those signal segments remain unchanged, and thus the MFCCs extracted from those signal segments remain unchanged. TD-PSOLA does not introduce deformation to all the MFCCs, which leads to a different consequence. We will discuss the disguise effect of TD-PSOLA by extensive experiments in Section 4.In this section we first introduce the GMM–UBM based baseline ASV system. Then we introduce our proposed ASV system which can recognize a speaker's identity from frequency-domain based VT disguised voices. Finally, we discuss the countermeasures of ASV for TD-PSOLA disguise.Speaker verification can be viewed as a hypothesis test between:H0: Y is from the hypothesized speaker SandH1: Y is not from the hypothesized speaker S.MathematicallyH0is represented by a modelλhypwhich characterizes the hypothesized speaker S.λhypis derived from a sequence of feature vectors (i.e., MFCC vectors in this paper)V={v1,v2,…,vT}, wherevtis an MFCC vector at timet∈[1,2,…,T].H1is represented by a modelλhyp‾, which represents the entire space of possible alternatives to the hypothesized speaker. For this alternative hypothesis modeling, the most common procedure is to pool recordings of several speakers to train a single model. This is also referred to as universal background model (UBM). For clarity we will denoteλhyp‾asλbkgin the following context.Decision betweenH0andH1is made by a logarithmic likelihood ratio given by Eq. (12), wherep(V|Hi)is the probability density function for the hypothesisHi.(12)Λ(V)=log⁡p(Y|H0)−log⁡p(Y|H1)=log⁡p(V|λhyp)−log⁡p(V|λbkg){⩾θacceptH0<θrejectH0For text-independent ASV, the most successful likelihood function has been GMM [9].For a D-dimensional MFCC vectorvt, the likelihood function is a weighted linear combination of R unimodal Gaussian densities defined as:(13)p(vt|λ)=∑i=1Rwipi(vt),where the mixture weightswimust satisfy∑i=1Rwi=1.pi(vt)is computed by aD×1mean vector,μi, and aD×Dcovariance matrix,Σi, as follows.(14)pi(vt)=1(2π)D/2|Σi|1/2exp⁡{−12(vt−μi)′(Σi)−1(vt−μi)}Collectively a GMM λ is denoted asλ={wi,μi,Σi}i=1,2,…R.Finally, the average log likelihood value of a model λ for a sequence of MFCC vectorsV={v1,v2,…,vT}is computed by Eq. (15)(15)log⁡p(V|λ)=1T∑t=1Tlog⁡p(vt|λ).Expectation–maximization (EM) [39] is used to estimate the model λ, which iteratively adjusts the GMM parameters to monotonically increase the likelihood function, i.e., for iterations k andk+1,p(V|λ(k+1))>p(V|λ(k)). In our experiments, UBM is built by the EM algorithm. For training a specific speaker's model, it has been strongly indicated that adapting parameters from UBM outperforms a decoupled system where the speaker model is trained independently of the UBM [40]. Hence we use Maximum A posteriori (MAP) estimation algorithm [41] for modeling a speaker in which model parameters are iteratively adapted from those of UBM.The proposed system is shown in Fig. 4. The UBM training, the speaker's model (λj) training and the probability computation are the same as the baseline system. It differs from the baseline in three aspects.1) We add a module ‘F0 Extraction’ in the training phase, where the fundamental frequency [22,23] sequence of speakerSjis extracted, and the average of this sequence,fj, is computed by the mean operation. The model of speakerSjconsists of its GMMλjandfj.2) In the testing phase, Y is a speech signal that may have been modified by VT. Its fundamental frequency sequence is extracted by the module ‘F0 Extraction’, and the average of this sequence,fY, is computed by the mean operation.fjandfYare used to compute the estimated scaling factorα′by the division (/) operation.3) In the testing phase,α′is used in the module Modified MFCC Extraction, whose output V is the recovered MFCCs from Y.Now let's introduce the estimation of scaling factors and the modified MFCC extraction algorithm in details.In our approach, estimated scaling factors are essential to the recovery of MFCCs from a disguised voice. Here we turn to fundamental frequency for the estimation of scaling factors.Fundamental frequency, often denoted as F0, is the lowest frequency in a harmonic series such as speech or music [22,23]. F0 of speech is produced by glottal excitation, best illustrated by a model [24,42] described in Fig. 5. A volume velocity glottal excitation functionug(t)excites a passive linear system. The glottal wave is modeled as a transfer functionug(t), which is considered to be the sum of a pulse trainpg(t)and a slowly varying functionvg(t). The functionpg(t)is called the excitation pulse function. Each individual excitation pulse has an associated time of occurrence, or its excitation pulse time. The excitation time is defined to occur at the time when the excitation pulse function reaches a zero value at the end of each glottal cycle. This time is also the instant of glottal closure. Fundamental frequency F0 is defined as:(16)F0=1/T0From the model we know that F0 is produced by the open and closure of glottis. It is one biometrical feature that reflects movements (structures) of one's glottis, and it is less related to the uttered contents.Voice transformation in FFT spectrum is to scale F0 by the same scaling factor [24,25].Hence, in training phase, the average of F0 of speakerSj,fj, is extracted and saved into the model ofSj, as shown in Fig. 4. In testing, the average of F0 of testing speech Y,fY, is computed. The estimated scaling factor,α′, is computed by Eq. (17).(17)α′=fY/fjWe use Praat [23] to extract the fundamental frequency. The proposed estimation method has demonstrated to be very effective by extensive experiments which will be presented in Section 4.VT changes MFCCs, and thus damages ASV performances. In order to recognize speaker's identity from the disguised voice, we need to recover the original MFCCs from them. A straightforward way is to reconvert modified voice by scaling factor1/α′before MFCC extraction. However, VT is not a lossless procedure. To have a transformed voice be transformed for a second time will result in a further loss of substantial frequency components, especially for a large modification. Besides, as prior knowledge of transformation types and details is unknown in most cases, this straightforward way is not universal.Here, we propose a universal modified MFCC extraction algorithm which derives the original MFCCs from transformed voices directly for all the frequency domain based VT methods we consider in this paper.From Eq. (4) or Eq. (6) in phase vocoder, we can learn that during transformation, instantaneous magnitudes (i.e., STFTMs or FFT spectrum) are modified, and from Eq. (10) and Eq. (11), we can also learn that the modified STFTMs are used to calculate MFCCs. Hence, once the original FFT spectrum is recovered, the original MFCCs can be retrieved. Inspired by Eq. (4) and Eq. (6), one of the two recovery algorithms can be adopted.1) The STFTMs are linearly interpolated by estimated scaling factor1/α′using Eq. (4).2) The STFTMs are relocated by estimated scaling factor1/α′using Eq. (6). Through extensive experiments, the first algorithm outperforms the second one for all VT methods. It is used in our proposed system to retrieve original MFCCs from disguised voices.Some practical issues must be taken into consideration. Let us denote the Nyquist Frequency asFn. In training, a speaker's model in database is derived from MFCCs, and each MFCC is computed from spectrum ranging from 0 Hz toFn/2. Accordingly, the spectrum of a testing speech clip must cover from 0 Hz toFn/2. We discuss it below.1) Ifα<1, FFT spectrum is condensed. To recover original MFCCs, FFT spectrum is scaled by1/α′to expand and reachesFn/2, which satisfies the above requirement, provided that the estimation of scaling factor is accurate.2) Ifα>1, FFT spectrum is expanded and those components exceedingFn/2are discarded. To recover the original MFCCs, this spectrum is scaled by1/α′. The resulting compressed spectrum covers from 0 Hz toFn/2/α′. An example is shown in Fig. 6(a). Consequently, spectrum fromFn/2/α′toFn/2must be compensated. Several compensation methods have been tried in our experiments, and the best one is to copy the spectrums fromFn/2/α′toFn/2/α′−Fn/2into the spectrum region fromFn/2/α′toFn/2, as illustrated in Fig. 6(b).In Section 4 we will show the detailed performances.As mentioned in Section 2.3, in TD-PSOLA, because the short-time signals are not re-sampled or altered but merely copied back, the MFCCs extracted from those signal segments remain unchanged. Hence, our proposed modified MFCC extraction algorithm in Section 3.2 for frequency-domain based methods is not applicable for TD-PSLOA. The straightforward way to reconvert the modified signal with scaling factor1/α′could be considered. This is reasonable because TD-PSOLA can preserve content and natural audibility only when the scaling factors are small. To have a transformed voice signal be transformed for a second time with a small factor will not result in a loss of abundant signal components. However, since the MFCCs of those unchanged short segments remain unchanged, it is apparent that an ASV system without countermeasures can resist TD-PSOLA to some extent. The extent of needing to adopt countermeasures becomes a question. In Section 4, we will investigate this problem by experiments.

@&#CONCLUSIONS@&#
Voice transformation, which has been integrated in many audio editing tools, can be used for disguise purposes to deceive human ears and ASV systems. Yet, little attention has been paid to this problem. In this paper, we give a very comprehensive study on ASV of VT disguises. The most significant contribution is that we propose concrete, practical and effective solutions for ASV to erase VT disguise and to recognize the identity of the hidden speaker behind VT disguised voices.The key issue is to recover the speaker's original features from his or her disguised voice. For frequency-based VT techniques, we propose an algorithm for the estimation of scaling factors using fundamental frequency, and a modified MFCC extraction algorithm which uses the estimated scaling factors to recover the original MFCCs from the disguised voices. We integrate these countermeasures into a GMM–UBM based ASV system.For a comprehensive investigation, we test our proposed system by frequency-domain based and time-domain based VT techniques. For frequency-domain VT, we take into account the leading commercial or free audio editors, including Adobe Audition, Audacity and GoldWave, and the latest and popular algorithm RTISI-LA. Experimental results show that the global EERs (|s|⩽11) are higher than 40% in the baseline system while they are around 2%–4% in our proposed systems. Global EERs (4⩽|s|⩽8) are over 40% in the baseline system while they are around 1%–3% in our proposed system. The experiments demonstrate that 1) the performances of the baseline ASV system are entirely damaged by VT disguise, and 2) our proposed system outperforms the baseline so significantly that its error rates have been lowered to an acceptable level in many speech applications.We also study the disguise effect of TD-PSOLA, the dominant time-domain VT technique, on ASV. Extensive experiments demonstrate that TD-PSOLA does not damage ASV performance. We then conduct analysis and comparison to explain the non-disguise effect of TD-PSOLA.As mentioned in Section 1, besides VT, there are another type of voice disguise: voice conversion (VC). VC disguises have different principles and follow different technical routes from VT disguises. In future work, we will focus on speaker verification from VC disguised voices.