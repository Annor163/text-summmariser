@&#MAIN-TITLE@&#
PSHREG: A SAS macro for proportional and nonproportional subdistribution hazards regression

@&#HIGHLIGHTS@&#
The %pshreg SAS macro fits Fine-Gray models for competing risks.The macro first modifies a given data set and then uses PROC PHREG for analysis.Many useful features of PROC PHREG can now be applied to a Fine-Gray model.Time-dependent effects can be accommodated by time-by-covariate interactions.For small data sets, the Firth correction is available.

@&#KEYPHRASES@&#
Regression analysis,Survival analysis,Competing risks,SAS software,Subdistribution hazard ratio,Cumulative incidence,

@&#ABSTRACT@&#
We present a new SAS macro %pshreg that can be used to fit a proportional subdistribution hazards model for survival data subject to competing risks. Our macro first modifies the input data set appropriately and then applies SAS's standard Cox regression procedure, PROC PHREG, using weights and counting-process style of specifying survival times to the modified data set. The modified data set can also be used to estimate cumulative incidence curves for the event of interest. The application of PROC PHREG has several advantages, e.g., it directly enables the user to apply the Firth correction, which has been proposed as a solution to the problem of undefined (infinite) maximum likelihood estimates in Cox regression, frequently encountered in small sample analyses.Deviation from proportional subdistribution hazards can be detected by both inspecting Schoenfeld-type residuals and testing correlation of these residuals with time, or by including interactions of covariates with functions of time. We illustrate application of these extended methods for competing risk regression using our macro, which is freely available at: http://cemsiis.meduniwien.ac.at/en/kb/science-research/software/statistical-software/pshreg, by means of analysis of a real chronic kidney disease study. We discuss differences in features and capabilities of %pshreg and the recent (January 2014) SAS PROC PHREG implementation of proportional subdistribution hazards modelling.

@&#INTRODUCTION@&#
Competing risks arise in the analysis of time-to-event data, if for some subjects the event of interest is precluded by a different type of event occurring before. Competing risks may be encountered, e.g., if interest focuses on a specific non-terminal event type such as dialysis onset. In such a situation, death before dialysis onset constitutes a competing risk.It has frequently been pointed out that in presence of competing risks, the standard product-limit method for estimating the survival function for the event of interest yields biased results; cf., e.g., Bakoyannis and Touloumi [1]. The main assumption of this method is that any subject whose survival time is censored will experience the event of interest if followed up long enough. This does not hold if competing risks are present, as the occurrence of the event of interest is precluded or its probability of occurrence is modified by an antecedent competing event. As a remedy, the cumulative distribution function, generally denoted cumulative incidence function (CIF), proposed by Kalbfleisch and Prentice [2] can be used. While the naive product-limit estimate of CIF, treating the competing risk as an independent censoring mechanism, will reach 1 with an infinite follow-up time, the proper CIF estimate never reaches 1 as a consequence of presence of a certain proportion of subjects who will experience the competing event.Two different ways of modelling competing risks data have been proposed. The first one analyses the cause-specific hazard of each event type separately, by applying Cox regression targeting each event type in turn, and censoring all other event types. By a complete analysis of all event types by such cause-specific Cox models, estimated cumulative incidence curves for an event of interest can be estimated using specialized software [3]. By contrast, the proportional subdistribution hazards (PSH) model proposed by Fine and Gray [4] directly aims at modelling differences in the cumulative incidence of an event of interest. Its estimation is based on modified risk sets, where subjects experiencing the competing event are retained even after their competing event. In case of censoring (which is the rule rather than the exception), a modification of this simple principle was proposed such that the weight of those subjects artificially retained in the risk sets is gradually reduced according to the conditional probability of being under follow-up had the competing event not occurred.The two different approaches to competing risks modelling are both appropriate but have different aims: while modelling the cause-specific hazards targets aetiologic research questions by modelling the effect of covariates on event rates among subjects at risk, the PSH model is useful for medical decision making and prognosis, as it models the absolute risk of an event. Differences between these two approaches are discussed, e.g., in Wolbers et al. [5] and Andersen et al. [6]. Here we focus on the latter approach.Programs for fitting a Fine-Gray regression model are available in R (e.g., package cmprsk by Gray [7]) and STATA (stcrreg command by StataCorp. [8]). Waltoft [9] describes a SAS macro for cumulative incidence curve estimation via Poisson regression. Zhang and Zhang [10] provide a SAS macro for the special case of computing adjusted cumulative incidence curves for two treatment groups. However, a publicly available and fairly general implementation of the Fine-Gray regression model in SAS has yet been missing. Our SAS macro %pshreg was written to fill this gap. The macro modifies an input data set by separating follow-up periods of patients with competing events into several disjoint sub-periods with declining weights, following the suggestions in Fine and Gray [4] and Geskus [11]. This allows using SAS's PROC PHREG to compute the proportional subdistribution hazards model. Thus, all options offered by PROC PHREG for verifying and relaxing the assumption of proportional subdistribution hazards can be used, including the computation and display of unscaled and scaled Schoenfeld-type residuals or extending the model by time-dependent effects of covariates.In very small data sets with few events, monotone pseudo-likelihood may cause parameter estimates to diverge to ±∞. This phenomenon typically occurs if events are observed in only one of two levels of a binary covariate. The application of the Firth-correction [12], which is readily implemented in PROC PHREG, may be useful in such circumstances [13,14].In the remainder of this manuscript we first briefly review the estimation of proportional subdistribution hazards models with time-fixed and time-dependent effects, and illustrate Firth's bias correction method. Later we explain the most important macro options in detail and explain the structure of the macro code. Subsequently, a worked example illustrates different aspects of application of the macro. The manuscript closes by discussing what has been achieved and the differences in features and capabilities of %pshreg and the recent SAS implementation of PSH modelling in PROC PHREG (SAS/STAT Version 13.1, released in January 2014).As a motivating example, we consider a study on chronic kidney disease (CKD) recently performed at the Medical University of Vienna including 273 patients, who were diagnosed with CKD between stages I and IV at their first visit of the University's outpatient department [15]. In this study, researchers were interested in modelling the time to dialysis onset, i.e., the time to progression to end stage renal disease. The continuous variables age per decade (age), log2 of urine osmolarity (logUosm), log2 of creatinine clearance (logCCR), log2 of proteinuria (logProt) and the binary variables beta-blocker (bblock), diuretic therapies (diur) and type of underlying renal disease (pkd), all measured at the day of diagnosis, were considered as important in prognosis of the cumulative incidence of dialysis onset. The median follow-up time of the study was 92 months. The event of interest, dialysis onset, could be observed in 105 patients (38.46%). CKD patients are at higher risk of mortality than the normal population, and thus, in our study, death before dialysis onset constitutes a competing event (n=35, 12.82%).Generally, we assume observations on m covariates in n subjects, and we let xil, tiand ɛidenote subject i's (i=1, …, n) values of covariate l (l=1, …, m), observed survival time and follow-up status, respectively. For the latter, we consider ɛi=0, ɛi=1, and ɛi=2 as denoting a censored time ti, an event of interest occurred at ti, and a competing event occurred at ti, respectively. Furthermore, we assume that there are r distinct times at which events of interest have occurred, and that t(1), …, t(r) denote the corresponding ordered event times.Without loss of generality, we assume that there is one event type of interest, dialysis onset in our example, and only one competing event, death. These two types of events are in the sequel denoted by event type 1 and event type 2. In the competing risks literature, the event types are also denoted as causes of an event. Let λk(t) and Λk(t) denote the cause-specific hazard functions and cause-specific cumulative hazard functions, respectively, for event type (cause) k, k=1, 2. The cause-specific cumulative incidence function F1(t), describing the cumulative probability of subjects experiencing event type 1 up to time t, is given byF1(t)=∫0tS(s)λ1(s)dsNote that S(t) is the survival function of time to the first of the two event types, given by S(t)=e−Λ1(t)−Λ2(t). Fk(t) has also been denoted as the ‘subdistribution’, reflecting the fact that it does not reach 1 in presence of a competing risk. In the absence of competing risks, the cumulative hazard and cumulative incidence (one minus survivor) function are connected by the relationship F(t)=1−e−Λ(t). This unique correspondence is lost with competing risks, because the cumulative incidence for the event of interest depends on the cause-specific hazard of the competing event [6]. Consequently, the Kaplan-Meier estimator of the cause specific cumulative incidence function, obtained by simply censoring all observations with a competing event, is biased since 1−Sk(t)≥Fk(t) (k=1, 2) [1]. Instead, the cumulative incidence function F1(t) at the ordered event times t(j), j=1, …, r, should be estimated by the non-parametric plug-in estimator [1]Fˆ1(t(j))=∑i=1jλˆ1(tj)Sˆ(t(j−1))=∑i=1jd1(j)n(j)Sˆ(t(j−1))where d1(j) is the number of events of type 1 observed at t(j), n(j) is the number of patients at risk for both events just before t(j),Sˆ(t)is the Kaplan-Meier estimator of the survival function of time to the first event at t andSˆ(t(0))=1.The proportional subdistribution hazard model was proposed by Fine and Gray [4] in order to estimate the effect of covariates on the cumulative incidence of the event of interest.We consider T as the (partly unobservable) random variable describing the time at which the first event of any type occurs in an individual, and ɛ∈{1, 2} as the event type related to that time. The subdistribution hazard of event type 1, h1(t, X), is defined as(1)h1(t,X)=limΔt→01ΔtPr{t≤T≤t+Δt∧ε=1|T≥t∨(T≤t∧ε=2),X}with X denoting a row vector of covariates, and '∨' and '∧' denoting the logical OR and AND, respectively. Following Fine and Gray [4], the subdistribution hazard in Eq. (1) can be modelled as a function of a parameter vector β throughh1(t,X)=h1,0(t)eXβwhere h1,0 is an unspecified baseline subdistribution hazard function.The partial likelihood of the proportional subdistribution hazards model was defined by Fine and Gray asL(β)=∏j=1rexp(x(j)β)∑i∈ℛ(t(j))wi(t(j))exp(xiβ)where x(j) is the covariate row vector of the subject experiencing an event of type 1 at t(j). For simplicity, no ties in event times are assumed here, but both the Breslow and Efron tie corrections can incorporate weights and can thus be used in this model. The risk set ℛ(t) is defined asℛ(t)={i;ti≥t∨(ti≤t∧εi=2)}and includes the set of individuals who, at time t, are at risk of event type 1 as well as those who have had a competing event before t. The subject- and time-specific weightswi(t)are needed as soon as censoring occurs. Subjects who are at risk for an event of type 1 at time t, and who have not failed from an event of type 2 before t participate fully in ℛ(t) withwi(t)=1, whereaswi(t)≤1for subjects with competing events at ti<t. Formally,(2)wi(t)=1ifti≥tGˆ(t)Gˆ(ti)ifεi=2∧ti<tHere,Gˆ(t)denotes an estimator of the survival function of the censoring distribution at t, i.e., the cumulative probability of still being followed-up at t.Gˆ(t)can be estimated by the product-limit method with reverse meaning of the censoring indicator:(3)Gˆ(t)=∏ti<t1−∑j=1nI(tj=ti∧εi=0)∑j=1nI(tj≥ti)In case that there is no random censoring, i.e., every subject was followed up until a specified administrative censoring date (end of follow-up), the Fine-Gray model simplifies considerably. Then, it can be estimated by a Cox regression model in which times to competing events are replaced by times to administrative censoring and censored [1]. This simplification assumes that it would have been possible for subjects who experienced an event to follow them up until the administrative censoring date.The proportional subdistribution hazards model can be estimated using any standard software for Cox regression that allows representation of times in counting process style, i.e., in (start, stop] syntax, and weighting [11]. In the following, we describe how to model the time to event type 1; in this case, unmodified data can be used on the subjects who either experience event type 1 or who are censored, but data of subjects who experience event type 2 has to be modified.In particular, the event history of each subject i is represented in counting process style as one or several disjoint time intervals. For individuals who did not fail from the event of type 2 before failing from the event of type 1, these episodes, described by (start time, stop time, status indicator) are just(0,ti,εi*), where the modified censoring indicatorεi*is 1 for event type 1, and 0 for a censored time. However, for subjects experiencing event type 2 before failing from the event of type 1 additional time intervals are created after their time to event of type 2. Assuming that the event times tiof such subjects are such that t(j)≤ti<t(j+1), the first time interval is given by (0, ti, 0), the second one by (ti, t(j+1), 0) and further time intervals by (t(j+1), t(j+2), 0), (t(j+2), t(j+3), 0), …, (t(r−1), t(r), 0). These time intervals are assigned the decreasing weights 1,wi(t(j+1)),wi(t(j+2)),…,wi(t(r)), respectively (Table 1). With this modification and weighting of the original data, the parameters of a PSH model can now be estimated using SAS's PROC PHREG.A fitted PSH model can be used to predict the CIF of the event of interest for a new individual characterized by a covariate row vector X. Let Ni(t)=I(ti≤t∧ɛi=1) such that dNi(t) is the increment in the counting process describing the status of subject i with respect to event type 1 in the interval [t, t+dt). (This counting process changes from 0 to 1 at the event time tiif the event type 1 occurred at that time.) The Breslow estimator of the baseline cumulative subdistribution hazard, relating to an individual with a zero covariate vector, is given byHˆ1,0(t)=∑i=1n∫0t1∑a∈ℛ(s)wa(s)exp(xaβˆ)dNi(s)whereβˆare the regression coefficients (covariate effect estimates) from the PSH model. With time-independent covariates X, the empirical cumulative distribution hazard for event type 1 is given byHˆ1(t,X)=Hˆ1,0(t)exp(Xβˆ). The empirical cumulative subdistribution hazard estimate of the CIF can then simply be estimated byFˆ1(t,X)=1−exp{−Hˆ1(t,X)}.PROC PHREG provides the possibility to compute the Breslow estimator of the baseline cumulative hazard function based on the estimates from a conventional Cox model. With appropriate data modification and weighting as described above, this baseline hazard function is exactly equal to the baseline subdistribution hazard function of a PSH model.Geskus [11] discusses the use of model-based variance estimators instead of the robust ones proposed by Fine and Gray [4] in PSH models, and argues that, since subjects who experience the event type 1 are given weights of 1, and each subject appears no more than once in each risk set, the use of model-based inference, i.e., by the model-based variance or by likelihood-based methods, is asymptotically correct. Hence, both the model-based and the robust estimator of variance can be used, as in an unweighted Cox regression analysis. Both options for inference are implemented in PROC PHREG and are therefore also available in our macro. In agreement with Gray's cmprsk R‘s package [7] we use the robust covariance by default.In fitting a Cox regression model, the phenomenon of monotone likelihood is observed if the likelihood converges while at least one entry of the parameter estimate diverges [13]. The same may happen in a PSH model, e.g., if no events of interest are observed in one of the levels of a categorical explanatory variable.In case of monotone likelihood, not only the parameter estimate but also its standard error diverges. Thus, inference based on standard errors becomes uninformative, even if based on values computed at the last iteration before the likelihood converged. The behaviour of the robust standard error as proposed by Lin and Wei [16], which was also proposed by Fine and Gray [4], under monotone likelihood is not yet fully understood. This standard error is based on DFBETA residualsDi=βˆ−βˆ(i), which are one-step approximations to the Jackknife values [17], i.e., the changes in parameter estimates if each observation in turn is left out from analysis. (Specifically, with D=D1, …, Dn, the robust variance matrix V is computed as V=D′D.) For computation of this standard error a finite estimate ofβ⌢would be needed which is not available under monotone likelihood.By adding an asymptotically negligible penalty function to the log likelihood, the occurrence of divergent parameter estimates can be completely avoided [12,13]. Furthermore, the penalty, suggested for exponential family models in canonical representation by Firth [12] as 1/2log|I(β)|, with I(·) denoting the Fisher information matrix, corrects the small sample bias of maximum likelihood estimates. This bias is usually low for Cox regression models unless monotone likelihood is observed. Estimation of bias-corrected PSH models can be based on modified score functions, including the weights as defined above.In case of monotone likelihood, it was proposed that inference should be based on the profile penalized likelihood function, since the normal approximation may fail because of the asymmetry of the profile penalized likelihood [13]. Let β=(θ, γ), with θ a scalar parameter of interest, and γ a vector of nuisance parameters. With ℓ(β) denoting the log of the likelihood, and ℓmax its maximum value, the profile log likelihood function of parameter θ is given by ℓθ(θ)=maxγℓ(β|θ=θ0), i.e., by the log likelihood fixed at θ=θ0 and maximized over all parameters γ. 2(ℓmax−ℓθ(θ)) has a limiting χ2 distribution with 1 degree of freedom [18, pp. 34-37]. Letℓα=ℓmax−1/2χ12(1−α). Thus, a (1−α)×100% confidence interval for θ can be obtained by {θα:ℓθ(θ0)≥ℓα}. Profile penalized likelihood confidence intervals are simply obtained by exchanging ℓ(β), ℓmax and ℓ(β|θ=θ0) by their penalized versions.Both the Firth bias correction and the associated profile penalized likelihood confidence intervals are implemented in PROC PHREG and are thus also available for PSH models fitted by the %pshreg macro.Similarly as in the Cox proportional hazards model, as a first explorative step, Schoenfeld-type residuals and weighted Schoenfeld-type residuals can be inspected in order to detect violations of the proportional subdistribution hazards assumption. Assume, that the subject that fails at the event time t(j) has a covariate row vector x(j), and for simplicity assume that there are no tied event times. For the event time t(j), a row vector of Schoenfeld-type residuals is defined byUˆ(j)=x(j)−x¯(βˆ,t), wherex¯(β,t)=S(1)(β,t)/S(0)(β,t),βˆis the maximum likelihood estimate of β,S(0)(β,t)=∑i∈ℛ(t)wi(t)exp(xiβ),S(1)(β,t)=∑i∈ℛ(t)xiwi(t)exp(xiβ), andwi(t)is the weight of subject i at time t as defined in Eq. (2). Weighted Schoenfeld-type residuals are scaled such that the smoothed residuals can directly be interpreted as changes in β over time. They are defined asUˆ(j)W=nε=1I(βˆ)−1Uˆ(j), with nɛ=1 denoting the number of events of type 1.The proportional subdistribution hazards model lends itself to accommodate non-proportional subdistribution hazards by including time-varying covariates defined by products of covariates with functions of time. The basic model is extended in the following way:h1(t,x)=h10(t)exβ(t)with β(t)=f(t, β). Considering a single covariate, then, in its simplest form, f(t, β) could be defined as β1+β2t, such that a covariate's effect is modelled as increasing or decreasing linearly with time. To allow for complex dependencies, flexible functions of time such as splines or fractional polynomials (β1+β2tp1+β2tp2), with p1, p2 selected from a pre-defined set of values, could be used [19].The most important parameters available in %pshreg are described in Table 2.Further options implemented in the macro can be used to control the interpretation of the value of the status indicator, to specify commands which are directly passed to PROC PHREG or to request subgroup-dependent estimators of the censoring distribution. The latter option may be useful if there are subgroups with different follow-up schemes, and will cause subgroup dependent estimation of the weights that are used for subjects with competing events. This option is similar to the cengroup option in the R package cmprsk. All options are described and their use is exemplified in the User's Guide of the macro, which is available at http://cemsiis.meduniwien.ac.aten/kb/science-researchsoftware/statistical-software/pshreg/.At the current output destination, the first page of output contains a list of the macro options used. In any case, the macro will also create a modified data set suitable to estimate a Fine-Gray model using PROC PHREG.With the default setting of the macro, action=estimate, PROC PHREG will be automatically invoked using the modified data set to estimate the Fine-Gray model, and its output is directly shown, without modification, in the output destination.If action=code, then SAS code will be written into the SAS Log window. This SAS code can be copied to the Editor window and submitted to estimate the Fine-Gray model.In a typical analysis situation, not only one but several multivariable models will be estimated, e.g., for checking nonlinear effects, interactions or confounding. It is not necessary to repeat the macro call in this case; once the modified data set is created, the user can apply PROC PHREG with different variable lists etc. in the same manner as shown in the example code of the SAS Log. In PSH analyses of large data sets, creation of the modified data set is often the most time-consuming part of analysis. Computing time can be considerably reduced in such analyses if the modified data set is created only once by a single macro call.The macro code is structured as follows:Step 1. Create a summary of all macro options in form of a SAS data set and send the data set to the current output destination.Step 2. EstimateGˆ(t)(cf. Eq. (3)), which serves as denominator of the time-dependent weights (cf. Eq. (2)) at event times t(1), …, t(r), using PROC LIFETEST.Step 3. Split the data set into two subsets: subset 1, consisting of all subjects with event indicators ɛi=0 or ɛi=1, and subset 2, consisting of all subjects with ɛi=2.Step 3.1. For each subject in subset 1, create a single time interval, with a _start_ time of 0. Set _stop_ time to ti, and the censoring indicator _censcrr_ to ɛi. Create a weight variable _weights_, which is constantly 1.Step 3.2. For each subject in subset 2, create the first time interval by setting _start_=0, _stop_=ti, and _censcrr_=0. Subsequent time intervals and corresponding weights are efficiently created using PROC SQL statements:•Look up the weight denominator of subject i asGˆ(ti).Extract all event times t(j) greater than ti.For each of these event times, compute the subject- and time-specific _weights_ variable aswi(t(j))=Gˆ(t(j))/Gˆ(ti), and create the episodes in counting process formulation as described in Table 1, setting _censcrr_=0.Step 4. Merge subset 1 with subset 2.Step 5. If requested by action=estimate, invoke PROC PHREG using the modified data set; otherwise (action=code), write the SAS statements of the PROC PHREG step into the Log window.%pshreg does not do any statistical calculations besides calling PROC LIFETEST to compute survival probabilities in order to generate the time-dependent weights. All statistical computation is passed over to PROC PHREG, which employs well-validated algorithms to estimate the models. All parameters to control the iterative estimation procedure offered by PROC PHREG (convergence criteria, ridging, etc.) can be used.In this section we illustrate an application of our macro %pshreg by analyzing the CKD data set described before. The primary aim of the analysis is to investigate the effects of several covariates on the cumulative incidence of dialysis in patients with chronic kidney disease. The variable Time contains the time span from diagnosis to the occurrence of the first event (dialysis or death) or end of follow-up in months. The variable status is the corresponding status indicator, it specifies whether a patient reached end stage renal disease and was started on dialysis (status=1), died (status=2) or experienced none of these events up to the last follow-up visit (status=0). Prognostic factors of interest are age in decades, osmolarity, creatinine clearance, proteinuria, beta-blocker, diuretic therapies and type of underlying renal disease (PKD). Their distribution is summarized in Table 3. Note that for further analyses, urine osmolarity, proteinuria and creatinine clearance were log2 transformed. Thus, their subdistribution hazard ratios refer to each doubling of their values.To generate a data set dat_crr prepared for PSH regression analysis using PROC PHREG, the following code can be submitted:%pshreg(action=code, data=CKDstudy, time=Time, cens=status, varlist=age logUosm logProt logCCR pkd bblock diur, out=dat_crr);The Output window will show a summary of the macro options, and a frequency of the status indicator:The PSHREG macro: summary of macro optionsMacro optionAssigned valueRemarkdataCKDstudyInput data settimeTimeTime variablecensstatusCensoring variablefailcode1Code for event of interestcencode0Code for censored observationtiedcensafterHow censored times tied with event times should be treatedadminAdministrative censoring time variablevarlistageList of covariableslogUosmlogProtlogCCRpkdbblockerdiurclassList of class variablesoptionsOptions to be passed to PROC PHREGfirth0Standard ML estimation, no Firth correctionidSubject identifierbyBY processing variablecuminc0Requests cumulative incidence curvesactioncodeNo output produced (see log file)weights0Standard model, no weighting of risk setsclean1Unnecessary variables removedcall_pshregoptData set with this call's macro optionsoutdat_crrOutput data set for standard Fine-Gray modelmissingdropDelete observations with missing covariatevaluesstatustab1Summary of status variable requesteddelwork1Temporary data sets deleted on exit-----------------------------------------------------------------macro version2014.09build201409301506The PSHREG macro: Summary of missing values in covariates and outcome variablesRemarkCovariatesOutcome (Time, status)TotalObservations deleted in input data set because of missing values:21021The PSHREG macro: Summary of status variableObs_statusCOUNTPERCENT1Censored11947.22222Events of interest10039.68253Competing events3313.0952The Log window displays the following SAS statements, which may be submitted to estimate the PSH model, or which may be further modified to include a different set of variables or change options controlling the computation or output of PROC PHREG:PROC PHREG DATA=dat_crr covs(aggregate);MODEL (_start_,_stop_)*_censcrr_(0)=age logUosm logProt logCCR pkd bblocker diur;ID _id_;WEIGHT _weight_;RUN;By default, %pshreg will request a robust covariance matrix by specifying covs(aggregate) in the first statement, proposed by Fine and Gray [4]. As pointed out by Geskus [11], this robust covariance matrix is not needed in a standard Fine-Gray analysis. Still, for compatibility with the R function crr, our macro will compute the robust covariance matrix.The following variables have been created by %pshreg: _start_, _stop_, _censcrr_, specifying the counting process representation, _weight_, which defines weighting bywi(t(j)),and _id_, which is a subject identifier. Figs. 1 and 2show the data of some patients, before and after processing by the macro. Names of independent variables are directly carried over from the input data set. Comparing Figs. 1 and 2, we learn that for patient no.151, who died after 6.84 years without earlier onset of dialysis, the macro created several disjoint episodes with time-dependent weights decreasing from 1 to 0. The juncture time points of these episodes are failure times of event type 1 greater than the time to death for patient no.151 that are observed in subjects not included in Figs. 1 and 2. Because of the generation of these additional episodes, the modified data set dat_crr has 2,710 observations (multiple observations for each patient with a competing event), while the original one contains only 273 observations (exactly one observation per patient).In our exemplified macro call we did not specify a subject identifier. Therefore, the macro interprets each observation in the input data set as contributed by a different subject which is independent from all other subjects, and generates the variable _id_. Alternatively, we could also use the ID option in the call to %pshreg to define a particular variable of the input data set as the subject identifier. In this way, subjects can be traced in the output data set. The ID statement is important for the purpose of computing the robust covariance matrix (recall its construction via the change in regression coefficients Dicaused by omitting each subject i in turn), and to identify outlying or influential observations.By copying the SAS code from the Log window into the Editor window, and submitting the code, the PSH model for time to dialysis is actually fitted. Part of the output of PROC PHREG is shown below:The PHREG ProcedureAnalysis of Maximum Likelihood EstimatesParameterDFParameter EstimateStandard ErrorStdErr RatioChi-SquarePr > ChiSqage1−0.139100.080021.1713.02190.0821logUosm10.712330.333281.1104.56820.0326logProt10.613390.072470.95571.6483<.0001logCCR1−1.912380.231151.03368.4487<.0001PKD11.234140.348881.01012.51330.0004BBlocker10.429040.234751.0893.34030.0676diur10.481490.232481.0484.28950.0383The PHREG ProcedureAnalysis of Maximum Likelihood EstimatesParameterHazard Ratio95% Hazard RatioConfidence Limitsage0.8700.7441.018logUosm2.0391.0613.918logProt1.8471.6022.129logCCR0.1480.0940.232PKD3.4351.7346.807BBlocker1.5360.9692.433diur1.6181.0262.553The standard errors, χ2 statistics and P-values are based on the robust sandwich covariance matrix. The column labelled StdErr Ratio contains the ratio of the robust to the model-based standard error. In case of model misspecification, model-based standard errors may understate the true variability of the regression coefficientsβˆif the model is not specified correctly, while robust standard errors are still appropriate [16]. Therefore, standard error ratios which are much larger than 1 may indicate problems with the assumptions of the model, such as violated linearity, additivity or proportional subdistribution hazards assumptions of model effects. In our experience standard error ratios less than 1 are less frequent than the opposite, but are possible in situations with no severe misspecifications. Such ratios less than 1 are caused by the particular covariance structure in the explanatory variables and arise by ‘chance’.If the option rl is included as an option in the MODEL statement, then the macro will produce an additional table with subdistribution hazard ratios (exp(β)) and associated 95% confidence limits. In the PROC PHREG output, the subdistribution hazard ratios are denoted as Hazard Ratios, but they should be interpreted as subdistribution hazard ratios.Similarly to standard hazard ratios or cause-specific hazard ratios, subdistribution hazard ratios refer to the comparison of two fictive patients who differ in only one of the covariates (e.g., in the binary variable pkd) by one unit, and who have equal values on the other covariates. Since the proportional subdistribution hazards model models differences in the CIF as effects of covariates, the subdistribution hazard ratios compare the estimated CIF between these two fictive patients by means of the ratio of their relative increments at any time point t, which are given by [F1(t+Δt|X)−F1(t|X)]/[1−F1(t|X)], with Δt→0. The subdistribution hazard ratio of 3.435 estimated for pkd means that at any time point t, the subdistribution hazard (increase of the CIF relative to 1-CIF) estimated for patients with pkd=1 is 3.435 fold the subdistribution hazard estimated for patients with pkd=0. Although in our example data set the cause-specific hazard ratio of pkd is very similar to its subdistribution hazard ratio, this is not generally the case, as the two quantities have different interpretation [5].For graphical visualization of the effect of osmolarity on the cumulative incidence of initiation of dialysis, one may create predicted CIF for fictive patients with different baseline urine osmolarities. Here we exemplify the necessary SAS statements to estimate and display these CIFs, using the output data set of the macro. The CIFs should be estimated for three (fictive) patients with baseline urine osmolarities of 315, 510 or 780 mOsmol/L (corresponding to the 10th, 50th, and 90th percentiles) and average values for all other covariates. We assume that the covariate values are saved in the data set datameans (Fig. 3). (The values given for logUosm are log2(315), log2(510), and log2(780).)First, we request PROC PHREG to compute estimated ‘survival’ curves for the three fictive patients, by submitting the BASELINE statement. Note that the macro does not need to be invoked again, as the modified data set dat_crr is still in the working library:PROC PHREG DATA=dat_crr covs(aggregate);MODEL (_start_,_stop_)*_censcrr_(0)=age logUosm logProt logCCR pkd bblock diur/rl;WEIGHT _weight_;BASELINE out=cuminccurves covariates=datameans survival=_surv_;RUN;The output data set cuminccurves contains, for the three fictive patients, ‘survival’ curves estimated from the weighted Cox regression analysis. These three estimated ‘survival’ curves can be simply transformed into CIFs:DATA cuminccurves;SET cuminccurves;cuminc=1 - _surv_;RUN;Finally, the CIFs are plotted:SYMBOL1 v=none i=steplj line=1 c=blue width=2;SYMBOL2 v=none i=steplj line=1 c=black width=2;SYMBOL3 v=none i=steplj line=1 c=green width=2;AXIS1 order=0 to 0.40 by 0.05 label=(angle=90 ‘Cumulative incidence of dialysis’);AXIS2 order=0 to 8 by 1 value=(‘0’ ‘1’ ‘2’ ‘3’ ‘4’ ‘5’ ‘6’ ‘7’ ‘8’) label=(‘Time in years’);LEGEND1 value=(‘315’ ‘510’ ‘780’) label=(‘Osmolarity’);PROC GPLOT DATA=cuminccurves;PLOT cuminc*_stop_=logUosm/vaxis=axis1 haxis=axis2 legend=legend1;RUN;As visualised in Fig. 4, patients with increased osmolarity have a higher predicted cumulative dialysis incidence probability than patients with lower osmolarity.The following code can be used to obtain weighted Schoenfeld-type residuals for the PSH model:PROC PHREG DATA=dat_crr covs(aggregate) outest=estimates;MODEL (_start_,_stop_)*_censcrr_(0)=age logUosm logProt logCCR pkd bblocker diur;OUTPUT out=schoenfeld_data wtressch=WSR_age WSR_logUosm WSR_logProtWSR_logCCR WSR_pkd WSR_bblocker WSR_diur;ID _id_;WEIGHT _weight_;BY _by_;RUN;The third line of the code shown above (the OUTPUT statement) creates a new data set, schoenfeld_data, containing the weighted Schoenfeld-type residuals for all variables in the model and all event time points. In the code below, we merge the data set of the Schoenfeld-type residuals with the data set estimates, created by PROC PHREG above, which contains the parameter estimates. For merging, it is necessary to specify a primary key variable. We can make use of the constant _by_, which is automatically generated by the macro, and which assumes the value of 1 for all observations in schoenfeld_data as well as in estimates.DATA schoenfeld_data;MERGE schoenfeld_data (keep=WSR_logCCR _stop_ _by_) estimates;BY _by_;rescaled_WSR_logCCR=WSR_logCCR+logCCR;LABEL rescaled_WSR_logCCR=“beta(t) of log2 of creatinine clearance” _stop_=“Time in years”;RUN;In the fourth line of the code above, we rescale the residuals by adding the parameter estimates. Rescaled and smoothed residuals have the interpretation of time-dependent parameter estimates. Smoothing can be performed using PROC LOESS as described below, and by making use of ODS GRAPHICS the raw and smoothed time-dependent parameters along with their 95% confidence limits can be displayed:ODS GRAPHICS on;ODS SELECT fitplot;PROC LOESS DATA=schoenfeld_dataPLOTS=residuals(smooth);MODEL rescaled_WSR_logCCR=_stop_/clm;RUN;ODS GRAPHICS off;The smoothed Schoenfeld-type residuals of logCCR reveal a time-dependent effect (see Fig. 5), showing a decreasing importance of that variable in time. A formal test of this graphical impression could be obtained by assessing the correlation of the Schoenfeld-type residuals with time or a suitable transformation of time (such as the log). In the code shown below, we create a new variable logstop defined as the natural logarithm of the time variable. Then, we call PROC CORR to compute correlations of the Schoenfeld-type residuals with the original and transformed time variables.DATA schoenfeld_data;SET schoenfeld_data;logstop=log(_stop_);RUN;PROC CORR DATA=schoenfeld_data;VAR _stop_ logstop;WITH WSR_age WSR_logUosm WSR_logProt WSR_logCCR WSR_pkd WSR_bblockerWSR_diur;RUN;The output of PROC CORR shown below indicates some issues with the proportionality assumption for the variable logCCR: the correlation coefficient of the weighted Schoenfeld-type residuals with log of time is 0.346 and highly significant.The CORR ProcedurePearson Correlation CoefficientsProb>|r| under H0: Rho=0Number of Observations_stop_logstopWSR_age−0.08184−0.04724Standardized Schoenfeld Residual age0.41820.6407100100WSR_logUosm−0.11007−0.15490Standardized Schoenfeld Residual logUosm0.27560.1238100100WSR_logProt−0.12749−0.04825Standardized Schoenfeld Residual logProt0.20620.6336100100WSR_logCCR0.307150.34600Standardized Schoenfeld Residual logCCR0.00190.0004100100WSR_pkd0.087290.09667Standardized Schoenfeld Residual PKD0.38780.3387100100WSR_bblocker0.087810.07293Standardized Schoenfeld Residual BBlocker0.38500.4709100100WSR_diur−0.07860−0.08767Standardized Schoenfeld Residual diur0.43700.3857100100The obvious time-dependency of the effect of logCCR can be modelled, e.g., by including an interaction (product) term between logCCR and logstop1=log(_stop_+1). By adding a constant of 1/12 to the time variable _stop_ (corresponding to one month) before taking the logarithm we achieve a value of 0 for the interaction term logstop1*logCCR at baseline, which makes the main effect of logCCR interpretable as the effect of logCCR at baseline. PROC PHREG's HAZARDRATIO statement can be used to compute the subdistribution hazard ratios (SHR) and 95% confidence intervals at different time points, e.g., at baseline, 6 months, 1, 3 and 5 years. These times have to be supplied in the scaling of logstop1 (log(T+1/12)), as −2.49, −0.54, 0.08, 1.13, and 1.63:PROC PHREG DATA=dat_crr covs(aggregate);MODEL (_start_,_stop_)*_censcrr_(0)=age logUosm logProt logCCR pkd bblocker diur logCCR*logstop1/rl;logstop1=log(_stop_+1/12);ID _id_;WEIGHT _weight_;HAZARDRATIO logCCR/at(logstop1=-2.49 -0.54 0.08 1.13 1.63);RUN;The results from this call are shown below:The PHREG ProcedureAnalysis of Maximum Likelihood EstimatesParameterDFParameter EstimateStandard ErrorStdErr RatioChi-SquarePr>ChiSqage1−0.124620.078461.1282.52280.1122logUosm10.657590.313751.0554.39270.0361logProt10.608900.073060.95569.4532<.0001logCCR1−2.641520.267780.81297.3099<.0001PKD11.141260.336830.96811.48010.0007BBlocker10.409150.227571.0493.23260.0722diur10.473900.224691.0054.44840.0349logstop1*logCCR10.839330.194860.78118.5532<.0001Analysis of Maximum Likelihood EstimatesParameterHazard Ratio95% Hazard Ratio Confidence LimitsLabelage0.8830.7571.030agelogUosm1.9301.0443.570logProt1.8381.5932.121logCCR...PKD3.1311.6186.058BBlocker1.5060.9642.352BBlockerdiur1.6061.0342.495logstop1*logCCR....logstop1 * logCCRHazard Ratios for logCCRDescriptionPoint Estimate95% Wald Robust Confidence LimitslogCCR Unit=1 At logstop1=−2.490.0090.0020.033logCCR Unit=1 At logstop1=−0.540.0450.0230.088logCCR Unit=1 At logstop1=0.080.0760.0460.127logCCR Unit=1 At logstop1=1.130.1840.1180.286logCCR Unit=1 At logstop1=1.630.2800.1650.474The interaction term has a highly significant and strong effect on the subdistribution hazard. Therefore, the subdistribution hazard ratios (denoted by Hazard Ratios by PROC PHREG) are markedly increasing from 0.009 at baseline to 0.280 at 5 years.Firth's bias correction is readily available in PROC PHREG. To exemplify the effect of the Firth correction in a PSH model we extracted a random ‘smallsample’ subset of 200 patients from the original study group. Twenty-eight of these patients experienced the event of interest (dialysis) and two of them died. For simplicity and to illustrate the impact of monotone likelihood on the results, we dichotomized the variables age at 60 years (dage60), osmolarity at 500 mOsmol/L (dUosm), proteinuria at 3g/L (dProt) and used only these variables and the log2 transformed creatinine clearance (logCCR) as covariates. Their distribution is summarized in Table 4.The Firth-corrected PSH model is obtained by submitting:%pshreg(action=code, data=smallsample, time=Time, cens=status, out=small_crr,varlist= dage60 dUosm dProt logCCR, options=%str(rl=pl), firth=1);The option rl=pl are passed to the options of PROC PHREG's MODEL statement. Likewise, setting firth=1 will also cause the keyword firth to be included as an option to the MODEL statement. rl=pl is a standard option of PROC PHREG and produces profile likelihood confidence intervals for exp(β), in our case: the subdistribution hazard ratios. Since we request the Firth correction, these confidence intervals are computed using profile penalized likelihood, as suggested by Heinze and Schemper [13]. The action=code option will generate the following code:PROC PHREG DATA=small_crr;MODEL (_start_,_stop_)*_censcrr_(0)=dage60 dUosm dProt logCCR/rl=pl firth;ID _id_;WEIGHT _weight_;TITLE “Firth-corrected PSH model”;RUN;Submission of this code leads to a bias-corrected PSH model:Firth-corrected PSH modelThe PHREG ProcedureAnalysis of Maximum Likelihood EstimatesParameterDFParameter EstimateStandard ErrorChi-SquarePr>ChiSqdage601−2.054961.478041.93300.1644dUosm10.187400.472250.15750.6915dProt10.941490.495473.61080.0574logCCR1−2.495390.6617814.21830.0002Analysis of Maximum Likelihood EstimatesParameterHazard Ratio95% Hazard Ratio Profile Likelihood Confidence Limitsdage600.1280.0010.966dUosm1.2060.5063.223dProt2.5640.9276.435logCCR0.0820.0210.275Using the same data set small_crr, we can also compute an uncorrected PSH model by submitting:PROC PHREG DATA=small_crr COVS(AGGREGATE);MODEL (_start_,_stop_)*_censcrr_(0)=dage60 dUosm dProt logCCR/rl;ID _id_;WEIGHT _weight_;TITLE “Uncorrected PSH model”;RUN;which results in:The PHREG ProcedureAnalysis of Maximum Likelihood EstimatesParameterDFParameter EstimateStandard ErrorStdErr RatioChi-SquarePr>ChiSqdage601−15.540920.408010.0001450.7792<.0001dUosm10.239840.495661.0370.23420.6285dProt10.910840.513741.0253.14340.0762logCCR1−2.580380.500170.74926.6152<.0001Analysis of Maximum Likelihood EstimatesParameterHazard Ratio95% Hazard Ratio Confidence Limitsdage600.0000.0000.000dUosm1.2710.4813.358dProt2.4860.9086.806logCCR0.0760.0280.202The parameter estimator of dage60 in the uncorrected PSH model diverges to −∞. PROC PHREG reports the parameter estimates at the iteration at which the likelihood convergence criterion is fulfilled. At that iteration, the model-based standard error for dage60 diverges, while the robust standard error collapses towards 0. Likewise, the 95% Wald confidence limits for the SHR based on the robust standard deviation collapse to [0,0], which is implausible given the small sample size. The Firth correction supplies plausible point estimates, providing a convergent value for dage60 and values similar to the uncorrected analysis for the other variables. The SHRs of those variables by the Firth-corrected analysis are just slightly closer to 1 than their uncorrected counterparts. This is expected since the Firth correction removes some of the small-sample bias from the parameter estimates, which is considered as causing some overestimation of effects. The small-sample bias is even more severe in this example as the number of events per independent variable is 14/4=3.5. For standard Cox regression, the Firth bias correction has recently been reported as almost unbiased even when the number of events per independent variable was only between two and six [20].Profile penalized likelihood confidence intervals in the output of the Firth-corrected model are provided on the hazard ratio scale. For illustration we have transformed these confidence intervals to the scale of coefficients (log hazard ratios) and compare them to the corresponding normal approximated confidence intervals in Table 5. We learn that indeed, due to skewness of the profile penalized likelihood of dage60, the profile penalized likelihood confidence intervals differ from the ones based on the normal approximation, and should be preferred [13]. Their drawbacks are the increased computational effort, since for computation of each limit of each parameter an iterative optimization is needed, and their assumption of a correctly specified model (see the discussion above). Nevertheless, in situations of a diverging parameter estimate, they are certainly the best choice as both the robust and the model-based covariance will give unusable results.So far, data analysists working with SAS had to switch to a different statistical programming environment such as R or STATA if they needed a PSH analysis. Although using multiple environments for an analysis project may have advantages in terms of flexibility, it is often inconvenient, as workflows cannot be saved as a single program file, and thus are harder to trace in case of revisions. The new macro enables a workflow that resides entirely in the SAS environment, using as the workhorse procedure the well-validated and documented PROC PHREG. We have compared the results obtained from our macro with those obtained by R‘s cmprsk package for the data sets presented here as well as for some other data sets, and could not find any relevant differences; the maximum difference in estimated parameter estimates was negligible if the time scale was truly continuous. However, with the possibility of tied event and censoring times, there might be slight differences as in our macro we clearly interpret censoring times which are tied with event times as occurring after event times. It is not fully documented how such ties are handled in cmprsk.For fitting standard proportional subdistribution hazards models, our SAS macro offers the same functionality as the crr function of the R package cmprsk. Moreover, we have extended the Fine-Gray model and its existing implementations in R and STATA by providing a bias correction in small samples, which effectively eliminates the occurrence of infinite parameter estimates. In addition, PROC PHREG enables the analyst to make use of several additional built-in features, such as Bayesian analysis, model assessment via martingale residuals, inclusion of frailty effects, estimation of effect contrasts, etc.; all of which not yet explored in context of the Fine-Gray model. It is also possible to apply nonlinear estimation techniques in combination with our macro, such as restricted cubic splines [21,22] or fractional polynomials [23].Very recently, a new SAS version 9.4 (including SAS/STAT version 13.1) has been released in which the Fine-Gray model has been made directly available in PROC PHREG, by specifying the code of the event of interest in a new option EVENTCODE of the MODEL statement. All other codes which are not contained in the list of censoring values are then treated as competing event codes. We have compared the functionality of this new option with our macro by re-analyzing our examples. Even with the new EVENTCODE option, it is not possible to:•apply variable selection (e.g., backward elimination),compute Schoenfeld-type residuals,apply the Firth correction or compute profile-likelihood based confidence intervals,use the ASSESS statement for assessing model assumptions using martingale residuals,include frailty effects.All these options are possible with %pshreg as it first modifies the input data set which can then be treated as any other survival data set, making full use of the functionality of PROC PHREG.A limitation of our current implementation of %pshreg (and of the new SAS 9.4 implementation of the Fine-Gray model) is that it does not allow for left-truncation or interval-censoring. Similarly, the macro and the SAS 9.4 implementation cannot handle time-varying predictors implemented via multiple records of follow-up time (counting process style input) per subject [24,25]. Further information on our macro, including full documentation of its options, several examples and numerical comparisons with cmprsk can be found in a Technical Report containing some example calls and output. The macro and the Technical Report are available under the GNU GPL-2 license at http://cemsiis.meduniwien.ac.at/en/kb/science-research/software/statistical-software/pshreg/.No benefits in any form have been received or will be received from a commercial party related directly or indirectly to the subject of this article.

@&#CONCLUSIONS@&#
