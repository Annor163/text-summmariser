@&#MAIN-TITLE@&#
Reinforcement-learning based dialogue system for human–robot interactions with socially-inspired rewards

@&#HIGHLIGHTS@&#
We integrate user appraisals in a POMDP-based dialogue manager procedure.We employ additional socially-inspired rewards in a RL setup to guide the learning.A unified framework for speeding up the policy optimisation and user adaptation.We consider a potential-based reward shaping with a sample efficient RL algorithm.Evaluated using both user simulator (information retrieval) and user trials (HRI).

@&#KEYPHRASES@&#
Human–robot interaction,POMDP-based dialogue management,Reinforcement learning,Reward shaping,

@&#ABSTRACT@&#
This paper investigates some conditions under which polarized user appraisals gathered throughout the course of a vocal interaction between a machine and a human can be integrated in a reinforcement learning-based dialogue manager. More specifically, we discuss how this information can be cast into socially-inspired rewards for speeding up the policy optimisation for both efficient task completion and user adaptation in an online learning setting. For this purpose a potential-based reward shaping method is combined with a sample efficient reinforcement learning algorithm to offer a principled framework to cope with these potentially noisy interim rewards. The proposed scheme will greatly facilitate the system's development by allowing the designer to teach his system through explicit positive/negative feedbacks given as hints about task progress, in the early stage of training. At a later stage, the approach will be used as a way to ease the adaptation of the dialogue policy to specific user profiles. Experiments carried out using a state-of-the-art goal-oriented dialogue management framework, the Hidden Information State (HIS), support our claims in two configurations: firstly, with a user simulator in the tourist information domain (and thus simulated appraisals), and secondly, in the context of man–robot dialogue with real user trials.

@&#INTRODUCTION@&#
In a goal-oriented vocal interaction between a machine and a human, the dialogue manager (DM) is responsible for making appropriate dialogue decisions to fulfil the user goal based on uncertain dialogue contexts. The Partially Observable Markov Decision Process (POMDP) framework (Kaelbling et al. (1998)) has been successfully employed in the Spoken Dialogue System (SDS) field (Young et al., 2010; Thomson and Young, 2010; Pinault and Lefèvre, 2011) as well as in human robot interaction (HRI) context (Roy et al., 2000; Lucignano et al., 2013), due to its capacity to explicitly handle parts of the inherent uncertainty of the information which the system has to deal with (e.g. erroneous speech recognizer, falsely recognised gestures, etc.). In this setup, the agent maintains a distribution over possible dialogue states, referred to as the belief state in the literature, and interacts with its perceived environment using a reinforcement learning (RL) algorithm so as to maximise some expected cumulative discounted reward (Sutton and Barto, 1998).Recent studies in SDS have shown the possibility to learn a dialogue policy from scratch with a limited number (several hundreds) of interactions (Gašić et al., 2010; Sungjin and Eskenazi, 2012; Daubigney et al., 2012) and the potential benefit of this approach compared to the classical use of a Wizard-of-Oz or developing a well-calibrated user simulator (Gašić et al., 2010). Following this idea, sample-efficient learning algorithms, as for instance the Kalman Temporal Differences (KTD) framework (Geist and Pietquin, 2010; Daubigney et al., 2012), can be employed to learn and adapt a system behaviour in an online setup, i.e. while interacting with users. The main shortcoming of this approach is the very poor initial performance. Lowering the length of this warm-up learning phase, defined as the phase when the system can hardly interact with real users due to a high level of exploration and poor performance, is still an open problem when such systems are to be applied to real-world domains. Some solutions can be to introduce some initial expert knowledge (Williams, 2008) or to find ways to collect more hints from the environment which will accelerate the policy learning.Moreover, problems addressed by RL generally introduce non-stationa-rity at several levels. Indeed, as in many real-world machine learning applications, adaptation to non-stationary environments is a desired feature. In the DM case, users with various levels of expertise (from novice to advanced) and characteristics (restless, bad pronunciations, bad hearing, etc.) can interact with the system. So, the latter should be able to cope with a wide range of behaviours, which may also change over time (switch to new users but also user self-adaptation to the system). Another source of non-stationarity arises when the policy iteration scheme (Sutton and Barto, 1998) is adopted. Policy iteration is an iterative procedure which aims at discovering the optimal policy by generating a sequence of monotonically improving policies. Each iteration consists of two stages: policy evaluation which computes the value function of a given policy and policy improvement which defines the improved policy over the value function. The fact that the value function changes together with the policy makes it non-stationary. In all non-stationary contexts (e.g. environment, optimization method) tracking the value function instead of converging to it seems preferable. A more detailed discussion about the advantages of tracking versus converging, even in stationary environments, can be found in Sutton et al. (2007). Most existing RL algorithms assume stationarity of the problem at hand and aim at converging to a fixed solution. Actually, few attempts to handle non-stationarity can be found in the literature. Among them, we can mention a class of methods which combine RL and planning paradigms such as the Dyna-Q algorithm (Sutton and Barto, 1998).In most works, the reward function used to learn the dialogue agent is exclusively based on objective features, such as duration and full completion of the user goal. The overall quality of such a function plays a crucial role in finding the optimal solution. However, recent studies have shown that such features, although objective, could not be collected with entire reliability from users (Gašić et al., 2010; Sungjin and Eskenazi, 2012). Anyhow, if the user's point of view is totally ignored or reduced to a rather simple satisfaction questionnaire, naturalness of the overall system can be impacted. In the PARADISE evaluation paradigm (Walker et al., 1997), subjective and objective features are correlated through linear regression. It is worth noting that subjective information is more easily produced by the user. Therefore, it may be interesting to gather some subjective features during the course of the dialogue in order to accelerate the policy learning instead of relying exclusively on an imprecise final appraisal.This idea could be linked to some works in social and human sciences (e.g. psychology, anthropology) which have shown to which extent acts as simple and spontaneous as facial expressions or gestures can convey social meaning affecting our perception and shaping our daily interactions (Richmond et al., 1991; Kunda, 1999; Custers and Henk, 2005). Also Vinciarelli et al. (2009) present a wide coverage survey of an emerging domain aiming to endow computers with social intelligence abilities. And among these abilities some of the most important are correct perception, accurate interpretation and appropriate generation of social signals. So in the same line of thought, in this proof of concept study, we are focusing on the potential interest of considering a subclass of these social signals with which a user conveys some raw assessments of the current situation during the course of the interaction. Indeed, we claim that positive/negative user appraisals gathered during the course of the dialogue can be used to partially address the two aforementioned bootstrap and tracking problems.By the fact that user appraisals can be gathered all along the dialogue, we intend to directly exploit them in a socially-inspired diffuse and interim reward function employed in online learning strategy. In that sense, the formulated problem can be closely related to the reward shaping one. In RL, reward shaping consists in supplying meaningful diffuse rewards to a learning agent with the objective to speed up the learning towards the same optimal policy than the one that we could reach with a sparse reward function, giving the meaningful reward only at the end of an episode (e.g. task completion). For example, El Asri et al. (2013) have investigated how a corpus of evaluated dialogues can be used to estimate a posterior diffuse reward function based on dialogue features representative of the system usability (e.g. dialogue length, task completion) in a way to address the temporal credit assignment problem in an offline setting (i.e. offline batch learning).Despite some recent attempts to use emotion as a judgement of the task progress with RL (e.g. Broekens and Haazebroek, 2007), little has been done in the goal-oriented DM problem context. Two of the reasons explaining this are that it requires both non-stationary and sample-efficient RL algorithms able to cope with this additional variability and reliable mechanisms to correctly estimate these appraisals from real behavioural cues (such as smiles or nods). Even if the latter point is not addressed (although discussed) in the present study, we propose a potential-based shaping reward method (Ng et al., 1999) to integrate some socially-inspired aspects in the RL scheme in combination with the use of the unified KTD framework (Geist and Pietquin, 2010). The latter expresses the problem of value function approximation as a filtering problem (Kalman filtering). This framework has several advantages and desirable properties for the DM problem (Daubigney et al., 2012). Among others, it is sample-efficient (being based on second-order statistics), it allows online/batch on-policy/off-policy learning, it offers ways to fit the exploration/exploitation dilemma through uncertainty estimation and it supports linear or non-linear parametrisation. Furthermore, as shown in Geist et al. (2009) on toy examples the KTD framework tracks the optimal solution rather than converging to it which is a desirable property for the targeted issues here.To illustrate the potential benefit of the proposed approach we first carry out a preliminary study on a tourist information retrieval task where a simulation setting is available to statistically validate the impact of adding this additional information to the learning stage. Then, in the context of the MaRDi project,11Man–Robot Dialogue project, funded by the French National Agency for Research.we consider a Pick-Place-Carry HRI task involving real users to obtain the first results in a more realistic setup. More exactly, a 3D simulation software is used where the human can interact with a robot through an avatar involving multimodal dialogues. Although objectively artificial, this platform provides an interesting test-bed module for online dialogue learning. Indeed, a better control over the global experimental conditions can be achieved (e.g. environment instantiation, robot's sensors, etc.). Hence, comparisons between different approaches and configurations are facilitated. Furthermore, this solution reduces the subject recruitment costs without strongly hampering their natural expressiveness (due to the capacities offered by the simulator).Preliminary results have been already presented in Ferreira and Lefèvre (2013a, 2013b). This extended journal paper offers a unified presentation of the proposed approach along with a direct application on a robotic task with real user trials.The remainder of the article is organised as follows. In Section 2 some background on the POMDP-based Dialogue Management problem, the RL paradigm and the KTD method are given. Then, in Section 3 the socially-inspired interim reward principle is detailed. Section 4 is dedicated to present the two considered tasks and the experimental setup. Then the following section details and comments on the various evaluation results obtained. Section 6 discusses some considerations relevant to the use of socially-inspired reinforcement, before concluding in Section 7 with some perspectives.This section briefly recaps some of the main notions required to follow the novelties proposed in the paper. Readers not familiar with machine-learning approaches for dialogue management are invited to glance at the given references to have a more precise picture of the current state-of-the-art of the field.The dialogue management problem has first been described in Levin et al. (1997) as a Markov Decision Process (MDP) to determine an optimal mapping between situations and actions. A MDP is a tuple {S, A, T, R, γ}, where S is the state space (discrete, continuous or mixed), A is the discrete action space, T is a set of Markovian transition probabilities, R is the immediate reward function,R:S×A×S→Rand γ∈[0, 1] the discount factor (discounting long term rewards). The environment evolves at each time step t to a state stand the agent picks an action ataccording to a policy mapping states to actions, π:S→A. Then state changes to st+1 according to the Markovian transition probability st+1∼T(.|st, at) and, following this, the agent received a reward rt=R(st, at, st+1) from the environment. The overall problem of MDP is to derive an optimal policy maximising the reward expectation. Typically the averaged discounted sum over a potentially infinite horizon is used,∑t=0∞γtrt. Thus, for a given policy and start state s, this quantity is called the function:Vπ(s)=E[∑t≥0γtrt|s0=s,π]∈RS. V* corresponds to the value function of any optimal policy π*. The Q-function may be defined as an alternative to the value function. It adds a degree of freedom on the first selected action,Qπ(s,a)=E[∑t≥0γtrt|s0=s,a0=a,π]∈RS×A. As well as V*, Q* corresponds to the action-value function of any optimal policy π*. If it is known, an optimal policy can be directly computed by being greedy according to Q*,π*(s)=argmaxaQ*(s,a)∀s∈S.The POMDP framework (Kaelbling et al., 1998), as a generalization of the fully-observable MDP, maintains a belief distribution b(s) over user states, assuming the true one is unobservable. Indeed, POMDP explicitly handles parts of the inherent uncertainty of the DM problem (e.g. word error rate, concept error rate). A POMDP policy maps the belief state space into the action space. That is why the optimal policy can be understood as the solution of a continuous space MDP. In practice, POMDP problems are intractable to solve exactly due to the curse of dimensionality (i.e. belief state/action spaces). Among other techniques, the HIS model (Young et al., 2010) circumvents the scaling problem for the DM by organising the belief space into partitions, grouping states sharing the same probability, and then mapping the master belief space (partitions) into a much reduced summary space where RL algorithms work reasonably well.Although variants have been proposed and tested, e.g. Pinault and Lefèvre (2011), HIS remains a reference. However, the choice of a Monte Carlo Control RL algorithm (Sutton and Barto, 1998) is still questioned and recent studies showed the interest of considering sample-efficient algorithms for the DM problem (Gašić et al., 2010; Daubigney et al., 2012). More especially Daubigney et al. (2012) showed that Kalman Temporal Differences (KTD) framework (Geist and Pietquin, 2010) offers a unified framework able to cope with all DM required properties. Indeed, it is sample-efficient, it allows on-policy/off-policy learning through two algorithms (respectively KTD-Q and KTD-SARSA) which can both perform online and offline learning, it provides ways to deal with the “exploration/exploitation” dilemma using uncertainty on value estimates, it allows value tracking, and it supports linear and non-linear parametrisation. Furthermore, KTD algorithms were favourably compared to different state-of-the-art algorithms able to deal with one single property at once, such as Q-learning, LSPI or GP-SARSA.The KTD framework (Geist and Pietquin, 2010) is derived from the well-known Kalman filter algorithm (Kalman, 1960) aiming at inferring some hidden variables from related past observations and applied to the estimation of the temporal differences for the action-value function optimisation.In the considered linear case, a parametric representation of the Q-function is chosen:Qˆθ=θTϕ(s,a), where the feature vector ϕ(s, a) is a set of n basis functions to be designed by the practitioner andθ∈Rnthe parameter vector to be learnt. Notice that a non-linear representation of the Q-function could be employed. However, just the very basic explanations are recalled here, for further details please refer to Geist and Pietquin (2010) and Daubigney et al. (2012). The components of the parameter vector θ are the hidden variables which are modelled as a random vector. Such parameter vector is considered to evolve following a random walk though an evolution equation:θt=θt−1+vt, withvta white noise of covariance matrixPvt. The latter allows to take into account the possible non-stationarity of the function. The observations correspond to the environment rewards which are linked to the hidden parameter vector through one of the sampled Bellman equations gt(θt) depending on the RL scheme employed (i.e. evaluation for on-policy or optimality for off-policy learning):gt(θt)=Qˆθt(st,at)−γQˆθt(st+1,at+1)(evaluation)Qˆθt(st,at)−γmaxaQˆθt(st+1,a)(optimality)Rewards are supposed to follow the observation equation: rt=gt(θt)+ntwhere a white noise ntwith covariance matrixPntis also considered. Two algorithms can be defined: KTD-SARSA which denotes the use of the sampled evaluation Bellman equation and KTD-Q, the use of the sampled optimality one.This section describes the socially-inspired reward principle employed in our proposition and how this kind of reward can be estimated from both simulated and real user appraisals.“Social signal” is a generic term which encompasses all signals that convey socially relevant information like (dis-)agreement, empathy, hostility. In human–human interaction they are expressed by means of behavioural cues (e.g. blinks, smiles, crossed arms, laughter, nods and the like). The term socially-inspired RL is employed here to denote a learning process exploiting a subclass of these signals. However this information can be used in multiple ways in the RL scheme (e.g. as part of user state in the dialogue model, as a meta-parameter influencing the exploration/exploitation scheme, or even as part of the system response for an emotional agent). In the present work we exclusively consider this information as a way to gather an additional reinforcement signal for speeding up dialogue policy optimisation in an online learning setting.So this work focuses on exploiting socially-inspired rewards (also denoted for short as social rewards afterwards) based on positive and negative appraisals emitted by the user and using them as additional interim feedbacks perceived by the system at each dialogue turn. In this scenario these appraisals are considered to be the user's assessments of the interaction evolution and thus implicitly about the overall task progress. Ergo, the social reward itself corresponds to the associated positiveness or negativeness of an appraisal and can be represented as a signed real value.We propose to use this newly defined social reward function as a shaping reward function. This kind of function is dedicated to help the learning algorithm by giving additional “shapings” to guide it towards a good policy faster. The memoryless shaping reward function, which is one of the most general shaping patterns, is adopted here. So, the overall reward function is the sum of the basic environment reward function Renv(objective) and the new social one Rsocial (subjective). The resulting transformed MDP M′ is defined by the tuple (S, A, T, γ, R′) where R′ is the reward function defined as: R′(st, at, st+1)=Renv(st, at, st+1)+Rsocial(st, at, st+1) whereRsocial:S×A×S→Ris a bounded real-valued function considered here as the social shaping reward function. Since the system is learning a policy for M′ with the idea of using it in M, the question at hand is: what form of social shaping reward function Rsocial can guarantee that the optimal policy in M′ will be optimal in M? In the case where no further knowledge of the T and R dynamics is available (no expert), a potential-based shaping reward leaves (near-)optimal policies unchanged (Ng et al., 1999). Hence, a potential-based shaping reward function is adopted for Rsocial, corresponding to function F in Ng et al.'s paper, and can be defined as follows:(1)Rsocial(st,a,st+1)=γψ(st+1)−ψ(st)where ψ is a real-valued function (denoted shaping potential function) used to evaluate each state and here associated to the user appraisal valence.In a preliminary study to evaluate the impact of social rewards on policy convergence the agenda-based user simulator from Schatzmann et al. (2006) is used wherein the user is simulated at the intentional semantic level (i.e. dialogue act level). This approach factors the user state into an agenda A and a goal G: S = (A, G), where G = (C, R).The goal G ensures that the simulated user reacts in an appropriate, consistent and goal-oriented manner. It consists of a set of constraints C specifying the required properties that the system should satisfy (they are the objects of the negotiation) and a set of requests R which represent the desired pieces of information (e.g. address, phone number, available schedules). The agenda A is a stack-like structure containing the pending user acts that are deemed necessary to elicit the information specified in the goal. For further details on this simulation method please refer to Schatzmann et al. (2006) and Keizer et al. (2010).Table 1gives a sample dialogue for the TownInfo task (Schatzmann et al., 2006) and illustrates how the simulation works and how tracking both the agenda and the goal evolution can serve to give the system some hints on what would be the current user's judgement on the interaction course. For instance, in A2, the presence of a negate act at the top of the agenda means that a user constraint has been violated (here drinks=beer) and thus can be perceived as a negative hint. In the same way, the affirm act in A3 underlines a positive situation. Hence the nature of the top dialogue act of the agenda can be considered as a useful cue to determine the valence of the user's appraisal with realistic outcomes (i.e. correctly reflecting what a real user appraisals would be in such situations).Table 2presents some simple positive and negative cues extracted from the agenda and goal internal structures in the user simulator during dialogue simulations. The underlying assumption is that combinations of such kind of cues might result in extracting some situation where a user is prone to generate some form of positive/negative appraisal to the system such as shouting “no no no that's wrong” and grimacing in a real setup. Hence, each of them is weighted in order to give more or less emphasis on specific features in the socially-inspired reward computation. Thus, the considered social signals are directly encoded into the reward function rather than in the simulated user behaviours. Although a continuous scale is possible, a five-point agreement scale (Likert scale) is adopted here for ψ with regard to the way subjective measures are gathered in PARADISE (Walker et al., 1997). Each level is associated with a representative real number associated with an agreement scale, from strongly negative (−−) to strongly positive (++). So, after a normalisation step the sum of all the weighted cues gives an overall scoreCstwhich is rescaled on a five-point Likert scale using a threshold ξ. Thus, at each time step t, a potential-based social shaping reward is computed using Eq. (1) and ψ function:ψ(s)=−1,ifCs<−ξ(−−)−0.5,if−ξ≤Cs<0(−)0,ifCs=0(neutral)0.5,if0<Cs≤ξ(+)1,ifCs>ξ(++)The process of social reinforcement reward computation can be decomposed into two steps:1.gathering of positive and negative cues from the factored user state;estimate of the social reward using the potential-based shaping reward function.An example of such a process is summarised in Table 3. The first column represents the analysed user state st(i.e. the corresponding agenda Atand goal Gtin Table 1). The second and the third columns are respectively the lists of positive and negative cues which have been detected (using the id from Table 2) and their associated value in brackets. For example, in the first row and third column, cue 2 corresponds to the number of items in the agenda and the value 6 is extracted from A3, the minus sign merely indicates the negativeness of the cue. The fourth column corresponds to the ψ value (i.e. the Likert score). It is computed applying some weights on the detected cue values. As an illustration, for the negative cue 3, 1/30 is chosen as weight because the maximum number of turns allowed by the system is 30. Consequently, 1/30 can be viewed as a normalisation value. Indeed, the objective at hand is to range each cue value in the interval [−1, 1] according to their valence and their relative importance. It is important to notice that such normalisation weights have been determined following some expert intuitions and are not considered as optimal. They have been chosen to deliver an average user appraisal of the dialogue progress. Different user profiles can be designed by varying these weights as well as the global patience (i.e. tolerance of inconsistent system behaviours) and initiative (i.e. number of conveyed pieces of information) levels of the simulated user to study to what extent user appraisals can help user adaptation capacities of a learning agent. The last column shows the resulting social reward, applying Eq. (1) with γ=0.95. The positive score 0.45 denotes a quite favourable evolution between s3 and s4. To compete with the environment reward the social reward can be rescaled using a weight coefficient before being added to Renvin R′.In real applications user appraisals must be detected from observable behavioural cues employed by the user to convey her judgement on her own state evolution. This could be done using several multimodal social detectors (e.g. emotion face tracking, gesture classification, social keyword spotting, etc.). These latter may produce a set of positive and negatives cues for each detector. For instance, the face tracker may produce a cue dedicated to smile detection the value of which is the probability of its inner model consisting in a positive cue. Then, a similar weighted interpolation mechanism as the one presented in Section 3.2 could be used to infer ψ(s) from the valued cues output by the various detectors. This could be done in a more principled and data-driven way than the previously described handcrafted settings, for instance using regression methods exploiting an annotated corpus comparable to Rieser and Lemon (2011) or El Asri et al. (2013) for reward estimation. However, here we intend to test the benefits of using socially-inspired rewards related to the user appraisal. And as such we mean to be able to control the level of uncertainty of the social cue detection in the system and not to depend on the intrinsic quality of the detection components. So, a simple workaround used in the HRI user trials consists in asking the user to make explicit her appraisal through an interface which allows her to rate the current state evolution towards the task completion after each transition on a five-point agreement scale which is directly considered as the ψ value.This section describes the two considered dialogue tasks as well as the experimental setup employed for simulated and real user trials.In this study two versions of a HIS-based dialogue system are considered. The first one is dedicated to the TownInfo task (Young et al., 2010) in the tourist information domain. A user wants to obtain some information (address, phone number) about a particular venue located in a virtual town based on some constraints such as its type, area, served food, price range, etc. This system has already been tested with real users in Schatzmann et al. (2006), and in a more recent and matured version, called CamInfo (Cambridge tourist information), in Gašić et al. (2010). In our experiments only the simulated version of the task will be used, where simulated user appraisal cues are obtained as described in Section 3.2. Thanks to an already available user simulator this task offers a convenient way for a preliminary evaluation and tuning of the proposed approaches.The second version, the MaRDi Dialogue System which is the true goal of our work, is designed to solve a Pick-Place-Carry task in an HRI context. In this setup, the robot and the user share a three rooms flat environment, in which there are different kinds of objects varying in terms of colour, type, and position (e.g. a blue mug on the kitchen table, a red book on the living room table). The user interacts with the robot using unconstrained speech (large vocabulary speech recognition) and pointing gestures to ask the robot to perform some specific object manipulation tasks (e.g. “move the blue mug from the living room table to the kitchen table”). A multimodal dialogue is used to solve ambiguities and to request missing information until task completion (i.e. full command execution) or failure (i.e. explicit user disengagement or wrong command execution). An example of such an interaction is given in Table 4.In our experimental setup, the MaRDi multimodal dialogue system is tightly coupled with a 3D simulation software. Here, the open-source robotics simulator MORSE (Echeverria et al., 2011) is used. This tool provides a realistic rendering (see Fig. 1) through the Blender Game Engine, supports a wide range of middleware (e.g. ROS, YARP), and proposes reliable implementations of realistic sensors and actuators which ease the integration on real robotic platforms afterwards. It also provides the operator with an immersive control (see Fig. 2) of a virtual human avatar in terms of displacement, gaze and interactions with the environment, such as object manipulation (e.g. grasp/release of an object).As shown in Fig. 3, 12 components are involved in the overall functioning of the MaRDi system. The four orange ones are those implicated in the user's input management, speech and gesture modalities in our case. The combination of the Google Web Speech API22https://www.google.com/intl/en/chrome/demos/speech.html.for Automatic Speech Recognition (ASR) and a custom-defined grammar parser for Spoken Language Understanding (SLU) is used to perform speech recognition and understanding. The Gesture Recognition and Understanding (GRU) module simply catches the gesture-events generated by the spatial reasoner during the course of the interaction. Then, the Fusion module temporally aligns the monomodal inputs and then merges them with custom-defined rules. Finally, the result of the fusion (i.e. N-best list of interpretation hypotheses and their related confidence scores) becomes the input of the multimodal DM.The three blue components are responsible of the context modelling. SPARK (Milliez et al., 2014) both detects user gestures and generates the per-agent spatial facts (perspective taking) which are used to dynamically feed the Contextual Knowledge Base. Taking into account the environment during the course of the interaction makes it a situated system. These two modules are responsible of the per-agent knowledge modelling which allows the robot to reason over different perspectives on the world, not only its own but also these of the users. Furthermore, a Static Knowledge Base containing the list of all available objects (even those not perceived) and their related static properties (e.g. colour) is used. More details on these aspects, for instance the notion of perspective taking, and their influence on the dialogue system can be found in Milliez et al. (2014) and Ferreira et al. (2015).The four yellow components are dedicated to the output restitution. The Fission module splits the abstract system action into verbal and non-verbal actions. The spoken output is produced by chaining a template-based Natural Language Generation (NLG) module with a Text-To-Speech Synthesis (TTS) component based on the commercial Acapela TTS system.33http://www.acapela-group.com/index.html.The Non-verbal Behaviour Planning and Motor Control (NVBP/MC) module produces arm gestures and head and body poses for the robot by translating the non-verbal action into a sequence of abstract actions such as grasp, moveTo, release.Finally, the green component is the DM, responsible for updating the internal belief state and making the next robot decision.To assess the performance of introducing socially-inspired information as an additional reinforcement signal, the off-policy KTD-Q algorithm (denoted BASELINE) is employed as our baseline due to its high performance in the conditions at hand (Daubigney et al., 2012). A setup close to the one described in Daubigney et al. (2012) is adopted here. Thus, the Q-function is parametrised using linear-based Radial Basis Function (RBF) networks, one per action and the Bonus-Greedy scheme (Daubigney et al., 2011) is adopted in training conditions, with β=1000 and β0=100. β and β0 are meta parameters used to scale the variance-based bonus added to the Q-value estimates to deal with the exploration/exploitation dilemma. The discount factor γ is set to 0.95 in all experiments.In this study, we distinguish two kinds of conditions to present our results. Firstly, the online training conditions, also denoted as controlled case, where the results are gathered with a continuously improving policy (and thus also exploring policy). Secondly, the testing conditions, denoted as no control, where an already trained policy acts greedily (without exploration ability and no additive bonus) to conduct the interactions.For the TownInfo task, a user simulator is employed and set to interact with the DM at a 10% concept error rate. The weight coefficient used to rescale the social reward is set empirically (grid search) to 4 to compete against the basic environment rewards. Similarly, ξ is set to 0.3 in all experiments. As presented in Section 3.2 all individual cues are manually weighted by following expert intuitions according to their nature and their considered importance in the appraisal valence determination of the targeted user profiles. However the weights assigned to each cue presented in Table 2 have not been tested individually during the evaluation. Hence, these handcrafted parameters are not considered as optimal or specifically tuned for the task at hand.Considering MaRDi, as already mentioned, some trials have been carried out with real users in a 3D virtual environment. At the beginning of each dialogue, a specific goal (a command with arguments in our case) is randomly generated taking into account the simulated environment settings and the current interaction history in order to select a possible command to perform. For example, “You want the robot to give you the white book on the kitchen table”. After each completed interaction the users evaluated the system in terms of task completion. In case of socially-inspired policy learning, throughout the course of the interaction the user is instructed to rate the current state of the dialogue. This is done using a five-star rating bar, accessible on the graphical interface. All the dialogues were recorded both in terms of audio and various kinds of meta-information (e.g. ASR N-Best list, detected gestures and related timestamps, etc.) but also high level annotations (e.g. environment settings, pursued goal, task success). In the full 3D-simulated multimodal architecture, it has been observed that each interaction takes from 7 to 10min to be achieved (object detections, robot movements and displacements, etc.). So, without loss of generality, a practical workaround to speed up the testing process consisted in using a fixed representation of the scene (a screenshot from the human point of view) and a web-based multimodal GUI instead of the fully-operational simulation setup. As for TownInfo, the weight coefficient use to rescale the social reward is set to 4.The authors consider that the average cumulative environment rewards can be a sufficient metric to compare the different approaches. This is explained by the fact that in the environment reward function the success (full user goal completion) is rewarded by a +20 bonus and failure and elapsed time (turn) respectively punished by 0 and −1. For comparison purposes all the experiments with social rewards presented in our plots are given in terms of the environment reward, Renv, only (since we try to converge towards to same optimal policy, see Section 3.1).

@&#CONCLUSIONS@&#
