@&#MAIN-TITLE@&#
Rao-Blackwellized particle filtering with Gaussian mixture models for robust visual tracking

@&#HIGHLIGHTS@&#
Rao-Blackwellized particle filtering with Gaussian mixture models.Incremental learning approaches to update the appearance models.Evaluations on a number of publicly available datasets for visual tracking.

@&#KEYPHRASES@&#
Visual tracking,Rao-Blackwellized particle filtering,Gaussian mixture model,Expectation–Maximization,

@&#ABSTRACT@&#
In this paper, we formulate an adaptive Rao-Blackwellized particle filtering method with Gaussian mixture models to cope with significant variations of the target appearance during object tracking. By modeling target appearance as Gaussian mixture models, we introduce an efficient method for computing particle weights. We incrementally update the appearance models using an on-line Expectation–Maximization algorithm. To achieve robustness to outliers caused by tracking error or partial occlusion in updating the appearance models, we divide the target area into sub-regions and estimate the appearance models independently for each of those sub-regions. We demonstrate the robustness of the proposed method for object tracking using a number of publicly available datasets.

@&#INTRODUCTION@&#
Visual tracking is one of important elements in surveillance or guidance systems and it has been a long-lasting research topic in computer vision. In general, visual tracking can be defined as the problem of sequentially estimating the state of a dynamic object using a sequence of noisy measurements. Particle filters have been widely used for visual tracking because they enable effective estimation of non-linear and non-Gaussian distributions for state space models [1–4]. However, particle filter-based tracking approaches suffer from occlusion, varying illumination conditions, and deformations of the target object which can cause large difference between the current observation and the appearance model.The Rao-Blackwellized particle filter (RBPF) [5] has been introduced to estimate high-dimensional joint distributions for tracking problems [6–8]. The Markov Chain Monte Carlo (MCMC) sampling method [9,10] and the variational particle filter [11] have been adopted to effectively handle the high dimensional tracking problem. The mean-shift algorithm has been another very successful technique for real-time tracking of non-rigid objects, as introduced in [12,13]. Han et al. proposed a new sequential kernel-based Bayesian filtering framework that can recursively approximate a mixture of Gaussians for the visual tracking problem [14].Visual tracking is inherently challenging due to the variability of the target appearance. To mitigate this problem, online learning algorithms have been used to update the appearance model during visual tracking. For example, Ross et al. [15] presented an online algorithm that incrementally learns and adapts a low dimensional eigenspace representation to reflect appearance changes of the target object. Zhou et al. [16] proposed a tracking approach that incorporates adaptive appearance models in a particle-filter framework. Babenko et al. [17] addressed the problem of learning an adaptive appearance model by training a discriminative classifier online. Recently, Kwon and Lee [18] proposed a tracking approach using a visual tracking decomposition scheme in order to reliably track objects in challenging conditions with large illumination and appearance changes.Our approach estimates the joint distribution of the target state and the appearance model. In general, estimating this joint distribution using a particle filter would be infeasible due to the high dimensionality. To resolve this problem, we present a fast and robust tracking algorithm based on a Rao-Blackwellized particle filter [5]. This algorithm achieves a substantial computational gain as one only has to sample some of variables and apply closed-form filtering to lower dimensional sub-networks for the rest of variables. Unlike [6], which combines RBPF and Kalman filtering while assuming a single Gaussian for the appearance of the target, we propose an adaptive appearance model represented by a Gaussian mixture model. Even though Gaussian distribution has some important analytical properties, a single Gaussian distribution is limited in modeling real data. For example, tracking is prone to failure in the subsequent frames if a single Gaussian distribution is used to model occlusion or drifting.Previous approaches [15,16] update a single appearance model using new observations that correspond to the best target state. In contrast, our approach estimates multiple appearance models allocating to each particle its appearance model. Because of this, our approach is more robust to corrupted observations from tracking errors or drifting problems.In this paper, we address Rao-Blackwellized particle filtering in which the appearance models are represented by Gaussian mixture models. The target state is estimated using particles whose weights are computed by marginalizing out our appearance models. The appearance models are incrementally updated through an online Expectation–Maximization (EM) algorithm. In addition, the appearance models for spatially divided sub-regions are individually estimated in the RBPF framework in order to achieve robustness to partial occlusion.The Rao-Blackwellized particle filter is an effective approach for the state-estimation problem in high-dimensional spaces. We decompose the problem of estimating the joint distribution into the problem of estimating the target state,xt, and the problem of estimating the appearance model,atconditioned on the given target state. This decomposition can be expressed as(1)pxt,at|z1:t=pxt|z1:tpat|xt,z1:t,wherez1:tare all observations until time t.In the Rao-Blackwellized particle filter, the distribution of the target state is estimated by marginalizing out the appearance model as(2)pxt|z1:t=∫atpxt,at|z1:tdat=κ∫atpzt|xt,atpxt,at|z1:t-1dat,whereκis a normalizing constant.Under the Markov assumption, Eq. (2) can be rewritten as(3)pxt|z1:t=κ∫atpzt|xt,at∫xt-1∫at-1pxt,at,xt-1,at-1|z1:t-1dat-1dxt-1dat.Finally, the distribution of the target state can be decomposed into the likelihood distribution, the predictive distribution for the appearance model, and the predictive distribution for the target state as(4)pxt|z1:t=κ∫atpzt|xt,at︸likelihood∫at-1pat|at-1pat-1|xt-1,z1:t-1dat-1︸predictivedistributionforappearancemodel·∫xt-1pxt|xt-1pxt-1|z1:t-1dxt-1︸predictivedistributionfortargetstatedat.In our approach, the distribution of the target state,p(xt|z1:t), is represented as a set of weighted particles and the distribution of the appearance model,p(at|xt,z1:t), is represented by a Gaussian mixture model. Theith particle consists of one target state,xt[i], including scale and position parameters, and its corresponding appearance model,at[i], containing the 128-dimensional orientation histogram [19] representing the target appearance. We estimate the distributionspxt[i]|z1:tandpat[i]|xt[i],z1:tfor all particles individually.Suppose that the prior distribution,p(xt-1|z1:t-1), at time t is available. Then the predictive distribution is determined by the motion model,p(xt|xt-1), which evolves the target candidates between time steps. Our approach adopts a random walk model that is based on a uniformly distributed density of the previous target state,xt-1, as(5)pxt|xt-1∼Uxt-1-b+vt,xt-1+b+vt,vt=(1-αp)vt-1+αp(xt-1-xt-2),whereαpis a constant learning rate. The uncertainty for the incremental change is given byb, andvtis the velocity of the target object, which is determined by position and scale differences between time steps.We also use the histogram of oriented gradients (HOG) [19] for the observations,zt, because they are robust to scale and illumination changes. In addition, we construct the integral images [20] to reduce the computational cost of producing histograms. To further improve computational efficiency, we construct the integral images from the minimum region which contains all the predicted particles generated by Eq. (5) instead of the entire image.In the case where a target object is partially occluded, the orientation histogram is very different from the appearance model because the entirely normalized histogram is computed using all the pixels in the image window. Thus, small occlusion may cause a large change in the entire orientation histograms. To mitigate this problem, we compute histograms for uniformly divided sub-regions and individually normalize them. Fig. 1shows an example tracking results demonstrating that estimating the observations and appearance models by dividing an image window into sub-regions is robust to partial occlusion. In our implementation, appearance models for B spatially divided sub-regions are independently estimated to ensure robustness to occlusion, as shown in Fig. 2. A similar idea has been proposed in [21] to handle partial occlusion and pose changes by representing the template object by multiple image patches.More formally, the appearance model,at, is composed of B independent appearance models for B sub-regions, i.e.,at=at1;at2;…;atB, and each of them is modeled by a mixture of G Gaussians as(6)pat|xt,z1:t=∏k=1Bpatk|xt,z1:t=∏k=1B∑j=1Gwtj,kηatk;μtj,k,Σtj,k,ηa;μ,Σ=1(2π)n2|Σ|12exp-12(a-μ)TΣ-1(a-μ),whereμtj,kandΣtj,k=σtj,k2Iare the mean and the covariance corresponding tojth Gaussian for thekth sub-region.Idenotes the identity matrix.For each appearance model,B×GGaussians are needed and the dimension of the variable for each Gaussian is reduced by a factor of B. Thus there is no additional computation time and memory usage compared with the case the window is not divided into sub-regions.The prior distribution,p(at-1|xt-1,z1:t-1), is given as(7)pat-1|xt-1,z1:t-1=∏k=1B∑j=1Gwt-1j,kηat-1k;μt-1j,k,Σt-1j,k.Using this, the predictive distribution can be determined by(8)pat|at-1pat-1|xt-1,z1:t-1=∏k=1B∑j=1Gwt-1j,kpatk|at-1kηat-1k;μt-1j,k,Σt-1j,k=∏k=1B∑j=1Gw̃tj,kηatk|μ̃tj,k,Σ∼tj,k.If we assume thatpatk|at-1kis a zero-mean Gaussian distribution with the covariance,σp2I, then the Gaussian components for the predictive distribution are determined by(9)w̃tj,k=wt-1j,k,μ̃tj,k=μt-1j,k,Σ∼tj,k=Σt-1j,k+σp2I.We update the posterior distribution of the appearance model,p(at|xt,z1:t), using the online EM algorithm [22] to allow the model to adapt to deformation and changes in illumination when new observations are obtained. It is based upon selective updating. For this purpose, we compare the current observations for each sub region,ztk, with the G Gaussian distributions that are represented byμt1,k,Σt2,k,…,μtG,k,ΣtG,k, as shown in Fig. 2, and we select thenth andlth Gaussian distributions corresponding to the most and the least probable distributions with the current observations based on Eq. (10).(10)n=argmaxcηztk;μt-1c,k,Σt-1c,k,l=argmincηztk;μt-1c,k,Σt-1c,k.If none of the G distributions matches the current observations, the least probable distribution is replaced with a new Gaussian distribution whose mean value is the current histogram,ztk, with a pre-defined initial variance. A new Gaussian distribution is added with a small weight. Otherwise, we increase the weight for thenth Gaussian distribution that matches the new observations, and the parametersμandσof thisnth Gaussian are updated by(11)wtn,k=(1-αm)wt-1n,k+αm,μtn,k=1-ρn,kμt-1n,k+ρn,kztk,σtn,k2=(1-ρn,k)σt-1n,k2+ρn,kztk-μtn,kTztk-μtn,k,whereρn,k=αmηztk;μt-1n,k,Σt-1n,kandαmis a constant learning rate. After this approximation, the weights are renormalized.In our approach, the initial Gaussian parameters, the mean, and the covariance, which are determined from the first image, are fixed to prevent the drifting problem, i.e., if the first Gaussian distribution is selected as the most probable Gaussian with the observation, then we do not update the first Gaussian parameters except the weight, and we select the least probable distribution except the first Gaussian. If the tracker loses the target object, it is likely to trigger the creation of a new Gaussian with a small weight; consequently the weight of the first Gaussian component increases. Thus, our tracker relies more on the first Gaussian automatically in the subsequent frames as it consistently loses the target object.After updating the distributionp(at|xt,z1:t)using the online EM algorithm, we calculate the appearance modelaˆtkthat corresponds to the maximum a posteriori solution as(12)aˆtk=arg maxatkpatk|xt,z1:t.We computeaˆtkusing the mean-shift algorithm [23], given by(13)aˆn+1k=∑j=1Gwtj,kμtj,kηaˆnk;μtj,k,Σtj,k∑j=1Gwtj,kηaˆnk;μtj,k,Σtj,k,where n is the number of iterations.Iterations are terminated when∑j=1Gwtj,kη(aˆn+1k|μtj,k,Σtj,k)⩽∑j=1Gwtj,kη(aˆnk|μtj,k,Σtj,k), or when it reaches the maximum iterations value.Our likelihood distribution is assumed to be a Gaussian whose mean is the appearance modelaˆt=[aˆt1;…;aˆtB]calculated by Eq. (12) and covariance isσo2I. Finally, the likelihood distribution is determined by(14)pzt|xt,at=∏k=1Bηzt;aˆtk,σo2I.Since exact evaluation of the posterior distribution in Eq. (4) is analytically intractable due to the complexity of the posterior, we need to resort to approximate inference. To effectively calculate the weight of theith particle, we employ the Laplace approximation [24] as(15)wt[i]=pxt[i]|z1:t≃κpzt|xt[i],aˆt[i]paˆt[i]|at-1[i∗]pat-1[i∗]|xt-1[i∗],z1:t-1,wherei∗is the index of the particle at timet-1, corresponding to theith particle at time t, which is determined by re-sampling [2].The weight of the particle is determined by two probabilities: one for the likelihood distribution in Eq. (14), and the other for the probability ofaˆtwith respect to the predictive distribution of the appearance model in Eq. (8). The probability ofatˆwith respect to the predictive distribution becomes lower if the appearance model is updated by observations not corresponding to the target object, and it becomes higher otherwise.The overall procedure can be summarized as Algorithm 1.Algorithm 1Rao-Blackwellized particle filtering with Gaussian mixture modelsInput:xt-1[i],wt-1[i],pat-1[i]|xt-1[i],z1:t-1︸represented by GMMsi=1,…,N•FORi=1,2,…,N–Determinei∗for i by re-sampling.–Drawxt[i]∼pxt|xt-1[i∗]by Eq. (5).–Estimatepat[i]|at-1[i∗]pat-1[i∗]|xt-1[i∗],z1:t-1by Eq. (8).–Calculate the HOG feature forxt[i].–Updatep(at[i]|xt[i],z1:t)by Eq. (11).–Calculateaˆt[i]by Eq. (12).–Calculate the weight of the particlewt[i]by Eq. (15).•END FOR•FORi=1,2,…,N–Normalizewt[i]=wt[i]∑jwt[j]•END FOROutput:xt[i],wt[i],pat[i]|xt[i],z1:t︸represented by GMMsi=1,…,N

@&#CONCLUSIONS@&#
