@&#MAIN-TITLE@&#
Algorithms for computing strategies in two-player simultaneous move games

@&#HIGHLIGHTS@&#
We present algorithms for computing strategies in zero-sum simultaneous move games.The algorithms include exact algorithms and Monte Carlo sampling algorithms.We compare the algorithms in the offline computation and the online game-playing.Novel exact algorithm dominates in the offline equilibrium strategy computation.Novel sampling algorithms can guarantee convergence to optimal strategies.

@&#KEYPHRASES@&#
Simultaneous move games,Markov games,Backward induction,Monte Carlo Tree Search,Alpha-beta pruning,Double-oracle algorithm,Regret matching,Counterfactual regret minimization,Game playing,Nash equilibrium,

@&#ABSTRACT@&#
Simultaneous move games model discrete, multistage interactions where at each stage players simultaneously choose their actions. At each stage, a player does not know what action the other player will take, but otherwise knows the full state of the game. This formalism has been used to express games in general game playing and can also model many discrete approximations of real-world scenarios. In this paper, we describe both novel and existing algorithms that compute strategies for the class of two-player zero-sum simultaneous move games. The algorithms include exact backward induction methods with efficient pruning, as well as Monte Carlo sampling algorithms. We evaluate the algorithms in two different settings: the offline case, where computational resources are abundant and closely approximating the optimal strategy is a priority, and the online search case, where computational resources are limited and acting quickly is necessary. We perform a thorough experimental evaluation on six substantially different games for both settings. For the exact algorithms, the results show that our pruning techniques for backward induction dramatically improve the computation time required by the previous exact algorithms. For the sampling algorithms, the results provide unique insights into their performance and identify favorable settings and domains for different sampling algorithms.

@&#INTRODUCTION@&#
Strategic decision-making in multiagent environments is an important problem in artificial intelligence. With the growing number of agents interacting with humans and with each other, the need to understand these strategic interactions at a fundamental level is becoming increasingly important. Today, agent interactions occur in many diverse situations, such as e-commerce, social networking, and general-purpose robotics, each of which creates complex problems that arise from conflicting agent preferences.Much research has been devoted to developing algorithms that reason about or learn in sequential (multi-step) interactions. As an example, adversarial search has been a central topic of artificial intelligence since the inception of the field itself, leading to very strong rational behaviors in Chess [1] and Checkers [2]. Advances in machine learning for multi-step interactions (e.g., reinforcement learning) have led to self-play learning of evaluation functions achieving master level play in Backgammon [3] and super-human level in Atari [4].The most common model for these multistage environments is one with strictly sequential interactions. This model is sufficient in many settings [5], such as in the examples used above. On the other hand, it is not a good representation of the environment when agents are allowed to act simultaneously. These situations occur in many real-world scenarios such as auctions (e.g., [6]), autonomous driving, and many video and board games in the expanding gaming industry (e.g., [7,8], including games we use for our experiments). In all of these scenarios, the simultaneity of the decision-making is crucial and we have to include it directly into the model when computing strategies. One of the fundamental differences of simultaneous move games versus strictly sequential games is that the agents may need to use randomized (or mixed) strategies in order to play optimally [9], i.e., to maximize their worst-case expected utility. This means that agents may need to randomize over several actions in some states of the game to guarantee the worst-case expected utility, even though the only information that is hidden is each player's action as they play it.This paper focuses specifically on algorithms for decision-making in simultaneous move games. We cover the offline case, where the computation time is abundant and the optimal strategies are computed and stored, as well as the online case, where the computation time is limited and agents must choose an action quickly. We are concerned both with the quality of strategies based on their worst-case expected performance in theory and their observed performance in practice. We compare and contrast the algorithms and parameter choices in the offline and the online cases, and thoroughly evaluate each algorithm on a suite of games. Our collection covers Biased Rock–Paper–Scissors, Goofspiel, Oshi-Zumo, Pursuit–Evasion Games, and Tron, all of which have been used for benchmark purposes in previous work. We also perform experiments on randomly generated games. These games differ in the number of possible actions, the number of moves before the game ends, the variance of the utility values, and the proportion of states in which mixed strategies are required for optimal play.Our experimental comparison shows that the algorithms perform differently in each case. The exact algorithms based on the backward induction are significantly better in the offline setting, where they are able to find the optimal strategy very quickly compared to the sampling algorithms. In some cases, our novel algorithm (DOαβ) solves the game in less than 2% of the time required by the standard backward induction algorithm. However, the exact algorithms are less competitive in the online setting. In contrast, the approximative sampling algorithms can perform very well in the online setting and find good strategies to play within a few seconds, however, they are not well-suited for offline solving of games.The paper is structured as follows. First, we make explicit the contributions of the paper in Subsection 1.1. In Section 2, we present a formal introduction of the simultaneous move games that we will use throughout the paper. Section 3 follows with a list and discussion of the existing algorithms in the related work. In Section 4, we describe in detail selected exact and approximative algorithms. We first describe the algorithms in the offline setting, followed by the necessary modifications used in the online case described in Section 5. In Section 6, we present our experimental results comparing the algorithms. Finally, we conclude in Section 7.This paper presents detailed descriptions and analysis of recent state-of-the-art exact [10] and approximative algorithms [11–13] that compute strategies for the class of two-player simultaneous move games. Furthermore, it presents the following original contributions:•We present the latest variants of state-of-the-art algorithms under a single unified framework and combine the offline and online computation perspectives that have been previously analyzed separately.We describe the first adaptation of backward induction and the double-oracle algorithm with serialized bounds (DOαβ) [10] to the online search setting in simultaneous move games using iterative deepening and heuristic evaluation functions.We describe a novel variant of Online Outcome Sampling [13] tailored for two-player simultaneous move games (SM-OOS) and provide its formal analysis.We provide a wide experimental analysis and a comparison of these and other algorithms on five different specific games and on randomly generated games.We replicate an experimental convergence analysis for approximative algorithms that is often used in the literature as a demonstration that sampling-based algorithms are not guaranteed to converge to an optimal solution [14], and we identify the sensitivity of the existing approximative algorithms to tie-breaking rules.A finite two-player game with simultaneous moves and chance events (also called Markov games, or stacked matrix games) is a tuple(N,S,A,T,Δ⋆,ui,s0), whereS=D∪C∪Z. The player setN={1,2,⋆}contains player labels, where ⋆ denotes the chance player, and by convention a player is denotedi∈N.Sis a set of states, withZdenoting the terminal states,Dthe states where players make decisions, andCthe possibly empty set of states where chance events occur.A=A1×A2is the set of joint actions of individual players. We denote byAi(s)the actions available to player i in states∈S. The number of actions available to player i,|Ai(s)|, is called the branching factor for player i. When the player is not specified, we mean the joint branching factor|A(s)|. The transition functionT:S×A1×A2↦Sis a partial function that defines the successor state given a current state and actions for both players.Δ⋆:C↦Δ(S)describes a probability distribution over possible successor states of the chance event. Induced byΔ⋆, we also defineP⋆(s,r,c,s′)as the probability of transitioning tos′after choosing joint action(r,c)from s, or simply 1 whenT(s,r,c)∉C. The utility functionui:Z↦[vmin,vmax]⊆Rgives the utility of player i, withvminandvmaxdenoting the minimum and maximum possible utility respectively. We assume zero-sum games:∀z∈Z,u1(z)=−u2(z). The game begins in an initial states0and a subset of a game that starts in some node s is called a subgame. An example of such a game is depicted in Fig. 1, more examples can be found in [15, Chapter 5].In two-player zero-sum games, a (subgame perfect) Nash equilibrium strategy is often considered to be optimal (the formal definition follows). It guarantees an expected payoff of at least V against any opponent. Any non-equilibrium strategy has its nemesis, which makes it gain less than V in expectation. Moreover, a subgame perfect Nash equilibrium strategy can earn more than V against weak opponents. After the opponent makes a sub-optimal move, the strategy will never allow it to gain the loss back. The value V is known as the value of the game and it is the same for every equilibrium strategy profile by von Neumann's minimax theorem [16].A matrix game is a single step simultaneous move game with action setsA1andA2(see Fig. 2). Each entry in the matrixArcwhere(r,c)∈A1×A2corresponds to a utility value reached if row r is chosen by player 1 and column c by player 2. For example, in Matching Pennies in the left side of Fig. 2, each player has two actions (heads or tails). The row player receives a payoff of 1 if both players choose the same action and 0 if they do not match. In simultaneous move games, at every decision states∈Dthere is a joint action setA1(s)×A2(s). Each joint action(r,c)leads to another stateT(s,r,c)that is either a terminal state or a subgame which is itself another simultaneous move game. A chance event is a states∈Cwith a fixed set of outcomes, each of which leads to a possible successor state. In simultaneous move games,Arcrefers to the value of the subgame rooted in stateT(s,r,c).A behavioral strategy for player i is a mapping from statess∈Sto a probability distribution over the actionsAi(s), denotedσi(s). We denote byσi(s,a)the probability that strategyσiassigns to a in s. These strategies are often called randomized, or mixed because they represent a mixture over pure strategies, each of which is a point in the Cartesian product space∏s∈SAi(s).22Notice that a pure strategy is also a mixed strategy that assigns probability 1 to a single pure strategy and probability 0 to every other pure strategy. However, as it is common in the literature, we sometimes refer to a mixed strategy to specifically mean not a pure strategy. This is mostly clear from the context, but we clarify where necessary.LetHbe a global set of histories (sequences of actions from the start of the game). Given a strategy profileσ=(σ1,σ2), we define the probability of reaching a history h under σ asπσ(h)=π1σ(h)π2σ(h)π⋆σ(h), where eachπiσ(h)is a product of probabilities of the actions taken by player i along the path to h (π⋆being chance's probabilities). Finally, we defineΣito be the set of all behavioral strategies for player i. We adopt a standard convention that the index −i refers to the opponent of player i.In order to define optimal behavior for this class of games, we now provide definitions of some fundamental concepts.Definition 2.1Strictly dominated actionIn a matrix game, an actionai∈Aiis strictly dominated if∀a−i∈A−i,∃ai′∈Ai∖{ai}:ui(ai,a−i)<ui(ai′,a−i).No rational player would want to play a strictly dominated action, because there is always a better action to play independent of the opponent's action. The concept also extends naturally to behavioral strategies. For example, in the game on the right of Fig. 2, both b and B are strictly dominated. In this paper we refer to the dominance always in this strict sense.Definition 2.2Best responseSupposeσ−i∈Σ−iis a fixed strategy of player −i. Define the set of best response strategiesBRi(σ−i)={σi|ui(σi,σ−i)=maxσi′∈Σi⁡ui(σi′,σ−i)}. A single strategy in this set, e.g.,σi∈BRi(σ−i), is called a best response strategy toσ−i.Note that a best response can be a mixed strategy, but a pure best response always exists [9] and it is often easier to compute.Definition 2.3Nash equilibriumA strategy profile(σi,σ−i)is a Nash equilibrium profile if and only ifσi∈BRi(σ−i)andσ−i∈BR−i(σi).In other words, in a Nash equilibrium profile each strategy is a best response to the opponent's strategy. In two-player zero-sum games, the set of Nash equilibria corresponds to the set of minimax-optimal strategies. That is, a Nash equilibrium profile is also a pair of behavioral strategies optimizing(1)V=maxσ1∈Σ1⁡minσ2∈Σ2⁡Ez∼σ[u1(z)]=maxσ1∈Σ1⁡minσ2∈Σ2⁡∑z∈Zπσ(z)u1(z).None of the players can improve their utility by deviating unilaterally. For example, the game of Rock, Paper, Scissors (depicted in Fig. 3) modeled as a matrix game has a single state and the only equilibrium strategy is to mix equally between all actions, i.e., both players play with a mixed strategyσi=σ−i=(1/3,1/3,1/3)giving the expected payoff ofV=0. Note that using a mixed strategy is necessary in this game to achieve the guaranteed payoff of V. Any pure strategy of one player can be exploited by the opponent; so while a pure best response to a fixed strategy always exists, it is not always possible to find a Nash equilibrium for which both strategies are pure. For the same reason, randomized strategies are often necessary also in the multi-step simultaneous move games. If the strategies also optimize Equation (1) in every subgame, the equilibrium strategy is termed subgame perfect.Finally, a two-player simultaneous move game is a specific type of two-player extensive-form game with imperfect information. In imperfect information games, states are grouped into information sets: two statess,s′are in an information set I if the player to act at I cannot distinguish whether she is in s ors′. Any simultaneous move game can be modeled using information sets to represent half-completed transitions, i.e.,T(s,a1,?)orT(s,?,a2). The matrix game of Rock, Paper, Scissors can also be thought of as a two-step process where the first player commits to a choice, writing it on a face-down piece of paper, and then the second player responds. Fig. 3 shows this transformation, which can generally be applied to every state in a simultaneous move game. Therefore, algorithms intended for two-player zero-sum imperfect information games may also be applied to the simultaneous move game using this equivalent form.

@&#CONCLUSIONS@&#
