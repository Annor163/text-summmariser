@&#MAIN-TITLE@&#
Dynamic texture recognition with video set based collaborative representation

@&#HIGHLIGHTS@&#
The weighted volume local binary pattern descriptor is formed to represent dynamic texture.The weights are learned with a regularized collaborative representation model.The representation residual is used for dynamic texture classification.

@&#KEYPHRASES@&#
Dynamic texture classification,Local binary pattern,Texture feature extraction,Collaborative representation,

@&#ABSTRACT@&#
Efficient feature description and classification of dynamic texture (DT) is an important problem in computer vision and pattern recognition. Recently, the local binary pattern (LBP) based dynamic texture descriptor has been proposed to classify DTs by extending the LBP operator used in static texture analysis to the temporal domain. However, the extended LBP operator cannot characterize the intrinsic motion of dynamic texture well. In this paper, we propose a novel video set based collaborative representation dynamic texture classification method. First, we divide the dynamic texture sequence into subsequences along the temporal axis to form the video set. For each DT, we extract the video set based LBP histogram to describe it. We then propose a regularized collaborative representation model to code the LBP histograms of the query video sets over the LBP histograms of the training video sets. Finally, with the coding coefficients, the distance between the query video set and the training video sets can be calculated for classification. Experimental results on the benchmark dynamic texture datasets demonstrate that the proposed method can yield good performance in terms of both classification accuracy and efficiency.

@&#INTRODUCTION@&#
Dynamic textures (DT) are textures with motion [1]. They are video sequences of moving scenes, which vary not only on the spatial distributions of intensity, but also on the dynamics over time. There are such video sequences in the real word, for example, the sequences of forest fire, waterfall, swarm of birds and humans in crowds, etc. In recent years, the study of DT has been receiving considerable attention, including DT modeling, synthesis, segmentation and classification. Effective feature extraction is a key step for DT classification. It is desirable that effective DT feature can characterize texture appearance and motion well. Also, the extracted DT feature can also be applied to video understanding. For example, the DT feature can be used to characterize appearance and motion of human for human action recognition [2–5].Different from static textures, dynamic textures exhibit certain stationary properties in the temporal domain. One challenging problem in DT classification is how to discriminatively describe DT for classification, i.e., texture appearances in the spatial domain and dynamics in the temporal domain. Existing approaches characterize DT by either patterns of motion field in DT, or the linear dynamic system or the combination of texture appearance features and motion features. The patterns of motion field [6,7] of DTs are discriminative and thus very useful for DT classification. In [8], the normal flow features are combined with periodicity features for DT, attempting to explicitly characterize motion magnitude, directionality and periodicity. Based on the velocity and acceleration fields of DT, Lu et al. [9] constructed the spatio-temporal multi-resolution histograms of the velocity and acceleration fields for DT classification, where image sequences at different spatio-temporal scales are estimated with the structure tensor method. In [10], the normal flow features and regularized complete flow features are compared, and it is concluded that normal flow contains information on both shape and dynamics of DT.Doretto et al. [11] showed that the spatial appearance and dynamics of DT can be modeled by a linear dynamic system (LDS). Thus, DT sequences can be discriminated with different dynamic parameters of the LDS. Based on the LDS representation, several methods have been proposed, where different distances between the model parameters of two LDSs are defined. In [12], the Binet–Cauchy kernel is used to compare the parameters of the two LDSs. Chan and Vasconselos [13] employed kernel PCA to learn a non-linear kernel dynamic texture and used the Martin distance to measure the similarity between the kernel dynamic textures for DT classification. However, these methods cannot deal with DT sequences with viewpoint changes. In order to handle viewpoint changes of DTs, the dictionary of the parameters of the LDS [14–16] is learned with K-means clustering and the bag-of-feature representation is used for DT classification.Since dynamic texture can be viewed as texture with motion, researchers combined the texture appearance features and the motion features to characterize DT. Some local operator based static texture feature extraction methods [17–21], therefore, are extended to DT. In [19,20], the fractal dimension of the 2D slice of the 3D spatio-temporal volume was proposed to characterize the self-similarities of DTs for classification. In [21–23], the distributions of 3D Gaussian third derivative filter are used to characterize the dynamic structure of DT. The volume LBP (VLBP) [24,25] is an extension of the LBP descriptor [17,26,27] widely used in static texture analysis by combining texture appearance and motion. The VLBP descriptor compares each pixel with its neighborhoods in the previous, current and posterior frames to encode each pixel as a sequence of binary codes. Then, three sequences of binary codes from the three frames are concatenated to form the VLBP descriptor to describe the local structure in DT. When the number of neighborhood points increases, the number of VLBP becomes very large. To make the VLBP computationally simple, LBP histograms from three orthogonal planes (LBP_TOP) [25], i.e., the XY plane, XT plane and YT plane, are extracted to characterize DT. Then the LBP histograms from the three planes are concatenated to form the LBP_TOP descriptor.Inspired by the success of collaborative representation in face recognition [28,29], in this paper we develop a novel collaborative representation based DT classification method. Our proposed method is based on the LBP based DT descriptor. In the proposed method, in order to extract robust features from DT with complex motions, we first model the DT sequence by the video set, which is formed by dividing the sequence into subsequences in the temporal domain. And each DT sequence can be represented by the video set based LBP descriptors. Considering the similarities between motion patterns of different DTs, we then propose a regularized collaborative representation model to represent the query DT sequence over all training DT sequences with the video set based descriptor. Finally, we classify the query DT sequence by the minimal representation residual to the training DT sequences. Experimental evaluations on the benchmark dynamic texture datasets demonstrate the effectiveness of the proposed method. The main difference from the previous studies lies in that we model DT with the video set and use the video sets to collaboratively represent DT in order to capture the complex motion.The rest of the paper is organized as follows. Section 2 presents in detail the proposed collaborative representation based DT classification scheme. Section 3 conducts experiments on the benchmark dynamic texture datasets and Section 4 concludes the paper.

@&#CONCLUSIONS@&#
We proposed a video set based collaborative representation method for dynamic texture classification, which is simple to implement and demonstrates promising performance. Each DT sequence was divided into subsequences to form the video set. The video set based DT descriptor with VLBP/LBP_TOP was extracted to represent the DT. Then, a collaborative representation model was proposed to use DT subsequences from all classes to represent the query DT sequence to ensure representation accuracy while reducing the intra-class variance. Finally, we used the representation residual associated with each class of training subsequences to classify DT. The proposed VSCR method was validated on two benchmark DT datasets: UCLA and DynTex. The experimental results showed that our proposed method can achieve good performance.