@&#MAIN-TITLE@&#
A multi-modal perception based assistive robotic system for the elderly

@&#HIGHLIGHTS@&#
We present a complete multi-modal perception driven non-intrusive domestic robotic system for the elderly.We present a novel multi-modal user’s intention-for-interaction detection modality.A fusion method to improve the speech recognition given the user’s position, available sensors, and recognition tools is presented.We present details of the complete implemented system along with relevant evaluations that demonstrate the soundness of the framework via an exemplar application whereby the robot helps the user find hidden or misplaced objects in his/her living place.The proposed framework is further investigated by conducting relevant user studies involving 17 elderly participants.

@&#KEYPHRASES@&#
Assistive technology,Elderly care,Intention detection,Multi-modal data fusion,Human–robot interaction,Robotic perception,

@&#ABSTRACT@&#
In this paper, we present a multi-modal perception based framework to realize a non-intrusive domestic assistive robotic system. It is non-intrusive in that it only starts interaction with a user when it detects the user’s intention to do so. All the robot’s actions are based on multi-modal perceptions which include user detection based on RGB-D data, user’s intention-for-interaction detection with RGB-D and audio data, and communication via user distance mediated speech recognition. The utilization of multi-modal cues in different parts of the robotic activity paves the way to successful robotic runs (94% success rate). Each presented perceptual component is systematically evaluated using appropriate dataset and evaluation metrics. Finally the complete system is fully integrated on the PR2 robotic platform and validated through system sanity check runs and user studies with the help of 17 volunteer elderly participants.

@&#INTRODUCTION@&#
As living conditions and health care facilities improve, the average life expectancy increases leading to a growing elderly population. For example, in France, in 2005, there were five young and adult people for one senior, in 2050, it is expected that there will be 10 young and adult people for every seven seniors [1]. Though most people age well, some become frail, at risk of disease and costly dependence. Therefore, the financial and organizational burden on the society is likely to rise. How can we provide quality care to people requiring constant assistance in various aspects, including those suffering from deterioration in cognitive capabilities due to aging, head trauma, and Alzheimer’s disease (AD)? This enlightenment has led to a growing necessity for new technologies that can assist the elderly in their daily living. One such technology is the deployment of assistive robots for the elderly. In fact, such kind of robotic systems serving various tasks and purposes in the social care and medical/health sectors beyond the traditional scope of surgical and rehabilitation robots are poised to become one of the most important technological innovations of the 21st century [2]. But, when robots leave industrial mass production to help with daily living activities, i.e., household chores, the requirements for robot platforms change. While industrial production requires strength, precision, speed, and endurance, domestic service requires a robust navigation in indoor environments, a dexterous object manipulation, and an intuitive way of communicating (speech, gestures, and body language) with users. In this perspective, many issues are still to be solved, such as perception and system integration.Making a robot a socially competent service provider in all the daily life areas is very challenging. Hence, we focus on the conception of a robotic system that provides mild memory assistance to the elderly, a requirement highly coveted for the elderly whom might exhibit mild cognitive impairment (MCI) or Alzheimer’s disease (AD) [3,4]. A serious issue for the elderly with MCI is forgetting where they have put objects that they use everyday, for example, keys, remote control, glasses, etc. This leads them to experience stress, loss of confidence, and become irritable, putting them at health risk especially considering their frailty [4]. Consequently, we consider deploying a robotic system that helps the elderly with MCI in locating everyday objects which are hidden (out of the user’s sight), or put in unusual places. The work presented in this paper is part of a French National Research Agency (ANR) funded research project called RIDDLE,11http://projects.laas.fr/riddle/an acronym for Robots for perceptual Interactions Dedicated to Daily Life Environment, which aims to make a step forward in these directions by combining the underlying multiple and uncertain perceptual analyses related to, (1) objects and space regarding the robot’s spatial intelligence, and (2) multi-modal communication regarding the robot’s transactional intelligence.To paint a clear picture, let us consider the following exemplar scenario. A person suffering from MCI, which we henceforth refer as the user, is carrying his/her normal everyday activity. Then, let us say the user wants to change the channel on the TV but realizes he/she could not remember where the remote control is. The user will then have to pose the question to the robot which is monitoring him/her stowed away in a non-interfering position. The robot will then answer the user’s questions/riddles about the object utilizing appropriate actions (speech, displacement, pointing action). Based on this scenario, we identify three main key functional requirements for the robot: (1) detecting the user at all times, (2) detecting when the user wants to interact with the robot, i.e., when the user wants to pose a question to the robot or needs its attention—called user’s intention-for-interaction, and (3) interaction via speech based communication. In this work, we assume the type and position of the objects are known a priori and we focus only on the highlighted three requirements. The considered objects are specifically eyeglasses, keys, mobile phone, wallet, remote control, and medication—frequently lost objects by the elderly as identified through a pilot study [4].The entire behavior of the proposed assistive system is based on a widely accepted reactive behavior which cycles through “monitor” and “interact” phases, e.g., [5–7]. In this behavior, the robot monitors the user until he/she demands it to do something or shows an interest to interact with it. Then the robot continues through the interaction phase where it interacts with the user to provide requested service or assistance. In line with this, we propose a domestic assistive robot system that incorporates a novel user’s intention detection mechanism to transition from monitoring phase to interaction phase. Therefore, we propose a scenario where the robot comes into a room, checks the presence of a user in this room, and then stows away at an observation place to monitor the user discretely. When the user expresses his/her intent to interact with the robot—either by looking at it, calling it, or a combination of both—the robot approaches the user and starts the close interaction phase. We refer to this as a non-intrusive behavior—the robot is not moving to stalk the user—with three distinct phases: user detection, user monitoring, and interaction phases (see Fig. 1 in Section 3). Initially when the robot correctly detects the user, it goes to the monitoring stage, actively reading the user’s intention. Then, when it detects the user’s intention-for-interaction, it makes a transition into the interaction phase which is carried out via speech modalities. During the speech based communication, depending on the utilized sensor, recognition tool, and human/robot (H/R) situation (distance variations), the communication quality can be affected [8,9]. Whenever possible an adaptive mechanism should be put in place to maximize the chances of having an ideal communication given the available resources.In this work, the robot considers that there is only one user to communicate with. The used language is French; it could be, nevertheless, generalized to any language. The person can freely move in his/her environment. However, when an interaction starts, after the intention-for-interaction step, the person’s position is fixed during the interaction process. The goal of the interaction phase is for the robot to find a lost object upon request. The robot has to indicate the direction of the object by pointing its head towards it and giving some verbal precisions about its location—at the moment no object displacement is managed by the robot. For example: “the remote control is under the table”. The objects are stored in a user-defined semantic map since the focus of the paper is not on object detection. All the algorithms presented in this work are embedded on the PR222PR2 (Personal Robot 2) is a robotic platform developed by Willow Garage: http://www.willowgarage.comrobotic platform using the Robot Operating System (ROS) middleware framework [10]. ROS is a collection of software frameworks for robot software development with a very active community and numerous publicly available packages that provide an operating system-like functionality on a heterogeneous computer cluster. Hence, we base all our implementations and associate robotic integration on ROS (based on C++ and Python programming languages). Furthermore, all essential algorithms are integrated on the robot, while data visualization modules are seamlessly integrated on an external computer without overloading the PR2 system. All sensors are embedded on the robot. The only exception is an Android smartphone, which is located within 2 m from the user (in his/her hand or on a table near him/her), that is used to capture audio signal.This paper makes the following four core contributions:1.A complete multi-modal perception driven non-intrusive domestic robotic system.A novel multi-modal user’s intention-for-interaction detection modality.A fusion method to improve the speech recognition given the user’s position, available sensors, and recognition tools.Details of the complete implemented system along with relevant evaluations that demonstrate the soundness of the framework via an exemplar application. The application is an assistive scenario whereby the robot helps the user find hidden or misplaced objects in his/her living place.The proposed framework is further investigated by conducting relevant user studies involving 17 elderly participants.This paper is organized as follows: Section 2 discusses related work briefly, Section 3 presents an overview of the adopted system. Sections 4–6 describe the user detection, the user’s intention detection, and close HRI (with speech modality) part, respectively. Then, Section 7 explains how the task-level coordination is realized along with relevant implementation details. Experiments and results on PR2 are detailed in Section 8, and finally, the paper finishes with concluding remarks in Section 9.

@&#CONCLUSIONS@&#
In conclusion, a multi-modal perception based architecture for non-intrusive domestic assistive robot has been described. The presented system exhibits non-intrusive characteristics as it only engages in a close HRI phase when the user expresses his/her intent. It relies on a multi-modal user detector, based on RGB-D data, to localize the user in the scene; a multi-modal user’s intention-for-interaction detector, based on RGB-D data and VAD; and various ASR APIs for reliable communication. Each perceptual component has been evaluated separately: a user detector with low MR (24.4% log-average miss rate), a user intention detector with more than 72% TPR, and an ASR with less than 15% best WER (at the preferred configuration). All of these combined led to a non-intrusive robotic system that demonstrated a 94% success rate during experimental runs with 17 elderly volunteers. The user study carried out with these participants also revealed an overall pleasant interaction experience. In addition, the paper also presents relevant implementation details (ROS nodes, and smach based task-level coordinator) that would be pertinent for the scientific community in general. Even though the framework is presented in the context of helping a user find hidden and/or forgotten objects, it is fundamentally generic and can be easily extended to various assistive tasks.In the near future, the presented system will be augmented with multi-modal action recognition modules to pave the way for more natural interactions and assistive contexts. It is also envisaged to deploy and test the overall system on a humanoid robotic system, specifically the new Romeo robot [64] from Aldebaran Inc. Additionally, several possible future prospects and research axes can be considered: (1) Integrating an automated object detection and recognition capability, possibly a vision and RFID based solution to handle small objects; (2) further improving the intention detection module with context information, e.g., audio activity detection to identify when the user is watching TV, cooking, or the like; and (3) endow more navigation capability to the mobile robot to navigate to the location of the asked object and provide improved assistance.