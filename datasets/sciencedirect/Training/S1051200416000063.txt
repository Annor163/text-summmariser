@&#MAIN-TITLE@&#
Detection of number of components in CANDECOMP/PARAFAC models via minimum description length

@&#HIGHLIGHTS@&#
The problem of CANDECOMP/PARAFAC (CP) model order selection is addressed.Computational efficiency is achieved via matricization of a tensor.Eigenvalues associated with the block or multi-mode matricization are exploited.Able to detect rank up to the square root of the product of all dimension lengths.Accuracy comparable to CP-decomposition based tensor rank detectors.

@&#KEYPHRASES@&#
Model order selection,Minimum description length (MDL),Core consistency diagnostic (CORCONDIA),CANDECOMP/PARAFAC,Tensor decomposition,

@&#ABSTRACT@&#
Detecting the number of components of the CANDECOMP/PARAFAC (CP) model, also known as CP model order selection, is an essential task in signal processing and data mining applications. Existing multilinear detection algorithms for handling N-dimensional data, whereN≥3, e.g., the CORe CONsistency DIAgnostic, rely on the CP decomposition which is computationally very expensive. An alternative solution is to rearrange the tensor as a matrix using the unfolding operation and then utilize the eigenvalues of the resultant matrices for CP model order selection. We propose to employ the eigenvalues associated with the unfolding along merged dimensions, namely, the multi-mode eigenvalues, as well as the n-mode eigenvalues for accurate rank detection. These multiple sets of eigenvalues are combined via the information theoretic criterion. By designing a sequential detection scheme starting from the most squared unfolded matrix, the identifiable rank is increased to the square root of the product of all dimension lengths, which renders the detection algorithm to estimate the rank that can exceed any individual dimension length. The conditions under which the proposed multilinear detection algorithm correctly detects the tensor rank are theoretically investigated and its computational efficiency and detection performance are verified.

@&#INTRODUCTION@&#
The CANDECOMP/PARAFAC (CP) (short for CANonical DECOMPosition and PARAllel FACtor analysis, also known as canonical polyadic decomposition) model [1–3] has a variety of applications in psychometrics [1], chemometrics [4,5], sensor array processing [6], telecommunications [7], blind source separation [8,9], neuroscience [3,10–13], data mining [14–16], as well as image recognition and classification [17–19]. In the CP model, a tensor is decomposed into a sum of rank-one component tensors, which are expressed as the outer product of vectors. In practice, each rank-one component corresponds to a natural source or signal. Finding the tensor rank or number of multilinear components in the underlying CP model of noisy tensor observations, also known as multilinear rank detection, is an important research topic.Existing approaches to multilinear rank detection include the DIFFerence in FIT (DIFFIT) [20], Numerical Convex Hull (NumConvHull) [21], CORe CONsistency DIAgnostic (CORCONDIA) [22], Automatic Relevance Determination (ARD) [23], and reconstruction-error-based CP rank selector [25]. These methods rely on the CP decomposition [24] for estimating the factor/loading matrices and diagonal core tensor. The DIFFIT and NumConvHull procedures examine the fitting error (or residual) of each candidate model and select the number of components that corresponds to the maximal curvature of the residual versus number of components curve. On the other hand, the CORCONDIA exploits the deviation of the estimated core tensor from the ideal super-identity core (i.e., core consistency) for each candidate model and determines the correct number of components by looking at the gap between the core consistency of neighboring models. Within a Bayesian framework, the ARD assumes a prior distribution (Gaussian or Laplace) on the entries of the loading matrices, and infers the number of components by solving anℓ2- orℓ1-regularized CP decomposition problem. In [25], the probabilistic upper bound of the reconstruction error of the CP model is derived and used for order selection. These techniques may be able to identify the typical tensor rank [26,27] that exceeds the size of individual dimension of the tensor. However, the CP decomposition algorithms usually rely on an iterative procedure, which may require a large number of iterations to converge and thus is computationally expensive.In matrix case, it is common to use the eigenspectrum for detecting the number of signals. Numerous bilinear or two-dimensional (2-D) rank detectors have been developed in the literature, including the classical information theoretic criterion based methods [28], such as minimum description length (MDL) [29] and Akaike information criterion (AIC) [30], as well as the random matrix theory (RMT) algorithms [31–35]. Note that AIC and MDL are suitable when the number of samples is much larger than the dimension length, while the RMT algorithms are designed for relatively small sample scenarios.However, these methods do not have direct multilinear counterparts that correspond to the multilinear rank detection problem at hand. To handle that, in [36–38], the N-D MDL whereN≥3, is proposed by stacking the measurement tensor into a matrix with the n-mode unfolding operation, and the eigenvalue spectrum obtained from the singular value decomposition (SVD) of the resultant matrix is used for N-D detection. In this paper, we contribute to the matrix-unfolding-based N-D rank detection methods by proposing an improved version of N-D MDL. Our contributions lie in twofold. First, using the generalized unfolding of the observation tensor,(2N−1−1)sets of eigenvalues are obtained and combined for accurate signal number detection. Moreover, by designing a sequential detection scheme, the number of identifiable signals is up to the size of the most squared unfolded matrix and is much larger than that in [36–38]. When the actual tensor rank is smaller than the size of the most squared unfolded matrix, which is the case in most scenarios of practical interests, the proposed N-D MDL is proved to be a consistent signal detector. Here by “consistency” it means that the proposed N-D MDL is able to correctly detect the number of signals when the length of any one of the dimensions tends to infinity. To distinguish the proposed N-D MDL from that developed in [36–38], we refer the former to as generalized N-D MDL.The remainder of the paper is organized as follows. In Section 2, we formulate the multilinear rank detection problem using the CP data model. In Section 3, we provide a brief review of state-of-the-art tensor-based multilinear rank detectors. By exploiting the properties of MDL and generalized matrix unfolding of the measurement tensor, the generalized N-D MDL is developed in Section 4. In Section 5, experiments on synthetic datasets are conducted to evaluate the performance of the proposed multilinear MDL rank detector. Finally, conclusions are drawn in Section 6.Notations: Scalars are denoted as italic letters, column vectors as bold-face lower-case letters, matrices as bold-face capitals, and tensors as bold-face calligraphic letters. The superscripts T, H and † represent transpose, conjugate transpose of a matrix or vector and matrix pseudo-inverse, respectively. The symbolCdenotes the complex number field which includes the real number fieldRas a special case. Thef(n)=O(g(n))meansf(n)/g(n)→casn→+∞, where c is a non-zero constant andf(n)=o(g(n))stands forf(n)/g(n)→0asn→+∞.An Nth-order tensorX∈CI1×I2×⋯×INis a multidimensional matrix whose elements are referenced by N indices:xi1,i2,…,iN∈C,in=1,2,…,In,n=1,2,…,N. Note that N is the order or dimension of the tensor, andInis the size or length of the nth dimension (or mode),n=1,2,…,N. TheIN,Rdenotes a super-identity tensor of sizeR×R×⋯×R, whose elements are equal to one when the indicesi1=i2=⋯=iNand zero elsewhere. The n-mode vectors of a tensorX∈CI1×I2×⋯×INare obtained by varying the nth index within its range (1,2,…,In) and keeping all the other indices fixed. The n-mode unfolding of a tensorX∈CI1×I2×⋯×IN, denoted as[X](n)∈CIn×(I1…In−1In+1…IN), is a matrix collecting all n-mode vectors ofX. The columns in the unfolded matrix are arranged in reverse lexicographical order of their indices(i1,…,in−1,in+1,…,iN)[39]. The n-mode product of a tensorX∈CI1×⋯×In×⋯×INand a matrixU∈CJn×Inalong the nth mode, denoted asX×nU, is a tensor of sizeI1×⋯×Jn×⋯×INobtained by multiplying the n-mode unfolding ofXfrom the left-hand side byU. The operators ∘ and ⊗ stand for the outer product and Kronecker product, respectively. Thevec(⋅)represents vectorization of a matrix or tensor by stacking its columns or 1-mode vectors on top of each other. The‖⋅‖1and‖⋅‖denote respectively theℓ1norm andℓ2norm of a vector, and‖⋅‖Frepresents the Frobenius norm of a matrix or tensor, which is defined as the square root of the sum of the squared magnitudes of its elements.Definition 2.1An Nth-order tensorX∈CI1×I2×⋯×INhas rank one if it equals the outer product of N vectorsa(1),a(2),…,a(N), witha(n)∈CIn×1forn=1,2,…,N, namely,(1)X=a(1)∘a(2)∘⋯∘a(N).In the CP model, the noise-free signalX∈CI1×I2×⋯×INis modeled as a sum of R rank-one tensors:(2)X=∑r=1Rar(1)∘ar(2)∘⋯∘ar(N),where the entries ofXare given by(3)xi1,…,iN=∑r=1Rar(1)(i1)ar(2)(i2)⋯ar(N)(iN),withar(n)=[ar(n)(1),ar(n)(2),…,ar(n)(In)]T,r=1,2,…,R,n=1,2,…,N.The rank of a tensorXis defined as the minimal number of rank-one tensors required to represent it [40–43]. In practice, each rank-one component typically corresponds to a natural source or signal.Defining the loading/factor matrix in the nth mode as(4)A(n)=[a1(n)a2(n)…aR(n)]∈CIn×R,(2) can be rewritten in terms of n-mode products as(5)X=IN,R×1A(1)×2A(2)⋯×NA(N).In practice, the data are contaminated by noise:(6)Y=X+Z,whereZis the noise tensor collecting independent and identically distributed (i.i.d.) zero-mean real or circularly-symmetric complex Gaussian samples with varianceσ2. The noise is assumed uncorrelated with the signals. Given the noisy measurement tensorY, our goal is to estimate the model order, i.e., the number of multilinear signals R.We review four state-of-the-art tensor-based methods for CP model selection: CORCONDIA, DIFFIT, NumConvHull, and ARD. Except for CORCONDIA, all of these rank detectors are generally applicable to the Tucker model [43–47], which can be considered as generalization of the CP model. For CP model selection, they rely on the PARAFAC to work.Given r as a candidate value for the number of components, the r-component CP decomposition ofYgives(7)Y≃IN,r×1Aˆr(1)×2Aˆr(2)⋯×NAˆrN,whereAˆr(n)is the estimate ofA(n),n=1,2,…,N. The CP decomposition can be accomplished by standard algorithms in the literature, e.g., the alternating least squares (ALS) algorithm [1,2]. An excellent review of the CP decomposition algorithms and their empirical performance comparison can be found in [24]. More recently, a semi-algebraic framework for approximating CP decomposition has been proposed in [48,49]. In this work, we use the ALS algorithm for CP decomposition because it presents a good trade-off between computational complexity and solution accuracy among all existing algorithms for CP decomposition [24].For the CORCONDIA, the core consistency (CC), which means the closeness to an ideal core, is used to estimate the number of components [22].From (7) the CP core is estimated as(8)GN,r=Y×1Aˆr(1)†×2Aˆr(2)†⋯×NAˆrN†.The CC in % is defined as [22](9)CC(r)=100(1−‖GN,r−IN,r‖F2r),and it is a measure of the similarity between the estimated core and ideal core. In the absence of noise, if the PARAFAC model is perfectly fitted withr=Rand if all factor matrices{A(n)}n=1Nare of full column rank,11The condition that{A(n)}n=1Nare of full column rank is necessary since otherwise the identity coreIN,rmay not be the unique solution to (7).GN,ris equal toIN,r, orCC(R)=100%.Note that for a one-component CP model the CC is always 100%, because there are no off-superdiagonal elements in a1×1×⋯×1core array. In fact, empirical results show that the CC of fitting anyr<R-component CP model is approximately equal to 100%. However, whenr>R, the CC typically decreases dramatically with the number of components. It follows that the valid model with the highest number of components is the one to choose:(10)RˆCORCONDIA=max⁡rs.t.CC(r)≥η,where0<η<100%is the threshold coefficient. Typically,50%≤η≤90%is used [22].For the DIFFIT and NumConvHull, the number of components is estimated based on the fitting/reconstruction error [20,21]. The fitting/reconstruction error, for a candidate number of components r, is expressed as:(11)e(r)=‖Y−IN,r×1Aˆr(1)×2Aˆr(2)⋯×NAˆr(N)‖F2.Both the DIFFIT and NumConvHull go through a model screening procedure prior to model selection. In the NumConvHull some models are filtered out such that the set of retained models(r,e(r))forms a convex hull, while the DIFFIT excludes some models so that for the remaining models[e(r−1)−e(r)]monotonically decreases as r increases. After model screening, both the DIFFIT and NumConvHull find the optimal r by maximizing the ratio of the difference in fitting error between consecutive models:(12)DIFFIT(r)=e(r−1)−e(r)e(r)−e(r+1),which is the curvature of the{r,e(r)}graph.The ARD [23] is a Bayesian approach that is originally derived for the Tucker model [43–47] selection problem. Note that the ARD also applies for the CP model. In this case, the CP ARD is a simplified version of the Tucker ARD since the core tensor is not updated [23].In CP ARD, the entries of the loading matrices of the CP model are assigned a Gaussian prior22In [23], the Laplace prior is also considered. Here we only introduce the ARD with Gaussian prior.:(13)Pr⁡(A(n))=∏r(αr(n)2π)In/2exp⁡{−αr(n)2‖ar‖2}.Assuming independence of the entries of the factor matrices across different modes, the negative log posterior after removing irrelevant constant terms is(14)−log⁡P=12σ2‖Y−IN×1A(1)×2A(2)⋯×NA(N)‖F2+12∑n=1N∑rαr(n)‖ar‖2−∑n=1N∑rInlog⁡αr(n)+12I1I2…INlog⁡σ2,whereINdenotes an Nth-order super-identity tensor with sizes matching the numbers of components/columns inA(n),n=1,2,…,N.The objective is to find both the number of components R as well as the model parameters{A(n)},{αr(n)}andσ2by minimizing (14), which is essentially anℓ2-regularized CP decomposition problem.Givenαr(n),r=1,2,…,n=1,2…,N, andσ2, like CP-ALS the regularized CP decomposition can be performed iteratively in N alternate steps, namely, estimation ofA(1)with{A(n)|n=2,3,…,N}fixed, estimation ofA(2)with{A(n)|n=1,3,…,N}fixed, until estimation ofA(N)with{A(n)|n=1,2,…,N−1}. In the nth step solvingA(n)can be cast as a ridge regression problem [50] of the form:(15)minA12σ2‖Y−AS‖F2+12∑rαr‖ar‖2,which admits a closed-form solution:(16)Aˆ=YST(SST+σ2diag{α})−1,whereαis a vector containing{αr}as its elements.OnceA(n)forn=1,2…,Nare estimated, the parameters{αr(n)}andσ2can be obtained via maximum a posteriori criterion as(17)αˆr(n)=In‖aˆr‖2,r=1,2,…,n=1,2…,N,(18)σˆ2=1I1I2…IN‖Y−IN×1Aˆ(1)×2Aˆ(2)⋯×NAˆ(N)‖F2.The ARD algorithm starts by guessing an upper bound of the number of componentsRmaxand randomly initializing the fat/wide factor matricesA(n)∈CIn×Rmax,n=1,2…,N. To reduce the computational complexity, the ARD algorithm does not wait until convergence to remove all excess components at once but gradually prunes them as the iteration proceeds. In this way, the column width of the factor matrices shrinks with the iteration, resulting in a lower time complexity of the algorithm. The implementation of the ARD algorithm for CP model selection is described in Algorithm 1. Note that after convergence the output of the ARD algorithm may not satisfyR1=R2=⋯=RN, in which case additional components need to be pruned in some modes so that the numbers of components in all modes are equal tomin⁡(R1,R2,…,RN).The unfolding, also known as matricization, of a tensor is the operation for transforming a tensor into a matrix. For an Nth-order tensorY∈CI1×I2×⋯×IN, its n-mode unfolding is a matrix collecting all n-mode vectors ofY. Fig. 1illustrates the 1-, 2- and 3-mode matrix unfoldings of a 3rd-order tensorY∈C2×2×2.In the n-mode unfolding of a tensor, the columns of the unfolded matrix correspond to only one mode. Next, we introduce the n-mode unfolding where the columns are generated from multiple modes of the tensor [51–54].Definition 4.1Given an Nth-order tensorY∈CI1×I2×⋯×IN, for1≤n1<⋯<nm≤N(1≤m≤N−1), its(n1,…,nm)-mode vectors are obtained by varying the(n1,…,nm)indices and keeping the remaining indices fixed. The(n1,…,nm)-mode unfolding ofY, denoted as[Y](n1,…,nm), is a matrix collecting all(n1,…,nm)-mode vectors. Both the entries in each column and the columns in the unfolded matrix are arranged in reverse lexicographical order of their indices [39].Example 4.1Given a 4th-order tensorY∈C2×2×2×2, its(1,2)-mode,(1,3)-mode and(2,3)-mode unfoldings are shown in Fig. 2.Define a set of indicesS={1,2,…,N}. Note that each division ofSinto two non-empty disjoint subsets{n1,…,nm}andS∖{n1,…,nm}corresponds to a pair of mutually-transposed unfolded matrices ofY∈CI1×⋯×IN: one is the(n1,…,nm)-mode unfolding of size(In1…Inm)×IT/(In1…Inm), and the other is theS∖{n1,…,nm}-mode unfolding of sizeIT/(In1…Inm)×(In1…Inm), whereIT=∏n=1NIn.The total number of such divisions ofS, which is the same as the number of pairs of mutually-transposed unfoldings ofY, is(20)(N1)+(N2)+⋯+(NN−1)2=2N−1−1,where(Nn)=N!(N−n)!n!is the total number of n-combinations of N numbers.Since each pair of mutually-transposed unfolded matrices has a common set of non-zero eigenvalues, we only choose the oblate matrix whose number of rows is less than or equal to number of columns. The resultant(2N−1−1)unfolded matrices are sorted in descending order of their number of rows as(21)Y(1),Y(2),…,Y(2N−1−1).LetPkandQkbe the numbers of rows and columns ofY(k), respectively, we have(22)P1≥P2≥⋯≥P2N−1−1,and fork=1,…,2N−1−1,(23)Pk≤Qk=ITPk.Example 4.2N=4,Y∈C4×3×5×8, there are totally2N−1−1=7unfolded matrices, namely,Y(1):[Y](1,3)=[Y](2,4)T∈C20×24, P1=20, Q1=24Y(2):[Y](2,3)=[Y](1,4)T∈C15×32, P2=15, Q2=32Y(3):[Y](1,2)=[Y](3,4)T∈C12×40, P3=12, Q3=40Y(4):[Y](4)=[Y](1,2,3)T∈C8×60, P4=8, Q4=60Y(5):[Y](3)=[Y](1,2,4)T∈C5×96, P5=5, Q5=96Y(6):[Y](1)=[Y](2,3,4)T∈C4×120, P6=4, Q6=120Y(7):[Y](2)=[Y](1,3,4)T∈C3×160, P7=3, Q7=160Denote the sets of eigenvalues of the sample covariance matricesY(k)Y(k)H/Qk,k=1,2,…,2N−1−1, as(24)ℓ1,1,ℓ2,1,…,ℓP1,1,(25)ℓ1,2,ℓ2,2,…,ℓP2,2,⋮(26)ℓ1,2N−1−1,ℓ2,2N−1−1,…,ℓP2N−1−1,2N−1−1.Next, we combine these(2N−1−1)sets of sample eigenvalues for multilinear signal number estimation using the MDL.The MDL detection algorithm was originally formulated using an information argument [55], although it can also be derived/interpreted in a Bayesian or Kullback–Leibler information framework [28]. The MDL determines the number of signals by minimizing the penalized negative log-likelihood function:(27)MDL(r)=log⁡(pr(Y,θˆr))+12log⁡(Q)⋅ν(r;P,Q)wherepr(Y,θˆr)is the likelihood function withθˆrbeing the maximum likelihood estimate of the parameter vector of the rth model, namely,θr, and the second term is the penalty term with ν being the number of free parameters inθr. In particular, in the context of multichannel time-series analysis [29], we haveν(r;P,Q)=r(2P−r)and the likelihood function in (27) is(28)log⁡(pr(Y,θˆr))=−Q(P−r)log⁡((∏i=r+1Pℓi)1P−r1P−r∑i=r+1Pℓi),where P and Q are the numbers of channels and samples, respectively.The MDL is widely used in practice due to its simplicity and consistency (that is, it returns the correct number of signals as the sample size tends to infinity) [29,56,57]. However, it tends to underestimate the number of signals when the sample size is small [58–64].For the tensor rank detection, i.e., detection of number of multilinear signals fromY, to reduce the detection error due to underestimation, a natural choice is that we employ the set of eigenvalues (24)–(26) associated to the most oblate unfolded matrixY(2N−1−1), namely, (26). Note thatY(2N−1−1)has the smallest number of rows but largest number of columns among all(2N−1−1)unfolded matrices ofY.To further improve the performance, we propose to construct the global eigenvalues by summing (or averaging)33Note that the ‘summing’ and “averaging” operations are equivalent in the sense that their defined global eigenvalues yield the same cost function of (30b).all2N−1−1sets of eigenvalues:(29)ℓi(G)=ℓi,1+ℓi,2+⋯+ℓi,2N−1−1,i=1,2,…,P2N−1−1.The global eigenvalues are substituted into the MDL criterion for detection:(30a)Rˆ(2N−1−1)=arg⁡minr∈{0,1,…,P2N−1−1−1}⁡MDLN−D(r),where(30b)MDLN−D(r)=Q2N−1−1(P2N−1−1−r)×log1P2N−1−1−r∑i=r+1P2N−1−1ℓi(G)(∏i=r+1P2N−1−1ℓi(G))1P2N−1−1−r+12r(2P2N−1−1−r)log⁡Q2N−1−1,withP2N−1−1andQ2N−1−1being respectively the numbers of rows and columns ofY(2N−1−1)in (21).The drawback of this detection scheme is that the number of detectable/identifiable signals is too small (onlyP2N−1−1−1). To maximize the identifiability and inspired by [36–38], we adopt a sequential detection procedure where the MDL incrementally utilizes the(2N−1−1)sets of eigenvalues in an order defined in (24)–(26). In the k-th sequential step, the global eigenvalues are formed by summing the first k sets of eigenvalues:(31)ℓi,k(G)=ℓi,1+ℓi,2+⋯+ℓi,k,i=1,2,…,Pk.Figs. 3–5illustrate the adaptive definition of the global eigenvalues in the first, second and third steps of the sequential rank detection algorithm, where the tensor data have the same size as in Example 4.2, and the number of components isR=6. The sequential detection algorithm is described in Algorithm 2.Note that the definition of global eigenvalues in (31) extends [36–38] in two aspects.•First, the global eigenvalues in [36–38] only use the n-mode eigenvalues, while the new definition exploits both the n-mode and multi-mode eigenvalues. Therefore, the generalized N-D MDL can identify much more signals than [36–38] forN≥4. More specifically, the maximum number of identifiable signals by the generalized N-D MDL is equal to the size of the most squared matrix minus 1:(33)Rmax=max1≤n1<⋯<nm≤N1≤m≤N−1⁡min⁡(In1⋯Inm,I1I2⋯INIn1⋯Inm)−1,which is on the order ofI1I2⋯IN. This is the highest identifiability we can expect from the matrix-unfolding-based N-D detection schemes. By contrast, the N-D MDL in [36–38] can detect at most[max⁡(I1,…,IN)−1]signals.Second, in the N-D MDL [36–38] the global eigenvalues are defined as the product of individual sets of eigenvalues, while in the generalized N-D MDL they are formed by adding individual sets of eigenvalues. Since the sum operation deflates the dispersion of noise eigenvalues, the global noise eigenvalues are more closely clustered leading to wider signal-noise spectrum gap and more efficient test of sphericity of noise subspace than that of individual eigenvalues. As shown in Theorem 4.1, the definition of global eigenvalues in (31) leads to a consistent detection criterion which correctly detects the number of signals R in the large-sample limit whenR≤Rmax, while in the original N-D MDL [36–38] the consistency is an open problem.Remark 4.1A generalization to (31) is obtained by defining the global eigenvalues as a weighted sum of individual sets of eigenvalues:(34)ℓi,k(G)=c1ℓi,1+c2ℓi,2+⋯+ciℓi,k,i=1,2,…,Pk,wherec1,c2,…,ciare the weighting coefficients. In this case, the penalty term in (32b) may need to be modified for better performance. This will be an interesting topic for further research.We analyze the asymptotic behavior of the generalized N-D MDL described in Algorithm 2 as the number of samples tends to infinity. Define(35)Q=Q1=min⁡{Q1,Q2,…,Q(2N−1−1)}as the number of columns of the most “squared” unfolded matrixY(1).Denote the highest mode rank of a tensorXas(36)rank1:2N−1−1(X)=max1≤n1<⋯<nm≤N1≤m≤N−1⁡rank([X](n1,…,nm))=max1≤k≤2N−1−1⁡rank(X(k)).First, we show that the generalized N-D MDL consistently estimatesrank1:2N−1−1(X)with probability one as Q tends to infinity.Theorem 4.1Suppose the highest mode rank of the signal tensorXsatisfies(37)rank1:2N−1−1(X)≤P1−1.The generalized N-D MDL inAlgorithm 2is a consistent estimator of the highest mode rank of the signal tensorX, namely,(38)RˆN−D=rank1:2N−1−1(X)a.s.asQ→+∞,where a.s. (short for almost surely) means “with probability one”.Remark 4.2According to (23) and (35), we haveQ≥max(I1,I2,…,IN). This implies that Theorem 4.1 holds when(39)max(I1,I2,…,IN)→+∞,i.e., when the maximum dimension length tends to infinity. In particular, in the context of array processing, the last dimension, i.e., Nth mode, corresponds to the temporal dimension, andIN=Tis the number of temporal samples. When T is infinitely large, it trivially holds that it is the maximum dimension length, and the condition (39) reduces toT→+∞, which is in accordance with the consistency condition for the 2-D MDL case [29,56,57].Next we prove that the highest mode rank of a tensor will not exceed the rank of the tensor.Theorem 4.2Given an Nth-order tensorXof rank R, for any1≤n1,…,nm≤N(1≤m≤N−1), the(n1,…,nm)-mode rank ofX, is less than or equal to R, namely,(40)rank([X](n1,…,nm))≤R.For any1≤n1<⋯<nm≤N(1≤m≤N−1), the(n1,…,nm)-mode matrix unfolding ofXis(41)[X](n1,…,nm)=∑r=1Rcr(1)∘cr(2)∈C(In1…Inm)×ITIn1…Inm,where(42)cr(1)=vec(ar(n1)∘⋯∘ar(nm))∈C(In1…Inm)×1,(43)cr(2)=vec(ar(1)∘ar(2)∘⋯∘ar(n1−1)ar(n1+1)∘⋯∘ar(nm−1)∘ar(nm+1)∘⋯∘ar(N))∈CITIn1…Inm×1The inequality (40) follows from the fact that each column of[X](n1,…,nm)is a linear combination ofc1(1),c2(1),…,cR(1).  □Remark 4.3Theorem 4.2 states that any n- or multi-mode rank of a tensorXwill not exceed its tensor rank R. Therefore, the highest mode rank ofXwill not exceed its tensor rank R as well, namely,rank1:2N−1−1(X)≤R.Remark 4.4Like the n-mode ranks [43], the multi-mode ranks of different modes of a tensor may be different.Remark 4.5According to Theorem 4.1, the generalized N-D MDL works as long as one of the n- or multi-mode ranks of the noise-free signal tensor is equal to its tensor rank R. To the best of our knowledge, the conditions under which the n- or multi-mode rank is equal to the tensor rank is an open problem in the literature that remains unsolved.For a third-order tensor, the maximum and typical ranks44The maximum rank is defined as the largest attainable rank of a tensor, whereas the typical rank is any rank that occurs with probability greater than zero (i.e., on a set with positive Lebesgue measure). See Section 2.1 in [27] for definitions.can be larger than individual dimension lengths (see Tables 1 and 2 in [27]). In this case the general N-D MDL fails, because its maximum number of detectable signals is less thanmax⁡(I1,…,IN). However, as far as we know, this scenario is not common in practical applications where the number of signals is often smaller than individual dimension lengths. Moreover, in the subspace/subset of tensors with tensor rank less than or equal toIn, it is shown in extensive Monte Carlo simulations (which randomly draw each entry of the tensor from a normal distribution with mean zero and variance one) that the rank of a tensor is equal to its n-rank with probability 1.55In the Monte Carlo simulation, the tensors are randomly generated in each trial and those with tensor rank less than or equal toInare kept and formed a set. To avoid an empty set, the sizes of tensors are chosen such that at least one typical rank is less than or equal toIn. Then we find out the percentage of the subset of tensors whose tensor rank equals its n-rank.For a fourth- or higher-order tensor, it seems that the tensor rank will not be larger than (33) (no counter-example is reported in the literature). Moreover, our empirical results reveal that the rank of the most squared unfolded matrixX(1)equals the tensor rank ofXwith probability 1.We conduct simulations to evaluate the performance of the generalized N-D MDL in estimating the tensor rank. The data are generated using the noisy CP model. The entries of the factor matrices are i.i.d. drawn fromN(0,1), i.e., real Gaussian distribution with zero mean and unit variance. The entries of the noise tensorZare i.i.d. drawn fromN(0,σ2). The signal-to-noise ratio (SNR) is defined asSNR=‖X‖F2σ2∏n=1NIn.The noise powerσ2is scaled to obtain different SNRs. For each SNR, 100 independent Monte Carlo runs have been conducted. The performance measure is the probability of correct detection (PoD), i.e.Pr⁡(Rˆ=R), averaged over signal and noise realizations.The following schemes are used in the performance comparison: CORCONDIA [22], Adaptive CORCONDIA [65], DIFFIT [20], NumConvHull [21], ARD using Gaussian prior – ridge ARD [23], ARD using Laplace prior – sparse ARD [23], reconstruction-error-based CP rank selector [25], MDL applied to individual n-mode eigenvalues,n=1,2,…,N, denoted as n-mode MDL, and the N-D MDL and N-D exponential fitting test (EFT) proposed in [38]. For CORCONDIA, three thresholds are adopted:η=50%,75%,90%.We use the MATLAB package in http://www.mortenmorup.dk/MMhomepageUpdated_files/Page327.htm for implementing the ARD algorithms. For CP decomposition and other tensor operations, we have employed the N-way toolbox for MATLAB [66]. Unless otherwise stated, for all parameters of the ARD and CP algorithms (e.g., initialization method, convergence criterion and maximum number of iterations) their default values are used. The MATLAB codes of all rank detection algorithms are available for download at http://www.ee.cityu.edu.hk/~hcso/publication.html.The ridge ARD and sparse ARD require a guess of the maximum possible number of components, i.e.,Rmax. When such information is unavailable a priori,Rmaxcan be estimated instead based on the uniqueness condition of the CP decomposition. According to [40,41,67], the R-component CP decomposition of an N-order tensor is essentially unique (up to permutation and scaling ambiguities of the factor matrices) provided that(44)min⁡(I1,R)+min⁡(I2,R)+⋯+min⁡(IN,R)≥2R+N−1.From (44),Rmaxcan be determined66The sufficient uniqueness condition of the CP decomposition in (44) has been relaxed in recent literature, e.g., [68,69], which implies an improved upper boundRmaxof the number of resolvable signals. Since the performance of the compared algorithms is not sensitive toRmax, we have not employed the improvedRmax.. For the ARD algorithms, the maximum number of iterations is set as 1000.We have conducted an extensive simulation study where the generalized N-D MDL shows competitive performance with the best of existing methods. Due to space limit, we only show results for a four-way array of size7×7×7×7.In Fig. 6, we consider a 3-component four-way array of size7×7×7×7. We see that the CORCONDIAs withη=50%,75%perform very well at low SNRs but poorly at high SNRs (cannot attain a PoD of 1 due to over-enumeration). The CORCONDIAs withη=90%performs better at high SNRs but shows inferior performance at low SNRs. The adaptive CORCONDIA presents a good trade-off between performance at low and high SNRs. The NumConvHull, ARD algorithms and reconstruction-error-based CP rank selector possess PoDs that increase monotonically with SNR and attain 1 at sufficiently high SNRs, but all have inferior performance compared to the generalized N-D MDL.In Fig. 6(b), the ‘N-D MDL (w/o multi-mode)’ denotes the proposed N-D MDL applied to n-mode unfoldings only (without considering the multi-mode unfoldings). It happens that the ‘N-D MDL (w/o multi-mode)’ performs equally to the N-D MDL [38]77For a larger array size, e.g.,I1=20,I2=30,I3=40, andI4=50, our simulations show that the ‘N-D MDL (w/o multi-mode)’ outperforms the N-D MDL [38].and the N-D EFT,88In EFT, the detection threshold is determined by Monte Carlo simulation using a preset probability of false alarm (Pfa) [31]. For typical Pfa value such as 0.01 and 0.1, the N-D EFT [38] turns out to perform poorly due to its tendency to overestimate the number of signals. To handle that, a very smallPfa=10−18is used in our simulations, just as done in [38]. However, with such a small Pfa, the number of trials in the Monte Carlo simulation is so large that it takes an unacceptably long time (months or longer) to calculate the thresholds. Therefore, we estimate the thresholds forPfa=10−18via the extrapolation method based on the simulated thresholds forPfa=0.1,0.01,0.001,0.0001.and by considering the multi-mode unfoldings, the generalized N-D MDL is superior to the N-D MDL proposed in [38].In Fig. 7, the number of components is increased toR=8. Note that all n-mode MDLs and N-D MDL in [38] fail since they can detect no more than 6 signals in this case and the results are not shown here. Note that all three CORCONDIAs and adaptive CORCONDIA fail as well. This is because estimation of the CP core in CORCONDIA involves inversion of the factor matricesAr(n),n=1,2…,N, as shown in (8). WhenR>min(I1,I2,…,IN)=7, at least one of theAr(n)s is column rank deficient. Consequently, the calculation of the core tensor is an ill-posed problem which admits infinitely many solutions and thus the identity core cannot be uniquely recovered. The DIFFIT, NumConvHull, ARD algorithms and reconstruction-error-based CP rank selector work properly but when SNR > 12.5 dB they have inferior performance compared to the generalized N-D MDL.We evaluate the performance of the proposed detection algorithm in two difficult scenarios: (i) rank-overlap [70]: some factor matrices contain collinear columns; (ii) distinct magnitudes of components: instead of using the identity tensor in (5), a diagonal core tensor with different values on the diagonal is used. The array sizes and number of components are the same as those in Fig. 6, i.e.,I1=I2=I3=I4=7andR=3.For the rank-overlap problem, we consider a simple example similar to [25]:(45)X=a1∘b1∘c1∘d1+a1∘b2∘c2∘d2+a2∘b2∘c3∘d3Fig. 8depicts the PoD versus SNR. Note that all three versions of the CORCONDIAs and adaptive CORCONDIA fail as well. This can be justified based on a similar argument to Fig. 7: due to rank deficiency of the first two factor matricesA=[a1a1a2]andB=[b1b2b2]cannot be uniquely inverted and as a result the CP core cannot be uniquely recovered. By contrast, all other tensor-based rank detectors suffer no performance degradation compared to Fig. 6(a).In Fig. 8(b), the N-D EFT and two N-D MDLs suffer a slight performance degradation compared to Fig. 6(b), and they perform even worse than the 3-mode and 4-mode MDLs. The 1-mode and 2-mode MDLs fail due to rank deficiency of the first two factor matrices. The generalized N-D MDL suffers almost no performance degradation and outperforms all the alternatives.In Fig. 9, we compare the performance of algorithms in the second difficult scenario, where we assume the elements on the core diagonal follow a Laplace distribution with zero mean and unit variance. We see that all matrix- and tensor-based rank detectors suffer a significant performance degradation to various degrees compared to Fig. 6. The generalized N-D MDL is still superior to all other algorithms except for the two CORCONDIAs with thresholdsη=50%,75%, which perform the best for low-to-intermediate SNRs. Fig. 10shows the histogram of estimated number of components at SNR = 25 dB. We see that for the CORCONDIAs, DIFFIT, NumConvHull, and reconstruction-error-based CP rank selector, the detection error comes from both underestimation and overestimation of the number of components, while for other rank detection algorithms, the deviation is due to underestimation only.Fig. 11shows the mean computational times in seconds of different rank detectors for a three-way array of various sizes. The experiments are run on a desktop PC with an Intel 3.6 GHz Dual Core i7 CPU and a 32 GB RAM. The parameters settings are described in the figure caption.In Fig. 11(a), the computational complexities of all tensor-based rank detectors are much higher than that of the generalized N-D MDL. As the size of the arrayInincreases from 5 to 30, the gap in computational time significantly increases, and whenIn=30, the mean computational time of the tensor-based rank detectors is tens of millions more than that of the generalized N-D MDL. Among the tensor-based rank detectors, the ARDs require relatively less computational time than others. As discussed in Section 3.2, this may be because in the ARDs, the model selection is integrated into the CP decomposition algorithm, and the removal of redundant components is performed not after the convergence of the CP decomposition algorithm but throughout the iterations. Note that the N-D EFT also requires longer computational time than the N-mode MDL.In Fig. 11(b), the generalized N-D MDL requires approximately 3 times larger computational time than those required in the n-mode MDLs. Note that for such a three-way array, the complexity of the generalized N-D MDL is similar to the N-D MDL [38]. Their slight difference may owe to the different arithmetic operations used in calculating the global eigenvalues (addition versus multiplication).

@&#CONCLUSIONS@&#
