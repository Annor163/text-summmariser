@&#MAIN-TITLE@&#
A test-suite reduction approach to improving fault-localization effectiveness

@&#HIGHLIGHTS@&#
This paper proposes a novel test-suite reduction.This approach analysis concrete path information as well as test-suite coverage.This method removes test-suite which has little or no effect on fault-localization.This method improves the evenness of distribution of concrete execution paths.The reduced test-suite can improve fault-localization effectiveness.

@&#KEYPHRASES@&#
Software debugging,Fault localization,Test-suite reduction,

@&#ABSTRACT@&#
In order to improve the effectiveness of fault localization, researchers are interested in test-suite reduction to provide suitable test-suite inputs. Different test-suite reduction approaches have been proposed. However, the results are usually not ideal. Reducing the test-suite improperly or excessively can even negatively affect fault-localization effectiveness. In this paper, we propose a two-step test-suite reduction approach to remove the test cases which have little or no effect on fault localization, and improve the distribution evenness of concrete execution paths of test cases. This approach consists of coverage matrix based reduction and path vector based reduction, so it analyzes not only the test cases coverage but also the concrete path information. We design and implement experiments to verify the effect of our approach. The experimental results show that our reduced test-suite can improve fault-localization effectiveness. On average, our approach can reduce the size of a test-suite in 47.87% (for Siemens programs) and 23.03% (for space program). At the same time, on average our approach can improve the fault-localization effectiveness, 2.12 on Siemens programs and 0.13 on space program by Tarantula approach.

@&#INTRODUCTION@&#
Software debugging, which includes locating and correcting faulty program statements, is an important and expensive activity in the software development and maintenance [1,12]. Traditionally, fault localization is a manual process which is tedious and error-prone. In recent years, there have been considerable researches on automated fault localization techniques [3–24], based on the execution information of both passed and failed test cases. In order to improve the effectiveness of fault localization, some test cases are deleted from test-suite to provide suitable test cases inputs. Thus, the effect of test-suite reduction on fault-localization effectiveness has been widely studied.In the previous research work, some researchers focus on proposing different approaches to reduce test cases, and then investigating how the reduced test-suite affect the fault-localization effectiveness according to their own experimental artifacts. Abreu et al. [25] and Baudry et al. [26] investigated the relationship between the number of the test cases and fault-localization effectiveness. Zhang et al. [27] found that the effectiveness of the fault-localization technique increases with the distribution evenness of execution traces of test cases. Chen et al. [28] proposed a lightweight test-suite reduction approach based on the conjecture of the covering interaction of requirements to detect more faults. In all the above approaches, researchers only analyze coverage information with respect to each test case, and they use their own experimental artifacts to verify the effect of test-suite reduction on fault-localization effectiveness, but there is not a consensus on how the removal of test cases from the test-suite can affect the fault localization. Hao et al. [29,30] proposed several statement-based reduction strategies and assumed that test cases redundancy or similarity can negatively affect fault-localization effectiveness. Yu et al. [31] presented vector-based reduction techniques and pointed out their strategies are more effective than the statement-based reduction technique. They found that statement-based reduction [29,30] techniques negatively affect the fault-localization effectiveness, and vector-based reduction [31] strategy provides negligible impact on the effectiveness of fault localization.Different from the previous research work, in this paper, we propose a two-step test-suite reduction technique. Traditional approaches [26–31] only consider the coverage information of test cases. The key intuition of our approach is that it analyzes both the test cases coverage and the concrete path information. Faults usually exist in the execution paths of failed test cases, so the statements in the paths of failed test cases may be more helpful in localizing faults. By contrast, the statements which exist in the paths of all test cases (including passed test cases and failed test cases) have little effect on fault localization. In addition, though researchers have pointed out that improving the distribution evenness of execution traces of test cases benefits fault-localization effectiveness, they only analyzed coverage information rather than concrete path information. Based on the above analysis, fault-localization requirements are proposed to emphasize the statements which are only executed by failed test cases. We propose a coverage matrix based reduction approach to remove the test cases which are not relevant to fault-localization requirements, and a path vector based reduction approach to improve the distribution evenness of concrete execution paths of test cases.The main contributions of this paper include:•We propose the concepts of coverage matrix based reduction and path vector based reduction for fault localization.We propose a new test-suite reduction approach, which analyzes both the test cases coverage and the concrete path information.We design and implement experiments to verify the effect of our approach, and the experimental results show that our approach can improve the effectiveness of fault localization.The framework of our technique is shown inFig. 1. A program instrumentor is implemented via Yacc [2]. It records both coverage vector and path vector simultaneously for every test case. The work is divided into two parts in Fig. 1 as follows: (1) The statements only executed by failed test cases are more likely to be faulty, so these statements should be paid attention to. The coverage matrix is established based on fault-localization requirements, and then the passed test cases weakly relevant to the fault-localization requirements are deleted based on the coverage matrix. We call this coverage matrix based reduction (see upper part of Fig. 1). The aim of coverage matrix based reduction is to emphasize the fault-localization requirements on fault localization. (2) The repeat times of loops decrease the distribution evenness of execution paths, and thus affect the effectiveness of fault localization. For the test cases with identical coverage vector, the corresponding path vectors are extracted, and the redundant test cases with identical path vectors are deleted. Next, loop standardization is executed and the test cases with similar paths are deleted. We call this path vector based reduction (see lower part of Fig. 1). The aim of path vector based reduction approach is to increase the distribution evenness of execution paths.Definition 1(Execution path): We consider each execution path of a program P to be a sequence of statements S=〈s1,s2,…,si,…〉. In this paper, the S executed by test case t is denoted as PATH(t). It is worthwhile to note that the statement simay occur several times if it is in the loop structure.(Coverage vector): The coverage vector (binary vector) of test case t is denoted as COVER(t), where COVER(t)=S′=〈s′1,s′2,…,s′n〉 (n is the number of statements of P).Fig. 2 shows an example program and its test cases. The program mid() has been used in many previous fault-localization studies [10] and test-suite reduction approaches [27,30,31] as example program. This program inputs three integers and outputs the median value of the three integers. It contains a fault on line 7—this line should be “m=x;”. To the right of each line of code is a set of eight test cases: inputs are shown at the top of each column, statement coverage is represented by bullets in the columns, and the pass/fail status is shown at the bottom of the columns. In terms of Definitions 1 and 2, PATH(t1)=〈1,2,3,4,6,7,13〉, COVER(t1)=〈1,1,1,1,0,1,1,0,0,0,0,0,1〉.We assume that the statements executed by failed test cases are more likely to be faulty than those not executed by failed test cases. The statements executed by all test cases (including passed and failed test cases) are less likely to be faulty. The coverage vector of each test case indicates whether a statement is executed or not. Based on this analysis, the statements only in the coverage vector of failed test cases are defined as fault-localization requirements, which are denoted as FLreq. Coverage matrix based reduction is proposed, by analyzing the statement coverage and the fault-localization requirements, to delete the test cases weakly relevant to fault-localization requirements.Definition 3(Coverage matrix): Given a target program P, which consists of statements s1, s2, …, sn. Let T={t1,t2,…,tm} be a test-suite for P, m be the number of test cases for P. The statements coverage matrix (binary matrix) COVER(T)={COVER(t1),COVER(t2),…,COVER(tm)}.The statements coverage matrix of program mid() (Fig. 2) is shown inFig. 3.Definition 4(Weakly irrelevant statements): Let T={t1,t2,…,tm} be a test-suite for program P, sk(1≤k≤n) be one of statements of P, COVER(T) be the statement coverage matrix of P. skis a weakly relevant statement of P for the test-suite T, if and only if for all pairs of i and j (1≤i≤m; 1≤j≤m; i≠j), COVER(ti–sk)==COVER(tj–sk). COVER(ti–sk) is the k-th element of COVER(ti). The weakly relevant statements do not need to be specially considered, because these statements are less likely to be faulty and less important for fault localization. The suspiciousness score of each statement is calculated in fault localization by comparing the statistical differences in passed and failed test cases. All of the passed and failed test cases execute weakly relevant statements, so the suspiciousness score of weakly relevant statements are relative small. The remaining coverage matrix which does not contain the weakly relevant statements is denoted as RCOV(T), the remaining coverage vector corresponding to test cases tiis denoted as RCOV(ti), and the remaining coverage vector corresponding to fault-localization requirements FLreq is denoted as RCOV(FLreq).In Fig. 3, the statements s1, s2, s3, s12, and s13 are weakly relevant statements of program mid() for the studied test-suite. The dotted line frame shows the remaining coverage vectors RCOV(ti). All the remaining coverage vectors form the remaining coverage matrix RCOV(T).Definition 5(Intersection and union of coverage vectors): For all pairs of COVER(t1)=〈x1,x2,…,xn〉 and COVER(t2)=〈y1,y2,…,yn〉, we define(2)COVER(t1)∩COVER(t2)=〈z1,z2,…,zn〉wherezi=xi∧yiCOVER(t1)∪COVER(t2)=〈z1',z2',…,zn'〉wherezi'=xi∨yi(1≤i≤n)Similarly, for COVER(t1)=〈x1,x2,…,xn〉, COVER(t2)=〈y1,y2,…,yn〉, …, COVER(tm)=〈p1,p2,…,pn〉,(3)COVER(t1)∩COVER(t2)∩...∩COVER(tm)=〈z1,z2,...,zn〉wherezi=xi∧yi∧...∧...∧piCOVER(t1)∪COVER(t2)∪...∪COVER(tm)=〈z1',z2',...,zn'〉wherezi'=xi∨yi∨...∨...∨pi(1≤i≤n)(Fault-localization requirements vector): The fault-localization requirements vector FLreq is obtained by analyzing the coverage vectors of all failed test cases. Let T={t1,t2,…tm} be a failed test-suite for P. For the localization of a single fault, the faulty statement should be executed by every failed test case, so the statement executed by all the failed test cases should be included. FLreq=COVER(t1)∩COVER(t2)∩…∩COVER(tm). By contrast, for the localization of multiple faults, the program should contain several faulty statements. One failed test case may not execute all of the faulty statements. One faulty statement should be executed by one or several failed test cases, so FLreq=COVER(t1)∪COVER(t2) ∪…∪COVER(tm). If the user does not know in advance how many faults there are, two alternatives can be chosen. (1) Faults can be located one by one. First, the user can assume that the program contains one fault and calculate the FLreq by “∩”, and then locate the first fault. The user can locate the other faults by the same method. (2) The user can assume that the program contains multiple faults, and calculate the FLreq by “∪”. The remaining coverage vector corresponding to FLreq is denoted as RCOV(FLreq).(Weakly relevant test cases): For test cases t1 and t2, t1 is weakly relevant to t2 if(4)RCOV(t1)∩RCOV(t2)=〈z1,z2,...,zn〉wherez1=z2=…=zn=0The model of coverage matrix based reduction is shown inFig. 4.Fault-localization requirements vector FLreq is obtained according to Definition 6.The coverage matrix based on fault-localization requirements is established by combining the coverage matrix of passed test cases with fault-localization requirements vector FLreq.In order to delete the test cases weakly relevant to fault-localization requirements easily, the weakly relevant statements are not considered, and the remaining coverage matrix RCOV(T) is obtained according to Definition 4.According to Definition 7, the passed test cases weakly relevant to FLreq are deleted.To illustrate the coverage matrix based reduction approach, let us consider the program mid() and passed test-suite {t1,t2,t3,t4,t5,t6}, and failed test-suite {t7,t8} (Fig. 2). According to Definition 6, FLreq=COVER(t7)∩COVER(t8)=〈1,1,1,1,0,1,1,0,0,0,0,0,1〉. Then, coverage matrix is established as shown inFig. 5. The weakly relevant statements are not considered and the remaining coverage matrix RCOV(T) is obtained. According to Definition 7, RCOV(t3)∩RCOV(FLreq)=0, so t3 is weakly relevant to FLreq. It is the same case for test cases t4 and t6. It is worthwhile to note that the failed test cases cannot be deleted. Finally, the passed test cases t1, t2, t5, and failed test cases t7, t8 are left after coverage matrix based reduction.The traditional methods [26–31] of test-suite reduction only analyze the statement coverage information. However, many test cases may have different PATHs, even though they have the same COVER. Different paths contain different semantic information such as control dependence and data dependence, which is important to fault localization. Thus, the traditional methods [26–31] will decrease the accuracy of fault localization because they ignore the differences of semantic information between different paths. In the previous work, the obtained consensus on the test-suite reduction is that improving the distribution evenness of execution traces of test cases can benefit fault-localization effectiveness. Improving the distribution evenness of execution traces is to make the execution times of each statement as even as possible. However, previous works improve the distribution evenness of execution traces only based on the coverage of test cases. For each test case, the researchers only analyze whether the statement has been executed or not in runtime, rather than distinguish the number of times the statement has been executed. Thus, previous works do not calculate the real execution times of each statement, so that the distribution evenness of execution traces cannot be really improved. In this paper, the distribution evenness of execution paths is improved to make the execution times of both path and statement as even as possible. Our approach uses path profiles to calculate the execution times of each statement. The proposed path vector based reduction approach deletes redundant test cases with similar or identical paths. As a result, it is really realized to improve the distribution evenness of execution paths.Definition 8(Identical paths): For all pairs of PATH(t1)=〈x0,x1,…,xi,…,x|path(t1)|〉 and PATH(t2)=〈y0,y1,…,yi,…,y|path(t2)|〉, |PATH(t)| is the number of statements of PATH(t). PATH(t1) and PATH(t2) are identical paths, denoted as PATH(t1)==PATH(t2), if(5)1.|PATH(t1)|==|PATH(t2)|2.x0=y0,x1=y1,…,xi=yi,…,x|path(t1)|=y|path(t1)|The number of times the statements in a loop structure being executed increase with the repeat times of the loop, and therefore more repeat times of the loops will decrease the distribution evenness of execution paths. Loop standardization is a process of transformation on PATH. Its purpose is to unify the loop structure and delete the test cases with more repeat times of loops, and therefore increase the distribution evenness of execution paths.If the program contains loop structures, partial series of PATH(t) would repeat for several times. The repeat statement sequences and its corresponding repeat times are recorded, and then this repeat statement sequences are deleted from PATH(t) (Fig. 7). The loop standardized execution path vector corresponding to PATH(t) is denoted as PATH′(t).Definition 9(Similar paths): For all pairs of PATH(t1)=〈x0,x1,…,xi,…〉 and PATH(t2)=〈y0,y1,…,yi,…〉, the corresponding loop standardized execution path vectors are PATH′(t1) and PATH′(t2), respectively. PATH(t1) and PATH(t2) are similar paths, denoted as PATH(t1)≈PATH(t2), if PATH′(t1) and PATH′(t2) are identical paths.Fig. 6 shows the model of path vector based reduction.(1)Program instrumentor records path vector for every test case. For the passed test cases with the same coverage vector, the corresponding path vectors are extracted. According to Definition 8, one of the passed test cases with identical path is reserved and the others are deleted.Loop standardization is executed and the passed test cases with similar path are deleted according to Definition 9. It is worthwhile to note that for the test cases with similar paths, only the test case with the lowest repeat times is reserved.To illustrate the path vector based reduction approach, let us consider the program and the passed test-suite: T={t1,t2,t3,t4,t5} (Fig. 7). These test cases have the same COVER, so the corresponding PATHs are extracted. To the right of the program are the PATH(ti) and the corresponding PATH′(ti) (1≤i≤5). According to Definition 8, PATH(t4)==PATH(t5), so we save one of them and delete the other one. Next, loop standardization is executed for test cases t1, t2, and t3. According to Definition 9, PATH(t1)≈PATH(t2)≈PATH(t3). The repeat times of PATH(t2) is less than that of PATH(t1) and PATH(t3), so the test cases t1 and t3 are deleted. We believe that PATH(t1) and PATH(t2) are similar paths because both of them have the same execution path: for→if(1)→for→if(2), although they have different repeat times. PATH(t3) and PATH(t4) are not similar paths because the execution path of PATH(t3) is for→if(1)→for→if(2) and the execution path of PATH(t4) is for→if(2)→for→if(1). Moreover, the statements executed more times will decrease the evenness of execution paths, so we delete the test cases with more repeat times. The reduced test-suite is {t2,t4} or {t2,t5}. The user can randomly choose one of them.Traditional test cases reduction approaches [26–31] only analyze whether the statements are executed or not. PATH(t1), PATH(t2), PATH(t3), PATH(t4) and PATH(t5) are not distinguished. So the traditional methods will decrease the accuracy of fault localization.To demonstrate the feasibility of our approach described in Section 2, we design and conduct an experiment to answer the following two research questions.Question 1: Can the reduced test-suite in our approach improve the fault-localization effectiveness compared with the whole test-suite collection?Question 2: How much does our approach reduce the size of the test-suite, and how much are the technique-execution costs of our approach?Tarantula [10] is proposed as an effective fault localization approach and is used by many scholars for comparisons in their papers [29–31]. It calculates statements’ suspiciousness according to the coverage information and execution results (success or failure) with respect to each test case. To evaluate the fault-localization effectiveness of our reduced test-suite, the reduced test-suite is applied to Tarantula. The key intuition of Tarantula is that statements in a program primarily executed by failed test cases are more likely to be faulty than those primarily executed by passed test cases. The suspiciousness of a statement s is calculated as follows:(6)suspiciousness(s)=failed(s)totalfailed(s)passed(s)totalpassed(s)+failed(s)totalfailed(s)In Eq. (6), failed(s) is the number of failed test cases which executed s, passed(s) is the number of passed test cases which executed s, totalfailed(s) is the number of total failed test cases, totalpassed(s) is the number of total passed test cases. The statements are ranked in terms of their suspiciousness, from the greatest score to the least score. It is worthwhile to note that the statements are assigned the best rank when they are tied.In our experiment, we use the Siemens programs [37] and space program [38] as the subject programs to evaluate the effectiveness of our approach, because these programs have been used in many previous fault localization studies [10–14] and test-suite reduction approaches [30–32]. All the programs are written in C. The Siemens programs are a suite of seven small programs, including print_tokens, print_tokens2, replace, schedule, schedule2, tcas and tot-info. Each program has more than 1000 test cases. The space program was developed by the European Space Agency. It has more lines of code and more test cases than Siemens programs.Table 1 shows the characteristics of each program: the name of the program, the description of the program, the number of faulty versions, the number of lines of code, and the number of test cases. Each faulty version contains a single fault that was gathered from real experience, although the faults may span multiple statements or even functions. Among the 170 programs, 21 versions are excluded: version 9 of schedules2, version 32 of replace, versions 1, 2, 3, 12, 32, and 34 of space have no failed test cases; versions 4 and 6 of print_tokens are only different in a header file; version 27 of replace, version 10 of print_token2, version 5, 6 and 9 of schedule, versions 25, 26, 30, 35, 36, and 38 of space have segmentation fault. This is also discussed in previous researches [31]. After removing the 21 versions, 123 versions of Siemens programs and 26 versions of space program are left to evaluate the effectiveness of our approach.In our experiment, we use 123 versions of Siemens programs and 26 versions of space program as the subject programs to evaluate the effectiveness of our approach. Before applying our strategy, we have done some checked tests for all test cases, whose results are passed or failed. The number of these passed or failed tests can be arbitrary in practice. And then the fault-localization requirements are obtained according to failed test cases. To evaluate the effect of coverage matrix based reduction and path vector based reduction on fault-localization effectiveness, we design and implement two experiments as follows:Experiment 1 (CMR+PVR): We use coverage matrix based reduction approach to delete the test cases which are weakly relevant to fault-localization requirements. Then we use path vector based reduction approach to improve the distribution evenness of execution paths of test cases.Experiment 2 (PVR): For comparison, we only use path vector based reduction approach to get the reduced test-suite.The percentage of reduction in test-suite size is measured by metric Reduction[31]. It calculates the ratio of the size of the reduced test-suite to its unreduced test-suite.(7)Reduction=(1−sizeofreducedtest-suitesizeofunreducedtest-suite)×100%We define metric EffectivenessChange to evaluate the effect of the test-suite reduced by our approach on the fault-localization effectiveness compared with the unreduced test-suite. The original equation Expense was published in [31] but we need to adapt the formula to EffectivenessChange.(8)EffectivenessChange=UnreducedRank−ReducedRanknumberofexecutablestatements×100%In Eq. (8), UnreducedRank is the rank of faulty statement when executing the unreduced test-suite, and ReducedRank is the rank of faulty statement when executing the reduced test-suite. Clearly, upper score indicates that the reduced test-suite is more effective on the fault localization. Positive EffectivenessChange indicates that the reduced test-suite can improve the effectiveness of fault localization, while negative EffectivenessChange indicates the reduced test-suite reduces the effectiveness of fault localization.Yu et al. [31] investigated Statement-based reduction approaches (SA) and Vector-based reduction approaches (VA) based on a set of experiments. Siemens programs and space program are also used in their experiments. They generate another 20 faulty versions of space program according to their own experience. After removing the 12 versions, they use 46 versions of space in their experiments. The number of faulty versions used by Yu et al. is different from ours. Thus, we can only compare our results with Yu et al. [31] on Siemens programs. InTables 2 and 3, the results of SA and VA are directly cited from [31].Table 2 shows the mean percentage of test-suite size reduction on Siemens programs. For example, for program replace, the mean reduction of 41.411% is achieved on the unreduced test-suite by CMR+PVR. This means that the reduced test-suite is only 58.589% of the unreduced test-suite. From Table 2, we can see that SA provides the greatest reduction of test-suite, and our approaches have similar reduction results to VA on average. PVR remove more test cases compared with CMR.In order to investigate the effect of reduced test-suite on fault-localization effectiveness, we calculate the mean EffectivenessChange for each program. In Table 3, all the EffectivenessChange of SA are negative, which means SA reduces the fault-localization effectiveness. VA have both positive and negative EffectivenessChange, that means on many versions and programs, reduced test-suites by VA improve the fault-localization effectiveness, but this impact is small and not always present. All the EffectivenessChange of CMR+PVR and PVR are above 0, indicating that all of the reduced test-suite can improve the effectiveness of fault localization compared with the unreduced test-suite. The results show that the reduced test-suite by CMR+PVR has a better effect compared with PVR, which means CMR benefits fault localization. PVR only uses path vector based reduction approach, and all the values are relatively large, indicating that PVR can improve the fault localization efficiently. The average EffectivenessChange of CMR+PVR is 2.122%. That is to say, according to formula 8, for a program with 300 lines of executable statements, the rank of the faulty statement can be raised by 6 positions in the ranking list of suspicious statements, after using the reduced test-suite produced by CMR+PVR.In Table 3, the mean EffectivenessChange of CMR+PVR and PVR are similar, that means CMR+PVR and PVR have similar effect on fault-localization effectiveness. Thus, we only investigate the effect of test-suite reduced by CMR+PVR on the fault-localization effectiveness compared with the unreduced test-suite.Fig. 8 shows the percentage of faults that can be located when a certain percentage of code is examined. The lower the percentage of code to be examined, the higher the effectiveness of the fault localization technique is. We find that our reduced test-suite can locate more faults than unreduced test-suite when checking the same number of codes. For example, in Fig. 8, using unreduced test-suite, less than 1% of the code must be examined to locate the faulty statement in 13.93% of the faulty versions. By contrast, using reduced test suites, with the same percentage of code examined, the percentage of fault located increases to 18.70%. The number of faulty versions in our experiment is 123. According to formula 8, when a programmer examines 1% of the code, our approach can discover 23 (123×18.70%) faulty versions, while the unreduced test-suite can only discover 17 (123×13.93%) faulty versions. This phenomenon also suggests that the reduced test-suite generated by our approach can improve fault-localization effectiveness.Table 4 shows the experimental results on the space program. It is worthwhile to note that the size of space program is much larger than those of Siemens programs. Although the mean EffectivenessChange is only 0.129%, according to formula 8, the rank of faulty statement rises up to an average of 8 positions in the ranking list of suspicious statements.In order to further investigate the test-suite size reduction and EffectivenessChange of each program, a boxplot11A boxplot is a standard statistical device for representing data sets. It consists of five important sample percentiles: the sample minimum, the lower quartile, the median, the upper quartile and the sample maximum. The box's height spans the central 50% of the data and its upper and lower ends mark the upper and lower quartiles. The middle of the three horizontal lines within the box represents the median.is used to depict the distribution of reduced test-suite size and the distribution of EffectivenessChange.Fig. 9 shows the percentage of reduction for CMR+PVR and PVR, respectively. The vertical axis for the chart represents the percentage of test-suite size reduction for each program.Fig. 10 shows the EffectivenessChange for CMR+PVR and PVR, respectively. The vertical axis for the chart represents EffectivenessChange for each program. In Fig. 9, the boxplots of program tcas are very narrow, which indicates that the number of reduced test-suite in total 41 faulty versions is similar. According to the results of our experiment, the size of reduced test-suite ranges mainly from 8 to 14. Most of the test cases are deleted because their execution paths are similar after PVR. The size of reduced test-suite is much less than that of unreduced test-suite, whereas the reduced test-suite does not affect the effectiveness of fault localization. In some faulty versions, the reduced test-suite can even improve the fault-localization effectiveness. In Fig. 9, the differences of boxplots between CMR+PVR and PVR are the minimum reduction value of print_tokens2, the maximum reduction value of tot_info, and the maximum reduction value of schedule. We make a concrete analysis of these differences (Table 5). For example, for faulty version 5 of schedule, 2293 (2650−357) test cases are deleted by CMR+PVR. According to formula 6, Reduction=(2650−357)/2650×100%=86.53%. It is the maximum reduction value of schedule in the boxplot of CMR+PVR. By contrast, the Reduction is only 19.43% ((2650−2130)/ 2650×100%) by PVR. Thus, the maximum reduction value of schedule by CMR+PVR is much bigger than that by PVR. In Fig. 10, the maximum EffectivenessChange value between CMR+PVR and PVR are also different. From Table 5 we can see that the fault appears in the position 66 by CMR+PVR. According to formula 8, EffectivenessChange=(161−66)/ 292×100%=32.53%. By contrast, the fault appears in the position 161 by PVR, EffectivenessChange=0, which means PVR only reduce test cases, and the rank of faulty statement cannot be raised. This EffectivenessChange is not the maximum EffectivenessChange value by PVR. In the same way, we can know the reasons for the difference of print_tokens2 and tot_info between CMR+PVR and PVR.In Fig. 10, some differences on print_tokens2, schedule, and tcas can be seen between CMR+PVR and PVR. We have investigated the difference of print_tokens2 and schedule above. Table 5 shows that for faulty version 34 of tcas, the fault appears in the position 23 by CMR+PVR. EffectivenessChange=(42–23)/141×100%=13.48%, which is the maximum EffectivenessChange value by CMR+PVR. By contrast, the fault appears in the position 38 by PVR. EffectivenessChange=2.84%, which is not the maximum EffectivenessChange value by PVR. Table 5 shows that for program tcas, the size of reduced test-suite by CMR+PVR is similar to that by PVR, so there is no obvious difference between CMR+PVR and PVR in Fig. 9.From Table 5 we can see that CMR+PVR delete more test cases than PVR, and the ranks of faulty statements by CMR+PVR have been raised compared with PVR, which means deleting the test cases weakly relevant to fault-localization requirements can improve the fault-localization effectiveness. Ochiai is also a coverage-based fault localization approach. Previous reports [25] indicated that Ochiai is more effective than Tarantula. In our previous work, a fault localization approach named Fault Localization based on State Dependency Probabilistic Model (SF) [39] is proposed. It uses path profiles to capture the behavior state information of each program element. Different from Tarantula and Ochiai which only consider the coverage information, SF analyzes both execution paths and frequencies, and the experimental results show that SF can locate more faults than Tarantula and Ochiai. In this paper, Ochiai and SF are also used to verify the effect of our test-suite reduction approach on fault-localization effectiveness.Fig. 11 shows that Ochiai and SF can locate more faults when executing the reduced test-suite.Two alternatives can be chosen for the localization of multiple faults. The effects of option s1 and 2 on fault localization are investigated. 3 2-fault versions and 3 3-versions are generated for every program of Siemens by injecting the faults from its original versions into the correct version.Table 6 shows that the test-suite reduced in option 1 is more effective on fault localization than test-suite reduced in option 2. It is worthwhile to note that the test-suite is reduced only one time in option 2, whereas the number of times the test-suite is reduced in option 1 increase with the number of faults.Table 7 summarizes the mean time of running CMR, PVR, and the mean time of running Tarantula and SF with reduced test-suite and unreduced test-suite, respectively. The experiment environment is Intel(R) Core(TM) i3-2350M CPU @2.30GHz, Memory 4.00GB. The running time is represented in seconds. Tarantula is more efficient than SF because it locates the faults based on the analysis of coverage information. However, our previous work proved that SF is more effective than Tarantula on locating faults. Table 7 shows when executing reduced test cases, Tarantula and SF can save some time compared with executing unreduced test cases, which means the reduced test-suite can decrease the fault-localization time.

@&#CONCLUSIONS@&#
