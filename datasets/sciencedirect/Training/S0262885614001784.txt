@&#MAIN-TITLE@&#
Discriminative and generative vocabulary tree: With application to vein image authentication and recognition

@&#HIGHLIGHTS@&#
A joint discriminative and generative vocabulary tree modelA non-parametric image patch layout model and matching methodEvaluation in vein authentication and recognition tasks with good performanceComprehensive in-depth discussion of the technique

@&#KEYPHRASES@&#
Vein identification,Vocabulary tree,

@&#ABSTRACT@&#
Finger vein identification is a new biometric identification technology. While many existing works approach the problem by using shape matching which is the generative method, in this paper, we introduce a joint discriminative and generative algorithm for the task. Our method considers both the discriminative appearance of local image patches as well as their generative spatial layout. The method is based on the popular vocabulary tree model, where we utilize the hidden leaf node layer to calculate a generative confidence to weight the discriminative vote from the leaf node. The training process remains the same as building a conventional vocabulary tree, while the prediction process utilizes a proposed point set matching method to support non-parametric patch layout matching. In this way, the entire model retains the efficiency of the vocabulary tree model, which is much lighter than other similar models such as the constellation model (Fergus et al., 2003). The overall estimation follows the Bayesian theory. Experimental results show that our proposed joint model outperformed the purely generative or discriminative counterpart, and can offer competitive performance than existing methods for both the vein authentication and recognition tasks.

@&#INTRODUCTION@&#
Finger vein authentication and recognition are new biometric identification technologies based on the fact that different fingers have different vein patterns [17,31], as illustrated in Fig. 1. Using vein image for recognition and authentication is non-intrusive and robust against finger surface condition [23]. The most attractive attribute is the strong immunity to forgery because the underline pattern is invisible inside the human body and only appears under the infrared light.A critical step in finger vein recognition is to match the query vein pattern to a set of database fingers, each with a set of example vein images. Many existing methods work by converting the vein image into shape representation and then performing shape matching. For example, Miura et al. [18] extracted the finger vein from unclear image using line tracking. Song et al. [26] proposed a mean curvature method to represent the vein image as a geometric shape and find valley-like structures with negative mean curvatures for matching. In [13], finger vein patterns were extracted by combining morphological operation and maximum curvature points in image profiles. In [35], from each raw vein image a best bit map was extracted for vein verification and recognition. Since shape can also be represented by the geometrical layout of feature points, local feature matching based vein recognition methods were also attempted. For instance, Yu et al. [41] extracted minutiae features for geometric representation of the vein shape and used the Hausdorff distance algorithm to evaluate possible relative positions of minutiae features. Wang [32] applied the Hausdorff distance based scheme to analyze interesting points for vein recognition.These above-mentioned works based on shape consistency can all be categorized as generative approaches, because the shape similarity tells how likely the query image can be observed given a hypothesized object class, i.e., given observation X, the posterior probability Po(l|X) for class l is estimated using Bayes rules Po(l|X)∝P(X|l)Pr(l), where P(X|l) is measured using shape similarity. For vein recognition problem, these methods rely on the consistency of the vein shape. Although the assumption holds in general, segmentation errors may occur due to poor finger vein image quality and severely degrade the recognition accuracy. To overcome the problem, multi-biometric systems were reported. For example, Yang et al. [36] exploited finger vein features in local moments, topological structure and statistics for recognition. Yang et al. [40,39] used the multimodal biometric approach to fuse the binary vein patterns and the normalized dorsal textures into one feature image for personal authentication. Methods based on score-level fusion have also been reported, such of [14] where finger veins, fingerprints, and finger geometry features were individually recognized and then combined. The difficulty in the fusion based approach is how to select the optimal combination weight, especially when multiple modalities are considered, the feature space grows exponentially.Another major category of approaches for visual recognition is the discriminative approaches where Po(l|X) is estimated directly from a classification function l=f(X). In the general image classification domain, the popular Bag of Feature (BoF) approach [6,25] and the more recent sparse-coding based method [30] both belong to the category. For vein recognition, Xi et al. [34] extracted PHOG based local image pattern to construct the discriminative “hyperinformation” for vein identification. There is yet no conclusion whether one is better than the other. For instances, although the asymptotic error of discriminative methods is lower than for generative ones, in binary classification problems, generative approaches often have better performance for smaller number of training examples [19]. For this reason, both discriminative and generative approaches are popular in research literature.The situation has attracted many studies on proposing hybrid methods to marry the benefits of both approaches. Some work by combining discriminative and generative information at the feature level. For example, Harr-like features can be used for training discriminative classifiers and also for reconstruction [29]. Tao et al. [27] showed that binary basis functions can be used in either discriminative or generative systems [11]. Fidler et al. [8] used LDA to construct discriminative basis which also contains sufficient reconstructive information. Grabner et al. [11] combined the errors from both discriminative and generative models to perform feature selection in the boosting framework. Yang et al. [38] applied the discriminative “soft-biometric” information in addition to the generative shape template for finger vein recognition.Systems based on using both discriminative and generative models at the classifier level have also been introduced by adopting multi-stage approaches. For instances, Lin et al. [16] computed the distribution of positive samples in the first stage and a new distribution for the negative samples in the second stage by learning a linear projection. Roth et al. [24] proposed the conservative learning framework by first learning generative PCA models to then supervise the boosted discriminative models. Qin et al. [22] combined the decision from local SIFT feature, vein shape and orientation feature using weighted SUM and SVM fusion rules. Asaari et al. [2] considered the matching scores from both the Band Limited Phase Only Correlation and the Centroid Contour Distance for finger identification.The situation has motivated us to also attempt the vein image recognition problem using joint discriminative and generative approaches. In this paper, we focused on performing recognition based on local image descriptors [6,25], because the appearance of these descriptors is discriminative, while their geometrical layout is generative. The scenario recalls the constellation model [33,7,12]. For example, Fergus et al. [7] modeled both appearance and shape in a scale-invariant space using respective Gaussian density function, and training for both Gaussian parameters was implemented using an iterative EM algorithm. Holub et al. [12] further extracted the discriminative Fisher score from the constellation model as image feature to train SVM with non-linear kernel.Possibilities include not only the constellation model. In this paper, we extend the vocabulary tree model [20] to also consider the geometrical layout of local image patches, making the algorithm jointly discriminative and generative. The reason for choosing the vocabulary tree model is due to its high efficiency in both training and recognition, and its wide adoption in large scale visual recognition problems. In this paper, by introducing a slightly different data structure and a novel prediction algorithm that turns the image-wise geometrical alignment error into per-leaf node estimation weight, we present a joint discriminative and generative vocabulary tree model for finger vein analysis. The introduced tree model presents several advantages. First, the learning process remains the same as building a conventional vocabulary tree. Second, in the prediction stage, a novel point set matching algorithm is designed for non-parametric matching of patch layout to retain the high efficiency of tree model; and third, the discriminative and generative information collaborate at the hidden leaf node layer under the Bayesian framework, and no heuristic rules are needed for fusion. Besides, we have also shown that, the proposed joint model is a generalization of tree model, where its discriminative end produces the vocabulary tree, and its generative end reduces to purely shape matching. In addition, as demonstrated by our comprehensive experimental evaluation, our proposed method obtained satisfactory accuracy on both the vein authentication and recognition problems.The vocabulary tree model is widely adopted for object recognition with local image descriptors, due to its efficiency and scalability. The advantages allow its performance to be further boosted by simply increasing the training sample size [20]. On the other side, in applications where it is difficult to provide large number of training samples, such as biometric identification or face recognition, other types of information should be explored. In this section, we introduce a method to enable a tree based model to consider both discriminative descriptor appearance and their generative geometrical layout. This is useful if the descriptors are noisy, or when the training data is insufficient.A typical vocabulary tree can be built in two steps: First a construction step that builds a tree with all descriptors from the training images; and second a registration step that creates a record-level Inverted Index (II) for each leaf node. The II records attributes of image class with at least one descriptor that reaches the leaf node. The query process works by first quantizing each descriptor to the closest leaf node. Assuming the number of leaf node is N, given the set of M descriptors X=[x1;x2;…;xM] from the query image, the tree predicts posterior Po(l|X) by factorizing it into per-leaf node estimation, i.e.(1)PolX=∑iNPlni,XPniX,where nirepresents the ith leaf node. P(ni|X) denotes the probability to observe node nigiven X, and(2)PniX=1Xhasdescriptorthatreachesnifindniasnearestneighbor,0otherwise.P(l|ni,X) is the vote from leaf ni. Simply letting P(l|ni,X)=Mi/M, where Miis the number of descriptors that reaches leaf ni, generates a strict histogram. [20] instead applied the TF–IDF scheme to calculate P(l|ni,X), where each leaf node i has a “path vector”dlifor class l. Given v the path vector from the query image,(3)Plni,X=vv−dlidlilintheIIofni0otherwise.According to Eq. (1), the voting process with a hidden leaf node layer is discriminative, and its graphical model is illustrated in Fig. 3a.Modeling the geometrical layout of local descriptors [25] is another popular line of local descriptor based object recognition approaches. The transformations f(⋅) between the query and the training images are firstly estimated, and then the one that leads to minimal alignment error signifies the detected/recognized object. This process is generative because the alignment error is used to obtain the likelihood P(X|l) such that(4)PolX=PXlPrlPX=PXlPrl∑kPXkPrk,where Pr(l) is the prior of label l. Pr(X|l) can be derived from the alignment error, e.g. assuming an a prior Gaussian distribution of descriptor location as(5)PXl=expfP−QlF2/σ,where {P,Ql} are the matched pairs of descriptor locations between the query and database image class l. Note that usually we have multiple images {Qlj},j=1,…,Klfrom the same class l. We simply take the maximal likelihood from all images of a class as the final likelihood score.It is important to have an efficient way to obtain the set of matched descriptor pairs between the query and the database images to solve f(⋅). This can be well supported by a tree model if each leaf node keeps an image-level II in addition to the record-level II as in the standard vocabulary tree. The image-level II records not only the class label, it also records from which database image each descriptor is from, as well as its 2D location in the original vein image. In this way, during the query process, when a descriptor x finds the nearest leaf node, it also finds the locations of matched image patches from the database at no additional search cost. In our model, the image-level II at each leaf node stores the following information for each image descriptor: The class label l, the database image index j, path vector dl, and the 2D locations q11In practice, if multiple descriptors from the same image get quantized to leaf node j, we will keep one of them. Also, if multiple descriptors from different images of the same Id get quantized to leaf node j, we will record the image Id in addition to the finger Id, as our spatial layout matching is per-image based (Subsection 2.2).. As illustrated from Fig. 2, our constructed tree model contains information to fully resemble the layout of image patches.Since the same tree model can implement either discriminative or generative classifier for vein recognition, it is interesting to examine if the tree can be further extended into a joint discriminative and generative model to improve performance. In this section we introduce a method to combine the two models under the Bayesian theory.In either the vocabulary tree or the generative tree-based classifier, the importance of the hidden leaf node layer has not been fully explored: In the generative tree-based classifier (Subsection 2.2), the leaf node layer does not contribute because the alignment is calculated at the image level; in the discriminative vocabulary tree (Subsection 2.1), the hidden leaf node layer is only related to a binary value for P(ni|X). In fact, the leaf node layer is a bridge between the discriminative and generative models, because the discriminative estimation from a leaf node that has been identified as an outlier in the generative alignment process should be lower-weighted. This provides the key idea in presenting our joint discriminative and generative tree model.To elaborate, to utilize P(ni|X) to evaluate the reliability of the estimation from leaf ni, the global geometrical alignment error should be decomposed into per-leaf node error. Hence, define(6)Prnil=1iflisintheimage‐levelIIofni,0otherwise.Revisiting Eq. (1), assuming the prior distribution of each leaf node Pr(ni) and each class Pr(l) to be uniform, then(7)PniX=∑kPXk,niPrnik∑jN∑kPXk,njPrnjk,=∑k∈NiPXk,ni∑jN∑k∈NjPXk,nj,whereNidenotes the set of class labels that exist in the II of leaf node ni.Taking Eqs. (6) and (7) into Eq. (1), we have(8)PolX=1C∑iNPlni,X︸Discriminative∑k∈NiPXk,ni︸Generative,whereC=∑jN∑k∈NjPXk,njis a constant to normalize Eq. (8) into a probability.Note that the termk∈Nishows that, although we are to estimate the probability of image label l, the layout can also be generated through other image labels in the II of ni. Intuitively one should expect the layout to be generated from the stated image label l only, and hence if other labels are ignored, the following simpler equation can be obtained:(9)PolX≃∑iNPlni,X︸DiscriminativePXl,ni/PXl︸Generative.P(X|l,ni) can be calculated by considering only the alignment error of descriptors quantized to leaf node ni. Based on Eq. (5),(10)PXl,ni/PXl=expfP−QlF2−fPi−QliF2/σ,where {P,Ql} and {Pi,Qli} are the matched pairs of all descriptors and those at leaf i respectively.Eqs. (8) and (9) are jointly discriminative and generative, because P(l|ni,X) is a discriminative score from Eq. (3), and P(ni|X) is the generative alignment error contributed by leaf ni. The underlining intuition is that, Eq. (3) applies the TF–IDF scheme to obtain a vote from leaf ni, and Eq. (10) further provides a confidence for this vote. For instance, if any descriptor is detected as an outlier, the estimation by Eq. (3) will be automatically lower-weighted by Eq. (10).Now the original Eq. (1) is generalized to a joint discriminative and generative approach, and we can flexibly switch between the discriminative approach, generative approach, and joint approach by the following strategies:Discriminative approach By setting σ to a sufficiently large number, P(ni|X)→1, and Eq. (10) reduces to Eq. (2) which makes Eq. (8) purely discriminative.Generative approach By setting P(l|ni,X)=Mi/M instead of Eq. (3), which is equivalent to giving each descriptor a constant weight 1/M, Eq. (8) reduces to Eq. (4) which is purely generative.Discriminative and generative approach By applying Eqs. (3) and (10) with suitable σ, Eq. (8) is a discriminative approach with generative weight.The wide adoption of the tree based fast search framework is largely due to its capacity at indexing the search database to allow highly efficient pattern search. By making use of such advantage, the vocabulary tree model presents a discriminative classification method, and in this paper, we further extend the classification process to be jointly discriminative and generative. At the same time, the tree construction process remains the same as constructing a conventional tree, and hence several existing tree construction algorithms can be applied. Below in Algorithm 2.1 we illustrate a K-mean tree construction process to obtain the joint discriminative and generative tree model in this paper. In fact, since the requirement to generate the generative weight is only to store the image-level II, in practice, many other nearest neighboring search algorithms can also be applied to index the database. Fig. 4illustrates the structure of a constructed tree, which stores both the discriminative appearance as well as the spatial layout as that logically presented in Fig. 2.

@&#CONCLUSIONS@&#
