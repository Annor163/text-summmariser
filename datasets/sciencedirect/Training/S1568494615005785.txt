@&#MAIN-TITLE@&#
Vector-based swarm optimization algorithm

@&#HIGHLIGHTS@&#
In a D-dimensional space, a random search algorithm called VBSO is introduced.Vectors with appropriate orientation gradually converge to a global optimum point.29 unimodal and multimodal benchmark functions are considered.The performance of VBSO is compared with that of a number of well-known algorithms.Simulations show that VBSO performs better than CEP, FEP, GA, PSO, GSA, DE and ODE.For several challenging cases, the performance of VBSO remains good enough.

@&#KEYPHRASES@&#
Evolutionary algorithms,Vector-based swarm optimization,Cooperation operator,Direct cooperation vector,Differential cooperation vector,

@&#ABSTRACT@&#
Evolutionary algorithms and nature-inspired optimization algorithms are widely used in solving nonlinear optimization problems. Considering a D-dimensional space, this paper introduces a new random search algorithm called vector-based swarm optimization (VBSO). In this method, vectors with appropriate orientation gradually converge to a global optimum point. In the VBSO, random weighting coefficients are used with a predetermined strategy. Using multiplication of these coefficients by suitable vectors, the randomness property is provided. Taking same conditions into account, the proposed algorithm is compared with a number of well-known intuitive algorithms. Considering 29 unimodal and multimodal benchmark functions, simulation results confirm that the VBSO performs faster and more accurate than intuitive algorithms, such as CEP, FEP, GA, PSO, GSA, DE and ODE, in most cases. To evaluate the performance of the proposed algorithm in coping with difficult situations, some challenging cases are considered. They may have large number of variables, few number of populations, few number of iterations, or may be shifted functions with huge number of optimums. Overall, the simulation results reveal that the performance of the VBSO is satisfactory for such challenging cases.

@&#INTRODUCTION@&#
Optimization is the attempt to maximize system's desirable properties while simultaneously minimizing its undesirable characteristics [1,2]. Optimization problems may become larger and more complex due to improvement of technologies in all aspects of science. Nowadays, random search algorithms are widely used in numerical and combinatorial optimization problems, instead of comprehensive space search methods. For large-size problems and problems with differential equations and/or non-linearity in objective functions, direct programming methods and comprehensive search algorithms are time consuming and may converge to local optimums. In recent years, evolutionary algorithms (EAs) and nature-inspired optimization algorithms (NIOAs), such as evolution strategy (ES) [3], evolutionary programming (EP) [4], genetic programming (GP) [5], genetic algorithm (GA) [1,6,7], particle swarm optimization (PSO) [8–10], differential evolution (DE) [2,11,12], ant colony optimization (ACO) [13–15], harmony search (HS) algorithm [16,17], cuckoo search (CS) algorithm [18,19], gravitational search algorithm (GSA) [20,21], artificial bee colony (ABC) algorithm [22,23], electromagnetism-like algorithm [24] have been increasingly employed to solve a variety of optimization problems [25–28]. Also, their advantages and disadvantages have been identified and several improved strategies have been provided [29–37]. In comparison with single-solution based methods or gradient based methods such as steepest descent algorithm, these algorithms are less likely to get trapped in local extremum. Due to this advantage, EAs and NIOAs are frequently used to solve numerical optimization problems [38,39].The motivation of this study is to propose an easy to understand, easy to use, reliable and robust optimization algorithm. The proposed algorithm should be applicable to a broad range of optimization problems and be able to suitably orient solutions towards global optimum. Also, it should not require heavy computations. To this end, a new evolutionary algorithm with high accuracy and high speed of convergence is introduced in a D-dimensional continuous space. One notable point of the proposed algorithm is that, except for one parameter, no parameters should be predefined. The proposed algorithm performs on the vectors in a D-dimensional search space, in which each vector represents a solution. Vectors with appropriate orientation gradually converge to a global optimum point. This is the main reason why this algorithm is called vector-based swarm optimization (VBSO). The main idea in VBSO is to use cooperation operator. It is divided in direct cooperation vector (VDir−C) and differential cooperation vector (VDif−C). VDir−Cis the vector which orients solutions towards the global optimum point. VDif−Cis the vector which is responsible for small-scale search around the VDir−C. The VBSO has several weighting coefficients that are chosen randomly using a predetermined strategy. Using multiplication of these coefficients by suitable vectors, the direct and differential cooperation vectors are formed. This combination guarantees the randomness property of the algorithm. To assess the advantages and disadvantages of the proposed algorithm, it is applied to various benchmark functions.This paper is organized as follows. First, a new random search algorithm called vector-based swarm optimization is introduced in a D-dimensional space. The structure of VBSO algorithm is explained in Section 2. In Section 3, standard benchmark functions and their properties are given. A new method of selecting VBSO weighting coefficients is proposed in Section 4. A comparison between the VBSO and several optimization algorithms such as DE, EM and GA is carried out in Section 5. In Section 6, the VBSO is tested on 29 benchmark functions and its performance is compared with those provided by several well-known random search algorithms. At the end, the concluding remarks are drawn.In general, a global minimization problem can be formulated as a pair (S, f), where S⊆RDis a bounded set on RDand f:S↦R is a D-dimensional real-value function. The problem is to find a point xmin∈S so that f(xmin) is a global minimum on S. More specifically, it is required to find an xmin∈S such that ∀x∈S:f(xmin)<f(x) where f must be bounded but it does not need to be continuous [40].Most of the evolutionary techniques follow the same sequence of steps [6,8,11]. In general, VBSO algorithm can be described as follows:• Step 1: Initialize a population of random vectorsAn initial population having Npopnumber of D-dimensional vectors (number of populations) is produced randomly, as follows(1)Vi,j[0]=vjlow+rand⋅(vjup−vjlow)wherei∈1,Npopis ith population vector, andj∈1,Drepresents jth element of Vivector. Considering initial population,Vi,j0is the value of jth element of ith vector. rand denotes a uniformly distributed random value in the interval (0,1),vjlowandvjupare lower and upper limits of jth element of corresponding vector, respectively. These vectors are generally called the parents of the first iteration. VBSO population is composed of a series of vectors. Each vector is a possible solution in the search space, which is gradually oriented to the optimal solution. Using addition, subtraction and multiplication operators, the orientation process is accomplished according to the fitness number of each vector.• Step 2: Calculate the fitness number of each vectorAccording to the cost function, the fitness number of each vector is evaluated at this stage.• Step 3: Generate a new population of vectors based on their fitness numbersThe aim of reproduction is to generate new solutions (offsprings). It is worth mentioning that variant EA/NIOA algorithms employ different reproduction methods.• Step 4: Go to Step 2, if the convergence condition is not satisfiedThe mechanism of VBSO reproduction is mainly based on definition of a cooperation operator. This operator is introduced by a suitable combination of multiple vectors in the search space. To form the cooperation operator, it is required to introduce direct and differential cooperation vectors.• Direct cooperation vectorConsidering ith vector of the population, the direct cooperation vector is given by an appropriate combination of cooperative vectors, as follows(2)VDir−Ck=w1⋅Vck+w2⋅Vak+w3⋅Vbk+w4⋅Vlbk+w5⋅Vrkwhere Vc, Vaand Vbare vectors denoting current solution, average of solutions and current best solution, respectively, for the kth iteration. Vlbis the best vector in the neighbourhood of ith vector [41,42] and Vris a random vector. Also,w1,w2,w3,w4,w5are cooperative weighting coefficients (CWC) in the interval (0,1). These coefficients should be selected in such a way that each element of VDir−Cis placed in the valid areaVi,j∈vjlow,vjup. Moreover, they must not be zero, simultaneously.Assumingw1=0,w2=0andw5=0, Fig. 1shows a two dimensional space problem, in which Vband Vlbproduce the direct cooperation vector. It can be seen from Fig. 1 that suitable VDir−Ccan be determined by considering proper portions of Vband Vlb. Similarly, settingw2=0andw5=0, Fig. 2shows the direct cooperation vector of three vectors, Vb, Vlband Vc. The main challenge with the direct cooperation concept is to determine the share of each cooperative vector, i.e. CWC. For variant optimization problems, CWC can take different values between 0 and 1 according to an acceptable relation, e.g.w1+w2+w3+w4+w5=2.• Differential cooperation vectorDifferential cooperation vector aims to search around direct cooperation vector. It helps algorithm to pass local optimums and converge to a global one. Differential cooperation vector in ith vector of population is calculated by considering proper portions of differential vectors, as follows(3)VDif−Ck=w6⋅Vak−Vck+w7⋅Vbk−Vck+w8⋅Vlbk−Vck+w9⋅Vrk−Vckwherew6,w7,w8andw9are the differential cooperation weighting coefficients (DCWC) in the interval (0,1). Also, the sum of DCWC is often smaller than that of CWC, e.g.w6+w7+w8+w9=1.Contrary to CWC, all of DCWC can be zero, simultaneously. Differential cooperation vector is a small-scale search vector, however, it effectively orients the current solution vector to other cooperation vectors. Assumingw6=0andw9=0, Fig. 3illustrates how VDif−Cis made using vectors Vband Vlb. Considering ith vector of population, Fig. 4and Eq. (4) show how the cooperation vector is obtained from direct and differential cooperation vectors(4)VCok=VDir−Ck+VDif−CkwhereVCokis the cooperation vector in kth iteration. As VDir−Corients solutions towards global optimal point and VDif−Cis responsible for small-scale search around the VDir−C, cooperation vector plays a vital role in the proposed algorithm.Most of EAs and NIOAs use mutation operator to increase the diversity [32,43,44]. It should be noted that inappropriate mutations may take solution vectors away from the optimal solution [7]. Because of using direct and differential cooperation vectors, VBSO algorithm has an intrinsic mutation capability. In addition, a second mutation transferring the search space origin to a point far enough from the current origin, as shown in Fig. 5, may be introduced. First, a mutation probability, mp, is chosen or randomly selected. Then, a random number is generated and if it is smaller than the mutation probability (mp>rand), the second mutation is occurred. In this case, jth element of the mutation vector in kth iteration is given by(5)Vmjk=randwhere rand is a uniform random number in (0,1). To increase the diversity and the convergence rate, the mutation vector can be defined as follows(6)Vmjk=d100randvjup−vjlowwhere d is a number depending on the number of iterations. It is equal to 1 at the beginning and gradually goes towards zero. After determining the cooperation and mutation vectors, the candidate offspring vector,Vcoffk, for the ith population vector is obtained from Eq. (7).(7)Vcoffk=VCok+VmkHaving shifted the initial coordinate system to the new one using the mutation vector, Fig. 5 demonstrates the candidate offspring vector.New vectors should be checked at each step to investigate whether they are inside the search space. This is called boundary check. Several methods have been suggested to implement boundary check [1,2]. According to the first method, ifVi,j>vjupVi,j<vjlowthe value of Vi,jwill be replaced byvjupvjlow. In the second method, if a vector element is outside the search space it will be replaced by its corresponding parent's element. In the third method, the vector element will be replaced by the corresponding Vbestif it is outside the search space. In this study, the first method is used.In order to obtain the next generation vectors from current offsprings, many selection methods have been used in EAs and NIOAs [27,32,34]. In the VBSO, the following selection methods are suggested to form the offspring vector, Voff.•Current offsprings are directly transferred to the next generation.Based on fitness numbers, the next population is selected from parents and offsprings. In this method, the parents and offsprings are evaluated and sorted in an ascending order. The first Npopvectors are transferred into the next population.Next generation vectors,Voffk+1, are generated from current vectors and candidate offspring vectors usingVoffk+1=aVk+(1−a)Vcoffk, where a is a random number between (0,1).In this study, the first and second selection methods are used. The VBSO flowchart is depicted in Fig. 6.To show the performance of the proposed algorithm and compare it with other heuristic algorithms, it is applied to several well-known benchmark functions. According to [2,20,40,45,46], the type and number of the chosen benchmark functions are important. 29 benchmark functions, f1–f23 as shown in Tables 3–8 and 10 as well as F1–F6 as shown in Table 9, are used to have a fair evaluation and comparison [20,40,45,46]. f1–f13 are large dimensional functions, f1–f7 are unimodal functions, f6 is a discontinuous step function with one minimum, f7 is a noisy quadratic function, and f14–f23 are low dimensional functions with a few local minimums. f8–f13 are multimodal functions, which are classified as the most difficult optimization problems. Also, the number of their local minimums increases with an exponential rate. To show the complexity of multimodal functions, f8, for example, is shown in Fig. 7with only two dimensions. F1–F6 are challenging benchmark functions including two unimodal, F1, F2, and four multimodal, F3–F6, functions. F2, F3, F5 are non-separable functions but F1, F4, F6 are separable ones. F3 has a very narrow valley from local optimum to global optimum whereas F4 has a huge number of local optimums [45,46]. Moreover, two important issues of the proposed algorithm are to check the convergence rate for unimodal functions and to check if the global optimum point is reached for multimodal functions.Engineering optimization problems can be divided in three main categories.•Unimodal functions with cost functions having a few local optimum points, in which the convergence rate is an important issue.Low dimensional multimodal functions, in which the key point is to be as close as possible to global optimum points.High dimensional multimodal functions with cost functions having many local optimum points. The number of local optimum points increases exponentially with problem dimensions. The key issue is to escape from numerous local optimums and converge to global ones.Due to their vital role in accuracy and convergence rate, coefficients of EAs must be selected precisely. According to problem's type and number of dimensions, the coefficients are often chosen by trial and error. The concept of parameter selection in the VBSO is rather different from that in other evolutionary algorithms. As evolutionary algorithms are based on Darwin theory of evolution and Mendel theory of genetic heritage, they are random processes. In the DE variants, for example, the required randomness is provided by using random vectors. In the VBSO, however, the randomness comes from multiplication of weighting coefficients by current, average, current best, local best, and random vectors. The VBSO algorithm has 10 weighting coefficients, 5 for the direct cooperation vector, 4 for the differential cooperation vector and 1 for the mutation probability coefficient. Therefore, the VBSO coefficients are chosen randomly with a predetermined strategy unlike other algorithms.As shown in Fig. 8, the direct cooperation vector works in a large search space and orients solutions towards the global optimal point. The differential cooperation vector is responsible for small-scale search around the direct cooperation vector. As a result, direct cooperation and differential cooperation vectors lead to ‘exploration’ and ‘exploitation’, respectively. Hence, there should be an appropriate tradeoff between them based on the number of iterations, as shown in Fig. 9.Because of their stochastic nature, a probability study of VBSO coefficients is required. To this end, four following methods are proposed and the first and fourth methods are used, in this study. According to the first method, the direct cooperation coefficients are given larger values than differential cooperation ones. Here, w1–w5 are randomly selected with a uniform probability distribution function (pdf). IfwDir−C=w1+w2+w3+w4+w5takes values between zero and two, its mean will be one. Similarly, w6–w9 are selected so thatwDiff−C=w6+w7+w8+w9takes values between zero and one. Hence,wDiff−Chas a mean of 0.5. As a result, the coefficients can be formulated as follows(8)wi=rand(0,2)ni=1,2,…,5,wi=rand(0,1)ni=6,…,9where n is the number of non-zero coefficients.The second method is useful for problems with many local optimums, which require heavier small-scale search. In this case, the coefficients are given by Eq. (9)(9)wi=rand(0,2)ni=1,2,…,5,wi=rand(0,2)ni=6,…,9The random numbers are selected with a normal pdf, in the third method. Therefore, the coefficients are obtained from(10)wi=randn(1,1)ni=1,2,…,5,wi=randn(1,1)2ni=6,…,9where randn(1, 1) is a random number with normal pdf. Also, its average and standard deviation are one. In order to avoid trapping in local optimums for multimodal problems with large number of local optimums, it is proposed to use only Vcurrent, Vbestand Vlb. The fourth method is the same as the second one withwi=0,i=2,5,6,9. The pseudo code of VBSO algorithm is as follows.Vector-based swarm optimization algorithmSet: D, Npop, number of iteration,vjlow,vjup, Iter=0Initialize population;Vi,j[0]=vjlow+rand⋅vjup−vjlow,∀i=1,2,…,Npop,j=1,2,…,DWhile (Iter≤NI) dofori=1, 2, …, NpopdoCalculate f(Vi)∀i=1,2,…,Npop,V=(Vi1,Vi2,…,ViD)Determine (Vb, Vlb, Vr, Va)Generate weighting coefficients (w1–w9)Direct cooperationVDir−Ck=w1⋅Vck+w2⋅Vak+w3⋅Vbk+w4⋅Vlbk+w5⋅VrkDifferential cooperationVDif−Ck=w6⋅Vak−Vck+w7⋅Vbk−Vck+w8⋅Vlbk−Vck+w9⋅Vrk−VckCooperation vectorVCok=VDir−Ck+VDif−CkMutationBoundary checkNext generation selectionend forIter=Iter+1end whileTo show the contribution and creativeness of the proposed algorithm, several differences between the VBSO and well-known optimization algorithms such as EM, GA and DE are mentioned in this section. The EM aims to direct sample points towards local optimizers by utilizing an attraction–repulsion mechanism. In fact, this is similar to the DE idea rather than the VBSO idea [24]. Unlike the VBSO, the main operator in the GA is crossover whereas the mutation is considered as a minor operator, typically with a low probability. Also, the mutation strategies for the GA and VBSO are totally different.The DE algorithm has 3 operators, namely crossover, mutation and selection. It uses mutation operation as a search mechanism. The DE also uses a non-uniform crossover that can take offspring vector parameters from one parent more often than it does from others. One notable difference between the DE and VBSO is that the latter has no crossover operator.Moreover, both of the DE and VBSO algorithms are created by considering two terms. They are base and difference vectors in the DE, and direct and differential cooperation vectors in the VBSO. However, these terms may be totally different and, hence, the two algorithms may perform very differently. In fact, the difference and base vectors may have very different orientations. This may lead to lower exploitation and accuracy especially for challenging functions. Furthermore, a candidate solution is formed by adding a random vector to a weighted difference between two random vectors in the mutation step of the DE. This step may not play the role of both proper orientation and small-scale search. In other words, the weighted difference between two random vectors in the DE does not guarantee a small-scale search around the base vector.On the contrary, the direct and differential cooperation vectors are made by proper portions of current, average, current best, local best, and random vectors, as shown in Figs. 1–3. Using this combination, the differential cooperation vector searches in different orientations and is able to find the proper orientation. As a result, it carries out a good small-scale search around the direct cooperation vector and, therefore, a good exploitation is achieved.In addition, evolutionary algorithms are random processes as they are based on Darwin theory of evolution and Mendel theory of genetic heritage. In the DE variants, the required randomness is provided by using random vectors. In the VBSO, however, the randomness is obtained from multiplication of random weighting coefficients by suitable vectors. As the VBSO coefficients are chosen randomly with a predetermined strategy, it is different from a generalized DE with randomized parameter values.In summary, the DE and VBSO are different in the main ideas, vectors used, operators and vector combinations and, therefore, have structural differences. In general, DE shows a good ability in global exploration, however, it is slow in exploitation of the solution [47]. The parameters of the DE are problem dependent and, therefore, it is difficult to adjust them for different optimization problems. Moreover, the performance of the DE may deteriorate significantly in case of dimensionality increase or when the premature convergence and/or stagnation occur [48]. Due to such differences, it is expected that VBSO and DE algorithms perform differently for identical optimization problems.In this section, the performance of the VBSO is assessed on 29 well-known benchmark functions. First, VBSO algorithm is applied to unimodal functions f1–f4 and multimodal functions f10–f11. The minimum objective function value for all of these functions is zero. Here, the dimension of benchmark functions, number of populations, iteration numbers and number of independent runs are 30, 50, 100 and 50, respectively. In this stage, the first selection method is employed to form the next generation. Also, the first method of weighting coefficients selection is used whereas no extra mutation is applied.Table 1shows the hardware specifications of computer used for simulations. The time duration of the first run is 0.43s, 0.43s, 0.7s, 0.44s, 0.46s, 0.49s for f1, f2, f3, f4, f10, f11, respectively.It is worth mentioning that it is crucial to consider the average value of several independent runs due to stochastic nature of random search optimization algorithms. In each run, the best value of objective function is obtained. For above-mentioned benchmark functions, the best, average and worst of best objective functions are illustrated in Fig. 10. Dotted, solid and dashed lines respectively indicate the best, average and worst of best objective functions so far. As the values of best of objective function versus iterations vary in a wide range, a logarithmic scale is used in this figure. It shows that the best, average and worst of best objective functions are relatively close to each other. Hence, it can be concluded that the VBSO works well. In particular, the cooperation operator helps the algorithm to have an appropriate orientation in its process for searching the problem space.As it is obvious form Fig. 10, the proposed algorithm has a noticeable accuracy and convergence rate. Despite having a small number of populations, global minimum points are approached in less than 100 iterations. For f11, the best values of objective function are zero after iteration 48. Obviously, a specified optimization algorithm cannot perform suitably for all optimization problems using a single set of parameters. Therefore, various strategies should be used for different types of problems. Considering all previously introduced parameter selection methods, the VBSO gives an appropriate performance for many types of optimization problems. However, the selection of a proper strategy helps the algorithm to solve such problems more efficiently.Different methods for selection of weighting coefficients, offsprings and mutation can be used to form different types of VBSO. Three of them, for example, are shown in Table 2.The performance of different types of the VBSO is compared with that given by several well-known random search optimization algorithms. The optimization algorithms used for comparison are fast evolutionary programming (FEP), classical evolutionary programming (CEP), real genetic algorithm (RGA), particle swarm optimization (PSO), gravitational search algorithm (GSA), differential evolution (DE) and opposite differential evolution (ODE). Other abbreviations used in Tables 3–10are as follows.•Best: best of best objective function valuesMedian: median of best objective function valuesStd: standard deviation of best objective function valuesS: statistical parametersD: dimensionMean: average of best objective function valuesWorst: worst of best objective function valuesF: objective functionNI: number of iterationsfmin: minimum value of objective functionTo compare the results, the proximity of a solution to the optimal solution needs to be measured. The accuracy of a solution can be measured by the values of the statistical parameters. When comparing the performance of different optimization algorithms for a minimization problem, the algorithm with the lowest mean value is the best. For unimodal functions f1–f7, the performance of different types of VBSO is compared with that of CEP and FEP. The comparison is made under same conditions and the results can be seen in Table 3[40]. The minimum point for these functions is zero. The number of populations and dimension number are 100 and 30, respectively.Due to lack of data provided by [20,40], a number of cells indicating ‘Best’ are left blank in Tables 3–8. As it is the most important statistical parameter, it is not often possible to have a fair comparison. For statistical parameters, the minimum values are in bold. Using VBSO-1 and VBSO-2, the minimum objective function values for f1, f2, f3, f4, f6 are zero (less than 10−320). In comparison with them, however, solutions given by FEP and CEP for f1, f2, f3, f4 are not good enough because they have not reached the optimal value. Also, VBSO-3 performs better than FEP and CEP for these functions. Function f5 is one of the most challenging unimodal functions used to test algorithms. As seen from Table 3, the only good result for f5 is given by VBSO-3 and results obtained from 4 remaining algorithms are not acceptable because they are far from the optimal value. For function f6, all results are good but CEP.For function f7, all types of VBSO perform better than FEP and CEP considering the proximity of solutions provided by VBSO variants to the optimal solution. In comparison with VBSO-1 and VBSO-2, VBSO-3 considers a higher weight for its differential cooperation vector. In addition to the intrinsic mutation, VBSO-3 also employs the 2nd mutation. As a result, it is able to explore the search space more accurately. Therefore, it can perform better for challenging objective functions at the cost of a slower rate of convergence.Furthermore, the performance of different types of VBSO is compared with that of CEP and FEP for high dimensional multimodal functions f8–f13. The comparison results can be seen in Table 4[40]. Except for f8, the minimum point for these functions is zero. It is −12,569.486618 for f8[2]. The number of populations and dimension number are 100 and 30, respectively.Clearly, all types of VBSO perform much better than FEP and CEP for functions f9, f10 and f11. For f13, however, VBSO-3 performs much better than FEP and CEP. Function f8 is one of the most challenging high dimensional multimodal functions. It can be seen from Table 4 that the only good solution is given by VBSO-3. Moreover, the only acceptable solutions for f12 are provided by VBSO-3 and FEP. However, VBSO-3 gives a much better solution.For low dimensional multimodal functions f14–f23, the performance of different types of VBSO is compared with that of CEP and FEP. It is worth reminding that the key point in such functions is to reach solutions as close as possible to global optimum points. Table 5 shows the comparison results [37]. Except for f15 and f20, the number of iterations for these functions is 100. It is 4000 and 200 for f15 and f20, respectively. The number of populations is 100.For f14, the minimum point has been reached by VBSO variants, however, it is not clear if FEP and CEP methods has reached the minimum point. For f15, VBSO-3 performs better than FEP and CEP. For f20, all types of VBSO have larger mean values than FEP and CEP. For functions f21−23, all types of VBSO obviously perform much better than FEP and CEP. Solutions given by FEP and CEP for f21, f22, f23 are not good at all. Also, all optimization algorithms give very good results for f16, f17, f18, f19. For f20, all types of VBSO, specially VBSO-3, perform well. As shown in Tables 3–5, the simulation results show that the VBSO is capable of coping with challenging optimization problems well. Also, it is shown that it has a superior performance in comparison with FEP and CEP algorithms. However, it might be useful to consider different conditions and compare the performance of VBSO with that of some other random search algorithms, such as RGA, PSO and GSA, for functions f1–f23. For functions f1–f7, the comparison results are demonstrated in Table 6[20]. The number of populations, independent runs, iterations and dimension number are 50, 30, 1000 and 30, respectively.For f1, f2, f6, the best results are given by VBSO-1 and VBSO-2. The results provided by RGA are not good whatsoever. Acceptable results for f3 are only given by VBSO variants. For f4, the best results are given by VBSO-1 and VBSO-2. The results obtained from RGA and PSO are not good at all. The only algorithm which can cope with f5 is VBSO-3. For f7, the best results are given by all types of VBSO.Considering high dimensional multimodal functions f8–f13, the performance of VBSO variants is compared with that of RGA, PSO and GSA and the comparison results are given by Table 7[20]. Except for f8, the minimum objective function value for these functions is zero. For f8, it is −12,569.486618. The number of populations, independent runs, iterations and dimension number are 50, 30, 1000 and 30, respectively.Acceptable results for f8 are only given by VBSO-3. Only VBSO variants can cope with f9, f11. For f10, the best results are given by VBSO variants. The results provided by RGA are not good whatsoever. The only algorithms which handle f12 are VBSO-3 and GSA. For f13, results provided by VBSO variants are acceptable, however, best results are provided by GSA and PSO.Considering low dimensional multimodal functions f14–f23, the performance of VBSO variants is compared with that of RGA, PSO and GSA and the results are given by Table 8[20]. The number of populations, independent runs and iterations are 50, 30 and 500, respectively.For f14, the results provided by GSA are not good whatsoever. Also, VBSO-1 and VBSO-2 have unacceptable mean values. The remaining algorithms lead to good results. Acceptable results for f15 are only given by VBSO-3. The results for f16, f17, f18, f19 are good and almost identical. For f20, the best results are given by RGA, PSO and VBSO-3. The worst results are obtained by GSA. The best results for f21−23 are given by VBSO-3, however, VBSO-1 and VBSO-2 give acceptable results. For these functions, the RGA gives unacceptable solutions. Also, the PSO and GSA give unacceptable solutions for f21.To select a proper VBSO strategy for different optimization problems, the following guidelines can be concluded from the simulation results. In most cases, VBSO-1 and VBSO-2 perform well for unimodal functions. As VBSO-3 considers a higher weight for its differential cooperation vector and employs the 2nd mutation, it is able to explore the search space more accurately. Therefore, it can perform well for challenging benchmark functions.When the dimension of benchmark functions is less than 100, random search algorithms often have excellent search abilities. However, the performance of many of them deteriorates quickly with increasing the dimensionality of the search space. To prove the effectiveness of VBSO when facing with high dimensional benchmark functions, it is applied to 6 challenging large-size shifted functions F1–F6[45,46]. Then, the results given by VBSO-3 are compared with those provided by DE and ODE, as shown in Table 9[45]. Due to having fbias, the minimum objective function value for these functions is not zero. Therefore, the error value, F(x)−F(xmin), is shown in Table 9.The number of populations, independent runs and dimension are 500, 25, 500, respectively. The iteration number for DE and ODE is 5000, while it is as little as 100 for VBSO. Considering the complexity of functions F1–F6, the solutions provided by DE are totally unacceptable, except for F6. For F3 and F4, the results given by ODE are not good at all. Unlike DE and ODE, VBSO-3 gives excellent solutions.Finally, an experiment is carried out for nine selected unimodal and multimodal functions to show how the idea of VBSO is different from that of the generalized DE with randomized parameters (GDE-R). In this experiment, three unimodal functions, three high dimensional multimodal functions, and three low dimensional multimodal functions are considered. The results, shown in Table 10, reveal that all types of VBSO perform better than the GDE-R.According to the simulation results, VBSO-1 and VBSO-2 perform well for most unimodal functions. Moreover, it is shown that VBSO-3 is more suitable for challenging benchmark functions, such as multi-modal and large-size shifted functions. In most experimental tests, VBSO variants have small amounts of the standard deviation. It means that the proposed algorithm demonstrates good exploration and exploitation capabilities in different independent runs. Considering large-size shifted functions, the VBSO performs well for an iteration number of 100. According to the simulation results, however, it is confirmed that the DE does not show a good exploitation for such functions for an iteration number of 5000. In other words, the VBSO performs better than the DE for challenging benchmark functions.

@&#CONCLUSIONS@&#
Considering a D-dimensional space, a new random search algorithm, called VBSO, was proposed in this paper. The novelty of VBSO algorithm was to introduce a cooperation vector. In this method, vectors with appropriate orientation gradually converged to a global optimum point. The VBSO employed random coefficients with a proper strategy. Considering different selection and mutation methods as well as various acceptable relations, 3 different types of VBSO were introduced. Taking 29 unimodal and multimodal benchmark functions into account, the results provided by the VBSO were compared with those given by several well-known random search algorithms. According to simulation results, the VBSO performed more accurate than CEP, FEP, GA, PSO, GSA, DE and ODE, in most cases. In addition, a number of challenging cases were considered to evaluate the performance of the proposed algorithm in coping with difficult situations. For such cases, it was shown that the performance of VBSO algorithm remained good enough. Overall, simulation results confirmed that VBSO-1 and VBSO-2 performed well for most unimodal functions whereas VBSO-3 is more suitable for challenging benchmark functions such as multi-modal and large-size shifted functions.