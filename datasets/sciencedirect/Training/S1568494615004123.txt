@&#MAIN-TITLE@&#
GA-based learning for rule identification in fuzzy neural networks

@&#HIGHLIGHTS@&#
GA-based approach within a three stages-learning for Fuzzy Neural Network systems.GA to identify relevant rules in a promising way from all possible fuzzy rules.Performance comparison with other 19 approaches reported in the literatures.

@&#KEYPHRASES@&#
Fuzzy rule identification,Genetic algorithm,Back-propagation learning algorithm,Mamdani-type fuzzy neural network,

@&#ABSTRACT@&#
Employing an effective learning process is a critical topic in designing a fuzzy neural network, especially when expert knowledge is not available. This paper presents a genetic algorithm (GA) based learning approach for a specific type of fuzzy neural network. The proposed learning approach consists of three stages. In the first stage the membership functions of both input and output variables are initialized by determining their centers and widths using a self-organizing algorithm. The second stage employs the proposed GA based learning algorithm to identify the fuzzy rules while the final stage tunes the derived structure and parameters using a back-propagation learning algorithm. The capabilities of the proposed GA-based learning approach are evaluated using a well-examined benchmark example and its effectiveness is analyzed by means of a comparative study with other approaches. The usefulness of the proposed GA-based learning approach is also illustrated in a practical case study where it is used to predict the performance of road traffic control actions. Results from the benchmarking exercise and case study effectively demonstrate the ability of the proposed three stages learning approach to identify relevant fuzzy rules from a training data set with a higher prediction accuracy than alternative approaches.

@&#INTRODUCTION@&#
A fuzzy neural network (FNN) is a hybrid intelligent system which combines the capability of fuzzy reasoning to handle uncertain information and the capability of neural networks to learn from processes [1,2]. FNN is a fuzzy system that uses the learning ability of neural networks to determine its parameters (fuzzy sets, fuzzy memberships and fuzzy rules) by processing data. An important topic in designing an FNN is that of identification of fuzzy rules [3–6]. The main aim when identifying fuzzy rules in FNNs is to learn and modify the rules from past experience. Various methods have been employed to identify the fuzzy rules in FNNs. Quek and Zhou [3] have classified rule identification methods into three categories: (i) Methods that use linguistic information from experts [5,7]. Although this type of approach converges faster during training and performs better, it is rather subjective. (ii) Methods that use numerical information [2,4]. These include unsupervised learning algorithms such as clustering, self-organizing and competitive learning algorithms. (iii) Methods that use supervised learning algorithms (particularly the back-propagation technique) to identify fuzzy rules in the FNNs [6,8]. These FNNs are basically multilayered, where inputs and outputs are fuzzy membership values that satisfy certain constraints. The back-propagation learning algorithm is often utilized in such an FNN to produce the mapping from inputs to outputs. In this case the FNN appears as a black box at the end of the training process.The use of genetic algorithm (GA) in the design of fuzzy systems (including rules identification, membership function adjustment, and training fuzzy rule-base models to represent specific data) has been the subject of considerably greater research effort than other approaches. This initiative has led to a fuzzy system with a learning process based on GA. GA has not only been employed to tune membership functions, but it has also been used to optimize the architecture of a FNN [9–11]. Wang et al. [9] proposed a GA-based approach for a feedback direct adaptive fuzzy–neural controller to tune online weighting factors. In particular, they have used a reduced-form GA to adjust the weightings of the FNN. A two-phase GA-based learning method for FNN is proposed in [10]; firstly to roughly estimate the optimal fuzzy weights, and secondly to provide better estimates for the shape of the membership function. Leng et al. [11] proposed a GA hybrid model for building a FNN system without a priori knowledge about the partitions of input space and the number of fuzzy rules. The GA attempts to identify and delete the least important neurons to yield a compact structure from the initial structure with a redundant architecture. The model also includes a hybrid learning method consisting of GA, back-propagation, and recursive least squares estimation for optimizing the initial network structure.A learning method for fuzzy rule-based systems using the iterative rule learning approach is proposed in [12,13]. The fuzzy rule base is constructed in an incremental fashion, where GA optimizes one fuzzy classifier rule at a time in [12], whereas the iterative genetic approach presented in [13] can include relations between variables in the antecedent of rules to improve the ability of fuzzy systems. Ishibuchi and Yamamoto [14] have proposed a GA-based approach for pattern classification problems consisting of two phases: candidate rule generation by rule evaluation measures in data mining, and rule selection by multi-objective evolutionary algorithms.In this paper an evolutionary GA based technique is employed within a three stage-learning approach for FNN systems, and we refer to here it as GAFNN. This proposed approach falls into the second category of rule identification methods. In the first stage of GAFNN, membership functions of both input and output variables are initialized by determining their centers and widths using a self-organizing algorithm. The GA based learning algorithm, the basic concept of which has been reported in our previous work [15], is performed in the second stage to identify the fuzzy rules. In the last stage, the derived structure and parameters are fine-tuned using the back-propagation learning algorithm.Starting with all possible fuzzy rules then using the GA to identify the relevant rules is a very promising approach. This approach was adopted by Castro and Camagro [16] who proposed a three stage-based approach for identifying fuzzy rules from numerical data: a feature selection process, a GA for deriving fuzzy rules and, finally, another GA for optimizing the rule base. The main disadvantage of this approach is that, depending on the number of input–output variables and the number of their fuzzy sets, the total number of possible rules can be extremely large, making it difficult to encode and generate the chromosomes. Consequently, the learning process can become overloaded. However, starting the GA process with all possible rules (if possible) is still preferred because it decreases the chance of missing any relevant rule which, in turn, minimizes the final error.The main advantage of the proposed GAFNN presented in this paper is that GA is only used to identify fuzzy rules. Using GA to optimize fuzzy membership functions and fuzzy rules simultaneously (which is avoided in our proposed method), makes the GA suffer from the curse of dimensionality, because every fuzzy rule represents a different subspace of the input variables. Another advantage of the proposed GA-based method is that the fine-tuning of the fuzzy rules weight is done in a separate learning stage (stage three). Consequently, integer representation (encoding) of the problem is used which reduces the length of the chromosome as well as making the size of the GA search space very small compared with other approaches where floating point numbers representation is implemented. The proposed GA-based learning approach is tested in a five-layer FNN with a well-examined benchmark dataset and its performance is analyzed through a comparative study with 19 other approaches reported in the literature. Furthermore, the proposed GAFNN is demonstrated in a case study which identifies appropriate road traffic control actions in the ring-road around Riyadh city. The effectiveness of the performance of GAFNN in this case study has been shown through a comparison with back-propagation NN.This section describes the structure and the function of the proposed GAFNN. The structure of GAFNN is of the Mamdani type and is also similar to the structure considered in [17] and [18]. The GAFNN has a total of five layers – the topology of which is shown in Fig. 1. Each layer performs an operation for building the fuzzy system. The input and output layers are represented as vectorsXN=x1,x2,…,xn,…,xNandYP=y1,y2,…,yp,…,yP, where N and P represent the number of input and output non-fuzzy variables, xnand ypare nth input and pth output variables respectively.Layer 1 (input layer): nodes at this layer are input nodes which represent input linguistic variables such as “age”, “weight”, and “speed” and directly transmit non-fuzzy input values to the next layer. Each node in this layer is connected to only those nodes of Layer 2, which represent the linguistic values of corresponding linguistic variables. The link weights, Wn,mbetween this layer and the next layer is unity. The outputon(1)of this layer is given as follows:(1)on(1)=xnwhere xnis the input of the input neuron Lnin Layer 1.Layer 2 (condition layer): this layer defines the fuzzy sets and membership functions for each of the input factors. Nodes in this layer act as a membership function and represent the terms of the respective linguistic variable, such as “low”, “medium”, or “high”. The input values are fed to this layer, which then calculates the membership degree. In our model this is implemented using the Gaussian membership function to ensure differentiability, ensuring compatibility with the back-propagation algorithm employed in the last stage of the learning process [18]. The connection weights in this layer are unity. The outputon,m(2)of input-label node ILn,mis given as follows:(2)on,m(2)=e−(on(1)−cn,m(2))2(σn,m2)2wherecn,m(2)andσn,m(2)are the centers (or means) and the widths (or variances) of the membership function for the input-label node ILn,mrespectively, where ILn,mdenotes the mth input label of the linguistic node n.Layer 3 (fuzzy-rules layer): this layer defines all of the possible fuzzy rules required to qualitatively specify how the output parameter is determined for various instances of the input parameters. Each node in this layer represents a fuzzy rule. The nodes in this layer perform the AND operation. The outputou(3)of a rule node RLuat Layer 3 is given as follows:(3)ou(3)=minn,mεF(on,m(2))where F is the set of indices of the nodes in Layer 2 that are connected to node RLuin Layer 3.Layer 4 (consequence layer): each node in the consequence layer represents a possible consequent part of a fuzzy rule (such as “low” and “high”). The connection weights Wu,pkof the links connecting nodes RLuin Layer 3 to OLp,kin Layer 4 represent certainty factors (CFs) of the corresponding fuzzy rules when inferring fuzzy output values. Each node of this layer performs the fuzzy OR operation to integrate the field rules leading to the same output linguistic variables. The initial values Wu,pkare set to unity. The outputop,k(4)of a consequence node OLp,kin Layer 4 is given as follows:(4)op,k(4)=maxuεG(ou(3)Wu,pk)where G is the set of indices of the nodes RLuin Layer 3 that are connected to node OLp,kin Layer 4.Layer 5 (output layer): this layer is the defuzzification layer, where each node at this layer represents a single output variable. In this layer, either the center of gravity (COG) or center of area (COA) method can be used to compute a crisp output signal for each node. In our experiment, we used COG; the output ypof an output node Dpin Layer 5 is given as follows:(5)yp=∑k∈H(op,k(4)×cp,k(4)×σp,k(4))∑k∈H(op,k(4)×σp,k(4))where H is the set of indices of the nodes OLp,kin Layer 4 which are connected to node Dpin Layer 5; andcp,k(4)andσp,k(4)are respectively, the center and width of the membership function of the output linguistic value represented by OLp,kin Layer 4. The weights of links from the nodes in Layer 4 to the nodes in Layer 5 are unity. Therefore, only the learnable weights in the GAFNN network are Wu,pkbetween Layers 3 and 4.The first stage in the proposed learning approach initializes (self-organizes) the membership functions of both input and output variables of the GAFNN by determining their centers and widths. To perform this stage, we have employed a self-organizing algorithm. Alternatively, if expert knowledge is available, it can be used in this stage. Kohonen's feature-map algorithm [19] is adopted in this work to identify the initial centerscn,m(2)andcp,k(4)of the membership functions which represent the input and output label nodes ILn,mand OLp,k. Kohonen's algorithm is a self-organizing approach which takes a set of N-dimensional objects as inputs and produces a low-dimensional (typically two dimensional) grid, called a map (Fig. 2).The inputs and the output of the GAFNN are represented as vectorsXN=x1,x2,…,xn,…,xNandYP=y1,y2,…,yp,…,yP, where N and P represent the number of the input and output variables; xnand ypare nth input and pth output variables respectively. It must be noted that the input and output vectors are non-fuzzy vectors. That is, each element in XNand YPhas a non-fuzzy value. The self-organizing algorithm adopts the following steps:Step 1: initializecn,m(2)value:(6)cn,m(2)(T)=min(xn)+12m+1M(max(xn)−min(xn))wherem∈1,2,…,M, m is the mth membership function and M is the number of membership functions that represent the terms of the respective linguistic, T is the training iteration, and variable xnis the nth element of the input vector XN.Step 2: find the closest:(7)||xn(T)−cn,closest(2)(T)||=min1≤m≤M||xn(T)−cn,m(2)(T)||Step 3: updatecn,m(2)value:(8)cn,m(2)(T+1)=cn,m(2)(T)+ε(T)xn(T)−cn,m(2)(T),ifcn,m(2)(T)=cn,closest(2)(T)cn,m(2)(T),otherwisewhere ɛ(T) is a monotonically decreasing scalar learning rate.Step 4: repeat steps 2 and 3 for T=T+1, while T<the limit on time iteration.Step 5: determine the widthσn,m(2)value:(9)σn,m(2)=cn,m(2)−cn,closest(2)2Similarly, the initial centerscp,k(4)and widthsσp,k(4)of the membership functions representing the output-label nodes can be derived, with the exception of the output vector YPthat is used as the training data instead of the input vector XN. It should be noted that the values of the centers and widths obtained here are all initial values, which will be fine-tuned in the final stage using back-propagation learning. Since the aim of this stage is to reflect the rough locations of the clusters that are formed by the input and output data samples, any other appropriate clustering algorithm (e.g. K-means) can be used in this stage.The proposed GA-based method is performed in the second stage to identify the fuzzy rules that are supported by a set of training data. A simple example of GAFNN with two input linguistic variables x1 and x2 and one output linguistic variable y is considered here to explain the design process of the presented GA-based learning approach. The self-organization learning algorithm in the first stage assigns each linguistic variable a number of fuzzy sets. Let us assume that we have three fuzzy sets {low (L), medium (M), high (H)}. Then the proposed GA-based method considers all possible rules for given fuzzy sets, as shown in the top part of Fig. 3. In this simple example, there will be a total of 27 possible rules. In fact these rules are made up of nine possible antecedents (preconditions) of fuzzy rules represented by nodes RL1,…,RL9 in the fuzzy-rules layer. Each antecedent has links with three possible decision fuzzy sets (nodes in Consequence Layer: L, M and H). For example, the three possible fuzzy rules associated with node RL1 are:Ifx1isLandx2isL,thenyisL.Ifx1isLandx2isL,thenyisM.Ifx1isLandx2isL,thenyisH.In this way the total number of rules includes all possible fuzzy rules associated with all nodes. However, at most, only one of these three fuzzy rules can be used for making decisions. We use a GA-based learning approach to identify only the appropriate and relevant fuzzy rules by filtering out all other redundant rules. A number of decisions must be made in order to implement the GA for generating appropriate fuzzy rules.Encoding: Here we propose integer strings as chromosomes to represent candidate solutions of the problem. The string is given by (g1, g2, …, gu, …, gU), where guis an integer (0≤gu≤K) which indicates the link of the fuzzy nodes RLu(i.e. nodes in the fuzzy-rules layer) with the output nodes (i.e. nodes in the consequence layer); U is the number of nodes in the fuzzy-rules layer; and K is the number of neurons in the consequence layer. In our example shown in Fig. 3, the chromosome Chihas nine integers representing gu, and 0≤gu≤3. The situation with gu=0 indicates there is no link between RLuand nodes in consequence layer; gu=1 indicates that there is a link with ‘L’ node in consequence layer and so on.Fitness function: The GA needs a fitness value assigned to each chromosome. In this paper, we use a set of training data to calculate the fitness of each chromosome based on the following error function:(10)FIT=1−1nd∑i=1nd(yi−yˆi)2where ndis the number of data, yiis the ith actual output, andyˆiis the ith model output. The second component of Eq. (10) represents the sum of mean squares errors (MSE) between actual outputs and model outputs. The GA aims to maximize this fitness function (10) in order to minimize the error value. This error value is dependent on the selected fuzzy rules.GA operators: Based on a number of experiments, we have selected GA operators and their parameters to be used for this application. The results of those experiments are given in Section 4.2. The GA operators used are the tournament selection, the elitist generation replacement, standard two-point crossover and a random mutation [20]. The mutation operator changes the integer at each position in the solution Chiwithin the allowed range (i.e. 0≤gu≤K) with a defined mutation probability. The initial population is created randomly. The stopping criterion for a GA run is to achieve the pre-specified error level.When the GA learning process is completed (e.g. when a pre-specified error level is achieved), we choose the best GA chromosome. This best chromosome is decoded to get the structure of the GAFNN by keeping only the rules that are indicated by the chromosome. A gene in a GA string with gu≠0 represents a fuzzy rule to be considered and with gu=0 to be ignored. The weight for all rules is assumed to be 1 at this stage. Then the error level (e) can be improved by using the back-propagation learning algorithm (stage three) to fine tune the rules weights. By doing so, we train the GAFNN with the relevant fuzzy rules only.After identifying the relevant fuzzy rules and the initial structure of the GAFNN, it can adjust its parameters using the back-propagation algorithm. The aim of this learning stage is to minimize the following error function:(11)E=12(yi−yˆi)2where yiis the actual output andyˆiis the model output for ith data.From the structure of the GAFNN shown in Fig. 1, it can be observed that there are only five types of adjustable parameters. These are: centerscn,m(2)and widthsσn,m(2)of input-label membership functions, and centerscp,k(4), widthsσp,k(4)of output-label membership functions and the connection weights Wu,pkof the links connecting nodes RLuin Layer 3 to OLp,kin Layer 4 (i.e. fuzzy rules weights).Once an input training vector XNis presented at the input layer during supervised learning, it is propagated forward through the neural network. Subsequently, error signalep(5)is calculated in Layer 5 and then feedback to previous Layers step by step.(12)ep(5)=−∂E∂op(5)=op−op(5)where opandop(5)are the target and actual outputs of node p in Layer 5.The error signal is used to adjust the parameters. The adjustments for the centersΔcp,k(4)and widthsΔσp,k(4)of the output labels are calculated as follows:(13)Δcp,k(4)=∂E∂cp,k(4)=∂E∂op(5)∂op(5)∂cp,k(4)=−ep(5)σp,k(4)op,k(4)∑Kσp,k(4)op,k(4)(14)Δσp,k(4)=∂E∂σp,k(4)=∂E∂op(5)∂op(5)∂σp,k(4)=−ep(5)op,k(4)cp,k(4)∑Kσp,k(4)op,k(4)−∑Kσp,k(4)op,k(4)cp,k(4)∑kσp,k(4)op,k(4)2Similarly, the centrescn,m(2)and widthsσn,m(2)of the membership functions of input-label nodes are adjusted as follows:(15)Δcn,m(2)=∂E∂cn,m(2)=∂E∂on,m(2)∂on,m(2)∂cn,m(2)=∂E∂on,m(2)on,m(2)2(on(1)−cn,m(2))(σn,m(2))2(16)Δσn,m(2)=∂E∂σn,m(2)=∂E∂on,m(2)∂on,m(2)∂σn,m(2)=∂E∂on,m(2)on,m(2)2(on(1)−cn,m(2))2(σn,m(2))3where(17)∂E∂on,m(2)=∑ueu(3)ifon,m(2)=miniεU(oi(2))0otherwisewhere U is the set of indices of the nodes in Layer 2 that are connected to node u in Layer 3.The adjustment for the connection weights Wu,pkis calculated as follows:(18)ΔWu,pk=∂E∂Wu,pk=∂Eop,k(4)op,k(4)∂Wu,pk=−ep,k(4)ou(3)In this section, we validate the performance of the GAFNN by using a well-known benchmark dataset (i.e. the Box–Jenkins time series [21]) and show the merits and capabilities of the GAFNN as compared to other models. Well-known benchmark datasets are used for the sake of easy comparison with 19 existing models published in the literature.Here, the GAFNN is applied to a non-linear system identification, using the gas furnace data (series J) of Box and Jenkins [21]. The data set used in this experiment was recorded from a combustion process of a methane–air mixture. During the process, the portion of methane was randomly changed, while maintaining a constant gas flow rate. The data set used here consists of 296 input–output pairs. The gas flow into the furnace is the input x(t) and the CO2 concentration in the outlet gas is the output y(t). The sample interval is 9s. The characteristic function of the CO2 time series data has significant deviation from the Gaussian behavior.The GAFNN is employed to provide identification of the CO2 concentration y(t). In order to compare the GAFNN results with other models, a similar experimental design has been used. It is assumed that the task is to identify the CO2 produced in a furnace y(t) at time t, given the methane gas portion from four time steps before x(t−4) and the last CO2 produced in the furnace y(t−1). The data set was converted tox(t−4),y(t−1):y(t)pairs which reduced it to 292 input–output pairs.All data sets used in this experiment were normalized using the min-max normalization technique given as follows:(19)Xn=X−XminXmax−Xminwhere Xnis the normalized value, and X, Xmin and Xmax are an instance of the minimum and the maximum values of the vector to be normalized. The normalized data was then divided into two parts with random sampling to take care of random effects. The first part 70% (204 records) was used for training the GAFNN, and the other 30% (88 records) unseen data was used for testing the trained GAFNN. Due to the stochastic nature of the approach the simulation is run 10 times, and the averaged performance index is considered (see next section).This identification problem is then mapped onto the five-layer GAFNN with the following configuration as shown in Fig. 4. The input layer consists of two nodes: x(t−4) and (t−1), whereas, the output layer consists of one node: y(t). The input and the output variables were divided into five linguistic labels (VS, S, M, L, and VL). Thus, the condition layer consists of 10 nodes and the consequence layer consists of 5 nodes. The initial fuzzy-rule layer consists of all possible combinations of input variables which, in this is case, is 25 nodes. They are fully connected with the Consequence Layer.The initial parameters (center and width) of the membership functions for all inputs and output variables were generated using the self-organizing algorithm described in Section 3.1. Fig. 5shows these initial membership functions. While other divisions of the domain regions and other shapes of membership functions are possible, we initially divide the input and output spaces into five fuzzy regions with the same Gaussian membership functions following the example presented in [17]. In this experiment we use the mean square error (MSE) as a performance index for the GAFNN:(20)MSE=1nd∑i=1nd(yi−yˆi)2where ndis the number of data, yithe actual output, andyˆiis the model output for ith data.In order to evaluate the impact of different GA operations and parameters on the GA performance, a sensitivity analysis has been carried out. In order to avoid random effects, the experimentation approach adopted involved conducting 10 runs with a particular selection of parameters and identifying the best solution (lowest MSE) over these runs, and the average of the best solutions from each of the 10 experiments. A mutation probability of 0.05 was considered to analyze how sensitive the GA performance is for two crossover types (one point and two-point) with different crossover probabilities ranging from 0.5 to 0.9. In the same way, the sensitivity of the performance of GA to mutation probability has been analyzed using a two-point crossover with a probability of 0.7 (which was found to be the best value). Finally, using a two-point crossover with a probability of 0.7 and a mutation probability of 0.05, the sensitivity of the performance of GA to different population sizes ranging from 20 to 120 has been analyzed. The peak performance of GA was achieved when two-point crossover with a probability of 0.7, mutation probabilities of 0.05 (and 0.06) and population sizes of 90 were used.The stopping criterion for a GA run is to achieve the pre-specified error level (e.g. MSE<0.001). The GA-based fuzzy rules identification method has been used to generate the fuzzy rules. Firstly, a candidate solution has been encoded into an integer string as a chromosome. The chromosome size has been set as 25, the total number of nodes in the fuzzy-rules layer. Each gene gu, where 0≤gu≤5, represents a fuzzy rule. Then an initial population containing N chromosomes has been generated randomly.

@&#CONCLUSIONS@&#
This paper has proposed a GA-based learning approach for FNNs. The proposed learning approach consists of three stages: first stage is initializing the membership functions of both input and output variables by determining their centers and widths using a self-organizing algorithm; a GA based learning algorithm is performed in the second stage to identify the fuzzy rules; in the last stage, the derived structure and parameters are fine-tuned by using the back-propagation learning algorithm. The structure of GAFNN is a Mamdani inference based FNN structure with five layers. The main features of the proposed GAFNN are the learning process of identifying the fuzzy rules and the learning process of adjusting rules weights in separate stages to ensure that only the relevant rules are trained.A well-known benchmark example was used to test the performance of the proposed GA-based learning approach. Moreover, the prediction capability of the proposed system was tested for forecasting the performance of traffic control actions on the current traffic state. Experimental results have demonstrated the ability of GAFNN to identify all the relevant fuzzy rules from the training data. Comparative analysis has shown that GAFNN has a competitive degree of prediction capability than other models.