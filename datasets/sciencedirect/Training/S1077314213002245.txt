@&#MAIN-TITLE@&#
Object detection based on spatiotemporal background models

@&#HIGHLIGHTS@&#
We propose two types of spatiotemporal background models (i.e., SLDP and StSIC).They integrate multiple different modeling approaches into a single framework.SLDP models an illumination-invariant local feature in a statistical framework.StSIC models spatiotemporal similarity of intensity changes among the pixels.Experimental results show our models are robust against various background changes.

@&#KEYPHRASES@&#
Background model,Object detection,Illumination changes,Dynamic changes,Background Models Challenge,

@&#ABSTRACT@&#
We present a robust background model for object detection and its performance evaluation using the database of the Background Models Challenge (BMC). Background models should detect foreground objects robustly against background changes, such as “illumination changes” and “dynamic changes”. In this paper, we propose two types of spatiotemporal background modeling frameworks that can adapt to illumination and dynamic changes in the background. Spatial information can be used to absorb the effects of illumination changes because they affect not only a target pixel but also its neighboring pixels. Additionally, temporal information is useful in handling the dynamic changes, which are observed repeatedly. To establish the spatiotemporal background model, our frameworks model an illumination invariant feature and a similarity of intensity changes among a set of pixels according to statistical models, respectively. Experimental results obtained for the BMC database show that our models can detect foreground objects robustly against background changes.

@&#INTRODUCTION@&#
One of the fundamental problems in computer vision is detecting regions or objects of interest from an image sequence. Background subtraction, which removes a background image from the input image, is widely used for detecting foreground objects in practical applications because it enables us to detect foreground objects without any prior knowledge. However, when background subtraction is applied to outdoor surveillance, the “long shot” scenes of cameras often include not only foreground objects but also background changes related to illumination conditions or disturbances in the scenes because the cameras are often installed in high locations to obtain a large field of view. In general, background changes that occur in outdoor scenes can be classified into two types; typical examples are shown in Fig. 1.•Illumination changes – changes relating to lighting conditions such as the sun rising, setting, or being blocked by clouds (see Fig. 1(a)),Dynamic changes – changes relating to the swaying motion of tree branches, leaves and grass, fleeting cloud, waves on water and so on (see Fig. 1(b)).To robustly detect foreground objects, we need to be able to handle these background changes. Many researchers have proposed background modeling approaches for dealing with these effects [1,2].Because illumination changes affect not only a target pixel but also its neighboring pixels, local feature-based approaches that use this characteristic have been proposed to cope with illumination changes [3–7]. The Local Binary Pattern (LBP) [5,6] is a well-known local feature for background modeling. The LBP is defined by the signed differences between a target pixel and neighboring pixels. The LBP is unaffected by local intensity changes caused by illumination changes because it is a binary pattern describing lower or higher intensity relations between neighboring pixels. The distance to a neighboring pixel depends on the scene context and should be decided on a case-by-case basis. The Radial Reach Filter (RRF) [7] extends the LBP to adaptively determine the distance. Both approaches assume that local features are unaffected by background changes. However, surveillance scenes also often include dynamic changes that significantly affect the local features in the background. It is therefore difficult for local feature-based background models to handle dynamic changes in the background.To cope with dynamic changes, statistical methods [8–13] have been used. In these approaches, the background is modeled by a probability distribution of the previously observed intensity values of each pixel. Background pixel values are usually observed with higher probabilities if we assume foreground objects are moving. When we use multiple distributions for the pixels, we can treat multi-modal backgrounds caused by dynamic changes. A Gaussian mixture model is used to represent the multiple distributions in the literature [8,9]. Non-parametric statistical methods [10–13] that use kernel density estimation have also been proposed. However, it is difficult for statistical background models to handle illumination changes, which vary intensity values rapidly and significantly.Hybrid methods [14–16], which use multiple different background models, have also been proposed. To avoid falsely classifying the object regions as background, Yoshimura et al. [14] used a local feature-based background model in addition to a model focused on each pixel, and combined the results using a logical OR operation. In contrast, to cope with both illumination and dynamic changes in the background, Shimada et al. [15] and Tanaka et al. [16] used both local feature-based and statistical background models, and combined the results using a logical AND operation. However, the methods cannot adapt to particular regions that are affected by both illumination and dynamic changes at the same time because they assume at least one of the background models employed by the hybrid methods can adapt to background changes correctly. These methods are a kind of tandem system, and a logical combination of the detection results does not improve the accuracy of the foreground detection. Zhao et al. [17] used a local feature defined by multiple point pairs that exhibit a stable statistical intensity relationship as a background model. However, their method is not suitable for online surveillance because it needs to scan the entire input sequence to analyze the stability between point pairs.To solve these problems, we integrate the methodologies of statistical and local feature-based approaches into a single framework. In this paper, we propose two types of spatiotemporal background modeling frameworks suitable for outdoor surveillance.1Our target scenes are mainly “long shot” scenes in the outdoors, and our proposed method is not intended for “close-up shot” scenes in which a foreground object is very large.1The first type [18,19] is based on a spatiotemporal local feature, where a statistical framework is introduced for an illumination-invariant local feature. The second type is based on a spatiotemporal feature, where similarity of intensity changes among a set of pixels is modeled using a statistical framework. By considering the similarity of intensity changes among the pixels, the second type can use the spatiality to handle not only illumination but also dynamic changes, while the first model uses spatial information only in defining an illumination-invariant feature to adapt to illumination changes. Our background modeling frameworks have properties of both statistical and local feature-based approaches because they use spatiotemporal information. Therefore, our spatiotemporal models can adapt to various background changes, even if some regions are affected by different types of background changes at the same time. To verify the effectiveness of our approaches, we report evaluation results obtained using the database of the Background Models Challenge (BMC21st ACCV Workshop on Background Models Challenge: http://bmc.univ-bpclermont.fr/.2).We present a spatiotemporal background model by applying a statistical framework to a local feature-based approach [18,19]. In practice, we apply a Gaussian mixture model (GMM) to a local feature called the local difference (LD) to get a statistical local feature called the statistical local difference (SLD). Finally, we define the statistical local difference pattern (SLDP) [18,19] for the background model using several SLDs.In most cases where illumination changes, there are small changes in the difference between a target pixel and its neighboring pixel because the values of pixels in a localized region increase or decrease proportionally. Owing to the invariance of the difference value with respect to illumination changes, the SLDP has the ability to tolerate the changes as shown in Fig. 2(a) because it uses the difference value as a local feature. Furthermore, our proposed method can also cope with dynamic changes because the SLDP can learn the variety of the changes as shown in Fig. 2(b). This is because a GMM, which can handle a multi-modal background, is applied to the LD, which is an important component of the SLDP. Thus, our background model can combine the concepts of statistical and local feature-based approaches into a single framework.A target pixel and its neighboring pixel in an observed image are described by the vectorspc=(xc,yc)Tandpj=(xj,yj)T, respectively.f(p)represents the image intensity at pixelp. We can then define the LD asXj=f(pc)-f(pj). In cases where illumination changes occur, the changes in the LD are small because the pixels in the localized region show a similar change. Therefore, the value of LD is stable under the illumination changes as shown in Fig. 2(a).We apply a GMM to the LD to represent probability density functions (PDF) for the LD. This gives the SLD. We define the SLDP(Xjt)(PDF for LD) at time t by(1)P(Xjt)=∑m=1Mwj,mtη(Xjt|μj,mt,Σj,mt),wherewj,mt,μj,mtandΣj,mtare the weight, the mean and the covariance matrix of the mth Gaussian in the mixture at time t respectively, andηis the Gaussian probability density. We construct the background model by updating the GMM (SLD). The updating method for the GMM is based on the method proposed by Shimada et al. [9].3This updating method [9] enables us to automatically control M (the number of Gaussian distributions) in response to background changes. In particular, for the pixels that observe background changes, M increases with the addition of new distributions. Conversely, for stable pixels whose intensity values are constant for a while, M decreases with the elimination or integration of the distributions. We can then achieve high accuracy with low computational cost compared with the conventional approach [8].3The SLD can handle dynamic background changes because its GMM can learn the variety of background hypotheses as shown in Fig. 2(b).In our method, each pixel has a pattern of the SLD in the background model; i.e., the SLDP [18,19]. The SLDP at time t is defined asSt={P(X1t),…,P(Xjt),…,P(XNt)}using a target pixelpcand N neighboring pixelspjthat radiate out frompc. Here, N represents the number of neighboring pixels (Fig. 2 shows an example forN=6). Note that all the neighboring pixels lie on a circle with radius r centered at a target pixelpc.Foreground detection using the SLDP involves a voting method that judges whether a target pixelpcbelongs to the background or the foreground. When the pattern of N LDs is given asDt={X1t,…,Xjt,…,XNt}, foreground detection based on SLDP is decided according to(2)Φ(pc)=backgroundif∑j=1Nϕ(Djt,Sjt)⩾TB,foregroundotherwise,whereTBis a threshold for determining whether a target pixelpcbelongs to the background or the foreground. In Eq. (6),ϕ(Djt,Sjt)is a function that returns 0 or 1, depending on whether the LDXjtmatches the SLDP(Xjt)at time t. For further details, we refer the reader to the literature [9].In Section 2, we presented a spatiotemporal background model (SLDP) by applying a statistical framework to a local feature-based approach. In case of the SLDP, the spatial information is used only to define local features to adapt to illumination changes. However, the spatial information is also useful in adapting to dynamic changes more robustly. Therefore, in this section, we propose another spatiotemporal background modeling, where the spatial information is used to adapt not only to illumination changes but also to dynamic changes. In other words, region-level statistical information, instead of pixel-level statistical information, is introduced.Previous statistical approaches [8–13] model the background using a probability distribution of the previously observed pixel values for each pixel (i.e., at the pixel level). Such pixel-level statistical background models cannot adapt to illumination changes that are not observed in the previous frames, and they also have difficulty handling heavy dynamic changes that result in complicated distributions far from usual dynamic changes. To solve these problems, we define a spatiotemporal background model by considering pixel clusters where all the pixels are close to one another and are similar in intensity change as shown in Fig. 3. The spatiotemporal background model can then adapt to dynamic changes and illumination changes robustly as follows.Adaptivity to dynamic changes: Each of such clusters usually consists of pixels that have a similar characteristic of dynamic changes because the characteristic of each dynamic change is associated with the place (e.g., when trees shiver in the wind, the sway of leaves causes similar changes in neighboring pixels of the trees.). Modeling the observation probability of features of the pixels within each cluster then allows the estimation of the effects of dynamic changes in each pixel, and therefore, the model can adapt to heavy dynamic changes more robustly. This is why spatial information is useful for adapting to heavy dynamic changes robustly.Adaptivity to illumination changes: By referring to similarity of intensity changes among the pixels within such clusters, we can easily identify whether each pixel of the same cluster belongs to foreground objects or the background (illumination changes) as shown in Fig. 3. This is because, in cases of illumination changes that affect not only the target pixel but also its neighboring pixels, the relationships among the pixels belonging to the same cluster are maintained essentially constant. Therefore, a feature, such as a difference among pixel values, is not affected by illumination changes, even if illumination changes affect the pixel values significantly. This is why spatial information is effective for adapting to illumination changes robustly.Using the spatial characteristics described above, we propose the spatiotemporal background modeling framework. In the rest of this section, we explain how to cluster the pixels and model the spatial characteristics.It is noted that scene points having similar surface normals show similar intensity changes against a smoothly moving distant light source. Koppal et al. divided a complex scene into geometrically consistent clusters (scene points that have the same or very similar surface normals) irrespective of their material properties and lighting [20]. The results of the literature [20] at least show that the pixels that are positionally and chromatically close to each other belong to the same cluster. Therefore, we assume that the pixels that are geographically and chromatically close to one another show similar changes against illumination changes. Under this assumption, we classify pixels into several clusters, in each of which all the pixels show similar changes against illumination changes. We employ K-means clustering to acquire the clusters.For the K-means algorithm, we use a chromatic and positional feature. We assume that, at the beginning, we can have a background image in which any foreground object does not show up. The chromatic and positional feature is defined as the quintetF={Nx,Ny,NY,NU,NV}, where Nx and Ny are the image coordinates normalized by the image size, andNY,NUand NV are the normalized pixel values in YUV color space. Using this feature, we classify the pixels into several clusters as shown in Fig. 4. Fig. 4 at least shows that each cluster contains the pixels that belong to the same part of the scene context (e.g., grass, road, sky, and walls of the buildings). Then, according to the literature [20], we regard that the pixels belonging to the same cluster show similar changes against illumination changes.We consider a target pixel at(x,y)belonging to a certain clusterC, and define an illumination invariant featureX={D,U,V}in YUV color space. Here, D is the difference between the target pixel intensity Y and the representative intensity of its clusterYr. We use a median intensity of the cluster as its representative intensityYr. We can then estimate the PDFP(Xt)at time t by kernel density estimation (KDE) using the S past samples. The proposed method employs a fast algorithm [12]4In the case of the parameter settings used for the experiments (S=250andh=9), the computation using the rectangular kernel [12] is about 3 times faster than that using a conventional Gaussian kernel [10].4of PDF estimation, which uses a rectangular function (Parzen window) as the kernel function W, instead of the Gaussian function that is often used in KDE.(3)W(u)=1hdif|u|⩽h2,0otherwise,where h is a parameter representing the width of the kernel (i.e., some smoothing parameter) and d is the dimension of the color space. Using this kernel, the PDF is represented as(4)P(Xt)=1S∑i=1SW|Xt-Xt-i|.By accumulating the PDF of the pixels belonging to the clusterC, we also estimate the PDF of the cluster as(5)PC(Xt)=1S|C|∑e=1|C|∑i=1SW|Xt-Xet-i|,where|C|is the number of pixels belonging to the clusterC. We construct the background model by updating two PDFs (i.e.,P(Xt)andPC(Xt)), and call this model “Spatiotemporal Similarity of Intensity Changes (StSIC).” The updating method for the PDF is based on a fast computation method proposed by Tanaka et al. [12]. Foreground detection using StSIC uses a threshold:(6)Ψ(Xt)=backgroundifP(Xt)⩾TSorPC(Xt)⩾TS,foregroundotherwise,whereTSis a threshold for determining whether the target pixel(x,y)belongs to the background or the foreground.StSIC is robust against dynamic changes because StSIC can learn the variety of changes that are observed repeatedly. By referring to the PDF of the cluster, StSIC can also adapt to changes happening around a target pixel. Furthermore, StSIC is robust against illumination changes that affect not only a target pixel but also its neighboring pixels. This is because StSIC classifies the pixels into several clusters so that the pixels belonging to the same cluster show similar changes against illumination changes. Therefore, StSIC can tolerate the effects of both illumination and dynamic changes by modeling the difference between the target pixel intensity and the representative intensity of its cluster.

@&#CONCLUSIONS@&#
