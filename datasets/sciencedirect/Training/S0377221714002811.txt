@&#MAIN-TITLE@&#
On the empirical scaling of run-time for finding optimal solutions to the travelling salesman problem

@&#HIGHLIGHTS@&#
New, general method for empirical scaling analysis of algorithm run-time.Run-time of best exact TSP solver scales witha·bnon random Euclidean instances.New scaling model agrees well with run-times measured for Euclidean TSPLIB instances.

@&#KEYPHRASES@&#
Combinatorial optimisation,Travelling salesman problem,Concorde,Empirical scaling analysis,

@&#ABSTRACT@&#
The travelling salesman problem (TSP) is one of the most prominent NP-hard combinatorial optimisation problems. After over fifty years of intense study, the TSP continues to be of broad theoretical and practical interest. Using a novel approach to empirical scaling analysis, which in principle is applicable to solvers for many other problems, we demonstrate that some of the most widely studied types of TSP instances tend to be much easier than expected from previous theoretical and empirical results. In particular, we show that the empirical median run-time required for finding optimal solutions to so-called random uniform Euclidean (RUE) instances – one of the most widely studied classes of TSP instances – scales substantially better thanΘ(2n)with the number n of cities to be visited. The Concorde solver, for which we achieved this result, is the best-performing exact TSP solver we are aware of, and has been applied to a broad range of real-world problems. Furthermore, we show that even when applied to a broad range of instances from the prominent TSPLIB benchmark collection for the TSP, Concorde exhibits run-times that are surprisingly consistent with our empirical model of Concorde’s scaling behaviour on RUE instances. This result suggests that the behaviour observed for the simple random structure underlying RUE is very similar to that obtained on the structured instances arising in various applications.

@&#INTRODUCTION@&#
The travelling salesman problem (TSP) is one of the most prominent combinatorial optimisation problems (Applegate, Bixby, Chvatal, & Cook, 2006; Lawler, Lenstra, Kan, & Shmoys, 1985). It has been studied for over fifty years by mathematicians, computer scientists and researchers from various other fields, largely motivated by the fact that it is conceptually simple (and can be easily explained to anyone unfamiliar with it), yet computationally very challenging. As a result, an extraordinary amount of work has been dedicated to this problem, comprising both, theoretical analyses as well as empirical investigations (Applegate, Bixby, Chvátal, & Cook, 2012; Arora, 1998; Johnson & McGeoch, 2002; Reinelt, 1994). In addition, the TSP has played, and continues to play, a pivotal role in the development of algorithmic techniques for solving hard combinatorial optimisation problems.Of the many types of TSP instances that have been studied, two-dimensional Euclidean instances represent the most commonly considered case. These instances arise in various transportation and logistics applications, as well as in the optimisation of production processes (such as drill-path optimisation in the fabrication of printed circuit boards), and can be easily visualised. TSPLIB, a collection of benchmark instances for the TSP that has been used extensively to evaluate TSP algorithms (Reinelt, 2012), contains predominantly 2D Euclidean TSP instances, and the same holds for a more recent collection of benchmark instances maintained by Cook (2012).Numerous theoretical results exist regarding the computational complexity of the TSP and various special cases (Arora, 1998; Hwang, Chang, & Lee, 1993; Papadimitriou & Steiglitz, 1982; Smith & Wormald, 1998). The general TSP is NP-hard (Garey & Johnson, 1979), and the same holds for the special case of 2D Euclidean TSP instances (Papadimitriou, 1977). However, while for the general TSP, no polynomial-time approximation algorithm exists (unless P=NP), for Euclidean distances, a polynomial-time approximation scheme is known; still, the time required for finding good solutions increases exponentially as the gap to optimality narrows (unless P=NP) (Arora, 1998). Therefore, it is commonly believed that the run-time of any exact TSP algorithm scales exponentially with instance size n, even when applied to (non-trivial) 2D Euclidean TSP instances. Smith and Wormald (1998) and Hwang et al. (1993) have established a worst-case time-complexity ofO(nn)for solving Euclidean TSP instances using geometric separator techniques1As clearly acknowledged by Hwang et al., the complexity result for the Euclidean TSP can be traced back to earlier work by W.D. Smith.1; it appears, however, that these techniques have not been exploited in any TSP solver currently available (Smith, 2009).Knowledge of the empirical complexity of the TSP is considerably sparser. State-of-the-art solvers can solve many types of TSP instances withn>1000within hours of CPU time on commodity hardware (see, e.g., Applegate et al., 2006), but relatively little is known about the scaling of their run-time with n for interesting distributions of TSP instances. Arguably the most prominent empirical scaling analysis for the TSP is found in the recent book by Applegate et al. (2006), who investigated the mean run-time required by Concorde, the state-of-the-art exact solver for the TSP, for solving random uniform Euclidean (RUE) instances. Based on a graphical analysis of empirical mean run-times observed for Concorde on large sets of RUE instances, illustrated in Fig. 16.1 of their book and reproduced in Fig. 1here, Applegate et al. (2006) observed (p. 496):The plot of mean values in Fig. 16.1 indicates that the running times are increasing as an exponential function of n, […]While the precise nature of the exponential function has not been further investigated, it is tempting to conclude that the scaling curve asymptotically approaches a straight line in the semi-logarithmic plot, indicating simple exponential scaling of the forma·bn, and that systematic deviations for small n may reflect the effects of preprocessing performed by the solver (Concorde carries out a limited number of iterations of the Chained Lin–Kernighan heuristic local search procedure during the initial stages of its computation).Concorde (Applegate et al., 2006, 2012) is of special interest, because it is currently the best-performing exact TSP solver. For example, it has been used to solve the largest non-trivial TSP instances for which provably optimal solutions are known. (As of this writing, the largest non-trivial TSP instance for which a provably optimal solution is known is TSPLIB instance pla85900, a 2D Euclidean instance with n=85,900 cities derived from a real-world circuit design application – see (Applegate et al., 2006, 2009)). Concorde is based primarily on a complex branch & cut algorithm that uses a multitude of heuristic mechanisms to achieve good performance on a wide range of TSP instances.Overall, we are interested in using empirical methods to characterise the computational complexity of problems and the performance achieved by state-of-the-art algorithms, with a focus on practically relevant combinatorial problems. We see such methods as complementary to theoretical approaches that can yield more rigorous, general results, but are often not applicable to algorithms that show state-of-the-art performance in practice. The empirical characterisations we aim for typically take the character of models that are based on the observed behaviour of an algorithm and that are capable of producing predictions that can be critically assessed using further computational experiments. Aside from their practical utility (e.g., for assessing the suitability of a given algorithm in a particular application context or guiding algorithm development), such models can in principle also inform theoretical research (see, e.g., Johnson, 2002; Sanders & Fleischer, 2001).In this work, we chose to study the empirical complexity of the 2D Euclidean TSP, primarily because of its status as a particularly prominent and well-studied NP-hard computational problem with a clearly established state-of-the-art solver of considerable practical importance, Concorde. In particular, we were interested in building parametric models for the scaling of Concorde’s run-time with instance size. We developed and thoroughly tested such models for the widely studied class of RUE instances and subsequently evaluated them on a variety of benchmark instances from TSPLIB. In doing so, we originally intended to make more precise the claim by Applegate et al. (2006), and to test to which extent it applies to more structured TSPLIB instances.To our surprise, we found that exponential scaling models of the forma·bnfor Concorde’s mean and median run-time over sets of RUE instances are not significantly more precise than a polynomial model (of degree about 3.78). A scaling model of the forma·bn, on the other hand, turned out to be surprisingly accurate, even when evaluated on instance sizes up to three times larger than those used for fitting its parameters. (To the best of our knowledge, this is the first time that such a root-exponential scaling model has been used to model the run-time of a state-of-the-art algorithm for TSP or any other NP-hard problem.) Furthermore, we found that Concorde’s run-time on RUE instances of a given size n appears to be log-normally distributed with a coefficient of variation (standard deviation over mean) that is independent of n, which implies that even high quantiles of these distributions show root-exponential scaling. Finally, we observed that the simple scaling model for Concorde’s run-time thus obtained agrees surprisingly well with the run-times observed on more structured instances from TSPLIB.The remainder of this article is structured as follows. In Section 2, we give technical details on the computational experiments that form the core of our study and describe the method we developed for conducting empirical scaling analyses in a statistically rigorous way. Section 3 presents the results from these experiments, and Section 4 provides additional discussion of those findings. Finally, in Section 5, we draw some general conclusions from our results and outline several directions for further investigation.The algorithm we study in this work is Applegate et al.’s well-known branch & cut solver, Concorde, version Concorde-03.12.19 (Applegate et al., 2012), the best performing exact algorithm for the TSP available today. Concorde makes use of an external linear-programming (LP) solver, and in the experiments reported here, we used it in conjunction with QSopt 1.01, an LP solver specifically designed to be used with Concorde. All parameters were left at their default settings, and the random number seed was always set to 23.We performed experiments on two widely studied classes of 2D Euclidean TSP instances: RUE instances, and instances from TSPLIB. RUE instances are generated by placing a number of points uniformly at random within a unit square; those points represent the cities to be visited in the corresponding TSP instance, and the distances between those are simply determined as the Euclidean distances between the respective points. For each instance size n, this random generation method induces a probability distribution over RUE instances. The RUE instances used in our study were generated using the portgen generator from the 8th DIMACS Implementation Challenge. Forn=500,600,…,2000, we generated 1000 instances and forn=2500,3000,3500,4000and 4500, 100 instances per instance size. Smaller instances are too easy for Concorde and result in floor effects, i.e., inaccurate CPU time measurements due to limited resolution of commonly used methods for process timing and distortions in scaling behaviour (as running time becomes dominated by Concorde’s preprocessing stages); larger instances require infeasibly long runs of Concorde. In addition, we have selected all instances from the TSPLIB web site (Reinelt, 2012) of 500–4500 cities that had edge types EUC 2D, CEIL 2D and ATT (these are all derived from 2D Euclidean distances); this resulted in a set of 29 instances.All runs of Concorde were performed on a cluster of identical machines, each equipped with two dual-core 2.4gigahertz AMD Opteron 2216 processors with2×1megabytes L2 cache and 4gigabytes main memory, running Cluster Rocks Linux version 4.2.1/CentOS 4. The programme was compiled with gcc-3.4.6-3, and only one CPU core was used for each run. The CPU time required by solving a given TSP instance with Concorde was directly taken from Concorde’s standard output. The run-time measurements thus obtained for individual instances were collected for each set of instances of a given size n, and descriptive statistics of the distribution of run-times over the instances in each set were computed from these data. Unless explicitly stated otherwise, we ensured that Concorde runs on all instances of a given set completed (i.e., for each instance, an optimal solution was found and proven to be optimal).Given instance sizesn1,n2,…,nk=500,600,…,1500andm=1000RUE instances per size, we used the following bootstrapping procedure to assess the predictions made by an empirical scaling model, M: For each instance sizeni, we independently drewr=1000samplesIi,1,…,Ii,r, where eachIi,jconsists of m instances and is determined by independent uniform random sampling with replacement from the full set of sizeniinstances. Then, for each of the 1000 series of instance setsI1,j,I2,j,…,Ik,j,j=1,…,1000, we fitted a parametric scaling modelMjto the corresponding observed run-times, using the nonlinear least-squares Marquardt–Levenberg algorithm as implemented in the widely used Gnuplot software. Next, for a given instance size n, we used modelsMjto obtain a set of predictionsP≔{P1(n),…,Pr(n)}, wherePj(n)is the prediction obtained from modelMj. From the set of predictions P, we determined the bootstrap percentile confidence interval for a given confidence levelαasCI≔[Q(0.5-α/2),Q(0.5+α/2)](see, e.g., Efron & Tibshirani, 1993), whereQ(x)denotes the x-quantile of the empirical distribution of the values in P; we used a standard value ofα=0.95. Confidence intervals on the model parameters were obtained analogously from the sets of parameter values for the modelsMj. Finally, we compared the actual run-times observed for the original set of instances for size n against the confidence intervals thus obtained. To the best of our knowledge, this is the first use of this type of bootstrap analysis, which has been developed by Hoos (2009) for this purpose, for characterising the empirical behaviour of an algorithm for any NP-hard problem.

@&#CONCLUSIONS@&#
The empirical analysis presented in this work demonstrates that the state-of-the-art exact TSP solver Concorde (Applegate et al., 2006) shows scaling of run-time with instance size n of the forma·bnfor the widely studied class of uniform random Euclidean (RUE) instances; this is the case for the median run-time (with constantsa≈0.21,b≈1.24), and also appears to hold for the mean and all quantiles of the distribution of run-time over instances of a given size n (although the evidence for means and high quantiles is weaker, due to unsolved instances forn⩾3000). Interestingly, the scaling model for Concorde’s run-time thus obtained is in good agreement with the run-times measured for Euclidean instances from TSPLIB, whose structure differs in many cases substantially from that of RUE instances. However, there are also classes of Euclidean TSP instances, such as the VLSI instances from the TSP webpage at http://www.math.uwaterloo.ca/tsp/index.html, for which Concorde shows significantly different scaling behaviour; we believe that this is caused by distributions of cities that give rise to many precisely identical edge lengths.From the methodological perspective, we believe that our work demonstrates (i) that the root-exponential scaling model could serve as a standard alternative hypothesis to simple exponential scaling and (ii) that the bootstrap approach used here provides a general and statistically sound way for assessing the empirical scaling behaviour for practically relevant algorithms.Considering the more theoretical side, it would be intriguing to explain analytically the scaling behaviour we have observed here. Due to the many heuristic components that are used in the actual codes, it seems unlikely that anyone would be able to produce such results for a state-of-the-art algorithm such as Concorde. However, it might be possible to prove at least the same asymptotic scaling (with much larger constants) for more abstract and simpler exact TSP algorithm that are more amenable to theoretical analysis. Previously mentioned work by Smith and Wormald (1998) could provide a good starting point for such an investigation; it may also provide a route towards building a high-performance solver for 2D Euclidean TSP instances that has theoretical worst-case time complexityO(nn).Overall, we feel that the results presented in this work challenge at least two common beliefs: (1) that the run-time for a state-of-the-art exact solver for an NP-hard optimisation problem such as the TSP should be expected to scale exponentially and (2) that empirical complexity results regarding the behaviour of a state-of-the-art solver on a class of rather simplistic instances say little or nothing about the empirical complexity of solving a diverse range of structured benchmark instances. Of course, the way in which our findings contradict both of these intuitions might be quite specific to Euclidean TSP instances and to Concorde. Even so, considering the status of Concorde as the long-standing champion of among exact TSP solvers as well as the prominence of Euclidean TSP instances, we believe them to be of broad interest and expect them to stimulate a substantial amount of further research on the TSP and other hard combinatorial problems.