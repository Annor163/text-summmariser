@&#MAIN-TITLE@&#
Heuristics and lower bounds for the simple assembly line balancing problem type 1: Overview, computational tests and improvements

@&#HIGHLIGHTS@&#
We compare 12 heuristics and 9 lower bounds for the simple assembly line balancing problem type 1.We improve the dynamic programming and the tabu search approach and identify them as best heuristics.We sharpen lower bounds by merging them and using problem reduction techniques.The success of heuristics and lower bounds strongly depends on the problem characteristics.

@&#KEYPHRASES@&#
Heuristic,Lower bound,Assembly line balancing,Reduction technique,Partitioning problem,

@&#ABSTRACT@&#
Assigning tasks to work stations is an essential problem which needs to be addressed in an assembly line design. The most basic model is called simple assembly line balancing problem type 1 (SALBP-1). We provide a survey on 12 heuristics and 9 lower bounds for this model and test them on a traditional and a lately-published benchmark dataset. The present paper focuses on algorithms published before 2011.We improve an already existing dynamic programming and a tabu search approach significantly. These two are also identified as the most effective heuristics; each with advantages for certain problem characteristics. Additionally we show that lower bounds for SALBP-1 can be distinctly sharpened when merging them and applying problem reduction techniques.

@&#INTRODUCTION@&#
Assembly lines are a common way to organise mass production of standardised products. They consist of ordered stations along a conveyor belt to which a set of tasks is assigned to. The cycle time determines how much time the stations’ workers and/or machines have to fulfil their tasks before passing on the workpiece to the next following station.The simple assembly line balancing problem type 1 (SALBP-1) is a fundamental and well-studied problem of assembly line design (Baybars, 1986; Scholl, 1999). The tasks j=1,…,n are defined by task times tjand their positions within the precedence graph. The goal is to minimise m as number of loaded stations given a fixed cycle time c. A list of all used symbols can be found in Table 1. Fig. 1illustrates SALBP-1 exemplarily. The nodes (tasks) of the precedence graph are indexed from 1 to 8 and above them stand their task times tj. For SALBP-1 a solution is feasible if (i) the tasks of each station do not have a task time sum larger than c and (ii) no direct or indirect predecessor of any task j is assigned to a later station than j is assigned to. The shaded regions identify a possible feasible solution with 4 stations. If one turns around the arrows’ directions in the precedence graph, one receives the reverse problem. A solution of the reverse problem (backward direction) is always a feasible solution of the original SALBP-1 (forward direction) after turning around the station order.Many general assembly line balancing problems (GALBPs) base on this simple logic and extend it, for example, with ergonomic considerations, space restraints and mixed-model production. Therefore algorithms should be analysed properly on their effectiveness on SALBP-1 before adapting them to more sophisticated GALBPs. Comparing the effectiveness of procedures only with the results reported in their original papers may be distorting due to different computational environments, incomparable CPU times, or different datasets. This explains the need for thoroughly conducted comparing studies. By now those exist only for some exact procedures (Baybars, 1986; Scholl & Klein, 1999), simple algorithms (Ponnambalam, Aravindan, & Mogileeswar Naidu, 1999) and priority rules (Otto, Otto, & Scholl, 2014; Scholl & Voß, 1996).SALBP-1 is NP-hard (Karp, 1972), so that heuristics are essential to obtain upper bounds for problems. Furthermore in order to assess the quality of found solutions, lower bounds methods are important in integer optimisation. Closing the research gap by a comparing study of upper and lower bounds for SALBP-1 is the first and main goal of this paper.The second goal is the improvement of some already-known procedures, namely tabu search, dynamic programming, lower bound 7 and 8, as well as SALBP-1 reduction techniques. It will also be discussed how to use problem reduction techniques for sharpening lower bounds.As benchmark dataset this study uses the collection of 269 instances from Scholl (1993) as well as the new systematically-generated 100-tasks and 1000-tasks problems from Otto, Otto, and Scholl (2013) denoted as SCHOLL, OTTO-100 and OTTO-1000, respectively, in the following. By now, there has not been thoroughly investigated in how far the success of SALBP-1 algorithms depends on the problem properties. Finding an answer to this question is the paper’s third goal.The article is organised as follows: Section 2 outlines the “General idea” of each examined heuristic briefly, states some “Experience” the author made during implementation and describes methodical improvements for some approaches. The “General ideas” require some knowledge about standard solution procedures in operational research and the “Experience” can usually not be fully understood without having read the original papers before. Section 3 explains the improvements proposed for lower bounds. Section 4 reports the computational result. Section 5 summarises and discusses the main findings.General idea: Falkenauer and Delchambre designed a genetic algorithm (GA) in which the genes are the station loads of the solution (chromosome). Thereby the genes on the chromosome are not necessarily ordered in the sequence of the stations in the final solution. Instead the precedence relations between the genes are kept in an additional genes’ precedence graph. This encoding technique is called group encoding. Falkenauer and Delchambre apply some of the usual genetic operators on the genes to obtain improved children. Thereby they use a fitness function which rewards well-filled stations more than it punishes less filled ones in a solution. After crossover the children become usually infeasible since tasks are assigned to more than one station and the precedence relations are violated. A complex healing process must follow therefore.Experience: Falkenauer and Delchambre’s proposed strategy to make children feasible (eliminating cycles in the genes’ precedence graph) needed often more than 500,000 iterations (>1minute CPU time) on OTTO-100 just to obtain one new solution. In our experiments, the time limit was often reached before repairing the children of the first crossover.General idea: Sabuncuoglu et al. introduce a genetic algorithm in which the tasks as genes are always ordered on the chromosome in a sequence that obeys the precedence graph (order encoding). They apply a crossover technique which completely avoids infeasibility. In contrast to GA-FD, chromosomes with equally loaded stations receive the highest fitness score. Additionally they apply a freezing technique which finalises the task assignments of the current first and last unfrozen station if they provide satisfying loads. It shall be noted that a better performing hybrid genetic algorithm incorporating priority rules and local search is published in Gonçalves and De Almeida (2002).Experience: Due to the smart crossover technique, GA-S is fast in creating new solutions (about 5000 per second on OTTO-100). The freezing often leads to a search stop before reaching an optimal solution or the given time limit. In those cases we revoke the algorithm with a twice that strict freezing policy. After preliminary tests we chose a population size of 100, a mutation probability of 1%, a final replacing probability of 1%, and an initial freezing parameter DPC of 10% which halves itself every time all stations are frozen.General idea: Nearchou designed a differential evolutionary algorithm (DEA) to tackle SALBP-1. We assume that learning about the tasks’ variability concerning their positions in good solutions is its main idea. The solutions (chromosomes) are sub-range encoded what can be linearly transformed into order encoding. Sub-ranges express an order position of a task by small float-point intervals between 0 and 1. For instances the solution (0.4,0.32,0.7) basing on the sub ranges0,13→1,13,23→2and23,1→3would mean an order encoding (2,1,3). Crossover works like in genetic algorithms, just mutation – which is performed in every iteration – is done differently. Given three sub-range encoded solutions xa, xb, xcof the population as vectors, one receives the mutant with xm=xc+w(xa−xb) where w denotes a small weight. Solutions after mutation and crossover need to be healed of infeasibility.Experience: Repairing infeasible solutions makes up 85% of the CPU time and leads to a low number of created solutions per second (about 60 per second on OTTO-100). For the parameters w, crossover probability and population size we opted for 0.3, 1 and 100, respectively, after some tests. Nearchou demands a replacement of some solutions when the population becomes too homogeneous but does not define how he measures homogeneity. Therefore, we replace one solution from the bottom half of the population space according to their fitness with a new random one after every five iterations.Bautista and Pereira test several versions of ant colony optimisation (ACO) for SALBP-1. Here only Bautista and Pereira’s best version (task-position policy, summed trail reading) is described and implemented. Very similar designs can be found in Boysen and Fliedner (2008) and Zhang, Cheng, Tang, and Zhong (2007). An ant colony algorithm which strongly relies on a local search is published in Bautista and Pereira (2007).General idea: Solutions are order encoded, and between each task j and each order position o exists a pheromone trail τjo. Solutions are constructed either in forward or in backward direction by adding one task after another. To select the next task for order positiono¯from all those without unassigned predecessors, a roulette wheel selection based on the task preferences is conducted. The task preference is the weighted product of the pheromone trails from order position 1 too¯, i.e.∑o=1o¯τjo, and a normalised task priority. As fitness function for updating τjo, Bautista and Pereira simply opt for the number of stations.Experience: To mention is their coarse-grained fitness function what Bautista and Pereira themselves admit as not perfect. Therefore the fitness function from Zhang et al. (2007) which rewards, like in GA-FD, well-loaded stations more than it punishes little-loaded ones is used here. Due to this change, 187 instead of 181 optimal solutions could be detected after 30seconds on SCHOLL. It shall also be mentioned that their steady normalisation of the task priorities counts for one-quarter of the CPU time but could be easily replaced by Scholl (1999) and Baybars (1986)-bandwidth scaled task priorities (not done in this study). Furthermore, restarting ACO 12 times with 15seconds CPU time on OTTO-100 provides a best solution with on average 0.08 stations less than executing ACO once with 180seconds CPU time on OTTO-100. This observation provides some evidence that the current ACO design does not explore the whole solution space sufficiently.General idea: As main feature, the algorithm attempts to find feasible SALBP-1 solutions by shifting and swapping tasks in a way which eliminates the exceeding of the cycle time (“overload”). The overload okfor station k is defined as max{0, t(Sk)−c}. That means solutions which exceed the cycle time in some stations are allowed but not those which violate the precedence relations. The last station to which any task ofPj∗is assigned to and the earliest station to which any task ofSj∗is assigned to set the boundaries within which task j can be shifted to. Two tasks can be swapped if each is within the shifting boundaries of the other one. Now to the steps of Tabu-SV: The procedure is initialised with a feasible solution of the related SALBP-2 (minimising the cycle time c given m as number of stations) with m equal to a lower bound of SALBP-1. The SALBP-2 solution is found with a simple priority rule. Afterwards, each iteration consists of 4 steps. (1) A highly overloaded station k∗ withok∗=maxk{ok}is randomly selected as “base station”. (2) For each task j in this base station, all possible shifts to and swaps with other stations are evaluated with respect to lowering the maximum station overload maxk{ok}. (3) The best move is gone and (4) the old assignment of the moved task(s) is set tabu for a certain dynamically adapted number of iterations. If through shifts and swaps no feasible SALBP-1 solution is detected after a certain number of iterations, m is increased by one. When a feasible SALBP-1 solution is found for the first time, m is decreased by one and never raised again. Additionally Scholl and Voß apply some advanced tabu search policies. Their version with EUREKA is not tested here.Experience: Our implementation – which adjusts parameters linearly depending on the time and not the iterations – detected a cycling in the Tabu-SV search for several instances. It is caused by the quasi-static selection of the base station. Therefore, Tabu-SV yielded – as similarly reported by Scholl and Voß – maximal 200 optimal solutions on SCHOLL within a few seconds but never comes further.To avoid cycling in Tabu-SV, two changes are proposed. First, the base station is now selected directly proportional to the quadratic station time overloadsok2via roulette wheel selection in 90% of the cases. Because this alone cannot avoid cycling completely, the base station is selected randomly from all stations k=1…m in the remaining cases. Second, solutions are now assessed according to the fitness function∑k=1mok2which measures the sum of the quadratic overloads over all stations. Through the changes Scholl and Voß’ conflict management, moving gap strategy and adaptive tabu length lose their influences on the results and may be omitted. For 1000-tasks instances and CPU times under 30seconds, their initial simple priority rule often provides better results than executing Tabu-SV with a random initial solution. Since this survey does not want to compare priority rules, it is started with a random solution instead. So, one ends up with a Simple-Tabu search: Beginning with a randomly generated feasible solution with m stations, the lowest-loaded station is selected, completely emptied by shifts and afterwards closed. One receives a solution with m−1 stations which obeys the precedence graph but usually not the cycle time. In each iteration a base station is selected as described above, all possible shifts and swaps for it examined in the same way as in Scholl and Voß and the best one according to the new fitness function gone. If a task j is moved from station k to station k′, task j is not allowed to move back to k for 10 iterations, i.e. the task-station combination (j, k) is tabu for the next 10 iterations. This tabu rule is only disabled if a shift or a swap brings a fitness value lower than ever reached for m−1 stations before (aspiration criterion from Scholl & Voß (1996)). When a solution with fitness value of zero is found, it is feasible for SALBP-1. Now again the lowest-loaded station is emptied and closed and the procedure continues.General idea: From Tabu-SV Lapierre et al. adopted the idea of allowing overloads but no precedence violations. Tabu-L is initialised with a randomly generated feasible SALBP-1 solution. In each iteration, it is decided at first whether to test either shifts or swaps, then a suitable station is selected, and finally the best shift (or swap) according to their fitness function is gone. The shift mainly selects little-loaded stations and tries to empty them by assigning one of their tasks to another station. The swap chooses approximately half-loaded stations and attempts to fill them by exchanging one of their tasks with a task of another station. The fitness of a solution is measured by a function which (i) rewards well-loaded stations more than it punishes little-loaded ones and (ii) penalise stations which exceed the cycle time. The change between testing shifts or swaps in an iteration and the heaviness of the punishment of overloads is managed by dynamically adapting parameters. Lapierre et al. use simple tabu rules for task reassignments and loading of stations.Experience: In our experiments the algorithm suffered on three major weaknesses. First, the shift steadily empties stations without being able to pay much attention to overloads. Thus, the overload punishment parameter p rises very fast and reaches values even exceeding the double data type (≈1.8E+308) soon. So, p becomes equal to a heavy punishment factor M what it was not designed for. Second, Lapierre et al. state that empty stations must be closed and cannot be reopened. With the neighbourhoods and parameters suggested in Tabu-L it needs usually less than 100 shifts to have less open stations than the optimal solution, and so further calculations never lead to any improvements. Therefore, our implementation always keeps m−1 stations open where m is the best upper bound found by Tabu-L so far (Scholl & Voß, 1996). Third, the tabu setting of tasks (and not task-station combinations as in Tabu-SV) for 25±5 iterations is very restrictive. So in a large number of iterations even a shift is not allowed. Because Lapierre et al. report distinctly better results for the 26 instances of the graph Scholl in SCHOLL than we could find, one may assume that they have implemented several procedures different to their description.General idea: The well-known Hoffman Heuristic (Hoffmann, 1963) seeks at first for the best load of the first station as long as none with zero-idle-time is found. Then with the remaining tasks the best load for the second station is calculated and fixed and so on. This conducted in forward and backward direction leads to exactly two solutions. Fleszar and Hindi propose a faster recursive implementation of the Hoffmann Heuristic and use a bidirectional search. Bidirectional means that at first 0,1,2,3,…,m stations are loaded in the forwards (backwards) direction and the remaining tasks are assigned in the backward (forward) direction; i.e. approx. 2m solutions in total. Additionally they propose seven problem reduction techniques as add-on. Sternatz (2014) recently published a distinctly improved version of MultiHoff which is not tested here. His enhanced MultiHoff does not enumerates tasks by decreasing tasks times but by more “intelligent” priority rules until the first zero-idle-time loads are found.Experience: The problem reduction techniques require much time for the implementation and are very prone to difficult to spot bugs. Appendix A clarifies and in some occasions corrects the description of SALBP-1 reduction techniques given by Fleszar and Hindi.General idea: Bautista and Pereira also build their Bounded Dynamic Programming on the Hoffmann Heuristic. Let us assume one knows not only one (as in the Hoffmann Heuristic) but a set of distinct partial solutions which load the first k stations, then (1) one selects at first the best b partial solutions of them according to their idle times in station k, (2) for each of these partial solutions one enumerates over the station loads for k+1 until either z zero-idle-time loads are found or the enumeration comes to an end, and (3) collects every found partial solution with k+1 stations in a pool.1The parameters b and z are called window_size and max_transitions, respectively, by Bautista and Pereira (2009).1From this pool the best b partial solutions are now selected to construct partial solutions with k+2 stations, and so forth. The procedure starts with k=1 and increases the number of stations until the first complete solution is found. Additionally, the lower bounds LB1 till LB3 are applied (see Appendix B) to reject poor partial solutions, the Hoffmann Heuristic is used as initial upper bound, duplicate partial solutions are eliminated on each stage k and the procedure repeated in the backward direction.Experience: Instead of pooling the partial solutions in a list to eliminate duplicates by pairwise comparisons, we store them in a tree following the encoding rule from Nourie and Venta (1991) what allows quicker lookups.Timed Bounded Dynamic Programming (t-Bounded-DP) builds on Bounded-DP and allows an approximation of the used CPU time.Instead of measuring the quality of a partial solution (state) in step (1) of Bounded-DP by the idle time of the station under construction, we use the total idle time of the partial solution for t-Bounded-DP. This change makes also lower bounds LB1 to LB3 quasi redundant, because solutions refused by the weak LB1 till LB3 would be normally not considered for the next stage anyway.2In our experiments, the results and times reported in Table 2 do not depend on the usage of LB1 till LB3 at all.2To lower the CPU time, LB1 till LB8 as global stopping criterion when having reached an optimum are applied and the search for an initial solution is conducted with MultiHoff instead of the Hoffmann Heuristic.In Table 2the found results are compared with those reported by Bautista and Pereira. It shall be highlighted that the average and maximum time grows almost linearly to the given parameter b with gradient 1 in our implementation and not exponentially. One can exploit this relation to predict parameter settings which are near to a given CPU time limit. For each direction it is started with a trial for b=z=10 and the required time is stopped. With this information parameter b is estimated by linear approximation in a way that let the search in forward direction end after 50% of the given CPU time limit and the search in backward direction when reaching the time limit. To avoid imprecise predictions for extreme cases, a further trial with b≔10b and z=10 is conducted if the estimated b is more than 100-fold higher than the tried one. If the estimated b is not larger than the already tried one, no further calculations are necessary. Figs. 2a and 2bshow, respectively, the really used CPU times and the average b parameters for given time targets of 10seconds on OTTO-100 and 180seconds on OTTO-1000 when only considering instances which are not stopped before finishing the final search in backward direction. The relatively high concentration between zero and five seconds on OTTO-100 is present through instances of order strength 0.9. Those instances do usually not use up the entire size b of the pool of partial solutions with k stations from which partial solutions with k+1 stations are constructed. Obviously the approximation systematically underestimates the needed CPU time. Therefore the tests are given a limit of only 90% of the targeted CPU time, e.g. 162seconds when targeting 180seconds.General idea: In principle similar to the later published Bounded-DP, Blum takes a set of partial solution with the k first stations loaded, constructs out of each of them bextsolutions with k+1 stations and collects them in a pool, and finally selects the best bbestsolutions of this pool to construct partial solutions with k+2 stations. Which tasks shall be added to station k+1 is decided similarly as in ACO. Blum uses (i) task-station policy (i.e. pheromone trails τjkbetween tasks and stations), (ii) summed trail reading (i.e.∑k=1k¯τjkas decision criterion to assign task j to stationk¯), (iii) ηjas combined priority rules processing time and number of direct successors, (iv) switches between assigning tasks according to their preferencesηj∑k=1k¯τjkdeterministically or with roulette wheel selection, and (v) applies LB1 to exclude non-promising partial solutions. For CPU times up to one minute bext=bbest=10 is used here, and for higher time limits bext=bbest=20 as suggested by Blum.Experience: The ACO components of the procedure seem to deliver useful guidance but are not the main driver of success. For instance, a 360seconds time limit with updating pheromones delivers 250 optimally solved instances on SCHOLL whereas without updating pheromones levels one detects 244. Furthermore, restarting Beam-ACO 12 times with 15seconds CPU time on OTTO-100 produces a best solution which requires on average 0.06 stations less than executing Beam-ACO once with 180seconds. This observation provides some evidence that also Beam-ACO suffers on convergence.General idea: SALOME is a station-oriented, depth-first, bidirectional branch-and-bound algorithm. Station-oriented means that branching is not done with respect to single task-station assignments but according to station loads as nodes. Thereby those station loads which do not induce raising the problem’s lower bound are explored immediately and the others are the last developed branches of the current node. The lower bounds 1, 2, 3, 5, 6 and 7 are calculated for each sub problem and LB4 together with the heads and tails once in the root (see Appendix B). Additionally, SALOME includes many logical tests like tree dominance rule (Nourie & Venta, 1991) or extended Jackson’s dominance rule to avoid branching of redundant or inferior partial solutions. Traditionally, SALBP-1 solution procedures search successively in forward and backward direction. SALOME constructs just one branch-and-bound tree and decides in each node by means of priority rules whether to look in the forward or backward direction. Despite SALOME is designed as exact procedure it can be applied as heuristic by limiting the CPU time. Finally, two recent improvements of the original SALOME design must mentioned which are not considered here. Sewell and Jacobson’s branch, bound and remember algorithm (Morrison, Sewell, & Jacobson, 2013; Sewell & Jacobson, 2012) memorises a large list of previously solved sub problems to avoid redundant computations. Vilà and Pereira (2013) develop branches in order of increasing idles times, establish a new logical test based on the maximum flow problem, and incorporate a modified version of extended duration augmentation rule (Fleszar & Hindi, 2003).Experience: The given CPU time is consumed most by the extended Jackson’s dominance rule (24%) and the lower bounds (31%) when run for 10seconds on OTTO-100. We also tested the original Pascal code from Scholl and Klein (1999) on the same computer as the new one. Thereby, the new object-oriented implementation was distinctly faster; e.g. on OTTO-100 with 180seconds time limit the average deviation to the best-known lower bound could be improved from 0.91 to 0.41.The Random Search creates order encoded solutions in forward direction by randomly selecting not-assigned tasks which do not have any not-assigned predecessors and puts them at the end of the task sequence having found by now. Additionally the simple maximum load rule (Jackson, 1956) is applied, i.e. those tasks which still fit in the current station are preferred to those which demand the closing of the current station to open a new one.Although the algorithm works with about 15,000 solutions per second on OTTO-100 quite fast, it must be noted that there is a dependency between the moment a task is added to the set of assignable tasks and the sequence position it is finally assigned to. Fig. 3demonstrates that fact. In this example the best solution (task 6 after task 5) has the least probability. To reduce this effect, the task sequences are alternating generated in forward and backward direction.Random Search assigns the next task from the list of available tasks randomly. As alternative one could apply a roulette wheel selection according to the task priorities raised to the power β=25 (β=45 for OTTO-1000 only).Before starting the search, normalised priority values are computed. Let ηrjbe the priority value for task j and rule r. For each rule r separately, the ηrjare normalised to values within the interval (Baybars, 1986; Scholl, 1999). So minj{ηrj}=1 and maxj{ηrj}=2 ∀r. Thereby the rules processing time, number of successors, number of direct successors and positional weight with priorities tj, ∣Sj∣,Sj∗and tj+t(Sj), respectively, are applied.Every single assignment of an available task j to the next order position consists of two steps. At first one of the rules r is randomly selected, and at second a roulette wheel selection overηjrβis performed to decide on the next task j.If the lower bound (LB) of a problem equals the upper bound, i.e. best-known solution value, the upper bound is proven optimal and the calculation can be disrupted immediately. Thus, strong lower bounds are valuable to assess the quality of a solution and to reduce computational time distinctly.For readers who are not fully familiar with the traditional lower bounds 1 till 7, we provide a brief summary in Appendix B based on the descriptions in Scholl (1999), Scholl and Klein (1997), and Sprecher (1999).A task’s tail estimates the minimal time required for a workpiece to be fully assembled after this task has been completed. The tail is computed as lower bound of this task’s successors adjusted by whether the task can join a station with any of its successors. Analogue, a task’s head provides the minimum station time requirement of its predecessors.Johnson (1988) introduces the traditional tails τ1j, τ2j, τ3jand τ4jderived, respectively, from LB1, LB2, LB3 and the one-machine scheduling problem.A further tail τ5jis suggested in Fleszar and Hindi (2003) based on the time requirement of the tasks which have to lie between two tasks with respect to the precedence graph: Let bej∈Pj′andλjj′the unrounded station requirement of the setSj∩Pj′containing all tasks which have to lie between task j and j′, thenτ5j=maxj′λjj′+pj′+τj′withpj′=tj′/c.λjj′equals the maximum ofτ1(Sj∩Pj′),τ2Sj∩Pj′andτ3Sj∩Pj′. We also sharpenλjj′with τ6 as described next.The new tail τ6japplies LB6 to the successors of task j and explains the necessary adjustments to check whether task j itself fits in the tail’s stations. Remember that there are three types of stations which are in the focus of LB6: d1 stations (denoted as D1) with the tasks from J(0.5,1]. d2 stations (denoted as D2) with the tasks fromJ13,0.5having not fitted in D1. And d3 stations (denoted as D3) if there exist tasks from J[q, 1−q] which do not fit in the stations from J(0.5,1−q] and D2, i.e.d3=maxLB1(J[q,1-q])-d2-|J(0.5,1-q]||q∈J0,13. So LB6=d1+d2+d3. If d3=0, all stations are loaded to more than one-third when D2 contains an odd number of tasks and to more than one-half when D2 contains an even number of tasks. That means after the correction, which incorporates the lowest possible load for the tail’s first station, one obtains a tailτ6j(d3=0)=⌈LB6(Sj)⌉-13,D2containsevennumberoftasksandD1=∅⌈LB6(Sj)⌉-0.5,D2containsevennumberoftasksandD1≠∅⌈LB6(Sj)⌉-23,otherwise.A more interesting case is d3>0 where there is at least oneq∈0,13in which the unroundedd3(q)=∑j∈J[q,1-q]pj-d2-|J(0.5,1-q]|>0. This equation says that there are γ1(q)=⌈d3(q)⌉+d2+∣J(0.5,1−q]∣ stations (denoted as Γ1) needed for the tasks J[q,1−q] and, of course, also γ2=∣J(1−q,1]∣ separate stations (denoted as Γ2) for the tasks from J(1−q,1]. That means in the worst case, the first station of Γ1 has an idle time ofγ1(q)-∑j∈J[q,1-q]pj=⌈d3(q)⌉-d3(q)since (i) all other stations are filled completely and (ii) J[0, q)=∅. About the stations belonging to Γ2, one knows that they are loaded at least with the station requirement min{pj∣j∈J(1−q,1]} of the smallest task larger than 1−q or do not exist if J(1−q,1]=∅. Nowd3′=maxd3′(q)|q∈J0,13withd3′(q)=⌈d3(q)⌉-max{d3(q)-d3(q),1-min{pj|j∈J(1-q,1]}}; where ⌈d3(q)⌉−d3(q) is the correction term for Γ1, 1−min{pj∣j∈J(1−q,1]} the correction term for Γ2, and the larger one of both is applied. It followsτ6j(d3>0)=d1+d2+d3′.LB7 takes the d+1 shortest of the d·m+1 tasks with the highest processing time and tests whether they fit in one station according to LB1, otherwise increases m by one. We strengthen this bound by trying to reject every station load not just with LB1 but also with the precedence graph. The next two paragraphs explain LB7a for the special case d=1 at first.Pseudocode 1LB7a.Function LB7a(m)Forh=1 to 15GetTash′th smallest task tupleIfTfulfils (i) Then Returnm+1IfTdoes not fulfil (ii) and (iii) Then ReturnmEndForReturnmLet be 〈j1,j2,…,jm,jm+1] the set of the m+1 largest tasks of a SALBP-1 instance ordered by increasing task times. Then (j1, j2) is the pair with the smallest task time sum, (j1, j3) with the second smallest, (j1, j4) or (j2, j3) with the third smallest, (j1, j4), (j1, j5), (j2, j3) or (j2, j4) with the fourth smallest, and so forth. In general, a task pair(jr1,jr2)can be the h’th smallest if r1+r2⩽h+2. Therebyjriis the task with the rihighest task time. Exploiting this observation, it is not time consuming to compute the 153We suggest 15 as subjective trade-off between decreasing additional effectiveness of LB7a and exponentially increasing CPU time.3smallest task pairs from 〈j1,j2,…,jm,jm+1].A task pair T=(j, j′) withj∈Pj′does not fit in one station if (i)tj+tj′>c, (ii)Lj<Ej′or (iii)tj+t(Sj∩Pj′)+tj′>c. Condition (ii) and (iii) reject only the pair (j, j′) as possible load, condition (i) rejects (j, j′) and every larger pair. Pseudocode 1 shows the behaviour of LB7a.The general case with d⩾1: Let be 〈j1, j2,…,jdm, jdm+1] the set of the dm+1 largest tasks ordered by increasing task times andT=(jr1,jr2,…,jrd,jrd+1)a tuple with tasks from this set. T can be the h’th smallest tuple if∑i=1d+1ri⩽h+∑i=1d+1i-1. Using pseudocode 1 again, one increase m by one if (i)∑j∈Ttj>c, and rejects T as load if (ii)Lj<Ej′for any task pair (j, j′) withj∈Pj′and j, j′∈T or (iii)∑j∈Ttj+t(⋃j,j′∈TSj∩Pj′)>c.A new lower bound is proposed by Fleszar and Hindi (2003), again, based on the time requirement of the tasks which have to lie between two tasks to obey the precedence relations. The following is tested for all station pairs (k1, k2) with 1⩽k1⩽k2⩽m: LetBk1,k2be the set of all tasks j which fulfil the inequalities k1⩽Ej, Lj⩽k2, then all tasks inBk1,k2must be allocated among the stations k1,…,k2. ForBk1,k2, none of the lower bounds 1 till 3 is allowed to exceed k2−k1+1 stations. Otherwise, the lower bound m can be increased by 1.Lower Bound 8 separates numerous subproblems from the original graph and allows therefore an easy integration of every other bounding technique. We sharpen LB8 by (i) using LB6 instead of LB1, LB2 and LB3 to calculate the station requirement ofBk1,k2(→LB8a), (ii) additionally applying τ6jto receive the heads and tails of task j (→LB8b), (iii) additionally calculating LB7a ofBk1,k2(→LB8c) and (iv) additionally using τ5j(→LB8d).Fleszar and Hindi (2003) remark that reduction techniques have a significant impact on lower bounds. Their reduction techniques are initialised with a strong upper bound m and try to simplify the problem by assuming that there exists a feasible solution with m-1 stations. If (i) a lower bound calculated for the reduced problem is higher than m−1 or (ii) one task is never part of a feasible packing which does not exceed the total idle time, the assumption is revealed as false by contradiction.We exploit this mechanism by assuming that there exists a feasible solution with an amount of stations equal to the best-known lower bound and applying the reduction techniques. If a contradiction occurs, the lower bound is increased by 1 and reduction revoked (destructive improvement); otherwise the lower bound is accepted. LB6 is tested after every single reduction and the time-consuming LB8c only after no further reduction can be found for the present lower bound.The algorithms were coded in VB.NET x64 release and ran on a 2.8gigahertz processor with 4gigabytes RAM. Direct and indirect predecessors and successors were saved in a list as well as a 0–1 matrix and created before starting the stopwatch.Table 3shows the computational results on all 3 datasets. The results for SCHOLL are compared with the optimal solution values and for OTTO-100/1000 with the best-known lower bounds.Random Search distinctly outstripped GA-FD and Tabu-L, as well as yielded results in the proximity of DEA and GA-S. Random Task Priority Search was slightly superior to ACO and reached MultiHoff on SCHOLL and OTTO-100 in the long run. MultiHoff always finished before reaching the lowest given time limits and was distinctly boosted on OTTO-100/1000 when using reduction techniques. The changes on Tabu-SV towards Simple-Tabu could break cycling, double the number of iterations, and thus improve results. For larger CPU times Simple-Tabu and t-Bounded-DP together clearly dominated all other heuristics (except of SALOME for instance 297 from OTTO-100).OTTO-100/1000 can be grouped by three properties. The graph structure contains chains of tasks which have only one successor each (CH), bottleneck tasks with many direct predecessors and successors (BN), or no willingly constructed features (MIX). The task times are normally distributed with a peak at 0.1c (BOT), with a peak at 0.5c (MID), or bimodal with peaks at 0.1c and 0.5c (BI). The order strength is 0.2, 0.6 or 0.9. Table 4shows the results split into all existing combinations of these properties for Simple-Tabu and t-Bounded-DP on OTTO-100/1000 with a time limit of 3600seconds. Simple-Tabu delivered strong results for MID instances, especially those with low order strength; whereas t-Bounded-DP worked better in all other cases. The average deviation to the best-known lower bound is 0.2/3.18 and 0.21/6.19 for Simple-Tabu and t-Bounded-DP, respectively, for 3600seconds CPU time on OTTO-100/1000.Table 5contains the results for LB1 till LB8. “Found LBMax” counts the number of cases where no other lower bound produced a better result, “Unique LBMax” where no other lower bound computed an equal or better result, and “Found optima” where the lower bound reached the optimal solution value. The column LBMax gives the results when combining LB1 till LB8 and avoiding redundant calculations. LB5, LB7 and LB8 were initialised with LB1. It shall be highlighted that LB1 found always the currently best-known lower bound for any combination of BOT or BI with order strength 0.2 or 0.6 in OTTO-100/1000. All “unique LBMax” of LB6 had the task time distribution MID and of LB8 the order strength 0.9.Table 6summarises the results for the improved versions. Thereby LB7a, the improvements in LB8a, LB8b and LB8c, as well as the application of reduction techniques paid off. LB8d’s calculation of the setsSj∩Pj′was time consuming but did not breed success. All improvements for OTTO-100/1000 were for instances with task time distribution MID.Combining LB8c with reduction techniques lead to strong improvements as displayed in the column LBR. Table 7compares LBMax and LBR with the best-known upper bound4Based on the results from our experiments and those reported in Morrison et al. (2013).4grouped by problem characteristics.

@&#CONCLUSIONS@&#
