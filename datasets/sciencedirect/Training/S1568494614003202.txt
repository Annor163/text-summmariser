@&#MAIN-TITLE@&#
Hybrid meta-heuristic optimization algorithms for time-domain-constrained data clustering

@&#HIGHLIGHTS@&#
General formulation of time-domain-constrained data clustering.A hybridization of the global optimization with Nelder–Mead method to produce an effective and efficient search algorithm.A set of computational experiments on synthetic signals shows the good performance with respect to fuzzy piecewise linear regression models.A novel application to determine the cut-off time in survival analysis.

@&#KEYPHRASES@&#
Time series clustering,Segmentation of multivariate time series,Nelder–Mead simplex search method,Particle swarm optimization,Genetic algorithms,Simulated annealing,

@&#ABSTRACT@&#
This paper addresses the question of time-domain-constrained data clustering, a problem which deals with data labelled with the time they are obtained and imposing the condition that clusters need to be contiguous in time (the time-domain constraint). The objective is to obtain a partitioning of a multivariate time series into internally homogeneous segments with respect to a statistical model given in each cluster.In this paper, time-domain-constrained data clustering is formulated as an unrestricted bi-level optimization problem. The clustering problem is stated at the upper level model and at the lower level the statistical models are adjusted to the set of clusters determined in the upper level. This formulation is sufficiently general to allow these statistical models to be used as black boxes. A hybrid technique based on combining a generic population-based optimization algorithm and Nelder–Mead simplex search is used to solve the bi-level model.The capability of the proposed approach is illustrated using simulations of synthetic signals and a novel application for survival analysis. This application shows that the proposed methodology is a useful tool to detect changes in the hidden structure of historical data.Finally, the performance of the hybridizations of particle swarm optimization, genetic algorithms and simulated annealing with Nelder–Mead simplex search are tested on a pattern recognition problem of text identification.

@&#INTRODUCTION@&#
The problem of time-series segmentation means partitioning a time series in K-time segments that are internally homogeneous [1]. Time-series segmentation has been applied in a wide range of fields such as signal analysis [2,3], industrial process monitoring [4–6], time series DNA micro-array analysis [7], loading identification for stable operation of thermal power units [8], automatic segmentation of traffic patterns [9,10], human motion analysis [11], geophysics environmental research [12], among others. The desired goals depend on the specific application and aim to locate stable periods of time, to identify changing points, or simply to express the original time series in a compact way.One of the most widely used methods for dealing with time-series segmentation problems is cluster analysis. Clustering methods refer to the process of dividing a set of objects into groups so that members of the same group are similar to each other and different from members of the rest of the groups. The clustering analysis problem can be formulated as one of optimizing a loss (or merit) function subject to a set of constraints. This approach is very versatile and has allowed additional information to be added to the clustering process by adding new constraints or modifying the objective function. This formulation allows the inclusion of information about the shape of the clusters, the distribution of data and the presence of noise and outliers. In this paper we consider constrained time-dependent clustering analysis. This approach enriches the cluster analysis by adding the time at which the data is obtained to the set of constraints and taking into account the fact that neighbouring observations in time may belong to the same cluster.K-means and Fuzzy c-Means (FCM) clustering methods have been adapted to solve multiple clustering analysis problem variants [13,14]. These methods do not incorporate a time structure to obtain the clustering. Steinley and Hubert [15] adapt the K-means algorithm to situations where observations are put in order (for instance, by the time instant they are collected). These authors propose a two-stage procedure, in the first stage an initial object order is identified through an auxiliary quadratic assignment optimization heuristic and in the second stage, dynamic programming recursion is applied to optimally subdivide the object set subject to the order constraints. This paper shows that the use of an order-constrained K-means strategy improves interpretability and cluster recovery. These solutions are very similar, in terms of sum of square errors (SSE), to the solution obtained without time structure.Łęski and Owczarek [2] adapt Fuzzy c-Regression Models (FCRM) described in Hathaway and Bezdek [16] to this problem and propose Time-Domain-Constrained Fuzzy c-Regression Models (TDC-FCRM), and an insensitive version for outliers of this method, the so-called ɛ-Insensitivity in Time-Domain-Constrained Fuzzy c-Regression Models (ɛTDCFCRM).In this paper we deal with the general problem of time-domain-constrained clustering analysis in which the observations obtained in each subinterval are generated by an arbitrary statistical model, but not necessarily a linear regression model as the references above consider.A promising field of research in clustering is the hybridization of evolutionary algorithms. Evidence of this is the literature review by Abul Hasan and Ramakrishnan [17] which contains more than 125 items where these algorithms are applied to cluster analysis. Hybrid algorithms try to make full use of the merits of various optimization techniques to obtain an efficient method for finding global optima over wide areas. So-called Particle Swarm Optimization has been successful in solving clustering problems [18]. The advantages of PSO algorithms can be summarized as follows: they avoid local optima, do a search in the entire solution space, are robust with respect to initialization parameters, viable, efficient with a smaller computational burden and it is simple to choose the correct parameter values. Table 1presents some hybrid algorithms, paying special attention to methods based on PSO, that are available in the literature. An exhaustive review is beyond the scope of this paper but it can be found in Abul Hasan and Ramakrishnan [17] and Rana et al. [18]. The main motivations of these methods are similar to optimization algorithms and they are essentially three: (i) to speed the process up, (ii) to improve the quality of the solution and (iii) to adapt the algorithm to a specific application.Fan and Zahara [35] propose the hybridization of the Nelder–Mead (NM) method and PSO for unconstrained optimization. They prove that the hybrid algorithm NM+PSO is a strategy that achieves an excellent trade-off between computational burden and quality of the solution. In Kao et al. [19] NM+PSO algorithm is applied to cluster analysis and K-means algorithm is used as an initialization process. This hybrid algorithm is called K-NM-PSO. In the numerical experiments reported it is shown that K-NM-PSO carries out a smaller number of iterations than PSO, NM-PSO, K-PSO and K-means for a similar level of error.In Vakil Baghmisheh et al. [36] a hybrid PSO is proposed, the so-called PS-NM algorithm, to the problem for crack detection in cantilever beams. This hybridization strategy is different from that given in Fan and Zahara [35] and Kao et al. [19]. These authors, at each iteration, initialize NM with the simplex defined by the N+1 best particles and update the rest of the population using a modified PSO method. The solution obtained by the NM algorithm replaces the N+1th best solution in the next iteration, while PS-NM applies the NM method to the best solution found by the PSO.In this paper we applied the hybridization framework given in Espinosa-Aranda et al. [37] to the time-domain-constrained data clustering. This framework is general enough to be applicable to the hybridization of algorithms that have been successful in cluster analysis such as PSO, simulated annealing and genetic algorithms. One iteration of this method applies several iterations of population-based algorithm until achieving ncimprovements of the objective function. Then, a fixed number of iterations of the NM algorithm is carried out, starting from the best found solution. The proposed algorithm combines both the global search ability of a population-based algorithm and the local search efficiency of NM. The parameter ncplays the role of a threshold that guarantees being in a new neighbourhood of solutions that can then be locally explored with NM. These algorithms can be described in the exact optimization framework given in García et al. [38], where the algorithm used in the column generation phase (in this case a population-based algorithm) is accelerated by means of the algorithm for solving the so-called restricted master problem (in this case NM). This algorithmic class has achieved remarkable results in network flow optimization problems [39] and global optimization [37].The contributions of this paper can be summarized as follows:-A general formulation of time-domain-constrained data clustering is described based on unconstrained bi-level optimization. This formulation is enough to consider any statistical model describing data inside each time cluster.An assessment of a class of algorithms, based on hybridization of a global optimization algorithm such as PSO, genetic algorithms or simulated annealing and the NM algorithm for the proposed model. The advantage of hybridization is illustrated by a hard problem of pattern recognition. An efficient hybridization based on a problem-oriented simulating annealing algorithm is described.A set of computational experiments has been carried out on synthetic signals to show that the proposed general methodology obtains results comparable to those described in the literature for linear regression models defined in time segments.The survival studies analyze which factors significantly affect the mortality of a given population. For this purpose they consider a priori a cut-off time value and through Multivariate logistic regression models or Univariate Cox regression analysis they assess which factors significantly affect the prediction of mortality. This cut-off time value is chosen a priori. For instance it is common to make use of 24-hour mortality [40,41], 30-day mortality [42], 6-month mortality [43], and long-term mortality (more than 4 years [44]). In this paper time-domain-constrained cluster analysis is applied to identify these cut-off values and this application has been tested on a survival analysis in a study of hip surgery.The rest of this paper is organized as follows. Section ‘Mathematical modelling of the time-domain-constrained data clustering problem’ presents a detailed description of time-domain-constrained data clustering problem. Section ‘Hybrid meta-heuristic algorithm for the TDC problem’ discusses hybrid methods based on particle swarm optimization, genetic algorithms and simulated annealing with a NM method. Section ‘Numerical experiments’ provides numerical experiments and illustrates the efficiency of the hybrid approach in solving unconstrained optimization problems. Finally, the conclusions of the paper are drawn in section ‘Conclusions’.Suppose we have a data set,{(xi,yi)}i=1Nwherexi∈ℝQis a vector of explanatory variables andyi∈ℝPis a vector of dependent variables. Indeed, data pairs (xi, yi) are labelled with the time instant tiat which each pair has been obtained. We assume the data are time ordered, this is, ti<ti+1 where i=1, …, N−1. We also assume that data pairs are drawn from the following regression model(1)yi=F(xi,ti;Θk)+ei;i=1,b,…,Nwhere eirepresents the error term modelled through a random variable with zero mean and Θkdenotes the vector of (unknown) parameters of the model that works for a specific period of time k. In this paper we address the problem of cluster analysis taking into account the time instant at which the different observations have been taken. The goal is to find K disjoint time segments (clusters) so the goodness-of-fit of regression model (1) to the observations in each segment will be the best. Thus, the mathematical model of the clustering problem can be written as follows:(2)minimizeJ(u,Θ)=∑k=1K∑i=1NuikL(xi,yi,ti;Θk)subject to:∑k=1Kuik=1;i=1,…,N∑i=1Nuik≥1;k=1,…,Kui−1,k−uik+ui+1,k≤1;k=1,…,Ki=2,…,N−1uik∈{0,1}where L is a loss function,Θ=(Θ1,Θ2,…,ΘK)∈ℝQ×Kis the matrix formed by all the parameter vectors of the regression models and u=(uik)∈{0, 1} are the membership variables that take the value uik=1 if the datum i is assigned to the time segment k and uik=0, otherwise. The first constraint requires that the object i must be assigned to some time period k, the second forces all time periods to have observations, the third asserts that all observations in the same cluster were obtained consecutively in time and the last imposes the binary nature of the variables uik. The usual loss function L is the sum of squared errors which is given by(3)L(xi,yi,ti;Θk)=||yi−F(xi,ti;Θk)||2where ||·|| means the Euclidean norm.The formulation of the time-domain-constrained clustering analysis problem (2) has the disadvantage of being an integer non-linear optimization problem. This kind of problem is generally difficult to solve which leads us to reformulate the model (2). Suppose T=[a, b] is the time horizon to be partitioned. Consider the boundary points a=s0<s1<s2<⋯<sK−1<sK=b (see Fig. 1) as decision variables. Then we define the setCk(s)=i∈{1,…,N}/sk−1≤ti<sk;k=1,…,KThe vectors=(s1,…,sK−1)∈ℝK−1is the complicating variable of the problem, since when it is fixed, the optimization problem (2) is separable in K independent parameter setting problems. Then the objective function in (2) can be reformulated in terms of s as(4)J˜(s):=MinimizeΘ∑k∑i∈Ck(s)L(xi,yi,ti;Θk)=∑kJ˜k(s)whereJ˜k(s)is given by the following statistical estimation problem of the unknown parameter Θk(5)J˜k(s):=MinimizeΘk∑i∈Ck(s)L(xi,yi,ti;Θk)Eq. (5) defines the lower level problem. Observe that Ck(s) can be the empty set or have an insufficient number of observations for the estimation problem (5) to be well posed. In these cases we consider thatJ˜(s)=+∞.The problem (5) depends on the application that is being addressed. In the least squares linear regression model, a closed-form formula gives the optimizer of the problem (5) andJ˜k(s)can be calculated explicitly.The problem (2) can be written as:(6)minimizeJ˜(s)subject tosk−1<sk;k=1,…,KA bi-level optimization problem (2) involves two optimization tasks (upper and lower level). The clustering is determined at the upper level and the lower level adjusts all statistical models for the upper level variable s. Now if we define(7)Jˆ(s):=J˜(sort(s))s∈[a,b]K−1+∞s∉[a,b]K−1where sort(s) arrange the components ofs∈ℝK−1in ascending order, and [a, b]K−1 is [a, b]×⋯×[a, b] hypercube inℝK−1, then (2) can be reformulated as an unconstrained optimization problem as follows:(8)minimizeJˆ(s)subject tos∈ℝK−1We will refer to problem (8) as the TDC problem.Espinosa-Aranda et al. [37] propose a meta-heuristic framework to incorporate a local search in promising regions into a population-based algorithm. In this article we propose a hybridization of a particle swarm optimization, genetic algorithms, and simulated annealing with a Nelder–Mead simplex algorithm.The PSO algorithm was first proposed by Kennedy and Eberhart [45]. This method was based on the simulation of simplified social behaviour of animals such as fish schooling and birds flocking. The PSO starts with a random population of solutions (particles) in the search space. In every iteration, each particle is updated by following two “best” values. The first, the so-called pbest, is the best solution that is found by the particle. The second, the so-called gbest, is the best solution that is found by all swarm members. After finding the two best values, the position of each particle at iteration t will be updated by the following equations:(9)vpkt=vpkt−1+c1·Rand()·(pbpkt−1−spkt−1)+c2·Rand()·(gbkt−1−spkt−1)(10)spkt=spkt−1+vpktvpktis the k-component of the velocity vector of the particle p at iteration t; the current values of pbest and gbest arepbpt−1and gbt−1; Rand() is a random number in [0,1]; c1 is a learning factor; c2 is a social learning factor; and finallyspktis the k-component of new position vector of the particle p at iteration t. The velocityvpktbelongs to [−Vmax, Vmax], where Vmax is a designated maximum velocity. If the velocity in one dimension exceeds the maximum, it will be set to Vmax.The original version has been improved in three ways. The first improvement is the introduction of inertia weight (ω) [46] or equivalently the constraint factor[47]. These parameters were developed to better balance the exploration and exploitation phases and they avoid the use of Vmax which was viewed as both artificial and difficult to balance. In the second, called LBEST, in order to avoid a premature convergence of the algorithm, each particle keeps track of the best solution, called lbest, attained within a local topological neighbourhood of particles instead of considering the overall best value in the whole particle swarm. The third modification is concerned with the velocity updating rule. The original version chooses a random (uniform distribution) point inside a hyperparallelepid. Standard PSO 2011 [48] propose making use of sampling in hyperspheres to update the velocity. Table 2shows the standard PSO algorithm applied to a time-domain-constrained data-clustering problem.Genetic algorithms [50] emulate the evolutionary behaviour of biological systems. They generate a sequence of populations of candidate solutions by using a set of stochastic transition operators to transform each population of candidate solutions into a descendent population. The three most popular transition operators are crossover, mutation and reproduction. A prototype GA is shown in Table 3.Simulated annealing is a popular local search meta-heuristic. The key feature of simulated annealing is that it provides a means to escape local optima by allowing hill-climbing moves in the hope of finding a global optimum.Simulated annealing starts with an initial solution s. At each iteration of a simulated annealing algorithm the current solution s and a newly selected solution s′ are compared. The new solution is generated (either randomly or using some pre-specified rule) in a neighbourhood N(s) of the current solution s. The candidate solution s′ is accepted based on the ruleℙ(Accepts′as next solution)=exp[−(Jˆ(s′)−Jˆ(s))/Tn]IfJˆ(s′)−Jˆ(s)>01IfJˆ(s′)−Jˆ(s)≤0Improving solutions are always accepted, while a fraction of non-improving solutions are accepted in the hope of escaping local optima in search of global optima. The probability of accepting non-improving solutions depends on a temperature parameterTn.Simulated annealing is outlined in Table 4. A key problem-specific choice concerns the neighbourhood function definition. The efficiency of simulated annealing is highly influenced by the neighbourhood function used. In this work the following problem-specific neighbourhood is proposed,(14)N(s):={s′∈[a,b]K−1/sk′=sk;for allk≠k′withk′∈{1,…,K−1}}The generation probability function has been chosen as a uniform distribution with probabilities proportional to the size of the neighbourhood N(s). To generate the new candidate s′∈N(s), we took three steps:(15)s′=sChoose a random numberk′∈{1,…,K−1}sk′′=a+Rand()·(b−a).The resulting SA is called SA*.The Nelder–Mead simplex (NM) was introduced in Nelder and Mead [51] as an algorithm applied to unconstrained optimization problems. NM converges to twice-differentiable and unimodal functions. However, the Nelder–Mead technique is a heuristic search method that can converge to non-stationary points. It is a direct search method that does not require calculation of derivatives (neither analytically nor numerically) with a strong convergence to a local minimum. The main advantage of this method is that it frequently gives significant improvements in the first few iterations and quickly produces satisfactory results.NM method uses non-degenerate simplices (polytopes of K vertices in K−1 dimensions) as an approximation for the local optimum of the problem. The initial simplex is successively updated at each iteration by discarding the vertex having the highest objective function value and replacing it with a new vertex having a lower objective function value, or the simplex is shrunk if the objective function does not decrease. The worst vertex is replaced by a new point according to four types of operations: reflection, expansion, contraction, and shrinkage through the centroid of the current simplex. Table 5shows a pseudocode for the NM algorithm.In this section we describe a framework for hybridizing a global optimization algorithm with a local search method. The idea behind the hybridization of these two algorithms (global and local optimizers) is to combine their advantages while avoiding their shortcomings. The essential idea of these hybrid algorithms is to use a global optimization algorithm to escape from a local optimum and when a promising region is detected a local search algorithm acts.These methods are probabilistic and maintain a population of candidates. The use of population sets helps the algorithm avoid becoming trapped at a local optimum. Moreover, to achieve faster convergence, the hybrid method applies a local optimization method starting from the best point in the population set. This hybrid algorithm maintains a trade-off between accuracy and computational cost. The hybrid meta-heuristic algorithm is shown schematically in Fig. 2.The initial population has S solutions (particles) that are randomly generated in the feasible space. The global optimization operator is iteratively applied to this population until in nciterations the best solution has been improved, that is, the valueJˆ*decreases. Then the vertices of an initial simplex are built from the best solution g.(16)gk=g+λkek,k=1,…,K−1where {ek} are the unit vectors composing a basis for the spaceℝK−1and λkare small constants. From this initial simplex nriterations of the NM algorithm are applied. The best found solution replaces the best solution in the population and the procedure is repeated until some stopping criterion is reached.The hybrid meta-heuristic algorithm is shown in Table 6.In this section, three numerical experiments are performed, with the following objectives:•Experiment 1. To compare the TDC model from the previous section with other time-domain-constrained clustering methods. This is the purpose of the experiment performed in Łęski and Owczarek [2].Experiment 2. To develop a real application of the proposed methodology. In this experiment we have considered a survival analysis for hip replacement surgery, the aim being to determine time windows which explain the evolution of mortality rates from the surgery.Experiment 3. To evaluate the computational performance of the PSO, GA and SA algorithms and their hybridization strategies based on the NM algorithm. Therefore, we have solved a TDC problem applied to pattern recognition which shows that the hybridization strategy is appropriate.All experiments were run using the Matlab program. The uniformly distributed random numbers, and the normally distributed, on the interval (0, 1) were generated using the Matlab “ran” and “rand” functions, respectively. The Cauchy random numbers were obtained using the inverse of the cumulative distribution function technique.Experiments 1 and 2 assess the TDC model and not the solution method (Experiment 3 addresses this). For this reason we have carried out a large number of iterations using PSO to ensure that an optimum of the TDC problem has been obtained and therefore the results are independent of the computational method used.In this experiment we use the linear regression model:(17)yi=F(xi,ti;Θk)=ak+bktiwhere Θk=(ak, bk) is a parameter vector to be estimated in each cluster k. We consider three methods of estimation of the linear regression model (5). The first, the classical least squares estimation called TDC-PSO, leading to a closed solution of the problem (5). The other two are robust estimation methods to alleviate the effect of outliers. The second of these methods is called TDC-PSO-R and it is implemented in Matlab [fit(x,y,’poly1’,’Robust’,’on’)]. The third method is called TDC-PSO-R* and it consists in fitting a straight line in a given cluster, detecting the greater error (may be an outlier) and removing the datum from the analysis, and repeating the process for the new data set. In this experiment, two iterations of the previous procedure were made.This experiment considers a set of synthetic signals generated by simulation. At the beginning these signals are formed from noise and from the instant tcp(characteristic point) they show a linear trend plus a noise component. The goal is to evaluate whether this approach is able to determine tcpfor which linear models change the definition. In all experiments a single characteristic point in each signal is considered, and so only two regression models are used (K=2).For each signal we consider a two-dimensional data set{(ti,yi)}i=1100where tiis the input-time data and yiis the output-signal value. The true model (but unknown in our approach) is defined by the following piecewise linear function:(18)yi:=0·ti+0+eii=1,2,…,401·ti−40+eii=41,42,…,100where eirepresents a realization of a random noise. Three types of signal were considered taking into account the kind of noise: (i) Gaussian, (ii) Cauchy and (iii) impulsive noise (Bernoulli–Gaussian).Each data pair (ti, yi) was generated as follows: for each value ti=i the value of yiwas obtained using (18) and adding Gaussian or Cauchy or Bernoulli–Gaussian noise ei. For Gaussian noise the zero mean and standard deviation equal to 7 were used. For Cauchy noise the following formula was used ei=tan(π(ri−0.5)), where riis a realization of a random variable with uniform distribution on the interval [0, 1]. For the impulsive noise a Bernoulli–Gaussian sequence was generated by ei=gi·qiwhere {qi} is a sequence of Bernoulli random variables, that is, a random sequence of zeros and ones, with parameter λ:Prob[qi]:=λ,qi=1,1−λ,qi=0.The sequence {gi} is a zero-mean Gaussian white noise with varianceσg2. Sequences {gi} and {qi} are statistically independent. In all experiments the following values of parameters were used: λ=0.2 and standard deviation σg=15.For each kind of noise 50 different signals were generated. Examples of these signals are presented in Fig. 3where signals with Gaussian noise, with Cauchy noise and with impulsive noise are depicted.Results are analyzed with the most common method used to locate the beginning and the end in biomedical signals and with the methods described in Łęski and Owczarek [2].In biomedical signal analysis, characteristic points tcpare calculated as a time for which the modulus of the mean in 5ms interval exceeds a given threshold. These thresholds are defined by the modulus of the mean of the signal plus β times the standard deviation of the noise. The mean as well as the standard deviation is calculated at a time window before or after the wave (to locate the beginning or the end of the signal). Usually, the constant β takes value 1, 2 or 3. In the experiments the time window (1, 20) was used to estimate the standard deviation of the noise, then the start of the wave was found.Fig. 4shows results for parameter β=1, 2and3 collected in the rows and to the different signals collected in columns. Each drawing represents a histogram of the points tcpassociated with the 50 signals of the same type. Graphical representations allow us to see if the sample mean is close to the true value tcp=40 (unbiased procedure) and assess the dispersion of data around this value (standard deviation of the estimator). In the statistical literature a general criterion for selecting an estimator, in this case tcp, is that it is unbiased (mathematical expectation of estimator is the true value of the parameter to be estimated) and minimum-variance. In the present example, it is desirable that the observations of the histograms shown be concentrated around tcp=40 and exhibit minimum variance. Fig. 4 shows that the best results are obtained for β=1 as it obtains less biased estimates.Fig. 5shows the results obtained by the proposed methodology. The rows are associated with TDC-PSO, TDC-PSO-R and TDC-PSO*-R models and the columns with the different types of signal. Each drawing represents a histogram of the estimated points tcp.As can be seen in Fig. 5 the previous procedures are unbiased but with very different variances. It is noted that the traditional least squares estimation (TDC-PSO) has a large variance in the signals with Cauchy noise and impulse noise. Cauchy noise (with variance ∞) introduces outliers in the signal whereas the impulsive noise variance is large and in same cases may introduce some outlier. These difficulties extend to the variance of the estimator.The best results are obtained by the robust methods TDC-PSO-R and TDC-PSO-R* and they are capable of significantly reducing the number of outlier estimates of tcp. Moreover, TDC-PSO-R* can eliminate and significantly reduce the variance of the estimator. To go into this more deeply, Table 7shows the sample mean and standard deviation of the estimations of tcpobtained by the previous methods and for the different types of signal. It can be observed that, for the classical method, the best result is achieved with β=1 and all these procedures are, in general, biased. The methods based on TDC get better results than the traditional method with respect to bias and variance of the estimator.A second contrast of the results is given by Łęski and Owczarek [2]. In this work, the same numerical experiment is done using a fuzzy clustering method with time-domain constraints. The authors compare Fuzzy c-Regression Models (FCRM) described in Hathaway and Bezdek [16], Time-Domain-Constrained Fuzzy c-Regression Models (TDCFCRM) and ɛ-Insesitivity in Time-Constrained Fuzzy c-Regression Models (ɛTDCFCRM). The traditional method FCRM does not consider time-domain constraints. The TDCFCRM method adds to the objective function of FCRM a penalty term associated with time-domain-constraints, so a penalty parameter η>0 is introduced to create a trade-off between the smoothness of membership degrees and the clustering error. Fuzzy approaches use continuous variables uikthat indicate the membership degree of observation i in cluster k. If η=0 the original FCRM method is obtained and if η→+∞ then uik=uk, that is, all the data in a cluster have the same membership degrees. The parameter ɛTDCFCRM is a robust method to tackle signals containing non-Gaussian noise and/or outliers. The parameter ɛ=0 forces ɛTDCFCRM to use absolute error instead of a square error to fit the linear regression parameters.The TDC-PSO method would be equivalent to TDCFCRM since the loss function is usually the sum of squares. The difference between the two methods is that in TDC-PSO time-domain constraints are considered explicitly, rather than through penalties. TDC-PSO-R and TDC-PSO-R* are complementary methods to ɛTDCFCRM to tackle non-Gaussian noise and outliers.The results obtained in Łęski and Owczarek [2] are shown in Table 8. The best fuzzy results are achieved with ɛTDCFCRM when η=0 and ɛ=0 (note that this method does not consider the time constraints). If we compare TDC-PSO-R* with ɛTDCFCRM (ɛ=0, η=0) we can see that TDC-PSO-R* has a less deterministic error (unbiased) for signals with Gaussian noise, a lower standard deviation (random error) for Cauchy noise signals and larger standard deviation for impulsive-noise signals.11Łęski and Owczarek [2] point out that for impulsive signals they use a variance withσg2=15. We think it is a typo and they really mean to say that σg=15. In this paper the tests are carried out with σg=15. Note that the signals with σg=15 show more complex behaviour than the signals with parameterσg=15in order to detect tcp.The results obtained show that the quality of the estimates obtained from this method is at least as good as the quality of the estimates obtained with fuzzy methods, therefore, the choice of research method should be made depending on the problem type being addressed. There are three key elements to take into account: (i) whether it is enough to consider linear models in each cluster, (ii) whether it is necessary to consider explicit time constraint and (iii) whether the fuzzy approach is suitable.In this section, we carry out a TDC-PSO application for the survival analysis of hip replacement surgery discussed by García-Navas [52]. In this clinical experiment, 247 hip prostheses were implanted, monitoring mortality in patients over the year following their surgery. A population of 68 out of 247 patients died and for each of them the number of days T they survived after surgery was recorded. This survival analysis determines which factors (demographic, health, etc.) influence an individual's response (exitus, or alive) in a time interval given a priori. Traditionally, this time interval is arbitrarily fixed using units such as a month or a year. In this section we explain how TDC-PSO is an analytical method which determines the time interval in which the risk of mortality due to the surgery is present. The goal is to increase the power of statistical tests used in survival analysis and to study different guidelines followed in the evolution of the surgery.Assume that the survival time, T, after surgery is a random variable. The probability that a person dies in the time interval [a, b] is given by the expression(19)ℙ(a≤T≤b)=∫abf(x)dxwhere f(x) is the probability density function (pdf) of the random variable T. From (19) the survival function is defined:(20)S(t)=1−ℙ(T≤t)=1−∫0tf(x)dxwhere 0 is the time of the surgery. Assume that pdf is a continuous function, therefore on taking the derivative of both sides of (20) with respect to t, the pdf is obtained:(21)f(t)=−S′(t)Fig. 6shows the data used in this application. In the first column, we can observe an empirical survival function and, by numerically differentiating, the pdf. The peaks of this function are associated with noise events, that is, the unexpected death of patients. For this reason, in the second column the pdf is smoothed by moving averages of order two and three. These figures display that the mortality rate can be broken down into three time periods. The first period from the surgery to a little over two months where the mortality risk is attributable to the operation itself. The second one is the period following discharge from the hospital. This period is a vulnerable time for patients (there is an increased risk of mortality). And the third period corresponds to a normalization of the situation before surgery.The approach described constitutes an unsupervised method for segmenting the empirical pdf. We applied the TDC-PSO method to the pdf for K=2, …, 5. A large number of iterations and particles were used in order to find a (near-)global optimum of the problem. The results are shown in Fig. 7. For K=2 two periods are determined, a first period of 1–8 days and a second period which corresponds to the rest of the year. This first period shows a clear risk associated with surgery. For K=3 we obtain the same solution as one obtained graphically with moving averages of order three, showing exactly the same periods. The solutions obtained for K=4 and K=5 can be viewed as a refinement of the solution with K=3 clusters. The intervening period of the K=3 solution is decomposed into a period of decreasing risk and one of increasing risk in the case K=4. In the case K=5 an initial period of 8 days is taken (as with K=2) and the last period is broken down into two periods. A qualitative analysis of these solutions gives a clear idea of what is happening. It shows three main periods and an opportunity to highlight the first 8 days and to partition or not the last stabilization period.The question that we now ask is how to know which of the previous solutions is most suited to describing the survival of the population. Analysis of covariance (ANCOVA) is a parametric statistical method that assesses whether the linear regression models of two clusters are statistically different from each other. Suppose we want to compare the goodness-of-fit of a statistical model 1 with the goodness-of-fit of another more complex statistical model (with more parameters) called model 2. The statistical contrast has the following expression:(22)Fk2−k1,N−k2=(R22−R12)/(k2−k1)(1−R22)/(N−k2)where k1 and k2 are the number of parameters estimated for each model,R12andR22are respectively the coefficients of determination for models 1 and 2, and N is the number of observations. The statistic (22), under the null hypothesis and ANCOVA model hypothesis, is distributed as an F-Snedecor with (k2−k1, N−k2) degrees of freedom. We propose a heuristic method based on the significance level22The theoretical support of this method requires: (i) the solution obtained by TDC-PSO algorithm with K+1 cluster must contain the solution obtained with K cluster, (ii) normal distribution of errors and (iii) homoscedasticity of errors. This score is purely descriptive and cannot be interpreted in terms of probability.of the statistic F in determining a proper number of clusters. In this heuristic method, we generate solutions for increasing values of K until obtaining a solution that is not “significantly” better than the previous one and between all the solutions generated. We choose the one with the maximum value of 1−α, where α is the significance level of an F-Snedecor with k2−k1, N−k2 degrees of freedom. The number of parameters adjusted is 2 for each cluster since a straight line is estimated in each cluster. The total number of parameters in each model are k1=2(K−1) and k2=2K. Table 9shows the results obtained with this procedure from cluster K=1 to cluster K=5. The first column is associated with the value of K. The second column shows the coefficient of determination obtained in each model. The third column gives the value of F defined by Eq. (22). This value compares the adjusted model in its row with a lower cluster (in the previous row). The last column shows the value of 1−α, where α is the significance level of F.In this paper we propose maximizing the significance level of the statistic F to determine the number of clusters. This value is better than the value of F since the significance level does not depend on degrees of freedom. Taking this criterion into account we take a number of clusters K=3 that agrees with the graphic solution obtained by visual inspection of smoothed moving averages of order 3 on the pdf.This experiment has the goal of evaluating three computational procedures for solving TDC. In this section PSO-based, GA-based and SA-based hybridizations are assessed with respect to their computational performance. These algorithms have been tested on the following pattern recognition problem. An image (see Fig. 8) that contains 13 lines of text is given. This image is defined by a matrix in which the cells represent the grey level of each pixel forming the image. The maximum value is 256 and is associated with white and the minimum value is 0 and represents the colour black. The matrix that defines the image in Fig. 8 has 318 rows and 941 columns of pixels.The problem addressed in this section is to automatically identify adjacent rows of pixels associated with a line of text. Each line written is separated by several rows of blank pixels. This is a problem of space-constrained data-clustering analysis that can be handled by the proposed methodology of time-domain-constrained data-clustering analysis. Fig. 9illustrates this identification. The row number plays a time role t and the response vector y is the sum of all columns of the matrix of the image. Each component of the vector y represents the sum of the grey level of all pixels in the same row. Fig. 9 shows the graphic representation of the vector y against the line number t. It is seen that the lines of pixels through the letters have lower values and a full line of blank pixels leads to the maximum value of the vector y.We want to divide the greyscale y into intervals, each of them corresponding to an image fragment. We included a constant function in each cluster k, that is:(23)yi=F(xi,ti;Θk)=akwhere Θk=akis the parameter for each cluster. Moreover, we consider as a loss function the following function(24)J(xi,yi,ti;Θk)=(yi−ak)2that leads to the sum of squared errors (SSE). The closed solution of problem (5) leads to the estimationak=y¯k, wherey¯kis the average value of y in cluster k.The figure in this example contains 13 text lines, so that we took K=27 clusters to identify (cluster 1= first blank band, cluster 2= first written line, cluster 3= second blank band, cluster 4= second written line, …, cluster 27= fourteenth blank band). The identification of these clusters is performed by fitting 26 rows (number of variables of the problem) that separate the different clusters. A measure of the computational difficulty of this problem is the number of solutions in the feasible space. Each solution of the feasible space corresponds to the choice of 26 rows among 318 rows that are contained in the figure, so that the total number of feasible solutions is given by the combinatorial number(25)31826≈1.0029e+38To illustrate the magnitude of the above number, consider that if there are 1000 million computers on Earth and each hard drive has on average 100Gb (1011 bytes) the total number of available bytes would be 1020. This shows that it would be impossible to write the complete solution of this problem explicitly. The test problem has a large feasible region which may not be explored exhaustively.Algorithm performance was monitored by the following standardization of the objective function value(26)Z*=Z−ZuZ0−Zuwhere Z is the current value of the objective function, Zuis the best known objective function value of the solution and Z0 is the objective function value of a random solution. To work with Z* eliminate the magnitude of the problem and consider that objective function values varies in [0, 1].The solution is depicted in Fig. 10. It can be seen that the clusters identified coincide with the centres of the letters, associating the upper part of the letter and the bottom part with the adjacent blank rows. The solution obtained has not identified the last text line. In contrast, it locates the rows 47 and 49 that subdivide a text line into its lower part and central part. This problem is highly regular (a repeating pattern), which creates a large number of local minima defined through a set of lines, another non-identified set and a third set where the text lines are subdivided into a central part, an upper part and/or a bottom part. This is the second computational difficulty of the test problem.This hybridization combines the advantages of a global optimization procedure and NM. On the one hand PSO, GA or SA allow an escape from local optima and on the other hand NM explores more efficiently the neighbourhoods of a local optimum. The parameter ncplays the role of filtering the neighbourhoods that will be explored by NM. In this computational experiment we have considered the values nc=1, nc=5 (to guarantee that the neighbour of the minimum has been left) and nc=∞ to obtain the original algorithm. The Standard PSO-2011 code33It has been put on line at Particle Swarm Central http://www.particleswarm.info.[48] and sa and simulannealbnd functions of Matlab program have been used to implement the algorithms.The number of particles used in PSO and GA algorithms is a key in the computational performance of the procedures. A large number of particles explores the feasible solution space more intensively and enables convergence to the optimal solution, but in contrast it needs a large number of objective function evaluations. To take into account the effect of this parameter the computational experiments have been carried out with 5, 25 and 125 particles. The SA algorithm does not work with population. At each iteration, the objective function is evaluated only once. For this reason we conducted 50 inner iterations (simulating a population) before considering whether or not there was a decrease.The stopping criterion of the NM algorithm has been to limit to 200 the objective function evaluations for each execution of the algorithm, and as the stopping criterion of the whole procedure the total number has been restricted to 4000 objective function evaluations. Observe the proportion of evaluated solutions for the procedure is an infinitesimal quantity as against the number of feasible solutions.The key parameters of this computational experiment are ncand the number of particles (elements of a population) S. Therefore we have taken the default values for the rest of the parameters and they are shown in Table 10. In Standard PSO 2011 3 informants are given by default. To make the effect of the motion of the best particle more important for the NM algorithm we have assumed for nc=1, 5 that the entire population is an informant.As all algorithms have a random nature each test has been replicated 10 times and the average evolution showed. Fig. 11shows the computational results for the PSO-NM algorithm. It is noted that in all cases hybridization improves the PSO algorithm. PSO-NM obtains solutions whose value in all cases is Z*≈0.5. Fig. 12shows the results obtained for GA-NM. The best results are obtained from a population of 125 individuals reaching values Z*≈0.25. In this case the hybridization strategy has no significant effect. This could be explained because the hybridization varies one of the 125 individuals but has no effect on the mutation parameters and crossover rules for GA algorithm, diluting the beneficial effect. Fig. 13shows the results obtained with SA and SA*. SA+NM and PSO algorithms have a similar behaviour while SA*+NM, because it is aimed at the problem, is superior to PSO-based and GA-based algorithms.

@&#CONCLUSIONS@&#
Time-domain-constrained data clustering analysis includes in the grouping process the time instant at which these data are obtained. The aim is to find time intervals in which the data come from the same statistical model. In this paper the problem is formulated as an unconstrained bi-level optimization problem and hybridizations of PSO, GA and SA with NM are proposed to solve it. Two novel aspects of this approach are: (i) time-constraints are taken into account explicitly and not by penalizing the objective function and (ii) the use of a free-derivative method allows general statistical models to be dealt with as black boxes.The validation of the methodology proposed is performed by three different tests. The first test comes from the field of signal processing in which the characteristic point of a signal is located. This example illustrates that the proposed methodology is competitive with respect to the traditional fuzzy-clustering methods that applied in the literature. The second test is a real application to survival analysis of hip replacement surgery. In this application it is shown that this new technique is a useful tool to determine the cut-off time value and to discover information from the data. The third test is a hard problem of pattern recognition and the goal is to point out the computational advantages of the proposed hybridizations. In this example we show that the global optimization procedure explores the feasible solution space intensively whereas the NM algorithm speeds up the convergence process. The numerical results show that the SA*+NM outperforms the PSO+NM, GA+NM and SA+NM algorithms.An aspect that requires specific research is the automatic determination of the number of clusters. A preliminary discussion is based on the F-Snedecor statistic in the hip surgery problem. A promising line of research is bottom-up segmentation algorithms [5].