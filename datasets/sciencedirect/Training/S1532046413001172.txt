@&#MAIN-TITLE@&#
Digital video analysis of health professionals’ interactions with an electronic whiteboard: A longitudinal, naturalistic study of changes to user interactions

@&#HIGHLIGHTS@&#
We perform a naturalistic and longitudinal study with a electronic whiteboard.We investigated how user interaction and usability problems changed over time.Results show that technical issue do not change while user related issue do change.User related issues include different work patterns with the whiteboards.Users become more efficient for some work patterns and less efficient for others.

@&#KEYPHRASES@&#
Usability evaluation,Longitudinal,Naturalistic,Video-based,Methodology,Electronic whiteboards,

@&#ABSTRACT@&#
As hospital departments continue to introduce electronic whiteboards in real clinical settings a range of human factor issues have emerged and it has become clear that there is a need for improved methods for designing and testing these systems. In this study, we employed a longitudinal and naturalistic method in the usability evaluation of an electronic whiteboard system. The goal of the evaluation was to explore the extent to which usability issues experienced by users change as they gain more experience with the system. In addition, the paper explores the use of a new approach to collection and analysis of continuous digital video recordings of naturalistic “live” user interactions. The method developed and employed in the study included recording the users’ interactions with system during actual use using screen-capturing software and analyzing these recordings for usability issues. In this paper we describe and discuss both the method and the results of the evaluation. We found that the electronic whiteboard system contains system-related usability issues that did not change over time as the clinicians collectively gained more experience with the system. Furthermore, we also found user-related issues that seemed to change as the users gained more experience and we discuss the underlying reasons for these changes. We also found that the method used in the study has certain advantages over traditional usability evaluation methods, including the ability to collect analyze live user data over time. However, challenges and drawbacks to using the method (including the time taken for analysis and logistical issues in doing live recordings) should be considered before utilizing a similar approach. In conclusion we summarize our findings and call for an increased focus on longitudinal and naturalistic evaluations of health information systems and encourage others to apply and refine the method utilized in this study.

@&#INTRODUCTION@&#
As hospitals worldwide move towards increased automation, a wide variety of information systems are becoming deployed in settings such as emergency departments. One such type of information system, the electronic whiteboard (EW), is being increasingly deployed as a replacement for the ubiquitous dry-erase whiteboards traditionally used as the central coordination and communication tool in emergency departments and hospital departments in general [1–5]. Previous research has shown that EW systems have certain advantages over the traditional dry-erase whiteboards. These advantages include distributed access to and updating of patient information, the ability to store and retrieve information for future use, integration with other clinical information systems, live tracking of patients and decreased mental workload for clinicians [1,2,6,7].However, as we move towards greater use of such health information technologies in real clinical settings, a wide range of human factor issues have emerged and it has become clear that there is a need for improved methods for designing and testing EWs that are to be integrated into complex work practices in settings such as the emergency department (ED) [8]. For example, Wong et al. [9] found the need for a number of enhancements to an EW after it was deployed as well as the need to conduct workflow review meetings to ensure adoption. A comparative study of manual whiteboards and EW systems [3] used interviews and observations of clinicians using a EW to identify issues related to the need for flexibility, the need for local customization by clinicians and the incorporation of new and emerging needs into EWs. In another line of research Riley et al. [10] have demonstrated how implementation of EWs can lead to inadvertent changes in power, work activities and professional control in clinical practice.In this paper we describe our work in conducting naturalistic video-based analyses of user interactions with a new EW system that has been deployed in two emergency departments at Danish hospitals. The goals of the study are twofold. The first to goal is to identify specific usability problems, potential inefficiencies and workflow issues associated with the clinicians’ actual daily use of the EW system. In relation to this, we are also interested in understanding if there would be differences in human factor and workflow issues in departments that have adapted to the same EW sometime after deployment as compared to a more recent deployment. The second goal of the study is to determine the practicality of the approach we have developed and used for evaluating the EW system in this study. This approach was designed to augment and complement an initial participatory design approach described by Rasmussen et al. [11] and Fleron et al. [12].Methods which have been previously used to evaluate the use of EWs have included approaches ranging from surveys and interviews given to EW users [10] to observation of users [13] to collecting and analyzing static digital photographs taken of EW screens [2,3]. While these methods are highly effective for capturing and documenting the work practices surrounding an EW system, e.g. group interactions or viewing the display from a distance, we find that there are some limitations to such methods. Among others, these limitations include difficulty in collecting long-term data regarding how the system responds to a wide range of user interactions in real settings. Also, we find that some of the methods previously used are somewhat obtrusive in nature and are thereby at risk of biasing the final results due to the Hawthorne effect [14–16].Due to the goals of this study and the described limitations of the methods used previously in studies of EW system, we therefore chose to employ a method whereby user interactions with the EW system were recorded in a continuous, naturalistic and unobtrusive manner. By continuously recording live user interactions over time with applications such as EWs, a large and rich data set can be collected that can be used to assess usability problems and help describe adoption issues when such applications are deployed in real clinical and emergency settings over time. Our previous work has shown that although laboratory testing of healthcare applications applying usability methods is needed, it is not sufficient for ensuring the safety and effectiveness of healthcare applications deployed in complex settings [17]. In the area of video analysis of user interactions, previous work has been published about video and screen recording and resultant analysis of healthcare professional interactions in the context of usability testing [18] and the extension of usability testing to more realistic simulations, termed “clinical simulations” [17]. However, there has been less work describing effective approaches to the naturalistic analysis of video recorded user interactions’ with systems and patients once a system has been deployed in a real clinical setting. To address this issue, we describe, alongside the results of the performed evaluation, the approach we have developed and applied to collecting and analyzing large data sets of continuous live screen recordings of user interactions with an EW system over time.

@&#CONCLUSIONS@&#
Using continuous screen recordings of user interactions with an EW system at two EDs we applied a longitudinal and naturalistic approach to studying the usability issues related to the usage of this system. Through the application of the approach and the analysis of the resulting recordings we found a wide range of system related usability issues that did not appear to change over time as the collective experience of the users at the different EDs increased. However, we found that certain work patterns related to different tasks did in fact change as the departmental experience with the EW system increased. In one work pattern we found that the users appeared to become more efficient with the EW system, which indicated that despite potential efficiency issues in the initial stages of use users have the ability to overcome these issues and learn to use the EW efficiently.In another work pattern we found that the users became less efficient as the department gained more and more experience with the EW system. We argue that a mismatch between this work pattern and the work practices of the ED coupled with the clinician’s possibly forgetting the functionality of the EW as they move further and further away from the initial stages of usage could cause this decrease in efficiency. This indicates that some aspects related to the EW system and the use of it, need to be rethought and possibly redesigned. For the work pattern regarding logging in to the EW system a potential solution could be to conduct a technical redesign aimed at providing the users with cues to remind them to log in before initiating activities. Another approach would be to periodically refresh the users’ training in an attempt to remind them how to use the EW system most efficiently. This would also assist in providing new users with the necessary training in a working environment characterized by a high degree of staff turnover.Through the study we also gained experience with the application of the methodology applied. We found that the method affords a number of advantages over more traditional usability evaluation methods. We especially find that the longitudinal and naturalistic nature of the method provides researchers and usability evaluators with a tool to uncover issues that would be difficult to reveal with other methods, e.g. the ability to expose usability issue that occur very infrequently or only under very specific conditions that can be difficult to predict using more traditional evaluation methods.However, the application of this methodology is not without limitations, disadvantages and challenges. Some of these are inherent in the methodology and can therefore not be avoided when applying the method, e.g. time-consuming initial evaluation of digital video data, increased evaluator responsibility and reduced control of the final results. These issues have to be accounted for when analyzing and reporting the results. Others issues could be mitigated by enforcing certain procedures throughout the application of the method, e.g. inspecting recordings regularly to avoid interruptions. In addition, it should be noted that the usability of electronic whiteboards may vary across settings and that usability may vary across functions used, requiring further analyses of such systems under different contexts to assess the generalizability of the findings from this study. In conclusion we encourage that more research be focused on longitudinal and naturalistic evaluations of health information systems and we encourage the use and refinement of the method described in this paper with the hope that researchers will continue to improve the systems that keep our hospital running smoothly and safely. This work will be important in order to help identify and mitigate usability issues and potentially detrimental impact of newly introduced health information systems on workflow, based on analysis of live interactions conducted in the implementation process.