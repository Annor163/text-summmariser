@&#MAIN-TITLE@&#
Title Paper: Natural computing: A problem solving paradigm with granular information processing

@&#HIGHLIGHTS@&#
Granular computing aspects of natural computing.Review of different granular soft computing research.Biological motivation, design principles, application areas, open research problems and challenging issues of these models.

@&#KEYPHRASES@&#
Natural computing,Granular computing,Soft computing,Hybrid model,Decision systems,

@&#ABSTRACT@&#
Natural computing, inspired by biological course of action, is an interdisciplinary field that formalizes processes observed in living organisms to design computational methods for solving complex problems, or designing artificial systems with more natural behaviour. Based on the tasks abstracted from natural phenomena, such as brain modelling, self-organization, self-repetition, self evaluation, Darwinian survival, granulation and perception, nature serves as a source of inspiration for the development of computational tools or systems that are used for solving complex problems. Nature inspired main computing paradigms used for such development include artificial neural networks, fuzzy logic, rough sets, evolutionary algorithms, fractal geometry, DNA computing, artificial life and granular or perception-based computing. Information granulation in granular computing is an inherent characteristic of human thinking and reasoning process performed in everyday life. The present article provides an overview of the significance of natural computing with respect to the granulation-based information processing models, such as neural networks, fuzzy sets and rough sets, and their hybridization. We emphasize on the biological motivation, design principles, application areas, open research problems and challenging issues of these models.

@&#INTRODUCTION@&#
Natural computing refers to going on in nature and to perform computation with its inspiration. With this visualization and understanding, the essence of computation is enhanced and opened a way to look insights of both natural sciences and computer science. It is understood that nature inspired models are not the alternative methods; rather, they have been proven substantially as a much more efficient paradigm to deal with various complex tasks. Several studies can be referred, where the hurdles faced by the classical computing have been crossed successfully by the nature inspired models. For example, there exist various ways to build complex multi parameter statistical models for general use in classification or prediction. However, nature has extensive experience in a particular area of this design space resulting in a model, namely, neural networks [1]. This inspiration has guided much of the machine learning and pattern recognition community towards its exploitation and exploration that has proved extremely successful. Similar can be said in the use of immune system [2] metaphors to underpin the design of techniques that detect anomalous patterns in systems, or of evolutionary methods for design. Moreover, it seems clear that natural inspiration has in some cases led to the exploration of algorithms that would not necessarily have been adopted, but proven significantly more successful than alternative techniques. Particle swarm optimization [3], for example, has been found enormously successful on a range of optimization problems, despite its natural inspiration having little to do with solving an optimization problem. Evolutionary algorithms [4] use the concepts of mutation, recombination and natural selection from biology; molecular computing [5,6] is based on paradigms from molecular biology; and quantum computing [7] is based on quantum physics that exploits quantum parallelism.There are also important methodological differences between various subareas of natural computing. For example, evolutionary algorithms and algorithms based on neural networks are presently implemented on conventional computers. On the other hand, molecular computing aims at alternatives for silicon hardware by implementing algorithms in biological hardware (bio-ware), e.g., using DNA molecules and enzymes. In addition, quantum computing aims at non-traditional hardware that would allow quantum effects to take place. Computer science undergoes now an important transformation by trying to combine the computing carried on in computer science with the computing observed in nature all around us. Natural computing is a very important catalyst of this transformation, and holds a lot of promise for the future research.The term natural computing though referred initially to describe only those systems that employ natural means, such as DNA or RNA strands, to perform computation, now its scope has broadened to cover the following three major areas:•Computing techniques that take inspiration from nature for the development of novel problem-solving methods.Study of natural phenomena by means of computation.Computing with natural means.Good surveys on natural computing explaining its different facets are provided in [8–13]. Natural computational models are most relevant in applications that resemble natural systems, e.g., real time control systems, autonomous robots and intelligent systems in general Natural computational systems have the characteristic features like – adaptability (to new environment, data), robustness/ruggedness (with respect to noise, faults, damage, parameter change, component failure), speed (real time response), abstraction and generalization (human-like learning for improving performance), and optimality (with respect to error rate). Any design engineer desires to have these features in his/her systems.Granulation is a process, among others such as self-reproduction, self-organization, functioning of the brain, Darwinian evolution, group behaviour, cell membranes, and morphogenesis, that are abstracted from natural phenomena. Granulation is inherent in human thinking and reasoning processes. Granular computing (GrC) [14,15] provides a nature inspired information processing framework, where computation and operations are performed on information granules, and it is based on the realization that precision is sometimes expensive and not very meaningful in modeling and controlling complex systems. When a problem involves incomplete, uncertain, and vague information, it may be difficult to differentiate distinct elements and one may find it convenient to consider granules for its handling. Accordingly, granular computing became an effective framework in the design and implementation of efficient and intelligent information processing systems for various real life decision-making applications. The said framework can be modeled with principles of neural networks, fuzzy sets and rough sets, both in isolation and integration, among other theories.In the present paper, we describe a brief overview of the granular information processing aspect of natural computing, and the significance of fuzzy sets, rough sets and neural networks, and their different hybridizations. Biological motivations, design principles and application areas of these individual models are stated along with some open research problems. Some characteristic features of granulation are explained through examples from existing literature. Finally, some challenging issues of the hybrid granular systems are mentioned.Granular computing is a problem solving paradigm with the basic element called granules. The construction of granules is a crucial process, as granules with different sizes and shapes are responsible for the success of granular computing based models. Further, the inter and intra relationships among granules play important roles. In the following section, we summarize these concepts and components briefly.The significance of a granule in granular computing is very similar to any subset, class, object, or cluster of a universe. The granules are composed of elements that are drawn together by indisccernibility, similarity, and functionality. Each of the granules according to its shape and size, and with a certain level of granularity may reflect a specific aspect of the problem or form a portion of the system domain. Granules with different granular levels represent the system differently. For example, an image can be described with three granules at the first level of granularity where each of the granules characterizes the regions of image with three basic colors, such as red, green and blue. At this level the information of the image may be categorized in a broader way, like greenery or bluish regions. If we go further into more details with respect to colors then each of the these three granules (color regions) can be described with their subsequent divisions. As an example, each of such divisions can characterize objects (granules) in a particular color such as tree, grass, bushes, where combination of these object regions forms the greenery region.Granulation is the process of construction, representation, and interpretation of granules. It is the process of forming larger objects into smaller and smaller into larger based on the problem in hand. Zadeh [14] described this idea as, “granulation involves a decomposition of whole into parts. Conversely, organization involves an integration of parts into whole.” This concept leads to the fact that granular computing involves in two basic operations, such as granulation and organization. Granulation starts from the problem space as a whole, partitions the problem into sub-spaces, and constructs the desired granules; while organization puts individual elements/granules together to form blocks and to build granules at expected levels. The criteria for the granulation process determine the action for granulating big granules into smaller or small into bigger. Further, the concept of partition and covering comes in the granulation process. A partition consists of disjoint subsets of the universe, and a covering consists of possibly overlap subsets. Partitions are a special type of coverings. Operations on partitions and coverings have been investigated in literature [16,17].Granular relationship among granules is a key factor in the process of granulation, as one needs to understand it very precisely for better solution. Granular relationship can be broadly categorized into two groups [18], such as inter-relation and intra-relation. The former is the basis of grouping small objects together to construct a larger granule based on similarity, indistinguishability and functionality; while the latter concerns the granulation of a large granule into smaller units and the interactions between components of a granule as well. A granule is a refinement of another granule if it is contained in the latter. Similarly, the latter is called coarsening of the former. These relationships function like set-containment in the set based domains.Computation with granules is the final step in granular computing process. Computing and reasoning in various ways with different granules based on their relationship and significance is the basis for the computation method. These operations, as described above, are broadly categorized as either computations within granules or computations between granules. Computations within granules include finding characterization of granules, e.g. membership functions of fuzzy granules; inducing rules from granules, e.g. classification rules that characterize the classes of objects; forming concepts that granules entail. On the other hand, computations between granules usually operate on the interrelations between granules, transformations from one granule to another, clustering granules, and dividing granules.Granular information processing is one of the human-inspired problem solving aspects of natural computing, as information abstraction is inherent in human thinking and reasoning process, and plays an essential role in human cognition. Among the different facets of natural computing fuzzy sets, rough sets, neural networks and their hybridization are well accepted paradigms that are based on the construction, representation and interpretation of granules, as well as the utilization of granules for problem solving. In this section, we provide an overview of these tools, emphasizing the characteristic features, biological motivation, design principles with some applications and open problems.Zadeh in 1965 [19] introduced the theory of fuzzy sets as a mean to model the uncertainty in natural language. Classical or Boolean logic, in its generalized form is well known as fuzzy logic that is based on fuzzy set theory. Fuzzy logic has the capability of handling the concept of partial truth (neither “complete true” nor “complete false”). The process of “fuzzification” is a method to generalize any specific theory from a concrete (discrete) to a continuous (fuzzy) form (see “extension principle” in [19]). Zadeh also described the concept of information granulation with the advent of fuzzy set theory and formulated the method of fuzzy information granulation. Fuzzy information granulation deals with the construction of fuzzy granules for a particular level of granulation based on the problem in hand. Although crisp or discrete granules have wide range of applications, the human thinking and reasoning process are better formulated with fuzzy granules.In fact, fuzzy information granulation is key to fuzzy logic, as it is key to human thinking and reasoning process. Fuzzy information granulation plays a vital role in the conception and design of intelligent systems. It is important to mention here that there is a long list of tasks which humans can perform with no difficulty and may be a massive task for machine to perform accordingly without the use of fuzzy information granulation [14].It has been an arguable point from a long period that most of the human generated knowledge or behavior is far away from the simple concept of precision. Various domains of human knowledge are highly imprecise (in the sense that they do not have precise boundaries) and difficult to quantify in any computation process. Although the conventional mathematical techniques are widely applied to the analysis of many non-mechanistic systems, which try to mimic the working principles of human beings, it is understood that the increase in the complexities of such systems needs the approaches that are significantly different from the traditional system and are highly effective when applied to mechanistic systems, but are far away from precision in which human behaviour plays an important role [20].The linguistic approach, as proposed by Zadeh [21], is one such method where words or sentences are used instead of numbers to describe complex or ill-defined phenomenon. Such linguistic characterization describes the behavior without sharply defining the boundaries and serves to provide an approximation rather than exact description of the value of a variable. Fuzzy inference system based on linguistic representation of information is a robust decisive tool, which mimics the way human beings make decisions based on imprecise data. The motivation behind the fuzzy logic based system design lies in its ability in exploiting the tolerance of imprecise, uncertainty and approximate reasoning in order to achieve tractability, robustness, low solution cost and close resemblance with human like decision making.The motivating factors in the use of fuzzy logic based system modelling can be listed in the following:•Fuzzy logic is conceptually easy to understand.Fuzzy logic is flexible.Fuzzy logic is tolerant of imprecise data.Fuzzy logic can model nonlinear functions of arbitrary complexity.Fuzzy logic can be built on top of the experience of experts.Fuzzy logic can be blended with conventional control techniques.Fuzzy logic is based on natural language.The last statement is perhaps the most important one and deserves more discussion. Natural language, which is used by human being, has been shaped by thousands of years of human history to be convenient and efficient. Sentences written in ordinary language represent a triumph of efficient communication.A fuzzy logic based system can be characterized as the nonlinear mapping of an input data set to a scalar output data. A fuzzy system consists of four main parts: fuzzifier, rules, inference engine, and defuzzifier. These components in the conventional schematic flow diagram of fuzzy system are shown in Fig. 1.In the process, firstly, a crisp set of input data is converted to a fuzzy set using fuzzy linguistic variables, fuzzy linguistic terms and membership functions. This step is known as fuzzification. Next, an inference is made based on a set of rules. Lastly, the resulting fuzzy output is mapped to a crisp output in the defuzzification step.In the recent past, several attempts have been made in the construction of fuzzy granules with desired level of granulation. The process of fuzzy granulation or f-granulation involves the basic idea of generating a family of fuzzy granules from numerical features and transforms them into fuzzy linguistic variables. These variables thus keep the semantics of the data and are easy to understand. Fuzzy information granulation has come up as an important concept in fuzzy set theories, rough set theories and the combination of both in recent years [14,22–24]. In general, the process of fuzzy granulation can be broadly categorized as class-dependent (CD) and class-independent (CI). With CI granulation each feature space is granulated/described irrespective of the classes of available patterns. The granulation process follows the concept of “decomposition of whole into parts”, and information of feature is described with the membership values corresponding to the linguistic properties. One can describe the feature with one or more number of overlapping linguistic variables with different properties for desired level of granulation. For example, the authors in [25] fuzzified the input feature value with the overlapping partitions of the linguistic properties low, medium and high. These overlapping functions along each of the axes generate the fuzzy granulated feature space in n-dimension and the granulated space contains G×n granules, where G is the number of linguistic properties. The degree of belonging of a pattern to a granule is determined by the corresponding membership function. While this model is effective in representing input in any form in a knowledge based system, the process of granulation does not take care of the class belonging information of features to different classes. This may lead to a degradation of performance in a decision making process, particularly for data sets with highly overlapping classes. On the other hand, in CD granulation, each feature explores its class belonging information to different classes. In this process, features are described by the fuzzy sets equal to the number of classes, and the generated fuzzy granules restore the individual class information. These are described in Fig. 2for four classes in a two dimensional plane. Note that the number of granules generated in CD granulation is Cnwhere C is the number of classes, whereas for CI granulation it is 3ncorresponding to three linguistic properties low, medium and high in an n dimensional plane.These granulated features, thus generated, can be considered as input to any classifier for classification. An example application of this concept is demonstrated recently by Pal et al. in [26] with various patterns including speech data and remote sensing images. Some comparative results for classification of vowel sounds are given in Figs. 3 and 4.Fig. 3(a) demonstrates the merit of the granulated feature space over non-granulated feature space, and CD over CI granulation considering the following three models with 1-NN classifier.•model 1: 1-NN classifier,model 2: CI fuzzy granulation + rough set based feature selection + 1-NN classifier,model 3: CD fuzzy granulation + rough set based feature selection + 1-NN classifier.It is clear from Fig. 3(a) that model 2 and model 3 with granulated feature space perform better compared to non-granulated model 1. Among the granulated feature spaces, model 3 with CD granulation is superior to CI. Similar performance comparison of these models for the same data using other classifies (e.g., 3-NN, 5-NN, Baye's maximum likelihood and multilayer perceptron is shown in Fig. 4. However, as expected, the granulated models take longer computation time; CD being the most expensive (Fig. 3(b)).Over the years, fuzzy logic has been applied successfully in many areas including•Pattern recognition and classification.Image and speech processing.Fuzzy systems for prediction.Fuzzy control.Monitoring.Diagnosis.Optimization and decision making.Group decision making.Hybrid intelligence system.The application areas can be broadly grouped in three categories such as fuzzy logic control, fuzzy expert systems, and fuzzy pattern recognition and image processing. In fuzzy control systems, the aim is to replace/ emulate human operators, in fuzzy expert systems design the aim is to replace/ emulate human expertise, whereas the fuzzy pattern recognition and image processing systems incorporate generalization of crisp decisions and uncertainty handling.One may note that first paper of Zadeh in pattern recognition appeared in 1966 [27] following a technical report in 1965, whereas his first paper in fuzzy control appeared in 1973 [28]. It means Zadeh had the concept (notion) of fuzzy classification mainly when he was developing the theory, although his fuzzy control systems got massive success in 1980's because of its implementation in Japanese products. Since techniques of pattern recognition and image processing interact with and support a large percentage of control systems (e.g., Mars rover control, camera tracking and docking at space shuttles, fuzzy camcorders, traffic flow control), applications of pattern recognition and vision systems have matured, especially because of commercial success of Japanese products based on fuzzy logic control.We pose a list of limitations of fuzzy logic based systems that certainly become open problems for the current research community.•Fuzzy systems lack the capabilities of machine learning, as well as a neural network-type memory. Therefore, hybrid systems (e.g., neuro-fuzzy systems) are becoming more popular for specific applications.Verification and validation of a fuzzy knowledge-based system typically requires extensive testing with hardware in the loop. This is an expensive affair.Determining exact fuzzy rules and membership functions is a hard task, although some fuzzy-genetic approaches have been made. One cannot often predict how many membership functions are required even after wide testing.Stability is an important concern for fuzzy control.A number of assumptions are required particularly in a fuzzy control system design, e.g.,–The plant is observable and controllable: state, input, and output variables are usually available for observation and measurement or computation.There exists a body of knowledge comprising a set of linguistic rules, engineering common sense, intuition, or a set of input-output measurement data from which rules can be extracted.The control engineer is looking for a “good enough” solution, not necessarily the optimum one.The controller will be designed within an acceptable range of precision.The problems of stability and optimality are not addressed explicitly: such issues are still open problems in fuzzy controller design.Dealing with uncertainty, imprecision and vagueness is the inherent characteristics of human thinking and behavioural process. The computing that exhibits such functionalities generally comes in the purview of natural computing. As described in the previous section, fuzzy logic based granular computing is one of such frameworks of natural computing. In another aspect of granular information processing phenomenon, rough set theory [29] has emerged that incorporates the above characteristics in its computation. Rough sets under set-oriented view are closely related to fuzzy sets, which lead to non-truth-functional fuzzy set operators. Fuzzy set theory deals with ill-defined and unsharp boundaries and rough set characterises a crisp set with a coarsely defined class boundary. In other words, rough sets are nothing but crisp sets with rough descriptions.The main advantages of rough set based granulation in data analysis is that it enables the discovery of data dependencies and performs the reduction/selection of features contained in a data set using the data alone, requiring no additional information, such as basic probability assignment in Dempster-Shafer theory, grade of membership or the value of possibility in fuzzy set theory. In addition, the rough set based granular method has many important advantages, such as it finds hidden patterns in data, finds minimal sets of data (data reduction), evaluates significance of data, generates sets of decision rules from data and facilitates the interpretation of obtained result.Rough mereologyRough mereological granules are constructed from rough inclusions and to approximate partial containment operators [30,31]). Rough mereology (RM) is a method for synthesis and analysis of objects in the distributed environments of intelligent agents. In a complex environment like synthesis of objects for a given specification to a satisfactory degree, or for control, rough mereology plays a key role. Moreover, it has been used recently for developing foundations of the information granule calculus. RM is an attempt towards formalization of the paradigm of computing with words based on perception, as formulated by Zadeh [14,21].RM is built based on the inclusion relation. This method is also considered as approximate calculus of parts, an approach to reasoning under uncertainty based on the notion of an approximate part (part to a degree). The paradigms of granular computing, computing with words and spatial reasoning are particularly suited to a unified treatment by means of RM. It is a generalization of the rough set and fuzzy set approaches [30,31]. This relation can be used to define other basic concepts like closeness of information granules, their semantics, indiscernibility / discernibility of objects, information granule approximation and approximation spaces, perception structure of information granules as well as the notion of ontology approximation. The rough inclusion relations together with operations for construction of new information granules from already existing ones create a core of a calculus of information granules.The theory of rough sets is motivated by practical needs to interpret, characterize, represent, and process indiscernibility of individuals. For example, if a group of students in a class is described by having symptoms of cold related deceases, many students would share the same symptoms, and hence are indistinguishable. This motivates to think of a subset of the students as one unit (i.e., granule), instead of many individuals. Rough set theory provides a systematic method for representing and processing vague concepts caused by indiscernibility in situations with incomplete information or a lack of knowledge. It deals with the approximation of sets under indiscernibility in terms of lower and upper approximations. Rough set theory along with the fuzzy set is motivated by the practical needs to manage and process uncertainty, inherent in real world problem solving, as performed by a human being.The basic assumption of rough set theory is that human knowledge about a universe depends upon their capability to classify its objects, and classifications of a universe and equivalence relations defined on it are known to be interchangeable notions. To improve the modeling capability of basic rough set theory, several extensions have been made in different directions. The present article briefs these extensions as follows:The granulation of objects induced by an equivalent relation is a set of equivalence classes, in which each equivalence class can be regarded as an information granule, as described by Pawlak [32]. One extension to this is based on tolerance relations instead of equivalence relations. These rough sets are sometime called incomplete rough sets. The granulation of objects induced by a tolerance relation generates a set of tolerance classes, in which each tolerance class also can be seen as a tolerance information granule. Another method of information granulation is characterized by using a general binary relation, where objects are granulated into a set of information granules, called a binary granular structure.With the notion of granular computing, a general concept portrayed by Pawlak's rough set is always characterized with upper and lower approximations under a single granulation. The basic operation involved in rough set is that it partitions the object space based on a feature set using some equivalence relation. The partition spaces thus generated are also known as granules, which become the elemental building blocks for data analysis. The rough representation of a set with upper and lower approximations is shown in Fig. 5. Thus the concept is depicted by known knowledge induced by a single relation on the universe, which includes equivalence relation, tolerance relation, reflexive relation, and many more. This clearly states that given any such relations, one can determine a certain granular structure (or called a granular space). Pawlak's rough set generally takes the following assumption in describing an objective/decision/target concept.Let A and B be two conditional sets of features and X⊆U be a decision feature, then the rough set of X is derived from the quotient set U/(A∪B). The quotient set is equivalent to the formula(1)A∪Bˆ={Ai∩Bi:Ai∈U/A,Bi∈U/B,Ai∩Bi≠∅}This relation clearly implies that an intersection operation can be performed between any Aiand Bi, and the decision feature is approximately described by using the quotient set U/(A∪B). Moreover, the decision feature can be described using a partition of the space that generates fine granules through combining two known granulations (partitions) induced from two-attribute subsets. Although it generates a much finer granulation and more knowledge, the combination/fining destroys the original granulation structure/partitions. However, this assumption cannot always be required in practice. Three practical cases are mentioned below to demonstrate its restrictions.•CASE 1 For the same object of a data set, if any contradiction or inconsistent relationship exists between its values under two attributes sets A and B, the intersection operations between their quotient sets and the target concept cannot be approximated by using U/(A∪B).CASE 2 For the same object or element, the decisions are different. Under such circumstance, the intersection operations between any two quotient sets will be redundant for decision making.CASE 3 For the reduction of the time complexity of knowledge discovery, it is unnecessary to perform the intersection operations in between all the sites in the context of distributive information systems.In such cases, the decision features need to be described through multi binary relations (e.g., equivalence relation, tolerance relation, reflexive relation and neighborhood relation) on the universe, and this is purely according to the problem in hand. For many practical issues, rough set theory is applied widely with the concept of multigranulation rough set framework based on multi equivalence relations. A detail description of multigranulation method of approximating the solution can be referred to [33,34].Rough set theory allows characterization of a set of objects in terms of attribute values; finding dependencies (total or partial) between attributes; reduction of superfluous attributes; finding significant attributes; and decision rule generation. Basically two main characteristics, namely, uncertainty analysis through lower and upper approximations, and granular computing through information granules have drawn the attention of applied scientists. Accordingly, these properties made the theory an attractive choice in dealing with real-time complex problems, such as in:•Pattern recognition and image processing.Artificial intelligence.Data mining and knowledge discovery.Emergency room diagnostic.Acoustical analysis.Power system security analysis.Spatial and meteorological pattern classification.Intelligent control systems.Measuring the quality of a single subset.The generalization of rough set reduction:•Reduction leads to over fitting (over training) in the training sample space.Rough sets characterize the ambiguity of decision information systems, but the randomness is not studied.Models,–Pawlak's RST.VPRS.Decision-Theoretical rough sets.Probabilistic rough sets.Covering and neighborhood.Concept lattice.Data,–Incomplete data.Uncertainty measure.Large data.Incremental learning.Algorithms,–Reduction.Hybrid.Neural networks.Genetic algorithm.Fuzzy sets.Data analysis.Applications.Human intelligence and discriminating power is mainly attributed to the massively connected network of biological neurons in the human brain. Artificial neural network (ANN) is a system composed of many simple processing elements (nodes) operating in parallel whose function is determined by network structure, connection strength of links (weight), and processing performed at computing elements or nodes. It provides an alternative information processing approach, though in a naive manner, to biological neural networks (BNN). Its major characteristics are adaptivity (adjusts to a change in environment/new data/information), speed (via massive parallelism), fault tolerance (to missing, confusing and/or noisy data), ruggedness (to failure of nodes/links), and optimality (as regards to error rates in classification).The ANN is usually trained with a given set of numerical input output data, where the numerical weights of the network are adapted and provide a knowledge mapping between input and output data. The trained network then uses the discovered knowledge to predict the behavior of the output for new set of input data. ANNs are a useful tool for knowledge discovery mainly because of these characteristics. Quite often, data sets are linguistic or symbolic in nature instead of discrete numerical (as mentioned above) and become unfit for the conventional neural networks. For those cases, the network must be capable of processing both numerical and linguistic data, which turns out to be a challenging task. To deal with this, a new version of neural network called granular neural network (GNN) [35–37] has been evolved by incorporating the granular concept in the neural network design. In general, the GNN is capable of processing granular data (such as numerical and linguistic data), extracting granular information, fusing granular data sets, compressing a granular data base, and predicting new data.In granular data, the granules can be a class of numbers, a cluster of images, a set of concepts, a group of objects, a category of data, etc. These granules are input and output of GNN, just like any natural data are input and output of biological neural networks in the human brain. Therefore, granular-data based GNN is more useful and effective to process natural information of granules than conventional numerical-data-based neural networks. In the following section various aspects of GNN are described with different designs architectures.The basic biological motivations of GNN are•It enjoys the similarity of ANN to biological neural networks in many senses and builds models to understand various nerves and brain operations by simulation.It mimics various cognitive capabilities of human beings and solves problems, through learning/adaptation, prediction, and optimization.It allows for parallel information processing through its massively parallel architecture, and makes the model more robust to fault tolerant.Granular neural network is the combination of neurons that construct, design and operate at the level of information granules. In other word, owing to the fundamental role of the information granules, the ensuing neural networks will be referred to as granular neural network. Not much attention has been paid to formal frameworks of information granulation applied to the design of neural networks.Development of GNNs involves two main phases: Granulation of numeric data where a collection of information granules is formed, and the construction of the neural network, where any learning that takes place with the neural network should be based on the information granules rather than the original data.ANN is a multi disciplinary field and as such its applications are numerous including•Finance.Industry.Agriculture.Physics.Statistics.Cognitive science.Neuroscience.Weather forecasting.Computer science and engineering.Spatial analysis and geography.GNNs, like ANNs, have several good features including, learning and generalization ability, adaptivity, content addressability, fault tolerance, self-organization, robustness, optimality and simplicity of basic computations. Additionally, GNN has the advantage in having the possibility to look inside the network structure, which is assumed to be a black box in case of ANN. Looking inside into the network enables one to gain more knowledge beforehand to process/optimize the parameters that govern the network decisions. However, both the models suffer from various limitations to yield the best results. These include, long learning time, difficulty in incorporating the available domain knowledge, instability problem, performance variation with different training conditions or change in network parameters, and handling ambiguous information. In addition, GNN has some crucial issues that need particular attention such as representation process of information granules and the kind of formalism to be used in diverse practical environments.Designing an integrated granular information processing system with more than one granular method has been popularly undertaken by the researchers to solve complex problems. The motivation is to combine the merits of individual techniques in order to design a system that can exploit the tolerance for imprecision, uncertainty, approximate reasoning, and partial truth in order to achieve tractability, robustness, and low cost solution in real-life ambiguous situations. Many attempts have been made in this regard, where the merits of three basic methods (neural networks, roughs sets and fuzzy sets) of information granulation are hybridized in different combinations [22,38]. During integration each paradigm helps other, rather than competing with. In the following sections, we briefly describe some such integration with applications.Integration of neural networks (NNs) and fuzzy logic provides a hybrid paradigm known as neuro-fuzzy (NF) computing [39] which is the most visible one realized so far among all other integrations in soft computing. This hybridization aims to provide more intelligent systems (in terms of performance, parallelism, fault tolerance, adaptivity, and uncertainty management) than the individual one to deal with real-life complex decision making problems.Both NNs and fuzzy systems work with the principle of adaptivity in the estimation of input-output function without any precise mathematical model. NNs handle numeric and quantitative information while fuzzy systems handle symbolic and qualitative data. Therefore, an integration of neural and fuzzy systems explores the merits of both and enables one to build more intelligent decision making systems. The judicious integration may be viewed another way as – if ANN provides the hardware, fuzzy logic provides the software of a system. In the NF paradigm, many research efforts have been made in the recent past [39–41]. NF hybridization is done broadly in two ways: NNs that are capable of handling fuzzy information to augment its application domain (named as fuzzy-neural networks (FNN)), and fuzzy systems augmented by NNs to enhance some of their characteristics such as flexibility, speed and adaptability (named as neural-fuzzy systems [39]. The details on these methodologies can be found in [39].Rough-fuzzy hybridization [22,42] is relatively new and has a strong promise in designing intelligent systems. It is based on the following premises: Both rough set and fuzzy set theories complement each other and constitute two important modules in uncertainty handling. While fuzzy sets deal with ambiguity due to overlapping concepts/regions, rough sets handle the same arising from granularity in the domain of discourse. Therefore these two can be judiciously integrated to form a paradigm for uncertainty handling which is stronger than either. In some applications, fuzzy set theory is used for linguistic representation of patterns, leading to a fuzzy granulation of the feature space, rough set theory is used to obtain dependency rules, which models informative regions in the granulated feature space. Another class of applications considers rough crisp granules in defining class exactness whereas fuzzy sets in modelling class overlapping character. Besides these, one may note that in real life problems, both the set and granules in Pawlak's rough set (Fig. 5) can be fuzzy. In other words, the concept of fuzziness can be incorporated in defining both the set and granules; thereby resulting in generalized rough sets [43]. Depending on whether the set and /or granules have fuzzy boundaries or not, one can have rough-fuzzy set of X (when X is fuzzy and granules are crisp), fuzzy-rough set of X (when X is crisp and granules are fuzzy), and fuzzy rough-fuzzy set of X (when both X and granules are fuzzy).Rough-fuzzy computing models have been used for case based reasoning, pattern recognition, image processing as well for mining large data sets, among others. Depending on whether the granules and computing are crisp or fuzzy, one may have ”granular fuzzy computing” and ”fuzzy granular computing”. In the former case granules are crisp and the computing is fuzzy, while in latter the computing is crisp with fuzzy granules. Some results in the aforesaid lines, referred from literature, are briefly described in the following sections.In an attempt in [44], the authors discussed the rough-fuzzy hybridization method of information granulation scheme for case generation that is used for classification task. In this study, fuzzy set theory is used for linguistic representation of patterns, thereby producing a fuzzy granulation of the feature space and rough set theory is used to obtain dependency rules. The fuzzy membership functions corresponding to the informative regions are stored as cases along with the strength values, and case retrieval is made using a similarity measure based on these membership functions. Unlike the existing case selection methods, the cases here are cluster granules and not sample points. An example (referred from [44]) of case generation is shown in Fig. 6.In the example (Fig. 6), a data set with two features (F1 and F2) and two classes, is considered. The granulated feature space has 32=9 granules. These granules are characterized by three membership functions (low, medium and high) along each axis, and have ill-defined boundaries. The following two dependency rules representing two cases are obtained from the reduced attribute table:class1←L1∧H2,dependency=0.5,andclass2←H1∧L2,dependency=0.4For assigning a label or classifying an unknown pattern, the case closest to the pattern, in terms of a similarity measure, is retrieved and its class label is assigned to that pattern. Ties are resolved using the parameter case strength (dependency). Detail description of the approach can be obtained from [44]. Comparative results of the rough-fuzzy case generation methodology with other case selection algorithms (Instance-based learning algorithm, IB3, Instance-based learning algorithm with reduced number of features, IB4 and Random case selection), are presented in Fig. 7[44] for the IRIS data set, in terms of number of cases, 1-NN classification accuracy, average number of features per case (n(avg)), and case generation (t(gen)) and retrieval (t(ret)) times. It can be seen from Fig. 7 that the cases obtained using the proposed rough-fuzzy methodology are much superior to random selection method and IB4, and close to IB3 in terms of classification accuracy. The method requires significantly less time compared to IB3 and IB4 for case generation. The average number of features stored per case (n(avg)) by the rough-fuzzy technique is much less than the original data dimension (n). As a consequence, the average retrieval time required is very low. IB4 also stores cases with a reduced number of features and has a low retrieval time, but its accuracy is much less compared to the proposed method. Moreover, all the cases involve equal number of features, unlike in [44] where the number of features may be different from case to case.In the above example, the granules considered are class independent. Recently, Pal et al. in [26] described a rough-fuzzy model for pattern classification where they formulated class-dependent granules. Fuzzy membership functions are used to represent the feature-wise belonging to different classes, thereby producing fuzzy granulation of the feature space. The fuzzy granules, thus generated, possess better class discriminatory information in classification of overlapping classes. Rough sets are used in the selection of a subset of granulated features. Some example classification results (referred from [26]) obtained with remote sensing images is shown in Fig. 8. It is clear from these figures that the model with rough-fuzzy granulation performs better than non-granulated model in segregating different areas by properly classifying the land covers.The automatic generation of feature based if-then rules, is essential to the success of many intelligent pattern classifiers, especially when their inference results are expected to be directly human-comprehensible. In this regard, several rough-fuzzy methods have been developed [45]. Jensen et al. described such an approach for rule induction and feature selection. Since, applications of rough set theory involve the processing of equivalence classes for their successful operation; it is natural to combine them into a single integrated method that generates concise, meaningful and accurate rules.Similar hybrid models are also used for clustering problems. For example, Maji and Pal [46] developed three hybrid algorithms, namely, rough-fuzzy c-means (RFCM), rough possibilistic c-means (RPCM), and rough-fuzzy possibilistic c-means (RFPCM) for clustering and compared the performance with different c-means algorithms. Here fuzzy sets enable handling of overlapping partitions, whereas rough sets deal with vagueness and incompleteness in class definition. Vagueness and incompleteness in class definition of the data sets are dealt with the concept of lower and upper approximations of rough sets, and the uncertainty arising from overlapping partitions of the classes is handled with the membership function of fuzzy sets. Both probabilistic and possibilistic memberships are involved simultaneously to avoid the problems of noise sensitivity of fuzzy c-means and the coincident clusters of possibilistic c-means (PCM). The algorithms are generalized in the sense that all the existing variants of c-means algorithms can be derived from the proposed algorithm as a special case. The effectiveness of the algorithm, along with a comparison with other algorithms, has been demonstrated both qualitatively and quantitatively on real-life brain MR images. We have referred some results from [46] in Fig. 9, which demonstrate the potentiality of the rough-fuzzy hybrid models (RFPCM) over others e.g., hard c-means (HCM), fuzzy c-means (FCM), fuzzy possibilistic c-means (FPCM), rough c-means (RCM), kernel based HCM (KHCM) and kernel based FCM (KFCM) in terms of DB index.With the cost of computational complexities, recently, there have been a large number of activities undertaken to integrate fuzzy set theory, neural networks, genetic algorithms, and rough set theory, for generating more efficient hybrid systems. Efficiency is primarily in terms of training time, network size and accuracy of solution. For example, consider the scheme of knowledge encoding in a fuzzy multilayer perceptron (MLP) using rough set-theoretic concepts as described in [47]. Here the basic domain knowledge is extracted in the form of different rules for determining the appropriate number of hidden nodes, while the dependency factors are used in the initial weight encoding. The training process refines the network subsequently. The comparative results on classification of speech, as an example (referred from [47]), are shown in Fig. 10. This demonstrates the superiority of the hybrid systems (involving rough, fuzzy and neural networks), over the fuzzy MLP (i.e., integration of fuzzy and conventional MLP).In another approach, a methodology is described for evolving a rough-fuzzy multi layer perceptron with modular concept. The modular concept is based on the strategy of “divide and conquer” that uses genetic algorithm in order to get a structured network suitable for both classification and rule extraction. Rough set dependency rules are generated directly from the real valued attribute table containing fuzzy membership values. A comparative result, referred from [48], is shown demonstrating its superiority. The result compares the performances of five models:•Model 1: Conventional MLP trained using back propagation (BP) with weight decay.Model 2: A fuzzy MLP trained using BP (with weight decay).Model 3: A fuzzy MLP trained using BP (with weight decay), with initial knowledge encoding using rough sets [32,47].Model 4: A modular fuzzy MLP trained with Genetic algorithms along with tuning of the fuzzification parameters. The modular term refers to the use of subnetworks corresponding to each class, that are later concatenated using GAs.Model 5: Modular genetic-rough-neuro-fuzzy algorithm [48].It is observed from Table 1that Model 5 performs the best (except the accuracy for Model 3) with the least network size as well as least number of sweeps for vowel classification.Recently, a fuzzy rough granular neural network (FRGNN) model based on the multilayer perceptron using a back-propagation algorithm for fuzzy classification is described in [49]. The authors have developed the network based on the input vector, initial connection weights determined by fuzzy rough set theoretic concepts, and the target vector. In this approach, fuzzy granules describe the input vector and fuzzy class membership values define the target vector. The domain knowledge about the initial data is represented in the form of a decision table, which is divided into sub tables corresponding to different classes. The data in each decision table is converted into granular form that determines the appropriate number of hidden nodes. The dependency factors from all the decision tables are used as initial weights.This investigation demonstrates the efficiency of integrating fuzzy rough sets with a fuzzy neural network, as well as provides a method that generates granular neural network architecture and improves its performance. The fuzzy rough set provides a means where the discrete real-valued noise data can be effectively reduced without the need for any user-supplied information. Additionally, the fuzzy partitions corresponding to each attribute can be automatically derived by fuzzy similarity relations and fuzzy logical connectives. As described in [49], we have referred the comparative results of the method with the rough-fuzzy-MLP for VOWEL data set as in Fig. 11. The results clearly support the superiority of FRGNN with fuzzy initial weights to rough-fuzzy-MLP.A hybrid model involving four constituents namely, rough sets, fuzzy sets, neural networks and genetic algorithm can be developed as shown in Fig. 12. Fuzzy granules encode linguistic information as input to a layered network, and represent the uncertainty arising from overlapping class boundaries. Rough granules encode the domain knowledge to incorporate them into network parameters so that the network can start learning from a better initial position; thereby reducing the overall learning time. Genetic algorithms are used to evolve the input granulation parameters, class boundaries and the network parameters during training. It therefore removes the use of conventional back propagation algorithms and reduces the possibility of the network getting stuck to local minima. The resultant granular network would gain in terms of performance, learning time and uncertainty handling.

@&#CONCLUSIONS@&#
An overview of the significance of natural computing with respect to the granulation-based information processing models is described. Models considered include fuzzy sets, rough sets, neural networks and their hybridization. Biological motivation, design principles, granular characteristics and applications of these models with open research problems are highlighted. Significance of fuzzy granulation (f-granulation) is explained through example results both from fuzzy set and rough set theoretic approaches. Among the various hybrid paradigms, rough-fuzzy computing has a strong promise in handling uncertainty in real life problems. It is also explained through examples how the said concept can be integrated with other soft computing theories resulting in application specific merits.It may be mentioned here that computational theory of perception (CTP) [50,51] provides capability to compute and reason with perception based information. Human have remarkable capability to perform a wide variety of physical and mental tasks without measurement and computations. They use perception of time, space, direction, speed, shape, possibility, truth and other attributes of physical and mental objects. Reflecting the finite ability of the sensory organs (and finally the brain) to resolve details, perceptions are inherently imprecise. That is, the boundaries of perceived classes are un-sharp, and the values of attributes they can take are granulated. In other words, perceptions are fuzzy (f)-granular. f-Granulation is inherent in human thinking and reasoning process, and plays an essential role in human cognition. Fuzzy-rough computing paradigm appears to have enormous future challenge in modeling the f-granular characteristics of CTP and human cognition.Some other challenging issues in the aforesaid hybrid granular systems include•Choice of granules: It is crucial to select the size, shape and number of granules depending on the problem. Granules could be class dependent, independent and/or evolved automatically.Continuous learning from data: The system should be intelligent enough to learn from data in a continuous, incremental way, able to grow as they operate, update their knowledge and refine the model through interaction with the environment.Computational complexities: This is an obvious issue in any hybrid system. The individual techniques of a hybrid system should symbiotically overcome the demerits/limitations of other techniques rather than escalating the issues.Relationship among different granular models developed with fuzzy set, rough set, ANN and their integrations.Synthesis of nature by means of computation with the aforesaid models.Soft computing methods mimic a variety of the phenomena observed in biological systems, nature, and human reasoning. Thus, a general framework that can cover all the computation methods and the characteristics may be developed. This framework may also lay a solid theoretical basis for the aforementioned future exploratory research problems.