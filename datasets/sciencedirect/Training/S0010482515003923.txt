@&#MAIN-TITLE@&#
A denoising algorithm for projection measurements in cone-beam computed tomography

@&#HIGHLIGHTS@&#
We develop a new denoising algorithm for CT projections.The proposed algorithms includes regularizations in terms of gradient and Hessian.We suggest solving the problem using a split Bregman iteration.We apply the proposed algorithm on simulated and real noisy CT projections.Our algorithm effectively suppresses noise and improves spatial resolution.

@&#KEYPHRASES@&#
Low-dose computed tomography,Sinogram denoising,Sparsity-based denoising,Cone-beam,Bregman method,Total variation denoising,

@&#ABSTRACT@&#
The ability to reduce the radiation dose in computed tomography (CT) is limited by the excessive quantum noise present in the projection measurements. Sinogram denoising is, therefore, an essential step towards reconstructing high-quality images, especially in low-dose CT. Effective denoising requires accurate modeling of the photon statistics and of the prior knowledge about the characteristics of the projection measurements. This paper proposes an algorithm for denoising low-dose sinograms in cone-beam CT. The proposed algorithm is based on minimizing a cost function that includes a measurement consistency term and two regularizations in terms of the gradient and the Hessian of the sinogram. This choice of the regularization is motivated by the nature of CT projections. We use a split Bregman algorithm to minimize the proposed cost function. We apply the algorithm on simulated and real cone-beam projections and compare the results with another algorithm based on bilateral filtering. Our experiments with simulated and real data demonstrate the effectiveness of the proposed algorithm. Denoising of the projections with the proposed algorithm leads to a significant reduction of the noise in the reconstructed images without oversmoothing the edges or introducing artifacts.

@&#INTRODUCTION@&#
There is always a great desire to reduce the amount of the radiation dose in x-ray computed tomography (CT) because of the health risks. Many methods have been proposed to address this problem. While many of these methods aim at improving the system hardware and imaging protocols [29,30], in recent years there has been an increased interest in the statistical and iterative reconstruction algorithms [40]. The major driving force behind this trend is the steady increase in the available computational power, commonly known as Moore׳s law, that can lead to clinically acceptable reconstruction times even for large volume reconstructions at high resolution. An optimal approach to reducing the radiation dose could include a combination of modifications to the system hardware and settings along with effective image reconstruction algorithms. To reconstruct images of high quality from a smaller number of measurements in the presence of significant noise, these algorithms should rely on accurate models of the imaging process and state-of-the-art signal processing tools.The statistical reconstruction algorithms for CT can be divided into three broad categories [22,2]. The first category of these algorithms is the pre-processing algorithms. These are algorithms that try to denoise or restore the projection measurements that can be then used by analytical or iterative reconstruction algorithms to produce an image. The second category includes the post-processing methods, which work in the image domain. In other words, they aim at improving the reconstructed image via denoising, removing artifacts, etc. The third category includes iterative reconstruction algorithms. They start with a usually poor initial estimate of the image and improve the image estimate in an iterative fashion. In each iteration, these iterative reconstruction algorithms compute the line integrals of the current image estimate (via forward projections) and compare the computed line integrals with the actual measurements. The difference between the computed line integrals and actual measurements is used to improve the image estimate. Each of the above three categories of algorithms have their own pros and cons. For instance, the third category (iterative reconstruction algorithms) is more powerful and is known to outperform the pre-processing and post-processing methods, but it requires substantially more computation. Between the first two categories of methods, the pre-processing algorithms require access to the raw projection measurements, which are not always available on commercial scanners. Moreover, it is usually more difficult to preserve the edges by pre-processing algorithms because sharp edges in the image domain are somewhat smoothed in the projection measurements. On the other hand, pre-processing algorithms can rely on more accurate noise models [21]. This is because it is much easier to accurately model the noise in the projection measurements, whereas the noise in the reconstructed image is much more complex and difficult to model because it is spatially varying and depends on the reconstruction algorithm used to produce the image. Furthermore, projections that are denoised/restored by the pre-processing algorithms can be used by any analytical or iterative reconstruction algorithm. Therefore, pre-processing algorithms are independent of the reconstruction algorithm used to produce the final image. The focus of this paper is on the pre-processing methods, specifically sinogram denoising algorithms.The denoising of the projection measurements, known as the sinogram in the CT community, can be a very effective approach to exploiting the knowledge of the physics of the imaging process to improve the quality of the images reconstructed from low-dose measurements. Sinogram denoising methods are of two types. The first type do not explicitly use the photon statistics. These algorithms can be as simple as that of using a shift-invariant low-pass filter that treats all measurements equally [21,31]. Considering that the noise in the sinogram is signal-dependent, this is clearly a sub-optimal approach. Therefore, improved performance has been reported when filter parameters are changed adaptively based on the characteristics of the signal and the noise [18,20,42,47]. Multiresolution denoising methods have also been proposed for sinogram denosing [19,37]. In these methods, the noisy signal is transformed into a wavelet of short-time Fourier domain, where the difference in the statistical distribution of the coefficients of the noise and the signal are used to remove or reduce the noise. Instead of using standard bases such as wavelets, another approach is to learn an overcomplete dictionary from a set of low-noise measurements [39]. A set of low-noise sinograms are used to learn an overcomplete dictionary, which can later be used for sparse representation of patches of noisy sinograms acquired under low dose. The idea is that the signal part of the noisy sinogram has a sparse representation in the learned dictionary, whereas the noise part does not have such a representation. Therefore, sparse approximation of the patches in the overcomplete dictionary will automatically lead to denoising.The second type of sinogram denoising algorithms include those that explicitly model the noise via the probability distribution function of the measurements. These algorithms find an estimate of the true measurements by maximizing the data likelihood. However, since the resulting problem is ill-posed, a regularization term is usually added to the data likelihood term [14,23,25,28,41]. There are many different choices of the regularization terms and optimization methods that can lead to quite different algorithms. Examples of regularization functions that have been previously used in sinogram denoising include the weighted sum of the squared differences between neighboring pixel values [49,23] or a Gaussian function thereof [28], splines [13], and Markov random fields characterized by a Gibbs distribution [25]. Moreover, the data likelihood term may have one of two forms: in the original raw data domain, the measurements are the photon counts that follow a Poisson or compound Poisson distribution [10,11], whereas in the Radon space (i.e., after the logarithm transformation) the measurements have an approximately Gaussian distribution [27,43]. These variations suggest that the resulting algorithms can be very different in terms of complexity and performance.In this paper, we propose a sinogram denoising algorithm for cone-beam CT. The proposed algorithm is based on minimizing a cost function that is composed of a data likelihood term and two regularization terms. The data likelihood term is based on the Poisson distribution of the photon counts, while the regularization terms reflect the assumption that the sinograms are piecewise-smooth. We will use an iterative split Bregman algorithm [16,48] for minimizing the cost function. We will apply the suggested algorithm on simulated and real cone-beam CT data and compare it with a recently published sinogram denoising algorithm based on bilateral filtering [28].Our focus in this study is on cone-beam CT, where the sinogram is composed of a set of 2D images, each corresponding to one rotation angle of the x-ray source and detector panel around the object. We denote a true and noisy projection with u and v, respectively, and assume that they are of sizem×n. Individual pixels of a projection, for example for u, is denoted asu(i,j),i=1tom,j=1ton.In a variational denoising formulation, the denoised image, u, is estimated from the noisy measurement, v, by minimizing a functional of the following form:(1)E(u)=Ψ(u,v)+R(u)whereΨ(u,v)is a measure of the distance between the noisy and the denoised images and R(u) is a regularization term that reflects some prior information about the image. When the noise has a Poisson distribution, an appropriate form forΨ(u,v)is [24]:(2)Ψ(u,v)=∫Ω(u−vlogu)where Ω denotes the image domain. As for the regularization term, there are many different functions to choose from and a smart choice should be based on two main considerations. First, the regularization term should properly model our prior knowledge about the signal that we want to denoise. Second, the minimization of the resulting functional E(u) in (1) should be tractable. Examples of the regularization functions that have been previously used in sinogram denoising are mentioned in the previous section. Another example is the total variation (TV) and its variations [36,7] that have proved to be very powerful in various image processing tasks including denoising. This regularization has the following form:(3)R(u)=∫Ω|∇u|where∇uis the gradient of u. Because it is based on the ℓ1-norm of the gradient, this regularizer is very effective in preserving image edges while suppressing the noise. Therefore, it has been tremendously successful in reconstruction, deconvolution, and denoising of piecewise-constant images. In recent years, there has been a growing body of research in improving the capabilities of this model [7]. Perhaps the most significant enhancements have been achieved by including higher-order differentials in the model [6,38]. This change leads to superior results on images that contain piecewise-smooth features. On this type of image, the above regularizer, (3), leads to artificial blocky features known as staircase effect because with this formulation piecewise-constant solutions are preferred. Including higher-order differentials, on the other hand, will encourage piecewise-smooth solutions. This can be very important for projection measurements in CT. Even though the imaged object, e.g. the human body, may be modeled as piecewise-constant, its projections will, in general, not be piecewise-constant. This is easy to visualize and we show a simple example in Fig. 1. This figure shows a 2D slice and a 1D profile from the low-contrast 3D Shepp–Logan phantom alongside a typical cone-beam projection of it. Even though the phantom itself is strictly piecewise constant, this is not the case for its projection. It is well documented that a regularizer of the form (3) does not achieve optimal performance on images of this type [6,26].Following the above discussion, we suggest a regularization function that includes the ℓ1-norm of both the gradient and the Hessian of the image, leading to a functional of the form:(4)E(u)=∫Ω(u−vlogu)+λ1∫Ω|∇u|+λ2∫Ω|∇2u|or in the discrete image domain:(5)E(u)=∑i,j(u(i,j)−v(i,j)logu(i,j))+λ1∑i,j|∇u(i,j)|+λ2∑i,j|∇2u(i,j)|The norms of the gradient and the Hessian in the discrete image domain are defined as follows:(6)|∇u(i,j)|=(Dxu(i,j)2+Dyu(i,j)2)1/2(7)|∇2u(i,j)|=(Dxxu(i,j)2+2Dxyu(i,j)2+Dyyu(i,j)2)1/2where Dx, Dy, Dxx, Dxy, and Dyyare the first and second-order difference operators defined as follows:(8)Dxu(i,j)=u(i,j+1)−u(i,j)Dyu(i,j)=u(i+1,j)−u(i,j)Dxxu(i,j)=u(i,j+1)−2u(i,j)+u(i,j−1)Dyyu(i,j)=u(i+1,j)−2u(i,j)+u(i−1,j)Dxyu(i,j)=u(i+1,j+1)−u(i+1,j)−u(i,j+1)+u(i,j)Here, we have provided the definitions for the interior pixels. For the boundary pixels we assume periodic boundary condition as in [32,46]. In the rest of the paper, we only work in the discrete domain but to simplify the expressions we drop the pixel indices and only show them when necessary.To derive a minimization algorithm for the functional E(u), we follow the split Bregman iterative framework [16] which is a very efficient algorithm for ℓ1-regularized problems. Split Bregman method can be considered as a member of larger families of algorithms such as the alternating direction method of multipliers [3] or proximal methods [8]. In the split Bregman method, first the unconstrained optimization problem is converted into a constrained problem by introducing new variables. For E(u) in (5), we write the corresponding constrained problem by introducing three new variables:(9)minimize∑(f−vlogf)+λ1∑|g|+λ2∑|h|subjecttof=u,g=∇u,h=∇2uThis constrained problem can now be solved through the following Bregman iteration:(10)Initialize:u0=v,f0=v,g0=∇v,h0=∇2v,b10=0,b20=0,b30=0while∥uk−uk−1∥2>ϵ[uk+1,fk+1,gk+1,hk+1]=argminu,f,g,h∑(f−vlogf)+λ1∑|g|+λ2∑|h|+μ12∑(f−u−b1k)2+μ22∑(g−∇u−b2k)2+μ32∑(h−∇2u−b3k)2b1k+1=b1k+uk+1−fk+1b2k+1=b2k+∇uk+1−gk+1b3k+1=b3k+∇2uk+1−hk+1endwhere μiare the algorithm parameters and biare auxiliary variables. In the Bregman iterative approach, the updates of the auxiliary variables bireplace the updates of the subgradients of the objective function. We should also note if u is an m-by-n image, i.e.,u∈Rm×n, thenf,b1∈Rm×n,∇u,g,b2∈(Rm×n)2, and∇2u,h,b3∈(Rm×n)4. In the rest of the paper, we denote the components of g and h asg=[gx,gy]andh=[hxx,hxy,hyx,hyy]and the same for∇u,∇2u, b2, and b3.Although it may seem that the above modification has made the problem harder, the efficiency of the split Bregman scheme lies in the fact that the minimization problem can now be split into smaller problems that can be solved much more easily. Therefore, the large minimization problem in the above algorithm is solved by iteratively minimizing with respect to each of the four variables, resulting in the following algorithm:(11)Initialize:u0=v,f0=v,g0=∇v,h0=∇2v,b10=0,b20=0,b30=0while∥uk−uk−1∥2>ϵFori=1:Nfk+1=argminf∑(f−vlogf)+μ12∑(f−uk−b1k)2uk+1=argminuμ12∑(fk+1−u−b1k)2+μ22∑(gk−∇u−b2k)2+μ32∑(hk−∇2u−b3k)2gk+1=argmingλ1∑|g|+μ22∑(g−∇uk+1−b2k)2hk+1=argminhλ2∑|h|+μ32∑(h−∇2uk+1−b3k)2endb1k+1=b1k+uk+1−fk+1b2k+1=b2k+∇uk+1−gk+1b3k+1=b3k+∇2uk+1−hk+1endTo avoid complicating the notation, we have not introduced additional indices for the variable updates in the For loop. In fact, for many problems only one iteration of this loop is sufficient for fast convergence of the overall algorithm [16]. The efficiency of the split Bregman scheme entirely depends on how fast the sub-problems can be solved. In the following, we will show that for our problem, the four sub-problems can be solved very efficiently.Minimization with respect to f:Returning to the notation with pixel indices, this sub-problem is:(12)fk+1=argminf∑i,j(f(i,j)−v(i,j)logf(i,j))+μ12∑i,j(f(i,j)−uk(i,j)−b1k(i,j))2This expression can be written as a sum of scalar minimization problems in terms of individual pixel values. Since the function to be minimized is convex in terms off(i,j), the solution can be found by setting the derivative to zero. Using basic calculus and the knowledge thatf≥0we can show that the following formula gives the exact solution to this problem:(13)fk+1(i,j)=B2+(B2)2+v(i,j)μ1whereB=uk(i,j)+b1k(i,j)−1μ1Minimization with respect to u: This subproblem can be solved through its optimality condition which can be written as [16,32]:(14)[μ1I−μ2(DxTDx+DyTDy)+μ3(DxxTDxx+2DxyTDxy+DyyTDyy)]uk+1=μ1(fk+1−b1k)−μ2(DxT(gxk−b2xk)+DyT(gyk−b2yk))+μ3(DxxT(hxxk−b3xxk)+2DxyT(hxyk−b3xyk)+DyyT(hyyk−b3yyk))where DxT, DyT, DxxT, DxyT, and DyyTare the backward-difference operators corresponding to the forward-difference operators defined in (8). For the pixels in the interior of the image, these operators are defined as:(15)DxTu(i,j)=u(i,j)−u(i,j−1)DyTu(i,j)=u(i,j)−u(i−1,j)DxxTu(i,j)=Dxxu(i,j)=u(i,j+1)−2u(i,j)+u(i,j−1)DyyTu(i,j)=Dyyu(i,j)=u(i+1,j)−2u(i,j)+u(i−1,j)DxyTu(i,j)=u(i,j)−u(i−1,j)−u(i,j−1)+u(i−1,j−1)Despite its long expression, (14) is a system of linear equations inuk+1. Due to the structure of the forward and backward difference matrices, the system matrix is diagonally dominant. When the size of the image is not small, an efficient algorithm for finding a very good approximate solution is the Gauss–Seidel method [17,32].Minimization with respect to g: This problem reads:(16)gk+1=argmingλ1∑i,j|g(i,j)|+μ22∑i,j(g(i,j)−∇uk+1(i,j)−b2k(i,j))2This problem is also equivalent to a set of scalar problems in terms of individual pixel values. Its solution is a simple extension of the soft thresholding operation [44,33]:(17)gxk+1(i,j)=max(|B(i,j)|−λ1μ2,0)Bx(i,j)|B(i,j)|gyk+1(i,j)=max(|B(i,j)|−λ1μ2,0)By(i,j)|B(i,j)|whereB(i,j)=[Bx(i,j),By(i,j)]=[b2xk(i,j)+Dxuk+1(i,j),b2yk(i,j)+Dyuk+1(i,j)]Minimization with respect to h: This problem is very similar to minimization with respect to g above. Its solution is similarly a generalization of the soft thresholding operation [33]:(18)hxxk+1(i,j)=max(|C(i,j)|−λ2μ3,0)Cxx(i,j)|C(i,j)|hxyk+1(i,j)=max(|C(i,j)|−λ2μ3,0)Cxy(i,j)|C(i,j)|hyxk+1(i,j)=max(|C(i,j)|−λ2μ3,0)Cyx(i,j)|C(i,j)|(19)hyyk+1(i,j)=max(|C(i,j)|−λ2μ3,0)Cyy(i,j)|C(i,j)|whereC(i,j)=[Cxx(i,j),Cxy(i,j),Cyx(i,j),Cyy(i,j)]=[b3xxk(i,j)+Dxxk+1u(i,j),b3xyk(i,j)+Dxyk+1u(i,j),b3yxk(i,j)+Dyxk+1u(i,j),b3yyk(i,j)+Dyyk+1u(i,j)]If we setλ2=0in Eq. (4) or (5), we get a simplified model with standard (i.e., first-order) TV regularization. This will greatly simplify the algorithm because the variables h and b3 will also be removed. We will refer to this simplified model as “TV sinogram denoising” and to the full model described above as “(TV + TV2) sinogram denoising” and will present the results for both models. This will allow us to see whether or not, and to what extent, the more complex model with two regularizers improves the results.

@&#CONCLUSIONS@&#
