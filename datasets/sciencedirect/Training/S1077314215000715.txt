@&#MAIN-TITLE@&#
Factorization of view-object manifolds for joint object recognition and pose estimation

@&#HIGHLIGHTS@&#
We address multi-view recognition problem by factorizing view-object manifold.We use a common manifold to represent view manifolds of different objects.We use the view manifold deformation for categorization.We extensively experiment to validate the robustness and strength of our approach.

@&#KEYPHRASES@&#
Homeomorphic manifold analysis,Object categorization,Object recognition,Instance recognition,Pose estimation,

@&#ABSTRACT@&#
Due to large variations in shape, appearance, and viewing conditions, object recognition is a key precursory challenge in the fields of object manipulation and robotic/AI visual reasoning in general. Recognizing object categories, particular instances of objects and viewpoints/poses of objects are three critical subproblems robots must solve in order to accurately grasp/manipulate objects and reason about their environments. Multi-view images of the same object lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g. visual/depth descriptor spaces). These object manifolds share the same topology despite being geometrically different. Each object manifold can be represented as a deformed version of a unified manifold. The object manifolds can thus be parameterized by its homeomorphic mapping/reconstruction from the unified manifold. In this work, we develop a novel framework to jointly solve the three challenging recognition sub-problems, by explicitly modeling the deformations of object manifolds and factorizing it in a view-invariant space for recognition. We perform extensive experiments on several challenging datasets and achieve state-of-the-art results.

@&#INTRODUCTION@&#
Visual object recognition is a challenging problem with many real-life applications. The difficulty of the problem is due to variations in shape and appearance among objects within the same category, as well as the varying viewing conditions, such as viewpoint, scale, and illumination. Under this perceptual problem of visual recognition lie three subproblems that are each quite challenging: category recognition, instance recognition, and pose estimation. Impressive work have been done in the last decade on developing computer vision systems for generic object recognition. Research has spanned a wide spectrum of recognition-related issues, however, the problem of multi-view recognition remains one of the most fundamental challenges to the progress of the computer vision.The problems of object classification from multi-view setting (multi-view recognition) and pose recovery are coined together, and directly impacted by the way shape is represented. Inspired by Marr’s 3D object-centric doctrine [2], traditional 3D pose estimation algorithms often solved the recognition, detection, and pose estimation problems simultaneously (e.g. [3–6]), through 3D object representations, or through invariants. However, such models were limited in their ability to capture large within-class variability, and were mainly focused on recognizing instances of objects. In the last two decades the field has shifted to study 2D representations based on local features and parts, which encode the geometry loosely (e.g. pictorial structure like methods [7,8]), or does not encode the geometry at all (e.g. bag of words methods [9,10]). Encoding the geometry and the constraints imposed by objects’ 3D structure are essential for pose estimation. Most research on generic object recognition bundle all viewpoints of a category into one representation; or learn view-specific classifiers from limited viewpoints, e.g. frontal cars, side-view cars, rear cars, etc. Recently, there has been an increasing interest in object categorization in the multi-view setting, as well as recovering object pose in 3D, e.g. [11–18]. However, the representations used in these approaches are mainly category-specific representations, which do not support scaling up to a large number of categories.The fundamental contribution of this paper is the way we address the problem. We look at the problem of multi-view recognition and pose estimation as a style and content separation problem, however, in an unconventional and unintuitive way. The intuitive way is to model the category as the content and the viewpoint as a style variability. Instead, we model the viewpoint as the content and the category as a style variability. This unintuitive way of looking at the problem is justified from the point of view of learning the visual manifold of the date. The manifold of different views of a given object is intrinsically low in dimensionality, with known topology. Moreover, we can show that view manifolds of all objects are deformed version of each other. In contrast, the manifold of all object categories is hard to model given all within-class variability of objects and the enormous number of categories. Therefore, we propose to model the category as a “style” variable over the view manifold of objects. We show that this leads to models that can untangle the appearance and shape manifold of objects, and lead to multi-view recognition.The formulation in this paper is based on the concept of Homeomorphic Manifold Analysis (HMA) [19]. Given a set of topologically equivalent manifolds, HMA models the variation in their geometries in the space of functions that maps between a topologically-equivalent common representation and each of them. HMA is based on decomposing the style parameters in the space of nonlinear functions that map between a unified embedded representation of the content manifold and style-dependent visual observations. In this paper, we adapt a similar approach to the problem of object recognition, where we model the viewpoint as a continuous content manifold and separate object style variables as view-invariant descriptors for recognition. This results in a generative model of object appearance as a function of multiple latent variables, one describing the viewpoint and lies on a low-dimensional manifold, and the other describing the category/instance and lies on a low-dimensional subspace. A fundamental different in our proposed framework is the way 3D shape is encoded. An object’s 3D shapes imposes deformation of its view manifold. Our framework, explicitly models the deformations of object manifolds and factorizes it in a view-invariant space for recognition. It should be notice that we ignore the problem of detection/localization in this paper, and only focus on the problem of recognition and pose estimation assuming that bounding boxes or masks of the objects are given.Pose recognition/estimation is fundamentally a six-degree-of-freedom (6DoF) problem [20], including 3DoF position [x, y, z] and 3DoF orientation [yaw, pitch, roll]. However, in practical computer vision and robotic applications, pose estimation typically means solving for the some or all of the orientation degrees of freedom, while solving for the 3DoF position is usually called localization. In this paper, we focused on the problem of estimating the 3DoF orientation of the object (or the 3DoF viewing orientation of the camera relatively), i.e. we assumed the camera looking at the object in a fixed distance. We firstly considered the case of 1DoF orientation, i.e. a camera looking at an object on a turntable setting, which results in a one-dimensional view manifold, and then generalized to 2DoF and 3DoF orientation. Generalization to recover the full 6DoF of a camera is not obvious. Recovering the full 6DoF camera pose is possible for a given object instance, which can be achieved by traditional model-based method. However, this is a quite challenging task for the case of generic object categories. There are various reasons why we only consider 3DoF viewing orientation and not full 6DoF. First, it quite hard to have training data that covers the space of poses in that case; all the state-of-the-art dataset are limited to only a few views, or at most, multiple views of an object on a turn-table with a couple of different heights. Second, practically, we do not see objects in all possible poses, in many applications the poses are quite limited to a viewing circle or sphere. Even humans will have problems recognizing objects in unfamiliar poses. Third, for most applications, it is not required to know the 6DoF pose, 1DoF pose is usually enough. Definitely for categorization 6DoF is not needed. In this paper we show that we can learn from a viewing circle and generalize very well to a large range of views around it.The rest of this paper is organized as follows. Section 2 discusses the related work, and Section 3 summarizes our factorized model and its application to joint object and pose recognition. Separately, Sections 4 and 5 describe how to learn the model and how to use this model to infer for category, instance and pose in detail. Section 6 evaluates the model and compares it to other state-of-the-art methods. Finally, Section 7 concludes the paper.

@&#CONCLUSIONS@&#
