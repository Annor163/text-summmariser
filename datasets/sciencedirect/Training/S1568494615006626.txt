@&#MAIN-TITLE@&#
A Bayesian analysis of spherical pattern based on finite Langevin mixture

@&#HIGHLIGHTS@&#
An algorithm for Bayesian learning of finite Langevin mixtures is proposed.The proposed approach is based on Markov Chain Monte Carlo (MCMC) techniques.An approach for model selection using Bayes factors is adopted.The model is applied to the challenging problems of visual scenes categorization and topic detection and tracking.

@&#KEYPHRASES@&#
Langevin mixture,Bayesian inference,MCMC,Spherical data,Topic detection and tracking,Image categorization,

@&#ABSTRACT@&#
Parameter estimation is a cornerstone of most fundamental problems of statistical research and practice. In particular, finite mixture models have long been heavily relied on deterministic approaches such as expectation maximization (EM). Despite their successful utilization in wide spectrum of areas, they have inclined to converge to local solutions. An alternative approach is the adoption of Bayesian inference that naturally addresses data uncertainty while ensuring good generalization. To this end, in this paper we propose a fully Bayesian approach for Langevin mixture model estimation and selection via MCMC algorithm based on Gibbs sampler, Metropolis–Hastings and Bayes factors. We demonstrate the effectiveness and the merits of the proposed learning framework through synthetic data and challenging applications involving topic detection and tracking and image categorization.

@&#INTRODUCTION@&#
As various disciplines have witnessed integration of digital technologies, high-dimensional sparse data is becoming more prevalent in every field of human endeavor. In the particular case of machine learning, such problems have been tackled using statistical learning, providing a rich and flexible techniques that can be applied to model data randomness and uncertainty. In this context, often one tries to understand this mass of data through analyzing informative patterns and describing the best possible model which succeeds in capturing the regularities in the data generating process. Nonetheless selecting an appropriate model that solves all aspects of application at hand is a major challenge as different approaches are needed, for distinct aspects, often depend on different representational choices. For instance, although modeling based on Gaussian mixtures [1–4] has provided good performance in some applications, recent works have shown that Gaussian model is sensitive to noise and irresistible to outliers when dealing with high-dimensional data. Indeed, among the challenges when using finite mixture modeling, there is the choice of appropriate parametric form of the probability density functions to represent the components.Compared to the Gaussian, Langevin distribution has been shown to be a good alternative [5–7]. Usually, it is adopted to model problems involving high-dimensional spherical (L2-normalized) vectors [5]. Indeed, it implicitly uses cosine similarity that is easy to interpret and simple to compute for sparse vectors, and has been widely used in text mining [8], spam filtering [6], gene expression analysis [9], and topic detection [10–12]. Works about directional data in general and spherical ones in particular have been developed thanks to the efforts of Watson, Stephens and others [13–23]. In this work, we shall consider finite Langevin mixtures. A key step in mixture-based modeling of data is parameter estimation. Many methods have been proposed in the literature in order to estimate mixture parameters, including frequentist (a.k.a. deterministic) and Bayesian approaches [24]. In this paper, we focus in developing parameter estimation and model selection from Bayesian perspective. We are mainly motivated by the fact that Bayesian learning has several desirable properties that make it widely used in several applications. For instance, it does not suffer over-fitting and prior knowledge is incorporated naturally in a principled way. In this paper we shall not motivate further Bayesian learning which has been widely discussed in the past (interested reader may refer to [25–27] for further details and interesting discussions).Rooted in the early work of  [28] Bayesian inference for the von Mises Fisher (vMF) distribution (3-dimensional case of the Langevin distribution) was proposed. This work was based on the development of a conjugate prior for the mean (Jeffreys prior was also developed for the polar coordinates) when concentration parameter is known. In the area of radio signals, authors in [29] applied Bayesian approach for finding the location of an emergency transmitter signal based on the von Mises (vM) distribution (2-dimensional case of the Langevin distribution) by developing conjugate priors using the canonical parameterization. A Gibbs sampler for vM distribution was introduced in [30] by developing conjugate priors for the polar coordinates. In  [31] authors provide a full Bayesian analysis of directional data using the vMF distribution, again using standard conjugate priors and obtaining samples from the posterior using a sampling-importance-resampling method. Compared to these methods, our work is not restricted to low dimensional data (i.e. von Mises (2D) or von Mises Fisher (3D)) which is a limited solution for many real-world problems. On contrary, we extend previous models to high dimensional data using Langevin mixture (for D>3) where both the concentration and mean parameters are unknown. In particular, we propose a Markov Chain Monte Carlo (MCMC) algorithm that relies on Gibbs sampler and Metropolis–Hastings (M-H) for the estimation of the parameters. To this end, we develop a conjugate prior for the Langevin distribution taking into account the fact that it belongs to the exponential family. As well as considering the estimation over model parameters, we also wish to consider the optimal number of components that best describe data at hand. One common approach is integrated likelihood [32] which we shall adopt for Langevin mixture in this paper. Note that, despite various efforts to use Bayesian inference to learn mixtures [33,34], to the best of our knowledge, none of the recent works has considered the case where the feature vectors to model are spherical so far.The rest of this paper is organized as follows. In Section 2 we briefly introduce Langevin finite mixture model. In Section 3 we present previous parameter estimation approaches and then we propose a pure Bayesian algorithm for the estimation and selection of Langevin mixture model. Experimental results in vital and challenging problems, namely topic detection and tracking and image categorization, are presented in Section 4. Finally, Section 5 concludes the paper.LetX→=(X1,…,XD)be a random unit vector inℝD.X→has D-variate Langevin distribution if its probability density function is given by [35]:(1)pD(X→|μ→,κ)=κD2−1(2π)D2ID2−1(κ)exp{κμ→trX→}on the (D−1)-dimensional unit sphereSD−1={X→|X→∈ℝD:∥X→∥=X→trX→=1}, with mean direction unit vectorμ→∈SD−1, whereμ→trdenotes the transpose ofμ→and non-negative real concentration parameter κ≥0. Furthermore, ID(κ) denotes the modified Bessel function of first kind [35]. Letp(X→i|Θ)be a mixture of M Langevin distributions (i.e. a linear weighted combination of M distributions). The probability density functionp(X→i|Θ)is then given by(2)p(X→i|Θ)=∑j=1MpD(Xi→|θj)pjwhereΘ={P→=(p1,…,pM),θ→=(θ1,…,θM)}denotes all the parameters of the mixture model such that θj=(μj, κj) andP→represents the vector of clusters probabilities (i.e. mixing weights) such that pj≥0 and∑j=1Mpj=1.An efficient way to estimate the parameters of underlying mixture model is to optimize the associated likelihood function, which plays a key role in many estimation approaches such as EM and Bayesian. Assuming that the unit vectors to cluster,X={X→1,…,X→N}, are independent and identically distributed, thus, the likelihood of Langevin mixture in Eq. (2) can be formulated as:(3)p(X|Θ)=∏i=1Np(X→i|Θ)One approach to estimate the Θ parameters of the mixture is to maximize the log likelihood(4)logp(X|Θ)=∑i=1Nlog∑j=1Mpp(Xi→|θj)pjMaximum likelihood estimation is generally implemented via the EM framework [36] which generates a sequence of models with non-decreasing log-likelihood on the data. Following EM, consider the complete data to be{X→i,Z→i}, whereZ→i=(Zi1,…,ZiM)denotes the missing vectors, such that∑j=1MZij=1with Zij=1 ifX→ibelongs to class j and 0, otherwise. The E-step in EM computes the posterior probabilities given by the following equation:(5)Zˆij=p(X→i|θj)pj∑j=1Mp(X→i|θj)pjwhereZˆij∈[0,1],∑j=1MZˆij=1and denotes the degree of membership ofX→iin the jth cluster. In the M-step, given the conditional expectation of complete log-likelihood, we update the parameters estimate by maximizing the complete data log likelihood from the E-step. A complete EM algorithm for Langevin mixture has been proposed in [5,6]. Unfortunately, maximum likelihood estimation does not provide tractable (closed form) solution for the parameters of Langevin mixture, especially that calculations include the ratio of Bessel function for the concentration parameter κ. Moreover, when the data at hand has high dimensionality and large number of components EM shows poor generalization and might lead to over- or under-fitting [33].As we previously discussed EM provides an elegant and simple way to estimate the parameters of a given model, yet, EM algorithm is sensitive to the initialization and generally converges to local solution in the best case. To avoid this problem, an alternative way is to use Bayesian estimation for Langevin mixture model.Bayesian estimation is based on finding the conditional distributionπ(Θ|X,Z)of parameters vector Θ which is brought by complete data(X,Z), whereZ={Z→1,…,Z→N}. We therefore select a prior distribution π(Θ) and then develop posterior distributionπ(Θ|X,Z)which is derived from the joint distributionp(Z,Θ,X)via Bayes formulaπ(Θ|X,Z)∝p(Z,Θ,X). The joint distribution of all variables can be written as:(6)π(Θ|X,Z)=p(θ→,P→|X,Z)∝p(P→)p(θ→)p(Z|P→)∏Zij=1p(X→i|θj)wherep(θ→)andp(P→)are the priors of θ andP→which we will describe in what follows.In order to derive our Bayesian algorithm we now turn to defining our priors over the parameters. Langevin distribution is a member of (curved)-exponential family of order D, whose shape is symmetric and unimodal. Thus, we can write it as the following [37]:(7)p(X→|θ)=H(X→)exp(G(θ)trT(X→)+Φ(θ))where G(θ)=(G1(θ), …, Gl(θ)),T(X→)=(T1(X→),…,Tl(X→))where l is the number of parameters of the distribution and tr denotes transpose. The conjugate prior11Ref. [38] contains an interesting discussion about the characteristics of conjugate priors and their induced posteriors in Bayesian inference for von Mises Fisher distributions, using either the canonical natural exponential family or the more commonly employed polar coordinate parameterizations.on θ, in this case, can be written as [27]:(8)π(θ)∝exp∑l=1SρlGl(θ)+λΦ(θ)whereρ=(ρ1,…,ρS)∈ℝSand λ>0 are referred as hyperparameters. To this end, Langevin distribution can be written as follows:(9)pD(X→|μ→,κ)=exp{κμ→trX→−aD(κ)}whereaD(κ)=−logκD2−1(2π)D2ID2−1(κ). Then, by letting Φθ=−aD(κ) andGθ=κμ→, the prior can be written as:π(θ)∝exp∑d=1Dκ0μ→0trμ→j−λap(κ0)The prior hyperparameters are:(κ0,μ→0,λ), whereμ→0∈SDis the mean of the observations, κ0 is the concentration parameter and λ is a non-negative integer. Having the prior at hand, the posterior is given as following:(10)π(θj|Z,X)∝π(θj)∏Zij=1p(X→i|θj)∝κ0D2−1(2π)D2ID2−1(κ0)λexp(κ0μ→0tμ→j)×κjD2−1(2π)D2ID2−1(κj)njexp(njκjμ→jt∏Zij=1X→i)∝(aD(κj))λ+njexp(Rjβjtrμ→j)where(11)Rj=∥κ0μ→0+njκj∏Zij=1X→i∥βj=κ0μ→0+njκj∏Zij=1X→iRjNext, we develop distributionπ(P→|Z)and according to Eq. (6), we have:(12)π(P→|Z)∝π(P→)π(Z|P→)We know that the vectorP→is defined on the simplex{(p1,…,pM):∑j=1M−1pj<1}, and hence a natural choice, as a prior, for this vector is a Dirichlet distribution with parameters η=(η1, …, ηM). Then,(13)π(P→|Z)=Γ(∑j=1Mηj)∏j=1MΓ(ηj)∏j=1Mpjηj−1∏j=1Mpjηj∝D(η1+n1,…,ηM+nM)whereDis a Dirichlet distribution with parameters (η1+n1, …, ηM+nM) and(14)π(Z|P→)=∏i=1Nπ(Z→i|P→)=∏i=1Np1Zi1⋯pMZiM=∏i=1N∏j=1MpjZij=∏j=1Mpjnjwherenj=∑i=1NIZij=j. One of the most common approaches to conduct Bayesian inference is Gibbs sampler [26], which we adopt in this paper. The standard Gibbs sampler for mixture models is based on the successive simulations ofθ→, Z, andP→and is given as follows [26]:Algorithm 11Initialization.Step t: For t=1, …•GenerateZ→i(t)∼M(1;Zˆi1(t−1),…,ZˆiM(t−1)).Computenj(t)=∑i=1NIZij(t)=j.GenerateP→(t)from Eq. (13).Generateθj(t), j=1, …, M, from Eq. (10) using M-H algorithm [39].The major problem in the M-H algorithm is the need to choose a proposal distribution. In order to tackle this problem, random walk Metropolis–Hastings algorithm is the most generic proposal where each unconstrained parameter is the mean of the proposal distribution for the new value. In our case, the distributions of parameters are considered as: Langevinμ˜j∼LM(μjt−1|μ˜j,κj)for the mean μ, where μjis a constant unit vector, κjis concentration parameter. As we know the concentration parameter is non-negative integer κ>0, we consider log-normalκ˜j∼LN(log(κjt−1),σ2)for the concentration parameter κ with meanlog(κjt−1)and variance σ2. With these proposals22It is worth mentioning that other proposals are possible. For instance, in [31] authors chose Gamma distribution as a proposal for concentration parameter κ. But our experiments have proven better results with log-normal distribution selected as proposal.at hand the random walk M-H algorithm is given by:M-H Algorithm:•Generateμ˜j∼LM(μjt−1|μ˜j,κj),κ˜j∼LN(log(κjt−1),σ2)andu∼U[0,1].Computer=π(θ˜j|Z,X)∏d=1DLM(μjt−1|μ˜j,κj)LN(κjt−1|log(κ˜j),σ2)π(θjt−1|Z,X)∏d=1DLM(μ˜j|μjt−1,κj)LN(κ˜j|log(κjt−1),σ2).if r<u thenθjt=θ˜jelseθjt=θjt−1.This algorithm requires some further enhancement. Indeed, it is crucial to find also the optimal number of components. In this paper we adopt integrated likelihood to determine the number of clusters M defined by [32]:(15)p(X|M)=∫π(Θ|X,M)dΘ=∫p(X|Θ,M)π(Θ|M)dΘwherep(X|Θ,M)is the likelihood function of finite mixture model taking into account the number of clusters, which is M in this case, Θ is the vector of parameters and π(Θ|M) is prior density. This integral is not analytically tractable and is generally computed via Laplace approximation, on the logarithm scale, as follows [32]:(16)logp(X|M)=logp(X|Θˆ,M)+logπ(Θˆ|M)+Np2log(2π)+12log|H(Θˆ)|where|H(Θˆ)|is the determinant of the Hessian matrix and Np=M(D+1)−1 is the number of parameters in the model. A simple and accurate solution to estimateΘˆis to choose Θ in the sample at whichp(X|Θˆ,M)achieves its maximum [40]. Moreover,H(Θˆ)is asymptotically equal to the posterior variance matrix, and hence, we could estimate it by the sample covariance matrix of the posterior simulation output [40]. Thus, the complete Bayesian algorithm of finite Langevin mixture is as follows:Algorithm 21Apply Algorithm 1.Select the optimal model M* such thatM*=argmaxMlogp(X|M).

@&#CONCLUSIONS@&#
