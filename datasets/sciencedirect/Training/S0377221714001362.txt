@&#MAIN-TITLE@&#
Factor screening for simulation with multiple responses: Sequential bifurcation

@&#HIGHLIGHTS@&#
Our method finds the inputs with important effects on one or more outputs.Our method is efficient because it is sequential.Our method determines the number of replicates through an improved statistical test.Our method includes a procedure to validate the assumptions of our method.We illustrate our method through a case study on a Chinese logistics network.

@&#KEYPHRASES@&#
Simulation,Design of experiments,Statistical analysis,

@&#ABSTRACT@&#
The goal of factor screening is to find the really important inputs (factors) among the many inputs that may be changed in a realistic simulation experiment. A specific method is sequential bifurcation (SB), which is a sequential method that changes groups of inputs simultaneously. SB is most efficient and effective if the following assumptions are satisfied: (i) second-order polynomials are adequate approximations of the input/output functions implied by the simulation model; (ii) the signs of all first-order effects are known; and (iii) if two inputs have no important first-order effects, then they have no important second-order effects either (heredity property). This paper examines SB for random simulation with multiple responses (outputs), called multi-response SB (MSB). This MSB selects groups of inputs such that—within a group—all inputs have the same sign for a specific type of output, so no cancellation of first-order effects occurs. To obtain enough replicates (replications) for correctly classifying a group effect or an individual effect as being important or unimportant, MSB applies Wald’s sequential probability ratio test (SPRT). The initial number of replicates in this SPRT is also selected efficiently by MSB. Moreover, MSB includes a procedure to validate the three assumptions of MSB. The paper evaluates the performance of MSB through extensive Monte Carlo experiments that satisfy all MSB assumptions, and through a case study representing a logistic system in China; the results are very promising.

@&#INTRODUCTION@&#
Whereas real-world experiments typically have only a small number of factors (less than ten?), simulation experiments may involve numerous factors or inputs; e.g., the ecological case study by Bettonvil and Kleijnen (1997) has 281 inputs. Screening is defined as searching for the really important inputs among the many inputs that may be changed in a simulation experiment; this is based on the so-called Pareto or sparsity-of-effects principle. We also refer to the general review of design and analysis of simulation experiments by Kleijnen, Sanchez, Lucas, and Cioppa (2005). This search should also cure the so-called “curse of dimensionality”, so that after the screening phase the analysts can apply classic metamodels such as low-order polynomial regression and Kriging models.There are several screening methods, including classic two-level designs, frequency domain experimentation, supersaturated designs, and aggregated designs, detailed by Kleijnen (2008). A recent publication on screening is Moon, Dean, and Santner (2012), assuming deterministic simulation (not random, discrete-event simulation), only two stages (not fully sequential), and a Kriging metamodel (not a polynomial metamodel). In this paper, however, we focus on the screening method called sequential bifurcation (SB), because it seems the most efficient and effective method—if its assumptions are satisfied. Several case studies illustrate that these assumptions may be realistic; see Kleijnen, Bettonvil, and Persson (2006); Wan, Ankenman, and Nelson (2010) and the case study in Section 4.SB is a sequential aggregated design. By definition, sequential designs select the input combinations (also called design or inputs points) as the experimental results become available; i.e., sequential methods learn from preceding experimental results. Aggregated designs imply that some individual inputs are treated as a whole; i.e., if the aggregate changes from one value to another value, then all its individual inputs do so. SB’s first step aggregates all inputs of the simulation model into a single factor, and tests whether this factor has a significant first-order effect. Depending on this test outcome, SB either discards the aggregated factor in case of a non-significant main effect or splits this factor into two subfactors; this splitting is called bifurcation. If there is a next step, then SB tests whether the two smaller subfactors have significant main effects; next, SB splits a significant subfactor into smaller subfactors. Finally, SB estimates the main effects of all significant individual inputs.The first paper on SB is Bettonvil and Kleijnen (1997); that paper assumes deterministic simulation. Next, SB is extended to random (stochastic) discrete-event simulation by Cheng (1997); Cheng’s method is modified by Yaesoubi, Roberts, and Klein (2010). SB for random simulation is also discussed by Kleijnen et al. (2006), illustrating SB through a supply-chain case study with 92 inputs. Wan, Ankenman, and Nelson (2006) combine Bettonvil and Kleijnen’s (1997) SB with a hypothesis-testing procedure, to control the type-I and type-II error probabilities; next, Wan et al. (2010) generalize their procedure to improve its efficiency and efficacy. Shen and Wan (2009) develop a controlled sequential factorial design (CSFD) that combines a traditional factorial design with sequential hypothesis testing; Shen, Wan, and Sanchez (2010) further improve the efficiency by combining CSFD and the design in Wan et al. (2010).Based on the preceding literature survey, we conclude that SB is a relatively new method that has attracted the attention of several researchers. However, none of these researchers examines SB—or other screening methods—for simulation with multiple responses (outputs, performance measures). In practice, a simulation model does provide multiple responses; e.g., a supply chain management case study with multiple simulation outputs is Shi, Liu, Shang, and Cui (2013).Naive SB for simulation with multiple outputs applies SB to each type of output successively (even if the simulation model has a single type of output, experimenting with this simulation model will generate several observations on this output for different input combinations; for brevity’s sake we write “output” instead of “type of output” if no confusion seems likely). We, however, propose multi-response SB (MSB). Our main conclusion will be that MSB may be more efficient (fewer simulated input combinations and replicates, so less computer time) and more effective (higher probability of finding important inputs). Note that efficiency is crucial for computationally expensive simulation.This paper is organized as follows. Section 2 extends SB to multiple outputs, using similar assumptions as SB does. Because an individual input may increase some output and decrease another output, MSB selects groups of inputs such that—within a group—all inputs have the same sign for a specific output. We detail MSB for only two output types. To determine the number of replicates, MSB applies a sequential probability ratio test (SPRT). This section also gives a procedure to validate the three assumptions of MSB. Section 3 compares the performance of MSB and SB through Monte Carlo experiments that satisfy all SB or MSB assumptions; this section also includes a more efficient rule for selecting the initial number of replicates per stage. Section 4 evaluates the robustness of MSB through a case study representing a logistic system in China; MSB turns out to require fewer simulation observations than naive SB. Section 5 presents the main conclusions. Appendixes are published as a single Online Supplement.The basic idea of our MSB is inherited from SB; i.e., MSB contains a sequence of steps in which the main effects of input groups are estimated and tested. Specifically, if a group is declared to be non-significant, then all inputs in the group are classified as unimportant and discarded in the next steps. However, if a group is declared significant, then this group is split into two subgroups for further evaluation. A basic MSB rule is to declare a group of inputs to be important if at least one of the (multiple) outputs shows significant changes. Moreover, the unique feature of MSB is its attempt to estimate the main effects of groups for all outputs, while minimizing the experimental effort compared with SB for a single output; details are given in the next subsections. We give the major MSB symbols and their definitions in Table 1, in which the superscript and subscript denote the output and input respectively (superscripts use parentheses, so superscripts are not interpreted as power coefficients); in SPRT, we follow Wan et al.’s (2010) symbols as closely as we can. This table will be explained in the next subsections.We use the following three basic assumptions for MSB, which Bettonvil and Kleijnen (1997) also use for SB.Assumption 1An adequate metamodel (of the input/output or I/O function implied by the underlying simulation model) for output l withl=1,…,nis a second-order polynomial.This metamodel is denoted by(1)y(l)=β0(l)+∑j=1Kβj(l)xj+∑j=1K∑j′=jKβj′;j(l)xj′xj+∊(l)(l=1,…,n).We distinguish the outputs of the simulation modelw(l)(see e.g. Eq. (3) below), the output of metamodely(l), and the metamodel’s predictoryˆ(l)which uses the estimated effectsβˆ(l)and deletes∊(l). Quantitative inputs are standardized (coded, normalized) such that-1⩽xj≤1and qualitative inputs have only two levels which are arbitrarily denoted as −1 or 1. For output l the intercept isβ0(l), the K main effects areβj(l), theK(K-1)/2two factor interactions areβj′;j(l)withj′<j, the K purely quadratic effects areβj′;j(l)withj′=j. Finally,∊(l)is the metamodel residual for output l with zero mean (because the metamodel is assumed to be “adequate”). Note that the standardization-1⩽xj⩽1makes the input effects scale-free so they are comparable when determining the important inputs. To estimate the main effect in (1), it is efficient to select only two values per input. In practice, the users of the underlying simulation model should provide these values and ensure that these values are realistic extreme values, given the goal of the simulation model.Assumption 2The signs of all first-order effects are known; i.e., it is known that eitherβj(l)⩾0orβj(l)≤0(j=1,…,K)for any given j and l.Assumption 2 is a basic assumption of all group-screening methods, because this assumption avoids cancellation of individual first-order effects within the group effect. In practice, Assumption 2 may indeed hold, as the case studies mentioned in Section 1 illustrate.Assumption 3If two inputs have no important first-order effects, then they have no important second-order effects either.Assumption 3 is the so-called “heredity property” discussed in Wu and Hamada (2009).Definition 1Letβj′-j(l)be the sum of the main effects of inputsj′through j for output l:(2)βj′-j(l)=∑i=j′jβi(l)By definition, changing the level of input i fromLi(l)toHi(l)increases output l, but this change may decreases another outputl′. So,Li(l)equals eitherLi(l′)orHi(l′)wherel≠l′; e.g.,Li(l)=Hi(l′)if input i has opposite effects on the outputs l andl′; see Table 2with its K simulation inputs andn=2simulation outputs l. Columns 4 and 5 of this table show that in example (a) the inputs 1 throughk1have the same signs, whereas inputsk1+1through K have opposite signs; i.e., changing fromLi(l)toHi(l)withl=1,2and1⩽i⩽k1increases both outputs, whereas changing fromLi(l)toHi(l)withl=1andk1+1⩽i⩽Kincreases output 1 but decreases output 2. In example (b) we wish to increase output 2 for all K inputs. Therefore, for1⩽i⩽k1we haveLi(1)=Li(2)andHi(1)=Hi(2), but fork1⩽i⩽Kwe haveLi(1)=Hi(2)andHi(1)=Li(2). This leads to the following definitions.Definition 2Changing the level of input i fromLi(l)toHi(l)makes output l increase.Letwj(l)denote output l when inputs 1 though j are at their high levelsH(l)and the remaining inputs (j+1through K) are at their low levelsL(l). Letw-j(l)denote output l when inputs 1 though j are atL(l)and the remaining inputs (j+1through K) are atH(l).Table 2 implies thatwK(1)denotes output 1 when all K inputs have the valuesH(1);w-K(1)is the output when all K inputs have the valuesL(1). Following Bettonvil and Kleijnen (1997), we callw-j(1)the mirror observation ofwj(1). The definition applies the so-called foldover principle, which was originally developed for real-life experiments—with a few factors—so that two-factor interactions do not bias the estimators of the main effect (Montgomery, 2007). In SB and MSB the foldover principle ensures that second-order effects (two-factor interactions and purely quadratic effects) do not bias the first-order estimators—which we prove as follows.The metamodel assumed in (1) implies thatwj(l)andw-j(l)enable the following estimator of the aggregated main effectβj′-j(l)(defined in (2)) that is not biased by second-order effects:(3)βˆj′-j(l)=wj(l)-w-j(l)-wj′-1(l)-w-(j′-1)(l)4.Consequently, an estimator of the individual main effectβj(l)that is not biased by second-order effects, is(4)βˆj(l)=wj(l)-w-j(l)-wj-1(l)-w-(j-1)(l)4.Note that the bias elimination in (3) and (4) requires mirror observations so it doubles the number of simulation observations compared with MSB and SB assuming a first-order polynomial metamodel.Let the symbol “l→l′” in a superscript mean that the outputl′is observed “for free” when we are interested in output l; i.e., running the simulation model to observe output l also gives an observation on the other outputl′. For example,wK(1→2)denotes the output 2 when all K inputs are atHj(1)(j=1,2,….,K);w-K(1→2)denotes the output 2 when all K inputs are atLj(1)(j=1,2,…,K). Therefore,wK(1→2)andwK(1)are observed for the same input combinationH1-K(1). This gives the following definition.Definition 4Letwj(l→l′)denote outputl′when inputs 1 through j are atH(l)and the remaining inputs are atL(l); likewise, the mirror outputw-j(l→l′)denotes outputl′when inputs 1 through j are atL(l)and the remaining inputs are atH(l).Now we define a “group” of inputs such that there is no cancellation of individual effects within the group. Table 2 gave an example of two groups with group 1 containing inputs 1 throughk1so both outputs increase and group 2 containing inputsk1+1through K so output 1 increases and output 2 decreases. This gives the following definition.Definition 5A group of inputs has either all n outputs increase or all n outputs decrease when changing all the individual inputs in this group from −1 to 1.The following two theorems and their corollaries represent the main contribution of this paper; their proofs are given in the online supplement, and examples are given in the next subsections.Theorem 1If inputsj′through j are in the same group and they have the same signs for outputs l andl′, then the unbiased group estimators of the group main effects for output l andl′are(5)βˆj′-j(l)=wj(l)-w-j(l)-wj′-1(l)-w-(j′-1)(l)4and(6)βˆj′-j(l′)=wj(l→l′)-w-j(l→l′)-wj′-1(l→l′)-w-(j′-1)(l→l′)4wherej′⩽jand corresponding terms in(5) and (6)are observed for the same input combination.Note that (5) is identical to (3); furthermore, (5) and (6) have corresponding terms; e.g.,wj(l)andwj(l→l′). The proof of this theorem is given in Appendix A.1.Corollary 1If outputs l throughl′either increase or decrease for the individual input j, then the unbiased main effect estimators are(7)βˆj(l)=wj(l)-w-j(l)-wj-1(l)-w-(j-1)(l)4and(8)βˆj(l′)=wj(l→l′)-w-j(l→l′)-wj-1(l→l′)-w-(j-1)(l→l′)4Eqs. (7) and (8) follow from (5) and (6) whenj′equals j.□Now we give a theorem in case the group of inputs has opposite signs for output l andl′, instead of the same signs as in the preceding theorem.Theorem 2If the inputj′through j are in the same group and they have opposite signs for outputs l and l’, then the unbiased group estimators of the group main effects for output l andl′are(9)βˆj′-j(l)=wj(l)-w-j(l)-wj′-1(l)-w-(j′-1)(l)4and(10)βˆj′-j(l′)=-wj(l→l′)-w-j(l→l′)-wj′-1(l→l′)-w-(j′-1)(l→l′)4wherej′⩽j.Note the minus sign in (10) immediately after the equality sign. The proof of this theorem is given in Appendix A.2.Corollary 2If an individual input makes one output increase and the other output decrease, then the unbiased main-effect estimator is(11)βˆj(l)=wj(l)-w-j(l)-wj-1(l)-w-(j-1)(l)4and(12)βj^(l′)=-wj(l→l′)-w-j(l→l′)-wj-1(l→l′)-w-(j-1)(l→l′)4.Eqs. (11) and (12) follow from (9) and (10) whenj′equals j.□Fig. 1gives some details for the case of two outputs and two groups. The case of two outputs with only one group is very straightforward, so we present its details in Appendix B. The case of more than two outputs is of less interest because most practical simulation models (e.g., our case study) give only two outputs; anyhow, details on the general case of n outputs are presented in Appendix C, and this case is also listed under future research.In this subsection, we concentrate on MSB in the simple and practical case of only two outputs (son=2). Table 2 has already illustrated a situation in which the individual inputs 1 throughk1in group 1 increase both outputs, whereas the inputsk1+1through K in group 2 increase output 1 and decrease output 2. Fig. 1 details the MSB procedure for this situation; we shall detail the SPRT (mentioned in this figure) in Section 2.4.In Appendix B we detail two special cases ofn=2outputs; namely, there is a single group of inputs sok1=Kbecause (i) each input makes both outputs increase, or (ii) each input makes one output increase and the other output decrease.To test the significance of the estimated main effects of groups of inputs or individual inputs, we follow Wan et al. (2010). Those authors give a testing procedure that is meant to control the type-I orαerror probability of the whole procedure and to control the type-II orβerror probability of each step. We claim that ideally their procedure should also control the type II error probability over the whole procedure with its sequence of steps; also see De and Baron (2012) for an interesting discussion of so-called familywise error probabilities, albeit in the context of clinical testing. We therefore consider Wan et al.’s (2010) SB and our MSB as heuristics that are better than apriori assuming that the majority of potentially important individual inputs are unimportant and experimenting with a small group of inputs that are subjectively assumed to be important.Like Wan et al. (2010), we assume that the simulation outputsw(l)(x)for input combinationxhave a Gaussian marginal distribution with heterogeneous variancesσ(l)2(x); moreover, the four input combinations in Theorems 1 and 2 may use common random numbers (CRN).Instead of applying the classic Student t statistic, Wan et al. derive a test based on Wald’s SPRT (the latter test is also discussed by Kleijnen (1987, pp. 54–55) in a simulation context, and recently by De & Baron (2012)). In general, SPRTs add one replicate at a time, and terminate as soon as a conclusion can be reached. Wan et al. (2010) apply their SPRT each time when they test a group effect (in the early stage) or an individual effect (in the final stage). Their SPRT adds one replicate at a time to the group being tested and may use CRN (also see our Theorems 1 and 2). Let r denote the current number of replicates when estimatingβj′-j, which denotes the sum of main effects of inputsj′through j, assuming a single output for the time being. The initial number of replicates or initial sample size isN0. Wan et al. select a value forN0that remains constant over all the stages of their SB; e.g.,N0=25in their Monte Carlo study and their semiconductor case study. We, however, conjecture thatN0may be smaller in the early stages because those stages estimate the sum of the positive main effects of bigger groups so the signal–noise ratio is larger (this estimate uses four observations, except for the very first stage—called stage 0—which uses only two different observations); we shall detail our rule for selecting the initial sample size, in Section 3.Our SPRT uses the estimated variance of the estimator ofβj′-j(l)based on the initial sample:Sj′-j(l)2=∑r=1N0;j′-jβˆj′-j;r(l)-βˆ‾j-j′(l)2N0;j′-j-1withβˆ‾j′-j(l)=∑r=1N0;j′-jβˆj′-j;r(l)N0;j′-j.Fig. 2gives an illustration for two outputs. In this figure, the two triangles—formed by the solid lines and the dotted lines—are the continuation regions for then=2outputs. The symbols “•” and “▴” represent the observed values of the test statistics as a function of the number of replicates. The SPRT checks whether the statistic∑r=1mj′-jβˆj′-j;r(l)-r0;j′-j(l)—with drift parameterr0;j′-j(l)defined below—crosses one of the termination boundaries (TB) such that TB1 denotes the TB of the region in which the effect is declared to be unimportant, and TB2 is defined analogously for important effects. Only if both outputs cross their TB1, the group is declared unimportant in MSB; obviously, MSB has a higher probability of declaring a group to be important that SB has. Furthermore, the maximum number of observations onβj′-j(l)is one more thanMj′-j(l); in general,Mj′-j(l)≠Mj′-j(l′)wherel≠l′. The final number of replicates when estimatingβj′-j(l)ismj′-j. The two triangular regions are defined by.•the two intercepts±aj′-j(l)=±a0;j′-j(l)Sj′-j(l)2, andthe two slopes±λ(l)=±Δ1(l)-Δ0(l)/4.The constanta0;j′-j(l)andr0;j′-j(l)are the solutions of rather complicated equations given by Wan et al.; see their Eqs. (5) and (6) and the Matlab code in Appendix C in their Online Supplement available at http://joc.pubs.informs.org/ecompanion.html. However, we correct an error in this code; i.e., we add their SumInt function, which is missing in their Online Supplement. Details of their SPRT algorithm are given in Wan et al.’s (2010)Fig. 3.Wan et al. state that the goal of their SB is to classify those inputs withβj⩽Δ0as “unimportant” and those inputs withβj⩾Δ1as “important”. Forβj⩽Δ0they want to control the type-I error probability such that it does not exceedα; forβj⩾Δ1they want the statistical power of the test to be not smaller thanγ. ForΔ0<βj<Δ1they want “reasonable” power. Notice that the slopes of the triangles increase asΔ1increases; i.e., fewer replicates are needed when estimating bigger effects. The number of observations for the four input combinations when estimatingβj′-jare equal if CRN are used, before beginning the test—as Wan et al. detail near the end of their Section 3.2.Because our MSB considersn⩾2output types, we use Bonferroni’s inequality replacing the type-I error probabilityαin this SPRT byα/nand the type-II error probability(1-γ)by(1-γ)/n. This change implies bigger triangles in Fig. 2, in which we continue sampling before accepting eitherH0stating that the group factor is unimportant for all output types orH1stating that the group-factor is important for one or more output types.Obviously, themj′-jreplicates enable the computation of the averageβˆ‾j′-j(l)ofβˆj′-j;r(l); the latter symbol denotes the estimated effect for output l of the group of inputsj′through j in replicater(r=1,…,mj′-j).By definition, “screening” means that the number of inputs K is too big to enable the estimation of all the individual effects of a second-order polynomial; this number of effects is denoted byq(K)soq(K)=1+K+K+K(K-1)/2. We shall see that the case study in Section 4 is a relatively small screening example; namely,K=26soq(26)=378. Unlike Wan et al. (2010, pp. 489–491), we do not use a central composite design (CCD) based on a resolution–V (R–V) design for all K inputs; our approach resembles the approach in Bettonvil and Kleijnen (1997, p. 187–189). The final result of MSB and SB are estimates of only (say)K1≪Kfirst-order effects of the inputs declared to be “important”. This result is based on the three major assumptions specified in Section 2.1. To validate these assumptions, we first estimate theq(K1)=(1+K1+K1+K1(K1-1)/2)individual effects of the second-degree polynomial withK1inputs; e.g., our case study hasq(5)=21≪q(26)=378. This estimation does not require screening, but “classic” design of experiments (DOE). This classic design uses a CCD including a R–V design (such a design, however, is not saturated at all; i.e., this design implies a number of input combinationsnCCDthat is much higher thanq(K1); various alternative designs are discussed in Kleijnen (2008, p. 51)). We shall use such a classic CCD (withnCCD=43≫q(5)=21) for the case study in Section 4.When running the simulation with theK1inputs declared important by MSB, we also need values for all the(K-K1)unimportant inputs. We decide to keep the unimportant inputs constant. The unimportant quantitative inputs we keep at their coded value 0; the unimportant qualitative inputs we keep rather arbitrarily at (say)+1. We also need to select the number of replicates for the CCD, denoted by (say)mCCD.Moreover, we should verify whether the(K-K1)inputs declared to be unimportant by the screening method are indeed unimportant. We decide to select (say)nvalcombinations of the K inputs (unimportant or important). Our selection ofnvaldepends on the computer time required per replicate and the computer budget. We select thesenvalcombinations such that we obtain a space-filling design for the quantitative inputs (important or unimportant). More specifically, we use a Latin hypercube sample (LHS); details on LHS are given in Kleijnen (2008, pp. 126–130). For a qualitative input j we sample without replacement its −1 and 1 values with equal probabilities of 0.5 (soPr(xj=-1)=Pr(xj=1)=0.5) such thatnval/2values are −1 (so the othernval/2values are 1). We randomly combine thenvalcombinations of the quantitative inputs with thenvalvalues of the qualitative inputs.We then simulate thesenvalinput combinations, usingmvalreplicates; to selectmval, we should examinemj(the SPRT’s final number of replicates needed to test the significance of individual inputs; see again Fig. 2 in Section 2.4 and also Fig. 3 in the next section). We might use CRN in thesenvalinput combinations.Next, we test the validity of the estimated second-degree polynomial with the parametersβˆ(l)for theK1important inputs, which implies that the remaining(K-K1)unimportant inputs have zero first-order and second-order effects. We therefore predict the output of type l for thenvalinput combinations, and compare these regression predictionsyˆ‾i(l)=∑r=1mCCDyˆi;r(l)/mCCD(i=1,…,nval)with the corresponding average simulated output valueswi‾(l)=∑r=1mvalwi;r(l)/mval. MSB declares an input j to be important ifβj(l)⩾Δ1(l); hence, asΔ1(l)decreases, the regression predictor uses more important inputs (and a bigger CCD) so its fit tends to increase. We therefore accept the regression predictor as valid ifw¯i(l)-yˆ‾i(l)⩽Δ1(l); this comparison is scale dependent. A Studentized statistic is scale-independent because it accounts for the noisevar^w¯i(l)=∑r=1mval(wi;r(l)-w¯i(l))2/[mval(mval-1)]andvar^yˆ‾i(l). Before presenting this statistic in (14), we need to discussvar^(yˆ‾i(l)).We have already definedyˆ‾i(l)=∑r=1mCCDyˆi;r(l)/mCCDwhere obviouslyyˆi;r(l)=xiβˆr(l)withxidenoting the vector of important inputs determined by the CCD andβˆr(l)is theq(K1)-dimensional vector with the elementsβˆj;r(l)which denote the estimated effect j for output l computed from replicate r (l=1,…,n,j=1,…,q(K1),r=1,…,mCCD). Hence(13)var^yˆ‾i(l)=∑r=1mCCDyˆi;r(l)-yˆ‾i(l)2(mCCD-1)mCCD.Note that we do not use the classic formulas which assume that the simulation outputs of type l have a constant varianceσ(l)2and that thenCCDcombinations do not apply CRN.To validate the regression metamodel, we use the following Studentized statistic with v degrees of freedom (a related statistic is proposed in (Kleijnen, 2008, p. 58)):(14)ti;v(l)=max(w¯i(l)-yˆ‾i(l)-Δ1(l),0)var^w¯i(l)+var^yˆ‾i(l)(i=1,…,nval).Because the two variablesw¯i(l)andyˆ‾i(l)have different variances, the correct degrees of freedom v of this statistic is not easy to determine: so-called Behrens–Fisher problem. We selectv=min(mCCD-1,mval-1). Because (14) impliesnvalstatistics for outputl(l=1,…,n), we use Bonferroni’s inequality; i.e., we replace the classicαvalue byα/(nval×n). We accept the metamodel if none of these statistics exceeds the critical value (say)tm-1(α/(nval×n))or—equivalently—ifmaxi;lti;v(l)⩽tv[α/(nval×n)]. Next, we may use thenvalobservationsw¯i(l)(i=1,…,nval)to re-estimate the regression parametersβ(l); we expect that the resulting new estimates do not deviate much from the old estimates.If this polynomial is accepted as valid, we test the remaining two assumptions; namely, known signs of all first-order effects in this polynomial, and heredity. If this validation procedure suggests that the MSB assumptions do not hold, then we need to look for a different screening method; see again Kleijnen (2008). Details follow in Section 4. Note that the second-order polynomial for the important inputs can be used after the screening phase, to optimize these inputs through response surface methodology (RSM).To evaluate the performance of our MSB, we first use a Monte Carlo laboratory (later, we use a case-study in Section 4). The reason is that MSB is based on specific assumptions defined in Section 2.1, so we should start our evaluation with situations that do satisfy these assumptions; our “laboratory” fully satisfies the assumptions. Note that realistic applications may be extremely expensive; i.e., a single simulation run may take hours or days; in the Monte Carlo lab a “simulation” run (an observation) takes only (micro) seconds, depending on the computer hardware and software.MSB is a “black-box” method; i.e., it selects a combination of the K inputsx=(x1,…,xK)′, and observes the resulting simulation outputw=(w(1),…,w(n))(next MSB uses all available I/O data to estimate the group effects, etc.). Our Monte Carlo lab, however, is a “white box”; i.e., we select specific values for the coefficients of the second-order polynomial in (1) and the variances of the replicates (which equal the variances of the residuals); moreover we make the replicates normally, independently, and identically distributed (NIID).Wan et al. also use a Monte Carlo lab to evaluate their SB, but they consider a single output type andK=10inputs (in Subsection 3.2 we shall consider two output types andK=100inputs). Because the selection of parameter values in a Monte Carlo experiment is rather unlimited, we follow Wan et al. as closely as we consider acceptable for our MSB. Since only one output is discussed in this subsection, we omit the superscript “(l)” for the time being.Like Wan et al. (2010, p. 488) we call inputs “unimportant” if they have main effects not exceedingΔ0=2and “important” if these effects are at leastΔ1=4; the residuals∊are normally distributed with mean zero and a standard deviation equal to1+|E(w)|. Note that these rather big standard deviations require many replicates (as we shall illustrate in Table 3and Fig. 3). Furthermore, like Wan et al. we select the type-I error probabilityα=0.05and the powerγ=0.90. In Monte Carlo experiments with additive noise∊, CRN would generate a linear correlation coefficient with the extreme value 1; we therefore do not use CRN in these experiments. Like Wan et al., we resample the values for the two-factor interactions, and we select the performance measure of the Monte Carlo experiment to be the estimated probability of declaring an individual input to be important, symbolized byPr^(DI); we also use 1000 macroreplicates.Wan et al. experiment withK=10inputs (which we think is not a typical value in screening). They consider three cases, but we do not examine the case with main effects exactly zero or the case with all main effects equal toΔ0=2. We do examine their case with five main effects betweenΔ0=2andΔ1=4and five main effects exceedingΔ1but not exceeding the value 6; their specific values are 2.00, 2.44, 2.88, 3.32, 3.76, 4.20, 4.64, 5.08, 5.52, and 6.00, which are also displayed in column 2 in Table 3. We illustrate Wan et al.’s SPRT for a Monte Carlo example with a single output (our SPRT for multiple outputs equals the SPRT for a single output with the type-I and type-II error probabilities changed using Bonferroni’s inequality; moreover, an input is important if it has a significant main effect for at least one output). Our Table 3 presents results for a fixed initial sample sizeN0=5orN0=25(N0=25is also selected by Wan et al.), and (see the last four columns)N0that is either 5 or 25 in the first stage and either 25% or 50% of the final number of replicates in the immediately preceding stage. The results show that the selection ofN0does not seriously affectPr^(DI). The last line of this table shows that a fixedN0(columns 3 and 4) requires more replicates than our variableN0; the number of replicates are added over all stages, like Wan et al. do.Fig. 3 details initial sample sizes that are 25% of the final number of replicates in the immediately preceding stage; i.e., these sizes are not fixed—except in the very first stage where allK=10inputs are either −1 or 1 andN0=5. We show results for macro-replicate 1. The initial number in the first stage isN0;1-10=5and this stage ends withm1-10=39. In the next stage, SB splits the total group of 10 factors into23=8and the remaining 2 factors (to increases the efficiency, SB and MSB select the number of inputs for the first new subgroup to be a power of two, while the remaining inputs form the second subgroup). The initial number of replicates is 25% of 39 so this number is 10 after rounding to the next integer. This stage ends with 214 replicates. Note that the final number of replicates tends to increase as the group size decreases (from stage to stage, the signal/noise ratio decreases). The conclusion of this subsection is that the new pilot-sample rule increases the efficiency of SB and MSB.We also study a screening problem withK=100inputs,n=2outputs, andq=2groups. We compare SB—applied per output—and MSB. We select the same thresholds as Wan et al. (2010) did; namelyΔ0(l)=2andΔ1(l)=4(l=1,2). As initial number of replicates in the first stage we selectN0;1-100=5, and as the initial number of replicates in the next stages we select 25% of the final number of replicates in the immediately preceding stage (Fig. 3 illustrated this selection rule forK=10). The prespecified type-I error rateαis 0.05 and the powerγis 0.9. Because there are two outputs, we use Bonferroni’s inequality and replaceαbyα/2=0.025and the type-II error rate1-γby(1-γ)/2=0.05. We also consider the following four problem characteristics; also see Table 4(the last two columns are discussed below):•Sparsity of effects; i.e., we select either 4 or 8 of the 100 main effects to be “important”. Rows 1 through 8 of Table 4 have 4 important effects; rows 9 through 16 have 8 important effects.Signal–noise ratio; the higher the noise is, the more replicates should be obtained. We set the standard deviation of∊(l)to either 5 or 10; see column 2.Variability of effects; i.e., we make either all important main effects to have the same valueβj(l)=5(see rows 11, 12, 15, 16) or we make them all different; namely, −5, −2, 2, 5 (soβj(l)=2,5) when there are four important inputs (see characteristic 1), andβj(l)=2,3,4,5when there are eight important inputs.Clustering of effects; the more clustered the individual important main effects are, the more efficient SB and MSB are expected to be. When there are four important inputs and they are clustered, then the important inputs are 1, 2, 99, and 100 (e.g., row 1), and the non-clustered inputs are 1, 10, 91, and 100 (e.g., row 2); when there are eight important inputs, then the clustered inputs are 1, 2, 3, 4, 97, 98, 99, 100, and the non-clustered inputs are 1, 10, 20, 30, 71, 81, 91, and 100.Because we experiment with two levels per characteristic, there are 16 combinations. These combinations are shown in the first three columns of Table 4. In all 16 combinations there areq=2groups: the±signs in this table mean that all important main effects are positive for output 1 and some important main effects are negative for output 2; e.g., in combination 1 the four important inputs 1, 2, 99, and 100 have positive main effects for output 1, but the inputs 99 and 100 have negative main effects for output 2.Like Wan et al. do in their Monte Carlo experiments, we use 1000 macroreplicates. We report the average number of replicates per stage, which quantifies the efficiency; see the last two columns of this table. Both MSB and SB require increasing number of replicates for higher number of important inputs, variability of effects, noise of simulation outputs, and lack of clustering of important inputs. For example, combination 14—withσ=10, eight important inputs with different values and even spread—requires the maximum number of replicates (namely, 1723 for MSB), whereas combination 3—withσ=5, only four important inputs and much clustering—requires the minimum number of replicates (namely, 119 for MSB). MSB requires only approximately half the number of replicates needed by SB; i.e., input combinations applied for one output in MSB are again used when screening for the other output. Therefore, we conclude that MSB requires fewer input combinations and number of replicates than SB does.Besides the efficiency, we also study the efficacy quantified throughPr^(DI). We presentPr^(DI)only for the combinations 2, 7, 9, and 14, because we obtain similar results for the remaining 12 combinations; see Fig. 4where the x-axis givesβj(l)and the y-axis givesPr^(DI)(e.g.,βj(l)=0, 2, and 5 in combination 2, andβj(l)=0, 2, 3, 4, and 5 in combination 9). We present results only for output 1 becauseβj(1)=βj(2). This figure shows thatβj(l)has an important positive effect onPr^(DI). In all combinations,Pr^(DI)=0whenβj(l)=0andPr^(DI)=1whenβj(l)=5;Pr^(DI)lies in the interval [0.025,0.95] whenΔ0(l)=2⩽βj(l)⩽Δ1(l)=4(e.g., whenΔ0(l)=2, then combination 9 gives estimated Type I error rates for MSB that are 0.04 – which is very close to the observed Type I error rate for SB – 0.03 and 0.02. In combination 14, MSB gives rates of 0.04 and 0.05, which are not significantly higher than the SB rates, which are 0.02). So, both MSB and SB give appropriate type-I error rates and power. However, MSB’sPr^(DI)exceeds SB’sPr^(DI)whenΔ0(l)⩽βj(l)⩽Δ1(l); e.g., in combination 9 (south-west corner of the figure), MSB’sPr^(DI)=0.7and SB’sPr^(DI)is only 0.38 and 0.43 whenβj(l)=3; also see combination 14 (south-eastern corner). Our explanation is that an input that is unimportant for one output has a chance to be important for the other output, so the probability of declaring this input to be important increases.In addition, we conduct another set of experiments in which the two outputs do not have effects with same magnitudes; i.e., the magnitudes for output 1 are doubled for output 2. It is realistic that the thresholds for the two outputs also differ; i.e.,Δ0(1)≠Δ0(2), andΔ1(1)≠Δ1(2). Therefore, we setΔ0(1)=2Δ0(2)andΔ1(1)=2Δ1(2); i.e.,Δ0(1)=4≠Δ0(2)=2,Δ1(1)=8≠Δ1(2)=4. The results for these experiments are presented in Appendix D; these results are very similar to the results for the former experiments. The conclusion of this section is that MSB is more efficient and effective than SB.In this section we discuss a case study concerning a Chinese third-party logistics (TPL) company that wants to improve its just-in-time (JIT) system for its customer, a car manufacturer. Appendix E shows a flow chart which depicts the flow of parts, truck scheduling, etc. Given China’s rapid economic growth, Feng, Zhang, Zhang, Fu, and Zhang (2010) estimate that the Chinese car market will grow 10–15% over the next decade. To satisfy this demand, the TPL customer expects to open another assembly plant. When this new plant becomes operational, the current TPL capacity will not meet the logistic needs. Management wants to maintain the current logistic performance, measured through the average cycle time (CT) of a part and the number of throughput (NT) per month (30days). A long CT conflicts with the JIT philosophy. NT is the sum of the shipments collected at the part suppliers and delivered to the assembly plants within a production cycle of 30days. Therefore, our goal is to identify important inputs, and to make the outputs CT and NT satisfy desired values.Our simulation model has 26 inputs that may affect CT or NT. These inputs together with their low and high values are detailed in the appendix. All inputs are quantitative, except for one input; namely, the queue discipline. The appendix also shows that only the inputs 1 through 5 have the same signs for both outputs. So we form two groups; namely, group 1 with inputs 1 though 5 and group 2 with inputs 6 through 26.For our SPRT we selectΔ0(CT)=2.5andΔ0(NT)=2000as the minimum acceptable CT and NT values. We selectΔ1(CT)=5andΔ1(NT)=3000as the performance improvement that we do not want to miss. Inspired by Fig. 3 for our Monte Carlo experiment, we select the initial number of replicates in the first stageN0;1-5andN0;6-26equal to 5, and the initial number of replicates in the next stages as 25% of the final number of replicates in the immediately preceding stage, but not smaller than 5. Because there are two outputs, we replace the type-I error rateα=0.05and type-II error rateβ=0.1byα/2=0.025andβ/2=0.05. Fig. 5shows MSB results per stage, where shaded blocks denote inputs declared to be important. Altogether, MSB requires 233 replicates (namely,m1-5+m6-26+…+m21) to identify the five important inputs labeled 4, 5, 14, 17, and 20; the inputs 4 and 5 are in group 1 (see the first left bifurcation) and the inputs 14, 17, and 20 are in group 2 (see the first right bifurcation).The appendix shows that SB requires 238 and 117 replicates for CT and NT respectively. So, altogether SB requires 355 replicates, whereas MSB requires only 233 replicates. SB and MSB identify the same inputs as being important; SB identifies the inputs 4, 5, 14, 17, and 20 for CT and input 17 for NT. Note that MSB and SB do not use the same input combinations in every stage.Finally, we validate the MSB assumptions. We use a CCD for the five inputs that MSB declares to be important (Wan et al. also use a CCD, but that CCD covers all 20 inputs in their case-study). This CCD enables the estimation of the 21 individual effects in the second-degree polynomial. We obtainmCCD=10replicates for each combination in the CCD, after considering the numbers in the last stages of Fig. 5. The unimportant quantitative inputs we fix at their coded value 0. The one unimportant qualitative input we set to +1, which denotes FIFO and is the current queueing rule of the TPL company. We use CRN.Analysis of variance (ANOVA) of the resulting I/O data shows that the second-order polynomials for CT and NT have highR2and adjustedR2values; namelyRCT2=0.9608,RCT2(adj)=0.9519,RNT2=0.9641, andRNT2(adj)=0.9588, whereas the polynomials without second-order effects have onlyRCT2=0.7022,RCT2(adj)=0.6683,RNT2=0.6988, andRNT2(adj)=0.6733. So, the second-order polynomials are much better, and we can indeed use them to predict the outputs; i.e., Assumption 1 holds for the important inputs; also see (1).Given that the two second-order polynomials for the five important inputs are valid, we now discuss their individual estimated coefficients which are given in the appendix. These coefficients show that the signs of the estimated main effects of the important inputs agree with the assumed signs; namely, inputs 4 and 5 have minus signs for both CT and NT, whereas inputs 14, 17, and 20 have opposite signs for these outputs. So we conclude that Assumption 2 (known signs of all main effects) holds for the important inputs. This assumption and Assumption 3 (heredity) are now further examined.To test that all first-order effects and second-order effects of all unimportant inputs are zero, we selectnval=10combinations. We use LHS with uniform sampling of values between −1 and 1 for all 25 quantitative inputs, and sampling the two values −1 and 1 for the qualitative input; this LHS is detailed in the appendix. We obtainmval=20replicates for each combination (we also obtain results for only 10 replicates per combination; see the appendix). The 10 LHS-combinations with their 20 replicates give the simulatedw¯and the predictedyˆ‾and their estimated variancesvar^(w¯)andvar^(yˆ‾); see Table 5. To test these prediction errors, we selectα=0.20; such a relatively high value is typical when applying Bonferroni’s inequality. This givest10-1(0.20/(10×2))=t9(0.01)=2.821with degree of freedomv=min(10-1,20-1)=9. This table shows thatmaxi;lti(l)=t1(CT)=2.28, so we accept the two metamodels. We conclude that all three MSB assumptions hold in this case study.

@&#CONCLUSIONS@&#
