@&#MAIN-TITLE@&#
Random walks in directed hypergraphs and application to semi-supervised image segmentation

@&#HIGHLIGHTS@&#
We develop the directed hypergraph theory and introduce it in computer vision.We develop the random walk concept in directed hypergraphs.We propose a directed hypergraph model for image data representation.We use random walks to perform image segmentation with directed hypergraphs.Results show an improvement compared to traditional undirected hypergraphs.

@&#KEYPHRASES@&#
Directed hypergraph,Random walks,Image segmentation,Semi-supervised learning,

@&#ABSTRACT@&#
In this paper, we introduce for the first time the notion of directed hypergraphs in image processing and particularly image segmentation. We give a formulation of a random walk in a directed hypergraph that serves as a basis to a semi-supervised image segmentation procedure that is configured as a machine learning problem, where a few sample pixels are used to estimate the labels of the unlabeled ones. A directed hypergraph model is proposed to represent the image content, and the directed random walk formulation allows to compute a transition matrix that can be exploited in a simple iterative semi-supervised segmentation process. Experiments over the Microsoft GrabCut dataset have achieved results that demonstrated the relevance of introducing directionality in hypergraphs for computer vision problems.

@&#INTRODUCTION@&#
Graph-based methods have played an important role in Computer Vision and Pattern Recognition (CVPR) due to their ability to represent relational patterns [1,2]. However, in many situations, a graph-based representation is incomplete, as only binary relations between nodes can be represented through graph edges. An extension is provided by hypergraphs [3,4], where each edge is a subset of the set of nodes. Hence higher-order relations between nodes can be directly modeled in a hypergraph, by the means of hyperedges. Since such a mode of representation is closer to the human visual grouping system, hypergraphs have been shown as more effective than graphs to solve many problems in applications of practical interest that includes VLSI design and partitioning [5], parallel scientific computing [6], database design [7] or categorical data clustering [8]. The introduction of hypergraphs in the image processing domain dates back to Bretto et al. [9] and has recently received an increasing attention [10–15]. In particular, hypergraph representations based on the INH (Image Neighborhood Hypergraph) model have achieved competitive results in unsupervised image segmentation [15–17] that outperform graph-based methods. Recently, Ding et al. [14,18] introduced new image hypergraph models and exploited them in an semi-supervised segmentation framework that relies on a transductive setting given by Zhou’s random walk formulation on a hypergraph [8] that generalizes the graph random walk formulation.All the above methods are based on undirected hypergraph models. While the notion of directionality (by the concept of a directed graph) has been developed for a large number of years in the graph theory, a generalization to the hypergraph case has been slow to appear and a standard mathematically formal definition is yet to be defined. First attempts for defining directed hypergraphs [19,20] considered a hyperarc (a generalization of a directed graph edge) as a single vertex connecting a set of vertices. In this paper we use a more complete generalization by defining a hyperarc as a connection between two sets of vertices [4]. In addition, the use of directed hypergraphs for practical applications remains marginal and only few papers report using directed hypergraphs for applications such as relational databases [20], natural language parsing [21], or for modeling high-level processes such as biochemical [22], wireless [23,24] or social networks [25]. The main goal of this paper is to introduce the notion of directed hypergraphs in the computer vision domain, and judge its relevance especially compared to simple undirected hypergraphs. For this purpose, we followed the idea of directed graphs that were recently introduced in image segmentation [26–28]. Few previous works that involve some image analysis report using directed hypergraphs, but only to model high-level relationships between visual elements, such as video events [29] or pre-segmented cells motion [30]. None of these representations are directly linked to the image analysis and processing methods or make use of the low-level image data. To the best of our knowledge, and unlike undirected hypergraphs, no image processing technique that involve directed hypergraphs and no directed hypergraph-based representation of image data has been investigated. This paper has the objective to investigate these two aspects and show that directed hypergraphs can be useful at image data representation.Image segmentation is a low-level image analysis process that aims at partitioning an image into a number of disjoint regions, such that the visual features are coherent among the pixels of a single region. Although humans can easily extract meaningful segments from an image, this task remains difficult at computer level, where unsupervised segmentation algorithms are still unable to produce satisfactory results. In fact, fully automated segmentation is known to be an ill-posed problem due to the absence of a clear definition of a semantically meaningful segmentation, and the difficulty to judge its objective quality. Prior information about the image to segment should then be provided to make the segmentation problem well-posed. Such information can be supplied by the user through a set of representative pixels that labels the existing regions in the image. This issue has been addressed as interactive segmentation, and has been successfully used for foreground extraction in intelligent scissors [31], GrabCut [32], or interactive graph cuts [27].Recent directions in this field rely on graph-based machine learning [33–36]. The main idea is to model the image data by the means of an affinity graph where each edge encodes the similarity of two neighboring pixels in the image. The segmentation problem is then formulated as an energy function minimization, where the target function to estimate is smooth with respect to the underlying graph structure. The graph is represented by a symmetric affinity matrix where each entry models the penalty if the two corresponding pixels belong to separated regions in the segmentation. Segmentation becomes then a labeling problem than can be solved by graph-based learning methods. Such methods include harmonic energy minimization [37], graph min-cuts [27,38], random walks [33], transduction by Laplacian regularization [35], geodesics [39], or watersheds [36].Traditional variational methods for interactive (or semi-supervised) segmentation such as active contours [40] approach the segmentation problem by minimizing energy functions that favor alignment of the object boundaries with regions of high intensity gradient. Later, Vasilevskiy et al. [26] noted that the direction of the gradient contains valuable information that can improve the segmentation. This concept has been translated to discrete optimization by using graphs with directed edges (by opposition to the above methods that relies on undirected symmetric graphs), resulting in asymmetric affinity matrices. Directed graphs for semi-supervised segmentation have been used in min-cut [27] and random walk [28] algorithms. In practice, asymmetric penalties can help at segmenting thin elongated structures and can further improve the segmentation results in regions of low contrast.As noted above, and following the idea in [27,28], the objective of this paper is to investigate the reliability of adding directional information in hypergraph-based models and to judge the relevance of using directed hypergraphs as a means for image data representation. Consequently, we introduce the notion of directed hypergraphs in computer vision problems by proposing a directed version of the INH (Image Neighborhood Hypergraph) model. It is important to note that this article is, to the best of our knowledge, the first attempt to use directed hypergraphs in image analysis as a low-level image data representation tool. A first application of this DINH (Directed Image Neighborhood Hypergraph) representation is developed in terms of semi-supervised image segmentation, where the labels of all pixels are predicted according to the labels of pre-defined seed pixels. The problem is expressed in terms of Markov random walks, and a solution is found by estimating the probability that a random walk, starting at an unlabeled pixel, will first hit a given seed pixel. The random walk should be designed with respect to the underlying directed hypergraph structure, and by analogy with the undirected case, we give a formulation of the transition matrix of a random walk in a directed hypergraph. A theoretical solution to the hypergraph-learning process is given by considering the random walk as an absorbing Markov chain[41]. The transition matrix also served as a basis to design a simple iterative algorithm for semi-supervised segmentation based on label propagation, which solution converges to the theoretical one. Experiments over the GrabCut [32] dataset achieved interesting results, particularly compared to the same algorithm with transition matrices obtained from graphs and undirected hypergraphs. These results essentially demonstrate that the introduction of directed hypergraphs is relevant in the computer vision domain, and that they provide a richer model of representation than undirected hypergraphs and simple graphs.The remainder of this paper is organized as follows: we first present some preliminary definitions for undirected and directed hypergraphs (Section 2). We then give our formulation of a random walk in a directed hypergraph as well as its transition matrix (Section 3), and describe our proposed directed hypergraph model to represent the content of an image (Section 4). We then present the theoretical solution to the learning process on directed hypergraphs in terms of random walks, and the iterative label propagation procedure used to approach this solution (Section 5). Finally, some experimental results are given and discussed (Section 6).The general undirected hypergraph theory [3] is well-known in the field of mathematics, and we refer the reader to [3,4] for more details and definitions. We will here focus on the definition of directed hypergraphs.A directed hypergraph (dirhypergraph) is an ordered pair:H→=(V;E→={ei→:i∈I}),where V is a finite set of vertices andE→is a set of hyperarcs with index setI={1,2,…,M},M=|E→|. Each hyperarcei→is writtenei→=ei+→=ei+,i;ei-→=i,ei-.The setei+is the set of vertices ofei+→and the setei-is the set of vertices ofei-→. The vertices ofei→are denoted byei=ei+∪ei-andE={ei:i∈I}. The hypergraphH=(V;E)is the underlying hypergraph of the dirhypergraphH→=(V;E→). The elementei+→is called the tail of the hyperarcei→, whereasei-→is its head. A limb is either a head or a tail. We do not allowei+=∅orei-=∅andei+∩ei-≠∅for alli∈I. The index i stored in each limb of a hyperarc allows to distinguish from two hyperarcs that have the same set of vertices either in its head or in its tail.A dirhypergraphH→=(V;E→)can be represented by two incidence matrices, the positive (or outer) incidence matrixH+and the negative (or inner) incidence matrixH-, representing respectively the tails and the heads of the hyperarcs. Entries ofH+are given by(H+)ij=h+(vi,ej→)=1ifvi∈ej+and 0 otherwise. Entries ofH-are given by(H-)ij=h-(vi,ej→)=1ifvi∈ej-and 0 otherwise. We will consider that any hyperarc e can be weighted by a positive functionw(e→)(called the weight of hyperarce→) representing the importance of the hyperarc in the dirhypergraph structure, and we will note W the diagonal matrix containing the hyperarc weights. The positive (or outer) degree of a vertexvi∈Vis given byd+(vi)=∑ej→∈E→w(ej→)h+(vi,ej→). The negative (or inner) degree ofviis defined asd-(vi)=∑ej→∈E→w(ej→)h-(vi,ej→). LetDv+andDv-be the diagonal matrices containing respectively the positive and negative degrees of the vertices. We define the positive and negative degrees of a hyperarcej→byδ+(ej→)=|ej+|andδ-(ej→)=|ej-|. LetDe+andDe-be the diagonal matrices containing the positive and negative degrees of the hyperarcs. The transpose of a matrix (or vector) A will be notedAT.A directed path or hyperpath from x to y inH→=(V;E→)is a sequencePx,y=(x=v1,e1,v2,e2,v3, …,vt,et,vt+1=y)such thatx=v1∈e1+,y=vt+1∈et-andvi∈ei-1-∩ei+fori⩾2.A random walk [33,42,43] is a particular case of a Markov random chain, a random process that consists on visiting a certain number of locations (or states) by taking random steps. Consider a starting location u. Then the next location visited by the random walk is taken randomly (following a given probability law) among all the neighbors of u. A random walk of length t is then a sequence oft+1locations{v0,v1,…,vt}withv0=uthe starting location andvt=vthe ending location. The random walks are useful in a machine learning manner, where the different locations are data points to classify using a few sample seed points. The learning process can be interpreted by the following assumption: given a random walk starting at an unlabeled location, what is the probability that it first reaches each of the seed points? The final label of the starting location is then taken from the ending seed point of the random walk with the highest probability. In a random walk each step is taken independently from the previous steps, and consequently its behavior is completely determined by a transition probability matrix P where entryPijis the probability for a random walk at locationvito “jump” to the locationvj. The “t-step” probabilities (representing the probabilities that a random walk starting at locationviwill reach the locationvjafter exactly t steps) are simply given by the tth power of the transition matrixPt.When the random walk is defined on a graph [33,43] the transition matrix is computed with respect to the underlying graph structure. In practice the random walk matrix is given byP=D-1A, where D denotes the diagonal matrix containing the vertex degrees and A is the graph adjacency matrix. It has been shown in many occasions that random walks on graphs have a strong connection with usual electric networks problems [33,42,44]. In this paper we are interested in designing such a learning process on a directed hypergraph. The next sections will give a formulation of the transition matrix associated to a natural random walk in such a directed hypergraph.Following the concepts introduced by Zhou [8], we can associate a natural random walk to an undirected hypergraph with the following transition rule: given the current positionu∈V, we first choose a hyperedgee∈Eover all the hyperedges incident to u with a probability proportional tow(e). We then choose a vertexv∈V,v≠uuniformly at random. With the definitions given by Zhou in [8], the probabilityp(u,v)associated to that transition rule is(1)p(u,v)=∑e∈Ew(e)h(u,e)d(u)h(v,e)δ(e).Obviously, a nonzero transition probability between u and v exists only if the two vertices are linked by at least one hyperedge (otherwiseh(u,e)orh(v,e)equals 0). The normalization by the degrees of u and e is motivated by, respectively, that a vertex with a large degree has a smaller chance to choose a distinct hyperedge for the transition, and that the transition between u and an incident vertex v is chosen uniformly at random. The transition matrix P of the random walk is then defined byP=Dv-1HWDe-1HT.At that time, very little attention has been dedicated to the study of random walks on hypergraphs, and especially on directed hypergraphs. A first tentative to define a random walk on a directed hypergraph was reported in [45], but this definition comes from the interpretation of a directed hypergraph as a bipartite graph, as we propose a direct formulation. More importantly, only the undirected case is further considered in [45] and consequently the application of a random walk on a directed hypergraph is not investigated. In [46], the authors propose the extension of a specific random walk named loop-erased random walk, but only consider directed hypergraphs where the tail is reduced as a single vertex. Consequently, we propose here a formal definition of a random walk on a directed hypergraph, as well as its associated transition matrix.By analogy with the random walks defined in [8] for undirected hypergraphs, we associate a random walk to a dirhypergraphH→=(V;E→)that has the following natural transition rule. Given the current positionu∈V, we choose a hyperarce→∈E→such thatu∈e+with a probability proportional tow(e→). If we consider that a transition can only be made from a tail to a head of a hyperarc (following the definition a directed hyperpath given above), then we choose a vertexv∈e-uniformly at random. The probabilityp(u,v)associated to that transition rule can be formulated as(2)p(u,v)=∑e→∈E→w(e→)h+(u,e→)d+(u)h-(v,e→)δ-(e→).With this formulation, a nonzero transition probability between u and v exists only if∃e→such thatu∈e+andv∈e-, which follows the intuitive idea that a transition can only be done in the direction given by a hyperarc. The probability is normalized by the outer degree of u to represent the probability of choosing a distinct hyperarc between those which contain u in their tails. The inner degree of the hyperarce→is also introduced to take into account that a vertex is chosen uniformly at random in the head ofe→. In matrix notation, the transition matrix P of the directed random walk is defined by(3)P=Dv+-1H+WDe--1H-T.Proposition 1The matrix P given by Eq.(3)is stochastic, i.e.∀i,j,pij⩾0and∀i,∑j,pij=1, and is then suitable for the definition of a random walk.It is easy to verify that : for allu,v∈V,p(u,v)⩾0. Now:∑v∈Vp(u,v)=∑v∈V∑e→∈E→w(e→)h+(u,e→)d+(u)h-(v,e→)δ-(e→)=∑e→∈E→w(e→)h+(u,e→)d+(u)∑v∈Vh-(v,e→)δ-(e→)=∑e→∈E→w(e→)h+(u,e→)d+(u)∑v∈Vh-(v,e→)|e-|=∑e→∈E→w(e→)h+(u,e→)d+(u)=∑e→∈E→w(e→)h+(u,e→)∑e→∈E→w(e→)h+(u,e→)=1.□The INH (Image Neighborhood Hypergraph) model has been found very effective at representing the image content [11,15]. In this section we present an improvement of the INH that takes into account directed relationships between pixels.LetI:V⊆Z2⟶F⊆Znbe an image. Elements of V are the collection of the image pixels, and elements of F are the visual features associated to each pixel. A distance d on V defines a grid (a connected, regular graph, without both loop and multi-edge, associated with a regular latticeLofRn). In this contribution, we will be concerned only with 8-connected grids defined by the distanced(v,v′)=max{|x-x′|,|y-y′|}, where(x,y)and(x′,y′)denote respectively the spatial coordinates of v andv′on the grid. Thus, we define theβ-neighborhood of a pixelv∈Vby:(4)Γβ(v)={v′∈V|d(v,v′)⩽β}.Letd′be a distance measure on F, we have a neighborhood relation on an image defined for each pixel v by:(5)Γλ,β(v)={v′∈Γβ(v)|d′(F(v),F(v′))⩽λ}.Hereβandλare real values, called respectively spatial threshold and feature threshold. Thanks to this neighborhood relation, to each image we can associate a hypergraph called Image Neighborhood Hypergraph (INH) [47]:(6)HΓλ,β=(V;({v}∪Γλ,β(v))v∈V).Each pixel v in the image generates a distinct hyperedgee(v), and v is called the center ofe(v). In this work, the feature distance between two pixels will be computed asd′(F(v),F(v′))=|I(v)-I(v′)|, whereI(v)denotes the gray level of pixel v. The choice of the thresholdsβandλwill be discussed in Section 6.In directed graphs (digraphs) for image representation, two neighboring pixels x and y in the image are linked by two distinct directed edges (arcs)(x,y)and(y,x). If the same weight is assigned to both arcs than the digraph reduces to an undirected graph. It was then suggested [27,28] to use two different weighting functionsw1andw2such that the penalty assigned to an arc(x,y)should be equal tow1((x,y))ifI(x)<I(y), and tow2((x,y))ifI(x)⩾I(y), leading to an asymmetric adjacency matrix. We will follow that concept by building two different sets of hyperarcs in our following Directed Image Neighborhood Hypergraph (DINH) model. For an image I with set of pixels V, each pixelvi∈Vwill generate two distinct hyperarcse1→ande2→:(7)e1→(vi)=e1+(vi);e1-(vi),e1+(vi)={vi}∪{v′∈Γλ,β(vi)|I(v′)<I(vi)},e1-(vi)={v′∈Γλ,β(vi)|I(v′)⩾I(vi)},(8)e2→(vi)=e2+(vi);e2-(vi),e2+(vi)={vi}∪{v′∈Γλ,β(vi)|I(v′)>I(vi)},e2-(vi)={v′∈Γλ,β(vi)|I(v′)⩽I(vi)}.Since the center pixelviis in each tail of both hyperarcs, it will exist a nonzero transition probability betweenviand every pixel in itsΓλ,βneighborhood. The presence of the two types of hyperarc is necessary to allow a transition from regions of low intensity to regions of high intensity (this transition is represented by the hyperarce1→), and also a transition from regions of high intensity to regions of low intensity (represented by the hyperarce2→). Finally, the Directed Image Neighborhood Hypergraph (DINH) associated to an image can be computed as:(9)H→(V)=V;E→=(e1→(v))v∈V∪(e2→(v))v∈V.With the definition of the hyperarcs of the DINH model given by Eqs. (7) and (8), assigning the same weights toe1→ande2→will lead to a symmetric transition matrix associated to the random walk formulation (2), equivalent to a transition matrix obtained from an INH model, up to a factor 2. Consequently, a hyperarce→(v)centered at a pixel v will be weighted by the following weighting function w:(10)w:E→⟶R+,e→(v)=e+(v);e-(v)↦exp(d′(v,v′)‾v′∈e-(v)).In other words, each hyperarc will be weighted by the exponential of the average value of the feature distance (defined in Section 4.1) between the center pixel v (in the tail) and every pixel in the head of the hyperarc. We should note that this definition favors transitions between pixels that are close according to the given feature distance. By definition, the tails ofe1ande2centered at a pixel v are different, and so are the weights associated to them. Consequently, the transition matrix of the directed random walk will be asymmetric.We consider the image segmentation problem in a transductive setting, in which a set of known labels of some pixels (called seeds) are used to predict the labels for all other pixels. More mathematically, let us reorganize the image pixels asV={v1,v2,…,vL,…,vN}, such thatVL={vi}i=1Lis the labeled pixels set andVU={vi}i=L+1Nis the unlabeled pixels set. NoteL={1,…,l}(withl⩽L) the set containing the labels, andy:V→Lthe function associating to each pixelvi∈Vits labely(vi)in the final segmentation. Our goal is to estimatey(vi)for each unlabeled pixelvi.In our framework, the pixels set is taken as the vertex set of the directed hypergraph defined by Eq. (9), and let P be the transition matrix of the random walk defined on this dirhypergraph by Eq. (3). Letf(k):V→[0,1]be the function associating to each pixelviits membershipf(k)(vi)to the label k, i.e. the probability for the pixelvito belong to label k in the final segmentation. Ifvj∈VLis a labeled pixel, we setf(k)(vj)=1if the (known) label ofvjis k, and 0 otherwise. We want to estimate the value off(k)for each unlabeled pixelvi, so its labely(vi)will be taken as the k which maximizesf(k)(vi).With our random walk interpretation of this learning process,f(k)(vi)will be calculated as the probability that a random walk, given by the transition matrix P and starting at locationvi, will first reach a labeled location belonging to the label k, before hitting a seed location with a different label. It is not difficult to see that simulating such a random walk and see what seeded point it reaches first may be impractical, because the number of unlabeled pixels is in practice considerably higher than the number of seed pixels. In addition, it is subject to random biases, since it is possible for a random walk to take a direction it has a small probability to take, and then be trapped in a sub-optimal solution. This section presents two ways of resolving this problem by estimating the probabilitiesf(k), one theoretical and one more practical, and the summary of the segmentation algorithm.The solution of the learning process in terms of random walk is to consider the latter as an absorbing Markov chain. More details about the absorbing Markov chains theory can be found in [41]. In an absorbing Markov chain (given by its transition matrixP′), some of the locations are absorbing states that can be considered as traps, where the random walk ends when it reaches one of those absorbing states. More formally,Pii′=1andPij′=0∀jwhenviis an absorbing state. Then if we pose all the seed labeled locations as absorbing states, and if we assume that every random walk starting at a given unlabeled location (which is called a transient state in terms of Markov random chains) can reach at least one absorbing state (not necessary after just one step), then the random walk is an absorbing Markov chain, with some interesting properties [42]. Thanks to the reorganization of the pixels set by grouping seed and unlabeled pixels together, we can split the transition matrix P of the Markov chain in the following canonical form:P=PLLPLUPULPUU,wherePULandPUUrepresent the submatrices of P restricted to the probabilities between respectively the unlabeled and seed locations, and between all the unlabeled locations.PLLandPLUare the submatrices corresponding to the transition probabilities between, respectively, all the seed locations, and the seed and unlabeled locations. Since the seed pixels are considered as absorbing states,PLLmust reduce to theL×Lidentity matrixILandPLUto0, a matrix with all entries equal to 0. It is not difficult to see that this is not the case with our formulation of the transition matrix P of a random walk in a directed hypergraph (see Eq. (3)). In fact, this formulation does not depend on the seeds, which will be placed after the construction of P. It is not a problem, because the submatricesPLLandPLUare not useful for the learning process (since we do not have to start a random walk from already labeled locations) and do not intervene in the following. Therefore,PLLandPLUcan easily be fixed to, respectively,ILand0after placing the seeds.The matrixN=(IU-PUU)-1is called the fundamental matrix for the absorbing chain given by P (in this caseIUdenotes the identity matrix of sizeN-L×N-L). It can be found in [41] that the matrix(IU-PUU)is invertible and thus the matrix N exists. The entryNijcan be interpreted as the expected number of times that a random walk starting atviwill pass throughvjbefore being absorbed by a seed location. Then the vectort=N1(where1is a column vector of all 1’s) gives the expected number of steps before absorption for each starting state. Consequently, let B be theN×Lmatrix which entryBijgives the probability that a random walk starting atviwill be absorbed by the absorbing statevj. Then B is calculated by:(11)B=NPUL=(IU-PUU)-1PUL.This can be interpreted as follows: to get the probability of starting atviand ending up a given absorbing statevj, we add up the probabilities of going tovjfrom all the transient states, weighted by the number of times we expect to be in those transient states starting fromvi.Now that we have the absorption probabilities from each starting state, we have to estimate the label probabilitiesf(k). Letf(k)=(f(k)(v1),…,f(k)(vL),…,f(k)(vN))Tbe the vector containing the label memberships of all pixels for the label k. As well as the matrix P, we can splitf(k)as:f(k)=(fL(k))T,(fU(k))TT.In full generality, the probabilityUimthat a random walk P starting atviwill reach the absorbing statevmis given by the solution to the system of equations [41](12)Uim=∑vj∈VPijUjm.Since the valuef(k)(vi)is interpreted as the probability that a random walk starting atviwill first reach the location belonging to the label k, it can be calculated as the sum of the probabilities of reaching each absorbing state labeled with k. If we noteVkthe set of all labeled pixels of label k, thus we have:(13)f(k)(vi)=∑vm∈VkUim,=∑vm∈Vk∑vj∈VPijUjm,=∑vj∈VPij∑vm∈VkUjm,=∑vj∈VPijf(k)(vj).We can clearly see thatf(k)(vi)is the harmonic average between all the values off(k)among the neighborhood ofvi, because∑jPij=1. Consequently,f(k)is a harmonic function with domain the state space of the absorbing Markov chain P. If we consider the absorbing states (i.e. the seed locations) of P as the boundaries of the harmonic functionf(k), then the solution to the learning process is the same as the one to the combinatorial Dirichlet problem[42,44], which is to find a harmonic function subject to its boundary values. Those values are known, because obviously we setf(k)(vj)=1if the label of the seed pixelvjis k, and 0 otherwise. The solution to the Dirichlet problem can be expressed in terms of absorbing Markov chains [42], sincef(k)is harmonic for P means thatPf(k)=f(k), which implies that(14)∀n,Pnf(k)=f(k),wherePnis the nth power of matrix P. It has been shown [41] that the nth power of P will approach a matrix of the formP∞=I0B0,so we can rewrite Eq. (14) as follows:f(k)=P∞f(k),fL(k)fU(k)=I0B0fL(k)fU(k),(15)fU(k)=BfL(k),fU(k)=(I-PUU)-1PULfL(k).The last equation is fundamental, because it allows us to compute the value of each functionf(k)at each unlabeled pixel. The final labeling of a given pixelvican be obtained by taking the highest probabilities among all the labels, i.e.(16)y(vi)=argmaxkf(k)(vi).A practical means to estimate the theoretical solution of the learning process given by Eq. (15) (since its direct computation is difficult due to the size of the matrices in use) is to adopt an iterative label propagation behavior [34,48], that has been chosen in this paper for its computational feasibility and quickness of convergence. It is based on the assumption that at each iteration of the algorithm, each unlabeled pixel will update its label membership by ”absorbing” the label information given by the pixels in its spatial neighborhood. Intuitively, the pixelvishould learn more information from a pixel which is more likely to belong to the same semantically meaningful object. Notepijthe likelihood thatviandvjshare the same label in the final segmentation, then the k label membership of an unlabeled pixelviat iteration step t will be computed by(17)f(k)(t)(vi)=∑vj∈Vpijf(k)(t-1)(vj).As that notation suggests, the likelihoodpijwill be taken from the transition matrix P defined by Eq. (3), withpij=Pij. Since the labels of the seed pixels are user-defined labels, they should not be updated and can be clamped at each iteration. Consequently, we can splitf(k)and P asf(k)(t)=fL(k)(t)T,fU(k)(t)TT,P=PLLPLUPULPUU,wherefL(k)(t)andfU(k)(t)correspond to the predicted label memberships for label k of the labeled and unlabeled labels respectively.PLUandPUUrepresent the submatrices of P restricted to the transition probabilities between respectively the labeled and unlabeled locations, and between all the unlabeled locations. Then the update rule given by Eq. (17) can be rewritten as [34,48]:(18)fU(k)(t)=PULfL(k)+PUUfU(k)(t-1).Here the iteration step offL(k)has been omitted since it remains the same in all iteration steps. Therefore, the labels of the unlabeled pixels can be predicted using Eq. (18) until a convergence is achieved. The final labeling after the learning process at a pixel v is then given byy(v)=argmaxkfU(k)(v). We will now show that the label memberships given by this iterative procedure converge to a solution that is equivalent to the theoretical solution found by considering the random walk as an absorbing Markov chain.Proposition 2The iterative update rule from Eq.(18)converges tofU(k)=(I-PUU)-1PULfL(k).When the number of iterations approaches+∞, the iterative update rule from Eq. (18) leads tofU(k)=limt→∞∑i=1tPUUi-1PULfL(k)+PUUtfU(k)(0),wherefU(k)(0)denotes the initial configuration of the label membership of the unlabeled pixels, andPUUtrepresents the tth power of the matrixPUU. We now have to show that the second termPUUtfU(k)(0)→0. If we assume that P is row-stochastic, i.e.∀i,j, we have0⩽Pij⩽1and∑jPij=1, sincePUUis a sub-matrix of P it follows that (with(PUU)ijdenoting the(i,j)th entry of matrixPUU)∃i,∑j=1N-l(PUU)ij<1.Since there is N pixels and L labeled points, noten=N-LandPUUis of sizen×n. Poserias the sum of the ith row ofPUU, i.e.ri=∑j(PUU)ij.PUUis nonnegative, so from Perron-Frobenius theorem [49] we know that the spectral radiusρ(PUU)(i.e. the maximum eigenvalue ofPUU) satisfiesρ(PUU)⩽maxiri, because∀i∑jPij=1we haveri⩽1, soρ(PUU)⩽1. Minc theorem [49] also states thatρ(PUU)⩽maxi∑m=1n(PUU)imrmri.Recall that we haveri⩽1∀iand that∃i,ri<1. So it comes that∑m=1n(PUU)imrmri<∑m=1n(PUU)imri=1,and consequentlyρ(PUU)⩽maxi∑m=1n(PUU)imrmri<1.Because the spectral radius ofPUUis less than 1, thuslimt→∞PUUt=0,limt→∞∑i=1tPUUt-1=(I-PUU)-1.ConsequentlyPUUtfU(k)(0)→0, and thereforefU(k)=limt→∞∑i=1tPUUi-1PULfL(k)+PUUtfU(k)(0),=(I-PUU)-1PULfL(k).□Another version of the proof can be found in [34]. SincefL(k)is immutable, the iterative update rule converges to a unique fixed point and gives us a theoretical guarantee of the feasibility of the algorithm. In addition, this fixed point is equivalent as the theoretical solution given by Eq. (15), which shows that the iterative algorithm is able to solve the labeling problem in terms of random walks.The proposed semi-supervised segmentation framework can be summarized as follows:1.Obtain a set of labeled pixels, either automatically or interactively.For each labeled pixel u with label k, setfL(k)(u)=1.For each unlabeled pixel v and for each label k, setfU(k)(0)(v)=0.BuildH→as the DINH representation of the image following Eq. (9).Compute the directed transition matrix P following Eq. (3).Sett=1andfU(k)(1)=fU(k)(0).Whilet<tmaxor|fU(t-1)-fU(t)|>δ,(a)For each label k, updatefU(k)by:fU(k)(t)=PULfL(k)+PUUfU(k)(t-1).Sett=t+1andfU(k)(t)=fU(k)(t-1).For each pixel v, give it label k such thatargmaxkfU(k)(t)(v).The stopping condition of our iterative algorithm is here controlled by the parametertmax>0if one wants to limit the maximum number of iterations, or the parameterδ>0if one wants to achieve a convergence over the label memberships.

@&#CONCLUSIONS@&#
In this paper, we investigated the utilization of directed hypergraphs in the computer vision domain. We proposed a generalization of undirected hypergraphs into directed hypergraphs, and defined a formulation of a random walk in a directed hypergraph. We proposed a DINH (Directed Image Neighborhood Hypergraph) model as a directed hypergraph representation of the image content, and adapted it to a semi-supervised image segmentation problem using a transition matrix computed from our random walk formulation. Experiments on the standard Microsoft GrabCut [32] datasets showed encouraging results in comparison with algorithms using undirected and directed graphs and undirected hypergraph-based image representation. In particular, results show that the introduction of directional information unto a hypergraph model can help at improving the segmentation results due to an asymmetric weighting of the hyperarcs. Consequently, we introduced the directed hypergraph concept in the computer vision domain and showed its relevance within this context.Our image segmentation application is the first attempt of using directed hypergraphs in image analysis problems. If the results converge to an improvement of the performance compared to algorithms using traditional undirected hypergraphs, our proposition is an open system and several ways can be imagined to get more satisfying results. The comparison with several state-of-the-art supervised segmentation algorithms showed that our iterative approach cannot compete in terms of quality with methods that are able to directly compute an optimal solution to the energy minimization framework, using powerful graph-based tools such as Laplacian regularization [33,35]. The translation of such tools to the undirected hypergraph case is difficult at the moment, due to the asymmetric nature of directed hypergraphs and the lack of prior theoretical work in this field. Proposing coherent definitions of matrix-based representation of directed hypergraphs such as Laplacian matrices can be a direction to explore. Furthermore, different directed hypergraph models for image representation have to be investigated, and particularly for the case of color images. An interesting idea can be to model a video (or an image sequence) by the means of directed hypergraphs, since directional features such as the optical flow can easily be computed directly from the data.Long-term further work in this direction will include the application of directed image hypergraph models in an unsupervised segmentation framework, for example by adapting existing hypergraph-based algorithms like spectral clustering [8] or reductive clustering methods [15] to the directed case.