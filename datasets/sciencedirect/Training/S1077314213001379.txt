@&#MAIN-TITLE@&#
Finite asymmetric generalized Gaussian mixture models learning for infrared object detection

@&#HIGHLIGHTS@&#
We develop a method for learning the parameters of multidimensional asymmetric generalized Gaussian mixtures (AGGM).We describe and illustrate an online algorithm, based on the developed AGGM, for pedestrian detection using infrared images.We implement a multiple-target tracking (MTT) framework using AGGM.We demonstrate the importance of the fusion of both visible and thermal images for MTT.

@&#KEYPHRASES@&#
Infrared,Multiple target tracking,Pedestrian detection,Foreground segmentation,Image fusion,Mixture models,Asymmetric generalized Gaussian,EM,MML,

@&#ABSTRACT@&#
The interest in automatic surveillance and monitoring systems has been growing over the last years due to increasing demands for security and law enforcement applications. Although, automatic surveillance systems have reached a significant level of maturity with some practical success, it still remains a challenging problem due to large variation in illumination conditions. Recognition based only on the visual spectrum remains limited in uncontrolled operating environments such as outdoor situations and low illumination conditions. In the last years, as a result of the development of low-cost infrared cameras, night vision systems have gained more and more interest, making infrared (IR) imagery as a viable alternative to visible imaging in the search for a robust and practical identification system. Recently, some researchers have proposed the fusion of data recorded by an IR sensor and a visible camera in order to produce information otherwise not obtainable by viewing the sensor outputs separately. In this article, we propose the application of finite mixtures of multidimensional asymmetric generalized Gaussian distributions for different challenging tasks involving IR images. The advantage of the considered model is that it has the required flexibility to fit different shapes of observed non-Gaussian and asymmetric data. In particular, we present a highly efficient expectation–maximization (EM) algorithm, based on minimum message length (MML) formulation, for the unsupervised learning of the proposed model’s parameters. In addition, we study its performance in two interesting applications namely pedestrian detection and multiple target tracking. Furthermore, we examine whether fusion of visual and thermal images can increase the overall performance of surveillance systems.

@&#INTRODUCTION@&#
Security of human lives and property has always been a major concern. Nowadays, developing video surveillance systems aimed at monitoring private and public areas has became one of the most active research fields due to the high amount of theft, accidents, terrorists attacks and riots. However, human attention is known to drop after just 30min when engaged in monotonous and repetitive activities [1]. This is the case for security personnel tasked to monitor relatively vast environments where suspicious events are rare. Therefore, automatic video surveillance techniques were proposed to allow automatic processing of the data acquired by surveillance cameras without requiring the continuous attention of human operators. Automatic video surveillance systems are employed in controlled and uncontrolled environments [2]. In controlled or indoor environments (i.e. airports, warehouses, and production plants) monitoring is easier to implement as it does not depend on weather changes [3,4]. Uncontrolled environment is used to refer to outdoor scenes where illumination and temperature changes occur frequently, and where various atmospheric conditions can be observed [4,5].Normally, when setting up a security system there are two major types of security cameras: visual-light, and infrared sensors. Visual-light or color cameras are employed vastly due to their lower cost compared to infrared sensors [6,7]. However, under low illumination sensing in visible spectrum becomes infeasible [8]. Thermal IR sensors measure the emitted heat energy from different objects, which make it invariant to changes in ambient illumination. Hence, IR imaging is a perfect choice for monitoring under low illumination conditions or even in darkness [9]. In order to show that thermal IR offers a promising alternative to visible imagery we will use it for pedestrian detection. Despite its robustness to illumination changes, IR has various drawbacks. One of its disadvantages is its sensitivity to outdoor temperature changes, which make it vulnerable to cold or warm air [10,11]. Some researchers decided to use both visible and infrared images together in order to increase the efficiency of surveillance systems [12,13]. It is widely known in the field of image fusion that the combination of thermal infrared and visible images is not trivial. Fusion techniques can be grouped into two classes: representative and analytical. Representative fusion uses both visible and infrared features together in order to generate a new image more informative or intuitive for a human observer. It is important to understand that the generation of such an image can be of a great importance in the case of human monitoring and is not required for automated video monitoring applications. On the other hand, analytical fusion combines available information from both sensors for a more robust analysis and interpretation of the image or video content. This method is based on the idea that combining both thermal and visible information can overcome the disadvantages of both visible-light images (i.e. shadows problem, sensitivity to variations in illumination and lights) and infrared images (i.e. sensitivity to outdoor temperature changes).Discovering and finding valuable information and patterns in multidimensional data depends generally on the selection of an appropriate statistical model and the learning of its parameters. In recent years a lot of different algorithms were developed in the aim of automatically learning to recognize complex patterns, and to produce intelligent decisions based on observed data. Finite mixture models are now among the most widely used statistical approaches in many areas and applications and allow a formal approach for unsupervised learning. In such context, classic interest is often related to the determination of the number of clusters (i.e. model selection) and the estimation of the mixture’s parameters. The isotropic nature of the Gaussian distribution, along with its capability to represent the data compactly by a mean vector and covariance matrix, has made Gaussian mixture (GM) decomposition a popular technique. However, Gaussian density has some drawbacks such as its symmetry around the mean and the rigidity of its shape, which prevent it from fitting accurately the data especially in the presence of outliers. Fig. 1shows an example of an IR image. We can notice that its intensity distribution is not symmetrical. It is clear that using the GM to represent this distribution is not efficient. In order to overcome problems related to the Gaussian assumption, some researchers have shown that the generalized Gaussian distribution (GGD) can be a good choice to model non-Gaussian data [14,15]. Compared to the GD, the GGD has one more parameter λ that controls the tail of the distribution: the larger the value of λ is, the flatter is the distribution; the smaller λ is, the more peaked is the distribution. Despite the higher flexibility that GGD offers, it is still a symmetric distribution inappropriate to model non-symmetrical data. In this article, we suggest the consideration of the asymmetric generalized Gaussian distribution (AGGD) capable of modeling non-Gaussian asymmetrical data. The AGGD uses two variance parameters for left and right parts of the distribution, which allow it not only to approximate a large class of statistical distributions (e.g. impulsive, Laplacian, Gaussian and uniform distributions) but also to include the asymmetry. As shown in Fig. 1(b) we can notice that the asymmetric generalized Gaussian mixture (AGGM) was able to accurately model the data and outperforms both the GM and the generalized Gaussian mixture (GGM).An important part of the mixture modeling problem concerns learning the model parameters and determining the number of consistent components (M) which best describes the data. For this purpose, many approaches have been suggested. The vast majority of these approaches can be classified, from a computational point of view, into two classes: deterministic and stochastic methods. Deterministic methods, estimate the model parameters for different range of M then choose the best value that maximizes a model selection criterion such as Akaike’s information criterion (AIC) [16], minimum description length (MDL) [17] and Laplace empirical criterion (LEC) [18]. Stochastic methods such as Markov chain Monte Carlo (MCMC) can be used in order to sample from the full a posteriori distribution with M considered unknown [19]. Despite their formal appeal, MCMC methods are too computationally demanding, therefore cannot be applied efficiently for online applications such as automatic video surveillance. For this reason, we are interested in deterministic approaches. In our proposed method, we use K-means algorithm to initialize the asymmetric generalized Gaussian mixture parameters and successfully solve the initialization problem. The number of mixture components is automatically determined by implementing MML criterion [20] into an EM algorithm based on maximum likelihood (ML) estimation. Our learning method can integrate simultaneously parameter estimation and model selection in a single algorithm and is consequently totally unsupervised. It is noteworthy that the proposed work is completely different from recent efforts published for instance in [21–23]. In fact, [21] proposed the use of the gradient and the ML methods for estimating the parameters of only a one-dimensional AGGD. The work in [22] has been devoted to image segmentation using ML estimation of one-dimensional AGGM with known number of components. In [23] a Bayesian nonparametric approach based on infinite GGM was developed for pedestrian detection and foreground segmentation.The rest of this paper is organized as follows. Section 2 describes the AGGM model and gives a complete learning algorithm. In Section 3, we assess the performance of the new model for pedestrian detection and multiple-target tracking; while comparing it to other models. Our last section is devoted to the conclusion and some perspectives.Formally we say that a d-dimensional random variableX→=[X1,…,Xd]Tfollows a M components mixture if its probability function can be written in the following form:(1)p(X→|Θ)=∑j=1Mpjp(X→|ξj)where ξjis the set of parameters of component j, pjare the mixing proportions which must be positive and sum to one, Θ={p1,…,pM,ξ1,…,ξM} is the complete set of parameters fully characterizing the mixture, M ⩾1 is number of components in the mixture. For the AGGM, each component densityp(X→|ξj)is an AGGD:(2)p(X→|ξj)=∏k=1dβjkΓ(3/βjk)Γ(1/βjk)1/2(σljk+σrjk)Γ(1/βjk)exp-A(βjk)μjk-XkσljkβjkifXk<μjkβjkΓ(3/βjk)Γ(1/βjk)1/2(σljk+σrjk)Γ(1/βjk)exp-A(βjk)Xk-μjkσrjkβjkifXk⩾μjkwhereA(βjk)=Γ(3/βjk)Γ(1/βjk)βjk/2and Γ(.) is the Gamma function given by:Γ(x)=∫0∞tx-1e-tdt,x>0. Note thatξj=(μ→j,β→j,σ→lj,σ→rj)is the set of parameters of component j whereμ→j=(μj1,…,μjd),σl→j=(σlj1,…,σljd), andσr→j=(σrj1,…,σrjd)are the mean, the left standard deviation, and the right standard deviation of the d-dimensional AGGD, respectively. The parameterβ→j=(βj1,…,βjd)controls the tails of the pdf and determines whether it is peaked or flat: the larger the value ofβ→j, the flatter the pdf, and the smallerβ→jis, the more peaked the pdf. The AGGD is chosen to be able to fit, in analytically simple and realistic way, symmetric or non-symmetric data by the combination of the left and right variances.LetX=(X→1,…,X→N)be a set of N independent and identically distributed vectors, assumed to arise from a finite AGGM with M components. Thus, its corresponding likelihood can be expressed as follows:(3)p(X|Θ)=∏i=1N∑j=1Mp(X→i|ξj)pjwhere the set of parameters of the mixture with M classes is defined byΘ=(μ→1,…,μ→M,β→1,…,β→M,σ→l1,…,σ→lM,σ→r1,…,σ→rM,p1,…,pM). We introduce membership vectors,Z→i=(Zi1,…,ZiM), one for each observation encoding to which component the observation belongs. In other words, Zij,j=1,…, M equals 1 ifX→ibelongs to class j and 0, otherwise. Taking into accountZ={Z→1,…,Z→N}, the complete-data likelihood is given by:(4)p(X,Z|Θ)=∏i=1N∏j=1Mp(X→i|ξj)pjZijFor the moment, we suppose that the number of mixture components M is known. The ML estimation method consists of getting the mixture parameters that maximize the log-likelihood function given by:(5)L(Θ,Z,X)=∑i=1N∑j=1MZijlogp(X→i|ξj)pjby replacing each Zijby its expectation, defined as the posterior probability that the ith observation arises from the jth component of the mixture as follows:(6)Z^ij=p(j|X→i)=p(X→i|ξj)pj∑j=1Mp(X→i|ξj)pjUsing Eq. (6) we can assign each vectorX→ito one of the M clusters. Now, using these expectations, the goal is to maximize the complete data log-likelihood with respect to our model parameters. This can be done by calculating the gradient of the log-likelihood with respect topj,μ→j,β→j,σ→lj, andσ→rj. When estimating pjwe actually need to introduce Lagrange multiplier to ensure that the constraints pj> 0 and∑j=1Mpj=1 are satisfied. Thus, the augmented log-likelihood function can be expressed by:(7)Φ(Θ,Z,X,Λ)=∑i=1N∑j=1MZijlogp(X→i|ξj)pj+Λ1-∑j=1Mpjwhere Λ is the Lagrange multiplier. Differentiating the augmented function with respect to pjwe get:(8)pˆj=1N∑i=1Np(j|X→i)By calculating the gradients of the complete log-likelihood with respect toμ→j,β→j,σ→lj, andσ→rj, we obtain the following for k=1,…, d:(9)∑i=1,Xik<μjkNZijA(βjk)σljkβjk(μjk-Xik)βjk-1-∑i=1,Xik⩾μjkNZijA(βjk)σrjkβjk(Xik-μjk)βjk-1=0(10)∑i=1,Xik<μjkNZijA(βjk)μjk-Xikσljkβjk3Ψ(3/βjk)-Ψ(1/βjk)2βjk-logμjk-Xikσljk+∑i=1,Xik⩾μjkNZijA(βjk)Xik-μjkσrjkβjk3Ψ(3/βjk)-Ψ(1/βjk)2βjk-logXik-μjkσrjk+∑i=1NZij1βjk-32Ψ(3/βjk)-Ψ(1/βjk)βjk2=0(11)∑i=1,Xik<μjkNZijA(βjk)βjkσljkμjk-Xikσljkβjk-∑i=1NZijσljk+σrjk=0(12)∑i=1,Xik⩾μjkNZijA(βjk)βjkσrjkXik-μjkσrjkβjk-∑i=1NZijσljk+σrjk=0whereΨ(x)=∂log[Γ(x)]∂x. It is easy to notice that the equations from (9)–(12) related to all AGGD parameters are non linear. Thus, we decided to use the Newton–Raphson method to estimate these parameters:(13)μjk≃μjk-∂2L(Θ,Z,X)∂μjk2-1∂L(Θ,Z,X)∂μjk(14)βˆjk≃βjk-∂2L(Θ,Z,X)∂βjk2-1∂L(Θ,Z,X)∂βjk(15)σˆljk≃σljk-∂2L(Θ,Z,X)∂σljk2-1∂L(Θ,Z,X)∂σljk(16)σˆrjk≃σrjk-∂2L(Θ,Z,X)∂σrjk2-1∂L(Θ,Z,X)∂σrjkwhere∂2L(Θ,Z,X)∂μjk2,∂L(Θ,Z,X)∂μjk,∂2L(Θ,Z,X)∂βjk2,∂L(Θ,Z,X)∂βjk,∂2L(Θ,Z,X)∂σljk2,∂L(Θ,Z,X)∂σljk,∂2L(Θ,Z,X)∂σrjk2, and∂L(Θ,Z,X)∂σrjkare given in Appendix A.Different model selection methods have been introduced to estimate the number of components of a mixture model. Among these methods the MML criterion has been shown to perform efficiently. The MML approach is based on evaluating statistical models according to their ability to compress a message containing the data (minimum coding length criterion). High compression is obtained by forming good models of the data to be coded. For each model in the model space, the message includes two parts. The first part encodes the model, using only prior information about its parameters and no information about the data. The second part encodes only the data in a way that makes use of the model encoded in the first part. When applying the MML, the optimal number of classes of the mixture is obtained by minimizing the following function (i.e. the message length) [20,24]:(17)MessLen≈-log(p(Θ))-L(Θ,Z,X)+12log|F(Θ)|+Np2-12log(12)where p(Θ) is the prior probability, —F(Θ)— is the determinant of the Fisher information matrix of minus the log-likelihood of the mixture, and Npis the number of parameters to be estimated and is equal to M(4d+1) in our case. In the following sections, we develop both p(Θ) and —F(Θ)—.We specify a prior p(Θ) that expresses the lack of knowledge about the mixture parameters. It is reasonable to assume that the parameters of different components in the mixture are independent, since having knowledge about a parameter in one class does not provide any knowledge about the parameters of another class. Thus, we can assume that our parameters(μ={μ→j},β={β→j},σl={σ→lj},σr={σ→rj},P=(p1,…,pM))are mutually independent, then:(18)p(Θ)=p(μ)p(β)p(σl)p(σr)p(P)In what follows, we will compute each of these priors separately. Starting with p(P), we know that P is defined on the simplex{(p1,…,pM):∑j=1Mpj=1}. Then, a natural choice as a prior for this vector is the Dirichlet distribution(19)p(P)=Γ∑j=1Mηj∏j=1MΓ(ηj)∏j=1Mpjηj-1where (η1,…,ηM) is the parameter vector of the Dirichlet distribution. When η1,…, ηM=η=1 we get a uniform prior over the space p1+⋯+pM=1. This prior is represented by(20)p(P)=(M-1)!For the parameter μ, we take a uniform prior for each μjk. Each μjkis chosen to be uniform in the region(μk-σlk⩽μjk⩽μk+σrk), then the prior for μ is given by(21)p(μ)=∏j=1M∏k=1dp(μjk)=∏k=1d1(σlk+σrk)MFor the parameter β, we adopt a uniform distributionU[0,h]for each βjk, where h is the maximum value permitted. Then the prior for β is given by(22)p(β)=∏j=1M∏k=1dp(βjk)=1hMdIt is known that(0⩽σljk⩽σlk)and(0⩽σrjk⩽σrk)for σland σr, respectively. Then, for both parameters σland σrwe take a uniform prior for eachσljkandσrjk(23)p(σl)=∏j=1M∏k=1dp(σljk)=∏k=1d1σlkM(24)p(σr)=∏j=1M∏k=1dp(σrjk)=∏k=1d1σrkMFinally, by replacing the priors in Eq. (18) by the expressions in Eqs. (20)–(24), we get(25)p(Θ)=(M-1)!hMd∏k=1d1σlkMσrkM(σlk+σrk)MThe Fisher information matrix is the expected value of the Hessian of minus the logarithm of the likelihood. It is difficult, in general, to obtain analytically the expected Fisher information matrix of a mixture. Therefore, we use the complete Fisher information matrix which determinant is equal to the product of the determinants of the information matrices with respect to the parameters of each mixture component:(26)|F(Θ)|=|F(P)|∏j=1M|F(μ→j)||F(β→j)||F(σ→lj)||F(σ→rj)|whereF(P)|,|F(μ→j)|,|F(β→j)|,|F(σ→lj)|, and|F(σ→rj)|are the Fisher information with regards toP,μ→j,β→j,σ→lj, andσ→rj, respectively. Regarding —F(P)— it is straightforward to show that:(27)|F(P)|=NM-1∏j=1MpjThe Hessian matrices when we consider the vectorsμ→j,β→j,σ→lj, andσ→rjare given by(28)F(μ→j)k1,k2=∂2L(Θ,Z,X)∂μjk1∂μjk2(29)F(β→j)k1,k2=∂2L(Θ,Z,X)∂βjk1∂βjk2(30)F(σ→lj)k1,k2=∂2L(Θ,Z,X)∂σljk1∂σljk2(31)F(σ→rj)k1,k2=∂2L(Θ,Z,X)∂σrjk1∂σrjk2where (k1,k2)∈(1,…,d). Using A to compute the derivatives in Eqs. (28)–(31), we obtain(32)|F(μ→j)|=∏k=1d-A(βjk)βjk(βjk-1)∑i=1,Xik<μjkNZij(μjk-Xik)βjk-2σljkβjk+∑i=1,Xik⩾μjkNZij(Xik-μjk)βjk-2σrjkβjkj=1,…,M(33)|F(β→j)|=∏k=1d-∑i=1NZij1βjk2+3Ψ′(1/βjk)2βjk4+3Ψ(1/βjk)-Ψ(3/βjk)βjk3-9Ψ′(3/βjk)2βjk4+A(βjk)∑i=1,Xik<μjkNZijμjk-Xikσljkβjk9Ψ′(3/βjk)-Ψ′(1/βjk)2βjk3+3Ψ(3/βjk)-Ψ(1/βjk)2βjk2+3Ψ(3/βjk)-Ψ(1/βjk)2βjk-logμjk-Xikσljk2+A(βjk)∑i=1,Xik⩾μjkNZijXik-μjkσrjkβjk9Ψ′(3/βjk)-Ψ′(1/βjk)2βjk3+3Ψ(3/βjk)-Ψ(1/βjk)2βjk2+3Ψ(3/βjk)-Ψ(1/βjk)2βjk-logXik-μjkσrjk2j=1,…,MwhereΨ′(x)=∂2log[Γ(x)]∂x2.(34)|F(σ→lj)|=∏k=1d∑i=1NZij(σljk+σrjk)2-A(βjk)βjk(βjk+1)∑Xk<μjkZijσljk2μjk-Xikσljkβjkj=1,…,M(35)|F(σ→rj)|=∏k=1d∑i=1NZij(σljk+σrjk)2-A(βjk)βjk(βjk+1)∑Xik⩾μjkZijσrjk2Xik-μjkσrjkβjkj=1,…,MIn the following steps, we summarize the algorithm used for the learning of our AGGM1The complete source code is available upon request.1:Algorithm 1.Input: Data setXand MmaxOutput:ΘM∗(the values of Θ when M∗ components are chosen) and M∗Step 1: ForM=1: Mmaxdo{1.Initialization.2.Repeat until convergence.(a)The expectation step using Eq. (6).(b)The maximization step using Eqs. (13)–(16).3.Calculate the associated message length using Eq. (17).} END FORStep 2: Select the modelM∗with the smallest message length value.In order to initialize the parameters, we used the K-Means algorithm. Note that we initialized both the left and right standard deviations with the standard deviation values obtained from the K-Means, as for the values of the shape parameters we initialized them to 2. It is noteworthy that this is equivalent actually to reducing the AGGM to a simple GM at the initialization step. Concerning the convergence, we stop the iterations when the log-likelihood does not change much from one step to the next. More interesting and detailed information on the convergence properties of the EM algorithm can be found in [25].

@&#CONCLUSIONS@&#
