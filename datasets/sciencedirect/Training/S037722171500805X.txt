@&#MAIN-TITLE@&#
Compounding problem in an interactive multiple objective optimization method

@&#HIGHLIGHTS@&#
We find a compounding problem in an earlier interactive multiobjective optimization method.The compounding problem means that the algorithm generates an unexpected solution.To avoid this problem, we develop an improved interactive method.

@&#KEYPHRASES@&#
Multi-objective programming,Interactive method,Compounding problem,

@&#ABSTRACT@&#
This note finds and resolves a compounding problem in the interactive multi-objective optimization method developed by Park and Shin (2012). The compounding problem means that the algorithm generates a solution unexpected by the decision maker. A concrete example of the compounding problem is demonstrated and, to remove the problem, an improved algorithm is then developed. We also show that the improved algorithm converges to the known optimal solutions faster than the earlier algorithm.

@&#INTRODUCTION@&#
In this note, we revisit the interactive multi-objective optimization (MOO) method developed by Park and Shin (2012) and find a compounding problem, meaning that the algorithm might generate an unexpected solution in relation to what the decision maker (DM) desires. We therefore modify the algorithm in order to avoid the compounding problem. Furthermore, we provide the results of computational tests made for the convergence of the improved algorithm, demonstrating that the improved algorithm converges to the known optimal solutions faster than the earlier algorithm.In the next section, we show a concrete example of the compounding problem and then describe why it occurs. The improved algorithm is then presented, which is followed by the computational experiments regarding the convergence of the algorithms.We recall the nonlinear MOO problem used in Park and Shin (2012) as follows:(1)maxf(x)=[f1(x),f2(x),−f3(x),−f4(x)]s.t.x1+x4=2.68x2−x5=1x2+x6=5.07x3−x7=94x3+x8=174.15where x = (x1, … , x8) ≥ 0 is the vector of decision variables, andf1(x)=−85.918+38.555x1+1.018x3−2.374x12+0.668x22+0.001x32f2(x)=54.549+10.567x2−0.085x12+0.004x1x3−0.707x22f3(x)=0.568−0.092x1−0.004x3+0.051x12−0.002x22+0.001x32−0.008x13f4(x)=0.549−0.396x2+0.114x22−0.001x1x3+0.002x12x2+0.014x23+0.001x24Note that the first two objective functions are to be maximized and the remaining two are minimized. The Park and Shin method is applied to problem (1) as follows:Steps 1 and 2: We begin with an initial solution, xh=1 = (2.680, 1.730, 94.070, 0, 0.730, 3.340, 0.070, 80.080), where h stands for the iteration number. Then, fh=1 = [ f1(x1), … , f4(x1)] = [106.970, 72.005, 9.001, 0.059]. Referring to solution x1, the variable vector x is decomposed into the basic variables xB = (x1, x2, x3, x6, x8) and the nonbasic variables xN = (x4, x5, x7). For xN, the reduced gradient vectors are calculated as in Table 1.Steps 3 and 4: Table 1 is presented to the DM along with questions of whether or not each of the tradeoff vectors is preferred. We see that an increase in x5 indicates improvements in every f1, f2 and f3, but a decline in f4. Suppose that the DM prefers this tradeoff (i.e., an increase in x5). Suppose she also says “don't know” for x4 and x7. We then construct the linear program,maxɛs.t.2.311w1+9.860w2+0.007w3−0.159w4≥ɛw1+w2+w3+w4=1;wi≥ɛ,∀iand solve it to obtain the weight vector, w = (0.25, 0.25, 0.25, 0.25). Note that, in this linear program, no restrictions are imposed for the nonbasic variables corresponding to the “don't know” response.Steps 5 and 6: The rescaled direction vector is calculated as z = (0, 3.340, 0.288, 0, 3.340, −3.340, 0.288, −0.288), toward which the current solution x1 will be moved. Thus, for various possible values of t ∈[0, 1], we prepare a series of objective value vectors, as shown in Table 2. We then present this table to the DM and ask her to choose the most preferred vector of the objective values.Now, we compare Table 2 and what the DM has expected in Step 3. Recall that f1 and f2 are to be maximized, whereas f3 and f4 are minimized. In Table 2, as the DM has expected, f1 and f2 continue to improve and f4 worsens, as the t value increases. Unfortunately, f3 is worsening, even though the DM has expected an improvement in f3. This is an example of compounding problem.To see why such a compounding problem arises, we recall the rescaled direction vector z in Step 5. This indicates no change in x4 but increases in both x5 and x7, for xN = (x4, x5, x7), thereby resulting in Table 2. An increase in x5 was what the DM wanted in order to improve f1, f2 and f3 in Step 3. However, an increase in x7 happens to occur together. In Table 1, the reduced gradient vector for x7 dictates improvements in both f1 and f2 (as well as f4), but a decline in f3, as the x7 value increases. Moreover, we compare the two reduced gradients of x5 and x7 for f3: 0.007 and −0.184. It follows that the speed of worsening f3, as the x7 value increases, is much faster than the speed of improving f3 as the x5 value increases. Therefore, as in Table 2, the increase in x7 results in the decline in f3. Again, what the DM desires is to improve all f1, f2 and f3, no matter what happens in x7.We have shown an example of compounding problem, that is, the algorithm increases the value of a nonbasic variable corresponding to the “don't know” answer, and hence yields an unexpected solution. In fact, whether to increase or decrease such a nonbasic variable value is unclear. Nevertheless, this is determined largely by the algorithm. The next solution depends heavily on that choice and, moreover, may be undesirable from the DM.Such a compounding problem may arise from different situations. If there are multiple “yes” and/or multiple "no" answers for the reduced gradient data presented at a fixed iteration, then the multiple nonbasic variable values may change at once. Note that the reduced gradient vectors of the nonbasic variables are usually conflicted, because one of these indicates an improvement in an objective function, but another signals a decline in the same objective function. Therefore, if the two or more nonbasic variable values change at once, then it may be difficult to anticipate the next solution and, moreover, it is possible to yield an undesirable solution.To avoid the compounding problem, we modify the earlier method so that the change in only one nonbasic variable value is allowed per iteration, as in the spirit of the Simplex method in the linear programs. We thus select one most prospective nonbasic variable and then alter its value alone per iteration. The most prospective variable means that, from altering its value in the current iteration, we may expect a maximal improvement in the DM's value function.Consider the following MOO problem:(2)maxU[f(x)]=U[f1(x),f2(x),…,fk(x)]s.t.x∈S={x|Ax=b,x≥0}where x∈ℜnis a vector of decision variables, fi(i = 1, … , k) are all continuously differentiable nonlinear objective functions to be maximized, and S ⊂ ℜnis the feasible region formed by m linear constraints (A is an m × n matrix, b ∈ ℜm) and the non-negativity condition. The ultimate objective of this problem is to maximize the value function of the DM, U: ℜk→ ℜ, subject to x ∈ S. The improved interactive method to solve problem (2) is summarized as follows:Step 1:Choose an initial solution, x1∈ S, and set the iteration number to h = 1.Decompose the decision variables x, referring to the current solution xh, into the basic and nonbasic parts, x = [xB, xN]. Similarly, partition A and ∇xf(x) as A = [B, N] and ∇xf(x) = [∇Bf(x), ∇Nf(x)], respectively. Calculate the vector of reduced gradients for the nonbasic variables in xN, rNifor all i, using the equation, rNi= ∇Nfi(xh) − ∇Bfi(xh)B−1 N.Present the DM with the rNivalues in the form of the (efficient) rNjvectors (note that the rNivector corresponds to the objective function-oriented arrangements of the individual reduced gradients, whereas the rNjvector is a result of the decision variable-oriented arrangements, and hence represents the reduced gradient vector of nonbasic variable xj). For each of these tradeoff vectors, ask the DM to answer whether or not the tradeoff is preferred. Keep all the responses in the following form:(3)wrNj≥ɛ,ifrNjispreferred(i.e.,said``yes"")wrNj≤−ɛ,ifrNjisnotpreferred(said``no""),where ε > 0. In the case of a “don't know” response, exclude it from consideration. If the DM's response to all the tradeoff questions is “don't know,” then stop; the current solution may become the final solution.Construct and solve the following linear programs: For each of the nonbasic variables xjanswered “yes” at the current iteration,(4)αj=maxwrNjs.t.alltherestrictionsgeneratedsofarintheformof(3)andΣwi=1;wi≥ɛ,∀i.For each of the nonbasic variables xjanswered “no” at the current iteration,(5)βj=minwrNjs.t.alltherestrictionsgeneratedsofarintheformof(3)andΣwi=1;wi≥ɛ,∀i.In both (4) and (5), ε is a small positive number such as 10−4.Denote an improving and feasible direction by d = (d1, …, dn), and let d = [dB, dN] according to the partition of x = [xB, xN] in Step 2. First, determine djin dN as follows:(6)Setαj+=max{αj|αj≥0}andβj−=min{βj|βj≤0}.(7)Ifαj+≥|βj−|,thendj={αj+,j=j+0,j≠j+.Otherwise,ifαj+<|βj−|,thendj={βj−,j=j−andxj>00,otherwiseSecond, calculate the dB vector as dB = −B−1NdN. Third, calculate the rescaled direction of the d vector as z = τmaxd, where τmax = min {−xjh/dj| dj< 0 for j = 1, … , n}. Fourth, prepare objective value vectors, {f(xh+ tz)}, for various possible values of t ∈ [0, 1].Present {f(xh+ tz)} to the DM, and ask her to select an optimal t value, t*. Then, xh+1 = xh+ t*z becomes a new improved solution and f(xh+1) becomes the new vector of objective values. If the DM is satisfied with the new solution, then stop. Otherwise, set h = h + 1 and return to Step 2.The only difference between the earlier method (Park & Shin, 2012) and the present method is how to determine the direction vector dN for xN in Steps 4 and 5. In the earlier method, the dN vector is given by wrNj, where the weight vector w is estimated using the following linear program (Roy & Wallenius, 1992):(8)maxɛs.t.alltherestrictionsgeneratedsofarintheformof(3)andΣwi=1;wi≥ɛ,∀i.There may thus exist multiple positive and/or negative numbers in the dN vector, which entails a simultaneous change in two or more nonbasic variable values at once, which causes the compounding problem. This change is apparent if there are two or more “yes” or “no” responses in Step 3 for the current iteration. Even if there is only one “yes” or “no” answer, such a simultaneous change may arise. This is because, as described in the previous section, the nonbasic variable value corresponding to the “don't know” response may increase or decrease together, depending on the estimated weight vector w.In the present method, using models (4) and (5) in Step 4, we find the maximum possible value of |wrNj|, for each of the nonbasic variables xjanswered “yes” or “no” by the DM at the current iteration. This maximum value means the maximum possible rate of increase in the value function U, as the xjvalue changes. This is because, given a linear proxy for U, the term wrNjrepresents ∂U/∂xj, implying the change in U according to the unit change in xj. Now using Eqs. (6) and (7) in Step 5, we select only one nonbasic variable that has the largest absolute value from among the maximum possible values computed in Step 4. This variable is referred to as the most prospective nonbasic variable in the current iteration. The direction for this variable is then given by its own maximum possible value, where its sign should be recovered. The directions for the other nonbasic variables are all set to zero. Therefore, only one nonbasic variable value can change per iteration, thereby avoiding the compounding problem.In this section, we report our computational experiments regarding the convergence of the earlier and present method. First of all, we reuse the MOO problem in (1), referred to as the new branch design problem in Park and Shin (2012). A brief overview of our test procedures is that we first establish a target assumed as the most preferred solution of the DM, then respectively run the two methods in order to hit the established target, and finally record the numbers of iterations taken until hitting the target.To set a target solution of the objective functions, three different types of value functions U: ℜk→ ℜ are employed: Weighted sum (∑wifi), weighted product (∏fiwi), and sum of weighted exponentials (∑(ewi−1)efi). For each type of these value functions, we randomly select a set of weights wifrom a (0, 1) uniform distribution. We then have a single objective problem to be maximized, solve it, and then assume that this solution [ fi**, i = 1, … , k] becomes the target of the given MOO problem. We now run the earlier and present method, respectively, in order to reach the target, starting from the same initial solution arbitrarily set independently of the target. We terminate the iterations if the following stopping rule is satisfied:(9)|fi*−fi**|fimax−fimin×100%≤5%for every i, where fi* represents the final solution from the earlier method or the present method, and fimax and fimin respectively are the maximum and minimum possible values obtained from individually optimizing each objective function i in the given MOO problem. We finally record the numbers of iterations taken until we stop.In addition, special attention should be paid to the product value function, because the existence of one or more negative fivalue among the k objective functions in the product function creates confusion as to the maximization objective. To avoid this, we modify fi, if its value possibly becomes negative over the feasible region S, to fi← fi+ Mi, where the big Miis a sufficiently large constant so that the fi+ Mivalue is guaranteed to be positive. As such, all the k objective function values become positive and the original meaning of maximizing individual objective functions fiis preserved.We carry out the above tasks ten times, respectively for the two methods, and then record the average numbers of iterations, as shown in the first three rows of Table 3. For the first two value functions, the present method converged much faster than the earlier method (and among the twenty trails in total, the cases that the earlier method was faster than the present method were very few). For the exponential value functions, a similar result yielded.We are aware of other MOO problems structurally similar to the new branch design example in (1), in the sense that each of the objective functions is a quadratic function and the set of constraints forms a convex polyhedron. For further computational tests, we could have three different design problems available from the literature. The first example is the polymer design problem, employed in Park and Kim (2005), where there are four objective functions (two mean and two variance functions) and nine decision variables (including slack as well as structure variables). Note that the size of this problem is similar to (1). The second design example is the microwave-assisted extraction problem, presented in Hu, Cai, and Liang (2008), where there are six objective functions and twelve decision variables. The last design example is the tire tread compound problem, employed in Köksalan and Plante (2003), in which eight objective functions and nine decision variables are involved.For each of these design problems, we execute the same computations that we have done for the new branch design problem in (1). Table 3 also summarizes the results. In both methods, as the number of objective functions increases, more iterations are required; and the product and exponential value functions require more iterations than the sum value function. However, for all types of value functions, the present method appears to be faster than the earlier method. During our computational study, a severe compounding problem arose frequently in the earlier method. This hindered a visible movement toward the target and hence gave rise to a slow convergence to the target. On the contrary, no such a compounding problem happened in the present method. Therefore, we could achieve a predictable solution per iteration and, moreover, accomplish a faster convergence to the most preferred solution.

@&#CONCLUSIONS@&#
