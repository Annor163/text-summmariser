@&#MAIN-TITLE@&#
Spatially aware feature selection and weighting for object retrieval

@&#HIGHLIGHTS@&#
We propose a more accurate similarity measurement for object retrieval.Our method improves two features of the BoW model.Spatial expansion can incorporate more latent visual words into a query.Visual word re-weighting can increase weights of reliable visual words.The combination of them can improve both precision and recall.

@&#KEYPHRASES@&#
Object retrieval,Bag-of-words,Spatial expansion,Visual word re-weighting,

@&#ABSTRACT@&#
Many recent image retrieval methods are based on the “bag-of-words” (BoW) model with some additional spatial consistency checking. This paper proposes a more accurate similarity measurement that takes into account spatial layout of visual words in an offline manner. The similarity measurement is embedded in the standard pipeline of the BoW model, and improves two features of the model: i) latent visual words are added to a query based on spatial co-occurrence, to improve query recall; and ii) weights of reliable visual words are increased to improve the precision. The combination of these methods leads to a more accurate measurement of image similarity. This is similar in concept to the combination of query expansion and spatial verification, but does not require query time processing, which is too expensive to apply to full list of ranked results. Experimental results demonstrate the effectiveness of our proposed method on three public datasets.

@&#INTRODUCTION@&#
Recent years have witnessed increasing interest in the problem of object retrieval [1–7] and its applications [8–12]. Typically, the aim of this task is to select from a collection of images which contain the same object as a query image. Many works have addressed this problem with a standard “bag-of-words” (BoW) framework [1,13,14,2,15] borrowed from text retrieval. In this framework, local features in an image are detected and then represented by D-dimensional descriptors. To reduce memory usage and achieve fast indexing, these descriptors are clustered and then quantized to form a vocabulary of “visual words”. An image is represented by a sparse frequency histogram over the visual vocabulary.Although the BoW model is simple to implement and robust to some types of image variation, an object in a target image still can fail to be retrieved from the dataset, mainly due to quantization used in this standard pipeline. As noted by many works over the past few years, quantization introduces two main problems:•Loss of recall: information about the raw features is lost, so matching features may be assigned to different visual words. This can decrease the matching score of images that contain the same object and therefore reduce recall.Loss of precision: features that do not correspond to the same location may be assigned to the same word. This can increase the matching score of images that do not contain the same object, and therefore reduce result precision.To address the first problem, a number of methods focus on improving the query recall. For example, soft-assignment [4,16] method maps a raw feature to multiple visual words; in [17,18] a more accurate approximate nearest neighbor assignment is implemented by combining k-means clustering and binary vector signatures; the dimension reduction and the indexing algorithm are jointly optimized in [19,20], such that the image representation provides accurate search results with low vector dimensionality. Other methods aim to learn a better metric for feature quantization [5] or group similar features when generating the vocabulary [21,6]. Query expansion [3,22,23] addresses the loss of recall in a post-processing step. It enriches the query model by adding query relevant words collected from the initial retrieval results. To address the second problem, a re-ranking process is required [2,17,24], which filters out highly ranked false positives in the initial retrieval results. These methods mainly rely on the spatial consistency of visual words in pairs of query and dataset images. Precision is increased after images that do not contain a significant number of spatially consistent matches are re-ranked lower.In this paper, we address loss of precision and recall by proposing a novel spatial co-occurrence measure. We use an image similarity measurement that takes into account the spatial layout of visual words, but can be embedded into the standard retrieval pipeline. We do this by creating a visual thesaurus that records spatial co-occurrence information for each pair of words in a vocabulary. Based on that, we explore two types of image-dependent contextual information to improve the BoW similarity measure: a voting-based method to include latent visual words and an information theory based measurement to re-weight the importance of each visual word. This enables us to improve both recall and precision of the standard BoW object retrieval method.Spatial expansion. A query can be expanded by including latent words, which are highly correlated with those already present in the query. The correlation is deduced from the degree of spatial co-occurrence of the visual words. In this way, the query recall can be improved.Visual word re-weighting. After additional words are introduced to the query, the tf-idf scheme weights each visual word according to its frequency in the individual image as well as in the corpus. However, tf-idf weights do not distinguish between word appearances in the foreground or background of an image. Therefore it might assign high weights to words that are not informative when searching for an object, or underweight words that are informative. Thus we develop a word weight that is more directly based on how often, and in what range of conditions, a word is correctly matched when it appears as part of the foreground object. Using the automatic training data collection method of [25], we select a subset of visual words that frequently occur as inliers to object matches robustly estimated between images. Under the standard tf-idf weighting, these visual words are not necessarily weighted strongly—for example they may occur in many images in the database, and therefore have a low idf weight. Based on these inlier measurements we use entropy to measure the importance of a visual word according to its spatial co-occurrence distribution. A re-weighting scheme is proposed in this paper to encourage these informative visual words. In this way, query precision can be improved.As illustrated above, and described in more detail in Section 4, our method captures the underlying spatial relationship among the visual words, and their importance as an indicator of a foreground object, and thereby improves both precision and recall. Our method is similar to recent methods [26,3], but inspired by those which also discover the spatial relationships among the visual words [27–30]. As we will explain in the following section in more detail, the key differences between our work and previous methods are that ours focus on improving precision and recall without the use of result re-ranking or issuing multiple queries. This allows a trade-off between effectiveness and efficiency by adjusting the query vector online to incorporate this spatial and relevance information, based on weights computed offline. In addition, our method can be applied wherever the BoW model is used.The remainder of the paper is organized as follows: Section 2 briefly reviews related works about the retrieval methods. Section 3 outlines our method and Section 4 describes the details of methods, including the definition of the visual thesaurus and two usages of the visual thesaurus: spatial expansion (Section 4.1) and visual word re-weighting (Section 4.2). We report the experimental results in Section 5. The paper is finally concluded in Section 6.

@&#CONCLUSIONS@&#
