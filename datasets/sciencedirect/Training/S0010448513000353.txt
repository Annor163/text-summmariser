@&#MAIN-TITLE@&#
Handling missing values and unmatched features in a CBR system for hydro-generator design

@&#HIGHLIGHTS@&#
Case base is constructed based on domain ontology to improve retrieval efficiency.Case representation is proposed and cases are represented by a unified tree model.The cost function is proposed to measure the semantic difference between two cases.The similarity function is defined based on the cost function.Experiments are executed to evaluate the performance of the proposed CBR system.

@&#KEYPHRASES@&#
CBR,Missing value,Unmatched feature,Hydro-generator design,

@&#ABSTRACT@&#
Hydro-generator design is a complex problem and case based reasoning (CBR) can improve its efficiency, but there are missing values and unmatched features which decrease the accuracy of CBR. In order to solve the problems brought by missing values and unmatched features, a similarity measurement is proposed by improving the edit distance which is widely used as a similarity measurement. In the proposed CBR system, the case base is constructed based on domain ontology to improve the retrieval efficiency. Then a case representation is proposed and cases are represented by a unified tree model. Next, by combining the edit distance with feature weights and the semantic meanings of case nodes, the cost function is proposed to measure the semantic difference and the conditions which make it a metric are discussed. Lastly, the similarity function is defined based on the cost function. A case study is presented to illustrate the use of the proposed CBR system, and then the experiments are executed to evaluate its performance in dealing with missing values and unmatched features respectively. The results validate that the proposed CBR system can handle missing values and unmatched features effectively.

@&#INTRODUCTION@&#
Hydro-generator design is a complex problem which consists of a rotor, stator, electromagnetic design, and so on. There are massive parameters in the process of design and the model of the hydro-generator is complicated, so plenty of domain knowledge is required in traditional design which designs the hydro-generator step by step according to design handbooks, and the design period is time-consuming. In addition, there is abundant experience which may not be in design handbooks. Case based reasoning (CBR) is useful and effective if similar design requirements take place. CBR is an approach which solves a new problem by retrieving past cases and revising their solutions. If CBR is utilized in hydro-generator design, it can avoid repeated design, save time and guarantee the design is reasonable.There are several advantages of CBR: (1) it has a relatively small computation cost of incremental learning [1]; (2) it does not rely on statistical assumptions and its justifications are human-understandable [2]; (3) it does not require an explicit domain model in case retrieval [3], for example, the designers have no need to know about the structure of hydro-generator exactly when they use CBR to retrieve the most similar case. But as for the construction of case base or case representation, the domain model may be required.CBR is widely used in many fields, such as fixture design [4], failure mechanisms identification [5], assembly process design [6], injection mould design [7], engine ignition system diagnosis [8], welding fixture design [9], hydraulic circuit design [10], ship structure design [11], and so on. The process of CBR is shown in Fig. 1.The CBR process contains four steps: retrieve, reuse, revise and retain, which are represented by text boxes in Fig. 1. A lot of researches have been carried out on CBR, and they emphasize on different aspects, such as the construction of the case base [12], feature selection [13], case representation [6,14], feature weighting method [15,16], revise method [6,10], and so on.The most important step of CBR is case retrieval, and there are some problems in practical application, such as missing values and unmatched features. As for missing values, there are several approaches. Agre [17] uses the number of possible feature values to estimate the feature distance between two cases, and the distance is represented asdis2(xi,yi)=1/Li×(1−1/Li), whereLiis the number of possible values of thei-th feature. Gu, Tong, Aamodt [18] and Surma [19] use the following equation to calculate the distance between two case features with missing values:dis(xi,yi)=1, ifxioryiis unknown. Ricci and Avesani [20] calculate the similarity according to the following equation:sim(xi,yi)=0.5, ifxioryiis unknown. Hruschaka and Ebecken [21] substitute the missing values by the corresponding feature value of the most similar complete case. Zhong, Guo, Ye, etc. [22] define a “missing degree” conception based on feature weights to describe the missing grade of the feature values. According to these literatures, we can see that most of these approaches use a fixed value or an estimated value to represent the distance between two case features with missing values. However, these methods do not have a unified support model. The distance between two features with missing values can be set as 1, 0.5 or some other values in different approaches, while all these approaches are effective. Then there are two problems: one is the value range of the fixed or estimated distances between two features with missing values; and the other is that there are a number of choices, but there is no principle for designers to determine which value is suitable in practice.In case retrieval, most of the similarity measurements are based on distance, and there are some distances, such as Euclidean distance which is represented asdis(X,Y)=[∑i=1Ndis(xi,yi)2]1/2and Manhattan distance which isdis(X,Y)=∑i=1N|dis(xi,yi)|, wheredis(xi,yi)is the normalized distance which is represented asdis(xi,yi)=|xi−yi|/|maxi−mini|. Although these distances are different, they all require that the features of two cases should be in one-to-one correspondence. However, sometimes it needs to retrieve in different products because they are similar, while different products have different features. That is, the features of different products may not be in one-to-one correspondence. For example, supposing that product1 and product2 are similar, and one case in product1 has features A, B and C, while the other case in product2 has features A, B and D, so the features C and D are unmatched features. The conventional distances cannot be utilized directly in this situation. The unmatched features should be eliminated firstly, and then conventional distances can be utilized. However, the conventional distances only use features A and B, so some useful information is ignored.In this study, the similarity function which is different from the conventional distances is proposed to solve the problems of missing values and unmatched features, and it is based on the edit distance which is used as a similarity measurement. This paper is structured as follows. Section 2 introduces the edit distance. Section 3 details the proposed CBR system. Firstly, the case base is constructed based on domain ontology. Then the case representation is introduced. Next, the cost function based on the edit distance is defined to measure the semantic difference between two cases, and the conditions which make it a metric are discussed. Subsequently, the similarity function is proposed based on the cost function and its calculation algorithm is introduced. In Section 4, a case study is presented and the performance evaluations for the proposed system in dealing with missing values and unmatched features are implemented respectively. Section 5 draws the conclusion.The edit distance which is also known as Levenshtein distance [23] is utilized to describe the difference between two strings originally. It is the minimum total number of edit operations which have to be applied to edit one string into the other and the edit operations contain insertion operation, deletion operation and substitution operation. The edit distance has been extended from string to graph. The edit distance is widely used in many fields, such as relational clustering [24], document clustering [25], structural pattern classification [26], shape recognition [27], handwritten character recognition [28], image categorization [29], the matching of tree ring sequences [30], and so on.As for string, given an alphabetV, the stringtis composed of an ordered sequence of letters fromVof finite length [26], andεdenotes the empty string. Then three edit operations are defined. Insertion operation(ε→v)denotes that lettervis inserted into a string. For example, transformation “beuty” to “beauty” is inserting letter “a”. Deletion operation(u→ε)denotes letteruis removed from a string, such as transformation “beauty” to “beuty”. Substitution operation(u→v)means a letteruis replaced by letterv, such as transformation “beauty” to “beeuty”. The edit distance between stringt1and stringt2is defined as the minimum total number of insertion operationsI, deletion operationsDand substitution operationsS, that is,ED(t1,t2)=minj[I(j)+D(j)+S(j)].The graph edit distance is similar to string edit distance. Because graph is composed of node and edge, there are 6 edit operations in the graph, which are node insertion operation(ε→v), node deletion operation(u→ε), node substitution operation(u→v), edge insertion operationε→(p,q), edge deletion operation(p,q)→ε, and edge substitution operation(p,q)→(r,s), whereu,vdenote the nodes, and(p,q)denotes the edge between nodespandq. The graph edit distance is also defined as the minimum total number of edit operations.In this study, the edit distance is introduced into CBR because it can measure the difference between two patterns which are not of equal length, and this advantage can be utilized to handle the problems of missing values and unmatched features.The elements of the proposed CBR system contain three parts: (1) construction of case base; (2) case representation and (3) similarity measurement. The implementations are presented as the following.There are abundant cases in the case base and CBR is an incremental learning approach, so the computational complexity is high if it calculates the similarities between query case and all cases in the case base in every case retrieval. A good method of organizing and storing cases would reduce the number of cases that must be matched and avoid unnecessary similarity calculations in case retrieval. Thus, it is a key problem for CBR that how to organize the abundant cases so that the retrieval is efficient, and the domain ontology is utilized to construct the case base in this study.Domain ontology can be utilized to describe the knowledge of one specific domain, and it is composed of classes (or concepts) and a class represents a type of product. The classes in domain ontology have relationships with each other, such as the inheritance relationship. There are cases of different types of products in the case base, and the construction contains three steps: (i) identifying all the classes of products; (ii) analyzing the relationship between classes and constructing the class tree; (iii) linking cases to the class which these cases belong to. Take the hydraulic turbine for example, the structure of its case base is shown in Fig. 2.In Fig. 2, class is represented by a text box with three parts and the top part is the class name while the contents in the other two parts are hidden. The hollow arrow which represents the inheritance relationship is directed from subclass to superclass. The case is represented by a box and the case name is underlined. The domain ontology is a hydraulic turbine, and different types of hydraulic turbine products are classes. All the cases are linked to corresponding classes. Generally, all the cases are linked to leaf class. Because the child class inherits from its parent class, the cases which belong to a child class also belong to a parent class. The aim of the case base is to improve the efficiency of case retrieval. The cases are classified into different classes, so the CBR system only needs to retrieve classes which satisfy the design requirements in case retrieval. Otherwise, it has to traverse all the cases in the case base every time. It reduces the number of cases that must be matched and also reduces the chance of retrieving wrong cases by partitioning the case base.A hydro-generator is composed of a number of components, such as a rotor and stator, and some components contain subcomponents and parts, for example, a brush is a subcomponent of a rotor. So the structure of a hydro-generator is a layered tree. In order to describe this structure clearly, all the cases are represented by trees. As the structure of hydro-generator, the case of hydro-generator should be divided into some subcases which represent components or parts, and some subcases may have their subcases. Every subcase has a set of features, so all the features are classified into different levels. Thus, the structure of a case is similar to the structure of a tree. Then all the cases are represented by a unified tree model. The tree model of the case is shown in Fig. 3.The blue text box represents the component or part of a hydro-generator, the orange circle represents a feature, and the green text box represents value. In the tree model, every blue text box is the root node of a subtree and all subtrees have the same structure. Every tree has a Class node which represents the class this component belongs to and a Description node which contains the features and its subcomponents are also taken as features. The case tree of a hydro-generator is composed of these subtrees. Due to this, the features form a layered model. The tree model in Fig. 3 hasHlayers, and every feature has a weight, such aswp,qn, wherenrepresents its parent component, andprepresents the layer where this feature is. The subcomponents also have weights. The weight is a number in the interval [0, 1], and its value depends on the structure of case tree. The sum of the weights of features and subcomponents which have a same parent node equals one. For example, components Shaft, Brush and features speed, runaway speed are all the rotor’s child nodes, so the sum of their weights equals one, that is,w2,11+⋯+w2,n1=1. All the features have values. The value node is in the same layer with its feature node and it has the same weight.The most important step in CBR is case retrieval, and its key problem is the similarity measurement. In order to solve the problems of missing values and unmatched features, the cost function and similarity function are proposed based on the edit distance, and the details are shown as follows.The edit distance can be utilized to measure the similarity, but it can only measure the structural difference, that is, how the structures of two case trees are different from each other. The edit distance between two case trees are the number of different nodes between them. But the nodes in hydro-generator case trees have semantic meanings, that is, different nodes represent different components or features. Thus, the changes of structure would bring the changes of semantic meanings, and this is the semantic difference. The structural difference is not enough, and the semantic difference should be measured. So the cost function is proposed.Case tree is also composed of node and edge, that is,Tree={V,G}, whereVandGdenote the sets of nodes and edges. But there is no direction or attribute on the edge in case tree, so the edge edit operations can be ignored. Then a case tree can be represented by a set of ordered nodes which isV=(v1,…,vm), wherev1,…,vmdenote the nodes.Definition 1Insertion operationInsert(V,v,j), whereVdenotes the case tree,vdenotes the node to be inserted, andjis the position where the new node to be inserted.Definition 2Deletion operationDelete(V,Tree(vj)), whereVdenotes the case tree,vjrepresents the node to be deleted, andTree(vj)denotes the subtree whose root node isvj. Ifvjis a leaf node, thenTree(vj)=vj; else,Tree(vj)contains nodevjand all its descendant nodes.Definition 3Substitution operationSubstitute(V,vj,v), whereVdenotes the case tree which is going to be edited,vjis the node to be substituted, andvis the new node.Compared with the edit distance, the cost function should consider two aspects. On one hand, the costs of different edit operations all equal one in the edit distance, but different edit operations may produce different costs, for example, as for a very important node, deleting it may bring more difference than substituting it. On the other hand, even if the same edit operation is executed on different nodes, the costs may be different. For example, substituting an important node would bring more difference than substituting an unimportant node. This kind of difference is called semantic difference because it depends on the semantic meanings of nodes and edit operations.As for the first aspect, the costs of insertion operation, deletion operation and substitution operation are respectively set asI,D,S, andI,D,S≥1. The features are assigned weights in case tree and editing different feature or value nodes may bring different costs, so the cost function should take the weights into account. Thus, a new edit operation of case tree which is weight modification is defined. The costs of edit operations can be represented by functiong(Tree1→Tree2), which is(1)g(Tree1→Tree2)={IinsertionD(n)deletionSsubstitution1weightI,D,S≥1,n∈N∗.The costs of deletion operation areD(n), wherendenotes the number of nodes to be deleted, andDare the costs of deleting one node. If the node to be deleted is a leaf node, thenn=1, else,nis the number of nodes of the subtree.As for the second aspect, different types of nodes are assigned different cost coefficients. All the nodes can be classified into four types, which are class nodes, keyword nodes, feature nodes, and value nodes, and feature nodes contain subcomponent nodes. The class nodes and keyword nodes are utilized to describe the product which this case belongs to, and they are not used in similarity calculation, so the cost coefficients of them are usually set as zero. In addition, because the feature and value nodes are layered, the level of these nodes should be considered. The cost coefficientKcan be defined as(2)K={h⋅ffeatureh⋅vvaluef,v≥1;h=H−t+1;t=1,2,…,H.The variablehrepresents the influence of level,His the total number of layers, andtis the layer where this feature node or value node is. Apparently, the cost coefficients of upper layer nodes are bigger than those of lower layer nodes.Definition 4Cost functionCost(Tree1→Tree2), supposing that the set of all the possible edit operation sequences which transformTree1intoTree2iseditset=⋃Ei, whereEiis thei-th edit operation sequence.Ei={Inserti,Deletei,Substitutei,Weighti,Ri}, whereInserti,Deletei,SubstituteiandWeightirespectively denote four types of edit operations ofEi, andRiis the sequence of these edit operations. Then the cost function is defined as(3)Cost(Tree1→Tree2)=mini[∑l=1LKl⋅gl⋅F(wl)]=mini[∑n1=1N1Kn1⋅I⋅wn1+∑n2=1N2Kn2⋅D⋅wn2+∑n3=1N3Kn3⋅S⋅wn3¯+∑n4=1N4Kn4⋅Δwn4]=mini[∑m1=1M1hm1⋅f⋅gm1⋅F(wm1)+∑m2=1M2hm2⋅v⋅gm2⋅F(wm2)]whereN1+N2+N3+N4=M1+M2=L, and(4)F(w)={winsertion or deletionw¯substitutionΔwweight,w¯=(wnew+wold)/2;Δw=|wnew−wold|.The cost function is the minimum of the total costs of edit operation sequences, and that sequence is called the shortest edit path. The cost function is expanded in three versions. In the first version,Kldenotes the cost coefficient of a node andgldenotes the costs of an edit operation.F(wl)denotes the calculation of weight and is defined in Eq. (4). Its value depends on the type of edit operations and the cost function is expanded according to different edit operations in the second version, soF(wl)is calculated according to edit operations. As for the insertion operation,wn1is the weight of the node which is inserted intoTree2; as for the deletion operation,wn2is the weight of the node which is deleted fromTree1. In substitution operation,wn3¯is the average weight of the node which is substituted, andwn3¯=(wn3old+wn3new)/2, wherewn3oldandwn3newrespectively denote the weights before and after the substitution. In weight modification,Δwn4are the increments of a node weight, andΔwn4=|wn4new−wn4old|. In the third version, the equation is expanded according to node types.According to the definition, there are four edit operations and two types of nodes in case tree, so there are eight different situations, such as inserting a value node, deleting a feature node, and so on. Now examples are given to illustrate the costs of different edit operations on different nodes. The class and keyword nodes are not shown in order to be simple, and this will not change the results.Examples of edit operations on the value node are shown in Fig. 4. There are three cases and they all have features “Pole height” and “Slots”, but the value nodes of feature Slots are different. In Case1, there is no value node for feature Slots, which means that it is a missing value, while in Case2, its value is 162, and in Case3, its value is 252. Thus, transformation(Tree1→Tree2)is inserting a value node of feature Slots,(Tree2→Tree1)is deleting the value node of Slots, and(Tree2→Tree3)is substituting the value node of feature Slots. Inserting or deleting a value node would not change the weights because the features are not changed. In order to illustrate the costs of weight modification, suppose that the feature weights change in three transformations. As for insertion,Cost(Tree1→Tree2)=h2vI⋅w22+∑i=12hi(f+v)|wi1−wi2|. Transformation(Tree2→Tree1)is deleting the value node, and the total costs areCost(Tree2→Tree1)=h2vD⋅w22+∑i=12hi(f+v)|wi1−wi2|.(Tree2→Tree3)is substituting the value node, and the costs areh2vS⋅w2¯+∑i=12hi(f+v)|wi2−wi3|, wherew2¯=(w22+w23)/2.The examples of edit operations on the feature node are shown in Fig. 5and the case trees use some features of electromagnetic design, where “FN” is the frequency and “HP” is the pole body height. There are two kinds of winding types which are lap winding and wave winding in electromagnetic design, and “TH” and “TVR” are features which are peculiar to lap winding and wave winding.There are three cases which all have features FN and HP, and they differ with each other in one feature. Transformation(Tree1→Tree2)is substituting feature TH with feature TVR, transformation(Tree1→Tree3)is deleting feature TH, and transformation(Tree3→Tree2)is inserting feature TVR. As for substitution, the features change, so the values would change and the weights may change, then the costs areCost(Tree1→Tree2)=h3(f+v)S⋅w3¯+∑i=13hi(f+v)⋅|wi1−wi2|. As for transformation(Tree1→Tree3), feature weights change due to the change of feature number, and the costs of weight modification areh3(f+v)⋅w31+∑i=12hi(f+v)⋅|wi1−wi3|, then the total costs areCost(Tree1→Tree3)=h3(f+v)D⋅w31+h3(f+v)⋅w31+∑i=12hi(f+v)⋅|wi1−wi3|. As for(Tree3→Tree2), the weights also change, and costs of weight modification areh3(f+v)⋅w32+∑i=12hi(f+v)⋅|wi2−wi3|, then the total costs areCost(Tree3→Tree2)=h3(f+v)I⋅w32+h3(f+v)⋅w32+∑i=12hi(f+v)⋅|wi2−wi3|.To be utilized as a similarity measurement, cost function should be a metric, so it should satisfy three conditions which are non-negativity, symmetry and triangle inequality. Because the costs of edit operations and cost coefficients can be set to any possible values, the cost function would not satisfy these three conditions generally. But if the values of these variables are restricted, it can still be a metric.Theorem 1The cost function satisfies the symmetry ifI=D.ProofAll the possible edit operation sequences in transformation(Tree1→Tree2)are composed of the following five simple transformations.(1) Supposing transformation(Tree1→Tree2)is inserting a feature node. The weights of features inTree1andTree2are{w11,w21,…,wn1}and{w12,w22,…,wn2,wn+12}, where the (n+1)-th node is the inserted one. Being similar to transformation(Tree3→Tree2)in Fig. 5, the total costs areCost(Tree1→Tree2)=Kn+1I⋅wn+12+Kn+1⋅wn+12+∑j=1nKj|wj1−wj2|, whereKm=hm(f+v),m=1,…,n+1; as for transformation(Tree2→Tree1), its shortest edit path is deleting the (n+1)-th feature node, andCost(Tree2→Tree1)=Kn+1⋅D⋅wn+12+Kn+1⋅wn+12+∑j=1nKj⋅|wj1−wj2|. Apparently, ifI=D,Cost(Tree1→Tree2)=Cost(Tree2→Tree1), so this transformation satisfies the symmetry.(2) Supposing transformation(Tree1→Tree2)is inserting a value node, and the value node is thei-th feature’s value. The weights would not change in this transformation. Being similar to(Tree1→Tree2)in Fig. 4, the costs areCost(Tree1→Tree2)=hivI⋅wi2; as for(Tree2→Tree1), the costs areCost(Tree2→Tree1)=hivD⋅wi2. Thus, it satisfies the symmetry ifI=D.(3) Supposing that transformation(Tree1→Tree2)is deleting a node. Being similar to (1) and (2), this transformation satisfies the symmetry ifI=D.(4) Supposing transformation(Tree1→Tree2)is substituting thei-th node. If it is a substitution of a feature node, then weights may change. The weights inTree1andTree2are{w11,…,wi1,…,wn1}and{w12,…,wi2,…,wn2}, then the total costs areCost(Tree1→Tree2)=Ki⋅S⋅wi¯+∑j=1nKj⋅|wj1−wj2|, wherewi¯=(wi1+wi2)/2andKm=hm(f+v); as for transformation(Tree2→Tree1), its shortest edit path is also substituting thei-th node, andCost(Tree2→Tree1)=Ki⋅S⋅wi¯+∑j=1nKj⋅|wj1−wj2|. Apparently, it satisfies the symmetry. If it is a substitution of a value node, thenKm=hmvin two transformations, so it satisfies the symmetry.(5) Supposing transformation(Tree1→Tree2)is only modifying the weights. The node weights inTree1andTree2are{w11,w21,…,wn1}and{w12,w22,…,wn2}. So the costs areCost(Tree1→Tree2)=∑j=1nKj⋅|wj1−wj2|, whereKj=hj(f+v); as for transformation(Tree2→Tree1), its shortest edit path is also modifying the weights, soCost(Tree2→Tree1)=∑j=1nKj⋅|wj1−wj2|. Apparently, this transformation satisfies the symmetry.The complicated transformation is a linear combination of the above five simple transformations, so the cost function satisfies the symmetry ifI=D.□In order to be a metric, the cost function should also satisfy the triangle inequality, which isCost(Tree1→Tree2)≤Cost(Tree1→Tree3)+Cost(Tree3→Tree2). Suppose that there is only one different leaf feature node between two case trees, and there are massive edit paths, but only two of them are the possible shortest paths. One is substituting the node, the other is deleting the node fromTree1and inserting it intoTree3. The two edit paths are shown in Fig. 5. For the first edit path, suppose that the different node is thei-th node, then the total costs of substitution areCost(Tree1→Tree2)=KiS⋅wi¯+∑j=1nKj|wj1−wj2|, wherewi¯=(wi1+wi2)/2andKm=hm(f+v),m=1,…,n. As for the second edit path, by deleting thei-th value node fromTree1,Tree3can be obtained and the costs areCost(Tree1→Tree3)=KiD⋅wi1+Ki⋅wi1+∑j=1,j≠inKj|wj1−wj3|; then by inserting thei-th value node intoTree3,Tree2can be obtained. Being similar to insertion in Fig. 5, the costs areCost(Tree3→Tree2)=KiI⋅wi2+Ki⋅wi2+∑j=1,j≠inKj|wj3−wj2|. Then the total costs are(5)Cost(Tree1→Tree3)+Cost(Tree3→Tree2)=Ki(D⋅wi1+I⋅wi2)+Ki(wi1+wi2)+∑j=1,j≠inKj(|wj1−wj3|+|wj3−wj2|).Letwi3=0, thenKi(wi1+wi2)+∑j=1,j≠inKj(|wj1−wj3|+|wj3−wj2|)=∑j=1nKj(|wj1−wj3|+|wj3−wj2|). According to the property of the inequality,∑j=1nKj(|wj1−wj3|+|wj3−wj2|)≥∑j=1nKj|wj1−wj2|, and becauseI=D,(6)Cost(Tree1→Tree3)+Cost(Tree3→Tree2)=KiI⋅(wi1+wi2)+∑j=1nKj(|wj1−wj3|+|wj3−wj2|)≥KiI⋅(wi1+wi2)+∑j=1nKj|wj1−wj2|.LetI+D≥S, that is,I≥(S/2), then(7)Cost(Tree1→Tree3)+Cost(Tree3→Tree2)≥KiI⋅(wi1+wi2)+∑j=1nKj|wj1−wj2|≥Ki⋅S2⋅(wi1+wi2)+∑j=1nKj|wj1−wj2|=KiS⋅wi1+wi22+∑j=1nKj|wj1−wj2|=Cost(Tree1→Tree2).According to Eq. (7), the cost function of a leaf feature node satisfies the triangle inequality ifI+D≥SandI=D. As for a leaf value node, the weights would not change, and the cost function also satisfies the triangle inequality ifI+D≥SandI=D. As for a non-leaf node, the deletion operation of a non-leaf node corresponds to the deletion of many leaf nodes, so the costs of substitution are apparently less than the costs of deletion and insertion operations. Thus, the cost function of a non-leaf node satisfies the triangle inequality. The costs of a complicated transformation are a linear combination of the costs of editing leaf nodes and non-leaf nodes, so the following theorem can be concluded.Theorem 2The cost function satisfies the triangle inequality ifI+D≥SandI=D.According to the definition of the cost function, it is nonnegative, and apparently,Cost(Tree1→Tree2)=0if and only ifTree1=Tree2. So the cost function satisfies the non-negativity, and the following deduction can be obtained.Deduction: The cost function is a metric ifI+D≥SandI=D.As a metric, the cost function can measure the costs of transforming two case trees into each other.The cost function can measure the semantic difference of transformation, but it cannot be utilized to measure the similarity directly, and it should be improved. On one hand, the increments of feature values should be considered. For example, suppose that there are two transformations of hydro-generator cases which are(Tree1→Tree2)and(Tree1→Tree3). Three case trees only differ in one value node, such as the value of feature damper bar number, and the values in three case trees are 2, 4 and 8. The edit sequences are all substituting this value node, so the edit distances all equal one, and the costs are allhvS⋅w. But in the first transformation, the value changes from 2 to 4, while it changes from 2 to 8 in the second transformation, so the similarity between two cases in the first transformation is bigger, while the edit distance and cost function cannot distinguish them, that is why they cannot be utilized to measure the similarity. On the other hand, the edit distance takes no account of the number of tree nodes. For example, suppose that there are two transformations(Tree1→Tree2)and(Tree3→Tree4), and the edit distances of two transformations are both 2, which means thatTree1andTree2differ in two nodes, and so do the other two case trees. While the average node number ofTree1andTree2is 5 the average node number ofTree3andTree4is 50. Apparently, the similarity between two cases in the second transformation is bigger. In order to overcome these drawbacks, the similarity function is defined based on the cost function.As for the first aspect, the costs brought by the increments of feature values should be calculated according to value types. The feature values can be classified into four types, which are simple numerical value, interval value, fuzzy value [7] and semantic value. The costs are defined asvalue_cost(value1,value2), and their numerical range is [0, 1]. The value costs of four value types are defined as follows.(1) The value cost of a simple numerical value. If the transformation is the substitution of a value node whose type is a simple numerical value, then the value cost is defined as(8)value_cost(value1,value2)=|value1−value2|max−minwhere max and min are the maximum and minimum of this feature value.(2) The value cost of an interval value. Supposing the type of the substituted value node is an interval value and the values in two case trees are[a1,b1]and[a2,b2], then the value cost is(9)value_cost(value1,value2)=|a2−a1|+|b2−b1||amax−amin|+|bmax−bmin|whereamaxandaminare the maximum and minimum of the lower bound, andbmax,bmindenote the maximum and minimum of the upper bound.(3) The value cost of a fuzzy value. Generally, all feature values of cases in the case base are certain values, while the feature values of the query case which is from the design requirements may be fuzzy values. The design requirements are input as query conditions into the system and some conditions may be fuzzy. Take the hydro-generator for example, the query condition rated speed may be a fuzzy value, such as high, middle and low. The triangular membership function in fuzzy mathematics can be utilized to deal with the fuzzy value. Supposing that case treeTree1is the case in the case base, and one of its feature values isvalue1, whileTree2is a query case, and its corresponding feature value isvalue2which is a fuzzy value. The steps are: (i) determining the universe and fuzzy variables; (ii) constructing the membership functions; (iii) calculating the membership degree of each fuzzy variable and checking that whether the fuzzy variable whose membership degree is the maximum equalsvalue2, then(10)value_cost(value1,value2)={0if the fuzzy value of value1is value21else.There is another situation which is similar to this. Thevalue1is a numerical value, andvalue2is an inequality which is composed of a numerical value and symbolR={>,<,≥,≤}. The value cost is(11)value_cost(value1,value2)={0if value1satisfies the inequality1else.(4) The value cost of semantic value. There are some semantic values in cases, such as color, material, and so on. The most popular method of dealing with semantic values is setting the value costs between different values by the experts according to their knowledge. Take the material in the design for example, the value costs are shown in Table 1.The value cost is calculated only when a substitution operation is executed on the same feature’s value nodes, in other situations, it is set as one in the calculation of similarity, that is,value_cost=1.As for the second aspect, the scale cost is defined by considering the number of editable tree nodes.(12)scale_cost(Tree1→Tree2)=ED(Tree1→Tree2)/N,N=(N1+N2)/2whereED(Tree1→Tree2)denotes the edit distance between two case trees andNis the average number of editable tree nodes.N1andN2are numbers of editable nodes in two case trees.The difference between two case trees can be measured accurately by combining the value cost and scale cost with the cost function, so the similarity function is defined based on them.Definition 5Similarity functionSim(Tree1→Tree2), supposing that the shortest edit path which is determined by the cost function isE={Insert, Delete, Substitute, Weight,R}, where Insert, Delete, Substitute and Weight denote four edit operations, andRdenotes the sequence of the edit operations. The similarity between two case trees is defined as(13)Sim(Tree1→Tree2)=exp[−α(scale_cost+∑l=1LKl⋅gl⋅F(wl)⋅value_cost)]=exp[−α(scale_cost+∑n1=1N1Kn1I⋅wn1+∑n2=1N2Kn2D⋅wn2+∑n3=1N3Kn3S⋅wn3¯⋅value_cost+∑n4=1N4Kn4⋅Δwn4)]=exp[−α(scale_cost+∑m1=1M1hm1⋅f⋅gm1⋅F(wm1)+∑m2=1M2hm2⋅v⋅gm2⋅F(wm2)⋅value_cost)]wherevalue_costandscale_costcan be calculated according to Eqs. (8)–(12), and other symbols have the same meanings as those in Eq. (3). The value cost is calculated only when the substitution is executed on the same feature’s value nodes, and it is set as one in other situations, so only some terms have value cost in the second and third versions of the equation. The coefficientαis a constant and it is utilized to adjust the similarity value. The functionexp(−x)is utilized to map the costs to the range [0, 1], and becauseexp(−x)is a monotone decreasing function, the smaller the costs are, the bigger the similarity is.Now three cases in Fig. 4 are taken as the examples to show the similarity calculation. Three cases in Fig. 4 differ with each other in feature Slots value. The editable node number inTree1isN1=3, andN2=4,N3=4. Transformation(Tree1→Tree2)is inserting a value node, so its edit distance isED=1, andN=(N1+N2)/2=3.5. The costs areh2vI⋅w22+∑i=12hi(f+v)|wi1−wi2|, and its value cost is one because it is not a substitution of a value node. Supposing thatI=D=S=1,f=2,v=1,α=0.1,h=H−t+1=1−1+1=1, and the weights all equal 0.5, then the similarity isSim(Tree2→Tree1)=exp[−0.1×(1/3.5+1×1×1×0.5)]=0.9244. Transformation(Tree2→Tree1)is deleting the same value node, and becauseI=D, the similarity is same. Transformation(Tree2→Tree3)is substituting the value node, so its edit distance also equals one.N=(N1+N3)/2=4, and its costs areh2vS⋅w2¯⋅value_cost+∑i=12hi(f+v)|wi3−wi2|. Because it is a substitution of the same feature’s value nodes, the value cost should be calculated. The maximum and minimum values of feature Slots are 468 and 144, sovalue_cost=(252−162)/(468−144)=0.278. Then the similarity isSim(Tree1→Tree3)=exp[−0.1×(1/4+1×1×1×0.5×0.278)]=0.9618.Now an example is presented in Fig. 6to illustrate the algorithm for computing similarity.There are two case trees in Fig. 6 and they both have features FN, HP and Slots, which are matched features. Features TVR and BVR in Case1 and feature TH in Case2 do not match each other, so they are unmatched features. Moreover, feature HP in Case1 has a missing value, so this example is representative. The algorithm of similarity calculation of transformation(Tree1→Tree2)is:(i)Calculating the costs of matched features, that is, features FN, HP and Slots in two case trees. Only edit operations on a value node which are shown in Fig. 4 are required in this step. As for features FN and Slots, substituting their value nodes if values are different in two cases and only the values of Slots are different. Supposing the value cost of the value node isV3, then the costs areh3vS⋅w3¯⋅V3; as for feature HP, inserting the value node and the costs areh2vI⋅w22, thus, the total costs areh3vS⋅w3¯⋅V3+h2vI⋅w22.Calculating the costs of unmatched features. There are two types of unmatched features. One situation is that two cases both have unmatched features but their names are different; the other situation is that one case has more features than the other. As is shown in Fig. 6, Case1 has two unmatched features while Case2 has only one. Edit operations on feature nodes which are shown in Fig. 5 are required in this step. According to Theorem 2, the costs of substituting a feature node are less than deleting and inserting it, so as for the first situation, substitution should be operated, and as for the second situation, deletions are utilized. Another problem is which feature satisfies the first situation. For example, features TVR and BVR in Case1 can both be substituted by feature TH. The costs of all possible substitutions should be calculated and the minimum one is selected. Supposing that the costs of substituting feature TVR with feature TH are the minimum, then the edit operations are substituting feature TVR with feature TH and deleting feature BVR, and the costs areh4(f+v)Sw4¯+h5(f+v)D⋅w51, wherew4¯=(w41+w42)/2. As for the value cost, it is calculated only when a substitution is executed on the same feature’s value nodes, so there is no value cost in substituting feature TVR’s value node with feature TH’s value node.Calculating the costs of weight modification. The weights contain feature weights and value weights, as for substitution, the weight modification isΔw, and as for deletion, if it is a deletion on value node, the weights do not change; if it is a deletion on a feature node, the weight modification is the weight of the deleted node. For example, as for feature node BVR, its weight modification isw51. So the costs of weight modification areh5(f+v)w51+∑i=14hi(f+v)Δwi.Calculating the scale cost. The calculation of scale cost requires edit distance and average editable node number. The editable node numbers areN1=9andN2=8, thus,N=8.5. The edit distance is 6, and thenscale_cost=6/8.5=0.706according to Eq. (12).Calculating the similarity. The similarity can be calculated according to Eq. (13) by using the results of the above steps. The constantαis usually set to a value in [0, 1] to adjust the similarity value.The similarity between two cases with missing values and unmatched features can be calculated according to the above algorithm.

@&#CONCLUSIONS@&#
