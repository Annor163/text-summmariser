@&#MAIN-TITLE@&#
Robust point matching method for multimodal retinal image registration

@&#HIGHLIGHTS@&#
An improved registration framework for multimodal retinal images.Using SURF–PIIFD approach to detect and describe local features.A single Gaussian robust point matching model for outliers removing.

@&#KEYPHRASES@&#
Image registration,Multimodal retinal image,Robust point matching,PIIFD,SURF,

@&#ABSTRACT@&#
In this paper, motivated by the problem of multimodal retinal image registration, we introduce and improve the robust registration framework based on partial intensity invariant feature descriptor (PIIFD), then present a registration framework based on speed up robust feature (SURF) detector, PIIFD and robust point matching, called SURF–PIIFD–RPM. Existing retinal image registration algorithms are unadaptable to any case, such as complex multimodal images, poor quality, and nonvascular images. Harris-PIIFD framework usually fails in correctly aligning color retinal images with other modalities when faced large content changes. Our proposed registration framework mainly solves the problem robustly. Firstly, SURF detector is useful to extract more repeatable and scale-invariant interest points than Harris. Secondly, a single Gaussian robust point matching model is based on the kernel method of reproducing kernel Hilbert space to estimate mapping function in the presence of outliers. Most importantly, our improved registration framework performs well even when confronted a large number of outliers in the initial correspondence set. Finally, multiple experiments on our 142 multimodal retinal image pairs demonstrate that our SURF–PIIFD–RPM outperforms existing algorithms, and it is quite robust to outliers.

@&#INTRODUCTION@&#
Image registration is an important element in the fields of computer vision, pattern recognition, and medical image analysis. In this problem, two or more images are aligned together in a same spatial axis to receive a comprehensive understanding. In this paper, we focus on digital retinal images which are widely used to diagnose varieties of diseases, such as diabetic retinopathy, glaucoma, and age-related macular degeneration [1,2]. Then, using the computer-assisted retinal image registration technique is helpful to assist doctors to diagnose diseases and make treatment planning. There are four main retinal registration applications: mono-modal registration, multimodal registration, temporal registration, and multi-images fusion. Mono-modal and multimodal retinal images are captured by the same sensor (e.g. fundus camera) and different sensors (e.g. red-free and fluorescein angiography) at the same time, respectively, while temporal retinal images are captured at different times. These applications align images to create a wider view, and integrated data information.Recently, many related registration approaches have been proposed for retinal image registration, can be classified into three classes: area-based, feature-based, and hybrid approaches, typically.The area-based approaches are widely used in image registration, they mainly use a certain similarity metric, such as mutual information (MI) [3–5], cross correlation (CC) [6], entropy correlation coefficient (ECC) [7], and phase correlation [8,9], to match the intensity difference of image pairs. In order to minimize the measures of match, some optimizations are applied, such as simulated annealing [10] and genetic algorithms. However, there are several shortcomings. (1) The huge searching space depends upon the complication of the transformation models. (2) The metric of similarity is always disturbed by nonoverlapping areas when faced with too small overlaps. (3) The optimization often meets local minima when handling with high order transformations, and it may have huge searching space to fall in a computational bottleneck. What is more, the performance of area-based approaches degrades when confronted with illumination, content, and texture changes.Feature-based approaches extract several features, such as the bifurcations of retinal vasculature, fovea, optic disc, and corners, whose number is much less than the number of pixels, and they are more appropriate for retinal image registration. Feature extraction and transformation estimation are two key components of these approaches. Typically, bifurcations [11–15], fovea, and optic disc [16,17] are common features in retinal image registration. Bifurcations are invariant feature to intensity, scale, rotation, and illumination variations, and dependent of vasculature detection [11]. The vascular tree is detected and bifurcations are labeled with surrounding vessel orientations. Then angle-based invariant is used to give a probability for every matching bifurcation pairs. However, it is difficult to extract bifurcations, fovea, and optic disc in poor quality, and unhealthy retinal images [18]. Thus, feature-based approaches based on an assumption that certain features are easy to be extracted. The bifurcations are used as landmarks, but some bifurcations with more than one correspondence, then the transformation can be estimated by a hierarchical strategy which makes the registration approach robust to unmatchable features and mismatches between image pairs [12]. Local features, such as Harris corner [19], scale invariant feature transform (SIFT) [20–22], speed up robust feature (SURF) [23], are also widely used general features and easier to extract than bifurcations, however these feature descriptors are not appropriate for multimodal registration. More precisely, SIFT and SURF descriptors are designed for mono-modal retinal image registration [24]. Shape context (SC) [25] only uses the locations of feature points to describe point set in log-polar histogram bins, it is rotation invariant, scale invariant, and affine invariant, but it is highly sensitive to outliers.Hybrid approaches integrate area-based with feature-based approaches to improve the registration performance. For instance, [7] combines mutual information technique and bifurcations to register retinal images. Chen et al. [18] presented a partial intensity invariant feature descriptor (PIIFD) for multimodal registration, even for poor quality images. It is a hybrid area-feature descriptor due to the surrounding area of each corner point is used to extract structural outline. However, the Harris-PIIFD registration framework cannot detect more repeatable and scale invariant key points, and its sensitive to large amount of mismatches. Ghassabi et al. [26] analyzed the problems related to SIFT, and proposed an uniform and robust scale invariant transform feature extraction (UR-SIFT) to instead Harris detector, and obtained an efficient UR-SIFT-PIIFD registration approach.From the angle of point matching, iterative closest point (ICP) [27] is widely used to register retinal images. Stewart et al. [28] proposed dual-bootstrap ICP. There are three steps in each bootstrap region, such as refining the transformation estimation, expanding the bootstrap region, and using higher order transformation model. Yang et al. [29] used the dual-bootstrap ICP algorithm to refine each estimate, and proposed the generalized dual-bootstrap ICP (GDB-ICP) algorithm. Edge-driven dual-bootstrap ICP (ED-DB-ICP) [30] combines SIFT key points and vascular features to register multimodal retinal images. All of those approaches only need one initial correct match to run iterative registering process successfully. However, their performance degrades when faced with poor quality images. Deng et al. [31] introduce graph matching method for retinal image registration, and they also combined graph-based matching and ICP to generate a registration framework called GM-ICP. The methods require sufficient feature points to obtain efficient performance. Although there are many approaches in retinal image registration, several challenges still exist in retinal image registration. Firstly, how to extract reliable, repeatable, and distinctive features in different modal retinal images. Secondly, how to find correspondences between multimodal retinal pairs, i.e. how to design robust descriptor for matching control point candidates. Thirdly, how to remove outliers, due to the initial matching correspondences are contaminated by outliers which highly impact the registration accuracy. As mentioned earlier, the Harris-PIIFD is designed for multimodal images, and poor quality images. It can register multimodal images successfully, but the fusion images contain some degree of dislocation and ghost, two main corresponding problems are features repeatable and outlier removal, due to the limitation of the Harris corner detector, and outlier rejection strategy, respectively.In this paper, we improve the Harris-PIIFD registration framework using SURF key points to solve the features repeatability, and propose a novel robust point matching algorithm to reject outliers and estimate transformation robustly. The improved SURF–PIIFD is useful to extract repeatable, rotation, scale invariant, intensity and affine partial invariant local features. In outliers rejection process, we assume that the inliers satisfy a single Gaussian distribution, then we search for the optimal mapping function in reproduced kernel Hilbert space with a Gaussian radial basis kernel function. A novel low-rank Gram matrix approximation is proposed to construct control points to speed up our algorithm. Thus, the described robust automatic multimodal retinal image registration framework named SURF–PIIFD–RPM.The rest of the paper is organized as follows: In Section 2, we introduce the improved retinal image registration framework. In Section 3, we present the improved SURF–PIIFD of feature descriptor. In Section 4, we devote the proposed robust point matching for outliers removing and transformation estimation. In Section 5, we describe the experimental settings and report the results. In Section 6, we give a discussion and conclusion.In this paper, we concentrate on the hybrid area-feature based registration method for multimodal retinal images. Our improved multimodal retinal image registration framework, as showed in Fig. 1, based on SURF–PIIFD and RPM contains the following four main parts:(1)Locate local feature points by SURF detector.Extract feature descriptor based on PIIFD.Feature matching and mismatches removing using RPM.Estimate the transformation using weighted least-squares.Note that image preprocessing is applied before extracting local feature descriptor, since the SURF detector can detect key points based on color images directly. Then we select green component from the input RGB image format, and scale the intensities of them to the full eight bits intensity range [0, 255]. In order to reduce the sensitivity of algorithm parameters and the different scale, the Harris-PIIFD framework suggests zooming out the input image to a fixed size. In our improved framework, we use the original image size for lossless intensities and repeatable key points.Following the matching algorithm of the Harris-PIIFD, we use the bilateral matching based on the unilateral best-bin-first (BBF) method [32]. In this way, we can obtain more accurate matching point pairs, though losing some pairs. In terms of our following robust point matching method, the threshold of the nearest neighbor criterion is set to 0.96 in this paper for getting more candidate matching pairs. Note that the larger the threshold, the more outliers may be obtained. Another important operation is to tune corresponding control point locations using cross correlation (e.g. function ‘cpcorr’ in Matlab), then refined matching point locations are used to estimate transformation parameters.Transformation models, such as rigid, affine, and second order polynomial (quadratic), are adaptively chosen to register image pairs according to the matches number. The reason why we use the quadratic transformation is that the surface of retina is approximately spherical [12]. Nonetheless, the difference between [18] and ours is that the former uses a hierarchical style from linear conformal to affine or second order polynomial transformation iteratively, our framework registers images only depending on the initial matching points. Thus our improved framework can estimate transformation parameters quicker than the former one [18].The detailed improvements of multimodal retinal image registration framework are explored in the following sections.The Harris corner detector is the most widely used local feature detector in image processing. It is based on the second comment matrix M. Given a point x=(x, y) in an image I, it is considered as a corner point if and only if det(Mx,y)−κtr2(Mx,y)>0, where κ is determined empirically and dependent on images to analyze [33]. Note that its values usually adopted range in [0.04, 0.16]. However, the Harris corner is not scale invariant, and it has low feature repeatability.Motivated by the problem of local feature detectors, this paper introduces and analyzes a widely used robust scale invariant feature, called speed up robust feature (SURF) [23]. In the feature-based retinal registration framework, the performance of registration approaches may be enhanced by highly repeatable local feature detectors. Note that local feature detectors are only providing locations of key points for our registration framework. In this paper, we consider that the repeatability is more important than the uniform distribution of key points in multimodal retinal images.Interest point detector of SURF is based on integral images [34], Hessian matrix [35], and scale space theory. The core idea of SURF is that the interest point candidates can be detected by the maximum determinant of the Hessian matrix. Given an arbitrary point x=(x, y) in an image I, we can give a 2×2 Hessian matrix HL(x, σ) in x at scale σ as follows:(1)HL(x,σ)=Lxxx,σLxyx,σLxyx,σLyyx,σwhere L(x, σ)=G(x, σ)*I(x), * denotes the convolution operation between image I and the Gaussian kernel, and the Gaussian function G(x, σ)=1/2πσ2exp(−(x2+y2)/2σ2). Lxx, Lxy, and Lyydenote the convolution of the Gaussian second order derivatives with the image I in point x.From the scale space, a new scale space is constructed using the difference of Gaussian (DoG) function as used in [20–22]:(2)D(x,σ)=L(x,kσ)−L(x,σ)where k denotes the linear scale difference between each image in each octave.However, SURF approximates the determinant of the Hessian matrix HL(x, σ) by using box filters, and SURF constructs the approximation D(x, σ) to det(HL(x, σ)) directly. More precisely,(3)det(Happrox)=DxxDyy−(wDxy)2where the relative weightwof the filter responses is used to balance the expression of the determinant of the Hessian matrix. Bay et al. [23] suggestw=0.9.Using the integral image, SURF approximates the different levels of scale space D(x, σ) by adjusting the size of the box filters instead of the original image as used in SIFT. Finally, interest points can be found by non-maximum suppression in a 3×3×3 neighborhood around each sample point. Following the SIFT, the second-order Taylor expansion around the key point is used to improve the locations of the key points. More details of SURF detector are well studied in [23,36].To our knowledge, PIIFD is the one of the best descriptor which is independent of vascular tree for multimodal retinal image registration. It is based on two assumptions, (1) the similar anatomical structure regions of one image would consist of similar outlines in the corresponding regions of another image, and (2) the gradient orientations at corresponding control point locations would point to the same or opposite directions in multimodal retinal image pairs.Compared with the most popular feature descriptor SIFT, there are several differences, (1) PIIFD uses a continuous averaging squared gradients to calculate the main orientation instead of the discrete orientation histogram, due to the former can improve the accuracy and computational efficiency. (2) PIIFD uses the fixed neighborhood size instead of selecting by the scale of the control point automatically, since the scale changes of retinal images is slight. (3) PIIFD convert the orientation histogram with 16 bins (0°, 22.5°, …, 337.5°) to a degraded eight bins orientation histogram (0°, 22.5°, …, 157.5°) by computing the sum of the opposite orientations. (4) A linear combination of two sub-descriptors to solve the opposite main orientations of the corresponding control points. The 4×4 orientation histograms with eight bins are defined as follows:(4)P=P11P12P13P14P21P22P23P24P31P32P33P34P41P42P43P44where Pijdenotes an orientation histogram with 8 bins, and the combined descriptor can be denoted as follows:(5)Dp=P1+rot(P1,π)P2+rot(P2,π)aP3−rot(P3,π)aP4−rot(P4,π)where P1 denotes the first row of orientation histograms matrix, and similarly for P2, P3, and P4. rot(P, π) rotates the orientation histogram matrix 180°. α is used to tune the proportion of magnitude in the local descriptor, and it can be defined as follows:(6)a=max(Pi+rot(Pi,π))max(Pi−rot(Pi,π)),i=3,4Therefore, the dimension of PIIFD is 4×4×8=128, and it is normalized to a unit length finally. Note that PIIFD is rotation invariant, and partially invariant to intensity, affine, and point view change.In this section, we firstly construct a single Gaussian model to eliminate outliers, then we estimate the transformation parameters using weighted regularization least-squares. Note that the outliers exist in both point sets, i.e. the input data is an initial correspondence set which is contaminated by outliers. More precisely, we assume that the inliers satisfy Gaussian distribution. In multimodal retinal image registration, initial correspondences are obtained by matching SURF–PIIFDs using bilateral BBF algorithm, and incorrect matches (i.e. outliers) may be falsely charged as inliers by some outlier removing methods sometimes. Thus, our robust point matching is proposed to solve this problem.Given two point sets: (1) the moving point set XM×D=(x1, …, xM)T, and (2) the fixed point set YN×D=(y1, …, yN)T. Following the idea of robust point matching method [37–39], we use a slightly simpler form to estimate the mapping function f with the given two point sets.(7)E(f,σ2)=−1MN∑i=1N∑j=1MηN(yi−f(xj)|0,σ2I)where we assume that yi−f(xj) satisfies a single Gaussian distribution, and η denotes the corresponding relation between matching points. In this paper, we have got the correspondence setC={(xl,yl)}l=1Lafter the initial matching, note that there is some sort of proportion outliers in point set C. Then we can rewrite the objective function as(8)E(f,σ2)=−2L∑l=1LN(yl−f(xl)|0,σ2I)where L≤M, N is the number of correspondences.Here we can introduce a special feature space, i.e. reproducing kernel Hilbert space (RKHS) [40], and then searching the functional form of the mapping model f using calculus of variation [41]. In RKHS, the moving point set and the fixed one satisfyX∈ℝDandY∈ℝD, respectively. Then we define an RKHS H with a positive definite kernel function f. In this paper, we use the well-known Gaussian radial basis kernel: k(xi, xj)=exp(−β∥xi−xj∥2), where β is a constant, which controls the speed of moving points. Thus we can define the kernel matrix (Gram matrix) K:(9)K=k(x1,x1)…k(x1,xL)⋮⋱⋮k(xL,x1)⋯k(xL,xL)According to the representation theorem [42], the solution of the Tikhonov regularization [43] risk minimization (ɛ=minf∈HE(f,σ2)+λ/2∥f∥K2, where λ>0 denotes a trade-off parameter) can be written as:f*(·)=∑lLhlK(xl,·)for somehl∈ℝL. Thus our final objective function can be rewritten in the following form:(10)E˜(H,σ2)=−1L2πσ2D/2exp−Y−KH22σ2+λ2trHTKHwhere tr(·) denotes the trace, H=(h1, …, hL)Tis an coefficient matrix of size L×D, D=2 denotes the dimension of point set in 2D retinal image registration.The matrix-valued kernel [44] plays a major role in the regularization theory, it provides an easy way to choose a preferable RKHS. However, in this paper, the computational complexity of the robust point matching method is O(M3), hopefully, low-rank kernel matrix approximation [45] can yield a large increase in speed with little loss in accuracy. Low-rank kernel matrix approximationKˆis the closest τ-rank matrix approximation to K, meanwhile satisfies both L2 and Frobenius norms.Using eigenvalue decomposition of K, the approximation matrix can be written asKˆ=QΛQT, where Λ is a diagonal matrix of size τ×τ with τ largest eigenvalues, and Q is an L×τ matrix with the corresponding eigenvectors. The object function of our method therefore can be rewritten as:(11)E˜(H˜,σ2)=−1L(2πσ2)D/2exp−Y−UH˜22σ2+λ2tr(H˜TKˆH˜)where UL×τ=QΛ, parameter matrixH˜of size τ×D instead of the original matrix H.In this paper, the aforementioned cost function is convex in the neighborhood of the optimal position and, most importantly, always differentiable. Thus, the numerical optimization problem can be solved by employing some gradient-based optimization methods, such as quasi-Newton method [46]. The derivative of the final objective function with respect to the coefficient matrixH˜is given by:(12)∂E˜∂H˜=KˆLσ2(2πσ2)D/2(UH˜−Y)∘expdiag(UH˜−Y)(UH˜−Y)T2σ2⊗1+λUH˜where 1 is an 1×D row vector of all ones. ° denotes the Hadamard product, ⊗ denotes the tensor product.The iteration process is using determination annealing technique which is a heuristic method to escape from the trap of local minima. Following the annealing framework [38], we set a big initial value to bandwidthσ2=(1/MND)∑i=1N∑j=1M∥(yi−xj)∥2, then we reduce it according to σ2=α×σ2, where α=0.75 in our whole experiments. The penalty parameter λ=0.1 is used to trade off the regularization term and the empirical risk, which solves the ill-posed problem in point matching. The parameter of kernel function β=5.0 is used to control the strength of interaction between points. Small values of β produce locally smooth transformation, while large values correspond to nearly pure translation transformation. The parameter τ is used to control the complexity of the method. In practice, τ≪M and we set τ=15, the computational complexity of our method will be reduced to O(N) approximately when the number of points is relatively large and well clustered. In other words, it enables our method to be applied to larger point set. Most importantly, all input point sets are normalized as distributions with zero mean and unit variance for linear rescaling of the initial correspondences.Consider two images of the retinal surface, we used weighted least squares to estimate the parameters of geometric transformation, such as rigid, affine, and quadratic.(13)Θ˜=argminΘ∑l=1Lwxl,ylyl−φxl,Θ22wherew(xl,yl)=1,ifexp(−(yl−f(xl))2/(2σ2))≥ζ0,otherwise, more precisely,w(xl,yl)=1denotes the inliers, whilew(xl,yl)=0denotes the outliers. In this paper, we set ζ=0.9 to reject outliers.Following the weighted least squares model, the rigid transformation between two retinal images relates X and X′ as follows:(14)X′=sRX+t,orφ(xl,Θ)=sRxl+t,s.t.RTR=I,det(R)=1where Θ={s, R, t}, R2×2 is a rotation matrix, t2×1 is a translation vector, and s is a scaling parameter. Similarly, the affine transformation is defined as φ(xl, Θ)=Bxl+t, where B2×2 denotes the affine matrix. In the retinal image registration, second order polynomial transformation is also well used, because of the surface of the retina is almost spherical [12], which is defined asφ(xl,Θ)=Pxl2+Qxl+t. All of these three transformations can be rewritten in matrix form:(15)φX,Θ=ΘXwhereX=1,x1,x2,x1x2,x12,x22T,Θ=θ11θ12θ13000θ21−θ13θ12000,θ11θ12θ13000θ21θ22θ23000, andθ11θ12θ13θ14θ15θ16θ21θ22θ23θ24θ25θ26for rigid, affine, and quadratic transformation, respectively.The Degree of Freedom (DoF) of rigid, affine, and second order polynomial is 4, 6, and 12, respectively. In other words, the successful estimation needs at least 2, 3, and 6 point pairs for rigid, affine, and quadratic transformation, respectively.

@&#CONCLUSIONS@&#
