@&#MAIN-TITLE@&#
Distinguishability of interval type-2 fuzzy sets data by analyzing upper and lower membership functions

@&#HIGHLIGHTS@&#
We deal with the problem of classifying interval type-2 fuzzy sets data.A generalized matching algorithm is introduced to compare interval type-2 fuzzy sets.The problem of evaluating the distinguishability of interval type-2 fuzzy sets is then addressed.The distinguishability of a collection of interval type-2 fuzzy sets is directly related to the obtained test set classification accuracy result.The methodology is applied to four different collections of interval type-2 fuzzy sets, elaborated from the same input data.Interesting experimental results are obtained which confirm the validity of the proposed method.

@&#KEYPHRASES@&#
Interval type-2 fuzzy sets,Similarity and dissimilarity measures,Distinguishability of interval type-2 fuzzy sets,Unconventional pattern classification,

@&#ABSTRACT@&#
In this paper, we deal with the problem of classification of interval type-2 fuzzy sets through evaluating their distinguishability. To this end, we exploit a general matching algorithm to compute their similarity measure. The algorithm is based on the aggregation of two core similarity measures applied independently on the upper and lower membership functions of the given pair of interval type-2 fuzzy sets that are to be compared. Based on the proposed matching procedure, we develop an experimental methodology for evaluating the distinguishability of collections of interval type-2 fuzzy sets. Experimental results on evaluating the proposed methodology are carried out in the context of classification by considering interval type-2 fuzzy sets as patterns of suitable classification problem instances. We show that considering only the upper and lower membership functions of interval type-2 fuzzy sets is sufficient to (i) accurately discriminate between them and (ii) judge and quantify their distinguishability.

@&#INTRODUCTION@&#
Research in data-driven inductive modeling systems resulted in the development of numerous classifiers that can automatically handle patterns defined as points inI=ℝn[52]. The main objective of classification problems is to accurately classify some labeled patterns of a given test set, based on some a priori knowledge of their (unknown) underlying data generating process that is available in the form of a training dataset. The classification model of a dataset is synthesized through a suitable learning (i.e., optimization) algorithm, guided by a proper objective function. There are many interesting practical pattern recognition problems that are intuitively defined on structured or unconventional patterns. Examples of such patterns are segmented images [3], audio/video signals [34], and metabolic networks [53]. In the literature, there are different formal representations to model such patterns; for instance labeled graphs, sequences of objects, and fuzzy sets [33,3,8,46,7,24,37,21,22,19,36,23,2]. When dealing with structured or unconventional data, usually the input spaceIis not directly interpretable as a common metric (or measure) space. Consequently, well-known classification systems, such as neuro-fuzzy networks and (adaptive) fuzzy inference systems [35,25,27,26,16], cannot be directly applied since they usually rely on Euclidean geometric properties of the input spaceI(such as angles and distances).Recently, to address these issues, two mainstream approaches are widely used, the kernel[38] and the dissimilarity[33] based techniques. With the kernel-based approach, an appropriate positive semi-definite (PD) kernel functionk:I×I→ℝis used to measure the similarity of the input data. Since PD kernels are Mercer's functions, the well-known Mercer's theorem applies, enabling the possibility of relying on the so-called kernel trick[38]. With the dissimilarity-based approach, a dissimilarity measured:I×I→ℝ+is used to construct the dissimilarity matrix, whereby a new prototype-based formal representation space of the input data is derived. It is worth mentioning that both d(·, ·) and k(·, ·), which are generally termed as matching algorithms, are intimately related, playing a fundamental role when dealing with structured or unconventional patterns [21,33].In this paper, we investigate the problem of classifying interval type-2 fuzzy sets (IT2FSs) by means of a general and light-weighted matching algorithm; the algorithm computes the similarity value between IT2FSs that, however, can be easily converted into a dissimilarity. Since the interpretation of an IT2FS as a pattern is not well-established in the technical literature, we refer to IT2FSs as unconventional patterns. The contribution of the paper is twofold. First, we show that, using different configurations of the proposed matching algorithm, enables the design of accurate and robust classifiers for IT2FSs. Second, and most importantly, we show that the classification results obtained in this setting can be used to evaluate the distinguishability property of IT2FSs. Intuitively, two fuzzy sets are distinguishable if their support on the input domain is diversely distributed. Mencar and Fanelli [28] have provided different definitions of distinguishability in the type-1 fuzzy set (T1FS) case, which include also a dissimilarity-based approach: “two diversely distributed T1FSs will have a higher dissimilarity value in comparison with the two T1FSs having an analogous distribution in terms of support”. We elaborate on the same dissimilarity-based interpretation to characterize distinguishable IT2FSs, relying on the achieved test set classification accuracy as a quantitative and formal indicator of distinguishability; the higher the test set classification accuracy results, the better the distinguishability of the analyzed IT2FSs. This fact enables judging over the outcomes of different procedures that are applied on the same data for generating a collection of IT2FSs. To date, few methods have been proposed for modeling words (concepts expressed in natural language) with IT2FSs [18,7,44,46], but judging over their outcome is subjective – to compare generated IT2FSs, the authors have mainly used the term “look more reasonable”. In this paper, we quantitatively and objectively compare the IT2FSs generated by the four methods described in references [18,7,44,46].The remainder of the paper is organized as follows. After a brief introduction to IT2FSs in Section 2, in Section 3 we introduce the context of similarity (and dissimilarity) measures, reviewing the state-of-the-art similarity measures for IT2FSs. Section 4 describes the proposed matching algorithm for IT2FSs, which is used as the core component in the classification. In Section 5, the performed experimental evaluations are discussed. Finally Section 6 concludes the paper, giving directions for future research.Type-2 fuzzy set (T2FS) is proposed as an extension to the type-1 fuzzy set [56]. T2FS enables handling additional levels of uncertainty by introducing the fuzzy membership function, which characterizes the membership value of an element as a T1FS [29,47]. Despite various efforts on making the cost of using T2FSs affordable, e.g., see [47,41,48,50,12,14,11], to compensate the computational complexities of T2FSs some variations are proposed; notably interval T2FS (IT2FS) [30] and shadowed fuzzy set (SFS) [43,45].In IT2FS, the membership grade of an element is an interval that enables modeling the first degree of uncertainty. However, due to the uniform distribution sitting on top of the intervals, in an IT2FS there is no way to discriminate different choices of membership degrees. SFSs [43,45], however, provide a framework with more freedom degrees for handling uncertainties than IT2FSs with lower computational complexity comparing to general T2FSs. SFSs are generated through redistribution of the fuzziness associated with fuzzy grades of T2FSs in shadowed sets [51]. In this paper, however, we elaborate on IT2FSs.An IT2FSA˜defined on the universe of discourseXis formally represented as(1)A˜={(x,μA˜(x))|x∈X,μA˜(x)={(u,1)|μ_A˜(x)≤u≤μ¯A˜(x),[μ_A˜(x),μ¯A˜(x)]⊆U=[0,1]}}.In (1), x is called primary variable, and[μ_A˜(x),μ¯A˜(x)]denotes the interval valued membership grade of x inA˜.X, as well asU, can be a continuous or a finite set, defining in turn continuous or finite interval type-2 fuzzy sets.An IT2FS is fully characterized by its so-called Footprint of Uncertainty (FOU), defined as:(2)FOUA˜=⋃x∈X(x,[μ_A˜(x),μ¯A˜(x)]).Throughout the paper we use IT2FS and FOU interchangeably. FOU, as can be observed from Eq. (2), is a bounded region depicting the uncertainties associated with the membership grades ofA˜. FOU is completely identified by two T1FSs, namely upper membership function (UMF) and lower membership function (LMF), that are defined as:(3)UMFA˜=FOU¯A˜={(x,μ¯A˜(x))|x∈X},(4)LMFA˜=FOU_A˜={(x,μ_A˜(x))|x∈X}.More detailed discussions on the T2FSs theory, their operations, and related applications can be found in [30,47,41,48,50,49,42,13,10,15,58,39,6,5].A dissimilarity measure onIis a bounded symmetric functiond:I×I→ℝ, such that∃d0∈ℝ,−∞<d0≤d(x,y)<∞,∀x,y∈I(where usually d0=0), andd(x,x)=d0,∀x∈I. If in addition d(·, ·) satisfies the triangular inequality and d(x, y)=d0⇔x=y, then it is called a metric dissimilarity measure (metric in short). Analogously, we can define the concept of similarity measure, since in fact the two concepts are intimately related [3,33]. When d(·, ·) assumes values within the unit interval [0, 1], we refer to it as a normal dissimilarity measure. The same argument is true for the similarity measure s(·, ·). Although formal requisites of (dis)similarity algorithms are important during the analysis of the problem at hand, the design of effective-in-practice (dis)similarity measures for pattern recognition applications remains mostly an engineering, strongly problem-dependent, challenge [33,21,24,37,3]. Pattern recognition and data mining problems usually deal with complex data types, which points out the necessity of defining ad hoc (dis)similarity measures satisfying fewer constraints.LetF˜(X)denotes the set of all IT2FSs onX. A similarity measure onF˜(X)is a bounded functions:F˜(X)×F˜(X)→ℝ+, which satisfies the following four axioms [4,54,57,55]:1(symmetry)∀A˜,B˜∈F˜(X),s(A˜,B˜)=s(B˜,A˜);(vanishing)s(D,Dc)=0, whereDis a crisp set andDcis its complement;(maximization)∀E˜∈F˜(X),s(E˜,E˜)=maxA˜,B˜∈F˜(X)s(A˜,B˜);(monotonicity)∀A˜,B˜,C˜∈F˜(X), ifA˜⊆B˜⊆C˜, thens(A˜,B˜)≥s(A˜,C˜)ands(B˜,C˜)≥s(A˜,C˜).Wu and Mendel in [54] have proposed a similarity measure for IT2FSs, called vector similarity measure, which reads as:(5)sv(A˜,B˜)=[s1(A˜,B˜),s2(A˜,B˜)]T,wheres1(A˜,B˜)measures the similarity between the shapes of the IT2FSsA˜andB˜.s2(A˜,B˜), however, calculates the proximity ofA˜andB˜. Both s1(·, ·) and s2(·, ·) are based on the analysis of the centroid of the two input IT2FSs. The final scalar similarity value is then obtained by multiplying the outcomes of s1(·, ·) and s2(·, ·).Zeng and Li [57] have proposed a similarity measure for IT2FSs based on the direct matching of the UMFs and LMFs,(6)sZL(A˜,B˜)=1−12n∑i=1n|μ¯A˜(xi)−μ¯B˜(xi)|+|μ_A˜(xi)−μ_B˜(xi)|,where n is the number of considered elements of the input domainX. Three other IT2FSs similarity measures are proposed by, respectively, Mitchell [31], Gorza lczany [9], and Bustince [4]. These IT2FS similarity measures are reviewed in [54].The matching algorithm that is proposed in this paper considers only UMFs and LMFs of the given IT2FSs. The basis of the definition is set on the following facts:1An IT2FS is completely characterized by its UMF and LMF that are T1FSs.Two IT2FSs are equal if and only if their corresponding UMFs and LMFs are equal. Two T1FSsAandBwith membership functionsμA(·)andμB(·)are equal(A=B)if and only if for all x in the domain,μA(x)=μB(x)holds.Accordingly, we define the IT2FS matching algorithm as a linear convex combination of the similarity of the upper and lower membership functions. This is achieved by exploiting two suitable, user-defined, normal similarity measures for T1FSs that we respectively represent as sUMF(·, ·) and sLMF(·, ·),(7)s(A˜,B˜)=βsUMF(UMFA˜,UMFB˜)+(1−β)sLMF(LMFA˜,LMFB˜),where β∈[0, 1] is a weighting parameter used to balance the importance of UMF and LMF in the overall computation. This parameter can be tuned with respect to the problem at hand – in the simplest case, β can be set to 0.5.s(A˜,B˜)as is expressed in (7) satisfies the four axioms mentioned in Section 3.1 if and only if sLMF(·, ·) and sUMF(·, ·), in turn satisfy those axioms. It can be observed that if the corresponding LMFs and UMFs of the two IT2FSs are equal, i.e.,LMFA˜=LMFB˜andUMFA˜=UMFB˜, the overall similarity, computed in Eq. (7), will be maximum regardless of the value of β. Moreover, employing symmetric core similarity measures for T1FSs yields in turn a symmetric similarity measure for IT2FSs. The matching method in Eq. (7) can be generalized toward using any kind of similarity measure between T1FSs (i.e., also non-normal T1FSs similarity measures), but we will restrict our analysis to normal similarity measures, since obtaining the corresponding normal dissimilarity measure would be straightforward,(8)d(A˜,B˜)=1−s(A˜,B˜).The generic nature of the proposed method permits us to tune the matching algorithm with respect to different contexts by using different specialized core similarity measures for T1FSs. Moreover, it is parsimonious in terms of required operations, since it needs the analysis of two T1FSs only.Positive definite (PD) kernel functions are similarity functions of particular importance in pattern recognition and data mining, since they constitute the core matching procedures of the so-called kernel machines, such as the well-known support vector machines (SVM). A number of PD kernel functions are reported in the literature [38]. We are interested here in PD kernels that rely on the dissimilarity evaluation, since they can be more easily extended to input domains with non-trivial geometry. In particular, extending the definition of a PD kernel function to IT2FSs must be conceived as a particular instance of the more general matching algorithm proposed in Eq. (7). To this end, we extend the definition of the recently proposed fuzzy information-theoretic kernel (FITK) [46,20] to IT2FSs.Livi and Rizzi [20] (see also [46,32]) have proposed a FITK based on Gaussian radial basis function (Gaussian RBF), which relies on the fuzzy symmetric divergence measure J(·, ·) for T1FSs, like for instance the fuzzy Rényi divergence of order α[1]. Since the symmetric fuzzy divergence between two T1FSsFi,Fjcan be interpreted as a nonnegative distance function, the resulting Gaussian FITK, shown in Eq. (9), is considered as a proper similarity measure:(9)k(Fi,Fj)=exp−12σ2×J(Fi,Fj)2,σ>0.Nonetheless, the underlying scheme of the FITK can be generalized to any distance-based PD kernel function, such as the following rational quadratic kernel [32]:(10)k(Fi,Fj)=1−J(Fi,Fj)2J(Fi,Fj)2+σ,σ>0.Both Gaussian RBF and rational quadratic kernels rely on the parameter σ>0, called kernel width, or kernel size. This parameter is of utmost importance since it permits adapting the kernel to the data distribution at hand through a proper optimization stage.Kernel functions are closed under pointwise addition, product, and multiplication by positive constant [38]. We make use of this property of kernel functions to plug any suitable PD kernel function conceived for T1FSs into the linear convex combination of Eq. (7). More precisely, we exploit them for implementing the core similarity functions sUMF(·, ·) and sLMF(·, ·), obtaining a fuzzy information-theoretic kernel for IT2FSs that we refer to as IT2-FITK in this paper.IT2-FITKs are symmetric, positive semi-definite, normal similarity measures, which decrease monotonically overℝ+∪{0}as the divergence increases. Moreover, it assumes values within the [0, 1] range, reaching its maximum, i.e., 1, if and only if the divergence between the arguments is zero, which means the arguments are equal. This means that the first and third axioms listed in Section 3.1 are satisfied. Since any (symmetric) fuzzy divergence J(·, ·) is a monotonic function of the membership values [17,1], the following relation holds:(11)Fi⊆Fj⊆Fk⇒J(Fi,Fj)≤J(Fi,Fk).Consequently, any IT2-FITK satisfies the monotonicity property. It is notable that any IT2-FITK (like FITKs for T1FSs [20,46]) vanishes only asymptotically. The second axiom, hence, is satisfied only theoretically, since in practice, the computation of the fuzzy divergence yields only finite values.In this section, we discuss the implementation and the experimental evaluation of the proposed matching algorithm, toward analyzing the distinguishability of collections of IT2FSs. We consider different configurations of the general matching scheme (7), performing a comparison with the IT2FS similarity measure of Eq. (6) that we refer to as ZL. It is worth noting however that the ZL measure is a specific case of the general approach that we have proposed in this paper – it is demonstrated in Appendix A. In fact, ZL takes into account only the (normalized) absolute difference between the upper and lower membership degrees of the two input IT2FSs. It is worth underlining that ZL similarity measure should be fed with the same number of values from LMF and UMF, while in our approach, in general, UMFs and LMFs may be discretized differently.The experiments we have conducted are in the setting of classification, considering IT2FSs as patterns of a given dataset. The achieved classification accuracy on the test set is firstly interpreted as a context-dependent indicator of effectiveness of the specific configuration for the matching algorithm on the specific collection of IT2FSs. Hence, we employ a simple classifier, based on the similarity evaluation, which operates directly on the space of IT2FSs. Lastly, and most importantly, we show that the obtained classification results can be used to elaborate on the distinguishability of the IT2FS data collection under analysis.The classification system that we use is based on the well-known k-NN rule [52], which relies on the evaluations of a similarity measure. Considering Eq. (7) in which two core similarity measures are combined, and since the core similarity measures themselves may depend on some other parameters, we perform a stage of parameter optimization on a validation set, using a standard genetic algorithm-based optimization scheme. LetPidenotes an instance of the parameters characterizing a particular configuration of the matching algorithm s(·, ·). We optimize(12)maxPif(s,STR,SV),where the objective function f(·, ·, ·) computes the classification accuracy achieved on the validation setSV, using s(·, ·) as the similarity measure with parametersPi. In the experiments, we perform 100 evolutions for the parameter optimization stage.We have considered some instances from three families of core similarity measures for T1FSs to generate the different configurations of the matching algorithm for IT2FSs shown in Eq. (7). The instances we assume for the sUMF(·, ·) and sLMF(·, ·) are based on fuzzy information processing concepts, geometric interpretations of fuzzy sets, and pure set-theoretic based measures. In the following, for the sake of notation, we useFito denote either the corresponding UMF or LMF of the IT2FSA˜i. It is worth to stress that we have chosen these particular configurations to be able to appreciate the behavior of our general matching algorithm by testing very different core similarity measures for T1FSs. Moreover, in the experiments we have focused on the algorithms conceived to match directly the UMF and LMF only. Therefore, we did not considered in this paper the IT2FS similarity measures proposed in [31,9,4,54].We consider the IT2-FITK that is equipped with two FITKs as core similarity measures. The resulting matching algorithm depends on three parameters, the β parameter and the two kernel size σ1, σ2, defining the width of the two core FITKs. In the following, we refer to the Gaussian FITK formulation – Eq. (9) – as IT2-FITK–G, and the rational quadratic FITK – Eq. (10) – as IT2-FITK–R.We also consider the cosine similarity kernel for both UMF and LMF core similarity measures that yields a PD kernel for IT2FSs, which we refer to as IT2-COS. Eq. (13) shows the definition of the cosine kernel,(13)s(Fi,Fj)=〈f_i,f_j〉‖f_i‖‖f_j‖,wheref_iis the vector interpretation of the membership function of the T1FS,Fi, 〈·, ·〉 is the inner product operator (i.e., the dot product), and finally ||·|| is the Euclidean norm. Since by definition each membership degree is a value in [0, 1], Eq. (13) yields values in [0, 1] only – reaching one only if the two input vectors have the same orientation (i.e., they are equal) and zero if they are orthogonal. This is a geometric-based similarity measure, conceiving T1FSs as proper real-valued vectors of an inner product space.We also test the general matching algorithm using two different set-theoretic based core similarity measures. The first measure is the consistency-index[40] for either UMFs or LMFs, which is calculated as,(14)s(Fi,Fj)=maxx∈X{μFi(x)⊤μFj(x)},where ⊤ is a t-norm (e.g., the minimum). We refer to the resulting IT2FS similarity measure as IT2-MI. For the second measure we use the Jaccard's index,(15)s(Fi,Fj)=Σ(μFi(x)⊤μFj(x))Σ(μFi(x)⊥μFj(x)),whereΣ(Fi(x))computes the cardinality of the finite fuzzy setFi. We refer to the resulting similarity measure as IT2-SH.It should be noticed that for the IT2FS similarity measures shown in Eqs. (13)–(15), we will optimize only β, since these measures are parameterless. Since in the ZL formulation (Eq. (6)) no parameter is involved, there is no need to perform any optimization on the validation set. The list of adopted core similarity measures used in the experiments are summarized in Table 1.In this section, initially we describe how we constructed different classification problem instances with various levels of difficulty. In a nutshell, a labeled pattern is a pair(A˜,l), whereA˜is an IT2FS, andl∈Lrepresents its class label;L={l1,l2,…,lc}denotes the set of labels defining the classification problem. A classification problem instance is defined as a triple of (disjoint) trainingSTR, validationSV, and testSTSsets, each containing a number of labeled patterns.To construct the classification problem instances, we have used four different methods introduced in [46,18,7,44] for generating IT2FSs. To generate 4 sets of IT2FSs, the same initial set of (raw) data is fed to the 4 methodologies. The data is a collection of interval end points representing the perception of different individuals on various simple concepts. The concepts are denoted by words from natural language. To be more descriptive, the dataset has been gathered by asking people to provide answers for the questions like “On a scale of 0–10, what are the endpoints of an interval that you associate with the concept ζ”, where ζ is a word like “some”, “small”, and “tiny”. 32 concepts were questioned and hence 32 sets of intervals, where each interval models the perception of an individual about the concept, were collected.Application of each method of [46,18,7] or [44] resulted in the construction of 32 IT2FSsA˜i,i=1,…,32, each one modeling a specific concept. Although each of the four referenced methods for generating IT2FSs uses the same input data, the resulting IT2FSs are different.We refer to the 4 collections of 32 plain IT2FSs generated by applying methodologies of [18], [7], [46], and [44] respectively as M1, M2, M3 and M4. Fig. 1shows the 32 plain IT2FSs of M1 (with solid red line) and of M2 (with shaded FOU). Figs. 2 and 3depict the 32 plain IT2FSs of M3 and M4, respectively.LetL={l1,l2,…,l32}be the set of labels corresponding to the 32 concepts, and let{(A˜i*,li),i=1,…,32}denotes the initial 32 plain patterns, which are obtained by one of the generating methods M1, M2, M3, or M4. Our aim is to create additional patterns (i.e., IT2FSs) for each classification problem instance starting from those 32 plain patterns. We generate 32×mTRpatterns for the training setSTR, 32×mVfor the validation setSV, and finally 32×mTSfor the test setSTS, where mTR, mV, mTSare positive integers. Let(A˜i*,li)be the plain pattern denoting the ith concept; since we conceive the experiment in the setting of classification, the ith concept is in fact a sort of representative of the ith class of the problem. To generate the additional patterns, we repeatedly apply a random Gaussian noise to the upper and lower membership functions ofA˜i*, obtaining different distorted versions of the plain pattern. Formally, the jth distorted patternA˜ijof the plain patternA˜i*is obtained as:(16)UMF(A˜ij)=UMF(A˜i*)+g_ΣrUMF,(17)LMF(A˜ij)=LMF(A˜i*)+g_ΣrLMF,where bothg_ΣrUMFandg_ΣrLMFare two random vectors distributed according to a multivariate Gaussian distribution with zero mean and spherical covariance matrixΣr=σr2I, where I is the identity matrix. Note that in Eqs. (16) and (17) the two random vectors are assumed to have a number of components equal to the size of the input domainX. The index r on the covariance matrix Σrspecifies the level of noise applied to the plain pattern. For instance, setting σr=0 we do not apply any noise to the plain patterns. Varying the noise level, i.e., decreasing (increasing) σr, we can define different classification problem instances of decreasing (increasing) difficulty.We defined 15 different classification problem instances, with mTR=mTS=mVS=15, generating a total 480 patterns for the training, validation, and test set of each instance. The adopted sequence{σr}r=115of decreasing noise levels is: 0.5, 0.4, 0.3, 0.2, 0.1, 0.08, 0.06, 0.04, 0.035, 0.03, 0.025, 0.02, 0.015, 0.01, 0.005. This sequence has been specifically conceived to obtain different problem instances, from very difficult to easy classification tasks. Since the optimization stage of the adopted classification system is affected by a random seed initialization, we repeated the test set evaluation on each problem instance five times, reporting the average test set classification accuracy together with the standard deviation. We report the results using three settings of k for the k-NN rule; specifically 1, 3, and 5. For both IT2-FITK–G and IT2-FITK–R we used the symmetric fuzzy divergence measure shown in [1, Eq. (13)].

@&#CONCLUSIONS@&#
