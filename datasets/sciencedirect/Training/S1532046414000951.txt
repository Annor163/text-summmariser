@&#MAIN-TITLE@&#
Analysis of obstetricians’ decision making on CTG recordings

@&#HIGHLIGHTS@&#
We used clinical evaluations of 634 CTG recordings by nine obstetricians.We analyzed majority voting and latent class model for agreement on CTG evaluations.Latent class model provides better and more natural way to combine the evaluations.Significant improvement was reached for the pathological evaluations in particular.The results suggest that clinicians use four classes instead of three ordinarily used.

@&#KEYPHRASES@&#
Cardiotocography,Fetal heart rate,Observer variation,Biomedical informatics,Decision making,Latent class analysis,

@&#ABSTRACT@&#
Interpretation of cardiotocogram (CTG) is a difficult task since its evaluation is complicated by a great inter- and intra-individual variability. Previous studies have predominantly analyzed clinicians’ agreement on CTG evaluation based on quantitative measures (e.g. kappa coefficient) that do not offer any insight into clinical decision making. In this paper we aim to examine the agreement on evaluation in detail and provide data-driven analysis of clinical evaluation.For this study, nine obstetricians provided clinical evaluation of 634 CTG recordings (each ca. 60min long). We studied the agreement on evaluation and its dependence on the increasing number of clinicians involved in the final decision. We showed that despite of large number of clinicians the agreement on CTG evaluations is difficult to reach. The main reason is inherent inter- and intra-observer variability of CTG evaluation.Latent class model provides better and more natural way to aggregate the CTG evaluation than the majority voting especially for larger number of clinicians. Significant improvement was reached in particular for the pathological evaluation – giving a new insight into the process of CTG evaluation. Further, the analysis of latent class model revealed that clinicians unconsciously use four classes when evaluating CTG recordings, despite the fact that the clinical evaluation was based on FIGO guidelines where three classes are defined.

@&#INTRODUCTION@&#
Interpretation of cardiotocogram (CTG). The CTG is a simultaneous recording of fetal heart rate (FHR) and uterine contractions. It is an integral part of every day clinical practice. However, since its introduction, it has been a subject of many controversies as well as malpractice litigations [1]. The evaluation of CTG is accompanied with high intra- and inter-observer variability from the very beginning. And even though guidelines, e.g. the most prominent FIGO guidelines [2], were introduced to tackle the heterogeneity of the CTG evaluation, high inter- and intra-observer variability is reported frequently even today [3].The FIGO guidelines consists of 3-tier classification system and in 1980s became the first internationally recognized guidelines. Since then national alternatives with minor tweaks were introduced [4–6]. The comparison of various guidelines and their statements was performed by de Campos and Bernardes [7] with conclusion that the guidelines are, in general, too complex and hard to follow and thus attribute to high inter- and intra-observer variability. To better interpret the CTG patterns and to lower the variability additional improvements were suggested. Schifrin stated [8] that the guidelines lack a definition that can identify the transition from normal to ominous CTG – the so called conversion pattern. Parer and Ikeda [9] and Parer et al. [10] proposed an extension of the guidelines to a 5-tier system. A comparison in [11] claimed this system to be superior to the classical guidelines. Recently, Tommaso et al. showed [12] that the NICHD1Eunice Kennedy Shriver National Institute of Child Health and Human Development.1guidelines have better sensitivity and specificity over 5-tier system. But in general, the performance of 5-tier was better since NICHD evaluated a lot of recordings as “intermediate”. Further, Coletta et al. claimed [13] that there is better sensitivity using the 5-tier system, though the contrary was claimed in [14]. Despite all the efforts, none of the major guidelines changes were thoroughly evaluated in a larger group settings exceeding several hospitals interested.Agreement on interpretation. The substantial inter-intra-observer variability makes it difficult to reach agreement on CTG interpretation. For the purpose of this paper, the agreement does not mean a discussion and consensus of all the clinicians in a consulting room. It means reaching an agreement over independently evaluated CTGs. Generally, the majority voting is a natural way to aggregate different opinions. When making decisions, people usually use weighted majority voting where weights are based on experience, reputation, work place, and other factors. However the determination of weights is subjective and could be misleading.Observer agreement measures. Among statisticians there is no general agreement on how the observer agreement should be measured. The kappa coefficient, proportion of agreement, and intraclass correlation coefficient are the most used measures for agreement [15] even though they have many flaws. For example, the kappa coefficient is influenced by prevalence and base rate and is not suitable for comparison across different studies [16,17]. Also it lacks a natural extension to multiple rates and multinomial classes. There is no single measure of agreement that could outperform the others [15]. The use of quantitative measures and reporting a single value of agreement is tempting, however the results are usually difficult to interpret.Goals and contributions. In our work we aim at examining the agreement of obstetricians using latent class analysis and majority voting. In Section 2.1 we briefly describe the process of annotation that was performed on the CTU-UHB2Czech Technical University – University Hospital Brno.2database [19]. In Sections 2.3.1 and 2.3.2 we further describe the most common method to aggregate different opinions – the majority voting together with an alternative – the latent class analysis. In Section 3.1 we examine stability of clinicians’ agreement using these two methods and show that the agreement is greatly improved especially on pathological class when using the latent class analysis. The latent class analysis shows us a different perspective on the controversial question of how many classes should be used for CTG evaluation. According to our results, the four class model yielded the best results, despite the fact, that clinicians had used guidelines with three classes (cf. Section 3.2).Evaluation of CTG recordings has been acquired using standalone application (CTGAnnotator[18]). The CTGAnnotator adopts the most commonly used display layout of CTG machines (in European format – 1min/cm and 30bpm/cm), and therefore poses no difficulty for clinicians to adjust. The evaluations were obtained from nine clinicians working on delivery wards of six Obstetrics and Gynaecology Clinics in the Czech Republic. All the clinicians are currently working in delivery practice with experience ranging from 10 to 33years (with a median value of 15years). The CTU-UHB intrapartum CTG database [19] was used for evaluation. All the experts had to undergo a basic training on the experiment methodology and the CTGAnnotator usage. Although we expected that all experts adhered to the FIGO guidelines criteria (as required by the Czech authorities3Czech Gynaecological and Obstetrical Society.3) we did not provide any special training for it nor we encouraged it. In our retrospective study we used evaluation of 60min of CTG recordings at the end of the first stage of labor. Clinicians evaluated the CTG recordings into three classes: normal, suspicious, and pathological (FIGO classes).We use proportion of agreement (PA) to measure the agreement between clinicians. The PA is simply probability that clinicians agree on evaluation. We decided to use PA, which is intuitive and understandable, instead of other complex statistical measures that could obscure the analysis.The different schemes of voting were thoroughly studied in social sciences. The famous Condorcet’s jury theorem (1786), details e.g. [20], states: if voters are right with probabilityp>1/2, then majority vote is more likely to be right than wrong and the probability of being right tends to 1 when number of voters goes to infinity. Intuitively it is expected that potential variability could be cancelled out by a high number of voters.Letyijbe a evaluation of the i-th example,i={1,2,…,N}, given by the j-th clinician,j={1,2,…,J}. Further letc∈Cbe a category to whichyijcould be assigned andδ(yij,c)be an indicator function that equals 1 when the j-th clinician evaluatesyij=cand 0 otherwise.The majority voting is a simple voting mechanism to aggregate evaluation from J clinicians. The probability that the i-th example is assigned to the c-th class is(1)μic=1J∑j=1Jδ(yij,c).The majority voting, or more precisely plurality voting, is simply choosing a class c for maximum ofμic. In the case of ties a flip of fair coin is performed.Problems with majority voting. Majority voting is simple and usually preferred method. However, there are some limitations when using majority voting on evaluation of CTG, which are summarized as follows:1.There is high inter- and intra-observer variability in clinical evaluation (see for example [3,22,23]) and agreement might not be reached.Each clinician has different expertise not only based on the length of his/her career (experienced vs. inexperienced) but also influenced by labor management at workplace; e.g. a clinician who is called only to the most serious cases could loose, to some extent, knowledge related to the normal cases.Clinicians could loose concentration/motivation or be simply distracted during annotation.The latent class analysis (LCA) is used to estimate the true (unknown/hidden) evaluation of CTG and to infer weights of individual clinicians’ evaluation – the latent class model (LCM). Letyi∈Y;Y={1,2,…,C}be the unobservable ground truth for the i-th example andαcj=(αc1j,αc2j,…,αckj,…,αcCj)be a multinomial parameter that represents probabilities that the c-th class corresponds to an evaluation in the k-th class,k∈C, assigned by the j-th clinician(2)αckj=P(yij=k|yi=c),αck⩾0,∑k=1Cαckj=1.The assumption forαckjis that the evaluation for different c and k are independent on the observed data. This assumption is violated in practice since some examples are more difficult than others and each clinician has different level of expertise. The approach dealing with dependence on observed data was described in [24], however no significant improvements were acquired. Unlike [25] we formulate the model in a simplified way that is, every clinician provides one evaluation for each example. With this simplification we completely rule out the possible violation of conditional independence between two evaluations assigned by a clinician to a certain example.The LCM considers clinical evaluation as a finite mixture of multinomial distributions. Finite mixture models [26] have fixed number of parameters and the standard method to estimate these parameters is expectation maximization (EM) algorithm [27]. Let us consider one particular set of evaluationsy1,…,yJ. Then, it is assumed that these evaluations are from a mixture of initially specified C components in some unknown proportionsp1,…,pC. Each data point is a realization of the mixture probability mass function(3)p(y1,…,yJ|θ)=∑c=1Cpcp(y1,…,yJ|θc),whereθinclude the unknown mixing proportionpc(prevalence) and the elements ofθc. Then, given a set of evaluationsD={yi1,…,yiJ}i=1Nand vector of parametersθ={αckj,pc}, the likelihood corresponding to C component mixture is(4)p(D|θ)=∏i=1N∑c=1Cpcp(yi1,…,yiJ|θc).We treat the unknown truthyias a latent (hidden) variable and use the EM algorithm to estimate it. We assume thatyi1,…,yiJare independent (i.e. all clinicians make their evaluation independently) and that the evaluations are from multinomial distribution. Then, the likelihood function of the parametersθgivenDcan be formulated as(5)p(D|θ)=∏i=1N∑c=1Cpc∏j=1J∏k=1C(αckj)δ(yij,k).The maximum likelihood is found by maximizing the log likelihood function(6)θˆML={αck1,…,αckJ,p1,…,pC-1}=arg maxθ{logp(D|θ)}.To maximize the log likelihood we followed the work of Dawid and Skene [25] and used the EM algorithm.Estimation using the EM algorithm. The hidden variables to be estimated are multinomial parameterαckj, prevalence of classespc, and true (unknown/hidden) evaluations{yi}i=1N. If we would have known the true evaluationy, the complete log-likelihood would be computed as(7)logp(D,y|θ)=∑i=1N∑c=1Cδ(yi,c)logpc∏j=1J∏k=1C(αckj)δ(yij,k).In the E-step we compute the conditional expectation ofyigiven the evaluations from cliniciansDunder the current estimates of parametersθ(8)E{logp(y|D,θ)}=∑i=1N∑c=1Cμiclogpc∏j=1J∏k=1C(αckj)δ(yij,k),whereμic=p(yi=c|yi1,…,yiJ,θ)is estimated probability of ground truth given theyijandθand is proportional to(9)μic∝pc∏j=1J∏k=1C(αckj)δ(yij,k).In the M-step we use the current estimates to maximize the conditional expectation. Taking a derivative of (8) equal to zero, the parametersαckjandpcare updated using the following equations(10)αckj=∑i=1Nμicδ(yij,k)∑i=1Nμic,pc=1N∑i=1Nδ(maxc(μic),c),wheremaxc(μic)assigns a class c that has the maximum probability. The E and M steps are repeated until convergence. The EM algorithm is guaranteed to converge to a local maximum only; therefore, it is usually restarted several times with different starting values. Another possible solution, used in this work, is to use the majority voting for initialization as proposed in [25]. The limit of log-likelihood convergence was set to10-3.The latent model is powerful not only for estimating the latent class from multiple, possibly noisy, evaluations but could be also used to infer the number of classes the clinicians are actually using. Employing the LCA we can infer the number of classes, for which the evaluation would yield the best score – irrespective of the number of classes the clinicians used. In (5) we supposed a fixed number of classes. However, the guidelines are not precise, nor they are strictly followed by clinicians, leaving an open space for alternative evaluation. Our goal is to examine whether choosing different number of classes offers better description of clinical evaluation in terms of model fit. The extension to encompass different number of classes is straightforward. We replace C by a number R representing different number of latent classes:(11)p(D|θ)=∏i=1N∑r=1Rpr∏j=1J∏k=1C(αrkj)δ(yij,k),where the same holds forαrkjas it did forαckj. In our experiments we used the value ofR={2,3,…,8}, obtaining modelsM2,M3,…,M8.Number of estimated parameters. The number of estimated parametersϑincreases rapidly with increasingR,J, and C and is computed as:ϑ=R-1+J·[R(C-1)]. If theϑexceeds number of examples the model is not identifiable. The model is also not identifiable if the probabilitiesαrkjare sparse.The latent class model (LCM) can also be used to rank contribution of individual clinicians. The scoring/ranking based on detection of spammers was proposed by Raykar and Yu [21], where random evaluations were penalized. In our work, we had to adapt the penalization to reflect clinical evaluation, which we do not expect to be random. We use the following accuracy based score that is fairly simple and easily interpretable. LetAjbe aC×Cconfusion matrix with entries[Aj]ck=αckj. The diagonal elements represent probabilities of correct classifications with respect to latent class,c=k, and off-diagonal elements represent probabilities of misclassification,c≠k. Consider the following confusion matrices for a good cliniciansAgand badAb(12)Ag=0.90.100.10.9000.50.5Ab=0.50.5000.050.9500.050.95.The good cliniciansAgperformed well on the first and second class and poorly on the third class, where the probability of correct decision dropped to 0.5. The bad clinicianAbcorrectly evaluated prevalently the third class. The accuracy based score forC=Kis defined as(13)Saccj=1C∑c=kAckj-∑c≠kAckj.The score simply equals to summation of diagonal elements with subtraction of off-diagonal elements. In the case of matricesAgandAbthe score yieldsSaccg=0.53andSaccb=0, respectively. The score for the worst possible clinician isSacc=-1and for the best possible isSacc=1.Ranking for different number of classes. The accuracy based score has a limitation in case the latent variable has different number of classes than the clinicians actually used. We focus only on the scenario whenR=4because of the best model fit. LetArkjbe a matrix[Aj]rk=αrkj, whereR≠C. WhenR=4andC=3the score is defined as(14)Saccj=1R∑r=k,r-1=kArkj-∑r≠k,r-1≠kArkj.The score allows misclassification over one latent class. The computation of the score is visualized in Table 1. We discuss the rationale for clinical evaluation later. Note that the score serves for ranking clinicians for particular choice of r. We detail the comparison of the models (i.e. their fit) for various r in the next section.We can use various techniques to evaluate a model fit and to determine which model is more appropriate for different values of R. Increasing R from two to eight increases the model fit but also increases the possibility of over-fitting. Additionally, higher R values lead to estimation of more model parameters. A trade-off between better model fit and number of parameters to be estimated is usually sought and tackled by penalizing the log likelihood using a function of parametersθthat are to be estimated. The two most common measures are the Akaike information criterion (AIC) [28] and Bayes information criterion (BIC) [29]. For a likelihood L the AIC and BIC are defined as:AIC(r)=-2lnL+2ϑ,BIC(r)=-2lnL+ϑlnN.The better is the model the lower BIC and/or AIC measures are obtained. Usually the AIC overestimates the number of R while BIC underestimates it. Therefore a compromise between these is often sought.We use majority voting for the description of stability, but any other method for aggregation could be used. The motivation for analysis of the stability of clinical evaluation follows: Let us consider majority voting of J clinicians. It would be interesting to know whether the created majority was obtained simply by a chance or whether the majority is stable and the possible variability in clinicians’ decision was cancelled out by using a sufficient number of clinicians. We summarize the definition of stability in Proposition 1.Proposition 1We consider a majority vote of J clinicians stable if a majority voting ofJ+1clinicians is not different (measured by proportion of agreement).In the proposition, the term “different”, our criterion, is not rigorous and allows various approaches to be used, i.e. statistical testing. However, the statistical evaluation is not that straightforward as the created majority votes are not independent. In order to analyze the stability we performed the following experiment: we computed majority votes (MV) for all combinations of cliniciansJq, whereq∈Q;Q={3,4,…,J-1}. Then we compared the majority obtained with the majority vote of all clinicians,J=9. The procedure is shown in Algorithm 1.Algorithm 1Stability of clinical evaluationInput:Q={3,4,…,8}number of clinicians,Yclinical evaluation of sizeN×J,mvJmajority vote of all J clinicians,mvbmajority vote of combination b of cliniciansResult:pa – proportion of agreementbeginforq∈Qdocomb←Jq– all combinations of q clin. from Jforb∈combdoYb=Y(:,b)– get evaluation of selected bmvb←majorityVoting(Yb)pa(q,b)←proportionOfAgreement(mvb,mvJ)endendend

@&#CONCLUSIONS@&#
