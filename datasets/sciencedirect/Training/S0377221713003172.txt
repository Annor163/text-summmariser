@&#MAIN-TITLE@&#
The benefits of differential variance-based constraints in portfolio optimization

@&#HIGHLIGHTS@&#
We suggest extensions to portfolio optimization with constraints.The constraints on a stock’s portfolio weight are determined by its sample variance.The suggested methods yield improved performance relative to existing methods.

@&#KEYPHRASES@&#
Mean–variance analysis,Portfolio optimization,Portfolio constraints,Estimation error,

@&#ABSTRACT@&#
The main problem of portfolio optimization is parameter estimation error. Various methods have been suggested to mitigate this problem, among which are shrinkage, resampling, Bayesian updating, naïve diversification, and imposing constraints on the portfolio weights. This study suggests two substantial extensions of the constrained optimization approach: the Variance-Based Constraints (VBC), and the Global Variance-Based Constraints (GVBC) methods. By the VBC method the constraint imposed on the weight of a given stock is inversely proportional to its standard deviation: the higher a stock’s sample standard deviation, the higher the potential estimation error of its parameters, and therefore the tighter the constraint imposed on its weight. GVBC employs a similar idea, but instead of imposing a sharp boundary constraint on each stock, a quadratic “cost” is assigned to deviations from the naive 1/N weight, and a single global constraint is imposed on the total cost of all deviations. Comparing ten optimization methods we find that the two new suggested methods typically yield the best performance, as measured by the Sharpe ratio. GVBC ranks first. These results are obtained for two different datasets, and are also robust to the number of assets under consideration.

@&#INTRODUCTION@&#
The Mean–Variance (MV) rule of Markowitz (1952, 1959) is probably the most commonly employed investment rule by both academics and professional investors, hence the intensive research on the theoretical and practical implication of the MV paradigm. If one assumes normality of returns and risk aversion, it is well known that the MV rule is optimal in the expected utility framework (see Tobin (1958) and Hanoch & Levy (1969)).2When distributions are not normal the MV rule provides an excellent approximation to expected utility as long as the returns are not extreme, see Levy and Markowitz (1979), Kroll, Levy, and Markowitz (1984) and Markowitz (1991). MV analysis has been extended in several different directions, some of which are: suggesting other measures of risk to replace variance (Markowitz, 1993; Grootveld & Hallerbach, 1999; Deng, Li, & Wang, 2005; and Huang, 2008); considering Prospect Theory preferences (Levy & Levy, 2004), and employing a fuzzy logic approach to optimization (Huang, 2007).2Therefore, the application of the MV rule is straightforward as long as the various parameters of the multivariate normal distribution are known. Thus, given the normality assumption, the criticism of employing Markowitz’s MV optimizer in a straightforward manner is not related to the theoretical aspects of this paradigm, but rather to the statistical estimation errors of the various parameters needed to employ this rule in practice. This problem, and various solutions for dealing with it, are the focus of this paper.The application of the MV rule presumes that the investors have exact knowledge of expected returns, variances and covariances. In practice, the ex-ante parameters are unknown and one must rely on the limited available information, which is generally composed from the historical returns. However, employing the historical distributions of returns, the MV rule typically yields unacceptable investment strategies, with extreme portfolio weights, and with many assets held short. Moreover, the aggregate percent of the portfolio in short position in the MV optimal investment strategy based on historical returns is generally very large, and may reach hundreds and even thousands of percent. While holding short positions is a legitimate investment strategy, many institutional investors refrain from having short positions, and typically restrict the investment weights (e.g., see Frost & Savarino, 1988, p. 29).In addition, one cannot be very confident about the portfolio weights generated by the MV optimizer with sample estimates, because these weights are very sensitive to the sample parameters, especially to the sample means (see Best & Grauer (1991) and Chopra & Ziemba (1993)). This is quite unsettling, as a manager may find that the MV optimal portfolio based on the sample of the last 50 monthly returns, for example, is completely different than the portfolio constructed based on the last 60months. Numerical analyses of the MV efficiency frontier reveals that even a little reduction in the mean of a given asset may shift the asset from a large long position to a large short position. Thus, even a small difference between the true unknown means and the sample estimated means, quite in the plausible error range, may yield a large deviation between the optimal investment strategy and the one recommended based on the estimated sample parameters.Estimation errors may also lead to incorrect theoretical conclusions. For example, Levy and Roll (2010) show that with historical mean returns and standard deviations the market portfolio is MV inefficient. Yet, little changes in these parameters, which are statistically legitimate, reveals that the market portfolio is located on the MV frontier. This finding indicates that sampling errors in the various means may greatly influence the conclusion regarding the validity of Sharpe (1964) – Lintner (1965) CAPM, let alone the MV efficiency of a particular portfolio under consideration.Because of the theoretical and practical importance of the MV rule, numerous papers suggest various ways to deal with the fact that the ex-ante parameters are unknown and sampling errors strongly affect the results. As a consequence of the extreme sensitivity of the optimal diversification weights to the potential errors in the sample estimates of the mean returns, and the typically large errors involved in estimating mean returns, some researchers suggest an extreme investment policy which ignores the historical sample means altogether, and focuses on the investment weights corresponding to the sample minimum variance portfolio (see Green & Hollifield (1992)). An even more extreme suggestion is to ignore all historical parameters, means and variance–covariance matrix alike, and to employ the naïve diversification strategy, namely, investing an equal proportion of 1/N in each of the N available assets (see, for example, DeMiguel et al. (2009) and Duchin & Levy (2009)). Most other suggested methods incorporate all the sample parameters, but with some modification, either to the parameters or to the optimization procedure. One possibility is to apply “shrinkage” to the sample parameters (Ledoit & Wolf, 2003, Jagannathan & Ma, 2003). Other methods which consider the sampling errors and suggest manners to mitigate these errors are the Bayesian approach (Jorion, 1986; Markowitz & Usmen, 2003), the Monte Carlo resampling approach (Michaud, 1989, 1998), and the Black and Litterman (1992) approach. Many of the suggested modifications are related hence, achieve similar results. In the next section we provide a brief literature review which covers the various methods examined in the present study.Which optimization method is better at handling the sampling errors, hence performs best? Is the relative ranking of the various methods invariant to the number of assets under consideration? If constraints are imposed on the portfolio weights to avoid extreme positions due to sampling error, what type of constraints should be imposed, and how stringent should they be? In answering these questions, one needs to consider two potential economic losses that should be balanced in the derivation of the portfolio weights:(a)Using the historical sample parameters may lead to a portfolio which is very different from the optimal portfolio which is based on the true parameters, due to the sampling errors.Drastically modifying the sample parameters (or completely ignoring them) or, alternatively, imposing severe constraints on the portfolio weights may lead, once again, to sub-optimality and economic loss. For example, if the parameter estimates are very close to the true parameters, imposing stringent constraints on the weights may hamper performance.Researchers who suggest modifications for the simple MV optimizer are aware of the importance of the historical sample data, as well as the above two possible losses. Hence, most of them do not advocate ignoring the historical data completely, but rather giving it some weight in the portfolio construction procedure. The main issue the investor faces is how much weight to give to the historical information. Indeed, the goal is to find a golden path that balances the two potential losses discussed above. Namely, one needs to employ the information from the sample returns, but simultaneously to restrict extreme diversification policies, which may be due to sampling errors.In this paper we compare the performance of the main optimization methods suggested in the literature. In addition, we suggest two novel methods that turn out to improve upon the performance of the existing methods mentioned above. We call these two new methods Variance-Based Constrained optimization (VBC) and Global Variance-Based Constrained optimization (GVBC). These methods are generalizations of the portfolio weight constraints method. The main idea in both new methods is to take into account the fact that the estimation error is not the same for all stocks: the estimation errors are larger for stocks with larger sample variances. The VBC method imposes constraints that are inversely related to the stock’s sample standard deviation: the lower the standard deviation, the lower the estimation error, and the looser the constraints imposed on the stock’s portfolio weight.The VBC method, however, does not imply a higher “cost” for more extreme weights, as long as they are within the allowed bounds. It also does not take into account the relative contribution of different stocks to the portfolio’s performance. For example, if we have two stocks with the same sample standard deviation but very different sample means, we may want to allow looser constraints for the stock with the higher mean, because it contributes more to the portfolio’s mean. The Global VBC (GVBC) method incorporates these two elements by formulating a quadratic “cost” on the deviation of weights from the naïve 1/N weight and imposing a single constraint on the total cost of all weights. We explain these two methods in detail in Section 3.Methodologically, the performance of the various optimization methods can be analyzed in several different frameworks. One can assume stable return distributions, or alternatively, one can employ out-of-sample analysis. The out-of-sample framework may yield very similar results to the stable case if indeed the return distributions are stable, but may yield different results when the distributions vary over time. While the stable distribution scenario measures purely the economic loss due to sampling estimation errors, the out-of-sample approach measures two combined effects: sampling errors for given distributions, as well as the instability of the distributions. Professional investors are rightfully interested in the combined effect, namely, out-of-sample analysis. However, the empirical out-of-sample performance of the various optimization methods under consideration are highly sensitive to the sample period selected for the analysis. One method may outperform the other if, say, the last decade is considered, and the opposite holds if the last two decades are considered. Kritzman, Page, and Turkington (2010) convincingly demonstrate this point by showing that the results of DeMiguel et al. (2009a) in favor of naïve diversification are overturned when long sample periods are employed. Thus, it may be very hard to reach general conclusions based on out-of-sample analysis. In contrast, the stable distribution approach allows a more controlled experiment, as the true parameters are known to a “referee” who compares the performance of the various suggested methods, where each method relies solely on sample information. This method is employed by Frost and Savarino (1988), Michaud (1998), Markowitz and Usmen (2003), Harvey, Liechty, and Liechty (2008), and it is the one we follow here.The structure of this paper is as follows. Section 2 provides a brief review of the methods that appear in the literature and are examined in this paper. Section 3 introduces the two new methods (VBC and GVBC) suggested here. Section 4 provides the methodology and results. Section 5 concludes.

@&#CONCLUSIONS@&#
