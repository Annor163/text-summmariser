@&#MAIN-TITLE@&#
A novel feature selection method for twin support vector machine

@&#HIGHLIGHTS@&#
The references have been revised uniformly.Several references published on the KBS journal are replaced with other proper ones.Additional references suggested by the reviewer are added in the new version.

@&#KEYPHRASES@&#
Pattern recognition,Feature selection,Twin support vector machine,Feature ranking,L,1,norm,Multi-objective mixed-integer programming,

@&#ABSTRACT@&#
Both support vector machine (SVM) and twin support vector machine (TWSVM) are powerful classification tools. However, in contrast to many SVM-based feature selection methods, TWSVM has not any corresponding one due to its different mechanism up to now. In this paper, we propose a feature selection method based on TWSVM, called FTSVM. It is interesting because of the advantages of TWSVM in many cases. Our FTSVM is quite different from the SVM-based feature selection methods. In fact, linear SVM constructs a single separating hyperplane which corresponds a single weight for each feature, whereas linear TWSVM constructs two fitting hyperplanes which corresponds to two weights for each feature. In our linear FTSVM, in order to link these two fitting hyperplanes, a feature selection matrix is introduced. Thus, the feature selection becomes to find an optimal matrix, leading to solve a multi-objective mixed-integer programming problem by a greedy algorithm. In addition, the linear FTSVM has been extended to the nonlinear case. Furthermore, a feature ranking strategy based on FTSVM is also suggested. The experimental results on several public available benchmark datasets indicate that our FTSVM not only gives nice feature selection on both linear and nonlinear cases but also improves the performance of TWSVM efficiently.

@&#INTRODUCTION@&#
Feature selection [18,15,6,11,19] is an important problem since it removes the irrelevant features and maintains the relevant features that are as close to the class as possible. The benefit of feature selection is twofold. On one hand, it is meaningful because it can identify the features that contribute most to classification. On the other hand, feature selection is helpful for solving the classification problem since it can not only reduce the dimension of input space and speed up the computation procedure but also improve the classification accuracy. There are mainly two types of feature selection methods: one is the general methods independent of any classifiers, e.g., Fisher score [38] and Laplacian score [12]; the other one is the wrapper-type method dependent on the classifier, e.g., the methods based on Bayesian network [14], based on neural networks [32] and based on support vector machine (SVM) [2,23]. The wrapper-type methods attract more attentions and often improve the performance of the original classifier as well [14,37,23,2,32].It is interesting to investigate the wrapper-type feature selection methods based on the twin support vector machine (TWSVM) [16,35]. TWSVM generates two nonparallel fitting hyperplanes and has superior performance than SVM [3,21,7,43] on both the classification accuracy and learning speed in many practical applications [16,35,33,30]. Particularly, it is suitable for some special datasets, e.g., “cross-planes” [16]. However, in contrast to SVM which owns many wrapper-type feature selection methods such as SVM-RFE [23] and RFSVM [2], TWSVM has no any corresponding one. The reason is that the current SVM-based feature selection methods cannot be used to TWSVM directly. In fact, for each feature, SVM provides a single weight corresponding to the single separating hyperplane whereas TWSVM provides two weights corresponding to two fitting hyperplanes, leading to some difficulties for feature selection.In this paper, we propose a novel feature selection method for TWSVM, called FTSVM for short. Our FTSVM includes two forms: linear FTSVM and nonlinear one. The former is formulated by the following steps: first of all, a basicL1-TWSVM is proposed by introducing theL1-norm regularization terms, due to the success inL1-SVM to obtain a sparse feature weight [44,20,8,13]; then, a feature selection matrix, a diagonal matrix with element either 1 or 0, is introduced in the proposedL1-TWSVM, resulting to two mixed integer programming problems (MIPPs); finally, the two MIPPs are solved simultaneously as a multi-objective mixed integer programming problem (MOMIPP) [39] by an greedy algorithm. The nonlinear FTSVM is constructed by kernel trick [16,35], which needs also to solve a MOMIPP by the proposed greedy algorithm. In addition, based on FTSVM, a feature ranking strategy is suggested, which ranks the features according to their contributions to the objective in the MOMIPP. The experimental results show that our FTSVM not only gives nice feature selection but also improves the performance of TWSVM.This paper is organized as follows. A briefly review ofL2-TWSVM is in Section 2, and the standardL1-TWSVM is proposed in Section 3. Our feature selection and ranking methods are formulated in Section 4, and experiments are arranged in Section 5. Finally, Section 6 gives the conclusions. For convenience, in Table 1, we present some notations used in the paper.Consider the binary classification problem withm1training samples belong to positive class represented byA1andm2training samples belong to negative class represented byA2in the n-dimensional real spaceRn, a classifier attempts to predict the new samples belong to either positive or negative class.TWSVM [16] seeks two nonparallel hyperplanes(1)f1(x)=xw1+b1=0andf2(x)=xw2+b2=0such that each one is the fitting hyperplane of one of the positive and negative classes, a new sample will be predicted to one class if it is closer to the corresponding fitting hyperplane. TWSVM keeps the fitting hyperplane as close as possible to its corresponding training samples and far away from the others. The purpose ofL2-TWSVM [35] is very the same as TWSVM, where the difference isL2-TWSVM introduces theL2regularization terms of the weight vectors into TWSVM to minimize the structural risk.L2-TWSVM solves following quadratic programming problems (QPPs)(2)minw1,b1,ξ112‖w1‖22+b12+c11‖A1w1+b1e‖22+c12e⊤ξ1s.t.-(A2w1+b1e)+ξ1⩾e,ξ1⩾0,and(3)minw2,b2,ξ212‖w2‖22+b22+c21‖A2w2+b2e‖22+c22e⊤ξ2s.t.A1w2+b2e+ξ2⩾e,ξ2⩾0,wherec11,c12,c21,c22>0are parameters,ξ1∈Rm2andξ2∈Rm1are slack vectors.The geometric meanings of (2) and (3) are clear. For example, for (2), its objective function makes the fitting hyperplanef1(x)=0of the positive class fit the positive class samplesA1, while the constraints keep the negative class far from this hyperplane to some extent (the bias of each negative training sample tof1(x)=0is no more than-1).The above linearL2-TWSVM has been extended to nonlinear classifier by kernel trick [35]. Define the inner product by the kernel functionK(·,·), nonlinearL2-TWSVM seeks two kernel generated surfaces(4)f1(x)=K(x,A)u1+γ1=0andf2(x)=K(x,A)u2+γ2=0,where A is the whole training set, includingA1andA2.The corresponding QPPs solved in nonlinearL2-TWSVM are(5)minu1,γ1,ξ112‖u1‖22+γ12+c11‖K(A1,A)u1+γ1e‖22+c12e⊤ξ1s.t.-(K(A2,A)u1+γ1e)+ξ1⩾e,ξ1⩾0,and(6)minu2,γ2,ξ212‖u2‖22+γ22+c21‖K(A2,A)u2+γ2e‖22+c22e⊤ξ2s.t.K(A1,A)u2+γ2e+ξ2⩾e,ξ2⩾0.As stated above,L1-SVM which is sparser thanL2-SVM [8,13] is proposed by replacing theL2norm regularization term inL2-SVM withL1norm. Therefore, we propose theL1-TWSVM that minimizesL1-norm of the feature weight vectors so that the features are sparser thanL2-TWSVM. Similar toL1-SVM, we introduce theL1-norm ofwi,i=1,2, and straightly transform the other parts inL2-TWSVM fromL2norm toL1-norm as follows(7)minw1,b1,ξ1‖w1‖1+c11‖A1w1+b1e‖1+c12‖ξ1‖1s.t.-(A2w1+b1e)+ξ1⩾e,ξ1⩾0,and(8)minw2,b2,ξ2‖w2‖1+c21‖A2w2+b2e‖1+c22‖ξ2‖1s.t.A1w2+b2e+ξ2⩾e,ξ2⩾0.Different from solving two dual QPPs inL2-TWSVM [35], the above problems can be solved as two differentiable linear programming problems (DLPPs) [9,22]. Definewi=pi-qi,Aiwi+bie=si-ti,i=1,2, then (7) and (8) are equivalent to(9)minp1,q1,s1,t1,b1,ξ1e⊤(p1+q1)+c11e⊤(s1+t1)+c12e⊤ξ1s.t.A1(p1-q1)+b1e=s1-t1,A2(p1-q1)+b1e⩽-e+ξ1,p1,q1,s1,t1,ξ1⩾0,and(10)minp2,q2,s2,t2,b2,ξ2e⊤(p2+q2)+c21e⊤(s2+t2)+c22e⊤ξ2s.t.A2(p2-q2)+b2e=s2-t2,A1(p2-q2)+b2e⩾e-ξ2,p2,q2,s2,t2,ξ2⩾0.Notesi=Ai(pi-qi)+bie+ti,i=1,2, the final problems we solved are(11)minp1,q1,t1,b1,ξ1e⊤(p1+q1)+c11e⊤(A1(p1-q1)+2t1)+c11m1b1+c12e⊤ξ1s.t.-A1(p1-q1)-b1e-t1⩽0,A2(p1-q1)+b1e-ξ1⩽-e,p1,q1,t1,ξ1⩾0,and(12)minp2,q2,t2,b2,ξ2e⊤(p2+q2)+c21e⊤(A2(p2-q2)+2t2)+c21m2b2+c22e⊤ξ2s.t.-A2(p2-q2)-b2e-t2⩽0,-A1(p2-q2)-b2e-ξ2⩽-e,p2,q2,t2,ξ2⩾0.The DLPPs (11) and (12) can be solved by many algorithms and toolboxes. In our experiments, we used a variation of the well known simplex algorithm for linear programming problem [5].For nonlinear case, ourL1-norm nonlinear TWSVM also seeks two kernel generated surfaces the same as (4). The corresponding problems are as follows:(13)minu1,γ1,ξ1‖u1‖1+c11‖K(A1,A)u1+γ1e‖1+c12‖ξ1‖1s.t.-(K(A2,A)u1+γ1e)+ξ1⩾e,ξ1⩾0,and(14)minu2,γ2,ξ2‖u2‖1+c21‖K(A2,A)u2+γ2e‖1+c22‖ξ2‖1s.t.K(A1,A)u2+γ2e+ξ2⩾e,ξ2⩾0,whereK(·,·)is the kernel function the same as nonlinearL2-TWSVM. The problems (13) and (14) could be converted to two DLPPs the same as the linear case ifK(Ai,A)in (13) or (14) is replaced withAiin (7) or (8),i=1,2, respectively.L1-TWSVM has at least two advantages: (1) the sparsity of the solution and the simplicity of learning process, but without loss the generalization ofL2-TWSVM; (2)L1-norm linear TWSVM can be directly as a feature selection method (i.e., if two weights of a feature in the weight vectorsw1andw2are equal to zeros, then the corresponding feature should be deleted). However, if we use the aboveL1-TWSVM to do feature selection directly, the efficiency may not be expected asL1-SVM, this is because if one of the two weights of a feature is not zero, then this feature should not be deleted even if the other weight is zero. In addition, the procedure of feature selection of linearL1-TWSVM is not suitable for the nonlinear case because of the unknown feature weight. These drawbacks restrict the ability of feature selection ofL1-TWSVM.In this section, we shall elaborate a feature selection method, named FTSVM, and a feature ranking method, named RTSVM, based onL1-TWSVM.In order to select features based onL1-TWSVM, we introduce a feature selection matrixE=diag(1or0)and seek two nonparallel hyperplanes(15)g1(x)=(xE)w1+b1=0andg2(x)=(xE)w2+b2=0.The problems (7) and (8) become to following two mixed integer programming problems (MIPPs)(16)minw1,b1,ξ1,E‖w1‖1+c11‖A1Ew1+b1e‖1+c12e⊤ξ1+σe⊤Ees.t.-(A2Ew1+b1e)⩾e-ξ1,ξ1⩾0,E=diag(1or0),and(17)minw2,b2,ξ2,E‖w2‖1+c21‖A2Ew2+b2e‖1+c22e⊤ξ2+σe⊤Ees.t.A1Ew2+b2e⩾e-ξ2,ξ2⩾0,E=diag(1or0),whereσ⩾0is a parameter to control the number of selected features (sincee⊤Ee=trace(E)).The geometric meanings of problems (16) and (17) are clear. In fact, the feature selection matrix E defines a subspace spanned by the selected features. Therefore, the training samplesA1andA2distribute in different subspace denoted for different E. The objective in (16) makes the hyperplanesg1(x)=0closer to the positive classA1in this subspace, while the constraints makeg1(x)=0far from the negative classA2to some extent in this subspace, as does in (17).Note that the above two problems (16) and (17) share the same matrix E but have different objectives, so they should be considered as a multi-objective MIPP (MOMIPP) [39].(18)minw1,b1,ξ1,E‖w1‖1+c11‖A1Ew1+b1e‖1+c12e⊤ξ1+σe⊤Eeminw2,b2,ξ2,E‖w2‖1+c21‖A2Ew2+b2e‖1+c22e⊤ξ2+σe⊤Ees.t.-(A2Ew1+b1e)⩾e-ξ1,ξ1⩾0,A1Ew2+b2e⩾e-ξ2,ξ2⩾0,E=diag(1or0),Once the solution of (18) is obtained, one can easily get the selected features from the feature selection matrix E. In addition, a new sample can be predicted by (15).Before solving the above problem (18), let us extend it to nonlinear case by kernel trick. Corresponding to the kernel generated surfaces (4) in nonlinearL1-TWSVM, our nonlinear FTSVM seeks two kernel generated surfaces with a feature selection matrix E as follows(19)h1(x)=K(xE,AE)u1+γ1=0andh2(x)=K(xE,AE)u2+γ2=0.Similarly, this also leads a MOMIPP(20)minu1,γ1,η1,E‖u1‖1+c11‖K(A1E,AE)u1+γ1e‖1+c12‖η1‖1+σe⊤Eeminu2,γ2,η2,E‖u2‖1+c21‖K(A2E,AE)u2+γ2e‖1+c22‖η2‖1+σe⊤Ees.t.-(K(A2E,AE)u1+γ1e)⩾e-η1,η1⩾0,K(A1E,AE)u2+γ2e⩾e-η2,η2⩾0,E=diag(1or0).Without loss of generality, we just consider to solve the nonlinear one (20), and the linear one can be obtained as a special case by using a linear kernel. As remarked earlier, the prediction of a sample by FTSVM is on the basis of the proximity between the sample and two surfacesh1,2(x)=0, and each objective in the MOMIPP (20) corresponds to one surface. Therefore, the two objectives in (20) has the same level for consideration. A natural idea is combining two objectives to one as follows(21)minu1,u2,γ1,γ2,η1,η2,Eσe⊤Ee+λ(‖u1‖1+c11‖K(A1E,AE)u1+γ1e‖1+c12‖η1‖1)+(1-λ)(‖u2‖1+c21‖K(A2E,AE)u2+γ2e‖1+c22‖η2‖1)s.t.-(K(A2E,AE)u1+γ1e)⩾e-η1,η1⩾0,K(A1E,AE)u2+γ2e⩾e-η2,η2⩾0,E=diag(1or0),whereλ∈(0,1)is a parameter to balance the two objectives of (20).There are many ways to solve the MOMIPP [27,10,4,40,28]. In this paper, we hire alternate iterative greedy algorithm [23] to find a local optimal value. To be more specific, the solution of (21) is obtained by fixing the integer part E to get the other real partuiandγi,i=1,2, which leads to solve two LPPs the same asL1-TWSVM in Section 3, and then fixing the real part to get the integer part which leads to exhaustively compute the objective for the possible E, alternately, until meet the terminate condition.To make the greedy algorithm work, the feature selection matrix E should be initialized first. Here, we propose a strategy to initialize E instead of randomly initializing it in RFSVM [23] to make the algorithm more stable. Specifically, the value of each feature is computed after solvingL1-TWSVM by(22)Valuefeature(i)=λ(c11‖K(A1(i),A(i))u1+γ1e‖1+c12e⊤(K(A2(i),A(i))u1+γ1e+e)+)+(1-λ)(c21||K(A2(i),A(i))u2+γ2e‖1+c22e⊤(e-K(A1(i),A(i))u2-γ2e)+),where(·)+replaces the negative elements with zeros,A1(i)means the ith column ofA1, so does theA2(i)andA(i). The score of the ith feature that reflects the importance among all the features is computed by(23)Scorefeature(i)=Valuefeature(i)∑jValuefeature(j).Because our FTSVM aims at minimizing (21) by the selected features denoted by E, a feature has a larger value by (22) (or it’s percentage of all features by (23)) would actually have additional effect on the minimum of (21). Note that for the selected features,σe⊤Eein (21) is a constant, and then this part is ignored in (22).Our greedy algorithm starts from an initialE(0)generated by (23) (ifScorefeature(i)is less than a certain threshold, typically1/n, thenEii(0)=0, otherwiseEii(0)=1), then one can easily solve (21) for the fixedE(0)by solving (20). When obtained(u1(0),γ1(0),u2(0),γ2(0)), the matrixE(1)is updated by replacing every diagonal element ofE(0)from 1 to 0 (or 0 to 1) if the objective of (21) decreases more than the tolerance. In our greedy algorithm, we set E changed n times from the first diagonal element to the end if the objective of (21) decreased less than the tolerance, else return to change the first element to the end, until none of the diagonal element in E can be changed to make the objective of (21) decrease more than the tolerance. After updated E,(u1,γ1,u2,γ2)can be updated again. The algorithm will be terminated if the objective of (21) decreased less than the tolerance. Using the sign(·)+, (21) can be simplified when a new(u1,γ1,u2,γ2)is obtained as follows(24)f(E)=σe⊤Ee+λ(c11‖K(A1E,AE)u1+γ1e‖1+c12||(e+K(A2E,AE)u1+γ1e)+‖1)+(1-λ)(c21‖K(A2E,AE)u2+γ2e‖1+c22||(e-K(A1E,AE)u2-γ2e)+‖1).We summarize the procedures of the greedy approach in Algrithm 1 to give our feature selection method.Algorithm 1Feature selection method forL1-TWSVM (FTSVM).Input: The training setA∈Rm×n. The appropriate parametersc11,c12,c21,c22,σ(and a kernel function for nonlinear case). Pick a fixed integer k, typically very large, for the number of sweeps through E, and a stopping tolerance tol, typically1e-6.Output:u1,γ1,u2,γ2,E.1: SetE=I, solve the LPPs (20) with the fixed E;2: Compute each feature score by (23), fori=1,…,n, ifScorefeature(i)<1/n, then setEii=0.3: Solve the LPPs in (20) with the fixed E, and denote its solution by(u1,γ1,u2,γ2).4: repeat5: Forl=1,2,…,knandj=1+(l-1)modn:(a) ReplaceEjjby 0 if it is 1 and by 1 if it is 0.(b) Compute the value (24) before and after changing E.(c) Keep the newEjjonly if the objective function (24) decrease more than tol. Else undo the change ofEjj, and goto (a) ifj<n.(d) Goto 5 if the total decrease in (24) is less than or equal to tol in the last n steps.6: Solve the LPPs in (20) with the new E, and compute the objective function (21) by the new solution(u1,γ1,u2,γ2).7: until the objective function decrease of (21) is less than tol if the solution(u1,γ1,u2,γ2)is changedThe time complexity of our approach to solve the MOMIPP (20) includes two parts, one is repeatedly computing (24) (step 5(b) in Algorithm 1) and the other is solving many (20) (step 6 in Algorithm 1). For the first part, it is easy to compute with the fixed(u1,γ1,u2,γ2),f(E)is computed no more than k times. For the other part, suppose it needs to solve t LPPs to converge a solution, the time complexity of this part is at mostO(t2mn+1)by using the well known simplex method [5] to solve the LPPs. Therefore, the time complexity of our approach is at mostO(t2mn+1). The number of the LPPs t would be decreased if the parameter tol is set larger, and in our experiments, we settol=1e-6and get a good result with the tolerable time consuming, anyone can increase tol to get a faster learning speed or decrease tol to get the more favorable features.In the following, we discuss the feature ranking (RTSVM) method based onL1-TWSVM. As discussed in Section 4.1, FTSVM searches for the optimal subspace spanned by the selected features that hyperplane is the closest to one class and the farthest from the other class in that subspace. One feature is selected and is important if it helps the classifier to find the optimal subspace, and the importance can be computed by (23). Owing to the contribution of the procedures of FTWSVM, the selected features can be straightly ranked by (23), which is is summarized in Algorithm 2.Algorithm 2Feature ranking method forL1-TWSVM (RTSVM).Input: The training setA∈Rm×n. The appropriate parametersc11,c12,c21,c22,σ(and a kernel function for nonlinear case). The feature selection matrix E.Output: The scores of the features.1: Solve each problem in (20) with the fixed E;2: Forl=1,2,…,n, Compute each feature score by (23).

@&#CONCLUSIONS@&#
In this paper, we have proposed a novel feature selection method (FTSVM) based on TWSVM withL1norm regularization. Different fromL1-SVM with only a single feature weight, the correspondingL1-TWSVM has two feature weights, resulting some difficulty for feature selection. Thus, a feature selection matrix is introduced, aiming at realizing feature selection. This approach leads to solve a multi-objective mixed integer programming problem by an alternate iterative greedy algorithm. As an adjunct to the feature selection, a feature ranking method (RTSVM) is also proposed. It is interesting to point out that different from many feature selection and ranking methods, both our FTSVM and RTSVM have the linear formations as well as the nonlinear ones. The preliminary experiments have demonstrated the efficiency of our FTSVM and RTSVM, especially for the nonlinear case. For practical convenience, we upload our FTSVM Matlab code upon http://www.optimal-group.org/Resource/FSTWSVM.html.For the further work, it is interesting to extend our FTSVM and RTSVM to the relevant nonparallel hyperplanes classifiers without any essential difficulties, e.g., GEPSVM [24], RTWSVM [31], NHSVM [29], STPMSVM [42], LSTPMSVM [34] and PPSVC [41]. Another practical topic is to find more efficient method to solve the MOMIPPs (18) and (20) since the current method is time consuming in some sense.