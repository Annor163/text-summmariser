@&#MAIN-TITLE@&#
Intelligibility enhancement of HMM-generated speech in additive noise by modifying Mel cepstral coefficients to increase the glimpse proportion

@&#HIGHLIGHTS@&#
Glimpse based modification boosts the 1–4kHz area particularly for vowels and nasals.Coarser frequency Mel cepstral modifications are more intelligible.In speech-shaped noise proposed modification is as intelligible as Lombard adaptation.In competing speaker spectral modification and adaptation are not as effective.

@&#KEYPHRASES@&#
Intelligibility of speech in noise,HMM-based speech synthesis,Mel cepstral coefficients,Glimpse proportion measure,

@&#ABSTRACT@&#
This paper describes speech intelligibility enhancement for Hidden Markov Model (HMM) generated synthetic speech in noise. We present a method for modifying the Mel cepstral coefficients generated by statistical parametric models that have been trained on plain speech. We update these coefficients such that the glimpse proportion – an objective measure of the intelligibility of speech in noise – increases, while keeping the speech energy fixed. An acoustic analysis reveals that the modified speech is boosted in the region 1–4kHz, particularly for vowels, nasals and approximants. Results from listening tests employing speech-shaped noise show that the modified speech is as intelligible as a synthetic voice trained on plain speech whose duration, Mel cepstral coefficients and excitation signal parameters have been adapted to Lombard speech from the same speaker. Our proposed method does not require these additional recordings of Lombard speech. In the presence of a competing talker, both modification and adaptation of spectral coefficients give more modest gains.

@&#INTRODUCTION@&#
In a conversation, humans vary the way they perceive and produce speech according to context. Humans are able to modulate various properties of their speech in order to maintain successful communication in varying contexts, including changes in the listening environment (Picheny et al., 1985; Summers et al., 1988; Lindblom, 1990; Howell et al., 2006; Patel and Schell, 2008; Cooke and Lu, 2010).For machines, that communicate using speech, to achieve human-like levels of intelligibility in varying contexts, they must also adjust appropriate properties of their speech output. Currently, systems such as text-to-speech (TTS) synthesisers are ‘deaf’ to the context: they speak in the same way, regardless of the listening environment.In this work we focus solely on automatic strategies to improve speech intelligibility in the presence of additive background noise. In particular we are interested in increasing the intelligibility in noise of synthetic speech generated by a TTS system which employs statistical parametric models – ‘Hidden Markov Model (HMM)-based’ speech synthesis (Zen et al., 2009). We work under the assumption that the noise signal is available and under the constraint that the signal to noise ratio (SNR) should remain fixed i.e., our intelligibility enhancement method should not merely increase the overall energy level of an utterance. An additional important design constraint is to create intelligible voices without relying on the availability of natural speech produced under matched conditions, since this approach is unlikely to scale to different SNRs and noise types. Our proposed method requires only conventional speech recordings made in quiet conditions, of the type normally used to build TTS systems.In a quiet listening environment, the intelligibility of state-of-the-art HMM-generated synthetic speech can be as good as that of natural speech (Yamagishi et al., 2008). However, in noisy environments, unmodified synthetic speech tends to reduce in intelligibility to a much greater extent than unmodified natural speech (King and Karaiskos, 2010). By modifying the synthetic speech, either via the statistical models or the acoustic features, it is possible to control the characteristics of the generated speech without the need for new data and so generate synthetic speech that is more intelligible in noise than the natural speech used for training (King and Karaiskos, 2010; Suni et al., 2010; Bonardo and Zovato, 2007). One way to do this is to reproduce some of the acoustic changes observed, in many previous studies, of natural speech produced in noise: so-called Lombard speech. Another strategy is to use this data through voice conversion techniques (Langner and Black, 2005) and adaptation techniques (Raitio et al., 2011). Research has also been conducted into the generation of hyperarticulated synthetic voices (Picart and Drugman, 2011; Nicolao et al., 2012) that come under the category of clear speech rather than speech specifically produced to counteract the effects of noise.Lombard speech is speech produced by a talker who is simultaneously listening to noise. It is more intelligible than speech produced in quiet, when each are played to listeners mixed with the same noise and at the same SNR that the talker was experiencing (Summers et al., 1988; Junqua, 1993; Lu and Cooke, 2008). It has also been found that Lombard speech has distinct acoustic differences to speech produced in quiet: overall intensity increases, fundamental frequency increases, flatter spectral tilt (more energy at high frequencies), speaking rate changes (longer vowels/shorter consonants) and formants tend to shift (F1 increased F2 decreased) (Summers et al., 1988; Junqua, 1993; Hansen, 1996; Garnier et al., 2006; Lu and Cooke, 2008). It remains relatively unclear which of these acoustic changes improve intelligibility (and why), or how and to what extent these changes depend on the noise signal. As a consequence, it is non-trivial to use the known properties of Lombard speech to automatically improve the intelligibility of (synthetic) speech in noise.In this work we present a method to increase the intelligibility of synthetic speech in noise. The method modifies speech automatically according to the known noise characteristics. Rather than using knowledge about speech production in noise (e.g., Lombard speech, as above), we use well-established models of speech perception in noise. Using these models we can obtain reliable estimates of the impact that noise has on the intelligibility of speech; these models also give reliable estimates for modified speech.Approaches that have been proposed to modify natural speech according to the noise signal include: modification of the local signal-to-noise ratio (SNR) (Sauert and Vary, 2006; Tang and Cooke, 2010) and objective measure based spectral power optimisation using the Speech Intelligibility Index (Sauert and Vary, 2011), a genetic algorithm optimisation for spectral weighting based on the glimpse proportion (GP) (Cooke, 2006; Tang and Cooke, 2012) and an optimisation algorithm using a spectro-temporal measure based on a multi stage perceptual model Taal et al. (2012).In previous work, we have demonstrated that simple changes in the spectral domain (McLoughlin and Chance, 1997) can result in significant gains in intelligibility for HMM-generated synthetic speech across a variety of noise and SNR conditions (Valentini-Botinhao et al., 2011a). On the other hand, changes to fundamental frequency and spectral peaks were not as effective. In the same study we also evaluated a number of objective intelligibility measures, to discover how well they could predict these gains. Taking into consideration both performance and computational complexity, we selected the glimpse proportion (GP) measure (Cooke, 2006) as being most suitable for the current task. We then proposed a method to extract cepstral coefficients which maximized the GP measure (Valentini-Botinhao et al., 2012a). Although we obtained intelligibility gains using these extracted cepstral coefficients to train a TTS voice, we also observed distortions in the synthetic speech. Because this method is applied at feature extraction time it requires a new model to be trained for each different noise type (and potentially each SNR) and consequentially cannot handle fluctuating noise. Our solution to this was to modify the generated speech instead (Valentini-Botinhao et al., 2012b), by modifying the Mel cepstral coefficients. In this paper we present the full development of this idea plus a detailed analysis of how the modification actually changes the synthetic speech. We also provide the complete mathematical derivation of the method.In Section 2 we define the utilized Mel cepstral coefficients and Section 3 we show how the GP measure operates. Section 4 shows in detail how we reformulate GP so that it is completely defined by the Mel cepstral coefficients, then Section 5 describes how to use this approximated measure as part of a method to modify these coefficients. We then present in Sections 6 and 7 convergence and acoustic analyses as well as subjective intelligibility scores for two different listening evaluations. Conclusions are given in Section 8.We can represent the spectrum of speech H(ejω) by a M-th order Mel cepstral coefficient set{cm}m=0M(Fukada et al., 1992):(1)H(ejω)=exp∑m=0Mcme−jmω˜(2)ω˜=tan−1(1−α2)sinω(1+α2)cosω−2αwhere α is the warping factor that controls the frequency scaling.We can choose α such thatω˜spans the frequency axis on a particular scale, such as for instance the Mel scale, creating so-called Mel cepstral coefficients (Fukada et al., 1992). When using the Mel scale warping we can represent the spectrum envelope with fewer coefficients than when using a linear frequency scale, without a loss in quality.According to Eq. (1), the magnitude spectrum is defined by the Mel cepstral coefficients as follows:(3)|H(ejω)|=exp∑m=0Mcmcos(mω˜)The glimpse proportion measure was originally proposed in the context of the Glimpse model for speech perception in noise (Cooke, 2003). The model is based on the ability of humans to obtain information from those time-frequency regions where speech is less masked by noise and therefore less distorted (Cooke, 2003).The GP measure (Cooke, 2006) is based on this concept: in a noisy environment, humans focus their auditory attention on ‘glimpses’ of speech that are not masked by noise. Rather than being a correlation, a distance or a ratio, the GP is based on detection: to measure the number of available glimpses of a given speech signal in a given noise, we need the speech and noise signals to be available separately.The GP correlates well with subjective scores for intelligibility of natural speech in noise (Cooke, 2006). In our own experiments, we also observed similar behaviour for the intelligibility of HMM-generated speech in noise (Valentini-Botinhao et al., 2011b) even when that speech has been modified (Valentini-Botinhao et al., 2011a). In that experiment, we modified parameters such as the fundamental frequency (F0) and spectral tilt to emulate Lombard speech properties; even under such modifications, GP was a reasonable intelligibility predictor (Valentini-Botinhao et al., 2011a). In all these different scenarios, GP outperformed most other measures in terms of accurate predictions of intelligibility of speech in noise. An attractive property of GP is that its implementation does not require any time delays.The GP measure is simply the proportion of spectro-temporal regions, so called ‘glimpses’, where speech is more energetic than noise. To detect such glimpses we compare speech and noise using the spectro-temporal excitation pattern (STEP) representation as shown in Fig. 1. To represent a signal in terms of STEP – see Fig. 2– we first decompose its waveform into different frequency channels using a Gammatone filterbank whose central frequencies are linearly spaced on the equivalent rectangular bandwidth (ERB) scale (Moore and Glasberg, 1996). For each channel, the temporal envelope is extracted with an absolute value operation, smoothed with a low pass filter and then averaged across limited time intervals. A glimpse is detected in a time frequency region when the speech STEP value in that region is higher than the noise value.The parameters that define the GP measure are: the range of the Gammatone filters’ centre frequencies (100–7500Hz), the number of Gammatone filters Nf(55 filters), the temporal integration time for the smoothing filter (8ms), the size of the time frame (30ms) and its period (10ms).In this section we show how we can approximate the GP measure so that it is completely defined by the short term magnitude spectrum of speech and consequently by the sequence of Mel cepstral coefficients.To obtain a closed and differentiable formula that relates spectral parameters to the GP measure we make the following approximations and correspondences:•the input signals are no longer the signal waveforms of speech and noise but the short term magnitude spectrum calculated from the short-time Mel cepstral coefficients of speech and from the short-time discrete Fourier transform of noise (approximation).the previous approximation implies that all operations are carried out in the frequency domain rather than the time domain (correspondence).the filtering operations in the time domain are replaced by multiplications in the frequency domain with a truncated version of the frequency responses of the infinite impulse response filters (approximation due to the truncation).the absolute value in the time domain is replaced by a power operation that can be represented in the frequency domain as the circular convolution operation (approximation).the hard threshold detection of glimpses is replaced by a soft decision threshold defined by a sigmoid function (generalization).The proposed approximated GP measure is then given by:(4)GP=100NfNt∑t=1Nt∑f=1NfL(yt,fsp−yt,fns)where the following scalars are defined as:yt,fspSTEP approximation for speech, at analysis window t and frequency channel f,yt,fnsSTEP approximation for noise, at analysis window t and frequency channel f, Ntnumber of time frames, Nfnumber of frequency channels, t analysis window index, f frequency channel index,L(·)a logistic sigmoid function defined as:(5)L(x)=11+e−ηxwhere η defines the slope of the curve.The STEP approximation as seen in Fig. 3is given by:(6)where N is the number of frequency bins of the spectrum,ht=, [|Ht(ω1)|…|Ht(ωN)|]⊤, vector Nx1 – magnitude spectrum of windowed speech signalsat analysis window t;Gf= diag([gf,1…gf,N]), matrix NxN – diagonal matrix, diagonal contains the Gammatone filter frequency response for frequency channel fS=diag([s1…sN]), matrix NxN – diagonal matrix, diagonal contain the frequency response of the smoothing filter;b=[b1…bN] vector Nx1 – coefficients of average filter;=circular convolution operation dimension N.In this section we show how to modify a sequence of Mel cepstral coefficients generated by a statistical model, such that the GP measure increases.11Although the formulation of the problem allows for the extension to other types of spectral parametrization such as the Mel Generalized Cepstral coefficients (MGC) (Koishida et al., 1996) we can not guarantee that the synthesis filter created from such modified MGCs is stable. Stability is always guaranteed for any value of Mel cepstral coefficients. To modify the MGC parameters it would be necessary to first transform them into a representation where stability is easily ensured like the MGC-LSP as proposed in (Koishida et al., 2000).To increase glimpses in a certain analysis window t we first define the following cost function:(7)GPt(ct)=100Nf∑f=1NfL(yt,fsp(ct)−yt,fns)The optimal spectral parameter vectorct=[ct,1ct,2…ct,M]⊤ would then be given by:(8)ct*=argmaxGPt(ct)The update equation for Mel cepstral coefficients using steepest descent is given by:(9)c(i+1)=c(i)+μΔc(i)(10)=c(i)+μ∇GPt(i)(ct)whereΔc(i) is the Mel cepstral coefficient increment in iteration i,∇GPt(i)(ct)is the gradient of the function defined in Eq. (7) in iteration i and μ is the stepsize. From now on we will drop the iteration index (i) and the argument (ct) for clarity. We can find the gradient vector as follows:(11)∇GPt=∂GPt∂ct=100Nf∑f=1Nf∂L(yt,fsp−yt,fns)∂ct(12)=100Nf∑f=1Nf∂L(yt,fsp−yt,fns)∂yt,fsp∂yt,fsp∂ctThe first term in the summation of eq.(12) can be written as:(13)∂L(yt,fsp−yt,fns)∂yt,fsp=ηL(yt,fsp−yt,fns)[1−L(yt,fsp−yt,fns)]The second term in the summation of Eq. (12) is given by:(14)∂yt,fsp∂ct=∂ht∂ct∂yt,fsp∂htThe first term on the right of Eq. (14) is a matrix of dimension MxN defined as:Hct≡∂ht∂ct=∂|Ht(ω1)|∂ct,1∂|Ht(ω2)|∂ct,1…∂|Ht(ωN)|∂ct,1∂|Ht(ω1)|∂ct,2∂|Ht(ω2)|∂ct,2…∂|Ht(ωN)|∂ct,2⋮∂|Ht(ω1)|∂ct,M∂|Ht(ω2)|∂ct,M…∂|Ht(ωN)|∂ct,MWhen the spectrum is modelled by Mel cepstral coefficients as in Eq. (1), the elements of this matrix are defined as:(15){Hct}m,k=∂|Ht(ωk)|∂ct,m=|Ht(ωk)|cos(mω˜k)where k is the index for the spectrum frequency bin and m as defined previously is the index for the Mel cepstral coefficient sequence. The second term of Eq. (14) depends on the definition of the STEP approximation in Eq. (6) and it is then given by:(16)∂yt,fsp∂ht=∂lt,f∂ht∂yt,fsp∂lt,f(17)=1N∂lt,f∂htSb(18)=1N∂Gfht∂ht∂lt,f∂GfhtSb(19)=1NGf∂lt,f∂GfhtSb(20)wherelt,f=(GfhtGfht) andΓNis the identity matrix of dimension N. The operation (ΓNGfht) defines a matrix NxN of the following form:whereenis the n-th column of the identity matrixΓN.Connecting Eqs. (13), (15) and (20), the gradient vector is given by:(21)∇GPt=100NfN∑f=1NfηL(yt,fsp−yt,fns)[1−L(yt,fsp−yt,fns)]In this section we explain how to reformulate the optimization problem in order to keep the overall energy of speech constant. For clarity, we drop the index t in the equations and use the continuous representation of the spectrum H(ejω).Let us first define the quantity we refer to here as the overall energy of speech in a certain time frame:(22)∑n=0N−1|s(n)|2=ψFrom Parseval's theorem we have that:(23)∑n=0N−1|s(n)|2=12π∫−ππ|S(ejω)|2dω=ψwhere S(ejω) is the discrete time Fourier transform of time signal s(n).This can be related to the spectral envelope H(ejω) and the frequency representation E(ejω) of the excitation signal:(24)ψ=12π∫−ππ|H(ejω)E(ejω)|2dωWe can assume that |E(ejω)| is constant over the frequency domain for both voiced and unvoiced segments. For voiced speech segments this is true if the size of the analysis window is set to two pitch periods and for unvoiced segments this is true because at these segments the excitation signal is white noise. Under this assumption and considering that the cepstral extraction method does not modify the excitation signal we can assume that in order to keep the energy in the time domain constant it is sufficient to keep the following constant:(25)ψ=12π∫−ππ|H(ejω)|2dωThe maximization of the glimpse function as given in Eq. (8) should then be solved subject to the above constraint. Solving a nonconvex optimization problem is however a hard task. One feasible solution is to perform at each iteration of the Steepest Descent method an energy normalization operation and alter the objective function and consequentially the gradient vector accordingly. Fig. 4shows this solution. To explain how the gradient should be modified we first need to define the operation that normalizes the energy of the spectrum.The following operation modifies the spectrum |H(ejω)| with overall energy ψ so that the resulting spectrum |H′(ejω)| has an overall energy equal to ψ′:(26)|H′(ejω)|=|H(ejω)|(1/ψ′)∫−ππ|H(ejω)|2dω=|H(ejω)|(ψ/ψ′)In order to modify the gradient we need to calculate the impact of this operation in the Mel cepstral coefficient domain. The normalization operation transforms a set of Mel cepstral coefficients cmthat model the spectrum |H(ejω)| with overall energy ψ, into parameterscm′that model a spectrum |H′(ejω)| with overall energy equal to ψ′ in the following way:(27)|H′(ejω)|=|H(ejω)|(ψ/ψ′)=exp∑m=0Mcmcos(mω˜)exp[log(ψ/ψ′)](28)=exp∑m=0Mcmcos(mω˜)−0.5logψψ′(29)=exp∑m=0Mcm′cos(mω˜)The energy-normalized Mel cepstral coefficientscm′are then given by:(30)cm′=c0−0.5logψψ′m=0cmm≠0Only the c0 coefficient changes, so we can write the energy normalized magnitude spectrum as:(31)|H′(ejω)|=|K′||D(ejω)|whereK′=exp(c0′)andD(ejω)=exp∑m=1Mcme−jmω˜.If ψ is equal to ψ′, i.e. the energy-normalization operation has no impact on the spectrum, we can see thatcm′is equal to cm. The only term in the gradient vector∇GP that needs to be adjusted is the one given by Eq. (15). To show how this term changes we adopt the discrete representation H(ω1), …, H(ωN) of the spectrum. Eq. (25) is then approximated to:(32)ψ=∑j=1N|H(ωk)|2With the energy normalization operation, the derivative in Eq. (15) becomes:(33)∂|H′(ωk)|∂cm=∂|K′||D(ωk)|∂cm(34)=∂|K′|∂cm|D(ωk)|+|K′|∂|D(ωk)|∂cm(35)=|K′|∂c0′∂cm|D(ωk)|+|K′||D(ωk)|cos(mω˜k)(36)=|H′(ωk)|∂c0′∂cm+|H′(ωk)|cos(mω˜k)(37)=|H′(ωk)|∂c0′∂cm+cos(mω˜k)The derivative term in the previous equation is given by:(38)∂c0′∂cm=∂c0∂cm−0.5ψ′ψ1ψ′∂ψ∂cm(39)=∂c0∂cm−1ψ∑l=1N|H(ωl)|2cos(mω˜l)(40)=∂c0∂cm−1ψ′∑l=1N|H′(ωl)|2cos(mω˜l)(41)∂c0′∂cm=0.0m=0−1ψ′∑l=1N|H′(ωl)|2cos(mω˜l)m≠0The derivative in Eq. (15) becomes then:(42)∂|H′(ωk)|∂cm=|H′(ωk)|m=0|H′(ωk)|cos(mω˜k)−1ψ′∑l=1N|H′(ωl)|2cos(mω˜l)m≠0Using this new gradient calculation, and normalising the speech energy at each iteration, guarantees that the energy of the speech signal is fixed during gradient descent optimization. Because the optimization is performed per analysis window, the energy of each window will not change, meaning that there is no reallocation of energy across windows and that the maximisation of the GP is bounded by the amount of energy initially available in the analysis window.A detection based measure like the GP or a ration based measure like the SNR predicts the effect of additive distortions by comparing the levels of speech and the distortion (in this case noise), not requiring any reference undistorted speech signal. These measures cannot predict the effect that modifying speech has on the intelligibility of the noisy mixture. An issue we face then when using the GP measure as an optimization criterion on its own is the need to limit the distortions caused by the modifications. Recent research on improving the GP measure so it can predict intelligibility of modified speech is described in Tang et al. (2013). Our current work is however based on the original measure (Cooke, 2006).To define an audible distortion, we use the Euclidian distance between the STEP representations of modified and unmodified speech. Including this as an explicit constraint is unfortunately rather cumbersome, so instead we use it as a stopping criterion. We also hypothesize that limiting the frequency resolution of the modifications should generate fewer distortions. This is implemented simply by setting the gradient vector for higher dimensions to zero, and so the method modifies only the first few Mel cepstral coefficients, which represent the coarse properties of the spectrum.In this section we present the details of how the statistical parametric models were built, an analysis of convergence, an acoustic analysis, then the design and results of our first listening experiment. In this first experiment we also test the idea of restricting the frequency resolution of the modifications by updating only the first few Mel cepstral coefficients.We used two different datasets recorded by the same British male speaker: normal (plain, read-text) speech data and Lombard speech. The Lombard speech was recorded while speech-modulated noise (modulated by the speech from a different male speaker (Dreschler et al., 2001)) was played over headphones at an absolute value of 84dBA.Table 1presents the eight different voices we built for this evaluation. The baseline unmodified voice N was created from a high quality average voice model adapted to 2803 sentences of the normal speech database (3h of material). Building a speaker-dependent voice was feasible however the normal speech dataset was not sufficiently phonetically balanced due to the reading material used for the recordings and hence we have decided to use the adaptive approach (Yamagishi et al., 2009). The modified voices N-M59, N-M10 and N-M2 were created from voice N by modifying all, just the first ten (c1 until c10), or just the first two (c1 and c2) Mel cepstral coefficients using the proposed method, as described in the previous section.We built the other set of voices N-L, L, L-E and L-E-M2 using the Lombard speech portion of the database in addition. Lombard voice L was built by further adapting all parameters (duration, excitation, spectral) of voice N using 780 sentences from the Lombard speech dataset (53min). The reason for not building a voice only with the Lombard dataset was again the lack of phonetic balance in the dataset. Voice N-L was also created from voice N by adapting this time only the Mel cepstral coefficients (i.e., spectral model parameters) to the Lombard data. Voices L-E and L-E-M2 are versions of voice L where we extrapolated the adaptation in all dimensions at an extrapolation factor of 1.2 for Mel cepstral coefficients and 1.35 for duration (voice L-E), and then further modified the two first Mel cepstral using the proposed method (voice L-E-M2).We trained and adapted the models using the described data sampled at a rate of 48kHz. We extracted the following acoustic features: 59 Mel cepstral coefficients (α=0.77), Mel scale F0 and 25 aperiodicity energy bands extracted using STRAIGHT (Kawahara et al., 1999). We used a hidden semi-Markov model as the acoustic model. The observation vectors for the spectral and excitation parameters contained static, delta and delta–delta values, with one stream for the spectrum, three streams for F0 and one for the band-limited aperiodicity. We applied the Global Variance method (Toda and Tokuda, 2007) to compensate for the over smoothing effect caused by the statistical nature of the acoustical modelling. For the GP-based Mel cepstral modifications we set the following values for the STEP calculation: 55 Gammatone filters with centre frequencies covering the range of 50–7500Hz (because the noise signal used for testing was sampled at 16kHz, and so the audio bandwidth was 8kHz), 8ms of temporal integration time for the smoothing filter and frame length and period of 30 and 10ms. For the steepest descent optimization we used a normalized step size defined at each iteration i as μ(i)=μ/||∇GPt(i)||(where μ=0.4 for N-M59 and μ=0.8 for N-M10 and N-M2). As stopping criteria we use both error convergence and a maximum threshold set to 10% of relative increase in distortion. We define distortion here as the Euclidian distance between the original and the modified STEP representation of speech.Fig. 5shows the convergence of the GP and distortion values. We can see that, as GP increases, distortion also increases as expected, and that the algorithm is well-behaved (i.e., it converges to a stable value within a reasonable number of iterations). The algorithm is frame-based, meaning that the stopping criteria are applied on a per-frame basis. For individual frames, the convergence is somewhat less smooth-looking than that illustrated in the figure. On average, 5 iterations are sufficient to meet one or other of the stopping criteria for each frame, and more often than not it is the distortion criterion that is met.We now examine the impact of the modification at sentence and phone unit levels in terms of GP values and the long term average spectrum (LTAS). The LTAS is calculated as the power spectral density averaged across time frames of 10ms length and 50% overlap. This averaged representation is then presented in (dB). First we present a broad analysis across the whole set of sentences used in the listening experiment. Table 2shows the average duration of speech and pauses, average F0 and average spectral tilt across all sentences used in the listening test for the normal (N), modified (N-M2) and Lombard (L) voices. As expected, the Lombard voice produces sentences with longer duration and longer pauses, greatly increased F0 mean and flatter spectral tilt. The modified voice N-M2 produces speech with flatter spectral tilt, though not to the same extent as the Lombard voice.For a more detailed inspection of the proposed method in operation, Fig. 6shows the glimpses (in black) detected in the presence of speech-shaped noise at −4dB SNR for (from left to right) a sentence generated by the unmodified voice N and the modified voices N-M59, N-M10 and N-M2. The glimpses are shown in the STEP domain. We can see that the glimpsed regions become larger and that new glimpses start to appear when we modify all, just the first ten and the first two Mel cepstral coefficients. We also see that when we modify fewer coefficients, the new glimpses tend to be in more coherent regions, creating larger glimpses rather than scattered small glimpses. This is an expected and desired result of modifying only those coefficients that define the coarse shape of the log magnitude spectrum.Fig. 7shows the GP value for each frame as defined in Eq. (7) for the same sentence as shown in Fig. 6, generated by the unmodified voice N, the modified voice N-M2 and the Lombard adapted voice L. We observe that, although the number of glimpses on average increases, the increase in glimpses differs between segments. Since the noise that was driving this modification is stationary, this variation comes from the speech signal itself: the different spectral shapes of the various phonetic units will result in fewer or greater numbers of glimpses. In this example sentence, the number of glimpses hardly increases in fricatives and stops, whereas the most substantial increases happen in vowels and nasals. This does not mean that fricatives and stops are not being modified though, but does mean that the proposed method fails to create more glimpses of them for the listener. Although we are not aiming to recreate the Lombard effect, we present the curve obtained from the voice L in the bottom plot of Fig. 6. Compared to the GP gains obtained by voice N-L over voice N, the voice L has smaller GP gains during vowels while fricatives’ GP values are slightly higher.For a further detailed analysis we computed the gain in (dB) of the LTAS of voice N-M2 over and above the LTAS of the original unmodified voice N, averaged across all test sentences, for speech-shaped noise at −4dB. Fig. 8shows the overall pattern of spectral gain at a sentence level and Figs. 9 and 10present the gain calculated for different phonetic classes averaged over all tokens of that class in the test set.From Fig. 8, we observe that, compared to voice N, voice N-M2 exhibits enhanced energy in the region of 1–4kHz and attenuated below 1kHz. One clear observation we can make when comparing the gains for specific phone classes as displayed in Figs. 9 and 10 is that the curves as well as the gain values vary substantially across different phonetic classes. In the first group (vowels, nasals and approximants) the gains are at least five times larger than those obtained for the second group (fricatives, affricates and stops). This is a consequence of the shapes and values of the unmodified speech LTAS for these classes.From the gain curves of the first group displayed in Fig. 9 we can see a similar pattern across vowels, nasals and approximants: a large enhancement varying from 8 to 12dB in the frequency region between approximatively 800Hz (this number varies across the different classes) and 5kHz as well as a apparent attenuation of around 2dB for the lower frequency region. For both vowels and approximants we see also a clear gain region between 5 and 8kHz that is separated by a gain valley at approximately 5kHz. The shapes of these gain curves follow the shape of the LTAS of these phonetic classes, for instance we can see a bump from 5 to 8kHz in the vowels and approximants. The nasals are the units that are most strongly enhanced reaching a maximum of 12dB gain which can be explained by the fact that they seem to be highly energetic with an even less flat spectrum than the other sounds.A similar trend for vowels, nasals and liquids can be seen in a study performed on Lombard speech of 5 male Spanish native speakers Castellanos et al. (1996) although interestingly we did not find this trend in the Lombard database that we recorded from our speaker.The gains obtained for the other class (stops, fricative and affricates) as shown in Fig. 10 are, as previously stated, much smaller. For both stops and fricatives an average maximum of 2dB increase was found and the region most enhanced is between 1 and 5kHz as seen for the other group. The affricates show even lower gains and narrow enhanced regions between 1 and 3kHz with a valley around 2kHz.In this listening test we evaluated the intelligibility of the eight different synthetic voices listed in Table 1 mixed with two noises: speech-shaped (ssn) and speech from a single competing female speaker (cs). To obtain similar intelligibility scores across each noise and to avoid ceiling effects, we mixed each noise at two different SNRs: −4dB for ssn and −14dB for the cs. These SNRs were chosen to give approximately 50% word accuracy for natural speech of the same speaker with the same material (Cooke et al., 2013).32 native English speakers listened to the noisy samples over headphones in soundproof booths. Each participant typed in what he or she heard for a total of six different sentences per condition, i.e., voice and noise type (16 conditions). Each sentence could only be played once and the same sentence was never played again in the same listening test. We used the first ten sets of the Harvard sentences (IEEE, 1969). The Harvard sentences are a group of 720 sentences organized in sets of 10, where each set is designed to be phonetically balanced. The sentences are also more representative of everyday speech than the semantically unpredictable sentences used in other TTS intelligibility listening experiments (King and Karaiskos, 2010). Another one of the sets was used as a practice session done prior to the experiment. All words were considered when calculating the subjective word accuracy rate.

@&#CONCLUSIONS@&#
We have presented a method for increasing the intelligibility of HMM-generated synthetic speech in the presence of noise, based on the glimpse proportion measure. The method operates on the Mel cepstral coefficients generated by acoustic models which have been previously trained only on natural read speech collected in quiet conditions, of the type normally used to build text-to-speech systems. The method updates the Mel cepstral coefficients iteratively via gradient descent such that the glimpse proportion increases, without changing the overall energy. We have observed that sentences generated with such modified Mel cepstral coefficients have a boost in frequencies between 1–4kHz and that this boost is highly dependent on the phonetic units (vowels and nasals are more enhanced than fricatives and stops). Results with a speech-shaped noise masker show that the modified voice is as intelligible as a synthetic voice trained with plain speech then adapted to Lombard speech. When mixed with a competing talker the gains are more modest for both the proposed method and for adaptation to Lombard speech.