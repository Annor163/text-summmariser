@&#MAIN-TITLE@&#
Sonification of in-vehicle interface reduces gaze movements under dual-task condition

@&#HIGHLIGHTS@&#
We studied the impact of using a sonified or silent interface while performing a visual primary task.We measured participants gaze and performance to both primary and secondary tasks.Participants used the sonified interface nearly exclusively be ear while performing the primary task.The reaction times in the primary task were increased in both sonified and silent conditions.

@&#KEYPHRASES@&#
Sonification,Dual-task,Eye-tracking,

@&#ABSTRACT@&#
In-car infotainment systems (ICIS) often degrade driving performances since they divert the driver's gaze from the driving scene. Sonification of hierarchical menus (such as those found in most ICIS) is examined in this paper as one possible solution to reduce gaze movements towards the visual display. In a dual-task experiment in the laboratory, 46 participants were requested to prioritize a primary task (a continuous target detection task) and to simultaneously navigate in a realistic mock-up of an ICIS, either sonified or not. Results indicated that sonification significantly increased the time spent looking at the primary task, and significantly decreased the number and the duration of gaze saccades towards the ICIS. In other words, the sonified ICIS could be used nearly exclusively by ear. On the other hand, the reaction times in the primary task were increased in both silent and sonified conditions. This study suggests that sonification of secondary tasks while driving could improve the driver's visual attention of the driving scene.

@&#INTRODUCTION@&#
According to recent surveys (http://www.drive-safely.net/top-ten-driving-distractions.html), the use of external devices such as a telephone, a music player, or the radio is responsible for most driving distractions. Sending a text message or having a phone conversation while driving is undoubtedly unsafe and can have fatal consequences (Victoria et al., 2013). In this paper we focus on the use of in-car infotainment systems (ICIS), i.e., a device installed in the car to provide services such as navigation systems, music player or telephone. To minimize its impact on driver's visual attention, the ICIS is designed to have a visual display located in the middle of the dashboard, and a control device close to the gear stick. In the US, the National Highway Traffic Safety Administration (NHTSA) has recently published a series of guidelines for designing in-car devices with a limited impact on driver's visual attention (NHTSA, 2013). Nevertheless, the use of such technologies in cars is still responsible for frequent off-road glances: Sodhi et al. (2002) found that tuning the radio while driving resulted in 42% of off-road glances (total task time of 21.1 s on average), Young et al. (2012) showed that using a portable music player not only resulted in increased off-road glances but also reduced a driver's ability to maintain a constant line position. Kujala et al. (2013) found that most tasks performed on a touch screen while driving increased the number of rapid steering wheel movements. New solutions to further reduce even more the number and the duration of off-road glances are required.Car driving is primarily a visual task (Sivak, 1996), whereas using an ICIS can rely on visual, auditory or both modalities. Visual-only ICIS has been largely addressed in the field of HMI ergonomics that provided design guidelines for improved information presentation (Singleton, 1971; Bastien and Scapin, 1992; Scapin and Bastien, 1997; Ziefle, 2010). Recently, Mitsopoulos-Rubens et al. (2011) demonstrated that list scrolling while driving significantly impaired driving performance (mean lane deviation and percentage of correct lane changes), but no gaze data were provided. Using an eye-tracker, Rydström et al. (2012) showed that both a touch screen and a rotary knob affected lateral control performance while doing alphanumeric input or list scrolling. Based on the occlusion technique, Baumann et al. (2004) indicated that a task performed with in-car navigation system must be portionable in 1–2 s chunks and the total task time is insufficient to evaluate the impact on driving performance. In other words, even if an ICIS has been optimized to be less visually demanding, it is still responsible for off-road glances and thus could impair the driving performances.Sonification, as a way to display information using the only auditory modality, appears to be a strong candidate to reduce eye movements while driving. Introduced by Kramer (1992), sonification is based on at least three fundamental concepts: (1) earcon,11The name “earcon” comes from the term icon as an icon for the ear (Sumikawa, 1985).i.e., an abstract sound for which the sound/meaning relationship is arbitrary and must therefore be learned by the user (Blattner et al., 1989). Earcons are usually based on acoustic and musical characteristics: pitch, rhythm, duration, etc. (2) auditory icon (Gaver, 1986), i.e., a sound which establishes a direct link with the object or concept that it represents, by referring to an easily recognizable sound from our daily environment, and is accordingly almost directly comprehensible by the user. This approach is preferred in the context of driving since it reduces the time needed by the drivers to learn the sounds. (3) spearcon (speech-earcon), i.e., accelerated text to speech synthesis employed to facilitate fast scrolling in long menus (Walker et al., 2006, 2013).An ICIS is generally comprised of a small display presenting different menus and sub-menus organized in a large hierarchical structure. Sonification of hierarchical menus must address two issues: (1) sound representation of the hierarchical position of each item and (2) sound representation of the semantic content of each item. Earcon based sonifications are easily learned but not very well adapted to complex hierarchies (Brewster et al., 1995, 1998; Brewster, 1998). Leplâtre and Brewster (2000) and Leplâtre (2002) developed a new implementation of hierarchical earcons more closely linked to the hierarchical position of the items but too confusing in terms of semantic representation. On the other hand and as expected, auditory icons provide less arbitrary sonification as illustrated in Barrass (1998) and Conversy (1998). Auditory icons were also used in Gaver's Sonic Finder (Gaver, 1986), a computer auditory interface in which actions were mapped to everyday sound events (e.g., selecting a file mapped to the sound of an object being hit), added to another layer of information (e.g., file size with object size). In a previous study, we developed an original approach based on a combination of earcons and auditory icons (Langlois et al., 2010; Misdariis et al., 2011) and preference tests were performed in a driving simulator. The result was an improvement of the model achieved by adding synthesized speech at lower levels of the menu. A detailed presentation of this sonification is given in Section 2.5. The main limitation, the absence of gaze data, is addressed in the present paper.Using a dual task paradigm, Jeon et al. (2009) measured the benefits of sonification on the performance to a visual primary task (a ball catching game) while navigating in a list of items. Results showed that sonification significantly improved both reaction times in the primary task and search times in the secondary task. However, no gaze data were collected to confirm that sonification reduced the visual demands of the secondary task. Sodnik et al. (2008) studied the navigation inside mobile phone menus (sonified or not) while driving in a simulator. The sonification was based on earcons mixed with synthesized speech and virtually placed in the space using a surround sound system. Results showed that sonification decreased the unsafe driving behaviors (e.g. unexpected deceleration) but did not improve menu navigation.This review of related works reveals a lack of gaze data supporting the claim that sonification can efficiently reduce off-road glances while navigating in the menus of an ICIS. Our study addressed this issue with an experiment in which participants' gaze was measured while they performed two simultaneous tasks: (1) a visual primary task on a computer screen that simulates the sustained visual attention required when driving, and (2) a secondary navigation task in which participants navigated inside the menus of a realistic ICIS mock up (either sonified or not). The remainder of this article describes the experiment in Section 2, an analysis of the results in Section 3 and a discussion in Section 4.

@&#CONCLUSIONS@&#
