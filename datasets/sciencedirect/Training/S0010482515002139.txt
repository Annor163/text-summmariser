@&#MAIN-TITLE@&#
Hybrid facial image feature extraction and recognition for non-invasive chronic fatigue syndrome diagnosis

@&#HIGHLIGHTS@&#
We proposed a hybrid facial feature based on clinical observations of TCM experts.Feature components are extracted from different facial regions with proper methods.Feature reduction and fusion are performed before prediction of CFS.The feature got an error rate less than 17.31% in cross-validation experiments.The whole method achieves an average accuracy of 88.32% for CFS prediction.

@&#KEYPHRASES@&#
Chronic fatigue syndrome,Feature extraction,Hybrid facial feature,Manifold preserving projection,Non-invasive CFS diagnosis,

@&#ABSTRACT@&#
Due to an absence of reliable biochemical markers, the diagnosis of chronic fatigue syndrome (CFS) mainly relies on the clinical symptoms, and the experience and skill of the doctors currently. To improve objectivity and reduce work intensity, a hybrid facial feature is proposed. First, several kinds of appearance features are identified in different facial regions according to clinical observations of traditional Chinese medicine experts, including vertical striped wrinkles on the forehead, puffiness of the lower eyelid, the skin colour of the cheeks, nose and lips, and the shape of the mouth corner. Afterwards, such features are extracted and systematically combined to form a hybrid feature. We divide the face into several regions based on twelve active appearance model (AAM) feature points, and ten straight lines across them. Then, Gabor wavelet filtering, CIELab color components, threshold-based segmentation and curve fitting are applied to extract features, and Gabor features are reduced by a manifold preserving projection method. Finally, an AdaBoost based score level fusion of multi-modal features is performed after classification of each feature. Despite that the subjects involved in this trial are exclusively Chinese, the method achieves an average accuracy of 89.04% on the training set and 88.32% on the testing set based on the K-fold cross-validation. In addition, the method also possesses desirable sensitivity and specificity on CFS prediction.

@&#INTRODUCTION@&#
Chronic fatigue syndrome (CFS) is a persistent weakened condition associated with a variety of somatic and psychological symptoms [1]. The socio-economic impact of CFS is substantial given the chronic nature and seriousness of the illness. Chronic fatigue syndrome (CFS) is a condition characterized by impairment of neurocognitive functions and quality of sleep and of somatic symptoms such as recurrent sore throat, muscle aches, arthralgia, headache, and post exertional malaise [1]. The precise pathophysiology of this condition is currently unknown. In addition, there is no clear consensus with regard to changes in blood composition or genetic factors, such as polymorphisms, which may predispose certain individuals to CFS [2]. Therefore, due to the absence of reliable biochemical markers, currently, the diagnosis of CFS is based on clinical symptoms alone [3]. As this diagnostic procedure relies on the experience and skills of the individual making the judgment, CFS can be accurately diagnosed by only a limited number of skilled medical practitioners [3]. The most commonly used criteria for CFS is defined by the Centres for Disease Control and Prevention (CDC) [1] and is often criticized for being too inclusive and putting insufficient emphasis on cognitive dysfunction, abnormal fatigability and post exertion malaise [4]. Consequently, an additional non-invasive diagnostic method that enables objective judgment to be made is urgently needed [3].Roberts et al. [5] found that CFS patients had a lower cortisol response to awakening and used the salivary cortisol response to awakening to assess hypothalamic–pituitary–adrenal (HPA) axis to form a non-invasive diagnosis method of CFS, but there are several diseases that can cause cortisol response decrease, such as hypothyroidism. Hence, it is more suitable for this method to play a role as a non-invasive screening method for CFS.Li et al. [6] found that essential qi and spirit, colour and texture of the face, eye and lip have distinct differences after they had observed 1169 patients in chronic fatigue. Xu et al. [7] analysed the skin colour of the forehead, two cheeks and the nose, chin and lips based on CFS patients’ face images. They concluded that the visual difference between the faces of CFS patients and healthy people can be used to diagnose CFS.Actually, image processing technology has been used to recognize driver fatigue. However, most existing methods focus on analysing blink [8–10], gaze [8], yawn [11–13] and a combination of such factors [8,14,23]. Features that identify driver fatigue include percentage of eye lid closure over the pupil over time (PERCLOS), Average Eye Closure Speed (AECS), Gaze distribution (GAZEDIS), Percentage of Saccade (PERSAC) and Yawn Frequency (YawnFreq). Some researches have been done on drowsiness related facial expressions [15–17]. The method proposed in [17] represents a class of main methods. They extract the features of the entire face, decrease the dimensionality of the features and recognize fatigue by machine learning algorithms. In the researches mentioned above, outstanding facial behaviours, like eye blinking and yawning, etc., are regularly used as indicators for fatigue detection. However, these features do not present so obviously in a chronically fatigued person [2], so there is a need to find other facial features to substantially represent chronic fatigue and to develop a new method to detect CFS with these features.The facial appearances of a chronically fatigued individual and a fatigued driver have both similarities and differences. Most fatigued drivers’ faces appear to be drowsy. However, facial appearance of a chronically fatigued individual seems to be in long-term and chronic change according to the opinions of physicians and Chinese medicine experts.Neu et al. [18] found that, in addition to fatigue, CFS patients presented with higher affective symptom intensity and worse perceived sleep quality. Polysomnography shows more slow-wave sleep and micro-arousals in CFS but similar sleep time, efficiency and light-sleep durations compared to controls.In improving recognizing accuracy, fusion of hybrid facial features performs more effectively than unimodal system [25,35,36]. Sim et al. [35] combined face and iris biometric traits with the weighted score level fusion technique to deal with non-ideal scenarios such as off-angles, reflections, expression changes, variations in posing, or blurred images. Eskandari et al. [36] applied particle swarm optimization (PSO) and backtracking search algorithm (BSA) to select optimized features and weights, achieved a robust recognition system by fusion of the multimodal biometric system and optimizing the weights assigned to each sub-system. Therefore, in this paper, we propose a Hybrid Appearance (HA) feature extracting and processing method for computer aided non-invasive diagnosis of CFS. The HA feature consists of colour, texture and shape features of different facial regions. The HA feature comes from observations of traditional Chinese medicine (TCM) experts on faces of CFS patients. We compare our HA feature with the Gabor features from Eye region, lower Eye lid region and Mouth region (EEM) presented in [8]. The cross-validation results on the training set demonstrate that HA feature obtained a more ideal error rate than EEM features. Based on the same HA feature and score level fusion approach, we compare principal component analysis (PCA) [20], locality preserving projection (LPP) [21] and manifold preserving projections (MPP) on the testing set. The results also show that the proposed method possesses benefits of excellent sensitivity, specificity and accuracy.The rest of this paper is organized as follows. Section 2 presents the hybrid features identification procedure and the acquisition methods, including feature identification, image pre-processing, facial region segmenting and feature extraction for each region. Section 3 describes the feature reduction and fusion approach. Section 4 carries out the experimental results of different feature and different method.Some CFS people are associated with sleep disorders; however, others are not. This study focuses on the former case.After one year’s cooperation with several traditional Chinese medicine experts, we found that CFS individuals and healthy ones with yellow skin have the following key differences in facial appearance when they have a neutral expression, as shown inTable 1.The obtained face images are pre-processed through two steps:1.Image normalizationThe pictures are acquired by a Nikon COOLPIX L21 camera with a fixed distance from the subjects. The subjects are asked to look into the camera, and some extra requests are made to ensure relatively perfect frontal images, such as a decorous pose and a neutral expression. The images are normalized to make the two eyes and the vertical coordinate of the mouth aligned. Then images are cropped to the size of 640×640, after face detecting [22] and feature points locating [26].Illumination calibrationThe illumination calibrating operation is based on quotient image, the ratio of different face albedo, which is constant and less sensitive to variable light sources [24]. SupposeIyandIarepresent two facial images of tested person y and standard person a respectively, the quotient imageQyis expressed byQy=Iy/Ia. Given three images (I1, I2, I3) under three different illumination conditions of standard person a, we can reconstruct the image of tested person y under target light source c byIyc=∑i=13(Ii⁎xic)⊗Qy, whereIycis an image of person y under the illumination condition c, and I1, I2, I3 are three illuminated images,xicis the projection coefficient under light source c, and⊗is the Cartesian product.The estimation ofQyis described in detail in literature [24], where minimal energy function is used on a training matrixA=[A1,A2,…,An]to calculateA¯, which is used in formulaQy=Iy/A¯x.After pre-processing of images, the facial regions are selected according to the TCM experts’ opinions on which facial regions can reflect appearance changes on CFS individuals. We add 12 active appearance model (AAM) feature points and ten straight lines across those points to ensure an adaptive segmenting of facial regions as can be seen fromFig. 1(a). Consequently, the problems of hair obstacle and face size diversity can be avoided, and the image normalization is simplified. The 12 feature points are selected from the 26 AAM feature points presented in [26], which are also shown inTable 2. The 12 feature points can be detected and tracked using the method proposed by Yi et al. [26]; hence, the ten straight lines can be located based on those points and used as an alignment for boundaries of facial regions. As can be seen from Fig. 1(a), l1 can be located by point G, and used as a right boundary of the right lower eye lid region; l2 can be located by point E, and used both as a left boundary of the right lower eye lid region and a right boundary of the forehead region; l3 can be located by point F, and used both as a right boundary of the left lower eye lid region and a left boundary of the forehead region; l4 can be located by point H, and used as a left boundary of the left lower eye lid region; l5 can be located by point A and B, and used as a lower boundary of the forehead region; l6 can be located by point C and D, and used as an upper boundary of two lower eye lid regions; l7 can be located by point M, and used as an upper boundary of the mouth region; l8 can be located by point N, and used as a lower boundary of the mouth region; l9 can be located by point Q, and used both as a right boundary of the left cheek region and a left boundary of the mouth region and l10 can be located by point P, and used both as a left boundary of the right cheek region and a right boundary of the mouth region. Thus, only the values of five variables, as shown in Fig. 1(a), named h1, h2, h3 and h4,w need to be determined.In [19], Deng pointed out that different people have different face structures and shapes. Nevertheless, it is reasonable to assume that the facial configuration is consistent with normal distribution, and different facial configurations have a great similarity. Which can be used to guide the coarse positioning of the facial regions. He presented a table of some statistical data in his thesis, and he argued those data can be used to provide favourable support for feature location. These statistics are the mean and variance of the ratio between the size of the each facial region and the distance between the two eyes. For example, let the distance between point C and D in Fig. 1 is d, the distance between point C to centre of the eyebrow is dF, the mean and variance of ratios between dFand d are 0.3 and 0.05, respectively. The values of five variables h1, h2, h3, h4 and w are determined according to the opinions of TCM experts and the statistical data of facial structure presented in [19]. The detailed method is as follows:Let dGE be the distance between point G and E, dAC be distance between point A and C, and d67 be distance between line l6 and l7, max(.) be the maximum value function, then the values of those five variables are: w=dGE; h1=0.3d; h2=max(dAC); h3=0.55 d67−h2; h4=d67−h2.The diagram of seven divided facial regions is shown in Fig. 1(b). In this schema, we use different filling colours to represent different kinds of feature extraction, that is gradient grey means that both the colour and the texture features will be extracted for the forehead region, black means Gabor features will be extracted from these regions, light grey means colour features will be extracted from these regions, and dark grey means both shape features and colour features will be extracted from the mouth region.Researches in [7] showed the value of colour component S of the forehead region. The value of colour component a of the left cheek and lower nose end are very different between the healthy and the sub-healthy. Among a variety of colour spaces, the CIELab colour space is the only device-independent one, which can avoid colour difference arising from different image acquisition devices [27]. The colour-components H and S correlate with human perception and are independent from each other in the HSI colour space. Hence, we extract the colour-component S of the forehead, and the colour-component a of the two cheeks and the lower end of the nose. Then the mean and variance for each region are calculated and merged as a two-dimensional colour feature. For the forehead and nose, the two-dimensional colour feature vectors are denoted asvc1andvc3respectively, and for the two cheeks, they are merged as one vector denoted asvc2.As a Gabor wavelet transformation can extract a feature in multi-scale and multi-directional spatial frequency, it can magnify grey level changes like a magnifier. Therefore, selecting a Gabor filter in a specific direction to filter a brow region can effectively enhance the features of vertical striped wrinkles. The shape of the puffy lower eyelid is similar to the sections of the Gaussian envelop of Gabor kernel functions. Thus, filtering the lower eyelid region by the Gabor filter can magnify the differences between puffiness and non-puffiness.Let the real part of the Gabor function behR(x,y)and the imaginary part the Gabor function behI(x,y), and the image after filtering is(1)s(x,y)=(hR⁎I)(x,y)2+(hI⁎I)(x,y)2In formula (1),(h⁎I)denotes the convolution of image I and filter h, subscript R is used to denote the real part of the Gabor function and subscript I is used to denote the imaginary part of the Gabor function. If x is the mother wavelet, after series of scaling and rotation transformations, you can get a set of self-similar filters, namely Gabor wavelets:(2)hmn(x,y)=a−mh(x′,y′),a>1,m,n∈Z,(3)x′=a−m(xcosθ+ysinθ),y′=a−m(−xcosθ+ysinθ)In formulas (2) and (3), Z represents the set of integers,θ=nπ/Kforn=0,1,…,K−1where K is the number of scales,a−mform=0,1,…,S−1is the scale factor (a>1) and S is the number of directions. We can get a set of Gabor filters of different scales in different directions among which m determines the degree of amplification of the textures while n determines the direction of the texture to be enhanced. Suppose the wavelet cluster includes S scales and K directions and the frequency interval is [Ul, Uh]; then, the value of a can be calculated bya=(Uh/Ul)−1/(S−1).We calculate the histogram of the frown region after applying the Gabor wavelet filter in each scale and direction. The results are shown inFig. 2. As can be seen from Fig. 2, the vertical striped wrinkles are subtle, and the histograms of the filtering results can be used to characterize the frown state. The Gabor filtering results of K scales are averaged in each direction for the frown region, the averaged results are merged into one vector, denoted asVG1.The filtering results of the lower eyelid region by Gabor amplitude in direction n (n=1,2,…,6) at scale 2 are shown inFig. 3, where we can see that the filtering results of puffiness and non-puffiness are very different. The Gabor filtering results of K scales are averaged in each direction for two lower eyelid regions separately, and the averaged results are merged into one vector, denoted asVG2.After observation of a large number of face images, we found that most CFS individuals curl their lips while most healthy individuals have mouth corner up or non-curled lips when they are in a neutral expression. We can extract edges of the lips according to the characteristics of the lip colour; then, we get the contour between the upper lip and the lower lip and perform curve fitting. Hence, the feature characterizing the behaviour of the mouth can be derived from the properties of the curve, and the colour feature of the two lips can be obtained. The methods are as follows:1.Get an adaptive thresholdT0for rough lip segmentation based on histogramsAfter locating the mouth region based on feature points M, N, P and Q in Fig. 1(a), we calculate histograms of the mouth region in colour channels R, G and B separately. For lips cover the most of mouth region, the peak values in three histograms should correspond to the typical values of the three colour components of lips. Suppose they arer0,g0andb0, the grey level of channel R is obviously larger than that of channel G and B, so we can take a segmentation threshold asT0=max{r0−g0,r0−b0}.Mark up the pixels to get the accurate grey level of lips in three channelsMark up all the pixels in the mouth region. If the R, G and B components of a pixel satisfy the formulari−gi>T0andri−bi>T0, then it will be marked as a lip pixel. Then, we calculate histograms again for components R, G and B separately based on the lip pixels marked above. The peak values in three histograms will correspond to the actual values of the three colour components denoted asr1,g1andb1, as shown inFig. 4.Fine lip segmentation in three channels.First, segment lips in three channels separately as follows. If components R, G and B of the pixel satisfy the formulasr∈[r1−T,r1+T],g∈[g1−T,g1+T]andb∈[b1−T,b1+T], then it will be marked as a lip pixel. We apply the approach proposed in [28] to acquire the optimal threshold T. Second, calculate the summation of segmenting results of three components.Contour finding and curve fittingCanny edge detector is applied to detect the edges of lips. We use the number of pixels constituting the contour to represent the contour length. The one with the largest number of pixels is the longest contour. First, we sort the contours in a descending order based on their length. Then, we calculate the coordinates of the centre point for the marked lip area and the contours separately. Finally, we check the distance between the centre of each contour and the centre of the marked lip area. The contour that corresponding to the shortest distance is the one we needed.We use the points of the contour to fit a quadratic function. The processing result is show inFig. 5.Feature extraction for mouth behaviour estimationThe form of a quadratic function applied for curve fitting is as follows:(4)f(x)=ax2+bx+cIn Eq. (4), c is the constant term of the quadratic equation. Take the upper left corner of the image as the origin to build a Cartesian coordinates system as shown inFig. 6. Draw a straight line l1 parallel to the x-axis. Suppose the distance between the line and the vertex of the parabola is t, the distance between two intersections of line l1 and the parabola is d. The vertex coordinates of the parabola can be obtained as follows:(5){x0=−b/2ay0=c−b2/4aIn the curled lip case, solve the following equation(6)ax2+bx+c=c−b2/4a+t,we can obtain(7)x1=−b+4at−2a,x2=−b−4at−2a,(8)d=x1−x2=2taIn Eq. (8), t is a constant andt>0. The variable a is the quadratic coefficient, anda>0.Similarly, in case of mouth corner up, we can obtain(9)d=x1−x2=2−taIn Eq. (9), t is a constant andt>0. a is the quadratic coefficient, anda<0. Let curve equals t2/d, then(10)curve=(td)2=|at|4The value of curve is proportional to the bending degree of the gap line between two lips.As can be seen from the above derivation, the sign of a determines the opening direction of the parabola. That is, whena<0, the mouth corner is up, and whena>0, the mouth curls its lips. From Eq. (10) we can see that the bend degree of the gap line between the two lips is proportional to the absolute value of a. Hence, a is selected as a feature of the gap line, denoted asvm1.In traditional Chinese medicine, the colour of the lips is an important cue for disease diagnosis [6]. Hence, the mean and variance of the colour components R, G and B of the lip region are calculated for colour features of two lips, denoted asvm2.Because the values of adjacent pixels in an image are highly correlated and due to the self-similarity of the Gabor wavelets, we merge all the Gabor features in eight directions into one vector for a specific facial region and down-sample the vector by a factorρ(ρ=64) to reduce the spatial dimensionality and normalize it to zero mean and unit variance. Factorρis selected according to the research results in [34]. Then, PCA is applied to reduce the dimensionality and enhance the discriminating ability. PCA is one of the most commonly used statistical techniques to reduce dimensionality. The optimal mapping is the leading Eigenvectors of the data’s total covariance matrix. However, PCA cannot retain the nonlinear structure of the data while MPP proposed by Zhang et al. [29] can preserve both local and global manifold structures of the face, and can also solve the non-orthogonal problem brought by LPP.Suppose the down-sampled Gabor feature vector isV. Then, the criterion proposed in [29] is applied to perform feature reduction. LetX=[x1,x2,…,xN]be the set of feature vectors of the N images andWMPPbe the projection matrixyi=WMPPTxi. The principle of the projection is that the projection resultsyiandyjof two adjacent pointsxiandxjin the original data space are still adjacent in the projected space. On the contrary, the projection resultsymandynof two distant pointsxmandxnin the original data space are still distant in the projected space. The criterion function for neighbouring points is defined as(11)J1=12∑ij(yi−yj)2Wnij=WMPPTXLnXTWMPP,(12)Wnij={1,ifxi∈Nk(xj)andxj∈Nk(xi)0,otherwise,(13)Ln=∑jWnij−WnThe criterion function for non-neighbouring points is defined as(14)J2=12∑ij(yi−yj)2Wfij=WMPPTXLfXTWMPP,(15)Wfij={1,ifxi∉Nk(xj)andxj∉Nk(xi)0,otherwise,(16)Lf=∑jWfij−WfIn the above equations,Nk(xj)represents a set of k nearest neighbouring points ofxj.The objective function is defined as(17)J=J1J2=WMPPTXLnXTWMPPWMPPTXLfXTWMPPThe problem of finding the minimum value of the objective function can be converted to the following generalized Eigenvalue solving problem.(18)XLnXTWMPP=λXLfXTWMPPLet the Eigenvectors corresponding to the top d smallest Eigenvalues in Eq. (18) bea1,a2,…,ad. Their Gramm–Schmidt orthography results area′1,a′2,…,a′d. Then,WMPP=a′1,a′2,…,a′dis the manifold preserving projection matrix. More details on the solving process ofWMPPcan be found in [29].The multimodal approach increases performance by providing multiple evidences of the same identity. The three possible levels of fusion are: (a) fusion at the feature extraction level; (b) fusion at the matching score level; (c) fusion at the decision level [30]. We apply the two-stage fusion scheme proposed by Yang et al. [31] to perform a multimodal information fusion. At first, MPP introduced in Part A Section 3 is applied to eliminate the redundancy of Gabor features according to Eqs. (11)–(18). Then, the two-stage AdaBoost algorithm [31] is used to fuse multimodal facial features:(a) At the first stage, initialize the weights wi, (i=1,…,2m) for yi=0 and wi, (i=1,…,2l) for yi=1 of different features in each facial region. Where m and l are the numbers of the negative and positive samples, respectively.For t=1,…, d(19)Normalizetheweights,wt,i=wt,i∑j=1nwt,jFor each feature j, train a weak classifier aj′(x) which is restricted to using a single feature, and evaluate the error εjwith respect to wi,εj=∑iwi|a′j(xi)−yi|.Choose the classifier, at′(x), with the lowest error εt.Update the weights,wt+1,i=wt,iβt1−εi, where(20)ei={0,ifxiisclassifiedcorrectly,1,otherwise.andβt=εt1−εt(b) At the second stage, the d Eigenvalues are fused together to recognize fatigue status by using the AdaBoost algorithm. AdaBoost is an algorithm for constructing a “strong” classifier as linear combination of the “weak” classifiers, as shown in formula (21).(21)f(x)=∑i=1dkia′i(x)where f (x) denotes the strong classifier and a′i(x) denotes the weak classifiers. The parameter kimeasures the importance that is assigned to the weak classifier.We call this approach HA-MPP-ADA for abbreviation. KNN is one of the most fundamental algorithms, which is easy to understand and implement. And cosine distance is used to measure the similarity between two vectors. To generate a full ROC curve instead of just a single point from a classifier, we convert the KNN classifier to a scoring classifier. Suppose that among the k nearest neighbouring vectors, there are x vectors belonging to the CFS class and k–x vectors belonging to the healthy class. Then, we use x/k as output of the KNN classifier, and the output is mapped into interval [0, 1] as a similarity score.The pictures are acquired with a Nikon COOLPIX L21 with a fixed distance from the subjects. The subjects are asked to look into the camera, and some extra requests are made to ensure relatively perfect frontal images, such as a decorous pose and a neutral expression. We capture facial images for 294 CFS volunteers and 297 healthy volunteers. Then, pre-processing is done on each image, which is segmented and cropped based on 12 feature points, ten straight lines and the ratios of face structure, as described in Part B, Section 2. The samples used in this study are shown inTable 3. Take 196 CFS patients and 198 healthy volunteers as training samples and 98 images for CFS patients and 99 images for healthy volunteers as testing samples.The CFS patients are diagnosed on the basis of clinical criteria proposed by the Centres for Disease Control and Prevention (CDC) [1] by consensus of an expert panel, including three physicians and three TCM experts, and followed by five interns responsible for sample collecting and coarse classification. The diseased/healthy status of each sample is judged by consensus according to the clinical criteria proposed by the CDC. A questionnaire survey has also been done to help the expert panel to make final judgements.In [8], only the facial features around the eyes and mouth, such as the multi-scale and multi-orientation Gabor wavelets filtering coefficients of the eye region, mouth region (we call these features EEM for abbreviation) are used for fatigue prediction, for they are thought to include enough information in fatigue expression extracting. In this paper, compared with this method, we extract more facial features as a hybrid appearance (HA) feature. PCA is applied to perform feature reduction, and K-nearest (K=5) neighbour classifier and cosine distance are used to perform classification.We use cross-validation technique presented in [32] and [33] for the training set (394 images) to select the best feature and the best method to recognize CFS from facial images. The cross-validation technique is a variant of the test protocols used in statistical classification. Many people have used it to improve the statistical reliability of tests of face recognition.The cross-validation processes for both methods (EEM and HA) are the following:(a)First, we randomly partition the training data sets into k distinct segments (we set k=6).Then, we train a classifier using data from k−1 of the segments and test its performance by evaluating the error function (recognition rate) using the remaining segment.The above process is repeated for each of the k possible choices of the segment to be omitted from the training process.Finally, we average the results over all the k trained classifiers.The recognizing accuracy of both methods (EEM and HA) on the training set (394 images) is reported inTable 4.As can be seen from Table 4, the HA feature presented in this paper has a much less error rate than the EEM feature does based on the same training set and cross-validation approach. Hence, the HA feature is selected to categorize CFS and healthy individuals. Based on the same HA feature, the same classifier, the same distance criteria and the same multimodal fusion method, PCA, LPP and the MPP method presented in this paper are used to perform feature reduction separately (we call these three approaches HA-PCA-ADA, HA-LPP-ADA and HA-MPP-ADA for abbreviation, respectively). The experiments are performed on the training set (394 images) according to the cross-validation processes mentioned above with k=10.In addition, in order to compare the performance of three methods (HA-PCA-ADA, HA-LPP-ADA and HA-MPP-ADA), we use sensitivity SE=TP/(TP+FP), specificity SP=FN/(TN+FN) and accuracy ACC=(TP+TN)/(TP+TN+FP+FN) to rank each method where TP, TN, FP and FN are the numbers of true positives, true negatives, false positives and false negatives, respectively. In order to find the most important factor that ranks the methods, the Youden’s index (abbreviated as YI) is calculated based on SE and SP where YI=SE+SP−1. The results of the experiments on 394 training images are shown inTable 5.After that, we also do pre-processing and feature extraction on the remaining testing set (197 images), which is then classified by the three methods mentioned above, and final recognition results are also reported in Table 5. The corresponding ROC curve is obtained, as shown inFig. 7.

@&#CONCLUSIONS@&#
