@&#MAIN-TITLE@&#
A hybrid multi-objective optimization algorithm for content based image retrieval

@&#HIGHLIGHTS@&#
We formulate a CBIR search as a multi-objective optimization problem.We propose an exploration-based strategy based on the NSGA-II algorithm.Abuse of either exploration or exploitation shows a negative impact on performanceResult improve when exploitation and exploration are combined, for all methods tested.Combining exploration and exploitation at different iterations is highly recommended.

@&#KEYPHRASES@&#
CBIR,Image retrieval,Relevance feedback,NSGA-II,multi-objective,

@&#ABSTRACT@&#
Relevance feedback methods in CBIR (Content Based Image Retrieval) iteratively use relevance information from the user to search the space for other relevant samples. As several regions of interest may be scattered through the space, an effective search algorithm should balance the exploration of the space to find new potential regions of interest and the exploitation of areas around samples which are known relevant. However, many algorithms concentrate the search on areas which are close to the images that the user has marked as relevant, according to a distance function in the (possibly deformed) multidimensional feature space. This maximizes the number of relevant images retrieved at the first iterations, but limits the discovery of new regions of interest and may leave unexplored a large section of the space. In this paper, we propose a novel hybrid approach that uses a scattered search algorithm based on NSGA II (Non-dominated Sorting Genetic Algorithm) only at the first iteration of the relevance feedback process, and then switches to an exploitation algorithm. The combined approach has been tested on three databases and in combination with several other methods. When the hybrid method does not produce better results from the first iteration, it soon catches up and improves both precision and recall.

@&#INTRODUCTION@&#
The purpose of CBIR systems is to allow users to retrieve pictures related to a semantic concept of their interest, when no other information but the images themselves is available. Usually, a CBIR system represents the images in the repository as a multi-dimensional feature vector extracted from a series of low level descriptors, such as color, texture or shape. The perceptual similarity between two pictures is then quantified in terms of a distance/similarity function defined on the corresponding multi-dimensional feature space. The implementation of CBIR systems has been a major topic of research for the last years, and a large number of features have been proposed. An extensive list of widely used image descriptors can be found in [1].A major problem with CBIR systems is the so-called “semantic gap”, which refers to the difficulty of translation of user's intentions into similarities amongst low level features. Relevance feedback is a framework inherited from traditional information retrieval that has extensively been used to increase the efficiency of CBIR systems. This consists of a mechanism that processes the user's interaction to iteratively refine the original query. At each iteration the system retrieves and displays a set of images, and asks the user to give feedback on whether each result retrieved is relevant or non-relevant to his/her search. This data is used as an input to an algorithm which adapts the similarity measure and search procedure to obtain a new set of results. The resulting algorithm defines the CBIR method and determines the retrieval performance.Most existing CBIR algorithms give priority to exploitation of known regions of interest, over further exploration of the solution space. This means that preference is given to images which are very close to samples which have already been classified as relevant by the user, according to some semantic criteria. This approach is justified in terms of maximizing retrieval performance at one particular iteration, but not in terms of achieving maximum performance along the entire relevance feedback process. While it is important to maximize retrieval at one particular iteration, it is also important to select the most informative samples for judgment. Exploitation may clearly yield a higher number of relevant samples at one particular iteration. However, relevance information for the next iteration will be restricted to judgments on the same regions. On the other hand, abuse of exploration would yield a low retrieval performance which may only be amortized after a large number of iterations. This would be of little interest in a real set up.Ideally, a relevance feedback method should be able to both maintain high levels of precision at the first iterations and explore the space for undiscovered regions of interest. Unfortunately, these are conflicting interests and it is not generally possible to design a method that achieves both objectives simultaneously. In this paper, we present a hybrid approach that combines a method based on a scattered search with other existing techniques by using them at different stages of the relevance feedback process. In particular, a scattered search algorithm based on NSGA II (Non-dominated Sorting Genetic Algorithm) is used only at the first iteration of the relevance feedback process. At this stage, the search is approached as a multi-objective optimization problem and it is solved by using NSGA II. Each objective consists of minimizing the distance to one of the pictures that are known relevant and each image in the repository is considered as a potential solution to the problem. Then, the method attempts to find a conveniently scattered representative set of non-dominated solutions and uses non-relevant selections to discard some of these solutions. From the second iteration onwards, a classical distance based method is used. Globally, the technique reaches a convenient balance between discovering new regions of interest and obtaining acceptable levels of precision at the first iterations.To the best of our knowledge, this is the first time in the literature that (a) CBIR has been approached from a multi-objective optimization perspective; and (b) a hybrid method that uses different strategies at different stages of the relevance feedback process has been proposed.Let us assume we have a search spaceXcomposed of m imagesX={xi}i=1m, each represented by an d-component feature vector xi={f(i,1), f(i,2), …, f(i,d)}. Let us callPandNthe sets of relevant (positive) and not relevant (negative) images, respectively, according to user judgments made at the current and previous iterations of the relevance feedback process. Then, the CBIR search can be modeled as an instance learning problem. This consists of classifying the elements in the setXas relevant or non-relevant, according to the information contained in the setsPandN.Probabilistic techniques and supervised learning methods are two major trends to face this classification problem. Most probabilistic methods (e.g. [2–6]) rely on estimating posterior probabilities or densities from the prior probabilities and the information provided by the user in the form of relevance judgments (see Fig. 1(a) for a typical distance-based probability density function constructed from already labeled relevant and non-relevant samples). Typical supervised (e.g. [7–11]) and semi-supervised (e.g. [12–14]) learning approaches include the use of Support Vector Machines (SVMs), that attempt to find the hyperplane which achieves a maximum separation between two classes. Apart from inherent difficulties in finding an optimal set of parameters to fine-tune the retrieval system in some of these methods, they tend to choose samples from areas which are relatively close to relevant samples, and far from non-relevant ones. Hence these algorithms also fail to explore other potential regions of interest.Other previous classical approaches are based on adapting the similarity measure and moving the query point so that more emphasis is placed on relevant elements and less on non-relevant samples [15–17]. Generally, the user selections are used to dynamically adjust the weights of each feature in the global measure of similarity, and to produce a new query point that represents the interest of the user in a more appropriate way. This approach works well under the assumption that there exists a single region of interest within the multi-dimensional search space. If this is the case, it is reasonable to assume that positive judgments with very different values on a feature indicate little interest in that feature, and the distance function can be adapted to this fact. Unfortunately, relevant images for a conceptual search usually appear scattered around small areas on the search space. If this happens, the new query point may lay far from any region of interest (see Fig. 1(b)). In addition, the search always concentrates around this point and the algorithm fails to explore the rest of the search space.Other successful techniques to CBIR have been proposed in the literature include the use of fuzzy logic, e.g. [18,19], Self Organizing Maps (SOM), e.g. [20], clustering techniques, e.g. [21] and genetic algorithms, e.g. [22–24]. Some of these techniques combine exploration and exploitation. However, the latter is always dominant and the search concentrates on areas surrounding relevant samples in the (possibly deformed) search space. This causes that most of the top scoring images are close to the samples which are already known to be relevant, failing to explore the space further.Let us assume the existence of a similarity function similarity(xi, xj) which produces an estimate of the resemblance between any two images by comparing their feature vectors; consider the setXas the images in the repository that the user has not given a judgment on; and denote byP={y1,…,yp}andN={z1,…,zn}the accumulated positive and negative information, respectively, about the concept or kind of image being searched for (relevant and non-relevant user judgments on this and any previous iteration).The objective is to select a setSwith s potentially relevant scattered pictures fromXfor the next iteration. The setSis used as both the iteration result and to request the user judgments that will be used in the next iterations. Hence, we are interested in both maximizing the number of relevant samples and the diversity of the elements. We consider the solution spaceXand pose this as a multi objective optimization problem in which the objectives to optimize are the similarities to the images inPaccording to similarity(xi, xj). To this end, each image xiinXis assigned a p-tuple of values [similarity(xi, y1), …, similarity(xi, yp)] that simultaneously represents an estimate of the resemblance between the image xiand each of the relevant selections inPand the degree of satisfaction of each objective. The p-tuple concept can in fact be applied to any point q in the (continuous) multidimensional feature space. In this case, each point q is assigned the p-tuple [similarity(q, y1), …, similarity(q, yp)].Under this multi-objective formulation, pareto optimality can be used as the criterion to find the setSfor the next iteration. A solutionx1∈Xis said to dominate a solutionx2∈Xif x2 does not satisfy any objective better than x1 and x1 satisfies at least one objective better than x2. Formally, ∀l∈{1, …, p}:similarity(x1, yl)≥similarity(x2, yl) and ∃l∈{1, …, p}:similarity(x1, yl)>similarity(x2, yl). The Pareto-optimal set is defined as the set of non-dominated solutions from the setX. The hypersurface formed by joining the Pareto-optimal solutions is known as a Pareto-optimal front [25,26].Fig. 2shows an illustrative example of the Pareto-optimal front corresponding to two positive samples and several unlabeled points in 2D. It can be seen that it gives an appropriate representation of the samples close and around the two positive samples regardless of the local density of points close to each of them separately.Unfortunately, the calculation of the Pareto-optimal set in a continuous space is NP-hard [27]. In a discrete and finite solution space, determining the Pareto-optimal set is a simple but also a time consuming operation. Every solution has to be compared against the rest and, in the worst-case scenario, this takes O(p·m2), with p the number of positive selections and m the number of solutions evaluated (the number of images inX). With typical values of m in CBIR systems, still this cost becomes prohibitive. In addition, the calculation of the Pareto-optimal set may yield a large number of non-dominated solutions, specially for large numbers of relevant selections. This would result in a non practical size for the setS. One possible solution to this problem is to choose a representative set for the Pareto-optimal set composed of a lower number of solutions conveniently scattered, in order to preserve the diversity of the solutions and reduce the size of the original set.To reduce the computational burden associated with the direct computation of a diverse Pareto-optimal front from the images inX, a two stage non-exact algorithm has been followed. At the first step we consider the entire multi-dimensional feature space as the solution space. In this space, a heuristic approach is used to calculate a set of scattered elements that represent the Pareto-optimal set. In the second stage, representative elements are used as seeds to drive parallel searches, this time considering the discrete solution space composed of the elements inX. The results produced by each search are combined to build the required setSfor the next iteration.For the first stage, a continuous search space is considered. In this space, a MOEA (Multi-Objective Evolutionary Algorithm) is used to determine a spread of solutions along a set which is close to the true Pareto-optimal front. The algorithm NSGA-II [28] has been chosen for this purpose. Algorithm 1 provides a summary of the process. Initially, a random population P0 is generated and its elements are sorted according to their non-domination level. This is calculated for each element e∈P0 as the number of elements in P0 that dominate e. Next, binary tournament selection, mutation and recombination operators are used to create a child population Q0 of size N. At each further iteration t, a combined population Rt=Pt−1⋃Qt−1 is first formed. Then, an optimized ad-hoc algorithm [28] is used to calculate the non-dominated fronts in Rt. The new parent population Ptis built by subsequently adding pareto fronts in ascending rank order until the size exceeds N. Every time that a pareto front is added, its elements are assigned the same rank as the pareto front. To respect the size (N) of the parent population, some solutions in the last accepted pareto front are discarded. These are the most crowded ones, according to the density estimate produced by an efficient nearest neighbor based method (assign_crowding_distances in Algorithm 1). Then, elements in Ptare sorted according to their rank, and using the crowding distance to break on ties. At the end of each iteration, a new children generation is built by using selection, crossover and mutation.Algorithm 1Pseudo-code for the NSGA algorithm.Assuming a similarity function that returns a maximum value when an element is compared to itself, relevant samples are known to be part of the optimal Pareto set. This allows us to include these samples as part of the parent population P0 in Algorithm 1, and accelerate the converge of the algorithm. To avoid that inconsistent solutions or solutions outside the border of the multi-dimensional search space are generated, all feature vectors produced in the process are repaired so that their features are all in their range.Once the NSGA-II algorithm has generated a representative diverse Pareto-optimal front, negative selections are used to impose further restrictions on the potential solutions to the problem. This consists of applying the principles of a nearest neighbor classifier to discard the feature vectors which are closer to a negative than to a positive selection, according to the function similarity(xi, xj). Finally, the representative set of spread solutions produced above is used to determine the elements inSfor the next iteration. Each feature vector in the representative set is used as a seed, and the function similarity(xi, xj) is used to build a separate ranking for each. These rankings are then combined by visiting them in an iterative fashion. Each time a ranking is visited, the next element is added to the setS(if it was not already present). This iterative process ends when the set contains the appropriate number of images to be shown at the next iteration. Note that when the set of relevant imagesPis composed of a single picture, the optimal Pareto set is the image itself and all pictures inXare ranked according to their similarity to this image, as produced by the similarity function similarity(xi, xj).Fig. 3shows an schematic representation of the algorithm used at the first iteration of the relevance feedback process. Although Pareto-optimality after the mapping is not guaranteed, the use of this approach presents three major advantages in our context. First, the parameterless diversity preservation mechanism of NSGA-II used at the first stage provides a representative spread set of solutions directly, with as many elements as the population size used. This implies that (a) it is not necessary to compute the entire Pareto-optimal set; and (b) the result does not need to be post-processed to obtain the desired representative set of spread solutions. Second, the stopping criteria (number of iterations) can be decided so that the response time is kept within reasonable limits. Third, the mapping used allows the computation of a complete ranking if this is required.

@&#CONCLUSIONS@&#
In this paper, we have presented a novel approach to relevance feedback in CBIR. Most existing algorithms give priority to exploitation of known regions of interest, over further exploration of the solution space. In addition, the same algorithm is used at all iterations of the relevance feedback process. We have designed an exploration based algorithm that boosts the results of other existing algorithms when it is used at the first iteration. This algorithm is based on a multi-objective formulation of the retrieval problem. Each relevant sample is considered separately and used to define an objective. Then, a conveniently scattered representative set of non-dominated solutions is calculated by using a heuristic approach that relies on the NSGA-II algorithm.The technique is sufficiently general to be considered a framework, in the sense that it is independent from the similarity/distance measure used. Although the use of a simple similarity measure has already produced better results than other alternative methods, there is still a scope for improvement. In particular, the performance of the method in combination with more complex similarity measures is still an issue that should be investigated further.In general, only few iterations of the the relevance feedback process are carried out. This restricts the benefits that could be obtained by alternating the NGSA approach with other exploitation based algorithms at further relevance feedback iterations. The application of the NSGA algorithm at a late iteration does not make it possible to amortize the cost in practical terms. However, other algorithms could be designed with this purpose in mind.