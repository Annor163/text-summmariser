@&#MAIN-TITLE@&#
A new method to determine basic probability assignment using core samples

@&#HIGHLIGHTS@&#
We proposed a method to determine basic probability assignment using core samples.We can generate basic probability assignment on single and compound hypothesis.Relevance ratios based on convex hulls are obtained in a data-driven manner.

@&#KEYPHRASES@&#
Data fusion,Dempster–Shafer theory of evidence,Basic probability assignment,Core sample,Convex hull,

@&#ABSTRACT@&#
The Dempster–Shafer theory of evidence (D–S theory) has been widely used in many information fusion systems. However, the determination of basic probability assignment (BPA) remains an open problem which can considerably influence final results. In this paper, a new method to determine BPA using core samples is proposed. Unlike most of existing methods that determining BPA in a heuristic way, the proposed method is data-driven. It uses training data to generate core samples for each attribute model. Then, helpful core samples in generating BPAs are selected. Calculation of the relevance ratio based on convex hulls is integrated into the core sample selection as a new feature of the proposed method. BPAs are assigned based on the distance between the test data and the selected core samples. Finally, BPAs are combined to get a final BPA using the Dempster’s combination rule. In this paper, compound hypotheses are taken into consideration. BPA generated by the proposed method can be combined with some other sources of information to reduce the uncertainty. Empirical trials on benchmark database shows the efficiency of the proposed method.

@&#INTRODUCTION@&#
Data fusion is an evolving technology used by human to integrate data from sensors continually and make inferences about the real world [1]. The information provided by single sensor may be lack of accuracy or limited. Thus, the use of multiple sensor is aimed to improve the accuracy and can provide the user with increased information about the environment [2]. Applications of data fusion ranges from environment analysis [3,4], clinical diagnosis [5,6], transportation management [7,8] and so on. In order to integrate information from multisensor efficiently, different strategies have been developed for data fusion. Classical Bayesian theory and Dempster–Shafer theory of evidence (D–S theory) [9,10] are two mainstream frameworks in data fusion. When comparing with the classical Bayesian theory, the merit of D–S theory is outweigh. As a useful tool to handle uncertainty, D–S theory has been a research hotspot in the field of data fusion [11–20].Dempster–Shafer theory of evidence, introduced by Dempster [9] and then developed by Shafer [10] since the early 1980’s, has now developed into a mature stage. But at the same time, it also has some problems. Although Dempster–Shafer theory of evidence is widely used and has good performance in practical applications [21–24], some basic problems are still not clarified. Within the framework of D–S theory, the construction of basic probability assignment (BPA) remains an important problem which can considerably influence final results. However, the determination of BPA remains an open issue. Till now, there is no general method to determine BPA. Many authors have addressed this problem through different approaches [25–28], but most of the existing approaches determine BPA heuristically. Yager [26] associated the D–S belief structure with a whole class of fuzzy measures, and discussed the entropy of a fuzzy measure. Xu [28] put forward a method to obtain BPA based on the normal distribution of data in each attribute. Denoeux [25] proposed a neural network classifier based on D–S theory. In Denoeux’s work [25], the determination of BPA is implemented in a multilayer neural network with the architecture of one input layer and two hidden layers. The weight vector in the neural network is used to determine BPA based on the distance of pattern to its k-nearest neighbor (k-NN) prototypes. Prototype is considered as a representative pattern for each class. This method shows excellent performance as compared to existing statistical and neural network techniques. However, prototype in Denoeux’s work is generated on every single class. Although each prototype is assumed to possess a degree of membership to each class, Denoeux’s methods do not consider the BPA on compound hypotheses and do not exploit all the strengths of the theory. Compound hypotheses are useful and important to represent the uncertainty and imprecise situation and hypothesis on compound set is reallocated proportionally among singleton during the combination process.Inspired by Denoeux’s work [29,30,25], a new method to determine BPA is proposed in this paper. The main contribution of this paper is the development of a new way to determine the BPA, in which compound hypotheses are taken into consideration. BPA generated by the proposed method can be combined with some other sources of information to reduce the uncertainty. In this method, the basic probability of a pattern to a class is assigned by calculating distance with core samples representing each classification. Since not every core sample is helpful to generate a BPA, whether and which core samples should be selected is quantified by relevance ratio based on convex hulls. For each attribute in the pattern, BPAs are generated and then combined using Dempster’s combination rule to get a final BPA.The present work extends the use of the Denoeux’s model where BPA can be assigned not only on single hypothesis but also on compound hypotheses. When evidence is not adequate, unlike classical probability theory in which possibilities should be determined forcedly, D–S theory provides a flexible way to determine BPA on both single hypotheses and compound hypotheses. Therefore, when existing information is insufficient to make a convincing conclusion solely on single hypotheses, is it reasonable to introduce compound hypotheses to modeling the uncertainty. Also, we develop some new features into the proposed method that uncertainty of subjectivity is reduced by core sample selection.The rest of this paper is organized as follows. Section 2 starts with some concepts on D–S theory and some necessary related concepts. The proposed method determining basic probability assignment using core samples and its detailed procedures are presented Section 3. In Section 4, experiments on pattern classification task are conducted using the proposed method. Conclusion is presented in Section 5.In this section, the main concepts underlying the Dempster–Shafer theory of evidence are recalled. The Dempster–Shafer theory of evidence, as introduced by Dempster [9] and then developed by Shafer [10], has emerged from their works on statistical inference and uncertain reasoning. Compared with the Bayesian probability model, the merits of D–S theory have already been recognized in various fields. First, the D–S theory can handle more uncertainty in real world. In contrast to the Bayesian probability model in which probability masses can be only assigned to singleton subsets, in D–S theory probability masses can be assigned to both singletons and compound sets. Thus, more evidence will be provided to illustrate the hypotheses or the distribution between the singletons. Second, in D–S theory, no prior distribution is needed before the combination of evidence from individual information sources. Third, the D–S theory allows one to specify a degree of ignorance in some situations instead of being forced to be assigned for probabilities. Some notations in D–S theory are introduced.In D–S theory, Let(1)Θ=θ1,θ2,…,θnbe the finite set of mutually exclusive and exhaustive events. The set of every subset of Θ, which has the cardinality of2Θ, is called the frame of discernment, denotes as(2)Ω=∅,θ1,θ2,…,θn,θ1,θ2,…,θ1,θ2,…,θnWhen some pieces of evidence assigns probability masses to the subsets of Ω, the resulting function is called a basic probability assignment or a mass function. Mathematically, a basic probability assignment is a function m mapping from the power set of Θ to0,1, satisfying(3)m(∅)=0(4)∑A⊆Θm(A)=1where∅is an empty set and A is any subsets of Θ and the mass functionm(A)represents how strongly the evidence supports A.The BPA obtained by two individual information sources are combined according to Dempster’s combination rule, defined as(5)m(A)=11-K∑B∩C=Am1(B)m2(C)with(6)K=∑B∩C=∅m1(B)m2(C)whereA,Band C are subsets of2Θ, and K is a normalization constant, called the conflict coefficient of two BPAs.After combination, mass function can be transformed into pignistic probability [31] for decision making. Pignistic probability for A is denoted as:(7)Ppig(A)=∑B⊆θcard(A∩B)×m(B)card(B)×(1-m(∅))Sincem(∅)=0, this equation can be simplified as:(8)Ppig(A)=∑B⊆θcard(A∩B)×m(B)card(B)wherecard(X)stands for the cardinality of set X.In the field of computational geometry, convexity [32] is a widely studied problem. The determination of the convex hull [33] is useful in many analysis methods and has successfully been applied to many fields, such as pattern recognition and image processing. If not special specified, the following concepts are based on 2-dimensional space. However, convexity properties also can be seen in one dimension and higher dimensions. In this paper, convex hull is analogously used to describe the shape of a set of data points.A subset S of the plane is called a convex set if and only if for every pair of pointsp,qin S, the line segmentpq‾is completely contained in S.S⊆Rdconvexifforallp,q∈S,pq‾∈SThe convex hullCH(S)of a set S is the smallest convex set that contains S. In Fig. 1, the convex hull of a set of points S is a convex polytope with vertices in S.A large number of algorithms for determining the convex hull in different dimensional spaces have been proposed [33,35–38].In 1-dimensional space, convex hull consists of data points with one attribute. The shape of convex hull in 1-dimensional space is described by interval length. The merging of two convex hulls is to find the intersection of two intervals. As for convex hull in 2-dimensional space, for example, given two finite sets of pointsP1andP2, ifCH(P1)∩CH(P2)=∅, then objectsP1andP2do not intersect. See Fig. 2.In 2-dimensional space, given two convex polygons, the polygon corresponding to the merged convex hulls is the smallest convex polygon containing their union. In efficient algorithms [36,39], convex hulls are merged by setting up bridges between each other. Fig. 3illustrates this concept:Two disjoint convex polygons are shown in Fig. 3. The merged hull consists of convex chains belonging to the polygons (shown in solid blue1For interpretation of color in Fig. 3, the reader is referred to the web version of this article.1lines), joined by bridges between the polygons (shown in dashed blue lines).Let us consider an m-class problem where a patternxs∈RPhas to be classified inM={1,2,…,m}classes, denote as(9)Θ={ω1,ω2,…,ωm}The training data with P attributes(10)X={xi={x1i,x2i,…,xPi}i=1,…,T}is acquired by data points with known classification, where T stands for the number of instances.Core sample is considered as a representative pattern to a set of data points. It represents the average features of a classification. For example, the P-dimensional core sampleCmrepresents the average features of a set of data points with known classificationωm. Typically, in an m-class problem, one has to deal with m core samples, which yield m estimations of its classifications. For every single hypothesis, a core sample is generated. It is generated by applying k-means to data points which has the known classificationωm. For each dimension in a core sample, we would perform k-means once. In our method, since compound hypotheses are taken into consideration to reduce uncertainty, the proposed method need to deal with extra estimations on compound hypotheses. For core sample representing compound classifications, we perform k-mean for data points with their corresponding classifications. For example, a core sample representing the average features of two classificationsωaandωbis denote as(11)Caba,b∈Manda≠bIt is obtained by performing k-means to all data points in X with the classificationωaandωb. No matter the hypothesis is simple or compound, there is a corresponding core sample representing its average features. That is to say, each core sample represents an estimation of the classification. If there are m classes in the training set, generally we need to deal with m core samples at least,2m-2core samples at most. From the vocabulary of D–S theory, all the2m-2core samples including∅and{Ω}can be called the frame of discernment. It should be pointed out that the number of helpful core samples varies from[m,2m-2], since not every core sample is helpful in generating BPAs. Only representative core samples are selected during the core sample selection. The strategy of core sample selection is discussed in Section 3.3.We assume that the assign of a given patternxsto the estimations of its classificationNi=1,2,…,n(Ni⊆M,card(Ni)⩾1) can be formed by calculating its distance with the selectedcard(Ni)core samples. Ifxsis far from one core sampleCNirepresenting the classificationNi, it is considered providing very little information thatxsbelongs to classificationNi. On the contrary, ifxsis close to one core sampleCNi, it is considered providing much information thatxsbelongs to classificationNi. If the patternxsyields n estimations of its classification, BPA should be assigned onn+1subsets of the frame of discernment,{CN1},{CN2},…,{CNn}and{Ω}. The basic probability assigned on{Ω}stands for the most uncertain situation where it is unable to make estimations on any specific classifications. Thus the basic probability is assigned on{Ω}to make sure the value of all mass functions sum up to 1.{Ω}only acknowledges that the pattern do belongs to one of the known classifications. It seems reasonable to definite a decreasing function on the distance:(12)ms{CNi}=αNiφdis(13)ms{Ω}=1-∑i=1nαNiφdiswhere0<αNi⩽1is a constant number given weigh to a monotonically decreasing function φ whereφ(0)=1andlimdis→∞φ(dis)=0. In [29], an exponential form was proposed for φ(14)φdis=exp(-γNi(dis)2)whileγNiis a constant number corresponding with the core sampleCNi. The optimization for the constant numberαNiandγNiis described in [30].As mentioned above, not every core sample is helpful in generating BPAs. In this paper, for a set of data points, core sample representing single classification should be generated. They provide necessary information for fundamental hypotheses. On the other hand, whether and which core sample representing compound classifications should be generated are discussed in the following context.With the help of convex hull, the core sample selection chooses helpful core samples. Intuitively, a rubber band spanned a set of data points is defined as a convex interval or a convex polygon containing all the data points. The length of the resulting convex interval or the shape of the resulting convex polygon can be called the convex hull of the data points. Based on the observations, robust estimation [40] shows the data points in the inner of a data set is generally more trustable than some extreme data points outside the data set. Thus, convex hull of a set of data points is calculated as a preprocessing step to show a common, relatively trustable boundary of the existing data points with known classifications. Convex hulls on data points with single classificationωmshould be generated first, which is denoted as(15)CH(Cm)m∈MWhen generating core sampleCN⊆Mwithcard(N)⩾2representing compound classifications, the question transfer into the similarity between single convex hullsCH(Cm)wherem∈N. For example, core sample(16)Caba,b∈M(a≠b,a≠∅,b≠∅)represents two classificationωaandωb. WhetherCabcan be generated depends on the following rules:•IfCH(Ca)overlaps withCH(Cb), it illustrates that data points in the training data consider more similarity than confliction to each other. D–S theory can process information by re-allocating the masses assigned on the non-single hypotheses proportionally into the single hypotheses during the combination process. Thus,Cabis supposed to be generated under this circumstance. We use the relevance ratio to quantify the degree of similarity. In this case, we set(17)Rab=1IfCH(Ca)andCH(Cb)are not overlapping, the degree of similarity could be measure by calculating the proportion ofCH(Ca)andCH(Cb)in merged convex hullCH(Ca+Cb). In this case, the relevance ratio is calculated according to the following equation:(18)Rab=CH(Ca)+CH(Cb)CH(Ca+Cb)whereCH(Ca)stands for the boundary of data with classificationωa,CH(Cb)stands for which with classificationωb.CH(Ca+Cb)stands for the merging ofCH(Ca)andCH(Cb).Rabdecreases asCH(Ca)andCH(Cb)are getting isolated to each other. In 1-dimensional space,CH(Ca)andCH(Cb)are the interval lengths of data with classificationωaandωbrespectively.CH(Ca+Cb)is the shortest continuous interval containingCH(Ca)andCH(Cb). A numeric experiment of calculating the relevance ratio using the Eq. (18) is presented in Section 4.Relevance ratio in our method is introduced to portray the similarity between two convex hulls. When two convex hulls are overlapping, existing evidence shows that two sets of data points are intersect. Therefore, by the evidence we obtained so far, they cannot be easily distinguished from each other. Thus, the similarity is relatively high in this situation. When two convex hulls are not overlapping, more isolated two convex hulls are, the less we consider they have similarity to each other. Quantified the similarity by relevance ratio, compound hypothesis whose relevance ratio under a specific degree is not encouraged in generating BPAs.For compound classificationN⊆Mwherecard(N)⩾2, a thresholdRTis denoted. IfRN>RT, we consider this hypothesis worth generating. Otherwise, the inner dissimilarity within compound hypothesis exceed some degree, therefore, this compound hypothesis should not be supported.It should be pointed out that in 1-dimensional space, we use interval length to measure the approximate shape of convex hulls and their intersections, while in 2-dimensional and 3-dimensional space we use area and volume to measure respectively.A flow chart of the main process of the proposed method is shown in Fig. 4. The details of this procedure are described as follows.Since multivariate data are common in practice, for a given multivariate data, every individual variate is considered as an attribute. Attribute can be multivariate if these multivariate are obtained from the same source. The proposed method can be used to obtain BPA for each attribute. We consider each attribute as a unique source of information. During the attribute division, test data with k attributes is divided and transformed into k attribute models.Then, core samples are generated representing single and compound classifications. Next, by calculating relevance ratio based on convex hulls, core sample selection determined which core samples representing compound classifications are helpful in generating BPAs. After that, BPA is generated for each attribute model. Then, BPAs from every attribute model are combined to get a final BPA for the whole training data. Finally, the final BPA can be transformed into pignistic probability for decision making.It should be pointed out that when evidence is highly conflicting, D–S rule may produce counter-intuitive results. In our approach, the way we generate BPA is aimed at avoiding highly conflicting BPAs. Unlike classical probability theory in which possibilities are forcedly assigned even the information is not adequate for decision making, in our approach, we assigned possibilities on single hypotheses as well as compound hypotheses. Especially, we can also assign possibilities on{Ω}. By doing this, only convincing evidences are assigned to single hypotheses. In other situations where it is hard to distinguish single hypotheses with each other, we assign BPA in a much more flexible manner (to assign possibilities on compound hypotheses to reduce uncertainty). Thus, highly conflicting evidence can be avoided. Therefore, in our approach, D–S rule is enough to handle conflict evidence. However, if some extreme cases happened, other methods, such as average [41] and weighted average [42] can be taken into consideration.

@&#CONCLUSIONS@&#
