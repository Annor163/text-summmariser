@&#MAIN-TITLE@&#
Response-adaptive designs for clinical trials: Simultaneous learning from multiple patients

@&#HIGHLIGHTS@&#
We develop a response-adaptive design for clinical trials with a discrete outcome set.Our design is flexible and exploits learning from multiple patients simultaneously.Our design improves expected outcomes of patients enrolled in the trial.Our design improves expected probability of correctly identifying better treatment.Our design could have improved outcomes of patients enrolled in a stent trial by 37 percent.

@&#KEYPHRASES@&#
OR in health services,Adaptive clinical trials,Markov decision process,Bayesian learning,Stents,

@&#ABSTRACT@&#
Clinical trials have traditionally followed a fixed design, in which randomization probabilities of patients to various treatments remains fixed throughout the trial and specified in the protocol. The primary goal of this static design is to learn about the efficacy of treatments. Response-adaptive designs, on the other hand, allow clinicians to use the learning about treatment effectiveness to dynamically adjust randomization probabilities of patients to various treatments as the trial progresses. An ideal adaptive design is one where patients are treated as effectively as possible without sacrificing the potential learning or compromising the integrity of the trial. We propose such a design, termed Jointly Adaptive, that uses forward-looking algorithms to fully exploit learning from multiple patients simultaneously. Compared to the best existing implementable adaptive design that employs a multiarmed bandit framework in a setting where multiple patients arrive sequentially, we show that our proposed design improves health outcomes of patients in the trial by up to 8.6 percent, in expectation, under a set of considered scenarios. Further, we demonstrate our design’s effectiveness using data from a recently conducted stent trial. This paper also adds to the general understanding of such models by showing the value and nature of improvements over heuristic solutions for problems with short delays in observing patient outcomes. We do this by showing the relative performance of these schemes for maximum expected patient health and maximum expected learning objectives, and by demonstrating the value of a restricted-optimal-policy approximation in a practical example.The costs of bringing a new drug to market have been estimated to be as high as $5 billion (Forbes, 2013). Clinical trials have been cited as a key factor in raising these costs, with phase III trials now representing about 40 percent of pharmaceutical companies’ R&D expenditures (Roy, 2012). The total cost of a clinical trial can reach $300–$600 million (English et al., 2010), potentially an order of magnitude higher when including the value of remaining patent life,33Given that the patent for a drug or an intervention is typically filed before clinical trials begin, shortening the trial length can significantly increase potential revenues, not to mention the potential health benefit for the patients outside of the trial. For example, the sales of the drug Atorvastatin (trade name: Lipitor) decreased by 42 percent, from $2.4 billion to $1.4 billion, after its expiration on November 30, 2011 (Forbes, 2012).and exceed $6000 per enrolled subject (Emanuel, Schnipper, Kamin, Levinson, & Lichter, 2003). Consequently, drug manufacturers face pressure to produce conclusive results faster and reduce the number of subjects required.Traditionally, clinical trials have followed a non-adaptive or a fixed design that randomizes patients to treatments in a constant proportion (probabilistically) throughout the trial. Such a design, in use for several decades, is well-understood by practitioners, and provides a clean way of separating treatments. Common reasons for the prevalence of such designs include a desire to maintain low probabilities of type I error and to protect against bias. However, these designs often result in lengthy trials, poor patient outcomes, and inconclusive results, leading to longer times for drug approval. In recognition of these issues, regulatory bodies, such as the U.S. Food and Drug Administration (FDA), have encouraged the use of adaptive designs (FDA, 2010a, 2010b).There exist several types of adaptive designs (see Chow and Chang, 2008 for a comprehensive list); a commonly used design, and the focus of this work, is the outcome- or response-adaptive design. Such designs, typically Bayesian in nature, employ learn-and-confirm concepts, accumulating data on patient responses, which is then used to make procedural modifications while the trial is still underway, increasing the likelihood of selecting the right treatment for the right patient population earlier in a drug development program. Adaptive designs can potentially increase the probability of finding the successful treatment, identify ineffective and unsafe drugs sooner, and require fewer patients in the trial, thereby reducing costs and shortening development timelines. Adaptive designs can also offer a safer alternative to fixed designs, allowing patients, who are initially allocated to a relatively unsafe treatment, to be switched to the safer treatment, as and when it becomes evident during the course of the trial. Henceforth, we will use the term adaptive to mean response-adaptive design.The inherent flexibility of a Bayesian adaptive design appears contrary to the established fixed design. Common criticisms of adaptive designs include perceptions of reduced ability to do classical tests of statistical hypotheses, especially control of type I error that FDA requires for regulatory approval (FDA, 2010a, 2010b). Berry and Eick (1995) argues that such objections are either due to a lack of understanding or involve issues that can easily be addressed, for example, by incorporating constraints into the adaptive design (Cheng & Berry, 2007). Berry and Eick (1995) and LeBlond (2010) propose the use of computer simulations to evaluate type I error rate in Bayesian approaches.44A commercial software that does this simulation is called FACTS™, see www.berryconsultants.com/software/ for details.In fact, the cholesterol-lowering drug Pravigard PAC was the first FDA approval that took a primarily Bayesian focus (Berry, 2006). In addition, the FDA has approved a number of medical devices whose submissions utilized a Bayesian statistical method (LeBlond, 2010). Further, inferential measures such as predictive probability make Bayesian approaches better suited for interim analyses as they provide the ability to quantify subsequent trial results given current information (Berry, 1985; 1987; 1993; Lee & Liu, 2008).Berry and co-authors were among the first to develop a truly Bayesian response-adaptive design (see, for example, Berry, 1978; Berry & Pearson, 1985). In their design, patient randomization to treatments happens sequentially, that is, one at a time and all previous patient response(s) are known and incorporated into the randomization decision(s) for the following patient(s). This design is reasonable for trials where a single patient is randomized at each period, as in the case of individualized therapy trials, or when there is minimal delay in observing outcomes. However, this design is not practically useful when multiple patients need to be randomized simultaneously. One could implement variations of this design, for example by randomizing all the patients at a stage with a probability calculated for a single patient using the existing sequential design. However, such designs are suboptimal in a model that recognizes available information and the timing of opportunities to gather more information, required for updating the policy. We address this gap by developing an adaptive design with multiple simultaneous randomizations to anticipate learning through the trial horizon; we call this the Jointly Adaptive design. Following existing literature, we assume that patients are exchangeable, their outcomes are observable before each randomization decision, there is no serial correlation in treatment effects, and the treatment effects remain the same at each stage of the trial.The key contribution of this paper is the development of a Bayesian MDP framework for finite-horizon problems that learns optimally from simultaneous multiple experiments, admits continuous controls, and can be used to evaluate treatments under multiple objectives. In the context of clinical trials, our contributions are the development of a practically implementable response-adaptive design (termed Jointly Adaptive) that learns simultaneously from multiple patients and optimally randomizes them to multiple treatments. Further contributions include consideration of a learning objective in addition to the health objective, and evaluation of the relative advantage of the Jointly Adaptive design over other implementable response-adaptive designs, the fixed design, and heuristics. We note that our model is generalizable to other MDP settings that involve learning from multiple simultaneous individual experiments, as in the case of customized consumer offers.The rest of the paper is organized as follows. Section 2 provides a brief overview of the literature. Section 3 presents the underlying model for the Jointly Adaptive design. Section 4 describes various adaptive designs and provides some theoretical guarantees. In Section 5, we present numerical results, including application to a recently conducted clinical trial. In Section 6, we summarize and discuss our conclusions as well as the scope and limitations of the adaptive designs.The majority of previous work on trial design appears in the field of statistics. The class of problems involving adaptive designs has its roots in the multi-armed bandit problem that balances maximizing reward using knowledge already acquired with undertaking new actions to further increase knowledge, commonly referred to as the exploitation vs. exploration tradeoff.The study of heuristics for the multi-armed bandit problem has a long history. Robbins (1952) is one of the earliest works on this topic that investigated the play-the-winner rule in a two-armed bandit problem. Bellman (1956) is one of the first to study the problem of sequential design of experiments using backward induction. Gittins (1979) employs a Dynamic Allocation Index, also called the Gittins Index, to solve bandit problems using forward induction; Katehakis and Veinott (1987) characterizes this index in a way that allows it to be calculated more easily.Berry (1978) is one of the first studies to fully incorporate a Bayesian learning approach in a two-armed bandit. Extensions to this model include: (a) Berry and Eick (1995), which considers an objective that incorporates the conflicting goals of treating patients as effectively as possible during the trial and, with high probability, correctly identifying the relative efficacy of each treatment, and (b) Cheng and Berry (2007), which proposes a constrained adaptive design to address the “treatment assignment bias” concern raised in the literature (e.g., Chalmers, Celano, Sacks, & Harry Smith, 1983); their constraint ensures that each treatment in the trial has a certain fixed minimum probability of being chosen at each allocation decision. We refer the readers to Berry and Fristedt (1985) for further applications and note that adaptive designs have typically focused on maximizing expected patient health.A related stream of literature has investigated asymptotically adaptive policies for bandit problems to achieve an optimal rate of regret. Lai and Robbins (1985) is a seminal study whose proposed adaptive policy achieves a O(log n) lower bound on the regret. Extensions of this study and other examples include Burnetas and Katehakis (1996), Auer, Cesa-Bianchi, and Fischer (2002), and Honda and Takemura (2010). For further details, we direct the readers to these papers and references therein.Another stream of related literature includes evaluation of adaptive treatment strategies, defined by sequences of decision rules on when and how to alter the treatment of a patient in response to outcomes (Murphy, 2005). Such designs share several features with adaptive trial designs, for example, the use of past patient responses. The trials of adaptive strategies to treat a patient can either follow a fixed design (with adaptive strategy replacing traditional treatment) or an adaptive design (where adaptive treatment strategies change dynamically). In this paper, we focus on adaptive trial designs for specific treatments but note that this approach is also applicable to consideration of adaptive treatment strategies.Most adaptive designs assume a constant delay in observing outcomes that corresponds with the next set of allocation decisions. Hardwick, Oehmke, and Stout (2006) relaxes this assumption by incorporating varying delays. In particular, they assume independent exponential response times such that a patient response may not be available at the next randomization opportunity. For our model, we assume a constant delay and no opportunities to learn before the first allocation decision. The important practical aspect that we capture in contrast to most prior work is that multiple patients receive treatment assignments simultaneously before the outcomes of the previous assignment can be observed but that delays are limited so that some sequential structure is retained.The Bayesian learning setup appears in many other areas besides clinical trials. Examples in the OR/MS literature include work on dynamic assortments in retailing (e.g., Caro & Gallien, 2007) and dynamic learning about employee performance to formulate an employer’s hiring and retention decisions (Arlotto, Chick, & Gans, 2013). Concerning dynamic pricing problems, the setup has been used to estimate unknown parameter(s) that characterize the underlying demand function. Recent examples include Aviv and Pazgal (2005), Farias and Van Roy (2010), and Harrison, Keskin, and Zeevi (2012). Readers are directed to Besbes and Zeevi (2009) as an example that uses classical statistics framework to learn about the underlying demand function. Harrison et al. (2012) discusses further connections to antecedent literature.A paper that is close to our work is Bertsimas and Mersereau (2007), which uses multi-armed bandit framework in an interactive marketing context. Our work differs from Bertsimas and Mersereau (2007), as follows. First, we allow for randomized strategies, while Bertsimas and Mersereau (2007) restrict choices to integers that restrict the control space. Second, we consider a maximum expected learning objective, defined as the expected probability of correctly identifying the most efficacious treatment at the end of the trial, in addition to the maximum expected successes objective that Bertsimas and Mersereau (2007) considers. Further, we analyze the tradeoff between the two objectives. Third, our design provides flexibility that, in essence, differs from that offered by Bertsimas and Mersereau (2007). For example, in restricted-optimal-policy approximation, our design allows for an optimal solution developed for a smaller problem to be applied to a larger cohort using a heuristic. Finally, our work is specifically tailored to the trials context, in contrast to Bertsimas and Mersereau (2007), which is in an interactive marketing context. We evaluate and compare multiple strategies, thus making it relevant for clinicians, regulatory agencies, and other concerned parties. This is also reflected in our numerical results, where, for example, we show the value of the optimal solution under a wide variety of initial conditions, reflecting the reality that clinicians differ widely in their prior beliefs about the success probability of a specific treatment.Our model incorporates uncertainty in parameter estimates, usually missing from fixed designs. While previous literature uses constraints to ensure a minimum probability of choosing a treatment (as in Cheng & Berry, 2007), our proposed Jointly Adaptive design (modeled in Section 3) includes such randomizations naturally. To the best of our knowledge, this is the first fully response-adaptive trial design that considers multiple patients, incorporates fixed observation delays, and can incorporate optimal solution for a learning objective.Finally, most studies on multi-armed bandit problems, including ours, assume statistically independent arms. Although not applicable to our setting, Mersereau, Rusmevichientong, and Tsitsiklis (2009) is an example of a study that considers correlated arms and shows that the known statistical structure among arms can be exploited for higher rewards and faster convergence.We formulate the problem as a Bayes-adaptive Markov decision process (BAMDP).55We follow the terminology of Duff (2003).Unlike the classical MDP setup, the underlying probabilities are unknown in BAMDP. Instead, we assume a parametric distribution on the transition probabilities at the beginning of the trial, capturing the beliefs of clinicians about each treatment. As more information is obtained in the trial, the beliefs are updated dynamically in a Bayesian fashion.The BAMDP state is a vector with dimension equal to the number of treatment–outcome combinations, also called health conditions. Each component of our Markov chain state, that we call the health information state, represents the number of patient observations accumulated in each health condition up to a given stage. The state thus captures the information observed so far (history) and is used to derive the distributions that describe the uncertainty in the transition probabilities. The controls are the probabilities of randomizing patients to each treatment. The rewards depend on the objective function chosen; we consider two objectives: patient health and learning about the efficacy of treatments.Below we specify the model for the simple case of a trial that consists of two treatments, henceforth referred to as treatments A and B, and two mutually exclusive outcomes, namely success (s) and failure (f). We believe that this simple case illustrates the workings of the model and indeed exemplifies many practical trials, although the model can easily be generalized to the case of multiple treatments and/or multiple outcomes. Further, we assume the following: (a) independent treatments, (b) independent and identically distributed (i.i.d.) patients, (c) no serial correlation in treatment effects, and (d) a constant number of patients allocated each period.The use of continuous controls in our model allows for probabilistic allocation of multiple patients and helps eliminate selection bias and facilitates blinding (similar to simple randomization in traditional fixed designs). However, this could potentially lead to imbalance between various treatment groups (similar to traditional fixed designs), especially if the number of patients enrolled in the trial is small. Techniques such as block and stratified randomization can be used to address the imbalance issue in fixed designs but they have their own disadvantages.66A description of such techniques can be found in many sources, for example, Suresh (2011).In the case of adaptive design, similar restrictions or correlation structures (tailored to adaptive design) can be put in place; however, we do not address these concerns in this paper.We describe a basic version of our model with two alternative treatments, two possible outcomes each, and serially independent trials. Additional treatments, outcomes, and more complex dependency structures can be incorporated without materially changing the framework of the model. Let T be the trial length or the total number of time periods in the trial, where a time period corresponds to the (constant) time between two allocation decisions, which we assume corresponds with the delay time from initial treatment to observed outcome. We note that the first set of randomizations (decisions) take place at timet=0,the first set of patient outcomes are observed at timet=1,and decisions for patients arriving at time t are made at timet−1,t ∈ {0, 1, .., T}. No decisions are made at timet=T.Let n be the number of patients allocated per period in the trial. ThenN=nTrepresents the total number of patients (observations) in the trial. LetJ={A,B}andO={s,f}be the set of independent treatments and outcomes, respectively. The corresponding set of health conditions, I, is obtained from a Cartesian product of those sets (J × O) such thatI={As,Af,Bs,Bf}.The health information state is a vector,ht∈H⊆Z|J|×|O|,defined as follows:ht=(htAs,htAf,htBs,htBf).Here,htj,o∈Z+represents the cumulative number of observed patients to date in health condition (j, o) at time t ∈ {0, 1, .., T}, for all j ∈ J, o ∈ O, such that∑j∈J,o∈Ohtj,o=nt.The controls,ut∈U⊆ℜ+|J|are defined as follows:ut=(utA,utB).Hereutj∈[0,1]is the probability of assigning a patient to treatment j ∈ J at time t such that∑j∈Jutj=1. Given two mutually exclusive treatments, we only need to calculate controls for one treatment. Without loss of generality, we assume it would be for treatment A(utA),such thatutB=1−utA. The set of allocations, dt, is defined as:dt=(dtA,dtB).Here,dtj∈Z+is the random number of patients assigned to treatment jat time t. If each patient assignment is independent,dtAis obtained fromutAas follows:dtA∼Bin(n;utA),77Bin denotes binomial distribution.such thatEdtA=nutAanddtB=n−dtA.88If additional information can be used in each patient assignment, for example, the assignments of other patients, thendtjcould correspond to a random selection of the integers above and belownutj,such that the expectation isnutj.Letptjrepresent the (unknown) probability of observing a success with treatment j ∈ J at time t. The vector of probabilities is defined as follows:pt=(ptA,ptB).Following existing literature,ptjis assumed to be Beta distributed with hyperparameters(αtj,βtj)such thatg(ptj)∼Beta(αtj,βtj)andEptj=αtjαtj+βtj,where g( · ) represents the probability density function (pdf).Given allocation dt, the (random) outcomes are observed in the next period, captured in the vectorkt+1∈K⊆Z|J|×|O|,that we define as the physical state, as follows:kt+1=(kt+1As,kt+1Af,kt+1Bs,kt+1Bf).Here,kt+1j,o∈Z+represents the number of observed patients in health condition (j, o) at timet+1,where the treatment j ∈ J is given at time t and the outcome o ∈ O is observed at timet+1,such that∑j∈J,o∈Okt+1j,o=n.Givenptj,the likelihood of observingkt+1jssuccesses out ofdtjis binomially distributed, i.e.Pr(kt+1js|dtj,ptj)∼Bin(kt+1js;dtj;ptj). Since the Beta distribution serves as a conjugate prior for the Binomial distribution, the posterior distribution ofpt+1jis given as follows:g(pt+1j)∼Beta(αtj+kt+1js,βtj+kt+1jf).Let(αt,βt)={(αtA,βtA),(αtB,βtB)}. If we denote the initial values (att=0) of Beta distribution hyperparameters by(α0,β0)={(α0A,β0A);(α0B,β0B)}and assume that the outcomes of patients in different health conditions are not informative of each other, then each(α0j,β0j)can be updated independently as follows:αtj=α0j+htj,sandβtj=β0j+htj,f,wherehtj,ocaptures all the (random) realizations from the past for that treatment–outcome combination. In the absence of any knowledge of treatment efficacy, a commonly assumed initial prior is non-informative, i.e.,(α0j,β0j)=(1,1)for all j ∈ J, equivalent to a uniform [0,1] distribution. Going forward we will use the term “initial priors” to mean “initial values of Beta distribution hyperparameters (att=0)” and use the two terms interchangeably.99The initial priors represent clinicians’ prior beliefs about each treatment’s efficacy.The above definitions directly imply the following: fort=1,ht=ktand fort=2,⋯,T,ht=ht−1+kt.1010To illustrate with a simple numerical example, supposen=4,t=5,T=10,and{(α0A,β0A);(α0B,β0B)}={(1,1);(1,1)}. Then, a state att=5may appear as follows:h5=(8,2,5,5),{(α5A,β5A);(α5B,β5B)}={(9,3);(6,6)}. One solution under the health objective is:u5=(0.7,0.3),d5=(3,1),and a potential next state isk6=(2,1,1,0),implying the following:h6=(10,3,6,5)and{(α6A,β6A);(α6B,β6B)}={(11,4);(7,6)}.The entries of the transition matrix at time t,Pt(ht+1|ht,dt,α0,β0),representing the probability of transitioning to stateht+1,given ht, dt, and (α0,β0), is then the product of individual probabilities, defined as follows:(1)Pt(ht+1|ht,dt,α0,β0)=∏j∈JPr(kt+1js|htjs,htjf,dtj,α0j,β0j)=∏j∈J∫01Pr(kt+1js|dtj,ptj)g(ptj|αtj,βtj)dptj,ifdtj∈Zandkt+1js≤dtjfor all j ∈ J, and 0 otherwise.Finally, the reward function, Rt, depends on the objective function chosen (described below in Sections 3.1.1 and 3.1.2). The entire formulation is a dynamic program, in which the objective is to maximize the expected value function (Vt) that captures expected total reward and solves the Bellman equation as follows:(2)Vt(αt,βt)=maxut{Rt+Ekt+1[Vt+1(αt+1,βt+1)]}.The optimal policy obtained from (2) serves as a basis for our proposed Jointly Adaptivedesign, described in Section 4.Below, we define the reward and value functions separately for the two objective functions we consider: patient health and learning.The patient health objective (PH) aims to maximize the number of patient successes in the trial. Following existing literature (e.g., Berry & Eick, 1995; Ning & Huang, 2010), we assign a reward of 1 for success and 0 for failure. Formally, the reward function is defined as:RT=0andRt=(kt+1As+kt+1Bs)fort=0,1,⋯,T−1.LetStdenote the value function (Vt) for the patient health objective. The dynamic program in (2) can be expressed as follows.For the terminal period (T), where no decision needs to be made,ST=0. ForT−1,the terminal decision stage, the optimal strategy is to allocate all patients to the treatment with highest expected success probability as follows:(3)ST−1(αT−1,βT−1)=nmaxjαT−1jαT−1j+βT−1j.Fort=0,1,⋯,T−2,(4)St(αt,βt)=maxut{∑j∈{A,B}αtjαtj+βtjdtj+Ekt+1[St+1(αt+1,βt+1)]}.The learning objective (LE) represents the probability of correctly identifying the most efficacious treatment at the end of the trial. Formally, the reward function is defined as:RT=max{Pr(pTA>pTB),Pr(pTB>pTA)}andRt=0fort=0,1,⋯,T−1.LettingPtdenote the value function (Vt) for the learning objective, the dynamic program in (2) can be expressed as follows:PT=max{Pr(pTA>pTB),Pr(pTB>pTA)},andfort=0,1,⋯,T−1,(5)Pt(αt,βt)=maxutEkt+1[Pt+1(αt+1,βt+1)].The total number of unique states in this setup that need to be solved equals∑t=1T(nt+k^−1)!nt!(k^−1)!(Bona, 2002), indicating that BAMDP state space increases exponentially with n and T, commonly referred to as the curse of dimensionality.Fig. 1 depicts the state space transition diagram for one patient and one period (left) and two patients and two periods (right). A consequence of the curse of dimensionality is the enhanced computational burden necessitating the use of state space approximation methods. While several approximation techniques are possible (and are indeed the focus of ongoing work), in this paper, we use a particular form of approximate dynamic approach to maintain tractability for a practical example. This particular approach, that we term restricted-optimal-policy approximation, utilizes the fully optimal policy for a restricted number of periods early on and a myopic policy for the remainder of the time periods (see Section 5.1.1).Below, we state some properties of our model, starting with the first result that shows that adding the information state preserves the Markovian nature. Unless stated otherwise, all proofs can be found in the Appendix.Proposition 1The BAMDP defined asM^={H,K,U,Pt(ht+1|ht,dt),Rt}is an MDP.The proof of the above statement follows the outline of Bertsekas (1995, Section 5.1), which shows that that the addition of the (imperfect) information state reduces the problem to one with perfect state information and preserves the Markovian dynamics, i.e.,P(ht+1;kt+1|h1,..,ht;k1,..,kt;u1,..,ut)=P(ht+1;kt+1|ht;kt;ut). We omit the proof and refer the readers to Bertsekas (1995).The following result shows that the success probability is a nondecreasing (nonincreasing) function ofαtj(βtj).Lemma 1LetPrαtj,βtj=Pr(ptj>y|αtj,βtj)represent the probability of success with treatment j ∈ J at time t such that 0 ≤ y < 1. Then the following stochastic order prevails:Prαtj+1,βtj≥Prαtj,βtj≥Prαtj,βtj+1.The following corollary is a direct consequence of the above lemma.Corollary 1Letptj=Pr(s|j,αtj,βtj),ptj+=Pr(s|j,αtj+1,βtj),andptj−=Pr(s|j,αtj,βtj+1). Then, the following stochastic order prevails:ptj+≥ptj≥ptj−.The following two results show how the value function changes if we observe an additional success or failure with a treatment. We use the following notation:αtj+=αtj+1,αt=(αtj,αtj′),αt+=(αtj+,αtj′); similar definitions hold forβtj+,βt, andβt+.Proposition 2St(αt+,βt)≥St(αt,βt)≥St(αt,βt+)for j, j′ ∈ {A, B}.IfEptj≥Eptj′,Pt(αt+,βt)≥Pt(αt,βt)≥Pt(αt,βt+)for j, j′ ∈ {A, B}.We compare the Jointly Adaptive design modeled in Section 3.1 with the existing sequential response-adaptive design that randomizes patients one at a time (e.g., Berry & Eick, 1995). While such a design, that we call Perfectly Adaptive, offers the greatest learning potential, its applicability in practice is limited. Implementing this design would result in a prohibitively long clinical trial and potentially deteriorating outcomes for the general patient population due to long approval times.We also compare the Jointly Adaptive design with two naive (suboptimal) implementable versions of the Perfectly Adaptive design. In the first design, that we call Isolated Adaptive, each patient is considered in isolation at each time period. Such a design is akin to having multiple independent clinical trials in isolation, each of which implements a Perfectly Adaptive design. The information set is reduced by a factor of n, implying reduced opportunities for learning and inferior outcomes.Another approach is to impose a constraint such that all patients are allocated to a single treatment at each time period. Such a design, that we call Restricted Adaptive, incorporates responses from all past patients similar to the Jointly Adaptive design. However, the design is constrained by the fact that all patients in a time period receive the same treatment.Finally, we benchmark against the traditional fixed design, primarily used to learn about treatment efficacy. We define a new design called Equal Allocation (EA), where patients are allocated to treatments in equal proportion.1111The majority of fixed designs randomize patients equally to the various treatments throughout the trial.We summarize the designs below.•ΠPA (Perfectly Adaptive):= 1 × nT patients arrive sequentially; decision is made for one patient at a time; incorporates learning from outcomes of all previous patients.ΠJA (Jointly Adaptive):= n patients per period × T periods; decision is made for all n patients simultaneously; incorporates learning from outcomes of all previous patients; each patient is randomized to all the available treatments.ΠRA (Restricted Adaptive):= n patients per period × T periods; decision is made for all n patients simultaneously; incorporates learning from the outcomes of all previous patients; all n patients receive the same treatment at each period.ΠIA (Isolated Adaptive):= n patients per period × T periods; each patient is considered in isolation; equivalent to n independent T period sequential trials with no learning across trials.ΠEA (Equal Allocation):= n patients per period × T periods;n|J|patients allocated to each treatment.Note that Πiindicates a class of policies, and πi∈ Πi,i={PA,JA,RA,IA,EA}indicates the optimal policy that belongs with that class of policy. Fig. 2 visually illustrates the differences between the three designs: ΠEA, ΠPA, and ΠJA. Below, we list some structural properties of the Jointly Adaptive design and provide numerical comparisons in Section 5.Theorem 1Given n, T, N,α0,β0, the following holds: (i)V0πPA≥V0πJA,(ii)V0πJA≥V0πIA,(iii)V0πJA≥V0πRA,and (iv)V0πJA≥V0πEA.Note that whenn=1,V0πPA=V0πJA=V0πRA=V0πIA.RemarkThe initial values of Beta distribution hyperparameters or initial priors can play a significant role. A strong initial prior, whose values are numerically large and not likely to be affected significantly with the observed information from trial. Conversely, a weak initial prior will be heavily influenced by the information obtained during the trial.The following result shows that under any policy that belongs to the class of optimally adaptive policies, each treatment is applied infinitely often (i.o.) in the limit.Lemma 2Letp¯jrepresent the (unknown) underlying probabilities of success with treatment (equivalently arm) j, such that0<p¯j<1,p¯j≠p¯j′,and j, j′ ∈ J. Then, for any optimal policy π ∈ Π,∑t=0T−1dtj,π→a.s.∞w.p. 1 as T → ∞ or n → ∞.The following result, consistent with Ghosal, Ghosh, and Samanta (1995, Proposition 1), shows that the optimal design, that tries each treatment infinitely often, identifies the better treatment w.p.1.Lemma 3Suppose for every j ∈ J,∑t=1Tdtj→T→∞a.s.∞. Then, for n < ∞,Pr{pTj>maxj′∈J∖{j}pTj′}→T→∞P1,and for T < ∞,Pr{pTj>maxj′∈J∖{j}pT,j′}→n→∞P1for all j, j′ ∈ J.The following result shows that the Jointly Adaptive design infers the “superior” treatment w.p. 1 in the limit.Theorem 2P0πJA→P1as T → ∞ or n → ∞.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
