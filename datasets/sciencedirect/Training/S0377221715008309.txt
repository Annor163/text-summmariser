@&#MAIN-TITLE@&#
A mean-shift algorithm for large-scale planar maximal covering location problems

@&#HIGHLIGHTS@&#
The CIPS method is computational expensive in removing potential locations.The mean-shift (MS) algorithm is introduced to optimize coverage maximization.The MS has been revised for large-scale PMCLP with point-based demand.We test the performance of MSMC against the CIPS approach on many data sets.Results illustrate MSMC’s outstanding performance in tackling large-scale PMCLPs.

@&#KEYPHRASES@&#
Location,Large scale optimization,Planar maximal covering location problem,Mean shift,

@&#ABSTRACT@&#
The planar maximal covering location problem (PMCLP) concerns the placement of a given number of facilities anywhere on a plane to maximize coverage. Solving PMCLP requires identifying a candidate locations set (CLS) on the plane before reducing it to the relatively simple maximal covering location problem (MCLP). The techniques for identifying the CLS have been mostly dominated by the well-known circle intersect points set (CIPS) method. In this paper we first review PMCLP, and then discuss the advantages and weaknesses of the CIPS approach. We then present a mean-shift based algorithm for treating large-scale PMCLPs, i.e., MSMC. We test the performance of MSMC against the CIPS approach on randomly generated data sets that vary in size and distribution pattern. The experimental results illustrate MSMC’s outstanding performance in tackling large-scale PMCLPs.

@&#INTRODUCTION@&#
Covering problems in facility location have received considerable research interest due to its applicability in the real world (Farahani, Asgari, Heidari, Hosseininia, & Goh, 2012). Each facility is able to provide services within a given critical distance, i.e., the coverage radius. A customer is considered served from a facility if the distance between them is less than or equal to the facility’s coverage radius. In reality, however, budget limits often constrain the number of service facilities to be located. This gives rise to the maximal covering location problem (MCLP) (Church & Velle, 1974), which seeks to maximize the coverage of customer demands by siting a given number of new facilities. In the last 40 years, MCLP and its extensions have been widely applied to study various location issues such as planning emergency facilities (e.g., Schilling, Revelle, Cohon, & Elzinga, 1980; Eaton, Daskin, Simmons, Bulloch, & Jansma, 1985; Murray & Tong, 2007), siting telecommunications equipment (e.g., Akella, Delmelle, Batta, Rogerson, & Blatt, 2010; Oztekin, Pajouh, Delen, & Swim, 2010; Shillington & Tong, 2011), location for business (e.g., Jones & Simmons, 1993; Pastor, 1994), and public services (e.g., Hougland & Stephens, 1976; Otto & Boysen, 2014).Most MCLP models have been built under the assumption that the candidate locations of the new facilities are known in advance. In other words, facilities can only be installed in discrete nodes. Some researchers (e.g., Mehrez, 1983; Mehrez & Stulman, 1982, 1984; Church, 1984) have relaxed this constraint and extended the discrete version of MCLP to consider facility location in a continuous space, i.e., facilities are allowed to be placed anywhere on a plane. This problem is known as the planar maximal covering location problem (PMCLP), originally defined in Church (1984). For PMCLP, it is possible to attain a greater demand coverage because many more desired locations are available for selection when making strategic facility location decisions such as infrastructure investment (Murray & Tong, 2007; Wei, 2008). Murray and Tong (2007) suggested that more general representations (points, lines or polygons) of demand can also be optimally served in a region. They introduced the extended planar maximal covering location problem-Euclidean (EPMCE) and applied it to emergency warning sirens siting in Dublin. Matisziw and Murray (2009) formulated the 1-facility continuous maximal covering problem (CMCP-1), where demand is considered as continuously distributed within a whole region (convex or non-convex). They addressed CMCP-1 by generating the medial axis, which can be viewed as a geometrical representation of the region. Later in Wei (2008), the multi-facility case of CMCP was solved based on Voronoi diagrams and the geometric properties of the region. Recently, several applications of PMCLP have been reported by geographical researchers (see for example Liu & Hodgson, 2013; Wei, Murray, & Batta, 2014; Wei & Murray, 2014b).To solve PMCLP, it is natural to reduce it to MCLP by finding a finite number of potential sites on the plane. With this set of discrete sites, MCLP can be solved by either exact or heuristic approaches. In other words, PMCLP can be addressed in two phases: I – identify a candidate locations set (CLS) and II – use exact or non-exact methods to address the degraded PMCLP for coverage maximization. Therefore, it is critical to identify a good CLS for solving PMCLP. We can analyze a CLS from various angles: (1) Coverage. The objective of the MCLP is to find a solution with maximal coverage. Hence, the coverage of the candidates in a CLS is an important consideration. (2) Size of the CLS. MCLP is NP-hard (Downs & Camm, 1996), whose size is determined by the numbers of demand nodes, potential locations, and facilities to be located. For a very large-scale MCLP, we have to apply heuristic algorithms to search for the optimal solution. However, using such non-exact methods may cause a loss of coverage (obtaining a local optimal solution only), or even fail to produce a feasible solution. Therefore, given a large number of demand points, it is highly desirable to reduce the number of potential sites in Phase I before solving MCLP. In addition to the above two essential principles, decision-makers in reality may also be concerned with the following objectives. (3) Time to generate the CLS. Is there an efficient way to generate the CLS in Phase I? This research question has largely been neglected in the literature, yet it is a significant issue to address in real life. For example, when a severe disaster (e.g., a storm, an earthquake etc.) takes place, it is an urgent task for the government to decide where to site search and rescue (SAR) stations in order to provide medical and rehabilitation services for as many victims as possible in a large area. The scale of such a problem can be remarkably large as the numbers of SAR stations and victims may be numerous. In the context of such emergency situations, the decision-maker must consider not only the above features of the candidate locations, but also the efficiency of solving such large-scale PMCLPs in real time. (4) Average distance to demand. Given that serving faraway demands will incur a high cost, planners may also be concerned about the distances between the covered demand points to the closest facility.A three-step procedure, often referred to as the circle intersect points set (CIPS) method in the literature, has been the dominating technique for creating the CLS since it was introduced by Church (1984). First, this method produces a demand and intersection points set (DIPS11Note that this set was also called CIPS in Church (1984) and thus the term CIPS had two meanings: the whole method and the points set. To avoid confusion, hereafter in this paper we use the term CIPS to denote the method, and DIPS to call the demand and intersection points set generated by the CIPS method at the first step. We thank an anonymous referee for this helpful suggestion.) by exploiting the geometric properties of coverage. The circles are centered to cover demand locations with predefined coverage radii under the Euclidean distance measure. The second step in Phase I, which is optional if the DIPS only has few members, is to remove all the dominated points from the DIPS in order to reduce the size of the CLS. It has been shown that the reduced DIPS, i.e., the final CLS, contains at least one optimal solution to the PMCLP (Church, 1984). Given this CLS, the last step is to solve MCLP in Phase II. The CIPS method greatly facilitates coverage maximization and contributes significantly to size reduction. Consequently, it has been widely applied and extended in many studies (e.g., Younies & Wesolowsky, 2004; Murray & Tong, 2007; Canbolat & Massow, 2009; Yildiz, Akkaya, Sisikoglu, & Sir, 2011) as a standard approach to address PMCLP. However, the CIPS method has high time complexity and is generally unable to handle large-scale PMCLPs (see Section 2.2 for the details).In this study we propose a mean-shift based algorithm for treating large-scale PMCLPs, i.e., MSMC. In MSMC, we introduce a revised mean-shift procedure that is less time consuming, and hence more suitable for solving large PMCLPs, than the traditional CIPS method. The mean-shift procedure has been successfully adapted to various application domains, such as cluster analysis in computer vision and image processing (Comaniciu & Meer, 2002). To the best of our knowledge, this work is the first attempt to solve location problems using the mean-shift procedure. The advantages for choosing the mean-shift procedure to identify the CLS for PMCLP are discussed in detail in Section 3.2.The remainder of the paper is organized as follows: in Section 2 we review PMCLP, and discuss the pros and cons of the traditional CIPS approach. In Section 3 we briefly introduce the mean-shift procedure and the core of the proposed MSMC algorithm. We present each step of MSMC in detail. In Section 4 we compare the performance of MSMC against the CIPS method on randomly generated data sets that vary in size and distribution pattern, and discuss the experimental results to reveal the various performance aspects of the MSMC and CIPS approaches. In Section 5 we conclude the paper, discuss the research limitations, and suggest topics for future research.PMCLP seeks to maximize the demand coverage on a plane. Unlike MCLP which locates facilities on a network or discrete locations, PMCLP concerns the siting of a given number of facilities anywhere on a plane; in other words, the number of potential sites for location is infinite in PMCLP. To address this issue, there is a need to generate a set of discrete potential locations from the continuous space, essentially reducing PMCLP to MCLP. To recap, solving PMCLP can be executed in two phases, namely I – identify a CLS and II – solve the MCLP given the CLS.In this section we first present the MCLP formulation, from which PMCLP naturally arises. We then detail Phase I of the CIPS approach to solve PMCLP proposed by Church (1984). Finally, we discuss the advantages and shortcomings of the CIPS method.MCLP has been mathematically formulated by Church and Velle (1974) as follows:(1)MaximizeZ=∑i∈IwiYi.subject to(2)∑j∈ΘiXj≥Yi,foralli∈I(3)∑jXj=p,(4)Xj={0,1},forallj∈J(5)Yi={0,1},foralli∈Iwherei=indexofdemandpoints(entiresetI);j=indexoffacilitylocations(entiresetJ);wi=weightofdemandnodei;Θi={j∈J|dij≤R};dij=theshortestdistancefromitoj;R=predefinedcoverageradiusoffacility;p=numberoffacilitiestobelocated;Xj={1,ifafacilityislocatedatj,0,otherwise.Yi={1,ifdemandiiscoveredbyanysitedfacility,0,otherwise.The objective function (1) maximizes the total weighted demand served by the facilities. Constraint (2) allows Yito equal 1 only when demand point i is served by at least one facility. The number of facilities to be sited is restricted to equal p in constraint (3). Constraints (4) and (5) impose the binary integer restriction on the decision variables.Given a CLS with sizef=|CLS|,the number of possibilities of facility location is(fp)=f!/(p!(f−p)!). OR-based methods like mixed integer programming can be applied to find the best solution for small-sized MCLPs. However, when the size of the CLS is sufficiently large, exploring the huge solution space is quite time consuming and may be prohibitive, so heuristic algorithms are required to search for acceptable solutions within a reasonable time. There are both exact and non-exact approaches for various types of MCLPs in the literature. However, given that MCLP is NP-hard, it is desirable to find ways to reduce the size of the CLS in Phase I as a smaller f can save considerable computing time in Phase II.According to Church (1984), the CIPS approach can be itemized as follows22Appendix A provides the pseudo codes of the key procedures discussed in this paper, including both the CIPS and MSMC approaches. Comprehensive discussion of the time complexity of each procedure is also given.:1.Generating DIPS. Draw the covering boundary for each demand point (DP) and identify the intersection points (IPs) of the boundaries, and combine the DPs and the IPs as the DIPS.Refining DIPS (optional for small-sized DIPS). Drop all the dominated points from the DIPS as final CLS.Solving MCLP given the CLS.Suppose that there are n demand points to be covered on the plane. Our interest is to generate a CLS in an efficient way to help address a large-scale PMCLP. Fig. 1gives a simple example to show the basic process of CLS generation using the CIPS method. There are four DPs placed on the plane according to their coordinates and the coverage radiusR=1in this instance. In Step 1, four circles centered at the blue DPs are drawn as shown in Fig. 1(a). As a result, five red IPs are recognized and their coverage boundaries are also plotted in Fig. 1(b). The next step is to recognize the dominating points from the nine nodes. According to Church (1984), point A is considered to dominate point B if point A covers the same, or more than, all the demand points that point B can serve. According to this rule, all the four DPs are considered to be dominated by the four corner IPs since a facility located at any one of the DPs can only serve one DP, i.e., itself, while each corner IP covers two DPs. At the end, however, the only member of the DIPS, or the CLS, is IP5 located at the origin because it has all the four DPs covered, dominating the other IPs.Admittedly, the CIPS is excellent in creating a CLS in terms of coverage maximization since the generated DIPS is guaranteed to contain at least one optimal solution to the PMCLP (Church, 1984). However, finding the dominating points can be time consuming, especially for large-sized PMCLPs. The total amount of time required to refine the DIPS depends on not only the number of DPs, but also the pattern of the demand distribution. Given n demand points on the plane, the DIPS may have at least n members when the DPs are too far away from one another, so no IP can be created. In this case, it is optional to recognize the dominating points. In the worst case where all the DPs gather inside a circle of radius (2 * R), the DIPS has2*(n2)+n=n2members, making it necessary to remove the dominated candidate locations from the DIPS. In other words, the size of the DIPS is c ∈ [n, n2]. Before incorporating the reduction technique in Step 2, we know that the final size f of the CLS will be less than or equal to c. However, it is difficult to predict the precise f since we need to compute regardless of whether these c sets containing the covered DPs are subsets of one another. In the worst case, the complexity will tend towardsO((c2)*n)=O(n5),as detailed in Appendix A. Therefore, the CIPS method may fail to produce the CLS for a large-sized PMCLP with point-based demands in a reasonable time, as suggested by Murray and Tong (2007) and Wei (2008).Motivated by the above observations, we set out to propose an algorithm that is more efficient in creating the CLS for solving a large-scale PMCLP.The mean-shift algorithm is an iterative mode-seeking method introduced by Fukunaga and Hostetler (1975). It presents a simple but efficient way to converge to the local density maxima (LDM) in any probability distribution. During the last decade, the mean-shift algorithm has been extensively studied and successfully applied to many applications in the computer vision domain, such as real-time object tracking (Comaniciu, Ramesh, & Meer, 2000), image segmentation (Tao, Jin, & Zhang, 2007), cluster analysis (Wu & Yang, 2007) etc.In this section we present a brief review of the original mean-shift procedure. We then propose a new algorithm MSMC based on a revised mean-shift procedure to generate the CLS for solving PMCLPs.Suppose that there are n data points{yi}i=1nin a d-dimensional Euclidean space Rd. The mean-shift algorithm assumes that they are independent, identically distributed samples drawn from a population with an unknown density function f(y). The multivariate kernel density estimator obtained with kernel K(y) and bandwidth h is defined as Silverman (1986):(6)f^(y)=1nhd∑i=1nK(y−yih),where the kernel K(y) is a scalar function under the conditions given in Fukunaga and Hostetler (1975). Among the various valid kernels, the radially symmetric kernel is often selected since it is more suited for the mean-shift algorithm (Comaniciu & Meer, 2002), satisfying:(7)K(y)=ck,dk(∥y∥2),where ck,dis a positive normalization constant that makes K(y) integrate to one. The function k(x), only for x ≥ 0, is called the profile of the kernel (Comaniciu & Meer, 2002).In order to locate the modes of the density function, the gradient of the density estimator is formulated by exploiting the linearity of Eq. (6) as follows:(8)∇^fh,K(y)≡∇f^h,K(y)=2ck,dnhd+2∑i=1n(yi−y)g(∥y−yih∥2)=2ck,dnhd+2[∑i=1ng(∥y−yih∥2)]mh(y),where(9)g(x)=−k′(x),(10)mh(y)=∑i=1nyig(∥y−yih∥2)∑i=1ng(∥y−yih∥2)−y.The first term of the product in (8) is proportional to the density estimate at y computed with the kernel G(y):(11)G(y)=cg,dg(∥y∥2).Eq. (10), i.e., the second term in Eq. (8), is the mean shift, which denotes the vector from the centre of the kernel y to the weighted mean computed with the kernel G. In other words, it always points towards the direction of maximum increase in density (Comaniciu & Meer, 2002). In the event thatmh(y)=0,the mean-shift algorithm converges to the LDM where the density estimatorf^h,K(y)has a zero gradient. Hence, given the bandwidth h, also known as the window size, and the initial starting points (SPs), the mean-shift procedure comprises the following steps:1.fix a kernel (window) around each SP;compute the mean-shift vector mh(yt) within the kernel (window);shift the kernel (window) to the mean:yt+1=yt+mh(yt);go to Step 2 until convergence.The above procedure has been shown to converge at the LDM where the multivariate kernel density estimator has zero gradient (see Comaniciu & Meer, 2002; Li, Hu, & Wu, 2007). Therefore, the mean-shift algorithm is actually a gradient ascent method which has good potential to search for LDM.Fig. 2is a demonstration of applying the mean-shift algorithm on the planar where the kernelg(y)=1. As a result, the mean-shift vectormh,G(y)=1n∑i=1n(yi−y),which means all the data points have identical weights. In Fig. 2(a), a window (circle) is centered at one data point and the mean-shift vector can be calculated after searching for the covered data points. Next, the centre of the disk keeps moving to the new mean location along the shift vector until reaching a LDM, as shown in Fig. 2(b), (c), and (d). These shift vectors compose a path leading to the stationary point, which is a desirable feature of our MSMC algorithm as we will see later in this section.As this work is the first attempt to solve location problems using the mean-shift procedure, there is a need to explain the rationale for it.33We thank an anonymous referee for this helpful suggestion.We summarize the reasons for choosing the mean-shift procedure to identify the CLS for PMCLP are as follows.•The PMCLP model meets all the requirements of the mean-shift method. A plane is a two-dimensional space and the demand points on it can be viewed as samples from an unknown distribution. Furthermore, the mean-shift algorithm requires no prior knowledge of the distribution because the mean-shift vector always points in the direction of maximum increase in the density. Therefore, it can be directly applied to identify the CLS for PMCLP.The aim of the mean-shift procedure is consistent with the first criterion of the CLS: coverage. Given a coverage region, a candidate location should cover as many demand points as possible. The identified LDM could be such ideal potential sites since they are located at the centres of the densest regions and should thus, cover the majority of demand points.The number of LDM could be relatively small, which is the second principle of a CLS. Comaniciu and Meer (2002) defined the term basin of attraction of a mode as the set of all the locations that get sufficiently close to the mode and will definitely converge to it eventually. Fig. 2 is an example to illustrate how the starting point in Fig. 2(a) is captured and shifted from the initial location to the mode in Fig. 2(d). This feature allows modes to attract nearby mean locations, so reducing the size of the set of unique stationary points significantly.The mean-shift procedure consumes less time to yield the CLS, which is another theoretical advantage. It requires multiple nearest neighbor searches during the execution of Step 2, so its time complexity is O(τ * s * n), where s and n are the numbers of starting points and data points, respectively, and τ stands for the number of iterations before convergence. For the original mean-shift procedure that choose the data points as initial starting points, it takes O(τ * n2) time. Admittedly, this algorithm is not highly scalable; however, it is still much more efficient than the CIPS method, which runs in O(n5) time in the worst case. Therefore, one would expect that the larger the n is, the faster is MSMC than the traditional CIPS method.A LDM is the mean, i.e., the centre of covered demand locations. Therefore, the distances to these served nodes are minimized, which has a practical advantage over the CIPS method in terms of distance reduction, even though distance reduction is not an objective in the original PMCLP formulation. This is because siting facilities at intersection points produced by the CIPS method may not be optimal in practice if the distance factor is taken into account, as noted by Wei and Murray (2014a). For example, in Fig. 3, the four DPs are closer to one another than twice of the given radius. Therefore, 12 red IPs can be found in Fig. 3(a). After removing the dominated nodes, there are eight candidate locations left and each one is able to cover all the DPs. In other words, any one of them, i.e., four red IPs and the original DPs themselves as illustrated in Fig. 3(b), can be chosen as the only member of the CLS. If one of the blue DPs, e.g., DP3 is selected, there is an overlap between the locations of the demand nodes and facility locations; if one of the red IPs, e.g., IP1 is chosen, it is certain that two DPs will lie on their coverage boundaries. In real-world applications, such as distribution of medical supplies, the decision-maker may still prefer locating a facility on the origin of the plane since it is generally considered that a facility close to the demand points can provide a better quality of coverage (Dessouky, Ordez, Jia, & Shen, 2006; Jia, Ordez, & Dessouky, 2007). However, this kind of optimal locations in practice cannot be produced using the CIPS method.In view of the above discussion, we consider that the mean-shift algorithm is a promising technique for identifying a good CLS for solving PMCLP.Similarly, our MSMC comprises the following three steps:1.Generating DIPS.Run the revised mean-shift procedure to identify LDM as final CLS.Solving MCLP given the CLS.The original mean-shift procedure was not designed for the PMCLP. Therefore, we need to configure and revise it in order to improve its performance.The original mean-shift approach has three main parameters: starting points (SPs), the bandwidth h and the kernel function g(y). The mean-shift procedure usually launches from the data points themselves. As mentioned in previous subsection, demand points (DPs) in the PMCLP are the data points for the mean-shift procedure, thus SPs = DPs. However, we suggest that both DPs and IPs should be used as the SPs to identify the CLS, i.e., SPs = DPs + IPs = DIPS. The reason for this modification is that starting only with the DPs makes the mean-shift procedure fail to discover the critical candidate locations such as the IPs. For example, in Fig. 1(a), if we simply choose the DPs as the initial SPs, then the algorithm will stop immediately and offer four DPs as the LDM. As shown in Fig. 4, MSMC starting with the DIPS is able to recognize five more LDM, including the origin. However, the side effect of this adaptation is that it increases the time complexity. Although determining the DIPS is a computationally affordable operation with a time complexity of O(n2), the size of the DIPS raises the time complexity of the mean-shift procedure from O(τ * n2) to O(τ * n3) in the worst case. However, on weighing the performance tradeoff, we suggest that the DIPS, rather than the DPs, be used as the initial starting points for MSMC, with a view to achieving maximal coverage.Bandwidth selection is an important topic in the research community since it directly affects the performance of density estimation, especially for the tracking of size-changing objects. Many approaches have been developed to address this issue and the bandwidth of the mean-shift procedure can be automatically selected in various practical applications (see for example Comaniciu, Ramesh, & Meer, 2001; Ming, Ci, Cai, Li, Qiao, & Du, 2012). In the context of PMCLP, it is natural to set the facility’s covering distance as the bandwidth in MSMC, i.e.,h=R.The kernel function in the mean-shift process essentially imposes additional weights on the data points according to their distances to the shifting mean (Comaniciu & Meer, 2002). Cheng (1995) summarized four types of kernel functions frequently used in mean-shift research, namely flat, Gaussian, Epanechnikov, and quartic kernel. The experimental results presented in Appendix B demonstrate that the flat kernel performs best in terms of coverage. Therefore, we suggest that a flat kernel be chosen where all the data points are assumed to have the same distance weight when identifying LDM, i.e.,g(y)=1as in Fig. 2. Moreover, if the demand points are associated with different weights wiin the PMCLP model, they should be taken into consideration when solving MCLP in Phase II, rather than in Phase I. The reason for forcing weights to be equal in computing the means and shift vectors is that we are able to recognize some saddle-like LDM without considering the weights. For instance, the origin found in Fig. 4 can be viewed as a saddle point. Suppose that one of data points, say, DP1 has a larger weight, it will pull the centered LDM to the right. In that case, we will lose this candidate location on the origin.The sharp-eyed readers may find in Fig. 4 that the origin dominates the other LDM, raising the question about the necessity of removing the dominated LDM using the same reduction technique employed by the second step of the CIPS method. The main motivation to add this step after identifying the LDM is that dropping the dominated LDM will reduce the size of the CLS, consequently saving time for solving MCLP. However, this operation is actually quite computationally demanding, as analyzed in Appendix A. In other words, using the reduction procedure requires much more time than what it saves because running the solver for MCLP is usually not the bottleneck of the algorithm’s whole performance, as we will see later in the experimental results.In conclusion, the MSMC algorithm incorporates and modifies the original mean-shift procedure in order to generate CLS more efficiently for large-sized PMCLPs. Furthermore, it also borrows helpful ideas from the traditional CIPS method, such as creating the DIPS as the SPs for MSMC.In the next section we generate a host of data sets to test the performance of the MSMC algorithm against the CIPS method.For comparison purposes, we re-organize the steps of the two algorithms as two time-lines illustrated in Fig. 5. We coded all the stages in Python 2.7, combining with the CPLEX® API44The Python API of CPLEX is part of IBM’s ILOG CPLEX optimization studio (Version: 12.5).for solving MCLP in Stage 3. We conducted all the experiments on an Intel Xeon computer with two processors (2.30 gigahertz) running 64-bit Windows 7 with 32 gigabyte of RAM.At the beginning, we generated four types of test data set using the scikit-learn module, which is a well-known Python package for data mining and data analysis. Although we added the standard deviation of the Gaussian noise to the data, the generated data sets were repeated exactly for the two algorithms since the random seed was fixed. We also standardized all the data sets by removing the mean and scaling to unit variance in order to specify their x and y coordinates over the range of (−2, 2). Most of their parameters are listed in Table 1, whose values are chosen arbitrarily. Random weights between 0 and 10 were assigned to the data points of the two data sets, namely Circles and Blobs. Therefore, the only parameter we needed to control was the number of data points, which was assigned asn={20,60,100,200,300,400,500}. Additional large-scale instances wheren={1000,2000,3000}are created to test the MSMC method only, since the CIPS is unable to solve them in a reasonable time. In other words, we generated 28 data sets to evaluate and compare the performance of two algorithms, and 12 larger-scale instances to assess the performance of MSMC in solving large-sized PMCLPs. Note that as n grows, a dramatically increasing number (up ton(n−1)) of the IPs will be produced in the cramped region with a higher density, causing stress-growing tests for both algorithms.In order to obtain a wealth of information about the experimental results, we define the following metrics based on the four criteria discussed above, where the subscript α is used to denote the two algorithms (CIPS:α=1; MSMC:α=2):1.The ratio of covered demands to total demand: zα∈ [0, 1], and the gap between them:Δz=z1−z2. Since it has been shown that the CIPS method is capable of yielding the optimal solution, the coverage ratio of the CIPS z1 is always larger than or equal to z2.The average weighted distance between all the covered data points to the nearest facility:wdα=∑k∈Ωwkdk′∑k∈Ωwk>0,whereΩ={yk}k=1Kis the set of the K covered demand points anddk′denotes the distance from the covered demand point k to the nearest facility located. This indicator can be viewed as the average distance to unit demand, which is essential for the facility owner to minimize the transportation cost or time in practice. For example, in locating emergency services, it is paramount to achieve coverage as fast as possible. In the business context, a smaller wd usually means greater cost saving under the condition of same coverage.The size of the generated locations c is the number of candidate locations after Stage 1, i.e., the number of elements in the DIPS. fαis that in the CLS after Stage 2. A smaller fαstands for better performance in reducing the size of the DIPS as it directly reduces the possible solution space for solving MCLP in Phase II.The total time consumed:Tα=tα1+tα2+tα3. The three terms on the right hand side of this equation represent the time cost in the three stages, respectively. Note that the unit of these metrics is in seconds.In sum, the four groups of indicators for evaluating these two approaches can be divided into two categories, namely solution quality and solution efficiency.

@&#CONCLUSIONS@&#
