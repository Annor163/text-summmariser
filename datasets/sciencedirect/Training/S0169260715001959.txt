@&#MAIN-TITLE@&#
Reliable emotion recognition system based on dynamic adaptive fusion of forehead biopotentials and physiological signals

@&#HIGHLIGHTS@&#
A new dynamic fusion method for designing an emotion recognition system is proposed.A weight is assigned to each classifier based on its performance.The performance of the classifiers during the training and testing phases is considered.Static weights in varying contexts such as emotions do not produce acceptable results.Dynamic weighting strategy improves the performance of the system considerably.

@&#KEYPHRASES@&#
Emotion recognition system,Dynamic adaptive fusion of classification units,Forehead bioelectric signals,Physiological signals,Human computer interactions,

@&#ABSTRACT@&#
In this study, we proposed a new adaptive method for fusing multiple emotional modalities to improve the performance of the emotion recognition system. Three-channel forehead biosignals along with peripheral physiological measurements (blood volume pressure, skin conductance, and interbeat intervals) were utilized as emotional modalities. Six basic emotions, i.e., anger, sadness, fear, disgust, happiness, and surprise were elicited by displaying preselected video clips for each of the 25 participants in the experiment; the physiological signals were collected simultaneously. In our multimodal emotion recognition system, recorded signals with the formation of several classification units identified the emotions independently. Then the results were fused using the adaptive weighted linear model to produce the final result. Each classification unit is assigned a weight that is determined dynamically by considering the performance of the units during the testing phase and the training phase results. This dynamic weighting scheme enables the emotion recognition system to adapt itself to each new user. The results showed that the suggested method outperformed conventional fusion of the features and classification units using the majority voting method. In addition, a considerable improvement, compared to the systems that used the static weighting schemes for fusing classification units, was also shown. Using support vector machine (SVM) and k-nearest neighbors (KNN) classifiers, the overall classification accuracies of 84.7% and 80% were obtained in identifying the emotions, respectively. In addition, applying the forehead or physiological signals in the proposed scheme indicates that designing a reliable emotion recognition system is feasible without the need for additional emotional modalities.

@&#INTRODUCTION@&#
Emotion/affect is an internally mental perception of an object or event that can be associated with an expressive overt behavior. Incorporation of emotional intelligence in computers can improve their interactions with humans. Since human computer interactions (HCIs) have become unavoidable, more accurate HCIs that are comparable with human–human interactions are more desired and beneficial for users. Emotional skills power computers to understand and recognize human emotional states and give an appropriate response [1,2]. For example, a person who is working with a computer in a state of impatience or boredom expects the computer's response to be faster and more accurate [1]. In addition, for a student in an online learning environment, learning efficiency would increase if the computer were able to detect his or her emotions and create an appropriate situation according to this state [1]. Various applications for computers and systems identify different emotions, such as human-like communications, robotics, learning environment, entertainment, and games, providing assistance or feedback to a person, human–computer mediated systems, medicine, mental health, etc. [1–3].Given the importance of emotions in human life, a new research field related to emotional phenomena was introduced by Rosalind Picard: affective computing [1]. The goal of affective computing is to design systems that are emotionally intelligent. To design an emotional system that detects different emotions, methods utilized by humans in their everyday communications such as facial expressions, speech and sounds, gestures, and body movements [4–11] as well as physiological patterns can be used [12–23]. For example, Motta and Picard [9] and D’Mello and Graesser [10] analyzed children's physical movements with the use of a body pressure measuring system (BPMS) to evaluate their interest in a learning environment and working with computers.It has been shown that specific physiological patterns in each emotion are created. Ekman and Levenson presented the first findings that considerable changes in the autonomic nervous system (ANS) are created in accordance with emotional scenarios [24]. Since physiological signals are the result of ANS activity, they cannot be easily imitated. These signals are the same among people with different languages, cultures, and even gender and age [12,13]. In one of the first studies that used physiological signals, Picard and her colleagues recognized eight emotional states using blood volume pressure (BVP), skin conductance (SC), respiration rate (RR), and facial muscle activity [14]. They used personalized imagery to elicit the desired emotions from an actor and achieved overall recognition accuracy of 81% using hybrid linear discriminant classification. Nasoz et al. applied galvanic skin response (GSR) and heart rate variability (HRV) to recognize six types of emotions induced by selected movie clips [17]. The researchers achieved the best recognition accuracy rate of 83% with the Marquardt backpropagation algorithm. Yuan Lin and his colleagues identified four music induced emotional states (joy, anger, sadness and pleasure) from electroencephalogram (EEG) signals. They attained an average recognition accuracy of 82% using the SVM classifier [25]. In another study, Frantizidis et al. proposed a two-step classification procedure for discriminating emotional states from EEG signals [26]. The first classification level involved arousal discrimination, and then valence discrimination was performed. Using Mahalanobis distance (MD) and SVM classifiers, overall classification rates of 79.5% and 81.3%, respectively, were obtained [26]. AlZoubi et al. designed an emotion recognition system for detecting eight emotional states (e.g., boredom, confusion, curiosity, delight, flow/engagement, surprise, and neutral) during interactions between students and a tutoring system [20]. The researchers used three physiological signals (electrocardiogram (ECG), electromyogram (EMG), and GSR) and combinations. In that study, single-channel and three-channel multimodal models were generally more diagnostic than two-channel models. Recently, Khosrowabadi et al. applied a six-layer biologically inspired feedforward neural network to discriminate human emotions from EEG [27]. In that study, EEG data were collected from participants while they were subjected to audio-visual stimuli. Overall classification accuracy of 70.8% and 71.4% was obtained for arousal and valence discrimination, respectively.Since the human body system may utilize a combination of emotional methods to represent a specific emotion, multimodal emotional systems have been proposed by researchers investigating affect [11–14,17,20,22,23,28–32]. An important consideration when designing a multimodal system is combining or creating fusion between the signals information. Fusion methods are usually implemented at the level of the extracted features or the results from individual classification units [33–37]. Fusing input signals due to a lack of the same time resolution usually is not considered.In the design of a multimodal emotion recognition system, as we described some of them, the conventional method of feature fusion as well as simple fusion of classification units has been applied more frequently [11–23,28–32]. For example, Kim et al. applied the simple feature fusion scheme by concatenating the features extracted from the physiological signals to identify four types of emotional states [13]. Koelstra et al. implemented a statistical weighting scheme at the decision-level fusion of the classification units to determine the contribution of each emotional modality. They applied EEG and peripheral physiological signals as emotional measures with multimedia content analysis (MCA) [32]. Their system using fusion of all the modalities recognized arousal, valence, and liking/disliking ratings of music video-induced emotions, with F1-scores of 0.616, 0.647, and 0.618, respectively. In addition, Wagner et al. evaluated different statistical decision-level fusion such as weighted majority voting, weighted average, maximum, minimum and median rules, and more as well as cascading specialist, for designing a multimodal emotion recognition system using facial, gestural, and speech signals [11]. They obtained the best average accuracy of 55% using an emotion-adapted decision-level fusion scheme for identifying four categories of emotions: positive-high, positive-low, negative-high, and negative-low.Although in feature-level fusion complete information on the signals is available, the system's response rate is decreased due to the high-dimensional features set [11]. In addition, the created feature set may contain redundant information that is not necessary for the system. In classifier or decision-level fusion, the signals create several independent classification units, and then the individual subsystem results are combined in a predetermined manner. In this type of fusion, if some of the classifiers produce incorrect results, other subsystems compensate and ultimately create the desired output. In addition, each unit applies only the features of some of the signals; therefore, the response rate of the system can be higher than the feature-level fusion scheme [11]. Despite all these advantages, in the classifier fusion schemes some of the signals may not be sufficiently informative, and therefore, their corresponding classification units do not act properly. This can undesirably influence the final accuracy of the ensemble system. Therefore, because of the varied performances of the modalities, their acceptability in the multimodal system is not the same [11,32]. The fusion process can be improved by using an effective weighting method, such that classification units with better performance obtain greater weights and therefore contribute more to the system's results [38].In this study, a new adaptive fusion method is proposed that combines the results of individual classification units in a multimodal emotion recognition system. The work is based on the idea that more effective units should have greater impact on the system result. In this regard, a dynamic weight is assigned to each classification unit that determines its acceptability in the multimodal emotional system. The weights of the classification units are determined by considering their performance in both training and testing phases. Bioelectric signals recorded from the forehead and physiological measurements make up emotional information in our multimodal emotion recognition system. The forehead electrodes are placed in such a way that although the minimum number of electrodes is used, emotional information of the brain, facial muscle activity, and eye movements is provided. In addition, we use three types of peripheral physiological measurements common in emotion-related studies to provide sufficient emotional information for a reliable emotion recognition system.In both types of signals, the minimum number of recording electrodes and sensors is utilized. However, the possibility of further simplifications of the system is examined. The emotion recognition system should be simplified as much as possible when it is used for practical applications. We explore whether utilizing the proposed fusion method with a smaller number of electrodes and sensors in an appropriate design of the system is feasible or not. This makes the recording process less obtrusive to users.The remaining of this paper is structured as follows. In Section 2, we first briefly describe the structure of the emotion recognition system. In this section, details about the experimental procedures, the physiological signals, methodology, and the proposed adaptive fusion scheme are provided. In Section 3, we present and discuss the results. More discussions and conclusions are presented in Sections 4 and 5, respectively.Designing a reliable emotion recognition system is a challenging task. The intensity of the elicited emotion, as well as its duration, is not the same for everyone. Regarding the action of different organs in an emotional state, emotions are beyond physical reactions but include the inner sense, thoughts, and other changes of which an individual may not be aware. Emotions are associated with personality, mood, and temperaments as well as an individual's experiences [32]. Thus, accurate identification of different emotional states is a complex issue.The basic structure of the emotion recognition system used in this work is depicted in Fig. 1. In each part, different methods have been examined by affective researchers to design an acceptable emotional system [4–23]. Eliciting an accurate emotional state comparable with the factual emotional situation, performing effective preprocessing, and using appropriate methods in feature extraction and classification units can improve the performance of the system. In addition, to design an emotion recognition system, several issues must be considered, including emotion stimuli and assessment method, the model of emotions, and whether the system is user dependent or independent.Emotion stimuli and assessment method: To elicit a specific emotion, appropriate stimuli should be presented. The emotion should be elicited as accurately as possible for the person comparable with real-world emotional situation.Picard et al. introduced five factors that can influence emotion elicitation results: subject elicited versus event elicited, laboratory setting versus real world, expression versus feeling, open recording versus hidden recording, and emotion purpose versus other purpose [14].In emotion-related studies, stimuli with controlled conditions are usually presented in the laboratory environment. In this work, we used selected emotional videos to elicit the desired emotional states and a self-assessment method to verify the authenticity of the elicited emotions [39,40]. Gross and Levenson showed that categorized emotional films performed better due to their dynamic nature [39]. The international affective picture system (IAPS) [41], music [13,18,23,25], thoughts and personal imagery, remembering emotional situations and events [14,42] in addition to multimodal methods [12] are other possible approaches for eliciting different emotional states.Model of emotions: Six basic emotions (happiness, sadness, fear, disgust, anger, and surprise) proposed by Ekman's model (1987), were considered in our study [43]. In the discrete model of emotion, emotional states are considered separately since they do not have common attributes. In the dimensional model, different emotions are categorized based on their comparable characteristics in two or more dimensions. An accepted dimensional model considers emotions in two dimensions, arousal and valence [44]. Arousal ranges from the low to high level of excitation while valence presents the pleasant and unpleasant nature of emotions. Fig. 2shows the basic emotions applied in this study in a two-dimensional model of arousal and valence.User dependent or independent: The emotion recognition system in this work is user independent [11,13,23,25,26,32,45]. However, the first studies in this field involved user-dependent methods [11,14]. Since the incidence of emotions as a result of differences in individual, ethnic, and cultural characteristics is not exactly the same for all people, emotion recognition systems can be considered dependent on users. But user-dependent systems require complex and time-consuming training processes for each person. In addition, user-dependent systems are not expected to generalize for novel individuals [20]. Such a system loses comprehensiveness and generality for the desired application.Two groups of subjects participated in our experiment: one for labeling emotional stimuli and the other for recording physiological signals. The first group was composed of 25 healthy subjects with an age range of 24–28 years (mean=26.2 and SD=1.95), and the second group included 25 healthy subjects with an age range of 23–32 years (mean=25.52 and SD=2.77). All subjects were male students. The subjects in the first group were not allowed to participate in the second phase. Before the study began, all subjects filled out and signed a questionnaire and an informed consent form. In the questionnaire, we inquired about the subjects’ health status and possible unwanted effects of the emotional videos on the subjects’ mental and psychological states. Subjects with these problems are excluded from continuing the experiment. All methods in this study were approved by the University of Tarbiat Modares's Medical Ethics Committee before we collected data.According to Ekman's discrete model of emotions, six basic emotions (anger, sadness, fear, disgust, surprise, and happiness) were considered in this work [43]. To elicit a specific emotion for the individual participating in the second phase of the experiment, a video that had been previously labeled (by the subjects in the first group) for that emotion was displayed.The first group of subjects participated in multiday sessions and labeled the video clips. After the participants viewed each video, they were presented a second questionnaire in which they rated the amount of emotion induced by using a self-assessment method on a scale from 0 to 8 indicating the low to high intensity levels of the evoked emotional state (0 means not at all, 4 somewhat, and 8 extremely or a great deal) [46]. The videos were ranked for each emotional state according to the scores, and those with highest scores were applied in the second phase of the experiment for recording the physiological signals.For the second group of the subjects, to evoke the desired emotion independent of each previous one, avoid habituation effects, and reduce the effects of emotional interferences, the selected emotional videos were displayed randomly, and the desired signals were recorded at the same time. Before the experiment was started for each subject, the emotional videos were randomly numbered from 1 to 6. Then the participant randomly chose the numbers, and the corresponding videos were displayed. The experimental protocol for eliciting each emotional state is depicted in Fig. 3.Before each recording, the subjects were directed to relax with their eyes closed for 2min. In addition to creating a fixed baseline mode, before an emotional video was displayed, a video with a neutral emotional state was presented, and the corresponding physiological signals were collected. In all cases, the laboratory environment conditions and the recording process such as temperature (21–25°C), light, and humidity of the room are kept the same for all subjects as much as possible. The intensity level of the sound and brightness of the monitor were adjusted so that the subject felt comfortable. After we showed each video, we asked the subjects whether they were comfortable in the experiment environment and whether they were tired. If they were tired, the experiment was continued at another session. Table 1briefly describes the specifications of the emotional videos, the recording process, and the signals.To ensure the authenticity of the elicited emotion, at the end of each recording, a second questionnaire similar to the first group was presented to the subjects [46]. Each subject rated the amount of emotion induced by using the described self-assessment method.Recorded signals from those subjects who gave the highest score were utilized in the subsequent steps to design the emotion recognition system. If the desired emotion was not elicited for a person (with a score lower than 4), another video was displayed. Fig. 4shows the appraisal rate of each emotion during the recording process.In this study, bioelectric signals recorded from the forehead (EEG, EMG and Electrooculogram (EOG)) as well as BVP, SC, and interbeat intervals (IBI) were utilized as emotional modalities. We used the FlexComp Infiniti11http://www.thoughttechnology.com.recording device and a laptop (Core i3 CPU with speed of 2.53GHz and 4GHz of RAM) to collect the signals. In all recordings, the sampling frequency was adjusted to 2048Hz.To make a recording that contained suitable emotional information with the minimum number of electrodes, configuration of the forehead electrodes was based on studies conducted by Firoozabadi (2008) and Rezazadeh et al. (2010) [18,47,48]. This configuration collected three types of biosignals, forehead electroencephalogram (fEEG), forehead electromyogram (fEMG), and forehead electrooculogram (fEOG), which are significant modalities for emotion-related studies. The electrodes were placed bipolar with 2cm spacing. Bioelectric signals were recorded from the Frontalis and Temporalis muscles, which have important roles in lifting the eyebrows and moving the mouth and cheeks. On the Frontalis muscle, electrodes were placed at the top of the eyebrows and at the beginning of the nasal muscle. On the Temporalis muscles, the electrodes were positioned on the top and bottom of the eye line. In our configuration, the electrodes were close to the Fp1, Fp2, F7, and F8 sites of the 10–20 system (international standard for EEG electrode placement). Reference electrodes were placed on the left and right ear lobes as well. Therefore, we used three pairs of forehead electrodes to collect bioelectric signals.To record the peripheral physiological signals, related sensors were attached by straps on the subjects’ fingers. The BVP sensor was placed on the fleshy part of the first joint of the middle finger, and two SC sensors were placed on the middle joint of the index and ring fingers. The IBI signal is derived from the BVP signal automatically by the FlexComp recording device. Fig. 5shows the placement of the forehead biosignal electrodes and the peripheral physiological sensors. In the following, we present more information on the recorded physiological signals.Electroencephalogram signal provides information about the electrical activity of the human brain. According to the 10–20 system, the temporal (T3–T6), frontal (F3, F4), and prefrontal (Fp1, Fp2) lobes play important roles in emotional activities [19,49]. Previous studies suggested that the left and right hemispheres of the brain are devoted to specific types of emotions [49]. The left anterior hemisphere is associated with approach or positive emotions while the right anterior hemisphere is responsible for the withdrawal response or negative emotions [49]. Since the alpha (8–12Hz) and beta (13–30Hz) subband frequencies [26,32] are dominant at rest and in a state of excitement and alertness, respectively, the amount of brain activity in these modes could be related to the dimensional model of emotion. The ratio of the beta band power to the alpha band power is an indicator of emotional arousal [50]. The alpha and the beta band power in the left and right hemispheres can be compared to determine emotional valence [45].Electromyogram signal, which indicates muscles activity, can be used to provide emotional information (facial EMG has been more considered in this area) [20,32]. Facial expressions are created as a result of the activities of each of the 44 symmetrical facial muscles. Determination of the most effective muscles and appropriate placement of the electrodes are important in obtaining desirable emotional information. In previous emotion-related studies, several facial muscles such as the Corrugator (moves the eyebrows), the Zygomaticus major and the Levator (lifts the corners of the lips), and the Masseter (clench the jaw) have been investigated more. Each muscle creates specific patterns of arousal and valence in different emotional states; for example, the Corrugator is associated with a high level of emotional arousal. In addition, the Zygomaticus major is used more in high-valence emotions and the corrugator in low-valence emotions [20,51].Electrooculogram is a biopotential that represents the quality of eye movements. The EOG shows the movements of a single electrical dipole oriented from the retina to the cornea [52]. The EOG, pupil size, and gaze direction signals has been used in affective and HCI-related researches [32,53]. Since the new placement method uses three pairs of forehead electrodes, emotional information can be retrieved from all three bioelectric signals [47,48].Blood volume pressure is the amount of blood flow in the vessels and is usually measured with plethysmography [32]. BVP specifies the amount of infrared light reflected by the skin. Other common signals used in emotion-related studies such as heart rate (HR), HRV, and IBI (which is approximately the reverse of the heart rate) can be determined from BVP. Previous studies showed that specific BVP patterns are created in different emotional states; for example, stress can increase blood pressure [32]. Studies have also shown that anger, fear, and sadness create greater HR than disgust [54]. Furthermore, low arousal emotional states are associated with decreased BVP and HR.Skin conductance is a measurement of the electrical resistance of the skin and changes due to the activity of the perspiratory glands. SC (GSR and electrodermal activity (EDA) can be applied interchangeably) increases with an arousing emotion [20,13]. Kim et al. showed a linear correlation between skin conductance and arousal changes in emotional states [13]. The results of previous studies suggest that using skin conductance and its characteristics can help distinguish among different emotions.Extracted physiological signals should be conditioned and preprocessed to obtain acceptable and trusted results. Different noises and artifacts may affect the quality of the signals. Quality of signals is dependent on numerous factors, the most important of which are recording equipment, environment (electromagnetic interferences), placement of the measurement sensors, and the preparation and collaboration of the subjects (movements, mental concentration, etc.). However, to reduce these undesirable effects, many activities can be performed. Due to the subtle nature of the physiological measurements, noise from movement of the electrodes and leads is unavoidable.For the recorded signals, we deleted the first and last 10s of the signals that were more exposed to artifacts [13]. It is performed to remove transient noise created as a result of the subjects’ movements during the recording process, mostly at the beginning and at the end of each recording [13]. In addition, a 50Hz notch filter was applied through the recording device to avoid power line interference. Using the electrode configuration for recording prefrontal signals, by using proper filtering, the fEEG, fEMG, and fEOG parts can be extracted. A Chebyshev type II filter with −60dB attenuation at the stop band was applied. Pass bands of the filters for separating fEOG, fEEG, and fEMG signals were adjusted in 1–4Hz, 5–50Hz, and 60–500Hz intervals, respectively.For designing a reliable emotion recognition system, choosing appropriate and efficient features of the signals is of special importance. The required emotional information is generated by the selected features. If the data specifying the desired emotion are more accurate, identifying the state will be easier. In this study, we consider the combination of the time and frequency domain features for the physiological signals. The features are listed in Table 2.One of the most important considerations for the applied features in designing an emotion recognition system is their simplicity and acceptable computational speed that make them suitable for real-time applications [13]. Thus, we used simple time and frequency domain features that did not require complex transformations and heavy computations. The 192 features described in Table 2were calculated for the signals. We normalized the features extracted for each emotion in order to determine the amount of changes that occurred from relaxation (or without emotion) to that state of specific emotion [17,23]. It is also performed to minimize individual differences between the subjects in experiencing a particular emotion.The features in Table 2 were also computed for the neutral emotional states and the values for the normalized features, Fnormalized, were determined as follows:(1)Fnormalized=Femotional−FneutralFneutralwhere Femotional and Fneutral are the feature values extracted from an emotional state and a neutral emotion, respectively.To use the most salient features and eliminate non-relevant ones and therefore speed up the training phase of the system, the best subset of the features for each signal was determined. Sequential forward floating selection (SFFS) was utilized for this goal [56]. SFFS is an improved version of the sequential forward selection (SFS) method. SFFS is similar to SFS in nature but does not calculate all combinations of the features.As a result of using floating search, the SFFS method avoids the nesting problem of the selected features [56]. Initially, a subset of the features is selected based on the accuracy of the system similar to other forms of wrapper methods. Then other features are added to or removed from the subset to create the best significant selected one. This process can be terminated before the features end [56]. Fig. 6briefly represents the SFFS algorithm.Following the preparation of the feature set, it is employed for training a classifier in order to identify the desired emotional states. Two types of classification methods namely Support Vector Machine and k-nearest neighbors were evaluated in this work.The SVM aims to generate a separating hyper-plane in such a manner that the margin between data of different classes is maximized. For a given training set of input–output pairs (xi, yi), wherexi∈ℝn, i=1, 2, …, l and y∈{1, −1}l, the following optimization problem must be solved [26]:(2)minω,b,ξ12ωTω+C∑i=1lξisubjecttoyi(ωTϕ(xi)+b)≥1−ξi,ξi≥0ξiMeasures the misclassification of xi, C>0 determines the penalty parameter of the error term and ω is the normal vector of the separating hyper-plane. In addition, function ϕ maps the training vector xito a higher dimensional space. The SVM finds a linear separating hyper-plane with the maximum margin. Different kernel functions, K(xi, xj)≡ϕ(xi)Tϕ(xj), can be used to support vector classification, such as linear, polynomial, radial basis function (RBF), and sigmoid. In the present study, we used RBF kernel functionK(xi,xj)=exp−γxi−xj2[26]. The grid search method was applied to optimize the parameters γ and C[58]. In addition, we implemented a pair-wise SVM, based on the one-against-one approach, for our multiclass emotion recognition problem [59].An instance is classified by the majority vote of its k-nearest neighbors, measured by a distance function [45]. A Euclidean distance measure, defined by the following equation, was used in our system.xi1andxi2are the ith features of the first and second class respectively.(3)d=∑i=1n(xi1−xi2)2In multimodal systems, fusion can be applied at three levels: the input data, extracted features, or classification units. Fusion at the decision level, or classification unit, is of greater interest, as it has more effective advantages, which include the following:(1)Increasing the response rate of the system, as a result of breaking the features set down into several smaller sets.The possibility of using more effective features of the signals independently.Improving the accuracy of the system, as a result of the capacity of the correct units to compensate for the error of those that are incorrect.In adaptive classifier fusion, more efficient units can make a greater contribution to the results. A weight is assigned to each classifier according to specific criteria that determine its reliability and importance in the multimodal system. The proposed method in this study is based on the dynamic fusion of the classification units [60]. In the following, we define several criteria that specify the weight of each unit. The testing phase results and the performance of the classifiers in the training condition are utilized to produce the desired weight of each classifier. The first criterion is defined regarding the classification units’ performance in identifying the emotional state; while the second knowledge component takes into account the information content of the signals, regardless of the classifiers’ performance. In addition, the third criterion creates the dynamic nature of the weight of each classification unit which is determined according to the performance of the classifier during the testing phase to identify the arousal level of the intended emotion.a) Classifier commitment coefficient: This coefficient reflects the commitment level of each classifier decision in the multimodal system. Let i=1, 2,…, P be the number of classifiers; j=1, 2,…, M be the number of classes or emotional states in our study; and αijbe the activation of the ith classifier for the jth class. The commitment coefficient that is defined as follows compares the performance of the classifiers in the identification of the desired classes [60].(4)αi=αil−1M−1∑j=1j≠lMαijwhere αilis the activation of the most activated class for the ith classifier.b) Cross-classifier correlation coefficient: This measure represents the amount of agreement between different classifiers in generating the system results. In this study, the mutual information coefficient as a correlation measure is determined for the calculated features of the signals. Mutual information indicates the amount of information that one classification unit conveys about other ones [38]. The mutual informationβm,njbetween the classification units, m and n for class j, is defined as [38]:(5)βm,nj=12logΣm⋅ΣnΣm,nwhere Σmand Σnare the variances of the features utilized by the classifiers m and n, and Σm,nis the covariance between these features. Cross-classifier correlation coefficient for the classifier i, is defined as:(6)βi=1M⋅1P−1∑j=1M∑i1=1i1≠iPβi,i1jc) Classifier confidence level: This measure is the amount of confidence of each unit in the multimodal system. For example, if the goal is to identify a person, in the appropriate lighting conditions, video data are better than audio. In the present study, this level of confidence for each unit is determined according to how to detect emotional arousal. Different emotional states can be identified in terms of the arousal and pleasure levels. As a measure of the subsystem abilities, if the amount of emotional arousal identified by a classifier is the same as the results for a higher number of the classifiers, it will receive a greater weight. In the testing phase, if the number of the classifiers that identically recognize arousal level (high or low) of the desired emotion is nmax, the confidence level coefficient of each classifier is defined as follows:(7)γi=nmaxAccording to these criteria, we define the weight of each classification unit as:(8)μi=αi∑k=1Pαk⋅βi∑k=1Pβk⋅γi∑k=1Pγk(9)ωi=μi∑k=1PμkSuch that ωi≥0 for i=1, 2 and∑i=1Pωi=1. Fig. 7shows the weights calculation procedures [60].In this work, a new adaptive weighting method for fusing the results of the classification units was evaluated to design a reliable emotion recognition system. In the structure of the system, we considered three subsystems with the idea that emotional information to be utilized for each part almost equally. In the first part of the system, only the features of the fEEG signals were applied. The second part was designed based on the combination of the fEMG and fEOG features. Finally, emotional information extracted from the physiological signals was used to create the third part of the system. Fig. 8shows the system structure.A total of 55, 45, 45, 19, 16, and 12 features were determined for the fEEG, fEMG, fEOG, BVP, SC, and IBI signals, respectively, which are shown in Table 2. The features of the forehead biosignals were calculated for each of the three channels. Then the best features of the signals were specified using the SFFS feature selection method.Features selected by establishing the described subsystems were applied to identify the emotional states. The SVM and KNN classification algorithms were evaluated to identify the emotions.To evaluate the proposed dynamically weighted classifier fusion-based emotional system (DWCFES), four other emotional systems were also designed to provide a basis for comparison. These systems include the feature fusion-based emotional system (FFES), the classifier fusion-based emotional system (CFES) with the majority voting method, a statically weighted classifier fusion-based emotional system (SWCFES), and a system that applies only the α and β parameters without the dynamic component in the proposed fusion scheme (DWCFES-Gamma). To design the FFES, selected features of the signals were merged into a feature set and applied for a single classifier. In the fusion of classification units using the majority voting method, an odd number of subsystems must be used to obtain acceptable results. To avoid complexity, in this study three subsystems were considered in the structure of the emotion recognition system. For the SWCFES scheme, another adaptive weighting method that statically determines the weight of each classification unit was considered. Static weighting schemes apply only the previous performance of classification units in such a way that units with better results during the training phase contribute more to the testing condition to produce the final result of the system. In the fusion method utilized, each classification unit is assigned a weight that is inversely proportional to its error produced, given the training conditions, meaning that the units with a smaller error in the identification of the desired emotion will have a greater weight. With nonnegative and normalization constraints, ωi≥0 and∑i=1Nωi=1, the weight of ith classification unit is defined as:(10)ωi=ei−1∑i=1Nei−1,i=1,2,...,Nwhere N is the number of utilized classification units and eiis the error of the ith classification unit estimated in the training conditions. ωiin Eq. (10) is an optimal weight that minimizes the squared error of the ensemble system according to the proof presented in [61]. In the DWCFES, SWCFES, and DWCFES-Gamma schemes, the subsystem results were fused using the weighted linear combination method and the system final results were obtained.We considered the emotion recognition system user independent; thus, the features of all the subjects were merged and applied to design the systems. We used the leave on out (LOO) method for cross validation because of the small sample sizes and the high-dimensional feature set. In each case, the data point to be classified was excluded from the feature set, and the remaining data were used to train the classifiers [14].

@&#CONCLUSIONS@&#
