@&#MAIN-TITLE@&#
Walking pattern classification using a granular linguistic analysis

@&#HIGHLIGHTS@&#
Based on previous research in the field of computing with perceptions.Highly interpretable and efficient linguistic model used to recognize the gait phases.Set of parameters that characterize relevant aspects of the gait.Fuzzy rule-based classifier that discriminates among different walking patterns.Parameters capable of recognizing among five walking patterns with an accuracy of 84%.

@&#KEYPHRASES@&#
Walking pattern classification,Human gait model,Linguistic modeling,

@&#ABSTRACT@&#
Classifying walking patterns helps the diagnosis of health status, disease progression and the effect of interventions. In this paper, we develop previous research on human gait to extract a meaningful set of parameters that allow us to design a highly interpretable system capable of identifying different gait styles with linguistic fuzzy if-then rules. The model easily discriminates among five different walking patterns, namely: normal walk, on tiptoes, dragging left limb, dragging right limb, and dragging both limbs. We have carried out a complete experimentation to test the performance of the extracted parameters to correctly classify these five chosen gait styles.

@&#INTRODUCTION@&#
Walking is a learned motor behavior and the simplest, more common and effective mean of locomotion that humans use to transport by only using the body. Among other moderate types of physical activity, walking has strong benefits on a number of aspects of human health.Gait is a complex task that requires precise coordination of the neural and musculoskeletal system. The measurements from a small number of cycles are representative of the gait pattern of a person. Gait analysis consists of studying the biomechanics of this human movement. The characteristics of our steps are influenced by the shape, position and function of our nervous, muscular and skeletal structures. Therefore, identifying walking patterns can help in the diagnosis of health status, disease progression and the effect of interventions (surgery, rehabilitation or medication), among other medical situations [1–3].The importance of detecting dragging limb or walking on tiptoes patterns has been highlighted by several authors [4,5], due to the fact that a low heel or toe clearance can lead to tripping, which is accounted for up to 50% of all falls. Nevertheless, there is a lack of suitable tools that allow, both users and experts, to measure the relevant characteristics of human gait in an ubiquitous, simple and economic way.One of the most common approaches to analyze human gait from a kinematic perspective consists of placing accelerometers in the user's body. In [6], authors provide a walking pattern identification (level walking, upstairs, and downstairs) based on an accelerometer placed in the user's ankle. The use of other sensors has also been studied in [7], where authors identify bilateral heel-strike events in data from an inertial measurement unit worn on the waist. Nevertheless, most of these solutions need specific and sophisticated sensors that require the collaboration of experts to be correctly positioned in certain parts of the body or footwear.An alternative to specialized sensors is the use of current smartphones. They contain diverse and powerful sensors, e.g., GPS, microphones, cameras, compasses and accelerometers, opening up exciting areas for research in data mining and human–computer interaction. These devices are routinely carried by millions of users and exploit a great range of communications capabilities, integrated hardware and software features.The accelerations captured during walking can be identified as quasi-periodic signals, i.e., signals with repetitive temporal patterns that include some variations in period and amplitude. Other examples of this type of phenomena are electrocardiograms and vibrations of musical instruments. Popular approaches to deal with quasi-periodic signals vary from Wavelets transform [8], classical curve fitting methodologies, Hidden Markov Models [9] and Neural Networks [10,11]. Fuzzy Logic (FL) is widely used for linguistic concept modeling and system identification, thanks to its capability for dealing with problems with imprecise information. Thus, FL has been successfully applied in classification, regression and control, achieving a good interpretability accuracy trade-off [12,13].In previous research [14–16], we used Fuzzy Finite State Machines (FFSMs) within the Granular Linguistic Model of a Phenomenon (GLMP) to produce a versatile and interpretable model of the human gait. For the shake of completeness, in this paper, we provide a brief description of these two concepts.The main structure of this paper is organized as follows:•In Section 3, we extend our previous human gait model, providing more detailed information about this quasi-periodic phenomenon. We describe the internal details of the FFSM used to recognize the gait phases, achieving a highly interpretable, understandable and efficient model. It is characterized for having linguistic variables and rules that allow to understand the biomedical issues that take part in the gait movement at different levels of detail.In Section 4, we use this model to calculate a set of parameters that characterize relevant aspects of the gait. The search and selection of this set of parameters is essential to perform the subsequent discrimination of walking patterns. These parameters are the result of a thorough research based on the relationship between the accelerations and gait phases, and have proven to be a good choice to discriminate the walking patterns.In Section 5, the obtained parameters are introduced in a fuzzy rule-based classifier that allows us to discriminate among five different walking patterns, namely: normal walk, on tiptoes, dragging left limb, dragging right limb, and dragging both limbs.Finally, in Section 6, we present the complete experimentation carried out to test the performance of this proposal. Ten different subjects were asked to perform the five different walking patterns at self-selected walking speed. The obtained parameters were able to recognize among the five different walking patterns with an overall accuracy of 84%.The core of our technology is the GLMP. The main element of this structure is known as Computational Perception (CP), which is based on the concept of linguistic variable developed by Zadeh [17–19]. CPs are computational models of units of information (granules) about the phenomenon to be modeled, i.e., CPs correspond to perceptions of specific parts of the phenomenon at certain granularity degree. A CP is a tuple (A, W) described as follows:A is a vector of linguistic expressions (a0, a1, a2, …, an) that represent the whole linguistic domain of the CP. The components of A are defined by the designer in accordance to the most suitable sentences from the typically used ones in the application domain of language. These linguistic expressions will be used to generate linguistic reports about the monitored phenomenon.W is a vector of validity degrees(w0,w1,w2,…,wn), wherewi∈[0,1], assigned to each linguistic expression ai. In the application context,wirepresents the suitability of aito describe the current perception. Since the designer chooses the components of A trying to cover all the possible values of CP, the total validity should be distributed among all linguistic labels. Therefore, typically, the components of A are associated to fuzzy sets forming strong fuzzy partitions [20] in the universe of discourse of CP, i.e.,∑i=0nwi=1.A GLMP consists of a network of Perception Mappings (PMs), which are the elements used to create and aggregate CPs. Each PM receives a set of input CPs and transmits upwards an output CP. In this network, each CP covers specific aspects of the phenomenon with certain granularity degree.In this paper, we describe a type of GLMP that allows us to model the relevant features of quasi-periodic phenomena evolving in time [21], applied to the human gait.The gait cycle can be divided into two successive events: the stance phase and the swing phase. By convention, the gait cycle starts when a foot makes contact with the ground. The stance phase then lasts until the same foot is lifted off the ground, at which time the swing phase starts. These two phases can be appreciated in Fig. 1.Normally, the stance phase represents the 60% of the total gait cycle and the swing phase represents the remaining 40%. These phases can be different among individuals but, when normalized to a percentage of the gait cycle, they maintain close similarity, indicating the absence of disorders [22].In previous works in this research line, we have built a model that linguistically describes the human gait with different granularity levels. In [16], we presented a FFSM that models CPs to recognize the human gait, building a model that is dynamically self-adapted to users and linguistically describes the human gait. We used this FFSM to describe the following states of the human gait: Uninterpretable (q0), Double limb support1(q1), Right limb single support (q2), Double limb support2(q3) and Left limb single support (q4). Fig. 2shows the designed model for the linguistic description of the human gait.Here, we analyze the internal working of this FFSM in detail, increasing the granularity degree of the linguistic descriptions. This detailed analysis allows designers to obtain relevant information about amplitude and temporal changes during gait modeling. In addition, we have extended our previous GLM of the human gait by analyzing the medio-lateral acceleration sign, a new component that will be thoroughly explained in Section 3.3.Our model differs significantly from others, e.g., based on machine learning techniques, because we use a linguistic model to represent the subjective designer's perceptions of the human gait process. This model is easily understood and does not require high computational cost. Nevertheless, we could use an automatic machine learning technique to tune the elements of the FFSM as explained in [15], where the interested reader can find a more detailed description of the FFSM paradigm and its applications.Since the human gait is a quasi-periodic phenomenon, the amplitude and temporal parameters may vary easily due to the comparison between the gait of different individuals, different footwear or different walking surfaces, among others. Moreover, even when a person tries to walk uniformly, there are changes in walking speed and applied force. Our model is automatically self-adapted, recognizing the gait states regardless these variations in the amplitude and period of the signal.The following subsections briefly describe each component included in this extended GLMP version of the human gait.In the case of this PMρ, the input corresponds to the accelerations produced during the human gait with a smartphone placed in the waist and centered in the front of the person. This measure provides us the superior–inferior acceleration ax, the medio-lateral acceleration ay, and the antero-posterior acceleration azat each time instant t (Fig. 3).The PMρis a tuple (U, y, g, T), where each component is explained as follows:U are the numerical values of the accelerations module (ρ). The accelerations module is calculated asρ=ax2+ay2+az2. The possible lack of accuracy when placing the smartphone centered in waist is solved since ρ is an invariant variable respect to the smartphone orientation.y is the output CPρ, y=(Ay, Wy), which describes the possible values of the acceleration module using the vector Ay= (Small (S), Medium (M), Big (B)).g is the output function Wy=g(ρ), where ρ corresponds to the numerical input data. The validity degrees of the output CP are obtained by means of uniformly distributed trapezoidal membership functions (MFs) forming a strong fuzzy partition. The linguistic labels associated to each NL expression are defined by their vertices as follows: {S(−∞,−∞,ρ¯−σρ,ρ¯−0.25·σρ), M(ρ¯−σρ,ρ¯−0.25·σρ,ρ¯,ρ¯+0.5·σρ), B(ρ¯,ρ¯+0.5·σρ,∞,∞)}, whereρ¯is the average acceleration module and σρis the standard deviation, which are dynamically calculated in a temporal sliding window. The interested reader is referred to [16] for more information about this automatic adjustment.T is a text generation algorithm that allows generating the sentences in Ay. Here, T produces the following linguistic expressions: “The amplitude of the accelerations module is {small ∣ medium ∣ big}”.U are the numerical values of the variation of the walking direction angle (θ), where θ is the angle between the accelerations vector(ρ→), and the walking direction represented by the antero-posterior accelerations(a→z)(Fig. 3). With these numerical values we calculate the derivative of θ (θ′). The use of the derivative of this signal makes the possible accuracy errors smaller than using the absolute values of the angle.y is the outputCPθ′, which describes the possible values of θ′ using the vector Ay= (Decreasing (D), Increasing (I)). It is mainly used to help us to determine when the double limb support phases finish.g is the output function similar to the corresponding one of PMρ. Here, the two linguistic labels associated to each NL expression are represented with trapezoidal MFs defined by their vertices as follows: {D (−∞, −∞, −0.1, 0.1), I (−0.1, 0.1, ∞, ∞)}.T produces the following linguistic expressions: “The walking direction angle is {decreasing ∣ increasing}”.U are the numerical values of the sign of the average medio-lateral acceleration (ay) during a temporal moving window of 0.5s.y is the outputCPa¯y, which describes the possible values of the sign ofa¯yusing the vector Ay= (Negative (N), Positive (P)). Since, a priori, the movement of both limbs are similar, this CP is mainly used to distinguish between the two legs.g is the output function that calculates the validity degrees of the outputCPa¯y. Here, the two linguistic labels associated to each NL expression are represented with trapezoidal MFs defined by their vertices as follows: {N (−∞, −∞, −0.05, 0.05), P (−0.05, 0.05, ∞, ∞)}.T produces the following linguistic expressions: “The average of the medio-lateral acceleration is {negative ∣ positive}”.U is the vector of input CPs, U=(u1, u2, …, um), where uiare tuples (Ai, Wi) and m the number of input CPs. Here, U are the input CPStateand the timespan t. Each state qihas an associated duration di, which is numerically calculated using t.y is the output CPDwhich describes the duration of each state qi. The possible linguistic expressions to describe this perception are contained in the vector: Ay= (too short to change, normal to change, normal to stay, too long to stay).g is the output function that calculates the validity degrees of CPD. When the validity degree of the state qi(wqi) takes the value zero, its duration ditakes value zero. However, whenwqitakes values greater than zero, its duration diincreases its value according to the time, measuring the duration of the state.Based on the duration diof each state, we define two temporal conditions described as follows:•The time to stay is the maximum time that the signal is allowed to stay in state qi. We define two linguistic labels that describe this perception: {normal to stay, too long to stay}.The time to change is the minimum time that the signal must be in state qibefore changing to other state qj. We define other two linguistic labels that describe this perception: {too short to change, normal to change}.According to the dynamic adjustment of these temporal conditions, the vertices that define their MFs are expressed in terms of the gait period K, that initially takes value 1 (1 gait cycle per second). The way of calculating this period will be explained in Section 4 (Eq. (7)).In this application, due to the similarities between the states q1 and q3, their temporal constraints are the same and they are represented by trapezoidal MFs defined by their vertices as follows:•Time to stay: {normal (−∞, −∞, 0.2·K, 0.3·K), too long (0.2·K, 0.3·K, ∞, ∞)}.Time to change: {too short (−∞, −∞, 0.05·K, 0.15·K), normal (0.05·K, 0.15·K, ∞, ∞)}.In the same way, the states q2 and q4 share the temporal constraints, which are defined as:•Time to stay: {normal (−∞, −∞, 0.35·K, 0.45·K), too long (0.35·K, 0.45·K, ∞, ∞)}.Time to change: {too short (−∞, −∞, 0.2·K, 0.3·K), normal (0.2·K, 0.3·K, ∞, ∞)}.T produces the following linguistic expressions: “The duration is {normal ∣ too long} to stay in qiand {too short ∣ normal} to change to qj”, with i∈{1, 2, 3, 4}.Here, we show how to build a FFSM using the aggregation of CPs. The set of components of this PM is explained as follows:U is the vector of input CPs: {CPρ,CPθ′,CPa¯y, CPD, CPState}.y is the output CPState, which describes the possible states of the gait as the states of a FFSM. Ay=(Uninterpretable (q0), Double limb support1(q1), Right limb single support (q2), Double limb support2(q3), Left limb single support (q4)). Note that the output CPStateis also an input of the vector U, that feeds the PMStateand calculates the next state at time instant [t+1] taking into account the current state at time instant [t].g is the aggregation function Wy=g(W1, W2, …, Wm), where Wiare the vectors of validity degrees of the m input CPs. The set of amplitude conditions (CPρ,CPθ′,CPa¯y), the temporal condition (CPD) and the current state of the signal (CPState) implement this FFSM model.The aggregation function g implements a set of fuzzy rules according to the states diagram showed in Fig. 4. We have rules to remain in each state qi(Rii) and rules to change from state qito state qj(Rij). The set of rules has been designed using the experts’ knowledge in human gait as follows:R11:IF (State[t] is q1) AND (ρ is B) AND (d1 is normal to stay in q1)THEN (State[t+1] is q1)R22:IF (State[t] is q2) AND (((ρ is S) OR (ρ is M)) OR (θ′ is D)) AND (d2 is normal to stay in q2)THEN (State[t+1] is q2)R33:IF (State[t] is q3) AND (ρ is B) AND (d3 is normal to stay in q3)THEN (State[t+1] is q3)R44:IF (State[t] is q4) AND (((ρ is S) OR (ρ is M)) OR (θ′ is D)) AND (d4 is normal to stay in q4)THEN (State[t+1] is q4)R12:IF (State[t] is q1) AND (((ρ is S) OR (ρ is M)) OR (θ′ is D)) AND (d1 is normal to change to q2)THEN (State[t+1] is q2)R23:IF (State[t] is q2) AND (ρ is B) AND (d2 is normal to change to q3)THEN (State[t+1] is q3)R34:IF (State[t] is q3) AND (((ρ is S) OR (ρ is M)) OR (θ′ is D)) AND (d3 is normal to change to q4)THEN (State[t+1] is q4)R41:IF (State[t] is q4) AND (ρ is B) AND (d4 is normal to change to q1)THEN (State[t+1] is q1)R01:IF (State[t] is q0) AND (ρ is B) AND (a¯yis positive)THEN (State[t+1] is q1)R03:IF (State[t] is q0) AND (ρ is B) AND (a¯yis negative)THEN (State[t+1] is q3)Ri0:ELSE (State[t+1] is q0)where State[t] is the gait phase at time instant t, ρ is the amplitude of the accelerations module, diis the duration of the state qiand State[t+1] is the gait phase in the next time instant. The final values of this gait phase State[t+1] are calculated as a weighted average of the individual rules, where the weight of each rule Rijcorresponds to its firing degree τij. This firing degree is calculated using the product for the AND operator and the bounded sum of Łukasiewicz [23] for the OR operator. We chose the state Uninterpretable (q0) as initial state of the phenomenon, having a validity degreewq0=1. The validity degree of the state q0 is obtained by means of Eq. (1):(1)wq0[t+1]=1−∑i=0n∑j=1nτijif∑i=0n∑j=1nτij≤10if1<∑i=0n∑j=1nτijThe validity degree of the rest of states is calculated by means of Eq. (2):(2)wqj[t+1]=∑i=0nτijif∑i=0n∑j=1nτij≤1∑i=0nτij∑i=0n∑j=1nτijif1<∑i=0n∑j=1nτijT produces linguistic expressions that can be adapted depending on the situation. When the signal is in the states q1, q2, q3 or q4 the template is the following: “The gait is in the {Double limbsupport1∣ Right limb single support ∣ Double limbsupport2∣ Left limb single support} phase”. However, when the signal is in the state q0, i.e., the model is not able to recognize the input signal properly, the system reports the following expression: “The model cannot explain the current situation”.This section describes how to calculate those parameters that can discriminate among different walking patterns. First, we describe how to calculate them at the level of each state or gait phase. Second, we divide each state in order to obtain additional parameters at the initial, middle and final level of each phase. Then, we also calculate other ones associated to the homogeneity and symmetry of the gait. Finally, we calculate some additional features that combine parameters from the same gait phase of the two different limbs.Based on the phases identification presented in Section 3, we can obtain a set of characteristic parameters of each state qi. Each phase can be represented by a rectangle of variable dimensions, as shown in Fig. 5. Note that this graphical representation allows us to acquire a first visual perception of the signal characteristics. The parameters that define these rectangles are calculated as follows:•Ti: Is the duration of state qiwithin a gait cycle. This duration corresponds to the time that the validity degree of the state qiis greater than zero, i.e.,wqi>0.ti¯: The horizontal coordinate of the center of each rectangle is the temporal “center of mass” of the accelerations module in the state qi:(3)ti¯=∑t=0Tit·ρ[t]·wqi[t]∑t=0Tiρ[t]·wqi[t]ρi¯: The vertical coordinate of the center of each rectangle is the average of the accelerations module during the state qi:(4)ρi¯=∑t=0Tiρ[t]·wqi[t]∑t=0Tiwqi[t]σti: The width of each rectangle is the standard deviation of the temporal distribution of the accelerations module during the state qi:(5)σti=∑t=0Ti(t−ti¯)2·ρ[t]·wqi[t]∑t=0Tiρ[t]·wqi[t]σρi: The height of each rectangle is the standard deviation of the accelerations module during the state qi:(6)σρi=∑t=0Ti(ρ[t]−ρi¯)2·wqi[t]∑t=0Tiwqi[t]The period of the gait is dynamically calculated using the parameterti¯. When each state qifinishes, a new period Ki[n] is obtained asti¯[n]−ti¯[n−1], being n the current gait cycle and n−1 the previous one. To prevent the system from being too sensitive to disturbances, we have introduced an inertia parameter (α=0.8) which avoids that the period of the signal K[t] changes abruptly. Thus, the period is calculated as follows:(7)K[n+1]=αK[n]+(1−α)Ki[n]where K[n] is the previous calculated period.Apart from obtaining the characteristic rectangles of each phase, and according to the typical decomposition in gait analysis, we divide each state qiinto three equal parts, namely, initial, middle and final subphases. In the double limb support phases, these subphases go from the heel strike and foot-flat of one limb, to the toe-off of the another one. In the single limb support phases, these subphases correspond to the initial swing, midswing and terminal swing. Then, we calculate, as explained at the beginning of this section, the representative rectangles of each of these subphases, as shown in Fig. 6.Here, we are interested in the relationship among rectangles of the same state qi. Specifically, we analyze the evolution of the vertical coordinate of the center of each rectangle for the three representative rectangles of each state (ρ¯ii,ρ¯imandρ¯if). As we will explain in Section 5, the relation among these vertical coordinates will be different according to the gait style of users. Fig. 7shows the typical relation between these three parameters according to the walking pattern. As we can visually appreciate, there is a characteristic pattern depending on whether the user is walking normal, on tiptoes or dragging the limb.In order to measure how these vertical coordinates vary, we calculate two additional parameters for each state, namely Aiand Ωithat calculate the difference among these values as follows:(8)Ai=∑n=1Nρ¯im[n]−ρ¯ii[n]NΩi=∑n=1Nρ¯if[n]−ρ¯im[n]Nwhere, n is the gait cycle number and N is the total number of gait cycles in the recorded data.Once the phases and subphases are recognized, we calculate two relevant features of the human gait (homogeneity and symmetry) for three of the characteristic parameters explained in Section 4.1, specifically, forρi¯,σρiand Ti.The homogeneity of a gait is obtained by comparing the same phase in two consecutive gait cycles. Thus, we can analyze the homogeneity during each state qiand obtain the parametersHρ¯i,HσρiandHTi.The symmetry of a gait is obtained by comparing a phase with its equivalent, i.e., by comparing the swing phases of the left and the right limbs or the two different double limb support phases. Thus, we can analyze the symmetry during the double limb support phases and obtain the parametersSρ¯13,Sσρ13andST13, or during the swing phases and obtain the parametersSρ¯24,Sσρ24andST24.Both, homogeneities and symmetries of the gait, are calculated using the Jaccard index [24], whose similarity function J(A, B) is defined in Eq. (9):(9)J(A,B)=1ifA=B=0A∩BA∪Botherwisewhere the intersectionA∩Bis implemented as min(A, B) and the unionA∪Bas max(A, B).For example, consider that the vertical coordinate during the left swing phaseρ¯2has been 7.89m/s2 and 7.34m/s2 during two consecutive gait cycles k−1 and k, respectively. Therefore, the homogeneity of these two cycles will be calculated using Eq. (10):(10)Hρ¯2[k]=J(ρ¯2[k],ρ¯2[k−1])=min(ρ¯2[k],ρ¯2[k−1])max(ρ¯2[k],ρ¯2[k−1])=7.347.89=0.93To generalize some of the parameters explained above, we have calculated six additional parameters that combine two of them, corresponding to the same gait phase that were calculated for each different limb, in a single one:•A13: is calculated as the average value between the parameters A1 and A3, i.e., the variation of the vertical coordinate between the middle (ρ¯im) and initial(ρ¯ii)subphases of the double support phase (q1 and q3).A24: is calculated as the average value between the parameters A2 and A4, i.e., the variation of the vertical coordinate between the middle(ρ¯im)and initial(ρ¯ii)subphases of the swing phase (q2 and q4).Ω13: is calculated as the average value between the parameters Ω1 and Ω3, i.e., the variation of the vertical coordinate between the final (ρ¯if) and middle (ρ¯im) subphases of the double support phase (q1 and q3).Ω24: is calculated as the average value between the parameters Ω2 and Ω4, i.e., the variation of the vertical coordinate between the final (ρ¯if) and middle (ρ¯im) subphases of the swing phase (q2 and q4).T31: is calculated as the average difference between the durations of the double support phases (q3 and q1) within a complete gait cycle.T42: is calculated as the average difference between the durations of the swing phases (q4 and q2) within a complete gait cycle.One limitation of the gait analysis arises from the variability of each walking pattern within the general population. Even people with no neuromusculoskeletal pathology have very different gait patterns. For this reason, experts need a tool that objectively helps them in the diagnosis of health status, disease progression and the effect of interventions in those pathologies that affect the ability to walk properly. Nowadays, to monitor the evolution of some rehabilitation processes experts use, among others, subjective methods that consist of recalling questionnaires. The alternative consists in going to clinical laboratories that analyze movement, or placing specific and sophisticated sensors to provide an objective measure that allows them to monitor the gait evolution of patients. In this sense, there is a lack of suitable tools that measure the relevant characteristics of human gait in a ubiquitous, simple and economic way. In this paper, we propose a system that allows experts to identify five well known walking patterns while they can monitor the evolution of patients along the time by only using a simple smartphone.With the aim of distinguish among different walking patterns, we initially tried to capture and identify the relevant gait phases and a big amount of parameters that are representative of these phases. Once this first step was completed, we noticed that these data provide enough information for this identification purpose.However, since our goal consists of providing a granular linguistic analysis for this identification, we tried to reduce the number of available parameters. Therefore, the second step consisted of applying a feature selection technique in order to extract the most discriminatory among the initial 50 parameters obtained from the gait analysis explained in Section 4. There are many machine learning algorithms that can be used to select the most appropriate attributes for a classification task. In this work, we have tried some of them, e.g., Support Vector Machines as attribute evaluator and a ranker as search method. In order to keep the interpretability of the whole system, we have finally chosen the decision tree algorithm proposed by Quinlan [25]. This algorithm chooses the most promising attribute to split on at each point and should, in theory, never select irrelevant or unhelpful attributes. The attribute space searching was done with a forward selection using a 10-fold cross validation. It was implemented using Weka software [26], a well-known machine learning tool.Finally, once we get a set of relevant features, we choose those ones that make sense from an interpretable point of view. Moreover, during this third step, we observed that there were some discriminatory parameters that can represented by the single ones explained in Section 4.4. Therefore, we chose five discriminative parameters that were fuzzified by means of linguistic labels whose definition has been obtained from the experimental training data:•A24 is defined by two linguistic labels represented by their vertices as follows:{Negative (−∞, −∞, −1.8, −1.4), Zero (−1.8, −1.4, ∞, ∞)}.Ω13 is defined by two linguistic labels represented by their vertices as follows:{Negative (−∞, −∞, −1.3, −1), Positive (−1.3, −1, ∞, ∞)}.Ω24 is defined by two linguistic labels represented by their vertices as follows:{Zero (−∞, −∞, 0.8, 1.3), Positive (0.8, 1.3, ∞, ∞)}.Sρ¯13is defined by two linguistic labels represented by their vertices as follows:{Low (−∞, −∞, 68, 72), High (68, 72, ∞, ∞)}.T42 is defined by two linguistic labels represented by their vertices as follows:{Negative (−∞, −∞, −1.5, 1.5), Positive (−1.5, 1.5, ∞, ∞)}.As we can appreciate in Figs. 8–11, it is easy to visually differentiate among walking patterns using these parameters. These figures represent in dashed lines the cutoffs of the linguistic labels explained before. Fig. 8 shows the use of Ω13 to discriminate between “normal” and “on tiptoes” patterns; Fig. 9 shows the use of A24 and Ω24 to discriminate the “dragging” pattern, does not making distinction among dragging both legs or only one of them; Fig. 10 shows that the symmetry during the double limb support(Sρ¯13)is much lower when the user is “dragging one limb” that in the rest of patterns; and, finally, Fig. 11 shows the use of T42 to discriminate between “dragging left limb” and “dragging right limb” patterns. In this way, we have designed a highly interpretable model that is able to linguistically describe the reasons that produce a specific walking pattern identification.Once we know which the five discriminatory parameters are, we used them to build an expert knowledge fuzzy rule-based classifier that differentiates among the chosen walking patterns. The set of fuzzy rules that represents this fuzzy rule-based classifier is the following:R1:IF (A24 is negative) AND (Ω13 is negative)THEN (the user walks normal)R2:IF (A24 is negative) AND (Ω13 is positive)THEN (the user walks on tiptoes)R3:IF (A24 is zero) AND (Ω24 is positive) AND (Ω13 is negative)THEN (the user walks normal)R4:IF (A24 is zero) AND (Ω24 is positive) AND (Ω13 is positive)THEN (the user walks on tiptoes)R5:IF (A24 is zero) AND (Ω24 is zero) AND (Sρ¯13is high)THEN (the user drags both limbs)R6:IF (A24 is zero) AND (Ω24 is zero) AND (Sρ¯13is low) AND (T42 is negative)THEN (the user drags the left limb)R7:IF (A24 is zero) AND (Ω24 is zero) AND (Sρ¯13is low) AND (T42 is positive)THEN (the user drags the right limb)where the firing degree τiof each rule Riis calculated using the product for the AND operator. The fuzzy rule-based classifier can be graphically represented by the fuzzy decision tree showed in Fig. 12.Note that the set of fuzzy rules provides a linguistic explanation about the obtained walking pattern. For example, when the system concludes that the user is dragging the right limb, it can be explained analyzing the antecedents of rule R7:•A24 is zero, which means that the initial and middle subphases of the swing phases are similar.Ω24 is zero, which means that the middle and final subphases of the swing phases are similar.Sρ¯13is low, which means that symmetry during the double supports is low because one limb is being dragged.T42 is positive, which means that the duration of state q4 (swing phase of the dragged limb) is greater than the duration of state q2 (left swing phase).Therefore, the classifier will provide the following message: “the subject is dragging the right limb, because the duration of the right limb swing phase is greater than the one corresponding to the left limb. Moreover, the symmetry during the double limb support is low due to this dragging right limb”. For a more detailed description about linguistic description of complex phenomena, we address the interested reader to [27,28].This section presents the experiments performed to show the potential and effectiveness of our approach. This experimentation can be considered only the first step for testing the viability and possibilities of a future complete and functional tool. We will need the collaboration of experts in different pathologies to tune the gait model and overcome the analysis of gaits affected by real physical or mental disorders, such as cerebral palsy and Parkinson's disease. The following subsections detail the most relevant aspects of this experimental phase.The objective of the experimentation was to demonstrate the discrimination power of the obtained parameters. With this aim, we divided it into a training and a test stage.During the training stage, we performed the feature selection to extract the relevant parameters, and tuned the fuzzy membership functions and rules of the fuzzy rule-based classifier as explained in Section 5.Then, in the test stage, we evaluated the capability of the designed model to correctly identify among the five different walking patterns.In order to evaluate the proposed approach, we collected the acceleration signals of 10 healthy adults, 5 women and 5 men, with ages ranging between 24 and 57 years (with an average age of 34 years).In order to collect the accelerations produced during walking, we developed an application based in the Android platform that allows the smartphone to acquire and store the accelerations produced during the gait. We have chosen Android-based smartphones because the Android operating system is free, open-source, and it has become a dominant entry in the smartphones marketplace.We attached the smartphone to a belt, on the waist and centered in the front of the person. The smartphone provides the timestamp, the superior–inferior (ax), the medio-lateral (ay), and the antero-posterior (az) accelerations with an average acquisition frequency of 100Hz.Each person received detailed instructions about how to perform each of the five different walking patterns and was asked to walk a distance of approximately 30m on a flat surface at a self-selected walking speed. The whole data collection process was always supervised by the same person, in order to avoid errors when placing the smartphone and with the aim of keeping a repetitive methodology.The collected data comprises approximately 20 complete gait cycles, in such a way that we were able to automatically extract around 15 complete gait cycles discarding the first and last steps, which are not very stable. This process was repeated five times for each walking pattern, producing a total of 25 datasets for each person and resulting in a total of 250 different gaits (50 gaits for each walking pattern).We used 200 gaits (40 of each walking pattern from all of the participants) in the training stage (80% of the available data) to perform the feature selection and tunning of the fuzzy membership functions and rules of the fuzzy rule-based classifier. The remaining 50 gaits (10 of each walking pattern from all of the participants) were used in the test stage (20% of the available data) to evaluate the performance of our proposal.

@&#CONCLUSIONS@&#
