@&#MAIN-TITLE@&#
Using hierarchical data mining to characterize performance of wireless system configurations

@&#HIGHLIGHTS@&#
We present a statistical framework for assessing wireless systems performance.A performance database of targeted WCDMA simulation runs is collected.Hierarchical data mining is used to find acceptable performance regions.The mined regions’ shapes illustrate the joint influence of parameters.Data mining assists the characterization of wireless systems performance.

@&#KEYPHRASES@&#
Wideband code division multiple access,Bit error probability,Simulation database analysis,Performance assessment,

@&#ABSTRACT@&#
This paper presents a statistical framework for assessing wireless systems performance using hierarchical data mining techniques. We consider WCDMA (wideband code division multiple access) systems with two-branch STTD (space time transmit diversity) and 1/2 rate convolutional coding (forward error correction codes). Monte Carlo simulation estimates the bit error probability (BEP) of the system across a wide range of signal-to-noise ratios (SNRs). A performance database of simulation runs is collected over a targeted space of system configurations. This database is then mined to obtain regions of the configuration space that exhibit acceptable average performance. The shape of the mined regions illustrates the joint influence of configuration parameters on system performance. The role of data mining in this application is to provide explainable and statistically valid design conclusions. The research issue is to define statistically meaningful aggregation of data in a manner that permits efficient and effective data mining algorithms. We achieve a good compromise between these goals and help establish the applicability of data mining for characterizing wireless systems performance.

@&#INTRODUCTION@&#
Data mining is becoming increasingly relevant in simulation methodology and computational science [13]. It entails the ‘non-trivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data’ [5]. Data mining can be used in both predictive (e.g., quantitative assessment of factors on some performance metric) and descriptive (e.g., summarization and system characterization) settings. Our goal in this paper is to demonstrate a hierarchical data mining framework applied to the problem of characterizing wireless system performance.This work is done in the context of the S4W problem solving environment [16,9]—‘Site-Specific System Simulator for Wireless System Design’. S4W provides site-specific (deterministic) electromagnetic propagation models as well as stochastic wireless system models for predicting the performance of wireless systems in specific environments, such as office buildings. S4W is also designed to support the inclusion of new models into the system, visualization of results produced by the models, integration of optimization loops around the models, validation of models by comparison with field measurements, and management of the results produced by a large series of experiments. In this paper, we study the effect of configuration parameters on the bit error probability (BEP) of a system simulated in S4W.The approach we take is to accumulate a performance database of simulation runs that sweep over a targeted space of system configurations. Akin to [14], this database is then mined to obtain regions of the configuration space that exhibit acceptable average performance. Exploiting prior knowledge about the underlying simulation, organizing the computational steps in data mining, and interpreting the results at every stage, are important research issues. In addition, we bring out the often prevailing tension between making statistically meaningful conclusions and the assumptions required for efficient and effective data mining algorithms. This interplay leads to a novel set of problems that we address in the context of the wireless systems performance domain.Data mining algorithms work in a variety of ways but, for the purposes of this paper, it is helpful to think of them as performing systematic aggregation and redescription of data into higher-level objects. Our work can be viewed as employing three such layers of aggregation: points, buckets, and regions. Points (configurations) are records in the performance database. These records contain configuration parameters as well as unbiased estimates of bit error probabilities that we use as performance metrics. Buckets represent averages of points. We use buckets to reduce data dimensionality to two, which is the most convenient number of dimensions for visualization. Finally, buckets are aggregated into 2D regions of constrained shape. We find regions of buckets where we are most confident that the configurations exhibit acceptable average performance. The shapes of these regions illustrate the nature of the joint influence of the two selected configuration parameters on the configuration performance. Specific region attributes, such as region width, provide estimates for the thresholds of sensitivity of configurations to variations in parameter values.Our major contribution is the development of a statistical framework for assessing wireless system performance using data mining techniques. The following section outlines wireless systems performance simulation methodology and develops a statistical framework for spatial aggregation of simulation results. Section 3 demonstrates a substantial subset of this framework in the context of a performance study of WCDMA (wideband code division multiple access [7]) systems that employ two-branch STTD (space–time transmit diversity [1]) techniques and 1/2 rate convolutional coding (forward error correction codes [7]). We study the effect of power imbalance between the branches on the BEP of the system across a wide range of average signal-to-noise ratios (SNRs). Section 4 extends the statistical framework to support computation of optimized regions of the bucket space. Such regions are computed by a well-known data mining algorithm [4,18]. Section 5 applies these concepts to the example in Section 3. Section 6 summarizes present findings and outlines directions for future research.Temporal variations in wireless channels have been extensively studied in the literature [19]. The present work uses a Monte Carlo simulation of WCDMA wireless systems to study the effect of these variations. The simulation traces a number of frames of random information bits through the encoding filters, the channel (a Rayleigh fading linear filter [6]), and the decoding filters. The inputs are hardware parameters, average SNR, channel impulse response, and the number of frames to simulate. The output is the bit error rate—the ratio of the number of information bits decoded in error to the total number of information bits simulated. Simulations of this kind statistically model channel variations due to changes in the environment and device movement across a small geographical area (small-scale fading[6]). We refer to this kind of channel variation as temporal variation because a system is simulated over a period of time. Further, we say that a given list of inputs to the WCDMA simulation is a configuration or a point in the configuration space.Spatial variations are due to changes in system configurations. We use this term to describe two quite different phenomena: changes in the average SNR and channel impulse response due to large-scale fading[6] and variations of hardware parameters. A typical approach to the analysis of spatial variations is to run several temporal variation simulations (i.e., compute bit error rates—BERs—at several points within a given area of interest) and plot 1D or 2D slices of the configuration space, as shown in Fig. 1. In this paper, we augment this approach with statistically meaningful aggregation of performance estimates across several points. The result of this aggregation is a space of buckets, each bucket representing the aggregation of a number of points. Moving up one level of aggregation in this manner allows us to bring data mining algorithms to operate at the level of buckets. The space of buckets mined by the data mining algorithm is then visualized using color maps. The color of each bucket is the confidence that the points (configurations) that map to this bucket exhibit acceptable average performance.Table 1summarizes some of the syntactic conventions used in this paper. Mathematically, we can think of the WCDMA simulation as estimating the mean E[xk] of a random variable xkwith some (unknown) distribution (xkis one when the information bit is decoded in error or zero when it is decoded correctly) [10]. Each BERxˆkj,1⩽j⩽nk, output by the simulation is an unbiased estimate of the BEP E[xk] of the simulated configuration ck. Instead of building a detailed stochastic model of the simulation (analytically, from the distribution of xk), we choose to work with the simpler distribution of the BEPbk=E[xk]≈xˆkj. Thus, each sample from the distribution of bkis realized by simulating a number of frames and obtaining an estimate of E[xk]. The distribution of bkis approximately Gaussian due to the Central Limit Theorem. Technically, we assume that the number of frames per estimatexˆkjis ‘large enough’ so that the Lindeberg condition is satisfied, that the variance ofxˆkjis finite, and that{xˆkj}j=1nkare i.i.d (independent and identically distributed). The i.i.d condition is very common in statistics and used to refer to the case that the random variables all have the same distribution (identically distributed) and are mutually independent of each other (independent), i.e., they do not exhibit correlations or other dependencies. We say that E[bk]=E[E[xk]] is the expected BEP of configuration ckunder Rayleigh fading.Let us now aggregate several points (i.e., random variables) into one bucket. The purpose of this aggregation is to reduce data dimensionality to a size that is easy to visualize, usually one or two dimensions. The basic idea is to linearly average all points that map to the same bucket but we must do so carefully, in order to preserve a meaningful statistical interpretation. Let{bk}k=1nbe Gaussian random variables with means{E[bk]}k=1nand variancesσk2k=1n. As in the previous paragraph, let each such variable bkbe the estimated BEP of some configuration ck, 1⩽k⩽n. For bucket C, define a bucket (mixture) random variable B as the convex combinationB=∑k=1npkbk,where the pk⩾0 and∑k=1npk=1. It is convenient to make{pk}k=1nthe probabilities of occurrence of the configurations{ck}k=1nin the dataset being analyzed. This setup underlines the dependence of the outputs on the distribution of the inputs and frees the user from having to provide values for the constants{pk}k=1n. It is well known that, as long as{bk}k=1nare mutually independent and Gaussian with means{E[bk]}k=1nand variancesσk2k=1n, B is Gaussian with meanE[B]=∑k=1npkE[bk]and varianceΣ2=∑k=1npk2σk2[2]. The expected value E[B] of the random variable B can be viewed as the expected BEP of bucket C={c1,c2,…,cn} in a Rayleigh fading environment, conditional on the (discrete) distribution of the configurations in C.The values{pk}k=1nare what the statisticians call prior probabilities. For most purposes of this paper, we simply estimate{pk}k=1nfrom available data. These values are explicitly or implicitly constructed during experiment design and we assume that they remain constant during experiment analysis. However, one can collect additional data as long as doing so does not change{pk}k=1n. Prior probabilities can come from a number of sources: channel sounding measurements, propagation simulations, hardware and budget constraints, or even educated guesses by wireless system designers. The rest of the paper silently assumes that the values{pk}k=1nhave been established beforehand. It is important to remember that even though the prior probabilities are for the most part transparent to the analysis presented here, they nonetheless always exist and all conclusions of data analysis are made conditional on the prior probabilities.This discussion of{pk}k=1ncan be interpreted as a deferral of the exact definition of B until experiment setup, or as parameterization of the analysis procedure. A natural question is whether or not this level of parameterization is sufficient. It is sufficient for the purposes of this paper but, strictly speaking, the interrelations between{bk}k=1nshould also be defined during experiment setup. Mutual independence of{bk}k=1nis a simplifying assumption and it might be desirable to model interactions between{bk}k=1nin practice. This implies adding covariance terms to Σ2 and re-thinking the distribution of B. Such analysis is necessarily specific to a particular experiment. For the sake of simplicity, the rest of this paper assumes mutual independence of variables in a given bucket.Point and bucket estimates of the expected BEP are meaningful performance metrics for wireless systems. Let us also estimate our confidence in these estimates. Confidence analysis enables wireless system designers to make more practical claims than point estimates alone. A statement of the form ‘this configuration will exhibit acceptable performance in 95% of the cases’ is often preferable to a statement of the form ‘the expected BEP of this configuration is approximately 5×10−4’. More precisely, we say that configuration ckexhibits acceptable performance when the expected BEP E[bk] of configuration ckis below some fixed threshold T. This statement is conditional on the temporal simulation assumptions, i.e., Rayleigh fading. Standard values for T are 10−3 for voice quality systems and 10−6 for data quality systems. Likewise, we say that bucket C (a subspace of configurations) exhibits acceptable average performance when the expected BEP E[B] of bucket C is below some fixed threshold T. This statement is conditional on both the temporal simulation assumptions and the distribution of configurations{ck}k=1nin the bucket (the prior probabilities).The confidence that configuration ck(resp. bucket C) exhibits acceptable (average) performance is P(E[bk]<T) (resp. P(E[B]<T)). Since bkand B are Gaussian, these probabilities can be estimated asP(E[bk]<T)≈Fnk-1T-bˆkσˆk/nk,P(E[B]<T)≈FN-1T-B^Σ^/N,where FN−1(·) is the CDF of the Student t distribution with N−1 degrees of freedom and nkand N are the sample sizes for configuration ckand bucket C, respectively. For configuration ck,bˆk=1nk∑j=1nkxˆkj,σˆk2=1(nk-1)∑j=1nk(xˆkj-bˆk)2,wherebˆkandσˆk2are the estimates of the expected BEP and the BEP variance at point ck, nk⩾2 is sample size, and{xˆkj}j=1nkare sample values. For bucket C, we substitute point estimates intoE[B]=∑k=1npkE[bk]andΣ2=∑k=1npk2σk2to obtainB^=∑k=1npˆkbˆk,Σˆ2=∑k=1npˆk2σˆk2,whereB^andΣ^2are the estimates of the expected BEP and the BEP variance at bucket C, and{pˆk}k=1nare the prior probabilities estimated from the dataset aspˆk=nk/∑i=1nni. Observe thatB^=∑k=1npˆkbˆk=1∑k=1nnk∑k=1n∑j=1nkxˆkjis exactly the sample mean of all observations in the bucket, butΣ^2is not the variance of this sample. This is the case because{{xˆkj}j=1nk}k=1nare not i.i.d. samples from the mixture distribution of B—they are samples from the constituent distributions of{bk}k=1n.Let us now apply the techniques developed so far to analyze the performance of a space of configurations. The wireless systems under consideration employ WCDMA technology with two-branch STTD and 1/2 rate convolutional coding. We require that the transmitter has two antennas (branches) separated by a distance large enough for their signals to be uncorrelated, but small enough for the mean path losses and impulse responses of their channels to be approximately equal at receiver locations of interest. We assume Rayleigh flat fading channels, which is reasonable for indoor applications in the ISM and UNII carrier frequency bands (2.4 and 5.2GHz, respectively). The goal is to study the effect of power imbalance between the branches on the BEP of the configurations across a wide range of average SNRs.This section presents a number of plots that summarize simulated BERs. We also outline the process of statistically significant sampling of the configuration space. The next section develops a data mining methodology that solves a practically important problem: given a dataset similar to the one presented next, find a region of the configuration space where we can confidently claim that configurations will exhibit acceptable (average) performance.Let us begin with an initial sample of the configuration space, as shown in Fig. 2(top). This figure shows the simulated BER as a 2D functionfˆ(S1,S2)of the average branch bit energy-to-noise ratios (SNRs) S1 and S2, in dB. The parallel simulation ran for three days on 120 machines (AMD Athlon 1.0GHz) at a speed of approximately 2.5 points per machine per day. 10,000 frames, or 800,000 information bits, were simulated for each of the 820 points S2=3,4,…,42; S1=3,4,…,S2. Sincefˆ(S1,S2)is symmetric [3], we show M=1600 points{ck}k=1Mfor a full cross-product of S1 and S2.Wireless system designers are more accustomed to 1D slices of the configuration space, e.g., the ones shown in Fig. 3. Define the branch power imbalance factorα=10-0.1|S1-S2|,where S1 and S2 are the average SNRs of the branches, in dB. (This definition applies as long as the mean path losses of the branches are equal.) By definition, 0⩽α⩽1, where zero corresponds to a total malfunction of one of the branches and one corresponds to a perfect balance of branch powers. The graphs in Fig. 3 were obtained by fixing α and varying the effective SNRS=10log10((100.1S1+100.1S2)/2),in dB (top), and fixing the effective SNR and varying α (bottom). (Note that fixing the effective SNR is equivalent to fixing total transmitter power.) The sample of configurations came from the dataset shown in Fig. 2 (bottom), described in detail later. However, this sample does not contain the exact points for typical slices, so we used a fitted surface—Fig. 4—to approximate the BERs for the slices in Fig. 3. We choose to work with the axes S1, S2 in Fig. 2 because it simplifies the discussion later.What can be gathered from Fig. 2 (top)? The deep valley along the diagonal is due to the fact that, provided that the effective SNR is fixed, we expect the BEP to be smallest when the branch power is balanced (S1=S2, α=1) [3]. Somewhat less expected were (a) the wide tolerance region where ∣S1−S2∣ is large (up to 12dB) but the BER is still small, (b) a very sharp decline in performance at the edge of the tolerance region, and (c) a region of high local variability in the upper part of the diagonal. The surface is truncated atmin1⩽k⩽M{bˆk}=3.75×10-6because smaller estimates of the (expected) BEP require an enormous computation time due to the convergence properties of Monte Carlo Estimation (more on this below).The initial sample looks reasonable and uncovers interesting trends in system performance, but it does not contain enough information to make statistically significant claims. Estimating the probability that a configuration exhibits acceptable average performance requires several samples per point ck. The simulation is computationally expensive and different regions of the configuration space exhibit different variability. Therefore, we must define tight stopping criteria for sampling. Fig. 2 (bottom) shows the output obtained with the following (per point ck) stopping criteria. The criteria are designed to achieve high estimation accuracy.1.Sampling{xˆkj}stops when the relative error in the estimatebˆkof the expected BEP E[bk] is smaller than the relative accuracy threshold β=0.1 times the current estimatebˆk, at a γ=0.9 confidence level, i.e., whenP(|E[bk]-bˆk|<βbˆk)⩾γ.We required nk⩾2 samples to obtain an estimateσˆk2of the BEP varianceσk2. Notice that the target is the relative error, not the absolute error, because the range of{bˆk}k=1Min the configuration space spans four orders of magnitude. Therefore, absolute error measures are misleading.Sampling{xˆkj}also stops when we can say, with confidence γ=0.9, that the expected BEP E[bk] is below the sampling threshold t=10−4, i.e., whenP(E[bk]<t)⩾γ.This work considers voice quality applications, so the exact value of the expected BEP is irrelevant as long as it is smaller than the performance threshold T=10−3. The sampling threshold t was set to an order of magnitude below the performance threshold T to avoid large approximation error of a fitted surface near T.Finally, sampling{xˆkj}stops when more than 50 samples of 10000 frames each are required to satisfy either of the previous rules. This rule fired in 5% of the cases, all at the boundary of the tolerance region and most in mid diagonal.Altogether, 5154 samples were collected for an average of 6.3 samples per point. Needless to say, the computational expense of such sampling remains too high for practical applications. While a large number of samples is typically desirable (for validation purposes), we will show that our data mining framework makes very effective use of data and thus requires fewer samples in practice. Let us now look at the data in more detail.We assumed that the samples output by the WCDMA simulation are Gaussian. Intuitively, we are simulating a large number of information bits (800,000) per BEP estimatexˆkj, so the Lindeberg condition for the Central Limit Theorem should hold. Fig. 5shows empirical evidence that this is the case. We have arbitrarily chosen one point among those with 20–30 sample values{xˆkj}j=1nkand plotted the empirical CDF of this sample against that of the Gaussian distribution with the mean equal to sample meanbˆkand the variance equal to sample varianceσˆk2. The curves are close to each other and the Shapiro–Wilk test yields W=0.98 (0⩽W⩽1) and p-value of 0.88. Other points also demonstrate similar curves and high values of W, but p-values vary significantly. This dataset contains sufficient samples to estimate{E[bk]}k=1Mwith high relative accuracy, but 6.3 samples per point are insufficient to formally justify a Gaussian assumption.It is also instructive to see some measure of how the sample variance is distributed across the configuration space. Figs. 6 and 7show sample sizes and sample standard deviation-to-mean ratios for the samples in Fig. 2 (recall that we prefer relative measures because the range of{bˆk}k=1Mis large). Both figures indicate high variance around the boundary of the tolerance region. This is not surprising because the edges of the tolerance region are relatively steep. Fig. 7 also shows relatively high variance at some points inside the tolerance region. This is because the simulation achieved the sampling threshold t=10−4 and stopped before it achieved the relative accuracy threshold β=0.1. Knowing this, one would expect a larger relative variance in the tolerance region. Let us examine why this is not the case.We treat the BEP as a continuous Gaussian random variable bk, but all sample values{xˆkj}j=1nkare discrete—they are ratios of two integers, the number of errors and the number of bits simulated. The simulation may not detect any bit errors when the expected BEP E[bk] is relatively small (e.g., one error in the number of bits simulated). Since no channel is perfect, zero is too optimistic an estimate for the expected BEP. Instead, we conservatively assume that at least three bit errors have been detected. This is why the smallest estimatebˆkof E[bk] is 3/800,000=3.75×10−6. However, using any constant cutoff prevents us from estimating the varianceσk2. We would need to simulate a large number of frames to estimateσk2when the expected BEP is small. Instead, we can empirically show that the probability that the expected BEP is smaller than the performance threshold T=10−3 is close to one. Letbˆk=3.75×10-6be the sample mean, nk=2 be the sample size, andσk2be the BEP variance at point ckwhere two independent simulations detected three or fewer bit errors each. Sampling{xˆkj}will stop because sample variance is zero, so the first stopping rule applies.We need to show that sampling can indeed stop, i.e., that the probability that the expected BEP is below the performance threshold T isP(E[bk]<T)≈Fnk-1T-bˆkσˆk/nk⩾0.995.This statement can only be false when(T-bˆk)nk/σˆk⩽64, orσˆk⩾2.2×10-5, almost an order of magnitude bigger than the conservative estimatebˆkof the expected BEP E[bk]. This is unlikely because Fig. 7 (bottom) shows that the sample standard deviation rarely exceeds the sample mean even by half an order of magnitude. In other words, we do not have accurate estimates for varianceσk2in the tolerance region. However, we can still reasonably conclude that configurations exhibit acceptable performance in this region.Consider a set of buckets{Ck}k=1Mwith corresponding random variables{Bk}k=1M. Given a number of sample values, the framework developed in Section 2 allows us to estimate the probabilities{P(E[Bk]<T)}k=1Mthat buckets{Ck}k=1Mexhibit acceptable average performance. (All arguments about buckets equally apply to points because a point is a special case of a bucket.) This section is concerned with finding an optimal subset of random variables from among{Bk}k=1M. This optimal subset corresponds to an optimal region of a 2D bucket space. We would like to find a sufficiently large admissible region Rmsuch that we are sufficiently confident that buckets in Rmexhibit acceptable average performance.There are many ways to define admissibility and we are interested in adopting a definition that is both meaningful in the wireless domain and permits effective data mining algorithms. Among a space of such admissible regions, we can define different optimality criteria and data mining then reduces to searching within this space. In this paper, a region Rmis admissible when it has a particular type of shape. We will explore three different criteria for the mining of optimal regions; the algorithms and these criteria are based on the algorithms in [4,18] and have been adapted to the problem of mining simulation data in this paper.Additional notation relating buckets to regions is introduced in Table 2. Let X and Y be two discrete parameters to the temporal (e.g., WCDMA) simulations such that X and Y partition the point space into disjoint buckets{Ck}k=1M. More precisely, let X,Y have ordinal domains DX,DY, let ∣DX∣=MX, ∣DY∣=MY, ∣DX∣∣DY∣=M, and assume that the mapρ:DX×DY→{Ck}k=1Mis bijective. In other words, X and Y define a discrete 2D space of buckets. Since the domains of X and Y are ordinal, this space is easily visualized as a 2D color map or a 3D perspective plot.For example, the average SNRs S1 and S2 in the previous section partition the space of configurations into buckets. Both S1 and S2 vary from 3 to 42 in steps of 1 (in dB), so MX=MY=40 and M=40×40=1600 (recall, from Section 3, that only 820 of these points were simulated and the remaining ones were symmetrically reflected). Furthermore, the domains of S1 and S2 are ordinal because the values of S1 and S2 are directly related to the powers of the transmitter antennas. In this case, the buckets are simply the points in the space of configurations. In general, buckets can be convex combinations of points, as detailed in Section 2. Recall that we defined the color of a bucket as the probability that the bucket exhibits acceptable average performance. Fig. 8shows these ‘colors’ as a perspective plot for the STTD example.Consider regions (subsets) of buckets in the bucket space. If the shape of these regions is unconstrained, there are 2Mpossible regions{Rm}m=12M. Let region Rm, 1⩽m⩽2M, consist of buckets{Cκ(m,i)}i=1η(m), where η(m), 1⩽m⩽2M, is a mapping from region number m to the number of buckets in this region, and κ(m,i), 1⩽m⩽2M, 1⩽i⩽η(m), is a mapping from region number m and bucket number i within region Rmto bucket number k, 1⩽k⩽M, that we use to subscript buckets{Ck}k=1M. The exact definitions of η(m) and κ(m,i) are not important as long as they generate all possible regions (subsets){Rm}m=12M.The shape of admissible regions should be constrained because unconstrained regions are hard to interpret and tend to overfit the training data. Besides, the problem of selecting an optimal unconstrained region is computationally intractable—all 2Mpossible regions must be considered, where M=1600 in the STTD example. The region shape can be constrained in a number of different ways (rectangular, x-monotone, etc.). Our restrictions on region shape are discussed next.Without loss of generality, assume that DX={1,2,…,MX} and DY={1,2,…,MY}. Intuitively, region Rmis rectilinear when its intersection with any horizontal or vertical line is connected. More formally, region Rmis rectilinear if and only if whenever buckets Cκ(m,i) at (xκ(m,i),yκ(m,i)) and Cκ(m,j) at (xκ(m,j),yκ(m,j)) are both in Rm, then (a) ρ(r,s)=Cκ(m,i) and ρ(r,t)=Cκ(m,j) imply buckets ρ(r,u) are also in Rmfor all u∈[s,t], and (b) ρ(r,t)=Cκ(m,i) and ρ(s,t)=Cκ(m,j) imply buckets ρ(u,t) are also in Rmfor all u∈[r,s]. Here [a,b] means all integers between the integers a, b, inclusive. We use Manhattan geometry to define connectedness. Region Rmis connected if and only if for every pair of buckets Cκ(m,i) and Cκ(m,j) in Rmthere exists a sequence of bucketsCκ(m,i)=Cκ(m,l1),Cκ(m,l2),…,Cκ(m,ln)=Cκ(m,j)in Rmsuch that for every 1⩽k<n‖ρ-1(Cκ(m,lk))-ρ-1(Cκ(m,lk+1))‖1=1.Intuitively, any two buckets are connected by a sequence of horizontal or vertical moves. Furthermore, we say that region Rmis admissible if it is both rectilinear and connected.This definition of admissibility can be viewed as a relaxed definition of convexity. Geometrically, it is easy to see that region Rmis admissible if and only if, when we look at Rmfrom left to right, its upper boundary first increases and then decreases monotonically (a pseudoconcave function), and its lower boundary first decreases and then increases monotonically (a pseudoconvex function). In other words, the region boundary need not be strictly convex or strictly concave, but it must be pseudoconvex or pseudoconcave. Admissible regions are informally summarized in Fig. 9. All admissible regions are composed of regions of four primitive types: W (region gets wider from left to right), N (region gets narrower), U (region slants up), and D (region slants down). Twelve combinations of these types yield all types of admissible regions: W, WU, WUN, WD, WDN, WN, UN, DN, U, D, N, and the empty region.Our choice of connected rectilinear regions is due to primarily heuristic considerations. These considerations are commonly applicable, but must be re-evaluated for each study. Both the connectedness and the rectilinearity restrictions can be justified for the STTD example (see next section). In general, it is easy to justify connectedness, but hard to justify rectilinearity. We advocate the use of connected rectilinear regions primarily because this shape is resistant to noise in the sample, not because we can analytically show that the region boundary is rectilinear. In data mining, the choice of region shape is most commonly dictated by the desired tradeoff between bias and variance [8]. Regions with flexible shape exhibit small bias (they can fit any data) but high variance (they can be overly sensitive to a particular dataset). Regions with rigid shape exhibit high bias but small variance. Connected rectilinear regions provide a reasonable tradeoff between bias and variance for many applications.Another prerequisite to finding regions with the desired properties is a definition of region ‘goodness’. Let us map bucket confidence P(E[Bκ(m,i)]<T) to a discrete range [0⋯1000] and define the hit of bucket Cκ(m,i) ashκ(m,i)=⌊1000P(E[Bκ(m,i)]<T)+0.5⌋(⌊X⌋ denotes the largest integer that does not exceed X), the support of bucket Cκ(m,i) assκ(m,i)=1000(this constant was chosen to make the discretization error reasonably small), the hit of region RmasHm=∑i=1η(m)hκ(m,i),and the support of region RmasSm=∑i=1η(m)sκ(m,i)=1000η(m).The key to efficient computation of optimized-confidence and optimized-support admissible regions is the definition of region confidence asΘm=Hm/Sm,where Hmis the hit and Smis the support of region Rm. Let us explore the implications of these definitions in more detail.Suppose, nκ(m,i)=6 samples have been collected for bucket Cκ(m,i) that consists of a single point. Let the sample mean beB^κ(m,i)=5×10-4and the sample standard deviation beΣˆκ(m,i)=8.87×10-4. Furthermore, suppose that five of these samples have the BER below 10−3 and one has the BER above 10−3. Then,P(E[Bκ(m,i)]<T)≈F510-3-5×10-48.87×10-4/6≈0.887.A purely model-free approach would interpret the above simulation results as ‘bucket Cκ(m,i) will exhibit acceptable average performance in 5 out of 6 cases.’ A strongly model-based approach would interpret the simulation results as ‘we are 88.7% confident that bucket Cκ(m,i) exhibits acceptable average performance.’ Our interpretation lies between the model-based approach and a model-free approach and posits that ‘bucket Cκ(m,i) will exhibit acceptable average performance in 887 out of 1000 cases.’ These interpretations provide confidence estimates under different simplifying assumptions.The model-free interpretation does not take either sample variance or sample distribution into account. This interpretation is only reliable for a sufficiently large number of samples, which is a luxury in our application. Our middle-ground interpretation explicitly accounts for sample variance and sample distribution. When sample size is small, our interpretation provides a statistically valid estimate of confidence that the bucket exhibits acceptable average performance. For a single bucket, this interpretation is as good as a strongly model-based interpretation, modulo a reasonably small discretization error. However, our interpretation diverges from the model-based interpretation at the region level.A strongly model-based analysis procedure would define a region random variableQm=1Wm∑i=1η(m)wκ(m,i)Bκ(m,i),where{Bκ(m,i)}i=1η(m)are bucket random variables,{wκ(m,i)}i=1η(m)are a priori (positive) constant weights, andWm=∑i=1η(m)wκ(m,i)is a normalization factor that maps these weights to probabilities of bucket occurrence in the region. A procedure similar to that in Section 2 would then be used to estimate P(E[Qm]<T) for a threshold T. The result of this calculation can be interpreted as the probability that region Rmexhibits acceptable average performance, conditional on the temporal simulation assumptions, the bucketing prior probabilities, and the region prior probabilities. However, as we shall see later, this definition of region confidence violates a property that permits an efficient data mining algorithm.We think of region confidence in terms of average bucket confidence over the whole region, namely,Θm≈1η(m)∑i=1η(m)P(E[Bκ(m,i)]<T).(If region size η(m) is large enough, we can reasonably expect the discretization errors to cancel each other.) This interpretation of Θmdoes not correspond to the strongly model-based probability that region Rmexhibits acceptable average performance. Instead, we define a region random variable Pmas the probability that any bucket Cκ(m,i) in region Rmexhibits acceptable average performance. Then, we estimate the expected value E[Pm] across the region Rmby the sample meanP^m≈Θmof estimates of bucket confidences{P(E[Bκ(m,i)]<T)}i=1η(m).How do these two definitions relate to each other? It is easy to show that they are equivalent only under very restrictive assumptions. Basically, we are assuming that the buckets are mutually independent, that population variance is small, and that the region is consistent, i.e., ‘good’ and ‘bad’ buckets are never mixed in the same region. Let bucket random variables{Bκ(m,i)}i=1η(m)be mutually independent, let the estimates{Σ^κ(m,i)2}i=1η(m)of bucket variances be approximately equal to zero, and let the estimates{B^κ(m,i)}i=1η(m)of bucket expected BEPs be either all greater than the performance threshold T or all smaller than the performance threshold T (i.e., all{T-B^κ(m,i)}i=1η(m)have the same sign). Then, bucket confidencesP(E[Bκ(m,i)]<T)≈Fnκ(m,i)-1T-B^κ(m,i)Σ^κ(m,i)/nκ(m,i),1⩽i⩽η(m), will be either all approximately equal to zero (B^κ(m,i)>T), or all approximately equal to one (B^κ(m,i)<T). Therefore, region confidence Θmwill be approximately equal to zero or one. Likewise, the strongly model-based region confidenceP(E[Qm]<T)≈Fη(m)-1T-Q^mΨ^m/η(m)will be approximately equal to zero or one because the estimateΨ^m2of region variance is (see Section 2)Ψ^m2=1Wm2∑i=1η(m)wκ(m,i)2Σ^κ(m,i)2≈0.The sign ofT-Q^mdetermines whether P(E[Qm]<T) is approximately equal to zero or one. After a minor rearrangement of terms,T-Q^m=1Wm∑i=1η(m)wκ(m,i)(T-B^κ(m,i)).We assumed that{T-B^κ(m,i)}i=1η(m)have the same sign, so we have shown that P(E[Qm]<T)≈Θm. The equality is asymptotically exact as all variance estimates{Σ^κ(m,i)2}i=1η(m)approach zero. This argument applies regardless of the distributions of{Bκ(m,i)}i=1η(m), as long as these random variables are mutually independent.We now pursue the definition of optimized regions. Given a slope τ, 0⩽τ⩽1, define the gain of region Rm, 1⩽m⩽2M, asG(Rm,τ)=Hm-τSm,where Hmis the region hit and Smis the region support. Let an optimized-gain admissible region Rτwith respect to slope τ, 0⩽τ⩽1, be an admissible region with the maximum gain G(Rτ,τ) over all admissible regions (this region need not be unique). Optimized-gain admissible regions are easy to define, compute, and analyze, but hard to interpret. Common practice is to define optimized-confidence and optimized-support admissible regions. Admissible region R∗ is an optimized-confidence admissible region with respect to a given support threshold 1000η, 0⩽η⩽M, if R∗ has the maximum confidence Θ∗=H∗/S∗ among all admissible regions with support of at least 1000η. Likewise, admissible regionR♢is an optimized-support admissible region with respect to a given confidence threshold θ, 0⩽θ⩽1, ifR♢has the maximum supportS♢=1000η(♢)among all admissible regions with confidence of at least θ. In other words, we can either fix the region confidence θ and find the largest regionR♢with confidence of at least θ, or we can fix the minimum region size (support) 1000η and find the most confident region R∗ with support of at least 1000η.Observe that τ in the definition of an optimized-gain admissible region is the relative importance of support vs. that of confidence. We can find a small region with high confidence or a large region with small confidence, but both objectives cannot be maximized simultaneously. Increasing τ will increase the confidence of the optimized-gain admissible region, but decrease its support. Likewise, decreasing τ will decrease the confidence of the optimized-gain admissible region, but increase its support. Therefore, we can find approximate optimized-confidence and optimized-support admissible regions by a binary search for the value of τ where the respective threshold is barely satisfied. The search can stop at a given level of precision Δτ, where the lower bound on Δτ can be found in [18] (they show that the number of steps in this search is logarithmic in the support 1000M of the bucket space). This algorithm is approximate because an optimized-confidence (resp. optimized-support) admissible region need not be an optimized-gain admissible region for any value of τ. Yoda et al. [18] argue that this approximation is reasonable for large datasets.Let us revisit the definition of region ‘goodness’. Geometrically, the buckets with the same value of X are the columns and the buckets with the same value of Y are the rows. An optimized-gain admissible region can be computed inO(MXMY2)time by a set of rules of the following form. Recall that a region of type W gets wider from left to right (see Fig. 9). Let RW(m,[s,t]) be the region of type W with maximum gain fW(m,[s,t]) over all admissible regions of type W that end in column m and span rows s through t in this column. Then, either (a) m is the first column of RW(m,[s,t]), or (b) RW(m,[s,t]) includes the region RW(m−1,[s′,t′]) with the maximum gain fW(m,[s′,t′]) over all admissible regions that end in column m−1 and span rows s′⩾s through t′⩽t in this column. Ref. [18] keeps the regions with maximum gain for every region type and every triple (m,[s,t]) in a dynamic programming table. These locally maximal regions then grow according to a set of rules that compute an optimized-gain admissible region. This efficient greedy algorithm for computing optimized-gain admissible regions depends on the property of the gain function that we refer to as monotonicity. Let 0⩽τ⩽1 be a slope andRm′andRm″be two admissible regions with gainsG(Rm′,τ)⩾G(Rm″,τ). The gain function G(Rm,τ) is monotonic if for any region Rkdisjoint with bothRm′andRm″G(Rm′∪Rk,τ)⩾G(Rm″∪Rk,τ),where the union of regions is defined in the obvious way. It is easy to see that our gain functionG(Rm,τ)=Hm-τSm=∑i=1η(m)⌊1000P(E[Bκ(m,i)]<T)+0.5⌋-1000τη(m)is monotonic because it is additive. However, a strongly model-based gain functionG(M)(Rm,τ)=P(E[Qm]<T)-τη(m)/Mis not monotonic even if we assume independence of bucket random variables{Bκ(m,i)}i=1η(m)that make up Qm. To the best of our knowledge, only monotonic gain functions are known to result in practical algorithms for computing optimized-gain admissible regions.What happens when no estimates of mean and/or variance are available for some bucket Ck? The answer to this question depends on problem-specific considerations. As was demonstrated in Section 3, it is sometimes possible to provide conservative estimates for these values. For example, we have empirically shown that the expected BEPs of some configurations {ck} are smaller than T=10−3 with confidence P(E[bk]<T)⩾0.995. Likewise, we know that as the effective SNR approaches negative infinity (in dB), the BEP approaches 0.5, which is the probability of correctly guessing the value of a random bit when the transmitter is turned off. Thus, we can let P(E[bk]<T)=0 for points with sufficiently small effective SNRs and a reasonable performance threshold T. If no such estimates are available, we can simply omit the missing buckets from the probability computation. This must be done with care because such buckets will contribute nothing to the confidence of the region. This fact can be used to reduce the computational expense of sampling.This section has highlighted the sometimes contradictory objectives that aggregation must satisfy: permit valid statistical interpretations and afford structure that can be exploited by data mining algorithms. Our approach has been a judicious mix of concepts from both statistics and data mining. We showed that our formulation of the data mining problem lies between the completely model-free approach and the strongly model-based approach. The next section applies the data mining methodology described here to the example in Section 3.This section continues the example in Section 3. First, we show that optimized-gain regions are both rectilinear and connected for this example. It immediately follows that optimized-support and optimized-confidence regions are also admissible. An optimized-support admissible region is presented next. We show that the elaborate region mining setup leads to simple engineering interpretations. Finally, we look at the performance of data mining when the number of samples is small. Threefold cross-validation shows that data mining performs well under these circumstances.Let the average SNRs S1=X and S2=Y partition the space of configurations in Fig. 2 into disjoint points (buckets){ck}k=1M, 1⩽M⩽1600. We now give an intuitive argument to justify the suitability of the data mining algorithm for the STTD study. Without loss of generality, consider only the points with X⩽Y, i.e., S1⩽S2. It is easy to extend all arguments to X>Y, but this adds little to the discussion.Let c1 at (x1,y1) and c2 at (x2,y1), x1<x2<y1, be two points in an optimized-gain region (of arbitrary shape) for some slope 0<τ<1 (see Fig. 10). This means that the confidences of these points are one, and thus the expected BEPs of these points are smaller than the performance threshold T. When x1,x2<y1 and y1 is fixed, the BEP is a monotonically decreasing function of x—increasing x decreases the power imbalance and increases the effective SNR, so the BEP must decrease. Therefore, the expected BEP of any point cuat (xu,y1), x1<xu<x2, is below the performance threshold T. Thus, the confidences of points {cu} are one and these points must also be in the optimized-gain region Rτ. Three more symmetric arguments of this kind show that optimized-gain regions are rectilinear.Likewise, let c1 at (x1,y1) and c2 at (x2,y2), x1<x2<y1<y2, be two points in an optimized-gain rectilinear region (refer to Fig. 10). Since c1 is in the optimized-gain region and x1<x2, the point at (x2,y1) is also in this region because it has a smaller BEP than c1. Since the optimized-gain region is rectilinear, there is a horizontal path from (x1,y1) to (x2,y1) and a vertical path from (x2,y1) to (x2,y2). Thus, there is a Manhattan path from (x1,y1) to (x2,y2). Arguments of this kind show that optimized-gain rectilinear regions must be connected as long as they are ‘wide enough’.To summarize, we have shown that optimized-gain (and thus optimized-support and optimized-confidence) regions are admissible. The data mining algorithm described in Section 4, which results in optimal admissible regions, is thus appropriate for the STTD example. We now show and interpret data mining results.Fig. 11shows an optimized-support admissible region for the confidence threshold θ=0.99. Intuitively, this is the largest admissible region where we can claim, with confidence of at least 0.99, that configurations exhibit acceptable performance. This claim is conditional on temporal simulation assumptions and on mutual independence of configurations in the region. The shape of this region confirms that, under a fixed effective SNR, the BEP is minimal when the average SNRs of the two branches are equal. The width of this region shows the largest acceptable power imbalance. For this example, the system tolerates power imbalance of up to 12dB. However, the width of the optimized region is not uniform. The region is narrower for small effective SNRs and wider for large effective SNRs. This means that configurations with low effective SNRs are more sensitive to power imbalance than configurations with high effective SNRs. None of these observations are news to an informed reader. The contribution of data mining in this context is not qualitative discoveries; it is statistically significant quantitative results.Let us see how the data mining algorithm performs when data is scarce. The initial sample of the configuration space in Fig. 2 (top) contains one sample value per bucket. The statistically significant sample in Fig. 2 (bottom) contains at least two additional sample values per bucket (recall that we required at least two sample values to estimate bucket varianceσk2). Therefore, threefold cross-validation is the most elaborate cross-validation procedure that this dataset affords. The regions in the top left, bottom left, and top right of Fig. 12have been computed for sample values in Fig. 2 (top) and the first two sample values per bucket in Fig. 2 (bottom). Each of these regions has been computed with two out of the three sample values per bucket. The region in the lower right has been computed with all data in Fig. 2 (bottom). All four regions are optimized-support admissible regions with the confidence threshold θ=0.95. The regions are overlaid on top of the color-coded bucket confidence values. Red (dark) corresponds to low confidence and white (light) corresponds to high confidence that configuration ckexhibits acceptable average performance w.r.t. the voice quality threshold T=10−3.The regions in all subfigures of Fig. 12 are identical except for the lower left corner. This is not surprising because this part of the configuration space exhibits high relative variance. Also, the data is symmetric but the regions are asymmetric in the lower-left corner. Recall that optimized-gain admissible regions, and thus optimized-support admissible regions, are not unique. The ties in region gains are broken arbitrarily. Therefore, region asymmetry is an additional indicator of region instability.Fig. 12 also shows that additional data improves image contrast but does not significantly affect region shape. Collecting additional sample values separates the points into ones with low confidence and ones with high confidence. A curious side effect occurs when the difference in confidence estimates of low-confidence points falls below the discretization error (1/1000). In this case, the ‘confidence slack’ 1−θ is allocated to arbitrary points with low confidence. One way to correct this situation is to raise the confidence threshold θ—after all, more accurate data should afford stronger claims. Another alternative is to lower the discretization threshold. In general, optimized regions work best when the data is noisy. A contour plot will suffice when the data is highly accurate.It can also be seen that the high contrast created by the sharp edge of the tolerance region is advantageous to data mining. The region is stable where the contrast is high. When the image is blurred, data mining tries to avoid the questionable boundary points.To summarize, this section has demonstrated that optimized-gain regions are rectilinear and connected for a non-trivial space of wireless system configurations. We have also shown that optimized-support admissible regions are easy to interpret. Finally, we have shown that data mining works well when sample sizes are small.We have demonstrated a hierarchical formulation of data mining suitable for assessing performance of wireless system configurations. WCDMA simulation results are systematically aggregated and redescribed, leading to intuitive regions that allow the engineer to evaluate wireless system configuration parameters. We have shown that the assumptions about region shape and properties made by data mining algorithms can be valid in the wireless design context; the patterns mined hence lead to explainable and statistically valid design conclusions. As a methodology, data mining is thus shown to be extremely powerful when coupled with statistically meaningful performance evaluation.This work is the first (known to the authors) application of data mining methodology to solve problems in wireless system design. Therefore, a large number of extensions are possible and called for. We outline possible extensions at the three levels of aggregation: points, buckets, and regions.At the point level, it may be advantageous to model temporal simulations more precisely. This paper assumes a ‘large enough’ number of frames per simulation and works with the distribution of estimated BEPs. We have shown reasonable analytical and empirical evidence that this distribution is Gaussian. The advantage of this problem formulation is the independence of spatial aggregation from the assumptions of temporal simulation. This helps introduce wireless engineers to the methodology of data mining for studying design problems. However, a stronger model of temporal simulation (e.g., Markov chains in [17]) may yield appreciable gains in software performance. This direction is worth pursuing because few research groups have access to parallel computing facilities of the scale used in this work. For instance, the initial sample of the configuration space in Fig. 2 (top) would take 1year of computation time on a modern workstation. The study presented in this paper would clearly be impossible without significant computational power.Aggregation of points into buckets is the least developed part of this work. Suppose that we would like to simulate the effects of interference on configuration performance. Assume that the distribution of the average strengths of the interfering signals is known a priori (e.g., estimated by ray tracing). We can either make this distribution known to the temporal simulation, or, alternatively, run several temporal simulations for different strengths of interfering signals. The former is more accurate and computationally more efficient, but the latter is more generic and simpler to implement. Bucketing of simulation results with varying simulation parameters is intended to approximate the performance of a single device under varying conditions. This paper does not employ such bucketing but instead builds all the necessary kinds of parameter variation into the temporal simulation (which can be argued to be the right way to do it). However, bucketing may be necessary when one has to work with a given dataset (e.g., measurements). Bucket space can be viewed as a configuration space for a more complex temporal simulation. Therefore, an in-depth treatment of bucketing is orthogonal to the primary topic of this paper, which is data mining.Significant work remains to be done at the region level as well. For instance, the assumption of small variance could conceivably be relaxed. One can also pursue the relatively difficult task of incorporating strongly model-based prior knowledge into the data mining algorithm, or the somewhat easier task of applying different kinds of region mining algorithms to problems in wireless system design.Defining additional case studies is another obvious direction for future work. We have studied a relatively small part of the parameter space of modern wireless systems. More studies of this type must be performed to highlight the merits and the shortcomings of data mining in this domain.Finally, the strict staging of data collection and data mining can be relaxed. One can fruitfully interleave the two activities and have the results of data mining drive subsequent data collection [12]. In data-scarce domains, it would be advantageous to focus the data collection effort on only those regions deemed most important to support a particular data mining objective. Methodologies for closing-the-loop in this manner are becoming increasingly prevalent [11]. This will also help define alternative criteria for evaluating experiment designs and layouts.

@&#CONCLUSIONS@&#
