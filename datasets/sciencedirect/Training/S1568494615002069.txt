@&#MAIN-TITLE@&#
TCP Pegas: A PSO-based improvement over TCP Vegas

@&#HIGHLIGHTS@&#
We find that Vegas suffers from many problems due to inaccurate estimation of BaseRTT.We use PSO technique to accurate estimation of the BaseRTTin TCP Vegas.Our accurate estimation of BaseRTTsolves rerouting and unfairness problem.Our algorithm decreases the number of the dropped packets.Our algorithm increases bottleneck utilization and fairness.

@&#KEYPHRASES@&#
Congestion control,TCP Vegas,Particle swarm optimization,Rerouting,Fairness,

@&#ABSTRACT@&#
TCP Vegas is a source algorithm that offers relatively rich performance in the Internet congestion control. But Vegas has some problems which have serious impacts on its performance. Rerouting is one of these problems. When route of a connection changes and round trip time increases, Vegas misinterprets it as the result of the network congestion and consequently decreases its own sending rate. As another important problem, when a flow joints to the network later than other flows and faces with congested queues, it wrongly considers the measured round trip time as its initial BaseRTT. It means that while other flows decrease their sending rates due to existing congestion, this flow does not sense the congestion and hence unfairly increases its sending rate. These problems mainly have roots in the Vegas estimation procedure of the propagation delay i.e. BaseRTT. In this paper we propose a novel algorithm, named Pegas, in which particle swarm optimization technique is used to dynamic estimation of BaseRTT. Simulation results show that Pegas solves the rerouting and unfairness problems and remarkably enhances Vegas performance in terms of dropped packets, bottleneck utilization, and fairness.

@&#INTRODUCTION@&#
Internet performance is tightly depending on transmission control protocol (TCP) congestion control function. When the number of packets sent to the network is much more than the network capacity, congestion occurs in the network and thereby packets are dropped. TCP congestion control aims to moderate the sending rate of flows in order to avoid congestion. Several versions of TCP are emerged to improve the performance of the primitive TCP [1–3]. TCP versions can be classified as loss-based algorithms (e.g., TCP Reno [4]) and delay-based algorithms (e.g., TCP Vegas [5,6] and Fast TCP [7]). TCP Vegas and FAST TCP use queuing delay instead of loss probability as the sign of congestion. These two versions of TCP aim to maintain a constant number of packets in queues. The number of packets in queues is guessed by comparing the difference between the encountered round trip time (RTT) and the BaseRTT. If the number of queued packets is high, the sending rate is decreased, while if it is low, the sending rate is increased. The difference between FAST TCP and TCP Vegas is how to adjust the sending rate. Vegas applies invariant size adjustments to the rate without considering the difference between the current and target rate whereas FAST TCP applies larger steps when the current rate is far from target rate and smaller steps when the current rate is close to the target rate to improve the convergence time in long distance networks. TCP Reno uses packet loss as the solely sign of congestion, and react to congestion. Studies have proved that Vegas outperforms Reno from the viewpoints of overall network utilization [6–8], fairness [9], throughput, and packet loss [6–9].Although Vegas achieves 37–71 percent higher throughput than TCP Reno [5,6], and drops much less packets compared to Reno, TCP Reno is mostly used version. The reason is because Vegas suffers from several problems that inhere in its congestion avoidance mechanism. Rerouting is one of these problems. When a route is changed by the network and RTT grows up, Vegas incorrectly recognizes the increased RTT as result of the network congestion and consequently decreases its own sending rate. This problem is due to the fact that Vegas adjusts its own congestion window size (cwnd) based on BaseRTT. As another problem, when different flows start in different times each one may encounter different RTT and considers it as its initial BaseRTT. Obviously when various sources consider different values for their BaseRTT, each one senses a different congestion level and hence unfairly will have a different sending rate.All the mentioned problems mainly have roots in the inaccurate estimation of BaseRTT. An accurately estimated BaseRTTcan basically lead to an improved performance of the network. We believe that the estimation of BaseRTTcan be formulated as an optimization problem that searches a vast solution space to find a value for BaseRTTthat improves the performance of the network. After this formulation, we employ the particle swarm optimization (PSO) technique [10] to solve this optimization problem by dynamically estimating (searching for) the best BaseRTT. PSO is a simple and general evolutionary algorithm that has few parameters to adjust. PSO quickly converges to the optimal point and is a time efficient algorithm. Simulation results in ns-2 [11] environment show that the proposed algorithm not only solves the rerouting and unfairness problems but also improves Vegas throughput, packets drop count, and bottleneck utilization.The rest of this paper is organized as follows. Section 2 briefly describes TCP Vegas algorithm and introduces its rerouting and unfairness problems and then presents an introduction to PSO algorithm. Related works are described in Section 3. In Section 4 we present our approach based on PSO to accurate estimation of BaseRTTand to solve Vegas problems. This section also addresses the convergence issue of proposed algorithm. Section 5 presents the simulation results, and finally Section 6 brings concluding remarks.In this section we study the TCP Vegas algorithm and introduce its rerouting and unfairness problems. Also, since we use PSO algorithm to overcome the weaknesses of TCP Vegas, we describe the fundamentals of PSO algorithm at the end of this section.TCP Vegas proposed in [5] uses the difference between the expected and actual throughput to estimate the available bandwidth in the network, regulate the transmission rate and avoid congestion. The concept is that when the network is not congested, the expected throughput and the actual throughput are close together. Otherwise, the actual throughput will be much smaller than the expected. TCP Vegas varies its own cwnd using the following calculations:(1)cwnd=cwnd+1diff<αcwnd−1diff>βcwndotherwise(2)diff=ExpectedRate−ActualRate(3)ExpectedRate=cwnd(t)BaseRTT(4)ActualRate=cwnd(t)RTTwhere BaseRTTis the minimum encountered RTT of the connection, cwnd(t) is the current congestion window size, RTT is the actual round trip time, and α and β are parameters whose values are typically set to 1 and 3, respectively. As a result, TCP Vegas is capable to detect network congestion in the early level and to prevent periodic packet losses that usually happen in TCP Reno.As we know when the propagation delay increases, the bandwidth–delay product(bw*d)increases. Since the expression(cwnd−bw*d)gives the number of packets in the buffers of the routers, and the aim of TCP Vegas is to keep the number of packets in the router buffer between α and β, it should increase the congestion window to keep the same number of packets in the buffers when the propagations delay increases [12,13].As Eqs. (1)–(4) indicate, in TCP Vegas the parameter BaseRTTis employed to compute the expected throughput and to adjust the cwnd based on it. According to [5,6], BaseRTTdenotes the RTT of a segment when the connection is not congested. In practice, BaseRTTdisplays the minimum of all measured RTTs and is a conjecture of the fixed delay and is the sum of propagation delay and packet processing time along the round trip path. If the route of a connection is changed, the RTT of the connection changes. If the new route has a shorter propagation delay, this does not make any significant problem, and most likely BaseRTTwill be updated. But, if the new route for the connection has a longer propagation delay, the Vegas end host cannot distinguish whether the growth in the round trip delay is as a result of the network congestion or is as a result of the rerouting. Without this knowledge Vegas misinterprets the increased RTT as a result of the network congestion and consequently decreases its own window size. This is simply the opposite of what the Vegas should do, that causes significantly throughput degradation in TCP Vegas.In the previous researches [13,14], it has been addressed that Vegas is unfair to older connections because of nature of its congestion control mechanism that uses a very simple procedure to estimate BaseRTT. Consider a Vegas connection that is initiated in an uncongested network. Undoubtedly, this connection BaseRTTis possibly close to the propagation delay. But when the network becomes congested, and a new connection is started, it estimates BaseRTTbigger than the older one. Now, although both connections have same RTT, latter connection cannot sense the existing congestion and thereby does not decrease its own sending rate. This means that the latter connection achieves higher bandwidth than the older connection. This unfairness, called sometimes persistent congestion problem [12], is due to the overestimation of BaseRTTby some connections.The other problem regarding the fairness of TCP Vegas is unfair treatment with connections that have different RTTs [9,15,16]. When multiple connections with different RTTs compete on a bottleneck link, those that have smaller RTT obtain more bandwidth than the ones have longer RTT.Particle swarm optimization [10,17,18] is one of stochastic global optimization techniques which was introduced by Eberhart and Kennedy in 1995. PSO performs better than other evolutionary computation methods in terms of solution accuracy and computation time saving. PSO is inspired from social behavior of animals and is based on the movement and intelligence of swarms. PSO uses a number of agents, called particles, which constitute a swarm moving around in a search space in order to find best solution. Each particle has a fitness associated with the solution quality, usually given by the objective function to be optimized. The best solution in iteration k has maximum fitness among other particles.Each particle is treated as a point in an N-dimensional space which is characterized by two vectors: position vector Xiand velocity vector Vi, that can be respectively expressed as Xi=[xi1, xi2, …, xiN] andVi=[vi1,vi2,…,viN]. Each particle records its personal best position, namely Pbest, and the best position among all the particles belonging to the swarm, namely Gbest. Pbest are associated with the best solution (highest fitness) that has achieved so far by that particle, and Gbest are associated with the best solution that has achieved so far by any particle in the swarm. So Pbest and Gbest for particle i in iteration k represented respectively by Pbesti(k)=[xi1, xi2, …, xiN] and Gbesti(k)=[xi1, xi2, …, xiN]. The basic concept of PSO lies in accelerating each particle toward its Pbest and the Gbest locations, with a random weighted acceleration at each time step (iteration). Each particle updates its position and velocity according to the following equations:(5)vi(k+1)=ωvi(k)+c1r1(Pbesti(k)−xi(k))+c2r2(Gbesti(k)−xi(k))(6)xi(k+1)=xi(k)+vi(k+1)where ω is the inertia factor, c1is the self-cognitive constant, and c2represents the social component factor (ω, c1, c2>0 are tuning parameters). Parameters r1 and r2 are independent random numbers uniformly distributed within [0,1].

@&#CONCLUSIONS@&#
