@&#MAIN-TITLE@&#
Part level transfer regularization for enhancing exemplar SVMs

@&#HIGHLIGHTS@&#
EE-SVM, a part based transfer regularization method that boosts E-SVM, is introduced.EE-SVM is further improved by transferring the statistics between the parts.All the proposed objectives result in convex formulations.Experimentally shown that EE-SVM has better performance for pose matched retrieval.All objectives are conveniently optimized by mapping to a classical SVM formulation.

@&#KEYPHRASES@&#
Exemplar SVMs,Transfer learning,Object detection,Image retrieval,Feature mapping,

@&#ABSTRACT@&#
Exemplar SVMs (E-SVMs, Malisiewicz et al., ICCV 2011), where an SVM is trained with only a single positive sample, have found applications in the areas of object detection and content-based image retrieval (CBIR), amongst others.In this paper we introduce a method of part based transfer regularization that boosts the performance of E-SVMs, with a negligible additional cost. This enhanced E-SVM (EE-SVM) improves the generalization ability of E-SVMs by softly forcing it to be constructed from existing classifier parts cropped from previously learned classifiers. In CBIR applications, where the aim is to retrieve instances of the same object class in a similar pose, the EE-SVM is able to tolerate increased levels of intra-class variation, including occlusions and truncations, over E-SVM, and thereby increases precision and recall.In addition to transferring parts, we introduce a method for transferring the statistics between the partsand also show that there is an equivalence between transfer regularization and feature augmentation for this problem and others, with the consequence that the new objective function can be optimized using standard libraries.EE-SVM is evaluated both quantitatively and qualitatively on the PASCAL VOC 2007 and ImageNet datasets for pose specific object retrieval. It achieves a significant performance improvement over E-SVMs, with greater suppression of negative detections and increased recall, whilst maintaining the same ease of training and testing.Content based image retrieval (CBIR), the problem of searching digital images in large databases according to their visual content, is a well established research area in computer vision. In this work we are particularly interested in retrieving subwindows of images which are similar to the given query image, i.e. the goal is detection rather than image level classification. The notion of similarity is defined as being the same object class but also having similar viewpoint (e.g. frontal, left-facing, rear etc.). A query image can be a part of an object (e.g. head of a side facing horse), a complete object (e.g. frontal car image), or a composition of objects (e.g. person riding a horse). For instance, given a query of a horse facing left, the aim is to retrieve any left facing horse (intra-class variation) which might be walking or running with different feet formations (exemplar deformation).Recently exemplar SVMs (E-SVM) [33], where an SVM is trained with only a single positive sample, have found applications in the areas of CBIR [3,40] and object detection [33]. Since the E-SVM is trained from a single positive sample (together with many negatives), it is specialized to that given sample. This means that it can be strict (on viewpoint for example), and the negatives give some background suppression. However, the single positive is also a limitation: only so much can be learnt about the foreground of the query (and this can lead to false detections), and more significantly it can lead to lack of generalization. In our context, generalization refers to intra-class variation and deformation whilst maintaining the viewpoint. Learning such generalization from a single positive is challenging given the lack of examples of allowable deformations and intra-class variation.In this work we propose a transfer learning approach for boosting the performance of E-SVMs using part-like patches of previously learned classifiers. The formulation softly constrains the learned template to be constructed from classifiers that have been fully trained (i.e. using many positives). For instance, the neck of a horse can be transferred from the tail of an aeroplane (see Fig. 1), or a jumping bike can borrow part of wheel patches from regular side facing bike or motorbike classifiers (see Fig. 2). The intuitive reason behind borrowing patches from other well trained classifiers is that these classifier patches bring with them a better sense of discriminative features and background suppression, and also bring some generalization properties. The result of the transfer learning is an enhancement of background suppression and tolerance to intra-class variation, hence better coping with occlusions and truncations in the query image. However, these enhancements incurs no (significant) additional cost in learning and testing. We term the boosted E-SVM, enhanced exemplar SVM (EE-SVM).Objects and parts don’t occur in isolation to each other. They appear with certain correlations in nature. For example, we don’t expect to see a zebra in a city scene with road and cars or a bicycle next to a sailing boat in the middle of the sea. Stemming from these observations, co-occurrence statistics are utilized in the computer vision problems such as object detection [12], and semantic segmentation and labeling of objects [25,38] in the scenes. Similarly parts also appear with certain correlations: occurrence of feet supports occurrence of a head, or seeing one wheel increases the chance of seeing another in the close neighborhood. Parts can also have negative correlations, i.e. it is not expected to see spider legs or parts of insects in the close neighborhood of vehicle-like patches. Hence we can transfer not only parts, but also their natural co-occurrence statistics. We include these co-occurrence statistics of the parts, in a convex formulation, for softly enforcing these positive and negative correlations, in the EE-SVM objective.We describe the relation with the prior work in Section 1, and then introduce the enhanced E-SVM in Section 2, and incorporate part correlations in Section 3. We relate introduced transfer learning methods with feature maps in Section 4. We give implementation details in Section 5. Finally we present a quantitative and qualitative evaluation in Section 6. Although it might be feared that judging the quality of retrieval results will be very subjective, we show that available annotation and measures from the PASCAL VOC [17] can be used for this task.Exemplar SVMs are utilized in a variety of problems including object detection[33], face recognition[28], transferring segmentations masks and semantic scene parsing[45,51], cross domain image matching (matching drawings to pictures)[2,40], transferring 3D geometry [1,33], and transferring labels to 3D point clouds [50]. Here we propose a method for improving the quality of E-SVMs, which will potentially boost these performances of all these methods.Our work uses the notion of parts as patches of classifier templates. Many other studies utilize shared parts across different classes. Torralba et al. [48] introduced a method for sharing small patch oriented templates in a boosting framework and Opelt et al. [34] extended this approach to shared boundary fragments. Fidler et al. [20] explored the shareability of features among object classes in a generative hierarchical framework. Stark et al. [44] proposed a method for transferring part-like shape features through explicit migration of model parameters for each part; however this transfer is manual at the moment. Ott and Everingham [35] introduced part sharing across classes for object detection in the framework of discriminatively trained part-based models [19]. In [22,42] HOG based templates are represented as a sparse reconstruction of shared parts (sparselets) which enable very fast evaluation of multiple detectors. Dean et al. [9] also use the notion of part sharing for efficient evalution of large number of detectors. In another perspective Aytar and Zisserman [4] use shared parts to reconstruct and evaluate a single detector very fast on large image collections using inverted file indexing. Endres et al. [16] proposes to learn a diverse collection of discriminative parts from object bounding box annotations and utilize it for object category detection. Recently mining mid-level discriminative patches for scene understanding [13,14,23,36,37,41] has attracted considerable attention for their automatic discovery of distinctive parts for scene recognition. Mid-level discriminative patches are also utilized for inferring the 3D surface normals given a single image [7], and aligning object parts in order to discover visual connections in space and time [27] .The proposed approach also has a strong relation to the line of work that focuses on enriching the image descriptors with the responses of mid-level and high-level classifiers [18,24,26,29,30,43,54]. These approaches either replace or augment the original low-level descriptor with the outputs of higher level classifiers. The proposed method also employs a similar augmentation scheme; however we augment the feature vector with the responses of previously learned classifier patches which are selected and relocated based on the quality of match with an E-SVM template learned from the query image.Our approach can be viewed both as a transfer regularization approach and a feature augmentation approach. Hence it constructs an equivalence between these two views. We explicitly prove this equivalence and discuss its implications in Section 4.This section discusses the E-SVM formulation and introduces the enhanced E-SVM objective. The formulation of the E-SVM [33] is:(1)minw,bλ∥w∥2+∑iNmax(0,1−yi(wTxi+b))where λ controls the weight of regularization term, w is the classifier vector, b is the bias term, xiand yiare the training samples and their labels, respectively. Note that there is only one positive sample in the training set and its error is weighted more (50 times in [33]) than the negative samples. In order to simplify the formulation, different weightings of positive and negative samples are not explicitly shown.In enhanced E-SVM, part based transfer regularization is incorporated to the E-SVM formulation. The objective is:(2)minw,b,αλ∥w−∑iMαiui∥2+γ∑iMαi2+∑iNmax(0,1−yi(wTxi+b))st:αi≥0,∀iwhere λ and γ controls the balance between the two regularization terms as well as the tradeoff between error term and regularization terms. ui’s are the classifier patches cropped from source classifiers and relocated on a w sized template padded with zeros other than the classifier patch (see Fig. 1), and αi’s are transfer weights. As αi’s are the reconstruction weights on ui’s and negative use of a part is not possible, non-negativity constraints are imposed on αi’s. Note that given a fixed set of ui’s the formulation is convex.The two limits of this formulation are E-SVM and reconstruction from the classifier patches. As γ → ∞, since αi’s will be forced to be zero due to infinite penalization,∑iMαiuiwill be a zero vector and (2) converges to the E-SVM formulation (1). As λ → ∞, w will be forced to be equal to∑iMαiuiand thus it will be forced to be constructed as a weighted combination of ui’s. Therefore by adjusting λ and γ we can obtain a midway solution between E-SVM and reconstruction from the other classifiers. Fig. 2 shows the smooth transition from reconstruction to E-SVM by changing γ with a fixed λ.Discussion. Transfer regularization is introduced as an adaptive SVM (A-SVM) [31,52] which transfers information from a single auxiliary classifier. Subsequently A-SVMs were extended to transfer from multiple classes [53]. The proposed formulation is also a transfer regularization objective which transfers from the parts of previously learned classifiers. The main difference to [53] is that we control the weight of transfer with an additional regularization term(γ∑iMαi2)where γ → ∞ indicates no transfer and γ → 0 indicates maximum transfer. The equivalence of this formulation to a “classical” SVM formulation and advantages will be elaborated in Section 4. Note that this formulation is not specific to E-SVM and this transfer regularization can also be applied to “classical” SVM formulations.Parts generally occur with certain correlations. The existence of a part hints about the existence or absence of some other parts. For instance the occurrence of an up-left window-corner-like part might increase the occurrence probability of up-right window-corner-like part and decrease the existence probability of a zebra-face-like part. Consequently, in addition to transferring parts from the well trained source classifiers, we can also transfer statistical correlations between parts. This section will elaborate on the incorporation of such pairwise statistical correlations in the EE-SVM objective.One common approach for enforcing pairwise statistics is performed through pairwise potential functions using Markov random fields (MRF) or conditional random fields (CRF) frameworks [5]. These potential functions are mostly non-convex and defined over discrete random variables; however, they can be efficiently optimized using graph-cuts or linear programming relaxations. Unfortunately they are not directly applicable for the SVM framework. Here we introduce a pairwise potential function that is convex and can be conveniently applied to convex regularized risk minimization frameworks, particularly SVMs. We are particularly interested in enforcing pairwise statistics in the transfer regularization formulations (i.e. Eq. 3) where the correlation is enforced on the activation of classifier parts ui’s in order to construct the classifier w. Assuming that a positive value of αirepresents the activation of the part uiandαi=0indicates that uiis not activated, the pairwise statistics can be captured through correlations between αi, αjpairs. The energy function to enforce this statistics can be defined as:(3)minαϕ(α)=∑i,jCij∥αi−αj∥2where Cijis the pairwise correlation between the variables αiand αj. Then, the complete transfer objective is:(4)minw,b,αλ∥w−∑iMαiui∥2+γ∑iMαi2+∑iNmax(0,1−yi(wTxi+b))+θ∑i,jM,MCij∥αi−αj∥2st:αi≥0,∀iwhere θ is the hyperparameter that controls the weight of enforcing statistical correlations.Intuitively a positive Cijenforces the values of αiand αjto be as close as possible, therefore if one activated the other will be as well, especially when Cijis a very high positive value. Conversely, a negative Cijforces them to be as distinct as possible. However this objective is not necessarily convex, particularly when Cijcontains negative values. In order to obtain a convex energy function, we introduce a slight addition to the pairwise potential and defineϕ¯as:(5)ϕ¯(α)=∑i,jCij∥αi−αj∥2−4∑iDii−αi2(6)=∑i,jCij+∥αi−αj∥2−∑i,jCij−∥αi+αj∥2where the pairwise correlation matrix C is decomposed into its positive and negative components asC=C++C−,D+is a diagonal matrix with entriesDii+=∑jCij+,andD−is a diagonal matrix with entriesDii−=∑jCij−. Intuitively by introducing more penalization (i.e.−4∑iDii−αi2) over highly negatively correlated αi’s, which is sensible since these αi’s have smaller probabilities to be activated, we can obtain a convex pairwise potential (6). Finally by pluggingϕ¯(α)into (4) as the pairwise potential, we obtain a convex minimization objective that combines transfer regularization and co-occurrence statistics of the parts:(7)minw,b,αλ∥w−∑iMαiui∥2+γ∑iMαi2+∑iNmax(0,1−yi(wTxi+b))+θ(∑i,jM,MCij+∥αi−αj∥2−∑i,jM,MCij−∥αi+αj∥2)st:αi≥0,∀iThis new version of EE-SVM will be referred to as EE-SVM-COR. The part correlations are learnt from the source filters, and the implementation details will be discussed in Section 5.Discussion. Another way of enforcing part correlations is to use inverse covariance matrix regularization, i.e.αTΣ−1α,assuming that Σ is a valid covariance matrix. However, due to the large number of parameters and the limited samples the estimated Σ may not be positive definite which prevents us from computing the inverse. In [21]Σ is treated as an affinity matrix (i.e. stronger correlation means higher affinity). They directly used it for constructing the precision matrix of a Gaussian and used it in the regularization asαT(I−λΣ)α,where λ is a hyperparameter that ensures the positive definiteness. We also treat the correlation matrix Cijas an affinity matrix and enforce part correlations in a similar manner. The detailed derivations of the formulations are given in Section 4.In this section, initially we will investigate transforming transfer learning formulations to a “classical” SVM formulation, which comes with the benefit of using easy and robust optimization for transfer learning approaches and makes it potentially possible to use for practical purposes without the need of expert knowledge in transfer learning. Several equivalence relations between transfer regularization and feature mapping will be investigated, and their corresponding implications will be discussed.Transfer regularization [52] is applied for many transfer learning approaches in object classification and detection. We’ll start with the general form of the formulation used by [47] for transferring from multiple sources which uses squared loss. Due to the robustness of the hinge loss over the squared loss, we substitute the squared loss term with the hinge loss and obtain:(8)minα,w,bλ∥w−∑iMαiui∥2+∑iNmax(0,1−yi(wTxi+b))st:∥α∥≤1where ui’s are the source classifiers and αi’s are the corresponding weights of transfer for each source classifier. After a slight modification to the transfer regularization formulation (i.e. from (8) to (2) by bringing the constraint into the objective), we will transform the problem to a “classical” SVM formulation through a feature augmentation approach. The derivation below steps through the rearrangements for mapping the transfer regularization objective to an equivalent “classical” SVM formulation where the feature vector is augmented with the responses of source classifier models. Letw=w^+∑iMαiui,and then the derivation is:(9)λ∥w−∑iMαiui∥2+γ∑iMαi2+∑iNmax(0,1−yi(wTxi+b))(10)=λ∥w^∥2+γ∑iMαi2+∑iNmax(0,1−yi(w^Txi+(∑iMαiui)Txi+b))(11)=∥w¯∥2+∑iNmax(0,1−yi(w¯Tx¯i+b))wherew¯=[λw^;γα1;γα2;...;γαM],x¯i=[1λxi;1γu1Txi;1γu2Txi;...;1γuMTxi],w¯is the transformed classifier andx¯iis the augmented feature vector with the responses of u’s on xi. The classifier w, the solution to the original problem (9), can easily be computed fromw¯sincew=w^+∑iMαiui. As is clear from (11), the transformed problem is a “classical” SVM formulation with feature augmentation, and it can be optimized efficiently using existing powerful SVM solvers. Note that this derivation is applicable to both EE-SVM objective and previously used transfer regularization formulations.Discussion. The major implication of this derivation is that transfer regularization can also be stated as a classical SVM minimization problem where the feature vector is augmented with the responses of source classifiers. This equivalence constructs a bridge between papers implementing feature augmentation or populating the features with the responses of high-level classifiers [15,18,24,26,32,49,54] and papers performing transfer regularization [46,47,52,53]. Another direct implication is that transfer regularization approaches [46,47,52,53], which requires specialized optimization, can be reformulated to be efficiently optimized with the state-of-the-art SVM solvers.Another way of converting (9) to a normal SVM formulation would be solving α analytically and substituting it into the original equation. Let the regularization part of (9) beR=λ∥w−Uα∥2+γ∥α∥2then the derivation is as follows:(12)∂R∂α=0→α=λ(γI+λUTU)−1UTwBy substituting α in to R we obtain:(13)M=λ(I−λU(γI+λUTU)−1UT)+λγ(γI+λUTU)−1UT(14)R=∥Mw∥2=wTMTMwSimilar to the derivation in (11), this regularization can also be implemented as a feature transformation (i.e.w¯=Mw,x¯i=(MT)−1xi) and solved using a normal SVM formulation.This section will discuss how to map the objective (7) to a “classical” SVM formulation by defining appropriate feature maps. Initially the formulation will be converted to a graph Laplacian form, and then by incorporation of some new variables, a few pseudo-training samples and appropriate rearrangements the objective will be transformed to a linear SVM objective.Assuming that Cijare the weights on a graph, ϕ(α) in (3) can be re-written in the form below:(15)ϕ(α)=∑i,jCij∥αi−αj∥2=2αTLαwhereL=D−C,Dii=∑jCij,where L is the graph Laplacian. Similarly we can rewriteϕ¯(α)in (5) as:(16)ϕ¯(α)=∑i,jCij∥αi−αj∥2−4αTD−α=2αTL¯αL¯=D¯−C,D¯ii=∑j|Cij|=D+−D−LetU=[u1,u2,..,uM]andw=w^+∑iMαiui=w^+Uα,rewriting (7) with appropriate substitutions:(17)minw,b,αλw^Tw^+αT(γI+2θL¯)α+∑iNmax(0,1−yi(w^Txi+αTUTxi+b))st:αi≥0,∀iConsidering bothL¯and I are positive-semi definite, we can decompose the termγI+2θL¯using the Cholesky decomposition asRRTthen introduceβ=RTαand rewrite:(18)αT(γI+2θL¯)α=αTRRTα=βTβ(19)αT=βTR−1Plugging in (18) and (19) to (17) we obtain:(20)minw^,b,βλw^Tw^+βTβ+∑iNmax(0,1−yi(w^Txi+βTR−1UTxi+b))st:[(R−1)Tβ]i≥0,∀i=minw¯,b∥w¯∥2+∑iNmax(0,1−yi(w¯Tx¯i+b))st:[(R−1)Tβ]i≥0,∀iwherew¯=[λw^;β],x¯i=[1λxi;R−1UTxi],Other than the linear constraints[(R−1)Tβ]i≥0,the objective becomes a linear SVM objective.Enforcing linear constraints in SVMs. For any classical SVM formulation:(21)minw,b∥w∥2+∑iNmax(0,1−yi(wTxi+b))we can implement a linear constraintaTw>0by introducing an additional training samplexN+1=[∞×a]with the labelyN+1=+1. Here ∞ × a is the multiplication of the vector a with the infinity which is approximated with a very high numeric value (i.e. 106).A similar approach will be used for implementing the linear constraints[(R−1)Tβ]i≥0,∀iin (20). Another additional training sample will be added for each row of(R−1)Tas each row introduces another constraint on β. Since the constraints only concern the β part ofw¯=[λw^;β],the initial parts of the additional training sample will be filled with a zero vector of the same length asw^. The rest of it will be filled by the appropriate row extracted from(R−1)Tand multiplied by infinity. In a formal definition, linear constraints are implemented by adding M additional samples to our training set as below:(22)x¯N+k=[0|w^|;∞×(R−1)k,:T],yN+k=+1,1≤k≤Mwherex¯N+kare the additional training samples that implement the constraints,0|w^|is the zero vector of the same length asw^,(R−1)k,:Tis the kthrow of(R−1)T.After handling the constraints as well, the transformation of the original objective (7) to a classical SVM formulation is completed. Therefore we can optimize (7) using available robust SVM optimization tools.In this section the details of the implementation will be discussed. Initially training source classifiers and E-SVM will be discussed. Then EE-SVM training procedures and obtaining part correlations will be explained.Source classifiers and part vocabulary. The source classifiers are linear SVM classifiers (templates) over HOG [8,19] features. The source classifiers are trained with three components for each class and without parts, similar to the procedure in [19]. While testing on the PASCAL VOC 2007 test set, the source classifiers are learnt from the 1000 classes of the ImageNet [11] 2012 challenge. In total, the 329 models out of 1000 are selected that have an AP over 30% on a small validation set of 1000 images (50 positive images per class). Together with the mirrored versions of these templates it totals to 329  ×  3  × 2=1974 source classifiers. To build the vocabulary of classifier patches, all patches with sizes of 5 × 5 are extracted from the components of the trained DPMs, then a k-means clustering is performed withk=10Kcenters. Other ways of composing the vocabulary could be learning the vocabulary via sparse reconstruction as in [42], or learning both the vocabulary items and the classifiers at the same time as in [22]; however this is beyond the scope of our work. While testing on ImageNet, the source classifiers are trained using 20 classes of the PASCAL VOC 2007 training set. All 5 × 5 classifier patches are used to form a vocabulary of 1948 items.E-SVM training. Similar to [33], each E-SVM is composed of 100 or slightly less HOG cells where the aspect ratio is chosen according to the query image. The E-SVM is trained with the given query image as the positive sample and randomly selected 2000 negative images from the PASCAL VOC 2007 training set. The training is performed iteratively in a similar fashion to [33] where mined hard negatives are incorporated to the learning after each iteration.EE-SVM training. The training procedure of EE-SVM, which is briefly visualized in Fig. 1, starts with training an E-SVM classifier from the given query image. After obtaining the E-SVM, for each 5 × 5 cell classifier patch a good match is searched for within the vocabulary. This linear search can be efficiently done using fast matrix multiplication since we have a limited number of vocabulary items. Even though we use 5 × 5 cell classifier patches for experimental validation, any other varying size and aspect ratio can also be applied. A good match is defined by thresholding the cosine similarity (normalized dot product) between the E-SVM patch (a 5 × 5 × 32 dimensional vector) and vocabulary items. This threshold value is fixed to 0.2. After determining where to transfer from, each patch is relocated on a w sized HOG template padded with zeros other than the transferred classifier patch. Finally learning of the EE-SVM is performed using the same set of training samples used for training the E-SVM and no new hard negatives are collected.Partial matching for occlusion and truncation. In occluded or truncated queries, the parts of the E-SVM classifier might not match well with the vocabulary items due to the distortion caused by occlusions and truncations. Hence instead of using good mathces of the E-SVM patch from the vocabulary items as defined above, ignoring a few cells (potentially occluded or truncated regions) while computing similarity would be a better idea. The matches obtained by this partial matching method is referred as partial good mathces of the E-SVM patch. Here we only use the most similar (i.e. cosine similarity) β percent of the cells for matching an E-SVM patch to a vocabulary item. For instance if the E-SVM patch and vocabulary items are 5 × 5 cells and β is 70%, then the top matching 18 (i.e. 70% of 25) cells are used for computing normalized dot product and deciding if the vocabulary item is a partial good match or not. Similar to the good match, the same threshold of 0.2 is used for the decision.Optimization. The optimization of the EE-SVM and EE-SVM-COR objectives are performed using the LIBSVM [6] package through the equivalent feature mapping formulations which are already discussed in Section 4. The only additional cost of EE-SVM and EE-SVM-COR over E-SVM is the transformation of training samples, and training another SVM, which constitutes less than 1% of the training time (i.e. mining hard negatives is costly). The test time complexity of EE-SVM and EE-SVM-COR is exactly the same as that of E-SVM.Part correlations. The entries of the pairwise part correlation matrix Cijare estimated using the pairwise sample correlation coefficient ρi, jwhich is computed from the joint occurrence of parts i and j in the source filters (i.e. the source filters are the samples). The computation of the correlation coefficient is:(23)ρi,j=cov(i,j)σiσj=E[(i−μi)(j−μj)]σiσjwhere cov is the sample covariance, σiis the standard deviation of occurrence of part i computed over samples (1 states occurrence and 0 states absence of part i in the given sample), μiis the mean occurrence of part i across all the samples, and E is the expectation.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
