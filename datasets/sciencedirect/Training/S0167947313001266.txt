@&#MAIN-TITLE@&#
OLS with multiple high dimensional category variables

@&#HIGHLIGHTS@&#
We generalize the linear within-estimator to two or more category variables.We describe a general procedure for projecting out dummy-encoded category variables.Parameters for dummy variables can optionally be estimated.

@&#KEYPHRASES@&#
Alternating projections,Fixed effect estimator,Kaczmarz method,Two-way fixed effects,Multiple fixed effects,High dimensional category variables,Panel data,

@&#ABSTRACT@&#
A new algorithm is proposed for OLS estimation of linear models with multiple high-dimensional category variables. It is a generalization of the within transformation to arbitrary number of category variables. The approach, unlike other fast methods for solving such problems, provides a covariance matrix for the remaining coefficients. The article also sets out a method for solving the resulting sparse system, and the new scheme is shown, by some examples, to be comparable in computational efficiency to other fast methods. The method is also useful for transforming away groups of pure control dummies. A parallelized implementation of the proposed method has been made available as an R-package lfe on CRAN.

@&#INTRODUCTION@&#
We consider OLS estimation of models of the form(1)y=Xβ+Dα+ϵ,whereyis a response vector of lengthn,Xis ann×k-matrix of covariates with corresponding parametersβ,Dis ann×g-matrix of dummies with corresponding parametersα, andϵis a normally distributed stochastic term.Dis assumed to arise from dummy-encoding of one or more category variables, and it is assumed that each of theseecategory variables have a large number of different values (large as in105–107).That is,Dis a block matrix,D=[D1D2⋯De]. The entries of eachDiconsist of 0 and 1, with 1 non-zero entry per row. Hence, the columns of eachDiare pairwise orthogonal. In general, however,Diis not orthogonal toDjfori≠j. We also assume thatkis reasonably small, and that the system without dummies is therefore manageable.Models like this have been used to analyse panel data in the econometrics literature, e.g. in Abowd et al. (1999), which studies wage as an outcomey, and where a dummy variable is introduced for each individual(D1), as well as for each firm(D2). As the individuals move between different firms, it is not a nested model. The dummy modelling is done to account for arbitrarily distributed time-constant unobserved heterogeneity, both among employees and among firms. In this setting, what is studied is the correlation between the firm effect and the individual effect on the wage. Other applications of similar models can be found in Bazzoli et al. (2008), Carneiro et al. (2012), Cornelißen and Hübler (2011), Jacob and Lefgren (2008), Aaronson and Barrow (2007), Abowd et al. (2006) and Gibbons et al. (2010), e.g. with school and teacher effects, worker and area effects, or hospital and doctor effects. Because the datasets, sourced from government registers containing hundreds of thousands, or even millions, of observations, are so large, there is a very large number of dummies, requiring special algorithms for their estimation.McCaffrey et al. (2010) compare some estimation schemes for these models. We present a new estimation algorithm for such models. In an appendix, we present some test runs on both simulated and real datasets, comparing the suggested method to a2reg by Ouazad (2008), the fastest one in McCaffrey et al. (2010). Our method has the following properties.•For the system (1),βˆis found without having to findαˆ, i.e. large matrices are avoided.Since the method is a direct generalization of the within-groups estimator, utilizing the Frisch–Waugh–Lovell theorem, it yields a covariance matrix forβˆ, which in the case ofe=2is the same as if (1) had been solved directly with standard OLS. In the casee>2, the covariance matrix may be slightly up-scaled. The other methods of McCaffrey et al. (2010) lack this property: either no standard errors are provided, or they need to be computed for bothαˆandβˆrequiring the use of various time-consuming methods. In the approach proposed here, however, it is possible to have a number of pure control dummies inDwhich are simply projected out of the system and not estimated, and we still get good estimates forβˆ. An example can be found in Markussen and Røed (2012).αˆmay optionally be retrieved, but identification of coefficients inαˆin the casee>2hinges on the ability of the researcher to find enough estimable functions suitable for the problem at hand. The estimation scheme provides a test for estimability. Some examples are given in the appendix.The method is comparable in speed to a2reg, sometimes faster and sometimes slower.In Andrews et al. (2008) it is shown that the above-mentioned correlation is negatively biased; the present approach does not concern itself with the appropriateness of those models to the solution of particular problems, or with their statistical properties. What it does concern itself with, however, is the OLS estimation of the parameter vectorsβandαonly.Although we occasionally refer to the firm-employee application mentioned above, it is only because it gives us convenient names for the category variables. While the main application of our results is in the analysis of panel data, the results are general in the sense that they do not depend on any particular application; any set of dummiesD, be it age-groups, hometown, school, workplace, may be transformed out of Eq. (1). The approach does not even depend on whether the underlying data are panel data; we are neither concerned with time-constant and time-varying covariates, nor, indeed, time in general, just with sets of possibly high-dimensional category variables in OLS estimation.Remark 1.1In the econometrics literature, having a time-constant dummy for each individual is referred to as having individual fixed effects. Thus, to (some) econometricians, theαabove is the fixed effects, whereasβis not. To statisticians, this use of the phrase “fixed effect” is confusing, as theβis also a fixed effect. Below, we use the phrase “fixed effect” in the econometrician’s sense. I apologize to statisticians who might (rightfully) find this confusing, but it is done to (hopefully) make the text more accessible to econometricians.A common strategy when working with a single category variable, is to centre the covariates and response on the group means, and do OLS on the projected system (Wooldridge, 2002, Section 10.5). Such centring consists of computing, for each category, e.g. individual, the mean of the covariate. The mean is then subtracted from the covariate’s values in the category. Thus, time-constant effects are removed, and only time-varying effects within each category matter in the estimation. That is, the category effect serves to replace all time-constant effects.Centring on the means is also referred to as “demeaning”, “time demeaning”, “fixed effects transformation”, “within transformation”, “within groups transformation” or “sweeping out the fixed effects”. It seems to be common knowledge that sweeping out more than one category variable may not be done by centring on the group means, or by other simple transformations of the data, see e.g. Abowd et al. (1999, p. 266), Andrews et al. (2008, p. 676), Cornelißen and Hübler (2011, p. 476), and Margolis and Salvanes (2001, p. 19). Other estimation methods have therefore been developed to meet this challenge. Several authors, (e.g., Abowd et al., 2002; Ouazad, 2008; Cornelißen, 2008; Schmieder, 2009), have implemented procedures for the estimation of such models.The main contribution of this work is Remark 3.2. It is indeed possible to sweep out multiple fixed effects, due to the Frisch–Waugh–Lovell theorem and certain other, relatively old results (von Neumann, 1949, Lemma 22, p. 475), and (Halperin, 1962, Theorem 1),11Halperin’s article is available here: http://bit.ly/HJ067o.now known as The Method of Alternating Projections. This leaves us with two systems, one of which is manageable with off-the-shelf OLS software; the other, a large, sparse system for the fixed effects. As a bonus we get a covariance matrix for the small system, theβs, a property which is lacking in many of the other estimation schemes. In Algorithm 6.1 and the discussion preceding it, we also suggest a method, the Kaczmarz method (Kaczmarz, 1937), for solving the sparse system.To the author’s knowledge, both the Kaczmarz method and Halperin’s method of alternating projections are little known in the econometric and statistical literature, even though they sit very nicely with the Frisch–Waugh–Lovell theorem. There is an application of Halperin’s theorem to the proof of convergence of the backfitting algorithm in Ansley and Kohn (1994), and the Kaczmarz method is actively in use in medical imaging, under the name “Algebraic Reconstruction Technique” (ART), (Andersen and Kak, 1984; Gordon et al., 1970; Hudson and Larkin, 1994; Herman, 2009).To get an intuitive graphical view of these methods, and why they matter when studying linear systems, recall that a line, a plane, hyperplane or linear subspace is merely the solution set of one or more linear equations. The intersection of such subspaces is the simultaneous solution of the corresponding sets of equations. Consider e.g. two intersecting lines in Euclidean plane. To find the intersection, we can start at a point anywhere in the plane, project it orthogonally onto one of the lines, project it again onto the other line, then back to the first line, and so on. We zigzag towards the intersection. Clearly, if the lines are orthogonal, we will reach the intersection in just two steps. If the lines intersect at an acute angle, more steps will be needed to get close. The Kaczmarz method is the generalization of this process to a finite set of hyperplanes in a high dimensional Euclidean space. von Neumann’s lemma is the generalization to two arbitrary subspaces of a Hilbert space. Halperin’s theorem generalizes this further, to a finite number of subspaces.The method presented falls in the broad category of sparse methods. Other recent advances in sparse methods may be found in Lee and Huang (2013) and Vidaurre et al. (2013), whereas a method for choosing between different models may be found in Ueki and Kawasaki (2013).In (1), we assume that the model is well specified in the sense that the only multicollinearities in the system occur inD. Some of these multicollinearities are due to the fact that we have constructedDfrom a full set of dummies, with no references. Others may be due to spurious relations between the category variables. We return to the mathematical details, once we have introduced some notation.The task is to compute the OLS estimatesβˆandαˆof the parameter vectorsβandαin (1). In particular we look at the casee=2, corresponding to two category variables, e.g. “firm” and “employee” as in Abowd et al. (1999) and Andrews et al. (2008).We now derive the Frisch–Waugh–Lovell theorem. To do this, we consider for the time being a full-rank versionDofD, i.e. we remove just enough linearly dependent columns fromDto get a new full-rank matrixDwith the same range and withrank(D)=rank(D). That is,D′Dis invertible. The manner in which linearly dependent columns are removed is not important for most of the results of this section, since the results are in terms of the range projection ofD, which only depends on the column space. We also remove the corresponding coordinates fromαto get anαr.We will have occasion to use several of the intermediate formulae later. An identity matrix of the appropriate size will generally be denoted byI. The transpose of any matrixMis denoted byM′.The normal equations of system (1), withDandαrare(2)[X′XX′DD′XD′D][βˆαˆr]=[X′D′]y.We recall some standard facts about these. We may write them as two rows(3)X′Xβˆ+X′Dαˆr=X′y(4)D′Xβˆ+D′Dαˆr=D′yand do Gaussian elimination of theαˆr-term in the last row to get(5)X′(I−D(D′D)−1D′)Xβˆ=X′(I−D(D′D)−1D′)y.Now, letP=I−D(D′D)−1D′and note thatP=P2=P′is a projection. Indeed,Pis the projection on the orthogonal complement of the column space ofD. Rewriting Eq. (5) withP, we get(PX)′(PX)βˆ=(PX)′Py,which shows thatβˆis the OLS solution of the projected system(6)Py=PXβ+Pϵ.We do not needαˆ, then, to findβˆ. Moreover, by multiplying through (4) withD(D′D)−1and noting thatD(D′D)−1D′=I−P, we get(7)(I−P)Xβˆ+Dαˆr=(I−P)y,which may be reordered asy−(Xβˆ+Dαˆr)=Py−PXβˆ,showing that the residuals of the projected system (6) are the same as the residuals of the full system (1).In practice, it sometimes happens thatβˆis not uniquely determined by (6). This is the same problem that affects the ordinary within-groups models withe=1, e.g. covariates inXwhich are constant for individuals. We do not treat this specification problem here. That is, we assumePXis full rank. Similarly, we assume thatQDis full rank, whereQis the projection onto the orthogonal complement of the range ofX. That is, loosely speaking,Xshould not explain any category inDfully, nor shouldDexplain any of the variables inXfully. In other words, barring those we have temporarily removed fromD, the system should be free of other multicollinearities.Now, recall that to find the covariance matrix ofβˆandαˆin system (1), we invert the matrix in (2) and multiply with the residual sum of squares divided by the degrees of freedom. Also, recall from general matrix theory the block inversion formula[ABCE]−1=[(A−BE−1C)−1−A−1B(E−CA−1B)−1−E−1C(A−BE−1C)−1(E−CA−1B)−1],which is easily shown to hold when the quantities in question are invertible. We use this on the matrix in (2). After some simplification, we get[X′XX′DD′XD′D]−1=[(X′PX)−1−(X′X)−1X′D(D′QD)−1−(D′D)−1D′X(X′PX)−1(D′QD)−1],so that the upper left entry of the inverse, i.e. the part that goes into the covariance matrix forβˆ, is(X′PX)−1. Now,(X′PX)−1is also the inverse of the corresponding matrix in system (6).Remark 2.1Getting the same matrix, and the same residuals, we can perform an OLS on system (6) in the normal way, adjusting only the degrees of freedom according to the number of parameters projected out byP, to find theβˆpart of the covariance matrix for system (1).The above result is known as the Frisch–Waugh–Lovell theorem. It is a standard way to eliminate the fixed effects from the equation in the casee=1, i.e. with a single fixed effect. In this case, the projectionPis the within transformation.Remark 2.2Note thatPis the projection ontoR(D)⊥, whereR(D)denotes the range ofD, i.e. the column space, and⊥denotes orthogonal complement. We haveR(D)=R(D)since we have only removed linearly dependent columns, soPandβˆdo not depend on the specifics of how we produceDfromD. Nor does the matrix(X′PX)−1.Although (6) shows us how to eliminate the fixed effects for arbitrarye≥1, and we even have an explicit matrix formula forP, due to size limitations with largen≈107, it is impractical to handle then×nprojection matrixPdirectly. However, we do not really need to find the matrixP; what we need to do is, to computePyandPX.To this end, for eachi=1..eletPibe the projection onto the orthogonal complement of the range ofDi,R(Di)⊥.Piis the within-groups transformation for category variablei. We have(8)R(D)⊥=R(D1)⊥∩R(D2)⊥∩⋯∩R(De)⊥,thus, in terms of projections,(9)P=P1∧P2∧⋯∧Pe.To see that (8) holds, assumeηis a vector on the left hand side, i.e.η∈R(D)⊥. Thenηis orthogonal to the column space ofD, thus to every column ofD. Since the columns ofDconsist of the columns ofD1,…,De,ηmust be orthogonal to eachR(Di)fori=1..e; we therefore haveη∈R(Di)⊥, and, in consequence,ηis in the intersection on the right hand side. Conversely, assume that for eachi=1..e, we haveη∈R(Di)⊥, i.e.ηis in the intersection on the right hand side. For eachi,ηis orthogonal toR(Di), and thus to every column ofDi. Again, since the columns ofDconsist of columns from theDis,ηis orthogonal to every column ofD, henceη∈R(D)⊥. Therefore, equality holds in (8).By using (Halperin, 1962, Theorem 1) on (9), we now have(10)P=limn→∞(P1P2⋯Pe)n.The convergence holds in the strong operator topology, i.e. pointwise on vectors, although for finite dimension, it is equivalent to both the weak and the uniform operator topology.This shows that the following algorithm converges.Algorithm 3.1Method of Alternating ProjectionsLetvbe a vector (typically a column ofXory). The following algorithm converges toPv. It is a direct generalization of the within-groups transformation (i.e. the casee=1).(1)Pick a tolerance, anϵ>0, e.g.ϵ=10−9. Letv0=v, andi=0.Letz0=vi. Forj=1..e, formzjby subtracting the group means of the groups inDjfromzj−1, i.e.zj=Pjzj−1.Letvi+1=ze. If‖vi+1−vi‖<ϵ, terminate with the vectorvi+1as an approximation toPv. Otherwise, increaseiby 1. Go to step (2).Remark 3.2By using Algorithm 3.1 onyand the columns ofX, we findPyandPX, and the estimation ofβˆfrom (6) is therefore manageable.Example 3.3In the case withe=2, i.e. with two fixed effects like “firm” and “individual”, Algorithm 3.1 amounts to repeating the process of centring on the firm means, followed by centring on the individual means, until the vector no longer changes.The rate of convergence for the method of alternating projections is analysed in Deutsch and Hundal (1997), though by general concepts for which the author has not found an intuitive description in terms of the covariates used to constructDin a typical panel data model. For the casee=2, Aronszajn (1950), cited in Deutsch and Hundal (1997, Corollary 2.9), has an estimate(11)‖(P1P2)n−P‖≤cos2n−1(R(D1)⊥,R(D2)⊥),where the functioncosdenotes the cosine of the (complementary) angle between subspaces. The inequality (11) was later shown in Kayalar and Weinert (1988, Theorem 2) to be an equality. The quantity on the right hand side is strictly smaller than 1 in finite dimensional spaces (Deutsch and Hundal, 1997, Lemma 2.3(3)). Thus, we have linear convergence, but the rate varies with the structure ofD. In Deutsch and Hundal (1997, Theorem 2.7 and Section 3), the casee>2is also handled, but the convergence rate is more complicated.Having foundβˆas per Remark 3.2, we now look at the problem of findingαˆ. Recall from (4) that(12)D′Dαˆr=D′ρ,whereρ=y−Xβˆare the residuals for the original system with dummies omitted.In the special case with a single category variable (e=1), the columns ofDare orthogonal, andD′Dis therefore diagonal;αˆris simply the group means of the residualsρ. This is the within-groups estimator. Ife>1,D′Dis not diagonal, though it is typically quite sparse, and the pedestrian approach is to apply a good sparse solver on (12), such as those evaluated in Gould et al. (2007). However, there is another avenue which has proven useful.We rewrite (7) as(13)Dαˆr=(I−P)(y−Xβˆ)=(y−Xβˆ)−(Py−PXβˆ)where the right hand side is readily computed when we haveβˆ. Now, although the derivation of (7) assumed that linearly dependent columns ofDhad been removed, we may consider the equation with the original rank-deficientD. As noted in Remark 2.2, the right hand side will be the same, and sinceDandDhave the same range, the equation withDwill still have a solution, i.e. we consider the equation(14)Dαˆ=(y−Xβˆ)−(Py−PXβˆ),whereαˆis only identified up to translation by the null-space ofD. We resolve the ambiguity by applying an estimable function to the solution.To find a solution to (14), we can apply the Kaczmarz method, (Kaczmarz, 1937). Following the method, we can view each equation in system (14) as defining a hyperplane inRg. The intersection of all the hyperplanes is the solution set of the system. The Kaczmarz method is a variant of the method of alternating projections, where we start with a vector and successively project it onto each hyperplane. The process is repeated until the vector stops changing. This will have brought us to the intersection, i.e. a solution to (14).In our case, the projection onto a hyperplane is very simple. Remember that each row ofDcontains exactlye1’s; the other entries are zero. We write rowiof system (14), withxinstead ofαˆ, as〈di,x〉=bi, wherediis rowiofD,biis thei’th coordinate of the right hand side, and〈⋅,⋅〉denotes the Euclidean inner product. The projection onto the solution set of rowiis(15)x↦x−(〈di,x〉−bi)di/‖di‖2,which is easy to compute;‖di‖2=efor everyi, and the inner product is merely a sum ofeof the coordinates ofx. In other words, the update toxrequires minimal computation.Solving (14) consists of starting with e.g.x=0, and applying the projections (15), fori=1..nin succession, and repeating this process until the change inxis smaller than some tolerance. A process entirely analogous to Algorithm 3.1, with theePisreplaced by thenprojections in (15). It is noted in Deutsch and Hundal (1997, Section 4) that their convergence rate results also cover affine sets, not only subspaces.Remark 4.1It is easily seen that consecutive duplicate rows inDmay be ignored, since the projection in (15) is idempotent.When we have found a solutionγof (14), we must apply an estimable function toγ, to obtain unique, meaningful coefficients. To see that this works, recall that an estimable function is a matrix operatorFwhose row space is contained in the row space ofD. Denote the null-space of a matrixMbyN(M). From matrix theory, the row-space ofMis the orthogonal complementN(M)⊥. Thus, we haveN(F)⊥⊂N(D)⊥, or, equivalently,N(D)⊂N(F). Hence, ifγ1andγ2are two arbitrary solutions of (14), i.e.Dγ1=Dγ2, we haveD(γ1−γ2)=0, thusF(γ1−γ2)=0, soFγ1=Fγ2. That is, the value of the estimable function does not depend on the particular solution of (14).Since the method described above is typically used with category variables with many different values, there is a real chance of spurious relations occurring between them, resulting in non-obvious rank-deficiency inD, and, it follows, an identification problem forαˆ.To be complete, we now recall a known identification result for the casee=2. This is needed to find estimable functions. Abowd et al. (1999) have analysed this problem in the case with two dummy-groups (firms and individuals). The problem was also treated much earlier in a different setting, by Eccleston and Hedayat (1974), and the references cited therein.In Abowd et al.’s approach, an undirected bipartite graphGis constructed in which each vertex consists of a firm or an employee. A firm and an employee are adjacent if and only if the employee has worked for the firm. There are no more edges in the graph, i.e.D′, with duplicate columns omitted, is the (vertex–edge) incidence matrix of the graph.They then analyse identifiability in terms of the connected components (or “mobility groups”) of the graphG, and show that it is sufficient to have a reference dummy in each of the connected components, (see Abowd et al., 2002, Appendix 1). That is, we have the theoremTheorem 5.1Various AuthorsIfe=2, the rank deficiency ofD, hence ofD′D, equals the number of connected components of the graphG.ProofTo put this result in the context of spectral graph theory, we provide the following reference. The matrixD′may be viewed as the incidence matrix of the graphG;D′Dis then the signless Laplacian ofG. Moreover, the graph is bipartite, with “firms” in one partition, “employees” in the other. By Cvetković et al. (2007, Corollary 2.2), the multiplicity of eigenvalue 0, i.e. the rank deficiency, is the number of connected components.□Remark 5.2For generale, we get ane-partite,e-uniform hypergraph. We are not aware of similar general results for such graphs. However, the casee=3has been analysed in Godolphin and Godolphin (2001), who offer a procedure for finding the estimable functions in terms of staircase partitions; nevertheless it is not as simple as finding graph theoretic connected components. The spectrum of the signless Laplacian is also an active field of research in graph theory, see Cvetković and Simić (2010) for a survey.

@&#CONCLUSIONS@&#
