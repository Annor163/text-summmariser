@&#MAIN-TITLE@&#
Artificial bee colony algorithm with memory

@&#HIGHLIGHTS@&#
Artificial bee colony with memory algorithm (ABCM) is proposed.ABCM introduces the memory ability of natural honeybees to ABC.ABCM is designed as simply as possible for easy implementation.Experiments on the benchmark functions show the superiority of ABCM.It bridges the gap between ABC and the neuroscience research of real honeybees.

@&#KEYPHRASES@&#
Artificial bee colony,Swarm intelligence,Bee memory,Foraging,

@&#ABSTRACT@&#
Artificial bee colony algorithm (ABC) is a new type of swarm intelligence methods which imitates the foraging behavior of honeybees. Due to its simple implementation with very small number of control parameters, many efforts have been done to explore ABC research in both algorithms and applications. In this paper, a new ABC variant named ABC with memory algorithm (ABCM) is described, which imitates a memory mechanism to the artificial bees to memorize their previous successful experiences of foraging behavior. The memory mechanism is applied to guide the further foraging of the artificial bees. Essentially, ABCM is inspired by the biological study of natural honeybees, rather than most of the other ABC variants that integrate existing algorithms into ABC framework. The superiority of ABCM is analyzed on a set of benchmark problems in comparison with ABC, quick ABC and several state-of-the-art algorithms.

@&#INTRODUCTION@&#
As a relatively new optimization method inspired by swarm intelligence, artificial bee colony algorithm (ABC) [1,2] imitates the foraging behavior of honeybees to perform its search mechanism. Many studies have confirmed that ABC can achieve very competitive performance comparing with the classical evolutionary algorithms [3]. More importantly, it is very simple to implement and only consists of three control parameters to be tuned, i.e., population size (number of food sources) SN, maximal number of generations (terminal condition) maxGeneration, and exploration parameter limit.11Since SN and maxGeneration are the general parameters of almost all evolutionary algorithms, it can be viewed that ABC only consists of one control parameter: limit.Accordingly, researchers have successfully applied ABC to numerous applications [4], i.e., numerical function optimization [5–8], scheduling problems [9–11], clustering [12,13] and program generation [14,15], etc.Many researchers have focused on the improvement of ABC from different perspectives. Inspired by particle swarm optimization (PSO), Zhu and Kwong [16] developed a gbest-guided ABC (GABC) to improve the search efficiency by incorporating the information of global best (gbest) solution into the original search equation of ABC. Gao and Liu [6] are influenced by differential evolution (DE) to propose a new search equation called ABC/best/1 together with a new chaotic initialization method, which improves the exploitation ability of ABC. Banharnsakun et al. [17] developed a modified search equation so that the solution direction is biased towards the best-so-far solution rather than a randomly selected neighbor one. Li et al. [18] and Xiang and An [19] followed similar concepts to combine the information of best-so-far solution to the search equation in different ways to accelerate the evolution efficiency. The newly proposed search equations are combined with the other search equations to parallel create multiple new solutions, where a greedy selection is applied to select the best one. They argued that the search efficiency can be improved significantly, however, which actually tends to perform an unfair comparison since they require larger number of fitness evaluations than ABC within a predefined maxGeneration. Das et al. [8] developed an improved ABC by introducing a fitness learning mechanism with a weighted selection scheme and proximity based stimuli to balance the exploitation and exploration of ABC.It has been recognized that most of the current ABC variants are dedicated to present new hybrid ABC algorithms or combining operators of existing algorithms into ABC, rather than modeling the natural behavior of honeybees in biology especially neuroscience. A very recent work by Karaboga and Gorkemli [20] indicated that by modeling the foraging behavior of artificial bees in a more accurate way, ABC can achieve better performance than standard ABC in terms of local search ability. They introduced a quick ABC algorithm (qABC) to imitate the real onlooker bees’ behavior. That is, Euclidean distance is used to help each onlooker bee to choose the fittest food source within a restricted dancing area rather than to deterministically choose the food source. Apart from this work, there is few study on improving ABC from the perspective of real honeybees.In this paper, we develop a new ABC variant named ABC with memory algorithm (ABCM). ABCM is inspired by the studies of natural honeybees in neuroscience [21–23] which indicate that in addition to the swarm behavior, honeybees consist of memory ability to allow them efficiently trace new nectar by associatively learning from their previous experience. According to this concept, ABCM imitates a memory mechanism to the artificial bees of ABC to memorize their previous successful experiences of foraging behavior. The memory mechanism is applied to guide the further foraging of the artificial bees, which leads to a more efficient search performance than traditional ABC without memory ability. ABCM is developed in a fairly simple way to preserve the advantages of ABC's simplicity as much as possible. Only one new control parameter related to the memory size called M is added in ABCM.In addition to our work, there are two recent studies which are also dedicated to introduce the memory mechanism into ABC [24,25]. Kiran and Babalik [24] added a memory board to save the solutions whose qualities are better than the average fitness value, where they are only used in the neighbor selection of the onlooker bees to increase the exploitative tendency of ABC. Bayraktar [25] integrated the short term tabu list (STTL) of tabu search to memorize the abandoned solutions, which will be prohibited to be repeatedly generated and visited. The memory mechanisms of these two studies are designed in a solution-level to memorize the entire solutions, while ABCM memorizes the successful search experience of each parameter by the neighboring food source and updating coefficient. In other words, ABCM is designed in a parameter-level, which is conceptually different from the aforementioned studies. It can be considered as a complementary study rather than a competitor.In what follows we will organize the paper as follows: Section 2 presents a brief introduction of ABC. Section 3 describes ABCM in details. Section 4 testifies our proposal in the numerical experiments. Finally, Section 5 concludes the paper.Artificial bee colony algorithm (ABC) maintains a population of individuals/solutions, which are specifically named food sources. The population consists of SN food sources, which are evolved by three groups of artificial bees, including employed bees, onlooker bees and scout bees.In the group of employed bees, each bee corresponds to a specific food source, which memorizes the position of its food source. The employed bees search the neighboring region of the food sources to seek better ones. Afterwards, the new food sources are updated and shared with onlooker bees. Onlooker bees work in a different way from employed bees. Exploitation is added by onlooker bees by means of roulette wheel selection. That is, each onlooker bee probabilistically selects a better food source according to its quality to process. The selected food source is evolved by the onlooker bee to search for a better position, which works similarly to the employed bees. Occasionally, a new kind of artificial bees named scout bee is sent to explore the search space. When a food source is not improved after a certain number of trials (defined by a control parameter limit) by employed bees and onlooker bees, this food source is considered to have a poor position that will be abandoned, where a complete new food source will be randomly generated by a scout bee to replace it. ABC iteratively sends the three groups of artificial bees to search the solution space until meeting a terminal condition, i.e., the maximal number of generations maxGeneration.Originally, the number of employed bees and the number of onlooker bees are equal to the number of food sources, that is, the population size SN. The algorithmic description of ABC is presented in the following parts.ABC consists a population of food sources with size SN. For the numerical optimization problem, each food source consists of a D-dimensional parameter vector, which encodes the candidate solution, i.e.,Xi={xi1,xi2,…,xiD}, i=1, 2, …, SN.Like the other evolutionary algorithms, ABC generates an initial population of food sources randomly. To cover the search space as much as possible, the initial food sources are uniformly placed within the search space constrained by the predefined minimum and maximum parameter bounds i.e.,Xmin={xmin1,xmin2,…,xminD}andXmax={xmax1,xmax2,…,xmaxD}. For parameter j in food source i, the initial valuexijis generated by(1)xij=xminj+rand(0,1)×(xmaxj−xminj),where, i=1, 2, …, SN and j=1, 2, …, D. rand(0, 1) is a random value ranging in [0, 1].As mentioned above, the number of employed bees are equal to the population size, that is, SN. Each employed bee maintains a specific food source. For each food source i, its employed bee performs a neighboring search to generate a new vector Viby updating its vector Xi. Let Vi=Xi, the neighboring search is performed by modifying one parametervijof Viwhere j∈{1, 2, …, D} is a randomly selected index. The solution search equation is described as follows(2)vij=xij+ϕij×(xij−xkj).Here, k∈{1, 2, …, SN} is a randomly selected neighboring food source that guides the vector update of Vi, where k should be different from i.ϕijis a random value ranging in [−1, 1].After obtaining Vi, it is evaluated and compared with Xi. If the quality of Viis better than Xi, Viwill replace Xiin the population. Otherwise, Xiwill be remained in the population. In other words, a greedy selection is used between Viand Xi.After searching the space, the employed bees return to the hive and share the nectar information of their sources with onlooker bees by dancing. Onlooker bees apply roulette wheel selection to select the food sources. That is, each onlooker bee prefers a food source depending on the nectar information distributed by the employed bees. Therefore, the probability of selecting a food source should be calculated. The probability of selecting a food source i by an onlooker bee is denoted by pi, which is calculated by(3)pi=fiti∑j=1SNfitj,where fitidenotes the fitness value of food source i, which is a problem-specific value measuring the quality of the candidate solution. The probability piis proportional to the fitivalue, where food sources with higher fitness values will be assigned higher probability values.For the minimization problems of numerical optimization, fitiis calculated by(4)fiti=11+fiiffi≥0,1+|fi|otherwise.where fiis the objective function value of food source i.Based on the calculated probabilities, each onlooker bee selects a food source to further search its neighboring area. The searching procedure of onlooker bees is the same as that of employed bees. That is, if an onlooker bee selects a food source i, a new vector Viis produced by Eq. (2). Xiand Viare compared, where the better one survives in the population.It is possible that a food source can not be improved even that the employed bees and onlooker bees have visited it many times. In this case, it is considered that this food source is an abandoned food source that performs poorly in the evolution process, which should be eliminated from the population. To maintain necessary population diversity, once an abandoned food source is found, a scout bee is sent to randomly generate a completely new food source to replace the abandoned one.To perform this procedure, each food source i is given a parameter named triali, which counts the number of continuously failed trials that the employed bees and onlooker bees have performed on it but cannot improve its quality. Once trialireaches a predefined threshold limit, this food source i is considered as an abandoned food source, which will be replaced by a new food source, and its trialiwill be reset to 0. Originally, ABC only allows one scout bee sent in each generation.The basic framework of ABC is described in Algorithm 1. It is a fairly simple algorithm which only consists of three control parameters: the population size SN, the terminal condition maxGeneration and exploration parameter limit.Algorithm 1Basic framework of ABC.In neuroscience, it has been investigated that the real honeybees have indeed evolved an impressive associative learning ability for their foraging behavior and dancing communication. That, honeybees have evolved sophisticated learning and memorizing capacities of which are similar to those of intelligent species [26]. By training with a discrimination task, an employed bee is capable of learning preferences to guide its further foraging behavior, which can be transferred to the unemployed bees by dancing communication. However, this kind of memory ability is not considered in the foraging behavior of standard ABC as well as its variants. That, each employed bee or onlooker bee searches for a new food source by randomly placing a neighboring food source as shown in termxkjof Eq. (2). Every time, a neighboring food source k is randomly picked up, regardless of the learning and memorizing abilities of each bee.Therefore, in this paper, ABC with memory algorithm (ABCM) is proposed to imitate the memory ability of artificial bees in ABC. A memory mechanism is introduced, which allows the artificial bees to memorize their previous successful experiences of foraging behavior. The memorized experiences are constantly updated and used to guide the further foraging of the artificial bees, leading to a more efficient search procedure than standard ABC. The detail of ABCM is described in the following parts.Basically, each parameter of each food source is associated with a new memory component denoted byM,(5)M=M11M12…M1DM21M22…M2D⋮⋮⋱⋮MSN1MSN2…MSND,where,Mijdenotes the memory of parameter j of food source i (j=1, 2, …, D and i=1, 2, …, SN).Initially, each memoryMijis remained empty. During the search procedure of ABC,Mijis constantly updated according to the experiences of foraging behavior of employed bees and onlooker bees. Conceptually, if an employed bee or onlooker bee is sent to update parameterxijby search equation (2) and a better food source is found, the corresponding updating information will be considered as successful experience and saved into the memoryMij.Mijwill memorize the following two components of Eq. (2):•Neighboring food source k;Updating coefficientϕij.In other words, if the used values of k andϕijcan improve the quality of parameter j of food source i, we memorize them and treat them as the successful experience.The memory structure is illustrated in Fig. 1. In ABCM, a new control parameter M is introduced to define the memory size. For each memoryMij, the memorized information includes the neighboring food sourcekij(m)and updating coefficientϕij(m), m=1, 2, …, M.In standard ABC, the neighboring food source k and the updating coefficient ϕ are generated randomly to perform the search equation (2). On the other hand, ABCM directly incorporates the memorized values saved in the memory to determine the neighboring food source and the updating coefficient. That is, if parameter j of food source i is selected to update, we randomly select a paired k and ϕ from the memoryMijto perform the search equation (2). In other words, the artificial bees will follow their previously successful experiences to forage the nectar, which is expected to significantly improve the search efficiency comparing with the traditional random-based ones.In the initialization, all memoryMis set to empty. During the searching procedure by artificial bees, the neighboring food source and updating coefficient of successful trials, that is, a better food source can be generated, will be memorized in the corresponding memory. During the experience collection process, ABCM is performed equivalently to standard ABC of which k and ϕ are randomly determined, until M successful trials are recorded, i.e., the memory is full.Once the memoryMijis found full, in the next time of updating parameter j of food source i, the neighboring food source k and updating coefficientϕijare randomly selected from one of the M memorized records inMij. If the food source i can be improved, memoryMijis remained unchanged. However, if the food source i cannot be improved, the used k andϕijwill be erased fromMijsince they are considered as failed experience.Mijbecomes not full so that standard ABC is called to randomly determine new k andϕijto fillMij.The above process is constantly executed so that the latest successful experiences will always be memorized by the artificial bees to guide their further foraging behavior. The detailed description of ABCM is shown in Algorithm 2.Algorithm 2Basic framework of ABCM.10 benchmark functions which have been widely studied in the ABC literature [3,5,20] are employed to verify the performance of ABCM. The definition of each function is presented in Table 1, including its mathematical formula, parameter bound and optimal (minimum) solution.Basically, the features of a function can be grouped by the modality and separability [3]. The modality of a function is denoted by the number of ambiguous peaks in its landscape, which causes the possibility that the algorithm may be trapped in one of such peaks, resulting the local optimum. If a function has only one peak, it is called a unimodal function, otherwise, it is a multimodal function. The separability is a measure of difficulty of different test functions, which includes separable functions and nonseparable functions. Separable functions are generally easier to solve than nonseparable functions, because each parameter is independent of the others. If all the parameters are independent, the entire optimization process can be performed by optimizing each parameter independently. For nonseparable functions, there are interrelation among their parameters, which can not be optimized separately. Therefore, nonseparable functions are more difficult than separable functions. The 10 benchmark functions studied in this paper cover different types of problems discussed above, that is, unimodal-separable (US), unimodal-nonseparable (UN), multimodal-separable (MS) and multimodal-nonseparable (MN).The dimension D of each function is set to 30 except Schaffer function (f5), Six Hump Camel Back function (f9) and Branin function (f10) with D=2 due to their definitions. The above configuration is the same as the ones reported in [20]. Specifically, the success rates of the compared algorithms are reported. The success of an algorithm is defined by that the error of its best value and the optimal value is less than a predefined acceptance threshold. The success rate is calculated as the number of successful experiments divided by the total number of runs. For the 10 examined benchmark functions, their features, dimensions D and acceptance thresholds are listed in Table 2.ABCM is primarily compared with standard ABC to testify the effectiveness of the memory ability for ABC. For ABCM, we conducted several experiments with different values of the newly introduced parameter M (memory size) so that the effect of this parameter can be analyzed.The parameter settings are remained the same as those of [20] so that we can further compare ABCM with the state-of-the-art algorithms, such as genetic algorithm (GA), PSO, DE and a recently proposed quick ABC (qABC). The population size SN is set to 25, and the maximal number of generation maxGeneration is equal to 10,000 so that the maximal number of fitness evaluations (NFEs) is around 500,000. The exploration parameter limit is calculated by(6)limit=D×SN.For each function, 30 independent runs are carried out, where the final results are the average values over these independent runs.

@&#CONCLUSIONS@&#
A novel ABC called ABC with memory (ABCM) has been proposed in this paper. ABCM introduces a new memory ability to the artificial bees of standard ABC to memorize its previous experiences of successful trials, which guide the further foraging behavior of the employed bees and onlooker bees. Essentially, ABCM is inspired by the biological study of real honeybees which potentially provides a new direction to consider the improvement of ABC. It is conceptually different from most of the other ABC variants that focus on integrating existing algorithms into ABC framework.In this paper, ABCM is designed to be implemented as simply as possible to realize the memory ability of the artificial bees. Experimental studies show that the proposed memory mechanism can significantly improve the final objective function values of standard ABC with only slightly more computational time. Our investigation also shows that ABCM presents promising results for the studied benchmark functions comparing with the state-of-the-art algorithms. In addition, we verify that ABCM is not so sensitive to the setting of its newly introduced parameter: memory size M.In the future, we will focus on the improvement of this topic from various perspectives. Firstly, the memory mechanism presented in ABCM mainly aims at recording and reusing the neighboring food sources k and updating coefficient ϕ of the successful trials. In the future, the memory mechanism will be introduced to adapt the search equations of ABC. Another research direction is the adaptation of the control parameter M. ABCM will also be applied to the other problems, such as the scheduling problems and program generation.