@&#MAIN-TITLE@&#
Enhancing the effectiveness of Ant Colony Decision Tree algorithms by co-learning

@&#HIGHLIGHTS@&#
ACO techniques provide a way to efficiently search for solutions via colearning.ACO applied to data mining tasks is one of these methods and the focus of this paper..The ACDT approach generates solutions efficiently and effectively.The ACDT approach is tested in the context of the bi-criteria evaluation function.The empirical results clearly show that the ACDT algorithm creates good solutions.

@&#KEYPHRASES@&#
Ant Colony Optimization,Ant Colony Decision Trees,Decision trees,Pareto front,Quality of decision trees,Ant-Miner,

@&#ABSTRACT@&#
Data mining and visualization techniques for high-dimensional data provide helpful information to substantially augment decision-making. Optimization techniques provide a way to efficiently search for these solutions. ACO applied to data mining tasks – a decision tree construction – is one of these methods and the focus of this paper. The Ant Colony Decision Tree (ACDT) approach generates solutions efficiently and effectively but scales poorly to large problems. This article merges the methods that have been developed for better construction of decision trees by ants. The ACDT approach is tested in the context of the bi-criteria evaluation function by focusing on two problems: the size of the decision trees and the accuracy of classification obtained during ACDT performance. This approach is tested in co-learning mechanism, it means agents–ants can interact during the construction decision trees via pheromone values. This cooperation is a chance of getting better results. The proposed methodology of analysis of ACDT is tested in a number of well-known benchmark data sets from the UCI Machine Learning Repository. The empirical results clearly show that the ACDT algorithm creates good solutions which are located in the Pareto front. The software that implements the ACDT algorithm used to generate the results of this study can be downloaded freely from http://www.acdtalgorithm.com.

@&#INTRODUCTION@&#
Our goal is to propose new approach for constructing more effective decision trees, concerning classification accuracy and decision tree growth. This solution is possible due to application of Ant Colony Optimization. Using Ant Colony algorithms in decision tree construction allows to construct variety of alternative decision trees presenting different local optima. In case of deterministic algorithms, each decision tree is constructed in the same way. Due to the pheromone updating rules, representing reinforcement learning schema, agent–ants construct better quality decision trees.This collaborative view of learning can occur without interaction between the learning agents, known as ensemble learning, or with interaction during the learning stage, known as co-learning [1]. This mechanism is a good way to obtain better treatment results, and should always be taken into consideration in comprehensive point of view.Results obtained during experimental study motivated us to propose new optimization criteria for decision tress evaluation. It is important in case of bi-criterion decision tree evaluation, where as heuristic function, as well as quality function we create via decision tree growth and classification accuracy. In this article we also discuss the decision forest as a more effective classification ensemble.The additional aim of this article is to arrange information about ACDT algorithm as well as its performance the Ant Colony Decision Tree approach is firstly proposed by Boryczka and Kozak [2]. In the beginning this algorithms created binary decision trees for discrete attributes occurred in data sets. The comparative study with classical approach has shown that ACDT offered better results in context of accuracy of the classification. In the following papers Boryczka and Kozak [3] adjusted this approach to continuous values of attributes. They proposed the inequality test.Boryczka et al. [4] have also undertaken construction the heterarchical algorithm of the ACDT approach with parallel implementation. Authors also tested the performance of the ACDT in practical, real-life data sets: H-Bond Data Set [5]. Otero et al. [6] proposed a modification of the ACDT approach by using a new splitting criterion derived from C4.5 approach in contrary to our proposition originated from CART algorithm.This article is organized as follows: Section1 comprises an introduction to the subject matter of this article. In Sections2 and 3, the Swarm Intelligence and Ant Colony Optimization is introduced. Section4 reviews Ant Colony Optimization in Data Mining. Decision Trees and Ant Colony Decision Trees are presented in Sections5 and 6. Section7 focuses on the evaluation criteria of the constructed decision tree. Section8 presents the experimental study that was conducted to evaluate the performance of the ACDT algorithm. Finally, we conclude with general remarks on this work, and some directions for future research are pointed out.There has recently been a number of research studies regarding the application of Swarm Intelligence to various data-mining problems. Swarm Intelligence describes the ability of groups of decentralized and self-organizing agents to exhibit highly organized behaviours. These global behaviours of swarms often allow the swarm as a whole to accomplish tasks which are beyond the capabilities of any one of the constituent individuals. Following the appearance of the most recent books, (“Swarm Intelligence” [7] and “Swarm Intelligence: From natural to artificial systems” [8]), the area of Swarm Intelligence became a much discussed topic in the fields of computer science, collective intelligence, and robotics. Today, the number of successful applications of Swarm Intelligence continues to grow. This natural phenomenon is an inspiration for Swarm Intelligence systems – a class of algorithms that utilizes the emergent patterns of swarms to solve hard computational problems.Swarm Intelligence is interesting because it provides a basis with which it is possible to explore collective (or distributed) problem solving without centralized control or the provision of a global model. The term “Swarm Intelligence” was coined in 1989 by Gerardo Beni and Jing Wang in the context of cellular robotic systems [9]. Beni and Wang proposed that groups of simple virtual agents could be programmed to collaboratively solve difficult tasks and described how collectively intelligent behaviours can be exhibited in systems of non-intelligent agents. An artificial system organized in this way would present three distinct advantages:1.Agents are simple and they will work together to solve problems.The overall system is very reliable due to high levels of redundancy.Problems are solved in a distributed way since each agent handles a portion of the problem.These three features are present in nearly all Swarm Intelligence systems and are what makes Swarm Intelligence approaches to problem-solving attractive for certain problems. In addition to these general characteristics, nearly all Swarm Intelligence systems exhibit the following features:Homogeneity – every member of the swarm follows the same rules and decision making processes.Locality – the actions and decisions of individuals are made on the basis of purely local information and from what agents learn via (direct or indirect) communication with others.Randomness – swarm members introduce randomness into their decision-making processes in order to explore new solutions.Positive feedback – as with Darwinian evolution, “good” solutions that emerge from the actions of swarm individuals are identified as having good “fitness” and are reinforced over time.To sum up, Swarm Intelligence [7,8] is the property of a system whereby the collective behaviours of unsophisticated agents that interact locally with their environment cause coherent functional global patterns to emerge. The characteristics of a swarm are distributed, with no central control or data source, no (explicit) model of the environment, perception of the environment, limited time to act and strong emphasis on reaction and adaptation. Perhaps the most well-explored Swarm Intelligence algorithms are the Ant Colony Optimization (ACO) methods that are applied to different optimization problems.In Ant Colony Optimization (ACO) a stochastic optimization strategy inspired by the collaborative path-finding behaviours that are exhibited by colonies of real ants is observed. In nature, ants are simple organisms that each possess very limited capabilities and, individually, are only able to accomplish the most simple of tasks. Amazingly, colonies of ants are able to collectively solve difficult problems that are far beyond the abilities of any single member of the group (manifested by emergency in global behaviour). Real ant colonies are a type of distributed and self-organizing system where the complex “global” behaviours exhibited by the colony as a whole are coordinated by indirect communication between the ants. The term “stigmergy” should be incorporated here to explain these interactions [10]. Specifically, ants communicate with one another by depositing pheromone (a chemical substance produced by each ant) on the ground as they move about. As ants make their random explorations of the environment, they are more likely to follow these pheromone trails. The pheromone on a given trail will intensify as more ants follow it, and decrease in intensity over time by the process of evaporation when ants fail to travel on it. The process of pheromone trail laying/following in real ant colonies is mimicked by virtual ant–agents in ACO systems.Generally, in ACO a population of independent agent–ants moves through an environment (most commonly a graph) that represents the solution space of some target problem. This graph representation of the solution space is referred to as a “construction graph”. By generating a path through the construction graph for a target problem, each ant generates a candidate solution to that problem. By repeatedly generating solutions and reinforcing paths which represent good solutions with pheromone, the optimal solution (or path) will eventually be found. The movement of ants in the construction graph is dictated by a stochastic transition rule based on two pieces of local information: pheromone level and heuristic value. Edges in the construction graph have a level of pheromone associated with them, and each node has a certain heuristic value. The amount of pheromone on an edge is a measure of how many ants have recently traversed the edge and as attempts to attract ants to edges which have been identified as components of good solutions; thus, it can be treated as a learning mechanism. The heuristic value of a node is derived from a priori knowledge about the target problem and attempts to capture the relative importance of a node in candidate solutions.An agent–ant located on some vertex in the construction graph will select an edge to traverse with a probability given by a transition rule that takes into account the pheromone level on each connected edge and the heuristic value of the vertices to which the edges connect. The evaporation of a pheromone value is often compared to the learning factor in the Reinforcement Learning algorithm. The decision will favour an edge with a relatively high pheromone level and one that connects to a vertex with a relatively high heuristic value. It should be noted that the transition rule only gives the probability of selecting a given “next move”. This probability is used to weight a random selection process where the next move is actually selected. This causes random decisions to be introduced into the system.Algorithms inspired by ant behaviour were first proposed by Dorigo et al. [11] and Dorigo [12] as a multi-agent approach to difficult combinatorial optimization problems [13], such as that of the Travelling Salesman Problem (TSP), the Quadratic Assignment Problem (QAP) and Scheduling. There is currently much ongoing activity in the scientific community to extend/apply ant-based algorithms to many different discrete optimization problems as described by Dorigo et al. [13,14]. Recent applications cover problems like Vehicle Routing, Sequential Ordering, Graph Coloring, Routing in Communications Networks, and so on [15].An essential step made in this direction was the development of the Ant System by Dorigo and Stützle [15], a new type of heuristic inspired by analogies to the foraging behaviour of real ant colonies which has proven to work successfully in a series of experimental studies. Diverse modifications of AS have been applied to many different types of discrete optimization problems and have produced very satisfactory results [14]. Recently, the approach has been extended by Doerner et al. [16], Dorigo and Caro [17], Dorigo et al. [18,19] to full discrete optimization metaheuristics, called Ant Colony Optimization (ACO) metaheuristics.The Ant Colony System (ACS) algorithm was introduced by Dorigo and Gambardella [20], Dorigo and Stützle [15] to improve the performance of Ant System, which allowed to find good solutions within a reasonable period of time for small-sized problems only. The ACS is based on three modifications of the Ant System: a different node transition rule, a different pheromone trail updating rule, and the use of local and global pheromone updating rules (to favour exploration).The node transition rule is modified to explicitly allow for exploration. An ant k in analyzed node i chooses node j to move to following the rule:(1)j=arg maxu∈Jik{[τiu(t)]·[ηiu]β}ifq≤q0Jifq>q0,where q is a random variable uniformly distributed over [0, 1], q0 is a tunable parameter (0≤q0≤1), andJ∈Jikis the analyzed node that is chosen randomly according to a probability:(2)piJk(t)=τiJ(t)·[ηiJ]β∑l∈Jik[τil(t)]·[ηil]βwhich is similar to the transition probability used by the Ant System. Therefore, we can easily notice that the ACS transition rule is identical to that of the Ant System, when, and is different when q>q0, and is different when q≤q0. More precisely, q≤q0 corresponds to the exploitation of knowledge available about the problem, i.e. heuristic knowledge about distances between nodes and learned knowledge that is memorized in the form of pheromone trails, whereas q>q0 favors more exploration.In the Ant System all ants are allowed to deposit pheromone after completing their tours. By contrast, in the ACO only the ant that has generated the best tour since the beginning of the trail is allowed to globally update the concentrations of pheromone on the branches. The updating rule is:(3)Δij(t+n)=(1−γ)·τij(t)+γ·Δτij(t,t+n),where (i, j) is the edge belonging to T+, i.e. the best tour since the beginning of the trail, γ is a parameter governing pheromone decay, and:(4)Δτij(t,t+n)=1L+,where L+ is the length of the T+.The local update is performed as follows: when, while performing a tour, ant k is in analyzed node i and selects nodej∈Jikto move to, the pheromone concentration on edge (i, j) is updated by the following formula:(5)τij(t+1)=(1−ρ)·τij(t)+ρ·τ0.The value of τ0 is the same as the initial value of pheromone trails. It was experimentally found that setting τ0=(n·Lnn)−1, where n is the number of nodes and Lnnis the length of a tour produced by the nearest neighbour heuristic, produces good results, as described by Dorigo and Gambardella [21] and Gambardella and Dorigo [22].Data mining (sometimes called data or knowledge discovery) involves the use of sophisticated data analysis tools to discover previously unknown, valid patterns and relationships in large data sets. These tools can include statistical models, mathematical algorithms, and machine learning methods (algorithms that improve their performance automatically through experience, such as neural networks or decision trees). Consequently, data mining consists of more than collecting and managing data – it also includes analysis and prediction. Data mining can be performed on data represented in quantitative, textual or multimedia forms. Data mining applications can use a variety of parameters to examine the data. These include association, sequence or path analysis, classification, clustering and forecasting. While data mining products can be very powerful tools, they are not self-sufficient applications. To be successful, data mining requires skilled technical and analytical specialists who can structure the analysis and interpret the output that is created. Generally, in data mining tasks any of four types of relationships are sought:•Classes: stored data is used to locate data in predetermined groups.Clusters: data items are grouped according to logical relationships or user preferences.Associations: data can be mined to identify associations.Sequential patterns: data is mined to anticipate behaviour patterns and trends.Different levels of analysis are available:•Artificial neural networks, where non-linear predictive models that learn through training and resemble biological neural networks in structure are analyzed.Evolutionary algorithms, optimization techniques that use processes such as genetic combination, mutation, and natural selection in a design based on the concepts of natural evolution.Decision trees; in this area tree-shaped structures that represent sets of decisions are created. These decisions generate rules for the classification of a dataset. Specific decision tree methods include Classification and Regression Trees (CART) and Chi-Square Automatic Interaction Detection (CHAID) [23]. CART and CHAID are decision tree techniques used for the classification of a dataset. They provide a set of rules that can be applied to a new (unclassified) dataset to predict which records will have a given outcome. CART segments a dataset by creating two-way splits, while CHAID segments using chi square tests to create multi-way splits. CART typically requires less data preparation than CHAID.Nearest neighbor method – a technique that classifies each record in a dataset based on a combination of the classes of the k record(s) most similar to it in a historical dataset. It is sometimes called the k-nearest neighbor technique.Rule induction, where the extraction of useful if–then rules from data based on statistical significance is presented.Data visualization, where the visual interpretation of complex relationships in multidimensional data is found. Graphical tools are used to illustrate data relationships.Data Mining and Ants Colony Optimization are an active field of research. One of the first works on this subject focuses on data clustering [24], and the suggested method has been applied to data mining in the area of decision tree construction with good results [2,3].The Ant-Miner, as proposed by Parpinelli et al. [25], was the first ant algorithm for rule induction. It has been shown to be robust and comparable with CN2 [26] and C4.5 [27] algorithms for classification. The Ant-Miner generates solutions in the form of classification rules. The Ant-Miner is an ant-based system and is more flexible and robust than traditional approaches. This method incorporates a simple ant system in which a heuristic value based on the entropy measure is calculated. The Ant-Miner has produced good results when compared with more conventional data mining algorithms, such as C4.5, ID3 and CN2.An adaptation of Ant Colony Optimization for classification is a research area that has still not been well explored and examined. The appeal of this approach, similarly to the evolutionary techniques, is that they provide an effective mechanism of conducting a more global search. These approaches are based on a collection of attribute–value terms, so it can be expected that these approaches will also cope better with attribute interaction than with greedy induction algorithms [28].There are many other characteristics of ACO which are truly important in data mining applications. It is interesting to mention that ACO, contrary to deterministic decision trees or rule induction algorithms, during rule induction tries to extenuate the problem of premature convergence to local optima because of the stochastic element which prefers a global search in the problem's search space. Secondly, ACO metaheuristics is a population-based one. It permits the system to search in many independently determined points in the search space concurrently and to use the positive feedback between ants as a search mechanism [29].The application of Ant Colony Optimization algorithms in decision tree construction has been previously described by Boryczka and Kozak [2,3]. The same methodology has been used in approach presented by Otero et al. [6], thought the heuristic function has been changed.ACO is not the only technology what is used in the decision tree construction. As far as we know, evolutionary computation (EC) were also used in this area. Binary decision trees were constructed in EC algorithms. This achievement was published almost simultaneously in articles by Chai et al. [30] and Kennedy et al. [31]. In papers published by Fu et al. [32,33], the basic population was assembled by decision trees firstly constructed by C4.5 algorithm, and the genetic operators were performed. In the final state, because of overfitting, the process of pruning ought to be performed.The memetic algorithm, proposed by Kretowski [34] in decision tree induction is also work mentioning. This approach is an extension of the previous version of evolutionary algorithm for decision tree construction [35]. The author was focused on adjustment of initial population and an appropriate mutation operator performance.When the goal is to construct a smaller decision trees than those observed in genetic algorithm application, one can propose genetic programming (GP) algorithm. The hybrid approach of GP and association rules was presented by Niimi and Tazaki [36]. Whereas GP with Simulated Annealing was presented by Folino et al. [37], the SA approach was tested with success by Heath et al. [38]. Authors, basing on C 4.5 algorithms, obtained good results.Data mining, the science and technology of exploring data sets in order to discover previously unknown patterns, is a part of the overall process of knowledge discovery in databases. In data mining, a decision tree is a predictive model which can be used to represent both classifiers and regression models. When a decision tree is used for classification tasks, it is referred to as a classification tree; when it is used for regression tasks it is called a regression tree.A decision tree is used to determine the optimum course of action, in situations having several possible alternatives with uncertain outcomes. The resulting chart or diagram (which looks like a cluster of tree branches) displays the structure of a particular decision, and the interrelationships and interplay between different alternatives, decisions, and possible outcomes. Decision trees are commonly used in operational research, specifically in decision analysis, to identify an optimal strategy in order to reach a goal. The evaluation function for decision trees will be calculated according to the following formula:(6)Q(T)=ϕ·s(T)+ψ·a(T,S),where:s(T) – the size of the decision tree T,a(T, S) – the accuracy of the classification object from a test set S by the tree T,ϕ and ψ – constants determining the relative importance of s(T) and a(T, S).(7)s(T)=1n,where n is the number of nodes.(8)a(T,S)=∑c=1K(TPc)|S|,where:TPc– number of correctly classified objects in the class c|S| – number of all objects in the test setC – number of decision classes.Constructing optimal binary decision trees is a NP-complete problem, where an optimal tree is one which minimizes the expected number of tests required to identify the unknown objects as shown in Hyafil and Rivest [39].The problem of designing storage efficient decision trees from decision tables was examined by Murphy and McCraw [40]. They showed that for most cases, construction of the storage optimal decision tree is an NP-complete problem, and therefore a heuristic approach to the problem is necessary. Constructing an optimal decision tree may be defined as an optimization problem in which at each stage of creating decisions we select the optimal splitting of the data [41].In most general terms, the purpose of the analysis via tree-building algorithms is to determine a set of if–then logical (split) conditions that permit accurate prediction or classification of cases.Our inspiration in this data-mining field is the use of decision trees in which ants make the decisions concerning the creation of appropriate transitions based on heuristic function and pheromone trail.The Classification and Regression Tree (CART) approach was developed by Breiman et al. [23] and is characterized by the fact that it constructs binary trees, namely, that each internal node has exactly two outgoing edges. The splits are selected using the Twoing Criteria and Gini index. When provided, CART can consider misclassification costs in the tree induction. It also enables users to provide prior probability distribution. CART looks for splits that minimize the prediction squared error. The prediction in each leaf is based on the weighted mean for node.A decision tree is built in accordance with the splitting rule that performs the multiple splitting of the learning sample into smaller parts. Data in each node have to be divided into two parts, with maximum homogeneity in the decision class.The twoing criterion will search for two classes that will together make up more than 50% of the data. The twoing–splitting rule will maximize the following change-of-impurity measure, which implies the following maximization problem for nodes ml, mr:(9)arg maxaj≤ajR,j=1,…,MPlPr4∑c=1Cp(c|ml)−p(c|mr)2,where:p(c|ml) – the conditional probability of class c provided in node ml,Pl– the probability of object transition into the left node ml,Pr– the probability of object transition into the right node mr,C – number of decision classes,aj– jth variable,ajR– the best splitting value of variable aj.Although the twoing–splitting rule allows us to build more balanced trees, this algorithm works more slowly than the Gini rule; for example, if the total number of classes is equal to C, then we will have 2C−1 possible splits.There are several other methods besides the above-mentioned twoing–splitting rules. Among the most often used are the Gini index, Entropy rule, χ2 rule, and the maximum deviation rule. It has been proved that the final tree is insensitive to the choice of the splitting rule. Thus the pruning procedure is much more important than the presented rules.It is interesting to note that the ACDT algorithm is mainly based on the standard version of Ant Colony Optimization. A number of slight modifications has been introduced both as a new discrete optimization algorithm for constructing decision trees and as a new metaheuristics approach in data mining procedures. The presented modifications are introduced in the main transition rule and are treated as an improvement of the quality of the classification mechanism. So, we have employed a classical version of ACO and simple changes concerning the main rules, dedicated to each of the agent–ants during construction of the tours incorporated in the scheme. Firstly, we have applied the classical splitting rule as presented in CART by Breiman. Secondly, we comply with the pheromone changes, which are useful knowledge for creating a reasonable division.The decision tree can represent any hypothesis acceptable for a given set of attributes. This means that the decision tree is a representation of the hypothesis that the dataset X consisting of n-pieces(10)X={x1,x2,…,xn},described by m attributes(11)A={a1,a2,…,am},and belonging to one of C decision classes, each object with a set X can be described as:(12)xi=([vi1,…,vim],ci),vij∈Aj,ci∈{1,…,C},wherevij, the value of the attribute ajfor objects xi; ciis the decision class for the object xi.Each node of the decision tree contains a test on the attributes(13)t:X→Rt,where Rtis the set of possible tests for the node Rt={r1, r2, …, rz}, where riis the ith test, and then(14)t:A→Rt.As a result, ritest creates Tisubtrees, and so on each node for test r1, r2, …, rzbuilds T1, T2, …, Tzpossible subtrees. So the hypothesis represented by a node can be written ∀x∈X as follows(15)h(x)=h1(x),t(x)=r1h2(x),t(x)=r2⋮⋮hz(x),t(x)=rz.In contrast, the decision tree T(S) can be used for classification according to the following formula (D-distribution):(16)ϵ(T(S),D)=∑(x,y)∈UD(x,y)·L(y,T(S)(x)),where L(y, T(S)(x)) is the function:(17)L(y,T(S)(x))=0,ify=T(S)(x)1,ify≠T(S)(x),where:T(S) – the decision tree T constructed on the basis of the training set S,T(S)(x) – a decision for the object x determined by its conditional attributes, by the decision tree T constructed on the basis of the training set S,U – a set of possible values for each attribute.In ACDT each ant chooses an appropriate attribute for splitting in each node of the constructed decision tree according to the heuristic function and pheromone values (Fig. 1). The heuristic function is based on the Twoing criterion (see Eq. (9)), which helps ants divide objects into two groups that are connected with the analyzed attribute values. In this way the attribute that will separate the objects is treated as the best condition for the analyzed node. The best splitting is observed when we classify the same number of objects in the left and right subtrees with the maximum homogeneity in the decision classes. The pheromone values represent the best way (connection) from the superior to the subordinate nodes, i.e. all possible combinations in the analyzed subtrees. For each node we calculate the following values according to the objects classified using the Twoing criterion of the superior node.In case of the ACDT algorithm, the value of heuristic function for each attribute-value pair in the node m is as follows:(18)∀i∈A,j∈Viηi,j=PlPr4∑c=1Cp(c|ml)−p(c|mr)2,where Viis the set of values of the attribute i.A decision tree is built in accordance with a splitting rule that performs the multiple splitting of the learning sample into smaller parts.The value of the heuristic function is determined according to the splitting rule employed in the CART approach (see formula (9)), and depending on the chosen criterion; whereas the probability of choosing the appropriate test in the node is calculated according to a classical probability used in ACO:(19)pm,mL(i,j)(t)=τm,mL(i,j)(t)·ηi,jβ∑ia∑jbiτm,mL(i,j)(t)·ηi,jβ,where:m is meant as mp,o, in other words the superior node where the test of attribute l and value o is performed.ηi,j– a heuristic value for the test of attribute i and value j,τm,mL(i,j)– an amount of pheromone currently available at time t on the connection between nodes m andmL(this concerns attribute i and value j),β – the relative importance with experimentally determined values 3, respectively.Please note that in the ACDT algorithm the choice of the splitting rule in the appropriate node is performed according to the random-proportional rule (19), only that when q>q0. In accordance to (1), we create the following formula: (18).(20)r=arg maxmL∈Rmc{[τm,mL(t)]·[ηm,mL]β}ifq≤q0Jifq>q0where J is chosen randomly according to a probabilitypm,mL(i,j)(t)(Eq. (19)).It means that when q≤q0 the greedy approach is performed in consequence of the maximum value of the heuristic-pheromone product (see formula (18)).In the case of the ACDT algorithm, according to the heuristic function value calculation (using the formula (18)) there is no need for additional tabu list. In each node, the value of heuristic function is calculated on the basis of objects that can be found with in it. The attribute-value couple (which was already used to a higher level in the decision tree) will have a value ηi,j=0.0.The pseudo-code of the ACDT algorithm is presented below. Lines 2–20 describe one iteration of this algorithm. At the beginning of its work, each ant builds one decision tree (lines 4–18). At the end of the loop, the best decision tree is chosen, and then the pheromone is updated according to the splits performed during the process of construction the decision tree, iteratively. While constructing the tree the agent–ants analyze previous structures, and some modifications are performed in the single node. This process is performed until the best decision tree is obtained. The process of building the decision tree is presented in Fig. 2.Algorithm 1Pseudo-code of the ACDT algorithm1initialization_pheromone_trail(pheromone);2for number_of_iterations do3best_tree = NULL;4for number_of_ants do5//build the decision tree6new_tree = null;7while (stop_condition_is_not_fulfilled)8heuristic = calculate_the_heuristic_function();9p = calc_the_choosing_probability(pheromone, heuristic);10//choice the test in the node11new_tree→test = roulette_wheel(pm,mL(i,j)(t));12endWhile13pruning(new_tree);14assessment_of_the_quality_tree(new_tree);15if new_tree is_higher_quality_than best_tree then16best_tree = new_tree;17endIf18endFor19update_pheromone_trail(best_tree, pheromone);20endFor21result = best_constructed_tree;The next problem that is worth further consideration is pheromone growing. The initial value of the pheromone trail is established depending on the number of attribute values (see formula (21)).(21)τm,mL(t=0)=log2(C)∑att=1|A||aatt|,where:|A| – number of attributes,|aatt| – number of possible values of attribute aatt,C – total number of decision classes.The pheromone updates are performed (22) by increasing the previous values on each pair of nodes (parent–child). Edges in the decision tree have a value of pheromone associated with them. In the pheromone matrix are stored all of the possible combination nodes (edges) of the decision tree.(22)τm,mL(t+1)=(1−γ)·τm,mL(t)+Q(T),where Q(T) determines the evaluation function of the decision tree (see formula (6)), and γ is a parameter representing the evaporation rate, which is equal to 0.1.In order to compare the influence of the decision tree growth evaluation method (indirectly – the quality) constructed by agent–ants we show three proposals. All of the approaches differ from the classical version by the formula that calculates decision tree growth (Eq. (7)), whereas (6) does not undergo change.The proposed changes were supposed to increase the stability of the created decision trees. In this way the following solutions are presented:•decision tree growth is determined by multiplicative inverse of its height(23)s2(T)=1h,where h is the height of the tree.decision tree growth is determined by multiplicative inverse of the sum of the height and number of nodes (n)(24)s3(T)=1h+n,decision tree growth is determined by multiplicative inverse of the product of the height and number of nodes(25)s4(T)=1h·n.Obviously, in each of these approaches, in Eq. (6) instead of s(T) we use s2(T), s3(T), s4(T), suitably.

@&#CONCLUSIONS@&#
The quality evaluation of decision trees constructed by agent–ants is a significant aspect of ACDT assessment. This problem seems to be a bi-criteria optimization problem. The method of evaluation as well as parameter ψ changes are the most important aspects in this approach.The problem of tuning parameter ϕ values constitutes a decisive role in this analysis. The experiment results confirm that this algorithm is suitable for a variety of applications. The ϕ parameter gives the possibility of calibration scalability of the presented approach. The decision tree growth evaluation method is also a crucial point. In the future we will propose a new optimization function considering decision trees. We want to make this process automatically, without user inferences.It is also important to study different evaluations of the quality of classification. We also plan to study the same issue in ensemble methods.