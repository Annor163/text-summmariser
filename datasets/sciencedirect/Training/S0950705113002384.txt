@&#MAIN-TITLE@&#
A fast algorithm for kernel 1-norm support vector machines

@&#HIGHLIGHTS@&#
This paper proposes a Column Generation Newton (CGN) algorithm for finding solution of the kernel 1-norm SVM.CGN is combining the Column Generation and the Newton Linear Programming SVM method.CGN is fast when solving the kernel 1-norm SVM.

@&#KEYPHRASES@&#
1-Norm SVM,Linear programming,Column generation,Newton algorithm,Kernel function,

@&#ABSTRACT@&#
This paper presents a fast algorithm called Column Generation Newton (CGN) for kernel 1-norm support vector machines (SVMs). CGN combines the Column Generation (CG) algorithm and the Newton Linear Programming SVM (NLPSVM) method. NLPSVM was proposed for solving 1-norm SVM, and CG is frequently used in large-scale integer and linear programming algorithms. In each iteration of the kernel 1-norm SVM, NLPSVM has a time complexity of O(ℓ3), where ℓ is the sample number, and CG has a time complexity between O(ℓ3) and O(n′3), where n′ is the number of columns of the coefficient matrix in the subproblem. CGN uses CG to generate a sequence of subproblems containing only active constraints and then NLPSVM to solve each subproblem. Since the subproblem in each iteration only consists of n′ unbound constraints, CGN thus has a time complexity of O(n′3), which is smaller than that of NLPSVM and CG. Also, CGN is faster than CG when the solution to 1-norm SVM is sparse. A theorem is given to show a finite step convergence of CGN. Experimental results on the Ringnorm and UCI data sets demonstrate the efficiency of CGN to solve the kernel 1-norm SVM.

@&#INTRODUCTION@&#
In recent years, 1-norm Support Vector Machine (SVM) has attracted substantial attentions for its good sparsity [1–4]. 1-Norm SVM uses 1-norm regularization to replace 2-norm regularization in the standard SVM and approaches a more sparse model representation [5–9]. In fact, the optimization problem of 1-norm SVM is a linear program. Many methods have been proposed for solving 1-norm SVM. But, none of them is efficient enough for solving the kernel 1-norm SVM.Since 1-norm SVM can be posed as a linear program, it can be solved by general mathematical programming methods, such as the simplex method and the interior-point method. However, these optimization methods are usually time consuming for problems in which the sample dimensionality is much less than the number of training samples.Some specific algorithms have proposed for solving the linear program of 1-norm SVM. Fung et al. [10] presented the Newton Linear Programming SVM (NLPSVM) method to solve 1-norm SVM by minimizing the exterior penalty function for the dual problem of 1-norm SVM. In NLPSVM, each iteration has a time complexity of O(min(m,n)3) when applying the Sherman–Morrison–Woodbury identity equation [10], where m and n are the number of rows and columns of the sample matrix or the kernel gram matrix, respectively. It is quite efficient when NLPSVM is used to solve the linear 1-norm SVM. But in the case of using kernel functions, m could be equal to n, which implies that NLPSVM is still time consuming when solving the kernel 1-norm SVM. Demiriz et al. [11] introduced CG to the linear programming boosting, which can also be applied to solve 1-norm SVM by a simple generalization. In CG, the time complexity of each iteration is usually between O(m3) and O(n3) when using the simplex method [12] to directly solve the subproblem, where m and n are the number of rows and columns of the active constraint coefficient matrix in the subproblem for each iteration, respectively. More specifically, for 1-norm SVM, the complexity of the last iteration of CG is cubic of the SV number. In the standard SVM, the idea of CG is widely applied to speed up the optimization of quadratic programming [13–16].In order to obtain a fast algorithm for the kernel 1-norm SVM, we propose a Column Generation Newton (CGN) method. This method is a combination of CG and NLPSVM. For the kernel 1-norm SVM, the coefficient matrix is a matrix with ℓ rows and ℓ columns, where ℓ is the sample number. Thus, NLPSVM would have a time complexity of O(ℓ3). Let n′ be the number of active constraints. Then CG thus has a time complexity between O(ℓ3) and O(n′3).In CGN, CG is used to construct a sequence of subproblems containing only active constraints, and NLPSVM is exploited to solve each subproblem. This means that CGN has a time complexity of O(n′3) in each intermediate iteration and a time complexity which is cubic of the number of ESVs in the last iteration. Since the complexity of CGN is much smaller than that of NLPSVM, it is expected that CGN is faster than NLPSVM when applied to the kernel case. Furthermore, CGN is faster than CG when the number of ESVs is smaller than that of SVs. As pointed out in [9], SVs is usually a small part of training samples and ESVs is only a small subset of SVs.The rest of the paper is organized as follows. Section 2 gives a brief review of 1-norm SVM, the NLPSVM method, and the CG method. Section 3 presents the CGN algorithm for 1-norm SVM and some schemes to speed up CGN. Experimental results on the Ringnorm and UCI data sets are presented in Section 4. Section 5 concludes the paper.

@&#CONCLUSIONS@&#
