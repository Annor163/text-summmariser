@&#MAIN-TITLE@&#
A rough set method for the minimum vertex cover problem of graphs

@&#HIGHLIGHTS@&#
We study the NP-hard problem of finding a minimum vertex cover of graphs based on rough sets.The problem of finding a minimum vertex cover of graphs can be translated into the problem of finding an optimal reduct of a decision information table in rough sets.A new method based on rough sets is proposed to compute the minimum vertex cover of a given graph.

@&#KEYPHRASES@&#
Attribute reduction,Decision information tables,Minimum vertex covers,Rough set theory,

@&#ABSTRACT@&#
The minimum vertex cover problem is a classical combinatorial optimization problem. This paper studies this problem based on rough sets. We show that finding the minimal vertex cover of a graph can be translated into finding the attribute reduction of a decision information table. At the same time, finding a minimum vertex cover of graphs is equivalent to finding an optimal reduct of a decision information table. As an application of the theoretical framework, a new algorithm for the minimum vertex cover problem based on rough sets is constructed. Experiments show that the proposed algorithm gets better performance in terms of the ratio value when compared with some other algorithms.

@&#INTRODUCTION@&#
Graph theory is a useful tool for data analysis and knowledge representation in computer science. The minimum vertex cover problem (MVCP) is a classical graph optimization problem, which is to find a minimal vertex cover with the least number of vertices [6]. Except the application in graph theory, MVCP also has been used in a wide variety of real-world applications, such as crew scheduling [42], VLSI design [3,23], nurse rostering [7] and industrial machine assignments [55].As shown in [5,15,32], the minimum vertex cover computation can be translated into the calculation of prime implicants of a Boolean function. Although one can generate all the minimal vertex covers (or a minimum vertex cover) of a graph by using the Boolean operation, it is a well known NP-hard optimization problem [15,27]. There are a number of approximation algorithms that have been proposed for this problem in the literature [1,2,14,18,20–22,35]. In [19], Gomes et al. conducted a comparative study of three approximation algorithms for MVCP via some numerical experiments. The results showed that the Greedy algorithm was faster than both the Round and Dual-LP algorithms, and it also had a superior performance on the ratio value. Avis and Imamura proposed a simple and effective approximation algorithm called the list heuristics for MVCP [1]. In recently, a competent algorithm called Vertex Support Algorithm (VSA) was proposed for efficiently solving MVCP [2].Rough set theory is another effective tool for data analysis and knowledge discovery. The notion of attribute reduction plays an important role in the theory of rough sets. An attribute reduct is a minimal subset of attributes that provides the same classification ability as the whole set of attributes [36,37]. So far, it has been widely used in pattern recognition [25,26,45], knowledge discovery [65] and machine learning [31].Many approaches have been proposed for the attribute reduction [9,10,30,33,56,57,59]. A beautiful theoretical result is based on the notion of a discernibility matrix. Skowron and Rauszer [43] showed that the set of all reducts is in fact the set of prime implicants of the discernibility function. However, as was shown by Wong [54], finding the set of all attribute reducts or an optimal reduct (a reduct with the minimum number of attributes), is an NP-hard problem. Various heuristic methods for the attribute reduction such as positive-region methods [24,39], information entropy methods [29,38,44,51,58,63] and discernibility matrix methods [8,40,49,50] have been developed.As we have discussed above, attribute redacts and minimal vertex covers can be obtained via the Boolean logical operation. It seems that there is some kind of natural connection between the two problems. The purpose of this paper is mainly to study MVCP based on rough sets. In fact, Wang et al. [52] studied MVCP from a viewpoint of covering-based rough sets. However, they did not propose any efficient algorithm for MVCP. Kulaga et al. investigated the attribute reduction of a consistent decision table based on graph theory [28]. The framework proposed in this paper is quite different from that of [28,52]. We show that the problem of finding the minimal vertex cover of a graph is equivalent to the problem of finding the attribute reduction of a decision information table. Also, finding a minimum vertex cover of graphs is equivalent to finding an optimal reduct of its decision information table. What's more, we present an effective algorithm based on rough sets for MVCP. This study may open new research directions and provide new methods for MVCP.The remainder of this paper is organized as follows. In Section 2, some basic notions related to rough sets and graph theory are introduced. In Section 3, a new decision information table induced from a given graph is constructed, and the relationship between the attribute reduction of the derivative decision information table and the minimal vertex cover of the graph is studied. In Section 4, a new approximate algorithm for MVCP based on rough sets is presented. Some numerical experiments are given to show the effectiveness of the proposed method in Section 5. Finally, some conclusions are drawn in Section 6.In this section, we briefly introduce some basic notions and results about rough sets and graph theory [6,37,46].A graph is a pair G=(V, E) consisting of a set V of vertices and a set E of edges such that E⊆V×V. Two vertices are adjacent if there is an edge joining them, and the vertices are then incident with such an edge. Two or more edges that link the same pair of vertices are said to be parallel edges. An isolated vertex is a vertex not adjacent to any other vertex. A loop is an edge with the same ends. The edge of a graph may be directed (asymmetric) or undirected (symmetric). An undirected graph is one in which the edges are symmetric.A vertex cover of a graph G is a subset K⊆V such that every edge of G has at least one end in K. A vertex cover is minimal if none of its proper subsets is itself a vertex cover. A minimum vertex cover is a vertex cover with the least number of vertices. Note that a minimum vertex cover is always minimal but not necessarily vice versa. Observe that a minimal vertex cover is not necessarily unique, it is also true for the minimum vertex cover. All the minimal vertex covers of a graph can be obtained via Boolean formulaes.Given a graph G=(V, E) and e∈E, let N(e) denote a set of vertices connected by the edge e. DenoteN={N(e)|e∈E}. Now we define a function fGfor G as follows, which is a Boolean function of m Boolean variablesv1*,v2*,⋯,vm*corresponding to the verticesv1,v2,⋯,vm, respectively.fG(v1*,v2*,⋯,vm*)=∧{∨N(e)|N(e)∈N},where ∨N(e) is the disjunction of all variablesv*such thatv∈N(e).The following lemma gives a method for computing the minimal vertex covers of a given graph.Lemma 1([15,32]).LetG=(V, E)be a graph. A vertex subsetK⊆Vis a minimal vertex cover of G iff⋀vi∈Kvi*is a prime implicant of the Boolean functionfG.Lemma 1 shows that iffG(v1*,v2*,⋯,vm*)=∧{∨N(e)|N(e)∈N}=⋁i=1t(⋀j=1sivj*),where⋀j=1sivj*,i≤t, are all the prime implicants of the Boolean function fG, thenKi={vj|j≤si},i≤t, are all the minimal vertex covers of G. The set of all minimal vertex covers of a graph G is denoted byC(G). We will also writeviinstead ofvi*in the discussion to follow.Example 1Let G=(V, E) be the following graph withV={v1,v2,v3,v4}and E={e1, e2, e3, e4, e5, e6} (Fig. 1).We have the Boolean function:fG(v1,v2,v3,v4)=(v1∨v2)∧(v2∨v3)∧(v1∨v3)∧(v1∨v4)∧(v1∨v4)∧v3.After simplification, we have fGin prime implicants as:fG(v1,v2,v3,v4)=(v1∧v3)∨(v2∧v3∧v4).Hence G has two minimal vertex covers:K1={v1,v3}andK2={v2,v3,v4}. K1 is the unique minimum vertex cover.An information table can be seen as a pair S=(U, A), where U and A, are finite, non-empty sets called the universe (a set of objects) and the set of attributes, respectively. With each attribute a∈A, we define an information function a:U⟶Va, where Vais the set of values of a, called the domain of a.Each non-empty subset B⊆A determines an indiscernibility relation:RB={(x,y)∈U×U|a(x)=a(y),∀a∈B}.Obviously, RBis an equivalence relation on U, it forms a partition U/B={[x]B|x∈U}, where [x]Bdenotes the equivalence class containing x w.r.t. B, i.e., [x]B={y∈U|(x, y)∈RB}.Let B⊆A and X⊆U, the two setsB_X={x∈U|[x]B⊆X},B¯X={x∈U|[x]B∩X≠∅},are called the lower and the upper approximation of X w.r.t. B, respectively. The lower approximationB_Xis also called the positive region of X.A decision table is a special information table with the form S=(U, A∪{d}), where (U, A) is an information table and d∉A. Usually, A is called the conditional attribute set and d is the decision attribute. Suppose U/d={D1, D2, ⋯, Dr} are the equivalence classes induced by d. The positive region of d w.r.t. B, denoted by POSB(d), is defined asPOSB(d)=⋃i=1rB_Di.Given a decision table S=(U, A∪{d}), an attribute subset B⊆A is a reduct (also called a relative reduct) of S if B is a minimal set such that POSB(d)=POSA(d). Various approaches to attribute reduction have been proposed in the literature. For our purpose, we introduce the following method based on the discernibility matrix and logical operation [43]. By the discernibility matrix method, one can get all the reducts of a decision table.Let S=(U, A∪{d}) be a decision table with n objects and (x, y)∈U×U. We defineM(x,y)={a∈A|a(x)≠a(y)},(x,y)∈DIS,∅,otherwise,where DIS is the set consisting of (x, y)∈U×U satisfying one of the following conditions: (1) x∈POSA(d) and y∉POSA(d); (2) x∉POSA(d) and y∈POSA(d); (3) x, y∈POSA(d) and d(x)≠d(y). M(x, y) is referred to as the discernibility attribute set of x and y in S, andM={M(x,y)|(x,y)∈U×U}is called the discernibility set of S. Note that the discernibility set can also be stored in a matrix form, which is a symmetric n×n matrix with the entry M(x, y).Note that if the decision table is consistent, i.e., POSA(d)=U, then M(x, y) can be rewritten as:M(x,y)=a∈A|a(x)≠a(y)},d(x)≠d(y),∅,otherwise.A discernibility function fSfor a decision table S is a Boolean function of m Boolean variablesa1*,a2*,⋯,am*corresponding to the attributes a1, a2, ⋯, am, respectively, and it is defined as:fS(a1*,a2*,⋯,am*)=∧{∨M|M∈M,M≠∅},where ∨M is the disjunction of all variables a* such that a∈M.By means of the operators of disjunction and conjunction, Skowron et al. showed that the attribute reduction computation can be translated into the calculation of prime implicants of a Boolean function [43].Lemma 2[43].LetS=(U, A∪{d})be a decision table. A subsetB⊆Ais a reduct of S iff⋀ai∈Bai*is a prime implicant of the discernibility functionfS.From Lemma 2, we can see that iffS(a1*,a2*,⋯,am*)=∧{∨M|M∈M,M≠∅}=⋁i=1t⋀j=1liaj*,where⋀j=1liaj*,i≤t, are all the prime implicants of the discernibility function fS, then Bi={aj|j≤li}, i≤t, are all the reducts of IS. Without any confusion, we will write aiinstead ofai*in the next.Example 2Let S=(U, A∪{d}) be a decision table as shown in Table 1. The discernibility matrix of S can be represented as Table 2. For simplicity, we use a separator-free form for sets, e.g., a1a2a3 stands for {a1, a2, a3}.By Table 2, the discernibility function of S is:fS(a1,a2,a3,a4)=(a1∨a2)∧(a2∨a3)∧(a1∨a3)∧(a1∨a4)∧a3=(a1∧a3)∨(a2∧a3∧a4).Thus, there are two reducts of S: B1={a1, a3} and B2={a2, a3, a4}.From Lemmas 1 and 2, one can see that there may exist some relationships between the attribute reduction in rough sets and the minimal vertex cover of a graph. In this section, we first introduce a decision table induced from a graph and then discuss the relationship between the attribute reduction of the derivative decision table and the minimal vertex cover of the given graph.We first introduce a simple representation of a graph called the incidence matrix. Given a graph G=(V, E) withV={v1,v2,⋯,vn}and E={e1, e2, ⋯, em}. The incidence matrix of G is the m×n matrixMG=(mij)m×n, where mij=1 if the edge eiand the vertexvjare incident, and 0 otherwise. For example, the incidence matrix of the graph shown in Example 1 is a matrix consisting of 6 rows (corresponding to the six edges, e1−e6) and 4 columns (corresponding to the four vertices,v1−v4) (Table 3):Next we always assume that the graphs of discourse are finite, undirected and without isolated vertices.Definition 1Let G=(V, E) be a graph withV={v1,v2,⋯,vn}and E={e1, e2, ⋯, em}, andMG=(mij)m×nbe the incidence matrix of G. Denote U={e1, e2, ⋯, em, em+1} and A=V. We call the pair S=(U, A∪{d}) an induced decision table from the graph G (IDS for short), where the information functions of IDS are defined as follows:•vi(ej)=mij, d(ej)=1, 1≤i≤n, 1≤j≤m;vi(em+1)=0,1≤i≤n, d(em+1)=2.From Definition 1, we can see that the decision attribute d of the IDS classifies the samples into two groups: {e1, e2, ⋯, em} and {em+1}.Example 3In Example 1, the IDS of G is shown in the following table:Proposition 1LetS=(U, A∪{d})be the IDS of a given graphG=(V, E). Then S is consistent.ProofBy the definition, we havePOSA(d)=⋃i=1rA_Di=⋃i=12A_Di. Denote D1={e1, e2, ⋯, em} and D2={em+1}. Recall that the graph G has no isolated vertices. This implies that for any ei∈U, there existsv∈Asuch thatv(ei)≠0. Note that for anyv∈A,v(em+1)=0. Thus, we can conclude thatem+1∉[ei]A(1≤i≤m). In fact,A_D1=D1andA_D2=D2hold. Thus, POSA(d)=U. So, S is a consistent decision table.By Red(S) we denote the set of all reducts of a decision table S. Now we give the main theorem of this section.Theorem 1LetS=(U, A∪{d})be the IDS of a given graphG=(V, E). ThenC(G)=Red(S).ProofBy Lemmas 1 and 2, to prove the result we only need to show thatfS(v1,v2,⋯,vn)=fG(v1,v2,⋯,vn). LetMbe the discernibility set of IDS andN={N(e)|e∈E}. DenoteM*={M∈M|M≠∅}. In the following, we will show thatN=M*. In fact, by Proposition 1, we know that S is a consistent decision table. Recall the definition of the discernibility set of S. For anyM∈M*, we haveM={v∈A|[v(ei)≠v(ej)]∧[d(ei)≠d(ej)]}={v∈A|v(ei)≠v(em+1)}={v∈A|v(ei)≠0}=N(ei).Thus,N=M*holds. By the definitions offS(v1,v2,⋯,vn)andfG(v1,v2,⋯,vn), it is easy to see thatfS(v1,v2,⋯,vn)=fG(v1,v2,⋯,vn). HenceC(G)=Red(S)holds.Theorem 1 shows that the set of minimal vertex covers of a given graph is the same as the set of reducts of its IDS. Thus, the problem of finding the minimal vertex cover of a graph can be translated into the problem of finding the reduction of a decision table. Also, finding a minimum vertex cover of a graph is equivalent to finding an optimal reduct of its IDS. This may provide us with new approaches based on rough sets to study MVCP.Example 4In Example 3, from Table 4, the discernibility matrix of the IDS can be represented as Table 5. In Table 5, we use a separator-free form for sets, e.g.,v1v2stands for{v1,v2}.By Table 5, we obtain:fS(v1,v2,v3,v4)=(v1∨v2)∧(v2∨v3)∧(v1∨v3)∧(v1∨v4)∧v3=(v1∧v3)∨(v2∧v3∧v4).According to Example 1, we havefS(v1,v2,v3,v4)=fG(v1,v2,v3,v4). HenceC(G)=Red(S).The intersection of all reducts of a decision table is the so-called core. The core attribute plays an important role in decision making. The next proposition gives a characterization of the core attribute of an IDS.Proposition 2LetS=(U, A∪{d})be the IDS of a given graphG=(V, E). Thenv∈Ais a core attribute of S iff v is a vertex with loops in G.Proof“⇒” Supposev∈Ais a core attribute of S. Let us assume thatvis a vertex of G without loops. Note that the core attribute is included in every reduct, thus by Theorem 1, we know thatvis included in each minimal vertex cover of G. Sincevhas no loops, it is easy to see thatV−{v}is a vertex cover of G. Hence there is a minimal vertex cover K such thatK⊆V−{v}. This impliesv∉K, which is a contradiction to thatvis included in each minimal vertex cover of G. Thus,vhas no loops in G.“⇐” Assumevis a vertex with loops in G. Ifvis not a core attribute of S, then by Theorem 1 and the definition, there exists a minimal vertex cover K⊆V such thatv∉K. Sincev∉Kandvhas a loop, so the ends of the loop are not contained in K, which is a contradiction to that K is a minimal vertex cover. Thus,vis a core attribute of S.□From Theorem 1 and Proposition 2, for a given graph G,v∈∩C(G)iffvis a vertex with loop in G, and we call it the core vertex of G. LetMG=(mij)m×nbe the incidence matrix of a graph G withV={v1,v2,⋯,vn}and E={e1, e2, ⋯, em}. It is easy to see thatvj0∈Vis a core vertex of G iff there existsei0∈Esuch that∑j=1nmi0j=1andmi0j0=1. Thus, we can easily obtain all the core vertices of a graph.Example 5In Example 1, it can easily be verified thatv3is the unique core vertex of G.In the next, we give a new method to obtain the positive region of an IDS.Proposition 3LetS=(U, A∪{d})be the IDS of a graphG=(V, E)withV={v1,v2,⋯,vn}andE={e1, e2, ⋯, em}. For anyei∈U(1≤i≤m)andB⊆A, if∑v∈Bv(ei)=0,thenei∉POSB(d).ProofRecall the definition of the IDS of a given graph, if∑v∈Bv(ei)=0holds, then we can conclude thatv(ei)=0for allv∈B. Thus, we haveei∈[em+1]B. Note that d(ei)≠d(em+1), this implies that ei∉POSB(d).The results above concentrate on the study of MVCP from a viewpoint of the attribute reduction of rough sets. However, by Lemmas 1 and 2, we can investigate the attribute reduction based on graph theory.Definition 2Let S=(U, A∪{d}) be a decision table with A={a1, a2, ⋯, am}.Mis the discernibility set as defined in Section 2.2 andM*={M∈M:(M≠∅)∧(M≠A)}. Denote V=A andE=M*. We call the pairG=(V,E)an induced graph from S.The construction of graphs in Definition 2 is similar to the one used in [28], but has the following differences. The first one is that the derivative graph is induced from an inconsistent decision table. The second one is that we have removed some redundant edges that will not play any role in solving the minimal vertex cover of graphs.Theorem 2LetG=(V,E)be an induced graph from a decision tableS=(U, A∪{d}).ThenRed(S)=C(G).ProofIt follows immediately from Lemmas 1 and 2, and Definition 2.From Theorem 2, we can see that finding the set of all reducts of a decision table can be viewed as finding the set of all minimal vertex covers of a graph. Thus, it provides us with new methods to obtain the attribute reduction of a decision table based on graph theory. We will leave this problem for a future study.In this section, we give a new approximation algorithm for MVCP based on the positive approximation.In [24], Hu and Cercone proposed a heuristic attribute reduction method, called the positive-region reduction, to search reducts in rough sets. Qian et al. [39] improved this method by using an accelerator (FSPA). The modified method can choose the same attribute subset as that of the original method while greatly reducing computing time. Furthermore, the FSPA method is the best one when compared to the other information entropy methods in terms of the running time and the reduct size [39]. Thus, here we will review this method introduced in [39]. Given a decision table S=(U, A∪{d}), for any B⊆A and a∈A−B. The significance measure of a in B is defined as:Sig(a,B,d)=γB∪{a}(d)−γB(d),whereγB(d)=|POSB(d)||U|, and |·| denotes the cardinality of a set.The following algorithm [39] is an accelerating algorithm that can largely reduce the computational time while producing the same reduct as its original version in [24].Algorithm 1([39]) A general improved feature selection algorithm based on the positive approximation (FSPA)Input: A decision table s=(U, A∪{d})Output: One reduct red1. red←∅;2. Compute the core attributes of s and put them into red, i.e., red={a∈A|γA(d)−γA−{a}(d)>0};3. i←1, R1=red, P1={R1}, U1←U;4. whilePOSC(d)≠POSA(d) do5.   Compute the positive region of positive approximationPOSPiU(d);6.Ui=U−POSPiU(d),i←i+1;7.   red←red∪{a0}, where a0 satisfies Sig(a0, C, d)=max{Sig(ak, C, d)|ak∈A−C};8. Ri←Ri∪{a0}, Pi←{R1, R2, …, Ri}.9. end while10. Output red.From Algorithm 1, we can see that computing the positive region of a decision table is one of the key steps for constructing an attribute reduction algorithm. Note that the IDS is a special and simple case of decision tables. By Proposition 3, we have the following algorithm for computing the positive region of an IDS.Algorithm 2An algorithm for computing the positive region of an IDSInput: An IDS s=(U, A∪{d}) and B⊆AOutput: The positive region POSB(d)Let U={e1, e2, ⋯, em, em+1} and E={e1, e2, ⋯, em}.1. for eachei∈Edo2.   if∑v∈Bv(ei)=03.     T←ei;4.   end if5. end for6. ifT≠∅ then7.   POSB(d)←E−T;8. else9.   POSB(d)←U;10. end ifIt is easy to see that the time complexity of Algorithm 2 is O(|U|). From Algorithm 1, we can see that computing the significance measure of an attribute is another main step for an attribute reduction algorithm. The following proposition gives a simple approach to compute it.Proposition 4LetG=(V, E)be a graph withV={v1,v2,⋯,vn}andE={e1, e2, ⋯, em}. S=(U, A∪{d})is the IDS of G. For anyB⊆AandD=A−B,then(2)(1)[em+1]B=U−POSB(d);max{Sig(a,B,d)|a∈D}=max{∑e∈[em+1]Ba(e)|a∈D}.Proof(1) By the proof of Proposition 1, we know thatPOSB(d)=B_D1∪B_D2, where D1={e1, e2, ⋯, em} and D2={em+1}. Thus we can conclude that[em+1]B=U−POSB(d).(2) By the definition and (1), we havemax{Sig(a,B,d)|a∈D}=max{|POSB∪{a}(d)||a∈D}=max{|U−[em+1]B∪{a}||a∈D}=min{|[em+1]B∪{a}||a∈D}=min{|[em+1]B∩[em+1]a||a∈D}=min{|{e∈[em+1]B|a(e)=0}||a∈D}=max{|{e∈[em+1]B|a(e)=1}||a∈D}=max{∑e∈[em+1]Ba(e)|a∈D}.By Algorithms 1 and 2, and Proposition 4, now we can design an algorithm for MVCP based on rough sets.Algorithm 3(VCAR) An accelerating algorithm for the MVCP based on the positive regionInput: A graph G=(V, E)Output: A vertex cover of G1: Compute the core vertices of G, denoted by C;2: Generate the IDS S=(U, A∪{d}) of the graph G;3: i=1;4: whilePOSC(d)≠POSA(d)5:  Ui←U−POSC(d);6:  C←C∪{a0}, where a0 satisfiesSig(a0,C,d)=max{∑e∈Uia(e)|a∈A−C};7:  i=i+1;8: end while9: Output a vertex cover C.Similar to the time complexity analysis of Algorithm 1[39], the time complexity of Algorithm 3 at the worst case is O(|E||V|+∑i=1|V||Ui||V|). The time complexity of Algorithm 3 is larger than that of algorithms in [1,18,19], whose time complexities are both O(|E||V|). However, the new proposed algorithm may find an optimal solution. This can be illustrated by the following example.Example 6In Example 1, it can easily be verified thatv3is the unique core vertex of G. Thus, in the first step of Algorithm 3, we haveC={v3}. We then can obtain that POSC(d)={e3}, POSA(d)={e7} and U1=U−POSC(d)={e1, e2, e4, e5, e6, e7}. The significance measures ofv1,v2andv4are 1/2, 1/3 and 1/6. We put the vertexv1with the maximum significance measure into C. However, note thatC={v1,v3}and POSC(d)=POSA(d). Thus, we obtain a vertex coverC={v1,v3}, which is the minimum vertex cover of G. But the algorithm in [18,19] finds a non-optimal solutionC={v1,v2,v3}.Proposition 5LetG=(V, E)be a graph withV={v1,v2,⋯,vn}andE={e1, e2, ⋯, em}.C is the vertex set obtained byAlgorithm3,then C is a minimal vertex cover of G.ProofTo prove the result, we first show that C is a vertex cover of G. By the first step of Algorithm 3, we suppose that the core vertex set of G is K⊆V. Thus, there are two possibilities: either (i) K=∅ or (ii) K≠∅. (i) In case K=∅, assume C is the output result of Algorithm 3. By the stopping criterion of Algorithm 3, then we have POSC(d)=POSA(d), where S=(U, A∪{d}) is the IDS of the graph G. By Proposition 1, we know that S is consistent. Thus we have POSC(d)=U=E∪{em+1}. Recall the definition of S, for any e∈E, d(e)≠d(em+1). Since POSC(d)=U, thus for any e∈E, we have [e]C⊆E. This means that for any e∈E, there isv∈Csuch thatv(e)≠0. Thus we can conclude that every edge of G has at least one end in C. (ii) In case K≠∅, we can obtain the same result. Hence C is a vertex cover of C. Next, we show that C is a minimal vertex cover of G. If C is not a minimal vertex cover of G, then there isv∈Csuch thatC−{v}is a vertex cover of G. This implies thatPOSC−{v}(d)=U. But by the steps 5 and 6 of Algorithm 3, we know thatPOSC−{v}(d)≠U, which contradicts to the assumption that C is not a minimal vertex cover of G. This completes the proof.Proposition 5 shows that Algorithm 3 can find a minimal vertex cover of a graph. Sometimes, the algorithm can find an optimal solution (see Example 6), but in most cases, it only finds a local minimum that is reasonably close to the global minimum (see the experimental results).To further illustrate effectiveness of the new proposed algorithm, we compare it with some classical algorithms for MVCP, which have been summarized in detail by Gomes et al. [19]. As shown in [19], the Greedy algorithm is the overall best one as compared to the Round and Dual-LP algorithms. For this reason, we only choose the Greedy algorithm, the List algorithm [1] and the VSA algorithm [2] in the experiment. The experiments are performed on a personal computer with Windows 7 and Intel (R) Core (TM) i7-4790 CPU 3.60GHz and 12.0GB memory. The algorithms are implemented in Matlab 7.8. For the purpose of exploring the statistical significance of the results, the Friedman test [17] and the Nemenyi post-hoc test [12,34] are given.Let n and m denote the number of vertices and the number of edges, respectively, of the input graph. Algorithms for MVCP are tested on randomly generated graphs. All the algorithms are executed 10 times on each random graph with the same number of vertices and edges. We test 30 random graphs. These graphs are divided into 3 sets as shown in Table 6. The column Density is the percentage of non-zero entries in the incidence matrix of the input graph.Table 7lists the experimental results for MVCP. The column VC is the optimal value for MVCP of the input graph, which is solved by the LINGO 11.0 software. The column Value is the value of a solution found by an algorithm. In the table, the column Time is the running time (in seconds) of the algorithm.It is easy to note from Table 7 that VCAR runs faster than VSA. However, the runtime of VCAR is slower than that of the Greedy and List algorithms. This is due to the fact that the time complexities of Greedy and List algorithms are both O(|E||V|). To further explore whether the runtime of the four algorithms are significantly different, we make a Friedman test. The null hypothesis of the Friedman test is that all the algorithms are equivalent in terms of runtime. The test result is p=0. This means that we can reject the null hypothesis at the 0.05 level of significance and accept the alternative hypothesis that all the four algorithms are different in terms of runtime. Furthermore, at the 0.05 level of significance, the Nemenyi test demonstrates that the runtime of Greedy is statistically better than those of List, VSA and VCPR, and there is no consistent evidence to indicate statistical runtime differences between List and VCAR.Expect the running time, there is another important index called the ratio value to measure the quality of a solution derived by an algorithm for MVCP [19]. The ratio value is defined as Value/Optimum, where Value is the value of a solution found by an algorithm and Optimum is the optimal solution value [19]. Note that for an approximate algorithm for MVCP, the smaller the ratio value is, the better the algorithm performs.Fig. 2shows the ratio values, plotted as points, of the Greedy, List, VSA and VCAR algorithms for the 30 random graphs. Table 8summarizes the ratio values according to the testing results of Table 7. In these tables, columns Min, Ave, Max and Std, represent, respectively, the minimum value, the average, the maximum value and the standard deviation of the ratio values. Table 8 shows that VCAR comes with a lower mean ratio value and the standard deviation than the ones produced by the Greedy, List and VSA algorithms, respectively. Additionally, Fig. 2 reveals that VCAR performs better than the other three algorithms in terms of the ratio values for all the graphs listed. The value found by the VCAR method is quite close to the optimal solution. These results mean that VCAR has a superior performance on the ratio values when compared with the Greedy, List and VSA algorithms. To further explore whether the ratio values of the four algorithms are significantly different, we make a Friedman test. The null hypothesis of the Friedman test is that all the algorithms are equivalent in terms of the ratio values. The test result is p=0. This means that we can reject the null hypothesis at the 0.05 level of significance and accept the alternative hypothesis that all the four algorithms are different in terms of the ratio values. Furthermore, at the 0.05 level of significance, the Nemenyi test demonstrates that the ratio value of VCAR is statistically better than those of Greedy, List and VSA. Combining with the computational time and the ratio values, we can infer that the new presented algorithm is an effective method for MVCP when dealing with small-scale graphs.To further demonstrate the effectiveness of the new approach, we have made some comparative results on large-scale random graphs. To do this, the Erdos-Renyi graph is used in this experiment [4,16]. In the G(n, p) model, a graph is constructed by connecting n vertices with probability p. 30 Erdos-Renyi graphs are used. The numbers of vertices of the 30 graphs vary from 500 to 2000, and the numbers of edges range from 2478 to 44,090. Table 9shows the comparative results. We can see that the overall average runtime of the Greedy, List, VSA and VCAR algorithms are 0.14s, 0.27s, 11.82s and 81.53s, respectively. The Friedman and Nemenyi tests indicate that Greedy is statistically better than List, VSA and VCAR in terms of runtime. The VCAR algorithm ranks 4 among the four algorithms. Although the runtime for VCAR is worse than that of Greedy, List and VSA, 70% instances can be solved quickly (in less than 100s). We also observe that VCAR always has the minimum solution value for all instances. Based on the results of the Friedman and Nemenyi tests, VCAR significantly outperforms the Greedy, List and VSA algorithms in terms of the solution values. We can infer that the new presented algorithm can find a good solution in reasonable time when dealing with large-scale graphs.In the above subsection, we have shown that the proposed algorithm based on rough sets gets a better performance on the ratio values when compared with some other algorithms. There are, however, also several remaining issues that deserve further investigation. For example, although we have improved the computational time, it is needed to improve for large data. There are some effective RS-based feature selection methods may help us to do this [13,40,41,60,64]. The proposed MVCP algorithm here is based on the positive-region method, but there are many RS-based feature selection methods such as information entropy reduction and discernibility matrix reduction [11]. All these methods have their advantages and limitations. How to combine these methods to get a better performance on both the ratio values and running time for MVCP is still needed. However, it should be note that the ratio value is an important index for MVCP, thus it needs to make sure that the generation procedure adopted of the RS-based methods can produce a minimal vertex cover as close to the optimal solution as possible. This is why we adopt the method for MVCP based on the positive-region (see Proposition 5). Furthermore, the evaluation function for attribute significance measure plays an important role in the RS-based method. Except the dependency function used in this paper, the entropy function [48] and other fitness functions [61,62] can also be adopted. Compared with other evaluation functions, the attribute significance measure used here requires only simple mathematical operators (see Proposition 4). Finally, we randomly select any one attribute when more attributes have the same significance in each round, some hybrid approaches can be applied to this process [40,53].

@&#CONCLUSIONS@&#
We have discussed the minimum vertex cover problem (MVCP) of graphs based on rough sets in this paper. As a consequence, we have shown that the problem of finding the minimal vertex cover of a graph is equivalent to the problem of finding the attribute reduction of a decision information table. Also, finding a minimum vertex cover of a graph is equivalent to finding an optimal reduct of its decision information table. Furthermore, a new approximate algorithm for MVCP based on rough sets has been presented. The computational experiments have demonstrated that the new proposed algorithm has a superior performance in terms of the ratio values. The results presented here are very important since this study may provide new methods for MVCP. However, the result with respect to the ratio value is experimental, and again an interesting open question is how to prove theoretically it. In addition, applying the proposed rough set model for MVCP and designing a faster algorithm in terms of computational time should be studied in our future work.