@&#MAIN-TITLE@&#
Improving scene attribute recognition using web-scale object detectors

@&#HIGHLIGHTS@&#
Humans often describe scenes by their affordances, which are suggested by objects.Object detectors trained at the web scale can improve scene attribute recognition.We experiment on a semi-supervised continuous learner and a supervised deep network.Learned models capture intuitive and useful object-scene attribute relationships.

@&#KEYPHRASES@&#
Affordances,Scene understanding,Semantic attributes,Semantic features,

@&#ABSTRACT@&#
Semantic attributes enable a richer description of scenes than basic category labels. While traditionally scenes have been analyzed using global image features such as Gist, recent studies suggest that humans often describe scenes in ways that are naturally characterized by local image evidence. For example, humans often describe scenes by their functions or affordances, which are largely suggested by the objects in the scene. In this paper, we leverage a large collection of modern object detectors trained at the web scale to derive effective high-level features for scene attribute recognition. We conduct experiments using two modern object detection frameworks: a semi-supervised learner that continuously learns object models from web images, and a state-of-the-art deep network. The detector response features improve the state of the art on the standard scene attribute benchmark by 5% average precision, and also capture intuitive object-scene relationships, such as the positive correlation of castles with “vacationing/touring” scenes.

@&#INTRODUCTION@&#
When searching for an image, a user may want to describe the desired characteristics of the scene, such as “cluttered”, “soothing”, or “open area”, instead of being limited to basic category labels such as “city”, and interactively refine search results according to those characteristics. When a learning system encounters an object or scene that is very different from any of the categories it has encountered before, it might be useful for the system to output at least a high-level semantic description of the object or scene. Similarly, given only a high-level semantic description of an object or scene category, it may be useful for a learning system to be able to generalize and recognize new instances of that category, without seeing any examples. These are only a few of the use cases for semantic attributes – human-nameable, descriptive properties of an object or scene [1–6]. In this paper, we focus on using semantic attributes to describe scenes. Attributes are particularly appropriate for describing scenes because scene categories can exhibit wide intra-class variation, the scene space is continuous (i.e. there are smooth transitions between scene categories), and a single image can contain multiple scene categories [7].Traditionally, scenes have been analyzed using global image features such as Gist [8] or HOG visual words [9,10]. In their pioneering work [8], Oliva and Torralba showed that scene classification is possible using global, “holistic” scene properties without the need to first perform individual object detection. These global scene properties can also be used to predict perceptual properties of the scene, such as degree of naturalness or openness, which have remained highly influential in modern work on scene attributes. In particular, the taxonomy of scene attributes in the recently developed SUN attribute database [7] includes several “spatial envelope” attributes inspired by the early Gist work of Oliva and Torralba.While global image features such as Gist can be used to recognize many types of scene attributes, a large number of interesting scene attributes are not well captured by global features. In particular, many scene attributes in the crowd-sourced SUN attribute database are more naturally characterized by localized image evidence. For example, Patterson and Hays [7] found that humans often describe scenes based on their functions or affordances, such as “camping” or “studying/learning”. A strong cue for “camping” (defined as “either an actual camp site, or scene in wilderness suitable enough for humans to make a tent and/or sleep” [7]) would be the presence of a tent. However, a tent is unlikely to be detected by global image features such as Gist. On the other hand, a suitably trained tent object detector would likely fire on the image, making it useful for recognizing the “camping” attribute. We hypothesize that scene attribute recognition can benefit from the integration of localized object detector information.Though object detectors in the early days of scene understanding may have been limited in scope and performance, recent years have seen the development of high-quality detectors trained at the web scale (e.g. [11–13]). In this paper, we demonstrate that scene attribute recognition can be improved by leveraging a large collection of modern object detectors trained at the web scale. We perform experiments using both the Never Ending Image Learner (NEIL) [13], which continuously learns object models in a semi-supervised manner from web images, as well as a state-of-the-art deep network [14] trained on the ImageNet Large Scale Visual Recognition Challenge 2012 dataset [15]. Both types of detectors have advantages: NEIL learns continuously with limited supervision and its models can be expected to improve over time, while deep learning features have been shown to provide state-of-the-art performance in diverse recognition tasks. We find that detector-based features perform better than any individual global image feature baselined in the SUN attribute database, including Gist, HOG2x2, self-similarity, and geometric context color histograms, as well as their combination.The core contribution of our paper can be summarized as follows. Motivated by recent findings that humans often describe scenes by their functions or affordances, which are largely suggested by the objects in the scene, we propose leveraging modern, web-scale object detectors to improve scene attribute recognition. We demonstrate the effectiveness of this idea on the standard scene attribute benchmark.

@&#CONCLUSIONS@&#
