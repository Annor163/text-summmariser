@&#MAIN-TITLE@&#
A simulation testbed for analyzing trust and reputation mechanisms in unreliable online markets

@&#HIGHLIGHTS@&#
Open-source approach for building market scenarios with varying population mixes.The effect of sources of information on consumer decisions.Benchmarking of various trust and reputation models.Based on the FIRE modelling approach and built in Eclipse AMP.Example experimental scenario to demonstrate testbed setup and functionality.

@&#KEYPHRASES@&#
Trust and reputation,Open online markets,Simulation testbed,Agent modeling,

@&#ABSTRACT@&#
Modern online markets are becoming extremely dynamic, indirectly dictating the need for (semi-) autonomous approaches for constant monitoring and immediate action in order to satisfy one’s needs/preferences. In such open and versatile environments, software agents may be considered as a suitable metaphor for dealing with the increasing complexity of the problem. Additionally, trust and reputation have been recognized as key issues in online markets and many researchers have, in different perspectives, surveyed the related notions, mechanisms and models. Within the context of this work we present an adaptable, multivariate agent testbed for the simulation of open online markets and the study of various factors affecting the quality of the service consumed. This testbed, which we call Euphemus, is highly parameterized and can be easily customized to suit a particular application domain. It allows for building various market scenarios and analyzing interesting properties of e-commerce environments from a trust perspective. The architecture of Euphemus is presented and a number of well-known trust and reputation models are built with Euphemus, in order to show how the testbed can be used to apply and adapt models. Extensive experimentation has been performed in order to show how models behave in unreliable online markets, results are discussed and interesting conclusions are drawn.

@&#INTRODUCTION@&#
Open distributed environments have successfully been modeled as multi-agent systems comprising agents that interact with each other under specific protocols and service-level agreements. Most of these systems take the availability of information for granted and just focus on the decision-making strategies of agents. In real-life environments, though, it is practically impossible for agents to have perfect information about the environment, properties and possible strategies or interests of others. Thus, agents have to make decisions under uncertainty.One way to tackle uncertainty in open distributed systems is through the definition of atrust and reputationscheme. Trust and reputation (TR) models may guide an agent in deciding with whom to (prefer to) interact. In fact, trust and reputation have been recognized as key issues in autonomic, peer-to-peer and grid computing, as well as in service-oriented architectures and e-commerce applications.In online markets, with millions of nearly-anonymous agents buying and selling a plethora of goods, self-interested selling agents may act maliciously by not delivering products with the same quality as promised. Thus, trust is a critical issue and it is important for buying agents to reason about the trustworthiness of sellers and determine with which sellers to interact. The higher the value of the products being transacted, the higher the importance of trust for the successful engagement of buyers and sellers.Trust in online markets seems to be more important than in physical ones (Bakos and Bailey, 1997), since neither seller identity nor product characteristics can be evaluated during the transaction. For this reason, users usually request reliable reports on past performance and truthful statements of future guarantees, and are more likely to participate in web transactions and relationships if they receive strong assurances that they are engaging in a trusting relationship.As commerce in high-value items becomes increasingly profitable on the Internet, online merchants and auctioneers face enormous challenges in overcoming the trust problem and creating attractive trading environments. In this context, trust and reputation systems provide a foundation for security, stability, and efficiency in the online environment because of their ability to stimulate quality and to sanction poor quality. Trust and reputation scores are assumed to represent and predict future quality and behaviour and thereby to provide valuable decision support for relying parties.Current work aspires to investigate issues related to trust and reputation in open online markets. Towards this aim we have developed Euphemus, a multivariate agent-based platform for simulating online markets, where agents offer a specific service that others may consume. Setting up a set of example scenarios, we have studied the impact of various sources of information and evaluation criteria in decision making, as well as the effect of altering agents’ behavior for various trust and reputation models.The paper is organized as follows: Section 2 discusses the state-of-the-art on the available trust and reputation models and testbeds. Section 3 discusses the basic trust and reputation concepts that Euphemus builts upon, the models that have been developed through Euphemus, the architecture and the data flow in the developed framework. Finally, Section 4 discusses the categories of the experiments conducted, while Section 5 summarizes work performed, probes on future extensions and concludes the paper.The terms of trust and reputation have been used in the literature in various ways, but there is no commonly accepted definition. Josang et al. (2007) divide trust in reliability trust and decision trust, where reliability trust focuses on dependence on the trusted party, as seen by the trusting party (Gambetta, 1990), while decision trust is the extent to which one party is willing to depend on something or somebody in a given situation with a feeling of relative security (McKnight and Chervany, 1996). One should strongly emphasize that reliability trust is a clearly subjective notion and is context-dependent, while decision trust refers to dependence and reliability as the previous one, but it also refers to utility, as well as a risk. Besides, every trust action entails some risk (Luhmann, 1990) and cooperation under conditions of large potential losses shows greater reliability than in other cases (Yamagishi et al., 1998).Reputation is obviously closely connected to the concept of trust. Nevertheless, it is obvious that there are significant and clear differences. Reputation can be defined as a meta-belief, a belief about others’ minds, or more specifically about others’ evaluations of the target (Conte and Paolucci, 2002) and is often expressed as a quantity derived from the underlying social network which is globally visible to all the members of the network (Josang et al., 2007).According to Huang et al. (2008), all trust- and reputation-related concepts, techniques and models can be defined with respect to the trust management process adopted, which has to address three fundamental questions: (a) why does an agent trust another, (b) how do agents judge or evaluate the trustworthiness of others and, (c) what does an agent do after obtaining the trustworthiness of others. Artz and Gil (2007) classify trust and reputation models along four axes:•Policy-based: These models employ schemes to promote trust in terms of exchanging credentials and imposing access strategies. Such schemes are network security credentials, trust negotiation, security policies and trust languages, distributed trust management, effect of the type of credentials etc. One may refer to work by Winsborough et al. (2000), Li et al. (2003) and Tonti et al. (2003) as representative approaches to policy-based TR models.Reputation-based: These models calculate trust based on the behavior of an entity and data may come from either third-party information or direct experience. Basic approaches are related to decentralization and referral trust, trust metrics in a web of trust, trust in P2P networks and grids, and application-specific reputation. One may refer to work by Sabater and Sierra (2002), Josang and Ismail (2002), Kamvar et al. (2003), Xiong and Liu (2004) and Guha et al. (2004) as representative approaches to reputation-based TR models.General models of trust: These types of models employ primitives drawn from psychology and sociology, in order to define the factors that affect trust. These factors deal with general characteristics of trust, computational and online trust models, game theory and agents, as well as software engineering metrics. Work by Friedman et al. (2000), Mui et al. (2002) and Huynh et al. (2006) are typical approaches of general trust models.Trust in information resources: From a different viewpoint, trust has to do with whether various sources of information or websites are reliable. In this context, research focuses on trust concerns in the Web and/or the Semantic Web, trust using hyperlinks, subjectivity analysis, provenance information, content trust, site design and human factors. Grandison and Sloman (2000), Clarke et al. (2001) and Gyongyi et al. (2004) propose such TR models.This classification of trust and reputation models is quite generic and covers most of the research approaches followed (including current work). Nevertheless, other classifications exist. One may refer to work by Sabater and Sierra (2005) for more information.Finally, it is worth mentioning that there are some fuzzy approaches to trust. Fuzzy logic offers a qualitative approach which is very useful for modeling several cognitive dynamics (Dubois and Prade, 1980) and provides the ability to handle uncertainty and imprecision effectively (Ross, 2004). Thus, it is ideally suited to the concept of trust. Indicatively, we refer to the work of Falcone et al. (2005) and Griffiths (2006).Many researchers have dealt with the analysis and effect of various trust and reputation mechanisms, through the development of respective simulation schemes/platforms. Although it is essential to have a scalable, multivariate platform for testing and assessing different models, no such generic framework exists, mainly because of the inherent software complexity generated by the models applied. Each proposed model is evaluated within a testbed, specifically designed to verify and evaluate that model. The main reason is that different models usually have different inputs, generate different outputs, trigger different interactions and are assessed against different key performance indicators, thus making the design of a generic trust evaluation testbed difficult.One of the most popular trust testbeds is ART – Agent and Reputation Testbed (Fullam et al., 2005). ART provides the trust and reputation research community with a unified platform for evaluation and competition. Agents use trust strategies to exchange expertise with others in order to appraise paintings and make money by providing accurate appraisals to receive more paintings in the next time step. However, ART is not flexible enough for carrying out realistic simulations and robustness evaluations for many of the proposed models. Harbers et al. (2007) report that some aspects of their model could not be verified in ART and that the performance of the trust model in ART depends too much on the opponents. Hang et al. (2008) face a similar problem and point out that ART does not support agents providing referrals. They overcome this by designing their own trust evaluation testbed where agents estimate the trustworthiness of others without preexisting knowledge. Finally, Kerr (2009) states that he was not able to use ART for his experiments and designed a new testbed in order to carry out the simulations needed (Kerr and Cohen, 2009). In any case, the ART community has faded out and the platform is not supported anymore, so the testbed has become obsolete.Some other trust testbeds have also been developed, which employ the prisoner’s dilemma (Axelrod, 1984) approach. Indicatively, we mention the work of Schillo et al. (2000) who propose a disclosed iterated prisoner’s dilemma with partner selection using a standard payoff matrix, and Mui et al. (2002) who propose an iterative prisoner’s dilemma game where the success of a strategy gives a greater number of descendants in the following generation. Dealing with trust through the prisoner’s dilemma though, seems to have significant drawbacks (Kendall et al., 2004). Agents have no opportunity to isolate untrustworthy opponents, since they are forced to interact with all competitors. Also, multidimensional trust modeling is not promoted, since agents only evaluate one aspect of their opponents’ behaviour.One should also mention SuppWorld (Sabater Mir, 2002), a framework adapted to the special characteristics of ReGreT (Sabater and Sierra, 2002), which allows to build simple scenarios oriented to test the basic aspects of trust and reputation models and also complex scenarios where social relations acquire a great relevance. It comprises two grid markets, one used as a home of producers and another as a home of manufacturers. Although efficient, SuppWorld is too focused on ReGreT and suffers from scalability issues.In addition, the SPORAS experiments (Zacharia, 1999), measuring the time for reputation models in electronic markets to converge to true reputations, have been widely-utilized for testing Yu and Singh’s model (Yu and Singh, 2002), ReGreT (Sabater and Sierra, 2002), AFRAS (Carbo et al., 2002a,b), as well as two online reputation mechanisms, eBay [http://www.eBay.com] and Bizrate [http://www.bizrate.com]. However, these experiments are too narrow in scope, evaluating reputation models based on only single-agent metrics, such as time to converge, while neglecting system-based success measures.Practically, the most complete approach on evaluating models is the Alpha Testbed – ATB (Jelenc et al., 2013). Having identified the basic TR testbed limitations, ATB provides an evaluation methodology for dealing with both models that have a decision making mechanism and ones that do not. Nevertheless, ATB cannot handle information from social interactions and virtual organizations, given that it tests only one agent at a time, while other agents are simulated. It is explicitly stated that system-level metrics that evaluate the performance of the entire system are not supported. Additionally, ATB is unfit to evaluate TR models that: (i) represent information in formats substantially different from the ones used in ATB and (ii) exchange or compute opinions with special protocols.It should also be stated that most of the above mentioned testbeds are far from being fully parametric testbeds, in the sense of easily programming new environments and new TR models/decision making mechanisms. In fact, when focusing on trust models and online markets, limited notable work has been done towards the construction of a generic testbed. Collins et al. (2002) propose MAGNET, a multi-agent testbed which implements a marketplace where agents can negotiate over multiple attributes. One weakness of the system is that critical issues such as segregation and centralization are not considered. Moreover, Seidel et al. (2009) introduce a new type of user interface for agent-based online markets, 3D Virtual Worlds, and provide a testbed for studying the interaction and behaviour of users in an online market setting. The marketplace in this platform is realized by means of auctions and fixed price product trade. Finally, we mention U-Mart (Terano et al., 2003), an artificial market testbed for economics and multi-agent systems, where both human and software agents can engage. The U-Mart project develops open-type simulation toolkits to study trading strategies of the agents, the behavior of the market, and their relationships. Although efficient, these testbeds are focused on a specific problem and are not generic enough for analyzing properties of e-commerce environments from a trust perspective.Another important issue in designing a testbed is that, although some models take into account the multi-objective aspect of trust, there are no works that experimentally verify the influence of those factors on trust. For example, Falcone et al. (2005) take into account three main beliefs regarding the trustee (ability/competence belief, disposition/availability belief, unharmfulness belief) and two main beliefs regarding the contextual factors (opportunity and danger beliefs). Also, Villata et al. (2013) mention that trust is not a monolithic and static concept and propose a framework in which trustworthiness is defined by sincerity and competence with respect to specific domains. Nevertheless, none of them experimentally tests or evaluates these trust aspects.Based on the above discussion, it is important to set up a framework that can support trust modeling and experimentation, especially for online markets. Such a framework must fundamentally:-Integrate as many sources of information as possible, in order to address issues related to uncertainty and malice and provide a detailed set of trust metrics.Combine a variety of alternative sources of information, since not all of these sources will always be readily available.Introduce reputation (as well as certified reputation) in order to allow for calculating trust in most circumstances.Adopt a decentralized model and support different types of distance functions, in order to simulate needs existing in open multi-agent systems (and consequently in open online markets).Be quite generic and customizable through parameters in order to easily design a model, simulate a particular application domain and validate a specific hypothesis.Draw from social network primitives and address the concept of localization and re-adaptation.Allow the development of evaluation metrics that will allow evaluation of a single TR model as well as system-wide evaluation.Support large-scale experimentation, as well as logging capabilities in order to allow for trustworthy evaluation.flow in the developed framework. FinallyBased on all the above arguments, we have developed Euphemus, a multi-agent framework simulating dynamic online markets that allows for aggregated TR model evaluation. The modeling elements of Euphemus are discussed next, while the code and documentation of Euphemus, along with the models designed and developed can be found at:http://olympus.ee.auth.gr/~asymeon/web/Euphemus_modeling/In order to study issues related to trust and reputation in online markets, we have developed a framework where agents interact taking under consideration the trustworthiness of others. In this section we discuss the main building blocks of the framework, its architecture and the flow of data.Euphemus adopts the modeling approach of the FIRE model (Huynh et al., 2006) and extends this model in order to build other TR models as well. Euphemus TR model incorporates 4 different sources of information for agents to calculate trust metrics, even under circumstances of uncertainty or lack of sufficient information, something usual in open multi-agent systems. These types of trust and reputation metrics defined are:•Interaction Trust (IT): This type of trust is based on the agents’ direct experience, which results from the direct interactions between them. In other words, an agent determines trust to another agent by using its knowledge from past interactions with it.Role-based Trust (RT): In this case, trust is determined by the relationships between the agents and the roles they have or by the rules of the environment. For example, an agent may have the predisposition to trust someone if he is member of the same company or team. These beliefs are largely dependent on the current application and are coded through some rules which are based on the agents’ roles in order to calculate a predefined value of trust.Witness Reputation (WR): In this case, an agent A calculates its trust in agent B by collecting referrals from other agents (witnesses) that have interacted with B in the past. Here, it is assumed that agents are willing to share their direct experience with others.Certified Reputation (CR): This type of reputation results from third-party referrals provided by the agent itself (the agent for whom trust is calculated and is called agent-target), in contrast to the previous cases where the agent-evaluator has to collect the required information on its own. The agent-target presents referrals from another agent with which it has interacted in the past, which certify its behavior, in order to convince the others for its reliability. Certified reputation can be calculated after a relatively few interactions; therefore its value is calculated fast and can be readily available. This feature increases greatly the robustness of the system.Every source of information is assigned to a component of the system, which is responsible for the calculation of the corresponding value of trust or reputation: Component IT→Interaction Trust, Component RT→Role-based Trust, Component WR→Witness Reputation and Component CR→Certified Reputation.The following assumptions have been made about the agents and the environment:•Agents are willing to share information with others concerning the experience from their interactions. This mostly refers to groups of agents having common interests, e.g. a group of consumers sharing their ratings after interacting with providers in order to assist each other in making the best choice.There are no consequences resulting from the loss of privacy, when the way an agent assesses the services of others is revealed.For the evaluation of an agent’s behavior and performance a rating system has been defined. In fact, the FIRE model rating mechanism has been extended in order to allow for multi-criteria evaluation. Therefore, aratingr is defined as:(1)r=(Agu,Agj,i,term1,ν1,term2,ν2,...,termn,νn,νtotal)whereAguandAgjare the agents that participated in the interaction i, νris the rating valueAgugave toAgjfor the term termrand νtotalis the total rating value given byAgutoAgj.Variables vrand vtotaltake values in the range [MinRatingValue,MaxRatingValue], where MinRatingValue indicates absolutely negative rating and MaxRatingValue absolutely positive.Rules(Rl) are defined according to FIRE as:(2)Rl=roleAgu,roleAgj,termr,inf,νtotalIfroleAguandroleAgjare the roles of agentAguandAgjrespectively, then the expected performance ofAgj(i.e. the expected rating value received) with respect to termrin an interaction i withAguis νtotal, where νtotaltakes values in the range of [MinRatingValue, MaxRatingValue], and inf is the level of influence of this rule on the resulting value, which is calculated by the component RT, or the belief strength of agentAguon the rule. Variable inf takes values in the range [0,1]. For example, Rl1=(buyer, seller, quality, 0.3, −0.2) expresses the belief of a buyer that an ordinary seller usually sells a product of slightly worse quality than agreed, but the reliability of this belief is low (0.3). Rules capture various settings or beliefs (which are mostly domain-specific) based on the roles of the evaluator and the target agent in order to assign a predetermined trustworthiness to the target agent and are used for the calculation of Role-based Trust (RT).For the calculation of thetrust valueresulting from each component we have also adopted the respective FIRE formula as the default setting. Thetotal trust value(T(Agu,Agj)) is the normalized aggregation of the weighted values calculated by the 4 components:(3)T(Agu,Agj)=∑k∊{I,R,W,C}hk.Tk(Agu,Agj)∑k∊{I,R,W,C}dkwhere•w1,wR wW,wCare coefficients corresponding to IT, RT, WR, CR components and reflect the importance of each of them in calculating trust in a specific application. These coefficients are determined by the end users of the system. We can exclude a component from participating in the calculation of the total value by setting the corresponding coefficient equal to zero.TK(Agu,AgJ)is the trust value calculated by the component K.hK=Wk·ρkAgu,Agj.ρkAgu,Agjis the reliability value of the component K.Moreover, a reliability value is calculated for each component and it reflects the confidence of the model for the produced value given the information that was taken into account in the calculation. Thetotal reliability value(ρT(Agu,Agj)) comes from the combination of the individual values calculated by each component as follows:(4)ρr(Agu,Agj)=∑k∊{I,R,W,C}hK∑k∊{I,W,R,C}WkIn FIRE all the formulas are used to calculate trust regarding a specific term every time (quality, honesty, utility, etc.). We use these formulas more generally without referring to a specific term, since these terms are taken into consideration through the form of the ratings.As already discussed, in a TR framework it is important to be able to develop and test as many models as possible. In order to provide this “plug-n-play” functionality, we decided to build Euphemus with theEclipse Agent Modeling Platform(AMP).TheEclipse Agent Modeling Platform (AMP)[http://eclipse.org/amp/] provides extensible modules and tools for representing, editing, generating, executing and visualizing Agent-Based Models (ABMs). An ABM comprises 5 concepts: Agents and Context Agents, Spaces, Attributes and Actions. The first 4 elements refer to structural components, while Actions determine the agents’ behavior by using various Functions. In other words, Agents, Context Agents, Attributes and Spaces define what we are modeling, while Actions give life to the model.Agents have Attributes and Actions associated with them. Context Agents are composite structures, capable of encapsulating other Agents, Spaces, Attributes and Actions. Spaces provide an environment where agents can have a physical or notional location. There are four kinds of spaces represented in Eclipse AMP: Continuous Space (objects can be placed anywhere with arbitrary position), Grid (lattice structure, such as a one-dimensional vector, a two-dimensional grid, a three-dimensional cube etc), Network (graph structure) and Geography (physical landscape). In a Grid, distance between two cells can be defined in three ways: Euclidean (the real distance), Moore (the number of edge or corner adjacent cells crossed to get between them) and Von-Neumann (the number of edge adjacent cells crossed to get between them).As far as Actions are concerned, there are various Action types (Schedule, Rule, Build, Initialize, Watch, Diffuse, Perform, Derive). An Initialize action is executed once and only once for every agent when the model is first started, at time 0, while a Rule is executed once for every agent for every epoch of the model [http://eclipse.org/amp/documentation/amp.pdf].In AMP, each simulation scenario is executed in epochs. The epoch number is used as the time unit for recording events that occur.Euphemus at its current form defines 2 types of agents: those who provide services (providers) and those who consume them (consumers). We denote provider agents with ProAgiand consumer agents with ConAgi. Without loss of generality, we assume that all providers offer the same service. However, they differ in their performance and this determines theutilityresulting from each interaction with a consumer. The utility resulting from the interaction of a ConAgiwith a ProAgi at epoch e is called UGiand takes values in the range of [MinUtilityGain, MaxUtilityGain]. A provider may interact with many consumers at any epoch, while a consumer does not have to consume services at every epoch.Following the interaction with a provider, consumers rate the service according to Eq. (1) indicating the assessment of certain characteristics of the provider in the specific interaction. Within the context of our implementation, we use indicatively 3 terms for the assessment. However, we could use more or less terms. We define that ConAgirates ProAgiin terms of the quality of the service provided, honesty on the provided service and the utility gained. Therefore,(5)term1=quality(6)term2=honesty(7)term3=utilityEach consumer ConAgihas aRating Vector (RaVi)where all ratings are stored and aRule Vector (RuVi)where all rules are stored. In the same manner, each provider ProAgihas aCertified Rating Vector (CRaVi)where it stores all ratings received after each interaction, in case it decides to use them in the future as a proof of reliability.Based on the problem at hand, we decided to model the market and the affinity/closeness between producers and consumers through a two-dimensional square grid with dimensions (CityWidth)×(CityHeight), where each cell can hold only one agent each time. The distance between two cells of the grid is defined as the number of edges or corners of adjacent cells that mediate between them (Moore distance). We have not selected the Euclidean distance because it is better for continuous spaces and not for discrete, such as a grid space. Von-Neumann distance could also have been used.We have assumed that when a consumer wants to use the service it may choose a provider within a particular distance (ConsumerVision). The aim of this attribute is to simulate situations in which only a portion of the providers’ population is available to a consumer (for example due to geographical restrictions). Finally, we have assumed that when a consumer wants to calculate a particular provider’s reputation value (WR component), it may ask for information only from the consumers within a particular distance (ReputationVision).The basic attributes of a provider agent (ProAgi) are:–Profile.In order to benchmark performance, we define the concept of ProviderProfileithat represents the utility gain it can provide, whenever ProAgiservices are consumed. We classify provider agents in four categories, based on their initial utility gain (the utility gain at epoch 1):Honesty.An HonestyValueiis assigned to ProAgi, each time a new experiment starts. Honesty ranges in [−1,1] and remains constant throughout an experiment lifecycle (−1: totally dishonest, 1: totally honest). The HonestyValueiof ProAgiis calculated based on end-user defined probabilities. It determines whether the provider agent (ProAgi) gives each time the maximum UGeaccording to its capabilities (i.e. its profile) or it gives smaller than that in order to have greater benefit.Adapted Utility Gain.Whenever a transaction takes place at a specific epoch e two values for the utility gain are defined for ProAgi: the UGiand the AdaptedUGi, which is the utility gain ProAgifinally gives. If a Provider is totally honest he gives exactly the maximum possible UGi, so AdaptedUGi=UGi. Otherwise, AdaptedUGi<UGiin order to increase profit, however risking to receive low rating.The basic attributes of a consumer agent (ConAgi) are:–Activity level.A consumer’s ActivityLevelidetermines the frequency ConAgirequests the service and is randomly defined upon agent initialization. ActivityLevelivalues range in [ConsumerMinActivityLevel, ConsumerMaxActivityLevel], which are end user defined in the [0,1] interval.Activity status.The ActivityStatusiof ConAgiis defined based on ActivityLeveliand determines whether ConAgiwill be active or not. If ConAgiis active, it selects services from provider agents within its ConsumerVision. If ConAgiis inactive, it remains idle.Mobility level.In order to simulate the dynamic character of an open multi-agent system and the adaptability of agents, we assume that ConAgis can move on the grid during the experiment lifecycle. By changing the position of an agent new trust relationships have to be created, while the perception on neighboring agents has to be adapted. A consumer’s MobilityLeveliis defined as the probability to change position on the grid at any given epoch.Subjectivity status.We assume that a consumer may beobjective, or it mayoverestimateorunderestimateproviders with respect to their quality of service. It affects the rating value ConAgi gives to provider ProAgj, and is constant during an experiment lifecycle.Trust Model.A consumer’s trust model ConTridefines the way it determines its trust in providers. This model is determined during experiment initialization either based on end user probabilities for a variety of alternative trust models or on a deterministic way.Total Utility.It is the sum of the AdaptedUGeof ConAgiinteractions at each epoch e.During experiment initialization, each consumer ConAgiis sprung at a random cell on the grid. A rule set RSi, if it exists, is added to the Rule Vector RaViand the activity level, as well as the subjectivity status of the agent are defined. In the same manner, each provider ProAgiis instantiated at a random cell on the grid and its ProviderProfileiis calculated based on the end user defined probabilities. Next, UG1 and HonestyValueiare defined for all agents and the experiment initiates.Five (5) Rules have been defined for Consumer Agents:Consumer Rule,Make Rating,Movement,Experiments InformationandPrint Sorted Total Utilities. The last two refer to the storage of the experiments results, so they will not be discussed. Rules are executed in a sequential manner, once for every Agent at each epoch. Consumer Agent Rules are analyzed below:–Consumer Rule.At the beginning of each epoch the consumer ActivityStatusiis determined, i.e. if the agent is active or not, based on its ActivityLeveli. If it is active, it has to decide which of the providers located within ConsumerVision it will select in order to interact and after the interaction takes places, it provides a rating for them. In more detail, the steps followed by each Consumer at each epoch are the following:If the Consumer is active, it lists all the providers that are located within ConsumerVision distance. If it is not active the Rule ends.If there are no providers within ConsumerVision, the Rule also ends.Otherwise, based on the TR model employed, the Consumer calculates thetotal trust value(T(Agi,Agj)) as well as thetotal reliability value(ρT(Agi,Agj)) for each Provider of the list as follows:a.It asks other Consumers within ReputationVision for ratings (either made by them or by others) referring to the specific Provider. If they do not own such ratings, they ask from their own neighbors, so there exist two levels of witness-based reputation. If there are relevant ratings, the Consumer stores them in the Rating Vector.It asks certified ratings from the Provider itself and stores them (if there exist) in the Rating Vector.It calculatesTk(Agi,Agj)andρk(Agi,Agj)for each component based both on the already existing ratings and the new ones.It calculates the total trust and reliability values based on the individual values.Then, it divides the listed Providers into two categories: a1 (HasTrustValue) for those that have a trust value and, a2 (NoTrustValue) for those that don’t have a trust value due to the lack of relevant information.– Make RatingThis Rule selects a Provider and calculated rating based on the characteristics defined (quality, honesty and utility in our case). In case Providers with NoTrustValue exist, a Provider is selected based on the Exploit-vs-Explore algorithm discussed below.– Exploit-vs-ExploreChoosing a provider from the list of ones that have a trust value (a1 list in Fig. 1) is a safe choice. However, it may not be the optimal performance a Consumer agent can get because it has not learnt enough about the provider population. On the contrary, choosing from the list of Providers that do not have a trust value (a2 list in Fig. 1) allows it to explore more about the provider population although it may risk losing utility if it encounters a bad provider. This dilemma is addressed in our work by using theBoltzmann exploration strategy[Kaelbling et al., 1996]. An agent tends to explore its environment first and then gradually move its stance towards exploitation when it learns more about the environment and we believe that this is usually the case. According to this strategy, an agent chooses action ak with a probability given by the following equation:(8)P(ak)=eER(ak)/T∑aieER(ai)/TwhereERaiis the expected result from choosing action ai and T is the temperature parameter, which is calculated in a way such that it decreases with time, so that the population exploration is decreasing. In our case, the Consumer has to decide between 2 actions: choose from category a1 the Provider with the greatest trust value or choose randomly a Provider from category a2.ERa1, i.e. the expected result from choosing from a1, is equal to the greatest trust value, whileERa2, i.e. the expected result from choosing from a2, is equal to the mean value of the Providers’ UGs that have been observed, namely the mean value of the UGs which have arisen from the Consumer’s interactions so far.Obviously, one may apply a different selection strategy if is deemed appropriate.– MovementThis Rule is executed after the Make Rating Rule. If the movement condition is true, the Consumer is randomly moved to another available location (where there is no other Agent). This rule has been designed in order to allow for dynamic environments, where agents change selling/buying strategy or when previous knowledge becomes obsolete.Fig. 1 depicts the mechanism executed by ConAgiat each epoch, where the Calculate trust step differs for each TR model applied (more on this in the Experiments section).For Provider Agents, three (3) Rules are defined: theUtility,Keep Best RatingsandExperiments Informationrule. TheUtilityrule calculates UGefor the next epoch e,Experiments Informationrule merely writes experiment parameters to a file, while theKeep Best Ratingsrule stores only the ratings r with total rating value above a specific threshold (ProviderRatingLimit). This data flow executed by ProAgiat each epoch is described in Fig. 2.This section describes the functions used in Euphemus for the calculation of the rating values and the other parameters used. It should be mentioned that these functions are only indicative and could be easily modified or even totally replaced by the final user.As far asqualityis concerned, we assume that it depends on the distance between consumer and provider. The smaller the distance, the better the quality of the service. This simulates the fact that there are usually certain circumstances (here location) which affect the provision of a service.We define two types ofrating in terms of quality: objective rating (νobj) and subjective rating (νsub).Objective ratingrepresents the rating that would be given by an impartial and objective external assessor and depends only on distance. Actually, it represents the impartial rating that would be given if quality features could be measured and objectively compared. So, the maximum rating (MaxRatingValue) is given for the minimum distance (1), while the minimum rating (MinRatingValue) is given for the maximum distance (CityWidth – 1). Objective rating is defined using an exponential function as:(9)νobj(dist)=a-b·exp(c·dist)where=MinRatingValue-MaxRatingValueexp(c)-exp[c(CityWidth-1)], a=MaxRatingValue + b·exp (f), dist is the Moore distance between ConAgiand ProAgj,cis a factor expressing the function’s rate of change and is determined by the user and cityWidth is the width of the place the agents are located.Subjective rating, which is the one finally given by the consumer, represents the agent’s personal perception about quality. It is calculated as follows:(10)νsub=νobj+sfwhere sf is a subjectivity factor depending on the Subjectivity Status of ConAgitowads ProAgjand(11)MinRatingValue-νobj<sf<MaxRatingValue-νobjFor the cases of overestimate and underestimate we choose indicatively a value in this range, while in the case of objective sf equals to zero. So, we have:(12)Sf=0ifSubjectiveStatusij=Objective13(MaxRatingValue-νobj)ifSubjectiveStatusi,j={Overestimate}13(MinRatingValue-νobj)ifSubjectiveStatusi,j={underestimate}As far asrating in terms of honestyis concerned, we assume that it is equal to the respective provider’s HonestyValuei. So, in this case all ConAgis are objective and impartial when assessing someone’s honesty. Subsequently, if we denote this rating with νhon, we have:(13)νhon=HonestyValueiRating in terms of utility(νutil) is proportional to the utility gain resulting from the interaction. The greater the AdaptedUGi, the greater the rating. We assume that the maximum rating (MaxRatingValue) is given for the maximum AdaptedUGi(which is MaxUG), while the minimum rating (MinRatingValue) is given for the minimum AdaptedUGi(which is MinUG). Replacing these values in the standard form of the linear equation (νutil=a·AdaptedUGi+b), we calculate the coefficients a and b. Thus, the resulting function that has been used is the following:(14)νutil=MaxRatingvalue-MinRatingValueMaxUG-MinUG·AdaptedUGi+MinratingValue·MaxUG-MaxRatingValue·MinUGMaxUG-MinUGFinally, thetotal rating(νtotal) depends on the individual ratings(νequal,νhon,νutil)and the weight given to each of them. Without loss of generality, we assume that all ConAgis give the same weight to each term (quality, honesty, utility). Total rating results from the linear combination of the individual ratings, where the factor of each rating equals to the corresponding weight. So, we have:(15)νtotal=wqualνqual+whonνhon+wutilνutilwhere wiis the weight of the rating νiandwequal+whon+wutil=1. Obviously, one may define other criteria and other weights for the rating functions.We assume that the maximum utility gain a provider can give in an interaction is determined by an increasing function. This means that during the application a provider has the capability to improve continuously his/her performance and consequently the maximum utility he/she can give in an interaction. Below we give analytically the function for the calculation of UGifor each ProviderProfileiin our analysis.(16)UGi=MaxGoodUtility-(MaxGoodUtility-UGi)·exp[-d·(i-1)],UG1∊(MinGoodUtility,MaxGoodUtility]IfProviderProfilei=GoodMaxOrdinaryUtility-(MaxOrdinaryUtility-UG1)·exp[-d·(i-1)],UG1∊(MinOrdinaryUtility,MaxOrdinaryUtility]IfProviderProfilei=OrdinaryMaxBadUtility-(MaxBadUtility-UG1)·exp[-d·(i-1)],UG1∊(MinBadUtility,MaxBadUtility]IfProviderProfilei=BadMaxVeryBadUtility-(MaxVeryBadUtility-UG1)·exp[-d·(i-1)],UG1∊(MinVeryBadUtility,MaxVeryBadUtility]IfProviderProfilei=VeryBadwhere UGiis the value of utility gain, i is the number of epoch and d is a factor which expresses the function’s rate of change and is defined by the user. For the AdaptedUGiwe have used the function:(17)AdaptedUGi=UGi·HonestyValueiThus, in the case of an honest Provider (HonestyValuei=1) AdaptedUGi=UGi, while in the case of a dishonest Provider AdaptedUGi∈[−UGi, UGi]. Once again, one may define any other function for calculating UGior AdaptedUGi.The Euphemus framework allows great flexibility to the user with respect to market setup, initialization and run parameters. This provides the capability of creating many sets of parameters for a given market scenario, allowing decoupling runtime settings from the framework itself. Tables 1–3discuss all these parameters, related to the market and the experiment (Table 1), the behavior of the Providers (Table 2) and the Consumers (Table 3), respectively. End user can easily create and store a parameterization through a parameters file.Moreover, as already mentioned, the way Euphemus has been built allows for totally modifying calculation functions. This can be easily performed through changing the respective Rules. Rating calculation functions can be modified through the ConsumerMake Rating Rule, where the end user can define how individual rating values (e.g. in terms of quality, honesty, utility) are calculated by consumer agents after interactions and how subjectivity affects these values. Functions calculating the UGi and AdaptedUGican be changed through provider’sUtility Rule. Finally, through theConsumer Rulethe end user can also modify the way trust is calculated (i.e. add a new trust model), allowing for comparing different trust models, as well as the way a consumer chooses a provider for interaction.

@&#CONCLUSIONS@&#
Within the context of our work we have developed a multivariate adaptive testbed, Euphemus, for the analysis of multi-objective trust and reputation mechanisms in agent-based online markets. In Euphemus agents are split in two groups, either providing or consuming a service. The Euphemus TR model draws primitive from the FIRE model, but also provides the ability to easily implement other TR models. It has been developed on AMP, in order to allow for easily reconfigurability, fast adaptation and large-scale experimentation. Based on the preceding discussion and the sets of experiments performed, Euphemus satisfies the fundamental criteria of a TR testbed, as defined in Section 2. Numerous experiments were performed in order to demonstrate the functionality of Euphemus. The aim of our testing market scenario was to assess the impact of various sources of information on various state-of-the-art TR models and various rating criteria in versatile populations, as well as to study the effect of providers’ dishonesty and consumers’ subjectivity.With respect to providers’ dishonesty, it is obvious that it strongly affects the overall utility of consumer populations and validates the theoretical assumptions. With respect to consumers’ subjectivity things are not so clear, though. In relatively trustworthy environments being objective contributes to the formulation of sounder opinions on providers and consequently to the achievement of greater profit. In untrustworthy environments though, and given the ability of providers to move in the grid (thus, go to “territories” where they have not been rated) subjectivity allows for risk and leads to better results. One should also mention that no optimal TR model has identified. Performance is strongly affected by the mechanism employed, thus giving room for further research.Regarding the effect of information sources, the hypothesis that the coupling of IT and WR (or NR in the case of REGRET) results to improved performance than IT itself is strongly supported, given that alignment of rating criteria has somehow been achieved. With respect to the other information sources, results strongly depend on the agent population and the provider distribution. Thus, it is hard to make a safe assumption.Future work includes the elaborate experimentation for the examined scenario in the cases of “biased” populations in order to identify specific population dynamics that may lead to the improvements of the quality of service. Additionally, Euphemus could be extended with respect to the credibility of third-party information, since WR and CR are prone to false or inaccurate information. Finally, provider adaptation and behavioral change could be considered through self-optimization functions that would lead to interesting marketplace dynamics.