@&#MAIN-TITLE@&#
Efficient multi-criteria optimization on noisy machine learning problems

@&#HIGHLIGHTS@&#
The Kriging-based EGO techniques performed better than the baseline LHS approach.The use of re-interpolation is crucial to cope with noise.Repeats can be necessary but also decrease the number of possible infill points.

@&#KEYPHRASES@&#
Machine learning,Multi-criteria optimization,Efficient Global Optimization,Kriging,Hypervolume indicator,

@&#ABSTRACT@&#
Recent research revealed that model-assisted parameter tuning can improve the quality of supervised machine learning (ML) models. The tuned models were especially found to generalize better and to be more robust compared to other optimization approaches. However, the advantages of the tuning often came along with high computation times, meaning a real burden for employing tuning algorithms. While the training with a reduced number of patterns can be a solution to this, it is often connected with decreasing model accuracies and increasing instabilities and noise. Hence, we propose a novel approach defined by a two criteria optimization task, where both the runtime and the quality of ML models are optimized. Because the budgets for this optimization task are usually very restricted in ML, the surrogate-assisted Efficient Global Optimization (EGO) algorithm is adapted. In order to cope with noisy experiments, we apply two hypervolume indicator based EGO algorithms with smoothing and re-interpolation of the surrogate models. The techniques do not need replicates. We find that these EGO techniques can outperform traditional approaches such as latin hypercube sampling (LHS), as well as EGO variants with replicates.

@&#INTRODUCTION@&#
In machine learning (ML), and in particular in classification, the quality of a learning algorithm can be measured by the number of correctly predicted instances on an usually independent set of test instances. Most learning algorithms are highly influenced by the hyperparameter settings.11A hyperparameter is a parameter of the learning algorithm which is set to a specific value prior to the learning phase and controls the ability of the learning algorithm to adapt to new data (generalization).Thus, finding good hyperparameters is essential for training a precise classifier. For solving this problem, we can define and perform a single-criteria optimization (SCO), where the classification error or any other quality indicator is being minimized. Due to the large runtimes of ML algorithms, it has been shown earlier [1â€“3] that methods like Efficient Global Optimization (EGO) [4] can help to solve the problem, especially when budgets are limited. In EGO the optimization is internally performed on a surrogate model. The expected improvement (EI) [4] deals as infill criterion in an sequential optimization process, recommending which point to explore/to evaluate in the ML task.However, in most cases the user is not only interested in high-quality prediction models, but also wants that model training and parameter tuning is performed in a reasonable amount of time. These objectives are usually conflicting, demanding for the concept of multi-criteria optimization (MCO). Nowadays, EGO-like algorithms are also available for multi-criteria optimization problems [5]. Up to now, this paradigm has received little attention in the ML community, and only little work has been spent to optimize the mentioned objectives at the same time.One reason for this lies in the nature of ML parameter tuning, where the evaluation of the learning process is usually subject to noise. Besides the usual noise of labeling the ML data, it is mainly caused by the randomness in the selection of training, validation and test data which lead to different results, even for deterministic ML models. A surrogate model has to cope with such noisy responses. The noise in the quality response will increase if we look for solutions with low runtime, which is usually connected with smaller training set sizes.Summarizing, the typical challenges of noisy MCO can be formulated as follows: (a) finding a good approximation of the Pareto front, (b) coping with the noise and (c) finding a good solution set with very restricted budgets of function evaluations.We aim at applying multi-criteria EGO variants to solve two criteria ML tasks. Therefore we can define the following research questions:Q1Is multi-criteria optimization with surrogate modeling (Kriging) possible for two criteria in the presence of strong noise?Is it also possible when the budget for function evaluations (i.e. ML training runs) is very restricted?Is it necessary to dampen the noise by averaging over repeated function evaluations, at the price of fewer allowed infills under the given budget?Are the multi-criteria EGO approaches better in finding good approximation sets than traditional design of experiments (DoE) techniques, e.g., LHS (see Section 2.3)? Are there significant differences between the different EGO approaches?The paper is structured as follows: in Section 1.2 we highlight related approaches. The basic idea of surrogate-based optimization is described for single-criteria problems in Section 2.1, and followed by a general introduction of MCO in Section 2.2. In Section 3 we describe the setup of the study for efficient two criteria ML experiments. The experimental results are discussed in Section 4 and we give concluding remarks in Section 5.

@&#CONCLUSIONS@&#
