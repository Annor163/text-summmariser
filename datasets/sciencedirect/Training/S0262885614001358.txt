@&#MAIN-TITLE@&#
Adaptive visual tracking using the prioritized Q-learning algorithm: MDP-based parameter learning approach

@&#HIGHLIGHTS@&#
We use an MDP formulation for optimal adaptation of tracking algorithms.We optimize the tracker control parameters using prioritized Q-learning.The proposed prioritized Q-learning approach is based on sensitivity analysis.The performance of our method is superior to other approaches.The proposed method can balance tracking accuracy and speed.

@&#KEYPHRASES@&#
Adaptive visual tracking,Prioritized Q-learning,Markov decision process,Dynamic parameter optimization,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Visual tracking technology is becoming increasingly important in diverse real-world applications. Visual tracking deals with the problem of arbitrary object motion estimation using image streams, and is one of the most complex problems in the field of computer vision. A number of visual tracking approaches have been intensely studied for decades [1], and some of them have been used successfully in many different tasks, such as video surveillance, gesture recognition, and human interaction. Early tracking methods relying on fixed appearance features of an object can only be used in strongly limited application environments [2]. Such approaches mostly construct an object model first, and perform tracking tasks without considering changes in object appearance or surroundings. The invariable and fixed feature based approaches are very often fragile and unacceptable, since dynamic changes in object appearance, illumination and background clutter can hardly be avoided in real-world applications. Most of them cannot observe the object of interest very well and encounter performance degradation or even corruption. Efficient visual tracking requires more flexible appearance models and should be able to handle variations in object appearance [3].Many researchers have tried to solve the appearance variability problem using the adaptive template tracking approaches, the generative model, the correlation-based environment model [4,5], discriminative trackers, and the hybrid of the generative and discriminative models. The adaptive template tracking approaches [6,7] represent the object of interest as a template and minimize the mismatch between the target template and a candidate patch [8]. The combination of static and adaptive template tracking is discussed by Matthews et al. [6] and Rahimi et al. [7]. However, template matching has an intrinsic limitation in model plasticity since only a single template is used as an appearance model. The generative tracking methods are studied for a more flexible model of appearance variation [9–12]. IVT (incremental learning for robust visual tracking) incorporates generative model learning to capture change in object appearance [9]. The generative tracking approaches can handle appearance changes more efficiently than adaptive template tracking, but cannot adequately distinguish target object appearance from the surrounding background, and thus are prone to fail when dealing with a cluttered background. The adaptive discriminative tracking methods [13,14] construct a discriminative classifier during tracking where the decision boundary is dynamically determined based on the updates of positive and negative sample patches. The discriminative model-based trackers model the surrounding environment as negative samples, and efficiently handle both appearance variation and background clutter by learning the decision boundaries between them [16–19]. Some approaches incorporate an auxiliary classifier [19] or a classifier combination [20] to obtain more stability in tracking performance. The hybrid trackers combine the discriminative and generative observation models, e.g., the discriminative model bootstrapped by a generative approach [21], the multiple generative models with discriminative learning in each frame [22], and the generative model for the long-term and the discriminative model for the short-term [23].A most critical problem is to estimate the motion of the object that changes its appearance, and to generate a correct and smooth trajectory. Many adaptive tracking algorithms try to solve variances in object appearance by incremental model updates [3,21]. They usually initialize an object appearance model in the beginning, and then incrementally update the model considering the variations of the object appearances from the changes, such as pose, scale, shape, illumination, or surroundings. However, the flexible structures themselves produce non-trivial performance-sensitive tracker control parameters, such as thresholds and adjusting parameters. Most adaptive tracking algorithms are exposed to a wide range of randomness in controlling their parameters, and fully automatic adaptation of an appearance model is not straightforward since the optimization of tracker parameters is intractable, in general. To the best of our knowledge, the determination of tracker parameters mainly relies upon experiments using a limited data set, and tracker performance cannot be guaranteed in real-world applications where a wide range of appearance variations occur. Thus, the optimization problem of tracker control parameters in connection with the variance modeling of object appearances needs to be addressed when constructing an adaptive and robust visual tracker.This paper is motivated by the fact that tracking performance degradation is caused from not only changes in object appearance but also the inflexible controls of tracker parameters and undesirable interaction effects between them. To the best of our knowledge, most researchers in adaptive visual tracking focus only on efficient adaptation of the appearance model [3,24]. However, the interaction between tracker control parameters and appearance model is not investigated, even though they greatly influence tracker performance. For example, after a successful update of an appearance model under environment changes, the threshold parameter decision of the effective discriminative classifier very often encounters another adaptation problem in the tracker control parameters, which are sensitive to the changes. An open challenge is to equip a visual tracking algorithm for a more flexible structure that can handle both the object appearance model and tracker control parameters. The main issues are how to describe the level of tracker algorithm flexibility using a set of parameters, and how to optimize the adaptive appearance model and tracker by controlling tracker parameters.In this paper, we present an optimization technique for an efficient tracker ensemble by combining the adaptive appearance model and the optimization capability of the Markov decision process (MDP) as shown in Fig. 1. MDP-based optimization approaches have been applied successfully in many dynamic systems exposed to uncertain environments [25]. The optimal modeling of object appearance is formulated as an MDP-based dynamic parameter optimization problem with uncertain and incomplete information. The agent optimizes tracker control parameters by interacting with the tracker ensemble such as the thresholds for a candidate area decision, a positive/negative patch determination, a strong/weak confidence candidate decision, and the tolerable distance between the motion- and model-based trackers. However, the decision of an optimal visual tracking structure for an adaptive appearance model is a very complicated problem, even though it is a promising direction for solving the uncertainty of adaptive appearance models [26–28]. Most parameter control approaches require precise models that describe interactions with environments, and it is uncertain and intractable to model such interactions, especially for real-time tasks. The uncertainty of optimizing the control parameters in a visual tracker is formulated as a sequential decision problem based on an MDP, with only incomplete information about tracker control parameters. The key problem is to solve the conflict between exploiting and exploring the behavior of the adaptive visual tracker to obtain an optimal performance. This kind of conflict usually requires intractable computation time, and no general mechanism has been invented yet. Either a Bayesian or a non-Bayesian method can be employed to adapt the MDP to the randomness of tracker control parameters with incomplete information [26]. The agent can optimize the visual tracking algorithm by keeping track of histories. However, strict real-time constraints on real-world visual tracking tasks prohibit the Bayesian approaches, which rely on known prior distributions from the histories, although much research into online Bayesian approaches has been done [29,30].The huge computation overhead of the MDP formulation is solved by the prioritized Q-learning approach, which approximates one-step Q-learning in real time based on parameter sensitivity analysis. The one-step Q-learning algorithm, a reinforcement learning method, is a direct non-Bayesian approach based on asynchronous dynamic programming (DP) [31,32] since the indirect methods [26] can only be used in the limited scope of real-time systems with sufficiently narrow horizons. One-step Q-learning produces decision processes without an explicit system model. However, an enormous amount of learning time cannot be avoided in finding the optimal performance due to the curse of a huge search space of the tracker control parameters. The proposed prioritized Q-learning algorithm explores the uncertainty in dynamic changes for performance optimization and guarantees in real-time by balancing tracking accuracy and real-time constraints. The major contributions of this paper are as follows:a)Optimal modeling of object appearance is formulated as an MDP-based dynamic parameter optimization problem with uncertain and incomplete information, and the optimal performance guarantee for the adaptive visual tracking algorithm is achieved by the MDP-based interactive framework, which combines adaptation of the appearance model with that of the tracker control parameters. The interactive relation between appearance model and tracker control parameters is investigated thoroughly, and tracker performance is optimized.The non-real-time property due to the huge search space is partitioned into low-dimensional smaller search spaces based on parameter sensitivity analysis and approximated by the proposed prioritized Q-learning, which leads to a significant reduction of the search space. It can efficiently balance tracker performance in accuracy and speed, limit the number of trials, and accelerate convergence to an optimal tracker performance.The proposed framework fully utilizes the interactive characteristics of the MDP framework by defining the prioritized Q-learning environment as tracker control parameters with incomplete information rather than the environment using the input image directly, as described by Bhanu and Peng [28] and Peng and Bhanu [33]. The proposed method explores the interactive synergy relation between the adaptive appearance model and tracker control parameters, and thus provides very flexible and high performance in the visual tracking scheme based on the MDP formulation.In Section 2, the optimal adaptive visual tracking approach is discussed. The MDP for optimal visual tracking is given in Section 3. Tracker parameter optimization using prioritized Q-learning is discussed in Section 4. In Section 5, experiment results are given, and concluding remarks and future directions are discussed in Section 6.In this section, we discuss adaptive visual tracking based on discriminative classification that interacts with the MDP-based optimizing functionality. Given a sequence of image frames with an initial bounding box that defines the object of interest in the first image frame, the goal is to find an object trajectory by estimating the object motion in terms of the consecutive object's bounding boxes. The object of interest is represented by geometric positions, shapes, and scales. In this paper, the motion- and model-based tracking algorithms are combined. The model-based algorithm relies upon the online discriminative classifier update at each video frame. The object of interest is initialized as a bounding box, and the positive and negative object patches are generated for the appearance model updates at each time-step. The positive and negative image patches are used to update the classifier of the model-based tracker during the tracking process. The Kalman filter is employed for the motion-based algorithm, which predicts the next tracking point based on object motion. The performance of the optimal adaptive tracking algorithm relies upon the plasticity of the appearance model update and the accuracy of the tracker ensemble, which is optimized by the MDP framework.We discuss the optimal adaptive tracking process based on MDP formulation. Given image sequence I={Ik}k=1Kand an initial bounding box, the tracker estimates a sequence of bounding boxes. A bounding box is represented by a target object state, which includes the position and other characteristics, such as scale and orientation with respect to image axes. The adaptive visual tracking system estimates a sequence of random variables sk∈Rdfor k=1,⋯, K, where d is the dimension of the object state space. Formally, the target object state at time-step k is represented as(1)sk=xyσxσyθ,where (x, y) is the centroid of the target object; σx and σy denote the aspect ratio between the initial and the current width and height of the bounding box, respectively; and θ ∈ [0 . . . 2π] denotes the orientation [3].The trajectory, i.e., the sequence of the object state, is denoted byT={sk,k∈N}k=1K, where skis an object state at time-step k, and N is the set of natural numbers. The trajectory is fragmented if the target object cannot be tracked or is not visible. An object state is defined as an optimal object state if it belongs to an optimal trajectory. An optimal trajectory T⁎ is a sequence of the optimal target object states, i.e., T*={sk⁎}k=1K, where sk⁎ is an optimal state at time-step k. In the proposed method, we construct a tracker ensemble, which combines the model-based tracker using distance metrics and the motion-based tracker. A similar strategy can be found in the tracking-learning-detection (TLD) approach [24], however, the main difference is that our method adapts the associated parameters flexibly during run-time, while TLD can use only fixed parameters determined empirically offline. The proposed tracking method produces a most-likely bounding box by selecting the highest confidence score based on the results of the motion- and model-based trackers. The bounding boxes from the motion- and model-based trackers are combined into a unique bounding box to construct a new part of the trajectory. During tracking, the parameters of the tracker ensemble algorithm are determined by interacting with the agent to explore an optimal visual tracking performance. The main parts of the adaptive visual tracking are discussed in this section, and optimization of the visual tracking is discussed in Section 4.Given an initial bounding box at the initial time-step, the initial appearance model M0 (the sets of positive and negative patches) is generated, the model-based tracker is built using M0, and the tracker ensemble begins the task of tracking. The model-based tracker rebuilds its discriminative classifier at each time-step using the appearance model, which is updated continuously as follows. Let Mkdenote the current appearance model at time-step k. Mkconsists of the set of positive patches MkPOSand the set of negative patches MkNEG. Note that the discriminative classifier for the model-based tracker is built using Mkat time-step k. Let mk+1 denote a newly generated appearance model at time-step k+1, and mk+1 consists of the set of new positive patches mk+1POSand that of new negative patches mk+1NEG.The distance metric between an object state skiand an optimal object state sk⁎ is defined by(2)ski−sk*=λ1xyki−xyk*+λ2σxσyki−σxσyk*+λ3θki−θk*,where ‖⋅‖ indicates Euclidean distance, and λ1, λ2, and λ3 are weight parameters. Then, mk+1POSis defined as follows:(3)mk+1POS=WarpPatchski:ski−sk*<ηPPTH,where Warp(Patch(ski)) indicates positive patch images within distance ηPPTHand are synthesized using geographical transformations. ηPPTHis a threshold parameter that controls positive patches and is adjusted by the MDP during tracking. Similarly, mk+1NEGis defined by:(4)mk+1NEG=WarpPatchski:ηPPTH<ski−sk*<ηPPTH,where ηNPTHis a threshold parameter that controls negative patches and is flexibly adjusted by the MDP.Now we will discuss the synthesis of positive and negative appearance models. Given the initial bounding box (initial image patch), the positive and the negative patches are generated as follows. The image patch of an object state s0⁎ in the first frame is denoted as Patch(s0⁎). The initial object state, which is the starting point of the trajectory, is represented as s0⁎=(x0,y0,σx=1,σy=1,θ=0), where (x0,y0) is the central position of the initial target object. After the optimal object state is determined at time-step k, the appearance model for time-step k+1 is generated by cropping and synthetic steps. At time-step k, mk+1 is decided from the optimal object state sk⁎. We crop the set of positive patches that satisfy the following condition:(5)mk+1CropPOS=Patchskiski−sk*<ηPPTH.For each cropped patch Patch(ski), we predict, hopefully, the next positive appearance patches by warping the cropped positive patches as follows. We generate the positive patches for the appearance model used in the k+1 time-step by geometric transformation as follows:(6)mk+1POS=WarpPatchski:Patchski∈mk+1CropPOS,where we generate warped positive patches based on geometric transformations as follows:(7)WarpPatchski=1+g1−g2g2+1+g1xy+g3g4,where the four parameters g=(g1,g2,g3,g4)Tare the geometric transformations defined in the homogeneous coordinates for each cropped patch, i.e., scaling, 2-D rotation matrix, x and y translations [6].Negative patches are initialized by cropping adjacent patches surrounding the initial bounding box, and geometric transformations are applied for each negative patch to generate syntactic negative patches. Formally, at k time-step, we crop the set of negative patches that satisfy the following condition:(8)mk+1CropNEG=PatchskiηPPTH<ski−sk*<ηPPTHWe generate the negative patches for the appearance model used in the k+1 time-step by geometric transformation as follows:(9)mk+1NEC=WarpPatchski:Patchski∈mk+1CropNEG.After the optimal object state is decided at time-step k, the appearance model is updated using Mkand mk+1, and the next appearance model Mk+1 is produced for the discriminative classifier update at time-step k+1. The next appearance model Mk+1 is generated using the models of the positive patches and negative patches, i.e., Mk+1={Mk+1POS,Mk+1NEG}. The Mk+1POSand Mk+1NEGare defined as follows:(10)Mk+1POS=ffilterPOSMkPOS∪mk+1POS,where ffilterPOSis a refinement filter that manages the quality and size of the positive patches. Similarly, the refinement process that manages the quality and size of the negative patches is applied:(11)Mk+1NEG=ffilterNEGMkNEG∪mk+1NEG.The initial optimal object state s0⁎ at time-step t=0, is determined by the initial bounding box. The next optimal object state is determined by estimating a confidence score among the candidate set of the next optimal object states. The candidate set of the next object state is denoted by(12)Ck+1=sk+1ihsk+1isk*=TRUE,where sk⁎ indicates the current optimal object state at time-step k, and h(·) denotes the candidate decision function, which determines a candidate search area within some distance from the current object state using some distance metrics. Given the initial bounding box at the initial time-step, the sets of positive and negative patches M0 generated by the appearance model manager, the tracker ensemble begins the tracking task. The model-based tracker employs k-nearest-neighbor (k-NN) regression as a discriminative classifier using the positive appearance model Mk+1POSand the negative appearance model Mk+1NEG. The k-NN for the positive model calculates the pixel-based Euclidian distances from a candidate patch sk+1i∈Ck+1 to the positive patches in Mk+1POS. INVPOS(sk+1i) indicates the resulting regression value, which is the inverse of the average distance of positive neighbors of sk+1iin Mk+1POS. Similarly, INVNEG(sk+1i), the inverse of the average distance of negative neighbors in Mk+1NEG, is calculated. Then, the weak confidence score of a candidate of sk+1iis defined by(13)CSweaksk+1i=INVPOSskiINVPOSsk+1i+INVNEGsk+1iforsk+1i∈Ck+1The weak candidate set is denoted by Ck+1weakand defined by Ck+1weak={sk+1i|CSweak(sk+1i)≥ηWCTH,sk+1i∈Ck+1},, where ηWCTHis the threshold score of a weak candidate. The allowable range is empirically decided between 0.47 and 0.85, and the optimal ηWCTHis decided by the MDP during run-time.The Kalman filter is employed for the motion-based tracker [34] to approximate tracking the bounding box. The Kalman filter is a recursive approach for the discrete linear filtering problem by estimating a state process that minimizes the squared error [35].Considering strict time constraints, we measure the distance indirectly by the overlap rate of the image patches of sk+1ito sk⁎ as follows. That is, the candidate decision function is defined using the overlap ratio between the current bounding box and possible bounding boxes within some distance. Let us denote the image patch of two object states saand sbas Patch(sa) and Patch(sb), respectively. The overlap ratio OL(sa, sb), which indicates the similarity between two patches of object states saand sb, is defined as the overlapped area divided by the union of the patches. Then the candidate decision function of a next candidate object state sk+1iis denoted by(14)hsk+1isk*=1ifOLsk+1isk*≥ηCATH0otherwise,where ηCATHis a threshold, which is a tracker control parameter and flexibly determined by the MDP during tracking. The optimal target object is a candidate object where the confidence score is the maximum. The optimal target object state at time-step k+1 in the model-based tracker is decided by(15)sk+1*=argmaxsk+1iCSweaksk+1isk+1i∈Ck+1weak.The tracker ensemble produces a bounding box by merging the bounding box of the motion-based tracker with the highest confidence score and that of the model-based tracker. At each time-step, the candidate object states in the previous time-step are remembered for possible failure recovery. There are four cases: i) both the motion- and model-based trackers return bounding boxes, ii) only the motion-based tracker returns a bounding box, iii) only the model-based tracker returns a bounding box, and iv) both the motion- and model-based trackers fail.In case i), the two bounding boxes are averaged into a unique bounding box if the tolerable distance TDkbetween the motion- and model-based trackers is within tolerance, i.e., less than threshold ηTDTH, which is defined as follows:(16)TDk=dkwk+hk,where dkis the distance between the model-based and motion-based trackers, wkand hkare the width and height, respectively. When the distance between object locations of the motion- and model-based trackers is not within tolerance, the tracker ensemble backtracks to the previous time-step for the failure recovery, the next candidate whose strong confidence score is maximum is tried, and so on. The strong confidence score of a candidate sk+1iis defined by(17a)CSstrongsk+1i=INV'POSsk+1iINV'POSsk+1i+INVNEGsk+1i,forsk+1i∈Ck+1where INV'POS(sk+1i) is the inverse of the average distance of 55% of high-ranked positive neighbors of sk+1iin Mk+1POS. The optimal target object state for the recovery at time-step k+1 in the model-based tracker is decided by(17b)sk+1*=argmaxsk+1iCSstrongsk+1isk+1i∈Ck+1strong,sk+1i≠sk+1*.The strong candidate set is defined by Ck+1strong={sk+1i|CSstrong(sk+1i)≥ηSCTH,sk+1i∈Ck+1}, where ηSCTHis the threshold. The allowable range of the strong candidate threshold is empirically decided between 0.61 and 0.79, and the optimal ηSCTHis decided by the MDP during run-time.In case ii), the bounding box produced by the motion-based tracker is used and the model-based tracker is treated as if it made a false negative error. The positive image patches are generated and added to the appearance model-based on the current object position so that the model-based tracker is retrained to avoid the error in the future. In case iii), the result of the motion-based tracker is treated as a false positive, and the bounding box generated by the model-based tracker is used. In case iv), i.e., if neither the motion-based tracker nor the model-based tracker produces an object location, the failure recovery is performed as in case i). If the permissible error recovery time expires, it is reported as a missing target. The optimal algorithm for adaptive visual tracking using the MDP formulation is outlined in Algorithm 1.Algorithm 1Optimal adaptive visual trackingInput: image sequence I={Ik}k=1K, and an initial state s0Output: T*={sk⁎}k=0K.Method:1.Generate the initial appearance model set using the initial state s0⁎.(a)M1POS=ffilterPOS(m1POS)M1NEG=ffilterNEG(m1NEG)Repeat until termination condition is encounteredDetermine the next object state using the model-based tracker from the candidate set.(a)Generate the next candidate set using the current optimal object state sk⁎ using Eq. (12), i.e.,Ck+1=sk+1i:hsk+1isk*=TRUECalculate the weak confidence score, i.e.,Ck+1weak=sk+1iCSweaksk+1i≥ηWCTH,sk+1i∈Ck+1Decide the next optimal object state using Eq. (15), i.e.,sk+1*=argmaxsk+1iCSweaksk+1isk+1i∈Ck+1weakAfter the motion-based tracker is executed,(a)invoke the failure recovery if necessary (Section 2.4),if time limit is reached, declare a “missing target”, orupdate trajectory by adding a new optimal object state:T*=sk*k=1k∪sk+1*=sk*k=1k+1Update the appearance model archive(a)Generate the mk+1POSand mk+1NEGusing sk+1⁎.Mk+1POS=fModelfilterPOS(MkPOS+mk+1POS)Mk+1NEG=fModelfilterNEG(MkNEG+mk+1NEG)Mk+1={Mk+1POS,Mk+1NEG};Update the discriminative classifier using the new appearance model Mk+1 and the tracker control parameters.The tracker control parameters are learned interactively by the MDP to guarantee the optimality of the adaptive visual tracker. Details are discussed in Sections 3 and 4.Much research in adaptive visual tracking focuses on the efficient update of appearance models [3,24], but does not investigate the efficiency of the tracker from the point of algorithm optimization. To the best of our knowledge, the interactive relation between appearance model and tracker control parameters has not been thoroughly investigated [3] even though it greatly influences tracker performance. It is necessary to adapt not only the object appearance model but also the tracker control parameters to obtain an optimal tracker. In this section, we discuss an MDP formulation for adaptive visual tracking, where the parameter set of the visual tracking algorithm defines the states of the environment, and interacts with the agent to explore an optimal parameter space.The agent uses interaction history such as actions and rewards distributed over the states. We introduce an MDP that formulates the optimization problem of adaptive visual tracking as a sequential decision process of the control parameters in accordance with uncertain environments. The state space of the MDP, the basis for making the decisions, is defined as the space of the tracker control parameters. The actions are the decisions of the tracker parameter adjustments made by the agent; the rewards are the evaluation basis for the decisions. The return function is defined as the estimation of future rewards, i.e., tracking performance that the agent wants to optimize. In the MDP framework, the agent takes the best next action, moves to the most promising state of the adaptive visual tracking system, and updates its distribution. The agent's objective is to maximize the return function, so it selects actions as a function of states based on a stochastic rule.Formally, the standard MDP is formulated as a tuple {X,A,T,R,γ}. The state space X is defined as a finite set where each state represents the aspects of the tracker control parameters. The state space X is denoted by X={X1,…,XL}, which can be thought of as the sets of random variables in the tracker control space. The discretized domain of possible values for a random variable Xiis denoted by Di. The tracker state is defined by assigning a value to each variable Xi, denoted byx={X1=x1,…,XL=xL}. Note that Xidenotes a threshold value or a system parameter in the visual tracking system. Thus, the statexin the space X is denoted by(18)x=xixi∈Dii=1L,where each dimension xiis a scalar that represents some aspect of the agent's environment, i.e., a tracker control parameter denoted by a random variable Xi. The action can be any decision that the agent needs to learn. Note that a state can be any factor that influences the agent's decision making. Here, the action is defined as the move to a next state that adjusts tracker parameters. An actiona∈A(x) denotes a decision of adjusting the control parameters of the adaptive visual tracker in the state space X at time-step t, where a finite set of actions available at each state is denoted by action space A. The action space A is denoted by A={A1,…,AL} where Aiis the horizon of a scalar action for xi∈Xi. The possible value set for action horizon Aiis denoted by Ei. The actionain the space A is denoted by(19)a=aiai∈Eii=1L,where each dimension aiis a scalar that represents the adjustment action of the agent for the tracker control parameter denoted by a random variable Xi. The probability function T defines a probability distribution over the next statex' of the state transition, given the state of the current tracker parameterxand an actiona, i.e., T:X×A×X↦[0,1]:(20)Τx,a,x'=Pxt+1=x'xt=x,at=a,where ∑x'∈XT(x,a,x')=1. The T indicates the uncertainty of the state transition by specifying the probability of moving from s to a next statex', and is assumed to satisfy the Markov property [25]. The reward function, R:X×A→ℜ, indicates an immediate payoff by taking actionain the current statexand the taken actiona. It is denoted by r=R(x,a). Here, the reward function is measured by tracking performance. Discount factor γ∈(0,1] is used as a discount rate for future rewards. It is interpreted that a reward received (k +1) time-steps later is only worth γktimes, if it were obtained immediately. The discount factor is used for balancing a tradeoff between short-term and long-term rewards when the current action is selected. We empirically determined the discount factor as 0.9. The objective of the MDP is defined based on the reward function and discounted factor to maximize the cumulative reward at time-step k, and is measured by discounted expected reward, i.e., E{∑k=1∞γkrt+k+1}.The standard MDP must satisfy the Markov property from a theoretical point of view. Most complex and realistic applications cannot satisfy the strict Markov property. However, the MDP can use only incomplete information since tracker control parameters are influenced by factors that are not controllable by the agent. There are two approaches in solving an MDP with incomplete information: Bayesian and non-Bayesian methods [26]. Bayesian approaches assume accumulated observations over stochastic dynamic distributions, which can be described using Bayesian rules. They are hardly applicable to adaptive visual tracking, which requires strict real-time constraints; even many online Bayesian approaches are studied [29,30]. Hence, we select a reinforcement learning (RL) approach, which is an adaptive non-Bayesian and a direct approach of asynchronous dynamic programming (DP) [31,32] since indirect non-Bayesian methods such as adaptive RTDP (real time dynamic programming) [26] require too high computation overheads to be used in real-time systems if a sufficiently narrow horizon is not available. The role of the RL approach is to solve the MDP problem from a viewpoint of long-term performance optimization based on the feedback of one-step immediate reward from the adaptive tracker.The RL approach is a straightforward framing of the problem of learning from interaction to achieve a goal, where the main issue is how to map situations to actions so as to maximize a numerical reward signal. Actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. The RL algorithm can control the adaptation of the tracker control parameters as a consecutive decision process via the MDP state transition, to obtain an optimal performance. The MDP model needs to justify the use of the RL approach for adaptive tracking optimization as the events of consecutive trials for deciding the MDP state observed in discrete time space. The RL approach relying on the Markov assumption is very useful in solving many real-world problems by approximating system behaviors and has been successfully applied [37]. The MDP formulized for adaptive visual tracking also relies on the assumption of the Markov property. The critical issue is to solve the conflict between controlling the tracker control parameters and exploiting its behavior with only incomplete information to obtain an optimal trajectory. Such kinds of conflict usually require very high computation requirements over a huge search space, which is intractable, in general.In this paper, we factorize the MDP so that the optimization problem of the adaptive visual tracking is treated as a sequential decision process of the control parameters influenced under uncertain environments. Recall that the state space S of the MDP is defined as the space of the tracker control parameters. The set of actions A denotes the adjustments of the tracker control parameters in the state space. However, tracker optimization can hardly be solved in real time using the standard MDP approach, since the search space grows exponentially with the number of states |X|. Instead of exploring the whole space directly, we partition the search space into smaller horizons with priorities which are empirically determined by parameter sensitivity analysis (Section 4.2). The proposed method might be less accurate in the tracking, owing to the partitioned approximations of the search space, but we can dramatically reduce the computational overhead to satisfy real-time constraints. Formally, the MDP is factorized into L MDPs as follows:(21)MDPi=XiAiTiRiγifori=1…L,where the state Xiis denoted by Xi=xiwith xi∈Diand the action of Aiis denoted by Ai=yiwith yi∈Ei,Τisas′=Pst+1=xi′st=xi,at=ai, and r=Ri(xi,ai). The reward is empirically defined as 1 if CSweakis greater than 0.45, otherwise it is −1 [46].We employ Q-learning algorithm as a RL approach to solve each factorized MDP which does not require an explicit specification of the transition probabilities. The online policy is approximated by the proposed prioritized Q-learning instead of standard Q-learning [25] considering the strict real-time constraints in the visual tracking applications. Temporal difference (TD) learning, one of the most successful RL approaches in solving MDP problems, can be used to provide an online, fully incremental learning capability, since the TD approach learns from each transition regardless of the subsequent action taken [25]. TD learning is a direct non-Bayesian method that produces decision processes without any explicit model. TD algorithms do not need any knowledge of probability distribution, such as state transition, but learn directly by experience. We chose Watkins' one-step Q-learning [32] which converges relatively faster than other TD approaches [25]. The one-step Q-learning algorithm estimates optimal action–value functions Q⁎, and is characterized by a model-free, off-polish strategy, simplicity, and effectiveness [38]. The one-step Q-learning accumulates optimal actions of states by evaluating action–value Q(s, a) to construct state–action functions. The action–value function of the one-step Q-learning learns optimal action–value functions without considering any policies, simplifies the dynamic analysis processes, and thus enables early convergences [39]. The detail of the prioritized Q-learning algorithm for the factored MDPs is discussed in the next section.Most adaptive visual tracking systems have system control parameters such as thresholds and adjusting parameters, which are sensitive to varying object appearance and environments. Even though they often greatly influence tracking performance, many adaptive tracking approaches, such as the generative algorithms and discriminative algorithms, do not focus on optimization of the tracker control parameters. The proposed visual tracking system formulizes tracker optimization as a decision sequence problem of tracker control parameters using the MDP. The optimization of visual tracking is formulated as a tracker parameter optimization problem with uncertain and incomplete information. Prioritized Q-learning is proposed to handle the huge computation requirements of the MDP by balancing tracking accuracy and real-time constraints.The tracker control parameter space of the adaptive tracking system is represented by random variables in the state space of the standard MDP:(22)X=XCAXPPXNPXSCXWCXTD,where XCA,XPP,XNP,XSC,XWCand XTDindicate the parameters for a candidate area decision, positive patch determination, negative patch determination, strong confidence candidate decision, weak confidence candidate decision, and tolerable distance, respectively. Optimal adaptive tracking is now reduced to the optimization of the tracker control parameters using the MDP. Table 1summarizes parameter notations and descriptions of the adaptive visual tracker.We need to evaluate how much each control parameter influences tracking uncertainty in order to apply prioritized Q-learning (Section 4.3). Assuming the tracker control parameters are independent of each other, the priority of the control parameters is determined by sensitivity analysis that ranks the importance of the tracker control parameters in determining output. Our analysis is based on the one-factor-at-a-time (OFAT) method [36], which is simple but the most common approach. Recall that the domain of a random variable Xiis denoted by Di. We empirically determined the lower and upper bounds of each discretization domain. The domain resolution is fixed at 20 for fair comparison in sensitivity between system parameters. The parameter sensitivity analysis based on OFAT is discussed next. We selected one tracker parameter, keeping others as nominal values, decide the nominal value of the selected parameter, and return the parameter as its nominal value. The above procedure is repeated for other tracker parameters until we obtain satisfactory distributions for each parameter. In Table 2, the domain boundary values of dominant parameters, the normalized average, and their variances are given.After the sensitivity of tracker parameter horizons is empirically analyzed, the parameter horizons are rearranged according to sensitivity in decreasing order. That is, the tracker control parameter space is reordered as follows:(23)X=XWCXTDXCAXNPXPPXSC.The one-step Q-learning learns the Q-values that back up the estimates of the optimal Q-value for each feasible state–action pair at each time-step. We initialize the Q-values empirically for the tracking performance guarantee in the beginning. Recall that the state s in the space S is denoted byx={{xi}i=1L|xi∈Di} where each dimensionxi∈Xiis a scalar that represents some aspect of the agent's environment, i.e., a tracker control parameter denoted by a random variable Xi, and the action a in the space A is denoted bya={{ai}i=1L|ai∈Ei} where each dimension ai∈Aiis a scalar that represents the adjustment action of the agent for the tracker control parameter denoted by a random variable Xi. Instead of exploring the whole search space of the Q-learning approach, we approximate by factorizing the whole search space into independent horizons, with the assumption that each parameter is independent of the others. Thus, we apply Q-learning sequentially on each parameter horizon until predefined performance criteria are satisfied or the time slice expires.Formally, the prioritized Q-learning is defined as a Q-learning sequence applied to each parameter horizon in decreasing order of performance sensitivity (Eq. (23)). The agent with highest priority observes state x(i,t) at each time-step t and has the estimated optimal Q-values produced for all preceding time-steps from the initialization phase. The agent selects an action a(i,t)∈Aiand executes it, and the agent receives the immediate reward r(a(i,t)) while the system state transits to x(i,t+1). Let Q(i,t)(x(i,t),a(i,t)) denote the estimate of Q(i,t)⁎(xi,ai) at time-step t. For each time-step t=0, 1,⋯, let StQ⊆{(x(i,t),a(i,t))|x(i,t)∈Xi,a(i,t)∈Ai} denote the set of feasible state–action pairs whose Q-values are updated at time-step t. For (x(i,t),a(i,t))∈XiQ, the Q-value at stage t+1 is backed up as follows [26]:(24)Qt+1xitait=1−αtxitaitQtxitait+αtxitaitrait+γftxi,t+1whereftxi,t+1=maxa∈AiQtxitaitand αt(x(i,t),a(i,t)) are the learning rate parameters at time-step t for the current scalar state–action pair. The Q-values for all the other admissible state–action pairs remain the same, i.e., Qt+1(xj,aj)=Qt(xi,ai) for all admissible (xj,aj)≠(x(i,t),a(i,t)). Note that an optimal policy is determined without constructing any model of the underlying MDP.The prioritized Q-learning is the subcase of one-step Q-learning where the Q-values XtQare learned independently at each time-step t. Individual Q-learning in the prioritized Q-learning converges to the optimal values under the conditions required for the convergence of one-step Q-learning [26]. One-step Q-learning convergence was proved by assuming that each feasible action is backed up in an infinite number of time-steps, and the learning rate α decreases over the time-steps t[32]. The prioritized Q-learning as one-step Q-learning has advantages in real-time applications since it requires fewer backups than that of asynchronous DP backup [26]. Let N and M be the number of the states and the maximum number of actions, respectively. The prioritized Q-learning backup requires only O(M), whereas general asynchronous DP backup requires O(MN) in asymptotic time complexity analysis. Even though one-step Q-learning has an advantage in calculating the backups, classic Q-learning requires a huge search space as follows:(25)DminL≤∏i=1LDi≤DmaxL,where |Di| is the dimension size of the discretized domain of a random variable Xiin the parameter space X, and L is the dimension of the state; |Dmin| and |Dmax| are the smallest and the largest dimension sizes, respectively. The worst case time complexity to calculate Q-learning is Ο(|Dmax|L) where ‘O’ is big O notation. On the other hand, the proposed prioritized Q-learning only requires the following search space:(26)LDmin≤∑i=1LDi≤LDmaxTherefore, the worst case time complexity of the proposed prioritized Q-learning is Ο(|Dmax|) which leads to a significant reduction of the search space.The value function defines the mapping from the set of possible states or state–action pairs to its measures of the expected sum of discounted rewards. The key idea of prioritized Q-learning is the use of separate value functions to organize and structure the search for optimal policies. The main steps of prioritized Q-learning are outlined in Algorithm 2.Algorithm 2Prioritized Q-learningInput: Image sequenceOutput: Q-value updates XtQ.Step 1Initialize the time slice.Begin with the highest priority MDP.Repeat until the termination condition is TRUE.(1)Choose y(i,t) from x(i,t) using ε-Greedy policy.Execute the action y(i,t), observe an immediate reward r(i,t) and observe the new statexit′.Update Q-value of MDPiusing the following equation.Qt+1xitait=1−αtxitaitQtxitait+αtxitaitrait+γftxi,t+1x(i,t)'=x(i,t).If the time slice has expired, declare “Time limit encountered” and set the termination condition to TRUE.If the successful tracking criterion is satisfied, declare “Success in tracking” and set the termination condition to TRUE.Otherwise, select the next priority MDP and repeat Step 3.In the prioritized Q-learning algorithm, the agent–environment interaction for each horizon Q-learning is separated into episodes as a standard Q-learning, where an episode is a separated period in the repeated interactions and has starting and terminal states [25]. Time constraint for a real-world application can be controlled by adjusting the time slice, and detailed discussion follows in Section 5.

@&#CONCLUSIONS@&#
