@&#MAIN-TITLE@&#
Refinement of worst-case execution time bounds by graph pruning

@&#HIGHLIGHTS@&#
Recent work showed that often only small parts of real-time programs are relevant for their worst-case execution time (WCET).Following this observation an iterative graph-pruning algorithm is proposed to refine WCET bounds derived by static analysis.The precision of all WCET analysis phases is improved (analyses based on abstract interpretation and longest path search).Evaluation using a commercial, state-of-the-art tool (aiT) using well-established real-time benchmarks (Debie, Papabench).A proof-of-concept implementation shows considerable improvements (up to 12%) and even gains in analysis time.

@&#KEYPHRASES@&#
Worst-case execution time analysis,Iterative refinement,Graph pruning,

@&#ABSTRACT@&#
As real-time systems increase in complexity to provide more and more functionality and perform more demanding computations, the problem of statically analyzing the Worst-Case Execution Time (WCET) bound of real-time programs is becoming more and more time-consuming and imprecise.The problem stems from the fact that with increasing program size, the number of potentially relevant program and hardware states that need to be considered during WCET analysis increases as well. However, only a relatively small portion of the program actually contributes to the final WCET bound. Large parts of the program are thus irrelevant and are analyzed in vain. In the best case this only leads to increased analysis time. Very often, however, the analysis of irrelevant program parts interferes with the analysis of those program parts that turn out to be relevant.We explore a novel technique based on graph pruning that promises to reduce the analysis overhead and, at the same time, increase the analysis’ precision. The basic idea is to eliminate those program parts from the analysis problem that are known to be irrelevant for the final WCET bound. This reduces the analysis overhead, since only a subset of the program and hardware states have to be tracked. Consequently, more aggressive analysis techniques may be applied, effectively reducing the overestimation of the WCET. As a side-effect, interference from irrelevant program parts is eliminated, e.g., on addresses of memory accesses, on loop bounds, or on the cache or processor state.First experiments using a commercial WCET analysis tool show that our approach is feasible in practice and leads to reductions of up to 12% when a standard IPET approach is used for the analysis.

@&#INTRODUCTION@&#
Real-time systems have seen a steady increase in complexity during the last decades [1]. Due to the increased processing power of modern processors, more elaborate algorithms are implemented, new functionality added, and existing functionality migrated to software. As the complexity of real-time software grows, the accurate static analysis of the software becomes more and more demanding. This is particularly problematic for real-time systems, where a provable bound of the program׳s execution time — its Worst-Case Execution Time (WCET) — is needed to guarantee the timely operation of the system.Foremost size and complexity of software are causing the analysis overhead to grow rapidly, as the number of potential states of the program under analysis increases. This is even amplified for WCET analysis, as the number of potential software states is further increased by potential hardware states, which also have to be considered to safely bound the WCET. Even when only small portions of a complex software program are relevant for the final WCET estimation, the analysis has to account for all of the program׳s code to derive a safe bound. This often reduces the precision of the WCET analysis, as irrelevant code parts interfere with the analysis of relevant code parts and lead to unnecessary overestimation of the statically determined WCET bound compared to the actual worst-case behavior.To address these two issues, we propose a novel technique based on graph pruning. An iterative algorithm allows us to discard irrelevant program parts and apply a standard WCET analysis to the remaining (relevant) program parts only. The iterative processing ensures safe bounds, while focusing only on relevant program parts reduces the size of the analysis problem and promises to reduce the overall analysis overhead. At the same time, more aggressive analysis techniques can be applied to the smaller remaining program. Together with the reduced interference from irrelevant code parts, this leads to a more precise and tighter WCET bound.The basic idea of our approach is to find the longest path for every basic block in the Control-Flow Graph (CFG) of a real-time program [2] using a fast (and potentially imprecise) WCET analysis. The basic blocks are grouped into sets according to the length of their respective paths. The sets are then processed iteratively by decreasing path length. During each iteration a subgraph of the original CFG is formed by unifying the subgraph of the previous iteration with the basic blocks from the currently considered set. A potentially more advanced WCET analysis is then applied to the program represented by the new subgraph. The algorithm terminates, with a possibly refined WCET estimate, as soon as a safe bound, valid for the original program, has been reached.The advantages of this approach are twofold. First of all, the analysis problems defined by the subgraphs at each iteration are much smaller than the original analysis problem. This promises to reduce the analysis overhead, while still providing tight bounds. Secondly, processing the sets of basic blocks according to their decreasing path lengths, eliminates the interference from other basic blocks, whose longest paths are known to be shorter. This improves the precision of the WCET analysis precisely for those code parts of the real-time program that impact the WCET estimate the most.We evaluate our approach using the well-established benchmark programs Debie1 [3] and PapaBench [4] for two embedded PowerPC processors from Freescale. The WCET analysis is performed by an unmodified version of the commercial tool aiT11http://www.absint.com/ait/using scripting and analysis annotations. Our approach treats the WCET analysis tool as a black box and is thus compatible with other tools. Our experiments show improvements of up to 12%, depending on the processor model, when a standard implicit path enumeration approach [5] is used. Targeting an advanced analysis technique, which accounts for architecturally infeasible execution paths, but does not scale to large analysis problems, the improvement is still as high as 5%. We also notice a general trend that the benefit from our pruning method increases with the size of the analysis problems. This property is promising, since it suggests accelerating returns for future analysis applications. Since we use an unmodified, and for our purposes unoptimized, version of the analysis tool, the iterative processing causes some overhead. We expect that this overhead can be eliminated by adapting the WCET analysis tool to the iterative processing, for example, by using incremental analysis techniques.The main contributions of this paper are as follows:•We present a novel WCET analysis technique based on graph pruning that focuses the analysis effort on relevant code parts of the real-time program.Due to reduced analysis overhead, more elaborate analysis techniques can be applied to the smaller sub-programs, leading to improved precision.We evaluate our approach using a commercial off-the-shelf WCET analysis tool and demonstrate considerable improvements of WCET bounds.The remainder of this paper is structured as follows. We first give some background and motivation in Section 2. We then describe our novel graph pruning technique in Section 3. Section 4 presents a detailed evaluation of our approach for two well-established real-time benchmarks and a realistic processor architecture. Related work is covered in Section 5 before concluding in Section 6.This section covers some basic definitions of control-flow graphs, paths, and WCET analysis, followed by a brief discussion of recent findings motivating this work.We assume that static WCET analysis proceeds in two phases as proposed by Theiling et al. [6]: (1) local worst-case execution times of individual basic blocks and control-flow edges are computed first; this is followed by (2) a longest path search over a weighted CFG, where the weights are given by the local WCETs.Weighted control-flow graph:A weighted Control-Flow Graph (CFG) is a tupleG=(V,E,r,t,W), where V is a set of nodes representing basic blocks that are connected by control-flow edges in E. We assume that every CFG contains a root node r and sink node t. The functionW:V∪E→Rassociates the basic blocks and edges with a weight.An ordered sequence of nodes(v1,…,vn), such that for0<i<nall edges(vi,vi+1)are in E, is called a path. The length of a path|p|is given by the sum of all its node and edge weights:∑0<i≤nW(vi)+∑0<i<nW((vi,vi+1)). A path p is a longest path, when there exists no other path q such that|p|<|q|.The weight of individual CFG nodes and edges is usually computed using abstract interpretation [7] that derives information on potential program and processor states at all relevant program points. This information is then combined to compute an upper bound of the local execution time of each basic block and CFG edge.From the local execution times a weighted CFG is constructed and longest path search is performed to find a path with maximal cost, thus representing the global WCET bound. We rely on the Implicit Path Enumeration Technique (IPET) for this search [5,8], which we explain in detail in Section 2.3.A Worst-Case Execution Path (WCEP) is a longest path in the weighted CFG as computed through longest path search. Solving the IPET problem does not immediately yield a WCEP — rather a family of paths, where each one may contain basic blocks several times, according to their IPET execution counts. Note that concrete paths can be derived from the IPET results, since Kirchhoff׳s law is respected by the execution counts.The WCET of a program can finally be bounded by the length of the WCEP as computed by longest path search. Since it is generally not feasible to compute the actual WCET of a program, we seek a (tight) estimation of this bound. For the purpose of brevity, we will use the term WCET to refer to the WCET bound.Abstract interpretation, introduced by Patrick and Radhia Cousot [7], allows to formally define abstractions of the concrete semantics of a programming language or any other form of program representation — such as compiler intermediate representations. The abstraction is linked to the concrete values through an abstraction function α and a concretization function γ, such that the information in the abstract domain M is always a safe approximation of the concrete semantics in L. We describe the concepts of abstract interpretation required in our work next, see the textbook by Nielson et al. [9] for a more complete introduction.State:A state is a pair(q,σ)∈S, whereq∈Qrepresents a context (e.g., a node in a program׳s CFG), andσ∈Σis an environment, e.g., a mapping of program variables to their values. The set of states is denoted byS=Q×Σ. For the sake of simplicity, we assume that the contexts are the nodes of the program׳s CFG, i.e., given a CFGG=(V,E,r,t):Q=V.The semantics of a program is specified by a semantic functionτ:S→S, which maps an input state to an output state, depending on the input state׳s context and environment, i.e., variable assignments. For the sake of brevity we will only consider forward semantics here, the following definitions equally apply for backward semantics.The trace semantics represents all possible executions of a program as a (potentially infinite) sequence of state transitions and is defined by the closure functionT(S):S→S+. Here,S+represents the set of all possible stringss1,s2,…over the program states,si∈S,∀i>2:τ(si−1)=si. Given a set of initial statesSinit⊆Sthe reachable states of a program can be defined asR={s|∃sinit∈Sinit:s∈T(sinit)}.The collecting semantics ignores the ordering in which states are visited and thus can be expressed as the least fixed point ofCS(S)=Sinit∪S∪{τ(s)|s∈S}. Note that at least the states reachable under the trace semantics (R) have to be reachable under the collecting semantics (but not necessarily the inverse).A Galois connection is a tuple(L,α,γ,M), where(L,⊑)and(M,⊑)are complete lattices representing the concrete and abstract program states respectively,α:L→Mandγ:M→Lare monotone functions, such thatγ○α⊒Lλl·landα○γ⊑Mλm·m. Galois connections have several properties that simplify the definition of and reasoning about static analyses [9].The application of a monotone functionf:L→Lin the concrete domain can be approximated by the application of a corresponding functionf^:M→M, iff∀m∈M:α○f○γ(m)⊑Mf^(m). It can furthermore be shown that the least fixed point (lfp) computed over the concrete semantics is then safely approximated by the fixed point computed over the abstraction (and vice versa):lfpf⊑Lγ(lfpf^)(and alsoα(lfpf)⊑Mlfpf^).In practice, a static analysis relies on the abstract domain only, since operating on the concrete domain often leads to undecidable problems. The goal of a static analysis, based on abstract interpretation, then is to compute a safe approximation of the concrete program states reachable under the collecting semantics of the program. Given the program׳s CFG, the abstract domain, and a set of abstract functions representing program semantics, a transition system is constructed that specifies how to derive abstract states for each program point. The transition system in turn allows us to derive an equation system that is solved iteratively by searching a (least) fixed point. Note that the equation system is traditional initialized with the least element (⊥) in abstract interpretation.The properties of Galois connections from above can be used to prove that once a Galois connection can be established between the collecting semantics and the abstraction of a static analysis, the correctness of the analysis follows automatically [7]. For the remainder of this paper, we assume that all static analyses used during WCET analysis are based on abstract interpretation and that these analyses are correct with regard to the underlying collecting semantics (omitting the details of the semantics itself).Based on the local execution times of a weighted CFG (represented as coefficients), and integer variables, which represent execution flow through basic blocks and control-flow edges, IPET specifies an ILP model (given below) for solving the longest path problem. We use functionXto map nodes and edges to their respective ILP variable; together with weighting functionWit directly defines the objective function (Eq. (1)) of the ILP that needs to be maximized. Structural (Eqs. (2), (5)) and non-negativity constraints (Eq. (4)) guarantee proper flow in the CFG, while arbitrary constraints (Eq. (3)), either provided by prior analyses or the user (e.g. loop bounds), restrict the ILP solution to legal execution paths in the program.(1)WCET=max∑e∈EW(e)X(e)+∑v∈VW(v)X(v)(2)s.t.∑e=(u,v)X(e)=∑e′=(v,w)X(e′)(3)∑e∈EaeX(e)○∑e′∈Ea′e′X(e′)+k,○∈{≤,≥,=}(4)X(i)≥0,∀i∈{V∪E}(5)X(r)=1Algorithm 1Basic-block-based criticality algorithm.Require:G=(V,E,r,t)CFG of the input program.Ensure:Crit[v]computed for allv∈V.1:I←AnalyzeAI(G)▹Perform abstract-interpretation-based analysis2:WCETG←CalcWCET(G,I,⊥)3:for allv∈Vdo4:WCETv←CalcWCET(G,I,v)5:Crit[v]←WCETvWCETG6:function CalcWCET(G,I,v)7:C←CreateConstraints(I)▹Flow constraints from analysis info8:C′←BlockConstraint(v)▹Force WCEP over block v9:returnSolveIPET(G,C∪C′)In our recent work [2,10] we have proposed a technique to profile the worst-case behavior of real-time programs using static program analysis. We define the criticality metric for each basic block in the CFG as the length of the longest path passing through a basic block divided by the global WCET. This yields a value between 0 and 1 indicating the relevance of a given basic block in relation to the WCET of the program. Algorithm 1 shows a straight-forward (and suboptimal) approach for computing the criticality metric for each basic block in a program׳s CFG. After an initial static analysis (see Section 2.2) and computation of the WCET bound for the original program (Algorithm 1 l. 2), the longest path through each of the program׳s basic blocks is computed (l. 4). In our concrete case, this is done by adding an additional ILP constraint to the IPET problem. It is generated for block v by BlockConstraint (l. 8) and constrainsX(v)to be greater or equal 1. The length of each restricted longest path in relation to the global WCEP exposes the worst-case relevance of all program fragments (l. 5). Table 1shows the criticality profiles of several WCET analysis problems for the Debie1 and PapaBench real-time programs. From the columnsI0,…,I5, the number of basic blocks in six predefined criticality intervals can be seen. The variation of block distributions hints at an underlying difference of program structure with regard to WCET-critical code. The information we extract in this way can, for instance, be used to guide program optimizations in order to improve the actual WCET of the program.We believe that the criticality metric is not only interesting for program optimization, but can also be used to guide the WCET analysis itself. A large number of basic blocks in Table 1 is relatively unimportant, with a criticality value below 0.8. Only 13% of the basic blocks of the debie-4a benchmark, for instance, are highly critical. The mean percentage of basic blocks, which are highly critical, i.e., have a criticality above 0.8, is 67% (only 54% for Debie1 benchmarks).Considering this observation, one could ask: is it possible to improve the precision and computation time of a WCET analysis by excluding the uncritical code parts from the analysis? Answering this, is precisely the goal of this work. We exclude uncritical code parts by pruning the control-flow graph and iteratively deriving a refined, but still provably correct, WCET bound. A detailed description of this approach follows in the next section.We will first present our pruning algorithm (Algorithm 2), which is then illustrated with the help of an elaborate example (Fig. 1). Then, the algorithm׳s correctness and complexity, algorithm variations and improvements are described at the end of this section.Algorithm 2Graph-pruning algorithm.Require:G=(V,E,r,t)CFG of the input program.S1,…,SnBlock sets sorted by criticality, from highest to lowest.1:ubwcet=02:fori=1→ndo3:ifubwcet≥WCETG(Si)then4:returnubwcet▹Terminate when no longer paths can exist5:LetV′←S1∪⋯∪Si,E′←E∩V′×V′in6:G′←(V′,E′,r,t)▹Construct and analyze a subgraph7:WCETSi←PrunedWCETIGP(G′,Si,ubwcet)8:ubwcet←max(WCETSi,ubwcet)▹Increase until a safe WCET9:returnubwcet10:functionPrunedWCETIGP(G′,Si,ubwcet)11:I←AnalyzeAI(G′)12:C←CreateConstraints(I)▹Flow constraints from analysis info13:C′←BlockConstraint(Si)▹Force WCEP over any block in Si14:returnSolveIPET(G′,C∪C′)In a nutshell, iterative graph pruning (IGP), presented as pseudo code in Algorithm 2, performs WCET analysis on a sorted sequence of basic block sets (S1,…,Sn) and terminates when an upper bound (ubwcet) is found to be a safe bound for the input program represented by the CFG G. Each set Siis made up of basic blocks, which have the same criticality value, based on the computation presented in Section 2.4. The setsSi,…,Snare then sorted based on their criticality from highest to lowest. However, instead of their relative criticality value, Algorithm 2 directly uses the respective WCET bound (WCETvin Algorithm 1) of a set viaWCETG(Si). Thus, the following relationship holds between basic block sets:∀i∈{1,…,n−1}:WCETG(Si)>WCETG(Si+1)At every iteration i, the vertex-induced subgraphG′is created from the union of the i first (and most critical) setsS1,…,Si(l. 5–l. 6).G′is then targeted by a full WCET analysis run (performed by PrunedWCETIGP, l. 10), which entails abstract interpretation on the program׳s subgraphG′to generate the weighting functionW′. This is followed by a constrained longest path search, which only considers those paths passing through blocks in the current set Si(BlockConstraint, introduced in Algorithm 1, here generates an ILP constraint over a sum of basic blocks:∑v∈SiX(v)≥1). All other paths inG′are uninteresting, since these paths have already been bounded in the previous iterations. Note that it is possible thatG′only contains infeasible paths. PrunedWCETIGPthen returns 0 and the current upper bound remains unchanged. If, at the end of an iteration, the current upper bound (ubwcet) is less than the just computed bound (WCETSi), it needs to be updated (l. 8).The algorithm terminates at the latest when all sets have been considered (i.e.,V′=VandG′is the same as the graph of the original program) or before when the termination condition (l. 3) is met. The latter case occurs when the remaining longest path induced by the remaining Si׳s is shorter than the current WCET bound ubwcet. We will prove that ubwcetis a valid bound for G in the next subsection.Example 1Consider the weighted CFG shown in Fig. 1a, where the block weights are shown in the lower right corner. The program is assumed to be executed by a processor equipped with a fully associative instruction cache with a least-recently used replacement strategy that consists of 2 cache blocks and that has a cache-miss penalty of 5. The memory layout of the CFG׳s basic blocks is shown in Fig. 1b, which indicates the assignment of basic blocks (BBi) to cache lines (). The layout was chosen to minimize an assumed average-case execution along the basic blocks BB0, BB2, BB4, BB6, and BB7. It is assumed thatBB3and BB5 are rarely executed in the average case.A first, unmodified WCET analysis performs an instruction cache analysis and a loop bounds analysis. The former determines that the execution of each basic block, except BB5, induces an instruction-cache miss in the worst-case (as indicated by the blue annotationin the upper left corner). The execution of BB5 even causes two cache misses (and). The respective cache-miss penalty is already included in the CFG weights. The loop bounds analysis then discovers that the number of loop iterations at BB4 depends on a variable x either assigned to 10 in BB1 or 7 in BB2 (highlighted in red). The loop bound thus is assumed to be 10.Based on these analysis results, static worst-case profiling is performed (see Section 2.4). The longest path (p1) covers BB0, BB2, BB4, BB6, and BB7 and has length 467 (13+24+10·19+10·23+10). The basic block with the next highest criticality of 0.976 is BB1. Its criticality is derived from path (p2), which is almost identical to p1. It merely passes through BB1 instead of BB2 and has a length of 456. The criticality of BB5 is 0.974, which is induced by path p3 that covers BB5 and all blocks of p1.BB3׳s criticality evaluates to 0.122, induced by a path covering BB0, BB1,BB3, and BB7.This yields four block sets that serve as input for our iterative pruning algorithm:S1={BB0,BB2,BB4,BB6,BB7},S2={BB1},S3={BB5}, andS4={BB3}, which are associated with path lengths of 467, 456, 455, and 57 respectively.In the first iteration the set S1 is used to construct a subgraph G1, shown in Fig. 2a, which is then reanalyzed. In contrast to the initial analysis run, there is only a single assignment to x, limiting the number of loop iterations at BB4 to 7. In addition, the cache analysis is able to prove that BB4׳s cache line () is always present in the instruction cache and thus may never cause a cache miss (indicated by the blue). Similarly, the analysis can prove that BB6 causes a single cache miss only, i.e., cache lineis persistent with regard to the enclosing loop (indicated by the blue). The tighter analysis results reduce the CFG weights and consequently improve the WCET bound of p1 from initially 467 to 276 (13+24+7·14+7·18+5+10). Note that only the reduced weight of BB6 is shown in Fig. 2a, while the path length accounts for an additional cache miss (+5). The upper bound ubwcetis now 276.The algorithm continues, since the path length associated with S2 is greater than the current upper bound (456>276). In the second iteration G2 is constructed fromS1∪S2and reanalyzed (see Fig. 2b). The loop bounds analysis has to consider both assignments to x again and consequently derives the initial loop bound of 10. The cache analysis similarly loses precision. When BB4 is executed it may cause a cache miss. However, it is (like BB6) persistent with regard to the enclosing loop. The longest path in G2 has length 377. However, this path covers all the basic blocks of p1. All executions along paths involving these blocks have been proven to be shorter than 276 in the previous iteration. This bound still holds, since the pessimism introduced by growing G1 into G2 can only impact paths actually passing through BB1. The longest path search on G2 is constrained to only consider these paths. The longest of these paths covers the same basic blocks as p2 and has a length of 366 (initially 456). The path is longer than the previous upper bound, consequently 366 is assigned to ubwcet.The path length associated with S3 is still longer than this bound (455>366), the algorithm thus continues. The next subgraph is constructed fromS1∪S2∪S3, which extends the CFG shown in Fig. 2b to also include BB5. This addition has no impact on the loop bounds analysis. The cache analysis, however, can no longer prove persistence of either BB4 or BB6, since any execution of BB5 evicts all blocks present in the cache. The CFG weights are identical to those of the original graph (see Fig. 1a). In particular, paths p1 and p2 have the same length as in the original graph (467 and 456 respectively). Tighter bounds for both of these paths have been proven before. The path search is thus again constrained to search for new paths whose lengths have not been bounded by the previous iterations. The analysis yields a path equal to p3 of length 455. The analysis was not able to improve the precision of this path. Still, since the path length is longer than the previous upper bound, 455 is assigned to ubwcet.Only one set, S4, remains, which has a path length smaller than the current upper bound (57<455). Since this path length bounds all remaining paths that were not considered during the iterative processing, no new paths can be discovered that could increase the WCET bound determined so far. The algorithm thus terminates and returns an improved WCET bound of 455 instead of the initial 467 (an improvement of 2.6%).One might believe that the above information could equally be derived by a static loop bounds analysis [11,12]. Indeed most analysis tools would compute an interval[7…10]for this example, covering the minimal and maximal number of loop iterations. However, to the best of our knowledge, none of the existing approaches is able to exploit the information that certain information is only valid when a given path is actually executed. A major challenge here is to encode this information as flow-facts, which so far has never been addressed by any work on static loop bounds analysis.Also note that our approach does not aim specifically at loop bounds. Apart from potentially tighter loop bounds, our technique may also yield more precise information on the values of registers and memory locations, branch conditions, addresses of memory accesses as well as the abstract states of the processor pipeline, the caches, and other hardware components.To show the correctness of our approach we have to consider the impact of graph pruning on the typical phases of a WCET analysis run. We assume, without loss of generality, that WCET analysis is performed in two phases [6]. A first phase, based on abstract interpretation [7], delivers local worst-case execution times for each basic block. In the second phase, a longest path search [5,8] is performed on a weighted CFG, computed from these local execution times.In our approach, abstract interpretation is applied to a subgraphG′of the original CFG G of the input program. In order to show correctness we thus have to investigate some properties of the subgraphG′.Lemma 1A subgraphG′=(V′,E′,r,t)constructed by Algorithm2is connected, i.e., for every CFG nodev′∈V′a path from r to t, passing throughv′, exists.This follows immediately from the way subgraphs are constructed. Remember that the blocks in the subgraphG′of the m-th iteration are computed by unifying all the basic block sets Si,i∈{1,…,m}, whose path lengths are longer than the path length associated with Sm, i.e.,V′=⋃i∈{1,…,m}Si.AssumeG′is not connected, i.e., a CFG nodev′has to exist that is not reachable from the root node r inG′. Sincev′∈V′it follows that a corresponding pathp=(r,…,n′,…,t)has to exists in the original graph G. The length of this path corresponds to the path length associated with Sm. AsG′is not connected, at least one node of p is not inV′. However, this is impossible, since the existence of p implies that this node either is in Smor another set Sk,k<m. The subgraphG′is thus connected.□A static analysis, based on abstract interpretation, applied to a subgraphG′=(V′,E′,r,t)delivers correct results with respect to the potential execution paths inG′.We first investigate the impact on the trace semantics and then show that the set of states reachable under collecting semantics is a superset of the states reachable under the trace semantics, i.e., represents a safe over-approximation. The correctness of the static analysis, when applied toG′, then follows from the existence of a Galois connection [7] and the fact that the static analysis has been proven correct with regard to the collecting semantics (see Section 2.2).Trace semantics: The traces associated with the subgraphG′can be characterized by the contexts of the traces׳ states (recall that contexts refer to nodes in G). We can thus define the set of reachable states for all traces along paths inG′asRG′={s|∃sinit∈Sinit:s∈T(sinit)∧∀(q,σ)∈T(sinit):q∈V′}, which is a subset ofR.Collecting semantics: Similarly, the collecting semantics of all executions on any path withinG′can be defined asCSG′(S)=Sinit∪S∪{τ(s)=(q,σ)|s∈S∧q∈V′}. Note that all states inSinitare guaranteed to be covered by the collecting semantics, since the CFG׳s root r is always inV′. It is evident thatlfpCSG′is a subset oflfpCSand thus is also a subset ofR. The set of states reachable under the collecting semantics onG′is, however, not identical toRG′. Instead it is an over-approximation, i.e.,RG′⊆lfpCSG′, as will be shown next. Assume a states∈Sthat is inRG′but not inlfpCSG′. Sinces∈RG′, a traceT(sinit)=s1,s2,…,sk,…has to exist, where all contexts of the states are inV′,sk=s, ands1=sinit∈Sinit. In particular, the initial statesinitis guaranteed to be inCS. The fixed point computation oflfpCSG′would then discover at least one new state of the trace on each iteration and after at mostkiterations would discoversk=s. This contradicts the assumption that s is not inlfpCSG′. It thus follows thatRG′⊆lfpCSG′.The collecting semantics onG′thus is a safe over-approximation with regard to the trace semantics onG′.Correctness: The correctness of any static analysis based on abstract interpretation applied toG′, which has been proven correct with regard to the original collecting semantics, then follows automatically from the existence of a Galois connection [7]. Note, however, the analysis’ results are only sound with regard to the subgraphG′. Naturally, some reachable states in the original program are not reachable when the execution is constrained to a subgraphG′, sincelfpCSG′⊆lfpCS⊆R.□Note that the proof above is very similar to the way traces are discriminated by Rival and Mauborgne [13,14]. The subgraphG′in our case corresponds to a partitioning of traces into two sets: (1) traces executing only instructions withinG′and (2) traces executing at least one instruction that does not belong toG′. The main difference is that in our case the results of the latter partition are not relevant, and thus can be discarded.Another way of defining the semantics of the program induced byG′is to modify the semantic function τ such that any state transition leavingG′leads to a final state. The resulting trace semantics is slightly less precise than the one from above, since the prefixes of traces eventually leavingG′are also considered. On the other hand, the reachable states under the collecting semantics and trace semantics then match. This, in addition, resembles the semantics of traditional program slicing [15].The previous two lemmas ensure that, independent of the concrete analysis performed, the local execution times obtained by abstract interpretation in the WCET analysis tool are sound with respect to a subgraphG′. It remains to show that the WCET bounds computed during the iterative processing are safe. This is done in two steps. First, it is shown that the WCET bound computed for all potential executions along paths in a subgraph still holds in a supergraph. We then show that the longest path search by Algorithm 2 computes a safe bound.Lemma 3The worst-case execution time bounds computed on a weighted subgraphG″=(V″,E″,r,t,W″)still hold for another weighted subgraphG′=(V′,E′,r,t,W′),V″⊆V′,E″⊆E′for all executions along paths inG″.Letp′be a path inG′such that its CFG nodes are all inV″andp″be the longest path inG″that covers the same basic blocks, i.e.,∀v∈p′⇒v∈p″. Assume that an execution exists whose execution time is smaller or equal to|p′|, but is larger than|p″|.The path lengths ofp′andp″might differ due to two reasons: either (1) the local execution time of some nodev∈p′increased, i.e.,W″(v)<W′(v)or (2) some loop bound of a loop containing some nodev∈p′increased, thus allowing v to appear more often inp′than inp″. Both cases imply that some state s was found to be reachable during abstract interpretation onG′that was not reachable onG″. This new state cannot appear in any actual execution trace along any path induced by the basic blocks ofp′, since the abstraction computed onG″already over-approximated all states reached by any trace inG″, including all those alongp′(see Lemma 2). This contradicts the initial assumption.□The worst-case execution time bound computed by Algorithm2for a subgraphG′is safe.Consider the subgraphG′and basic block set Sm, both of the m-th iteration (Sm⊂G′) as well as the subgraphG″of the previous iteration, with its boundWCET(G″).Lemma 3 showed that all paths inG′whose CFG nodes are inG″are uninteresting for our algorithm, since their worst-case execution times have already been bounded by a previous iteration.It remains to bound those paths inG′for which no corresponding path can be constructed inG″, i.e., those paths that contain at least one node in Sm. To bound the WCET ofG′, three cases for the longest pathp′inG′need to be considered:1.|p′|>WCET(G″): Since|p′|is longer than the previously established WCET bound, it follows thatWCET(G′)=|p′|. Suppose a path q exists inG′for which an execution can be constructed whose execution time is longer than|p′|. If q consists of CFG nodes fromG″only, it follows that the WCET of q is bounded byWCET(G″)(see Lemma 3), which contradicts the initial assumption. If q contains at least one node in Sm,|q|is a safe bound for the actual execution time of the execution along q. However,p′is a longest path inG′containing at least one node in Sm. It is thus impossible that|q|>|p′|. It follows that|p′|is an upper bound on the actual WCET of any execution inG′.|p′|≤WCET(G″): As the length ofp′is not longer than the previously established bound, it follows thatWCET(G′)=WCET(G″). The longest path throughG″also represents the longest path throughG′, ignoring any additional overestimation caused by blocks in Sm. The proof is analogous to the case above.No feasible path containing a block in Smexists: This case happens when the abstract interpretation finds that no execution inG′exists that passes through a block in Sm, i.e., none of the conditions of the branches leading to a block in Smcan be satisfied. It follows thatWCET(G′)=WCET(G″). Note that paths over these blocks might become feasible in later iterations, e.g., when code making the, yet unsatisfiable, conditions satisfiable is added. The proof is analogous to the case above.Using induction, we can finally prove that Algorithm 2 delivers safe WCET bounds for all subgraphs considered during the iterative processing.□The previous lemmas prove that applying abstract interpretation on subgraphs is sound and that the algorithm computes safe bounds with respect to the subgraphs considered during the iterative processing. It remains to show that no other paths exist, which could be longer than the WCET bound returned by the last iteration.Theorem 1The last iteration of Algorithm2computes a safe WCET bound.Lemma 4 shows that the WCET of subgraphG′of the m-th iteration is a safe bound with respect to all the execution paths inG′. Assume now that the m-th iteration is indeed the last iteration of the algorithm. Two cases have to be considered: (1) Smis the last block set or (2) the algorithm terminates early (Algorithm 2, l. 3), since the longest path associated with Smis shorter than the boundWCET(G′)returned by the algorithm.In the first caseG′covers all paths in G and thus also safely bounds the length of all these paths (see Lemma 4).In the latter case, only the paths inG′were explicitly bounded during the iterative processing. The remaining paths have to contain at least one node from the remaining block sets. Suppose that q is such a path for which an execution exists whose execution time is larger thanWCET(G′). This path has to contain at least one nodev∈Sk,m<k<n, and consequently|q|≤WCETG(Sk). As the block sets are ordered in descending order, i.e.,∀i∈{1,…,n−1}:WCETG(Si)>WCETG(Si+1), this contradicts the assumption thatWCETG(Sm)<WCET(G′). It follows thatWCET(G′)is a safe WCET bound.□The number n of setsS1,…,Snis an upper bound on the iterations that will be performed by IGP. The former is again bounded by the number of basic blocks in the input program. Since its iterations are linear in the number of blocks, IGP is dominated by the complexity of the WCET analysis, i.e., abstract interpretation and longest path search.The block sets, which are assumed as input in Algorithm 2, can be efficiently computed using the criticality algorithms. This may be performed either during a preprocessing step [2] or on demand [2,10], while the graph pruning algorithm iterates.It may be the case that the WCET analysis tool targeted by graph pruning can be configured for different levels of precision. This usually involves a trade-off between tightness (precision) of the WCET bound and longer analysis runtime. IGP can be used to incorporate analyses varying in precision. Running the higher-precision analysis on a previously pruned graph would be a straight-forward way of further improving the WCET bound. But Algorithm 2 can also be modified to make use of multiple levels of precision directly. To do this, we replace function PrunedWCETIGPin Algorithm 2 with the variant given in Algorithm 3. This algorithm performs a second, more precise WCET analysis to lower the estimate ofWCETSifor the current graphG′, whenever an imprecise analysis would increase the overall WCET bound. For this purpose we assume that PrunedWCETPrecise calculates a WCET bound similar to the steps performed by PrunedWCETIGPin Algorithm 2, but using a higher-precision abstract interpretation and carrying over more information into a larger IPET problem, which thus becomes harder to solve in general. PrunedWCETFast represents our standard less-precise analysis.Algorithm 3WCET computation function PrunedWCETIGP-TSusing two-stage analysis.Require:G′=(V′,E′,r,t)A CFG.SiThe set of newly added blocks.ubwcetThe current upper bound of the WCET.1:WCETSi←PrunedWCETFast(G′,Si)2:ifWCETSi>ubwcetthen3:WCETSi′←PrunedWCETPrecise(G′,Si)4:returnWCETSi′5:returnWCETSiAnother way of incorporating a higher-precision analysis into graph pruning would be to run PrunedWCETPrecise on the pruned subgraph exactly once after the iterative processing. This would reduce the computational overhead and further lower the WCET bound (down to the path length of the next block set). One could even avoid the iterative processing entirely, by heuristically constructing a subgraph and applying the precise analysis to this subgraph. This would foremost reduce the computational overhead and leave the burden of reducing overestimation on the precise WCET analysis.

@&#CONCLUSIONS@&#
