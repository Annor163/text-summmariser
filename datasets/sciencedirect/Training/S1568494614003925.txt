@&#MAIN-TITLE@&#
Binary image denoising using a quantum multilayer self organizing neural network

@&#HIGHLIGHTS@&#
A quantum version of the multilayer self organizing neural network (MLSONN) architecture is proposed.Single qubit rotation gates are designated as the interconnection weights between the different layers.A quantum measurement at the output layer destroys the quantum states.The measured states use linear indices of fuzziness as the system errors.Applications reveal efficient performance over conventional counterpart.

@&#KEYPHRASES@&#
Image denoising,MLSONN,Hopfield network,Fuzzy set theory,Quantum computing,Quantum multilayer self organizing neural network,

@&#ABSTRACT@&#
Several classical techniques have evolved over the years for the purpose of denoising binary images. But the main disadvantages of these classical techniques lie in that an a priori information regarding the noise characteristics is required during the extraction process. Among the intelligent techniques in vogue, the multilayer self organizing neural network (MLSONN) architecture is suitable for binary image preprocessing tasks.In this article, we propose a quantum version of the MLSONN architecture. Similar to the MLSONN architecture, the proposed quantum multilayer self organizing neural network (QMLSONN) architecture comprises three processing layers viz., input, hidden and output layers. The different layers contains qubit based neurons. Single qubit rotation gates are designated as the network layer interconnection weights. A quantum measurement at the output layer destroys the quantum states of the processed information thereby inducing incorporation of linear indices of fuzziness as the network system errors used to adjust network interconnection weights through a quantum backpropagation algorithm.Results of application of the proposed QMLSONN are demonstrated on a synthetic and a real life binary image with varying degrees of Gaussian and uniform noise. A comparative study with the results obtained with the MLSONN architecture and the supervised Hopfield network reveals that the QMLSONN outperforms the MLSONN and the Hopfield network in terms of the computation time.

@&#INTRODUCTION@&#
It has always been a challenging task for the computer vision research community to extract objects faithfully from a noisy perspective. In recent years the growth and use of digital images has seen dramatic proportions. However, only a small portion of these images are relevant or contains the necessary information. The nontrivial contents, usually in the form of interesting objects, are sufficient to represent the semantic meanings in most cases and consequently play an important role in many image applications such as content-based retrieval. Therefore, several classical techniques have evolved over the years targeted in this direction. For example, graph-theoretic approaches [1,2] make use of energy function optimization to solve the extraction problem; edge-linking methods [3] connect a subset of the fragments produced by edge detection to form a closed contour for the interesting object, etc. Although these approaches work well in some cases, the tendency to solve the extraction problem with little consideration of human visual perception makes them to have undesirable performance under some complicated conditions such as in cluttered or noisy images. An effort has been made in this article to denoise binary images from varied noisy perspectives taking into consideration the human visual perception in the form of the nature of noises encountered. The technique employed resorts to a soft computing paradigm inspired by the quantum mechanical principles to achieve robustness and time efficiency.The article is organized as follows. Section 2 provides an in-depth survey on the related works pertaining to object extraction and image denoising available in literature. The focus of research work of this article is mentioned in Section 3. Section 4 elucidates the requisites of the basics of quantum computing relevant to this article. The principle of operation of the multilayer self organizing neural network architecture (MLSONN) [27] is highlighted in Section 5. Section 6 provides an insight into the perspectives of the supervised Hopfield network. Section 7 introduces the proposed quantum multilayer self organizing neural network architecture (QMLSONN) architecture. It also discusses the dynamics of operation and the corresponding network weight adjustment procedures of the network architecture. The experimental results of application of the proposed QMLSONN, MLSONN [27] and the Hopfield [24] network architectures on Gaussian and uniform noise affected artificial and real life binary images are illustrated in Section 8. Section 9 concludes the article with future directions of research.Among various extraction approaches, saliency-based ones perform better as they well accord with human visual perception. Itti et al. [4] combined multiscale features into a single topographical saliency map and adopted a dynamical neural network to select the attended areas that roughly contained the interesting objects. Ma and Zhang [5] generated a contrast-based saliency map and extracted objects by fuzzy growing. Achanta et al. [6] outputted a frequency-tuned saliency map and binarized it with an adaptive threshold. Hou and Zhang [7] constructed the saliency map by analyzing the log-spectrum of the image and used a simple threshold to detect pro-objects. However, nearly all existing saliency-based approaches suffer from the integrity problem, viz., the extracted result is either a small part of the object (referred to as sketch-like) or a large region that contains some redundant part of the background (referred to as envelope-like). Yu et al. [8] proposed an automatic object extraction technique using complementary saliency maps. In this approach, they integrated two kinds of “complementary” saliency maps (i.e., sketch-like and envelope-like maps). Hence, the extraction process is decomposed into two sub-processes, one used to extract a high-precision result based on the sketch-like map, and the other used to extract a high-recall result based on the envelope-like map. Then a classification step is used to extract an exact object based on the two results to do away with the integrity problem.Other efforts in this direction rests on the use of wavelets for the purpose of image denoising and compression [9–11]. Among the filtering approaches, several efforts have also been invested. Smolka et al. [12] proposed a novel approach to the problem of impulsive noise removal in color digital images. The proposed switching filter is based on the rank weighted, cumulated pixel dissimilarity measures, which are used for the detection of image samples contaminated by impulsive noise process. The introduced adaptive design enables the filter to tune its parameters to the amount of impulsive noise corrupting the image. The comparison with existing denoising schemes shows that the new technique more efficiently removes the impulses introduced by the noise process, while better preserving image details. An important feature of the new filter is its low computational complexity, which allows for its application in real-time applications. Letexier and Bourennane [13] proposed a generalized multidimensional Wiener filter (MWF) for denoising of hyperspectral images (HSIs) by considering the multidimensional data as a third-order tensor. MWF needs to flatten the tensor for attaining the best results. However, flattening is always orthogonally performed, which may not be adapted to data. They estimated the relevant directions of tensor flattening that may not be parallel either to rows or columns. When rearranging data so that flattening can be performed in the estimated directions, the signal subspace dimension is reduced, and the signal-to-noise ratio is improved. They adapted the bidimensional straight-line detection algorithm that estimates the HSI main directions, which are used to flatten the HSI tensor. But the main disadvantages of these classical techniques lie in that an a priori information regarding the noise characteristics is required during the extraction process. As a result, a plethora of intelligent efforts has been invested to do away with the flaws and failings of the classical techniques.Neural networks have often been employed by researchers for dealing with the daunting tasks of extraction [14–16], classification [17–19] of relevant object specific information from redundant image information bases and identification and recognition of objects from an image scene [20–22]. Several neural network architectures, both self-supervised and unsupervised, are reported in the literature, which have been evolved to produce outputs in real time.Kohonen's self-organizing feature map [23] is centered on preserving the topology in the input data by subjecting the output data units to certain neighborhood constraints. The Hopfield's network [24] proposed in 1982, is a fully connected network architecture with capabilities of auto-association. A photonic implementation of the Hopfield network is also reported in [25], where a winner-take-all algorithm is used to emulate the state transitions of the network.Several attempts have also been reported in [26] where self organizing neural network architectures have been used for object extraction and pattern recognition. Several neighborhood based network architectures like the multilayer self organizing neural network (MLSONN) [27] are used for similar tasks. The underlying approach is centered on employing a fully connected multilayer network architecture with each neuron of one layer connected to its neighboring neurons in the previous layer. Such a network architecture, upon stabilization leads to the detection and extraction of objects by means of self supervision of inputs. The MLSONN [27] architecture is a feedforward network architecture and resorts to some fuzzy measures of the image information so as to derive the system errors therein. A generalized self-organizing multilayer neural network incorporating fuzziness measures is designed in [28] for object extraction. Results of a simulation study using synthetic and real images are seen to be quite satisfactory.Bhattacharyya et al. [29] proposed a novel neural network architecture suitable for image processing applications. The proposed architecture referred to as the bi-directional self-organizing neural network (BDSONN) architecture, comprises three interconnected fuzzy layers of neurons and it is devoid of any back-propagation algorithm for weight adjustment. The fuzzy layers of neurons represent the fuzzy membership information of the image scene to be processed. One of the fuzzy layers of neurons acts as an input layer of the network. The two remaining layers viz. the intermediate layer and the output layer are counterpropagating fuzzy layers of neurons. These layers are meant for processing the input image information available from the input layer. The constituent neurons within each layer of the network architecture are fully connected to each other. The intermediate layer neurons are also connected to the corresponding neurons and to a set of neighbors in the input layer. The neurons at the intermediate layer and the output layer are also connected to each other and to the respective neighbors of the corresponding other layer following a neighborhood based topology. The proposed architecture uses fuzzy membership based weight assignment and subsequent updating procedure. Some fuzzy cardinality based image context sensitive information are used for deciding the thresholding capabilities of the network. The network self organizes the input image information by counter-propagation of the fuzzy network states between the intermediate and the output layers of the network. The attainment of stability of the fuzzy neighborhood hostility measures at the output layer of the network or the corresponding fuzzy entropy measures determine the convergence of the network operation. The network is found to outperform the MLSONN architecture [27] in terms of the quality of the objects extracted from noisy perspective. A color version of the same architecture to operate on pure and true color images has been proposed by Bhattacharyya et al. in [30]. The proposed architecture is referred to as the PBDSONN architecture, It outperforms the parallel version of the MLSONN architecture as well.Quantum computation has evolved from the theoretical studies of Feynman, Deutsch and others, to an intensive research field since the discovery of a quantum algorithm which can solve the problem of factorizing a large integer in polynomial time by Shor (1994). Matsui et al. [31] proposed a Qubit Neuron Model which exhibits quantum learning abilities. This model resulted on a quantum multi-layer feed forward neural network proposal [32] which implements quantum mechanics (QM) effects, having its learning efficiency proved on non-linear controlling problems [33]. Aytekin et al. [34] proposed an automatic object extraction technique based on the quantum mechanical principles. Ezhov [35] proposed a new model of quantum neural network to solve classification problems. It is based on the extension of the model of quantum associative memory and also utilizes Everett's interpretation of quantum mechanics. Quantum entanglement is responsible for the associations between input and output patterns in the proposed architecture. In the field of artificial neural networks (ANN), some pioneers introduced quantum computation into analogous discussion such as quantum associative memory, parallel learning and empirical analysis [36–39]. They constructed the foundation for further study of quantum computation in artificial neural networks. Menneer developed her Ph.D. thesis in which deeply discussed the application of quantum theory to ANN and demonstrated that QANNs are more efficient than ANNs for classification tasks [40]. Ventura and Martinez developed a quantum associative memory, its structure and learning manner in quantum version [41]. Weigang tried to develop a parallel-SOM and mentioned the once learning manner in quantum computing environment [42]. He also proposed an Entanglement Neural Networks (ENN) on the base of the quantum teleportation and its extension with intelligent sense. In AI and quantum computing, there is more recent review from Hirsh et al. [43–46].According to Perus, quantum wave function collapse “is very similar to neuronal-pattern-reconstruction from memory. In memory, there is a superposition of many stored patterns. One of them is selectively ‘brought forward from the background’ if an external stimulus triggers such a reconstruction” [47,48].A quantum neural computer consists of an ANN where quantum processes are supported. The ANN is a self-organizing type that becomes a different measuring system based on associations triggered by an external or internally generated stimulus [37]. A quantum learning system might acquire some form of intentionally, and begin the bridging of the physical/mental gap. Menneer and Narayanan [38] have applied the multiple universes view from quantum theory to one-layer ANNs. In 1997, Lagaris et al., [49] developed Artificial Neural Networks for Solving Ordinary and Partial Differential Equations. The using of quantum parallelism is often connected with consideration of quantum system with huge dimension of space of states. The applications described further are used with some other properties of quantum systems and they do not demand such huge number of states. The term “image recognition” is used here for some broad class of problems [50]. Quantum computation uses microscopic quantum level effects to perform computational tasks and has produced results that in some cases are exponentially faster than their classical counterparts. The unique characteristics of quantum theory may also be used to create a quantum associative memory with a capacity exponential in the number of neurons [41]. Quantum Artificial Neural Networks (QANNs) are more efficient than Classical Artificial Neural Networks (CLANNs) for classification tasks, in that the time required for training is much less for QANNs [38]. The repetition is not required by QANNs, since each component network learns only one pattern causing the training set to be learned more quickly. Perus reported this as a basis for Quantum Associate Networks [51]. Quantum systems can realize content_addressable associative memory [52]. Hu [53] developed a research about quantum computation via neural networks applied to image processing and pattern recognition. He proved that the error in measurement produced by quantum principle is half the error produced by a classical approach. Bhattacharyya et al. [54] proposed a quantum version of the multilayer self organizing neural network (MLSONN) [27] for speeding up the extraction of binary objects from noisy backgrounds. The proposed technique introduced the quantum backpropagation mechanism for adjustment of network system errors. However, the mathematical foundations behind the dynamics of operation of the network have not been established. They illustrated the efficacy of the proposed architecture on only Gaussian noise affected binary images. Hence, the technique was not tested on different types of noises.With a view to arrive at an efficient self supervised neural networking paradigm suitable for image denoising, a quantum multilayer self organizing neural network (QMLSONN) is proposed in this article. Since the proposed architecture emulates the MLSONN [27] architecture, QMLSONN comprises three processing layers viz., input, hidden and output layers. The different layers contains qubit based neurons. Single qubit rotation gates are designated as the network layer interconnection weights. A quantum measurement at the output layer destroys the quantum states of the processed information thereby inducing incorporation of linear indices of fuzziness as the network system errors used to adjust network interconnection weights through a quantum backpropagation algorithm [54].In contrast to the architecture [54] which is efficient in extracting binary objects from only Gaussian noise affected binary images, results of application of the proposed QMLSONN are demonstrated on a synthetic and a real life binary image with varying degrees of Gaussian and uniform noise. This illustrates the generalization capabilities of the proposed QMLSONN architecture. A comparative study with the results obtained with the MLSONN architecture and the supervised Hopfield [24] network has also been carried out. The comparative results reveal that the QMLSONN outperforms both the self supervised MLSONN and the supervised Hopfield [24] network in terms of the computation time.Quantum computing is concerned with creating computational devices based on the principles of quantum mechanics. It defines quantum mechanical operations like superposition, entanglement [55] on them. Superposition is the property of quantum mechanics. Due to the linear dynamics the linear combination of each possible solution in a quantum mechanical system is also a solution. Entanglement is the property which generally gives rise to non-local interaction among bipartite correlated states. These properties have some profound implications. Problems like large integer factorization which is one of the cornerstones of today's digital security, is rendered solvable in reasonable amount of time by a quantum computer whereas it was practically impossible to track on a standard classical computer.The most basic element of quantum computer is a qubit. It is represented as a linear superposition of two eigenstates |0〉 and |1〉. It is represented as(1)|ϕ〉=α|0〉+β|1〉where, α and β are the probability amplitudes subject to the normalization criteria(2)|α|2+|β|2=1The basic operations on these qubits are represented as unitary operators in the Hilbert space. These unitary transformations can be used to realize various logic functions. These are the basic quantum gates [31] and contain the essence of quantum computation.The single qubit rotation gate is defined as(3)R(θ)=cosθ−sinθsinθcosθThis operates on a single qubit to transform it to(4)R(θ)=cosθ−sinθsinθcosθ×cosϕ0sinϕ0=cos(θ+ϕ0)sin(θ+ϕ0)In quantum mechanics a system is described by its quantum state. In mathematical languages, all possible pure states of a system form a complete abstract vector space called Hilbert space, which is typically infinite-dimensional. A pure state is represented by a state vector (or precisely a ray) in the Hilbert space. In the experimental aspect, once a quantum system has been prepared in laboratory, some measurable quantities such as position and energy are measured. That is, the dynamic state of the system is already in an eigenstate of some measurable quantities which is probably not the quantity that will be measured. For pedagogic reasons, the measurement is usually assumed to be ideally accurate. Hence, the dynamic state of a system after measurement is assumed to “collapse” into an eigenstate of the operator corresponding to the measurement. Repeating the same measurement without any significant evolution of the quantum state will lead to the same result. If the preparation is repeated, which does not put the system into the previous eigenstate, subsequent measurements will likely lead to different results. That is, the dynamic state collapses to different eigenstates.The values obtained after the measurement is in general described by a probability distribution, which is determined by an “average” (or “expectation”) of the measurement operator based on the quantum state of the prepared system. The probability distribution is either continuous (such as position and momentum) or discrete (such as spin), depending on the quantity being measured. The measurement process is often considered as random and indeterministic. Nonetheless, there is considerable dispute over this issue. In some interpretations of quantum mechanics, the result merely appears random and indeterministic, whereas in other interpretations the indeterminism is core and irreducible. A significant element in this disagreement is the issue of “collapse of the wavefunction” associated with the change in state following measurement.In any case, our descriptions of dynamics involve probabilities and averages. For measurement purpose, we introduce the von Neumann measurement strategy which establishes one of a set of basis participating states as output. To continue this process, we first select a basis at random and ensure that the system exists in the basis states. Quantum computing initiates some probabilistic measurement procedure that transforms the states in superposed form to a specific state required. To accomplish multi-qubit measurement, a succession of single-qubit measurements in the defined basis are performed. Suppose, at initial, the system is in the state |ψ〉, then the probability of finding the state ϕ will be |cϕ|2.MLSONN [27] is a self-supervised architecture efficient in extracting binary objects from a noisy and blurred background. It comprises several layers, viz. the input layer, any number of hidden layers and an output layer for obtaining the final processed output. The interconnection strategy employed between the layers follows a second order neighborhood topology. The network is characterized by forward propagation of incident information and subsequent processing at the hidden layers and the output layer. The network system errors are calculated at the output layer using the linear indices of fuzziness of the intermediate outputs obtained. These errors are used to adjust the network interconnection weights through the standard backpropagation algorithm. These intermediate outputs are fed back to the input layer for further processing. This process is continued until the network errors stabilize. Details regarding the operation and dynamics of the network can be obtained in [27].The Hopfield network [24] is a fully connected network architecture with every node connected to every other node (but not to itself) and the connection strengths or weights are symmetric in that the weight from node i to node j is the same as that from node j to node i, i.e.wij=wji, andwii=0∀i,j. Fig. 1shows a 3-node Hopfield network.Thus the flow of information in the network is not in a single direction, i.e. it is possible for information to flow back from a node to itself via other nodes. So there is feedback in the network. As such it is referred to as feedback or recurrent network.The state of the net at any given time is given by the vector of the node outputs (x1, x2, x3). The network starts in some initial state and chooses a node at random to let it update its output or ‘fire’. This means that it evaluates its activation in the normal way and outputs a ‘1’ if this is greater than or equal to zero or ‘0’ otherwise. The network now finds itself in the same state as it started in or in a new state which is at Hamming distance of one from the old. Another new node is chosen at random and the process is continued. In this way, the network changes its state.A greater insight into the dynamics of the network can be obtained by defining an energy function for the network. Consider two nodes i, j in the network connected by a positive weight where j outputs a ‘0’ and i outputs a ‘1’. In this situation, if j is given a chance to update or fire, the contribution to its activation from i is positive and this may well serve to bring j's activation above threshold and make its output a ‘1’. A similar situation would prevail if the initial output states of the two nodes had been reversed since the connection is symmetric. On the other hand, if both units are ‘on’, they are reinforcing each other's current output. The weight may therefore be thought of as fixing a constraint between i and j that tends to make them take on the value of ‘1’. A negative weight would tend to enforce opposite outputs. One way of viewing these networks is therefore as constraint satisfaction nets.This can be substantiated quantitatively by means of a suitable energy function as(5)eij=−wijxixjThe energy of the network is found by summing over all pairs of nodes as(6)E=∑pairseij=−∑pairswijxixjThis may be written as(7)E=−12∑i,jwijxixj1/2 is due to the fact that the sum includes each pair twice (aswijxixjandwjixixj) andwij=wji,wii=0.It is now instructive to see what is the change in energy when a node fires. Let node k is chosen to be updated. Then the energy can be written as(8)E=−12∑i≠k,j≠kwijxixj−12∑iwkixkxi−12∑iwikxixkSince,wik=wki, the last two sums can be combined. Hence(9)E=−12∑i≠k,j≠kwijxixj−∑iwkixkxiSince xkis constant throughout, it may be written as(10)E=S−xk∑iwkixiwhere(11)S=−12∑i≠k,j≠kwijxixjBut the sum is just the activation of the kth node. Hence(12)E=S−xkakLet the energy after k has updated be E′ and the new output bexk′. Then(13)E′=S−xk′akThen the change in energy for a change in output Δxkis given by(14)ΔE=−ΔxkakNow, there are two cases to be considered.1.ak≥0. Then the output goes from ‘0’ to ‘1’ or stays at ‘1’. In either case Δxk≥0. There Δxkak≥0. So ΔE<0.ak<0. Then the output goes from ‘1’ to ‘0’ or stays at ‘0’. In either case Δxk<0. There once again Δxkak≥0. So ΔE<0.Therefore for any node to be updated, ΔE<0, so the energy of the network decreases or stays the same. But the energy is bounded below by a value obtained by putting all xi, xj=1 in Eq. (7). Thus E must reach some fixed value and then stays the same. Once this has occurred, further changes in the network's state can still take place as ΔE=0 is still allowed. However, for this to be the case (Δxk≠0 and ΔE=0), ak=0 must be satisfied. This implies that the change must be of the form 0→1. There can be at most N (of course there ma be none) of these changes, where N is the number of nodes of the network. After this there can be no more changes in the network's state and a stable state has been reached.As QMLSONN is the quantum version of the MLSONN architecture [27], the processing nodes of the different layers are simply qubits like〈α11|〈α12|〈α13|…〈α1n|………………………………………〈αm1|〈αm2|〈αm3|…〈αmn|for the input layer,〈β11|〈β12|〈β13|…〈β1n|………………………………………〈βm1|〈βm2|〈βm3|…〈βmn|for the hidden layer and〈γ11|〈γ12|〈γ13|…〈γ1n|………………………………………〈γm1|〈γm2|〈γm3|…〈γmn|for the output layer.The interconnection weights are in the form of rotation gates and also follow a second order neighborhood topology. A schematic of the QMLSONN is shown in Fig. 2.Since, the QMLSONN operates on real world input images, the processing qubits of the different layers of the network correspond to the pixels of the input images. The input information is fuzzified in the range [0, 1] and fed to the qubits of the input layer of the network architecture. Since the qubits operates on quantum states, these inputs are transformed to qubits by converting into phase information of [0, (π/2)] in quantum states. These qubit information from the different nodes are summed up and the neighborhood topology based interconnection matrix between the different layers act on these summed up qubit information using a characteristic sigmoidal activation to propagate to the next layer. The use of the sigmoidal activation function is to cope up with the nonlinearities in the system inputs. This process continues from the hidden layer to the output layer as well.When input data (inputi) is given into the network, the input layer first converts the fuzzified input value [0, 1] into the phase [0, (π/2)] in quantum states.(15)yiI=π2(inputi)Here, (inputi) are the input fuzzified pixel intensities andyiIare these fuzzified inputs converted into quantum inputs. Since, there are m×n input neurons which correspond to the image pixels of dimension m×n, so the output of the input layer is given by(16)ujh=∑i=1m×n∑j=12l+1f(ψipjhll)f(yiI)−f(λjh)where ψipjhllare the connection weights between input (i) and hidden (h) layer andλjhis the threshold of jth hidden neuron. The first summation corresponds to the input layer neurons and the second summation corresponds to every qubit of each row in the second order neighborhood geometry.From Eq. (16), we get,(17)ujh=∑p=1m×n∑j=12s+1f(ψipjhll)f(yiI)−f(λjh)=∑p=1m×n∑j=12s+1e(iψipjhll+yiI)−f(λjh)where s is the order of neighborhood.(18)=∑p=1m×n∑j=12s+1(cos(ψipjhll+yiI)+isin(ψipjhll+yiI)−cos(λjh)−isin(λjh))Now,(19)yjh=π2g(δjh)−arg(ujh)whereg(δjh)is the sigmoidal activation function.(20)=π2g(δjh)−tan−1∑p=1m×n∑j=12s+1sin(ψipjhll+yiI)−sinλjh∑p=1m×n∑j=12s+1cos(ψipjhll+yiI)−cosλjh(21)=π2g(δjh)−tan−1(zh)wherezh=∑p=1m×n∑j=12s+1sin(ψipjhll+yiI)−sinλjh∑p=1m×n∑j=12s+1cos(ψipjhll+yiI)−cosλjhHere arg(ujh) means the phase extracted from a complex number u andδjhis the reversal parameter of jth hidden neuron.Similarly,(22)uk=∑j=1m×n∑k=12l+1f(ψhqkoll)f(yjh)−f(λk)where ψhqkollare the connection weights between hidden and output layer and λkis the threshold of kth output neuron. Thus,(23)uk=∑q=1m×n∑k=12s+1eiψhqkolleiyjh−eiλk=∑q=1m×n∑k=12s+1ei(ψhqkoll+yjh)−eiλk(24)=∑q=1m×n∑k=12s+1(cos(ψhqkoll+yjh)+isin(ψhqkoll+yjh)−cos(λk)−isin(λk))Now,(25)yk=π2g(δk)−arg(uk)where g(δk) is the sigmoidal activation function.(26)=π2g(δk)−tan−1∑q=1m×n∑k=12s+1sin(ψhqkoll+yjh)−sinλk∑q=1m×n∑k=12s+1cos(ψhqkoll+yjh)−cosλk(27)=π2g(δk)−tan−1(zk)wherezk=∑q=1m×n∑k=12s+1sin(ψhqkoll+yjh)−sinλk∑q=1m×n∑k=12s+1cos(ψhqkoll+yjh)−cosλk.In this way, the outputs are generated at the output layer of the QMLSONN architecture. A quantum measurement destroys the quantum states of the outputs at the output layer and convert the same to either 0 or 1 depending on a probability. However, since these outputs are to be further processed, these are retained in a safe custody. The probability amplitudes of the quantum states are compared with 1's and 0's using the linear indices of fuzziness to compute the system errors. These errors are used in a quantum backpropagation algorithm (to be discussed next) to adjust the interconnection weights of the network layers. The retained outputs are fed back to the input layer for further processing. This process is repeated until the network system errors fall below a certain tolerable limit whence the object gets extracted from the noisy background.The QMLSONN network architecture is characterized by two types of network interconnections, viz., the input to hidden layer interconnections and the hidden to output layer interconnections. Both of these types of interconnections follow second order topology. These are the {{p, j}, {l, l}}th input to hidden layer interconnection weights ψipjhlland the {{h, q}, {l, l}}th hidden to output layer interconnection weights ψhqkoll. The network interconnection weights are adjusted using the quantum backpropagation algorithm [56].As mentioned in Section 7.1, the QMLSONN generates quantum output states at the output layer of the network architecture. However, a quantum measurement is a must for retrieving the classical equivalents of those quantum outputs for determining the network system errors. But a quantum measurement would mean that the corresponding quantum output states would be lost or destroyed forever due to decoherence. For this purpose, the quantum output states are retained for further processing and the quantum measurement is carried out on the copy of the quantum output states. After the quantum measurement is carried out, the quantum output states of the pixels decohere to 0 or 1 with certain probabilities. Moreover, since the QMLSONN operates in a self-supervised manner, these probabilities are used to determine the network system errors by using the linear indices of fuzziness [27] as given by(28)Ei=min(outputi−0,1−outputi)where Eiis the system error for the ith neuron at the output layer and outputiis the corresponding classical probability amplitude of decohering to 0 or 1.In general, the output outputiwill not be the same as the target or desired value ti. For a pattern P, the error is represented as(29)E=12∑pP∑nN(ti−outputi)2The procedure for learning the correct set of weights is to vary the weights in a manner such that the error E is reduced as rapidly as possible. This can be achieved by moving in the direction of negative gradient of Ei. In other words, the incremental change for a particular pattern P and the {{h, q}, {l, l}}th hidden to output layer interconnection weights ψhqkollis(30)Δψhqkoll=−∂Ei∂ψhqkoll(31)=−(ti−outputi)[2sin(yk)cos(yk)∂yk∂ψhqkoll](32)=−(ti−outputi)2sin(yk)cos(yk)11+zk2cos(ψhqkoll+yjh)Re(uk)+sin(ψhqkoll+yjh)Im(uk)Re(uk)2Thus, the weight update equation takes the form(33)ψhqkollnew=ψhqkollold−η∂E∂ψhqkolloldwhere, η is the learning coefficient.For the {{p, j}, {l, l}}th input to hidden layer interconnection weights ψipjhll, using chain rule one gets,(34)∂E∂ψipjhll=∂E∂yjh×∂yjh∂ψipjhllwhere,(35)∂E∂yjh=−(tn,p−outputn,p)2sin(yk)cos(yk)∂yk∂yjh(36)∂E∂yjh=(tn,p−outputn,p)2sin(yk)cos(yk)1+zk2cos(ψhqkoll+yjh)Re(uk)Re(uk)2+sin(ψhqkoll+yjh)Im(uk)Re(uk)2Now,(37)∂yjh∂ψipjhll=−11+zj2cos(ψipjhll+yiI)Re(ujh)Re(ujh)2+sin(ψipjhll+yiI)Im(ujh)Re(ujh)2Similarly,(38)ψipjhllnew=ψipjhllold−η∂E∂ψipjhllold

@&#CONCLUSIONS@&#
