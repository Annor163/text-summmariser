@&#MAIN-TITLE@&#
Improving cross-validated bandwidth selection using subsampling-extrapolation techniques

@&#HIGHLIGHTS@&#
A two-stage subsampling-extrapolation bandwidth selection procedure is proposed.An automatic nested cross-validation method is developed to select the subsample size.The extrapolated bandwidth selectors achieve a smaller mean square error.The second-order extrapolated bandwidth selector has a relative convergence raten−1/4.

@&#KEYPHRASES@&#
Bandwidth selection,Cross-validation,Extrapolation,L,2,distance,Nonparametric kernel density estimator,Subsampling,

@&#ABSTRACT@&#
Cross-validation methodologies have been widely used as a means of selecting tuning parameters in nonparametric statistical problems. In this paper we focus on a new method for improving the reliability of cross-validation. We implement this method in the context of the kernel density estimator, where one needs to select the bandwidth parameter so as to minimizeL2risk. This method is a two-stage subsampling-extrapolation bandwidth selection procedure, which is realized by first evaluating the risk at a fictional sample sizem(m≤sample sizen)and then extrapolating the optimal bandwidth frommton. This two-stage method can dramatically reduce the variability of the conventional unbiased cross-validation bandwidth selector. This simple first-order extrapolation estimator is equivalent to the rescaled “bagging-CV” bandwidth selector in Hall and Robinson (2009) if one sets the bootstrap size equal to the fictional sample size. However, our simplified expression for the risk estimator enables us to compute the aggregated risk without any bootstrapping. Furthermore, we developed a second-order extrapolation technique as an extension designed to improve the approximation of the true optimal bandwidth. To select the optimal choice of the fictional sizemgiven a sample of sizen, we propose a nested cross-validation methodology. Based on simulation study, the proposed new methods show promising performance across a wide selection of distributions. In addition, we also investigated the asymptotic properties of the proposed bandwidth selectors.

@&#INTRODUCTION@&#
Cross-validation methodology has long been a popular method for selecting tuning parameters in non and semiparametric models. However, it has also been criticized for its high variability and its corresponding tendency to overfit the data.This paper develops new methods for the improvement of the conventional cross-validation procedures. It is based on a blending ofU-statistic estimation and asymptotic theory. These new methods are realized by estimating the cross-validation risk with small training sets, then extrapolating the results to the desired sample size. The extrapolation step requires some asymptotic theory, but only the rate of convergence, not any unknown constants. We will show that such a two-stage procedure can dramatically reduce the high variability and overfitting that is the major liability of the conventional unbiased cross-validation.We view our results as part of the following paradigm: when one is estimating nonparametrically a statistical property of samples of target sizem, such as the risk inherent in using a particular model, then one can do a much more accurate estimation when the targetmis much smaller than the actual sample sizen. The intuition is that there are many, many more subsamples of sizen/2, say, than there are subsamples of sizenorn−1.To motivate our extrapolation methodology, we will here show how it works when used in risk estimation in the context of nonparametric kernel density estimation. In the process we will also show that for this problem the risk function for arbitrarymis surprisingly simple. In particular, cross-validation estimation at an arbitrary training sample size ofmdoes not require repeated subsampling at sizem, thereby greatly speeding up and improving accuracy of the methods we propose. We believe this to be a major new insight in the kernel density estimation literature.To simplify notation, consider a univariate random variableX∈R. In statistical practice, we often know little about the underlying distribution ofXwhich is crucial in exploratory or inferential analysis (Silverman, 1986). So, our main task is to estimate the unknown density functionf(x)based on a set of observations. In this paper, we focus on the nonparametric kernel density estimator (Fix and Hodges, 1951). Given an i.i.d. sample of sizen,Xn=(X1,…,Xn), the kernel density estimator atxis defined for a kernelKas(1.1)fˆh(x∣Xn)=n−1∑i=1nKh(Xi−x)(x∈R),whereh>0is called the bandwidth parameter. HereKh(t)=h−1K(t/h)and functionKis the kernel function. As the choice ofKdoes not greatly affect the density estimation (Hardle et al., 1994), throughout this paper we consider a commonly used location kernel function, the Gaussian kernel.(1.2)Kh(x−x0)=(h2π)−1e−(x−x0)2/(2h2)∼N(x0,h2).However, our proposed methodologies do not depend on the choice ofK, and the theoretical results in this paper will be stated in terms of an arbitrary symmetric kernel functionKof orderr(r≥2). For the definition of the order of a kernel function, please see Turlach (1993).Although one has free choice of the kernel function in a density estimator, the choice of the bandwidthhis generally viewed as much more crucial. In order to select the optimal smoothing parameterh, we need to evaluate how closelyfˆhcan approximateffor a given data set. Most bandwidth selectors are based on first choosing a risk function that measures the error made in using a particular bandwidthh. One can then estimate the risk function for a given data set and choose the bandwidth that minimizes the empirical risk. Such bandwidth selectors are referred to as data-driven methods.The main result of this paper is to propose a two-stage subsampling-extrapolation bandwidth selection procedure. This work is closely related to the rescaled bagging cross-validation method of Hall and Robinson (2009) and the partitioned cross-validation method of Marron (1987). Recent work involving bagging and subsampling in problems other than kernel density estimation includes Meinshausen and Buhlmann (2010) and Shah and Samworth (2012). Unlike the bandwidth selectors discussed in Park and Marron (1990) and Sheather and Jones (1991), which are based on asymptotic theory, our proposed methodology is a hybrid of the cross-validation method and the asymptotic theory. As such it does not require the estimation ofR(f″)or a third-stage estimation ofR(f‴). (By convention, we denoteR(g)=∫g2(x)dxfor any given functiong.) Hence, it is more straightforward to implement than plug-in estimators. Most importantly, it can be used in a wide variety of problems where plug-in methodology is not available.We present an extensive simulation study in Section  4.1 to compare the proposed methods with the conventional cross-validation estimator. It will be seen that our bandwidth selectors achieve a smaller expected integrated square error that is much closer to the theoretical optimum than the standard cross-validation. Moreover, a comparison of the proposed methods to indirect cross-validation (Savchuk et al., 2011, 2010; Mammen et al., 2012) can be found in Section  4.2. In addition, we compare our methods to the asymptotic selection of the subsample sizemthat was described in Marron (1987).In this section, we will derive a simpleU-statistic form estimator for the risk that arises fromL2distance. It is a new representation for the unbiased risk estimator and enables us to calculate the aggregated risk at subsamples of sizem(m≤n)much more efficiently than the repeated bootstrapping done in Hall and Robinson (2009) or the partitioning method used in Marron (1987).Define the integrated square error (ISE) as(2.1)ISE(h)=∫{fˆh(x∣Xn)−f(x)}2dx.If one wants to evaluatefˆhover all possible samples of sizen, one can consider the mean integrated squared error (MISE), also known as theL2risk.(2.2)RiskL2(h,n)=MISE(h)=EXn[∫{fˆh(x∣Xn)−f(x)}2dx].By Fubini’s theorem, the risk in (2.2) can be decomposed in the following fashion:RiskL2(h,n)=∫EXn{f2(x)−2f(x)fˆh(x)+fˆh2(x∣Xn)}dx. Furthermore, one can omit terms independent ofhand focus on the relative risk, denoted asRL2(h,n)=−2EXn{∫f(x)fˆh(x∣Xn)dx}+EXn{∫fˆh2(x∣Xn)dx}. It can be shown by simple algebra that the first expectation inRL2(h,n)can be represented asE{Kh(X1−X2)}. Moreover, the second expectation equalsE{(Kh∗Kh)(X1−X2)}+n−1E{(Kh∗Kh)(0)−(Kh∗Kh)(X1−X2)}, where∗is the convolution operator.If we denote(2.3)Ah(X1,X2)=(Kh∗Kh)(X1−X2)−2Kh(X1−X2),(2.4)Bh(X1,X2)=(Kh∗Kh)(0)−(Kh∗Kh)(X1−X2),then(2.5)RL2(h,n)=E{Ah(X1,X2)}+n−1E{Bh(X1,X2)}.Note that the only dependence on sample sizenon the right hand side of (2.5) occurs in the multipliern−1. In the case of Gaussian kernel,(Kh∗Kh)(X1−X2)=K2h(X1−X2)and(Kh∗Kh)(0)=1/(2hπ). One can also denoteMISE(h)as∫bias2(fˆh(t))dt+∫Var(fˆ(t))dt. Then,E{Bh(X1,X2)}=n∫Var(fˆh(t))dtcorresponds to the integrated variance, andE{Ah(X1,X2)}=∫bias2(fˆh(t))dt−∫f(t)2dtis the relative integrated squared bias.Following the footsteps of Ray and Lindsay (2008) and Lindsay and Liu (2009), we propose to estimate the risk at sample sizesmthat may be much smaller than the actual sizen. Ourm<nparadigm motivates us to hypothesize that these estimators will have much lower variability. Our simulations verify this. We also know that the cross-validation criterion for bandwidth selection tends to choose the bandwidths that are too small, and so overfit the density (Loader, 1999). As we will show, the new methodology particularly avoids the overfitting problem by reducing the chances of selecting a small bandwidth. Throughout this paper, we usemto represent the subsample size, which we might also call the fictional sample size. As we will see, it is closely related to the training sample size in cross-validation.Denote the relative risk evaluated at fictional sizemas(2.6)RL2(h,m)=E{Ah(X1,X2)}+m−1E{Bh(X1,X2)}.This formula gives an important insight into the risk estimation problem. The only place the fictional sizemappears is as a coefficient. If we take the derivative ofRL2(h,m)(2.6) with respect tohand set it equal to zero, we thereby identifyhas a potential minimum to the risk for thatm. We can invert this thinking and solveRL2′(h,m)=0withhfixed, thereby finding themthat leads to optimization of the risk.(2.7)m∗(h)=−ddhE{Bh(X1,X2)}/ddhE{Ah(X1,X2)}.The uniqueness of the solution shows that for eachhthere is exactly onemfor which it is potentially optimal. In particular, ifKis the Gaussian kernel andfis the standard normal, we have(2.8)m∗(h)={(h2+1)(h2+2)}3/2−h3(h2+2)3/222h3(h2+1)3/2−h3(h2+2)3/2.If we desire the optimalhfor a particular fixedm0, we solve the inverse problemm0=m∗(hopt)forhopt. This normal theorym∗curve (2.8) will be examined later in light of the asymptotic theory (see Fig. 1). For now we note that in the normal examplem∗is decreasing inh. It follows that the optimal bandwidthhfor eachmis a decreasing function ofm.Remark 1For difficult densities, the theoretical curvem∗(h)is not monotonic, and so many methods based on asymptotics are likely to fail at some sample sizes. For example, if the curve has the shape∽, with two regions of decrease separated by a region of increase, then for some values ofmthere will be three solutions inhtom=m∗(h). These will correspond to two local minima to the risk curve and the local maximum in-between. The central region in whichm∗(h)is increasing corresponds to values ofhthat are never optimal for any value ofm. For an example of this, see Fig. 6, where we show them∗(h)curve for the claw density that will be discussed in Section  4.1.Because the relativeL2risk at fictional sizem,RL2(h,m)(2.6), involves the unknown density functionf, we cannot use it directly to find the optimal bandwidth. In practice, one needs to first estimate the risk based on a set of observations and then select an estimated optimal bandwidth by minimizing the estimated risk score. A straightforward (and unbiased) estimation forRL2(h,m)is by constructing aU-statistic based on a kernel function of size two.Define(2.9)UL2,m=(n2)−1∑1≤i<j≤nψL2,h(xi,xj),whereψL2,h(x1,x2)=Ah(x1,x2)+m−1Bh(x1,x2)is a symmetric kernel function of size two, withAhandBhdefined earlier in (2.3)(2.4). Because a generalU-statistic is a function of the order statistics,UL2,m(2.9) therefore is the best unbiased estimator for the relative risk in this nonparametric context (Fraser, 1954).Note thatUL2,mis equivalent to the unbiased cross-validation formula whenm=n. That is, both of them are unbiased estimates for the relative MISE and are functions of the order statistics (modulo terms that do not depend onh). In addition, the un-rescaled bagging cross-validation bandwidth selector proposed by Hall and Robinson (2009) is actually nothing more than the bandwidth selector based on (2.9) if one makes the bootstrap size equal tom. However, the simple expression for ourU-statistic risk estimator enables us to compute the aggregated risk much more efficiently than bootstrapping. First, we can generally compute the completeU-statistics, being of order two, much more efficiently than subsampling subsets of sizem. Secondly, the calculations can be done for allmat once, in effect.If we minimize (2.9) overhby setting the derivative inhto zero, we have(2.10)mˆ∗(h)=−∑1≤i<j≤nddhBh(xi,xj)/∑1≤i<j≤nddhAh(xi,xj).This equation describes the dependence structure betweenmandhin minimizing the estimatedL2risk for a given sample of sizen. In practice, one can construct aU-statistic form estimate for the risk at a fictional sample sizem. Then, by minimizing theU-statistic risk estimate one can obtain the optimal bandwidth choice for a given value ofm. We call this the simple subsampling bandwidth selector. We note that the empirical curvemˆ∗(h)is not necessarily strictly decreasing inh. If this happens, root selection rules must be applied.Remark 2It is easy to plot the empirical curvemˆ∗(h)for any particular data set. If the empirical curve is not monotonic decreasing in the region ofmvalues of interest, we would recommend against using extrapolation or plug-in methods.In this section we will examine several ways to usem<nrisk estimation to improve performance inL2risk minimization. They will proceed in order of increasing sophistication in their use of asymptotic theory.The simplest way to use the reduced variability of the risk estimator for smaller values ofmis to pick the bandwidthhselected based onUL2,m(h). In addition to reducing variance inhestimation, however, one is introducing bias, and so one must consider the trade-off that occurs between bias and variance.We carried out a simulation study that indicated that asmdecreased the optimal bandwidthhmbecame larger (see the 2012 Pennsylvania State University Ph.D. thesis of Q. Wang which is available electronically from Pennsylvania State University library). The average integrated square error decreased as one used fictional sizem<nuntilmreached a certain threshold, beyond which further improvement in MISE could not be achieved. For instance, in the standard normal case half-sampling withm=n/2gave us the smallest simulated MISE, with an improvement of about 12% overm=nfor samples of sizen=100. This conclusion agrees with the result in Hall and Robinson (2009).Although using a fictional sizem<ncan help to reduce the variability of the bandwidth selector and therefore achieve a smaller MISE, it turns out there are simple techniques to reduce this bias without increasing variance. We will show next in Section  3.2 how to do this.We now propose a two-stage, subsampling-extrapolation, approach in bandwidth selection. We will first develop a first-order extrapolated bandwidth selector. Motivated by the rule-of-thumb criterion, the two pieces we combine are the estimated optimal bandwidth atm<nand the rate of convergence of the estimator asn→∞. Later we will offer further refinements to this method. We will then provide a simulation comparison of all methods.Recall theU-statistic form risk estimator,UL2,m(2.9), computed based on squared distance and evaluated at a fictional sample sizem. We denote the corresponding simple subsampling bandwidth selector ashˆL2,m, which minimizes theUrisk estimate at sizem. AlthoughhˆL2,m(m<n)was less variable thanhˆL2,n, it tended to be biased larger than the optimal bandwidth choicehoptat sample sizen. This is due to the fact thatm∗(h)(2.7) is decreasing inh, and we have evaluated the risk at a sizemless than the original sample sizen.Our goal is to take advantage of the small variability of the risk estimate at fictional sizem<nand also try to remove the incurred bias inhˆL2,mby referring to the asymptotic relationship betweenmandhon the log–log scale. We can motivate our approach using the following well-known theory. If the densityfis known, the optimal bandwidth for minimizing the asymptotic MISE based on an order-2 kernel can be written ashopt(m)=m−1/5C(f),whereC(f)is a constant depending onf. A typical rule-of-thumb bandwidth selector estimates the constantC(f)from the data in some way. For example, if we assume that both the true distribution and the kernel function are Gaussian, we then havehˆrot=1.06σˆm−1/5, whereσˆis an estimate for the population standard deviation.We have derived an explicit formula for findingm∗(h), the value ofmfor whichhis optimal. This asymptotic formula can be rearranged to say that, in the limit asmgets large, this relationship can be represented as(3.1)logm=C−5loghˆrot.HereCis a constant independent of the bandwidthhˆrot. This equation represents a straight-line relationship with slope−5on the log–log scale. One may ask whether this simple linear relationship is a good approximation for the exact relationship betweenlogm∗(h)andloghfound in (2.7). Fig. 1 displays the comparison between the rule-of-thumb criterion and the optimal risk criterion on the log–log scale when the underlying distributionfis the standard normal, and the kernel functionKis Gaussian. It can be seen that for a fixed value ofmthe rule of thumb always yields a smaller bandwidth than the one given by the exact risk curve, but their left hand asymptotes match.Our derivation of (3.1) from the rule-of-thumb selection rule was heuristic. We therefore show more formally that this relationship is valid for any arbitrary smooth kernel functionKand density functionf. The following lemma verifies this statement. For proof, please see Appendix.Lemma 1AssumeKis a smooth symmetric kernel function of orderr(r≥2), andfis a probability density function that is(2r−1)th order differentiable. By minimizing theL2risk, we can obtain the explicit expression of the fictional sample sizemfor whichhwould be optimal:m∗(h)=−ddhE{Bh(X1,X2)}/ddhE{Ah(X1,X2)},whereAh(x1,x2)andBh(x1,x2)are defined in   (2.3) and (2.4). Moreover, it can be shown thatlogm∗(h)+(2r+1)logh→constant  ,ash→0.And,dlogm∗(h)/(dlogh)→−(2r+1),ash→0.According to Lemma 1, the optimal risk curve approaches the rule-of-thumb straight line on log–log scale aslogh→−∞. In particular, when one considers a Gaussian kernel, the order of the kernel functionris 2. In this case, we would havedlogm∗(h)/(dlogh)→−5ash→0. This confirms that the asymptotic slope of the optimal risk curve in Fig. 1 is indeed −5, the same as the slope of the rule-of-thumb straight line.As a result, if one knows the bandwidth selected form, one simple way to remove the bias introduced by usingm<nis to extrapolate the bandwidth selected at sizemto the optimal value atnbased on the approximate linear relationship betweenlogm∗andlogh. We summarize the two-stage bandwidth selection procedure based on linear extrapolation as follows:1.Subsampling stage: Construct theU-statistic estimate for theL2risk at a fictional sizem(m≤n)and obtain the subsampling bandwidth selector, denoted ashˆL2,m.(3.2)hˆL2,m=argminh>0UL2,m(h).Extrapolation stage: ExtrapolatehˆL2,mto an approximation forhˆL2,nby referring to the approximate linear relationship betweenlogm∗andloghas discussed in Lemma 1. This gives the estimator(3.3)hˆ1=(m/n)1/5hˆL2,m.We callhˆ1the first-order extrapolated bandwidth selector. Intuitivelyhˆ1should have low bias whenmis close tonand when(logh,logm∗(h))relationship in minimizing theL2risk resembles a straight line. The fact thathˆ1is a shrunken version ofhˆL2,mmeans that it has less variance and so, as we shall see, the variability of the bandwidth selector is reduced significantly compared with the traditional cross-validation bandwidth selector.For any particular densityfthere will exist an optimal choice of the fictional sizemsuch that it optimizes the trade-off between bias and variance of the bandwidth selector. Fig. 2shows the density curves for the sampling distributions of the simple subsampling bandwidth selectorhˆL2,mand the first-order extrapolated bandwidth selectorhˆ1at different fictional sample sizesm. These density curves were plotted based on drawingR=500samples of sizen=100from the standard normal distribution. This plot demonstrates graphically how the first-order extrapolation corrected for the bias incurred by usingm<nwhile simultaneously reducing variability over standard cross-validation (m=n). In particular, it greatly decreased the selection of values ofhthat were “too small”, corresponding to overfitting. Later in Table 4 it will be seen that the first order extrapolation improved the MISE efficiency ratio,MISEopt/EISE(hˆ1), of the standard cross-validation from 64% to over 80%. Here EISE stands for expected integrated square error. The use of EISE rather than MISE in assessing bandwidth selectors was suggested by Jones (1991). In our simulation section we will determine the optimal value ofp=m/nfor a number of sampling distributions. It will be shown there that the optimal value ofpvaries somewhat based on the smoothness of the density, butp=0.3worked well for many. One possible strategy for the extrapolation estimator is to use a fixed value ofpregardless of the data. We will compare this strategy with a few others in the simulation section.The first-order extrapolated bandwidth selector introduced in Section  3.2 is based on an approximate linear relationship betweenlogm∗(h)andlogh. We have shown in Lemma 1 that the true optimal risk curve approaches the rule-of-thumb straight line ashgoes to 0. That is, the linear approximation is most accurate whenhis fairly small. Therefore, we wonder whether we could improve the approximation and seek a more accurate relationship betweenlogm∗(h)andloghthat would be useful for smaller values ofn.Notice from Fig. 2 that the first-order extrapolated bandwidth tends to be biased, more and more so as the range of extrapolation increases. As the optimal risk curve ofm∗(h)is decreasing inh, we propose to consider a second-order correction. We start by noticing that the explicit expression form∗(h)can be written asm∗(h)=(1/2π)h−2+2hμ2(ϕ)C02+o(h)4h3C1+6h5C2+o(h5),whereμj(ϕ)=∫xjϕ(x)dxandϕis a Gaussian kernel, andCij=∫f(i)(x)f(j)(x)dxfori,j∈Zwith the assumption thatf(0)(x)=f(x). In addition,C1=μ4(K)C04/24+(μ2(ϕ))2C22/8andC2=(3/96)μ2(ϕ)μ4(ϕ)C24.To find the correct second order expansion, we need to take this expansion to the next term. We use the approximationlog(1+x)≈xforxclose to 0, and writelog{−ddhE(Bh(X1,X2))(2h2π)}=log{1+2h3μ2(ϕ)C02(2π)+o(1)}≈2h3μ2(ϕ)C02(2π)log{ddhE(Ah(X1,X2))(4h3C1)−1}=log{1+(3C2/2C1)h2+o(1)}≈h2(3C2/2C1).Therefore, the overall magnitude of the second-order error on the log scale ish2.We then assume(3.4)m∗(h)≈m̃(h)=C0h−5eah2,whereC0is a constant anda∈R. Note that the adjustment termeahgoes to 1 ash→0but is strictly bigger than 1 ifh>0anda>0.We propose to approximate parameteraby matchingm∗(h)andm̃(h)at two values ofhand solving for the unknowns. Leth0andc0h0(c0>1)be two chosen bandwidths. The two equationsm∗(h0)=C0h0−5eah0m∗(c0h0)=C0(c0h0)−5ec0ah0then provide a way to solve for the unknown parametera. In particular,aˆ=1h02(c02−1)log{c05m∗(c0h0)m∗(h0)}≥0.Then, for any unknownh, formula (3.4) can be represented asm∗(h)=m∗(h0)m∗(h)m∗(h0)≈m∗(h0)(h/h0)−5eaˆ(h−h0)≔m∗∗(h).We propose to invert the curvem∗∗determined by the last approximation to estimate an optimal bandwidth for any particularmincludingm=n. We note that the inversion relationship can be expressed as an explicit correction to the log–log linear relationship.(3.5)logm∗∗(h)=logm∗(h0)−5(logh−logh0)+aˆ(h2−h02).As seen in (3.5),m∗∗is in fact just an exponential curve fitted through the two points(logh0,logm∗(h0))and(log(c0h0),logm∗(c0h0))and with slope −5.Fig. 3illustrates the theoretical curves of(logh,logm(h))relationship whenfis the standard normal. The solid line is based on the rule-of-thumb criterion; the dashed curve represents the exact relationship betweenlogm∗(h)andloghin minimizing theL2risk (2.8); the dotted curve displays the relationship of the second-order extrapolation based on formula (3.5), usingc0=2andh0=0.5as an illustration.It can be clearly seen that the second-order extrapolation curve was surprisingly close to the true optimal risk curve for the standard normal case. It should provide less bias than the first-order extrapolation, but it could add variability. In other words, we might expect the second-order extrapolation method to outperform the linear extrapolation bandwidth selector for cases where the density function is fairly smooth.In our implementation of this second-order extrapolation in the simulation, we choseh0in (3.5) to be the subsampling bandwidth selectorhˆL2,mat the fictional sizem=pn. As a result,m∗(h0)≈m. We also usedc0=2, and estimatedm∗by using Eq. (2.10):m∗(c0h0)≈mˆ∗(c0hˆL2,m)=−∑1≤i<j≤nddhBc0hˆL2,m(Xi,Xj)∑1≤i<j≤nddhAc0hˆL2,m(Xi,Xj).Parameterawas then estimated byaˆ={hˆL2,m2(c02−1)}−1log{c05mˆ∗(c0hˆL2,m)/m}.Under this scheme, given a sample of sizen, the optimal bandwidth atn, as extrapolated fromm(m<n)based on Eq. (3.5) is the root of the following score function, denoted ashˆ2. We call it the second-order extrapolated bandwidth selector.(3.6)logn=logm−5(logh−loghˆL2,m)+aˆ(h2−hˆL2,m2).Eq. (3.5) indicates thatm∗∗(h)is bigger thanC0h−5, so we would expecthˆ2to be smaller than the first-order extrapolation bandwidth selectorhˆ1for a given value ofm.To illustrate the advantage of the second-order extrapolation in comparison with the first-order extrapolation, we revisited the numerical study presented in Fig. 2 but now implemented the second-order extrapolation technique. In Fig. 4we compared the simulated density plots of the first-order and second-order bandwidth selectors when the fictional sample sizem=0.1n,0.3n,0.5nor1.0n. It is clearly seen that for small values ofm, the improvement of the bandwidth selector based on second-order extrapolation is noticeable.We will see in our simulation section that the optimal fictional size for both first and second-order extrapolation depends on the choice ofp=m/n. In real problems one cannot determine the optimal choice of the fictional sizem=pnthat minimizes the risk atn. As a result, when implementing the two-stage bandwidth selection procedure, one needs to either fix the choice ofmprior to bandwidth selection, or implement an automatic, data-driven method to pick the best choice ofmbased on a data set. Previous literature, such as Hall and Robinson (2009), Shah and Samworth (2012), and Shao (1993), suggest to usem≤n/2. However, in their papers there does not exist an automatic selection method for picking the best choice ofmin the subsampling (or bagging) procedure. Here we propose a nested cross-validation methodology in selecting the optimal fictional sample sizemthat overcomes the drawback of choosing fictional sizemsubjectively. This method seems to perform consistently well across a wide selection of distributions (Section  4.1). We note that this two-layer cross-validation is made computationally feasible by ourU-statistic estimation of the bandwidth selector curve.Letp=m/nbe the fixed proportion of data used in the fictional sample. We consider a cross-validation strategy for selectingp. Letfˆp,n∗be the estimator of the density based on extrapolation (first or second-order) for a data set of sizen∗by subsampling of sizem=pn∗. As we show below, from a sample of sizenwe can estimate theL2risk offˆp,n∗unbiasedly as a function ofpprovided thatn∗<n. We again acknowledge we will get more stable estimation whenn∗is not close ton, and so usen∗=n/2in our simulation. We choose the value ofpthat minimizes the estimated risk at sizen∗. We then use this selected value ofpto create the extrapolation estimator on the full sample ofndata points.Denote the data set of sizenasXn=(X1,…,Xn). We can take a random subsampleSof sizen∗<n, sayS=(X1∗,…,Xn/2∗)withn∗=n/2, without replacement out ofXn. We then consider a grid ofpvalues, i.e.pj(1≤j≤J). For each choice ofpand a subsampleS, one can realize the subsampling-extrapolation bandwidth selector by first finding the optimal bandwidth at sizepn∗and then extrapolating it frompn∗ton∗. We denote the extrapolated bandwidth at sizen∗ashˆp,S. Note thathˆp,Sis dependent on both the proportion choicepand the subsampleS.We want to consider theL2risk of using bandwidthhˆp,Sat sample sizen∗as a function ofpand then determine the optimal choice ofpat sizen∗. We denote the risk at sizen∗as(3.7)RL2(p,n∗)∝E{∫fˆp,n∗(x)2dx}−2E{∫f(x)fˆp,n∗(x)dx},wherefˆp,n∗(x)=n∗−1∑i=1n∗Khˆp(x−Xi), andhˆpis the bandwidth extrapolated frompn∗ton∗.Since the density estimatorfˆp,n∗(x)depends on both the random subsampleSand the choice ofp, the risk functionR(p,n∗)cannot be estimated by the standardUestimator in (2.9). However, one can estimate the first expectation in (3.7) unbiasedly by(nn∗)−1∑S{n∗−2∑i∑j(Khˆp∗Khˆp)(xi−xj)}. The second expectation in (3.7) can be estimated unbiasedly by(nn∗)−1∑S{(n−n∗)−1∑Xi∉Sfˆp,n∗(Xi∣S)},whereSrepresents a subset of sizen∗taken out ofXn.In practice, one can repeatedly draw subsamples of sizen∗out ofXn, denoted asS1,…,SB. For a particular value ofp, we denote the first-order extrapolated bandwidth at sizen∗ashˆp,S1,…,hˆp,SB. The estimated risk of using proportionpat sizen∗is then(3.8)RˆL2(p,n∗)∝1B∑b=1B{1n∗2∑i∑j(Khˆp,Sb∗Khˆp,Sb)(xi−xj)}−2B∑b=1B{1n−n∗∑Xi∉Sbfˆp,n∗(Xi∣Sb)}.There exists a choice ofpthat minimizesRˆL2(p,n∗)which is the optimal proportionpat sizen∗. We might hope that the bestpforn∗also yields satisfactory performance at sample sizen. If the optimal choice ofpdoes not largely rely on the subsample sizen∗, then it would be reasonable to identify the optimal choice ofpfornbased on a smaller sizen∗using the nested cross-validation methodology.Remark 3Marron (1987) presents a method that is closely related to bagging cross-validation which he calls partitioned cross-validation. Marron (1987) shows that the proportion of subsample,p=m/n, achieves its optimal value atpMarron=(σ/C1)−5/4n−3/8, whereC1={[∫K2(x)dx]3/5}{∫x4K(x)dx}{[∫(f(2)(x))2dx]2}20{[∫x2K(x)dx]11/5}{[∫(f(2)(x))2dx]8/5},andσ2=8{∫(f(x))2dx}{∫[K∗(K−L)(x)−(K−L)(x)]2dx}25{[∫K2(x)dx]7/5}{[∫x2K(x)dx]6/5}{[∫(f(2)(x))2dx]3/5}.Here, functionLis defined asL(x)=−xK′(x), and∗is the convolution operator. The constant∫[K∗(K−L)(x)−(K−L)(x)]2dxin the definition ofσ2can be obtained with the help of Corollary 6.4.1. in Aldershof et al. (1995). We refer to Marron’s formula as MCV. Based on Marron’s formula,pgoes to zero asngoes to infinity. To compute the optimal partition size, one needs to estimate the integrated square derivatives of the unknown density function using two-stage estimators such as those proposed in Jones and Sheather (1991). We will compare Marron’s formula with our nested cross-validation proposal through simulation studies in Section  4.1.We have now developed four possible methods for bandwidth selection based on subsampling plus extrapolation: we have the first- and second-order extrapolations carried out at a fixed, pre-chosenp, plus the two extrapolation methods usingpchosen from a nested cross-validation. In order to investigate the performance of the proposed two-stage bandwidth selection procedures, we conducted the following simulation studies.We considerR=500random samples of sizen(n=100or 200) drawn independently from a certain distribution. A list of the seven distributions under consideration can be found in Table 1, among which there are bimodal/multimodal, outlying, and heavy-tailed distributions. A particularly difficult density is the claw, with 5 extreme spikes, each with only 10% of the data. As we will see later in Table 4, it will create an outlier in our results. Other literature that consider the claw density and notice its unusual behavior include Marron and Wand (1992) and Loader (1999).For each subset taken out ofXn, we consider ten possible values ofp=m/n, i.e.p=0.1,0.2,…,1.0. In selecting the optimal choice ofp, we first compare the performance between the nested cross-validation withn∗=n/2and Marron’s formula (see Remark 3). We compute percent of times eachpvalue is selected out of the 500 replications based on each method. Tables 2 and 3reveal that the selection ofpis quite noisy forn=100, but does improve forn=200. In making comparisons between the two methods, one should notice that Marron’s optimalpis not exactly the minimizer of Eq. (3.7) but the asymptotic minimizer ofMSE(p)=E{(hˆp−hMISE)2}, withhMISEbeing the minimizer of MISE. Regardless of the different target functions for minimization, Marron’s formula has less variation than our nested method, and so might be expected to show somewhat better performance. We will investigate this point in our simulation section.Next, we conduct an empirical investigation of how well one could estimate the optimalpusing a nested cross-validation withn∗=n/2. The optimal proportionpopt,nthat is dependent on sample sizen, is the minimizer of Eq. (3.7) withn∗=n. For several samples from each distribution, we plot the empirical expected integrated square error (EISE) curve as a function ofpbased on formula (3.8) in order to see whether we could do a good job at estimating the risk as a function ofp. In Fig. 5each of the panels displays three empirical curves (dashed curve) based on three random samples of sizen=100, taken from the corresponding distribution, and the theoretical true risk curve (solid curve) at sizen=100. (The curves for t(2) distribution are omitted here, as they show similar pattern as in the normal case.) Although the empirical curves are quite variable, the shape of each curve seems to follow the truth in most of the cases except the claw density. Thus, we will use the nested cross-validation methodology in the following discussions.We then compare the performance of different bandwidth selectors in terms of the efficiency ratio,MISEopt/EISE(hˆp), where EISE stands for expected integrated square error. We use the exactMISE(h)formula in Theorem 2.1 of Marron and Wand (1992) to compute the theoretical optimal bandwidth for normal mixtures, and approximate the theoretical optimal bandwidth for t(2) by simulation. Notice that in practice without replication of size-nsamples, the optimal choice ofpat sizenis not obtainable. We first determine how the optimal choice ofpvaries over our sampling distributions. In Table 4EX1 and EX2 stand for first- and second-order extrapolations respectively. In the last two columns of Table 4, one can see the relative efficiency ratio that can be attained when one uses the optimal value ofpfor that density. The optimalpseems to vary somewhat over the densities. One can also see that the second-order extrapolation, with optimalp, seems to do better than first-order extrapolation. After comparing the EISE over a grid of values ofp, we choose to usep=0.3for the fixed-pfirst-order extrapolation andp=0.2for the fixed-psecond-order extrapolation; these values seem to yield satisfactory results across a wide range of distributions.Then, we focus on the comparison between the four proposed bandwidth selectors, i.e. the first- and second-order extrapolated bandwidth selectors with pre-chosenpor with nested cross-validation, and the unbiased cross-validated bandwidth selector. In addition, we also include the comparison between Marron’s formula (see Remark 3) and nested cross-validation in selecting the optimalpin the context of first-order extrapolation. Because Marron (1987) only focuses on the study of first-order extrapolation, the comparison between Marron’s optimalpand the nested cross-validation method is only fair for the first-order extrapolated bandwidth selectorhˆ1. We use MCV to stand for Marron’s formula and NCV to stand for nested cross-validation. In formula (3.8)hˆpis set to be the first-order extrapolated bandwidth for EX1 and the second-order extrapolated bandwidth for EX2. The realization of the second-order extrapolated bandwidth selector is based on settingh0=hˆL2,mandc0=2.Our first observation from Table 4 is that the results from the claw density are distinctly different from the rest. This density is considered in Loader (1999) to demonstrate that plug-in methodologies could perform very poorly relative to conventional cross-validation. Van Es (1992) shows that the relative rate of convergence of ordinary cross-validation bandwidth selector is faster for non-smooth cases, such as in the case of claw density. It is noted in Marron and Wand (1992) that the true MISE curve of the claw density has local minima when the sample sizen≤53(see Fig. 6). That is,m∗(h)function has the∽shape in the region ofmof interest as seen in Fig. 7. It is clear from this plot why extrapolation of this curve can work so poorly at some sample sizes. Only whennincreases to above 100, does the MISE curve start to have an obvious global minima. In this example, we have found that the efficiency ratio for the plug-in bandwidth selector of Sheather and Jones (1991) is as small as 9.9% forn=100and 6.1% forn=200. Our extrapolation methods do perform much better than plug-in methods in the case of claw density. Moreover, a user who follows our advice not to use extrapolation methods or plug-in whenmˆ∗(h)is non-monotonic in the region of interest would end up using conventional cross-validation most of the time.If we ignore the claw density, we can make the following general observations: If we compare the methods that use first-order extrapolation, it is clear that for both sample sizesn=100andn=200, the fixed-pmethod yields slightly better results than nested cross-validation. The performance of Marron’s formula is similar to our proposed nested cross-validation in the context of first-order extrapolation at sample sizen=100, but MCV shows a small systematic superiority over NCV atn=200. However, the realization of Marron’s formula involves complex formulas as defined in Remark 3. Overall, we could obtain over 80% efficiency in all cases. The fixed-psecond-order extrapolation is almost a clear winner over the fixed-pfirst-order extrapolation. Moreover, the nested cross-validation using second-order extrapolation gives very similar performance as the fixed-psecond order extrapolation. Both seem promising bandwidth selection tools.In conclusion, we note that fixed-pfirst-order extrapolation is a computationally inexpensive way to improve upon standard cross-validation whenm∗(h)is reasonably behaved. For additional refinements, one can apply the second-order extrapolation.Savchuk et al. (2011, 2010) and Mammen et al. (2012) discuss a modification of the unbiased cross-validation (UCV) method, called indirect cross-validation (ICV), that aims to reduce the large variability of UCV bandwidth selector with the help of a selection kernel.Given a selection kernel of the following formL(u;α,σ)=(1+α)ϕ(u)−ασϕ(u/σ),indirect cross-validation is to first find the UCV bandwidth selector based onL, sayhˆL, and then rescale it to a bandwidth using Gaussian kernelϕ. We denote the latter bandwidth selector ashˆϕ. They argue that the conventional cross-validation works more stably with a more complicated kernel function than a second-order Gaussian kernel in bandwidth selection. The relationship betweenhˆLandhˆϕis approximated based on expressions of their asymptotic optimal bandwidth, which can be written ashˆϕ=(R(ϕ)μ2L2R(L)μ2ϕ2)1/5hˆL,where the rescaler only depends on the selection kernel.Following a referee’s suggestion, we will show below a numerical comparison between our proposed subsampling-extrapolation bandwidth selectors and the ICV bandwidth selector. To be comparable to the results in Savchuk et al. (2011, 2010), we consider the same simulation setting and choices of distribution functions: we considerR=1000independent samples of sizen(n=100and 250) generated randomly from standard normal or a bimodal normal mixture0.5N(−1,4/9)+0.5(1,4/9). Notice that their bimodal normal mixture is essentially the same as our Mixture 1 shown in Table 1. In addition, we computeEˆ{ISE(hˆ)/ISE(hˆ0)}as the measure to evaluate the performance of each method. This measure is used in Savchuk et al. (2011, 2010) to evaluate the performance of the ICV bandwidth selector. It can be seen in Table 5that our proposed bandwidth selectors perform comparably, or even better, than the indirect cross-validation bandwidth selector.Remark 4Although indirect cross-validation bears similarity with our subsampling-extrapolation method, we use extrapolation techniques differently. Their method is to shift the optimal risk curve based on a selection kernel, saymL∗, to one that is based on a second-order Gaussian kernel, saymϕ∗, at sample sizen. In comparison, our method is to move along the samemϕ∗curve from a subsample sizemto the original sample sizen. It seems possible to implement extrapolation techniques in indirect cross-validation. That is, one can implement subsampling-extrapolation method on the risk curve based on a selection kernel, then shift the selected bandwidth to one that is based on a Gaussian kernel. However, the investigation of this possibility is beyond the scope of this paper.The asymptotic properties of the first-order extrapolated bandwidth selectorhˆ1is easy to obtain, since it is a simple multiple of the subsampling bandwidth selector. Whenp=m/nis considered as a fixed constant, Hall and Robinson (2009) show that the relative convergence rate ofhˆ1is of ordern−1/10. Although this is the same rate as for the UCV bandwidth selector, the asymptotic variance inhˆ1is reduced by usingmless thann. More specifically, the asymptotic variance can be written as(m/n)4/5(C1+(m/n)C2), where bothC1andC2are constants. The asymptotic variance can be reduced by a factor of(m/n)4/5to say the least. IfC1is much smaller thanC2, the reduction could be of a factor of(m/n)9/5. When one considers half-sampling, the reduction in the asymptotic variance is around 50%.Marron (1987) considers the case whenpis dependent on sample sizen. Marron (1987) shows that the optimal choice ofpthat minimizes AMISE is of ordern−3/8(see Remark 3). With this optimal value ofpthe relative rate of convergence ofhˆ1can achieven−1/4, as shown in Eq. (3.3) in Marron (1987). We have verified that the result in Marron (1987) agrees with Hall and Robinson (2009) whenpis considered constant.We have noticed from numerical results that the second-order extrapolated bandwidth selector does improve over the first-order extrapolation method. Here we want to investigate whetherhˆ2has better asymptotic properties thanhˆ1.Denoteg(h)=log(m/n)−5(logh−loghˆL2,m)+a(h2−hˆCV,m2), wherehˆ2is the solution ofg(h)=0. Using Taylor series to expandg(h)aroundhˆ1, we havehˆ2≈hˆ1−a5p−2/5(1−p2/5)hˆ13.Becausea>0and0<p<1,hˆ2reduces the positive bias inherited inhˆ1.Whenpis a fixed constant, it is easy to see that the relative convergence rate ofhˆ2is the same ashˆ1. Whenpis dependent onn, one can follow the proof in Marron (1987) and show that the optimal value ofp, as a function ofn, that minimizes the asymptotic mean square error ofhˆ2is of ordern−3/8. With this choice ofp, the relative convergence rate ofhˆ2can achieven−1/4. (Please see Appendix for more details.)In short, the relative rate of convergence for the second-order extrapolated bandwidth selector is the same as the first-order extrapolation method in both cases thatpis a fixed constant andpis dependent onn. However, we have illustrated that it can improve upon the first-order extrapolation methodology and reduce the positive bias in finite sample scenarios.This paper has been focusing on the discussion of one particular scenario, risk estimation and bandwidth selection in a kernel density estimator. However, the proposed subsampling-extrapolation procedures can be easily generalized to other important problems in which one seeks an optimal smoothing parameter as long as some basic asymptotic results about rates of convergence are known. In addition, the subsampling-extrapolation technique could be in theory extended to other interesting applications, such as variance estimation and quantile estimation; we will study these applications in another paper. In particular, when considering the estimation of aU-statistic variance, the application of extrapolation techniques can enable one to relax the restriction of the kernel size needed in the unbiased variance estimator of a generalU-statistic devised in Wang and Lindsay (2014).From another aspect, the two-stage bandwidth selection procedure can be easily applied to cases where the risk is evaluated based on the Kullback–Leibler distance. We have preliminary simulation result showing that similar conclusions hold when one changes the loss function fromL2to Kullback–Leibler distance. The asymptotic properties of Kullback–Leibler risk (also called likelihood cross-validation) are quite complex, as discussed in Hall (1987). van Es (1991) found the rate of convergence of a likelihood cross-validation bandwidth selector for a bounded kernel functionKover the unit interval, a more general result for any arbitrary kernel does not seem present.There are a number of possible ways to tune our methods. For example, we usedn∗=n/2for nested cross-validation without further inspection. We did not try to tune the risk estimation forp, even though the selection mechanism showed some significant bias atn=100. Notice from Tables 2 and 3 that at sample sizen=100, nested cross-validation tends to over-select small values ofp. One possible solution to avoid selecting smallpis to implement the “1-SE rule” (Breiman et al., 1984) in nested cross-validation based on a reasonable estimation of the standard error of the risk estimator. According to the 1-SE rule, one would choose the largestpvalue that leads to a risk estimateRˆL2(p,n∗)no more than one standard error above the minimum risk score. We have investigated the application of jackknife variance estimator in the 1-SE rule. However, due to the large positive bias of jackknife variance estimator, there was no gain in applying the 1-SE rule in this scenario. In addition, as seen in Fig. 5 many of the empirical risk curves are fairly flat, indicating that choosing a smallerpvalue in the stage of nested cross-validation may not have a large effect on the final bandwidth selector.

@&#CONCLUSIONS@&#
