@&#MAIN-TITLE@&#
Risk-sensitive dividend problems

@&#HIGHLIGHTS@&#
We deal with the dividend payout problem with unbounded reward functions.We consider the exponential function and the power function to aggregate the future total discounted rewards.We use the dynamic programming techniques to provide a solution.Making use of the model under consideration we provide the structure and properties of the optimal strategy.

@&#KEYPHRASES@&#
Markov decision process,Dividend payout,Risk aversion,History-dependent policy,Fixed point problem,

@&#ABSTRACT@&#
We consider a discrete time version of the popular optimal dividend payout problem in risk theory. The novel aspect of our approach is that we allow for a risk averse insurer, i.e., instead of maximising the expected discounted dividends until ruin we maximise the expected utility of discounted dividends until ruin. This task has been proposed as an open problem in Gerber and Shiu (2004). The model in a continuous-time Brownian motion setting with the exponential utility function has been analysed in Grandits et al. (2007). Nevertheless, a complete solution has not been provided. In this work, instead we solve the problem in discrete time setup for the exponential and the power utility functions and give the structure of optimal history-dependent dividend policies. We make use of certain ideas studied earlier in Bäuerle and Rieder (2011), where Markov decision processes with general utility functions were treated. Our analysis, however, includes new aspects, since the reward functions in this case are not bounded.

@&#INTRODUCTION@&#
The dividend payout problem in risk theory has been introduced by de Finetti (1957) and has since then been investigated under various extensions during the decades up to now; see, for instance, Grandits et al. (2007) or Yao et al. (2011). The task is to find in a given model for the free surplus process of an insurance company, a dividend payout strategy that maximises the expected discounted dividends until ruin. Typical models for the surplus process are compound Poisson processes, diffusion processes, general renewal processes or discrete time processes. The reader is referred to Albrecher and Thonhauser (2009) and Avanzi (2009), where an excellent overview of recent results is provided.In Gerber and Shiu (2004) the authors propose the problem of maximising the expected utility of discounted dividends until ruin instead of maximising the expected discounted dividends until ruin. This means that an insurance company is equipped with some utility function that helps it to measure the accumulated dividends paid to the shareholders. If this utility is increasing and concave, the company is risk averse (see Remark 2.2). To the best of our knowledge, there is only one work (Grandits et al., 2007), in which this idea was taken up. More precisely, Grandits et al. (2007) consider a linear Brownian motion model for the free surplus process and apply the exponential utility function to evaluate the discounted dividends until ruin. It turns out that the mathematics involved in the analysis of this problem is quite different from the one used in the risk neutral case and only partial results could be obtained. In contrast to the same problem with a risk neutral insurance company, where the optimal dividend payout strategy is of a barrier type (see e.g., Asmussen and Taksar, 1997), the authors in Grandits et al. (2007) are not able to identify the structure of the optimal dividend policy rigorously. They show imposing some further assumptions that there is a time dependent optimal barrier.We study the same problem but with a discrete time surplus process. The risk neutral problem within such a framework can be found in Section 1.2 in Schmidli (2008) or in Section 9.2 in Bäuerle and Rieder (2011). By making use of the dynamic programming approach the authors in Bäuerle and Rieder (2011) and Schmidli (2008) prove that the optimal dividend payout policy is a stationary band-strategy. Albrecher et al. (2011), on the other hand, consider a discrete time model that is formulated with the aid of a general Lévy surplus process but the dividend payouts are allowed only at random discrete time points. This version can again be solved by the dynamic programming arguments. However, the problem with a general utility function is more demanding. Like in the continuous time setting (Grandits et al., 2007), it requires a sophisticated analysis. It is worth mentioning that Markov decision processes with general utility functions have been already studied in Kadota et al. (1998) and Bäuerle and Rieder (2014). Moreover, there are also some papers, where the specific utility functions are considered. For example, Jaquette (1973); 1976) and Chung and Sobel (1987) are among the first who examined discounted payoffs in Markov decision processes with the decision maker that is equipped with a constant risk aversion, i.e., grades her random payoffs with the help of the exponential utility function. The common feature of all the aforementioned papers is the fact that they deal with bounded rewards or costs. Therefore, their results cannot be directly applied to our case, where the payoffs are unbounded. We make use of the special structure of the underlying problem and show that the optimal dividend payout policy is a time dependent band-strategy. The value function itself can be characterised as a solution to a certain optimality equation. Furthermore, we also study the dividend payout model with the power utility function. As noted in Bäuerle and Rieder (2014), the original Markov decision process can then be viewed as a Markov decision process defined on the extended state space. We employ these techniques to solve our model, but only in the first step, where we use an approximation of the value function in the infinite time horizon by value functions in the finite time horizons. In contrast to the exponential utility case, we can only partly identify the structure of the optimal dividend payout policy. However, we are able to show that there is a barrier such that when the surplus is above the barrier, it is always optimal to pay down to a state below the barrier. The value function is again characterised as a solution to some optimality equation. Summing up, the optimal dividend payout problem with the exponential utility function can be solved completely in the discrete time case, in contrast to the continuous-time problem in Grandits et al. (2007), whilst for the case with the power utility function we are at least able to identify the important global structure of the optimal policy.The paper is organised as follows. In the next section we introduce the model together with mild assumptions and general history-dependent policies. Section 3 is devoted to a study of the exponential utility case. We show first that the value function J for discounted payoffs satisfies an optimality equation and give a lower and an upper bound for J. Then, we identify properties of the minimiser of the right-hand side of the optimality equation. This enables us to show that the minimiser indeed defines an optimal policy, which is a non-stationary band-policy. The non-stationarity is based only on the time-dependence. The power utility case is treated in Section 4. We pursue here a little different approach, but it also leads to an optimality equation. The policies obtained in this setting are really history-dependent. Nonetheless, we are still able to show that the optimal policy is of a barrier-type. In Section 5 we provide the policy improvement algorithm for the model with the exponential utility. Finally, Section 6 is devoted to concluding remarks and open issues.We consider the financial situation of an insurance company at discrete times, sayn∈N0:=0,1,2,…. Assume there is an initial surplusx0=x∈X:=Zand x0 ≥ 0. The surplus xn + 1 at time n + 1 evolves according to the following equation(2.1)xn+1=xn−an+Zn+1,ifxn≥0andxn+1=xn,ifxn<0.Here an∈ A(xn) ≔ {0, …, xn} denotes the dividends paid to the shareholders at time n, and Zn + 1 represents the income (possibly negative) of the company during the time interval from n to n + 1. More precisely, Zn + 1 is the difference between premium and claim sizes in the (n + 1)st time interval. Further, we assume that Z1, Z2, … are independent and identically distributed integer-valued random variables with distribution(qk)k∈Z,i.e.,P(Zn=k)=qk,k∈Z. A dividend payout problem in the risk theory can be viewed as a Markov decision process with the state space X, the set of actions A(x) available in state x (for completeness, we put A(x) = {0} for x < 0) and the transition probability q( · |x, a) of the next state, when x is the current state and a is the amount of dividend paid to the shareholders. Note that the dynamics of Eq. (2.1) implies that q(y|x, a) = qy − x + afor x ≥ 0 and q(x|x, a) = 1 if x < 0. For the set of admissible pairs D ≔ {(x, a): x ∈ X, a ∈ A(x)} we define the functionr:D↦Ras r(x, a) = a for x ∈ X.The feasible history spaces are defined as follows Ω0 = X, Ωk= Dk× X and Ω∞ = D∞. A policyπ=(πk)k∈N0is a sequence of mappings from Ωkto A such that πk(ωk) ∈ A(xk), where ωk= (x0, a0, …, xk) ∈ Ωk. Let Γ be the class of all functions g: X↦A such that g(x) ∈ A(x). A Markov policy isπ=(gk)k∈N0where each gk∈ Γ. By Π and ΠMwe denote the set of all history-dependent and Markov policies, respectively. By the Ionescu–Tulcea theorem (Neveu, 1965), for each policy π and each initial state x0 = x, a probability measurePxπand a stochastic process(xk,ak)k∈N0are defined on Ω∞ in a canonical way, where xkand akdescribe the state and the decision at stage k, respectively. ByExπwe denote the expectation operator with respect to the probability measurePxπ.Ruin occurs as soon as the surplus gets negative. The epoch τ of ruin is defined as the smallest integer n such that xn< 0. The question arises as to how the risk-sensitive insurance company, equipped with some utility function will choose its dividend strategy. More precisely, we shall consider the following optimisation problemsupπ∈ΠExπUγ(∑k=0∞βkr(xk,ak))=supπ∈ΠExπUγ(∑k=0τ−1βkak),x≥0,where β ∈ (0, 1) is a discount factor and either(1)Uγis the exponential utility function, i.e.,Uγ(x)=1γeγxwith γ < 0, orUγis the power utility function, i.e., Uγ(x) = xγwith γ ∈ (0, 1).Let Z be a random variable with the same distribution as Z1. Throughout the paper the following assumptions will be supposed to hold true.(A1)EZ+<+∞,where Z+ = max {Z, 0};P(Z<0)>0.Assumption (A2) allows to avoid a trivial case, when the ruin will never occur under any policy π ∈ Π.Remark 2.1In our study, we assume that the random variables {Zn} only take integer values and the initial capital is also integer. From the proof of Lemma 1.9 in Schmidli (2008), it follows that in our problem we can restrict without loss of generality to the integer dividend payments.If the function Uγis strictly concave and increasing as in our case, then the quantityUγ−1(E[Uγ(X)])is called a certainty equivalent of the random variable X. From the optimisation’s point of view it does not matter which valueUγ−1(E[Uγ(X)])orE[Uγ(X)]we study, because the inverse functionUγ−1is monotonic. However, the certainty equivalent has an important meaning. If we apply the Taylor expansion, then the certainty equivalent can be written as followsUγ−1(E[Uγ(X)])≈EX−12l(EX)Var[X],wherel(y)=−Uγ′′(y)Uγ′(y)is called the Arrow–Pratt function of absolute risk aversion. Hence, the second term accounts for the variability of X (for a discussion see Bielecki and Pliska, 2003). If Uγis concave like in our case, then l( · ) ≥ 0 which means that the variance is subtracted. This fact implies that the decision maker is risk averse.In this section we assume that the insurer is risk averse and grades her random payoffs by taking the expectations of the exponential utility function of these random rewards. More precisely, we assume that the decision maker is equipped with the constant risk coefficient γ < 0. The objective of the risk averse insurer is to maximise the expected discounted payoff function:J˜π(x)=ExπUγ(∑k=0∞βkr(xk,ak))and to find a policy π* ∈ Π (if exists) such that(3.1)J˜(x):=J˜π*(x)=supπ∈ΠJ˜π(x)for all x ∈ X. It is obvious that the optimal policy π* would depend on γ and β. Clearly,J˜(x)=1γfor every x < 0.Our discounted model with the exponential utility function reveals some kind of non-stationarity that is implied by a discount factor. Therefore, one can extend the state space in the following wayX˜:=X×I,with I ≔ [γ, 0) (cf. also Di Masi and Stettner, 2000; Bäuerle and Rieder, 2014). If the process is in the state (x, θ) and the insurer selects an action a ∈ A(x), then the probability of moving to a next state (x′, θ′) is q(x′|x, a), if θ′ = θβ and is 0, ifθ′≠θβ. The second component of the state space keeps the track of the discount factor that changes over time in a deterministic way.Furthermore, we can define an extended history-dependent policyσ=(σk)k∈N0,where σkis a mapping from the set of extended feasible histories up to the kth day to the action set A such that σk(hk) ∈ A(xk) with hk= (x0, θ, a0, …, ak − 1, xk, θβk), k ≥ 1 and h0 = (x0, θ). Recall that (xm, am) ∈ D form∈N0. Let Ξ be the set of all extended history-dependent policies. Note that for any σ ∈ Ξ (θ ∈ I is fixed), there exists a policy π ∈ Π that is equivalent to σ in the following sense:πk(·|ωk):=σk(·|hk),k∈N0.Obviously, π depends on θ ∈ I. Therefore, for simplicity of notation we shall still use the original policies π ∈ Π, and the expectation operatorExπ,where x is the first component of the initial state. The dependence on θ ∈ I will be denoted by adding the second variable to the value function.For any initial state(x,θ)∈X˜we defineJ(x,θ):=infπ∈ΠJπ(x,θ),whereJπ(x,θ):=Exπ(exp{θ∑k=0∞βkr(xk,ak)})for π ∈ Π. Obviously, 0 ≤ Jπ(x, θ) ≤ 1 for all x ∈ X, θ ∈ I and π ∈ Π. Observe next that our optimisation problem (3.1) is equivalent to the problem of minimising Jπ(x, γ) over π ∈ Π. ByC(X˜)we denote the space of bounded continuous real-valued functions onX˜.Theorem 3.1For every(x,θ)∈X˜the function J is a solution to the following discounted optimality equation(3.2)J(x,θ)=mina∈A(x)[eθr(x,a)∑x′∈XJ(x′,θβ)q(x′|x,a)]=mina∈{0,…,x}[eθa(∑k=a−x∞J(x−a+k,θβ)qk+∑k=−∞a−x−1qk)]Clearly, J(x, θ) = 1 for x < 0 and all θ ∈ I. Consider the truncated payoff functions rm(x, a) = min {m, r(x, a)} withm∈N. From Proposition 3.1 in Di Masi and Stettner (2000) there is a unique functionwm∈C(X˜)such that(3.3)wm(x,θ)=mina∈A(x)[eθrm(x,a)∑x′∈Xwm(x′,θβ)q(x′|x,a)]and wm(x, θ) = Jm(x, θ) for all(x,θ)∈X˜. Here, Jm(x, θ) denotes the optimal discounted payoff function with r replaced by rmin J(x, θ). Clearly, the sequence (wm(x, θ))m ∈ Nis non-increasing for each(x,θ)∈X˜. Therefore, limm → ∞wm(x, θ) ≕ w(x, θ) exists. It is obvious thatwm(x,θ)=Jm(x,θ)≥J(x,θ),for(x,θ)∈X˜.Hence,(3.4)w(x,θ)≥J(x,θ),for(x,θ)∈X˜.On the other hand, letting m → ∞ in (3.3), making use of the dominated convergence theorem and the fact that A(x) is finite for each x ∈ X, we infer that(3.5)w(x,θ)=limm→∞mina∈A(x)[eθrm(x,a)∑x′∈Xwm(x′,θβ)q(x′|x,a)]=mina∈A(x)[eθr(x,a)∑x′∈Xw(x′,θβ)q(x′|x,a)]for(x,θ)∈X˜. Hence, for any a ∈ A(x)w(x,θ)≤eθr(x,a)∑x′∈Xw(x′,θβ)q(x′|x,a).Iterating this inequality (n − 1) times we conclude thatw(x,θ)≤Exπ(exp{θ∑k=0n−1βkr(xk,ak)}w(xn,θβn)).Since w ≤ wn≤ 1, we have thatw(x,θ)≤Exπ(exp{θ∑k=0n−1βkr(xk,ak)}).Letting n → ∞ and applying the dominated convergence theorem we have that w(x, θ) ≤ Jπ(x, θ) for(x,θ)∈X˜. Since the policy π was chosen arbitrarily, we get that(3.6)w(x,θ)≤J(x,θ),for(x,θ)∈X˜.Now the assertion follows from (3.4) and (3.6).□Among all functions w which satisfy Eq. (3.2) and have the property that w(x, θ) = 1 for x < 0 and w(x, θ) ∈ (0, 1] for all(x,θ)∈X˜,the value function J is the largest solution. This fact follows from the last part of the proof.Theorem 3.1 was proved in the literature for the general state space, weakly continuous transition probabilities and bounded costs or rewards (Bäuerle and Rieder, 2014; Di Masi and Stettner, 2000). However, we deal with unbounded payoffs, therefore we have to truncate them at the level m and then let m tend to infinity. Such a procedure may have a meaning from the numerical point of view.Let us now consider the policy π+ = (g, g…), where g(x) = x+ for every x ∈ X. Hence, this policy asks the insurer to pay out everything at each time point until ruin occurs. Denote byπ→the “1-shifted” policy forπ=(πk)k∈N0,that is,π→=(π→k)k∈N0,where(3.7)π→k(·|ωk)=πk+1(·|x0,a0,ωk)forωk∈Ωkandk∈N0.Lemma 3.4For any x ≥ 0 and θ ∈ I the following inequalities holdeθxh̲(θ)≤J(x,θ)≤eθxh¯(θ),withh̲(θ):=∏k=1∞E(exp{θβkZk+})andh¯(θ):=∑m=−∞∞Emπ+(exp{θ∑k=1τ−1βkxk})qm,where x1 = m and the empty sum is 0.We start with the upper bound. Since x ≥ 0, then τ ≥ 1. For the policy π+ we have thatJ(x,θ)≤Jπ+(x,θ)=Exπ+(exp{θ∑k=0τ−1βkxk})=eθxExπ+(exp{θ∑k=1τ−1βkxk})=eθxExπ+[Exπ+(exp{θ∑k=1τ−1βkxk}|a0,x1)](underπ+wehavethatx1=Z1)=eθxExπ+[EZ1π→+(exp{θ∑k=1τ−1βkxk})]=eθx(∑m=−∞∞Emπ+(exp{θ∑k=1τ−1βkxk})qm),where in the last equality we make use of the fact thatP(Z1=m)=qmandπ→+=π+.On the other hand, the lower bound can be obtained as follows. First, we claim that for π ∈ Π and(x,θ)∈X˜with x ≥ 0(3.8)eθx∏k=1n−1E(exp{θβkZk+})≤Exπ(exp{θ∑k=0n−1βkr(xk,ak)}).We proceed by induction. Clearly, eθx≤ mina ∈ A(x)eθr(x, a). Assume now that (3.8) holds for some n ≥ 1 and every x ≥ 0 and θ ∈ I. Letπ=(πn)n∈N0be any policy. Then, it follows thatExπ(exp{θ∑k=0nβkr(xk,ak)})≥eθr(x,π0(x))Exπ[Exπ(exp{θ∑k=1nβkr(xk,ak)}|a0,x1)]≥mina∈A(x)eθr(x,a)Exπ[1[x1≥0]Ex1π→(exp{θ∑k=1nβkr(xk,ak)})+1[x1<0]](by(3.8))≥mina∈A(x)eθaExπ[1[x1≥0]eθβx1∏k=2nE(exp{θβkZk+})+1[x1<0]]≥mina∈A(x)eθaExπ[1[x1≥0]eθβx1+1[x1<0]]∏k=2nE(exp{θβkZk+}).Furthermore, we have thatmina∈A(x)eθaExπ[1[x1≥0]eθβx1+1[x1<0]]≥mina∈A(x)eθa(∑k=a−x∞eθβ(x−a+k)qk+∑k=−∞a−x−1qk)≥mina∈A(x)eθa(∑k=1∞eθβ(x−a+k)qk+∑k=−∞0eθβ(x−a)qk)=mina∈A(x)eθa(1−β)(∑k=1∞eθβ(x+k)qk+∑k=−∞0eθβxqk)=eθx(1−β)eθβxEeθβZ1+=eθxEeθβZ1+.Hence, we conclude thatExπ(exp{θ∑k=0nβkr(xk,ak)})≥eθxEeθβZ1+∏k=2nE(exp{θβkZk+})=eθx∏k=1nE(exp{θβkZk+}).Therefore, (3.8) holds for everyn∈N,x ≥ 0, θ ∈ I and π ∈ Π. Now letting n → ∞ in (3.8) and making use of the dominated convergence theorem we obtain the lower bound for J(x, θ).□Note that since β < 1, we obtain by Jensen’s inequality and assumption (A1) thath̲(θ)≥exp(θ∑k=1∞βkEZk+)>0,θ∈I.This observation is essential in Theorem 3.8, where we have to take the logarithm ofh. Also note thath¯(θ)≤1since θ ∈ I.Let x ≥ 0. For any θ ∈ I let us denoteG(x,θ):=∑k=−x∞J(x+k,θ)qk+∑k=−∞−x−1qk,then(3.9)J(x,θ)=mina∈A(x)[eθaG(x−a,θβ)],x≥0.Lemma 3.6The function J( ·, θ) is decreasing for each θ ∈ I andJ(x,θ)≤eθ(x−v)J(v,θ)for x ≥ v ≥ 0.Suppose 0 ≤ v < x. Then, it follows thatJ(x,θ)=min{G(x,θβ),…,eθ(x−v−1)G(v+1,θβ),min{eθ(x−v)G(v,θβ),…,eθ(x−0)G(0,θβ)}}=min{G(x,θβ),…,eθ(x−v−1)G(v+1,θβ),eθ(x−v)J(v,θ)}≤eθ(x−v)J(v,θ).Observe that for v = x − 1 we obtain from the above inequality thatJ(x,θ)≤eθJ(x−1,θ)<J(x−1,θ).This fact finishes the proof.□Now letf*:X˜↦Abe the largest minimiser of the right-hand side in (3.9) for x ≥ 0 and let f*(x, θ) = 0 for x < 0 and all θ ∈ I. Then, f*( ·, θ) ∈ Γ.Lemma 3.7For x ≥ 0 it holds that f*(x − f*(x, θ), θ) = 0 andJ(x,θ)=eθf*(x,θ)J(x−f*(x,θ),θ).By (3.9) we have that J(x − f*(x, θ), θ) ≤ G(x − f*(x, θ), θβ). By Lemma 3.6 (set v ≔ x − f*(x, θ)) it follows thatJ(x,θ)=eθf*(x,θ)G(x−f*(x,θ),θβ)≥eθf*(x,θ)J(x−f*(x,θ),θ)≥J(x,θ).Thus we have equality and, in particular, J(x − f*(x, θ), θ) = G(x − f*(x, θ), θβ), which implies that a = 0 minimises the expression eθaG(x − f*(x, θ) − a, θβ). We claim that a = 0 is the only minimiser of the above expression. Note that, if f*(x, θ) = x, then J(x − f*(x, θ), θ) = J(0, θ) and the result holds true. If, on the other hand, f*(x, θ) < x, thenJ(x,θ)=eθf*(x,θ)G(x−f*(x,θ),θβ)<eθ(f*(x,θ)+a)G(x−f*(x,θ)−a,θβ)for a = 1, …, x − f*(x, θ). This fact implies that a = 0 is indeed the only minimiser and, consequently, f*(x − f*(x, θ), θ) = 0.□Letξ(θ):=sup{x∈N0:f*(x,θ)=0}. Thenξ*:=supθ∈Iξ(θ)<∞andf*(x,θ)=x−ξ(θ),forallx>ξ(θ).Fix θ ∈ I and let x ≥ 0 be such that f*(x, θ) = 0. Note that such x exists for each θ ∈ I, because f*(0, θ) = 0 for all θ ∈ I. From (3.9) we obtain that(3.10)J(x,θ)=∑k=−x∞J(x+k,θβ)qk+∑k=−∞−x−1qk.Furthermore, by (3.10), Lemmas 3.4 and 3.6 we have thatJ(x,θ)≥∑k=1∞J(x+k,θβ)qk+∑k=−∞0J(x,θβ)qk≥∑k=1∞eθβ(x+k)h̲(θβ)qk+∑k=−∞0eθβxh̲(θβ)qk=eθβxh̲(θβ)(∑k=1∞eθβkqk+∑k=−∞0qk)=eθβxh̲(θ).Hence,eθβxh̲(θ)≤J(x,θ)≤eθxh¯(θ),which implies thatx≤lnh¯(θ)−lnh̲(θ)θ(β−1)=:s(θ).The function s( · ) is continuous on I and is finite for each θ ∈ I. Additionally,limθ→0−s(θ)=limθ→0−(h¯′(θ)h¯(θ)−h̲′(θ)h̲(θ))/(β−1)=(∑k=1∞βkE(Zk+)−∑m=0∞Emπ+(∑k=1τ−1βkxk)qm)/(1−β)<+∞,which follows by assumption (A1). Thus, we have shown thatsupθ∈Iξ(θ)<+∞. Now let x > ξ(θ). We know from Lemma 3.7 that f*(x − f*(x, θ), θ) = 0, which implies by definition of ξ(θ) that f*(x, θ) ≥ x − ξ(θ). On the other hand, by (3.9) we obtain thatJ(ξ(θ),θ)≤eθ(f*(x,θ)−(x−ξ(θ)))G(ξ(θ)−f*(x,θ)+(x−ξ(θ)),θβ)=J(x,θ)e−θ(x−ξ(θ))≤J(ξ(θ),θ),where the last inequality follows from Lemma 3.6. Thus, because f* is the largest minimiser of (3.9), we obtain0=f*(ξ(θ),θ)≥f*(x,θ)−(x−ξ(θ))≥0,which implies that f*(x, θ) = x − ξ(θ).□Let x0 ≥ 0. If f*(x0, θ) = a0 and f*(x0 + 1, θ) > 0, then f*(x0 + 1, θ) = a0 + 1.By definition of f* we have thatJ(x0,θ)=eθa0G(x0−a0,θβ)×{≤eθaG(x0−a,θβ),fora=0,…,a0<eθaG(x0−a,θβ),fora=a0+1,…,x0,which further yields thateθ(a0+1)G(x0−a0,θβ)×{≤eθ(a+1)G(x0−a,θβ),fora=0,…,a0<eθ(a+1)G(x0−a,θβ),fora=a0+1,…,x0.Again from the definition of f* we obtain thatJ(x0+1,θ)=mina∈A(x0+1)eθaG(x0+1−a,θβ)=min{G(x0+1,θβ),mina∈A(x0)eθ(a+1)G(x0−a,θβ)}.Since f*(x0 + 1, θ) > 0, it holds thatG(x0+1,θβ)≥mina∈A(x0)eθ(a+1)G(x0−a,θβ).This fact, in turn, together with the previous observation yields by shifting the index thateθ(a0+1)G(x0−a0,θβ)×{≤eθaG(x0+1−a,θβ),fora=0,…,a0+1<eθaG(x0+1−a,θβ),fora=a0+2,…,x0+1.Thus, it follows that f*(x0 + 1, θ) = a0 + 1.□Recall now that γ is a constant risk averse coefficient of the insurer. Consider the following policyπ*:=(g˜0,g˜1,…),whereg˜n(·):=f*(·,γβn). We note that π* ∈ ΠM. Clearly, since γ ∈ I, then γβn∈ I for alln∈N0.Corollary 3.10Under policy π* the ruin occurs with probability 1, i.e.,Pxπ*(τ<+∞)=1for every x ∈ X.Assume that the surplus process equals x0 ∈ X. If x0 ≥ 0, then either x0 ≤ ξ(γ) or x0 > ξ(γ). However, from Theorem 3.8 we know that in both cases the risk reserve (surplus) just after dividend payment is always less or equal to ξ(γ) ≤ ξ*. Therefore, the ruin will occur, if there appears a sequence of length ξ* + 1 of negative incomes. But the probability that such a sequence appears, equals(3.11)P(Z1<0,…,Zξ*+1<0)=(∑m=−∞−1qm)ξ*+1,which is positive by (A2). If the ruin has not occurred up to the lth day, where l ≔ ξ* + 1, then again xl≤ ξ(γβl) or xl> ξ(γβl). But from Theorem 3.8in both cases the risk reserve just after dividend payment is always less or equal to ξ(γβl) ≤ ξ*. The probability that there exists a sequence of length ξ* + 1 of negative incomes is(∑m=−∞−1qm)ξ*+1. Thus, considering the statesxk(ξ*+1),k∈N0,we may define the following eventsAk={Zk(ξ*+1)+1<0,Zk(ξ*+1)+2<0…,Z(k+1)(ξ*+1)<0},k∈N0.By the second Borel–Cantelli lemmaP(Aki.o.)=1. Therefore, the ruin must occur.□The Markov policy π* is optimal, i.e.,J˜(x)=J˜π*(x)=1γJπ*(x,γ)=1γJ(x,γ)for x ∈ X.From Theorem 3.1 and the definition of π* we obtain for every x ∈ X thatJ(x,γ)=mina∈A(x)[eγr(x,a)∑x′∈XJ(x′,γβ)q(x′|x,a)]=eγg˜0(x)∑x′∈XJ(x′,γβ)q(x′|x,g˜0(x)).Assume that x ≥ 0. Iterating the last equality n times under the Markov policy π*, we obtain that(3.12)J(x,γ)=Exπ*(exp{γ∑k=0(τ−1)∧(n−1)βkak}(J(xn,γβn)1[τ≥n]+1[τ<n])).Observe now that0≤Exπ*(exp{γ∑k=0(τ−1)∧(n−1)βkak}J(xn,γβn)1[τ≥n])≤Exπ*1[τ≥n]=Pxπ*(τ≥n).But by Corollary 3.10,Pxπ*(τ≥n)→0as n → ∞. Hence, letting n → ∞ in (3.12) and making use of the dominated convergence theorem we obtain thatinfπ∈ΠJπ(x,γ)=J(x,γ)=Exπ*(exp{γ∑k=0τ−1βkak})=Jπ*(x,γ)for x ∈ X. The conclusion follows by multiplying the above display by the number 1/γ.□A function g ∈ Γ is called a band-function, if there exists numbersn∈N0andc0,…,cn,d1,…,dn∈N0such that dk− ck − 1 ≥ 2 for k = 1, …, n, 0 ≤ c0 ≤ d1 ≤ c1 ≤ d2 ≤ ⋅⋅⋅ ≤ dn≤ cnandg(x)={0,ifx≤c0x−ck,ifck<x<dk+10,ifdk≤x≤ckx−cn,ifx>cn.A Markov policyπ=(gm)m∈N0is called a band-policy, if gmis a band-function for everyn∈N0.The optimal Markov policy π* is a band-policy.Recall thatg˜n(·)=f*(·,γβn)forn∈N0. By Theorem 3.8 we haveg˜n(x)=x−ξ(γβn)for all x > ξ(γβn). For x ≤ ξ(γβn) we have to distinguish different cases. Ifg˜n(x)=0for all x = 0, …, ξ(γβn), then clearlyg˜nis a band-function. If there exists a 0 < x0 ≤ ξ(γβn) such thatg˜n(x)=0for x = 0, …, x0 − 1 andg˜n(x0)>0,then by Lemma 3.9g˜n(x0)=1. If furtherg˜n(x0+m)>0for m = 1, …, ξ(γ) − x0 − 1 then by inductiong˜n(x0+m)=g˜n(x0+m−1)+1=⋯=g˜n(x0)+m=m+1.Ifg˜n(x0+1)=0we either haveg˜n(x)=0for x = x0 + 1, …, ξ(γβn) or there exists an x1 such that x0 < x1 ≤ ξ(γβn) andg˜n(x0+m)=0for m = 1, …, x1 − x0 − 1 andg˜n(x1)>0. Now we proceed in the same way as with x0. After a finite number of steps we reach ξ(γβn). In any caseg˜nis a band-function.□In the risk neutral dividend payout problem, the optimal policy is a stationary band-policy, i.e., it consists of the same band-function at all time points. The risk neutral problem can formally be obtained as a limitlimγ→01γ(eγx−1). Hence, the exponential utility function only implies some kind of non-stationarity of the optimal policy and thus does not really make it necessary to consider history-dependent policies.In this section we assume that the insurer is equipped with the power utility function Uγ(x) = xγ, where γ is a fixed number from the interval (0, 1). The decision maker wishes to maximise the following expected discounted payoffJ^π(x):=ExπUγ(∑k=0∞βkr(xk,ak))and to find a policy π* ∈ Π (if exists) such that(4.1)J^(x):=supπ∈ΠJ^π(x)=J^π*(x)for all x ∈ X. Clearly,J^(x)=0for x < 0. In Lemma 4.4we show that under our assumptionsJ^(x)<+∞for each x ≥ 0. Moreover, note that for x ≥ 0J^(x):=supπ∈ΠExπUγ(∑k=0τ−1βkak).Contrary to the exponential utility function, the power utility function reveals certain non-separability, that is implied by the fact that the expectation operator is only linear. Therefore, we again extend the state space by defining the new state spaceX^:=X×[0,∞)(cf. Bäuerle and Rieder, 2014). In this case, the second component is responsible for the accumulated payoffs so far. If the process is in the state (x, y) and the insurer selects an action a ∈ A(x), then the probability of moving to a next state (x′, y′) is q(x′|x, a), ify′=y+aβand is 0, ify′≠y+aβ. Hence, we can observe that the second component is again established in a deterministic way, but it differs from the previous case, since y′ depends on the action chosen by the insurer.Let us define the feasible extended histories of the process up to the kth day as follows11We use the same symbol hkas in the previous section to denote an extended feasible history of the process up to the kth day. But there is no confusion, since hkin this subsection refers only to the power utility case. The same remark applies to the policy σ, the set Ξ and the functions J and Jπdefined below.h0=(x0,y0)andhk=(x0,y0,a0,x1,y1,…,ak−1,xk,yk),k≥1,where (xm, am) ∈ D for eachm∈N0and with ym + 1 given by the recurrence equationym+1:=ym+amβ,m∈N0.Then, we can define, as usual, an extended history-dependent policyσ=(σk)k∈N0,where σkis a mapping from the set of feasible extended histories up to the kth day to the action set A such that σk(hk) ∈ A(xk) with hkdefined above. Let Ξ be the set of all such policies. Note that for any σ ∈ Ξ (y ≥ 0 is fixed), there exists a policy π ∈ Π that is equivalent to σ in the following sense:πk(·|ωk):=σk(·|hk),ωk∈Ωk,k∈N0.Obviously, π must depend on y. Thus again, for simplicity of notation we shall still use the original set of policies Π, and the expectation operatorExπ,where x is the first component of the initial state. The dependence on y ≥ 0 of a policy will be indicated by writing the second variable to the value function.In what follows, we put forn∈N,π ∈ Π, x ≥ 0 and y ≥ 0Jn,π(x,y):=Exπ(∑k=0(n−1)∧(τ−1)βkr(xk,ak)+y)γ=Exπ(∑k=0(n−1)∧(τ−1)βkak+y)γandJn(x,y):=supπ∈ΠJn,π(x,y).Moreover, for π ∈ Π, x ≥ 0 and y ≥ 0 we setJ(x,y)=supπ∈ΠJπ(x,y),whereJπ(x,y)=Exπ(∑k=0τ−1βkak+y)γ.If x < 0, then J(x, y) = yγfor y ≥ 0. Obviously,J(x,0)=J^(x).Before we formulate our first result, we introduce a specific subset of policiesΠ^⊂Π. Let F be the set of functionsf:X^↦Asuch that f(x, y) ∈ A(x) for all y ≥ 0 and let(fk)k∈N0be a sequence of functions with fk∈ F. Then,Π^is the set of all policiesπ=(πk)k∈N0defined in the following wayπk(ωk):=fk(xk,yk),ωk∈Ωk,k∈N0,whereyk:=yk−1+ak−1β,k∈N,and y0 ≔ y ≥ 0 is a fixed number. Furthermore, we shall identify a policyπ∈Π^with the sequence(fk)k∈N0by writingπ=(fk)k∈N0.Next for any functionh:X^↦Rwe define an operator T as follows(4.2)Th(x,y):=βγmaxa∈A(x)[∑x′∈Xh(x′,a+yβ)q(x′|x,a)].Let f ∈ F be the maximiser of the right-hand side in (4.2), i.e., f(x) attains the maximum on the right-hand side of (4.2) for all x ∈ X. We also setTfh(x,y):=βγ∑x′∈Xh(x′,f(x,y)+yβ)q(x′|x,f(x,y)).Note that Tfh = Th.Theorem 4.1For eachn∈N0the value function Jnsatisfies the equation(4.3)Jn+1=TJnwith Jn(x, y) = yγfor x < 0 and J0(x, y) = yγ. Letf¯l∈Fbe such thatJl+1=Tf¯lJlfor l = 0, …, n. Then,π¯=(f¯n,…,f¯0)is optimal for Jn + 1, i.e.,Jn+1=Jn+1,π¯.Let n = 1. Then, by the definition of J1 we have thatJ1(x,y)=supπ∈ΠExπ(β0a0+y)γ=maxa∈A(x)(a+y)γ=(x+y)γ.On the other hand,TJ0(x,y)=βγmaxa∈A(x)(a+yβ)γ=(x+y)γ=Tf¯0J0(x,y)withf¯0(x,y)=x.Assume now thatJl=TJl−1=Tf¯l−1Jl−1withf¯l−1∈Ffor all l = 1, …, n and let(f¯n−1,…,f¯0)be an optimal policy for Jn. We show thatJn+1=TJn=Tf¯nJnand(f¯n,…,f¯0)is optimal for Jn.We have thatJn+1(x,y)=supπ∈ΠExπ(∑k=1n∧(τ−1)βkak+a0+y)γwith the convention that the empty sum equals 0 (when τ = 1). Recall thatπ→denotes a “1-shifted” policy, see (3.7). We further get the following(4.4)Jn+1(x,y)=βγsupπ∈ΠExπ(∑k=1n∧(τ−1)βk−1ak+a0+yβ)γ=βγsupπ∈ΠExπ[Exπ((∑k=1n∧(τ−1)βk−1ak+a0+yβ)γ|a0,x1)]≤βγsupa0∈A(x)[∑m=−∞∞supπ→∈ΠEx−a0+mπ→(∑k=1n∧(τ−1)βk−1ak+a0+yβ)γqm]=βγmaxa∈A(x)[∑m=−∞∞Jn(x−a+m,a+yβ)qm]=TJn(x,y).Letf¯nbe such thatTJn(x,y)=Tf¯nJn(x,y). Putπ¯=(f¯n,f¯n−1,…,f¯0). By induction assumptionπ¯→=(f¯n−1,…,f¯0)is optimal for Jn. Hence, it follows that(4.5)TJn(x,y)=βγmaxa∈A(x)[∑m=−∞∞Jnπ¯→(x−a+m,a+yβ)qm]=maxa∈A(x)βγ∑m=−∞∞Ex−a+mπ¯→(∑k=0(n−1)∧(τ−1)βkak+1+y+aβ)γqm=maxa∈A(x)∑m=−∞∞Ex−a+mπ¯→(∑k=0(n−1)∧(τ−1)βk+1ak+1+y+a)γqm≤supπ∈ΠExπ(∑k=0n∧(τ−1)βkak+y)γ=Jn+1(x,y).Thus, (4.4) and (4.5) yield that Jn + 1 = TJn. The fact thatπ¯defined above is optimal for Jn + 1 follows from repeating the calculations in (4.5) applied toTf¯nJn.□The next result can be concluded from Theorem 4.1.Theorem 4.2The function J satisfies the following equation(4.6)J(x,y)=βγmaxa∈A(x)[∑x′∈XJ(x′,a+yβ)q(x′|x,a)]=βγmaxa∈A(x)[∑k=−∞∞J(x−a+k,a+yβ)qk]forx∈N0and y ≥ 0.It is obvious that the sequence of functions(Jn(x,y))n∈N0is increasing for each(x,y)∈X^. Hence, w(x, y) ≔ limn → ∞Jn(x, y) exists for every(x,y)∈X^. Since Jn≤ J, then it follows that w ≤ J. On the other hand, for any policy π ∈ Π we obtain that Jn, π≤ Jn. Letting n → ∞, making use of the monotone convergence theorem and taking the supremum over π ∈ Π, it is easily seen thatJ(x,y)=supπ∈ΠExπ(∑k=0τ−1βkak+y)γ≤w(x,y).Consequently, w = J. Eq. (4.6) follows from (4.3) by letting n → ∞ and replacing the maximum with the limit.□The counterpart of Theorem 4.2 is Theorem 4.1(a) in Bäuerle and Rieder (2014). However, again as in the exponential case this theorem was proved for general state space, weakly continuous transitions and bounded costs.The next lemma provides the following bounds for the function J.Lemma 4.4For anyx∈N0and y ≥ 0 it follows that(x+y)γ≤J(x,y)≤(x+βEZ+1−β+y)γ.Let π be a policy such that π0( · |ω0) = x and πk( · |ωk) = 0 for k ≥ 1. Then, J(x, y) ≥ Jπ(x, y) = (x + y)γ. The upper bound for the function J is due to the Jensen inequality and Theorem 9.2.3(a) in Bäuerle and Rieder (2011) that gives the upper bound for the risk neutral setting.□For simplicity for anyx∈N0and y ≥ 0 we defineG(x,y):=∑k=−∞∞J(x+k,y)qk.From (4.6) we obtain that(4.7)J(x,y)=βγmaxa∈A(x)G(x−a,a+yβ),x∈N0,y≥0.Lemma 4.5For all 0 ≤ v ≤ x we have that J(x, y) ≥ J(x − v, y + v).Suppose that 0 < v ≤ x. For part (b) observe thatJ(x,y)=max{{βγG(x,yβ),…,βγG(x−v+1,y+v−1β)},max{βγG(x−v,y+vβ),…,βγG(0,y+xβ)}}=max{{βγG(x,yβ),…,βγG(x−v+1,y+v−1β)},βγmaxa∈A(x−v)G(x−v−a,y+v+aβ)}≥J(x−v,y+v),where the last inequality is due to (4.7).□In what follows let f* ∈ F be the largest maximiser of the right-hand side in (4.7). For completeness, set f*(x, y) = 0 for x < 0.Lemma 4.6Forx∈N0and y ≥ 0 it follows that f*(x − f*(x, y), y + f*(x, y)) = 0.By (4.7) we obtainJ(x,y)≥βγG(x,yβ),which implies thatJ(x,y)=βγG(x−f*(x,y),y+f*(x,y)β)≤J(x−f*(x,y),y+f*(x,y))≤J(x,y),where the second inequality follows from Lemma 4.5. Hence,βγG(x−f*(x,y),y+f*(x,y)β)=J(x−f*(x,y),y+f*(x,y)),which implies that a = 0 maximises the expressionβγG(x−f*(x,y)−a,y+f*(x,y)+aβ). We claim that a = 0 is the only maximiser of this expression. Obviously, if f*(x, y) = x, then the result follows. If, on the other hand, f*(x, y) < x, thenJ(x,y)=βγG(x−f*(x,y),y+f*(x,y)β)>βγG(x−f*(x,y)−a,y+f*(x,y)+aβ)for a = 1, …, x − f*(x, y). This fact, in turn, implies that a = 0 is the only maximiser, which concludes the proof.□Letξ(y):=sup{x∈N0:f*(x,y)=0}. Thenξ*:=supy≥0ξ(y)<∞.Fix y ≥ 0. Letx∈N0be such that f*(x, y) = 0. Clearly, suchx∈N0exists. From (3.9) we have thatJ(x,y)=βγ(∑k=−x∞J(x+k,yβ)qk+(yβ)γ∑k=−∞−x−1qk)≤βγ(∑k=−x∞(x+k+β1−βEZ++yβ)γqk+(yβ)γ∑k=−∞−x−1qk)(byLemma4.4)=∑k=−x∞(βx+βk+β21−βEZ++y)γqk+yγ∑k=−∞−x−1qk≤(∑k=−x∞(βx+βk+β21−βEZ++y)qk+y∑k=−∞−x−1qk)γ(bytheJenseninequality)≤(βx+βEZ++β21−βEZ++y)γ=(βx+β1−βEZ++y)γ.On the other hand, making use again of Lemma 4.4 we have that J(x, y) ≥ (x + y)γand, consequently,(x+y)γ≤(βx+β1−βEZ++y)γif and only ifx≤β(1−β)2EZ+,which is independent of y and implies the result.□The next result is a counterpart of Lemma 3.9 and provides further properties of the function f* ∈ F.Lemma 4.8Letx0∈N0and y0 ≥ 1. If f*(x0, y0) = a0 and f*(x0 + 1, y0 − 1) > 0, then f*(x0 + 1, y0 − 1) = a0 + 1.By definition of f* and (4.6) we have thatJ(x0,y0)=βγG(x0−a0,y0+a0β)×{≥βγG(x0−a,y0+aβ),fora=0,…,a0>βγG(x0−a,y0+aβ),fora=a0+1,…,x0.The above display implies that(4.8)βγG(x0−a0,y0+a0β)×{≥βγG(x0−(a−1),y0+a−1β),fora=1,…,a0+1>βγG(x0−(a−1),y0+a−1β),fora=a0+2,…,x0+1.On the other hand, we also obtain thatJ(x0+1,y0−1)=maxa∈A(x0+1)βγG(x0+1−a,y0−1+aβ)=max{βγG(x0+1,y0−1β),maxa∈A(x0)βγG(x0−a,y0+aβ)}.Since f*(x0 + 1, y0 − 1) > 0, we infer thatβγG(x0+1,y0−1β)≤maxa∈A(x0)βγG(x0−a,y0+aβ).This fact and (4.8) yield thatβγG(x0+1−(a0+1),y0−1+(a0+1)β)≥βγG(x0+1−a,y0−1+aβ),for a = 0, …, a0 + 1 andβγG(x0+1−(a0+1),y0−1+(a0+1)β)>βγG(x0+1−a,y0−1+aβ),for a = a0 + 2, …, x0 + 1. Thus, it follows that f*(x0 + 1, y0 − 1) = a0 + 1.□Let y0 ≥ 0 be fixed and let x0 ∈ X be the initial state. Consider the following policyπ*:=(πk*)k∈N0generated by f* in the following way(4.9)πk*(ωk):=f*(xk,yk),ωk∈Ωkk∈N0,where(4.10)yk:=yk−1+f*(xk−1,yk−1)β=y0+∑m=0k−1βmf*(xm,ym)βk,k∈N.Obviously,π∈Π^.Corollary 4.9Under policy π* ruin occurs with probability 1.We proceed along similar lines as in the proof of Corollary 3.10. Let(x0,y0)∈X^with x0 ≥ 0. Then, either x0 ≤ ξ* or x0 > ξ*. Observe that in both cases the risk reserve just after the dividend payment is less or equal to ξ*. The first case is obvious. In the second case, we deduce from Lemma 4.6 that f*(x0 − f*(x0, y0), y0 + f*(x0, y0)) = 0, which means by Lemma 4.7 that x0 − f*(x0, y0) ≤ ξ(y0 + f*(x0, y0)) ≤ ξ*. Hence, the ruin occurs, if there appears a sequence of length ξ* + 1 of negative incomes. The probability of such event equals(∑m=−∞−1qm)ξ*+1,see also (3.11). If the ruin has not occurred up to the lth day with l = ξ* + 1, then either xl≤ ξ* or xl> ξ*. Now the remaining part follows from the proof of Corollary 3.10.□For every(x,y)∈X^it holds thatJ(x,y)=Jπ*(x,y).From Theorem 4.2 and the definition of π* given in (4.9) we have thatJ(x,y)=βγ∑x1∈XJ(x1,y1)q(x1|x,f*(x,y))forx∈N0and y ≥ 0 with y1 defined in (4.10) and y0 ≔ y. Hence,(4.11)J(x,y)=βγ∑x1≥0J(x1,y1)q(x1|x,f*(x,y))+βγ∑x1<0(y1)γq(x1|x,f*(x,y))=βγ∑x1≥0J(x1,y1)q(x1|x,f*(x,y))+∑x1<0(y+f*(x,y))γq(x1|x,f*(x,y)).Iterating (4.11) (n − 1) times and making use of the policy π* we arrive at the following equation(4.12)J(x,y)=(βγ)nExπ*(J(xn,yn)1[τ>n])+Exπ*[(y+∑k=0(n−1)∧(τ−1)βkf*(xk,yk))γ1[τ≤n]].From Lemma 4.4 and the concavity of x↦xγ, we obtain the following bound for the first term in (4.12)(4.13)(βγ)nExπ*(J(xn,yn)1[τ>n])≤(βγ)nExπ*((xn+yn+β1−βEZ+)γ1[τ>n])≤(βγ)nExπ*(xnγ1[τ>n])+(βγ)n(β1−βEZ+)γ+(βγ)nExπ*((yn)γ1[τ>n])By (4.10) the third term in (4.13) can be written as follows(4.14)(βγ)nExπ*((yn)γ1[τ>n])=Exπ*[(y+∑k=0n−1βkf*(xk,yk))γ1[τ>n]].Next by (2.1) we get for the first term in (4.13) the following(4.15)0≤(βγ)nExπ*(xnγ1[τ>n])≤(βnExπ*(xn1[τ>n]))γ≤(βnExπ*[(x−∑k=0n−1f*(xk,yk)+∑k=1nZk+)1[τ>n]])γ≤(βnx+βnnEZ+)γ.By our assumption (A1) and (4.15) the first term in (4.13) converges to 0 as n → ∞. Observe that the same remark also applies for the second term in (4.13). Summing up, from (4.12), (4.13) and (4.14) we obtain thatJ(x,y)≤limn→∞Exπ*(y+∑k=0(n−1)∧(τ−1)βkf*(xk,yk))γ.Now the monotone convergence theorem yields thatJ(x,y)≤Jπ*(x,y).□We close this section with a conclusion for our original model.Corollary 4.11Let y0 ≔ 0. Then, π* is optimal for the original optimisation problem, i.e.,J^(x)=J^π*(x).Note that in the case of a power utility, the optimal policy is history-dependent, but depends on the history only through the accumulated discounted dividends given by (yk) in (4.10).It is well-known that the logarithmic utility function U(x) = log (x) can be obtained as a limit from the power utility sincelimγ→01γ(xγ−1)=log(x).Indeed, the problem can then be treated for the logarithmic utility in a similar way. The optimality equation is given byJ(x,y)=log(β)+maxa∈A(x)[∑k=−∞∞J(x−a+k,a+yβ)qk]and we can follow the same line of analysis. It is worth mentioning that the power and logarithmic utility functions are examples of the so-called HARA (hyperbolic absolute risk aversion) utilities, whereas the exponential utility function belongs to the CARA (constant absolute risk aversion) class of utilities. The reader is referred, for instance, to Bäuerle and Rieder (2011); 2014), Çanakoğlu and Özekici (2010),Föllmer and Schied (2011) and references cited therein, for further properties of the aforementioned functions.In this section we provide one numerical tool to solve these problems which is known under the name Howard policy improvement. We restrict the presentation here to the exponential utility function: Start with an arbitrary policy of the form π = (f( ·, γ), f( ·, γβ), …) induced by a decision rule f where we assume that f is such that f(x, θ) ≥ x − s* for all x > s* and all θ. Note that we defines*:=supθ∈[γ,0)s(θ)(see the proof of Theorem 3.8). Take, e.g., f(x, θ) = x+. We write Jf≔ Jπ. Next we compute the largest minimiser h(x, θ) of the expression,a↦eθaGf(x−a,θβ),a∈{0,1,…,x}whereGf(x,θ):=∑k=−x∞Jf(x+k,θ)qk+∑k=−∞−x−1qk.We claim the followingLemma 5.1The new decision rule h and the corresponding value function Jhhave the following properties:(a)h(x − h(x, θ), θ) = 0 for all x and θ.h(x, θ) ≥ x − s* for all x > s* and all θ.eθxh̲(θ)≤Jh≤Jf≤eθxh¯(θ).(a)If h(x, θ) = 0 or h(x, θ) = x the statement is true. Now let 0 < h(x, θ) < x and suppose that h(x − h(x, θ), θ) > 0, i.e. there is an a* > 0 s.t.eθa*Gf(x−h(x,θ)−a*,θβ)≤Gf(x−h(x,θ),θβ).On the other hand by the definition of h we have for all a > h(x, θ):eθh(x,θ)Gf(x−h(x,θ),θβ)<eθaGf(x−a,θβ).Combining these inequalities leads to (note that x − h(x, θ) − a* ≥ 0)eθh(x,θ)Gf(x−h(x,θ),θβ)<eθ(h(x,θ)+a*)Gf(x−h(x,θ)−a*,θβ)≤eθh(x,θ)Gf(x−h(x,θ),θβ)which is a contradiction. Thus, the statement is shown.We show first for x > s* and arbitrary θ that h(x, θ) > 0. In order to do this, consider the expression eθaGf(x − a, θβ) for a = 0 and a = f(x, θ). By definition we obtain for a = f(x, θ) thateθf(x,θ)Gf(x−f(x,θ),θβ)=Jf(x,θ)≤eθxh¯(θ).For a = 0 we obtain thatGf(x,θβ)=∑k=−x∞Jf(x+k,θβ)qk+∑k=−∞−x−1qk≥eθβxh̲(θβ)(∑k=−x∞eθβkqk+∑k=−∞−x−1qk)=eθβxh̲(θβ)EeθβZ+=eθβxh̲(θ).Furthermore, observe thateθβxh̲(θ)≥eθxh¯(θ)⇔x>s(θ).Thus, the inequality holds, in particular, ifx≥s*=supθ∈[γ,0)s(θ). This implies that 0 cannot be a minimiser, so h(x, θ) > 0 for all x > s* and all θ. This fact and point (a) imply the conclusion.From the definition of h we obtain:Jf(x,θ)=eθf(x,θ)Gf(x−f(x,θ),θβ)≥eθh(x,θ)Gf(x−h(x,θ),θβ).Iterating this inequality yieldsJf(x,θ)≥Exh[exp(γ∑k=0(τ−1)∧(n−1)βkak)(Jf(xn,γβn)1[τ≥n]+1[τ<n])(γ∑k=0(τ−1)∧(n−1)βkak)].The property of h shown in (b) now implies that ruin occurs with probability 1 under h and thus as in the proof of Theorem 3.11 we obtain with n → ∞ that Jf≥ Jh.□From the proof it follows that in case f ≠ h, the inequality Jh(x, θ) ≤ Jf(x, θ) is strict for at least one (x, θ). Now suppose no improvement is possible, i.e., h = f. Hence Jfis another solution of (3.2). By Remark 3.2Jf≤ J. On the other hand by the definition of J we have Jf≥ J which implies J = Jf.Finally if the iteration does not stop we obtain a non-increasing sequenceJf0≥Jf1≥⋯≥J. DenoteJ̲:=limn→∞Jfn. ObviouslyJ≥J. Next from the definition of an improvement it followsJfk+1(x,θ)≤mina∈A(x)[eθaGfk(x−a,θβ)]≤Jfk(x,θ).Letting k → ∞ we obtain (note that lim and min  can be interchanged since A(x) is always finite) thatJ̲(x,θ)≤mina∈A(x)[eθa(∑k=a−x∞J̲(x−a+k,θβ)qk+∑k=−∞a−x−1qk)]≤J̲(x,θ)henceJis another solution of (3.2) which implies thatJ=J.In this paper, we study the discrete time problem, suggested by Gerber and Shiu (2004), of maximising the expected utility of discounted dividends until ruin. We restrict our attention to the integer-valued surplus process and to integer payments. To the best of our knowledge, the only paper that examines a similar issue (with the exponential utility) is Grandits et al. (2007), where the wealth of insurance company is driven by a Brownian motion with drift. However, the authors have not been able to solve the problem rigorously. Namely, assuming that a certain integral equation for the barrier function b(t) has a desirable solution (see Standing Assumption in Grandits et al., 2007), they prove that b(t) is indeed the barrier they search for (a barrier function is a band function with n = 0 in Definition 3.1). Moreover, the numerical experiments provided in Section 1.4 in Grandits et al. (2007) are given without their convergence proofs. This fact and the lack of a complete solution in continuous time and any solution in discrete time to Gerber and Shiu’s suggestion indicate that the problem is not straightforward from the mathematical point of view. Firstly, similar as in Bäuerle and Rieder (2014) and Grandits et al. (2007) we note that the optimal strategy is time dependent in a certain way. In order to get rid of non-stationarity we extend the state space to the two-dimensional space. Within such a new framework our problem becomes stationary. Secondly, since our dividend payments can be unbounded we cannot directly apply the results from Bäuerle and Rieder (2014) and Di Masi and Stettner (2000) to deduce that the value function satisfies the corresponding Bellman equation for the exponential and power utility functions. Nonetheless, we are able to show that in both cases the value iteration algorithm works (see Theorems 3.1 and 4.1) and in the exponential function case Howard’s policy improvement algorithm works (see Section 5). These facts, in turn, may have a significant meaning, when one thinks of numerical examples. Moreover, we are also able to describe the structure of optimal strategies for both utility cases and to prove for the exponential function case that the optimal policy is a band-policy.Numerical experiments are difficult. Let us first recall that the maximisation of the expectation of discounted dividends in the model given by (2.1) withP(Z1=1)=p=1−P(Z1=−N),where p ∈ (0, 1) andN∈N,was a challenging analytical problem. The reader may find the complete solution, for instance, in Morrill (1966), where it was shown that the optimal policy is of barrier type. Our problem, as already mentioned, is non-stationary and non-separable. Therefore, the methods that solved analytically the risk neutral problem are useless here. Moreover, as was noted by Gerber and Shiu (2004), in contrast to the risk neutral problem one can expect that in the model with exponential function the optimal policy is not of barrier type. This fact does not make easier potential calculations. Obviously, since obtaining an analytical solution is a challenge, one can think of numerical methods used in dynamic programming such as value iteration, policy improvement or others, see Powell (2011). However, our surplus process proceeds on the spaceX=Zand even the simple aforementioned case (P(Z1=1)=p=1−P(Z1=−1)) requires some truncation of the state space to a finite one in order to obtain numerical results. In addition, in this model we meet one more obstacle that has not been treated so far, namely the exponential and power utility functions that imply non-stationarity and non-separability. Therefore, the problem of calculating numerically optimal strategies and value functions for models with these or other utility functions is left open.

@&#CONCLUSIONS@&#
