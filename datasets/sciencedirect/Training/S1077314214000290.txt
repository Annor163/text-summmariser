@&#MAIN-TITLE@&#
Recommendations for recognizing video events by concept vocabularies

@&#HIGHLIGHTS@&#
Characterizing a universal concept vocabulary suited for representing video events.The concept vocabulary should contain more than 200 specific and general concepts.Be diverse by covering object, action, scene, people, animal and attribute concepts.Increase the number of concepts rather than improve quality of individual detectors.Contain detectors that are appropriately normalized.

@&#KEYPHRASES@&#
Event recognition,Concept representation,Concept vocabulary,

@&#ABSTRACT@&#
Representing videos using vocabularies composed of concept detectors appears promising for generic event recognition. While many have recently shown the benefits of concept vocabularies for recognition, studying the characteristics of a universal concept vocabulary suited for representing events is ignored. In this paper, we study how to create an effective vocabulary for arbitrary-event recognition in web video. We consider five research questions related to the number, the type, the specificity, the quality and the normalization of the detectors in concept vocabularies. A rigorous experimental protocol using a pool of 1346 concept detectors trained on publicly available annotations, two large arbitrary web video datasets and a common event recognition pipeline allow us to analyze the performance of various concept vocabulary definitions. From the analysis we arrive at the recommendation that for effective event recognition the concept vocabulary should (i) contain more than 200 concepts, (ii) be diverse by covering object, action, scene, people, animal and attribute concepts, (iii) include both general and specific concepts, (iv) increase the number of concepts rather than improve the quality of the individual detectors, and (v) contain detectors that are appropriately normalized. We consider the recommendations for recognizing video events by concept vocabularies the most important contribution of the paper, as they provide guidelines for future work.

@&#INTRODUCTION@&#
We consider the problem of recognizing events in arbitrary web video, such as the ones depicted in Fig. 1. Among the many challenges involved, resulting from the uncontrolled recording condition of web videos and the large variations in the visual appearance of events, probably one of the most fundamental questions in event recognition is what defines an event in video? The Oxford English dictionary defines an event as “anything that happens”. With such a broad definition it is not surprising that the topic has been addressed in the computer vision and multimedia retrieval community by many researchers from diverse angles [4,55,44,5,25,54,36].In this paper, we study representations that contribute to defining events for automatic recognition. We are inspired by findings from cognition, where research has repeatedly shown that humans remember events by their actors, actions, objects, and locations [46]. Studying event representation based on such high-level concepts is now within reach because of the continued progress in supervised concept detection [48] and the availability of labeled training collections like the ones developed in benchmarks such as TRECVID [47], ImageNet [7] and several other venues [34,11]. Different from concepts, which represent a single person, object, scene or action in videos, events are commonly defined as a more complex interaction of several persons, objects, and actions happening in a specific scene [31]. In this paper, we name the set of available concept detectors as the vocabulary and we study how to construct a vocabulary suited for effective recognition of events in video.The state-of-the-art in event recognition represents a video in terms of low-level audiovisual features [16,38,50,35,15,19,37]. In general, these methods first extract from the video various types of static and/or dynamic features, e.g., color SIFT variations [53], MFCC [15], and Dense Trajectories [38]. Second, the descriptors are quantized and aggregated [38]. The robustness and efficiency of various low-level features for recognizing events are evaluated in [50,15,33]. Despite their good recognition performance, especially when combined together [35,15,38,33], low-level features are incapable of providing an understanding of the semantic structure present in an event. Hence, it is not easy to derive how these event definitions arrive at their recognition. Therefore, essentially different representations are needed for events. We focus on high-level representations for event recognition.Inspired by previous work in object recognition [51,22], scene recognition [22,40] and activity recognition [41], many have explored high-level representations for recognition of events [28,31,2,58,14,30,8,24]. All these works follow a general pipeline consisting of three consecutive steps to arrive at a high-level video representation. First, frame extraction, where the video is decoded and a subset of frames is extracted. Second, concept detection, where each extracted frame is represented by a vector of predictions from vocabulary concept detectors. Finally, video pooling, where the frame representations are averaged and aggregated into the video level representation. The obtained high-level representation is not only semantically interpretable, but is also reported to outperform the state-of-the-art low-level audiovisual features in recognizing events [31,33]. Rather than training vocabulary concept detectors and event detectors separately, recent work aims for jointly learning the vocabulary concept and event detectors [26,57,1,27]. In these works, the vocabulary concept detectors are trained to optimize the event detection, without explicitly optimizing the individual concept detector accuracy. As a consequence, the vocabulary concepts do not necessarily have a semantic interpretation needed to explain the video content. In this paper, we follow [31,14,58,30] and train concept and event detectors separately.Identifying a universal vocabulary of concepts suited for representing events is an important question that has been ignored in the literature. To the best of our knowledge, all the previous work on high-level representations for event recognition relies on an arbitrary set of concepts as the vocabulary. By contrast, we focus in this paper on characterizing the vocabulary which is most effective for representing events. We investigate the concept vocabulary from two perspectives: first by characterizing the composition, where we investigate what concepts should be included in the vocabulary. Second by characterizing the concept detectors, where we study how to create vocabulary concept detectors that are most suited for representing events. Before detailing our research questions, we discuss related work that we consider most relevant to these two perspectives.

@&#CONCLUSIONS@&#
