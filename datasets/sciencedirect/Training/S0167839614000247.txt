@&#MAIN-TITLE@&#
A structure-preserving matrix method for the deconvolution of two Bernstein basis polynomials

@&#HIGHLIGHTS@&#
A structured matrix method is used to deconvolve two Bernstein basis polynomials.The solution is obtained by solving a constrained optimisation problem.The solution in the paper is better than the solutions from other methods.The computational results are analysed theoretically.

@&#KEYPHRASES@&#
Polynomial deconvolution,Bernstein basis polynomials,Structure-preserving matrix methods,

@&#ABSTRACT@&#
This paper describes the application of a structure-preserving matrix method to the deconvolution of two Bernstein basis polynomials. Specifically, the deconvolutionhˆ/fˆyields a polynomialgˆprovided the exact polynomialfˆis a divisor of the exact polynomialhˆand all computations are performed symbolically. In practical situations, however, inexact forms, h and f of, respectively,hˆandfˆare specified, in which caseg=h/fis a rational function and not a polynomial. The simplest method to calculate the coefficients of g is the least squares minimisation of an over-determined system of linear equations in which the coefficient matrix is Tœplitz, but the solution is a polynomial approximation of a rational function. It is shown in this paper that an improved result for g is obtained when the Tœplitz structure of the coefficient matrix is preserved, that is, a structure-preserving matrix method is used. In particular, this method guarantees that a polynomial solution to the deconvolutionh/fis obtained, and thus an essential property of the theoretically exact solution is retained in the computed solution. Computational examples that show the improvement in the solution obtained from the structure-preserving matrix method with respect to the least squares solution are presented.

@&#INTRODUCTION@&#
The Bernstein polynomial basis is used in computer aided geometric design because of its elegant geometric properties and superior numerical properties with respect to those of the power basis (Farouki and Goodman, 1996). Algorithms for the addition, multiplication, division and subdivision of Bernstein basis polynomials have been developed (Farouki and Rajan, 1988; Goldman, 2002), and this paper considers the deconvolution of two Bernstein basis polynomials h and f, that is, the computation ofg=h/f. Ifh=hˆandf=fˆwherehˆandfˆare the exact forms of h and f respectively,fˆis an exact divisor ofhˆ, and all computations are performed symbolically, thengˆ=hˆ/fˆis a polynomial. In practical problems, however, the polynomials h and f are inexact, in which caseg=h/fis a rational function and not a polynomial. It is shown in this paper that in this circumstance, a polynomial is returned when a structure-preserving matrix method is used to compute g. This solution must be compared with the simplest solution, that is, the least squares (LS) solution of an over-determined set of linear equations, in which case a polynomial approximation of a rational function is returned.The approximation of a rational function by a polynomial may be adequate in some applications, but other applications may require that the algorithm for polynomial deconvolution return a polynomial, and not an approximation of a polynomial. The requirement that a polynomial be returned arises in the computation of multiple roots of a polynomial because the algorithm includes a series of deconvolutions in which the denominator polynomial is an exact divisor of the numerator polynomial (Uspensky, 1948). The discussion above shows that if the polynomials are subject to error and/or the computations are performed in finite precision arithmetic, it cannot be guaranteed that the result of each deconvolution is a polynomial. In this circumstance, the algorithm returns incorrect results or fails, and it is shown in this paper that a structure-preserving matrix method returns polynomial solutions to these deconvolutions, which is, as noted above, essential for the computation of multiple roots of a polynomial.The deconvolution of two Bernstein basis polynomials is considered in Section 2. It is shown that the problem can be cast as the computation of the vectorgˆ, which stores the coefficients ofgˆ, from the equation,(1)D−1T(fˆ)gˆ=hˆ,whereD−1is a diagonal matrix of combinatorial factors,T(fˆ)is a Tœplitz matrix whose non-zero entries are the coefficients offˆ, andhˆstores the coefficients ofhˆ. The Tœplitz structure ofT(fˆ)is retained when inexact polynomials h and f are specified, but h, the vector that contains the coefficients of h, does not lie in the column space ofD−1T(f)in this circumstance, and thus (1) is replaced by(2)D−1T(f)g≈h.The approximation in (2) implies that a computed solution w has a non-zero error,‖D−1T(f)w−h‖>0, from which it follows that the entries of w are the coefficients of a polynomial that is an approximation of a rational function. It is shown in Section 3 that if the Tœplitz structure ofT(f)is retained in the computations, that is, a structure-preserving matrix method is used, an exact polynomial, and not a polynomial approximation of a rational function, is returned when an approximate solution of (2) is computed.The application of a structure-preserving matrix method to the deconvolution of two Bernstein basis polynomials is considered in Section 4. This method requires the iterative solution of a non-linear equation, and the condition for its convergence is discussed in Section 5. Section 6 contains two examples that compare the solutions obtained from the method of LS with the solutions obtained from a structure-preserving matrix method. The solutions are discussed in Section 7, and Section 8 contains a summary of the paper.This section considers the deconvolution of two Bernstein polynomials and it is shown it can be considered in the matrix form (1).Letfˆ(y),gˆ(y)andhˆ(y)be Bernstein polynomials of degrees m, n andm+nrespectively,(3)fˆ(y)=∑i=0maˆi(mi)(1−y)m−iyi,(4)gˆ(y)=∑i=0nbˆi(ni)(1−y)n−iyi,(5)hˆ(y)=∑i=0m+ncˆi(m+ni)(1−y)m+n−iyi.It follows from (3) and (4) that the coefficientscˆiin (5) are given bycˆk=∑i=max(0,k−n)min(m,k)aˆi(mi)bˆk−i(nk−i)(m+nk),k=0,…,m+n,which can be written in matrix form as(6)(D−1T(fˆ))bˆ=cˆ,whereD−1=diag[1(m+n0)1(m+n1)⋯1(m+nm+n−1)1(m+nm+n)]∈R(m+n+1)×(m+n+1),T=T(fˆ)∈R(m+n+1)×(n+1),bˆ∈Rn+1,cˆ∈Rm+n+1andT=[aˆ0(m0)aˆ1(m1)aˆ0(m0)⋮aˆ1(m1)⋱⋮⋮⋱aˆ0(m0)aˆm(mm)⋮⋱aˆ1(m1)aˆm(mm)⋱⋮⋱⋮aˆm(mm)],bˆ=[bˆ0(n0)bˆ1(n1)⋮⋮bˆn(nn)],cˆ=[cˆ0cˆ1⋮⋮cˆm+n].Previous work (Winkler and Yang, 2013a, 2013b) has shown it is numerically advantageous to expressbˆas the product of a diagonal matrixQ∈R(n+1)×(n+1)and a vectorpˆ∈Rn+1of the coefficientsbˆi,bˆ=Qpˆ,Q=diag[(n0)(n1)⋯(nn)],pˆ=[bˆ0bˆ1⋯bˆn]T,and thus (6) can be written as(7)(D−1T(fˆ)Q)pˆ=cˆ,where the coefficient matrix is of order(m+n+1)×(n+1). It is better to calculate the coefficients ofgˆ(y)from (7) than from (6) because numerous experiments showed thatκ(D−1T(fˆ)Q)<κ(D−1T(fˆ)),whereκ(X)denotes the condition number of X. It therefore follows that (7) is more stable than (6), and thus improved solutions are expected from (7).The magnitudes of the entries in the coefficient matrix and right hand side vector of (7) may differ by several orders of magnitude, and it is therefore advantageous to normalise them. This issue is discussed in Winkler et al. (2012), Winkler and Yang (2013a), and it is shown that the normalisation of the entries in the coefficient matrix, and the normalisation of the entries in the right hand side vector, by their geometric means yield significantly improved results. The geometric mean of the terms that contain the coefficients offˆinD−1T(fˆ)Qis (Winkler and Yang, 2013a)(8)λ=(∏i=0m|aˆi(mi)|)1m+1(∏k=0n(nk))1n+1(∏i=0mPi)1(n+1)(m+1),wherePi=∏j=ii+n(m+nj),i=0,…,m,and thus the normalised form offˆ(y)is(9)f¯(y)=∑i=0ma¯i(mi)(1−y)m−iyi,a¯i=aˆiλ.The normalised form ofhˆ(y)is(10)h¯(y)=∑i=0m+nc¯i(m+ni)(1−y)m+n−iyi,c¯i=cˆiμ,where the geometric mean μ of the coefficientscˆiis(11)μ=(∏i=0m+n|cˆi|)1m+n+1.It therefore follows from (9) and (10) that (7) becomes(12)(D−1T(f¯)Q)p¯=c¯,wherec¯∈Rm+n+1andp¯∈Rn+1are, respectively,c¯=[c¯0c¯1⋯c¯m+n]Tandp¯=[b¯0b¯1⋯b¯n]T,and it is required to compute the coefficientsb¯iof the polynomialg¯(y),g¯(y)=∑i=0nb¯i(ni)(1−y)n−iyi.The normalisation of the coefficients offˆ(y)retains the Tœplitz structure of T, but the inclusion of the diagonal matricesD−1and Q implies that the coefficient matrix in (12) is not Tœplitz, but it is still structured. It is appropriate to consider, therefore, a structure-preserving matrix method for the solution of (12), and this issue is addressed in the next section.This section considers the application of a structure-preserving matrix method to the solution of (12), and the difference between the solution obtained using this method, and the solution obtained by solving a LS problem, is described.If exact polynomials are considered and all computations are performed in infinite precision arithmetic, the coefficients ofg¯can be computed from the LS solution of (12),p¯=(D−1T(f¯)Q)†c¯,A†=(ATA)−1AT,and the error is zero. The pseudo-inverseA†of a matrix A should be computed from the singular value decomposition (SVD) of A and not from the SVDUSVTofA†because this can lead to large errors, such thatUSVT≠A†.If the inexact polynomialsf(y)andh(y)are considered, then (12) is replaced by an approximate equation whose LS solution defines the coefficients of a polynomial that is an approximation of a rational function. This is the simplest method of computing the coefficients ofg¯(y), but it fails to consider the structure of the coefficient matrix. Sincef(y),g(y)andh(y)are inexact forms offˆ(y),gˆ(y)andhˆ(y)respectively, they are given by(13)f(y)=fˆ(y)+δfˆ(y)=∑i=0mai(mi)(1−y)m−iyi,g(y)=gˆ(y)+δgˆ(y)=∑i=0nbi(ni)(1−y)n−iyi,(14)h(y)=hˆ(y)+δhˆ(y)=∑j=0m+ncj(m+nj)(1−y)m+n−jyj,where(15)ai=aˆi+δaˆiandcj=cˆj+δcˆj,and normalisation of the coefficientsaiandcjby, respectively, the geometric means λ and μ, which are defined in (8) and (11), is implicitly included. This normalisation by the geometric means of the coefficients off(y)andg(y)follows from the discussion in Section 2.It follows from (12) that it is necessary to compute the LS solution of(16)(D−1T(f)Q)p≈c,wherep=[b0b1⋯bn]Tandc=[c0c1⋯cm+n]T.The vector c does not lie in the column space ofD−1T(f)Q, but the approximation (16) can be transformed to an equation by perturbingT(f)by a Tœplitz matrixB(z)that has the same structure asT(f), and perturbing c by a vector t. A structure-preserving matrix method yields, therefore, a polynomial solution to the deconvolution problem because (16) is replaced by(17)(D−1(T(f)+B(z))Q)p=c+t,where the entries ofB(z)and t are the coefficients of the polynomialss(y)ande(y)respectively,(18)s(y)=∑i=0mzi(mi)(1−y)m−iyi,and(19)e(y)=∑i=0m+nti(m+ni)(1−y)m+n−iyi,and the matrixB(z)and vector t are given by, respectively,B(z)=[z0(m0)z1(m1)z0(m0)⋮z1(m1)⋱⋮⋮⋱z0(m0)zm(mm)⋮⋱z1(m1)zm(mm)⋱⋮⋱⋮zm(mm)],t=[t0t1⋮⋮tm+n].If the matrixB(z)and vector t are chosen such thatc+tlies in the column space ofD−1(T(f)+B(z))Q, then (17) has an exact solution, and thus the solution vector p contains the coefficients of the polynomial formed from the deconvolution,(20)h(y)+e(y)f(y)+s(y)=∑i=0m+n(ci+ti)(m+ni)(1−y)m+n−iyi∑i=0m(ai+zi)(mi)(1−y)m−iyi.An infinite number of pairs of polynomials(s(y),e(y))satisfy (17), but it is desired to compute the coefficientsziandtiof minimum magnitude, that is, the solution of (17) that is nearest the solution defined by the given inexact data is sought. This is expressed mathematically as a LS minimisation with an equality constraint, the LSE problem,(21)min‖z‖2+‖t‖2such that(D−1(T(f)+B(z))Q)p=c+t,where‖⋅‖=‖⋅‖2,(22)z=[z0z1⋯zm]Tandz={z0,z1,…,zm}.It follows from (21) that a structure-preserving matrix method perturbs the given inexact polynomials the minimum amount, such that the perturbed polynomials satisfy the exact divisor condition. This is desirable because the computed solution satisfies a property of the theoretical solution, which is different from the LS solution, which does not satisfy this condition.This section considers the application of the method of structured total least norm (STLN) (Ben Rosen et al., 1996) to the deconvolutionh/f, such that the result is a polynomial and not a rational function. This method is therefore used to obtain the solution of (21).The residual associated with an approximate solution(z,p,t)of (17) is(23)r(z,p,t)=(c+t)−(D−1(T(f)+B(z))Q)p,and thusr˜:=r(z+δz,p+δp,t+δt)=(c+(t+δt))−(D−1(T(f)+B(z+δz))Q)(p+δp).It therefore follows that, to first order,(24)r˜=r(z,p,t)+δt−(D−1(T+B)Q)δp−(D−1(∑i=0m∂B∂ziδzi)Q)p,where the last term on the right hand side represents the polynomial multiplicationδs(y)g(y), ands(y)is defined in (18). The simplification of this term requires that the polynomial multiplicationg(y)s(y)=(∑i=0nbi(ni)(1−y)n−iyi)(∑i=0mzi(mi)(1−y)m−iyi),which can also be expressed ass(y)g(y)=(∑i=0mzi(mi)(1−y)m−iyi)(∑i=0nbi(ni)(1−y)n−iyi),be considered. These polynomial multiplications can be expressed in matrix forms as, respectively,(25)(D−1Y(p)R)zand(D−1B(z)Q)p,where z and z are defined in (22),Y(p)∈R(m+n+1)×(m+1)is a Tœplitz matrix,p={b0,b1,…,bn}andR=diag[(m0)(m1)⋯(mm)].It therefore follows from (25) that(YR)z=(BQ)p,and the differentiation of both sides of this equation with respect to z yields(YR)δz=(∑i=0m∂B∂ziδzi)Qp,and thus (24) simplifies to(26)r˜=r(z,p,t)+δt−(D−1(T(f)+B(z))Q)δp−(D−1YR)δz.The jth iteration in the Newton–Raphson method for the calculation ofz,pand t is obtained from (26),(27)[HzHpHt](j)[δzδpδt](j)=r(j),wherer(j)=r(j)(z,p,t),Hz=D−1YR∈R(m+n+1)×(m+1),Hp=D−1(T+B)Q∈R(m+n+1)×(n+1),Ht=−I∈R(m+n+1)×(m+n+1),and the values of z, p and t at the jth iteration are[zpt](j)=[zpt](j−1)+[δzδpδt](j),[zpt](0)=[0p00].The initial values of z and t arez(0)=0andt(0)=0because the given data is the inexact data, and the initial valuep0of p is calculated from (23),(28)p0=argminw‖(D−1T(f)Q)w−c‖.Eq. (27) is of the form(29)C(j)δy(j)=q(j),q(j)=r(j)∈Rm+n+1,whereC(j)∈R(m+n+1)×(2m+2n+3),δy(j)∈R2m+2n+3, and(30)C(j)=[HzHpHt](j),δy(j)=[δzδpδt](j),y(j)=y(j−1)+δy(j).It follows from (21) that, of all the solutions that satisfy the constraint equation, the solution that is nearest the LS solution defined by the given inexact data is required. The function to be minimised is therefore(31)‖[z(j)−z(0)p(j)−p0t(j)−t(0)]‖=‖[z(j−1)+δz(j)p(j−1)+δp(j)−p0t(j−1)+δt(j)]‖:=‖δy(j)−w(j−1)‖,whereδy(j)is defined in (30),δy(0)=w(0)=0and(32)w(j−1)=−(y(j−1)−y(0))=−[z(j−1)p(j−1)−p0t(j−1)]∈R2m+2n+3.The minimisation of (31) subject to (29) yields the LSE problem,(33)minδy(j)‖δy(j)−w(j−1)‖subject toC(j)δy(j)=q(j),which can be solved, at each iteration, by the QR decomposition (Golub and Van Loan, 1996), whereC(j),q(j)andw(j−1)are updated between successive iterations. Algorithm 1shows the application of the method of STLN to the deconvolution of two Bernstein polynomials.Optimality conditions for the method of STLN, its formulation for the 1- and ∞-norms, and its relation to the Newton–Raphson iteration, are considered in Ben Rosen et al. (1996). The method can be extended to non-linear structures in the coefficient matrix and/or right hand side vector, which yields the method of structured non-linear total least norm (Ben Rosen et al., 1998). This method has been used for the computation of a structured low rank approximation of the Sylvester resultant matrix (Winkler and Hasan, 2013).This section considers the convergence of Algorithm 1 for the iterative solution of the LSE problem, which is defined in (21). It follows from steps (3c)–(3f) of this algorithm, and (32), that the jth iteration of this LSE problem yieldsδy(j)=(Q[R1−T0]q+Q[0z1])(j−1)=(Q[R1−T0]q+[Q1Q2][0Q2T]w)(j−1)=(Q1R1−Tq)(j−1)+(Q2Q2T)(j−1)(−y(j−1)+y(0)).It follows from (30) that Algorithm 1 converges iflimj→∞‖y(j)−y(j−1)‖=limj→∞‖δy(j)‖=limj→∞‖(Q2Q2T)(j−1)(y(0)−y(j−1))+(Q1R1−Tq)(j−1)‖=0,and thus the convergence of (33) is dependent on the matrices and vectors at each iteration, and its a priori determination is therefore difficult. Computational experiments showed, however, that convergence is achieved in fewer than 5 iterations, even for high degree polynomials that are corrupted by noise and have multiple roots.This section contains two examples that illustrate the application of the method of STLN to the computation ofg=h/f, and the solution of each example is compared with the LS solution of (16). Also, it was stated in Section 2 that it is numerically advantageous to include the diagonal matrix Q of combinatorial factors in the coefficient matrix, and this is confirmed numerically by including the solutions obtained when (16) is written as(34)(D−1T(f))b≈c,b=Qp.Three error measures and two condition numbers were computed for each example:The error r1between the theoretically exact solution, that is, the coefficients ofgˆ(y), and the solution of the LSE problem (33).If this solution defines the polynomialg1(y), then the forward error in the coefficients ofg1(y)is(35)r1=‖g1−gˆ‖‖gˆ‖.If this solution defines the polynomialg2(y), then the forward error in the coefficients ofg2(y)is(36)r2=‖g2−gˆ‖‖gˆ‖.The residual of the computed solution of the constraint is, from (23),(37)r3=‖r(z,p,t)‖‖c+t‖.The condition numberκ(C(j))of the coefficient matrix of the constraint equation in the LSE problem (33) at termination of the iterative procedure, for Q included in the coefficient matrix, and Q included in the solution vector.The condition numbersκ(D−1T(f)Q)andκ(D−1T(f))of the LS problem, for Q included in the coefficient matrix and Q included in the solution vector, respectively.Noise was added in the componentwise sense to the coefficients offˆ(y)andhˆ(y), and it therefore follows from (13) and (14) that the coefficients of the polynomialsδfˆ(y)andδhˆ(y)are, respectively,(38)δaˆi=εriaˆi,i=0,…,m;δcˆj=εrjcˆj,j=0,…,m+n,whereriandrjare uniformly distributed random variables in the interval[−1,1], and ε is the reciprocal of the upper bound of the componentwise signal-to-noise ratio. The normwise error models follow easily from these componentwise error models,(39)‖δaˆ‖⩽ε‖aˆ‖and‖δcˆ‖⩽ε‖cˆ‖,and thus ε is also the upper bound of the reciprocal of the normwise signal-to-noise ratio.Example 6.1Noise with componentwise signal-to-noise ratio 108 was added to the coefficients of the Bernstein forms of the exact polynomials,fˆ(y)=(y−0.30)5(y−0.70)4(y−1.40)5(y−7.00)6,andhˆ(y)=(y−0.30)8(y−0.70)7(y−1.40)8(y−7.00)6(y+1.80)3(y+0.90)4,and these inexact polynomials were then normalised, thereby obtaining the polynomialsf(y)andh(y), which are defined in (13) and (14). The results are shown in Table 1and it is seen thatr1=r2, and thus the relative errors of the solutions in the LS and LSE problems are equal. The errorr3is approximately equal to10−16, and it therefore follows from (37) and the discussion in Section 3 that the computed solution is a polynomial, and not a polynomial approximation of a rational function. This value ofr3must be compared with the values ofr1andr2, which are about eight orders of magnitude larger, and thus the difference between the solutions from the LS and LSE problems is clear.Table 1 shows that the inclusion of Q in the coefficient matrix causes a reduction of about three orders of magnitude inκ(C(j))with respect to its value when Q is included in the solution vector, as shown in (34), which is in accord with the results in Winkler and Yang (2013a, 2013b). The table also shows that, for Q included in the coefficient matrix,κ(C(j))is about one order of magnitude larger than the condition number ofD−1T(f)Q, which is the coefficient matrix of (16). Also, one iteration is required for the convergence of the LSE problem when Q is included in the coefficient matrix, but four iterations are required for convergence when Q is included in the solution vector. □Example 6.2The procedure described in Example 6.1 was implemented for the polynomials,fˆ(y)=(y−0.30)6(y−0.40)4(y−0.50)4(y−0.60)5(y−0.70)4,andhˆ(y)=(y−0.30)8(y−0.40)6(y−0.50)6(y−0.60)6(y−0.70)6(y−0.80)3(y−0.90)4(y−0.99)4.The results are shown in Table 2and they are similar to the results in Table 1 because better results are obtained when Q is included in the coefficient matrix. It is also seen thatκ(C(1))κ(D−1T(f)Q)=7.19×1025.83×103≈0.1,and thus the condition number of the coefficient matrix of the constraint equation in the LSE problem (33) is about one order of magnitude smaller than the condition number of the coefficient matrix of (16). Also, only one iteration is required for the solution of the LSE problem when Q is included in the coefficient matrix, but 54 iterations are required when Q is included in the solution vector.  □

@&#CONCLUSIONS@&#
