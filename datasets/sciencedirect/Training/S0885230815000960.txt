@&#MAIN-TITLE@&#
Speaker-adapted confidence measures for speech recognition of video lectures

@&#HIGHLIGHTS@&#
A new, particular logistic regression model is proposed to improve confidence measures for automatic speech recognition.Speaker-adapted models are proposed to further improve confidence measures.Empirical results are provided showing that speaker-adapted models outperform their non-adapted counterparts.The improvement of confidence measures shown to be useful on an interactive speech transcription application.

@&#KEYPHRASES@&#
Confidence measures,Speech recognition,Speaker adaptation,Log-linear models,Online video lectures,

@&#ABSTRACT@&#
Automatic speech recognition applications can benefit from a confidence measure (CM) to predict the reliability of the output. Previous works showed that a word-dependent naïve Bayes (NB) classifier outperforms the conventional word posterior probability as a CM. However, a discriminative formulation usually renders improved performance due to the available training techniques.Taking this into account, we propose a logistic regression (LR) classifier defined with simple input functions to approximate to the NB behaviour. Additionally, as a main contribution, we propose to adapt the CM to the speaker in cases in which it is possible to identify the speakers, such as online lecture repositories.The experiments have shown that speaker-adapted models outperform their non-adapted counterparts on two difficult tasks from English (videoLectures.net) and Spanish (poliMedia) educational lectures. They have also shown that the NB model is clearly superseded by the proposed LR classifier.

@&#INTRODUCTION@&#
Significant advances in the field of Automatic Speech Recognition (ASR) have been achieved over the last decades. Nowadays, automatic transcriptions of spontaneous speech in moderately noisy environments have reached an accurate enough quality (Rousseau, 2011; Sundermeyer et al., 2011; Swietojanski et al., 2013). This quality can be even better when ASR systems are adapted to specific scenarios (Leggetter and Woodland, 1995; Gales, 1998; Gauvain and Lee, 1994; Digalakis et al., 1995; Wiesler et al., 2014; Martinez-Villaronga et al., 2013). Nonetheless, ASR is still far from producing error-free transcriptions and, consequently, its performance in many applications is not completely satisfactory.To further improve the usefulness and performance of the current technology, researchers have proposed to compute a normalised score or confidence measure (CM) to indicate the reliability of the ASR output. This score has been computed at different levels: phoneme, word, phrase or sentence. Nevertheless, CM at the word level has been the main focus in the literature due to its usefulness for the vast majority of applications (Wessel et al., 2001; Kim and Ko, 2005; Gao et al., 2009; Bardideh et al., 2007; Wang et al., 2010; Junfeng and Yeping, 2011; Yadav and Patil, 2013).One widely used word-level CM has been word posterior probability (Wessel et al., 2001). From then on, many works have focused on combining word posterior with additional sources of knowledge. The combination has been addressed as a classification problem in the vast majority of the works. Most well-known classifier algorithms have been tried: linear, Gaussian mixtures, neural networks, decision trees, support vector machines, etc. For further reference, a still good comprehensive survey can be found in Jiang (2005).In the framework of CM as a classification problem, significant improvements were achieved by means of a combination of word-dependent (specific) and word-independent (generalised) naïve Bayes (NB) classifiers (Sanchis et al., 2012). Nonetheless, NB is learned by means of a generative criterion, the maximum likelihood estimate (MLE), which involves some issues. In particular, MLE overfits due to the unseen data. This issue was addressed in NB work by using a complex backing-off smoothing technique. But still, MLE aims at modelling the distribution underlying a given sample, which does not guarantee the solution to be the best suited for classification. Indeed, better fitted criteria may improve overall performance. For instance, the maximum mutual information (MMI) (Heigold et al., 2010) aims at better discriminating between classes without explaining the data. This criterion has been widely exploited in the literature for the maximum entropy (ME) models (Guiasu and Shenitzer, 1985; Yu et al., 2011).Nevertheless, despite the success of MMI training in many applications, there is no direct relationship between maximising the MMI and minimising the probability of classification error. Instead, there are better suited criteria, which guarantee the minimisation of the classification error rate (CER) such as the minimum classification error (MCE) or the mean squared error (MSE). Therefore, we propose a logistic regression (LR) model to be learnt by means of the MSE to surpass NB performance.On the other hand, speaker model adaptation has proved to be very effective for the improvement of recognition performance (Leggetter and Woodland, 1995; Gales, 1998; Gauvain and Lee, 1994; Digalakis et al., 1995). However, adaptation of the CMs to the speaker is nowadays unexplored. There is an increasing number of interesting scenarios in which CMs can be very useful and information about the speaker is available, such as the online lecture repositories. These repositories usually count with a large number of speeches delivered by a reduced number of speakers. Improving CM performance in these academic repositories is highly motivated since manual transcription is not affordable for such a large amount of speeches. Moreover, ASR performance is usually poor due to the amount of technical concepts, very different native and non-native accents, etc. In this scenario, interactive speech transcription (IST) guided by CMs can help in massively producing acceptable transcripts for large amounts of videos with limited manual effort (Silvestre-Cerda et al., 2013).Motivated by the scenario depicted above, we propose to adapt the CM models to the speaker in an attempt to improve CM classification and IST performance. To do so, we formulate the speaker adaptation to extend both the published NB and the proposed LR models.The rest of the content is organised as follows: the inclusion of speaker dependence into the NB model is described in Section 2. Section 3 proposes the LR model and formulates its corresponding speaker-dependent version. Section 4 describes the evaluation of the proposed models on two challenging tasks based on ASR transcripts from videoLectures.net and poliMedia repositories. Comparative results are presented including also conditional random field (CRF) models (Seigel, 2013; Seigel and Woodland, 2011; Fayolle et al., 2010). Section 5 proves that the increased CM performance results in better amended transcripts for videoLectures.net when integrated into an IST application. Finally, Section 6 raises the conclusions.In this section, we introduce a speaker-adapted confidence estimator model. The model is designed to extend the naïve Bayes (NB) approach that was successfully applied to speech recognition (Sanchis et al., 2012) as well as to machine translation (Sanchis et al., 2007). Thus, let us first briefly recall the speaker independent NB model.The NB model is formulated on the framework of confidence estimation addressed as a classification problem. On this framework, the recognised words are labelled as correct (c=1) or incorrect (c=0) by means of the class posterior given the word (w) and a vector of input scores (x):(1)cˆ=argmaxcp(c|w,x)=argmaxcp(c|w)p(x|c,w)where Eq. (1) is obtained by applying Bayes’ rule and then ignoring the class-independent term. Also, the values of all the involved variables in the latter equation are assumed to be discrete. Discretisation avoids the need of explicitly modelling the probability distribution of continuous-valued features, while it renders a more flexible and data-driven model. Details on discretisation and several different approaches can be found in Seigel (2013). Here, we just discretised by dividing the feature domain into a fixed number of evenly-spaced bins. The optimal number of bins was tested on the development set.The estimation ofp(x|c,w)is usually biased due to the training data sparsity. More robust estimations can be obtained by simplifying the problem with the following strong independence assumption (the “naïve Bayes assumption”):(2)p(x|c,w)=∏dDp(xd|c,w)Therefore, the basic problem is to estimatep(xd|c,w)for each class-word pair andp(c|w)for each target word. Given N training samples{(xn,cn,wn)}n=1N, these probabilities can be computed as the maximum likelihood estimate (MLE):(3)p(c|w)=N(c,w)N(w)p(xd|c,w)=N(c,w,xd)N(c,w)where {N(·)} are suitably defined event counts on a given training data set. However, the MLE quickly overfit the training data. In order to prevent this overfitting, a particular backing-off smoothing method was introduced in Sanchis et al. (2012).We propose to extend the NB into a naïve Bayes speaker-adapted model (NB+spk). For that, a new variable s is introduced into Eq. (1) to identify the speaker:(4)cˆ=argmaxcp(c|w,x,s)=argmaxcp(c|w,s)p(x|c,w,s)Consequently, the problem is turned into computingp(xd|c,w,s)for each class-word-speaker triplet andp(c|w,s)for each word-speaker pair, which analogously can be simply estimated by means of their MLE:(5)pˆ(c|w,s)=N(c,w,s)N(w,s)(6)pˆ(xd|c,w,s)=N(c,w,xd,s)N(c,w,s)As in the non-adapted (speaker-independent) NB approach, MLE overfitting can be prevented by using a straightforward extension of the backing-off smoothing method proposed in Sanchis et al. (2012).In this section, we propose a new CM model based on logistic regression (LR) models. It is worth noting that, for binary classification problems such as the CM problem considered in this work, these models are equivalent to more general conditional random field (CRF) models. In what follows, after describing the proposed speaker-dependent LR models, we briefly discuss how to discriminatively learn them using the MSE training criterion (Section 3.1). In Section 4, this criterion is empirically compared with a similar yet different criterion that is commonly used in CRF training.The proposed approach resembles the ones presented in Estienne et al. (2008). However, that work formulated the classification problem as a generative model, and only the posterior of the features was attempted to be learnt in a discriminative way. Furthermore, the purpose was to mimic NB, so no improvements were obtained. Hence, in contrast to Estienne et al. (2008), here we do model the class posterior; define simpler input functions for the LR model; introduce a standard L2 regularisation to avoid the complex set of maximum entropy constraints with cut-offs; and use the MSE learning criterion, optimised with the simple and fast iRPROP+ (Igel and Hüsken, 2003) algorithm.The assumption of a general LR distribution for the class posterior yields the following classification rule:(7)cˆ=argmaxcp(c|w,x)=argmaxcexp(∑iλifi(c,w,x))Z(w,x)wherewis the recognised word and x=(x1, …, xD) is a D-dimensional vector of discretised input features. On the other hand,Zis a normalisation constant, which does not affect classification; λ(·) are a set of data-driven parameters; and f(·) a set of functions which give the model expressiveness.As discussed before, the NB model in Sanchis et al. (2012) introduced several convenient assumptions: conditional independence amongst the D scores, discretisation of the continuous-valued scores, etc. Hence, we propose now a particular definition for the f(·) functions to make the LR model behave similarly to the NB model in terms of classification.Let i be the triplet of labels (c˜∈{0,1},w˜∈{1,…,W},x˜d˜∈{1,…,Xd˜}) indexing the classes, the known vocabulary and the values of the score numberd˜∈{1,…,D}respectively.Xd˜accounts for the total number of different possible discrete values ofxd˜. For each possible triplet, let us define the following function:(8)fc˜,w˜,x˜d˜(c,w,x)=δc˜(c)·δw˜(w)·δx˜d˜(x)with δ(·) being the Kronecker delta andδx˜d˜(x)≡∏d′Dδx˜d˜(xd′)·δd˜(d′)=δx˜d˜(xd˜).It becomes clear from the latter definition that the set of functions{fc˜,w˜,x˜d˜}serves merely to activate the corresponding weights{λc˜,w˜,x˜d˜}. Thus, it is the set of weights alone which will render the classification, and they are to be learned exclusively from data, as detailed in Section 3.1. Also, it should be noted that each of the defined functions does not involve more than one score. This is precisely equivalent to assuming naïve Bayes over the scores, as in Eq. (2).Furthermore, in order to prevent overfitting, additional weights and functions to be active independently of one or more label values are necessary:(9)fc˜,∅,∅(c,w,x)=δc˜(c)fc˜,∅,x˜d˜(c,w,x)=δc˜(c)·δx˜d˜(x)fc˜,w˜,∅(c,w,x)=δc˜(c)·δw˜(w)These terms enable a behaviour similar to the smoothing in the NB model, which backs off to less specific probabilities under certain conditions.Finally, it should be noted that the presented model typically involves a huge number of weights to be estimated, of orderO(vocabulary×number of features×mean number of values per score). Fortunately, the computation time can be halved by defining a new set of weightsλ(·)≡λc˜=1,(·)−λc˜=0,(·), and the corresponding activation features:(10)fw˜,x˜d˜(w,x)=δw˜(w)·δx˜d˜(x)f∅,∅(w,x)=1f∅,x˜d˜(w,x)=δx˜d˜(x)fw˜,∅(w,x)=δw˜(w)in this way, Eq. (7) adopts the following expression:(11)p(c|w,x)=11+exp((−1)c∑iλifi(w,x))Speaker dependence can be easily introduced into Eq. (11), yielding a logistic regression speaker-adapted (LR+spk) model:(12)p(c∣w,x,s)=11+exp((−1)c·(∑iλifi(w,x)+∑jλjfj(w,x,s)))where speaker dependence has been formulated as a separated sum over j for the sake of clarity. Now, the number of weights to be estimated is increased by S times, S being the number of known speakers. In this case, the new index j should map the triplet of labels (w˜∈{∅,1,…,W},x˜d˜∈{∅,1,…,Xd˜},s˜∈{1,…,S}).Thus, speaker adaptation results in the addition of the following functions:(13)fw˜,x˜d˜,s˜(w,x,s)=δw˜(w)·δx˜d˜(x)·δs˜(s)fw˜,∅,s˜(w,x,s)=δw˜(w)·δs˜(s)f∅,x˜d˜,s˜(w,x,s)=δx˜d˜(x)·δs˜(s)f∅,∅,s˜(w,x,s)=δs˜(s)As discussed in Section 1, the weights of the discriminative models can be estimated to minimise the MSE, which may be preferable for classification problems instead of the MMI criterion and the MLE criterion for generative models. Given N training samples{(xn,cn,wn)}n=1N, the MSE can be formulated as an optimisation problem by means of the objective:(14)FMSE(λ)=∑n=1N(cn−pλ(cn=1|wn,xn))2However, there is no closed form solution for the optimal λ under the minimum MSE constrain. Fortunately, any simple gradient descent based optimisation algorithm can succeed in finding the solution despite the MSE not being a convex criterion. In this work, we opted for the simpler iRPROP+ (Igel and Hüsken, 2003) iterative algorithm, which provides faster convergence than other more expensive methods such as generalised iterative scaling (GIS) (Darroch and Ratcliff, 1972). A recent evaluation of different optimisation algorithms on a large task can be found in Wiesler et al. (2013).Another common issue of many training criteria, including MSE, is that they easily overfit the weights to the training data. Since there is no clear way to smooth discriminatively trained models, a typical amendment is to add a L2 regularisation term to the objective:(15)F(λ)=FMSE(λ)−C2∑i(λi−λi(0))2where λ(0) can be either a reliable estimation of the weights or simply 0.For our model,λi(0)=0is a clever guess, since it prevents the features from having an overrated impact. During experimentation, the zero regularisation made the feature-independent term λ∅,∅ drop quickly to zero after a few iterations. This behaviour can be interpreted as an increased generalisation of the model, since λ∅,∅ is proportional to the logarithm of the class prior p(c) from the generative point of view. Thus, for two different models yielding the same performance on a certain test, the one with λ∅,∅ closer to zero is likely to perform better on a new test with different prior distribution.

@&#CONCLUSIONS@&#
We have introduced a new particular logistic regression model to improve the reliability of the confidence measures for automatic speech recognition. Also, as a main contribution, we have proposed the use of speaker-adapted models.The experiments have shown that speaker-adapted models outperform their non-adapted counterparts on two difficult tasks from English (videoLectures.net) and Spanish (poliMedia) educational lectures. The proposed logistic regression model achieved comparatively good results.Finally, a simple real application of interactive speech transcription guided by confidence measures has confirmed that the gains obtained by the proposed models translate into a noticeable improvement of the resulting semi-supervised transcriptions for an equal level of user effort.