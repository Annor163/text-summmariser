@&#MAIN-TITLE@&#
Local online kernel ridge regression for forecasting of urban travel times

@&#HIGHLIGHTS@&#
Local online kernel ridge regression (LOKRR) is developed for forecasting urban travel times.LOKRR takes into account the time varying characteristics of traffic series through the use of locally defined kernels.LOKRR outperforms ARIMA, Elman ANN and SVR in forecasting travel times on London’s road network.The model is based on regularised linear regression, and clear guidelines are given for parameter training.

@&#KEYPHRASES@&#
Forecasting,Travel time,Prediction,Time series,Kernel method,Machine learning,

@&#ABSTRACT@&#
Accurate and reliable forecasting of traffic variables is one of the primary functions of Intelligent Transportation Systems. Reliable systems that are able to forecast traffic conditions accurately, multiple time steps into the future, are required for advanced traveller information systems. However, traffic forecasting is a difficult task because of the nonlinear and nonstationary properties of traffic series. Traditional linear models are incapable of modelling such properties, and typically perform poorly, particularly when conditions differ from the norm. Machine learning approaches such as artificial neural networks, nonparametric regression and kernel methods (KMs) have often been shown to outperform linear models in the literature. A bottleneck of the latter approach is that the information pertaining to all previous traffic states must be contained within the kernel, but the computational complexity of KMs usually scales cubically with the number of data points in the kernel. In this paper, a novel kernel-based machine learning (ML) algorithm is developed, namely the local online kernel ridge regression (LOKRR) model. Exploiting the observation that traffic data exhibits strong cyclic patterns characterised by rush hour traffic, LOKRR makes use of local kernels with varying parameters that are defined around each time point. This approach has 3 advantages over the standard single kernel approach: (1) It allows parameters to vary by time of day, capturing the time varying distribution of traffic data; (2) It allows smaller kernels to be defined that contain only the relevant traffic patterns, and; (3) It is online, allowing new traffic data to be incorporated as it arrives. The model is applied to the forecasting of travel times on London’s road network, and is found to outperform three benchmark models in forecasting up to 1h ahead.

@&#INTRODUCTION@&#
The short term forecasting of traffic variables such as speeds, flows and densities is one of the primary goals of Intelligent Transportation Systems (ITSs), with applications in dynamic signal control, advanced traffic management systems (ATMSs) and advanced traveller information systems (ATISs) (Vlahogianni et al., 2004). To date, a wide range of methods have been used for short term traffic forecasting, which can be broadly separated into two categories: (1) parametric methods, and; (2) machine learning (ML) methods.1ML methods are often referred to as nonparametric methods (Vlahogianni et al., 2004) or computational intelligence (CI) methods (Karlaftis and Vlahogianni, 2011).1The former type includes statistical (space) time series methods such as the (space–time) auto-regressive integrated moving average (ST)ARIMA model family (Billings and Yang, 2006; Cheng et al., 2010; Kamarianakis and Prastacos, 2005; Williams and Hoel, 2003), state space models (Okutani and Stephanedes, 1984; Stathopoulos and Karlaftis, 2003), and Bayesian networks (Anacleto et al., 2013a; Fei et al., 2011; Sun et al., 2004, 2005, 2006; Zheng et al., 2006). Comprehensive reviews can be found in Vlahogianni et al. (2004) and, more recently Vlahogianni et al. (2014). These methods typically assume that the data being described are stationary. That is, they must have constant mean and variance. If this assumption is not satisfied, then the data must be transformed through differencing, or some other transformation (Kendall and Ord, 1990). It is often found that these assumptions are difficult to satisfy, leading standard parametric methods to perform poorly. This has led to the recent development of local parametric model specifications that attempt to model the local characteristics of traffic data in time and/or space (Ding et al., 2010; Kamarianakis et al., 2012; Min and Wynter, 2011; Min et al., 2009, 2010).An alternative approach is to model the data directly in a nonlinear machine learning (ML) framework. ML methods typically make minimal explicit assumptions about the data generating process, and instead try to learn the characteristics of the data through exposure to examples (Mitchell, 1997). ML methods have often been shown to outperform parametric methods in the literature, although a recent comparison study by Chen et al. (2012) suggests that data preprocessing is just as important as model choice. The most widespread ML method in the short term traffic forecasting literature is the artificial neural network (ANN). ANNs have a long history of successful implementation in traffic forecasting, and readers are directed to Dougherty (1995) for a review of the early work. As the power of computers has increased, researchers have developed increasingly sophisticated ANNs for forecasting traffic variables both on highways (see, e.g., Chen and Grant-Muller, 2001; van Lint et al., 2005; van Hinsbergen et al., 2009; Huang and Sadek, 2009; Li and Rose, 2011) and on urban networks (see, e.g. Ledoux, 1997; Yin et al., 2002; Vlahogianni et al., 2005, 2007). The most successful implementations circumvent the traditional black box problem of neural networks by explicitly incorporating domain knowledge into the model structure. For example, the topological structure of the road network can be explicitly represented in the nodes of the hidden layer of an ANN, making the internal function of the model more transparent (van Lint et al., 2005; van Lint, 2006). In a similar vein, Vlahogianni et al. (2005) frame their genetically optimised modular ANN as a multivariate non-linear time series model, fed with spatially and temporally lagged data. The recent work of (Chan et al., 2013a,b; Chan et al., 2012a,b) has further enhanced the position of ANNs at the forefront of the traffic forecasting literature. Other ML methods that have been applied to traffic forecasting include fuzzy rule based systems (Dimitriou et al., 2008) and hybrid models (Van Der Voort et al., 1996; Hofleitner et al., 2012). Karlaftis and Vlahogianni (2011) summarise the main differences (and similarities) between ML methods and parametric methods.Recent developments have seen the application of kernel methods (KMs) to traffic forecasting. The term kernel method is an umbrella term for a broad set of techniques that share a common characteristic. They comprise two components: (1) a function that maps the input data into a high (possibly infinite) dimensional space, known as a feature space, and; (2) a learning algorithm capable of discovering linear patterns in that space (Shawe-Taylor and Cristianini, 2004). Mapping to the feature space is accomplished efficiently using a kernel function, hence the term KM. Because linear relations are sought in the feature space, a broad range of theoretically well founded and efficient linear algorithms can be used. To date, many linear algorithms have been kernelised including ridge regression (Hoerl and Kennard, 1970; Saunders et al., 1998), the generalised portrait (Vapnik and Lerner, 1963; Boser et al., 1992), principal components analysis (Schölkopf et al., 1997) and canonical correlation analysis (Hotelling, 1936; Hardoon et al., 2004) amongst many others. KMs are modular in nature, meaning that any kernel algorithm can be applied using a particular kernel, and vice versa (Shawe-Taylor and Cristianini, 2004). This gives them great flexibility as a tool for solving a wide range of practical problems. KMs are an attractive approach for modelling nonlinear and nonstationary data because they combine the advantages of principled, linear learning algorithms such as ordinary least squares (OLS) with nonlinear solutions.The most widely used KM in traffic forecasting is Support Vector Regression (SVR). Wu et al. (2004) first used SVR for the forecasting of travel times on Taipei’s freeway system. Travel times over three distances for 28 consecutive days are used to train a model to forecast the following 7days’ travel times in a one-step-ahead scenario. The results are compared to a current–time predictor and a historical mean predictor and are found to be superior in all cases. The performance of SVR compares favourably with that of ANNs. Vanajakshi and Rilett (2007, 2004) compare the performance of SVR and ANNs in forecasting travel times on San Antonio’s freeway system using a forecast horizon varying from 2min to 1h ahead. It is found that SVR performs better than ANNs when the size of the training set is small. Zhang and Xie (2008) used ν-SVR for highway traffic volume forecasting, and found the method to outperform a multi-layer feedforward neural network (MLFNN). Other KMs have been applied to traffic forecasting with similar success. Xie et al. (2010) apply Gaussian processes regression (GPR) to highway traffic volume forecasting, and the results are compared with the ν-SVR model of Zhang and Xie. The results are found to be similar, but the GPR model has the advantage of providing error bounds on the forecasts.One of the main challenges in applying KMs lies in deciding what information to include in the kernel. In KMs a kernel induced feature space is constructed from a database of historical data patterns to store the relevant information about a particular problem. This feature space must contain sufficient richness of patterns in order to produce accurate forecasts, while not being so large as to sacrifice computational efficiency. Wang and Shi (2013) designed an SVR model in which the input space is defined using chaos theory and wavelet theory, namely the Chaos-Wavelet Analysis-Vector Machine (C-WSVM). It is demonstrated that selection of the correct input space to an SVR model can improve forecasting performance. However, the number of data points required in the kernel is not considered, and time varying input spaces are not investigated. In long traffic series, it is not feasible to use the entire historical dataset to forecast at each point in time because this would involve the construction of very large kernels. Therefore, typically one seeks to select a subset of the data that will be most informative. The most common approach taken in the literature is to use a small subset of the most recent observations, which makes the unrealistic assumption that a sufficiently diverse range of traffic states have been observed recently. For example, Wu et al. (2004) use just 5weeks of data to train their SVR model, while (Hong, 2010, 2011, 2012; Hong et al., 2011) use just 1month of data. While the ability to produce strong performance on smaller training data sets is one of the advantages of SVR (Vanajakshi and Rilett, 2004), the use of such a short period of training data means that there is unlikely to be sufficient richness of unusual traffic patterns. Furthermore, as time passes the training data will lose its relevance due to the yearly seasonal trends in traffic data.One way to address the latter problem is to use online or sequential training in KMs. For example, Castro-Neto et al. (2009) implemented an online SVR algorithm for short term traffic flow forecasting. Instead of retraining the model every time new data become available, which is computationally expensive, the model is iteratively updated three samples at a time and the solution support vectors are changed accordingly. The advantage of this approach is that new information is incorporated into the existing structure of the solution. Empirical results show that the method outperforms Holt’s exponential smoothing, and multi-layer perceptron (MLP) ANNs. Gaussian Maximum Likelihood (GML) produces better results under typical conditions but the OL-SVR model performs well under non-recurrent conditions due to the fact it adapts well to new data. Given that non-recurrent events are more difficult to predict this is a significant benefit. However, it does not address the issue that only very recent traffic patterns are included in the kernel, in this case 15days.It is well known that road traffic exhibits strong cyclic patterns, usually characterised by a peak period in the morning and evening, with intervening periods of lower traffic. This phenomenon is described statistically as seasonal temporal autocorrelation. Moreover, traffic conditions in the peak periods are more variable than those in the intervening periods, a phenomenon known as heteroskedasticity (Fosgerau and Fukuda, 2012). From the perspective of pattern analysis, this temporal locality implies that data pertaining to the period between peaks will be largely ineffective in forecasting the peak periods, and vice versa, motivating the development of models that produce forecasts based on similar traffic states. The simplest way to achieve this is to arbitrarily divide the data into time periods, such as AM peak and PM peak, and construct a separate model for each (e.g. Hong et al., 2011; Hong, 2012; Stathopoulos and Karlaftis, 2003). A more considered approach is to attempt to identify traffic states based on historical data and construct a model for each one (Kamarianakis et al., 2010, 2012; Min and Wynter, 2011). In operation, the models are switched according to the current traffic state. One can also make the assumption of smooth transition of traffic states, and construct a model in which the parameters vary smoothly by time of day (Anacleto et al., 2013a, 2013b). The latter model type is designed to model the seasonal autocorrelation and heteroskedasticity in traffic data.In this study, a novel local kernel based algorithm is developed for the forecasting of traffic series, namely local online kernel ridge regression (LOKRR). LOKRR has its roots in OLS, making it relatively simple to interpret and implement compared with other KMs. The structure of the paper is as follows: In Section 2, the LOKRR algorithm is described, beginning with a motivation for the model. In Section 3, a case study is introduced, in which LOKRR is used for forecasting Unit Travel Times (UTT, seconds/metre) collected using automatic number plate recognition (ANPR) on London’s road network. In Section 4, the results are presented and analysed. Finally, in Section 5, some conclusions and directions for future research are given.To motivate the model in the current context, the example of forecasting travel times on a single road section (link) is used. Assume a stream of observations of a traffic variable such as flow, density or travel time [z1,z2,…,zTare observed at times t=1,2,…,T, obtained at an interval of τ=5min over a number of days. The task is to make use of observations of the process up to time t to forecast the value of zt+1. Each day, (60/5)*24=288 observations are made. Now assume that because of computational constraints the maximum kernel size of a kernel based model is set to 10,000*10,000, which is a reasonable upper limit. The standard approach is to use the most recent observations of the process to construct a single kernel. Using this approach entails that a maximum of 10,000/288=34.7days of data could be included in the kernel. This approach has two clear drawbacks. Firstly, 34.7days is a relatively short time in this context, and it is unlikely that sufficient variation in traffic patterns will have been observed over this period in order to produce reliable forecasts, particularly under abnormal conditions. Secondly, because of the strong cyclic pattern present in traffic data, historical traffic patterns recorded at temporal lags that are either close to zero or close to divisible by 288 are likely to be much more informative than those at other lags. For example, when forecasting the level of flow at 9AM on a Tuesday morning, historical data pertaining to midnight on a Saturday is unlikely to be useful. Furthermore, the data with which the model is trained is important: If the model is trained on data from August and applied for forecasting traffic in December, it is likely to encounter problems due to the differences in traffic between August and December.Consider as an alternative, the extreme example where zt+1 is forecast as a function of only those travel times that were observed at the same time of day on previous days. With the same maximum kernel size, one could build a kernel containing the travel time patterns obtained on the previous 10,000days. Of course, in practice, one would not have access to 10,000days (27.4years) of historical data, and the nonstationarity in long term travel patterns would render data collected beyond a certain time threshold irrelevant. However, this example serves to highlight the fact that if knowledge of the cyclic nature of traffic is directly incorporated into the model structure, then there is potential for having access to a much richer source of information about the time point to be forecast without increasing the kernel size. In fact, it is possible to include much more informative data in a far smaller kernel. Using the same example, one could capture the behaviour of the link at a single time point over the course of a year in a kernel of size 365*365.The advantage of the formulation outlined above is that it directly incorporates seasonality into the model structure. However, it has the drawback that it may exclude a significant amount of data that may be of interest. Returning to the example of forecasting some traffic variable on a road link, if one were to commute to work on the same road at approximately the same time each day, one may observe that the road tends to become congested at approximately the same time each day, and may be able to make statements such as “if I leave after 9am there is always too much traffic”, or “if I set off before 8 am my journey is usually pretty quick”. However, there is usually significant variation around such trends. For instance, on some days a link may become congested earlier or later than usual; or the congestion may be slightly more or less severe, and one might find oneself making a statement such as “It’s especially busy today, something must have happened”, or “wow, it’s really quiet, it’s usually really busy by now”. These intuitive observations summarise the variability inherent in traffic data, and a successful traffic forecasting model should be capable of modelling this variability.Fig. 1shows an illustration of the variability in Unit Travel Times (UTT, seconds/km) on a road link in London, UK. There are two recurrent peaks on this link, a small one in the morning and a larger one in the evening. Although they occur at roughly the same time each day, the time at which they begin and end, as well as their magnitude, differs within a time window. Within this window, the observed historical patterns are likely to be informative and should be included in the kernel. From the perspective of kernel based methods, this means that there is a temporal window extending in both directions around each point within which past information is informative and a kernel can be constructed. Based on this observation, a local kernel based model of traffic data is proposed, in which local kernels are constructed around each time point. This model is described in the following subsections.The LOKRR model is based on a simple form of regularised linear regression, called ridge regression (RR) (Hoerl and Kennard, 1970). Beginning with the well-known case of multiple linear regression, one seeks to solve a system of equations of the following form:(1)y=Xw+εwhere X is (n∗p) and of rank p,n is the number of observations, p is the number of variables, w is (p∗1) and unknown ,E[ε]=0, andEεε′=σ2In. The solution of this system for w is:(2)w=(X′X)-1X′yThis solution is viable when X′X is nearly a unit matrix. However, if this is not the case then this solution is sensitive to the number of errors in the data. To overcome this problem, a regularisation constantλcan be introduced:(3)w=(X′X+λIp)-1X′y;λ≥0λis called the ridge parameter and the resulting algorithm is known as ridge regression. The matrix(X′X+λIp)is always invertible ifλ>0, allowing the solution of ill-posed regression problems. Ifλ=0, Eq. (3) is equivalent to Eq. (2).Linear RR can be converted to a nonlinear algorithm. Eq. (3) can be rearranged in terms of w as follows (Shawe-Taylor and Cristianini, 2004):(4)w=λ-1X′(y-Xw)=X′αw can be written as a linear combination of the training data points,w=∑i=1naixi, withα=λ-1(y-Xw). Therefore, α can be computed as:(5)α=λ-1(y-Xw)⇒λα=(y-XX′α)⇒(XX′+λIn)α=y⇒α=(G+λIn)-1ywhere G=XX′, and is known as the Gram matrix. It can be seen from Eq. (5) that the solution is now written in terms of the data points and the weight vector w need not be solved explicitly. This formulation is computationally more efficient than Eq. (3) when p>n, which is often the case in machine learning applications. However, the main benefit of expressing the algorithm in this way is that, given a valid kernel function K, G can be replaced with a kernel matrix K as follows:(6)α=(K+λIn)-1yThis algorithm is known as kernel ridge regression (KRR). A forecast for a new data point can be computed as:(7)g(x)=y′(K+λIn)-1kwhere K is a kernel matrix of inner products between training vectors and k=k(xi,x) is a vector of inner products between the test vector xiand the training vectors x. KRR is called a regularization network in the ANN literature (Poggio and Girosi, 1990). The method is also strongly related to Kriging (Krige, 1951), which is termed GPR in the machine learning community (Rasmussen and Williams, 2006). There are many kernels that one could use, but the most widely used and generally applicable kernel is the Gaussian radial basis function (RBF) kernel (Keerthi and Lin, 2003):(8)K(x,z)=exp-‖x-z‖22σ2where σ is the kernel bandwidth, and controls the smoothness of the function. In the following description of the methodology, use of the Gaussian RBF kernel is assumed, and the parameter selection described in Section 2.6 is specific to this kernel type.To take into account the long term seasonality and trend in traffic data, the LOKRR model described here is online. Online model training involves processing the data one example at a time, making a forecast, and then updating the model based on the predictive error (Shawe-Taylor and Cristianini, 2004). Online learning allows new data to be incorporated into the model as they arrive, and old data to be removed, enabling the model to adapt to changes in the distribution of the data.LOKRR is based on the sliding window kernel recursive least squares (KRLS) algorithm of Van Vaerenbergh et al. (2006), which is equivalent to an online KRR (OKRR) algorithm. Given a stream of training data points {(x1,y1),(x2,y2),…}, the training data at time t is constructed as yt=[yt,yt−1,…,yt−N+1]′ and Xt=[xt,xt−1,…,xt−N+1]′, where ytand xtare the values of the dependent and independent variables at time t, respectively, N is the size of the window and t=1,2,…,N. In the context of traffic forecasting, an autoregressive model form is usually specified: Given a series of observations of a traffic variable z1,z2,…,zT,yt=zt+1 and xt=[zt,zt−1,…,zt−m+1], where m is the embedding dimension of the series. However, it is possible to include other variables such as a time varying mean, observations of another traffic variable, or other data such as precipitation or incidents.At time t, a regularised kernel matrixKtλ=(Kt+λIt)is constructed from Xt. At the following time step, the data window slides along by one point to time t+1 and the most recent observation is added to the training data and the oldest training point is removed. A new matrixKt+1λis constructed. Thus, at each time point the kernel matrix is constructed from only the previous N training examples. The main computational burden of standard KRR lies in calculating the inverse of the regularised kernel matrix in Eq. (7). OKRR requires the inverse to be recalculated at each time step, which is infeasible in an online setting. Fortunately, methods exist for updatingKtλ-1without computing the inverse from scratch. To remove a row and column, a kernel matrix K and its inverse K−1 can be partitioned as follows (Van Vaerenbergh et al., 2006, p. 8):(9)K=ab′bD,K-1=ef′fGThe inverse of the submatrix D is required, which can be calculated from the submatrices of K−1 as follows:(10)D-1=G-ff′/eTherefore, the updated inverse is calculated from the known elements of K−1. To add a row and column to the inverse, K−1 and K are partitioned as follows:(11)K=Abb′d,K-1=Eff′gThen one can take advantage of the following formula:(12)K-1=A-1(I+bb′A-1Hg)-A-1bg-(A-1b)′ggwhere(13)g=(d-b′A-1b)-1Here, the superscript H of AHdenotes the conjugate transpose of A. A−1 is the inverse kernel matrix obtained at the previous time step, demonstrating that the inverse kernel matrix does not need to be recalculated from scratch. These steps reduce the computational complexity of inverting the matrix from O(N3) to O(N2), where N is the number of data samples in the kernel.OKRR has the advantage over standard KRR that it can incorporate new information in the model structure without a significant increase in computation time. However, it has two limitations that limit its application to traffic series. Firstly, a single kernel is constructed using only the data of the N time points immediately preceding time t. This is reasonable in cases where time series exhibit sudden regime changes such as the Wiener system reported in Van Vaerenbergh et al. (2006). However, in seasonal data, this is likely to be insufficient since the most informative data comes not just from the immediately preceding time points, but from the same times on previous days, weeks or months. The second drawback is that it uses a single set of parameters (ridge parameterλand kernel parameter(s)) to describe the behaviour of the system across all times. This limits the ability of the model to account for heteroskedasticity. To address these two limitations, a local version of OKRR is proposed, namely Local (L)OKRR, in which a separate kernel is defined for each time of day, each with its own set of parameters.Consider, again, a stream of training data points {(x1,y1),(x2,y2),…}. The goal is to forecast ytusing a subset of the observed patterns. Assume the dataset has a regular seasonal component with order S. The training dataset is constructed as yst=[y1,t,y2,t,…,yn,t]′ and Xst=[x1,t,x2,t,…,xn,t]′, where s=1,2,…,N is the index of the seasonal component, for example the day index, and t=1,2,…,S is the time of day index. This formulation is shown diagrammatically in Fig. 2.The rows of the table indicate the successive days in the training dataset. The columns represent the successive time intervals in a day. N is the total number of days in the training dataset. In the example outlined above of forecasting travel times, S=288. Under this formulation, S kernels K1,K2,…,KSare constructed from the columns of Fig. 2. For example, to forecast the travel time at t2, the first column of training examples is used. Compared with a single kernel model, this would appear to be a large number of kernels, however, each of them can have a much smaller dimension than a single kernel model. Furthermore, a smaller kernel needs to be updated at each time step, which is computationally more efficient than updating a single large kernel. Eq. (14) shows a local temporal kernel constructed in this way.(14)Kst=k(x1,t,x1,t)k(x1,t,x2,t)⋯k(x1,t,xN,t)k(x2,t,x1,t)k(x2,t,x2,t)⋯k(x2,t,xN,t)⋮⋮⋱⋮k(xN,t,x1,t)k(xN,t,x2,t)⋯k(xN,t,xN,t)The model form shown in Fig. 2 restricts the model to include only those patterns that were observed at exactly the same time of day. However, it was argued in Section 2.2 that this is overly restrictive. To address this, a local temporal window is formed around each time t, which is denoted as w and the training data is constructed as:(15)ystw=[{y1,t-w,…,y1,t-1,y1,t,y1,t+1,…,y1,t+w},…,{yn,t-w,…,yn,t-1,yn,t,yn,t+1,…,yn,t+w}]′(16)xstw=[{x1,t-w,…,x1,t-1,x1,t,x1,t+1,…,x1,t+w},…,{xn,t-w,…,xn,t-1,xn,t,xn,t+1,…,xn,t+w}]′Based on this, kernels K1,K2,…,KSare defined according to:(17)Kt=K1,1K1,2⋯K1,nK2,1K2,2⋯K2,n⋮⋮⋱⋮Kn,1Kn,2⋯Kn,nwhere(18)K1,1=k(x1,t-w,x1,t-w)⋯k(x1,t-w,x1,t-1)k(x1,t-w,x1,t)k(x1,t-w,x1,t+1)⋯k(x1,t-w,x1,t+w)⋮⋱⋮⋮⋮⋱⋮k(x1,t-1,x1,t-w)⋯k(x1,t-1,x1,t-1)k(x1,t-1,x1,t)k(x1,t-1,x1,t+1)⋯k(x1,t-1,x1,t+w)k(x1,t,x1,t-w)⋯k(x1,t,x1,t-1)k(x1,t,x1,t)k(x1,t,x1,t+1)⋯k(x1,t,x1,t+W)k(x1,t+1,x1,t-w)⋯k(x1,t+1,x1,t-1)k(x1,t+1,x1,t)k(x1,t+1,x1,t+1)⋯k(x1,t+1,x1,t+w)⋮⋱⋮⋮⋮⋱⋮k(x1,t+w,x1,t-w)⋯k(x1,t+w,x1,t-1)k(x1,t+w,x1,t)k(x1,t+w,x1,t+1)⋯k(x1,t+w,x1,t+w)Including a window of patterns around t increases the amount of local temporal information available to the model. However, it also increases the kernel size. The kernel defined in Eq. (14) is n*n, whereas the kernel defined in Eq. (17) is (n*((2*w)+1))*(n*((2*w)+1)). As w increases, the dimension of K increases by 2n. Therefore, it is preferable to keep w small. However, it should be noted that, in an online setting, this formulation is still efficient compared with using a kernel defined on all the data.The formulation outlined above allows for local tuning of the model parameters. Given a model with S=288, 288 separate kernels are defined, each of which has its own set of parameters. The training of these parameters has no added computational cost over the training of a single set of parameters since the model still requires the inversion of a single kernel matrix at each time step. Therefore, each of the 288 models can be trained simultaneously. This enables the model to capture temporal heteroskedasticity by allowing the kernel bandwidth and the ridge parameter to vary by time of day. Parameter selection in kernel based models is an active research area, and various methods, mainly heuristic in nature, have been used to improve the parameter selection process. For example, in SVMs, genetic algorithms (Üstün et al., 2005; Wu et al., 2009; Cai et al., 2009; Li and Yang, 2008), chaotic immune algorithms (Wang et al., 2009a), particle swarm optimisation (Li et al., 2010), immune particle swarm optimisation (Wang et al., 2009b) differential evolution (Lahiri and Ghanta, 2008; Li and Cai, 2008) and ant colony optimisation (Zheng et al., 2008) have been used to select parameters, amongst others. Although this research has been instrumental in improving the speed of training of machine learning algorithms, in an applied setting it is desirable to have a parameter selection process that is interpretable and simple to implement, particularly when the model is to be applied in a large number of cases. In the following subsections, the selection method is described for each of the model parameters in the context of using a Gaussian RBF kernel, which are λ, σ and w. The parameter space is derived from the data, making parameter selection straightforward in any application.The regularisation constantλis a free parameter that needs to be tuned. Usually,λis determined by cross-validation over a space of values. It is sensible to try to limit this space of values in order to limit the computation time required to train models. Exterkate (2013) showed that appropriate values ofλcan be found by relatingλto the signal to noise ratio φ. First the R2 is obtained from an OLS fit of y on X. If the OLS fit were the true model, then φ0=R2/(1−R2). From this,λ0can be determined for a Gaussian kernel from the signal to noise ratio asφ0=1/λ0. Exterkate recommends the following grid be used to trainλ:{18λ0,14λ0,12λ0,λ0,2λ0}. In the study, Monte Carlo simulation was used to compare the mean squared prediction error (MSPE) obtained from this grid with that obtained from the true values ofλ, and it was found that estimatingλresulted in an increase in MSPE of only around 0.4%. It follows that the added computational effort associated with training a larger grid of values forλis not justified. The method of Exterkate is therefore recommended for tuningλin LOKRR.The second free parameter in the LOKRR model is the kernel parameter. In the case of the Gaussian RBF kernel defined in Eq. (8), this is the kernel bandwidth σ, which determines the smoothness of the forecasting function. A large value of σ results in a smooth function while a small value results in a more complex function. There is a wealth of literature on the training of σ in Gaussian kernels, particularly in the context of SVMs, and some of the methods that have been used to date were listed in Section 2.6. However, it is important to recognise that there is a small range of values within which σ can vary, which is based on the variance of the data in a given kernel. Caputo et al. (2002) showed that the optimal values of σ lie between the 0.1 and 0.9 quantiles of||x-x′||2and so can be estimated from the Euclidean distance between the data points in the kernel. This is the scheme used for automatic σ estimation for SVM models in the R package Kernlab (Karatzoglou et al., 2004, 2006). Exterkate (2013) proposed a similar method for selecting σ in the context of KRR.While it is likely that more sophisticated training schemes will yield slightly more accurate results, the size of the improvement is unlikely to justify the increased computational burden of training extra parameter combinations. Therefore, in this study, the approach of Caputo et al. is used to estimate the median, 0.25 and 0.75 quantiles of each kernel, and these three values of σ are used to train each model. Not only does this reduce the size of the parameter space that needs to be searched, it also provides a principled way of training LOKRR models on other datasets.The window size w is an important parameter that decides how much local temporal information to include in the model. The larger w gets, the more information is included in the model pertaining to each day of the training data. However, as mentioned in Section 2.5.2, an increase in w of 1 results in an increase in the dimension of the kernel of 2n. Therefore, it has a significant effect on the computation time required to train and update models. Holding the overall size of the kernel static, an increase in w necessitates a 2w+1-fold decrease in n. Therefore, a smaller w is desirable in order to maintain computational efficiency. The value of w is determined in the model training process and depends on the application. However, it is recommended to restrict its maximum value. The values tested here are w=1,2,3. Because of strong seasonal temporal autocorrelation in the data it is assumed that a maximum window size of 3 is sufficient. In other applications, a strategy of holding the kernel size static, and reducing the training length as w increases is recommended in order to examine the tradeoff between w and training data length.Data normalisation, also referred to as scaling, is important in kernel methods because it brings all of the independent variables into the same range. This ensures that no single variable dominates over the rest of the variables when the distance between the training vectors is calculated in the kernel. There are various types of normalisation that could be used, including standardization to produce z-scores, scaling by domain (i.e. [0,1]) and minmax normalisation (Juszczak et al., 2002). Each kernel is normalised separately. This enables each kernel to capture the distribution of the data around time t, and brings the kernel parameters for each time point into the same range. This also enables the heteroskedasticity in the data to be examined through the model parameters. In the context of traffic data, it would be expected that the variance in σ would be higher during the peak periods than in the inter peak periods.The data used in this study are Unit Travel Times (UTTs, seconds/metre) collected on London’s road network, as part of the London Congestion Analysis Project (LCAP) coordinated by Transport for London (TfL). LCAP TTs are observed using automatic number plate recognition (ANPR) technology. Cameras operate in pairs, which are called links. The camera at the start of a link is called its start camera, and the camera at its end its end camera. A diagram of an ANPR link is shown in Fig. 3.Individual vehicle TTs are aggregated at 5min intervals to produce a regularly spaced time series with 288 observations per day. Due to the comparatively poor quality of data collected during the night time period, only data collected between 6AM and 9PM are used in the analysis (180 observations per day). To test the performance of the LOKRR model, the ten LCAP links with the lowest levels of missing data are selected. These links are shown in Table 1, along with the average data frequency (number of vehicles per observation). The spatial location of the links in the network is shown in Fig. 4. Links with low levels of missing data are selected in order to minimise its effects on the results.Fig. 5shows the time series of each of the test links over the course of 10weeks (weekends included). Evidence of the seasonal component in the data is present in the majority of links. However, there is significant variation from day to day. Higher unit journey times are observed in the AM and PM peak periods to varying degrees in all of the links, but there is clear heterogeneity in the magnitude (height) and the duration of the peaks from day to day. Between links there is also considerable variation in the observed traffic patterns. Links 26, 442, 2448 and 881 appear to have fairly stable traffic patterns. However, other links such as 1815 and 1799 are characterised by few very large peaks.In total there are 154days for which data are available, collected between January and July 2011. To test the models, the data are divided into three sets; a training set, a testing set and a validation set, which are 80days (52%), 37days (24%) and 37days (24%) in length, respectively. The testing set runs from Tuesday 28th April to Wednesday 1st June 2011 and the validation set runs from Thursday 2nd June to Friday 8th July 2011. The sizes of the training and testing sets are fixed in this experiment for simple comparison of results with the benchmark models described in Section 3.6.A sliding window validation approach is used to train the models. At the first time step, the training set is used to build a model to forecast the first day of the testing set. Following this, the first day of the training set is removed and replaced with the first day of the testing set. A model is then built using the new training set to forecast the second day of the testing set, and so on and so forth. The selected model is the one that minimises the average training error across the days in the testing set. To measure the training error, the root mean squared error (RMSE) index is used. This is defined as follows:(19)RMSE=1n∑t=1n(yt-yˆt)2where ytandyˆtare the observed and forecast values, respectively.An autoregressive model structure is used to train the models. In an autoregressive model, m previous observations of the series are used to forecast the next value, where m is the embedding dimension. Assume a stream of input/output pairs {(x1,y1),(x2,y2),…} is to be arranged into a vector of dependent variables yt=[yt,yt−1,…,yt−N+1]′ and a matrix of independent variables Xt=[xt,xt-1,…,xt-N+1]′. In an embedded time series, the independent variable vector at time t is constructed as follows2An intercept term is also added in the LOKRR model, but is omitted from the description both for clarity and because it is not required for all time series models.2:(20)xt=[yt,yt-1,…,yt-(m-1)]In this formulation, the temporal ordering of the data is not made explicit beyond the definition of the window w. Therefore, all data patterns in the kernel are considered equally and a forecast is made based only on the similarity of the data pattern observed at time t with the patterns in the historical data. It has been demonstrated in previous studies that including a time varying average as a variable in the model can improve the predictive performance of pattern based models (Smith and Demetsky, 1996). The sample time varying averageμˆtcan be calculated as:(21)μˆt=1n∑i=1nyi,twhere n is the number of days in the training data. The mean is appended to the vector defined in Eq. (20) to give:(22)xt=[yt,yt-1,…,yt-(m-1),μˆt]Includingμˆtin the model introduces the notion of temporal ordering into the feature space defined by the kernel. Essentially, this has the effect of making a prior assumption that those patterns observed at near times to the current pattern are likely to be more informative than those observed at different times. At longer forecasting horizons and larger values of w, the effect of including the average variable is greater.The performance of the LOKRR model is assessed in terms of its ability to forecast travel times at four forecast horizons, which are 15, 30, 45 and 60min ahead. The data are not aggregated to achieve these forecasts because this reduces the temporal granularity of the data. Instead, forecasts are made of 5min aggregated UTTs, at 3, 6, 9 and 12 steps into the future.There are two strategies that can be used to make this type of forecast, which are iterated and direct. In the first strategy, a one step ahead forecast is made and the forecast point is fed back into the model and used to forecast the subsequent time step. This process is iteratively repeated until the desired number of time steps have been forecast. In the second strategy, forecasts are made directly. This is achieved by sampling the series at the desired temporal interval, and using this series to construct a model for each time interval. For example, given 5min averaged data, every second point is used to forecast at the 10min interval and every third point to forecast at the 15min interval. In this study, the second strategy is used as preliminary testing revealed that direct forecasting was more effective than iterated forecasting, both with the LOKRR model and the comparison models described in Section 3.6. Given an embedding dimension m and a time delay τ, the embedded, delayed series xt(τ) is defined as:(23)xt(τ)=[yt,yt-τ,yt-2τ,…,yt-(m-1)τ,μˆt]A separate delayed, embedded series is constructed for each of the forecast intervals.Along with RMSE, three additional performance measures are used. The NRMSE is defined as:(24)NRMSE=RMSEymax-yminwhere ymax and ymin are the maximum and minimum values of the series, respectively. NRMSE is scale free, allowing comparison of performance between links. A feature of the RMSE is that it varies with the variability within the distribution of error magnitudes, the square root of the number of errors, and the average-error magnitude. It has been argued that an absolute error measure is more appropriate when comparing the performance of models (Willmott and Matsuura, 2005). In this case, the mean absolute percentage error (MAPE) is used as percentages are easy to interpret conceptually. The MAPE is defined as:(25)MAPE=1n∑t=1nyt-yˆtytThe mean absolute scaled error (MASE) is a generally applicable measurement of forecast accuracy (Hyndman and Koehler, 2006). The MASE compares the absolute errors in the forecast values with the absolute errors of a naïve forecast, where the previous value of the series is used as the forecast of the next value:yˆt=yt-1. As the naïve model is the simplest possible model, any more complicated time series model should outperform it as a minimum requirement. The MASE is defined as:(26)MASE=1n∑t=1n|et|1n-1∑i=2n|yi-yi-1|where etis the forecast error for a given period and the denominator is the average forecast error of the naïve forecast method. A MASE value less than 1 indicates that the forecast model outperforms the naïve model, while a value greater than 1 indicates the opposite.The ARIMA model is a linear statistical time series model that has been widely used in many time series forecasting applications, including traffic forecasting. ARIMA is well described in the literature, and readers are directed to the texts of Box et al. (1994) and Hamilton (1994) for detailed treatments. ARIMA is commonly used as a benchmark model against which to judge the performance of time series forecasting models. An ARIMA(p,d,q) model is described by its autoregressive order p, its moving average order q, and the number of differences d. In time series with cyclical trends, a seasonal (S)ARIMA(p,d,q)*(P,D,Q) model is often used, where P is the seasonal autoregressive order, D is the order of the seasonal difference, and Q is the seasonal moving average order. To maintain consistency with the LOKRR approach, the multi-step forecasts are made in a single step, rather than recursively.3This approach was found to produce markedly superior results to the iterated approach in preliminary testing.3As the model building procedure of ARIMA requires a continuous time series, τ embedded, delayed series are constructed and concatenated to form a single series on which the model is trained. For example, consider a time series with its origin at t=0. If τ=3, three separate series are constructed, one beginning at t=0, one at t=1 and one at t=2, which are merged into a single series, under the reasonable assumption that each series will have the same properties.The function auto.arima in the R package forecast is used to train the model (Hyndman and Khandakar, 2007). The Akaike Information Criterion (AIC) is used to determine the parameters of the ARIMA models, which is a common alternative to the Box–Jenkins procedure (Ozaki, 1977). Nonseasonal and seasonal model specifications are considered in the model building procedure.SVR is a kernel based machine learning method that has demonstrated strong performance in forecasting of traffic variables in recent years. An excellent introduction to SVR is given in (Smola et al., 2004). SVR is similar to KRR, but has the advantage of maintaining a sparse representation of the solution. The ε-insensitive variant of SVR (ε-SVR) is used here. There are a minimum of three parameters to train in an ε-SVR model, which are:(1)The kernel parameter(s).The regularisation constant C (similar to λ in KRR).The width of the tube ε.In this case, to maintain consistency, the same RBF kernel is used as shown in Eq. (8) and the kernel parameter σ is determined using the same method as the LOKRR model. The value of C is varied in the range C={0.1,1,10,100}. ε is varied in the range ε={0.0001,0.001,0.01,0.1}. After initial feasibility testing, the kernel size is limited to 70days to ensure training times are acceptable. This means a kernel of size 180*70=12,600 is created for each link. SVR is implemented using the kernlab package in R (Karatzoglou et al., 2004).The ANN is the most widely applied traffic forecasting method in the literature. In time series forecasting, ANN models with recurrent architectures have displayed the best results. The Jordan (1986) and Elman (1990) networks are two examples of these. Here, the Elman network is selected as the comparison model as a preliminary analysis revealed superior performance over the Jordan network on this dataset. It must be noted, however, that different ANN structures may yield different results to the Elman network presented here. There is one parameter to be determined in the Elman network, which is the number of input layers. Here, the number is varied between 1 and 10. To train the Elman network, the RSNNS library in R is used, which is based on the Stuttgart Neural Network Simulator (Bergmeir and Benítez, 2012).

@&#CONCLUSIONS@&#
