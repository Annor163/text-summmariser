@&#MAIN-TITLE@&#
Consensus of multiple correspondences between sets of elements

@&#HIGHLIGHTS@&#
A method to deduct a consensus correspondence given some correspondences.We explain in detail 3 methods: Voting, Iterative and Agglomerative.A complete evaluation on known datasets.

@&#KEYPHRASES@&#
Consensus,Points’ correspondence,Feature extraction,Linear solver,Hamming distance,Image registration,

@&#ABSTRACT@&#
In many pattern recognition and computer vision problems, it is often necessary to compare multiple sets of elements that are completely or partially overlapping and possibly corrupted by noise. Finding a correspondence between elements from the different sets is one of the crucial tasks that several computer vision, robotics or image registration methods have to cope with. The aim of this paper is to find a consensus correspondence between two sets of points, given several initial correspondences between these two sets. We present three different methods: iterative, voting and agglomerative. If the noise randomly affects the original data, we suppose that, while using the deducted correspondence, the process obtains better results than each individual correspondence. The different correspondences between two sets of points are obtained through different feature extractors or matching algorithms. Experimental validation shows the runtime and accuracy for the three methodologies. The agglomerative method obtains the highest accuracy compared to the other consensus methods and also the individual ones, while obtaining an acceptable runtime.

@&#INTRODUCTION@&#
Suppose we have several correspondences between sets and there is some level of intersection between them. If we want to establish a consensus correspondence between the whole correspondences, we face two main problems. First, there are discrepancies between the element mappings. If our scenario is based on an automatic method, these differences are gauged by the features or the weights on these features. Contrarily, if the scenario is based on a human-machine interaction (for instance semi-automatic medical or forensic recognition), the strategy is based on the experience of the specialist. If such elements represent regions of segmented images, some subjects may think that the area is more important than the colour, but other specialists may think differently. Second, the intersection between sets is not null although some elements belong to only one or few sets.Several applications can benefit from our proposed solution to finding consensus correspondences. For instance, suppose some human specialists manually extracted the minutiae of a pair of fingerprints and deducted the correspondence between these minutiae. Some discrepancies can appear in these correspondences due to different localizations of the minutiae and also different mappings between minutiae. Our method can help present a final minutiae correspondence and therefore a final distance between these fingerprints.Our method could also be applied to pattern recognition problems in medical images. In this case, the localization of some parts in an image and the correspondence between these local parts between images is based on the experience of the specialist. Again, our method could be used to deduct a final correspondence between local parts of two images. Finally, we could also use our method in an automatic framework. Suppose we want to solve the automatic image registration problem. In this case, local descriptors can be extracted from images and also different matching algorithms can be used to find correspondence between images. In this case, we could use our method to deduct a consensus correspondence with the aim of increasing the quality of image registration. In the experimental section, we have applied our method to automatic image registration.Imageregistration is the process of transforming different sets of data into one coordinate system. Data may be multiple pictures, multiple views or data from different sensors or times. It is used in computer vision, medical imaging, analysing images in general and data from satellites. Registration is necessary in order to be able to compare or integrate the data obtained from these different measurements. Interesting image registration surveys are [1] and [2], which explain the problematic of this goal. Given two images to be aligned, the image registration process is usually composed of three main steps [3]. First, a set of salient points is extracted from each of the two images. Second, a correspondence between the extracted points is deducted. Third, an alignment, for instance a homography, is deducted with the initial correspondence. In this process, it is usual to deduct a final correspondence adapted to the homography.In the first step, several methods have appeared to select salient points in images [3], for example SIFT [4], Harris corners [5] or SURF [6]. These methods are based on assigning some local features (for instance, a vector of 128 features) to each extracted point or pixel of the image. Each local feature usually depends on the information on the image given a radius and an angle. The second step is based on finding a correspondence between the extracted salient points. In the second step, matching algorithms have been used with outlier rejection. That is, explicitly considering some points can be generated due to noise and so, they do not have to be mapped with elements of the other set. For instance, Bipartite (BP) [7], or a new version called Fast Bipartite (FBP) [8,9], is one of the algorithms used to find a correspondence between points or between graphs if the second order relations between points are considered. This algorithm obtains the point correspondences but it does not deduct the homography and it uses the features located at each point (for instance SIFTs or SURFs) or the second order features located at the relations between points. It is based on reducing the problem into a linear assignation problem and applying a linear solver such as the Hungarian method [10]. In the third step, the homography is extracted that transforms the coordinate system of one of the images to the other one given an initial correspondence. Iterative Closest Point (ICP) [11] is an algorithm employed to minimize the difference between two clouds of points. ICP is often used to reconstruct 2D or 3D surfaces from different scans. It only uses the position of the points but not the local features and an initial correspondence. It is usual to use ICP together with RANSAC [12], which is a method to discard points that do not fit on the deducted homography and correspondences and so eliminates the spurious correspondences. That is, points to be considered that have appeared due to noise in the images or sensors. Other more sophisticated algorithms have appeared that consider the features of each point and also the homographies such as [13], which is based on the Expectation Maximisation algorithm. In [14], they propose a method to deduct the vector field given two images and also the best correspondence between salient points. Finally, the Hough transform [15–17] is a technique used to find imperfect instances of objects represented by sub-sets of salient points within an image by a voting procedure. This voting procedure is carried out in a parameter space, from which object candidates are obtained as local maxima in a so-called accumulator space that is explicitly constructed by the algorithm for computing the Hough transform.Fig. 1 shows two images in which four different sets of points and correspondences have been found. The salient points extractors and matching algorithms are: 1.(a) SIFT extractor [4] and Hungarian method [10]. 1.(b) SURF extractor [6] and Hungarian method [10]. 1.(c) Harris corners [5] and matchFeatures function from Matlab [18]. 1.(d) SIFT extractor [4] and PF-Registration [19,20].Several mapping combinations have been formed and all of them containing mistaken mappings. Nevertheless, due to the noisy nature of the errors, mistaken mappings tend to be non-repetitive. For this reason, if a consensus correspondence is defined, the final correspondence tends to have less mapping errors than the original ones. Moreover, the final sets of points are the union of the points in all the sets. Therefore, the consensus correspondence has the advantage of being composed by a larger set of point correspondences. With a larger set of correct mappings, the image registration process (for instance ICP + RANSAC) tends to obtain a better image alignment. Fig. 2 schematically shows the consensus method. In this case, we suppose there are three different correspondences fa, fband fcwith their pairs of sets. The intersection of sets is not null. Our method deducts the sets A and A′ and the consensus correspondence f.A method to deduct a correspondence consensus given only two correspondences was presented in [21] and [22] for sets and in [23] for graphs. In the current paper, we present a method to deduct a correspondence consensus but given several correspondences. The fact of increasing the number of correspondences involved in the process not only derives in an increase of the computational demand but also an increase of the complexity of the problem at hand. For this reason, we present three different alternatives. The first one is based on a voting process [24]. In this case, each vote is an element-to-element mapping. The second one is an incremental method. The algorithm sequentially executes the two-correspondence method [21] and [22]. Finally, the third one is an agglomerative method, which obtains the consensus through a minimisation function inspired in the Bipartite matching algorithm [7]. Note that all of these algorithms obtain a consensus correspondence in a sub-optimal way. This is because the computational cost of an optimal algorithm is exponential respect the number of sets and the order of the sets and so they are too computationally demanding in a real application.To the best of our knowledge, we are the first ones that tackle the problem of finding a consensus correspondence given a set of correspondences. We can mention method [25] to deduct a consensus distance given several distances obtained from the same two images but using different features. Although they applied the method for fingerprint matching, authors comment it can be easily extended to other type of images and features. The most important difference of this method and ours is that the inputs are some initial global distances and not some initial correspondences.In the experimental section, we apply our method to increase the quality of correspondences between salient points extracted from images. Nevertheless, the method we present does not have to be seen as an image registration method but a method to increase the quality of the correspondences given some sets of elements and some initial correspondences between them. Since the used databases are composed of images and the homographies between them, we can easily deduct the correct position of the salient points in the transformed image. Thus, we can validate the quality of the correspondences through the end point error. Moreover, a general image registration process can use our method since we assume that when the correspondences’ quality increases, so it does the image registration quality.The rest of the paper is as follows. In Section 2, we explain the basic definitions. In Section 3, we briefly explain the two-correspondence method [21] and [22]. In Section 4, we explain three methods to solve the multiple consensus problem: iterative, voting and agglomerative. In Section 5, we present the results derived from the experimental validation. Finally, in Section 6, we conclude the paper.In this section, we define four concepts and methods. The first three ones are: (1) the mean of a set of elements given any domain of the elements, (2) the distance between two sets considering outlier rejection and (3) the Hamming distance between correspondences. Finally, considering these three definitions, we define the mean correspondence given a set of correspondences.Suppose we have a set of elementsA={a1,…,an,}on the domainai∈T. The meana¯∈Tof the elements in A is defined as,(1)a¯=argmin∀a∈T{∑i=1ndist(a,ai)}being dist any distance measure defined on the domainTof these elements. This function can be minimised using optimal or sub-optimal minimisation methods depending on several features, such as the definition of the distance function or the dimension of the problem.Given two sets of elements and a correspondence between them, we say that the outlier elements on both sets are the elements that are not mapped by the correspondence and the inlier elements are the ones that are mapped. Since both sets can have different cardinality, the number of outliers and inliers in both sets can be different. To formalise this situation, it is usual to consider some extra elements in both sets, which are usually called null elements. Thus, the elements in the set have to be considered outliers if are mapped to null elements in the codomain set. In the same way, the elements in the codomain set have to be considered outliers if their argument value is a null element. From now on, we consider that given two sets and a correspondence between them, both sets have the same order and the correspondence is bijective.More formally, we have two sets of elementsA={a1,…,an,an+1,…,an+m}andA′={a′1,…,a′m,am+1′,…,an+m′}with ordern+m. The first n elements in A are original elements and the m rest of the elements are null elements. The attribute of null elements is not inTand it is represented by symbol ɛ. Then,an+1∈ɛ,…,an+m∈ɛ. Similarly holds for the first m elements in A′ and the n rest of elements in A′. Therefore,am+1∈ɛ,…,an+m∈ɛ.Moreover, there is a bijective correspondencef(ai)=aj′that maps elements of both sets. We define the cost of this correspondence Cost(A, A′, f) as the addition of individual element costs in a similar way as in the Graph Edit Distance [26],(2)Cost(A,A′,f)=∑i=1n+mc(ai,aj′)wheref(ai)=aj′and c is defined as a distance function over the domain of attributesT∪{ɛ}. Distance c is application dependent and it has to cope with the case that two original elements are mapped (both in domainT) and the case that one of them is a null element (one of them in domainTand the other has value ɛ). If both mapped elements are null elements thenc(ɛ,ɛ)=0.The distance between sets dS(A, A′), which delivers the minimum cost of all the correspondences, is defined as(3)dS(A,A′)=min∀f:AxA′{Cost(A,A′,f)}The correspondence that obtains this distance is known as the optimal correspondence f*, and it is defined as(4)f*=argmin∀f:AxA′{Cost(A,A′,f)}Bipartite algorithm [7,8] is currently the most used method to solve the error-tolerant graph-matching problem. Although our framework is centred on correspondences between sets instead of between graphs, our approach to solve the consensus correspondence is based on this algorithm due to its flexibility to cope with null elements. The algorithm converts this linear minimisation problem into an assignment problem [10] in which any correspondence f is related with a combination. They define matrix F such thatF[i,j]=1iff(ai)=aj′andF[i,j]=0otherwise. With the calculation of a cost matrixC[i,j]=c(ai,aj′), they convert Eq. 4 into(5)f*=argmin∀f:AxA′{C∘F}where ○ represents the Hadamard product. Then the cost of the correspondence can be obtained through,(6)Cost(A,A′,f)=∑i,j=1n+m(C∘F)[i,j]The Bipartite algorithm is composed of two main steps. The first step defines the(n+m)×(n+m)cost matrixCand the second step applies a linear solver such as the Hungarian method [10] or the Jonker-Volgenant method [27] to this matrix and obtains matrix F. Fig. 3shows the cost matrixCof Bipartite algorithm.Quadrant Q1 denotes the combination of substituting costs Ci, jbetween non-null elements. The diagonal of quadrant Q2 denotes the costs Ci, ɛ of mapping non-null elements to null elements. Similarly, the diagonal of quadrant Q3 denotes the costs Cɛ, jof mapping null elements to non-null elements. Q4 quadrant is filled with zero values since the substitution between null elements has a zero cost. Recently, other matrices have been defined [8,9,27], with the aim of reducing the computational cost.The Hamming distance between two correspondences is the number of element mappings that do not have the same value. Assume fkand ftare two bijective correspondences between setsA={a1,…,an,an+1,…,an+m}andA′={a′1,…,a′m,am+1′,…,an+m′}. We define the Hamming distance  dH(fk, ft) between the correspondences fkand ftas(7)dH(fk,ft)=∑i=1n+m(1−∂(a′x,a′y))being x and y such thatfk(ai)=ax′andft(ai)=ay′. Function ∂ is the well-known Kronecker Delta.(8)∂(a,b)={1ifa=b0ifa≠bSuppose we have two sets A and A′ and also N bijective correspondences between these setsf1,…,fN, similarly to Eq. 1, the mean correspondencef¯between sets A and A′ is defined as,(9)f¯=argmin∀f∈AxA′{∑k=1NdH(f,fk)}where dHis the Hamming distance defined in Eq. 7. If we try to minimise this function using sub-optimal methods, we encounter that the Hamming distance takes discrete values and so, the derivative function is not defined in the whole domain. This property denies the use of classical optimisation methods based on the gradient [29]. One choice would be a brute force method that obtains all possible combinations and selects the correspondence that minimises the summation. Nevertheless, the number of combinations is so large that it could not be solved using a personal computer in most of the usual applications.We propose a standard minimisation approach that aims to find an optimal element e* that globally minimises a specific function. Usually, this function is composed of an empirical risk ∇(e) plus a regularization term Ω(e) weighted by a parameter λ [28]. The empirical risk is the function to be minimised per se, and the regularisation term is a mathematical mechanism to impose some restrictions. Parameter λ weights how much these restrictions have to be imposed.(10)e*=argmin∀e{λ·∇(e)+(1−λ)·Ω(e)}We present a method to find an approximation of the mean correspondence given a set of correspondences between two sets. Therefore, we want to findf¯*such that the following equation holds,(11)f¯*=argmin∀f:AxA′{λ·∇(f)+(1−λ)·Ω(f)}We supposef¯*approximates the mean correspondencef¯and it has some specific features. From now on, we refer to it as “consensus correspondence” instead of “mean correspondence”. This is because we cannot talk anymore about a mean due to the sub-optimality of the method, and also because of the definition of the regularisation term. Nevertheless, we want to approximate the consensus to the mean since we assume the noise randomly appears and it has a non-repetitive behaviour. Thus, the mean of the correspondences tends to reduce the impact of this noise.In the next two sections, we first summarise a method presented in [21] and [22] to compute a consensus correspondence given only two correspondences and then we present three alternatives to compute the consensus correspondence given several correspondences. The first and simplest one is based on a voting method. The second one is an iterative algorithm that updates the consensus correspondence given several rounds of the algorithm that obtains a consensus given two correspondences. The third one is an algorithm that obtains the consensus through an optimisation process given the whole set of correspondences at a time.Assumefk: AkxA′kand ft: AtxA′tare two correspondence functions between the two output setsAk={a1k,a2k,…,ankk}andAt={a1t,a2t,…,antt}and the two input setsA′k={a1k′,a2k′,…,ank′k}andA′t={a1t′,a2t′,…,ant′t}. The order of Akand A′kis nk, and the order ofAtand A′tis nt, since the correspondencesfkandftare defined to be bijective. Note that we assume there is some level of intersection between both input sets, and also both output sets, thereforeAk∩AtandA′k∩A′tare non null sets, although it is not strictly necessary. Also, it may happen that nk≠ nt. The problem at hand is to define a consensus correspondencef¯:AxA′given fk: AkxA′kand ft: AtxA′t. The domain A is defined asA=Ak∪Atand the codomain setis defined asA′=A′k∪A′t. The order of A and A′ is n.Fig. 4 shows an example of this method. In Fig. 4a, one extractor algorithm has selected the squared points (Akand A′k) and another extractor algorithm has selected circular points (Atand A′t). After that, a point set registration algorithm has been applied to the square points, and another point set registration algorithm (or the same one) has been applied to the circle points (Fig. 4b). These algorithms have found the blue correspondences for the square points (fk) and the red correspondences for the circle points (ft). Some new elements have appeared, which are marked as null (∅), to allocate the original elements that the point set registration algorithm considers they are outliers. Between the blue and red correspondences there are some discrepancies not only in the selected points, but also in the elements mappings. Moreover,nk=3andnt=4. In Fig. 4c, we show the obtained consensusf¯. Notice A has one extended point and A′ has two extended points and so,n=6.The method published in [21] and [22] obtains a consensus correspondencef¯*through a minimisation process composed of two terms as in Eq. 11: 1) the Loss function, which is the sum of the Hamming distances between the current correspondence and the original correspondences and 2) the regularisation term, which is the cost of the current correspondence (Eq. 2).(12)f¯*=argmin∀f:AxA′{λ·(dH(fk,f)+dH(ft,f))+(1−λ)·Cost(A,A′,f)}The algorithm deducts correspondencef¯*in a sub-optimal way in three steps. First, the element mappings that both correspondences coincide are similarly assigned onf¯*. Second, a matrix H is computed with the remaining elements and third, a linear assignation method is applied to a matrix H, which is defined as,(13)H=[λ·(1−(Fk+Ft))+(1−λ)·(Ck+Ct)]where 1 represents an all ones matrix. Fkand Ftare the correspondence matrices and Ckand Ctare the cost matrices (Section 2.2). Fig. 5 shows the correspondence matrices Fkand Ft. The first set of columns and rows are the ones that elements belong to both sets but mappings are different. The other set of columns and rows are elements that only belong to one of the sets. In this algorithm, these matrices are squared and their dimension is n minus the number of mappings that are similar. By construction, ifFk[i,j]=1, thenFt[i,j]=0and ifFt[i,j]=1, thenFk[i,j]=0. For this reason, the whole cells in the resulting matrix(1−(Fk+Ft))defined in Eq. 13 have a value of 0 or 1.Fig. 6shows the cost matrices Ckand Ct. Similarly to the correspondence matrices, rows and columns are clustered depending if its elements belong to both sets, or only to one of them. In the cases that a row represents an element that belongs to the set and the column represents an element that does not belong to the set, then a specific cost is considered Cɛ, j. On the opposite combination, the cost is Ci, ɛ. See [8] for more details.Fig. 7shows two correspondences and the resulting correspondence matricesFkandFtand cost matricesCkandCt. Blue lines represent correspondencefkand red lines represent correspondenceft.The iterative method we propose in Section 4.1 uses this solution, which can be described with following function:(f¯*,A,A′)=PairConsensus(λ,ft,At,A′t,fk,Ak,A′k).Suppose we have two sets{A1,…,AN}and{A′1,…,A′N}composed ofAk={a1k,a2k,…,ankk}andA′k={a1k′,a2k′,…,ank′k}. Moreover, there is a set of bijective correspondences{f1,…,fN}where fk: AkxA′k. The paired sets Akand A′khave the same order nkbut, in general, non-paired sets have different order. In the next three sub-sections, we describe three methods (voting, iterative and agglomerative) to obtain a consensus correspondence.Clustering or classification methods based on a voting process [24] select as a final result the one that most of the contributions decided to be the best one. Nevertheless, in some frameworks, different parties only partially contribute on the final result. In these cases, a minimisation process is needed to arrive at the solution. In example of Fig. 1, both feature extractors obtain some different points and so both correspondences do not contribute on a complete solution, since they do not relate the points that only appear on the other extractor. When several feature extractors are considered, the number of discrepancies increases and so, the initial correspondences tend to be less complete. Algorithm 1 shows how to obtain the consensus correspondence based on a voting process. We suppose the initial sets have been extended to include the whole different elements. The algorithm has two main steps. In the first one, the correspondence matricesF1,…,FNare defined whereFk[i,j]=1iffk(aik)=ajk′andFk[i,j]=0otherwise. In the second step, the final consensus correspondencef¯*is defined to be a bijection such that minimises expression(15)∑i,j=1n+m((N−∑k=1NFk)∘F)[i,j]whereF[i,j]=1iff¯*(ai)=aj′andF[i,j]=0otherwise. Moreover ∘ represents the Hadamard product and N is a matrix with all cells with the value N. Since this expression is linear, it can be minimised by a linear solver such as [10] or [27] represented by function LinearSolver in Algorithm 1. Note, the number of votes of an specific element mapping, for instanceaik→ajk′, is∑k=1NFk[i,j]. Therefore, searching for the correspondence that minimisesN−∑k=1NFkis congruent to taking into consideration the voting method.Algorithm 2 computes the consensus correspondence in an iterative manner by executing N-1 times the method published in [21] and [22] summarised in Section 3, which we called PairConsensus. Note function PairConsensus returns the domain and codomain sets together with the current correspondences. This is because these sets can be extended to assure the new consensus correspondence to be bijective, and outliers are considered. Clearly, the order in which correspondences are chosen affects on the final result, as it is usual in the learning iterative methods.The main function to be minimised in this case is(16)f¯*=argmin∀f:A^xA^′{λN·∑k=1NdH(f^k,f)+(1−λ)·Cost(A^,A^′,f)}It is composed of a Loss function∑k=1NdH(f^k,f)that has the aim of moving the solution to the mean correspondence, and also a Regularisation termCost(A^,A^′,f)that has the aim of reducing the cost of the consensus. Given different solutions, the best one is the correspondence that has the minimum cost. This function is inspired in the one presented for the 2 correspondence case (Eq. 12) but in this case, sets A and A′ have been enlarged with null elements to tackle the problem of discrepancies and allowing the whole possible combinations of correspondences. Thus, setsA^andA^′represent the union ofA1,…,ANandA′1,…,A′Nplus some extra null elements. For this reason the original correspondencesf1,…,fNhave also been enlarged with correspondences between null elements, and they are represented asf^1,…,f^N.Note this function is clearly non-continuous since a simple swap of a pair of node mappings would cause an abrupt change on the Hamming distance and also on the correspondence cost. This is the reason why the application of methods such as [29] would not converge. Moreover, these methods do not guarantee the correspondence to be bijective while minimising this function. Again, we decided to solve this optimisation problem through the Bipartite framework [7–9]. First of all, we need to define the enlarged correspondence matrix Fkand the enlarged cost matrix Ck(Fig. 8). Both matrices are composed of four quadrants. The left upper quadrant represents the set of combinations between elements inA^that also belong to Akand elements inA^′that also belong to A′k. This quadrant in Fkand Ckis defined in a similar way as the whole matrices Fkand Ckin the 2 correspondences case (Section 3). The second quadrant represents the combinations between elements inA^that also are in Akand elements inA^′that are not in A′k. Similarly, the third quadrant represents the combinations between elements inA^that are not in Akand elements inA^′that also are in A′k. These two last quadrants are useful to allow elements to be considered outliers. The fourth quadrant is composed of correspondences between null elements.Considering matrix Fk, in the first quadrant, cells are 0 or 1 valued. If an element belongs to the original set Ak, then the sum of the column or row in Fkthat represents this element is 1. Otherwise, the whole column or row in Fkis 0. The whole cells in the rest of quadrants are 0 since there are not any mappings between these elements. Considering matrix Ck, in the first quadrant, there are four different types of cells. The ones that both elements belong to Akand A′k, then the costCi,jkof mapping these elements are considered. If the elements do not belong to the sets Akor A′k, then the cost of assigning it to a null element is considered,Cɛ,jkorCi,ɛk. Finally, the ones where both elements do not belong to Akor A′khave a cost of 0. The other quadrants have been defined as the original Bipartite algorithm (Fig. 3). Nevertheless, note there are some cells in the diagonals of the second and third quadrant that have a 0 value. These are the cases where the element does not belong to Ak(in the second quadrant) or does not belong to A′k(in the third quadrant).Similarly to Eq. 12, the consensus correspondence is achieved through the following expression(17)f¯*=argmin∀f:A^xA^′{[λ·(1−1N·∑k=1NFk)+(1−λ)·1N·∑k=1NCk]∘F}Algorithm 3 computes the agglomerative consensus. First, matricesF1,…,FNandC1,…,CNare computed such that all of them have the same number of columns and rows (Fig. 8). Function CostMatrix computes matrices Ckas commented above. Then a linear solver such as [10] or [27] is applied on the resulting matrix H.Parting from the example proposed in Fig. 4, we propose the inclusion of a third party which extracts an additional set of points (green triangles) and enounces its own correspondence (green lines) as shown in Fig. 9. We intend to explain the global consensus algorithm proposed in the previous section by using this example.The first quadrant of matrices Fa, Fband Fcare set as follows (the other quadrants are 0 valued), Fig. 10.The cost matrices Ca, Cband Ccare constituted as shown in the next figure. Note the cost of assigning an element to a null is constant and represented byCɛ,jkorCi,ɛk. Fig. 11.Fig. 12 shows a final consensus. A new element has appeared between the subject's legs of the right image. This means that the linear solver has selected one of the cells on the diagonal of the second quadrant. Note this result is dependent not only on the original data and the considered distance between elements, but also on the weight λ and the cost of mapping an element to a null elementCi,ɛkor vice versa,Cɛ,jk.The experimental validation has been performed using a database comprised of 5 sequences called “BOAT”, “EAST_PARK”, “EAST_SOUTH”, “RESIDENCE” and “ENSIMAG” [30]. These sequences are composed of 11 pictures taken from the same object, but from different points of views and scales. We extracted from each picture the 75 most reliable salient points and their features using 5 methodologies: FAST [31], HARRIS [5], MINEIGEN [32], SURF [6] (Matlab native functions with default parameters) and SIFT [4] (the function and the parameters are available in [33]). Then, we matched the first image of the sequence to the 10 remaining images in its sequence using the Matlab function MatchFeatures (MF) [18] (with a Maximum Ratio = 1) and the Bipartite Graph Matching method (BP) [7] (with an Outlier_Cost = 0.2 using normalised costs). Both the Maximum Ratio and the Outlier_Cost were set to obtain the most possible number of pairings for each matching algorithm, however, if a duplicated or non-bijective mapping was found, it was discarded. Thus, we generated all combination of correspondencesfalg,featuresequence,k, wheresequence={BOAT,   EASTPARK,   EASTSOUTH, RESIDENCE,   ENSIMAG},k=1..10,alg={MF,BP}andfeature={FAST,HARRIS,MINEIGEN,SURF,SIFT}. The original database in [30] provides the homographies that convert the first images of the five sequences to the rest of images in the sequences. Using this homography, we generated a ground truth correspondence cifor every pair of images. To summarise, our experimental scenario has a total of 5 sequences × 100 pairings × 2 algorithms × 5 feature extractors = 5000 quartets. Each quartet is composed of the two sets, the deducted correspondence and the ground truth correspondence:{Afeaturesequence,k,Afeature′sequence,k,falg,featuresequence,k,cfeaturesequence,k}. The database is public in [34]. Fig. 13 shows an example of this database. Thanks to the ground truth correspondence, we can deduct which element mappings are correct and which ones are not.As commented in the introduction, inliers are elements that the correspondence function maps to elements of the other set. In the image registration field, inliers are salient points from one of the images that are paired to salient points of the other image. In contrast, outliers are salient points that the method has considered that do not appear on the other set. One of the methods to validate the quality of the image registration process is the end point error [35], which is defined as the addition, through the whole inliers, of the Euclidean distance between the pixels position of the arriving point achieved through the ground truth correspondence and the obtained correspondence.For these experiments, we opted to run the Agglomerative Method and the Iterative Method for the entire dataset a total of 54 times, considering 3 subsets s of salient points detected in each image (s = 25,50,75) × 2 configurations of the OC (Cɛ,j=Ci,ɛ=0.1andCɛ,j=Ci,ɛ=0.5) × 9 configurations of the gauging parameter (λ = 0, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 0.66, 1). For the case of the Voting Method, given the fact that neither the OC nor λ exist, only 3 runs of the algorithm (for each the aforementioned values of s) are presented. Additionally, we present the 10 results obtained using the initial methods (5 computed through MF using MaxRatio = 1 and 5 computed through BP using O.C. = 0.2) for the three values of s. Finally, it must be noted that the highest values for each row will be marked in bold to identify which extractor-matcher combination (in the case of the individual methods) or parameter configuration (in the case of the consensus methods) obtains the best results.We have performed three different sets of experiments. In the first ones, we show the number of detected inliers (Tables 1and 2). In the second ones, we show the number of correct inliers (Tables 3 and 4) and also the average cost of mapping the inliers (Tables 5 and 6). This set of experiments is useful to validate that the optimisation algorithm really tends to minimise the cost function. That is, from the obtained inliers, how similar the generated correspondences are to the ground truth correspondence (the average number of green lines in Fig. 13). Finally, in the third experiments, we analyse the end point error obtained through the consensus correspondence (Tables 7 and 8). This last experiment shows our method from the application point of view, since the aim of image registration is to find the correspondence between points such that the end point error is minimised. Together with these experiments, we show the runtime spent to match two sets of sets (Table 9) and to find a consensus correspondence of the three consensus methods (Table 10). In [34], the Matlab code is available. The consensus correspondence obtained in the Iterative method depends on the order of presentation of the data. The order of presentation is the same order of the data in ourdatabase [33].Table 1 shows the average number of inliers detected by the five feature extractors and given the two commented initial matching algorithms. We present these results to have a baseline of the inliers that each method is capable to detect. To begin, it is noticed that BP based algorithms tend to obtain more inliers than those using MF, but clearly, this fact depends on the parameterisation of these two algorithms.Table 2 showsthe number of inliers obtained by the consensus methods. For the case of the Agglomerative Method, when OC = 0.1, the highest number of detected inliers was obtained using the highest value of λ. Contrarily, when OC = 0.5, we obtained the same number of detected inliers on any configuration of λ except for the highest value. This is caused by the nature of the linear solver used for the consensus, which given a high OC value tends to detect more inliers and vice versa. Separately, the average of the Individual Methods, the Iterative Method and the Voting Method obtained fewer inliers than the best configuration of the Agglomerative Method. Notice that as the value of s is modified, we practically obtain a linear increase in terms of number of detected inliers either in the individual methods or in the consensus methods.So far, it is observed that using the Agglomerative Method with a high Outlier Cost is the best form to obtain a high density of detected inliers, regardless of the value set for λ. Nevertheless, obtaining more or less inliers is not necessarily a good metric per se, since our aim is to increase the number of correct inliers, that is, the ones similar to the ground truth correspondence. Tables 3 and 4 show the number of correct inliers found with each method. Additionally, as previously found in [22], the HARRIS and SURF extractors present the best affinity in terms of detection of correct inliers with both the MF and BP matching algorithms.Notice that if we compare these results with the ones on Table 1, we not see a high rate of correct inliers with respect to the detected ones. This is what makes these datasets interesting, since we want to show the ability of the consensus methods to deduct a bigger amount of correct inliers even if the input correspondences have a low quality.Table 4 shows the number of correct inliers for the three consensus methods. Once again, the Agglomerative Method outperforms the other ones. Also, selecting a value of 0.1 ≤ λ ≤ 0.2 guarantees for the Agglomerative Method and the Iterative Method to find the best result, effectively showing that an optimal consensus is obtained when the λ parameter is properly set so that both terms contribute. In the case of the Agglomerative Method, gauging the OC parameter did not represent much of a difference compared to the case of the values shown in Table 2. Therefore, selecting for instanceOC=0.1guarantees a considerably high number of correct inliers and a reduction of spurious inliers. In this experiment, using different values of s does not deliver a linear increase in terms of number of correct inliers detected; in fact, a small improvement is obtained as the number of used points increased from 50 to 75.Table 5shows the average cost generated through mapping the inliers normalised by the number of correct inliers detected. Except for the SIFT (higher) and SURF (lower) cases, average costs are almost similar.Table 6 shows the same metric applied to the consensus correspondences (lower values are put in bold). In the Agglomerative case, the best combinations of explored parameters isλ=0.2andOC=0.1. This is the same approximate value for parameter λ in which the Agglomerative Method (Table 4) has the highest accuracy. Also, notice that this achieved cost is clearly lower than the average cost obtained by the Individual Methods costs (Table 5). Although the Iterative Method does not perform as good as the Agglomerative Method, it is still capable of minimising the cost of the consensus correspondence in high values of λ. Nevertheless in these cases, as shown in Table 4, accuracy is also reduced. Finally, the Voting method obtains a slightly larger cost than the average value (last column in Table 5), and only performs better than SIFTs and, in some cases, than the Iterative Method. Adding the fact that the cost increase is not significant as s is increased, these results validate the supposition that through the consensus correspondence, the impact of the noise is reduced and so the quality of the mappings increases.Theend point error is useful to show how a method performs when applied to image registration. A low value of the end point error implies that the obtained correspondence is close to the ground truth. Table 7 shows the end point error normalised by the correctly detected inliers. The MF algorithm obtains almost twice the end point error with respect to the BP algorithm. Once again, the SURF method is the one that most reduces the end point error.Table 8 shows the normalised end point error of the consensus methods (lower values are put in bold). The behaviour of this metric is quite constant among several configurations of λ and OC. Nevertheless, whens=25in the Agglomerative Method, the minimum value is lower than the minimum of the Individual Methods (SURF-BP) in Table 7 and it is 16 times better than the average value (last column in Table 7). Using the Iterative method, we notice that between s = 50 and s = 75 the values increase in some cases.  However, when comparing all of these values to the ones obtained by the Agglomerative method, we observe that the Iterative method obtains poorer end point error values (approximately 10 times larger than the ones obtained by the Agglomerative method). Meanwhile, the Voting Method behaves slightly worse than the Agglomerative method. The most important observation for this metric is the fact that except for the Voting Method, when the number of salient points involved increases, the normalised end point error is reduced. This indicates that when the number of salient points used is increased, the obtained consensus correspondence becomes closer to the optimal one.Finally, Tables 9 and 10 show the runtime results. Table 9 shows the average runtime in seconds spent to compare two sets of elements that represent images in the datasets. The time to extract the salient points is not considered. Tests were performed using a PC with Intel 3.4 GHz CPU and Windows 7 operating system. Notice MF algorithm is always faster than BP regardless the extractor, and as the number of salient points is increased, this difference increases as well. However, as s increases BP has a polynomial increase while MF is lineal. This is evidently due to the nature of both methods, with MF being a linear comparer and the BP algorithm having O(n3) complexity.Table 10 shows theaverage runtime in second spent to obtain a consensus correspondence. In the five datasets, the faster method is Voting, followed by Agglomerative and finally, the Iterative. In the case of the Iterative Method, this runtime corresponds only to one permutation of data input.In conclusion, the Agglomerative Method obtains the highest number of correct inliers, the lowest average normalised cost and the lowest end point error, although it is slightly slower than the Voting method. For this reason, if we have the data all at once, we conclude it is the best method of the three ones explored. In the case that the data is not available all at once, the Iterative method could be used as an online learning process.In the previous experimental section we intended to validate our consensus methodologies in the specific case when the number of inputs is equal to 10, however, it is also of special interest to analyse if by including less input individual correspondences, the methods will have a significant decrease of success in terms of the aforementioned values. Thus, for this experiment we ran the three consensus methodologies using separatelyn=2,3,…,9,10input individual correspondences. We decided to prearrange an order of the individual methodologies according to their accuracy obtained from Table 3, in order to always use the strongest correspondences first. For the Agglomerative and Iterative Methods, we set the consensus algorithms parameters toλ=0.1andOC=0.5, which are the values that delivered the best results on Section 5.2. The number of salient points used was set tos=50. To further test our method in more images, we include 8 additional dataset repositories from [36][37], consisting in 7 sequences called “BARK”, “BIKES”, “GRAF”, “LEUVEN”, “TREES”, “UBC” and “WALL”, also including their respective homographies which converts the first image (img1) into the other 5 (img2 to img6) in a similar way as the ones of the “Tarragona Exteriors” dataset. Therefore, the results reported in this experimental section whenn=10may vary from the ones reported in Section 5.2.We call this a trade-off validation, since our aim is to show if it the fact of including more or less correspondences alters the output.Figs. 14–18show the five previously analysed variables as the number of included initial correspondences varies. First, it can be noticed that the number of detected correspondences always increases as the number of initial correspondences does. This is a natural output of any consensus methodology which, by including more initial inputs, generates more correspondences. Nevertheless, our real interest resides in Fig. 15, where it is clearly noticeable that the Iterative Method fails to increase the rate of correct inliers when n > 2. This leads to conclude that the Iterative Method is not an effective option, since the fact of adding more initial correspondences reduces its efficiency. Separately, both the Agglomerative Method and the Voting Method increase steadily, but is the first one (as shown in Section 5.2) the one which obtains more correct inliers. Figs. 16 and 17 show the average normalised cost and the average end point error respectively, and it can be seen that is the Agglomerative Method the one who best minimises the cost. Finally, in Fig. 18 it can be seen that there is practically no difference between using the Agglomerative Method or the Voting Method in terms of computational demand, and within each method the difference is not significant. For instance, in the Agglomerative Method performing the consensus of two initial correspondences could take around 18 s, while using 10 initial correspondences takes 118 s to compute. This runtime difference is significant when taking into account that the number of correct correspondences duplicates from 128 (whenn=2) to 298 (whenn=10), as shown in Fig. 15. According to the system needs, it is essential to always prioritise either accuracy or runtime, and as shown in this validation, consensus methodologies are no exception.Most of the image registration methods are based on first extracting a set of salient points of both images to be aligned, then finding a correspondence between them and finally, adapting the homography that transform the coordinates of one of the images into the other. Clearly, there are several solutions in the literature to extract salient points and to match these points. The quality of these two processes is crucial to deduct a good homography. Instead of only using one of the methods to extract points or to match them, this paper proposes a method that globally considers the whole combinations. Since we assume the noise randomly affects the data in a non-repetitive way, the experimental validation shows the consensus correspondence tends to obtain a better homography (lower end point error) than the individual ones, and a reduced normalised cost. Three methods have been discussed; Agglomerative, Iterative and Voting. If the data is available all at once, the Agglomerative obtains the best correspondence. Contrarily, in the applications that not all the data is available at once, the Iterative method could be a good solution. Note there is a considerable impact in runtime if the second scenario is considered. It is important to keep in mind that consensus methods are always slower than obtaining the correspondence given only one feature extractor and one matching algorithm.As a future work, we propose to apply this method to palmprint forensics [19]. Given a latent (and sometimes a tiny patch) of a palmprint, it is usual that several experts manually decide the correspondences between minutiae to arrive at an agreement if both palmprints belong to the same subject. We propose the initial correspondences be the ones manually deducted by the specialist, and then find the consensus correspondence in an automatic manner.

@&#CONCLUSIONS@&#
