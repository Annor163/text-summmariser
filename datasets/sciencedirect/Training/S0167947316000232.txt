@&#MAIN-TITLE@&#
On point estimation of the abnormality of a Mahalanobis index

@&#HIGHLIGHTS@&#
We seek an estimate of the population proportion with extreme Mahalanobis index.Nine point estimates are examined in an extensive simulation study.Maximum likelihood estimates have substantial bias.Methods based on polynomial approximations give low bias, but can be out-of-range.An adapted median estimator that always gives sensible estimates is proposed.

@&#KEYPHRASES@&#
Bernstein polynomials,Mahalanobis distance,Median estimator,Plug-in maximum likelihood,Quadrature approximation,Unbiased estimation,

@&#ABSTRACT@&#
Mahalanobis distance may be used as a measure of the disparity between an individual’s profile of scores and the average profile of a population of controls. The degree to which the individual’s profile is unusual can then be equated to the proportion of the population who would have a larger Mahalanobis distance than the individual. Several estimators of this proportion are examined. These include plug-in maximum likelihood estimators, medians, the posterior mean from a Bayesian probability matching prior, an estimator derived from a Taylor expansion, and two forms of polynomial approximation, one based on Bernstein polynomial and one on a quadrature method. Simulations show that some estimators, including the commonly-used plug-in maximum likelihood estimators, can have substantial bias for small or moderate sample sizes. The polynomial approximations yield estimators that have low bias, with the quadrature method marginally to be preferred over Bernstein polynomials. However, the polynomial estimators sometimes yield infeasible estimates that are outside the 0–1 range. While none of the estimators are perfectly unbiased, the median estimators match their definition; in simulations their estimates of the proportion have a median error close to zero. The standard median estimator can give unrealistically small estimates (including 0) and an adjustment is proposed that ensures estimates are always credible. This latter estimator has much to recommend it when unbiasedness is not of paramount importance, while the quadrature method is recommended when bias is the dominant issue.

@&#INTRODUCTION@&#
The Mahalanobis distance is frequently used in multivariate analysis as a statistical measure of distance between a vector of scores for a single case and the mean vector of the underlying population or a sample of data. It was developed by Mahalanobis (1936) as a distance measure that incorporates the correlation between different scores. See also DasGupta (1993). The Mahalanobis distance of a vectorx, of sayν1variables (scores), from a population meanμis defined as(1)Δ=(x−μ)′Σ−1(x−μ),whereΣis the population covariance matrix. The square of the Mahalanobis distance,Δ2, is sometimes referred to as the Mahalanobis index (Huberty and Olejnik, 2006, p. 271). If the population follows a multivariate normal distribution (MVN) andxis an observation from this same distribution, then the Mahalanobis index follows a central chi-square distribution onν1degrees of freedom. In this paper, interest focuses on estimatingP, the proportion of the population that gives a more unusual Mahalanobis index than(x∗−μ)′Σ−1(x∗−μ)wherex∗is a specified vector, under the assumption that the population distribution is a MVN distribution. That is(2)P=Pr{(x−μ)′Σ−1(x−μ)>(x∗−μ)′Σ−1(x∗−μ)},wherex∼MVN(μ,Σ). For example,x∗might be a patient’s profile from a set of medical tests, whenPwould be the proportion of the population with a profile that is more unusual than that of the patient.The corresponding Mahalanobis distance in a sample, of saynobservations, is defined as(3)D˜=(x−x̄)′S−1(x−x̄),wherex̄andSare the sample mean vector and sample covariance matrix, respectively. Under the assumption thatxand the sample data are from the same MVN distribution, the sample Mahalanobis index(D˜2)is proportional to a centralFdistribution withν1andν2≡n−ν1degrees of freedom. See, for example, Mardia et al. (1979).We were initially motivated by the need to estimate the abnormality of a single patient’s profile in neuropsychology. The problem arises, for example, when psychologists need to assess how a patient with some brain disorder or a head injury is different from the general population or some particular subpopulation. This assessment is usually based on the patient’s scores in a set of tests that measure different traits or abilities. The abnormality of the case’s profile of scores can then be expressed in terms of the Mahalanobis index between this profile and the mean of the normative population or normative sample. The degree of abnormality is measured by(4)P̂=Pr{(x−x̄)′S−1(x−x̄)>(x∗−x̄)′S−1(x∗−x̄)},wherex∗is the case’s profile and is treated as a fixed quantity.A Hotelling’sT2significance test for testing whether the case could belong to the normative population is proposed in Huizenga et al. (2007). Their test is based on the centralFdistribution to which the Hotelling’s test statistic is proportional. Crawford et al. (2016) give a confidence interval for the probability (P) of getting a more extreme profile than the case. The confidence interval is based on a non-centralFdistribution with a non-centrality parameter that is proportional to the case’s Mahalanobis index. The confidence intervals are correct, in that their coverage levels equal the nominal confidence level exactly. In contrast, thep-value from the Hotelling’sT2test provides an obvious point estimator ofP, but it is biased. Indeed, the problem of finding an unbiased estimator ofPhas not been resolved.Here we consider a number of obvious estimators ofPand propose some new, less obvious estimators. The bias and mean square error of all the estimators are compared in extensive simulations. No estimator is uniformly better than all alternatives, but a small selection of the estimators is clearly to be preferred. As well as bias and mean square error, other criteria and desirable qualities in an estimator are also considered. In this paper, no distributional assumptions are made about the source ofx∗, other than when testing whetherx∗could be the profile of a member of the normative population.The need to estimate the value ofPfor Mahalanobis distances does not only arise in psychology. In the literature, the commonly used estimates ofPare thep-value computed from the chi-square distribution of the sample Mahalanobis index, or thep-value from the centralFdistribution associated with Hotelling’sT2test. For example, in remote sensing image analysis, Foody (2006) was interested in measuring the closeness of an image pixel to a single class centroid. For that, he used the Mahalanobis distance and converted the calculated Mahalanobis distance, of a particular image pixel from a specified class centroid, to its associatedp-value from the chi-square distribution. He then interpreted thep-value as the probability of obtaining a Mahalanobis distance as extreme as that observed for a particular pixel with respect to a specified class, thus effectively equating thep-value toP.In environmental and health science, Liu and Weng (2012) used Mahalanobis distance in public health studies to enhance the resolution of satellite imagery. They conducted a spatial–temporal analysis of West Nile Virus outbreak in Los Angeles in 2007 using sensing variables and infective mosquito surveillance records. Mahalanobis distance was used to identify and map the risk areas where habitat was suitable for infective mosquitoes. Liu and Weng (2012) calculated the distance between a vector of environmental variables and the mean vector of environmental factors at the closest locations of mosquito infections. Locations with smaller values of Mahalanobis distances indicated a more favorable habitat for the mosquitoes and hence an area of higher risk. They assumed that Mahalanobis distance follows a chi-square distribution, from whichPwas calculated for each map pixel. Pixels withPbetween 0.6 and 0.9 (0.9 and 1.0) were considered moderate risk (high risk) areas and then a risk map was produced.In analytical chemistry, Shah and Gemperline (1990) were interested in analyzing the near-infrared reflectance spectra of raw materials. They used Mahalanobis distance as a classification technique for pattern recognition to classify new samples by comparing them to measurements of predetermined classes. Each sample was classified according to thep-value associated with its Mahalanobis distance from the class centroids. Shah and Gemperline (1990) needed to estimate thep-value for each new sample and used the chi-square distribution to estimate these probabilities. They considered samples with probability levels between (0–0.01), (0.01–0.05) or (0.05–1.0) to be nonmembers, outliers or members, respectively.A sample Mahalanobis distance has an exactFdistribution. Hence, unsurprisingly, theFdistribution has also been frequently used to quantify the rarity/commonness of a Mahalanobis distance. For example, Lu et al. (2005) used the two groups Hotelling’sT2test for detecting differential expressions in genetic microarrays. They conducted a microarray experiment in which samples from a disease group and from a normal group were obtained. They based their test for gene differential expression on the scaledFdistribution of the Hotelling’sT2statistic. Some other important applications of the Hotelling’sT2statistic, Mahalanobis distance and the associatedp-values include, for example, multivariate outlier detection (e.g. Garrett, 1989; Hardin and Rocke, 2005) and multivariate quality control charts (e.g. Sullivan and Woodall, 1998; Johnson and Wichern, 2007). However, some methodological researches in causal inference argue that Mahalanobis distances do not work very well when the dimension,ν1, ofxis greater than 8 or the normality assumption is not fulfilled. See, for example, Stuart (2010) and the references therein.We conducted a simulation study to test the performance of the samplep-value associated with theFdistribution, denoted byP̂F, and that associated with the chi-square distribution, denoted byP̂χ2, in estimating the probabilityP. Simulation results show that both are biased estimates ofP. We propose some alternative estimators ofPand compare them in terms of their bias and root mean square error in the simulation study. Some of the proposed estimates have much lower biases than the estimators derived from theFand chi-square distributions.Three of the alternative point estimators ofPare based on its confidence intervals. The first uses the frequentist median of the non-centrality parameter, and is denoted byP̂D. The second proposed estimator uses the Bayesian median of the non-centrality parameter or its frequentist median, whichever is greater. We call it the modified median estimator and denote it byP̂MD;P̂DandP̂MDonly differ whenPapproaches 100%. The third estimator in this group is a Bayesian estimator; it is based on the idea of probability matching priors and is denoted byP̂BY. We propose another two new estimators ofPbased on the mean of the non-centrality parameter of a non-centralFdistribution; these are denoted byP̂MandP̂R. Estimators derived from a Taylor expansion (P̂T)–Bernstein polynomials of degree 4, 7 and 10 (P̂B4,P̂B7andP̂B10) and a quadrature polynomial approximation of degree 4, 7 and 10 (P̂Q4,P̂Q7andP̂Q10)–are also proposed and shown to be approximately unbiased in the broad range of situations examined in the simulation study.The paper is organized as follows. In Section  2, we briefly discuss the two frequently used point estimators ofP. The new twelve proposed point estimators ofPare detailed in Section  3. All the fourteen point estimators are then compared in Section  4, where we present and discuss the results of the simulation study. In Section  5, we examine the behavior of each estimator at different observed values of the Mahalanobis index. We also briefly consider the median error of the estimators (rather than average error) and mean absolute error. Concluding comments are given in Section  6.We aim to derive an unbiased estimate of(5)P=Pr{χν12>λ},whereχν12is the population Mahalanobis index(x−μ)′Σ−1(x−μ), which follows a chi-square distribution onν1degrees of freedom, andλ=Δ2is the Mahalanobis index of the case. That isλequals(x∗−μ)′Σ−1(x∗−μ), wherex∗is the vector of a case’s profile of scores fromν1tests. The probabilityPis the proportion of the population that has a profile more extreme than the case. A minimum variance unbiased estimator ofλis readily available (see Section  3.2) but obtaining an unbiased estimator ofPis much harder.Letx̄andΣ̂denote the maximum likelihood estimates ofμandΣ, respectively, henceΣ̂=[(n−1)/n]S. Simple estimates ofPcan be obtained by replacing the unknown parameters in Eq. (2) with their maximum likelihood estimates. This gives our first estimator,(6)P̂F=Pr{(x−x̄)′Σ̂−1(x−x̄)>(x∗−x̄)′Σ̂−1(x∗−x̄)}.It is well-known that(7)P̂F=Pr{Fν1,ν2>[ν2(n−1)ν1]T2},whereλ0=(x∗−x̄)′S−1(x∗−x̄).T2=nλ0/(n+1)is Hotelling’sT2statistic andFν1,ν2is a centralFdistribution onν1andν2degrees of freedom. ThenP̂Fis thep-value from testing the null hypothesis that the case is a member of the control population. Consequently, it is commonly used as a point estimate of the proportion of the normative population with more extreme profiles than the case.Another frequently used plug-in maximum likelihood estimate, denoted byP̂χ2, is thep-value from the chi-square distribution. Instead of replacingμandΣ−1byx̄andΣ̂−1everywhere in (2),P̂χ2is obtained by only making this replacement on the right-hand side of the inequality. Thus(8)P̂χ2=Pr{(x−μ)′Σ−1(x−μ)>(x∗−x̄)′Σ̂−1(x∗−x̄)},and we have that(9)P̂χ2=Pr{χν12>nn−1λ0}.Simulation results in Section  4 show thatP̂χ2is generally better thanP̂Fas an estimate ofP. However, both are biased andP̂χ2underestimatesPin most cases, with absolute bias that is getting higher for larger values of the true parameterP.Based on the work of Reiser (2001), Crawford et al. (2016) proposed a method for constructing confidence intervals onP. The observable sample statisticF0=[nν2/(n−1)ν1]λ0has a non-centralFdistribution withν1,ν2degrees of freedom, respectively, and a non-centrality parameternλ, i.e.(10)F0=[nν2(n−1)ν1]λ0∼Fν1,ν2(nλ).To construct a confidence interval forP, defineLαas the value ofnλfor whichF0is theα-quantile ofFν1,ν2(nλ). Then, a100(1−α)%confidence interval forPis given by(11){1−G(Lα/2/n),1−G(L1−α/2/n)},whereG(.)is the cdf of a chi-square distribution onν1degrees of freedom.Using the same technique, a point estimator ofPis given by its median estimate. We have thatL0.5is the value ofnλat whichF0is the median of theFν1,ν2(nλ)distribution. The first of our new estimators,P̂D, is defined as(12)P̂D=1−G(L0.5n).AlthoughP̂Dis a biased estimator ofP, simulation results in Section  4 show that it usually has a smaller bias and mean square error thanP̂Fat all values of the true parameterP.Asnλdecreases, so does the median ofFν1,ν2(nλ). Sinceλ≥0, a lower bound on the median ofFν1,ν2(nλ)is the median ofFν1,ν2(0). (Fν1,ν2(0)is the ordinary centralFdistribution onν1andν2degrees of freedom.) IfF0is less than this lower bound, one approach is to setnλto zero. This is the standard approach adopted in the construction of confidence intervals, where the same problem arises, as discussed in Reiser (2001). The problem arises wheneverF0is small, even if it is above the lower bound. To illustrate, supposeν1=4,ν2=20andλ0=0.4, soF0=2.09. Then calculation givesP̂D=99.49%. Thus when a patient’s estimated Mahalanobis index is 0.4, then 0.51% is the estimate of the proportion of the normal population with a smaller true Mahalanobis index than the case. However, if 0.4 were the true Mahalanobis distance of the case, then the actual proportion of the normal population with a smaller Mahalanobis distance than the case is calculated at 1.75% (P=0.9825), from a chi-square distribution on 4 degrees of freedom. The disparity between 0.51% and 1.75% is substantial and, moreover, intuitively one would expect uncertainty to result inP̂Dbeing less extreme thanP, rather than being greater than it. Asλ0decreases the situation worsens. Whenλ0=0.2,P̂D=99.99%, whileP=99.53%whenλ=0.2.A pragmatic solution was proposed by Garthwaite et al. (2016). They supposed an individual’s sample Mahalanobis index wasλ0and considered the question: “What proportion of the population will have a true Mahalanobis index that is bigger than this individual?” under two situations(i)the individual is the casethe individual is a randomly chosen member of the population.Letλ˜be the Mahalanobis index of a randomly selected individual from the population and letP˜be the proportion of the population with a larger Mahalanobis index thanλ˜. ThenP˜is a random variable and, before observingλ0,P˜has a uniform distribution over the interval (0, 1). In consequence,λ˜has a chi-square distribution onν1degrees of freedom. This chi-square distribution can be taken as the prior distribution in a Bayesian analysis in which there is a single datum,λ0. Note that there is nothing arbitrary about this prior distribution; it is the distribution ofλ˜becauseP˜∼U(0,1). The likelihood follows from Eq. (10) asF0=[nν2/(n−1)ν1]λ0|λ˜∼Fν1,ν2(nλ˜). We obtain the posterior distribution ofλ, and compute its normalizing constant through numerical integration. A simple search procedure is used to find the posterior median ofλ˜, sayMλ. We then useMλto enhance the median estimatorP̂Din (12) and propose the modified median estimator,P̂MD, as(13)P̂MD=min{P̂D,1−G(Mλ)}.Obviously,P̂MDandP̂Donly differ whenλ0is small and then the differences are slight in absolute terms(|P̂MD–P̂D|), thoughP̂D/P̂MDis far from 1 for very smallλ0. This is illustrated, forν1=4andν2=20, in Figs. 1 and 2, whereP̂Dand1−G(Mλ)are plotted againstλ0at different observed values of0≤λ0≤32and0≤λ0≤0.5, respectively. AsP̂MDtakes the lower value ofP̂Dand1−G(Mλ), Fig. 1 shows thatP̂DandP̂MDare identical forλ0>4.5, while Fig. 2 shows that, asλ0gets small,P̂MDis clearly less than 100% (as common sense dictates it should be) whileP̂Dapproaches 100%.As expected, simulation results in Section  4 show that the bias and mean square error of the modified estimatorP̂MDare nearly identical to those of the median estimatorP̂D. We recommendP̂MDoverP̂Dfor use in practice to avoid the problem of gettingP̂D=1, as discussed above.Bayesian100(1−α)%credible intervals quite often have the same endpoints as frequentist100(1−α)%confidence intervals if the Bayesian intervals are based on uninformative prior distributions. Indeed, there has been substantial interest in probability matching priors  (Datta and Mukerjee, 2004), which are designed to give credible intervals that match confidence intervals. To construct our next estimate ofP, we suppose a prior distribution has been found that gives posterior credible intervals which match the confidence intervals specified in Eq. (11). Treating the confidence intervals as exact credible intervals, they determine a posterior distribution for the proportionP. We use a sufficiently large number, sayR, of one-sided credible interval limits to construct the posterior distribution. The posterior mean is then used as a point estimate, sayP̂BY, ofP.Specifically, we estimate the interval limitLr/Ras the value ofnλfor whichF0is the(r/R)-quantile ofFν1,ν2(nλ), forr=1,…,R. As in (11), the(r/R)-quantile of the posterior distribution ofPis given by1−G(Lr/R/n). The posterior meanP̂BYis then computed as(14)P̂BY=1−∑r=1RG(Lr/R/n)R.In practice, we takeR=500, so that we use the quantiles(0.002,0.004,…,0.998)which is a sufficiently fine partition for our purpose and is not expensive in computing time.Based on simulation results in Section  4, the estimateP̂BYis a badly biased estimate ofP. This result illustrates an important fact: while posterior distributions obtained from exact probability matching priors will (by design) give interval estimates with good frequentist properties, the posterior mean may be far from meeting the frequentist definition of unbiasedness.Our next proposed estimators ofPare based on the estimated mean value ofλ, sayλ̄. IfF0is given by Eq. (10), then(15)nλ̄=ν1(ν2−2)ν2F0−ν1is the uniformly minimum variance unbiased estimator of the non-centrality parameter of the non-centralFdistributionFν1,ν2(nλ). However, it is well-known that this is not always positive and is therefore inadmissible. See, for example, Johnson et al. (1995). To avoid a negative estimate ofλ, put(16)λ̂=Max{1n[ν1(ν2−2)ν2F0−ν1],0}.Usingλ̂, we propose the estimatorP̂MofPas(17)P̂M=1−G(λ̂).Unfortunately, based on the simulation results in Section  4,P̂Mcan have marked bias as an estimator ofP.The estimatorλ̂in (16) is also inadmissible (Chow, 1987), but Rukhin (1993) showed that, forν2>4,(18)λ˜=1n[ν1(ν2−4)ν2F0]is an admissible estimator ofλ. We base our next estimate,P̂R, ofPonλ˜and put(19)P̂R=1−G(λ˜).However, as illustrated in Section  4,P̂Mis generally better thanP̂Rin terms of bias and mean square error.We expand the cdf of the chi-square distributionG(X)aboutλas(20)G(X)≃G(λ)+(X−λ)g(λ)+(X−λ)22g′(λ),whereg(.)is the pdf of a chi-square distribution withν1degrees of freedom. We setXequal toλ̄in Eq. (15) and take the expected value of both sides of (20). This gives(21)E{G(λ̄)}≃G(λ)+Var(λ̄)2g′(λ),where, from the variance ofF0withν2>4,Var(λ̄)is given by(22)Var(λ̄)≃v0+v1λ+v2λ2,withv0=2ν1(ν1+ν2−2)/(n2(ν2−4)),v1=4(ν1+ν2−2)/(n(ν2−4))andv2=2/(ν2−4). (As defined earlier,F0∼Fν1,ν2(nλ).)In the light of (21), to obtain an approximately unbiased estimate ofG(λ)to the second order, it seems natural to base an estimate onλ#say, such that(23)G(λ#)−G(λ̄)=−Var(λ̄)2g′(λ).We start with the case whereλ̄is greater than the mode of the chi-square distribution. In this caseg′(λ)is negative,λ#>λ̄and we can write(24)G(λ#)−G(λ̄)=(λ#−λ̄)g(ξ),for someξ∈(λ̄,λ#). From (23) and (24) we have(25)λ#−λ̄=−Var(λ̄)2g′(λ)g(ξ).We define another estimate,λ∗, by replacingξin (24) withλ:(26)λ∗−λ̄=−Var(λ̄)2g′(λ)g(λ).Suppose|λ̄−λ|is large relative to|λ̄−λ#|. Ifλ>λ#, thenλ∗>λ#andλ∗will be better thanλ#as an estimate ofλ. Ifλ<λ̄, thenλ∗<λ#andλ∗will again be better thanλ#as an estimate ofλ. On the other hand, supposing that|λ̄−λ|is small relative to|λ̄−λ#|, theng(ξ)≃g(λ)andλ∗≃λ#. The consequence is thatλ∗defined in (26) is expected to be better thanλ#, in terms of the mean square error, as an estimate ofλ. The other case in whichλ̄is less than or equal to the mode of the chi-square distribution can be treated similarly.It remains now to estimate the right hand side of (26). We find an unbiased estimate, sayVar̂(λ̄), ofVar(λ̄)expressed as(27)Var̂(λ̄)=u0+u1λ̄+u2λ̄2,whereu0,u1andu2are chosen such thatE{Var̂(λ̄)}=Var(λ̄). Specifically, equating the corresponding coefficients ofλinE{Var̂(λ̄)}to those in (22), we getu0=2ν1(ν1+ν2−2)/(n2(ν2−2)),u1=4(ν1+ν2−2)/(n(ν2−2))andu2=2/(ν2−2). It is straightforward to show that(28)g′(λ)g(λ)=1λ(ν12−1)−12.However, no simple unbiased estimate can be found for1/λ, instead, we estimate it as1/λ̄. The estimatorλ∗is finally expressed as(29)λ∗=λ̄−Var̂(λ̄)2[1λ̄(ν12−1)−12].Using this Taylor based estimate, our proposed approximately unbiased estimateP̂TofPis given by(30)P̂T=1−G(λ∗).Simulation results show thatP̂Tis usually one of the better estimates ofP. More information is given in Section  4.None of the point estimators we have proposed so far attain approximate unbiasedness uniformly for all values ofP. This motivates another set of point estimators that are approximately unbiased uniformly for allP. Using polynomial approximations, we aim to base the proposed estimator ofPin this section on a good global estimate of the non-centrality parameterλ. This means that in searching for an approximately unbiased estimate ofP, we cover wide areas of the chi-square cdf and do not locally search around some estimate ofλ, as was proposed in Section  3.3 when using the Taylor expansion. In principle, estimates based on global approximation of the cdf should prove better, in terms of bias, than an estimate based on a local approximation.We introduce a set of unbiased estimates ofP, denoted for now byP̂P, which are based on approximating the probabilityPin (5) as a polynomial function of degreerinλ. From Weierstrass’s Theorem, any function of a variable,λsay, can be approximated by a polynomial ofλ, provided the function satisfies weak regularity conditions. NowP=Pr(χν12>λ)is a function ofλthat meets these regularity conditions, so we may put(31)P=Pr(χν12>λ)≃∑i=0raiλi.The coefficientsai(i=0,…,r)are known functions inν1(see below).The key to exploiting Eq. (31) is that the moments ofF0are also polynomials inλ. Specifically, withF0defined by Eq. (10), theith moment,E(F0i), is a polynomial ofλof degreei. WritingPin the polynomial form (31), it can therefore be estimated by another polynomial inF0as follows(32)P̂P=∑i=0rbiF0i,where the coefficientsbi(i=0,…,r)are determined such that(33)∑i=0rbiE(F0i)=∑i=0raiλi.This ensures the approximate unbiasedness ofP̂Pin estimatingP. The coefficientsbi(i=0,…,r)can be obtained by equating the coefficients of the corresponding terms of the polynomials on the two sides of (33). To do that, we used the computer algebraic system Maple 16.Although this computer algebraic system does not give explicit symbolic formulas for the raw moments of a non-centralFdistribution without using special functions, it can efficiently give simple explicit forms of the raw moments of a non-central chi-square distribution up to any required orderr. The former can then be obtained from the latter by using the following straightforward relationship between the corresponding raw moments of the two distributions (Bain, 1969):(34)μi′[Fν1,ν2(λ)]=μi′[χν12(λ)]Γ(ν2/2−i)(ν2)iΓ(ν2/2)(2ν1)i,whereμr′[.]is theith raw moment.It has to be noted here that theith raw moment of a non-centralFdistribution is finite only forν2>2i. This puts a constraint on the valid numberrof polynomial terms to be used in the proposed approximation. Ifnis the size of the control sample, thenrmust be strictly less than(n−ν1)/2.Now, to apply the approach outlined in Eqs. (31)–(33), it remains to find a suitable polynomial approximation to be used in (31). We use two different approximations, the first is based on Bernstein polynomials while the second is a quadrature polynomial approximation.From Weierstrass’s Theorem, any continuous real valued functionf(x)defined on a closed interval[a,b]can be approximated by a polynomial function. See, for example, Lorentz (1986). In 1912, Bernstein gave a simple probabilistic constructive proof for Weierstrass’s Theorem by introducing the Bernstein polynomialsBr(f;x)as a series of polynomials that converge uniformly to any continuous bounded functionf(x)on the closed interval[0,1]asr→∞. See, for example, Chapter (7) in Phillips (2003). Therth Bernstein polynomialBr(f;x)forf(x)is defined as:(35)Br(f;x)=∑i=0r(ri)xi(1−x)r−if(i/r).The polynomial functionBr(f;x)uniformly approximatesf(x)on[0,1]in the sense thatlimr→∞sup0≤x≤1|Br(f;x)−f(x)|=0(e.g. Theorem 1, Section VII.2 in Feller, 1965).We use Bernstein polynomials to obtain a polynomial approximation for the chi-square cdf to be used in (31). Although the domain of the chi-square cdf is[0,∞), we use an affine transformationx=(λ−a)/(b−a), for any two arbitrary valuesaandb, so as to work on the [0, 1] interval. The two end-pointsaandbare initially chosen such that the probability of getting a sample value of the non-centrality parameterλoutside the interval[a,b]is fairly negligible. Therefore, we initially takea=L0.999/nandb=L0.001/nwhere, as before,Lαis the value ofnλfor whichF0is theα-quantile ofFν1,ν2(nλ). As will be shown at the end of Section  3.4, the accuracy of the polynomial approximation is influenced by the choice ofaandb. For extremely large values ofb, say above the 0.9999-quantile of the chi-square distribution, polynomial functions of small degreerare not guaranteed to give a good approximation. Also, accuracy is greatly enhanced ifais chosen to be just below the sample median valueL0.5/nofλ. Hence, as a rule of thumb, ifL0.5/nis greater than the mode of the chi-square distribution, our final choice ofaisa=0.99(L0.5/n).We then approximatePin (31) by itsrth Bernstein polynomial inλof the form(36)Br(P;λ)=1−∑i=0r(ri)(λ−ab−a)i(b−λb−a)r−iG(a+(b−a)ir).Clearly, the above expression ofBr(P;λ)is a polynomial of degreerinλ, and we denote its coefficients byai(i=0,…,r). The explicit form of these coefficients was obtained using the computer algebraic system Maple 16. The coefficients ofλon the left hand side of (33) are equated to their corresponding coefficients in the Bernstein polynomial approximation (36) so as to obtain the values ofbiand, hence,P̂Pin Eq. (32). In this paper, we obtain the estimateP̂Pforr=4, 7 and 10, and denote it byP̂B4,P̂B7andP̂B10, respectively.We adopt the quadrature formula of Sahai et al. (2004) to obtain another polynomial approximation forP. This quadrature formula gives polynomial approximations to the integration of real valued continuous functions defined on the closed interval [0, 1]. Specifically, a functionf(x)on [0, 1] is approximated by a polynomial inxof degreeras(37)Q(x)=∑i=0r(rxi)(r(1−x)r−i)f(xi),wherexi=i/r(i=0,…,r)partition the interval [0, 1] intorequal segments. The polynomial functionQ(x)can then be easily integrated over any sub-interval in [0, 1]. It has been shown empirically that the approximation has good accuracy even whenris small. For example, it was used by Richard et al. (2010) to obtain an efficient polynomial approximation of degree 9 to the normal distribution function and its inverse function.Here, we apply the quadrature formula to approximate the density function of a chi-square distribution onν1degrees of freedom with a polynomial of degreer−1, which then yields the approximation in (31) after a straightforward symbolic integration of the polynomial.To work on the [0, 1] interval, we adopt the approach discussed in Section  3.4.1 for choosing the two end-pointsaandb, with the same affine transformationx=(λ−a)/(b−a). The probabilityPcan now be approximated as(38)PQ=1−Pr(χν12<λ)≃1−G(a)−∫x=0λ−ab−a{∑i=0r−1((r−1)xi)((r−1)(1−x)r−i−1)(b−a)g[a+ir−1(b−a)]}dx,whereg[.]is the density function of a chi-square random variable withν1degrees of freedom.The coefficients ofλin the expression ofPQin (38) above are again denoted byai(i=0,…,r), with their explicit forms obtained using Maple 16. The coefficients ofλin the left hand side of (33) are equated to their corresponding coefficients in the quadrature polynomial approximationPQin (38) so as to obtainbi(i=0,…,r). This gives another form ofP̂P(Eq. (32)). Here we determine it forr=4, 7 and 10, and denote the resulting estimators byP̂Q4,P̂Q7andP̂Q10, respectively. Simulation results show that these estimators are usually marginally better than those based on Bernstein polynomials.In general, polynomial functions give approximations that are accurate only on specific intervals of the domain of the underlying approximated function. The accuracy of the polynomial approximations that we use is highly related to the values selected as the two interval end-points,aandb. Figs. 3–5show Bernstein and quadrature polynomial approximations for three choices of[a,b]. The cdf of a chi-square distribution on degrees of freedomν1=4is plotted together with its Bernstein approximation and quadrature polynomial approximation, each of degreer=7.Fig. 3 shows that both polynomial approximations give good accuracy when[a,b]is the rather short interval [0, 18] of the cdf domain. The valueb=18is near the boundary of plausible values for aχ42variate, as 18 is the 0.999 quantile of aχ42distribution. For extremely large values ofb, neither polynomial approximation of degree 7 is expected to attain good accuracy. This can be seen in Fig. 4, whereb=30. For the same extreme value ofb=30, ifais above the mode of this chi-square distribution (i.e.a>2), remarkably better accuracy is obtained, especially for the quadrature approximation. This is shown in Fig. 5, wherea=4andb=30. This argument motivates our choice ofaandbthat was discussed in Section  3.4.1, as the behavior of both polynomial approximations tends to be the same for all values ofν1. As seen in Fig. 3 and Fig. 5, the Bernstein polynomial approximation can be more accurate than the quadrature polynomial approximation for values nearb, but the quadrature approximation overall attains better accuracy over the whole interval[a,b].In practice, we expectν1, the number of test scores on each individual, to be small, whilen(n=ν2+ν1), the number of people in the control sample, may be large. Therefore, we conducted a simulation study that examined combinations ofν1=2, 4 and 8, withν2=10, 20 and 80. Results of some other combinations are available on request from the authors. Based onN=100,000samples for each combination, we tested the performance of each of the proposed estimators in terms of their average bias,∑i=1N(P̂i−P)/N, and the root of mean square error,∑i=1N(P̂i−P)2/N, denoted by SE (standard error) in tables.Mahalanobis distance is invariant under affine transformations of location and scale parameters. Since all the methods proposed in this paper depend only on the sample Mahalanobis index,λ0, we chose, without loss of generality, to set the population mean (μ) equal to0and the population variance (Σ) equal to the identity matrixIν1. The true values ofPthat we examined were 1%, 2.5%, 5%, 10%, 20% and 40%. These true probabilities were attained from Eqs. (2) and (5) by choosing each corresponding case’s profile of scores,x∗, as a vector of equal elements. We also examined some cases where the scores in the profile,x∗, are not necessarily equal to each other. But these cases gave almost identical results to those of the profiles with equal scores. We therefore give simulation results only for profiles with equal scores.Table 1shows the simulation results forν1=2and 4, andν2=10. Whenν1=2,P̂χ2,P̂Q4andP̂Tare the best three estimators in terms of the bias and root mean square error.P̂χ2is slightly better thanP̂Q4andP̂Tup toP=10%, but forP=20%and 40%, the estimatorsP̂Q4andP̂Tare remarkably better thanP̂χ2in terms of their bias, withP̂Q4being the best. Atν1=4, the table shows thatP̂χ2,P̂Q4andP̂Tare again the best three estimators, withP̂Q4showing less bias thanP̂χ2for true values ofPof 10% or more. This suggests that, at small values ofν2, the best two competitor estimates areP̂χ2andP̂Q4, where the former is doing better at smaller values of the true probabilityP.An important point from Table 1 is the cautionary message that each of the methods shows noticeable bias for some values ofPat some combinations ofν1andν2. Four methods perform particularly poorly:P̂F,P̂BY,P̂M,P̂R.Table 2shows the simulation results atν1=2, 4 and 8, andν2=20. For all listed values ofν1, the three estimatesP̂χ2,P̂Q4andP̂Q7are doing better than the others. For small values ofP, the bias of bothP̂Q4andP̂Q7is generally less than that ofP̂χ2, while the root mean square error ofP̂χ2is always less than those ofP̂Q4andP̂Q7. ComparingP̂Q4toP̂Q7, it can be seen in Table 2 thatP̂Q7is better thanP̂Q4in terms of bias, although the root mean square error ofP̂Q4is slightly greater than that ofP̂Q7for all values of the true probability exceptP=1%. This suggests thatP̂Q7is the best estimate atν2=20as it has rather small values of both bias and root mean square error for all values of the true probabilityP.Simulation results forν2=80(ν1=2, 4 and 8) are presented in Table 3. This value ofν2is large enough for the estimatorsP̂Q10andP̂B10to be computed. All estimators are doing well at this very large sample size and all have similar bias and root mean square error. However, the estimators based on polynomial approximations are still slightly better than the others. Specifically, the quadrature based estimatorsP̂Q4,P̂Q7andP̂Q10have very low bias as shown in Table 3, with the bias ofP̂Q10always less than or equal to those ofP̂Q4andP̂Q7.The simulations in Section  4 show that the estimators based on polynomial approximations performed well, in that they had the minimum bias among the reported estimators. However, this does not mean that the estimates they produce are always sensible. Specifically, whenλ0is very small they can give estimates of the proportion that are greater than 100%, and whenλ0is very big they can give estimates that are less than zero. This problem comes to light by studying the behavior of the proposed estimators at different values in the domain ofλ0.For example, atν1=4andν2=24, Fig. 6(a) shows that the twelve estimators all have similar patterns for0<λ0<30. But a closer look at the part of the domain where0<λ0<0.5(Fig. 6(b)) reveals thatP̂B4,P̂B7,P̂Q4andP̂Q7are not monotonically decreasing withλ0and they exceed 100% at some values ofλ0. With the same degrees of freedom, another problem appears in Fig. 6(c), where bothP̂Q4andP̂Q7are below zero for some values of12<λ0<30.Similar problems appear at large sample sizes as well. For example, atν1=4andν2=80,P̂Q4is slightly below zero for some values of22<λ0<32and as shown in Fig. 6(d),P̂Q4,P̂Q7andP̂Q10all exceed 100% for some values of0<λ0<0.2. The problem does not arise with other estimators—estimates are always in the range 0%–100% forP̂F,P̂χ2,P̂D,P̂MD,P̂BY,P̂M,P̂RandP̂T. However, as Fig. 6(b) shows, the estimate ofPapproaches 100% asλ0approaches 0. This is clearly unrealistic as the case’s valuex∗will not equal the population meanμ, even ifx∗equals the sample meanx̄. As in Section  3.1.2, a pragmatic approach is to treat the case’s profile as that of a randomly chosen control when the case’s profile seems nearer toμthan would be expected of a control’s profile.In Section  4, mean square error and average error (bias) were used to evaluate the performance of the various estimators considered in this paper. Alternative evaluation criteria include average absolute error (AAE=∑i=1N|P̂i−P|/N) and median error (ME  =  median(P̂1−P,…,P̂N−P)). Here, we briefly examine the performance of our estimators under these criteria. In theory, the median estimatorP̂Dshould give a median error of 0 and have a lower AAE than other estimators whose median error is small. Its closely related estimator,P̂MD, should also perform well.Results forν1=8andν2=40are presented in Table 4. It can be seen that the median error in estimatingPis 0.0 for bothP̂DandP̂MDfor each of the tabulated values ofP. In contrast, the median error of every other estimator is never 0.0 except forP̂B10whenP=40%; otherwise the median error of the other estimators is typically quite marked.A fuller examination of the median errors given byP̂DandP̂MDis provided in Table 5, where results are given for these estimators for all the combinations ofν1andν2that were considered in Tables 1–3. The two estimators give identical median error for every combination and that error is very small in every case. Hence, if we want an estimator that has very small median error, then bothP̂DandP̂MDcan fill that role. The average absolute error is marginally better with theP̂MDestimator, but the differences are very slight. However,P̂MDis the preferable estimator because it will not give unrealistic estimates ofP, whileP̂Dwill sometimes estimatePas 100% when that is not a credible estimate. Consequently, if a point estimator ofPis required, one reasonable choice is to giveP̂MDas the estimator and say that it gives small median error without making any claim about its bias (average error).The task that motivated this paper seemed straightforward: find a good point estimator of the abnormality of a Mahalanobis index. The answer is less straightforward, as the best choice of estimator will depend on the purpose for which the estimator is required. The following summarizes our findings.1.The most common criteria used to choose an estimator are bias and mean square error; the minimum variance unbiased estimator is often the preferred estimator if such an estimator can be found. Under these criteria the best estimators are those based on a quadrature polynomial approximation,P̂Q4,P̂Q7andP̂Q10, provided occasional negative estimates are not a problem. (The negative estimates would presumably be set to 0.) OnlyP̂Q4can be used forν2=10;P̂Q7is best forν2=20;P̂Q10andP̂Q7are marginally the best (P̂Q4is almost as good) forν2=80.If mean square error is to be minimized and bias is unimportant, thenP̂χ2is the best estimator, but it displays substantial bias even whenν2is large.Sometimes, an estimate ofPis to be used as an input into further analysis. Commonly though, an estimate ofPis to be communicated to others (perhaps in a journal paper or a technical report) and then a good descriptive statistic is required. In that context, the best estimator would seem to be the modified median estimator,P̂MD. It should be referred to as the median estimator as that is accurate: it is designed to give low median bias rather than low average bias and, indeed, its median bias is very low. It is preferable to the median estimate (P̂D) because it always gives sensible estimates whileP̂Dsometimes gives estimates that are unrealistically small when judged by common sense.Based on our simulation results, we recommend thatP̂MDshould generally be used as the point estimator ofP. However, if unbiasedness of the required estimate is crucially important we recommend thatP̂Q4should be used forν2<20andP̂Q7should be used forν2≥20. Out-of-range values of these two estimators need to be artificially constrained so as not to lie outside the interval [0, 1].This work is part of an on-going project that develops statistical methods for analyzing single patient data, and these recommendations are implemented in software for making inferences from Mahalanobis distance about the abnormality of an individual’s test score profile. Previous methods that we have developed are well-used by neuropsychologists (see, for example, papers that cite Crawford and Garthwaite, 2002, 2005, 2007) so it is likely that the recommendations will influence practice. The work in this paper makes these recommendations well-informed.

@&#CONCLUSIONS@&#
