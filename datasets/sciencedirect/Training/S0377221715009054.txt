@&#MAIN-TITLE@&#
Hesitant analytic hierarchy process

@&#HIGHLIGHTS@&#
We develop a new analytic hierarchy process method.Hesitant information is allowed in this method.We use stochastic method to handle hesitant information.This method is illustrated by a real-life example.

@&#KEYPHRASES@&#
Uncertainty modeling,Decision support systems,Comparison matrix,Simulation methodology,

@&#ABSTRACT@&#
In traditional analytic hierarch process (AHP), decision makers (DMs) are required to provide crisp judgments over paired comparisons of objectives to construct comparison matrices. To enhance the modeling ability of traditional AHP, we propose hesitant AHP (H-AHP) that can consider the hesitancy experienced by the DMs in decision. H-AHP is characterized by hesitant judgments, where each hesitant judgment can be represented by several possible values. Different probability distributions can be used to further describe hesitant judgments according to the DMs’ preferences. Based on a hesitant comparison matrix (HCM) that consists of hesitant judgments, we define two indices to measure the consistency degree and the consensus degree of the HCM respectively. From a stochastic point of view, a new prioritization method is developed to derive priorities from HCMs, where the results are with probability interpretations. We provide a step by step procedure for H-AHP, and demonstrate this new method with a real-life decision making problem.

@&#INTRODUCTION@&#
Analytic hierarch process (AHP) can be used for structuring, measurement, and synthesis. It has wide applications, such as the decisions of choice, prioritization/evaluation, resource allocation, benchmarking, quality management, public policy, and forecasting etc. (Saaty, 1989, 1994, 1977, 1980, 2008; Saaty & Vargas, 1987). In a multi-criteria environment, AHP is built on a decision maker (DM)’s intrinsic ability to structure his/her perceptions or his/her ideas hierarchically. With the produced dimensionless ratio-scale priorities, AHP assists the DMs to make reliable decisions.To resolve the AHP problems, four main steps should be followed. (1) Modeling: determining a top-to-bottom form as a hierarch with different levels of criteria, sub-criteria and alternatives. (2) Evaluation: constructing comparison matrices based on a 1–9 scale. (3) Prioritization: using prioritization methods to derive local priorities of objectives in each level of the hierarchy. (4) Synthesis: utilizing aggregation procedures to synthesize the local priorities into global priorities of alternatives.In AHP, the DMs usually provide crisp values for judgments over paired comparisons of objectives with respect to a criterion. So we call the judgments represented by crisp values as crisp judgments. If the DMs are uncertain about the judgments, this uncertainty can be measured by intervals which can be called interval judgments (Saaty & Vargas, 1987). Both crisp judgments and interval judgments are based on the 1–9 scale. If it is difficult to determine crisp or interval values, the judgments can also be represented by fuzzy values (Ishizaka & Labib, 2011; Ishizaka & Nguyen, 2013; Saaty, 1980; Van Laarhoven & Pedrycz, 1983; Zadeh, 1965).However, the existing representations of judgments still have limitations in practice. More possible representations should be developed to represent uncertainty experienced by the DMs due to the increasing complexity of modern society. Motivated by a real-life AHP problem, that is to assess the strategic positions of islands and reefs of China, we need to deal with a hesitant case, where the military experts prefer to retain several possible values to represent some judgments rather than crisp or interval values. For example, to make the judgment over a paired comparison of objectives, a military expert is hesitant about the values 4 or 5 based on the 1–9 scale. So we have two possible values in this case, not a single value represented by a crisp judgment, or a margin of error represented by an interval judgment. From a probability point of view, 4 and 5 should be with the same probability 0.5. Similar idea of this kind of uncertainty can also be found in the concept of hesitant fuzzy sets proposed by Torra (2010).In the process of human reasoning and concept formation, hesitancy is a common phenomenon, especially in decision making. We use several possible values to indicate a judgment in AHP to describe the hesitancy experienced by the DMs in decision. As distinct from crisp judgments and interval judgments, we call the judgment represented by several possible values a hesitant judgment. Based on hesitant judgments and under the framework of AHP, we propose a hesitant AHP (H-AHP) in this paper with a series of new concepts and methods.The rest of this paper is organized as follows. Section 2 gives necessary descriptions of AHP. Section 3 develops the concept of hesitant comparison matrices (HCMs), then focuses on consistency and consensus of HCMs respectively. In Section 4, we develop hesitant preference analysis as a new prioritization method for HCMs. Section 5 gives related discussions. Based on the developed concepts and methods above, Section 6 concludes a step by step procedure for H-AHP. A real-life example is introduced to illustrate our results in Section 7. Section 8 closes this paper with some conclusions.In recent decades, AHP and related research topics have widely attracted much attention of researchers (Ivlev, Vacek, & Kneppo, 2015; Kułakowski, 2015; Siraj, Mikhailov, & Keane, 2015; Zhu, Xu, Zhang, & Hong, 2015). In this section, we discuss four main steps of analytic hierarchy process (AHP) in detail, which are modeling, evaluation, prioritization and synthesis to provide a fundamental for our new method.Modeling is the first step of AHP, that is to hierarchically structure a problem. With respect to a decision goal or control criterion, the hierarchy consists of several levels with criteria, sub-criteria and alternatives from top to bottom. As usual, we begin with the alternatives, then go up with the simplest sub-criteria until determining all objectives in each level.In the evaluation step of AHP, the decision makers (DMs) provide judgments over paired comparisons of objectives with respect to a criterion in an upper level. Saaty (1990) gave the 1–9 scale shown in Table 1, which is a scale of absolute numbers used to assign numerical values to judgments made by comparing two objectives.The judgments over paired comparisons of objectives are collected by comparison matrices. LetA=(aij)n×nbe a comparison matrix, it can be shown as follows:(1)A=[1a12a13…a1n1a23…a2n⋮1⋮⋮1/aij⋮⋱⋮…1]In the evaluation step of hesitant AHP (H-AHP), the DMs are allowed to provide hesitant judgments to construct hesitant comparison matrices (HCMs), which are discussed in Section 3 in detail.Consistency is a basic requirement for comparison matrices to guarantee meaningful results, and consistency check and consistency improving are two main research topics.Checking consistency of comparison matrices is a crucial step to avoid misleading solutions. Saaty (1977) developed an eigenvector method (EVM) to derive priorities from comparison matrices, and then defined a consistency index to measure their consistency degrees.For a set of objectivesX={x1,x2,…,xn}, and a constructed comparison matrixA=(aij)n×n, the EVM is based on solving the equation:(2)Aω=λmaxω,∑i=1nωi=1where λmax  is the maximum eigenvalue of A, and ω is the priority vector of the objectives.The consistency index is defined asCR=CI/RI, whereCI=(λmax−n)/(n−1), and RI is a random consistency index. For different sizes of A, the average RI is shown in Table 2(Saaty, 1977).If CR < 0.1, it means that the error in measurement is considered to be acceptable, then A is said to be with the acceptable consistency; otherwise, A is called unacceptable.Besides the EVM that derives priorities from comparison matrices, another popular method is a row geometric mean method (RGMM) developed by Crawford and Williams (1985). For the comparison matrix A, and according to the RGMM, the priorities ωi(i=1,2,…,n)can be simply found as the geometric means of the rows of A:(3)ωi=(∏j=1naij)1n∑i=1n(∏j=1naij)1nBased on the priorities ωi(i=1,2,…,n), Aguaron and Moreno-Jiménez (2003) developed a geometric consistency index (GCI) to measure consistency of A:(4)GCIA=2(n−1)(n−2)∑i<jlog2eijwhereeij=aijωj/ωi.Compared with the consistency index CR, Aguaron and Moreno-Jiménez (2003) further proposed the thresholds of GCI for different sizes of A, denoted byGCI¯(n). The results are shown in Table 3.IfGCIA<GCI¯(n), then A is with the acceptable consistency, which means that the RGMM captures the essential idea of transitivity, and the priorities obtained by the RGMM are meaningful. From the computational point of view, GCI can be easily obtained due to its linear complexity. In the rest of the paper, we will use it as a basis to develop new indices for HCMs.For the comparison matrices with the unacceptable consistency, many methods have been developed to improve consistency. For example, Xu and Wei (1999) proposed an iterative method; Ishizaka and Lusti (2004) developed an expert module to construct consistent comparison matrices; Li and Ma (2007) proposed an iterative method to detect and improve inconsistent judgments; Cao, Leung, and Law (2008) developed a heuristic approach; Ergu, Kou, Peng, and Shi (2011) proposed a method to identify and improve inconsistent judgments based on the theorem of matrix multiplication and vectors dot product.In the rest of the paper, we use the iterative method given by Xu and Wei (1999) as a basis to develop consistency improving and consensus improving methods of HCMs due to its relationship with the traditional EVM and advantages in computation. For the comparison matrixA=(aij)n×n, the algorithm of this consistency improving method can be restated in Algorithm 1.Algorithm 1Consistency improving for individual comparison matrices.Input: A comparison matrixA=(aij)n×n; a parameter λ.Step 1:Calculate the maximal eigenvalue λmax (A) of A and the normalized principal right eigenvectorω=(w1,w2,…,wn)Tby Eq. (2).CalculateCRA=(λmax−n)/((n−1)RI).If CRA< 0.1, then go to Step 6; otherwise, go to Step 4.LetA′=(aij′), whereaij′=(aij)λ(ωi/ωj)1−λ, λ ∈ [0, 1].LetA=A′, then go to Step 1.LetA′=A, then end.Output: The comparison matrix A′ with the acceptable consistency.In a context with a group of DMs, AHP requires that individual judgments should be aggregated, where aggregating individual judgments (AIJ) and aggregating individual priorities (AIP) are two basic approaches. However, Hauser and Tadikamalla (1996) argued that many circumstances make the aggregation difficult, and the confidences of the DMs to the results may be reduced if they lack faith in some aggregated judgments. Altuzarra, Moreno-Jiménez, and Salvador (2010) claimed that this kind of aggregation is usually accepted only if judgments among the DMs are not strongly divergent. Therefore, consensus reaching, which is defined as the full and unanimous agreement among the DMs regarding all judgments, is a guarantee for the DMs to adequately represent their preferences.Consensus reaching is relevant to the revisions of judgments and the establishment of consensus paths. For example, Bryson (1996) used consensus relevant information to assess the levels of consensus, and then developed some strategies to increase the consensus levels. MarÍa, JimÉnez, Joven, Pirla, and Lanuza (2005) developed a spreadsheet module to search for consensus. Yeh, Kreng, and Lin (2001) developed a computer-aided approach to achieve consensus. Altuzarra et al. (2010) developed a semiautomatic Bayesian approach to build consensus. Under a row geometric mean prioritization method, Dong, Zhang, Hong, and Xu (2010) proposed some models to achieve consensus.Generally, the existing methods for consensus building can be roughly divided into automatic methods which are time saving, effective and practical without the interactions of the DMs, and feedback methods which generate suggestions for the DMs to revise their judgments. The automatic methods pay attention to the efficiency, while the feedback methods stress on the DMs’ interventions. In this paper, we shall develop an automatic consensus improving method for HCMs considering of efficiency.In the prioritization step of AHP, many prioritization methods have been developed to derive priorities of objectives in each level of the hierarchy, such as the EVM, the RGMM, the logarithmic least squares method (Crawford & Williams, 1985), the logarithmic goal programming method (Bryson, 1995) and the fuzzy programming method (Mikhailov, 2000). Among these methods, only the EVM and the RGMM require to measure consistency of comparison matrices. Without consistency check, the prioritization methods may give different ranks, and lead to wrong solutions (Saaty, 2003; Saaty & Hu, 1998; Saaty & Vargas, 1984, 1987). With respect to HCMs, we develop hesitant preference analysis as a new prioritization method, where the RGMM is used to guarantee reliable results.The local priorities, obtained by the prioritization methods in each level of the hierarchy, are aggregated into global priorities in the synthesis step. The weighted arithmetic average and the geometric means are the two most used aggregation procedures to obtain the global priorities of alternatives. For example, letωi(i=1,…,m)be the local priorities associated with the criteriai(i=1,…,m), andωij(i=1,…,m;j=1,…,n)be the local priorities associated with the alternativesj(j=1,…,n)with respect to the criteriai(i=1,…,m), then the global prioritiesWj(j=1,…,n)of the alternatives with respect to all criteria is determined byWj=ω1jω1+ω2jω2+⋯+ωmjωm. The synthesis step of H-AHP is the same as AHP.In this section, we define the concepts of hesitant judgments and HCMs, then propose an expected geometric consistency index to measure consistency degrees of individual HCMs, and then give an expected geometric consensus index to measure the consensus degrees of multiple HCMs. Based on the two indices, two Algorithms are proposed to improve consistency and consensus, respectively.Based on the 1–9 scale, the hesitant judgment is characterized by several possible values. The definition is as follows.Definition 1For a paired comparison of the objectives i and j, if a judgment following the 1–9 scale consists of several possible values to represent the dominance of i over j, then it is called a hesitant judgment.For convenience, we use y to indicate a hesitant judgment, and collect all the possible values of y represented asy=(y(l)|l=1,…,|y|), where |y| is the number of possible values in y. So the hesitancy experienced by the DMs in making judgments can be described by these possible values. Generally, all the possible values for a hesitant judgment have the same probability, and the sum of the probabilities is 1. However, the DMs can also specify probability distributions according to their preferences.For the hesitant judgmenty=(y(l)|l=1,…,|y|), let p(l) be the probability of y(l), then y is characterized by the probability distributionsp=p(l)(l=1,…,|y|), and can be rewritten as(5)y=(y(l)(p(l))|l=1,…,|y|)where∑lp(l)=1.Remark 1If a hesitant judgment is represented byy=(y(l)|l=1,…,|y|), then we assume thaty(l)(l=1,…,|y|)have the same probability 1/|y| without explicitly explanation.For a set of objectivesX={x1,x2,…,xn}, assume that the DMs give hesitant judgments over paired comparisons of the objectives, then we can construct a HCM defined as follows.Definition 2Let X be the fixed set, then a HCM is defined asY=(yij)n×n, whereyij=(yij(l)(pij(l))|l=1,…,|y|)is a hesitant judgment representing the dominance of xiover xj. Moreover, yijshould satisfy the following conditions:(6)yijρ(l)yjiρ(l)=1,yii=1,|yij|=|yji|,pijρ(l)=pjiρ(l),i,j=1,2,…,n,and(7)yijρ(l)≤yijρ(l+1),i<jwhereyijρ(l)is the l th possible value in yij,pijρ(l)is the probability ofyijρ(l).If|yij|=1for alli,j=1,2,…,n, it means that there is no hesitancy experienced by the DMs, then the HCM reduces to the comparison matrix.It should be noted that the similar idea of representing the judgments with probability interpretations in AHP has also been considered by other researchers. For example, Rosenbloom (1997) recommended treating the judgments as stochastic variables, and then used a three point discrete approximation to describe the distribution of the stochastic variables. Stam and Silva (1997) argued that the stochastic nature of the judgments can reflect either subjective probabilities that a particular alternative better achieves a given goal, or objective probabilities that reflect uncertain consequences of selecting a particular alternative. Moskowitz, Tang, and Lam (2000) claimed that attempting to make precise judgments can be time consuming and cognitively demanding. They also recommend using stochastic variables to account for inconsistency and imprecision. Hahn (2003) considered that the judgments may be subject to error, and it is nature to use stochastic variables.Although the hesitant judgments can be considered as a special case of the stochastic methods that represent the judgments, we propose this new representation due to its simplicity that captures hesitancy and close relationship with crisp judgments. Moreover, in our practical example, the invited military experts considered that the hesitant judgment is a simple and natural approach to present their uncertainty.We now develop a consistency index for HCMs. The consistency thresholds are established to define the acceptable consistency of HCMs. Then we develop a consistency improving method to improve inconsistent HCMs until they are acceptable.Given a HCMY=(yij)n×n, whereyij=(yij(l)(pij(l))|l=1,…,|yij|). yijare characterized by the probability distributionspij=pij(l)(l=1,…,|yij|),i,j=1,2,…,n. If stochastically choosing a valueyij(l)from each yijfollowing pij, then we can get a comparison matrixY(l)=(yij(l))n×n. According to Eq. (4), we calculate the geometric consistency index of Y(l), denoted byGCIY(l).Let Y be a hesitant-judgment space, then we can define an expected geometric consistency index to measure the consistency degree of Y as follows:(8)E(GCI)Y=(∏i,j=1n1|yij|)∑YGCIY(l)Being consistent with the established thresholds developed by Aguaron and Moreno-Jiménez (2003), ifE(GCI)Y≤GCI¯(n), then Y is with the acceptable consistency.Based on Monte Carlo simulation, the computation of E(GCI)Yis shown in Algorithm 2.Algorithm 2Computing the expected geometric consistency index.Input: A HCMY=(yij)n×n; the maximum value of iterationP=10,000; an initial value of iterationp=1; an initial value of the expected geometric consistency indexE(GCI)Y=0.Step 1:If p ≤ P, then we stochastically get a comparison matrixY(l)=(yij(l))n×nfrom Y; otherwise, go to Step 4.According to Eqs. (3) and (4), we get the priority vectorω=(ω1,ω2,…,ωn), and the geometric consistency indexGCIY(l), respectively.LetE(GCI)Y=E(GCI)Y+GCIY,p=p+1. Go to Step 1.LetE(GCIY)=E(GCPI)Y/P, then end.Output:E(GCIY).Example 1Given a HCM asY1=[1(1,2)1/5(1/3,1/2)(1,1/2)11/7(5,6)5711/2(3,2)(1/5,1/6)21].By Algorithm 2, we haveE(GCI)Y1=0.4681. According to Table 3, sinceE(GCI)Y1>GCI¯(4), then Y1 is inconsistent.Consistency improving is necessary for inconsistent HCMs. Since hesitant judgments can be considered as a special case of the stochastic methods that represent the judgments mentioned in Section 3.1, we develop a stochastic consistency improving method. The procedure is shown in Algorithm 3.Algorithm 3Consistency improving for individual HCMs.Input: A HCM Y; a parameterλ=0.1used in Algorithm 1.Step 1:Let Y be the input of Algorithm 2, then we output E(GCI)Y.IfE(GCI)Y>GCI¯(n), then we stochastically get a comparison matrix Y(l) from Y; otherwise, go to Step 6.According to Eqs. (3) and (4), we get the priority vectorω=(ω1,ω2,…,ωn), and the geometric consistency indexGCIY(l), respectively.Let Y(l) be the input of Algorithm 1, then output an improved comparison matrix of Y(l), denoted by Y′(l).Use Y′(l) to replace Y(l) in Y. Go to Step 1.LetY′=Y, then end.Output: The HCM Y′ with the acceptable consistency.Remark 2Algorithm 3 is convergent.ProofIn Algorithm 3, since the comparison matrix Y(l) which is stochastically obtained fromY=(yij)n×nfollows the probability distributions pij(i,j=1,2,…n), and since Algorithm 1 which improves consistency of Y(l) is convergent according to Xu and Wei (1999), then Y(l) is convergent everywhere, and the limit matrix of Y(l) is also a stochastic comparison matrix. Therefore, Algorithm 3 is convergent, which completes the proof.□Example 2(Continued with Example 1) Since Y1 is not consistent, then we improve its consistency according to Algorithm 3. The results are as follows:Y1′=[1(1,1.5618)0.2135(0.3333,0.5113)(1,0.6403)10.1825(3.7055,6)4.68335.479310.6502(3,1.9559)(0.2699,0.1667)1.53801],E(GCI)Y1′=0.3466. SinceE(GCI)Y1′<GCI¯(4), then the improved HCMY1′is with the acceptable consistency.The consensus reaching process is important to get reliable results with a certain level of agreement among the DMs. As usual, the aggregated judgments that are aggregated by individual judgments are set as the judgments of the group. Then the consensus index can be defined by measuring the distance or the error between individual judgments and the aggregated judgments (Ben-Arieh & Easton, 2007; Ben-Arieh, Easton, & Evans, 2009; Dong et al., 2010). Motivated by this idea, we define a collected HCM to collect all original hesitant judgments provided by the DMs. Then an aggregated comparison matrix is further defined by aggregating all the possible values. Based on the aggregated comparison matrix, a consensus index is defined for HCMs, and a consensus improving method is then proposed.Assume that there are m hesitant judgmentsyi=(yi(l)(pi(l))|l=1,…,|yi|;i=1,…,m), then the definition of collected hesitant judgments is as follows.Definition 3Foryi=(yi(l)(pi(l))|l=1,…,|yi|;i=1,…,m), a collected hesitant judgment is defined asy⌢=(y1′,…,ym′), whereyi′=(yi(l)(pi(l)/m)|l=1,…,|yi|;i=1,…,m)are hesitant judgments with normalized probabilities.Remark 3If there exist same judgments iny⌢, then the probabilities of the same judgments should be added together for the simplicity in presentation.For example, given two hesitant judgments asy1=(2(0.4),3(0.6)),y2=(3(0.4),4(0.6)). According to Definition 3, we can get the collected hesitant judgment denoted byy⌢=(2(0.2),3(0.3),3(0.2),4(0.3)). Since there exists the same judgment 3 iny⌢, then the corresponding probabilities 0.3 and 0.2 should be added together as 0.5. So y can be rewritten asy⌢=(2(0.2),3(0.5),4(0.3)).Further assume that there are m HCMsYk=(yijk)n×n(k=1,…,m), and according to Definition 3, we can get a collected HCM denoted byY⌢=(y⌢ij)n×n, where(9)y⌢ij=(yij1(pij1/m),…,yijm(pijm/m))(i,j=1,2,…,n)are collected hesitant judgments.Example 3Given two HCMs asY1=[1(1,2)1/5(3,1/2)(1,1/2)17(5,6)5711/2(3,2)(1/5,1/6)21],Y2=[13(2,3)(1/4,1/3)1/311/75(1/2,1/3)711/2(4,3)1/521].Then according to Eq. (9), we have the collected HCMY⌢asY⌢=[1(1,2,3)(1,1/2,1/3)1(5,1/2,1/3)7(4(0.25),3(0.5),2(0.25))(1/5(2/3),1/6(1/3))(1/5,2,3)(1/4(0.25),1/3(0.5),1/2(0.25))1/7(5(2/3),6(1/3))11/221].For a hesitant judgmenty=(y(l)(p(l))|l=1,…,|y|)which is characterized by the probability distributionsp=p(l)(l=1,…,|y|), it is convenient to aggregatey(l)(l=1,…,|y|). Lety¯be an aggregated value, then it can be computed by(10)y¯=∏l=1|y|(y(l))p(l)Given a collected HCMY⌢=(y⌢ij)n×n, where each hesitant judgmenty⌢ijis characterized by the probability distributionsp=pij(l)(l=1,…,|y⌢ij|). Then we can get an aggregated comparison matrixY¯=(y¯ij)n×nfromY⌢, where(11)y¯ij=∏l=1|yij|(yij(l))pij(l)For the HCMsYk=(yijk)n×n(k=1,…,m), we can get their aggregated comparison matrixY¯by Eq. (11). Applying Eq. (3) toY¯, we can get the aggregated priorities, denoted byω¯i(i=1,…,n), as follows:(12)ω¯i=(∏j=1ny¯ij)1n∑i=1n(∏j=1ny¯ij)1n(i=1,2,…,n)With respect to one HCMYτ=(yijτ)n×n(τ∈{1,…,m}), letpτ=pijτ(l)(l=1,…,|yijτ|)be the probability distributions that characterizeyijτ(i,j=1,2,…,n). Following pτ, we stochastically get a comparison matrix from Yτdenoted byY(l)=(yij(l))n×n.Based onω¯i(i=1,2,…,n)and Y(l), and according to Eq. (4), the geometric consensus index of Y(l) denoted byGCIY(l)can be computed as(13)GCEIY(l)=2(n−1)(n−2)∑i<jlog2(yij(l)ω¯j/ω¯i)Let Yτbe a hesitant-judgment space, then the consensus degree of Yτcan be defined as the expected value ofGCEIY(l):(14)E(GCEI)Yτ=(∏i,j=1n1|yijτ|)∑YτGCEIY(l)IfE(GCEI)Yτ≤GCI¯(n), then Yτis with the acceptable consensus; otherwise, it is unacceptable. The algorithm to getE(GCEI)Yτby Monte Carlo simulation is shown as follows.Algorithm 4Computing the expected geometric consensus index.Input: A set of HCMs Yk(k=1,…,m); the aggregated priority vectorω¯=(ω¯1,…,ω¯n)associated with Yk(k=1,…,m); the maximum number of iterationP=10000; an initial value of iterationp=1; an initial value of the expected geometric consensus indexE(GCEI)Yτ=0(τ∈{1,…,m}).Step 1:If p ≤ P, then we stochastically get a comparison matrixY(l)=(yij(l))n×nfrom Yτ; otherwise, go to Step 4.According to Eq. (13), we getGCEIY(l).LetE(GCEI)Yτ=E(GCEI)Yτ+GCEIY(l),p=p+1. Go to Step 1.E(GCEI)Yτ=E(GCEI)Yτ/P, then end.Output:E(GCEI)Yτ.(Continued with Example 3) For the two HCMs Y1 and Y2, according to Algorithm 4, we haveE(GCEI)Y1=0.5359andE(GCEI)Y2=0.6759. Obviously,E(GCEI)Y1>GCI¯(4)andE(GCEI)Y2>GCI¯(4). So both Y1 and Y2 are not with the acceptable consensus.Continued with the assumptions and results in Algorithm 4, ifE(GCEI)Yt>GCI¯(n), then the consensus degree of Yτshould be improved to approximate the aggregated comparison matrixY¯. Similar to the stochastic consistency improving method introduced in Algorithm 3, we stochastically get a comparison matrixYτ(l)=(yijτ(l))n×nfrom Yτ, then improve its consensus degree to get an improved comparison matrix denoted byYτ′(l)=(yijτ′(l))n×n, where(15)yijτ′(l)=(yijτ(l))(1−λ)(ω¯iω¯j)λ,(0<λ<1).Further we compute the geometric consensus index ofYτ′(l)as follows:(16)GCEIYτ′(l)=2(n−1)(n−2)∑i<jlog2(yijτ′(l)ω¯jω¯i).IfGCEIYτ′(l)>GCI¯(n), then we continue to improve the consensus degree until it is acceptable. The procedure is shown in Algorithm 5.Algorithm 5Consensus improving for one HCM with respect to a set of HCMs.Input: A set of HCMs Yk(k=1,…,m); the aggregated priority vectorω¯=(ω¯1,…,ω¯n)corresponding to Yk(k=1,…,m).Step 1:Let Yk(k=1,…,m)andω¯=(ω¯1,…,ω¯n)be the inputs of Algorithm 4, then output the expected geometric consensus index of the HCM Yτ(τ∈{1,…,m}), denoted byE(GCEI)Yτ.IfE(GCEI)Yτ>GCI¯(n), then we stochastically get a comparison matrixYτ(l)=(yijτ(l))n×nfrom Yτ; otherwise, go to Step 7.ComputeGCEIYτ(l)according to Eq. (13).CalculateYτ′(l)according to Eq. (15). IfGCEIYτ(l)>GCI¯(n), then go to the next step; otherwise, go to Step 6.LetYτ(l)=Yτ′(l), then go to Step 3.UseYτ′(l)to replaceYτ(l)in Yτ, then go to Step 1.LetYτ′=YτandE(GCEI)Yτ′=E(GCEI)Yτ, then end.Output: The HCMYτ′with the acceptable consistency and the correspondingE(GCEI)Yτ′.Remark 4Algorithm 5 is convergent.ProofAccording to the proof of Algorithms 3, 5 can be proven immediately for the convergence.□Based on Algorithm 5, the consensus improving method for multiple HCMs Yk(k=1,…,m)is shown in Algorithm 6.Algorithm 6Consensus improving for multiple HCMs.Input: A set of HCMs Yk(k=1,…,m).Step 1:Get the collected HCMY⌢from Yk(k=1,…τ,…,m)by Eq. (9), the aggregated comparison matrixY¯by Eq. (11), and the aggregated priority vectorω¯=(ω¯1,…,ω¯n)by Eq. (12).Let Yk(k=1,…,m)andω¯be the inputs of Algorithm 4, then output the expected geometric consensus indices of Yk(k=1,…,m)denoted byE(GCEI)Yk(k=1,…,m).Find a HCM, denoted by Yτ(τ∈{1,…,m}), which has the maximum value amongE(GCEI)Yk(k=1,…,m). IfE(GCEI)Yτ>GCI¯(n), then go to the next step; otherwise, go to Step 6.let Yτandω¯be the inputs of Algorithm 5, then output a HCM denoted byYτ′.LetYτ=Yτ′, then go to Step 1.LetYk′=YkandE(GCEI)Yk′=E(GCEI)Yk, then end.Output: The collected HCMY⌢; the HCMsYk′(k=1,…,m)with the acceptable consensus; and the correspinding expected consensus indicesE(GCEI)Yk′(k=1,…,m).Remark 5Algorithm 6 is convergent.ProofSince the main loops that Algorithms 3 and 5 in Algorithm 6 are convergent, its convergence can be proven immediately.□Example 5(Continued with Example 4) For the two HCMs Y1 and Y2 and by Algorithm 6, we can get the results in the second iteration as follows:Y1′=[1(0.8117,1.7769)0.2146(0.3333,0.4964)(1.2320,0.5628)10.2128(3.8495,5)4.65934.699210.7155(3,2.0144)(0.2598,0.2)1.39761],Y2′=[10.4185(1.6742,2)(0.2750,0.4334)2.389210.26393.0991(0.5973,0.5)3.788710.7230(3.6363,2.3074)0.32271.38311],Y⌢=[1(2.3892(0.25),1.2320(0.25),0.5528(0.5))(4.6593(0.5),0.5973(0.25),0.5(0.25))(3.6363,3,2.3074,2.0144)(0.4185(0.25),0.8117(0.25),1.7769(0.5))1(4.6992,3.7887)(0.3227(0.5),0.2598(0.25),0.2(0.25))(0.2146(0.5),1.6742(0.25),2(0.25))(0.2128,0.2639)1(1.3976,1.3831)(0.2750,0.3333,0.4334,0.4964)(3.0991(0.5),3.8495(0.25),5(0.25))(0.7155,0.7230)1],E(GCI)Y1′′=0.3385andE(GCI)Y2′′=0.3347.Obviously,E(GCI2)Y1′<GCI¯(4)andE(GCI2)Y2′<GCI¯(4), so the improved HCMsY1′andY2′are with the acceptable consistency.Due to the hesitant judgments provided by the DMs are characterized by probability distributions, the derived priorities from HCMs should be with probability interpretations. Therefore, we now develop a new prioritization method for HCMs that can not only produce priorities from HCMs with probability interpretations, but also reflect the original hesitancy experienced by the DMs, which is called hesitant preference analysis.Hesitant preference analysis is a stochastic method to deal with hesitant judgments. This idea is motivated by stochastic multi-criteria acceptability analysis-2 (Lahdelma & Salminen, 2001), which is a multi-criteria decision support method. With respect to the HCM which consists of hesitant judgments, we let it be a hesitant-judgment space, and then analyze this space to obtain the priorities of objectives.With respect to the objectivesxi(i=1,2,…,n), and assume a HCMY=(yij)n×n, whereyij=(yij(l)(pij(l))|l=1,…,|y|), we stochastically obtain a comparison matrixY(l)=(yij(l))n×nfrom Y following the probability distributionspij=pij(l)(l=1,…,|yij|). Letfij(yij(l))be the density function ofyij(l), then the joint probability distribution ofyij(l)(i,j=1,2,…,n)is specified by a density function f(y(l)) represented as a product as follows:(17)f(y(l))=∏i,j=1nfij(y(l))With respect to Y(l) and according to Eq. (3), we can get a priority vector denoted byω(l)=(ω1(l),ω2(l),…,ωn(l)). According to ω(l), we rank the objectives from the best rank(r=1)to the worst rank(r=n)by a ranking function defined as(18)ranki(ω(l))=1+∑k=1nρ(ωk(l)>ωi(l))=rwhereρ(ture)=1andρ(false)=0.Let Y be a hesitant-judgment space and based on the ranking function, the judgments that make the objective xiranking r in the space can be analyzed as follows:(19)Yir(y(l))={y(l)∈Y:ranki(ω(l))=r}Based on Eqs. (17) and (19), we can further measure the acceptable probabilities of xiranking r, which is defined as a rank acceptability indexbir. It is computed as the expected volume ofYir(y(l)):(20)bir=∫Yir(y(l))f(y(l))dy(l)wherebir∈[0,1],∑r=1nbir=1.The Monte Carlo simulation process to computebiris shown in Algorithm 7.Algorithm 7Computing the rank acceptability index.Input: A HCMY=(yij)n×n; the maximum value of iterationP=10,000; an initial value of iterationp=1; initial values of the rank acceptability indicesbir=0(i,r=1,2,…,n).Step 1:If p ≤ P, then we stochastically get a comparison matrixY(l)=(yij(l))n×nfrom Y; otherwise, go to Step 5.According to Eq. (3), we get the priority vectorω=(ω1,ω2,…,ωn).Based on w and according to Eq. (18), we calculate the ranks r of xi(i=1,2,…,n).Letbir=bir+1(i=1,2,…,n),p=p+1. Go to Step 1.Letbir=bir/P, then end.Output:bir.On the basis of the acceptable indices, the acceptable probabilities of the objectives for each rank can be clearly identified. Furthermore, the acceptable probabilities for all ranks of each objective can be aggregated as a holistic evaluation of priorities that are with probability interpretations. Some techniques can be utilized for the aggregation, such as the ordered weighted averaging (OWA) operator (Yager, 1988) which provides a parameterized class of mean operators and has wide applications in practice (Herrera, Herrera-Viedma, & Verdegay, 1996; Rinner & Malczewski, 2002; Yager, 2007; Yager & Kacprzyk, 1997).Let hibe the holistic evaluation of xi, then it is computed as(21)hi=∑r=1nwrbirwhich can be called a holistic priority of xi.Since the higher the ranks, the more important the acceptable probabilities, then the basic requirements for the weights that aggregate the acceptable probabilities should be nonnegative, normalized, and non-increasing when the rank decreases.For the OWA operator, Yager (1988) introduced an attitudinal character defined as(22)AC(w)=∑j=1nn−jn−1wjwhere AC(w) ∈ [0, 1]. This character can be determined by the DMs to reflect their preferences in the aggregation process. IfAC(w)=0, then it corresponds to the vectorw=(0,0,…,1); ifAC(w)=1, thenw=(1,0,…,0); ifAC(w)=0.5, thenw=(1/n,1/n,…,1/n). The bigger the value AC(w), the more the DMs favor the arguments that rank higher in the aggregation. Therefore, to satisfy the basic requirements of the weights, we set 0.5 < AC(w) ≤ 1.Based on the attitudinal character, many models have been developed to determine the weights (Fullér & Majlender, 2001, 2003; Liu, 2006; Liu & Chen, 2004; Liu & Yu, 2012; Majlender, 2005; O'Hagan, 1988). Particularly, Liu (2008) proposed a general model to obtain the weights which can be controlled by the attitudinal character and an objective function. Motivated by the model of Liu (2008), we develop the following model:(23)min∑r=1nF(wj)s.t.∑j=1nn−jn−1wj=AC(w),0.5<AC(w)≤1∑j=1nwj=1,wj≥0,j=1,2,…,nwhere F is a strictly convex function on [0, 1], and at least second-order differentiable.With different values of the attitudinal character AC(w) and different representations of the objective function F(x), we can obtain the desirable weights that emphasize different ranks of objectives. The settings of AC(w) and F(x) will be discussed in detail in Section 5.Based on the discussions above, we give the following procedure of hesitant preference analysis to get the holistic priorities.Step 1:Construct a HCM associated with a set of objectives.Calculate the rank acceptability indicesbir(i,r=1,2,…,n)by Algorithm 7.Require the DMs to determine the value of AC(w) and the representation of F(x), then we calculate the weightswi(i=1,2,…,n)of the objectives according to Eq. (23).Calculate the holistic prioritieshi(i=1,2,…,n)for each objective by Eq. (21).Example 6(Continued with Example 5) Let the collected HCMY⌢in Example 5 be the constructed matrix, we now follow the step by step procedure.LetY⌢be the constructed HCM.Calculatebirby Algorithm 7. The results are listed in Table 4.Since a graphical visualization of rank acceptability indices should be a valuable aid for the DMs, we describe the results in Table 4 by a three-dimensional-column chart in Fig. 1which can also clearly reflect the hesitancy experienced by the DMs in providing their preferences.Assume that the DMs determine thatAC(w)=0.7andF(x)=x2, then according to Eq. (23), we havew1=0.43,w2=0.31,w3=0.19andw4=0.07.According to Eq. (21), we have the holistic prioritiesh1=0.1222,h2=0.2550,h3=0.2980andh4=0.2248.Therefore, we get the priority order of the objectives as x3≻x2≻x4≻x1.

@&#CONCLUSIONS@&#
In this paper, we have developed a hesitant analytic hierarch process (H-AHP) method as an extension of traditional AHP. H-AHP is characterized by hesitant judgments that use several possible values to represent the judgments over paired comparisons of objectives. Different probability distributions can be used to describe hesitant judgments according to the preferences of decision makers. On the basis of hesitant judgments, we have defined a hesitant comparison matrix (HCM) as a basic tool to collect hesitant judgments in H-AHP. To check the consistency degrees of individual HCMs and the consensus degrees of multiple HCMs, we have developed an expected geometric consistency index and an expected geometric consensus index, respectively. The consistency improving and the consensus improving methods of HCMs have also been developed to produce the consistent HCMs. By analyzing hesitant-judgment spaces of HCMs, a new prioritization method, hesitant preference analysis, has been developed to derive holistic priorities of objectives, where the obtained holistic priorities are with probability interpretations. A step by step procedure of H-AHP has been concluded. Following this procedure, we have given a real-life example to illustrate our results. However, more researches related to each aspect of H-AHP can be further explored in the future work.