@&#MAIN-TITLE@&#
Weighted Word Pairs for query expansion

@&#HIGHLIGHTS@&#
A graph of terms can be effectively used for query expansion.Such a graph is extracted from documents thanks to a LDA based methodology.Proposed method achieves good performances on standard datasets.

@&#KEYPHRASES@&#
Text retrieval,Query expansion,Explicit relevance feedback,Pseudo-relevance feedback,Probabilistic Topic Model,

@&#ABSTRACT@&#
This paper proposes a novel query expansion method to improve accuracy of text retrieval systems. Our method makes use of a minimal relevance feedback to expand the initial query with a structured representation composed of weighted pairs of words. Such a structure is obtained from the relevance feedback through a method for pairs of words selection based on the Probabilistic Topic Model. We compared our method with other baseline query expansion schemes and methods. Evaluations performed on TREC-8 demonstrated the effectiveness of the proposed method with respect to the baseline.

@&#INTRODUCTION@&#
Most retrieval systems show relative weaknesses in retrieving relevant documents, especially when few keywords are used to model user information needs. Information retrieval models, that have been proposed through the years, often rely on the bag of words model for document and query representation and can be grouped into three main categories: set-theoretic (including boolean), algebraic and probabilistic models (Christopher, Manning, & Schtze, 2008; Baeza-Yates & Ribeiro-Neto, 1999). It is well known that the “bag of words” model assumes both documents and queries representable as feature vectors. The elements of such vectors can indicate the presence (or absence) of a word or take into account its occurrence frequency, but the information about the position of that word within the document is completely lost (Christopher et al., 2008); then, the elements of the vector are simply weights computed in different ways. In this context, the relevance of a document to a query can be measured as the distance between the corresponding vector representations in the space of features.It has been found that common users are used to perform short queries, 2 or 3 words on average (Jansen, Spink, & Saracevic, 2000; Jansen, Booth, & Spink, 2008). Unfortunately, the shortness of a query can cause common information retrieval systems failures due to the inherent ambiguity of language (polysemy, etc.). Since most text retrieval systems relying on a term-frequency based index generally suffer from low precision (or low quality document retrieval), a typical solution adopted to reduce this query/document mismatch is expanding the initial query using words or phrases with a similar meaning or some other statistical relation to the set of relevant documents (Carpineto, de Mori, Romano, & Bigi, 2001): this strategy is often referred as query expansion.In this work we propose a query expansion method that automatically extracts a set of Weighted Word Pairs from a set of topic-related documents provided by the relevance feedback. Such a structured set of terms is obtained by using a method of term extraction previously investigated in Colace, De Santo, Greco, and Napoletano (2013, 2014), Clarizia, Greco, and Napoletano (2011) and based on the Latent Dirichlet Allocation model (Blei, Ng, & Jordan, 2003) implemented as the Probabilistic Topic Model (Griffiths, Steyvers, & Tenenbaum, 2007).Evaluation has been conducted on TREC-8 repository. We compared the proposed Weighted Word Pairs (WWP) with a method for term extraction based on the Kullback Leibler divergency (Carpineto et al., 2001). Our approach achieves overall better performances and demonstrates that a structured feature representation has a greater discriminating power than a feature vector made of weighted words.According to the Information Retrieval (IR) theory, the representation of queries and documents is based on the Vector Space Model (Christopher et al., 2008): a document or query is a vector of weighted words belonging to a vocabularyT:d={w1,…,w|T|}.Each weightwnis such that0⩽wn⩽1and represents how much the termtncontributes to the semantics of the documentd(in the same way forq). In the term frequency-inverse document frequency (tf-idf) model, the weight is typically proportional to the term frequency and inversely proportional to the frequency and length of the documents containing the term.Given a query, the IR system assigns the relevance to each document of the collection with respect to the query, by using a similarity function as defined in the following:(1)sim(q,d)=∑t∈q∩dwt,q·wt,d,wherewt,qandwt,dare the weights of the term t in the queryqand documentdrespectively.Performance of IR systems can be improved by expanding the initial query with other topics-related terms. These query expansion terms can be manually typed or extracted from feedback documents selected by the user himself (explicit relevance feedback) or automatically chosen by the system (pseudo-relevance feedback) (Baeza-Yates & Ribeiro-Neto, 1999).A general query expansion framework is a modular system including one or several instances, properly chained, of the following modules: Information Retrieval (IR), Feedback (F), Feature Extraction (FE), Query Reformulation (QR).A general scheme is represented in Fig. 1and can be explained as follows. Let us consider a generic IR system and a collection of indexed documentsD. The user performs a search in the IR system by typing a queryq. The IR system computes the relevance of each document of the corpus with respect to the query through the Eq. (1). As a result of the search, a set of ranked documentsΩres={d1,…,dN}⊆Dis returned to the user.Once the result is available, the module F assigns a judgement of relevance, also known as relevance feedback, to each document ofΩres. The relevance can be manually or automatically (pseudo-relevance) assigned. In case of manual, the user provides the explicit feedback by assigning a positive judgment of relevance to a subset of documentsΩfback={d1,…,dM}⊆Ωres. In case of automatic feedback, the module F arbitrarily assigns a positive judgment of relevance to a subset of documents, usually the top M documents retrieved fromΩres.Given the set of relevant documentsΩfback, the module FE selects a set of featuresgthat are then added to the initial queryq. The selected features can be weighted words or more complex structures such as the Weighted Word Pairs proposed in this paper. The Query Reformulation (QR) module adapts the resulting set of featuresgin order to be added to the initial query and then handled by the IR system. The new expanded queryqeis then given as input to the IR system in order to perform a new search. As a result, a new set of documentsΩres={d1′,…,dK′}is retrieved.The query expansion framework described above is quite general. We can use any of the existing IR systems, as well as any of the existing methods of feature extraction, etc. According to this framework, we can make objective comparisons between different system configurations. In this paper we propose a new method of query expansion that use a set of structured features extracted from a minimal relevance feedback. We considered two different open source IR systems: Apache Lucene (Foundation, 2011) that supports structured query based on a weighted boolean model, and the Indri Lemur Toolkit (Ogilvie et al., 2002) that supports an extended set of probabilistic structured query operators based on Inquery. Moreover we compared our feature extraction method with one in the state of the art by considering both the explicit and the pseudo-relevance feedback schemes.The idea of taking advantage of additional knowledge to retrieve relevant documents has been largely discussed in the literature, where manual, interactive and automatic techniques have been proposed (Efthimiadis, 1996; Christopher et al., 2008; Baeza-Yates & Ribeiro-Neto, 1999; Carpineto & Romano, 2012; Na, Kang, Roh, & Lee, 2005).A better specialization of the query can be obtained with additional knowledge, which is typically extracted from exogenous (e.g. ontology, WordNet, data mining) or endogenous knowledge (i.e. extracted only from the documents contained in the collection) (Bhogal, Macfarlane, & Smith, 2007; Christopher et al., 2008).In this work we focus mainly on those query expansion techniques which make use of the relevance feedback. We can distinguish between three types of procedures for relevance assignment: explicit feedback, implicit feedback, and pseudo feedback (Baeza-Yates & Ribeiro-Neto, 1999). The feedback is usually obtained from assessors and indicates the relevance degree for a document retrieved in response to a query. If the assessors know that the provided feedback will be used as a relevance judgment then the feedback is called explicit. Implicit feedback is otherwise inferred from user behavior: it takes into account which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions. Pseudo relevance feedback (or blind feedback) assumes that the top “n” ranked documents obtained after performing the initial query are relevant: this approach is generally used in automatic systems.Since human labeling task is enormously boring and time consuming (Ko & Seo, 2009), most existing methods make use of pseudo relevance feedback. Nevertheless, fully automatic methods can exhibit low performance when the initial query is intrinsically ambiguous. As a consequence, in recent years, some hybrid techniques have been developed which take into account a minimal explicit human feedback (Okabe & Yamada, 2007; Dumais, Joachims, Bharat, & Weigend, 2003) and use it to automatically identify other topic related documents. Such methods uses many documents as feedback, about 40, and achieve a mean average precision of about 30% (Okabe & Yamada, 2007). We will show that the proposed method achieves the same performance of hybrid techniques but using the same minimal explicit feedback.Whatever the technique that selects the set of documents representing the feedback, the expanded terms are usually computed by making use of well known approaches for term selection as Rocchio, Robertson, CHI-Square, Kullback–Lieber, etc. (Carpineto et al., 2001; Carpineto & Romano, 2012; Cao, Nie, Gao, & Robertson, 2008). In this case the reformulated query consists in a simple (sometimes weighted) list of words.Although such term selection methods have proven their effectiveness in terms of accuracy and computational cost, several more complex alternative methods have been proposed, which consider the extraction of a structured set of words instead of simple list of them: a weighted set of clauses combined with suitable operators (Callan, Croft, & Harding, 1992; Collins-Thompson & Callan, 2005; Lang, Metzler, Wang, & Li, 2010; Metzler & Croft, 2007).Others propose methods based on language modeling to integrate several contextual factors in order to adapt document ranking to the specific query context (Bai & Nie, 2008) or to integrate term relationships Bai, Song, Bruza, Nie, and Cao, 2005. The latent semantic analysis (LSA) has been extensively used in information retrieval, especially for term correlations computing Park and Ramamohanarao, 2009.Furthermore, several existing term selection methods use language models combined with exogenous knowledge, like thesaurus(Cao, Nie, & Bai, 2005), wordnet (Zhang, Deng, & Li, 2009; Pinto, Martinez, & Perez-Sanjulian, 2008) or ontology (Bhogal, Macfarlane, & Smith, 2007).The input of the feature extraction module is the setΩfbackand the output is the vectorg={b1,…,b|G|},containing the weights of the|G|word pairs{(v,u)p}. The setGis the vocabulary of word pairs. The entire extraction process is divided into 4 steps, and it is showed in Fig. 2.The input of this step is the set of documentsΩfback={d1,…,dM}, where each document is represented as a vector of weights. Each weight is associated to a word of the vocabularyT. The outputs of this step are:1.the a priori probability that a wordvioccurs inΩfback:πi=P(vi),∀vi∈T;the conditional probability that a wordvioccurs inΩfbackgiven that another wordvsoccurred inΩfback:ρis=P(vi|vs),∀vi,vs∈Tandvi≠vs;the joint probability that a pair of words,viandvj, occurs at the same time inΩfback:ψij=P(vi,vj),∀vi,vj∈Tandvi≠vj.The exact calculation of the a prioriπiand the approximation of the joint probabilityψij, can be obtained by using a smoothed version of the generative model introduced in Blei et al. (2003) called Latent Dirichlet Allocation (LDA), which makes use of Gibbs sampling (Griffiths et al., 2007). Onceπiandψijare known, the conditional probabilityρiscan be easily obtained through the Bayes’ rule.The Latent Dirichlet Allocation (LDA) theory introduced by Blei et al. (2003) and Griffiths et al. (2007), considers a semantic representation in which a document is represented in terms of a set of probabilistic topics z. More formally, let us consider a wordviof a documentdmas a random variable on the vocabularyTand z as a random variable representing one of the topic between{1,…,K}. To obtain a word, the model considers three parameters assigned:α,ηand the number of topics K. Given these parameters, the model choosesθmthroughP(θ|α)∼Dirichlet(α), the topic k throughP(z|θm)∼Multinomial(θm)andβk∼Dirichlet(η). Finally, the distribution of each word given a topic isP(um|z,βz)∼Multinomial(βz). The output obtained by performing Gibbs sampling on a set of documentsΩfbackconsists of two matrixes:1.the words-topics matrixΦthat contains|T|×Kelements representing the probability that a wordviof the vocabulary is assigned to topick:P(u=vi|z=k,βk);the topics-documents matrixΘthat containsK×|Ωfback|elements representing the probability that a topic k is assigned to some word token within a documentdm:P(z=k|θm).The probability distribution of a wordumwithin a documentdmof the corpus can be then obtained as:(2)P(um)=∑k=1KP(um|z=k,βk)P(z=k|θm).In the same way, the joint probability between two wordsumandymof a documentdmof the corpus can be obtained by assuming that each pair of words is represented in terms of a set of topics z and then:(3)P(um,ym)=∑k=1KP(um,ym|z=k,βk)P(z=k|θm).Note that the exact calculation of Eq. (3) depends on the exact calculation ofP(um,ym|z=k,βk)that cannot be directly obtained through LDA. If we assume that words in a document are conditionally independent given a topic, an approximation for Eq. (3) can be written as:(4)P(um,ym)≃∑k=1KP(um|z=k,βk)P(ym|z=k,βk)P(z=k|θm).Moreover, Eq. (2) gives the probability distribution of a wordumwithin a documentdmof the corpus. To obtain the probability distribution of a word u independently of the document we need to sum over the entire corpus:(5)P(u)=∑m=1MP(um)δm,whereδmis the prior probability for each document (∑m=1|Ωfback|δm=1). In the same way, if we consider the joint probability distribution of two words u and y, we obtain:(6)P(u,y)=∑m=1MP(um,yv)δm.Concluding, once we haveP(u)andP(u,y)we can computeP(vi)=P(u=vi)andP(vi,vj)=P(u=vi,y=vj),∀i,j∈{1,…,|T|}.The inputs of this step are the probabilityρisand the value H which is the number of special words (named roots) that will be selected to build the output set{ri}.We define arootas a special word of the vocabularyThaving a high probability to occur given that other words occurred in the setΩfback. Following this model, each word of the vocabulary can be a possible root. In our model we consider a small number of roots,H≪|T|selecting them according to the highest occurrence probability. The choice for the number H is made after a parameter tuning stage. As we will see later in the paper, when the number of documents is small, usually H is equal to 4 or 5.To compute the probability of each root given the remaining words of the vocabulary, we introduce a graphical simplification. For each root, let us consider a directed acyclic graph (dag) that describes the relations between a rootriand the remaining words (vpar(ri)) of the vocabulary, see Fig. 3a. Then, the probability of each root can be computed by using the factorization property:(7)P(ri|vpar(ri))=∏s≠iP(ri|vs)=∏s≠iρis.Once theP(ri|vpar(ri))∀iare computed, we can select the best H roots{ri}, by choosing those that have the highest probability.The inputs of this step are the probabilitiesπi,ψij,ρisand the roots{ri}, while the outputs are two sets of probabilities describing root-root relationsΨroot, and root-words relationsΨiwords,∀ri.Once the H roots have been selected, we have H dags. Starting from these dags we build undirected graphs (ugs) by considering the undirected relations between roots and words instead of directed relations. The ugs are described by the following probabilities:Ψiwords={ψis}s=1,…,T,i≠s∀i=1,…,H.Moreover, we build an undirected graph ug between the H roots, see Fig. 3b. Such a graph describes all the possible associations between pairs of roots. The probabilities associated to this graph are:Ψroots={ψij}i,j=1,…,H,i≠j.Combining the ug between roots and the H ugs between roots and words, we obtain a preliminary version of the weighted words pairs, that is displayed in Fig. 3c. as a graph.The inputs of this step are the setsΨrootsandΨiwords,∀i, while the output is the vectorg={b1,…,b|G|}containing the weights of the|G|word pairs{(v,u)p}.Note that if we choose H roots, we have H(H−1)/2 root-root pairs, while the total number of possible root-word pairs is(|T|(|T|-1)/2)×H. As a consequence, the total number of pairs is H(H−1)/2+(|T|(|T|-1)/2)×H. For instance for H=4 and|T|=100, we have 19806 pairs.The scope of the query expansion is to add some topic related terms to the initial query. If we use the Weighted Word Pairs to expand the query we have to add 19,806 pairs of words that would be not efficient. For this reason, we perform an optimization stage to reduce the total number of pairs. We set a boundary condition of the optimization procedure by considering a maximum number of pairs equal to|G|.The optimization stage, in addition to reduce the number of pairs, allows to neglect weakly related pairs according to a fitness function which is discussed in Appendix A. In particular, our optimization strategy, given the number of roots H and the desired max number of pairs|G|, search for a thresholdλand a set of thresholds{μi}i=1,…,Hfor cutting weak relations. More in details:1.λ: threshold that establishes the number of root-root pairs. A relations between two roots is relevant ifψij⩾λ.μi: threshold that establishes, given a root i, the number of root-word pairs. A relationship between the wordvsand the rootriis relevant ifψis⩾μi.Onceλand{μi}i=1,…,Hare known, the final WWP graph is obtained by selecting the right pairs fromΨrootsandΨiwords,∀i. The graph is composed of|G|words pairs and is represented as a vector of weightsg={b1,…,b|G|}associated to the|G|words pairs{(v,u)p}p=1|G|. Each weightbprepresents the joint probability between two words, namelybp=ψij.A graphical depiction of the WWP is showed in Fig. 3d. In practice this graph is a reduced version of the graph showed in Fig. 3c. Furthermore, in Fig. 4we show the WWP extracted from the topic 402 of TREC-8 “Behavioral genetics”, while in Table 1we show its tabular representation. Double circle represents roots while single circle represents simple words. Dotted lines stand for relations between roots and words, while solid lines stand for relations between roots.In Algorithm 1 we show the pseudo-code of the procedure for graph building. Each function recalled in the pseudo-code has been described above.Algorithm 1WWP graphg={b1,…,b|G|}buildingRequire: Data, M documentsΩfback={d1,…,dM}on the vocabularyTRequire: Parameters,|G|,H,α,η,K{πi,ψij,ρis}∀i,j,s∈T=ProbabilityComputation({d1,…,dM},α,η,K){rh}h=1H=RootSelection({ρis}∀i,s∈T,H)forh=1to HdoΨhwords=RootWordsPairsSelection(rh,{ψhj}∀j∈T)end forΨroots=RootRootPairsSelection({rh}h=1H,{ψij}∀i,j∈T)(λ,{μi}i=1H)=Optimization({d1,…,dM},Ψroots,{Ψhwords}h=1H,|G|)g=GraphSelection(Ψroots,{Ψhwords}h=1H,λ,{μi}i=1H)Once the optimal WWP structure has been extracted from the feedback documents, it must be translated into an expanded query. This process, according to Fig. 1, is called query reformulation and is carried out by considering a WWP graph (Fig. 4) as a simple set of Weighted Word Pairs (see tabular representation of a WWP in Table 1). In fact, at this stage there is no more need to distinguish between roots and simple words, although this hierarchical distinction was fundamental for the structure building process. Note that the query reformulation process depends on the IR system considered.There are several open source libraries providing full-text search features. We have chosen Apache Lucene (Foundation, 2011) and Lemur Project (Ogilvie et al., 2002) since they handle complex query expansions through custom boolean weighted models. Considering Lucene as IR (Foundation, 2011), the WWP plain representation (Table 1) is translated according to Lucene boolean model as follows:(behavioral genetics)ˆ1 OR (condit AND behavior)ˆ0.029 OR (studi AND behavior)ˆ0.055 ….Every word pair is searched with a Lucene boost factor chosen as the corresponding WWP weightψij, while the initial query is added with unitary boost factor (default).When Lemur is used as IR module, WWP plain representation is translated into an expanded query using Indri query language as follows:#weight(0.50 #combine(behavioral genetics) 0.50 #weight(0.029 #band(condit behavior) 0.055 #band(studi behavior) ….Lemur toolkit (Ogilvie et al., 2002) provides belief operators which allow to combine beliefs (scores) about terms, phrases, etc. There are both unweighted and weighted belief operators. With the weighted operators, weights can be assigned to certain expressions in order to control how much of an impact each expression within the query has on the final score.

@&#CONCLUSIONS@&#
In this work we have demonstrated that a Weighted Word Pairs hierarchical representation is capable of retrieving a greater number of relevant documents than a less complex representation based on a list of words. These results suggest that our approach can be employed in all those text mining tasks that consider matching between patterns represented as textual information and in text categorization tasks as well as in sentiment analysis and detection tasks. The proposed approach computes the expanded queries considering only endogenous knowledge. It is well known that the use of external knowledge, for instance Word- Net, could clearly improve the accuracy of information retrieval systems and we consider this integration as a future work.Given the maximum number of roots H and the maximum number of pairs|G|, several WWP structuregtcan be obtained by varying the parametersΛt=(τ,μ)t. To find the best parametersΛtwe perform an optimization procedure that uses a scoring function and a searching strategy. As we have previously seen, agtis a vector of featuresgt={b1t,…,b|G|t}in the spaceGof the words pairs. Each document of the setΩfbackcan be represented as a vectordm=(w1m,…,w|G|m)in the spaceG. A possible scoring function is the cosine similarity between these two vectors:(A.1)S(gt,dm)=∑n=1|G|bnt·wnm∑n=1|G|bnt2·∑n=1|G|wnm2,and thus the optimization procedure would consist in searching for the best set of parametersΛtsuch that the cosine similarity is maximized∀dm. Therefore, the bestgtfor the set of documentsΩfbackis the one that produces the maximum score attainable for each document when used to rankΩfbackdocuments. Since a score for each documentdmis obtained, we have:St={S(gt,d1),…,S(gt,d|Ωfback|)},where each score depends on the specific setΛt=(λ,μ)t. To find the bestΛtwe can maximize the score value for each document, which means that we are looking for the graph which best describes each document of the repository from which it has been extracted. This optimization procedure need to maximize all|Ωfback|elements ofStat the same time. Alternatively, in order to reduce the number of the objectives being optimized, we can at the same time maximize the mean value of the scores and minimize their standard deviation, which turns a multi-objective problem into a one-objective one. Finally the Fitness (F) will be:F(Λt)=ESt-σSt,where E is the mean value of all the elements ofStandσis the standard deviation. By summing up, the best parameters are such that:(A.2)Λ∗=argmaxt{F(Λt)}.As discussed before, the space of possible solutions could grow exponentially. For this reason, we considered|G|≤50. Moreover, since the number of possible values ofΛtis in theory infinite, we clustered each set ofλandμsseparately by using the k-means algorithm. In practice, we grouped all the values ofψijandρisin a few number of clusters. In this way the optimum solution can be exactly obtained after the exploration of all the possible values ofψijandρisused as thresholds.