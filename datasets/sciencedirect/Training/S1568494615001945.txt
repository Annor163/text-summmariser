@&#MAIN-TITLE@&#
An integrated PSO for parameter determination and feature selection of ELM and its application in classification of power system disturbances

@&#HIGHLIGHTS@&#
A new learning method for power system disturbances is introduced using extreme learning machine.Simultaneously optimize the feature subset and model selection for the ELM using PSO.Proposed method can improve convergence accuracy and generalization performance of ELM.

@&#KEYPHRASES@&#
Wavelet transforms,Feature selection,Particle swarm optimization,Extreme learning machine,

@&#ABSTRACT@&#
This paper presents a performance enhancement scheme for the recently developed extreme learning machine (ELM) for classifying power system disturbances using particle swarm optimization (PSO). Learning time is an important factor while designing any computational intelligent algorithms for classifications. ELM is a single hidden layer neural network with good generalization capabilities and extremely fast learning capacity. In ELM, the input weights are chosen randomly and the output weights are calculated analytically. However, ELM may need higher number of hidden neurons due to the random determination of the input weights and hidden biases. One of the advantages of ELM over other methods is that the parameter that the user must properly adjust is the number of hidden nodes only. But the optimal selection of its parameter can improve its performance. In this paper, a hybrid optimization mechanism is proposed which combines the discrete-valued PSO with the continuous-valued PSO to optimize the input feature subset selection and the number of hidden nodes to enhance the performance of ELM. The experimental results showed the proposed algorithm is faster and more accurate in discriminating power system disturbances.

@&#INTRODUCTION@&#
The power-quality (PQ) study has become a more important subject recently. According to established reports, the industry is losing huge amount of resources due to power outages and PQ problems. PQ may be defined as the continuous availability of electric power that confirms to accepted standards of phase, frequency, and voltage. Power system disturbance is defined as a variation in these standards. The PQ issues and related phenomena can be attributed to the use of solid-state switching devices, unbalanced and non-linear loads, industrial grade rectifiers and inverters, computer and data processing equipments, etc. which are being increasingly used in both the industry and home appliances. These devices introduce distortions in the phase, frequency and amplitude of the power system signal thereby deteriorating PQ. Subsequent effects could range from overheating, motor failures, inoperative protective equipment to power inrush, quasi static harmonic distortions and pulse type current disturbances. To ensure the PQ, power disturbances detection becomes important to find the location and disturbance types. Traditionally, PQ was judged by visual inspection of the disturbance waveforms, so the engineer's knowledge plays a critical role. As always, the PQ engineer is inundated with an enormous amount of data for inspect. Thus it is desirable to develop automatic methods for detecting, identifying, and analyzing various disturbances [1–3]. Such classification methods include two major steps namely the feature extraction using signal processing techniques and classification using application of a statistical or neural classifier to the constructed feature vectors. In this regard wavelet transform (WT) [4–7] plays a significant role in the detection and localization of the power signal disturbances. However, WT alone cannot accurately classify the various types of disturbances occurring in the power signal. In order to identify the type of disturbance present in the power signal effectively, several authors have presented different methodologies based on combination of wavelet transform and artificial neural network (ANN). Using the properties of WT and the features of the decomposed waveform along with ANN algorithm, it is possible to extract important information from a disturbance signal and determine the type of disturbance that caused power quality problem to occur. The energy of the distorted signal will be partitioned at different resolution levels in different ways depending on the power quality problem in hand. The standard deviation can be considered as a measure of the energy of a signal with zero mean [8].In the past two decades, support vector machine (SVM) [9] has become an increasingly popular technique in machine learning domains due to demonstrated, commendable superiorities compared with many other learning methods. However the main obstacle of SVM for practical use is that a rather complex quadratic programming problem must be solved and this is demanding due to its high computational complexity and computing time cost. When the datasets are larger, this defect is more obvious. SVM as a binary classifier tries to map the data from a lower-dimensional input space to a higher-dimensional feature space so as to make the data linearly separable into two classes [10,11]. When using the one versus-all approach to make binary classifiers applicable to multicategory problems, C (number of classes) binary classifiers should be built for SVM to distinguish one class from all the rest of the classes. Similarly, when using the one-versus-one comparison approach, C(C−1)/2 binary classifiers should be built for SVM to distinguish between every two class combination. Thus, it can be seen that, when the number of classes C increases, the complexity of the overall classifier also increases. Furthermore, proper kernel and its parameter selection are still open problems for further research.Artificial neural network methods provide an attractive alternative to the above approach for a direct multicategory classification problem [12]. Neural networks can map the input data into different classes directly with one network. Besides, neural networks have attractive virtues such as flexibility, parallel computation, easy implementation and more cognitive like human thinking, etc. However, there are also some problems that remain unsolved, for example, lacking of efficient and fast learning algorithm for large scale problems and hidden layer neuron number selection problem. The learning speed of feed forward neural networks is in general far slower than required and it has been a major bottleneck in their applications [13]. Two key reasons may be: (1) the slow gradient-based learning algorithms are extensively used to train neural networks and (2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Of the many neural networks has been proposed, single layer feed forward network (SLFN) with sigmoid or radial basis activation function is found to be effective in solving a number of real world problems. Normally, the free parameters of the network are learnt from the given training samples using gradient descent algorithms. The gradient descent algorithms are relatively slow and have many issues related to its convergence.In order to overcome these problems, a novel learning algorithm for single hidden layer feed forward neural networks called extreme learning machine was proposed recently [14–16]. In ELM, the input weights (linking the input layer to the hidden layer) and hidden biases are randomly chosen, and the output weights (linking the hidden layer to the output layer) are analytically determined by using Moore–Penrose (MP) generalized inverse. ELM not only learns much faster with higher generalized performance than the traditional gradient-based learning algorithms but also avoids many difficulties faced by gradient-based learning methods such as stopping criteria, learning rate, learning epochs and local minima [17]. The ELM appears to be suitable in applications which request fast prediction and response capability. The learning speed of the ELM classifier is very fast making it suitable for real time power signal disturbance monitoring task.The main contribution of this paper is the thorough experimental exploration of the ELM for power system disturbances classification. Further the performances of the ELM approach in terms of classification accuracy are evaluated: (1) by automatically detecting the best discriminating features from the whole considered feature space and (2) by solving the model selection issue. One of the advantages of ELM over other methods is that the parameter that the user must properly adjust is the number of hidden nodes only. But the optimal selection of its parameter (hidden neurons) can improve its performance [18,19]. PSO can help to optimize the parameters of the ELM algorithm to obtain higher classification accuracy. Therefore, a, novel PSO based wrapper model is proposed to obtain the optimal number of hidden nodes and to select the beneficial subset of features which result in a better classification of power disturbances. Moreover, PSO has some advantages over other similar optimization techniques such as genetic algorithm (GA), namely the following. (a) PSO has the control parameter for the balance between the global and local exploration of the search space. This feature enhances the search capability of PSO. (b) PSO has memory, namely, information of good solutions is retained and shared by all particles; whereas in GA, previous knowledge is destroyed once the population changes. (c) PSO exhibits algorithmic simplicity. The GA consists of three major operators, selection, crossover and mutation. However, PSO comes with a single operation of velocity calculation. (d) PSO has a simple implementation and this induces reduction of computation and eliminates the necessity to select the best operator for a given optimization. (e) Quite often PSO is superior in terms of convergence, speed, and accuracy than other biologically inspired optimization algorithms. The proposed method is applied on features extracted from raw PQ signal using discrete wavelet transform.This paper is organized as follows. Section 2 describes the wavelet based feature extraction technique for power system disturbances classification. Section 3 presents the recently developed ELM algorithm and issues related to the ELM algorithm for classification problems. Section 4 discusses the particle swarm optimization algorithm for selecting the feature subset and ELM parameters. Section 5 presents the architecture of the proposed PSO-ELM multiclass classification technique with the particle swarm optimization algorithm for selecting the feature subset and ELM parameters. Performance evaluation of the proposed approach over existing methods in the literature using ten different types of power system disturbances data sets are presented in Section 6. Finally, Section 7 summarizes the conclusions based on this study.Wavelet transform has been proven to the very efficient in signal analysis. It finds applications in different areas of engineering due to its ability to analyze the local discontinuities of signals. The main advantages of wavelets is that they have a varying window size, being wide for slow frequencies and narrow for the fast ones, thus leading to an optimal time–frequency resolution in all the frequency ranges. The use of wavelets for PQ studies is most appropriate because most disturbances on a power supply system are non-stationary transients [20]. Basically, a wavelet is a function ψ∈L2(R) with a zero average.(1)∫−∞+∞ψ(t)dt=0The continuous wavelet transform (CWT) of a signal x(t) is then defined as(2)CWTψx(a,b)=1|a|∫−∞+∞x(t)ψ*t−badtwhere ψ(t) is called the mother wavelet, the asterisk denotes complex conjugate, a and b (a,b∈R) are scaling (dilation) and translation parameters, respectively. The scale parameter a will decide the oscillatory frequency and the length of the wavelet, the translation parameter b will decide its shifting position. In practical applications, the continuous wavelet function can be transferred to a discrete form via sampling. In (2), the factors a and b are replaced byaomandnboaom,respectively.(3)ψm,n(t)=ao−m/2ψ(ao−mt−nbo)where m indicating frequency localization and n indicating time localization. Generally, one can choose ao=2 and bo=1. This choice will provide a dyadic-orthonormal wavelet transform and provide the basis for multiresolution analysis.In multi-resolution analysis (MRA), wavelet and scaling functions are used as building blocks to decompose and construct the signal at different resolution levels. The main goal of MRA is to develop representations of a signal at various levels of resolution. MRA is composed of 2 filters in each level which are low pass filter (LPF) and high pass filter (HPF) [21].A set of orthogonal scaling function ϕ(t) and wavelet function ψ(t) can divide function space into a series of orthogonal high-frequency and low-frequency spaces [22]. The translated and scaled version of the wavelet used in MRA is time–frequency picture of the decomposed signal f(t). The original signal f(t) can be decomposed to:(4)f(t)=∑kcj(k)ϕjk(t)+∑j=1k∑kdj(k)ψjk(t)where j is the level number of the wavelet decomposition. cjand djare the weighted sequences of the space projection of f(t), respectively. The decomposition for four levels is shown Fig. 1.The feature extraction is very important in signal processing operations, because the rough and large data sets cause difficulties, when a network is trained. DWT is an excellent method which enhances the disturbance and reduces the data to coefficients, corresponding to the similarity of the signal to the wavelet at a particular location and time-scale. Ten kinds of power quality disturbances are considered in this paper. They are voltage oscillatory transients, voltage sags, voltage swells, harmonic distortions, interruptions, flicker, sag with harmonics, swell with harmonics and notch. The power quality signals corresponding to these ten classes are generated in Matlab using parameterized models with different parameter values [7]. The power quality disturbance signals of ten cycles (50Hz, 10/50=0.2s) were sampled at 256points/cycle and the sampling frequency was 12.8kHz. The fourth order Daubechies wavelet (db4) was adopted to analyze the sampled disturbance signals since it has the best similarity to power disturbance signals. The sampled disturbance signals were decomposed with 13 levels MRA to ensure that all disturbance features are extracted in both high and low frequency spectrum. The first scale signal has frequency range of f/2 to f/4, where f is the sampling frequency of the time domain disturbance signal. The second, third, fourth, fifth and sixth signals have the frequency ranges of f/4 to f/8, f/8 to f/16, f/16 to f/32, f/32 to f/64 and f/64 to f/128, respectively. One approximation and 13 detail coefficients have been obtained with 13-level decomposition for each disturbance signal. It is sufficient to use only the detail coefficients of the 13-level decomposition (cD1, cD2, cD3, cD4, cD5, cD6, cD7, cD8, cD9, cD10, cD11, cD12 and cD13). Thus, 13 feature sets (each set has 2560 coefficients) are obtained from only one power disturbance signal. These features will be used for classification but the number of detail coefficients is excessive for the training of classifiers. So it is necessary to reduce data set in size.Fig. 2shows the plot of level 1, level 2 and level 3 DWT coefficients for the disturbances harmonics, interruption, flicker and pure sine wave. The coefficients show variation for the disturbance interruption from which the disturbance duration can be determined but there is no variation for pure sine wave because the signal is smooth.To reduce data set in size, the wavelet energy criterion is applied to wavelet detail coefficients. The wavelet energy is the sum of square of wavelet transform coefficients. The energy at each decomposition level is calculated using the following equations:(5)EDi=∑j=1N|Dij|2,i=1…l(6)EAl=∑j=1N|Alj|2where i=1…l is the wavelet decomposition level from level 1 to level l. N is the number of the coefficients of detail or approximate at each decomposition level. EDiis the energy of the detail at decomposition level i and EAlis the energy of the approximate at decomposition level l. Thus for an “l” level decomposition, the feature vector adopted is of length l and is denoted by ED1ED2ED3…EDl. In the proposed work, the PQ signal is decomposed up to 13 levels, thereby making the feature vector dimension 13.Fig. 3illustrates the Wavelet energy distribution of Voltage harmonics and interruption. Differences between these energy patterns provide the differentiation of features. These distinctive features are important for the performance of classifier.This paper investigates a comparative study of three other classification techniques for power disturbances classification. The techniques include the back propagation network (BPN), probabilistic neural network (PNN) and the SVM neural network classifiers. Following briefly describes these classifiers.A two layer feed forward neural network is used for learning the feature vectors. The training parameters and the structure of the BPN used in this study are given as follows. They were selected to obtain best performance, after conducting several different experiments, such as the number of hidden layers, the size of the hidden layers, value of the learning rate, and type of the activation functions. The tan-sigmoid function is used as the transfer function in the hidden-layer neurons and the number of hidden neurons is selected as 10. The output layer is comprised of one neuron to identify the disturbance class. The transfer function used in the output layer is purelin because the output should indicate the classes from 1 to 10. Levin berg-Marquardt training function (TrainLM) is used to speed up the training process. The network is trained with a back propagation algorithm. The error measure is given as(7)E=∑j=1Q∑k=1M(dkj−xkj)2where Q is the number of training samples, M the number of output neurons, and dkjis the target value. The weights are updated as(8)Δwk+1=−η∇wE+αΔwkwhere∇wEthe gradient of E with respect to w, and α is the momentum constant and η is the learning rate.The gradient-descent algorithm was implemented in batch mode. The performance of a gradient-descent algorithm is very dependent on the learning rate. If the learning rate were too large, the training would oscillate back and forth. If the learning rate were too small, it would take a long time to reach convergence. To overcome this problem an adaptive learning rate that attempts to keep the step size as large as possible without causing oscillation is used. The learning rate is made responsive to the complexity of the local error surface. The learning rate used in this work is 0.1.The PNN is fundamentally based on the well known Bayesian classifier technique commonly used in many classical pattern recognition systems. PNN is a kind of feed forward neural network. The PNN structure is a direct neural network implementation of Parzen nonparametric probability density function (pdf) estimation and Bayes classification rule [23]. The standard training procedure of PNN requires a single pass-over all the patterns of the training set [24]. The learning speed of the PNN model is very fast, making it suitable for fault diagnosis and signal classification problems in real time.It is a four-layer feed forward neural network that is capable of realizing or approximating the optimal classifier. The first layer of the PNN, designated as an input layer, accepts the input vectors to be classified. The nodes of the second layer, which is designated as a pattern layer, are grouped in K groups depending on the class kithey belong. These nodes, also referred to as pattern units or kernels, are connected to all inputs of the first layer. Although numerous probability density function estimators are possible, here the Gaussian basis function is used as the activation function.(9)fij(x;cij,σ)=1(2π)d/2σdexp−12σ2(x−cij)T(x−cij)where i=1, …, k, j=1, …, Miand Miis the number of pattern units in a given class ki. Here, σ is the standard deviation, also known as spread or smoothing factor. It regulates the receptive field of the kernel. The value of this parameter has a profound influence on the performance of a PNN. While too small values will cause individual training cases to have too much of an influence, thereby losing the benefit of aggregate information, extremely large values will cause so much blurring that the details of density will be lost, often distorting the density estimate badly. The input vector x and the centers cij∈Rdof the kernel are of dimensionality d. Finally, exp stands for the exponential function, and the superscript T denotes the transpose of the vector. Obviously, the total number of the second-layer nodes is given as a sum of the pattern units for all classes.(10)M=∑i=1kMiNext, the weighted outputs of the pattern units from the second layer that belong to the group kiare connected to the summation unit of the third layer (designated as summation layer) corresponding to that specific class ki. The weights are determined by the decision cost function and the a priori class distribution. In the general case, the positive weight coefficients wijfor weighting the member functions of class kihave to satisfy the requirement(11)∑j=1Miwij=1for every given classki,i=1,…,kConsequently, each node of the summation layer estimates the class-conditional probability density function pi(x|ki) of each class ki, defined as(12)pi(x|ki)=1(2π)d/2σd1Mi∑j=1Miexp−12σ2(x−xij)T(x−xij)where xijis the jth training vector from class ki, x is the test input vector, d is the dimension of the speech feature vectors, and Miis the number of training patterns in class ki.Thus, in the output layer of the PNN, also known as competitive layer, the Bayesian decision rule is applied to distinguish the class ki, to which the input vector x belongs.(13)D(x)=argmaxip(ki)pi(x|ki),i=1,…,kSVM performs classification tasks by constructing optimal separating hyper-planes (OSH). OSH maximizes the margin between the two nearest data points belonging to two separate classes [25]. So the following inequality is valid for all input data:(14)yi(wTxi+b)≥1,for allxi,i=1,2,…,lThis is a convex quadratic programming (QP) problem and Lagrange multipliers are used to solve it. Those training points, for which the equality in (14) holds, are called support vectors (SV). After solving problem, the optimal bias is given by:(15)b*=yi−w*Txifor any support vector xi. The optimal decision function (ODF) is then given by:(16)f(x)=sgn∑i=1lyiαi*xTxi+b*whereαi*'sare optimal Lagrange multipliers. SVM uses soft margin with non-negative slack variables for high noise level input is expressed as follows ξi, i=1, 2, …, l.(17)yi(wTxi+b)≥1−ξifori=1,2,…,lTo obtain the OSH, it should be decrease theϕ=(1/2)||w||2+C∑i=1lξiksubject to (17), where C is the penalty parameter, which controls the tradeoff between the complexity of the decision function and the number of training examples, misclassified.In the nonlinearly separable cases, the SVM map the training points, nonlinearly, to a high dimensional feature space using kernel function K(xi, xj), where linear separation may be possible. The kernel functions of SVMs are linear, polynomial, sigmoid and Gaussian radial basis function (GRBF) with the following equation:(18)K(xi,xj)=exp−||xi−xj||22σ2where σ is the parameter of the GRBF kernel function. After a kernel function is selected, the QP problem will change and after training, the decision function becomes,(19)f(x)=sgn∑i=1lyiαi*K(x,xi)+b*The performance of SVM can be controlled through the term C and the kernel parameter which are called hyperparameters. These parameters influence on the number of the support vectors and the maximization margin of the SVM.ELM is a single hidden layer feed-forward network, where the input weights are chosen randomly and the output weights are calculated analytically. Activation functions such as sigmoid, sine, Gaussian, and hard-limiting functions can be used for the hidden neuron layer, while linear activation function is used for the output neurons [26,27]. ELM uses no differential or even discontinuous functions as an activation function.Let V be H×n input weights and b be H×1 bias values for each hidden neuron and W be C×H output weights. For a multicategory classification (C-distinct classes) problem with N observations {Xi, Ti, i=1, 2, …, N} the outputs of the ELM network with H hidden neurons are defined as(20)yk=∑j=1HwkjGj(V,b,Xi),k=1,2,…,Cwhere Gj(·) is the output of the jth hidden neuron, and Gj(·) is the activation function. For sigmoidal hidden neurons, the output of the jth hidden neuron Gj(·) is defined as(21)Gj(V,b,Xi)=tanhbj+∑k=1Nvjkxik,j=1,2,…,HSimilarly, for the radial basis function, output Gj(·) is defined as(22)Gj(V,b,Xi)=e||Xi−Vj||/2bj2Note that in the radial basis function, the term b acts as the width of the Gaussian hidden neuron. Eq. (20) can be written in matrix form as(23)Y=WYhwhere Yhis an H×n matrix, which is defined as(24)Yh=G1(V,b,X1)G1(V,b,X2)⋯⋯G1(V,b,XN)⋮⋮⋮GH(V,b,X1)GH(V,b,X2)⋯⋯GH(V,b,XN)The targettikis defined as(25)tik=1ifci=k,−1otherwise,k=1,2,.…,Cwhere ciis the class label for Xi.In the ELM algorithm, the input weights (V) and bias (b) are chosen randomly for a given number of hidden neurons. By assuming the network output (Y) is equal to the coded class label (t), the output weights (W) are calculated analytically as(26)W=YYh+whereYh+is the Moore–Penrose pseudo inverse of matrix Yh.In summary, the ELM algorithm consists of the following steps:1.Select the number of hidden neurons and a suitable activation function for a given problem.Randomly choose the input weight (V) and bias (b).Analytically calculate the output weight (W).Use the calculated weights (W, V, b) for estimating the class label.The estimated class label is calculated as(27)Cˆi=argmaxk=1,2,…,CyikThe behavior of ELM classifier with respect to the initial parameters changes considerably with the number of hidden neurons [26]. To illustrate this behavior, an experiment is conducted by varying the number of hidden neuron from 10 to 120 in steps of 10. The variation in the training and testing accuracies are given in Table 1. The mean classification efficiencies are calculated for different number of hidden neurons. From Fig. 4(a), the training efficiency increases with increase in number of hidden neurons and reaches a maximum at 100 hidden neurons. From Fig. 4(b), the testing/generalization efficiency increases up to certain number of hidden neurons and start decreasing afterwards. The testing efficiency reaches a maximum when the numbers of hidden neurons are between 90 and 110.Here, one has to find the optimal number of hidden neurons and the corresponding wiand B such that the analytically calculated output weights ensure better generalization ability of the ELM network.The PSO algorithm belongs to the category of swarm intelligence techniques, which are inspired by the social behavior of flocking animals such as birds, fish, and ants. PSO [28] was first developed and introduced as a stochastic optimization algorithm. It is a population-based algorithm that exploits a population of individuals to probe for promising regions in a search space. An individual's behavior is affected either by the best personal or best global individual. The performance of each individual is measured using a fitness function similar to evolutionary algorithms. The population is referred to as a swarm and individuals are called particles. The particle moves in a multidimensional space with adaptable velocity. In PSO, the particles remember the best position in the past and the best position ever attained by the particles. This property helps the particles to search the multidimensional space faster.Let us consider a swarm of size S. Each particle Pi(i=1, 2, …, S) in the swarm is characterized by: (1) its current position pi(k)∈ℜd, which refers to a candidate solution of the optimization problem at iteration k; (2) its velocityvi(k)∈ℜd;and (3) the best position Pbesti(k)∈ℜdidentified during its past trajectory. Let Gbest(k)∈ℜdbe the best global position found over all trajectories traveled by the particles of the swarm. Position optimality is measured by means of one or more fitness functions defined in relation to the considered optimization problem. During the search process, the particles move according to the following equations:(28)vi(k+1)=wvi(k)+cr11(Pbesti(k)−xi(k))+c2r2(Gbest(k)−xi(k))(29)xi(k+1)=xi(k)+vi(k+1)In the velocity updating process, the values of parameters such as w, c1 and c2 should be determined in advance. The constants c1 and c2 represent the weighting of the stochastic acceleration terms that pull each particle toward the Pbestiand Gbest positions.In greater detail, these parameters are considered as scaling factors that determine the relative pull of the best position of the particle and the global best position. Sometimes, they are referred as the cognitive and social rates, respectively. They are the factors determining how much the particle is influenced by the memory of its best location and by the rest of the swarm, respectively. The inertia weight w is used as a tradeoff between the global and local exploration capabilities of the swarm. Large values of this parameter permit better global exploration, while small values lead to a fine search in the solution space. Eq. (28) allows the computation of the velocity at iteration t+1 for each particle in the swarm by combining linearly its current velocity (at iteration t) and the distances that separate the current particle position from its best previous position and the best global position, respectively. The particle position is updated with Eq. (29). Both (28) and (29) are iterated until convergence of the search process is reached. Typical convergence criteria are based on the iterative behavior of the best value of the adopted fitness function(s) or/and simply on a user-defined maximum number of iterations.This section describes the proposed PSO-ELM system for the classification of power system disturbances. As mentioned in Section 1, the aim of this system is to optimize the ELM classifier accuracy by automatically: (1) detecting the subset of the best discriminative features (without requiring a user-defined number of desired features) and (2) solving the ELM model selection issue (i.e., estimating the best value of the number of hidden nodes). To attain this, the system is derived from an optimization framework based on PSO as shown in Fig. 5.Given a training set {Xi, Yi}, hidden node output function G(W, B, X), maximum number Lmax of hidden nodes,Step 1: Select the optimal number of input features (i.e., input nodes N) and hidden nodes (L) using PSO algorithmStep 2: Randomly assign hidden nodes parameters (Wi, Bi), i=1, 2, …, LStep 3: Calculate the hidden layer output matrix YHStep 4: Calculate the output weightVˆ=YYH+.whereYH+is the Moore–Penrose generalized pseudo-inverse of the hidden layer output matrixIn order to effectively use particle swarm optimization algorithm for ELM optimization, the binary coding system was used to represent the particle. Fig. 6shows the binary particle representation used in this work. The particle therefore, is comprised of two parts, (1) A candidate subset f of features among the d available input features (discrete-valued) and (2) the value of the number of hidden neurons (continuous-valued). Since the first part of the position vector implements a feature detection task, each component (coordinate) of this part will assume either a “0” (feature discarded) or a “1” (feature selected) value.pf1∼pf13represents the feature mask andph1∼phncrepresents the value of the number of hidden neurons, ncis the number of bits representing the number of hidden neurons. A seven bit coding is employed in the present study to encode the number of hidden neurons and thirteen bits for feature mask. In this work the total length l of the bit string is chosen as twenty.Let f(i) be the fitness function value associated with the ith particle pi. The choice of the fitness function is important because it is on this basis that the PSO evaluates the goodness of each candidate solution pifor designing PSO-ELM classification system. Classification accuracy and the numbers of selected features are the two criteria used to design a fitness function. Thus, for the particle with high classification accuracy and a small number of features produce a high fitness value. Solve the multiple criteria problem by creating a single objective fitness function that combines the two goals into one. As defined by formula (30), the fitness has two predefined weights: (i) wAfor the classification accuracy; and (ii) wFfor the selected feature. The weight accuracy can be adjusted to a high value (such as 100%) if accuracy is the most important. The particle with high fitness value has high probability to effect the other particles’ positions of the next iteration, so it should be appropriately defined.(30)fitnessi=wA×acci+wF×1−∑j=1nFfjnFwAis the weight for the ELM classification accuracy acci, wFthe weight for the number of selected features, fjthe value of feature mask “1” represents that feature j is selected and “0” represents that feature j is not selected, and nFis the total number of features. For the fitness definition, the classification accuracy (acc) or hit rate denoting the percentage of correctly classified examples is evaluated by Eq. (31). The numbers of correctly and incorrectly classified examples are indicated by cc and uc, respectively.(31)acc=cccc+uc×100To precisely establish a PSO-based feature selection and parameter optimization system, the following main steps (as shown in Fig. 7) must be proceeded. The detailed explanation is as follows:Step (1) Generate randomly an initial swarm of size S.Step (2) Initialize randomly the velocity vectorsvi(i=1,2,…,S)associated with the S particlesStep (3) For each position pi∈ℜd+1 of the particle pi(i=1,2,…,S) from the swarm, train an ELM classifier and compute the corresponding fitness function fitnessi.Step (4) Set the best position of each particle with its initial position, i.e.,pbi=pi,(i=1,2,…,S)Step (5) Detect the best global position pgin the swarm exhibiting the maximum value of the considered fitness function over all explored trajectories.Step (6) Update the velocity of each particle usingvi(t+1)=wvi(t)+cr11(pbi(t)−pi(t))+c2r2(pg(t)−pi(t))where w is an inertia weight factor. r1 and r2 are random variables from the range [0,1]. c1 and c2 are acceleration constants used for regulating the relative velocities with respect to the best personal and global positions respectively.Step (7) Update the position of each particle usingpi(t+1)=pi(t)+vi(t+1)Step (8) For each candidate particle pi(i=1,2,…,S), train an ELM classifier and compute the corresponding fitness function fitnessi.Step (9) Update the best position pbiof each particle if its current position pi(i=1,2,…,S) has a maximum fitness function.Step (10) If the maximum number of iterations is not yet reached, return to step 5, else go to step 11.Step (11) Select the best global positionpg*in the swarm and train an ELM classifier fed with the subset of detected features mapped bypg*and modeled with the number of hidden neurons encoded in the same position.Step (12) Classify the power system disturbances signals with the trained ELM classifier.The proposed experimental framework was articulated around the following three main experiments.The first experiment aimed at assessing the effectiveness of the ELM approach in classifying power system disturbance signals directly in the whole original feature space (i.e., with all the 13 available features). The second experiment was devoted to analyze the generalization capability of the ELM and the SVM, classifiers with feature reduction based on a PSO. The third experimental part as an objective to assess the capability of the proposed PSO-ELM classification system to boost further the accuracy of the ELM classifier using automatic feature detection and model selection-oriented optimization process. The described experiments were implemented using the MATLAB7.5 environment, on a Pentium duo core, 2.88GHz, with 3GB of memory.Ten classes (C1–C10) of different PQ disturbances are taken for classification and they are as follows: C1 – Voltage Sag, C2 – Voltage Swell, C3 – Interruption, C4 – Harmonics, C5 – Flicker, C6 – Oscillatory Transient, C7 – Sag with harmonics, C8 – Swell with Harmonics, C9 – notch and C10 – Spike. The power quality signals corresponding to these ten classes are generated in Matlab using parameterized models with different parameter values. Data generation by parametric equations for classifiers tests has advantageous in some ways. It was possible to change training and testing signal parameters in a wide range and in a controlled manner. The signals simulated with this way were very close to the real situation [23,24].In the practical power systems, the characteristics of each event may vary significantly. Therefore, to study the effects of such variations, 1000 different cases were generated. Fifty cases for training and another fifty cases were generated for testing for each class of events by randomly changing various parameters. The parameters used to vary the classes of events are depth, angle, starting time and duration of the events.The depth of the event is the change in the amplitude of a signal. The angle represents the phase shift at which the signal is captured. The starting time is the time at which the event starts. The duration is the time period of the event.In this study the above parameters were varied according to the IEEE recommended practice in [29]. For example, for the sag events, the four parameters were varied randomly. The depth of the sag is varied from 10% to 90% of the magnitude of the pure sine waveform. The angle of the signal is varied from 0% to 100% of the entire period (which is a realistic assumption since the captured waveforms in a practical monitoring system could have a phase shift that may vary from 0 to 2π). In addition, the starting time of the sag is varied from 0% to 80% of the total length of the signal. Moreover, the duration of the sag is varied from 10% to 100% of the total length of the waveform. In this study, each event accommodates 10 cycles of the captured signal. Therefore, duration of 10% corresponds to one cycle of the waveform.For the swell event, the magnitude of the pure sine waveform was increased between 10% and 90%. For the harmonics events, 2nd, 3rd, 5th, 7th and 9th harmonics are used to randomly contaminate the ideal waveforms. In the case of the flicker events, the amplitude of the simulated signals was changed periodically to introduce the effect of a flicker. To achieve this, the magnitude of the target waveform was varied as a function of another sine wave. This results an oscillation in the amplitude of the target waveform, which varied randomly from 50% to 70% of the fundamental frequency. The parameters which were changed randomly in the case of transient events are the oscillation frequency of the transients (varied from 10 times to 15 times of the fundamental frequency) and the amplitude of the overshoot (varied up to 150% of the amplitude of the pure signal). The signal having both sag and swell along with harmonics are considered as separate classes in this study. In addition to these PQ events, notch is also considered, which is of short duration as compared to sag or swell. The voltage notch phenomenon is a repetitive event.Both the training and testing signals are sampled at 256points/cycle and the normal frequency is 50Hz. Ten power frequency cycles which contain the disturbances are used for a total of 2560 points. Wavelet transform of these data samples are then performed to decompose the signals up to 13th level.The first experiment aimed at assessing the effectiveness of the ELM approach in classifying power system disturbance signals directly in the whole original feature space (i.e., with all the 13 available features). For comparison purpose, three other classification approaches, namely, the back propagation network, probabilistic neural network and the SVM neural network classifiers are implemented. From the simulation results, it is found that the ELM classifies the PQ events more effectively than other classifiers.In the experiments, the nonlinear SVM is considered based on the popular Gaussian kernel (referred to as SVM-RBF or simply SVM). It is known that the generalization performance of SVM usually depends closely on the combination of C and γ[30]. C∈{2−24, 2−23, …, 224, 225} and γ∈{24, 23, …2−9, 2−10}. The best performance is obtained for C as 65,000,000 and γ as 2.0 for rbf kernel. In addition, for comparison purpose, in the first experiment, the SVM classifier with two other kernels are implemented, which are the linear and the polynomial kernels, leading thus to two other SVM classifiers termed as SVM-linear and SVM-poly, respectively. The polynomial kernel's degree d was varied in the range [2,5] in order to span polynomials with low and high flexibility. From the experimental results it is observed that polynomial kernel with order 3 provides highest classification accuracy. Although there are many variants of BP algorithm, a faster BP algorithm called Levenberg–Marquardt is used in this paper. For Levenberg–Marquardt BPN and ELM, the parameter to be selected is hidden neurons only. For BPN and ELM, the numbers of hidden neurons are gradually increased and the nearly optimal number of hidden neurons for BPN and ELM [31] are then selected based on cross validation method. The number of hidden neurons selected for classification problem with ELM and BPN are 100 and 10 respectively. For ELM and BPN sigmoidal function is used as activation function. After several simulations the spread factor selected for PNN is 0.01.Training is carried out in two categories; one is training the classifiers with less number of training samples and another is with large number of training samples. In the first case, 200 datasets are used for training the classifiers. In the second case the classifiers are trained with 500 datasets. The 500 datasets are used for testing the classifier. The same system is trained and tested with PNN and BPN with same samples to compare the performance of the classifiers with SVM and ELM. All the simulations for the ELM, SVM, PNN and BPN algorithms are carried out in MATLAB 7.5 environment running in a Pentium duo core, 2.88GHz CPU.An extensive series of studies are carried out in order to ascertain the overall performance of the four networks. The test sets (which not included as part of the training sets) are composed of over 500 cases including different parameters. The classification accuracy of classifiers for 200 training datasets is listed in Table 2. The simulation results show that SVM has higher performance than BPN, ELM and PNN. As observed from Table 6, the training time of ELM is less than SVM, PNN and BPN. Tables 3 and 4proved that classification accuracy of ELM is better than SVM with large number of training samples. The performance of BPN and PNN varied obviously when training samples increased.As reported in Table 5, the overall accuracy achieved with the ELM classifier on the test set was equal to 94.40. The result is better than those achieved by the SVM-linear, the SVM-poly, the SVM-RBF, the PNN and the BPN classifiers. Among SVM classifiers, the highest overall accuracy achieved with the classifier based on the Gaussian kernel (SVM RBF) on the test set was equal to 93.60%. The overall accuracy was equal to 88.50% for the SVM-linear classifier, 91.60% for the SVM-poly classifier.From Table 6, the training time of ELM is less than SVM, PNN and BPN and the classification accuracy of ELM is better than other classifiers with large number of training samples. Though ELM has fast learning speed, SVM possesses better generalization performance than ELM, PNN and BPN. Hence SVM can be used with less number of training samples.ELM takes much less total training time than does the SVM algorithm. As mentioned before, the SVM algorithm has to build C(C−1)/2 binary classifiers to distinguish between every two class combinations. Further, compared with BPN, ELM takes significantly lower training time. Compared with SVM, the reduction for ELM algorithm in CPU time is also above 2000 times. Since the number of support vectors obtained by SVM is much larger than the hidden nodes required by ELM, the testing time spent for the obtained SVM is 119 times longer than the testing time for ELM, meaning that after training, the classifier may response to new external unknown stimuli much faster than SVM in real deployment.Table 6 also presents the number of hidden nodes for ELM and the number of support vectors of SVM corresponding to the best classification performance. It can be seen that the number of hidden nodes for ELM is always smaller than the number of support vectors for SVM, indicating a more compact network realized by ELM.For a multicategory classifier, although the overall classification performance is important, one may also have to look at the classification performance of individual classes. A good classifier is the one that produces a good overall classification performance along with good individual class classification performance. To assess this, the classification power of all of the studied algorithms on each disturbance class has been investigated.The confusion matrices for ELM and SVM algorithms are shown in Tables 7 and 8, respectively. The tables contain the classification results in terms of number of correct classifications (diagonal elements) and misclassifications (the numbers outside the diagonal elements). Those test data that failed to be classified into any of the classes C1–C10 are found in column NC (not classified). After a careful comparison of the two tables, 100% classification accuracy is obtained for three disturbances C2, C4 and C8 for SVM and C1, C2 and C10 for ELM. Regarding misclassification, both algorithms happen to misclassify the disturbance Interruption (C3) with a similar probability. C1, C5, C6, C7, C9 and C10 are classified correctly 90.33% mean success rate for SVM and C4, C5, C6, C7, C8 and C9 are classified correctly 91.66% mean success rate for ELM. This means that the overall success rate is found to be about 93.60% for SVM and 94.40% for ELM. The results proved that the classification accuracy of ELM is better than SVM. The method proposed in this paper can effectively classify different kinds of PQ disturbances. Fig. 8shows the comparison of individual class accuracy results of ELM and SVM.The second experiment was devoted to analyze the generalization capability of the ELM and the SVM, classifiers with feature reduction based on a PSO. The particle swarm optimization is a powerful meta-heuristic technique in the artificial intelligence field; therefore, this study proposed a PSO-based approach, to specify the beneficial features and to enhance the classification accuracy rate.In this phase, the parameter values of the developed method were set as follows. Both the cognition learning factor c1 and the social learning factor c2 were set to 1.2. The maximum iteration number is 20 and the number of particles was set to 10. The accuracy's weight c1 is adjusted to 95%, and the feature size's weight wFis set to 5%. If there are N total features, then there will be 2Nkinds of subset, different from each other in the length and features contained in each subset. The optimal position is the subset with least length and highest classification accuracy.These features and the process of the particle swarms searching for optimal solutions for power system disturbances classification using SVM and ELM are given in Tables 9 and 10respectively. The best features at a particular iteration are listed as best solution in which each number represents one feature of the dataset. In order to show the effect of including (or excluding) a feature, the classification accuracy is also presented wherein the highest accuracies together with the lowest feature dimensions are displayed in bold.Experimental results show that the use of unnecessary features reduces classification accuracy and feature selection is used to reduce redundancy in the information provided by the selected features. The classification accuracy and feature subset length are two criteria which are considered to assess the performance of algorithms. Even by using six features, ELM gives good accuracy of classification (97.20%).From Fig. 9, it is found that the SVM classifier shows a relatively low sensitivity to the curse of dimensionality as compared to the ELM classifier. But the SVM classifier still preserves its superiority when integrated in a feature reduction-based classification scheme. The classification accuracy is improved from 93.60% to 95.20% using 11 features.From Table 11, with the optimized features for SVM, it takes more than 400s to train SVM, whereas ELM takes less than a second to train, yet yielding a remarkable performance improvement of 2.8% with 100 neurons. The number of support vectors for the SVM is also much higher than the number of neurons used in the ELM network, causing the recall and testing time to be very costly.The third experimental part had for objective to assess the capability of the proposed PSO-ELM classification system to boost further the accuracy of the ELM classifier using automatic feature detection and model selection-oriented optimization process. The optimization mechanism combined the discrete-valued PSO with the continuous-valued PSO to optimize the input feature subset selection and the number of hidden neurons for ELM. By incorporating two types of PSO, the parameters and the input features of ELM were optimized simultaneously.The detail parameter setting for PSO algorithm is shown in Table 12.The swarm size is set to 20 particles. The searching range of hidden neurons is 0–127. The accuracy's weight wAis adjusted to 95%, and the feature size's weight wFis set to 5%. Table 13gives the optimal selected features and the number of hidden neurons for ELM. The best features at a particular iteration and the corresponding neurons are listed as best solution.In order to evaluate the effectiveness and feasibility of the proposed method, a comparison in terms of percentage of the classification accuracy between the results of Baseline and PSO-ELM (feature selection only) is made and comparatively presented in Table 14. As seen from Table 14, the performance of the classification process with the proposed feature selection and parameter optimization method exceeds the performance of the other two methods.From Table 15, hundred percent classification accuracy is obtained for 5 disturbances (C2, C3, C4, C8 and C10) for PSO based ELM and for 3 disturbances (C1, C2 and C10) for baseline method. Fig. 7 shows the comparison of individual class accuracy results of Baseline, PSO-ELM (feature selection only) and PSO-ELM (feature selection parameter optimization). From the results, it is clear that the selection of an appropriate number of hidden neurons and feature selection of ELM significantly enhances its performance and renders its classification accuracy. Fig. 10shows the comparison of individual class accuracy results of Baseline, PSO-ELM (feature selection only) and PSO-ELM (feature selection & parameter optimization).The classification efficiency itself may not indicate the statistical significance of accurate classification of the ELM over the other existing methods [32]. Hence, in this paper, a statistical significance test called ‘binomial test’ is conducted between PSO-ELM classifier and other classifiers (BPN, PNN, SVM and ELM). A simple example shows used to compare two algorithms. For the purpose of understanding, consider binomial test between ELM and BPN. A brief description of the binomial test is given below.Let N be the number of test samples for which the proposed classifier (ELM) and BPN classifier produce different results. Let S (success) be the number of times ELM predicts the class label correctly rather than BPN and F (failure) be the number of times BPN predicts the class label correctly rather than ELM. Now, compute the probability of this result as(32)E=∑j=SNN!j!N−j!pjqN−jwhere p and q are the probability of success for ELM and BPN. If there is no difference between the two algorithms then p=q=0.5. If E is a very small number then the proposed ELM algorithm is better than the BPN with high confidence.From the 500 test samples, the class labels obtained using BPN and ELM classifiers differ only in 119 samples. Out of 119 samples, PSO-ELM classifier predicts class label accurately in 112 samples (S=112) and MRAN classifier predicts the class label accurately in 07 samples (F=07). The probability (E) for this case is 8.979e−26. From the result, it shows the proposed ELM classifier is better than BPN classifier with high confidence for this problem. Similarly, we the binomial test is conducted for PNN, SVM-RBF and ELM classifiers and the results are given in Table 16. From the binomial tests, the performance of the PSO-ELM classifier is better than all other classifiers with high confidence.

@&#CONCLUSIONS@&#
In this paper, a fast and efficient classification method called the ELM algorithm for multicategory power system disturbances is presented. Its performance has been compared with other methods such as the BPN, PNN, and SVM algorithms. SVM for multicategory classifications is done by modifying the binary classification method of SVM to a one-versus-all or one-versus-one comparison basis. This inevitably involves more classifiers, greater system complexities and computational burden, and a longer training time. ELM can perform the multicategory classification directly, without any modification. The main novelty of this paper is in the proposed PSO-based approach, which aims at optimizing the performances of ELM classifiers in terms of classification accuracy by detecting the best subset of available features and solving the tricky model selection issue. The fact that it is entirely automatic makes it particularly useful and attractive. It has the great potential to improve the performance of the automatic PQ monitoring equipments with on-line classification abilities. From the obtained experimental results, the proposed method can be applied for classifying power system disturbances signals on account of their superior generalization capability as compared to traditional classification techniques.