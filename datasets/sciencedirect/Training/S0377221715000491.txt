@&#MAIN-TITLE@&#
Algorithms for the continuous nonlinear resource allocation problem—New implementations and numerical studies

@&#HIGHLIGHTS@&#
Extension and update of the literature survey in Patriksson (2008).Improvement of the relaxation algorithm (in theory and practice).A comprehensive numerical study of non-linear problems at a very large scale.

@&#KEYPHRASES@&#
Resource allocation,Convex optimization,Lagrangian duality,Applications,Numerical analysis,

@&#ABSTRACT@&#
Patriksson (2008) provided a then up-to-date survey on the continuous, separable, differentiable and convex resource allocation problem with a single resource constraint. Since the publication of that paper the interest in the problem has grown: several new applications have arisen where the problem at hand constitutes a subproblem, and several new algorithms have been developed for its efficient solution. This paper therefore serves three purposes. First, it provides an up-to-date extension of the survey of the literature of the field, complementing the survey in Patriksson (2008) with more then 20 books and articles. Second, it contributes improvements of some of these algorithms, in particular with an improvement of the pegging (that is, variable fixing) process in the relaxation algorithm, and an improved means to evaluate subsolutions. Third, it numerically evaluates several relaxation (primal) and breakpoint (dual) algorithms, incorporating a variety of pegging strategies, as well as a quasi-Newton method. Our conclusion is that our modification of the relaxation algorithm performs the best. At least for problem sizes up to 30 million variables the practical time complexity for the breakpoint and relaxation algorithms is linear.

@&#INTRODUCTION@&#
We consider the continuous, separable, differentiable and convex resource allocation problem with a single resource constraint. The problem is formulated as follows: Let J ≔ {1, 2, …, n}. Letϕj:R→Randgj:R→R,j ∈ J, be convex and continuously differentiable. Moreover, letb∈Rand − ∞ < lj< uj< ∞, j ∈ J. Consider the problem to(1a)minimizexϕ(x):=∑j∈Jϕj(xj),(1b)subjecttog(x):=∑j∈Jgj(xj)≤b,(1c)lj≤xj≤uj,j∈J.We also consider the problem where the inequality constraint (1b) is replaced by an equality, i.e.,(2a)minimizexϕ(x):=∑j∈Jϕj(xj),(2b)subjecttog(x):=∑j∈Jajxj=b,(2c)lj≤xj≤uj,j∈J,where aj≠ 0, j ∈ J, and the sign is the same for all j ∈ J. Further, we assume that there exists an optimal solution to problems (1) and (2). For brevity, in the following discussions we define Xj≔ [lj, uj], j ∈ J.Problems (1) and (2) arise in many areas, e.g., in search theory (Koopman, 1999), economics (Markowitz, 1952), stratified sampling (Bretthauer, Ross, and Shetty, 1999), inventory systems (Maloney and Klein, 1993), and queuing manufacturing networks (Bitran and Tirupati, 1989). Further, these problems occur as subproblems in algorithms that solve the integer resource allocation problem (Mjelde, 1983, Section 4.7; Ibaraki and Katoh, 1988, pp. 72–75; Bretthauer and Shetty, 2002b), multicommodity network flows (Shor, 1985, Section 4.2), and several others. Moreover, problems (1) and (2) can be used as subproblems when solving resource allocation problems with more than one resource constraint (Federgruen and Zipkin, 1983; Mjelde, 1983), and to solve extensions of problems (1) and (2) to a nonseparable objective function ϕ (Dahiya, Suneja, and Verma, 2007; Mjelde, 1983). The books Mjelde (1983), Ibaraki and Katoh (1988), and Luss (2012) describe several extensions, such as to minmax/maxmin objectives, multiple time periods, substitutable resources, network constraints, and integer decision variables.Many numerical studies of the problems (1) and (2) have been performed; for example, see Bitran and Hax (1981), Nielsen and Zenios (1992), Robinson, Jiang, and Lemke (1992), Kodialam and Luss (1998), Kiwiel (2007), Kiwiel (2008a), and Kiwiel (2008b). Our numerical study is however timely and well motivated, since except for those by Kiwiel (2007); 2008a); 2008b), where the quadratic knapsack problem is studied, none of the earlier approaches study large-scale versions of problem (1) or (2). There are also several algorithms (e.g., Nielsen and Zenios, 1992, Section 1.4; Stefanov, 2001) which are claimed to be promising, but have not been evaluated in a large-scale study. Only one earlier study (Kodialam and Luss, 1998) evaluates the performance of algorithms for the problems (1) and (2) with respect to variations in the portions of the variables whose values are at a lower or upper bound at the optimal solution (see Section 6.2), and this is done for modest size instances (n = 104) only. Further, no study has been done on the computational complexity for non-quadratic versions of the problems (1) or (2). Our numerical study also incorporates improvements of the relaxation algorithm, as presented in Sections 4.3.4 and 4.3.5, and utilizes performance profiles (Dolan and Moré, 2002).As a final note on the compuational tests, we only consider problem instances where the dual variable corresponding to the resource constraint (1b), respectively (2b), can be found in closed form; otherwise, we would need to implement a numerical method in some of the steps, e.g., a Newton method. We consider only customized algorithms for the problem at hand, since we presume that they perform better than more general algorithms under the above assumption.Patriksson (2008) presents a survey of the history and applications of problems (1) and (2). Since its publication several related articles have been published; the survey of Patriksson (2008) is therefore complemented in Section 2. Section 3 presents a framework of breakpoint algorithms, resulting in three concrete representatives. Section 4 presents a framework of relaxation algorithms, and ultimately six concrete example methods. In Section 5 we describe a quasi-Newton method, due to Nielsen and Zenios (1992), for solving the problem (2). Section 6 describes the numerical study. A test problem set is specified and the performance profile used for the evaluation is defined. In Section 7, we analyze the results from the numerical study. The structure is such that we first compare the relaxation algorithms, second the pegging process, and third the best performing algorithms among these two with the quasi-Newton method. Finally, we draw overall conclusions.We here extend the survey in Patriksson (2008), using the same taxonomy, and sorted according to publication date.Mjelde (1983)K. M. Mjelde, Methods of the allocation of limited resources, Section 4.7(Problem)ϕj∈ C2; linear equality (aj= 1); lj= 0.The ranking algorithm of Luss and Gupta (1975).Applications in capital budgeting (Hansmann, 1968; Shih, 1977), cost-effectiveness problems (Kirsch, 1968; Mjelde, 1978; Pack, 1970), health care (Fetter, 1973), marketing (Luss and Gupta, 1975), multiobjective optimization (Geoffrion, 1967), portfolio selection (Jucker and de Faro, 1975), production (the internal report leading to Bitran and Hax, 1979), reliability (Bodin, 1969), route-planning for ships or aircraft (Dantzig, Blattner, and Rao, 1966), search (Charnes and Cooper, 1958), ship loading (Kydland, 1969), and weapons selection (Danskin, 1967).A monograph on resource allocation problems containing a comprehensive overview of the resource allocation problem, including extensions to several resources, non-convex or non-differentiable objectives, integral decision variables, fractional programming formulations, etc.ϕj(xj)=12(xj−yj)2; linear equality (aj= 1); lj= 0.Pegging.Shor and Ivanova (1969), in which the motivating linear programming application is described.The problem arises within the framework of a right-hand side allocation algorithm for a large-scale linear program.ϕj(xj)=qj2xj2−rjxj; linear inequality (aj> 0); lj= 0.Pegging.Algorithms for the problem (Bretthauer and Shetty, 2002a; 2002b; Melman and Rabinowitz, 2000; Pardalos and Kovoor, 1990) as well as for the case of integer variables.A numerical illustration (n = 6).ϕj(xj)=qj2xj2−rjxj,qj> 0; gjconvex in C2 with g′(xj) > 0.A combination of a bracketing algorithm on the Lagrangian dual derivative, and a secant algorithm for the Lagrangian dual problem.Algorithms for the problem (Brucker, 1984; Calamai and Moré, 1987; Helgason, Kennington, and Lall, 1980; Pardalos and Kovoor, 1990).The problem arises as a subproblem in a gradient projection method for a general quadratic programming problem over a scaled simplex.ϕjand gjincreasing; gjconvex in C2 withgj′>0.Multiplier search.Multiplier search methods (Bretthauer and Shetty, 1995), pegging methods (Bretthauer and Shetty, 2002a; 2002b).The problem arises as a subproblem in branch–and–bound methods for the integer programming version of the problem, such as for the quadratic knapsack problem, stratified sampling, manufacturing capacity planning, linearly constrained redundancy optimization in reliability networks, and linear cost minimization in reliability networks.ϕj(xj)=qj2xj2−rjxj,qj> 0; gjconvex in C2 with g′(xj) > 0; studies also the special case of a linear equality.Iterative descent process using strictly convex quadratic separable approximations of a nonseparable original objective f ∈ C2; subproblems solved using the pegging algorithm of Stefanov (2001).General references on convex programming over box constraints; Helgason et al. (1980), Dussault, Ferland, and Lemaire (1986), and Pardalos and Kovoor (1990) for example algorithms for separable convex programming.Numerical QP (n = 6, gjquadratic) illustration; numerical comparison with an augmented Lagrangian algorithm for a small problem (n = 2).ϕj(xj)=qj2xj2−rjxj,qj> 0; linear equality.Breakpoint search algorithm applying median search of all breakpoints.Breakpoint search algorithms: Brucker (1984), Calamai and Moré (1987), Pardalos and Kovoor (1990), and Maculan, Santiago, Macambira, and Jardim (2003); sorting and searching methods: Knuth (1998) and Kiwiel (2005).Develops a general O(n) breakpoint algorithm; shows that the algorithms of Pardalos and Kovoor (1990) and Maculan et al. (2003) may fail even on small examples; presents a modification of the breakpoint removal in the algorithm of Calamai and Moré (1987). Numerical experiments (n ∈ [50 × 103, 2 × 106]) for uncorrelated, weakly, and strongly correlated data; the new algorithm wins in CPU time over those in Brucker (1984) and Calamai and Moré (1987) by 23 percent, and 21 percent, respectively, on average.ϕj(xj)=qj2xj2−rjxj,qj> 0; linear equality.A family of breakpoint search algorithms that include several choices of breakpoints for a median search and updates of quantities used for evaluating the piecewise linear implicit constraint function at the median point.Applications in resource allocation (Bitran and Hax, 1981; Bretthauer and Shetty, 1997; Hochbaum and Hong, 1995), hierarchical production planning (Bitran and Hax, 1981), network flows and transportation (Cosares and Hochbaum, 1994; Helgason et al., 1980; Nielsen and Zenios, 1992; Shetty and Muthukrishnan, 1990; Ventura, 1991), constrained matrix problems (Cottle, Duvall, and Zikan, 1986), quadratic integer programming (Bretthauer, Shetty, and Syam, 1996; 1995; Hochbaum and Hong, 1995), Lagrangian relaxation (Held, Wolfe, and Crowder, 1974), and quasi-Newton methods (Calamai and Moré, 1987); O(nlog n) sorting algorithms for the solution of the Lagrangian dual problem (Held et al., 1974; Helgason et al., 1980), O(n) algorithms based on median search (Brucker, 1984; Calamai and Moré, 1987; Cosares and Hochbaum, 1994; Hochbaum and Hong, 1995; Maculan and de Paula, 1989; Maculan, Minoux, and Plateau, 1997; Pardalos and Kovoor, 1990) and approximate median search methods with O(n) average-case performance (Pardalos and Kovoor, 1990); primal pegging algorithms with O(n2) worst-case performance (Bitran and Hax, 1981; Bretthauer et al., 1996; Michelot, 1986; Robinson et al., 1992; Ventura, 1991; Zipkin, 1980)Develops several variants of O(n) breakpoint search algorithms, including some ideas earlier proposed in, e.g., Pardalos and Kovoor (1990), Cosares and Hochbaum (1994), Hochbaum and Hong (1995), and Maculan et al. (1997); remarks that the more complex choices made in Maculan and de Paula (1989), Pardalos and Kovoor (1990), Cosares and Hochbaum (1994), Hochbaum and Hong (1995), and Maculan et al. (1997) also means that for some simple cases cycling may occur, and also provides convergent modifications for each of them. Numerical experiments (n ∈ [50 × 103, 2 × 106]) for uncorrelated and weakly and strongly correlated data, and for both exact and inexact computations of the median; comparisons made with the O(n) versions from Brucker (1984) and Calamai and Moré (1987), reporting that a version (Algorithm 3.1) using exact medians is about 20 percent faster than the other ones; refers to an as yet unavailable technical report from 2006 for more extensive tests and comparisons with pegging methods.ϕj(xj)=qj2xj2−rjxj,qj> 0; linear equality.Pegging.Applications: same references as in Kiwiel (2008a); breakpoint search methods (Calamai and Moré, 1987; Cosares and Hochbaum, 1994; Held et al., 1974; Helgason et al., 1980; Hochbaum and Hong, 1995; Kiwiel, 2007; Maculan and de Paula, 1989; Maculan et al., 1997; Maculan et al., 2003; Pardalos and Kovoor, 1990); pegging methods (Bitran and Hax, 1979; 1981; Bretthauer et al., 1996; Luss and Gupta, 1975; Michelot, 1986; Robinson et al., 1992; Shor, 1985; Ventura, 1991).Develops a basic pegging algorithm and proposes several implementational choices for the solution of the reduced problem and the updates; shows that the algorithms in Michelot (1986), Robinson et al. (1992), and Bretthauer et al. (1996) fail on a small example, and that there is a gap in the convergence analysis in Bitran and Hax (1981) (which is also closed) that affects algorithms whose analyses rest on that in Bitran and Hax (1981) (e.g., Ventura, 1991); provide more efficient versions of several of these methods, including the introduction of incremental updates which reduce computations, and a more efficient stopping criterion. Numerical experiments (n ∈ [50 × 103, 2 × 106]) for uncorrelated and weakly and strongly correlated data; comparisons made with the breakpoint search method of Kiwiel (2008a) which uses exact medians; on average the latter is 14 percent slower while at the same time it has a more stable run time; it is remarked that the advantage of pegging over breakpoint search has been reported also in Ventura (1991) and Robinson et al. (1992).ϕj′/gj′monotone and invertible,gj′positive; consider∑j=1ngj(xj)⊲bwhere⊲∈{≤,=,≥}.Pegging algorithm using binary search on the value ofϕj′/gj′.Applications: resource allocation (Bitran and Hax, 1981; Hochbaum, 1994; Luss and Gupta, 1975; Zipkin, 1980), the singly constrained multi-product newsvendor problem (Abdel-Male, Montanari, and Morales, 2004; Erlebacher, 2000; Hadley and Whitin, 1963), production/inventory problems (Bretthauer and Shetty, 2002b; Bretthauer, Shetty, and Syam, 1995; Bretthauer, Shetty, Syam, and White, 1994), stratified sampling (Bretthauer et al., 1999; Bretthauer and Shetty, 2002b; Bretthauer et al., 1995), “core subproblem” (Ali, Helgason, Kennington, and Lall, 1980; Bretthauer et al., 1996; 1995; Jucker and de Faro, 1975; Moré and Toraldo, 1989; Robinson et al., 1992); algorithms: breakpoint methods (Bretthauer and Shetty, 1995, Stefanov, 2001, Luss and Gupta, 1975) and relaxation methods (Bretthauer and Shetty, 2002a; 2002b; Kodialam and Luss, 1998).Claims (without a proof) that the complexity of the algorithm is O(n), but the algorithm presented makes use of a mean-value method for the evaluation of the breakpoints which in the worst case results in 2n − 1 iterations. We also note that each iteration consist of O(n) operations, whence the complexity is O(n2). Numerical experiments (n ∈ [10, 104]) (ϕ quadratic, gjlinear,⊲chosen uniformly randomly).ϕjand gjconvex; gjstrictly monotone andgj([ϕj′/gj′]−1)is either strictly increasing or strictly decreasing for all j.Breakpoint search algorithm using a refined pegging method (5-sets pegging); generalizes the quadratic breakpoint algorithm in Pardalos and Kovoor (1990) and its extension in Kiwiel (2008a) such that it applies for the problem setting in Bretthauer and Shetty (2002b).Generalizes the quadratic breakpoint algorithm in Pardalos and Kovoor (1990) and its extension in Kiwiel (2008a) such that it is valid for fjand gjas in Bretthauer and Shetty (2002b). Applications in resource allocation (Bretthauer and Shetty, 1995; 2002b; Bretthauer et al., 1996; Bretthauer, Shetty, Syam, and Vokurka, 2006; Chao, Liu, and Zheng, 2009; De Waegenaere and Wielhouwer, 2002; Nielsen and Zenios, 1992; Pardalos and Kovoor, 1990; Patriksson, 2008; Ventura and Weng, 1995).Discuss their algorithms’ advantages compared to other articles presented in Patriksson (2008).ϕj′invertible: linear equality (aj> 0).Pegging; improves the pegging algorithm in Bitran and Hax (1981) by allowing the primal box constraints to be checked implicitly, using bounds on their Lagrange multipliers.Applications to portfolio selection (Markowitz, 1952), multicommodity network flows (Ali et al., 1980), transportation (Ohuchi and Kaji, 1984), production planning (Tamir, 1980).Compares their methodology with the method in Bitran and Hax (1981) on random test problems. On randomized quadratic continuous knapsack problems (n ∈ [5 × 103, 2 × 106]) their algorithms wins by 8–10 percent, except for the smallest problems where the method in Bitran and Hax (1981) wins by 12.5 percent. Two other types of test problems are also investigated, wherein the quadratic continuous knapsack problem arises as a subproblem: quadratic network flows, and portfolio optimization. In the former case, the algorithm (based on conjugate gradients) is taken from Arasu (2000). Here, the pegging algorithm proposed wins over that in Bitran and Hax (1981) by 10–48 percent, with n ∈ [200, 5 × 103]. In the portfolio optimization algorithm, which is based on a progressive hedging method described in Arasu (2000), the quadratic continuous knapsack problem arises as a subproblem. Here, the range of n is not completely disclosed; however, the speed-up over the method in Bitran and Hax (1981) is reported to be 21–25 percent.ϕjstrictly convex; gj(xj) = xj.Pegging.Koopman (1953) as an origin; Kodialam and Luss (1998) for computational examples; Patriksson (2008) as a survey.This book extends the resource allocation problem (discussed only in Chapter 2) in several ways, including equitable optimization through the use of minmax/maxmin objectives, multiple time periods, substitutable resources, network constraints, and integer decision variables.ϕjstrictly convex; gj(xj) = xj.The problem at hand is a subproblem in a branch–and–bound procedure for the solution of an integer-restricted version of the problem.Applications to the newsvendor problem (Abdel-Male et al., 2004; Erlebacher, 2000), resource allocation (Bitran and Hax, 1981; Hochbaum, 1994), production (Bretthauer et al., 1995), and stratified sampling (Bretthauer et al., 1999); efficient methods for the continuous relaxation (Bretthauer and Shetty, 1995; Kodialam and Luss, 1998; Stefanov, 2001; Zhang and Hua, 2008); heuristics for the integer program based on rounding of the solution to the continuous relaxation (Bretthauer and Shetty, 1995; Hua, Zhang, and Liang, 2006); algorithms for the integer program based on the solution of continuous problems and branch–and–bound (Bretthauer and Shetty, 1995; 2002b).Utilizing the algorithm from Zhang and Hua (2008) to solve the continuous relaxations (and rounding to produce feasible solutions), the authors develop a branch–and–bound algorithm. It is compared with the methods from Bretthauer and Shetty (1995) as well as with branch–and–bound algorithms utilizing a variety of tree search principles, on instances with quadratic objectives, according to problem generation principles from Bretthauer and Shetty (2002b) (n ∈ {10, 15, 500, 1000, 2000}).ϕjstrictly convex quadratic; linear equality (aj= 1).Sorting of breakpoints.Previous algorithms for the problem (Cosares and Hochbaum, 1994; Megiddo and Tamir, 1993).Two numerical applications: (1) an economic dispatch problem (n = 5), and an academic example (n ∈ [200, 104]); in the latter example the results are favorably compared with the Matlab solver QUADPROG.ϕj(xj)=qj2xj2−rjxj; qj> 0; linear equality.Breakpoint search utilizing a heap data structure, based on an initial multiplier estimate using a secant Newton method.Applications (Calamai and Moré, 1987; Dai and Fletcher, 2006; Helgason et al., 1980; Nielsen and Zenios, 1992; Shetty and Muthukrishnan, 1990); multiplier algorithms (Brucker, 1984; Calamai and Moré, 1987; Helgason et al., 1980; Maculan et al., 2003; Pardalos and Kovoor, 1990); pegging methods (Bitran and Hax, 1981; Bretthauer et al., 1996; Kiwiel, 2008b; Michelot, 1986; Robinson et al., 1992; Shor, 1985; Ventura, 1991); quasi-Newton methods (Cominetti, Mascarenhas, and Silva, 2014; Dai and Fletcher, 2006).Numerical experiments (n = 6.25 × 106) on random test problems, examining (a) the best number of initial (secant) Newton iterations, and (b) the performance against a primal pegging algorithm. Overall, breakpoint search is favorable on its own given a good initial mulitiplier estimate; otherwise, 3 or 4 iterations of the secant Newton method is a good initialization procedure.ϕj(xj)=qj2xj2−rjxj; qj> 0; gj(xj) = xj.Compares a breakpoint algorithm with CPLEX and concludes that the breakpoint algorithm outperforms CPLEX.Applications in resource allocation and algorithms (Patriksson, 2008).Presents an open source library for the continuous convex separable quadratic knapsack problem and concludes that the library can be useful for further studies of the problem at hand.ϕj(xj)=qj2xj2−rjxj; qj> 0; gj(xj) = ajxj.Pegging.Algorithms for the more general integer version of the problem; algorithms by Bretthauer and Shetty (1995); 2002a); 2002b); other specialized algorithms for the problem (Bitran and Hax, 1981; Kiwiel, 2008b; Pardalos and Kovoor, 1990; Robinson et al., 1992; Stefanov, 2001).A detailed numerical example (n = 8); favorable comparisons with a Matlab solver (n ∈ {50, 100, 200}).ϕj(xj) = −aj(1 − exp( − cjxj)), aj, cj> 0; X is a scaled unit simplex.The KKT conditions, as defined in (3), are relaxed into a system of non-smooth equations through the utilization of the Fischer–Burmeister (Fischer, 1992) smoothing function; a Newton-like algorithm is then employed for these equations for a sequence of values of the smoothing parameter. The algorithm is shown to asymptotically and superlinearly converge to the unique optimal solution.Survey (Patriksson, 2008); methodologies (Stefanov, 2004); applications (Koopman, 1999).Three sets of numerical experiments. Main experiment (n ∈ [102, 104]) on randomized problems; compares with the pegging algorithm in Stefanov (2004), noting that the proposed algorithm is faster. The proposed methodology is however terminated based on a nonzero value of the Fischer–Burmeister smoothing function, whence the final solution need not be feasible or optimal. Second set of experiments on a problem taken from Wilkinson and Gupta (1969) (n = 4), showing no comparisons. Third experiment on data from the Bureau of Water Conservatory (n = 5), showing no comparisons.ϕjstrictly convex quadratic; linear equality.An approximative Newton method for the Lagrangian dual problem utilizing a secant globalization and variable fixing.Applications to resource allocation (Bitran and Hax, 1981; Bretthauer and Shetty, 1997; Hochbaum and Hong, 1995), multicommodity network flows (Helgason et al., 1980; Nielsen and Zenios, 1992; Shetty and Muthukrishnan, 1990), Lagrangian relaxation using subgradient methods (Held et al., 1974), quasi-Newton updates with bounds (Calamai and Moré, 1987), semismooth Newton methods (Ferris and Munson, 2004); related methods (Dai and Fletcher, 2006; Kiwiel, 2008b), where the latter method is shown to be equivalent to the proposed one when there are only lower bounds or lower bounds.Numerical experiments (n ∈ [50 × 103, 2 × 106]) comparing the proposed method to a secant method (Dai and Fletcher, 2006), breakpoint search (Kiwiel, 2008b), and median search (Kiwiel, 2008a on problems with uncorrelated, weakly correlated, and correlated data. The proposed Newton method is overall better—about 30 percent better on the larger instances. A further test is made on the classification problems described in Dai and Fletcher (2006) arising in the training in support vector machines; as the Hessian is non-diagonal, a projected gradient method is used, leading to subproblems of the type considered. Here, Newton’s method is superior.ϕjand gjconvex, and in C2 on an open set containing [lj, uj]. Further, ϕjis decreasing on [lj, uj] and gjis increasing on [lj, uj], and ∑j ∈ Jgj(lj) < b < ∑j ∈ Jgj(uj). Test instances include resource renewal [ϕj(xj):=cjxj(e−1/xj−1)for xj> 0, ϕj(xj) ≔ −xjfor xj< 0, and gjlinear], weighted p-norm over a ball [ϕj(xj) ≔ cj(xj− yj)pand gj(xj) ≔ |xj|rwith p, r ∈ {2, 2.5, 3, 4}], sums of powers [ϕj(xj):=cj|xj−yj|pj,andgj(xj):=|xj|ri], convex quadratic over a simplex [ϕja fourth-power polynomial, and gj(xj) ≔ xj], and log-exponential [ϕj(xj) ≔ ln [∑iexp(aijxj+ dij)], gjlinear].A damped feasible interior-point Newton method for the solution of the KKT conditions.Survey (Patriksson, 2008); application to resource renewal (Melman and Rabinowitz, 2000); methodologies (Brucker, 1984; Kiwiel, 2008a; Melman and Rabinowitz, 2000; Pardalos and Kovoor, 1990; Robinson et al., 1992).The algorithm is introduced for problems where subsolutions are not available in closed form. Shows that the linear system defining the Newton search direction is solvable in O(n) time. Numerical experiments (n ∈ [102, 106]) conclude that the interior point method wins over breakpoint search, often by an order of magnitude.Algorithms based on the Lagrangian relaxation of the explicit constraint (1b) have an older history than the relaxation algorithms. This is probably due to the fact that the relaxation algorithm quite strongly rests on the Karush–Kuhn–Tucker (KKT) conditions, which did not become widely available until the end of the 1940s and early 1950s with the work of John (1948), Karush (1939), and Kuhn and Tucker (1951). Lagrangian based algorithms have been present much longer and the famous “Lagrange multiplier method” for equality constrained optimization is classic in the calculus curriculum. Indeed, Lagrange multiplier techniques for our problem (1) date back at least as far as to Beckmann (1952); see Patriksson (2008) for a survey of the history of the problem.Considering problem (1) and introducing the Lagrange multiplier μ for constraint (1b), we obtain the following conditions for the optimality of x* in (1):(3a)μ*≥0,g(x*)≤b,μ*(g(x*)−b)=0,(3b)xj*∈Xj,j∈J,and(3c)xj*=lj,ifϕj′(xj*)≥−μ*gj′(xj*),j∈J,(3d)xj*=uj,ifϕj′(xj*)≤−μ*gj′(xj*),j∈J,(3e)lj≤xj*≤uj,ifϕj′(xj*)=−μ*gj′(xj*),j∈J.For a fixed optimal value μ* of the Lagrange multiplier the conditions (3c)–(3e) are the optimality conditions for the minimization overx∈∏j=1nXjof the Lagrangian function defined on∏j=1nXj×R+,L(x,μ):=−bμ+∑j=1n{ϕj(xj)+μgj(xj)}.Given μ ≥ 0 its minimization overx∈∏j=1nXjseparates into n problems, yielding the Lagrangian dual function(4)q(μ):=−bμ+∑j=1nminimumxj∈Xj{ϕj(xj)+μgj(xj)}.By introducing additional properties of the problem, we can ensure that the function q is not only concave but finite onR+and moreover differentiable there. Suppose, for example, that for each j, ϕj( · ) + μgj( · ) is weakly coercive on Xjfor every μ ≥ 0 [that is, that either Xjis bounded or that for every μ ≥ 0, ϕj(xj) + μgj(xj) tends to infinity whenever xjtends to ± ∞], and that ϕjis strictly convex on Xj. In this case the derivative q′ exists onR+and equalsq′(μ)=ϕj′(xj(μ))+μgj′(xj(μ)),where x(μ) is the unique minimum of the Lagrange function L( ·, μ) over∏j=1nXj. Thanks to this simple form of the dual derivative, the maximum μ* of q overR+can be characterized by the complementarity conditions (3a), and the conditions (3) are the primal–dual optimality conditions for the pair of primal–dual convex programs.If we assume that μ* ≠ 0, we search for μ* > 0 such that q′(μ*) = 0 [or, in other words, g(x(μ*)) = b], that is, we need to solve a special equation in the unknown entity μ, where the function q′ is implicitly defined, but is known to be decreasing since q is concave. This equation can of course be solved through the use of any general such procedure [for example, bisection search takes two initial valuesμ¯andμwithq′(μ¯)<0and q′(μ) > 0, then iteratively cancels part of the initial interval given the sign of q′ at its midpoint(μ¯+μ̲)/2], but the structure of q′ makes specialized algorithms possible to utilize.From the above optimality conditions for the Lagrangian minimization problem, we obtain that(5)xj(μ)={lj,ifμ≥μjl:=−ϕj′(lj)/gj′(lj),uj,ifμ≤μju:=−ϕj′(uj)/gj′(uj),j∈J.xj,ifϕj′(xj)+μgj′(xj)=0,In a rudimentary algorithm we order these indices (or, breakpoints)μjlandμjuin an increasing (for example) order into {μ1, …, μN}, where N ≤ 2n due to the possible presence of ties. Finding μ* then amounts to finding an index j* such that that q′(μj*) > 0 and q′(μj* + 1) < 0; then we know that μ* ∈ (μj*, μj* + 1) and q′(μ*) = 0. Hence from Equation (5), we know for all j ifxj*=lj,xj*=ujorlj<xj*<uj. Now by fixing all variablesxj*that equal the corresponding lower or upper bound we can ignore the bound constraint (1c) and find an analytical solution of the problem.Two decisions thus need to be made: how to find the index j*, and how to perform the interpolation. Starting with the former, the easiest means is to run through the indices in ascending or descending order to find the index where q′ changes sign. If we have access to the indices j+ and j− for whichq′(μj+)>0whileq′(μj−)<0,then we can choose the midpoint index, check the corresponding sign of q′, and reduce the index set accordingly. Given the sorted list, we can also find this index in some randomized fashion.As remarked above, algorithms such as bisection search can be implemented without the use of the breakpoints, and therefore without the use of sorting, as long as an initial interval can somehow be found; also general methods for solving the equation q′(μ) = 0, such as the secant method or regula falsi, can be used even without an initial interval; notice however thatq∉C2,whence a pure Newton method is not guaranteed to be well-defined.While the sorting operation used in the ranking and bisection search methods takes O(nlog n) time, it is possible to lower the complexity by choosing the trial index based on the median index, which is found without the use of sorting; the complexity of the algorithm is then reduced to O(n). It is not clear, however, that the latter must always be more efficient, since the “O” definition calls for n to be “large enough”.We also remark that in the case when the problem (1) arises as a subproblem in an iterative method, as the method converges the data describing the problem will tend to stabilize. This fact motivates the use of reoptimization of the problems, which most obviously can be done by using the previous value of the Lagrange multiplier as a starting point and/or utilizing the previous ordering of the breakpoints; in the latter case, the O(nlog n) sorting complexity will eventually drop dramatically.In Section 3.1 we consider the breakpoint algorithm for the equality problem (2). In Section 3.2 we describe three pegging methods and in Sections 3.3–3.5 we apply these pegging methods to the breakpoint algorithm. Finally, in Section 3.6 we briefly discuss the convergence and time complexity of the breakpoint algorithm.We now consider problem (2) where the inequality of the primal constraint is replaced by an equality. For the problem to be convex, the resource constraint (2b) has to be affine, i.e., g(x) ≔ ∑j ∈ Jajxj− b. Beside the resource constraint, the Lagrangian and the optimality conditions will take the same form as for problem (1) but with one important difference; μ is unrestricted in sign, whence the condition (3a) is replaced by “g(x*) = b”.The origin of the pegging process is found in the relaxation algorithm; see, e.g., Bitran and Hax (1981). The purpose of pegging is to predict properties of the primal variables in the optimal solution from an arbitrary dual value. In Sections 3.2.1–3.2.3 we show how to predict if an optimal primal variable value equals its lower or upper bound, or is strictly within any of the bounds.If we can determine if a variable equals its lower bound at the optimal solution then we add its variable index to a set L and reduce the original problem. Similarly, if we know that a variable equals its upper bound at the optimal solution then we add the variable index to the set U. Using the sets L and U when solving problem (1) or (2) will be referred to as 2-sets pegging.Assume that we have a lower limitμand an upper limitμ¯on the optimal dual value, that is,μ̲≤μ*≤μ¯. From (5) we can define the setsL(μ̲):={j∈J∣μ̲≥−ϕj′(lj)/gj′(lj)}andU(μ¯):={j∈J∣μ¯≤−ϕj′(uj)/gj′(uj)}. LetJk:=J∖{L(μ̲)∪U(μ¯)}and letbk:=b−∑j∈L(μ̲)gj(lj)−∑j∈U(μ¯)gj(uj). Hence we can define a subproblem of problem (1) as follows:(6a)minimizexϕ(x):=∑j∈Jkϕj(xj),(6b)subjecttog(x):=∑j∈Jkgj(xj)≤bk,(6c)lj≤xj≤uj,j∈Jk.Similarly we can define a subproblem of problem (2) as follows:(7a)minimizexϕ(x):=∑j∈Jkϕj(xj),(7b)subjecttog(x):=∑j∈Jkajxj=bk,(7c)lj≤xj≤uj,j∈Jk.Consider problem (6). Assuming that μ* > 0, the constraint (1b) has to be fulfilled with equality. For any given dual variable μkwe can determine the primal solution xk(μk) of problem (6) from condition (5). We know from Section 3 that all optimality conditions (3) except the resource constraint (1b) are satisfied. Moreover, we know that the resource constraint has to be fulfilled with equality, in order for the solution to be optimal. Substituting xkinto the resource constraint will be referred to as explicit evaluation; this leaves us with three cases, namely(8a)∑j∈Jkgj(xjk)=bk,(8b)∑j∈Jkgj(xjk)<bk,or(8c)∑j∈Jkgj(xjk)>bk.If (8a) is fulfilled for xkthen all optimality conditions are met and x* = xk. Consider next the case (8b); clearly xkis not optimal but we know that x* is such that∑j∈Jkgj(xj*)>∑j∈Jkgj(xjk)since∑j∈Jkgj(xj*)=bk. The function gjis convex and differentiable and can increase in one interval and decrease in another (consider, e.g.,gj(xj)=xj2), which implies that no predictions can be made of the size ofxj*relative to that ofxjk. Hence, we need gjto be monotone. For problem (1), Bretthauer and Shetty (2002b, Section 2) consider four cases equivalent to the following:Case 1:For all j ∈ J, gjis decreasing andμ(xj):=−ϕj′(xj)/gj′(xj)is increasing in xj.For all j ∈ J, gjis increasing and μ(xj) is decreasing in xj.For all j ∈ J, gjis decreasing and μ(xj) is decreasing in xj.For all j ∈ J, gjis increasing and μ(xj) is increasing in xj.If Case 3 or 4 holds it is possible to find a closed form of the optimal solution to the problem (1), see Bretthauer and Shetty (2002b, Proposition 10). Considering problem (2), Cases 3 and 4 cannot occur, since the resource constraint is affine, i.e.,μ(xj):=−ϕj′(xj)/aj. Hence, only Cases 1 and 2 are of interest here.Note that if μ(xj) is increasing in xjthen xj(μ) is nondecreasing and vice versa. This is an essential property for the validity of the pegging process. We can indeed state the following proposition (a similar one can be stated for problem (2), but without the assumption μ* > 0):Proposition 1Pegging for Cases 1 and 2Consider problem(1)and assume that μ* > 0.(i)If Case 1 holds, and if(8b)holds, thenxj*=ljfor allj ∈ L(μk).If Case 1 holds, and if(8c)holds, thenxj*=ujfor all j ∈ U(μk).If Case 2 holds, and if(8b)holds, thenxj*=ujfor all j ∈ U(μk).If Case 2 holds, and if(8c)holds, thenxj*=ljfor all j ∈ L(μk).A proof of (i) is given; the proofs for (ii)–(iv) are analogous. From (8b) we have that∑j∈Jkgj(xj(μk))<bk.In Case 2, for all j gjis increasing and xj(μ) is nonincreasing, which implies that gj(xj(μ)) is nonincreasing in μ for all j. Hence, we have thatμk=μ¯≥μ*which implies thatxjk≤xj*for all j since xj(μ) is nonincreasing in μ for all j. Hence, for j ∈ U(μk) we know thatxjk+1=uj=xj*,i.e., we can peg j ∈ U(μk).□As in the 2-sets pegging principle of Section 3.2.1 we determine if a variable takes the value of the lower or upper bound at the optimal solution. Additionally for the 3-sets pegging we determine if a variable belongs to the open interval between the lower and upper bound, i.e., ifxj*∈(lj,uj). Assume that we know thatμ*∈(μju,μjl); then it follows from (5) thatxj*∈(lj,uj)and there is no need to check ifxjkequals the lower or upper bound which will reduce future calculations. 3-sets pegging is used for the quadratic knapsack problem (24) in (Kiwiel, 2008a, Section 3). The method described in Kiwiel (2008a, Section 3) can be generalized according to the following proposition:Proposition 2Relax primal variables from lower and upper boundsAssume that Case 1 or 2 inSection 3.2.1holds and that for some values ofμandμ¯,μ̲<μ*<μ¯holds. Ifμ̲,μ¯∈[μjl,μju]holds for some j ∈ Jk thenlj<xj*<uj.Assume that Case 2 holds, i.e.,μ=−ϕj′/gj′is decreasing. (The proof for Case 1 is analogous.) Assume thatμ̲,μ¯∈[μju,μjl]holds for some j ∈ Jk. Since−ϕj′/gj′is decreasing we have thatμ̲,μ¯∈[μju,μjl]implies thatμ*∈(μju,μjl),and from (5) it then follows thatlj<xj*<uj.□As in the 3-sets pegging principle in Section 3.2.2 we determine if a variable takes the value of the lower or upper bound or if the variable strictly belongs to the interval between the lower and upper bound. For 5-sets pegging we also determine if a variable is larger than the lower bound or smaller than the upper bound. 5-sets pegging for problem (1) is used in De Waegenaere and Wielhouwer (2012), generalizing a method from Kiwiel (2008a). Assuming that we know thatxj*<uj,there is no need to check ifxjkequals the upper bound; this might reduce future calculations. The proof of the following proposition follows from the monotonicity of gjand xj(μ) (see De Waegenaere and Wielhouwer, 2012 for a proof).Proposition 3Relax primal variables from lower or upper boundAssume that Case 1 or 2 inSection 3.2.1holds. Let j ∈ Jk. Ifμ*<μ¯≤μjl,thenxj*>ljand ifμ*>μ̲≥μjuthenxj*<uj.Consider Case 1 or 2 in Section 3.2.1 for problem (1). Let median( · ) denote a function which provides the median of a finite vector; let μmbe the median breakpoint, and define the total use of the resource due to variables that equal the lower and upper bounds asβl:=∑j∈{N∣μjl≤μm}gj(lj),andβu:=∑j∈{N∣μju≥μm}gj(uj),respectively. In the spirit of Bretthauer and Shetty (2002a, Section 3.1) and Kiwiel (2007, Algorithm 3.1), we present the following algorithm:Initialization:SetN:=J,k:=1,andbk:=b.Compute breakpointsμl:=(μjl)j∈N,μu:=(μju)j∈Nas in (5), and letμk:=(−∞,(μl)⊺,(μu)⊺,∞)⊺.Step 0 (check ifμ=0is optimal):If∑j∈Ngj(xj(0))≤b,forxj(μ)is determined from (5), thenμ*=0,xj*=xj(0)forj∈N. Stop.Iterative algorithm:Step 1 (Stopping test):Ifμk=∅then findx*andμ*from problem (1) relaxed from lower and upper bounds.Otherwise, letμm:=median(μk).Step 2 (Compute explicit reference):Determineδ:=∑j∈{N∣μju<μm<μjl}gj(xj(μm))+βu+βl,wherexj(μ)is determined from (5).Ifδ>bk,then go to Step 3.1.Ifδ<bk,then go to Step 3.2.Otherwise(δ=bk)letμ*:=μm,findx*from (5), and stop.Step 3.1 (Update and fix lower bounds):For allj∈N: Ifμm≥μjlthen letN:=N∖{j}andxj*:=lj.Letμk+1:=(μjk)j∈{N∣μm<μjk},bk+1:=bk−βl,andk:=k+1.Go to Step 1.Step 3.2 (Update and fix upper bounds):For allj∈N: Ifμm≤μjuthen letN:=N∖{j}andxj*:=uj.Setμk+1:=(μjk)j∈{N∣μm>μjk},bk+1:=bk−βu,andk:=k+1.Go to Step 1.For problem (2), the algorithm is similar except Step 0 vanishes.If we can determine if the value of a variable xjis strictly within the bounds for all μ such thatμ̲<μ<μ¯,then we do not have to check if xjviolates the bounds when we determine xjfrom (5) in future iterations (see Proposition 2). This might save us some operations. Define a set of indices for the lower and upper limit being within the interval of the lower and upper breakpoint for a variable:M:={j∈N∣μ̲,μ¯∈[μjl,μju]}. From Proposition 2 we have that if j ∈ M thenlj<xj*<uj. Hence if j ∈ M we do not have to check if xjviolates the bounds in future iterations. But we should have in mind that to determine if j ∈ M requires some extra operations. If we let the initial values of the limits beμ= − ∞ andμ¯=∞,thenμ̲,μ¯∉[μjl,μju]and we note that there is no need to check if j ∈ M, as long asμ= − ∞ orμ¯=∞. Hence, to avoid unnecessary operations we start by checking if j ∈ M whenμ> − ∞ orμ¯<∞; this is in contrast to the algorithms in Kiwiel (2008a, Section 3) and De Waegenaere and Wielhouwer (2012).We finally define the contribution to the resource constraint from the variables including M: γ(μ) ≔ ∑j ∈ Mgj(xj(μ)). Note that the value of γ depends on the value of the parameters in ϕj, gjand μ. If we can separate the parameters from the multiplier μ, i.e., if γ is additively and/or multiplicatively separable [that is, γ(μ, ·) = γ1(μ)γ2( · ) + γ3( · )] then the values of γ2 and γ3 can be calculated successively so that no calculations are done more than once. Consider, for example, the negative entropy function, ϕj(xj) ≔ xjlog (xj/aj− 1) and the resource constraint function gj(xj) ≔ xj. Then,γ(μ,a):=∑j∈Mgj(xjk(μ))=∑j∈Maje−μ=γ2(a)γ1(μ),where γ1(μ) = e−μand γ2(a) = ∑j ∈ Maj, i.e., we update γ2 in Steps 3.1 and 3.2 such that ifμ̲,μ¯∈[μju,μjl]then γ2 ≔ γ2 + aj. For the quadratic knapsack problem, this approach is applied in Kiwiel (2008a, Section 3). We next present an algorithm that applies the usage of M; Steps 0 and 1 are similar to the algorithm MB2 in Section 3.3:Initialization:SetN:=J,k:=1,bk:=b,γ:=0,M:=∅,μ̲:=−∞,andμ¯:=∞.Compute breakpointsμl:=(μjl)j∈N,μu:=(μju)j∈Nas in (5), and letμk:=(μlμu).Iterative algorithm:Step 2 (Compute explicit reference):Determineδ:=∑j∈{N∣μju<μm<μjl}gj(xj(μm))+γ(μm)+βu+βl,wherexj(μ)is determined from (5).Ifδ>bk,then go to Step 3.1.Ifδ<bk,then go to Step 3.2.Otherwise(δ=bk)setμ*:=μm; find optimalx*from (5), and stop.Step 3.1 (Update and fix lower bounds):Letμ̲=μm.Forj∈N: Ifμ̲≥μjlthen letN:=N∖{j}andxj*:=lj.Ifμ̲,μ¯∈[μju,μjl]then letN:=N∖{j}andM:=M∪{j}; updateγ1andγ2.Letμk+1:=(μjk)j∈{N∪M∣μm<μjk},bk+1:=bk−βl,andk:=k+1.Updateγand go to Step 1.Step 3.2 (Update and fix upper bounds):Letμ¯:=μm.Forj∈N: Ifμ¯≤μjuthen letN:=N∖{j}andxj*:=uj.Ifμ̲,μ¯∈[μju,μjl]then letN:=N∖{j}andM:=M∪{j}; updateγ1andγ2.Letμk+1:=(μjk)j∈{N∪M∣μm>μjk},bk+1:=bk−βu,andk:=k+1.Go to Step 1.Remark 1If γ is not separable then the updates of γ1 and/or γ2 in Steps 3.1 and 3.2 vanish.As in the algorithm MB2 in Section 3.3 we peg the variables whose values equal the lower or upper bounds, and as in MB3 we determine if a variable is strictly within the bounds. Further, we determine if a variable is smaller than the upper bound or larger than the lower bound as in Section 3.2.3. Define a set L− of indices where the optimal solution is known to be strictly smaller than the upper bound in the optimal solution, i.e.,L−:={j∈N∣xj*<uj},and similarly define a set U+ of indices where the variable is known to be larger than the lower bound in the optimal solution, i.e.,U+:={j∈N∣xj*>lj}. Define, respectively, the total use of the resource due to variables whose values equal the lower and upper bounds asβl:=∑j∈{N∪L−∣μjl≤μm}gj(lj)andβu:=∑j∈{N∪U+∣μju≥μm}gj(uj). We present an algorithm that applies 5-sets pegging where Steps 0 and 1 are similar to the algorithm MB2 in Section 3.3:Initialization:SetN:=J,k:=1,bk:=b,γ:=0,andM=L−=U+:=∅,μ̲:=−∞,andμ¯:=∞.Compute breakpointsμl:=(μjl)j∈N,μu:=(μju)j∈Nas in (5), and letμk:=(μlμu).Iterative algorithm:Step 2 (Compute explicit reference):Determineδ:=∑j∈{N∪U+∪L−∣μju<μm<μjl}gj(xj(μm))+γ(μm)+βu+βl,wherexj(μ)is determined from (5).Ifδ>bk,then go to Step 3.1.Ifδ<bk,then go to Step 3.2.Otherwise(δ=bk)setμ*:=μmand find optimalx*from (5), and stop.Step 3.1 (Update and fix lower bounds):Letμ̲:=μmForj∈N: Ifμ̲≥μjlthen letN:=N∖{j}andxj*:=lj.Forj∈L−: Ifμ̲≥μjlthen letL−:=L−∖{j}andxj*:=lj.Forj∈N: Ifμ̲≥μjuthen letN:=N∖{j}andL−:=L−∪{j}.Forj∈U+: Ifμ̲∈[μju,μjl]andμ¯≥μjuthen letU+:=U+∖{j}andM:=M∪{j}; updateγ1,γ2.Letμk+1:=(μjk)j∈{N∪M∪L−∪U+∣μm<μjk},bk+1:=bk−βl,andk:=k+1.Go to Step 1.Step 3.2 (Update and fix upper bounds):Letμ¯:=μm.Forj∈N: Ifμ¯≤μjuthen letN:=N∖{j}andxj*:=uj.Forj∈U+: Ifμ¯≤μjuthen letU+:=U+∖{j}andxj*:=uj.Forj∈N: Ifμ¯≤μjlthen letN:=N∖{j}andU+:=U+∪{j}.Forj∈L−: Ifμ¯∈[μju,μjl]andμ̲≤μjlthen letL−:=L−∖{j}andM:=M∪{j}; updateγ1,γ2.Letμk+1:=(μjk)j∈{N∪M∪L−∪U+∣μm>μjk},bk+1:=bk−βu,andk:=k+1.Go to Step 1.Note that when we determine xj(μm) for j ∈ U+ we do not need to check if xj= lj, and for j ∈ L− we do need to check if xj= uj. This will in some cases save us some operations. Further when we determine if xj∈ M we only need to check if this holds for j ∈ U+ or j ∈ L− depending on if we update the lower or upper bound of the dual variable. This differs from the algorithm in De Waegenaere and Wielhouwer (2012) since that algorithm finds M from U+∪L−. Note that we make use of information from earlier iterations when updating δ. When updating δ in De Waegenaere and Wielhouwer (2012) information from earlier iteration is negligible hence some operations are repeated.Remark 2Consider iteration k. In Step 3.1 we only need to check ifμm∈[μju,μjl]andμ¯≥μjufor j ∈ U+ since we know from earlier iterations that if j ∈ N thenμ¯≰μjl. Similar holds for Step 3.2.Similar to Kiwiel (2008a) and De Waegenaere and Wielhouwer (2012) it is possible to show that the breakpoint algorithms converge to the optimal solution. Consider the time complexity for algorithm MB2, MB3 and MB5. Assuming that the median function median( · ) is linear, we have Cin operations in each of the Steps 0–3.2 for some constants Cifor i ∈ {0; 1; 2; 3.1; 3.2} corresponding to Step 0–3.2 respectively. Since we use a median search function the number of iterations will terminate in log2(n) iterations (in the worst case). Hence the time complexity of the algorithm is O(nlog2(n)). However in Brucker (1984) a proof for a time complexity of O(n) is given for the breakpoint algorithm solving the quadratic knapsack problem (24).In a relaxation algorithm for the problem (1) we iteratively solve the problem relaxed from constraints (1c), i.e., we solve the following problem:(9a)minimizexϕ(x):=∑j∈Jkϕj(xj),(9b)subjecttog(x):=∑j∈Jkgj(xj)≤b.From problem (9) we obtain a solutionx^. Together withx^we also obtain an estimateμ^of the multiplier value μ* from the optimality condition. Then we adjust the solutionx^for constraints (1c) by determining xjfrom(10)xj:={lj,ifx^j≤lj,uj,ifx^j≥uj,x^j,iflj<x^j<uj.At the beginning of the algorithm we must determine whether constraint (1b) is satisfied with equality at an optimal solution. From the optimality condition (3a) we have that if the inequality constraint (1b) is satisfied then μ* = 0. Hence, forμ^=0we findx^jby solvingϕj(x^j)+μ^gj(x^j)=0; if ∑j ∈ Jgj(xj) ≤ b, where the value of xjis determined from (10), then we have found the optimal solution to problem (1). Otherwise, we know that μ* > 0 and that the inequality constraint (1b) can be regarded as an equality. Hence, we solve the problem (9), obtaining a solutionx^. LetL(x^):={j∈Jk∣x^j≤lj},U(x^):={j∈Jk∣x^j≥uj}denote the sets of variables that are either out of bounds atx^or equal a lower, respectively an upper, bound.In order to simplify the remaining discussion, we consider Case 2 in Section 3.2.1, i.e., μ(xj) to be monotonically decreasing and gjis increasing; Case 1 is treated analogously. Calculate the total deficit and excess atx^,respectively, as(11)∇:=∑j∈L(gj(lj)−gj(x^j)),Δ:=∑j∈U(gj(x^j)−gj(uj)).Now, if Δ > ∇ then we setxj*=ujforj∈U(x^); if Δ < ∇ we setxj*=ljforj∈L(x^); otherwise Δ = ∇ and we have found the optimal solution. If Δ ≠ ∇, then we reduce the problem by removing the fixed variables, and adjusting the right-hand side of the constraint (1b) to reflect the variables fixed. If any free variables are left, we re-solve problem (9) and repeat the procedure, otherwise we have obtained an optimal solution.The rationale behind this procedure is quite simple and natural: Suppose that Δ > ∇ holds. We have thatμ^=−ϕj′(x^j)/gj′(x^)for j ∈ Jk∖{L∪U}. Lets∈U(x^)andi∈Jk∖U(x^). Since the functions−ϕj′/gj′are decreasing, it follows that−ϕs′(us)gs′(us)≥−ϕs′(x^s)gs′(x^s)=μ^=−ϕi′(x^i)gi′(x^i)≥−ϕi′(ui)gi′(ui).Denote by b+ the right-hand side in the following iteration given that Δ > ∇ holds:b+:=b−∑j∈U(x^)gj(uj). Also let(x^′,μ^′)denote a pair of relaxed optimal primal–dual solutions in the following iteration. We must have thatμ^′≤μ^,since∑j∈Jk∖U(x^)gj(x^j)=b−∑j∈U(x^)gj(x^j)≤b−∑j∈U(x^)gj(uj)=b+=∑j∈Jk∖U(x^)gj(x^j′);hence, for at least onej∈J∖U(x^)we have thatx^j′≥x^j,and therefore,μ^′=−ϕj′(x^j′)gj′(x^j′)≤−ϕj′(x^j)gj′(x^)=μ^follows. This derivation was first described by Bitran and Hax (1981).Since in each iteration at least one variable is pegged to an optimal value, the algorithm is clearly finite. The most serious disadvantage of the algorithm may be the requirement that the problem without the variable bounds present must have an optimal solution. The computational efficiency of this method is also determined by whether or not it is possible to provide an explicit formula for eachx^jin terms of the multiplier.For the breakpoint algorithm we determine x from (5) for an arbitrary multiplier μ; then we explicitly evaluate the optimality of x by substituting it into the resource constraint (1b). The explicit evaluation leaves us with one out of three possible scenarios: (8a)–(8c). For the relaxation algorithm the traditional method to evaluate a solution to the problem (9) is to calculate the total deficit and excess atAPTARABOLDx^(see Section 4). Also this evaluation leaves us with three possible scenarios, namely(12a)Δ=∇,(12b)Δ>∇,(12c)Δ<∇.Evaluating the optimality from (12a) to (12c) will be referred to as an implicit evaluation. For the relaxation algorithm we next show that the implicit and explicit evaluations are equivalent. Propositions 4 and 5 below state the relations between explicit and implicit evaluation. The proof of Proposition 5 is similar to the proof of Proposition 4.Proposition 4 (Relation between explicit and implicit evaluation forgjmonotonically increasing) If for all j ∈ J gjis monotonically increasing, then the explicit evaluation (8a)–(8c) is equivalent to the implicit evaluation (12a)–(12c), i.e., (12a) ⟺ (8a), (12b) ⟺ (8b), and (12c) ⟺ (8c).For all j ∈ J, letx^jbe the solution to (9). Let ∇ and Δ be defined as in (11). We have from (10) that∑j∈Jgj(xj)=∑j∈J∖{U∪L}gj(x^j)+∑j∈L{gj(lj)−gj(x^j)+gj(x^j)}+∑j∈U{gj(uj)−gj(x^j)+gj(x^j)}=∑j∈J∖{U∪L}gj(x^j)+∑j∈L{gj(lj)−gj(x^j)}+∑j∈Lgj(x^j)−∑j∈U{gj(uj)−gj(x^j)}+∑j∈Ugj(x^j)=∑j∈Jgj(x^j)+∇−Δ=b+∇−Δ,where the last equality follows from the fact thatx^is the solution to the relaxed problem. Hence,x^must satisfy the resource constraint. We know that Δ, ∇ ≥ 0 since gjis increasing. Hence, if Δ = ∇ then (8a) holds, if Δ > ∇ then (8b) holds, and if Δ < ∇ then (8c) holds.□gjmonotonically decreasing) If gjis monotonically decreasing for all j ∈ J, then the explicit evaluation (8a)–(8c) is equivalent to the implicit evaluation (8a)–(8c), i.e., (12a) ⟺ (8a), (12c) ⟺ (8b), and (12b) ⟺ (8c).In the breakpoint algorithms in Section 3, we use the dual variable to determine if xjequals a bound or if it lies in between the bounds; see (5). In relaxation algorithms the traditional way to solve the problem (1) is to determine the primal optimal solutionx^jof the relaxed problem (9) and then simply check ifx^jis within the lower and upper bounds; see (10). An alternative is to find the optimal dual variableμ^of the relaxed problem (9) and then (similar to the breakpoint algorithm in Section 3) determine the primal variables from (5). Of course this requires us to determine the breakpoints. However, for the variables that violate the bounds (1c) we do not have to determine xjfrom the relation ϕj(xj) + μgj(xj) = 0. Hence, instead of evaluatingL(x^)andU(x^)from the primal variable as in Section 4, we can evaluateL(μ^)andU(μ^)from the dual variable as in Section 3.2.1.DefineΦj(xj):=ϕj′(xj)/gj′(xj)and assume that there exists an inverse to Φj. It follows from the optimality conditions (5) thatΦj(x^j(μ^))=−μ^,Φj(lj)=ϕj′(lj)/gj′(lj)andΦj(uj)=ϕj′(uj)/gj′(uj). Hence we have that:L(μ^):={j∈J∣μ^≥−ϕj′(lj)/gj′(lj)}⟺L(x^):={j∈J∣x^j≤lj},andU(μ^):={j∈J∣μ^≤−ϕj′(uj)/gj′(uj)}⟺U(x^):={j∈J∣x^j≥uj}.Considering the performance of a relaxation algorithm two decisions need to be made: should the relaxed solution of the problem be evaluated implicitly from ∇ and Δ or explicitly from ∑gj(see Section 4.1)? Should the algorithm solve the primal or dual relaxed problem (see Section 4.2)? An overview of the relaxation algorithm and its possible realizations is shown in Fig. 1. The leftmost path in the figure is the classic primal relaxation algorithm of Bitran and Hax (1981), the rightmost path in the figure is implemented in Stefanov (2001), and the algorithm that applies the right path in Step 1 and the left path in Step 2 is implemented in Kim and Wu (2012). Beside these two paths no other paths have been explored. Our intention is to evaluate the theoretically and practically best performing paths. Since no earlier studies have applied 3- or 5-sets pegging for the relaxation algorithm, our intention is to apply these two more sophisticated pegging methods.In Section 4.3.1 we present an algorithm corresponding to the leftmost path in Fig. 1; in Section 4.3.2 we present an algorithm which utilizes the rightmost path in Step 1 of Fig. 1 and then changes to the left path in Step 2 of the figure; in Section 4.3.3 an algorithm corresponding to the rightmost path of Fig. 1 is presented; and in Section 4.3.4 an algorithm that utilizes the rightmost path in Step 1 of Fig. 1 and then utilizes the theoretically best path in Step 2 is presented. All algorithms in Sections 4.3.1–4.3.4 utilize 2-sets pegging. In Section 4.3.5 we describe how 3- and 5-sets pegging can be utilized in these.We first assume that Case 1 in Section 3.2.1 holds for problem (1). Define parameters to calculate the total deficit and excess, asα+k:=∑j∈Ukgj(x^j),α−k:=∑j∈Lkgj(x^j),β−k:=∑j∈Lkgj(lj)andβ+k:=∑j∈Ukgj(uj). Hence we have thatΔk=α+k−β+kand∇k=β−k−α−k. In the next iteration when we reduce the resource bkwe hence do not have to re-calculate the part of the pegged variables which defineβ+korβ−k(see Steps 3.1 and 3.2). We present an algorithm for problem (1) which is similar to the algorithm in Section 2 of Bretthauer and Shetty (2002b):Initialization: Setk:=1,Jk:=J,andbk:=b.Step 0 (Check ifμ=0is optimal):Letμ=0and find the solutionAPTARABOLDx^to the relaxed problem (9), i.e., solveϕj′(x^j)+μgj′(x^j),j∈Jk.If∑j∈Jkgj(xj)≤b,forxjdetermined from (10), thenμ*=0,xj*=xjforj∈Jk,and stop.Iterative algorithm:Step 1 (Solve relaxed primal problem):Forj∈Jk,findx^jkby solving the relaxed problem (9).SetL:=∅andU:=∅.Step 2 (Implicit evaluation):DetermineU(x^k)andL(x^k)while computingΔk:=α+k−β+kand∇k:=β−k−α−k.IfΔk>∇k,then go to Step 3.1.IfΔk<∇k,then go to Step 3.2.IfΔk=∇k,then setxj*:=ljforj∈L(x^k),xj*:=ujforj∈U(x^k),xj*:=xjkforj∈Jk∖{L(x^k)∪U(x^k)},and stop.Step 3.1 (Peg lower bounds):Setxj*:=ljforj∈L,bk+1:=bk−β−kandJk+1:=Jk∖L.IfJk+1:=∅then stop, else setk:=k+1and go to Step 1.Step 3.2 (Peg upper bounds):Setxj*:=ujforj∈U,bk+1:=bk−β+kandJk+1:=Jk∖U.IfJk+1:=∅then stop else setk:=k+1and go to Step 1.We need to clarify some of the steps of the algorithm. In Step 1, we findx^k+1from, or partly from,x^k. Assume, for example, that ϕj(xj) = xjlog (xj/aj− 1) and gj(xj) = xj; then,xjk+1=ajbk+1/∑j∈Jk+1aj=bk+1/(ω−∑j∈K(x^k)aj), whereω=∑j∈Jkajand K ≔ U if the upper bound was pegged at iteration k and K ≔ L if the lower bound was pegged at iteration k. If |K| < |Jk + 1| then this will save us some operations. A similar update ofx^for the quadratic knapsack problem is performed in Robinson et al. (1992, Section 3), Bretthauer et al. (1996) and Kiwiel (2008b, Section 5.1).As in Kiwiel (2008b, Algorithm 3.1), our algorithm will stop if Δk= ∇k, while the algorithm in Bretthauer and Shetty (2002b, Section 2) stops only ifL(x^k)∪U(x^k)=∅. Moreover, in Steps 3.1 and 3.2, we peg the variables that violate the bounds and calculate bkexplicitly, while in Bretthauer and Shetty (2002b, Section 2) the index j is added to the set of violated bounds (L or U) and bkis calculated as b − ∑j ∈ Lgj(lj) − ∑j ∈ Ugj(uj).According to Proposition 1, if Case 2 in Section 3.2.1 holds, Step 2 in the algorithm is modified as follows (an analogous modification can be defined for the relaxation algorithms in Sections 4.3.2–4.3.5):Step 2’ (Implicit evaluation):DetermineU(x^k)andL(x^k)while computingΔk=α+k−β+kand∇k=β−k−α−k.IfΔk>∇k,then go to Step 3.2.IfΔk<∇k,then go to Step 3.1.IfΔk=∇k,then setxj*:=ljforj∈L(x^k),xj*:=ujforj∈U(x^k),xj*:=xjkforj∈Jk∖{L(x^k)∪U(x^k)},and stop.Remark 3For the equality problem (2), μ is unrestricted; hence the algorithm for problem (2) will be similar to the above, except we ignore Step 0.In Step 1 we have to calculatex^j|Jk| times. In Step 2 we need to find xjwhich needs at most 2|Jk| comparisons. We also have to calculate ∇kand Δk, which implies 2|L∪U| operations.Instead of evaluating the primal variables in Step 1 as in the algorithm PIR2 in Section 4.3.1, we can evaluate the dual variable μ. Note that if we evaluate the dual variable, then we have to determine the breakpoints in the initialization. Our modification of the algorithm in Section 4.3.1 takes the following form (Steps 0 and 3 are same as in PIR2 and are therefore not repeated):Initialization:Setk:=1,Jk:=J,andbk:=b.Calculate breakpointsμl:=(μjl)j∈Jk,andμu:=(μju)j∈Jkas in (5).Step 1 (Solve relaxed dual problem):Find the optimal dual variableμ^kof the relaxed problem (9).Step 2 (Implicit evaluation):DetermineL(μ^k)andU(μ^k)while computingΔk:=α+k−β+kand∇k:=β−k−α−k.IfΔk>∇k,then go to Step 3.1.IfΔk<∇k,then go to Step 3.2.IfΔk=∇k,then setxj*:=ljforj∈L(μ^k),xj*:=ujforj∈U(μ^k)xj*:=xjkforj∈J∖{L(μ^k)∪U(μ^k)},and stop.Remark 5In Step 1 we need to find the optimal dual solution μkto the relaxed problem. In Step 2 we need at most 2|Jk| comparisons but we only need to calculatex^j(μ)for j ∈ {L∪U}. For the evaluation we need to calculate ∇kand Δk, which implies 2|L∪U| operations.As in the algorithm DIR2 in Section 4.3.2 we evaluate the dual variable but instead of evaluating the relaxed solution implicitly we do it explicitly. Define βl≔ ∑j ∈ Lgj(lj) and βu≔ ∑j ∈ Ugj(uj). The algorithm follows (Steps 0 and 3 are same as in PIR2 in Section 4.3.1 and are therefore not repeated):Initialization:Setk:=1,Jk:=J,andbk:=b.Calculate breakpointsμl:=(μjl)j∈Jk,μu:=(μju)j∈Jkas in (5).Step 1 (Solve relaxed dual problem):Find the optimal dual variableμ^kof the relaxed problem (9).Step 2 (Explicit evaluation):DetermineU(μ^k)andL(μ^k)and calculateδ(μ^k):=∑j∈Jk∖{U(μ^k)∪L(μ^k)}gj(xj(μ^k))+βl+βu.Ifδ(μ^k)>bk,then go to Step 3.1.Ifδ(μ^k)<bk,then go to Step 3.2.Ifδ(μ^k)=bk,then setxj*:=ljforj∈L(μ^k),xj*:=ujforj∈U(μ^k)xj*:=xjkforj∈J∖{L(μ^k)∪U(μ^k)},and stop.The algorithm uses the principle of the algorithm in Stefanov (2001, Algorithm 1).Remark 6In Step 1 we need to find the value ofμ^kfrom the relaxed problem. In Step 2 we need at most 2|Jk| comparisons and we need to calculatexjkfor j ∈ Jk∖{L∪U}. For the evaluation we need to calculateδ(μ^k),which implies |Jk| operations.Consider the implicit evaluation in Section 4.3.2. We calculate ∇kand Δkfrom∇k:=∑j∈L(μ^k)gj(lj)−∑j∈U(μ^k)gj(x^jk)andΔk:=∑j∈U(μ^k)gj(x^jk)−∑j∈L(μ^k)gj(uj),which implies 2P|Lk∪Uk| operations, where P is an integer associated with the number of operations it takes to calculate gj(xj). Moreover, we have to determinex^jfor j ∈ {Lk∪Uk}, which implies Q|Lk∪Uk| operations, where Q is an integer associated with the number of operations it takes to determinexj(μ^).Now consider the explicit evaluation: We have to calculateδ(μ^k):=∑j∈Jk∖{U(μ^k)∪L(μ^k)}gj(xj(μ^k))+∑j∈L(μ^k)gj(lj)+∑j∈U(μ^k)gj(uj),which implies P|Jk| operations. Moreover, we have to determinex^jfor j ∈ Jk∖{Lk∪Uk}, which implies Q|Jk∖{Lk∪Uk}| operations.Hence, if (P + Q)|Jk| < (2P + 2Q)|U(μk)∪L(μk)| or, equivalently, |Jk| < 2|U(μk)∪L(μk)|, then using the explicit evaluation of the relaxed solutionAPTARABOLDx^in Step 2 would require less operations, and it would be more successful to use the algorithm in Section 4.3.3. If however |Jk| > 2|U(μk)∪L(μk)|, then there will be less operations if we use the algorithm in Section 4.3.2. So, we propose a new algorithm that utilizes the cardinalities of the sets Jk,U(μ^k)andL(μ^k): from the cardinalities we make the decision whether to use an explicit or implicit evaluation in Step 2. We consider the following modification of the algorithm PIR2 in Section 4.3.1:Initialization:Setk:=1,Jk:=J,andbk:=b.Calculate breakpointsμl:=(μjl)j∈Jk,μu:=(μju)j∈Jkas in (5).Step 1: (Solve relaxed dual problem):Find the optimal dual variableμ^kof the relaxed problem (9).Step 1.1: (Implicit or explicit evaluation?):DetermineU(μ^k)andL(μ^k).If|Jk|<2|U(μ^k)∪L(μ^k)|then use explicit evaluation (continue with algorithm DER2 in Section 4.3.3), otherwise use implicit evaluation (continue with algorithm DIR2 in Section 4.3.2).From the proof of convergence of the relaxation algorithm (see Section 4.4) we have that the algorithm improves the lower or the upper bound for the dual variable in each iteration. Hence, similar to the breakpoint algorithm (see Sections 3.4 and 3.5) it is possible to apply 3- and 5-sets pegging for the dual relaxation algorithms DIR2, DER2 and DBR2. Similar to the relaxation algorithm we will denote a relaxation algorithm that uses 3-sets-pegging with a suffix “3”, e.g., DBR3, and one that uses 5-sets-pegging with a suffix “5”, e.g., DBR5. The implementation of 3- and 5-sets pegging is similar for the three dual relaxation algorithms; therefore only DIR3 and DIR5 are given (Step 0 is similar as for PIR in Section 4.3.1):DIR3:Initialization:Setk:=1,Jk:=J,Mk:=∅,bk:=b,μ̲=−∞,andμ¯=∞.Calculate breakpointsμl:=(μjl)j∈Jk,μu:=(μju)j∈Jkas in (5).Step 1 (Solve relaxed dual problem):Find the optimal dual variableμ^kof the relaxed problem (9).Step 2 (Implicit evaluation):DetermineL(μ^k)andU(μ^k)fromJkand computeΔk=α+k−β+kand∇k=β−k−α−k.IfΔk>∇k,then go to Step 3.1.IfΔk<∇k,then go to Step 3.2.IfΔk=∇k,then setxj*=ljforj∈L(μ^k),xj*=ujforj∈U(μ^k)xj*=xj(μ^k)forj∈{Jk∪Mk}∖{L(μ^k)∪U(μ^k)},and stop.Step 3.1 (Peg lower bounds):Update lower bound,μ̲:=μ^k,and resourcebk+1:=bk−β−k.Setxj*:=ljforj∈Lk,Jk+1:=Jk∖LkandMk+1:=Mk.Forj∈Jk+1: Ifμ̲,μ¯∈[μju,μjl]thenJk+1:=Jk+1∖{j},Mk+1:=Mk+1∪{j}.IfJk+1=∅then find optimal solution and stop, else setk:=k+1and go to Step 1.Step 3.2 (Peg upper bounds):Update upper bound,μ¯:=μ^k,and resourcebk+1:=bk−β+k.Setxj*:=ujforj∈Uk,Jk+1:=Jk∖UkandMk+1:=Mk.Forj∈Jk+1: Ifμ̲,μ¯∈[μju,μjl]thenJk+1:=Jk+1∖{j},Mk+1:=Mk+1∪{j}.IfJk+1=∅then find optimal solution and stop, else setk:=k+1and go to Step 1.In Steps 3.1 and 3.2, if Jk + 1 = ∅ then we can find the optimal solution from M since we know that for all j ∈ M it holds that lj< xj< uj, i.e., we do not have to consider the constraint (1c). Further, in Step 3.1 (and analogously for Step 3.2) when searching for j ∈ Jk + 1 such thatμ̲,μ¯∈[μju,μjl],we only have to consider j ∈ Jk + 1∖Uk, since we know that if j ∈ Ukthenμ^k≤μju. Note that for the case whereμ^k=μjuthe corresponding index j will continue to belong to Jk + 1. Finally we note that we do not have to check ifμ̲,μ¯∈[μju,μjl]ifμ= − ∞ and/orμ¯=∞.DIR5:Initialization:Setk:=1,Jk:=J,Mk=U+k=L−k:=∅,bk:=b,μ̲=−∞,andμ¯=∞.Calculate breakpointsμl:=(μjl)j∈Jk,μu:=(μju)j∈Jkas in (5).Step 1 (Solve relaxed dual problem):Find the optimal dual variableμ^kof the relaxed problem (9).Step 2 (Implicit evaluation):DetermineL(μ^k)andU(μ^k)fromJk∪L−k∪U+k,computeΔk=α+k−β+kand∇k=β−k−α−k.IfΔk>∇k,then go to Step 3.1.IfΔk<∇k,then go to Step 3.2.IfΔk=∇k,then setxj*=ljforj∈L(μ^k),xj*=ujforj∈U(μ^k)xj*=xj(μ^k)forj∈{Jk∪Mk∪L−k∪U+k}∖{L(μ^k)∪U(μ^k)},and stop.Step 3.1 (Peg lower bounds):Update lower bound,μ̲:=μ^k,and resourcebk+1:=bk−β−k.Setxj*:=ljforj∈Lk,Jk+1:=Jk∖LkandL−k+1:=L−k∖Lk.Forj∈{Jk+1∖U+k}: Ifμm≤μjlthenJk+1:=Jk+1∖{j}andL−k+1:=L−k+1∪{j}.Forj∈{U+k∖Uk}: Ifμ̲≤μjlthenU+k:=U+k∖{j}andMk:=Mk∪{j}.SetMk+1:=MkandU+k+1:=U+k.IfJk+1∪L−k+1∪U+k+1=∅then find optimal solution and stop,else setk:=k+1and go to Step 1.Step 3.2 (Peg upper bounds):Update upper bound,μ¯:=μ^k,and resourcebk+1:=bk−β+k.Setxj*:=ujforj∈Uk,Jk+1:=Jk∖UkandU+k+1:=U+k∖Uk.Forj∈{Jk+1∖L−k}: Ifμm≤μjlthenJk+1:=Jk+1∖{j}andU+k+1:=U+k+1∪{j}.Forj∈{L−k∖Lk}: Ifμ̲≤μjlthenL−k:=L−k∖{j}andMk:=Mk∪{j}.SetMk+1:=MkandL−k+1:=L−k.IfJk+1∪L−k+1∪U+k+1=∅then find optimal solution and stop,else setk:=k+1and go to Step 1.Remark 7Note that for DBR2 in Section 4.3.4 we determine whether we should continue with DIR or DER from|Jk|<2|U(μ^k)∪L(μ^k)|. This condition needs to be modified for the 3- and the 5-sets algorithms DBR3 and DBR5. This is done by substituting the condition mentioned by|Jk∪Mk|<2|Uk(μ^k)∪Lk(μ^k)|and|Jk∪Mk∪L−k∪U+k|<2|Uk(μ^k)∪Lk(μ^k)|for 3- and 5-sets pegging, respectively.For the inequality problem (1), optimality and validation for the pegging process for the primal relaxation algorithm PIR2 was established by Bretthauer and Shetty (2002b, Propositions 1–9). Let k* be the iteration where the algorithm terminates. Then Bretthauer and Shetty state that the primal relaxation algorithm generates the following solution for problem (2):(13a)μ=μk*=−ϕj′(xjk*)/gj(xjk*),j∈Jk*,(13b)ρj=ϕj′(ljk*)+μk*gj′(xjk*),j∈L,(13c)ρj=0,j∈Jk*∪U,(13d)λj=−ϕj′(ujk*)−μk*gj′(xjk*),j∈U,(13e)λj=0,j∈Jk*∪L,(13f)xj=lj,j∈L,(13g)xj=uj,j∈U,(13h)xj=xjk*,j∈Jk*.Bretthauer and Shetty establish that this solution fulfills all KKT conditions, and therefore is optimal.The optimality and convergence for the equality problem (2) can be established similarly to the proof for the inequality problem given in Bretthauer and Shetty (2002b) except for the proof for feasibility of the dual variables corresponding to the lower and upper bounds (Propositions 8 and 9 in Bretthauer and Shetty, 2002b). Additionally we do not have to prove that μ* ≥ 0 (Proposition 4 in Bretthauer and Shetty, 2002b) for the equality problem. Therefore we give a complementary proof for the feasibility of the dual variables corresponding to the lower and upper bounds. For the equality problem (2) we have that gj(xj) = ajxjand in the proof we will assume that aj> 0.Lemma 1Consider the equality problem(2)and algorithm PIR2 inSection 4.3.1.(a)If ∇k> Δkthenμk*≥μk.If ∇k< Δkthenμk*≤μk.The proof is similar to that of Proposition 7 in Bretthauer and Shetty (2002b).□For problem (2), the solution (13) generated by PIR2 in Section 4.3.1 satisfies the feasibility of the dual variables (ρjand λj) corresponding to the lower and upper bounds.(i) Forj∈Jk*∪U,we have from (13c) that ρj= 0.(ii) For j ∈ L, we have from (13c) thatρj=ϕj′(lj)+μk*aj. We know that all variables xjwith j ∈ L were pegged in the iterations k where ∇k> Δk. For these iterations k we havex^jk≤lj. Further, from the convexity of ϕjand the assumption that aj> 0, we have thatμ(xj)=−ϕj′(xj)/ajis decreasing in xj. Hence,(14)ρjaj=ϕj′(lj)aj+μk*≥ϕj′(x^jk)aj+μk*=−μk+μk*≥0,where the last inequality follows from Lemma 1(a).(iii) Forj∈Jk*∪L,we have from (13c) that λj= 0.(iv) For j ∈ U, we have from (13e) thatλj=−ϕj′(uj)−μk*ajuj. We know that all variables xjwith j ∈ U were pegged in the iterations where ∇k< Δk. For these iterations k we havex^jk≥uj. Further, from the convexity of ϕjand the assumption that aj> 0, we have thatμ(xj)=−ϕj′(xj)/ajis decreasing in xj. Hence,(15)λjaj=−ϕj′(uj)gj′(uj)−μk*≥−ϕj′(x^jk)gj′(x^jk)−μk*=μk−μk*≥0,where the last inequality follows from Lemma 1(b).□The algorithms in Sections 4.3.2–4.3.4 will also converge to the optimal solution since they are equivalent to PIR.Remark 8If we introduce the additional assumption that ϕjand gjare twice differentiable and thatgj′>0,an alternative convergence result for the dual relaxation algorithm is found in Stefanov (2001).Consider algorithm PIR2 in Section 4.3.1. In Step 0 we need at most 2n comparisons to determine the primal variables and C0n operations, for a constant C0, to determine if the solution is feasible or not. In Step 1, we solve the relaxed problem, which gives C1n operations for a constant C1. In Step 2 we perform at most 2n comparisons to determine the lower and upper sets and we compute Δkand ∇k, which gives at most C2n operations for a constant C2. Steps 3.1 and 3.2 give at most 2n + 1 operations. Further, in the worst case the algorithm only pegs one primal variable in each iteration, which results in n iterations. Hence, we conclude that the algorithm has a complexity of O(n2).Nielsen and Zenios (1992, Section 1.4) develop a quasi-Newton method for finding the dual optimal solution μ* of the problem (2). It is assumed that the objective term ϕjis strictly convex with a derivativeϕj′whose range isR. They compare their numerical method with three linesearch methods: Helgason et al. (1980), Censor and Lent (1981), and Tseng (1990). Their results show that their numerical method always performs well compared with the other algorithms. They implement their algorithms on a massively parallel computer. This is not our intention. But since the algorithm seems to perform well on parallel computers it makes sense to evaluate it on non-parallel computers.Let fj, j ∈ J, be the inverse ofϕj′such that(16)forj∈J,ϕj(fj(μ))=μ,μ∈R.Similar to (5) we conclude that(17)xj(μ)=max{lj,min{fj(ajμ),uj}}.The heart of the algorithm is, like in the breakpoint algorithm, to find μ such that the primal constraint (1b) is fulfilled. In other words find μ such that(18)Ψ(μ):=b−∑j∈Jajxj(μ)=0.Nielsen and Zenios (1992, Section 1.4) define two functionsΦj+andΦj−:(19)Φj+(μ):={min{fj(ajμ),uj},ifaj>0,max{lj,fj(ajμ)},ifaj<0,j∈J,and(20)Φj−(μ):={min{fj(ajμ),uj},ifaj<0,max{lj,fj(ajμ)},ifaj>0.j∈J,Note that for j ∈ J, if aj> 0 and fjis concave and increasing thenΦj+is concave and if aj> 0 and fjis convex and decreasing thenΦj−is convex. Further, we define two sets of indices such that J+ ≔ { j ∈ J∣aj> 0 } and J− ≔ { j ∈ J∣aj< 0 }. Define two approximation of Ψ such that(21a)Ψ+(μ):=b−∑j∈JajΦ+(μ)=b−∑j∈J+ajmin{g(ajμ),uj}−∑j∈J−max{lj,g(ajμ)},and(22a)Ψ−(μ):=b−∑j∈JajΦ−(μ)=b−∑j∈J−ajmin{g(ajμ),uj}−∑j∈J+max{lj,g(ajμ)}.Note that if aj> 0 and fjis concave then Ψ+ is convex and if aj> 0 and fjis convex then Ψ− is concave. Define the sub- and superdifferentials of Ψ+, respectively, Ψ−, as(23a)∂Ψ+(μ):={d∈R∣(Ψ+(μ′)−Ψ+(μ)≥d(μ′−μ)∀μ′∈R},(23b)∂Ψ−(μ):={d∈R∣(Ψ−(μ′)−Ψ−(μ)≤d(μ′−μ)∀μ′∈R}.Further, defineμɛ*andxɛ*as the approximate dual and primal solution such that|Ψ(μɛ*)|<ɛwhere ɛ > 0. The algorithm (NZ) follows (Nielsen and Zenios, 1992, Linesearch 4):Initialization:Setɛ>0,k=0,μ0∈R.Iterative algorithm:Step 1 (Compute step size):IfΨ(μk)>ɛthenΔμk+1:=−Ψ(μk)dkwheredk∈∂Ψ+(μk); go to Step 2,else ifΨ(μk)<−ɛthenΔμk+1:=−Ψ(μk)dkwheredk∈∂Ψ−(μk); go to Step 2,elseSetμɛ*:=μkand determinexɛ*from (17). Stop.Step 2 (Dual variable update):setμk+1:=μk+Δμk+1.Step 3: Setk:=k+1and go to Step 1.The algorithm converges to a valueμɛ*,such that|Ψ(μɛ*)|<ɛif the objective function components ϕjis such that the corresponding function Ψ+(μ) is convex or if the corresponding function Ψ−(μ) is concave (Nielsen and Zenios, 1992, Proposition 8). For some problems, the inverse of the derivative might however result in imaginary values. One solution to this problem is to consider the equivalent maximization problem of (2), i.e., to maximizex− ϕ(x).Remark 9In practice we will choose ɛ > 0 and the algorithm will in most cases stop such that |Ψ(μk)| > 0. Hence we will end up with an approximate solution of the optimal dual variable μ*. The map from the dual space to the primal might not be linear. Hence the primal error might be larger than we expect, i.e.,|xɛ*−x*|>>|μɛ*−μ*|. However, there are methods for generating primal optimal solutions from any Lagrangian dual vector (see for example Larsson, Marklund, Olsson, and Patriksson, 2008). Another plausible method to find the optimal solution from the approximate solutionμɛ*is to use a breakpoint or a relaxation algorithm, starting fromμɛ*.This section serves to provide an overview of the procedure for the numerical study. In Section 6.1 we define problem instances for the numerical study. Some theory on how the problem instances can be designed follows in Section 6.2. In Section 6.3 we give a brief overview of performance profiles (Dolan and Moré, 2002) which are used for the evaluation of the numerical study. Finally, in Section 6.4, we describe the computational environment.For the numerical study we consider five common special cases of problem (2); it also covers the inequality problem (1) when μ* > 0. Only finite values of the lower and upper bounds (2c) are considered, i.e., for j ∈ J, lj> −∞ and uj< ∞. The five problem cases are briefly specified next:The convex separable quadratic problem is the special case of (2), where(24)ϕj(xj)=wj2xj2−cjxjforj∈J,where wj, cj> 0, j ∈ J. Numerical studies of algorithms for problem (24) are widely explored; e.g., see Nielsen and Zenios (1992) and Kiwiel (2007); 2008a); 2008b). In our numerical study the parameters are randomized such that aj∈ [1, 30], wj∈ [1, 20], cj∈ [1, 25], lj∈ [0, 3] and uj∈ (3, 11].If we have a large population and would like to perform a statistical research among the population, it is practically infeasible to examine every single individual in the population. Instead we can stratify the population into n strata. An example of a stratum might be people of a certain age. Let M be the number of individuals in the entire population and Mjthe number of individuals in strata j. If we want to minimize the variance of the entire population we need to allocate the number of samples xjfrom each strata from:(25)ϕj(xj)=ωj(M−xj)ρ2(M−1)xjforj∈J,where ωj= Mj/M and ρjis an estimate of the variance for strata j. From b in the resource constraint we specify the total sample size. In our numerical study the parameters are randomized such that aj∈ [1, 30], mj∈ [5, 30], cj∈ [1, 4], lj∈ [1, 3] and uj∈ (3, 15].According to Bretthauer et al. (1999), often the objective terms for sampling problems can be written as(26)ϕj(xj)=cj/xjforj∈J.In our numerical study the parameters are randomized such that aj∈ [1, 4], cj∈ [5, 30], lj∈ [0, 3] and uj∈ (3, 6].In the theory of search problem it is determined how a resource b of time should be spent to find an object among n subdivisions of an area with the largest probability. It is assumed that we know the probability mjfor an object to be found in subdivision j. The objective component ϕjdescribes the probability of finding the object in subdivision j and takes the form:(27)ϕj(xj)=mj(e−bjxj−1)forj∈J.This problem is widely explored by Koopman (1953); 1999) and the problem is possible to apply to a large variation of search problems, e.g., searching for refugees fleeing from Cuba (Stone, 1981). In our numerical study the parameters are randomized in the following intervals: mj∈ [0.5, −8], bj∈ [0.1, 3], aj∈ [1, 3], lj∈ [0, 0.1] and uj∈ (0.1, 5].The negative entropy function mentioned in Nielsen and Zenios (1992):(28)ϕj(xj)=xjlog(xjaj−1)forj∈J.In our numerical study the parameters are randomized such that cj∈ [50, 250], lj∈ [20, 100] and uj∈ (30, 210].Similarly to the numerical study in Kodialam and Luss (1998), we divide our set of test problem instances into groups containing different portions of the activities within the lower and upper bounds at the optimal solution. LetH:={j∈J∣lj<xj*<uj}. Then, the percentage is determined as |H|/n. To motivate this approach, we refer to the variance of CPU times for different portions of active activities in Kodialam and Luss (1998). Further, similar to the numerical study in Kiwiel (2007), we consider different problem sizes n since the theoretical CPU time for different algorithms vary between O(n) and O(n2).For the problem set, we pseudo randomize the parameters lj, ujand all the parameters associated with ϕjand gj. In the numerical study we use a linear resource constraint such that gj(xj) = ajxj, where aj> 0 for all j ∈ J. This simplifies the design of the problem set, since ϕjis convex and lj< ujfor all j ∈ J. We have that(29)xj*={xj*,ifμ*=−ϕj′(xj′)/aj,lj,ifμ*≥−ϕj′(lj)/aj≥−ϕj′(uj)/aj,uj,ifμ*≤−ϕj′(uj)/aj)≤−ϕj′(lj)/aj.By using the properties of (29) we can determine H such that |H|/n = y for any y ∈ [0, 1].Dolan and Moré (2002) propose a performance profile for the evaluation of optimization software. The method is briefly summarized as follows: Assume that we have a set A of algorithms that consist of naalgorithms and a problem set P that consists of npproblem instances. Let tp, adenote the time it takes for algorithm a ∈ A to solve problem p ∈ P. A performance ratio rp, ais introduced:(30)rp,a(tp,a):=tp,amin{tp,l∣l∈A}.For a problem p, the performance ratio is a measure of how fast algorithm a is relative to the fastest algorithm solving problem p. Fix a constant rMsuch that rM≥ rp, afor all p ∈ P, a ∈ A and let rp, a= rMif algorithm a fails to solve problem p. Further, we introduce the distribution(31)ρa(τ)=1np|{p∈P∣rp,a≤τ}|,for each algorithm, where | · | denote the cardinality of a set and τ ∈ [1, rM]. For algorithm a, the distribution ρadescribes the percentage of problem instances that are solved at least as fast as τ times the fastest algorithm for problem p. Note that ρa(1) is the percentage for algorithm a being the fastest. Moreover,limτ→rMρs(τ)is the probability that algorithm a will solve a problem p in P. If we have a large problem set P then ρa(τ) will not be affected much by a small change in P (Dolan and Moré, 2002, Theorem 1).The algorithms are implemented in Fortran 95, compiled with gfortran under mac OS X 10.8.2 (2.5 GHz Intel Core i5, 4 GB 1600 MHz DDR3).In this section we present the results from numerical experiments of the problems defined in Section 6.1. If nothing else is mentioned, 100 problem instances of each problem is evaluated for each problem size. Further, the problem instances are designed such that different values of |H|/n are considered, see Section 6.2.The development of numerical experiments for the problem (1) is illustrated in Table 1, where we cite the size of the largest test problem reported during each decade; the table is an extension of Table 2 in Patriksson (2008).In Section 7.1 we present performance profiles for the relaxation algorithms defined in Section 4. In Section 7.2 we evaluate the pegging process for both the breakpoint and the relaxation algorithm defined in Section 3. In Section 7.3, the best performing relaxation algorithm and breakpoint algorithm are compared with the quasi-Newton algorithm in Section 5. In Section 7.4 we give a critical review of the numerical study. Finally, in Section 8, we make some overall conclusions.In the following Figs. 2–4we show performance profiles, as defined in Section 6.3, for three competing sets of algorithms. To summarize the appearance of these plots, for each plot and corresponding competing set of algorithms, the graph for one algorithm shows the portion (between 0 and 1 on the y-axis) of the problem instances considered that are solved within τ (on the x-axis) times the fastest algorithm in the test for problem p. In particular, the value of ρa(1) is the portion of the problems in which algorithm a is the fastest, andlimτ→rMρa(τ)is the probability that algorithm a will solve a problem p in P. (The latter information is particularly illustrative and relevant for Fig. 4.)We evaluate the different paths in Fig. 1. First, recall that PIR2 determines the primal variables while DIR2 determines the dual variable. As can be seen in the leftmost performance profile in Fig. 2, DIR2 is the fastest in 67.8 percent of the problem instances solved. The result is what we can expect from theory, since DIR2 does not need as many operations in Step 1; see Sections 4.3.1 and 4.3.2. PIR2 is faster in 32.2 percent of the problems solved; the latter cases stem mainly from to the negative entropy problem (28) but also from the theory of search problem (27) and the stratified sampling problem (25) when n and |H|/n is small. The reason for PIR2 to be faster in almost all cases for the negative entropy problem is due to the computationally simple expression of the primal variablesxjk(μk)=aj∑j∈Jkcjwhile the dual variable is evaluated fromμk=log∑j∈Jkcj−logb. Also the computations of the breakpoints might be a larger part of the total time when the equations for the primal/dual variables are simple.We recall that DER2 applies explicit evaluation, DIR2 applies implicit evaluation and DBR2 applies the theoretically most profitable evaluation. The results are in favor of DBR2 and DIR2; see the rightmost performance profile in Fig. 2. DBR2 is fastest in 70.6 percent, DIR2 is fastest in 25.9 percent and DER2 is fastest in 4.7 percent of the problems solved. (Note that 70.6 percent + 25.9 percent + 4.7 percent = 100.2 percent; in some cases two algorithms are equally fast.)Fig. 2 also shows that the performance of DIR2 and DBR2 are quite similar. For just a few cases DIR2 is more than 1.30 times slower than the fastest algorithm while for as few cases DBR2 is only 1.2 times slower than the fastest algorithm. On average DBR2 performs somewhat better than DIR2.

@&#CONCLUSIONS@&#
We have complemented the survey in Patriksson (2008) on the resource allocation problem at hand, and introduced, and critically evaluated, new implementations of breakpoint and relaxation algorithms for its solution.The results show that our new implementations (DIR2 and DBR2) of the relaxation algorithm outperform the earlier algorithms (PIR2 and DER2). Hence we should evaluate the dual variable for the relaxation algorithm, i.e., DIR2 outperforms PIR2. Moreover, it is more profitable in theory, as well as in practice, to apply blended evaluation, i.e., DBR2 outperforms DER2 and DIR2. Our results, as well as the results in Robinson et al. (1992), Kodialam and Luss (1998), and Kiwiel (2008b), imply that the relaxation algorithm is to prefer when a closed form of the dual variable μ can be found.We introduced 3- and 5-sets pegging for the relaxation algorithm and showed that it is most profitable to apply 5-sets pegging, which also holds for the breakpoint algorithm.For the problems considered, MB5 and DBR5 have a practical time complexity of O(n) also for very large values of n. We also showed that the relaxation algorithm DBR5 performs better than both the Newton-like algorithm NZ and the breakpoint algorithm MB5. Potential future improvements include the implementation of a pegging method for a Newton-like algorithm and/or a hybrid of the different algorithms (NZ, DBR5 and MB5), and, hence, it would be of interest to compare the best algorithms from our study to these.The findings made herein can most certainly be profitably utilized also in the efficient solution of the more complex versions of the resource allocation problem discussed, for example, in the books by Mjelde (1983), Ibaraki and Katoh (1988), and Luss (2012).