@&#MAIN-TITLE@&#
Mahalanobis distance based on fuzzy clustering algorithm for image segmentation

@&#HIGHLIGHTS@&#
Introduce the Mahalanobis distance which is more efficient than the Euclidean distance to FCM based algorithm.Introduce a regularization term to improve the segmentation results.Combine the Mahalanobis distance and the regularization term with the FCM algorithm based on KL information.

@&#KEYPHRASES@&#
Image segmentation,Fuzzy C-means,Mahalanobis distance,KL information,Entropy,

@&#ABSTRACT@&#
Conventional Fuzzy C-means (FCM) algorithm uses Euclidean distance to describe the dissimilarity between data and cluster prototypes. Since the Euclidean distance based dissimilarity measure only characterizes the mean information of a cluster, it is sensitive to noise and cluster divergence. In this paper, we propose a novel fuzzy clustering algorithm for image segmentation, in which the Mahalanobis distance is utilized to define the dissimilarity measure. We add a new regularization term to the objective function of the proposed algorithm, reflecting the covariance of the cluster. We experimentally demonstrate the effectiveness of the proposed algorithm on a generated 2D dataset and a subset of Berkeley benchmark images.

@&#INTRODUCTION@&#
Until now, a great deal of attention has been paid to image segmentation [1–3]. There are many ways to implement the segmentation, and they can be divided into three categories in terms of their mathematic foundations: threshold-based [4,5], cluster-based [6,7] and statistics-based algorithms [8,9]. Among of them, cluster-based algorithms are widely used in image segmentation where Fuzzy C-means (FCM) algorithm [10–12] is one of the most popular methods due to its simplicity and extendibility. FCM algorithm uses the Euclidean distance to describe the dissimilarity measure between pixels and cluster centers. Besides the algorithm introduces the fuzzy set theory to define a fuzzy membership function and the exponentially weight of the fuzzy membership function can express the fuzziness of the objective function in the FCM algorithm. It means that if the pixel has a bigger dissimilarity measure with a cluster center it has a less degree to belong to the cluster and vice versa [13]. From image segmentation point of view, the FCM algorithm does not take the relations among neighboring pixels into account. Apart from that, the exponential weights in its objective function are a rather unnatural choice [14,15] and the divergences of intensities of pixels in segmented regions are also ignored. These make FCM algorithm sensitive to noise and cluster divergence.To improve the FCM algorithm, Kim et al. [16] suggested a fuzzy model which can express a given unknown system with a few fuzzy rules. Krishnapuram and Keller [17] proposed the Possibilistic C-Means (PCM) algorithm, in which the membership of the data-points is interpreted as its possibility belonging to a class and an appropriate objective function is constructed from a possibilistic point of view. Detroja et al. [18] proposed a new approach for fault detection and isolation based on the possibilistic clustering algorithm. Pal et al. [19] proposed Possibilistic-Fuzzy C-Means (PFCM) model and used it as a candidate for fuzzy rule-based system identification. Gong et al. [20] introduced a tradeoff between weighted fuzzy factor and a kernel metric to improve the FCM algorithm. Tan et al. [21] presented a novel initialization scheme to determine the cluster number and obtain the initial cluster centers for the FCM algorithm. In spite of those ways, the FCM algorithm improves mainly the following three aspects: neighborhood system, objective function and dissimilarity measure.From a neighborhood relationship point of view, Ahmed et al. [22] presented FCM with constrains (FCM_S) algorithm, in which the Euclidean distance from a pixel to a cluster center and the Euclidean distance from its neighboring pixels to the cluster center are calculated, respectively, and then a dissimilarity measure between the pixel and the cluster center is defined by using the weighted averaged value of those distances. Although they introduced neighborhood information into the objective function, the algorithm is insufficient to outliers. Moreover, in algorithm iteration, the Euclidean distances from neighboring pixels to cluster centers needed to be calculated repeatedly. This makes the algorithm time-consuming. Wherefore, Szilagyi et al. [23] proposed the Enhanced FCM (EnFCM) algorithm in such a way that a linearly-weighted sum image is in advance formed by weightily averaging each pixel and its neighborhoods. Although the smoothed image may decrease the affect of noise, the detail information is lost during the smoothing process and this will lead to misclassifications. In addition, the weighted coefficient which may affect the segmentation result is given by users. Cai et al. [24] defined a similarity measure which is related to neighborhood and spatial information as the weighting coefficient in Fast Generalized FCM (FGFCM). They used two parameters to control the influence of neighborhood system and spatial information in regularization. To speed up the algorithm, EnFCM and FGFCM carry out image segmentation on gray levels rather than pixels. Unlike [24], Krinidis and Chatzis [25] proposed the Fuzzy Local Information C-Mean (FLICM) algorithm and used spatial and spectral information of neighboring pixels to define a fuzzy factor, which controls the balance between noise reduction and detail protection. The algorithm is performed on the original image rather than filtered image as used in [23,24] which may cause detail missing.In addition, some researchers attempted to redefine the objective functions to improve FCM algorithm. In the proposed Entropy-based FCM (EFCM), Miyamoto and Mukaidono [26] used a regularization, which is implemented by information entropy, to define their objective function and a coefficient of the regularization to indicate the fuzziness of the objective function. However, EFCM may lead to misclassification for divergent clusters (clusters with large variances). To overcome this problem Ichihashi et al. [27] proposed KL (Kullback–Leibler) information-based FCM (KLFCM), in which an additional variable in regularization is used to improve the KL information. Furthermore, Miyamoto et al. [28] proved that the additional variable can control the cluster size.Apart from improving the objective function, the definitions of dissimilarity measures can also improve the FCM algorithm. Euclidean distance defines the straight line distance between two data points in feature space. As a dissimilarity measure, it is sensitive to noise because it does not consider covariance of data points. Therefore, Euclidean distance based FCM algorithm can be improved by defining dissimilarity measures with other distance metrics. Chen and Zhang [29] used the Kernel-induced distance as dissimilarity measure in FCM algorithm and showed that the kernel method is an effective approach to construct a robust image clustering algorithm. However, the Kernel-induced distance utilizes only mean information of clusters in an image. To take more information into consideration, Carvalho et al. [30] used the mean vector of an image and a symmetric matrix to define the adaptive quadratic distance as a dissimilarity measure, where the symmetric matrix can be defined in several ways, hence different definitions lead to different results. Han et al. [31] applied the divergence distance which combines both mean and covariance information to the FCM algorithm. By comparing the results of algorithms with divergence distance and Euclidean distance, they proved the effectiveness of the algorithm based on divergence distance. Mahalanobis distance [32] is also defined by mean and covariance of a cluster but Krishnapuran and Kim [33] proved that the distance cannot be used directly as dissimilarity measure for designing cluster algorithm. So Liu et al. [34] used Mahalanobis distance by defining the matrix through pixels and their mean. When the matrix is a unit matrix, the Mahalanobis distance is equal to the Euclidean distance [35]. Liu et al. [34,36] added the log determinant of the covariance matrix to Mahalanobis distance to define a new dissimilarity measure. A data point that falls together with the mean of a cluster would have a non-zero dissimilarity measure indicating that this measure is not a well defined distance. This paper proposed a Mahalanobis distance based FCM (MFCM) algorithm in which Mahalanobis distance is used to define the dissimilarity measure and a regularization term is added to the objective function which is defined in reference [27].The paper is organized as follows. In Section 2, the differences of Euclidean and Mahalanobis distance are compared and the proposed algorithm is described in detail. In Section 3, a generated dataset on two dimension (2D) plane, a synthetic image and a subset of Berkeley benchmark images are segmented and the effectiveness of the proposed algorithm is evaluated qualitatively and quantitatively. A conclusion is given in Section 4.LetX={xi:i=1,2,…,N}be the observed image, where i is in responding of the pixel index,xi=(xi1,xi2,…,xid)Tis the feature vector of pixel i, d is the dimension of the pixel, and N is the number of pixels in imageX.The objective function of the FCM algorithm can be formulated as(1)JFCM=∑i=1N∑j=1cuijmdijwhere c is the number of clusters, and j is the index of the cluster,U=[uij]N×cis the membership matrix expressing the fuzzy segmentation,uijrepresents the degree of membership with whichxibelongs to the jth cluster and satisfies∑j=1cuij=1. The fuzzy factor m is the weighting exponent ofuijand describes the degree of fuzziness of the algorithm,dij=‖xi−μj‖2represents the Euclidean distance measuring the dissimilarity between the pixel vectorxiand the mean vector of the jth clusterμj=(μj1,μj2,…,μjd)T. The FCM algorithm based on the Euclidean distance is sensitive to noise and the weighting exponent in objective function is suboptimal [37].Miyamoto and Mukaidono [26] proposed to use an entropy term as regularization for the objective function(2)JEFCM=∑i=1N∑j=1cuijdij+λ∑i=1N∑j=1cuijlog⁡uijwhere λ is the degree of the fuzziness of the algorithm. However, this objective function is sensitive to divagating clusters. Ichihashi et al. [27] proposed the KLFCM algorithm which improved the EFCM algorithm by using the KL information instead of entropy, that is,(3)JKLFCM=∑i=1N∑j=1cuijdij+λ∑i=1N∑j=1cuijlog⁡(uijαj)where the elements ofα={α1,α2,…,αc}control the sizes of clusters and avoid misclassification of edge points belonging to the larger cluster being segmented to the adjacent smaller cluster [28].Using Eq. (3) as objective function together with the Euclidean distance as dissimilarity measure still shows being highly sensitive to noise and divergence of clusters. To overcome this shortcoming, we introduce to use the Mahalanobis distance(4)dij=(xi−μj)TΣj−1(xi−μj)whereΣjis the covariance matrix of the jth cluster. WhenΣjis a unit metric, the Mahalanobis distance reduces to the Euclidean distance. To compare the Euclidean distance with the Mahalanobis distance, a dataset with three clusters has been generated in 2D plane where the data points subject to a Gaussian distribution. The mean vectors, covariance matrices and numbers of data points in clusters are listed in Table 1whereμjk(j=1,2,3;k=1,2) represents the components of the mean vector of the jth cluster andσjk(j=1,2,3;k=1,2,3,4) represents the components of covariance of the jth cluster. Fig. 1shows the generated dataset, in which the data points for Clusters 1, 2 and 3 are in red, green and blue, respectively, and the black and pink lines indicate the Euclidean distances(Ed)and Mahalanobis distances(Mdr,Mdg,Mdb). It is clear that the same Euclidean distance in each cluster corresponds to different Mahalanobis distances because the Mahalanobis distance takes the covariance of each cluster into account. Hence, Mahalanobis distance describes dissimilarities in different clusters more accuracy than Euclidean distance does.Fig. 2shows boundaries of clusters calculated by the Euclidean and Mahalanobis distances. The boundaries calculated by the Euclidean distance leads to straight lines and misclassify the data points which diverge far from their cluster center, while the boundaries based on the Mahalanobis distance lead to curves that coinciding with the curvature representing the covariance of the cluster. It means that when the covariance of a cluster is larger (for example Cluster 2), the Mahalanobis boundary can leave more space for it and when the covariance of a cluster is smaller (for example Cluster 1), the Mahalanobis boundary can narrow its area.According to the above analysis, it can be concluded that the Mahalanobis distance is more accuracy than the Euclidean distance when it is used as dissimilarity measure for image segmentation. Therefore, the Mahalanobis distance is introduced to the fuzzy clustering algorithm. However, if we use the Mahalanobis distance to the objective function by just substitute Eq. (4) into Eq. (3) directly, the optimal covariance matrix∑jcannot be calculated by minimizing the objective function. To solve the problem, Benaichouche et al. [37] defined a dissimilarity measure with the log determinant of covariance matrix. While the dissimilarity measure of a data point that coincides with the mean of a cluster would be equal to the log determinant of covariance matrix of the cluster rather than zero. This is not conformed to the definition of a distance. Therefore, in this paper, the Mahalanobis distance based FCM algorithm (MFCM) is proposed, in which the Mahalanobis distance is exploited to define the dissimilarity measure, and a regularization related to the covariance of the clusters, which can be used to solve the covariance matrix, is added to the objective function(5)JMFCM=∑i=1N∑j=1cuijdij+λ∑i=1N∑j=1cuijlog⁡|Σj|+λ∑i=1N∑j=1cuijlog⁡(uijαj)where the second term is the regularization expressing the disorder of the points in each cluster,|Σj|is the determinant of the jth covariance matrix. Through taking the covariance into consideration the proposed clustering algorithm using the Mahalanobis distance as dissimilarity measure can solve the problem that marginal points of a large cluster are segmented to a smaller cluster. With the second term in Eq. (5), covariance of clusters can be calculated by its partial derivation.To obtain the best segmentation result, the objective function should be minimized by the partial derivative of the objective function. The degree of the membership should satisfy the following constraint(6)∑j=1cuij=1As a result, to derive the optimaluij, the Lagrangian function has to be constructed as follows(7)L=∑i=1N∑j=1cuijdij+λ∑i=1N∑j=1cuijlog⁡|Σj|+λ∑i=1N∑j=1cuijlog⁡(uijαj)+∑i=1Nεi(∑j′=1cuij′−1)The optimaluijwhich can minimize L holds(8)∂L∂uij=dij+λlog⁡|Σj|+λlog⁡(uijαj)+λ+εi=0To minimize the objective function, let Eq. (8) be equal to 0, thenuijcan be calculated,(9)uij=αjexp⁡(−dij+λlog⁡|Σj|λ−λ+εiλ)Eliminateεifrom Eqs. (6) and (9),uijis written as(10)uij=αjexp⁡(−dij+λlog⁡|Σj|λ)∑j′=1cαj′exp⁡(−dij′+λlog⁡|Σj′|λ)whereαj, as a variable for controlling cluster size, satisfies the constrain(11)∑j=1cαj=1In the same way withuij, the estimate ofαjyields(12)αj=∑i=1NuijNThere are no constraints forμjandΣj, so minimizing the objective function in Eq. (5) yields(13)μj=∑i=1Nuijxi∑i=1Nuij(14)Σj=∑i=1Nuij(xi−μj)(xi−μj)Tλ∑i=1NuijThe process of the proposed algorithm can be summarized as follows.(1)Set constants, including cluster number c, fuzzy factor λ, maximum iteration number T and stopping condition η; Initialize iteration countert=0and membership matrixU(0)=[uij(0)]N×c.Calculate the mean vectors and covariance matrices of the clustersμ(t)={μ1(t),μ2(t),…,μc(t)}andΣ(t)={Σ1(t),Σ2(t),…,Σc(t)}according to Eqs. (13) and (14).Calculate the variableα(t)={α1(t),α2(t),…,αc(t)}according to Eq. (12).Calculate the membership matrixU(t+1)=[uij(t+1)]N×caccording to Eq. (10).If max{|uij(t+1)−uij(t)|}<η, or iteration countert=T, then stop; otherwise,t=t+1and go to step (2).

@&#CONCLUSIONS@&#
