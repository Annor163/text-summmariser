@&#MAIN-TITLE@&#
Maximizing lipocalin prediction through balanced and diversified training set and decision fusion

@&#HIGHLIGHTS@&#
Unsupervised Kmeans preprocessing for balancing and diversifying training set.Enhanced classification of lipocalins by fusion of classifiers.Superior generalization on blind testing data sets.ReliefF based feature ranking.

@&#KEYPHRASES@&#
Lipocalins,Diverse input patterns,Balanced training set,Boosted random forest,KNN,Classifier fusion schemes,

@&#ABSTRACT@&#
Lipocalins are short in sequence length and perform several important biological functions. These proteins are having less than 20% sequence similarity among paralogs. Experimentally identifying them is an expensive and time consuming process. The computational methods based on the sequence similarity for allocating putative members to this family are also far elusive due to the low sequence similarity existing among the members of this family. Consequently, the machine learning methods become a viable alternative for their prediction by using the underlying sequence/structurally derived features as the input. Ideally, any machine learning based prediction method must be trained with all possible variations in the input feature vector (all the sub-class input patterns) to achieve perfect learning. A near perfect learning can be achieved by training the model with diverse types of input instances belonging to the different regions of the entire input space. Furthermore, the prediction performance can be improved through balancing the training set as the imbalanced data sets will tend to produce the prediction bias towards majority class and its sub-classes. This paper is aimed to achieve (i) the high generalization ability without any classification bias through the diversified and balanced training sets as well as (ii) enhanced the prediction accuracy by combining the results of individual classifiers with an appropriate fusion scheme. Instead of creating the training set randomly, we have first used the unsupervised Kmeans clustering algorithm to create diversified clusters of input patterns and created the diversified and balanced training set by selecting an equal number of patterns from each of these clusters. Finally, probability based classifier fusion scheme was applied on boosted random forest algorithm (which produced greater sensitivity) and K nearest neighbour algorithm (which produced greater specificity) to achieve the enhanced predictive performance than that of individual base classifiers. The performance of the learned models trained on Kmeans preprocessed training set is far better than the randomly generated training sets. The proposed method achieved a sensitivity of 90.6%, specificity of 91.4% and accuracy of 91.0% on the first test set and sensitivity of 92.9%, specificity of 96.2% and accuracy of 94.7% on the second blind test set. These results have established that diversifying training set improves the performance of predictive models through superior generalization ability and balancing the training set improves prediction accuracy. For smaller data sets, unsupervised Kmeans based sampling can be an effective technique to increase generalization than that of the usual random splitting method.

@&#INTRODUCTION@&#
Lipocalins are a part of calycin super-family along with FABPs (fatty acid binding proteins), Triabin, avidins and metalloprotease inhibitors (Bo Akerstrom et al., 2006). Lipocalins have significant diversity at the sequence level and perform a wide variety of biological functions. Apart from their diversity at sequence level as well as in functionalities, they are found in a variety of the organisms ranging from unicellular bacteria to multi-cellular plants and animals. Initially they are identified as the transporters of small hydrophobic molecules. Later they are found to be involved in immune-modulation (Logdberg and Wester, 2000) and are used as biomarkers for various diseases (Xu and Venge, 2000). Lipocalins are also found to have important roles in cell regulation and cancer (Bratt, 2000). Some of the animal lipocalins are found to behave as allergens (Virtanen et al., 1999). Artificial lipocalins are known as Anticalins (Skerra, 2008). Anticalins are being engineered to have highly specific molecular recognition functionality and offer a profitable technology over the conventional antibodies as promising reagents.The experimental determination of lipocalins is an expensive and time consuming process. Moreover the detection of putative lipocalins using sequence similarity search methods is far elusive as the members of the lipocalin family share very low sequence similarity (Flower et al., 2000) often below the twilight zone (Rost, 1999). However the crystallographic structure of lipocalins reveals a conserved folding pattern that consists of eight beta strands and three structurally conserved regions (SCRs). This conserved pattern is having a close similarity with the one which is found in FABPs (Flower et al., 1993). Hence the presence of the structurally conserved pattern had inspired researchers to use the machine learning based prediction methods such as SVM for identifying the structurally diverse lipocalins by using sequential and structural features (Pugalenthi et al., 2010; Ramana and Gupta, 2009). Basically the sequence similarity scores are obtained by using the computationally significant comparison methods using sequence alignment algorithm, etc. In a nutshell, these methods use the primary sequences as their inputs. Whereas the machine learning methods use the underlying biological significant features that are extracted from the primary sequences as their inputs. So an intelligent pre-processing of input data for extraction of useful biologically significant sequence features is required. The appropriate choice of the extracted features will dictate the degree of success in solving the problems by applying machine learning methods.In specific both of the methods (Pugalenthi et al., 2010; Ramana and Gupta, 2009) used the features derived from the predicted secondary structure and evolutionary information in the form of position specific scoring matrices (PSSM) along with other sequence based features. A detailed sequence and structural analysis of lipocalins was carried out to deduce the lipocalin fold and assigned LIR2 to lipocalin family by Adam et al. (Adam et al., 2008).Previously machine learning methods have been successfully used for annotating the protein sequences belonging to various specific protein families (Chou, 2001; Pugalenthi et al., 2007; Shen and Chou, 2007; Kandaswamy et al., 2013). In this paper, we have attempted to enhance the prediction performance by using protocols for diversifying and balancing the training set as well as by applying classifier fusion schemes. Diversified training data set yields greater generalization ability and balanced training data set provides unbiased prediction performance. The classifier fusion schemes were used to achieve improved prediction accuracy in comparison to that of any individual classifier. The unsupervised Kmeans clustering was used to create the balanced and diverse training set and probability based fusion scheme for combining the results from the classifiers. The results of the experiments using these protocols have established that a balanced and diverse training set facilitates the machine learned models to have the superior generalization ability with unbiased performance as compared to that of a randomly created training set.We have chosen the Ramana and Gupta datasets (Ramana and Gupta, 2009) for immediate comparison and robust analysis. This dataset consist of two parts, the first consists of 136 lipocalins and 166 non lipocalins for training and testing the models. The second part consists of 42 lipocalins, 25 FABPs and 28 Triabins, and is completely separate and mutually exclusive of the first. This second part of the data set is exclusively used for testing the machine learning models in order to get the unbiased prediction metrics and henceforth referred as the test set II.The selection of apposite input features for any machine learning model plays an important role in accurately classifying the input instances. Discovering the best combination of direct and derived features that are distinctively responsible for accurate classification is an extremely difficult task as there is no standard technique available for it. However, one can try to identify them through trial and error basis. A better combination of input features for a given classification problem can be identified through intelligently experimenting with different combinations of features with the aid of the problem's domain knowledge. For this study, we have used a combination of three sequence-based features, namely: amino acids composition, property group composition and physiochemical n-grams feature. The descriptions about these three features are described as follows:The percentage composition of each of the twenty different amino acid residues (aa) is used as the first component of the input feature vectors and calculated using the formula:(1)PCaa,i=Caa,iCres,i×100where aa denotes a specific one of the 20 amino acid residues, PCaa,idenotes the amino acid percentage composition of specific type ‘aa’ in the ith sequence. Caa,idenotes the total count of amino acid of specific type aa in the ith sequence. Cres,idenotes the total count of all residues in the ith sequence (i.e. sequence length).The percentage composition of different amino acid property groups is used as the second component in the input feature vector. The eleven different amino acid property groups (Nath et al., 2013) which are chosen for this purpose are given in Table 1. The percentage counts of amino acid property group are calculated using the formula:(2)PCpg,i=Cpg,iCres,i×100where pg denotes a specific one of the 11 different amino acid property groups. PCpg,idenotes the percentage composition of specific amino acid property group ‘pg’ in the ith sequence. Cpg,idenotes the total count of specific amino acid property group ‘pg’ in the ith sequence. Cres,idenotes the total count of all residues in the ith sequence.This feature is used as the third component in the input feature vector and captures conservation of multiple physicochemical groups along the sequence. We have taken a sliding window of length 2. In a sliding window, if both the amino acids share one or more same physicochemical groups, then the frequency of those physicochemical groups counts are incremented. The same eleven physicochemical groups as mentioned in Table 1 were taken for this purpose.(3)Physicochemical−2grams:Tiny=Σi=1N−1CT(i,i+1)where N denotes length of the protein sequence. CT denotes tiny count which is equal to one if two consecutive amino acids at positions i and (i+1) belonging to tiny amino acid property group (T) otherwise zero. i denotes the variable position along the amino acid residue in the protein sequence which varies from 1 to N−1.If the conditions(aai∈T*)and(aai+1∈T*)are satisfied simultaneously, then CT(i, i+1) is equal to 1 otherwise 0 where T*={Ala,Cys,Gly,Ser,Thr}. In the similar way the physicochemical-2g for the remaining ten physicochemical groups are calculated.Given: sample sequence: ‘AAARNDD’Sliding window size=2.Initialize: the initial frequency count for each physicochemical group is set to zeroThe possible physiochemical 2-g are: AA, AA, AR, RN, ND, and DD.AA→increment the counts for tiny, small, non polar and hydrophobic groups by 1.AA→increment the counts for tiny, small, non polar and hydrophobic groups by 1.AR→no increment.RN→increment the counts for polar and hydrophobic groups by 1.ND→increment the counts for small, polar and hydrophobic groups by 1.DD→increment the counts for small, polar, charged, acidic and hydrophobic groups by 1.Training with a diversified data set, which includes every concept and sub-concept of the entire input space belonging to both positive and negatives classes, is very essential for complete learning in any supervised learning model. Both the inter-class imbalance (that causes bias towards the dominating classes) and the intra-class imbalance (that preclude some subclass instances within a particular class) in the training set often tend to produce majority class/subclass classifier which in turn degrades the prediction performance of machine learning models (Jo and Japkowicz, 2004). One of the reasons behind incomplete learning and imbalance data set is the presence of rare cases or less common cases. The creation of training set through random selection will cause the over-representation of the more common cases and under-representation or no-representation of the rare/minority cases in the training set and thereby the model will have less or no opportunity to learn the rare case sub-concepts. Ideally, there should be a balanced representation of both common and rare cases (sub-classes) from all the classes in the training data. So ideally a training set must consist of completely diverse and well balanced input instances which is crucial for perfect learning without any classification bias for achieving the true prediction performance of the classifier. In this paper, we have used the Kmeans clustering algorithm for creating the homogeneous groups in the data. The gist of clustering is given a similarity measure (clustering criterion), it tries to find hidden patterns in the dataset and groups together the more similar entities (Jain et al., 1999; Rui and Wunsch, 2005), and this is useful in creating the diversified data set.This algorithm partitions the given set of input instances into K clusters, namely C1, C2, C3… CK, and each of which is represented by their centroids. This algorithm starts with an initial number of clusters K, which is to be predetermined using some heuristic procedure in specific to a particular problem. The Kmeans clustering proceeds by minimizing the sum-of-squared distances between patterns to their corresponding cluster centroids (Macqueen, 1967; Larose, 2004). During iterations, each input instance is assigned to its nearest centroid according to the Euclidean distance between them. Then the centroid position is recalculated. This iterative process continues till the convergence criterion is satisfied. The convergence criterion may be a pre-defined number of iterations or no movement of instances between clusters, etc. The pseudo-code of the algorithm is as follows:All the feature vectors are normalized before applying the Kmeans clustering and to reduce the chance of sticking to the local minima, the clustering process is repeated for 10 times with a different set of the initial cluster centroid positions. The Kmeans clustering was applied separately on both positive and negative data sets.The Kmeans algorithm needs the initial value for K and to get adequate diversity the number of clusters for Kmeans clustering ‘K’ must be optimal. To determine the appropriate value for K (separately for both positive class and negative class instances), we have plotted graph with the ratio of intra-cluster variance to the inter-cluster variance along the y-axis and the number of clusters along the x-axis and the cluster number after which there is no significant decrease in the ratio was taken as the optimal value for ‘K’.An ideal training set constitutes a set of completely diversified and well balanced set of input feature vectors that is essential to achieve perfect learning. We have created our training set by selecting equal number of representatives from each cluster formed by Kmeans clustering on both positive and negative data sets, in order to satisfy the criteria for near perfect training set.(4)Training Set=T+∪T−(5)T+=∪i=1Kopt+Ri1+(6)T−=∪j=1Kopt−Ri1−where T+→set of equal representations from +ve class clusters. T−→set of equal representations from −ve class clusters. K+opt→optimal K for the positive class. K−opt→optimal K for the negative class. R+i1→first element of ith cluster of +ve class. R−j1→first element of jth cluster of −ve class.The remaining representatives from each cluster are used to form the testing set.Ensemble learning methods first train multiple learners and combine their outcome appropriately to solve the given problem. Ensemble classifiers usually outperform single classifiers and they are robust to the presence of noise in the data and to over fitting of inputs (Polikar, 2006). Boosting, bagging and stacking are three representatives of ensemble methods.Boostingis an iterative procedure (Freund and Schapire, 1996; Schapire, 2003) that combines many weak base learners linearly to construct a strong classifier with improved accuracy. This is also a kind of sequential ensemble method where the subsequent learners are evolved from the previously experienced learners. During each of the iterations, the incorrectly classified instances from the positive class and negative class data sets are given more weights so that the learning is concentrated on these hard and difficult to classify examples that are present in the training set.Bagging(Breiman, 1996) is implemented in the random forest classification algorithm. The Random Forest (Breiman, 2001) consist of many individual decision trees. Classifier ensembles promote an optimal trade-off between diversity and accuracy. Different base classifiers making errors in different parts of the hypothesis space give better accuracy when they are properly combined together. In random forest, bootstrap samples from the training set with randomly selected feature subsets are evaluated at each node of the decision tree. The final decision is made by decision fusion of all trees by majority voting. Random forests have been successfully applied to many classification and prediction tasks (Kandaswamy et al., 2011; Nath, 2012). Major steps of the random forest are summarized below:1.A bagged sample is drawn from the training data.A decision tree is grown without pruning on the bagged sample, where at each node a randomly selected subset of features from the full feature subset is evaluated.Fusing the decisions from all the individual trees.The AdaBoost_random forest classifier is the combination of AdaBoost(one of the promising variant of boosting method) and the random forest algorithm (a variant of bagging method). Recently, some of the authors have also successfully applied boosted random forest for classification and prediction (Thongkam et al., 2008; Saravanan and Lakshmi, 2013). The pseudo-code of the AdaBoost (Friedman et al., 2000) algorithm with random forest as a weak learner is given below:Real AdaBoost is one of the popular modifications of the AdaBoost algorithm. The major steps are same except that it involves the calculation of real valued class probability estimates. We have performed experiments using both discrete and real Adaboosting algorithms and the one which gave highest sensitivity (real Adaboosting) was chosen as the main classification protocol.This algorithm calculates the nearest neighbouring instances and assigns the class to the test instances by taking the majority vote of the class of the neighbouring instances (Larose, 2004; Witten and Frank, 2005). It does not learn the relationship between the different attributes and the class attribute. KNN (K nearest neighbour) is a lazy learning method in that it stores all the training examples and no explicit model is constructed. The new test instance is assigned to a class on the basis of the similarity measure between the stored training instances and the test instance. 1NN is the simplest case of KNN in which only the closest neighbour is considered and its class-value is assigned to the test instance. This classifier gave highest specificity and chosen as the second classifying protocol for the purpose of decision fusion. The pseudo-code of KNN algorithm is as follows:There are various decision fusion methods proposed in literatures (Kittler et al., 1998; Kuncheva, 2002; Kuncheva, 2004) for further improving the classification results. In this paper, we have analysed the results from the possible combinations of classifiers using different fusion schemes for selecting the best combination. We have used WEKA machine learning platform for implementing all the learning algorithms and decision fusion methods (Hall et al., 2009).The relative performances of the prediction models are evaluated through four parameters that are derived from the values of confusion matrix, namely TP: true positive (the number of correctly predicted lipocalins), TN: true negative (the number of correctly predicted non-lipocalins), FP: false positive (the number of incorrectly predicted non-lipocalins) and FN: false negative (the number of incorrectly predicted lipocalins). These performance parameters are calculated by using the following formulas.Sensitivity: expresses the percentage of correctly predicted lipocalins(7)Sensitivity=TP(TP+FN)×100Specificity: expresses the percentage of correctly predicted non-lipocalins(8)SpecificityTN(TN+FP)×100Accuracy: expresses the percentage of correctly predicted both lipocalins and non-lipocalins(9)Accuracy=TP+TN(TP+FP+TN+FN)×100ROC (receiver operating characteristic) is a curve between true positive rate and false positive rate at various threshold cut-offs. If this curve is closer to left and top side of the ROC space, then it indicates the prediction model is more accurate. ROC curves can be summarised by a single numerical quantity known as the AUC. It is an important statistical property to compare the relative performance of the prediction methods. AUC can take values from 0 to 1. The value of 0 for the worst case, 0.5 for worthless prediction and 1 indicates the perfect prediction.It is used as a valuable measure for selecting a model in binary classification problems and is a measure of both sensitivity and specificity. Its value ranges from −1 to +1, where a value of +1 means accurate prediction, a value of zero means random prediction and −1 means total disagreement.(10)MCC=(TP×TN)(FP×FN)(TP+FN)(TP+FP)(TN+FP)(TN+FN)

@&#CONCLUSIONS@&#
