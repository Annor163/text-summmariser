@&#MAIN-TITLE@&#
Implementing QR factorization updating algorithms on GPUs

@&#HIGHLIGHTS@&#
The first implementation of QR factorization updating algorithms on GPUs.We achieved speed-ups over full QR factorization of over 13× in some cases.The accuracies of our results were comparable with serial-computed solutions.

@&#KEYPHRASES@&#
QR factorization,QR updating,GPGPU computing,

@&#ABSTRACT@&#
Linear least squares problems are commonly solved by QR factorization. When multiple solutions need to be computed with only minor changes in the underlying data, knowledge of the difference between the old data set and the new can be used to update an existing factorization at reduced computational cost. We investigate the viability of implementing QR updating algorithms on GPUs and demonstrate that GPU-based updating for removing columns achieves speed-ups of up to 13.5× compared with full GPU QR factorization. We characterize the conditions under which other types of updates also achieve speed-ups.

@&#INTRODUCTION@&#
In a least squares problem we wish to find a vector x such that:minx||Ax-b||2where A is anm×nmatrix of input coefficients withm⩾nand b is a length m vector of observations. To do this we can use the QR factorization of A:||Ax-b||2=||QTAx-QTb||22=||Rx-d||22=R10x-fg22=||R1x-f||22+||g||22.R1is upper triangular and thus||R1x-f||22=0can be solved by back substitution, leaving||g||22as the minimum residual.This approach fits a linear model to observed data. For example, we can use it to model the relationship between an individual’s wage and variables such as their age, education and length of employment. Each row of A will contain the values of these variables for one person, with the corresponding entry in b being the observed value of their pay. In order to incorporate extra observations (in this example, people) we add rows to A, while if we wish to remove observations we must delete rows. Similarly, we can add variables to the problem by adding columns to A and remove them by deleting columns.QR factorizations are computationally expensive, but when elements are added to or removed from A it is not always necessary to recompute Q and R from scratch. Instead, it can be cheaper to update the existing factorization to incorporate the changes to A. We aim to accelerate the updating algorithms originally presented in [1] by implementing them on a GPU using CUDA. These algorithms have been shown to outperform full QR factorization in a serial environment [1], and we have previously demonstrated that their implementation on a GPU can outperform a serial implementation by a wide margin [2]. Other papers have investigated implementing full QR factorization on GPUs, for example by using blocked Householder transformations [3] or a tile-based approach across multicore CPUs and multiple GPUs [4,5]. Another study achieved speed-ups of 13x over the CULA library [6,7] for tall-and-skinny matrices by applying communication-avoiding QR [8]. Updating and downdating algorithms [9] have been implemented in shared- and distributed-memory parallel environments, including parallel out-of-core updating for adding rows[10], block-based downdating [11] and MPI-based parallel downdating [12]. To the best of our knowledge there is no prior work on implementing all four updating algorithms on GPUs.QR factorization decomposes them×nmatrix A into them×morthogonal matrix Q and them×nupper trapezoidal matrix R. Matrix updates entail the addition or removal of contiguous blocks of p columns or p rows. When a block of columns or rows is added during an update, this block is denoted U. The location of an update within A is given as an offset, k, in columns or rows from the top left corner. The updated matrix is denotedÃand has the corresponding updated factorizationQ̃R̃.Section 2 details the implementation of Givens and Householder transformations on the GPU. Section 3 summarizes the updating algorithms and describes how we implemented them, before Section 4 presents performance results. We show the speed-ups achieved by our implementation over the full QR factorization routine used by CULA. We also investigate the accuracy and stability of the updating algorithms. Section 5 concludes and discusses future work.Householder and Givens transformations are standard tools for computing the QR factorization [13,14] and they are also key components of the factorization updating algorithms. Householder transformations can be readily implemented on GPUs using CUBLAS [15]. To better exploit the instruction bandwidth of GPUs we use a blocked Householder approach [3,14] built on BLAS level 3 operations that combinesnbHouseholder transformations into a single matrix:P=P1P2…Pnb=I+WYTwhere W and Y are matrices with the number of rows equal to the length of the longest Householder vector and number of columns equal to the number of transformations.Efficient GPU parallelization of Givens rotations is more challenging. A Givens rotation alters entries in two rows of the matrix and therefore multiple rotations can only be conducted in parallel if they affect distinct rows. This leads to the following scheme, illustrated in Fig. 1, which has been applied on both distributed and shared memory systems [16]:1.Each processorp1,…,pkis assigned a strip of rows (Stage 1).Starting at the lower left corner of the matrix,p1zeroes the entries in the first column of its strip using Givens rotations (Stage 2).Whenp1reaches the top of the first column of its strip,p2takes over zeroing entries in the first column. In parallelp1begins zeroing entries in the second column (Stage 3).The algorithm continues in this way until the matrix is upper trapezoidal.We now summarize the QR updating algorithms originally presented in [1] and describe how we implemented them for GPUs.Adding a block of p columns, denoted U, to A before column k gives:Ã=A(1:n,1:k-1)UA(1:n,k:m).Multiplying through byQTyields:QTÃ=R(1:m,1:k-1)QTUR(1:m,k:n).SinceR̃(1:m,1:k-1)=R(1:m,1:k-1)we need only perform a QR factorization ofQTUR(1:m,k:n)to form the remainder ofR̃. To do this efficiently we use Householder transformations to makeQTU(n+1:m,1:p)upper trapezoidal, and then selectively apply Givens rotations to makeR̃upper trapezoidal. We cannot use Householder transformations for the second stage as they would makeR̃full.Note that the use ofQTUto update R means thatQ̃must also be formed if further updates are required. This can by accomplished by post-multiplying Q with the same Householder and Givens transformations used to formR̃.Fig. 2illustrates the algorithm for an example wherem=10,n=5,p=3andk=3. TakingQ,R, anm×pblock of columns U and an indexk,0⩽k⩽n+1as input, the implementation proceeds as follows:1.QR factorize the lower(m-n)×pblock ofQTUusing Householder transformations. This is the area shaded in red in Stage 1 of Fig. 2.Ifk=n+1then the update is complete.If not, transpose the section shown in red in Stage 2 of Fig. 2.Apply Givens rotations to make the transposed section upper triangular and also to update Q and d. These three updates are independent so separate CUDA streams can be used simultaneously for each.Transpose back the (now reduced) red section in Stage 2 of Fig. 2.When a block of p columns is deleted from A starting at column k, the modified data matrix becomes:Ã=A(1:m,1:k-1)A(1:m,k+p:n).Multiplying through byQT:QTÃ=R(1:m,1:k-1)R(1:m,k+p:n).AgainR̃(1:m,1:k-1)=R(1:m,1:k-1)and we can formn-(k-1)-pHouseholder transformations to reduce just the right-hand portion of R:Hn-p…HkR(1:m,k+p:n)=R̃(1:m,k+p:n).This QR factorization of a submatrix of R can be performed efficiently using blocked Householder transformations.Fig. 3shows the stages in the reduction process for an example wherem=10,n=8,p=3andk=3. Unlike adding columns, Q is not required for the update but R, an indexk,0⩽k⩽n-p+1, and a block widthnbare. The implementation proceeds as follows:1.If the right-most p columns of A were deleted, the update is complete.Blocked Householder QR factorization is applied to the(p+nb+1)×nbsubmatrix to the right of the removed columns (Stage 1). This is repeated across submatrices to the right until R is upper trapezoidal (Stage 2).When a block of p rows, U, are added to A, the updated data matrix is:Ã=A(1:(k-1),1:n)UA(k:m,1:n).We can permute U to the bottom ofÃ:PÃ=AUand thus:QT00IpPÃ=RU.With n Householder transformations we can eliminate U and formR̃:Hn…H2H1RU=R̃and becauseA=QR:Ã=PTQ00IpH1…HnHn…H1RU=Q̃R̃.As with the removing columns update we are performing a QR factorization on R to give an upper trapezoidalR̃. This can be done efficiently with blocked Householder transformations, and we can further reduce the operation count by exploiting the fact that R already has many zero entries. Instead of applying the full W and Y matrices to all of U and R, we instead apply the transformations separately to U and the non-zero part of R using a matrixV=[v1v2…vnb]with Householder vectors as its columns [1] and an upper triangular matrix T. The product ofnbHouseholder matrices can be defined as:H1H2…Hnb=I-VTVT,with:V=Inb0vm+1:m+pand:T1=τ1,Ti=Ti-1-τiTi-1V(1:p,1:i-1)Tvi0τi,i=2:nb,whereτiis the Householder coefficient corresponding to the Householder vector contained in theith column of V. As the assignment of individualτielements along the diagonal of T is independent of the calculation of the other elements of T, the entire diagonal can be assigned in parallel within a single kernel before repeated calls of CUBLAS gemv are used to form the rest of T.V and T are applied to the trailing submatrix ofRUby:I-VTTVTTRU=Im+p-kb-Inb0VTTInb0VTR(kb:kb+nb-1,kb+nb:n)R(kb+nb:m,kb+nb:n)U(1:p,kb+nb:n)=(Inb-TT)R(kb:kb+nb-1,kb+nb:n)-TTVTU(1:p,kb+nb:n)R(kb+nb:m,kb+nb:n)-VTTR(kb:kb+nb-1,kb+nb:n)+(I-VTTVT)U(1:p,kb+nb:n)and also used to update the right-hand side of the original problem:de=d(1:kb-1)(Inb-TT)d(kb:kb+nb-1)-TTVTed(kb+nb:m)-VTTd(kb:kb+nb-1)+(I-VTTVT)e,wherekbis the column index in the blocked update where the recently reduced block began, and e contains values added to b corresponding to the rows added to A[1].We calculate this update using R, an indexk,0⩽k⩽m+1, and ap×nblock of rows U. Note that the value of k does not affect the algorithm as the added rows can be permuted to the bottom of A. Fig. 4shows an example wherem=8,n=6,p=4. We proceed as follows for eachp×nbblock in U:1.Stage 1 in Fig. 4: use Householder transformations to reduce the block’s entries to zeros and to modify R’s corresponding nonzero entries.Construct T as described above.Stage 2 in Fig. 4: update R and b by multiplying with T and V.Removing a block of p rows from A from row k onwards gives:Ã=A(1:(k-1),1:n)A((k+p):m,1:n).We first permute the deleted rows to the top of A:PA=A(k:k+p-1,1:n)Ã=PQRand then construct a series of Givens matrices, G, to introduce zeros to the right of the diagonal in the first p rows of PQ, thus removing the rows of Q that correspond to the rows deleted from A. These transformations are also applied to R, which yields:PA=A(k:k+p-1,1:n)Ã=P(QG)(GTR)=I00Q̃SR̃givingÃ=Q̃R̃. Note that Householder transformations cannot be used because H would be full and constructed so as to eliminate elements of PQ, which would causeR̃to be full.We calculate this update usingQ,R, an indexk,0⩽k⩽m-p+1, and a block height p. Fig. 5shows an example of the reduction process wherem=12,n=5,p=4andk=5. A strip of rows Z is identified within Q corresponding to the removed rows fromA,Z≔Q(k:k+p-1,1:n).The implementation proceeds as follows:1.Assign the variable Z to the strip to be reduced by Givens transformations.Apply Givens transformations to reduce Q to the form shown in the centre of the top of Fig. 5, with zeros in the first p columns and in Z, and with the identity matrix embedded in the first p columns of Z.Apply the same Givens transformations to R as well.FormQ̃andR̃from the elements shaded red in Fig. 5.

@&#CONCLUSIONS@&#
