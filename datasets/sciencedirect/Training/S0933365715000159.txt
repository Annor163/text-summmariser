@&#MAIN-TITLE@&#
Automatic negation detection in narrative pathology reports

@&#HIGHLIGHTS@&#
Three methods were proposed to detect negation in narrative pathology reports.A machine learning-based approach shows potential advantages.A lexicon-based approach benefits most by customizations to the corpus.Negation rules and patterns were designed in a syntax-based approach.Different approaches for each section may improve the overall performance.

@&#KEYPHRASES@&#
Negation detection,Lexicon-based approach,Syntax-based approach,Machine-learning-based approach,Pathology reports,

@&#ABSTRACT@&#
ObjectiveTo detect negations of medical entities in free-text pathology reports with different approaches, and evaluate their performances.Methods and materialThree different approaches were applied for negation detection: the lexicon-based approach was a rule-based method, relying on trigger terms and termination clues; the syntax-based approach was also a rule-based method, where the rules and negation patterns were designed using the dependency output from the Stanford parser; the machine-learning-based approach used a support vector machine as a classifier to build models with a number of features. A total of 284 English pathology reports of lymphoma were used for the study.ResultsThe machine-learning-based approach had the best overall performance on the test set with micro-averaged F-score of 82.56%, while the syntax-based approach performed worst with 78.62% F-score. The lexicon-based approach attained an overall average precision of 89.74% and recall of 76.09%, which were significantly better than the results achieved by Negation Tagger with a similar approach.DiscussionThe lexicon-based approach benefitted from being customized to the corpus more than the other two methods. The errors in negation detection with the syntax-based approach producing poorest performance were mainly due to the poor parsing results, and the errors with the other methods were probably because of the abnormal grammatical structures.ConclusionsA machine-learning-based approach has potential advantages for negation detection, and may be preferable for the task. To improve the overall performance, one of the possible solutions is to apply different approaches to each section in the reports.

@&#INTRODUCTION@&#
A large portion of clinical findings mentioned in clinical reports (e.g., discharge summaries, radiology reports) are negated or discursive references about the disease in general. Accurately identifying whether these findings are present or absent for the patient's case is critical to extract pertinent information from the reports and index them. To detect whether particular findings are negated is of great significance to make a proper decision on diagnosis and prognosis. For example, constitutional symptoms such as fever, weight loss and night sweats are known to be of prognostic value in non-Hodgkin lymphoma (NHL) [1,2]. The presence or absence of these symptoms can be used to define two categories for each stage of NHL: A (if symptoms absent) and B (if symptoms present) [3].In our structured pathology reporting project [4], we have established a pipeline system to extract medical entities and populate structured reports based on the entities. Negation detection, as one of the most important components of the system, definitely can affect the performance of the whole system.Generally, negation detection includes the detection of negation cues (specific terms indicating negation) and scope (the text negated by the terms). In the following example, “no evidence of malignancy”, where “no evidence of” is the negation cue and “malignancy” is in the scope negated by the cue.Previous works suggest that a small set of words cover a large portion of negation cues. It is evident that “no”, “denies/denied”, “without”, and “not” are the most frequently used terms to indicate the absence of clinical observations [5]. Several rule-based approaches that utilized lexical pattern matching have been widely applied to the clinical domain. For example, Schadow et al. developed a parser that separated observation statements into smaller phrases by some punctuation, conjunctions or prepositions, and the entire phrase is negated by the negation cue detected on it [6]. However, such a simple method would not work well when a negated concept was separated from the negation cue by one of the separation signals above.Negfinder used a left-to-right, rightmost-derivation parser to detect negations in surgical notes and discharge summaries and achieved a sensitivity of 95.7% and a specificity of 91.8% without extracting syntactical structures of sentences and phrases [7]. However, it could not detect negated concepts correctly if the negation cue was far away from the intended target concepts, since it terminated a concept list or negation if there were more than three intervening words between concepts or between a negating phrase and a concept.NegEx, a regular expression-based algorithm, which is simple to implement, has shown its success in detecting negations in discharge summaries with a recall of 77.8%, and precision of 84.5% [8]. It relied on three types of terms to determine whether a condition is negated, namely trigger terms, pseudo-trigger terms, and termination terms. Trigger terms like “no” and “not” indicate that the clinical conditions within the scope of the trigger term should be negated. Pseudo-trigger terms, such as “not rule out” and “gram-negative”, appear to indicate negation but identify double negatives or modify meanings instead. The default scope of the negation is a five-word window. Termination terms, e.g., “but” and “though”, can terminate the scope of the negation before the end of the window. Since NegEx did not take into account any syntactic clue to determine the negation scope, it had faced difficulty in determining the scope of the negation phrase in some difficult cases. Without any customization, its application to the pathology domain had lower performance, probably because the negation and pseudo-negation phrases that were used by NegEx may not adequately cover the spectrum of phrases in pathology reports [9]. It also revealed that failure to correctly map the text phrase to the unified medical language system (UMLS) [10] concept was one major source of errors, as the augmentation with human coded concepts boosted the average recall and precision of the negation tagger by 28% and 13%, respectively.For complicated negation cases, defining negation scope is still a challenging task. The above approaches could perform reliably when a negated concept is close to a negation cue, but unsatisfactorily when they are separated with multiple words not mapped to a controlled terminology (e.g., UMLS, systematized nomenclature of medicine—clinical terms (SNOMED CT) [11]). To reduce some of NegEx's errors, Meystre et al. used a flexible window, which was extended to the end of the sentence, to the next negation or conjunctional phrase if closer [12]. Syntactic information is also useful to resolve this problem. A hybrid approach by combining regular expression matching with grammatical parsing has been purposed to detect negations [13]. The results showed that the structure grammar rules developed using linguistic principles were more powerful than detecting negated concepts at a fixed distance from negation cues. One of the limitations was the comprehensiveness of such a manually derived negation grammar. Another approach named DepNeg [14], used dependency parses which directly encode thematic roles like subject and object, performed better than the original clinical text analysis and knowledge extraction system (cTAKES) [15] in terms of F-score and accuracy (with about 1.6% and 1.2% increase, respectively) and was able to identify complicated negations that were wrong in cTAKES by a limited set of dependency rules compiled from a small data set.There has also been some effort with statistical or machine learning methods to detect negation as well. For example, as part of the assertion task, negation detected by machine learning techniques has been the state of the art in the 2010 i2b2 challenge [16]. For example, Patrick et al. established a conditional random fields (CRF) model by adopting several features, and gained more than 92% F-score on the “absent” category in the task [17].Inspired by the approaches mentioned above, we decided to use three different methods to identify negation in free-text pathology reports, which are lexicon-based approach, syntax-based approach and machine-learning-based approach. The lexicon-based approach highlighted the importance of the utilization of lexicons tailored to the domain; the syntax-based approach focused on the extraction of rules to cover the negation patterns based on the dependency output from the parser; the machine-learning-based approach emphasized on the extraction of useful features to build the statistic model.The study protocol was approved by Royal Prince Alfred Hospital, Sydney, Australia.In total, 284 pathology reports of lymphoma were collected for this study. They were scanned, optical character recognition (OCR)-ed and de-identified. We randomly selected 227 reports as a training set, and 57 reports as a test set.The corpus was annotated manually following the annotation guidelines developed according to Tumours of Haematopoietic and Lymphoid Tissue Structured Reporting Protocol (1st edition 2010) [18]. Besides instances of medical entity types were annotated, we also designed a linguistic category named “Lexical Polarity Negative” (terms of lexically bound polarity that is to negate a medical entity, such as “no” and “not”) to annotate phrases that trigger negation. The boundaries of most entities could cross the boundary of a base noun phrase (e.g., an “Architecture” entity—“diffuse effacement of normal architecture”, a “Diagnosis” entity—“malignant lymphoma of CLL type”), except that a “Clinical Impression” entity should be either a noun phrase or an adjective phrase. And for machine learning purpose, we also assigned a relation type called “Negate” between an instance of “Lexical Polarity Negative” and a medical entity. The annotation process was carried out with a two-phase validation and gold standard development. One computational linguist was trained to annotate the corpus, and then a senior computational linguist reviewed each annotation, as a validator for the development of the gold standards. Although intra-annotator agreement cannot be evaluated in this method, it is assumed that high level of consistency has been attained, as the overall F-score on the self-validation (a 100% train and test strategy: applies the computed model with baseline feature set to validate the training data until no improvement of the performance can be made [17]) is about 99.9%. This is probably because computational linguists can reliably achieve higher consistency than pathologists in annotating a large corpus of pathology reports, which was indicated in Patrick et al.’s study [19]. For some complex cases, we also consulted pathologists for advice. In addition, our pilot study on ten reports shows that over 95% consistency was attained between the negations annotated by a pathologist and those by a linguist. This indicates that linguists were competent to accomplish the annotation task, which is consistent with Vincze's work, which shows a consistency between 95% and 98% [20].In this study, only pertinent negations within a sentence were considered. Phrases that indicate uncertainty (e.g., “unlikely”) were not considered; normal findings and test results (e.g., “absence of CD15 expression”, “non-diagnostic changes”) were not considered either. Negative prefix or suffix was also not considered, because they are often semantically ambiguous or they are part of an entity (e.g., “non-Hodgkin malignant lymphoma”). Finally, 318 and 96 pertinent negations were identified in the training data and test set, respectively.

@&#CONCLUSIONS@&#
We developed three different approaches, which were lexicon-based, syntax-based and machine-learning-based to detect negation in narrative pathology reports. The results show that the machine-learning-based approach is equal best (statistically) and with the potential to improve with more extensive training data. The performances on most document sections vary by approach, indicating that to improve the overall performance one of the possible solutions is to apply different approaches to each section. The study also reveals that using a supervised machine-learning-based approach to extract medical entities can overcome some limitations of dictionary look-up method for concept recognition, although the incorrect extracted entities still account for most errors in the evaluations of the test set. However, due to the small sample size and the simple process for gold-standard development, a more thorough and comprehensive validation should be carried out. We plan to implement the negation detection module presented here to establish how it performs in our project to automatically populate structured reports from narrative pathology reports.