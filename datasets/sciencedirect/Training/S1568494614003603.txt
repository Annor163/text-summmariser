@&#MAIN-TITLE@&#
A multi-objective evolutionary method for learning granularities based on fuzzy discretization to improve the accuracy-complexity trade-off of fuzzy rule-based classification systems: D-MOFARC algorithm

@&#HIGHLIGHTS@&#
This contribution presents a fuzzy discretization procedure.The procedure is used for learning granularities and fuzzy partitions of fuzzy rule based systems.This procedure is integrated within a multi-objective evolutionary algorithm (MOEA).The MOEA concurrently performs a tuning and a rule selection process.The aim of the overall method is to improve the complexity-accuracy trade-off of fuzzy models.

@&#KEYPHRASES@&#
Fuzzy discretization algorithm,Granularity learning,Fuzzy rule-based classification systems,Evolutionary algorithms,Multi-objective evolutionary fuzzy systems,Complexity-accuracy trade-off,

@&#ABSTRACT@&#
Multi-objective evolutionary algorithms represent an effective tool to improve the accuracy-interpretability trade-off of fuzzy rule-based classification systems. To this aim, a tuning process and a rule selection process can be combined to obtain a set of solutions with different trade-offs between the accuracy and the compactness of models. Nevertheless, an initial model needs to be defined, in particular the parameters that describe the partitions and the number of fuzzy sets of each variable (i.e. the granularities) must be determined. The simplest approach is to use a previously established single granularity and a uniform fuzzy partition for each variable. A better approach consists in automatically identifying from data the appropriate granularities and fuzzy partitions, since this usually leads to more accurate models.This contribution presents a fuzzy discretization approach, which is used to generate automatically promising granularities and their associated fuzzy partitions. This mechanism is integrated within a Multi-Objective Fuzzy Association Rule-Based Classification method, namely D-MOFARC, which concurrently performs a tuning and a rule selection process on an initial knowledge base. The aim is to obtain fuzzy rule-based classification systems with high classification performances, while preserving their complexity.

@&#INTRODUCTION@&#
Linguistic Fuzzy Rule-Based Systems (FRBSs) are used to model complex non-linear relationships, in which the input–output variables are characterized by imprecision and vagueness. With respect to other modeling approaches, they attempt to describe phenomena in a more human-like way of thinking. Linguistic FRBSs have been extensively used in several fields and applications, in particular they have been usefully applied for classification tasks. In this case, they are often referred to as Fuzzy Rule-Based Classification Systems (FRBCSs) [1].A FRBS can be designed by an expert by hand or can be generated automatically according to a set of data that describes a certain phenomenon. Since the automatic definition of a FRBS can be considered an optimization problem, in the last decades the use of Evolutionary and in particular Genetic Algorithms (EAs/GAs) has been proposed to generate FRBSs, exploiting their ability to find near optimal solutions in complex search spaces and to incorporate a priori knowledge. The hybridization between FRBSs and GAs is currently known as Genetic Fuzzy System (GFSs) [2,3].One of the main advantages of using linguistic FRBSs is their interpretability, that is the possibility for a human being to understand the fuzzy model [4]. In order to consider not only the accuracy, but also the interpretability during the design or optimization process, GFSs have been extended to a multi-objective approach: Multi-Objective Evolutionary Algorithms (MOEAs) have been used to design FRBSs with different trade-offs between two or more objectives, since they are able to find a set of compromise solutions when dealing with multi-criteria optimization problems [5,6]. Nowadays, the hybridization between MOEAs and FRBSs is often referred to as Multi-Objective Evolutionary Fuzzy Systems (MOEFSs) [7].In this proposal we focus on how to obtain highly accurate classification models, without neglecting their complexity. When building a FRBCS, several factors affect these two properties and among them one important factor is the definition of the initial Data Base (DB). When defining a suitable DB, one of the problems that arise is how to determine the number of fuzzy sets associated to the variables of the DB, i.e. the granularity of each variable, as pointed out in [8]. Thus, the first objective of this paper is to propose a novel granularity learning process to generate suitable granularities for defining the initial fuzzy partitions of the DB. Since this approach usually leads to an increased complexity of the generated models, we address this additional issue by integrating the granularity learning process within a multi-objective approach: in fact, although the main objective is the improvement of the model's precision, it has been demonstrated that a well-designed MOEFS manages to both improve precision and reduce complexity in comparison to a single-objective approach, due to the reduction of the overfitting and to a better exploration ability [9].In summary, the objective of this proposal is twofold: in a first stage we use an algorithm based on fuzzy discretization to extract suitable granularities and construct a DB, so that the initial DB model highly fits the considered dataset, thus fostering the accuracy. Then, we integrate this mechanism within a MOEFS, which evolves the initial model and takes care of maintaining or even decreasing the complexity while improving the accuracy. In particular, the proposed method, namely Multi-Objective Fuzzy Association Rule-Based Classification Model with granularity learning based on Discretization (D-MOFARC), comprehends the following steps:•Step 1. A Fuzzy Discretization Algorithm is designed, in order to learn automatically suitable granularities for each variable and to generate the correspondent fuzzy partitions. This approach is based on the concept of discretization [10,11], which represents the process of transforming the range of values of a continuous attribute in a set of intervals and assigning a discrete value to each interval. We extend this approach to the case of fuzzy partitions, considering attributes interdependencies. Therefore, after obtaining a set of intervals for each variable, a fuzzy set is assigned to each interval, instead of a discrete value, to obtain the fuzzy labels associated to each variable. This process has been integrated with a tree-based generation mechanism that considers attribute partitioning interdependencies.Step 2. An initial Rule Base (RB) associated to the previous fuzzy partitions is created by extracting candidate fuzzy association rules. To this aim, the first two steps of the Fuzzy Association Rule-Based Classification Model for High-Dimensional Problems (FARC-HD) method proposed in [12] have been used. Since the extracted rules do not use all the labels generated in the initial step, the initial fuzzy partitions are finally refined by removing the unused labels.Step 3. A new specific MOEA is designed to concurrently perform the tuning of MFs in the DB and the selection of rules in the RB. This algorithm is a modified version of the Strength Pareto Evolutionary Algorithm 2 (SPEA2)[13], and aims to improve the accuracy while maintaining the complexity of the initial model, at the same time. As said, the multi-objective approach has been preferred because it has been demonstrated that the use of the accuracy as first objective enhances models’ performance, while the use of the number of rules as second objective can limit models’ complexity [9].The proposed method has been tested over 35 datasets, including small size datasets and high dimensional and large scale datasets. The obtained results show that the proposed method statistically improves the precision, with respect to the results obtained when using a single-objective approach.This proposal is arranged as follows. Section 2 introduces some preliminary concepts about the problem of learning granularities and about discretization methods in general and the CAIM discretization algorithm [14] in particular. Section 3 illustrates the characteristics of the fuzzy discretization approach. In Section 4 the features of the proposed method are introduced and described in detail. Section 5 illustrates the experimental framework, in which the experimental setup is described and the obtained results are presented and discussed. Finally, in Section 6, some concluding remarks are pointed out.In this section some preliminary concepts are briefly introduced. The first part deals with the problem of specifying suitable granularities. In the second part the concept of discretization is described and a specific discretization algorithm, i.e. CAIM [14], is presented.A FRBS is mainly composed of a Knowledge Base (KB) and an inference engine system. The KB includes the fuzzy rules in the RB and the linguistic variable descriptions in the DB. The inference engine system is composed of a fuzzification interface to transform crisp data into fuzzy sets, an inference system that uses them together with the KB to produce the output, a defuzzification interface to transform the fuzzy output into a crisp value.Several approaches have been proposed to automatically design the KB of FRBSs, by using numerical data that provide a description of a problem. For example in [15], a method to automatically learn the data base of a Mamdani fuzzy system for regression problems is described. Regarding classification problems, most of the proposed approaches have focused on learning a RB, using a predefined DB. In this case, the main issue is the identification of suitable fuzzy partitions for the variables involved. In particular, one of the problems concerns the identification of the granularities associated with the fuzzy partitions in the DB. Granularities can affect both the accuracy and the interpretability of FRBCSs and the choice of suitable granularities is crucial, as discussed in [8]. In this contribution, the authors have pointed out that there does not exist a unique optimal granularity, but the choice depends on the type of the problem and also on the RB learning method.The easiest strategy consists in fixing a single a-priori granularity and creating uniform fuzzy partitions for all the variables [16,17]. Despite its simplicity, this approach can not be the most appropriate since it does not consider at all the available knowledge of a problem and usually leads to models with a bad trade-off between accuracy and complexity.Another possibility is to use multiple granularities for the same variable in the DB. For example, in [18] the authors have used a simple method to generate an initial set of candidate fuzzy association rules for classification, then they have applied a pre-screening approach to reduce the number of these rules. The associated DB has been created by considering multiple granularities. In a further stage, the initial RB is reduced by applying a genetic algorithm, therefore the selection of suitable granularities is promoted by the elimination of unnecessary rules, so that the granularities that never appear in any rule are discarded.Also in [19], the authors have used multiple granularities in the DB. The method they have proposed aims at learning linguistic rules to generate regression models, by means of a hierarchical system. To this aim, the structure of the KB of a FRBS has been extended in a hierarchical way, so that linguistic rules can be defined over linguistic partitions with different granularity levels. This approach improves the modeling of problems in certain subspaces.Two further proposals dealing with the problem of choosing suitable granularities can be found in [20,21], where two MOEAs have been proposed to learn FRBSs to generate regression models. In this case, the granularity learning process has been integrated in the evolutionary algorithm. To this end, the concepts of virtual and concrete fuzzy partitions have been proposed: the former has been defined by partitioning in an uniform way each linguistic variable with a fixed maximum granularity; the latter takes into account, for each variable, the granularities determined by the evolutionary process. Since RB and DB parameters have been defined on the virtual partitions, whenever a fitness evaluation is required, the virtual partitions are mapped to the concrete partitions by employing appropriate mapping strategies.Another interesting contribution to this topic can be found in [22]. In this work, the authors have used a two-levels MOEA to obtain a set of FRBSs with different trade-offs between accuracy and complexity. The presented approach uses a high level MOEA to determine the number of fuzzy sets for each variable and multiple distributed low-level evolutionary heuristics to tune the membership functions of the candidate variable partitions.In a recent proposal [23], a method has been presented to generate single granularity-based fuzzy classification rules for multi-objective genetic fuzzy rule selection. This method is made of four steps: in the first one, a set of candidate fuzzy rules is generated by a heuristic procedure using multiple granularities. At the end of this phase, a single granularity for each variable is chosen, according to the frequency of employed partitions and the importance of the multiple granularity-based extracted rules. Then, these granularities are used to extract again a set of fuzzy rules, which will be evolved by an MOEA to perform a multi-objective genetic fuzzy rule selection process.The concept of granularities applied to interval type-2 fuzzy models has been considered in [24]. In this contribution, the authors have proposed a method to design type-2 fuzzy models, exploiting a new performance index to drive the construction of the fuzzy models. The proposed index is formulated in a such a way that promotes the generation of fuzzy intervals that “cover” the experimental data, being at the same time as narrow (i.e. specific) as possible. The underlying idea is that an optimal granularity allocation throughout the membership functions used in the fuzzy model leads to the best design. The design process is performed by means of a genetic algorithm that helps in further improving the results.The problem of designing optimal linguistic terms is also addressed in [25], where the authors have proposed an approach to design FRBCSs that includes the genetic design of optimal linguistic terms. A genetic simulated annealing algorithm has been used to design the optimal linguistic terms and their fuzzy-set-based semantics (including the granularity) for each variable in the dataset.An interesting analysis has been presented in [26] about the intrinsic implementation of the principle of justifiable granularity [27] in clustering algorithms. In fact, clustering techniques represent another possibility to construct fuzzy models [28]. This proposal describes a first approach in the analysis of the relationships between the size of the cluster found by a clustering algorithm and the intrinsic implementation of the principle of justifiable granularity.Nevertheless, the methods described so far present some limitations when dealing with high dimensional problems, due to the increase of the search space and of the time required by the fitness evaluation in the MOEA.A possible approach for learning granularities is represented by discretization algorithms, which transform continuous attribute's values into a set of intervals and assign to each interval a numerical and discrete value. The aim is to minimize the number of discrete intervals while maximizing the interdependency between class labels and discrete attribute values, to prevent an excessive information loss during the discretization. A discretization scheme D on a continuous attribute A divides the continuous domain of A into n discrete intervals and can be represented as follows:(1)D:{[d0,d1],(d1,d2],⋯,(dn−1,dn]},where d0 and dnare the minimum and the maximum values of the attribute A, respectively, and the values diidentify the cut points for the discretization D.The process consists of two steps: first, the number of intervals for each attribute is found and then the boundaries of the intervals are determined. Usually the first task is not performed automatically and the number of intervals must be specified by the user. Discretization algorithms can be grouped into two categories:•Unsupervised (or class-blind) algorithms generate intervals without considering the class labels of each pattern.Supervised algorithms discretize attributes by considering the interdependence between class labels and the attribute values.We will focus on this latter type. In [14] a discretization algorithm is proposed, that automatically determines the number of intervals for data partitioning and concurrently finds the boundaries of each interval. The method is named CAIM, from the name of the criterion optimized to measure the dependency between the class C and the discretization D for the attribute A. The criterion is defined for a given attribute by means of a matrix, called quanta matrix. Table 1shows the quanta matrix for a generic attribute A, where S is the number of classes, qiris the total number of patterns that belong to the ith class and have the value of the attribute A in the interval (dr−1, dr], Mi+ is the total number of patterns belonging to the ith class, M+ris the total number of values of the attribute A that are in the interval (dr−1, dr], for i=1, ⋯, S and r=1, ⋯, n and M is the total number of patterns.The CAIM criterion is expressed as follows:(2)CAIM(C,D|A)=∑r=1n(maxr2)/(M+r)nwhere n is the number of intervals, maxris the maximum value among all qirvalues and i=1, ⋯, S.The space of all the possible discretization schemes is large and the optimal scheme cannot be identified using exact algorithms, due to the time necessary to perform this process. Thus, the CAIM algorithm searches for an approximation of the optimal scheme by finding local maximum values of the CAIM criterion. This approach has demonstrated to be computationally not expensive and to lead to good approximations of the optimal discretization scheme. This algorithm is executed in two steps:•For each continuous attribute A, a set of candidate interval boundaries B (i.e. the candidate cut points) and an initial discretization scheme are created: first, the minimum (d0) and maximum (dn) values of the attribute's domain are found and added to the set of candidate cut points. Then, all the distinct values of A in the dataset are considered and for each adjacent pair of values, a midpoint is calculated. All the midpoints are added to the set of candidate cut-points B. The algorithm starts by considering the a discretization scheme (D:[d0, dn]) that covers all the possible values of a continuous attribute and with the criterion GlobalCAIM=0.These new cut-points are added to the initial discretization according to the maximization of the CAIM criterion. From all candidate cut-points, the algorithm selects the point that gives the highest value of the CAIM criterion, then this point is added to the discretization if CAIM>GlobalCAIM or if there are less than S intervals in the discretization. In fact, it is assumed that each discretized attribute needs at least the number of discretization intervals to be equal to the number of classes, since this guarantees that the discretized attribute can improve the subsequent classification. After the addition of a cut-point to the discretization, the GlobalCAIM criterion is updated and assigned with the CAIM value of the added cut-point.A detailed description of the algorithm can be found in the original proposal [14]. The CAIM algorithm generates separately a single discretization D for each attribute. However, when considering FRBSs, there is an interdependency among attributes that reflects in the extracted rules, thus a discretization approach should take into account this aspect. Therefore, in the following section we propose a new discretization algorithm that tries to exploit the interdependence among attributes to generate fuzzy discretizations, i.e. the initial fuzzy partitions.In order to extract an initial set of fuzzy rules from a dataset, it is necessary to build an associated DB and in particular to choose a granularity for each variable [8,23]. We already highlighted how an approach in which the granularity is fixed by hand may not be the best choice, since it leads to models with a bad trade-off between accuracy and complexity.To this end, we present a fuzzy discretization method to generate automatically a promising set of labels for each attribute, without specifying any apriori granularity. For each attribute, the proposed approach generates a set of promising fuzzy partitions rather than a single partition, so that a wider number of labels will be available for the rule extraction process. The number of fuzzy partitions is fixed to a maximum of 3 for each attribute.As explained in the previous section, the CAIM algorithm can be used to discretize the input space of an attribute, but it does not consider possible dependencies among attributes to generate this discretization. Therefore, we have extended the CAIM algorithm and we have designed a novel method that takes into account attributes interdependences by using dependency trees: a tree is constructed for each attribute by setting this attribute as root node and the branches are used to assess the level of dependency between the root attribute and the other ones.In addition, we are interested not only in determining granularities (i.e. the number of fuzzy sets), but also in defining the initial fuzzy partitions (i.e. the shapes of fuzzy sets) for each attribute. Thus, once the discretizations are obtained, a further process is applied to fuzzify these discretizations, i.e. to transform them into fuzzy partitions.The proposed procedure, named Fuzzy Discretization Algorithm, comprehends the following three steps:•Ordering the attributes according to a fitness, based on the CAIM criterion.For each attribute, generating a set of discretizations.Transforming each discretization into a fuzzy partition to obtain the initial DB.When dealing with classification datasets, attributes are supposed to be more or less important for the classification task. To establish an order among them, the CAIM algorithm is initially executed and a discretization for each attribute is obtained. The discretizations are evaluated with respect to the CAIM criterion (2), thus the corresponding attributes can be ordered from best to worst. Therefore, the CAIM algorithm itself is used to establish an order among attributes. The construction of the dependency trees will follow this order, so that more important attributes will have more influence in determining the resulting fuzzy partitions.In this step a set of discretizations is generated for each attribute by using dependency trees. A dependency tree is a tree in which an attribute is set as the root node and then other attributes are added as children, by choosing the ones that present the highest interdependency with the father. The details of the process are explained in the following.At each step, to each node of the tree a specific discretization is associated. For the root nodes, in the previous step a discretization has already been created by the CAIM algorithm, but this discretization is discarded, since the CAIM algorithm usually promotes discretizations with a small number of cut points, especially when a dataset presents few classes. On the contrary, the first discretization for each attribute is generated by considering equidistributed intervals and by fixing the number of cut points to a higher value, in order to promote better granularities. In this proposal, the number of cut points for the initial discretizations has been fixed to 4, to generate at least 5 fuzzy labels for each attribute.Based on the discretization of each parent node, new attributes are selected as children in a recursive manner. Let's consider a parent node attribute A and its corresponding discretization DA. The input patterns are sorted with respect to the values of attribute A and they are grouped according to the intervals i=1, ⋯, I defined by the discretization DA, where I is the number of intervals of discretization DA. Let's now consider the group of patterns corresponding to an interval i. The CAIM algorithm is applied on these patterns, considering all the attributes Tj≠A (j=1, ⋯, S) and for each of them a discretization is obtained. These discretizations are evaluated according to the CAIM criterion and the attribute corresponding to the discretization that maximize the criterion is chosen as child node B for the interval i in the tree.This process is repeated for each interval in the discretization DA. After generating all the children of a node (i.e. a child for each interval) and their corresponding discretizations, the entire step is repeated recursively for each child (Fig. 1) and it stops when one of the following conditions occurs:•only two patterns of the dataset fall in an interval.the depth of the tree reaches Depthmax, fixed to 3.After this step, a dependency tree for each attribute has been built and for each node of the tree a discretization has been generated. However, for each attribute a maximum of three discretizations are stored, in particular the first encountered in the trees’ generation process, that are supposed to be the best ones. Thus, the order considered for the construction of the dependency trees influences the decision about the selected fuzzy partition.In this step, the discretizations corresponding to each attributes are transformed into fuzzy partitions, by applying the concept of strong fuzzy partition [2], which guarantees the fulfillment of many interpretability con- straints. Thus, a triangular fuzzy set is generated across each interval in the following manner: the centroid ciis set in correspondence with the middle point between the boundaries of the interval (di−1, di], while the left and right parameters βland βrof the labels between two adjacent centroids ciand ci+1 are set in correspondence with the adjacent centroids themselves (Fig. 2). This mechanism ensures the creation of strong fuzzy partitions.The discretization approach presented so far has been integrated within a new Multi-Objective Evolutionary method, which evolves an initial KB by performing concurrently a rule selection process and a tuning process. The aim is to improve the precision of the initial fuzzy model through the tuning of the MFs, while maintaining or decreasing the complexity by means of the selection of rules. This proposed method can be summarized in three stages:•Data base extraction. The Fuzzy Discretization Algorithm described in Section 3 is used to construct a set of initial fuzzy partitions.Rule base extraction. An initial RB associated to the previous fuzzy partitions is created. To this aim, the first two steps of the Fuzzy Association Rule-Based Classification model for High Dimensional problem (FARC-HD) proposed in [12] have been used. This method exploits the Apriori algorithm for mining fuzzy association rules. The synergy between the Apriori algorithm and fuzzy association rules leads to the construction of accurate classification models, in fact this method is able to extract a set of high quality classification rules from huge amounts of data in a reasonable time. Furthermore, unlike Wang and Mendel's method, this approach generates more interpretable rules, since in each rule just a subset of variables is used. After generating the initial RB, some of the labels belonging to the initial fuzzy partitions are never used in any of the extracted rules, therefore the initial partitions are refined by removing the unused labels and an initial DB is generated.Data base tuning and rule selection. A new specifically designed MOEA is proposed, which concurrently performs the MFs tuning of the DB and the selection of rules from the RB. This algorithm is a modified version of the Strength Pareto Evolutionary Algorithm 2 (SPEA2)[13], and aims to improve the accuracy while reducing the complexity of the initial model at the same time.The Fuzzy Discretization Algorithm generates a maximum of three fuzzy partitions for each attribute, which are used to derive the initial RB. To this aim, the first two steps of the approach presented in [12] have been applied. This method uses fuzzy association rules to codify the information extracted from a dataset, that are an extension of association rules, used to represent dependencies between itemsets in a database [29,30]. Fuzzy association rules can consider not only binary or discrete values, but also quantitative values, and can be used as classification rules if their consequent is expressed with a class label. The generation of the RB is performed in two consecutive steps. In the following we briefly describe these two steps (for a more detailed explanation see [12]).In this first step, a set candidate association rules is extracted: for each class a search tree is built, in order to list all the possible frequent itemsets of a class. Itemsets are constructed using a search tree. The root level of each tree (level 0) is generated as an empty set and all the one-itemsets constitute the first level of the search tree (level 1). The further level (level 2) for an attribute A is constructed by considering all the two-itemsets that combine the one-itemset of attribute A with all the one-itemsets for the other attributes. The same procedure is used to construct the following levels of the tree. No repeated itemsets appear in the tree. Once all the frequent itemsets have been listed, a candidate fuzzy association rule is constructed for each itemset, by setting the itemset itself in the antecedent and the corresponding class in the consequent. The maximum depth of the trees is fixed to three, in order to generate rules with few antecedent conditions, thus keeping the model simple.The rule extraction process generates a large number of rules, which can cause a problem of rule redundancy. To decrease this number by selecting only the best rules, a subgroup discovery technique is used, in particular the pattern weighting scheme described in [31]. Each pattern is associated to a weightw(i)=1/(i+1), where i stores how many times the pattern has been covered by a rule. Initially, all the weights assume the same valuew(0)=1. For each class, the algorithm selects the best rule, then the weights related to the patterns covered by this rule are decreased. In this way, the patterns that are still uncovered will have a greater possibility of being covered in the following iterations. When the i counter reach a threshold kt, the correspondent pattern is deleted. The remaining rules are sorted again and the procedure is repeated until either all patterns have been deleted, or there is no rule left in the rule base. To evaluate the quality of fuzzy rules, a modification of the wWRAcc’ measure described in [31] has been used. The wWRAcc’ measure has been modified in order to handle fuzzy rules. The new measure is defined as follows:(3)wWRAcc″(A→Cj)=n″(A·Cj)n′(Cj)·(n″(A·Cj)n″(A)−n(Cj)N)where n″(A) is the sum of the products of the weights of all covered patterns by their matching degrees with the antecedent part of the rule, n″(A·Cj) is the sum of the products of the weights of all correctly covered patterns by their matching degrees with the antecedent part of the rules, n(Cj) is the number of patterns of class Cjand n′(Cj) is the sum of the weights of patterns of class Cj.Since the rule prescreening process can remove a large number of rules, after this step it is possible that some of the labels that belong to the original fuzzy partitions do not appear anymore in any of the remaining rules. Therefore, the initial DB needs to be refined by removing all the unnecessary labels, thus contributing to decrease the complexity of fuzzy partitions.In this step, the knowledge base previously obtained is improved by concurrently applying a tuning of the DB parameters and a rule selection process. A modification of the SPEA2 algorithm[13] is designed, with the aim improving the accuracy-complexity trade-off. In the following subsections the main features of this algorithm are presented. They are:•Objectives.Coding scheme and initial gene pool.Crossover and mutation operators.Each chromosome is associated with a bi-dimensional vector, whose elements express the fulfillment degree of the following two objectives, respectively:•Classification error minimization. It is represented by the complement of the number of the classification rate, i.e. the error rate. To compute this classification error, the following function has been used:(4)Fitness(C)=1−#HitsNwhere #Hits is the number of patterns correctly classified and N is the total number of patterns.Complexity minimization. It is represented by the number of selected rules.A double coding scheme for both rule selection (CS) and tuning (CT) is used:Cp=CSpCTp, where Cpis the chromosome representing the individual p. TheCSp=(cS1,…,cSm)part is represented by a binary-coded string with m genes, where m is the number of initial rules. Each gene contains a value of “1” if the corresponding rule is selected, “0” otherwise. TheCTppart uses a real coding scheme and codifies three definition parameters for each triangular MF.CTp=C1C2…CnCi=(a1i,b1i,c1i,…,amii,bmii,cmii)i=1,…,nwhere miis the number of labels in the database for each of the n variables.Each triangular MF is represented as MFj=(aj, bj, cj), where j=(1, …, k) and k is the granularity of the considered fuzzy partition. Each MF parameter can assume the values within the following variation intervals:(5)[Iajl,Iajr]=[aj−(bj−aj)/2,aj+(bj−aj)/2][Ibjl,Ibjr]=[bj−(bj−aj)/2,bj+(cj−bj)/2][Icjl,Icjr]=[cj−(cj−bj)/2,cj+(cj−bj)/2]The first individual of the first population codifies the KB obtained by the previous step. The remaining individuals of the first population are generated randomly, with each value within the corresponding variation intervals.Crossover and mutation operators have been specifically designed. Each offspring is obtained in the following way.•First, the CTpart of the offspring is obtained by applying blend crossover (BLX)-0.5 [32] to the CTpart of the parents.Then, the binary part CSis generated depending on the CTparts of parents and offspring. For each gene in the CSpart, the following steps are performed:-Each gene of the CTpart which represents the corresponding MFs of the rule, is considered for both parents and offspring. The MFs of these three rules are extracted.Between the offspring rule and each parent rule, euclidean normalized distances are computed by considering the center points of the MFs involved in these rules. The differences between each pair of centers are normalized by the amplitudes of their respective variation intervals.The parent's rule closer to the offspring's rule is selected and its value is duplicated in the CSpart of the offspring.This process is repeated until each gene in the CSpart of the offspring is obtained. In each step four offsprings are generated, although after applying mutation only the two best offspring are maintained. This type of crossover prevents the recovery of a bad rule already discarded, while permits the recovery of a rule that can be still considered good due to its MFs configuration.The crossover operator performs a better exploration in the CSpart, therefore the mutation operator does not need to add rules. It simply changes randomly a gene value in the CTpart and sets to zero a random gene in the CSpart, with probability Pm.The application of these operators brings some advantages: the crossover between individuals with very different rules allows the algorithm to explore different parts of the search space, while the mutation promotes rule extraction, since it is used to remove unnecessary rules.Some changes have been introduced to the original selection mechanism of SPEA2, to improve the algorithm's search ability.•A mechanism to prevent incest has been included, based on the concepts of CHC presented in [33]. This avoid premature convergence in the real coding (CT) part, which has a greater influence on the algorithm convergence and represents a wider search space than the binary coding part (CS). In the CHC approach, parents are crossed only if their Hamming distance divided by 4 exceeds a threshold. To follow this approach, the real coding scheme needs to be converted in a binary one, thus each gene is transformed using a gray code with a fixed number of bits per gene (BGene). The threshold value is initially set to L=(#CT×BGene)/4, where #CT is the number of genes in the CTpart of the chromosome. This value is decreased by 1 at each generation of the algorithm, therefore in further generations closer solutions can be crossed.A restart operator has been introduced to renew the external population when we detect that all the crossover are allowed. Actually, to prevent premature convergence, the first restart is applied if 50% of crossovers are detected at any generation (the required ratio can be defined as %required=0.5). Each time the restart is performed, the required ratio is updated ad follows: %required=(1+%required)/2.The external population after the restart includes the individuals with the best value in each objective, and the remaining individuals are initialized as follows: the CSpart is copied from the most accurate individual, while the values in the CTpart are generated randomly. In this way, the most accurate and interpretable solutions obtained so far are preserved.Some constraints to the application of restart have been introduced: (a) a new restart cannot be applied if the most accurate solution has not been improved; (b) the restart is not applied at the end, when the approximation of the Pareto front is well formed and needs to be preserved; (c) restart is disabled if the midpoint of the total evaluations number is reached and it has been never applied before.A mechanism to promote the most accurate solutions has been introduced. At each stage of the algorithm, between restarting points, the number of solutions in the external population (P¯t+1) that can be used to constitute the mating pool is reduced progressively and the most accurate solutions are preferred. To this end, solutions are sorted according their accuracy and the number of eligible solutions is reduced progressively from 100% at the beginning to 50% at the end of each stage. This mechanism is disabled in the last evaluations (when restart is disabled too), in order to obtain a wide and well-formed Pareto front.The method proposed and described in the previous sections has been evaluated by comparing its results with the results obtained by applying a recent and well-performing GFS for classification problems, named a Fuzzy Association Rule-Based Classification model for High Dimensional problems proposed in [12]. This algorithm currently belongs to the state-of-the-art of classification algorithms, since it has been demonstrated to outperform in accuracy some of the most widespread classification algorithms. In particular, the results presented in [12] show that it performs better than three other GFSs (FH-GBML, 2SLAVE, SGERD), two approaches to obtain fuzzy associative classifiers (LAFAR and CFAR) and five classic approaches for associative classification (C4.5, CBA, CBA2, CMAR, CPAR).This algorithm has been demonstrated to generate good fuzzy rule-based models, in particular when dealing with high dimensional problems.The experiments have been performed by considering 35 real-world datasets, whose characteristics are described in Table 2. The number of instances (#Inst), the number of attributes (#Attr) (numerical and nominal attributes Num/Nom are highlighted, respectively) and the number of classes (#Cls) of each dataset are shown. The web link to the Knowledge Extraction based on Evolutionary Learning(KEEL)-data set repository is also reported [34], from which the datasets can be downloaded. The instances that presented missing values have been removed from the datasets (in particular, from cleveland and crx datasets).To carry the different experiments out, a ten-fold cross-validation model has been applied: each dataset has been randomly split into ten folds, each containing 10% of the patterns of the dataset. Then, a single fold has been used for testing and the remaining folds for training. The cross-validation process has been repeated ten times, with each fold used exactly once for testing. For each of the ten partitions, three trials of the algorithm have been executed and finally the results have been averaged out over 30 runs. Due to the multi-objective nature of the evolutionary algorithm included in the D-MOFARC method, for the comparison with the single-objective FARC-HD approach we have selected the most accurate solution by considering the training error and averaging it on all the Pareto fronts obtained by each run of the algorithm.To evaluate the results, statistical analysis has been adopted [35,36], in particular non-parametric tests, following the recommendations presented in [37], where a set of simple and robust non-parametric tests for statistical comparisons of FRBCSs has been described. The Wilcoxon's signed-ranks test [38,39] for pair-wise comparison has been used, considering a confidence level of α=0.05. A wider description of this test and a software to perform it can be found on the web site available at: http://sci2s.ugr.es/sicidm/.This section shows the results of the experiments described in the previous section. Table 3summarizes the average number of rules/conditions (#R/#C) and classification percentages in training (Tra) and test (Tst) of the most accurate solution from each of the obtained Pareto fronts, for the D-MOFARC approach and of the best solution for FARC-HD [12]. The overall mean values for each method are highlighted in the last row.The two methods have been compared by applying the Wilcoxon's signed-ranks test, in order to understand if they are statistically equivalent (null-hypothesis). When considering the accuracy, the Wilcoxon's test is based on computing the differences between the average errors on the test set, whereas when considering the complexity the test is computed by taking into account the average number of rules that are obtained by a pair of algorithms. A normalized difference DIFF has been adopted when considering the number of rules, which is defined as(6)DIFF=MeanRules(x)−MeanRules(RA)MeanRules(x)where MeanRules(x) represents the number of rules obtained on average by the x algorithm (D-MOFARC) and RA is the reference algorithm (FARC-HD). This difference expresses the improvement in percentage with respect to the reference algorithm.Table 4shows the statistics obtained by applying the Wilcoxon's signed-rank test on the accuracy achieved on the test set and on the number of rules, comparing the results obtained by applying the D-MOFARC approach with the results achieved by using the FARC-HD algorithm. The ranks R+ and R− and the p-value are shown and a further column is added to highlight if the null-hypothesis is rejected or not, with a significance level α=0.05.When considering the accuracy on test, the p value >α, therefore the test rejects the null-hypothesis of equivalence. The R+ and R− values highlight that the results of the proposed method outperform the results obtained by the FARC-HD algorithm. A further comparison has been drawn between the two methods with respect to the average number of rules of the most accurate solutions. In this case, the null-hypothesis is not rejected but the two approaches can be considered statistically different with a significance level of 0.065, i.e., a 93.5% confidence. Nevertheless, by looking at the average values presented in Table 3 we can notice that the average number of rules obtained by applying the D-MOFARC method is slightly lower than the average value obtained by applying the FARC-HD algorithm.Fig. 3presents a representative example of the rule sets obtained by the D-MOFARC (a) and the FARC-HD (b) approaches, respectively, when considering the third fold of the bupa dataset. As well, Fig. 4represents the rule sets obtained by the D-MOFARC (a) and the FARC-HD (b) approaches, respectively, when considering the third fold of the newthyroid dataset. In these specific cases, the figures highlight that the proposed method induces the generation of more compact rule sets and smaller granularities with respect to the FARC-HD algorithm.Although the results presented so far only consider the best solution (considering the training error) obtained by the D-MOFARC method, it actually generates a set of Pareto-optimal solutions, each of them representing a trade-off between accuracy and complexity. Even though it was not our main purpose, it is possible for a decision maker to choose the solution that better satisfies the required trade-off for a certain problem.To this aim, we have used a procedure already adopted in [40,41]. This procedure is based on the analysis of three representative solutions of the Pareto front approximations, i.e. the most accurate solution, called FIRST, the least complex solution, denoted as LAST, and the median solution between the FIRST and the LAST. To compute these solutions, for each of the 30 trials we have generated the Pareto front approximations and have ordered the solutions in each approximation from the most to the least accurate. Then, for each approximation, we have selected the most accurate solution (FIRST), the intermediate solution in the Pareto (MEDIAN) and the least complex solution (LAST). Finally, for the FIRST, MEDIAN and LAST solutions, we have computed the mean values and the standard deviation values over the 30 trials, considering the number of rules, the number of conditions, the accuracy of the training set and the accuracy of the test set. In Table 5we have reported the results described so far. In the second column, we have also reported the cardinality values of the obtained Pareto front approximations, averaged over 30 runs.In addition, we have represented some Pareto-optimal sets in Fig. 5, in particular for heart, bupa, pima and segment datasets, respectively. Each Pareto front has been drawn by choosing a representative fold of the considered dataset and reporting the number of rules in the x-axis and the correct classification rate on training in the y-axis. The Pareto-optimal solutions are represented with dark square symbols in the graphs. In addition, the correspondent values for the correct classification rate on test are depicted with light square symbols. We have also depicted the correct classification rate on training (dark triangle symbols) and test (light triangle symbols) of the solution obtained by the application of the FARC-HD algorithm on the same folds. The observation of the Pareto fronts points out that in some cases there exist solutions with a test accuracy level almost equivalent to the best solution's test accuracy, while their complexity is considerably lower than the best solution's complexity.

@&#CONCLUSIONS@&#
