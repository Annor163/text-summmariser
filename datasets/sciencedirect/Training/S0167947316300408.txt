@&#MAIN-TITLE@&#
A fast and objective multidimensional kernel density estimation method: fastKDE

@&#HIGHLIGHTS@&#
A multidimensional, fast, and robust kernel density estimation is proposed: fastKDE.fastKDE has statistical performance comparable to state-of-the-science kernel density estimate packages in R.fastKDE is demonstrably orders of magnitude faster than comparable, state-of-the-science density estimate packages in R.A Python-based implementation of fastKDE is available at https://bitbucket.org/lbl-cascade/fastkde.

@&#KEYPHRASES@&#
Empirical characteristic function,ECF,Kernel density estimation,Histogram,Nonuniform FFT,NuFFT,Multidimensional,KDE,

@&#ABSTRACT@&#
Numerous facets of scientific research implicitly or explicitly call for the estimation of probability densities. Histograms and kernel density estimates (KDEs) are two commonly used techniques for estimating such information, with the KDE generally providing a higher fidelity representation of the probability density function (PDF). Both methods require specification of either a bin width or a kernel bandwidth. While techniques exist for choosing the kernel bandwidth optimally and objectively, they are computationally intensive, since they require repeated calculation of the KDE. A solution for objectively and optimally choosing both the kernel shape and width has recently been developed by Bernacchia and Pigolotti (2011). While this solution theoretically applies to multidimensional KDEs, it has not been clear how to practically do so.A method for practically extending the Bernacchia–Pigolotti KDE to multidimensions is introduced. This multidimensional extension is combined with a recently-developed computational improvement to their method that makes it computationally efficient: a 2D KDE on 105samples only takes 1 s on a modern workstation. This fast and objective KDE method, called the fastKDE method, retains the excellent statistical convergence properties that have been demonstrated for univariate samples. The fastKDE method exhibits statistical accuracy that is comparable to state-of-the-science KDE methods publicly available inR, and it produces kernel density estimates several orders of magnitude faster. The fastKDE method does an excellent job of encoding covariance information for bivariate samples. This property allows for direct calculation of conditional PDFs with fastKDE. It is demonstrated how this capability might be leveraged for detecting non-trivial relationships between quantities in physical systems, such as transitional behavior.

@&#INTRODUCTION@&#
Numerous facets of scientific research implicitly or explicitly call for the estimation of probability densities. For example, the ubiquitous histogram is one of the most basic and commonly used tools for displaying information about relative occurrences of one or more quantities. Univariate histograms are by far the most common, and they are often used to ascertain or display probabilistic information that is otherwise difficult to encapsulate using simple measures of location or spread: e.g., Schumacher and Houze (2003) and Bony et al. (2004). Multivariate probability density functions (PDFs), which encode information about the joint occurrence of two or more related variables, are less commonly analyzed. However, multivariate PDF analyses expose or encode information that can provide unique insights and open new research pathways: e.g., Larson et al. (2002), Marchand et al. (2010), Lee et al. (2014) and AghaKouchak et al. (2014).As with univariate histograms, multivariate histograms are the workhorse for multivariate PDF analysis. More sophisticated methods than the histogram for estimating PDFs, including both parametric and non-parametric methods, are used less frequently in contemporary scientific research. However, these more sophisticated methods can provide unique and valuable benefits above what histograms can provide. Parametric methods, such as maximum likelihood parameter estimation, can yield compactly-encoded and high-fidelity quantitative information about the relative distribution of quantities. For example, if one can assume that two quantities are jointly and normally distributed, then a maximum likelihood estimation of the normal distribution parameters will yield 5 parameters corresponding to the means and covariances that uniquely and completely describe the relationship between the two quantities. Non-parametric methods, such as kernel density estimation (KDE), can yield an empirical estimate of the true PDF, without assuming any form for the distribution.These existing PDF analytic methods also have some drawbacks that can be prohibitive in certain situations. Parametric analysis requires an assumption about the form of the underlying distribution. If the distribution can be isolated to one of a finite number of forms, then a model selection method (e.g.,  Wit et al., 2012) can be used to choose the best PDF model. However, if such an assumption cannot be made, for example because the PDF may result from a mixture of some large number of unknown PDFs, then parametric methods may be unsuitable. Non-parametric methods (histograms and KDEs) are less restrictive, but both methods require selection of either a bin width or a kernel and kernel bandwidth. This choice is not always straightforward, and it may require some user-intervention for common use cases (Silverman, 1986). The difficulty of making this choice compounds with the addition of variables; the kernel bandwidth becomes a kernel bandwidth matrix for multivariate KDE, and there is a similarly complex choice to be made about bin boundaries for histograms. There are a variety of automatic bandwidth selection methods available, (e.g., smoothed cross validation Duong and Hazelton, 2005), but these methods have a number of drawbacks, such as poor performance for large sample sizes and/or complex distributions and high computational expense (Heidenreich et al., 2013). In a review of automatic selection methods, Heidenreich et al. (2013) recommend a variety of different methods, depending on dataset characteristics (including sample size, distribution smoothness, and skewness). These drawbacks appear to have inhibited their widespread adoption as a fundamental data analysis tool.To get around this difficulty of a potentially subjective choice of kernel shape and kernel bandwidth, Bernacchia and Pigolotti (2011) derive a method for objectively determining both the kernel shape and the kernel bandwidth (Luedicke and Bernacchia, 2014 implemented it in Stata). Effectively they define a Fourier-based (and typically low-pass) filter on the empirical characteristic function (ECF) of a given dataset that yields an empirical kernel that is optimal in the sense that the integrated, squared difference between the resulting KDE and the true PDF is minimized. They coin this the ‘self-consistent’ density estimate. O’Brien et al. (2014) show that a non-uniform fast Fourier transform (nuFFT) can be used to calculate the ECF in a computationally efficient and accurate way; they use this new method to automate the estimation of thousands of univariate PDFs from the output of a climate model. Theoretically, the Bernacchia and Pigolotti (2011) optimal KDE can be applied to datasets of arbitrary dimensionality, and O’Brien et al. (2014) note that the nuFFT can also be applied to efficiently calculate multidimensional ECFs. However, the Fourier filters implemented in both of these studies are inherently one dimensional (Bernacchia and Pigolotti, 2011; O’Brien et al., 2014).In this manuscript we augment the Bernacchia and Pigolotti (2011) KDE method such that it can be applied to datasets of arbitrary dimensionality. We show that when combined with the nuFFT-based ECF calculation of O’Brien et al. (2014), the resulting multivariate PDF estimation method is optimal, fast, and unencumbered by the need for user-selected parameters (Section  2). We further demonstrate that this method can be used to directly, rapidly, and accurately infer conditional probability distributions (Section  3). We apply this method to examine the distribution of precipitation and temperature in California, USA conditioned on global mean temperature (Section  4), which shows a number of intriguing features that prompt further investigation.In order to describe the way in which we have augmented the KDE method of Bernacchia and Pigolotti (2011), it is necessary to review some relevant details of the method. Bernacchia and Pigolotti (2011) originally presented their derivation for a univariate KDE, but a trivial substitution of vector notation into their derivation shows that it applies also to multivariate KDE. Consider a generic, multivariate kernel density estimatePKDE(x→)onD-dimensional multivariate datax→j(forj=1…Nobservations of variablesx→=(x1,…,xD)) using the arbitrarily-shaped kernelK(x→), which is equivalent to the convolution of the kernel function and the set of delta functions centered on the data:(1)PKDE(x→)≡1N∑j=1NK(x→−x→j)=1N∑j=1N∫ℛDK(s→)⋅δ(x→−x→j−s→)ds→,whereδ(x→)is the Dirac delta function (which can be viewed as the limit of a normal distribution as all elements of the covariance matrix approach 0).The KDE can be represented equivalently by its inverse Fourier transform pairϕKDE:ϕKDE(t→)≡ℱ−1(PKDE(x→))=κ(t→)⋅C(t→),whereℱ−1represents the multidimensional inverse Fourier transform from data coordinatesx→to frequency-space coordinatest→,κ≡ℱ−1(K)is the inverse Fourier transform of the kernel, andCis the ECF of the data defined as:(2)C(t→)≡1N∑j=1Neix→j⋅t→.Bernacchia and Pigolotti (2011) derive a transform kernelκˆthat statistically minimizes the mean squared difference between the true PDFPand the resulting optimal KDEPˆKDE. This optimal transform kernel is defined as:(3)κˆ(t→)≡N2(N−1)[1+1−4(N−1)N2|C(t→)|2IA→(t→)],whereIA→(t→)represents a frequency filter that is 1 for the set of frequenciesA→used in the KDE and 0 otherwise. We use the nomenclature of Bernacchia and Pigolotti (2011), where the setA→is referred to as the set of ‘accepted’ frequencies.In extending the Bernacchia and Pigolotti (2011) KDE method to arbitrary dimensions, there are a couple of considerations. The ECF can be calculated in a computationally efficient manner using a nuFFT as shown by O’Brien et al. (2014). The nuFFT method naturally extends to arbitrary dimensions (Greengard and Lee, 2004), so a multidimensional ECF can efficiently be calculated using nuFFT methods. However, the selection of the filterIA→requires special consideration for multidimensional KDE.The choice of the filterIA→is critical for the success of this KDE method. Primarily, the filter must be specified such that(4)|C(t→)|2≥Cmin2=4(N−1)N−2fort→∈A→.This primary filter thresholdCminis necessary for stability of the estimation method (i.e., positivity of the argument of the square root in Eq. (3) forκˆ). Secondarily, the setA→may exclude an additional subset of otherwise acceptable frequencies. The choice of this secondary set of excluded frequencies is somewhat arbitrary, though it must be bounded and it must grow with increasingNforPˆKDEto converge to the true distribution asNincreases (Bernacchia and Pigolotti, 2011). Bernacchia and Pigolotti (2011) define the original univariate filter by setting a cutoff frequencyt∗such that half of the ECF values within[−t∗,t∗]are above|Cmin|. O’Brien et al. (2014) choose a cutoff frequency that is defined by the first occurrence of three consecutive frequencies below|Cmin|; ECF values at frequencies above this cutoff are set to 0.Neither of the filters described by Bernacchia and Pigolotti (2011) or O’Brien et al. (2014) extend simply to multiple dimensions. The natural multidimensional extension of the Bernacchia and Pigolotti (2011) filter would correspond to choosing a cutoff threshold frequency hypersurfaceT(which corresponds to a curve inℛ2and a surface inℛ3) such that approximately half of the ECF values contained within are above|Cmin|. It is not immediately clear how to objectively and automatically choose such a surface for arbitrary dimensionality. The natural multidimensional extension of the O’Brien et al. (2014) filter would correspond to finding a contiguous and closed hypervolume of below-threshold values (|C|2<|Cmin|2) and setting values of the ECF outside that hypervolume to 0. Forℛ2this would correspond to a hypothetical 2D moat of below-threshold values surrounding the origin in frequency space. If such a moat exists, finding its contiguous hypervolume objectively and automatically is relatively straightforward using a standard flood-fill search algorithm. A contiguous set of below-threshold frequencies is guaranteed to separate high and low frequencies inℛ1; this property allowed its simple use in determining a cutoff threshold in the filter of O’Brien et al. (2014). Unfortunately however, this property is not guaranteed to hold in higher dimensions. It is entirely possible that, for example inℛ2, a contiguous hypervolume (area) of below-threshold values surrounding the origin does not exist. For a filter to be generically useful for multidimensional KDE, it should be guaranteed to work in all sensible cases.Instead of defining the set of accepted frequenciesA→such that above-threshold values (|C|2≥|Cmin|2) within some hypersurface are accepted and those without the hypersurface are rejected, we have defined a filter based on selecting a subset of contiguous hypervolumes of above-threshold values. We assume that the multidimensional ECF consists of a finite set of contiguous hypervolumes of above-threshold values. At least one such contiguous hypervolume containing thet→=0→frequency is guaranteed to exist, sinceC(0→)=1due to normalization, and|Cmin|2<1. In our Python/Cython-based implementation of this KDE method, we sort these contiguous hypervolumes based on the distance from their centers-of-mass to the origin. The user of the method has the option to specify how many of these hypervolumes (as an integer value or a fraction of the total number), in order of their distance to the origin, are retained in the setA→. By default (and throughout this manuscript), only the single hypervolume centered att→=0→is used. Fig. 1depicts this filter on one of the distributions discussed in Section  3.2. The colored contours in the center of Fig. 1 show the single hypervolume that is included in the construction of the KDE; the additional contiguous hypervolumes are ‘discarded’ (set to 0).This filter satisfies the convergence conditions described by Bernacchia and Pigolotti (2011). The set of frequencies included in this lowest contiguous hypervolume filter are bounded since they will always be contained within a finite-sized hypervolume around the origin. Further, for a given number of contiguous hypervolumes included in the setA→, this bound grows as the number of data points increases, since|Cmin|2∝N−1(meaning the volumes of the contiguous hypervolumes grow with increasingN). With these two conditions met, this filter should allow the KDE to converge toward the true PDF as the number of data points increases.To demonstrate the efficacy of the lowest hypervolume filter, we examine the convergence properties of our implementation of the Bernacchia and Pigolotti (2011) KDE method with the nuFFT-based speed improvement of O’Brien et al. (2014). For simplicity, hereon we refer to this implementation as the fastKDE method. Fig. 2(a) shows the integrated, squared error (ISE) of the fastKDE as a function of the number of data samples denoted byN. We approximate this integral using the midpoint rule:(5)ISE=∑i1=1i1=M1…∑iD=1iD=MD(PˆKDE(x→i)−P(x→i))2∏j=1DΔxj,wherePis the true PDF;M1andMDare the number of points in the 1st andDth dimensions (and there is a corresponding summation symbol for each dimension);xi→=(xi11,…,xiDD)is a regular multidimensional lattice of data samples; andΔxjis the spacing of the grid in thejth dimension (which is constant to due the use of regular grids).For direct comparison with univariate results from Bernacchia and Pigolotti (2011) and O’Brien et al. (2014), we test the convergence of a univariate standard normal distribution. Fig. 2(a) shows that the ISE decays close to, but slightly slower than,N−1. This is consistent with theoretical arguments about the convergence rate of the method, and it is consistent with empirical results (Bernacchia and Pigolotti, 2011; O’Brien et al., 2014). We perform the same tests for bivariate and trivariate standard normal distributions, and Fig. 2 shows that the method exhibits similarly good performance even for multivariate data. The convergence rate appears to systematically decrease as dimensionality increases, which is presumably due to the curse of dimensionality (Scott, 2008). However, the decrease is relatively minor, with the exponent ofNin the convergence rates for the 1, 2, and 3 dimensional KDEs being −0.92 ± 0.03, −0.91 ± 0.01, and −0.89 ± 0.01 respectively.Fig. 2(b) shows that the time required to compute the KDE asymptotically scales asN1for all three dimensionalities. This result is consistent with the complexity analysis of O’Brien et al. (2014), which suggests that the speed of the nuFFT-based KDE should scale as풪(N⋅qD+MD⋅log(MD)), whereqis the size of the convolution kernel used in the nuFFT,Nis the number of samples, andMDis the total number of grid points on which the KDE is estimated. For the convergence tests shown in Fig. 2, we useq=28(which yields approximately double precision accuracy for the nuFFT approximation) andM=257for each dimension. Accordingly, forN⋅qD≫MD⋅log(MD), the number of operations required for the estimate (and therefore the timeTrequired) should scale asT∝qDN1.The timing curves in Fig. 2(b) exhibit two intriguing features indicative of additional factors playing important roles in the computational aspects of the method: the curvature of the 3D timing curve, and the relative spacing of the timing curves. The timing curve for the 3D standard normal distribution has a minimum at approximately 104 samples; the 3D KDE for a sample size of 16 takes more time than for a sample size of 8192. We hypothesize that this is associated with the lowest contiguous hypervolume filter. The filter relies on an implementation of a classical flood-fill search algorithm, which first searches the ECF for a new above-threshold value and then recursively searches all neighbors for other above-threshold values.11While the algorithm is recursive, we specifically avoid the use of recursive functions in our implementation, since we have found that such an implementation can result in stack violations for large contiguous volumes.Once the recursive search is exhausted, the algorithm repeats until the entire ECF space has been searched. While this flood-fill search requires at leastMDfloating point comparisons, there are additional overhead costs associated with the management of the linked-list structure that we use to track the list of unsearched neighbors. If the ECF space consists of a large number of relatively small contiguous hypervolumes, then the search will incur a relatively large overhead, which could make the entire KDE method take more time than would otherwise be expected. The second intriguing feature of Fig. 2 is that the vertical spacing of the timing curves is smaller than predicted by the complexity analysis. The computational time should scale likeqD, meaning that the ratio of timings for the same number of points in the 3D vs 2D cases (or 2D vs 1D) should beqD/qD−1=q=28. This ratio, as expressed by the vertical spacing of these curves, is roughly the same for the 3D vs 2D and 2D vs 1D cases, in accord with this prediction. But the ratio is approximately 13 instead of 28. This means that the timing of this KDE method scales slower with increasing dimensionality that we would expect based on a simple complexity analysis of the algorithm. It is not immediately clear why the dimensional scaling of this method is better than we anticipated.Overall, Fig. 2 demonstrates that the fastKDE method exhibits both the excellent convergence properties of the Bernacchia and Pigolotti (2011) method and the excellent computational efficiency of the O’Brien et al. (2014) method for multivariate KDE. By design the normal distribution tests have no covariance structure, and so they do not illustrate whether the optimal kernel used in the fastKDE method can handle input data with non-trivial covariance. In developing this method, we anticipated that covariance should not be an issue, since the kernel effectively inherits the covariance properties of the underlying data. This can be shown theoretically by considering a 2nd-order Taylor expansion of the optimal kernel (Eq. (3)) aboutt→=0→:(6)κˆ(t→≈0→)≈1+12!∑k=1D∑j=1D2N−2(∂2C∂tj∂tk(t→=0→))tjtk(7)=1−1N−2∑k=1D∑j=1Dσjktjtk.Eqs. (6) and (7) are relatively simple compared to Eq. (3) because atC(t→=0→)=1(normalization), and because derivatives of the ECF at(t→=0→)are proportional to sample moments: the 1st order derivatives of the ECF are identically 0 at the origin assuming that the sample mean is removed prior to transformation,22In practice, we remove the sample mean prior to applying the nuFFT, and we add it back to the resulting real-space PDF estimate.and the 2nd-order derivatives of the ECF yield the 2nd-order moments (σjk) of the input samples. In comparison, the 2nd-order Taylor expansion of the ECF is(8)C(t→≈0→)=1−12∑k=1D∑j=1Dσjktjtk,which differs from Eq. (7) only by a factor of2(N−2)−1in the second term.This implies that in the vicinity of the origin, the optimal kernel has the same shape and orientation as the ECF, and therefore it has the same shape and orientation in real space. The main difference in the two Taylor approximations is that the optimal kernel has the factor of2(N−2)−1in the second term, which originates from the 2nd-order derivatives of the kernel. This implies that these derivatives, and therefore the variances and covariances of the optimal kernel, asymptotically approach 0 asNapproaches infinity; the kernel asymptotically approaches a multidimensional delta function asNincreases. For finiteN, however, the kernel effectively inherits its main shape and orientation from the underlying data.We demonstrate that these properties result in convergence even for PDFs with non-trivial covariance structure by examining the convergence properties of the fastKDE method on two additional bivariate PDFs: a PDF with a relatively subtle transition in the relationship between two variables, and a complex mixture of normal distributions. The first distribution, which we label the transition PDF (PT), is derived as follows: given an abscissa variable, the ordinate variable is sampled by first transforming the abscissa values by a sigmoid function and then adding samples from a Normal distributionN. The abscissa variable is sampled from a Gamma distributionΓ, with the abscissa values centered over the transition point of the sigmoid function. We construct a PDF in this manner to simulate a noisy transition in a bivariate physical system:(9)PT(x,y)≡Γ(x0−x+kθ|k,θ)⋅N(y|μ(x),σy),where(10)μ(x)≡Δy⋅tanh(x−x0)+y0,and(11)PT(y|x)=N(y|μ(x),σy).Fig. 3(a) showsPT(x,y)along with a fastKDE estimate based on 10,000 samples fromPT(x,y)using only the lowest contiguous hypervolume in the ECF filter. The contours of the fastKDE estimate andPT, which use the same contour interval, overlap strongly. This is true even in the region near point atx=305where the mean ofytransitions from a value of 198 to a value of 202, which induces a localized covariance inxandy. To illustrate the degree to which this non-trivial covariance information is correctly encoded by the fastKDE method, we estimate the conditional distributionPT(y|x)by dividing the bivariate fastKDE estimatePˆ(x,y)by the marginal fastKDE estimatePˆ(x). Fig. 3(b) shows that the contours of constantPT(y|x)from the fastKDE conditional estimate follow those of the true conditional distribution; in particular, the contours in the fastKDE estimate exhibit the sigmoid transition atx=305. However, the fastKDE estimate of the conditional is somewhat noisier: especially in data sparse regions, such as abovex=310. While it may appear that this transition PDF has hardly any covariance structure, such subtle transitions in real physical systems can result in profound changes in system behavior. Therefore, the ability to reliably discern such transitions from noisy data can be quite important for explaining the behavior of physical systems.Similar to the convergence of the fastKDE estimates of the standard normal distributions in Section  3.1, Fig. 2(a) shows that the fastKDE estimate of the transition PDF falls as a power law ofN. However, in contrast to the standard normal examples, the error improves at a slower rate: −0.803 ± 0.004. Despite the slower error improvement with increasingN, this power law behavior shows that the fastKDE estimate asymptotically approaches the true transition PDF as samples are added. This suggests that as theoretically predicted above, (a) the kernel is shaped and oriented in such a way that it provides a reasonably good PDF estimate for lowN, and (b) the kernel approaches a delta function asNapproaches infinity, giving data points only a very local influence on the PDF. The time required to compute the fastKDE estimate (Fig. 2(b)) is nearly identical to that required for the bivariate normal distribution: approximately 10 s for 2,000,000 samples.Interestingly, the mean ISE of the conditional estimate (calculated as the mean of the ISE of the individualPˆ(y|x)curves for eachxvalue) exhibits similar asymptotic convergence for relatively largeN. ForNlower than approximately 103, the mean ISE decreases relatively slowly with increasingN. ForNgreater than approximately 103, the mean ISE of the conditional estimate decays at a rate almost identical to that of the joint distribution. The additional time required to estimate the marginal ofxis essentially negligible, and so the time required to compute the conditional fastKDE is nearly identical to that of the joint fastKDE.From the perspective of data analysis, these are excellent properties for a data analysis tool. Many data analyses performed in the physical sciences assume a simple (linear) covariance structure: e.g., regression and/or correlation analysis. While the Spearman rank correlation coefficient deviates from the assumption of linearity inherent in the more common Pearson correlation coefficient, it still only provides an indication of the degree to which two quantities are related. In the synthetic example provided in Fig. 3(b), the conditional PDF shows what such analyses cannot: that there is a localized and relatively subtle transition in the two quantities. The ability to accurately, objectively, and rapidly estimate conditional PDFs could provide opportunities for physical insight that would be difficult or impossible to obtain using other methods.The second non-trivial bivariate distribution against which we evaluate the fastKDE method is a mixture of normal distributions. This mixture-distribution is specifically designed to challenge KDE methods: it has a complex structure that includes both long- and shortwave components, it has major structures oriented along several axes, and none of these structures are aligned with axes corresponding to the abscissa or ordinate values. We define this mixture distribution as a random sample from any one of 350 normal distributions. The first 349 distributions are normal distributions, with variances of 0.3 and no covariance, sampled with equal probability (1/678); and the last distribution is a broader normal distribution, with its covariance matrix defined by the covariance matrix of the means of the 349 normal distributions, sampled with probability 349/678.The locations of the first 349 normal distributions are defined as follows. The logo of the lead author’s home institution was resized to 54 by 72 pixels, and color intensity values were discretized to either 1 or 0. These values were rendered as an ASCII text value and hand modified to remove artifacts of the resizing and digitizing processes. The image-space indices of the non-zero values in the ASCII text, of which there are 349, are chosen as the initial coordinates of the first 349 normal distributions. These initial coordinates are then transformed such that the coordinates are centered at(0,0)and they are rotated by an arbitrary 37° degrees counterclockwise. The location of the last normal distribution is chosen to be centered on(0,0)with a width and height that spans the first 349 normal distributions and with principal axes parallel to the abscissa and ordinate axes. This produces a mixture-distribution with a long-wave feature (the last normal distribution) combined with a complex structure of relatively shortwave features (the 349 normal distributions) with major structures aligned with two different axes: along the coordinate axes and along 37° and 127° counterclockwise from the abscissa axis. Fig. 4(a) shows the resulting PDF.Fig. 4(b) and (c) show a fastKDE estimate of 106 and 104 samples from the mixture-distribution shown in Fig. 4(a). The estimate from 104 points shown in Fig. 4(c) represents the general features of the true PDF, with proper orientation, including the wording at the bottom right of the PDF and the orientation and general form of the building to the upper left of the wording. However, this estimate lacks many of the fine details, and one can barely discern that the feature in the lower right indeed corresponds to wording. It appears that the optimal kernel used in the fastKDE estimate of 104 samples contains a bandwidth that provides a compromise between the high variance (high wavelength) of the overall PDF and the small variance (low wavelength) features. In contrast, the estimate based on 106 points resolves well all of the high-frequency features. However, it appears that the use of a relatively narrow bandwidth kernel that allows resolution of these high-frequency features comes at the detriment of the low-frequency (high variance) background normal distribution. The imprint of individual kernels in the background PDF is evident in Fig. 4(b). Fig. 2 shows that the integrated, squared error of the fastKDE estimate follows a complicated decay as samples are added. Regardless, the error decay suggests that the fastKDE method asymptotes to the underlying mixture-distribution as the number of samples increases.To demonstrate the performance of fastKDE relative to a publicly-available KDE package with automatic bandwidth selection methods, we repeated the convergence and timing tests shown in Fig. 2 using the kernel smoothing, ks, package from R. We focus on the two complex distributions described in Section  3.2: the transition and mixture distributions. We use two bandwidth selection methods available in the ks package that represent two mainstream, contemporary approaches to multivariate bandwidth selection: the plug-in selection method (Wand and Jones, 1995), and the smoothed cross validation selection method (Duong and Hazelton, 2005). Though our implementation of the fastKDE algorithm is written in Python, we use R for this analysis for two reasons: (1) Python does not presently have such automatic bandwidth selection methods as part of its core numerical/mathematical libraries, and (2) R is a widely used statistical language that arguably has the most cutting-edge bandwidth selection methods available.Fig. 5(a) shows the integrated, squared error (similar to Fig. 2) for the fastKDE and R estimates. For direct comparison, all KDEs are performed on the same samples, and identical 513×513 grids are used for both fastKDE and R (though different grids are of course used for the transition and mixture distribution samples). The R-based estimates were run only up to sample-sizes of 16,384 and 8192 (transition and mixture distribution respectively) due to excessive computational time. The transition distribution exhibits almost identical error convergence characteristics as shown in Fig. 2. In contrast, the mixture distribution appears to exhibit better convergence for large sample sizes; note the steepening of the fastKDE ISE curve that occurs nearN=104, which indicates faster convergence. The 513×513 grid used in Fig. 5 has finer resolution than the grid used for Fig. 2, but the fastKDEs are otherwise identical. As a result, the estimates shown in Fig. 5 permit higher frequencies. It appears that for a PDF with so much structure, permitting high frequency components allows the fastKDE to continue to improve as points are added. The converse is true, as shown in Fig. 2(a); having too low a resolution effectively saturates the error in the fastKDE, such that additional points have no impact on the quality of the resulting KDE.Both the plug-in and smoothed cross-validation bandwidth selection methods offer similar performance in R. For the transition distribution, both converge at a rate similar to the fastKDE method, although it appears that the error for the fastKDE is lower for sample sizes larger than approximately 104. For the mixture distribution, the convergence rate is similar to the fastKDE for small sample sizes, but neither method appears to exhibit the sharp jump in convergence rate that occurs at a sample size of approximately 104. It is not clear what is the origin of this convergence rate discontinuity for the fastKDE method, nor why neither the plug-in nor smoothed cross validation methods exhibit this jump in convergence. Examination of Fig. 1 shows that there are several large, contiguous hypervolumes that are barely detached from the first contiguous hypervolume. It is possible that the jump in convergence rate is associated with these large hypervolumes abruptly joining the first contiguous hypervolume as sample size increases and|Cmin|2lowers. We recreated a version of Fig. 1 with 20,000 samples instead of 10,000 (not shown), and indeed this results in the first hypervolume joining with the large, previously disconnected regions.The main difference between fastKDE and the R-based KDE methods is in both the absolute time required for each KDE, and in the rate at which that time increases as samples are added; Fig. 5(b) shows the timing for all methods. Both the fastKDE and R-based KDE estimates were run on Cray XE6 nodes at the National Energy Research Scientific Computing Center (NERSC; on the machine called ‘hopper’). Each node contains 2 twelve-core AMD ‘MagnyCours’ 2.1-GHz processors and 32 GB of 1333 MHz RAM per node. Despite being run on the same computational architecture, the absolute differences in timings between fastKDE and R should be interpreted cautiously due to large possible differences related to programming language overhead and to code structure. Regardless, it is notable that fastKDE consistently performs faster than either R-based method, with fastKDE being approximately 4 orders of magnitude faster for sample sizes of approximately 104. We note that we ceased testing the R-based methods when the time-per-KDE exceeded approximately 2 h because the total wall time required for our (admittedly linear) test code to run would exceed the total allowable 96 h wall time for standard computational jobs on the ‘hopper’ system at NERSC.The most notable difference between the two methods is that fastKDE asymptotically approachesN1scaling for large sample sizes, whereas the R-based methods have a dependence on sample size that is clearly steeper than linear. Therefore, even if the performance of the R-based KDE methods could be tuned to be absolutely faster than fastKDE for the range of sample sizes shown, the super-linear dependence of the R methods on sample size would mean that fastKDE would inevitably be faster at some larger sample size.To demonstrate the utility of this method on a real dataset, we apply the fastKDE method to a dataset of 119 values of global mean temperature, and temperature and precipitation from California, USA. The California data are generated by spatially averaging station observations within the state and by temporally averaging values within the wet season, taken here to be the 6-month period beginning in November and ending in April the following calendar year (Karl and Koss, 1984). The data begin in November, 1895 and extend through April, 2014, which result in 119, semi-annual wet-season data points, extending from 1896 to 2014. The global mean temperature anomalies (relative to the 20th-century mean) are constructed as spatiotemporal annual averages of temperature data from the Global Historical Climatology Network-Monthly (GHCN-M) data set and the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) (Smith et al., 2008). These data are publicly available from NOAA’s National Centers for Environmental Information (NCEI) (formerly known as the National Climatic Data Center (NCDC)). Fig. 6shows these data.It is well established that the statistics of temperature have not been stationary during the industrial era due to the effects of greenhouse gas on global temperatures (Pachauri et al., 2014). We hypothesize that the joint PDF of California temperature and precipitation is correspondingly nonstationary. If this hypothesis is correct, then statistics that treat this dataset as stationary, such as the return intervals estimated on this dataset by AghaKouchak et al. (2014), may need a more sophisticated treatment that explicitly accounts for this nonstationarity.In order to test this hypothesis, we apply the fastKDE method to the dataset described above and shown in Fig. 6. We use the trivariate fastKDE to directly estimate the joint PDF of CA temperature and precipitation conditioned on global mean temperature. Fig. 7(a) depicts the 3D conditional PDF as a projection onto the CA precipitation–temperature plane, with contour colors representing the global mean temperature. The blue closed contours, which correspond to the relatively cool global temperatures from late in the 19th century, are asymmetric with their primary axis oriented from lower-right to upper-left in the figure. The orientation of these contours indicates a positive correlation between CA temperature and precipitation, with warm years that tend to be associated with wet years. In contrast, the red closed contours, which correspond to the relatively warm global temperatures of the present, exhibit a negative correlation; warm years tend to be associated with dry years. Following the contours from the bottom to the top, the transition from positive to negative correlation occurs in the vicinity of global mean temperature anomalies of 0.15 °C (yellow contours).The conditional relationship between CA temperature and precipitation exhibits a similar transition. The thick colored line in the center of the projected PDF in Fig. 7(a) shows the mean CA temperature and precipitation conditioned on global mean temperature anomaly (same color coding as the contours). It is clear from this curve that as global mean temperature increases, so does CA mean temperature. The same is not true for precipitation. In the cooler part of the century (the blue portion of the curve), mean CA precipitation increases linearly as CA mean temperature increases. This relationship has the same sign as the positive correlation indicated by the conditional probability contours. However, when global mean temperature anomalies reach approximately 0.15 °C, the direction of the relationship reverses, such that mean precipitation decreases as mean temperature increases. Again, this relationship has the same sign as the negative correlation indicated by the contours. Fig. 6 shows that global mean temperature anomalies go permanently above 0.15 °C just prior to 1980. This timeframe is consistent with a well-documented shift in the climate of the north Pacific Ocean that occurred in 1977 (e.g.,  Deser and Phillips, 2006). We hypothesize that this change in both the correlation and in the trend in the mean indicates a transition in the nature of weather systems that transport moisture to California stemming from the 1977 climate shift. We will evaluate this hypothesis in a future manuscript. It is notable that the precipitation–temperature relationship appears to curve toward the left at the end, which may suggest that California is approaching another transition.The nonstationarity of this dataset is also clearly evident in the conditional distribution of CA mean temperature shown in Fig. 7(b). As global mean temperatures increase, the distribution of temperatures monotonically shifts toward warmer temperatures. This shift results in some dramatic changes in the probabilities of extremely warm and extremely cold years. For global mean temperature anomalies of −0.3 °C, the probability of CA temperatures greater than 10 °C isP≈0.01. However, for global mean anomalies of approximately 0.6 °C, such warm years are far more common (P≈0.08). Likewise, years colder than 7 °C occur with probabilityP≈0.1for global mean temperature anomalies of −0.3 °C, whereas they almost never occur (P<0.01) for global mean anomalies of 0.6 °C. This nonstationarity is particularly dramatic when framed in terms of conditional recurrence intervals. What used to be a 1-in-100 year event (CA temperature greater than 10 °C) is now an approximately 1-in-13 year event.The conditional distribution of CA precipitation, shown in Fig. 7(c), is also quite non-stationary, though it does not exhibit a monotonic shift like the temperature distribution in Fig. 7(b). Rather than simply shifting, the precipitation distribution appears to broaden as global mean temperature anomalies increase toward 0.15 °C, and the distribution narrows again beyond. Similarly, and in accord with Fig. 7(a), the precipitation distribution also appears to shift slightly toward wetter values as global anomalies approach 0.15 °C, and they rebound toward drier values beyond. It is clear from Fig. 7(a) and (c) that the relationship between global mean temperature and precipitation is not simple, though it is not immediately clear why. This non-monotonicity should be investigated in future work.

@&#CONCLUSIONS@&#
