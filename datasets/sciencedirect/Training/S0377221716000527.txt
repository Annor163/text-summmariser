@&#MAIN-TITLE@&#
Value added, educational accountability approaches and their effects on schools’ rankings: Evidence from Chile

@&#HIGHLIGHTS@&#
We propose a robust model to estimate contextual value added efficiency.An assessment of the reforms in the Chilean system is presented.Performance differences decrease when controlling for contextual variables.

@&#KEYPHRASES@&#
Efficiency,Order-m,School effectiveness,Value added,

@&#ABSTRACT@&#
Value added models have been proposed to analyze different aspects related to school effectiveness on the basis of student growth. There is consensus in the literature about the need to control for socioeconomic status and other contextual variables at student and school level in the estimation of value added, for which the methodologies employed have largely relied on hierarchical linear models. However, this approach is problematic because results are based on comparisons to the school’s average—implying no real incentive for performance excellence. Meanwhile, activity analysis models to estimate school value added have been unable to control for contextual variables at both the student and school levels. In this study we propose a robust frontier model to estimate contextual value added which merges relevant branches of the activity analysis literature, namely, metafrontiers and partial frontier methods. We provide an application to a large sample of Chilean schools, a relevant country to study due to the reforms made to its educational system that point out to the need of accountability measures. Results indicate not only the general relevance of including contextual variables but also how they contribute to explaining the performance differentials found for the three types of schools—public, privately-owned subsidized, and privately-owned fee-paying. Also, the results indicate that contextual value added models generate school rankings more consistent with the evaluation models currently used in Chile than any other type of evaluation models.

@&#INTRODUCTION@&#
The development of indicators to evaluate the quality of education is a core element of countries’ efforts to implement improvements in their education systems (Battauz, Bellio, & Gori, 2011). In many countries, this concern has motivated the adoption of accountability systems (Kane & Staiger, 2002), whose main objective is to evaluate school quality and report these results to parents, principals, teachers, or policy makers, who will use them to make choices about schools, to improve their professional practice or to develop educational policies.11By way of example, see, for instance, the detailed information regarding school performance in the UK disclosed in http://www.education.gov.uk/schools/performance/.The available empirical evidence in this regard has contributed to strengthen this tendency, showing that well designed accountability systems (i.e. those which find the responsibility attributable to each of the participants in the educational system) enable organizational improvement inside each school (Rouse, Hannaway, Goldhaber, & Figlio, 2007), as well as optimizing the educational outcomes (Carnoy & Loeb, 2002; Hanushek & Raymond, 2005). An underlying requisite of any accountability system is to use a robust methodology to disentangle what share of the students’ achievement can be attributed to the school, and what share is simply the result of other variables beyond the school’s control.In terms of methodology, the general consensus is that students’ educational achievement depends both on their personal characteristics as well as those of their school and context. In order to analyze these scenarios, the most common and accepted methodology is multilevel regression models (McCaffrey, Lockwood, Koretz, Louis, & Hamilton, 2004), also known as hierarchical linear models, or regression models with random effects (Goldstein, 2003; Raudenbush & Bryk, 2002). The key characteristic of these methods is their capacity to disentangle what proportion of variance in student achievement can be explained by student variables (level 1), and what share can be explained by aggregate, or school, contextual variables (level 2). When multiple levels are considered, such as hierarchical systems of students nested in schools, it is possible to obtain a better understanding and measurement of the causes that explain students’ learning processes (Aitkin & Longford, 1986). The multilevel approach is highly relevant when attempting to make decisions, specific to each student, school, or context, that contribute useful information to develop new improvement processes in schools, discourage managers’ opportunistic behavior, signal a correct resource endowments policy (by establishing rewards and penalties), and make decisions on public policies.The initial stages of research on school accountability were characterized by the use of cross sectional measures to estimate school performance (e.g. the mean annual results of standardized tests), but the current practice is to rely on panel data methods to evaluate student performance in order to estimate the academic growth of students throughout their school life—ideally also controlling for other relevant variables (Goldstein et al., 1993; Goldstein & Thomas, 1996; Gray, Jesson, Goldstein, Hedger, & Rasbash, 1995; Mortimore, Sammons, & Thomas, 1994; Sammons, 1995). In this context of school accountability, the value added (VA) of a given school can be broadly defined as the contribution that it makes to students’ net progress (i.e. to the learning objectives) after the effects of other variables, external to the school, have been removed (Meyer, 1997). The basic value added model compares schools’ performance controlling for students’ previous achievement.More complex value added models are also available, and over the last few years there has been a growing tendency to use contextual VA models, which allow researchers to control for socioeconomic status (SES), ethnic background, gender, and other variables that are not under the school’s control or responsibility. Thus, contextual VA models provide an estimation of the net performance of schools by removing the effect of previous achievement and other preexisting differences among students (Ballou, Sanders, & Wright, 2004). It is generally agreed that contextual variables should be used to estimate VA models, especially when setting some form of accountability, or when disseminating the results, since results might be questionable if they do not take into account contextual characteristics of both students and schools. Although there is no consensus as to what specific contextual variables should be included in the model (Tekwe et al., 2004), socioeconomic status (SES) is usually one of them.According to their characteristics, VA indicators emerge as an attractive methodology for several actors interested in measuring or improving school performance, including: (i) governments (which need to rely on objective accountability measures); (ii) politicians (who want to guarantee that the assessment of schools considers their ethnic and socioeconomic diversity); (iii) researchers (who need to study those factors contributing to school effectiveness using net indicators, which are not spuriously contaminated by the characteristics of students); (iv) teachers and school managers (who want objective measures of their performance, tuned for their specific student populations); (v) parents (who need to choose schools for their children according to their real capacity to add value to their students); and (vi) society as a whole, since this entails a more accurate and fair evaluation of the schools in the country (Drury & Doran, 2003; McCaffrey et al., 2004).It is also crucial to understand that school effectiveness studies—including VA analysis—require using some kind of methodology to compare the schools being evaluated with a benchmark. In the case of VA research, the most popular methodology is multilevel regression models (see, for instance Goldstein et al., 1993; Gray et al., 1995; Cervini, 2009; Goddard & Goddard, 2001). An implicit assumption of this approach is to use the average school as a benchmark. However, this approach is not free from criticisms such as, for instance, that using the average as a benchmark is not an incentive for excellence (Bock, Wolfe, & Fisher, 1996; Kupermintz, 2003; McCaffrey, Lockwood, Koretz, & Hamilton, 2003), that according to traditional VA models test scores must be vertically scaled, and that the appropriate functional form for the model is not granted in advanced (Murphy, 2012; Ray, Evans, & McCormack, 2009).An attractive approach to overcome this criticism is to consider the models derived from the activity analysis literature, which evaluate school performance by comparing any given school with the best observed performance. Instead of using a regression line as a benchmark, these methodologies consider a nonparametric frontier built either using Data Envelopment Analysis (DEA), or its nonconvex variant, namely, Free Disposal Hull (FDH).22We can also find parametric variants to this literature, among which SFA (Stochastic Frontier Analysis) is the most popular. Parametric and nonparametric methods have both advantages and disadvantages, some of which have been recently outlined by Badunenko, Henderson, and Kumbhakar (2012).In addition to explicitly defining an optimal benchmark, frontier models also allow several outputs to be used simultaneously (i.e. several concurrent measures of student and school performance), offering greater flexibility to estimate VA.In this line of research, there has been a growing interest in developing approaches to estimate school effectiveness. For instance, Silva Portela and Thanassoulis (2001), De Witte, Thanassoulis, Simpson, Battisti, and Charlesworth-May (2010) and Portela, Camanho, and Keshvari, (2013) have developed methodologies to estimate basic VA models, whereas Thieme, Prior, and Tortosa-Ausina (2013) have proposed a model to analyse contextual effects in multilevel settings with cross-sectional data. However, the existing methodologies have not been able to estimate contextual VA, namely, to develop a frontier model able to estimate school VA effects controlling for students’ previous achievement, and also for contextual variables at student and school levels. This development is crucial to further explore the use of frontier models to estimate contextual VA models in real world applications.For this reason, the aim of this paper is both empirical and, to a lesser degree, methodological. Regarding the latter (at the methodological level) we propose a robust frontier model to estimate contextual value added (CVA) which combines both methodological contributions from multilevel modeling to school VA, as well as relevant proposals in the field of activity analysis methods—namely, the so-called metafrontiers (Battese, Rao, & O’Donnell, 2004) as well as the partial frontier methods (Cazals, Florens, & Simar, 2002).Regarding the former (at the empirical level), we use this novel approach to analyze school effectiveness in Chile. This application is especially relevant for this country which, since the 1980s, has been implementing a series of reforms to its educational system (see Mizala & Romaguera, 2000), with strong emphasis on accountability measures. Among other reforms, the government transferred the management of public schools from the Ministry of Education to city councils, and allowed for the participation of private schools in the public system through a voucher system. Simultaneously, an accountability system was created, consisting of national standardized tests of educational achievement applied annually to all students in 4th, 8th or 10th grade. The average school results of this assessment, called the SIMCE test (Sistema de Medición de la Calidad Educativa, or Measurement System of Educational Quality), are reported to parents, and used by the Ministry of Education as a measure of school quality. More recently, some new laws have been passed which emphsize further the use of accountability measures. Specifically, the Law #20529 features a new system of insurance of educational quality (Sistema Nacional de Aseguramiento de la Calidad de la Educación) and a new national agency of education quality (Agencia de Calidad de la Educación). This public institution should classify the educational institutions according to their school effectiveness and school performance into four groups. Specifically, in its article 18 this law indicates that the classification should be based primarily on the SIMCE average results achieved by the educational institutions. In addition to this, the law considers that the procedure to make the classification could eventually include VA models, stating explicitly that the agency should consider the results from the learning process in all the evaluated areas as well as the characteristics of the school’s students including, among others, their vulnerability and, when applicable, indicators of progress or value added (article 17). Currently this agency has developed several alternative methodologies for performing the classification of schools, but none of them has included measures of value added—despite the fact that the Law #20529 contemplates it, and it is a possible alternative considered in some research initiatives. Moreover, the methodologies considered correspond to variations in what we will later refer to as Model 1.The resulting classification will be obviously contingent to the different models chosen as well as the estimation procedures, as well as data availability. Therefore, these definitions might have important consequences for Chilean schools, among which we should consider that this Law also contemplates issues ranging from involvement in organizational aspects or the closure of low-performing schools. This article aims, among other issues, to contribute to the existing debate in Chile on this issue, comparing the effect on the classification of schools using value-added vs. status models (such as those that the agency is currently contemplating). This is possible due to some particularities of the SIMCE test calendar, since 2009 was the first time in which the same student that took the SIMCE test in 8th grade had also taken the test in 4th grade in year 2005.This scenario allows us to apply our model to a large sample—47,076 students from 948 primary schools. All students took Mathematics and Language SIMCE tests. The sample was made up of 4th and 8th grade students (9 and 13 years old, respectively), for which we have socioeconomic information on their families (at student level), obtained via a questionnaire for parents. In an attempt to achieve a reliable and homogeneous sample at school level, we only included those students in schools who took both exams and for whom we had socioeconomic information, whose schools met the requirement of having more than 30 students meeting these criteria, and that this value corresponded to 60 percent of the students at the school who took the exam in 2009.The rest of the article is organized as follows. In Section 2 we describe the relevant theoretical framework. In Section 3 we detail the methodology used. The background of the empirical application and the description of the database used are presented in Section 4. The comparative results between models are discussed in Section 5, and the main conclusions of the study are outlined in Section 6.All national or state accountability systems attempt to improve learning and instruction processes, but they differ significantly in the way they control for both the quality and progress of schools. This heterogeneity leads to different perceptions of which schools should be rewarded, and which should be encouraged to improve, among other recommendations.There are several frameworks to classify these evaluation models (Carlson, 2001), but most of them take into account two fundamental aspects. First, one may distinguish between two different approaches for monitoring school performance, namely, status models and growth models. Status models use a single year to evaluate students’ academic achievement (i.e. cross sectional data), whereas growth models use two or more years (i.e. panel data). Second, in both approaches one may distinguish between models that use contextual variables (both at student and school level) to evaluate school achievement, and those which do not. This scenario, and the core questions that these models try to answer, can be represented in a 2 × 2 matrix as in Table 1.As Table 1 shows, Model 0 (which is also referred to as type 0 model) only considers for the evaluation the outputs related to students’ academic achievement in a given time period. In the literature on education these models are usually referred to as academic achievement status models without contextual variables. As indicated by Tekwe et al. (2004), the distinguishing characteristic of status-based models is the absence of adjustment for students’ incoming knowledge level. This would imply that the differences among schools in terms of the average knowledge of their incoming students is convoluted with the assessment of teaching quality. An implicit assumption is that all students and schools have optimal and similar backgrounds. Therefore, accountability systems based on this model consider that students’ academic achievement is entirely attributable to schools, disregarding evidence in the literature that a large share of students’ academic achievement might be attributable to contextual factors, which are non-controllable, and not attributable to the school itself (Teddlie & Reynolds, 2000).The strongest criticisms suggest that this model could generate perverse incentives for the attainment of the objective being pursued, endowing fewer resources to those students with relatively worse results who do not help their schools to achieve their objectives. Simultaneously, this could generate selection of students within schools, or lead to self-selection (Wilson, 2004). In spite of these disadvantages, we analyze Model 0 as a first step, because it is the approach currently used in Chile to evaluate its schools since 1999. Therefore, it is of interest to compare its results with those that could be yielded by other models proposed in this study.Model 1 extends the variables considered. While, analogously to model 0, it includes the outputs related to students’ academic achievement at a given moment of time, it also considers input variables not attributable to the school, either at student or at school level. This model corresponds to an academic achievement status model with contextual variables, according to the literature on education. A recent example of this type of approach is the study by Thieme et al. (2013), which proposes a multilevel model incorporating contextual variables both at student and school levels. Despite the remarkable progress it represents, by not considering as input the initial academic achievement of students, it assumes that it is the same, and optimal, for all students, a situation which is obviously far from reality, and could lead to misinterpretation of results. It should be noted that the methodologies recommended by the Chilean agency for the quality of education (“Agencia de Calidad de la Educación”) for the classification of schools correspond to this family of models.Model 2 corresponds to a “pure” value-added model; the only inputs and outputs it considers are the results of students’ academic achievement, both at the beginning and at the end of the educational process under evaluation. The educational research literature considers that value added measures (gain) are more informative measures of the effectiveness of institutions, since they allow the effect of the school’s student progress to be isolated (Wilson & Piebalga, 2008), and contribute to reducing incentives for dishonest behavior. Two relatively recent studies are consistent with this model, namely, Silva Portela and Thanassoulis (2001) and De Witte et al. (2010). However, as in the previous model, they have the disadvantage of not controlling for non-school elements which influence this particular process.Model 3 overcomes the disadvantages of models 1 and 2. In the education research literature this type of model emerged strongly as a refinement of measures of growth, and has been called CVA (contextual value added). The CVA was first used in 2006 in British schools, and it is a measure intended to isolate the real impact of the school on students’ progress. This type of modeling involves obtaining results that consider a number of factors such as gender, ethnicity, and language of origin, among others. The difference between the model estimate and the result that the student actually achieves is what is referred to as CVA (Wilson & Piebalga, 2008).There is an open discussion about this point, with arguments for and against using several contextual variables in VA models (Timmermans, Doolaard, & de Wolf, 2011; Willms & Raudenbush, 1989). In our case, we have decided to include only the SES, basically for three reasons. First, it is important to understand that, in the Chilean context, the most important contextual variable explaining school performance is SES, as repeatedly demonstrated in many studies (Auguste & Valenzuela, 2003; Bellei, 2009; Contreras, Sepúlveda, & Bustos, 2010; Gauri, 1999; Hsieh & Urquiola, 2006). For instance, Manzi, Strasser, San Martín, and Contreras (2008) show that a very large and stable percentage of between-school variance is accounted for by socioeconomic factors; in fact, over 60 percent of this variance is explained by the combination of the individual SES and school average SES. This result is consistent with PISA decomposition of variance (OECD, 2010), which shows that Chile is one of the countries with the largest percentage of between-school variance explained by socioeconomic factors. Manzi et al. (2008) also show that the effect of other variables is negligible, and that the type of school does not explain a relevant share of between school variance once socioeconomic factors are controlled for. These results corroborate the extent of socioeconomic segregation in the Chilean educational system (see also Carrasco & San Martín, 2012; Valenzuela, Bellei, & Ríos, 2014).Considering the previous arguments, it is important to understand that, in the case of Chile, the most important contextual variable to analyze school performance is SES both at individual and school (average) levels, and due to this reason we include these variables and no other. Second, and from a broader perspective, models with different types of contextual variables usually yield similar results (Timmermans et al., 2011), which would include both basic and advanced value added models (Harris, 2011). However, interestingly, although different VA models might use different contextual variables, there is a wide consensus on the inclusion among them of SES (e.g. see the OECD report on VA models in different countries). Third, using SES as the only control variable enables achieving a balance between the amount of information used and the sample size. It is debatable if the increased amount of information considered when including more contextual variables offsets the problems derived from the curse of dimensionality, a relevant issue to control for in nonparametric models for efficiency measurement (Simar and Wilson, 2008, p. 441, chap. 4). Actually, in our particular case the number of contextual variables which we summarize in our composite index is very high (4) and, therefore, we consider this to be a case in which a composite index may be particularly convenient.Despite the great advances that models 1 and 2 represent, when activity analysis methods are considered to evaluate them, model 3 in Table 1 best isolates the real impact of the school on students’ progress, as indicated by many contributions from the traditional literature on school evaluation—which generally use parametric multilevel analysis. Therefore, evaluating model 3 considering activity analysis methods has some unexplored advantages that will be part of our aims. It should be considered that, up to now, the Chilean agency for the quality of education has not considered type 2 or type 3 models for the classification of schools.As indicated in the introduction, in recent years there has been considerable progress in the evaluation methodology of school performance, especially regarding the development of multilevel models (Bryk & Raudenbush, 1992; Goldstein, 1995). The general concept is that students’ academic achievement depends on their personal characteristics, and the characteristics of the school, and its context. To analyze these situations, the different levels are considered as hierarchical systems of students and schools, with individuals and groups defined in separate hierarchies, using variables that are defined at each level (Hox, 2002).This significant progress can solve the main methodological problem of the pioneering studies in this field, by breaking down the various nested effects that explain students’ educational outcomes. The percentage of student achievement due to the different variables at different organizational levels—district, school, class, and student—can also be determined.In this particular area, there are many statistical models for estimation which differ in several regards such as the definition and inclusion of adjustment variables (Tekwe et al., 2004). However, the most prominent position is to include adjustment variables, especially when establishing some form of accountability or dissemination of the results, since the equity is questionable if the background characteristics of students and schools are not taken into account (McCaffrey et al., 2004; McCaffrey et al., 2003).Despite the many methodological and empirical contributions, this research is not without its criticisms (Kupermintz, 2003; McEwan, 2003). One of them is related to the nature of their estimate as a comparison with the average, assuming no real incentive for performance excellence. Indeed, the vast majority of value-added studies used multilevel regression or analysis.An alternative is found in the models that consider activity analysis techniques, mainly using nonparametric frontier methods (mostly Data Envelopment Analysis, DEA, and its nonconvex counterpart, Free Disposal Hull, FDH). They provide relevant advantages such as the ability to compare with the optimal or, more importantly, the possibility to specify several inputs and outputs simultaneously. In the field of education many studies have adopted these techniques (see, for instance Mizala, Romaguera, and Farren, 2002; Thanassoulis, Kortelainen, Johnes, and Johnes, 2011; Portela, Camanho, and Keshvari, 2013; Johnes and Johnes, 2009, among others). However, these methods are not exempt from general criticisms. On the one hand, regarding the nature of these methods, their deterministic and probabilistic features, the curse of dimensionality, or their heavy reliance on the absence of outliers have been a source of continuous concern. On the other, in the specific field of education, their main disadvantage has been to consider only student-level data, which would yield estimations that incorrectly assume that schools are operating with the optimal endowment of inputs (both, controllable or uncontrollable), without establishing thus a multilevel analysis.From the perspective of the non-parametric deterministic technology, Cazals et al. (2002) pointed out the problem of lack of statistical properties and the impact that the presence of outliers can cause. From the perspective of student data, another criticism is that multilevel analysis avoids the resources schools allocate in order to sustain the education process (McCaffrey et al., 2004; McCaffrey et al., 2003). Both problems are addressed in this paper, by providing an integrated approach with the use of metafrontier approaches (Battese et al., 2004) and the use of robust partial order-m frontiers (Cazals et al., 2002).Some previous research initiatives have taken these considerations into account. Specifically, our aims and methods are consistent with previous literature such as Silva Portela and Thanassoulis (2001) who, following model 2, decompose the overall efficiency into two different effects, namely, school effect and student-within-school effect. Later on, De Witte et al. (2010) refined this methodology, proposing a robust approach based on Cazals et al.’s (2002) ideas for the estimation. A more recent contribution by Thieme et al. (2013), based on model 1, considered both ideas, performing a multilevel decomposition in which additional variables are factored in so as to provide a more comprehensive analysis. However, despite their interest, these previous studies disregard the existence of contextual factors—at both the student and school level—in the assessment of school performance.In contrast, our proposal here is based on the definition of a contextualized value-added robust multilevel nonparametric frontier assessment that separates the net effects of student and school, controlling for socioeconomic status, both at the student and school level, eliminating (or at least drastically reducing) the potential problems caused by the existence of outliers and dimensionality problems, as will be explained in the following section.

@&#CONCLUSIONS@&#
