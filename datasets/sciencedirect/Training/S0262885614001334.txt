@&#MAIN-TITLE@&#
Recognizing in the depth: Selective 3D Spatial Pyramid Matching Kernel for object and scene categorization

@&#HIGHLIGHTS@&#
We introduce the 3DSPMK for object and scene recognition in depth images.Our model repeatedly subdivides a cube inscribed in the point cloud.Then, a weighted sum of histogram of visual word occurrences is computed.Results on publicly available benchmarks have been reported.

@&#KEYPHRASES@&#
3D Spatial Pyramid Matching Kernel,Object recognition,Scene classification,Point clouds,Depth images,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
We humans look at a picture and are able not just to see a pattern of color and texture, but to comprehend it, categorizing all the objects and even the scene itself. Can we do the same with simply a depth image? For instance, look at the images in Fig. 1. Can't we localize and recognize the objects? Can't we categorize the scenes?Object recognition in RGB images has seen huge progress in recent years, much thanks to the popular Bag of Words (BoW) approach [2,3]. The brilliant idea behind this type of representation consists in characterizing an image by an orderless set of quantized local features, i.e. the well-known visual words. This approach has inspired a lot of research efforts which have obtained impressive results recently (e.g.[4–8]). Furthermore, the BoW model is the basic recipe for most of the methods submitted to the PASCAL VOC Challenge [9]. Methods using Support Vector Machines (SVMs) with Spatial Pyramid Matching Kernels (SPMKs) [10,4] have been systematically obtaining the best results. The most recent improvements have been achieved by incorporating multiple local features such as SIFT [11], SURF [12] and color SIFT [6] into the BoW pipeline [13,6].So, we can say that the categorization problem in RGB images is a well established field of research. However, nowadays, we are witnessing how a new generation of depth cameras, such as Kinect, are capable of offering quality synchronized images of both color and depth information. The introduction of these sensors represents an opportunity to explore how to increase the capabilities of scene categorization and object recognition approaches (e.g.[1,14,15]).In this paper, we propose to go beyond the traditional BoW representation for RGBD images, and propose a characterization for the point clouds associated to the depth images.We build a discriminative approach for recognizing object and scene categories in point clouds, which can simply use the information extracted from depth images. Inspired by the works of Lazebnik et al. [4] and Knopp et al. [16], we introduce a novel framework which uses 3D local features. This new methodology is depicted in Fig. 2. We start extracting 3D local descriptors (such as 3D SURF [16] and NARF [17] descriptors) from a point cloud provided by a depth camera. Note that we do use a single depth image as input. These descriptors are then quantized, e.g. using K-means, so as to obtain a 3D visual vocabulary. We introduce a kernel-based image categorization approach, which works adapting the SPMK [4] to work in 3D, i.e. the 3D Spatial Pyramid Matching Kernel (3DSPMK). This novel strategy involves repeatedly subdividing a cube inscribed in the 3D point cloud, building histogram representations at increasingly fine sub-volumes, and computing a weighted sum of histogram intersections. We thoroughly explore how the 3D spatial binning and pyramids affect the performance, and propose selective hierarchical volume decomposition strategies, based on representative and discriminative sub-volume selection processes, which dramatically reduce the volume to consider, while jointly preserve the classification accuracy and increase the computational efficiency of the kernel.A preliminary version of this paper appeared in Ref. [18], where we only addressed the problem of object recognition with 3D SURF descriptors in the RGB-D object dataset [15]. This paper contains a more detailed formulation of the Selective 3DSPMK approach. We also extend our approach to work with any type of 3D local descriptor (e.g. NARF [17]), and to solve novel problems, such as scene categorization. Additional experiments, using more RGBD datasets, have been incorporated as well. We also explore how to combine RGB and depth information into the same pipeline, in order to increase the global performance of the system.The rest of this paper is organized as follows. Section 2 describes related work. Our novel approach, the 3DSPMK, is detailed in Section 3. Results are presented in Section 4. We finally conclude in Section 5.

@&#CONCLUSIONS@&#
