@&#MAIN-TITLE@&#
Using multiobjective optimization for biclustering microarray data

@&#HIGHLIGHTS@&#
A new multiobjective modeling for the biclustering problem.A new hybrid multiobjective algorithm gradually conceived for best results.Extracting relevant biclusters with large sizes compared to classical methods.

@&#KEYPHRASES@&#
Biclustering problem,Gene expression data,Evolutionary algorithm,Multiobjective combinatorial optimization,

@&#ABSTRACT@&#
Microarray data analysis is a challenging problem in the data mining field. Actually, it represents the expression levels of thousands of genes under several conditions. The analysis of this data consists on discovering genes that share similar expression patterns across a sub-set of conditions. In fact, the extracted informations are submatrices of the microarray data that satisfy a coherence constraint. These submatrices are called biclusters, while the process of extracting them is called biclustering.Since its first application to the analysis of microarray [1], many modeling and algorithms have been proposed to solve it. In this work, we propose a new multiobjective model and a new metaheuristic HMOBIibeafor the biclustering problem. Results of the proposed method are compared to those of other existing algorithms and the biological relevance of the extracted information is validated. The experimental results show that our method extracts very relevant biclusters, with large sizes with respect to existing methods.

@&#INTRODUCTION@&#
Using microarray technologies, the expression levels of thousands of genes under several conditions may be measured in a single experiment. A microarray data may be presented in a matrix form A=(X, Y), where X={i1, i2, …iN} is a set of N genes and Y={j1, …, jM} a set of M conditions. aij∈A (i∈X, j∈Y) represents the expression level of gene i under condition j (see Table 1).Functionally related genes are likely to behave similarly across distinct conditions, however, identification of such genes and conditions is not obvious. In data mining, the problem is known as biclustering or co-clustering, and is applied in many fields such as marketing, psychology and bioinformatics. Biclustering is the extraction of submatrices B=(I, J) (I⊂X, J⊂Y) (called biclusters) of maximal size and meeting some coherence constraints. Commonly, the coherence is measured by the Mean Squared Residue (MSR) [1]. Computational complexity of biclustering mainly depends on its formulation, though the most interesting variants of biclustering are NP-complete [2].Many heuristics have been proposed for solving the biclustering problem such as greedy algorithms [1,9–12], divide-and-conquer algorithms [15,16], enumeration algorithms [13,14], and metaheuristics (mainly evolutionary and bioinspired) [3–8,31].Mitra et al. proposed the first multiobjective algorithm for the biclustering problem NSAG2B [8]: an evolutionary metaheuristic based on NSGA-II [19] and CC heuristic. In this approach, the CC heuristic (see Section 3.1 for more details) is applied first to all the initial solutions and then after the application of variation operators. Furthermore, solutions (biclusters) are encoded by fixed size binary string, with a bit string for the genes appended by another bit string for the conditions. A bit is set to one if the corresponding gene/condition is part of the bicluster, and to zero otherwise.Liu et al. proposed the MODPSFLB algorithm (multiobjective dynamic population shuffled frog-leaping biclustering) [31]. In MODPSFLB, the feasible solutions are regarded as frogs and pareto optimal solutions are preserved in a frog population updated by an ϵ-dominance relation and the computation of crowding distance. Then the next generation of frog population is dynamically adjusted according to a dynamic population strategy. Each solution is presented as a binary vector.However, most approaches do not consider the biological relevance of the extracted biclusters in the modeling of the problem.Here, we introduce an improved version of our previous biclustering model [17,18] and a powerful hybrid metaheuristic that, together, outperform state-of-the art heuristics.The paper is organized as follows: Section 2 presents the main concepts of multiobjective optimization. Then, a multiobjective model for biclustering problem is discussed. Section 3 presents state-of-the-art multiobjective optimization algorithms. Then, it elaborates on the different proposed metaheuristics. Experimental protocol and results are detailed in Section 4. Finally, Section 5 concludes the paper.In practice, many real life problems (bioinformatics[33], data mining[34,35], agriculture[36,37], transportation[38,39], etc.) require the optimization of several conflicting objectives simultaneously. These problems are called: Multiobjective Optimization problems (MOPs). In a MOP, the optimal solution is not a single solution but a set of solutions defined as pareto optimal solutions. A solution is pareto optimal if it is not possible to improve a given objective without deteriorating at least an other objective. Multiobjective problems are generally NP-hard and their resolution cannot be performed in an exact manner within a reasonable time.In this section, some multiobjective optimization concepts are defined. Then, we present our multiobjective biclustering modeling strategy.Definition 2.1Multiobjective optimization problemA Multiobjective Optimization Problem (MOP) may be defined as:MOP=minF(x)=(f1(x),f2(x),…,fn(x))s.t.x∈Swhere n (n≥2) is the number of objectives, x=(x1, …, xk) is the vector representing the decision variables, and S represents the set of feasible solutions associated with equality and inequality constraints and explicit bounds, F(x)=(f1(x), f2(x), …, fn(x)) is the vector of objectives to be optimized.The search space S represents the decision space of the MOP. The space in which the objective vector belongs to is called the objective space. The vector F can be defined as a cost function from decision space in the objective space that evaluates the quality of each solution (x1, …, xk) by assigning an objective vector (y1, …, yn) which represents the quality of the solution (fitness).As the criteria are usually in conflict, it is not usual to have a solution x*, associated with a decision variable vector, where x* is optimal for all the objectives. Other concepts were established to consider optimality. A partial order relation could be defined, known as dominance relation.Definition 2.2Pareto dominanceFor a minimization multiobjective problem, an objective vector u=(u1, …, un) is said to dominatev=(v1,…,vn)(denoted byu≺v)if and only if no component ofvis smaller than the corresponding component of u and at least one component of u is strictly smaller, that is:u≺v:∀i∈[1,…,n]:ui≤vi∧∃i∈[1,…,n]:ui<vi.Definition 2.3Pareto optimalityA solution x*∈S is pareto optimal if for every x∈S, F(x) does not dominate F(x*), that is, ∀x∈S, F(x)⊀F(x*).Definition 2.4Pareto optimal setFor a given MOP(F, S), the pareto optimal set is defined as P*={x∈S/¬∃x′∈S, F(x′)≺F(x)}.Definition 2.5Pareto frontFor a given MOP(F, S) and its pareto optimal set P*, the pareto front is defined as PF*={F(x), x∈P*}.The pareto front is the image of the pareto optimal set in the objective space. Obtaining the pareto front of a MOP is the main goal of the multiobjective optimization. However, given that the pareto front can contain a large number of points, a good approximation of the pareto optimal set may contain a limited number of pareto solutions that satisfy two properties [21]:•The closeness to the pareto optimal set.The diversification of the solutions.Indeed, the closeness to the pareto optimal set ensures the quality of the selected solutions and the diversification avoids redundancy and allows a better representation of the pareto optimal set.The diversification of the solutions may be measured using distance measures such as: Crowding distance.Definition 2.6Crowding distanceThe crowding distance distxof a solution x is a measure of the search space around x which is not occupied by any other solution in the population [21]. For each objective function fj, the vectors are sorted and an infinite crowding distance is assigned to the solutions with the smallest or largest values in this dimension. The crowding distance value of a solution is computed by adding the entire individual crowding distance values in each objective function which are calculated according to:distxij=fj(xi+1)−fj(xi−1)|fj,max−fj,min|fj,max and fj,min are the population maximum and minimum objective value of jth objective.Solutions with high crowding distance are considered better solutions, as they introduce more diversity in the population.Extracting biclusters from a data matrix can be seen as a combinatorial optimization problem, where each bicluster optimizes a set of quality criteria [2]. In gene expression data, the quality of a bicluster is defined by its size, its coherence and its mean rows variances. These criteria are independent and notably conflicting. In fact, a non perfect coherence can always be improved by removing a row or a column, i.e. by reducing its size. On the other hand, a flat bicluster (bicluster with constant rows) is perfectly coherent. We can therefore deduce that the problem of biclustering is a multiobjective optimization problem.In what follows, we define the different objective functions. Then, we present a literature review. Finally, we discuss our modeling strategy.In order to optimize the coherence of a bicluster, the dissimilarity measure Mean Squared Residue (MSR) is generally considered [1]. Given a bicluster B=(I, J). The residue r(aij) of an element aij∈B is given by the equation:r(aij)=aij−aiJ−aIj+aIJ.where aiJrepresents the mean of the ith row of B, aIjrepresents the mean of the jth column of B and aIJthe mean of all the elements in B.The mean squared residue of the bicluster is:(1)MSR(I,J)=1|I||J|∑i∈I,j∈Jr(aij)2The lower the MSR value of the bicluster is, the better is its coherence. A bicluster with MSR value less then δ is called δ-bicluster.A bicluster with a low MSR value indicates that the expression levels fluctuate in unison [1]. This includes flat biclusters (biclusters with the same expression value for each gene for all the conditions). For gene expression data, we are interested in biclusters with genes that exhibit significant fluctuations across the different conditions i.e. biclusters with significant rows variances. To evaluate the variance of the bicluster's rows we define the mean rows variances denoted Rvar.(2)Rvar(I,J)=1|I||J|∑i∈I,j∈J(aij−aiJ)2.

@&#CONCLUSIONS@&#
