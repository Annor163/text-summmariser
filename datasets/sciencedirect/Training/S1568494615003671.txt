@&#MAIN-TITLE@&#
Hybridizing firefly algorithms with a probabilistic neural network for solving classification problems

@&#HIGHLIGHTS@&#
Hybridizes the firefly algorithm with simulated annealing, where simulated annealing is applied to control the randomness step inside the firefly algorithm.A Lévy flight is embedded within the firefly algorithm to better explore the search space.A combination of firefly, Lévy flight and simulated annealing is investigated to further improve the solution.

@&#KEYPHRASES@&#
Firefly algorithm,Lévy flight,Simulated annealing,Probabilistic neural networks,Classification problems,

@&#ABSTRACT@&#
Classification is one of the important tasks in data mining. The probabilistic neural network (PNN) is a well-known and efficient approach for classification. The objective of the work presented in this paper is to build on this approach to develop an effective method for classification problems that can find high-quality solutions (with respect to classification accuracy) at a high convergence speed. To achieve this objective, we propose a method that hybridizes the firefly algorithm with simulated annealing (denoted as SFA), where simulated annealing is applied to control the randomness step inside the firefly algorithm while optimizing the weights of the standard PNN model. We also extend our work by investigating the effectiveness of using Lévy flight within the firefly algorithm (denoted as LFA) to better explore the search space and by integrating SFA with Lévy flight (denoted as LSFA) in order to improve the performance of the PNN. The algorithms were tested on 11 standard benchmark datasets. Experimental results indicate that the LSFA shows better performance than the SFA and LFA. Moreover, when compared with other algorithms in the literature, the LSFA is able to obtain better results in terms of classification accuracy.

@&#INTRODUCTION@&#
Classification is one of the key data mining tasks. Classification maps data into predefined groups or families. It is a form of supervised learning because the classes are learned before the data is examined. The goal of a classification method is to create a model that correctly maps the input to the output by using historical data, so that the model can be used to develop output when the desired output is unknown.Several techniques have been successfully used for classification problems, including the neural network (NN) [1], support vector machine (SVM) [2], naive Bayes (NB) [3], radial basis function (RBF) [4], logistic regression (LR) [5], K-nearest neighbours (KNN), and the iterative dichotomiser 3 (ID3) [6].The Neural Network is one of the most well-known and widely used techniques for classification. The NN model was first proposed by Rosenblatt in the late 1950s [7]. Since that time, a lot of NN models have been developed, including feed-forward networks, RBF networks, the multi-layer perceptron, modular networks, and the probabilistic neural network (PNN). These models differ from each other in terms of architecture, behaviour and learning approaches, hence they are suitable for solving different problems such as series forecasting [5], stock market prediction [8], weather prediction [9], and pattern recognition [10].The PNN is one of the appropriate approaches for solving classification problems. It is a general NN model that is based on the notion of ‘the gradient steepest descent method’, which enables a reduction of errors between the actual and predicted output functions by permitting the network to correct the network weights [1,6,11–14]. Recently, the hybridization of metaheuristics with different kinds of classifiers has been investigated and the developed models, some of which are described below, show better performance than the above-mentioned standard classification approaches.Single-based and population-based metaheuristics can be used to train a NN. Single-solution-based approaches include the tabu search [15] and the simulated annealing (SA) approach [14], the latter of which is based on a Monte Carlo model that was applied by Metropolis et al. to replicate energy levels in cooling solids. The population-based approach in combination with a NN has attracted great interest because NNs combined with evolutionary algorithms (EAs) result in better intelligent systems than when relying on NNs or EAs alone [16,17]. Among these population-based approaches, a particle swarm optimization (PSO) algorithm in isolation and a hybridization PSO algorithm with a local search operator have been employed to train a NN [18,19]. Other swarm intelligence methods such as ant colony optimization (ACO) have also been employed [20,21]. In addition, Chen [22] proposed a novel hybrid algorithm based on the artificial fish swarm algorithm. The genetic algorithm (GA) [23], differential evaluation [24], improved bacterial chemotaxis optimization (IBCO) [25], electromagnetism-like mechanism-based algorithm (EMA) [26], and harmony search algorithm (HSA) [27–30] are some of the other important methods that have been proposed in recent years. The efficiency of metaheuristic algorithms can be attributed to their ability to imitate the best features in nature and the ‘selection of the fittest’ biological systems. The two most important characteristics of metaheuristic algorithms are diversification and intensification [31]. The aim of intensification, which is also called exploitation, is to search locally and more intensively, while diversification, which is also called exploration, aims to ensure that the algorithm globally explores the search space. The two terms might appear to be contradictory, but their balanced combination is crucial to the success of any metaheuristic algorithm [7,31–33].For firefly algorithm (FA) optimization, the diversification component is represented by the random movement component, while the intensification component is implicitly controlled by the attraction of different fireflies and the attractiveness strength. Unlike the other metaheuristics, the interactions between exploration and exploitation in the FA are intermingled in some ways, which might be an important factor in its successful solving of classification problems.In this work, we investigate combining the FA with a SA algorithm and hybridizing the FA with Lévy flight in order to attempt to improve the performance of the PNN by creating an effective balance between exploration and exploitation during the optimization process, which we attempt to achieve by controlling the randomness steps and exploring the search space efficiently in order to find the optimal weights of the PNN classification technique. Such hybridization requires a fine balance between diversification and intensification to ensure faster and more efficient convergence and to ensure the quality of the solutions in order to find the optimal weight of the PNN classification technique. Therefore we start from the first iteration to calculate the accuracy by modifying the PNN using the FA and SA to improve the quality of the best solution. To our knowledge, this is the first attempt to hybridize the FA with SA for classification problems.The rest of the paper is organized as follows: Section 2 presents the background and literature on FAs and Section 3 describes the proposed method. Section 4 presents a discussion of the experimental results and Section 5 provides details of the computational complexity of the proposed method. Section 6 concludes the work presented in this paper.The FA was initially developed by Yang [34] as a population-based technique for solving optimization problems. It was motivated by the short and rhythmic flashing light produced by fireflies. Theses flashing lights enable fireflies to attract each other and assist them to find a mate, attack their prey, and also protect themselves by creating a sense of fear in the minds of predators [35]. The less bright fireflies are easily attracted by the brighter fireflies and the brightness of the light of a firefly is affected by the landscape [7,36]. This process can be formulated as an optimization algorithm because the flashing lights (solutions) can be formulated to match with the fitness function to be optimized. The FA follows three rules: (1) fireflies must be unisex, (2) the less bright firefly is attracted to the randomly moving brighter fireflies, and (3) the brightness of every firefly symbolizes the quality of the solution.Łukasik and Żak employed a FA for continuous constrained optimization tasks and it was found to consistently outperform PSO [37]. Yang employed and compared a FA with PSO for various test functions and found that the FA obtains better results than PSO and also a GA in terms of efficiency and success rate. It has also been found that the broadcasting ability of the FA gives better and quicker convergence towards optimality [34]. In a similar work by Yang and Deb, experimental results revealed that the FA outperforms other approaches such as PSO [38]. Sayadi introduced a FA with local search for minimizing the makespan in permutation flow scheduling problems and the initial results indicated that the proposed method performs better than an ACO algorithm [39]. Gandomi applied a FA to mixed variable structural optimization problems and the empirical results showed that the FA is better than PSO, GA, SA, and HSA [40]. Another work on the FA can be found in [35], where it was successfully applied to solve the economic emissions load dispatch problem. In light of the foregoing, it can be seen that the FA is more effective than some other methods, which motivated us to further investigate its performance with respect to the classification problem.In the FA, the form of the attractiveness function of a firefly is denoted by the following:(1)β(r)=β0exp(−γr2)where r is the distance between any two fireflies, β0 is the initial attractiveness at r=0 (set to 1 in this work), and γ is an absorption coefficient that controls the decrease of the light intensity (also set to 1 in this work).The distance between any two fireflies i and j at positions xiand xj, respectively, can be defined as a Cartesian or Euclidean distance as follows:(2)rij=||xi−xj||=∑k=1d(xi,k−xj,k)2where d is the dimensionality of the given problem.The movement of a firefly i, which is attracted by a brighter firefly j, is represented by the following equation:(3)xi=xi=β0*exp(−γrij2)*(xj−xi)+α*rand−12.where the first term is the current position of a firefly, the second term is used to consider the attraction of a firefly towards the intensity of the light of neighbouring fireflies, and the third term is used for the random movement of a firefly when it cannot ‘see’ any brighter ones. The coefficient α is a randomization parameter determined by the problem of interest, while rand is a random number generator consistently distributed in the space [0,1] [41].According to Specht (1991) and Paliwal and Kumar (2009), the PNN is an effective approach for solving classification problems. A PNN has a relatively faster training process than the back-propagation NN and has an intrinsically analogous structure that ensures convergence with an optimal classifier because the size of the representative training set is maximized and training samples can be added or removed without extensive retraining [11,42]. A PNN consists of four layers: input, pattern, summation, and output, as shown in Fig. 1.The input layer is the first layer of neurons. Each input neuron represents a separate attribute in the training/test datasets (for example, from x1 to xn). The number of inputs is equal to the number of attributes in the dataset. The values from the input data are then multiplied by the appropriate weights w(ij), as determined by the PNN algorithm shown in Fig. 1, and are transmitted to the pattern layer. The output layer is the last layer that typically contains only one class because only one output is usually requested. During the training phase, the goal is to determine the most accurate weights to be assigned to the connector line. In this phase, the output is computed repeatedly, and the result is compared to the preferred output generated by the training/test datasets.As shown in Fig. 2, the procedure starts from initial weights that are randomly generated by the original PNN classification model. The values from the input data are then multiplied by the appropriate weights w(ij), as determined by the PNN algorithm.In this work, we focus on exploration and exploitation [33] because the balance of these two components is crucial to the success of any metaheuristic algorithm [33,31]. Therefore we propose three hybrid methods for data classification and the algorithms in these methods are based on the FA. The FA was chosen in order to obtain the optimal parameter settings for training a PNN and to achieve the best accuracy. The random step length in the FA (the random length between the current position of a firefly and neighbouring fireflies) is not limited and is not controlled in the original firefly mechanism. The first hybrid algorithm tries to speed up the convergence of the FA to find the optimal solution by integrating it with SA. In this algorithm, the best result using a FA after evaluating the population is passed to the SA algorithm to generate a neighbour solution. The second hybrid algorithm consists of a FA integrated with a Lévy flight algorithm. In this algorithm, we try to control the random step in the firefly mechanism. In the firefly mechanism, the third term in the movement step is random, so we use Lévy flight to control the randomization parameter in order to achieve a balance between exploration and exploitation. In sum, the two hybridization methods proposed are a FA with a SA algorithm (denoted as SFA) and a FA with Lévy flight (denoted as LFA). The third hybrid algorithm integrates SFA and LFA (denoted as LSFA).In this method, denoted as SFA, the FA is hybridized with SA to solve classification problems. Hybridization of the FA with SA is employed to achieve a balance between exploration and exploitation in order to ensure efficient convergence and an accurate solution.The SA algorithm is based on a Monte Carlo model that was applied to replicate energy levels in cooling solids. The cooling process is very important in a SA algorithm. The temperature values are chosen based on the cooling approach. Generally, the cooling approach for a temperature remains steady for several decades, thus getting stuck in local optima is avoided through allocating prospects to deteriorate moves or by recognizing the worst solution the algorithm begins with is the initial solution. After the initial temperature and final temperature parameters and the cooling rate are initialized, the algorithm produces neighbour solutions of the current solution, then accepts the solution if it is superior to the existing solution. However, if the outcome is not better than the existing one, it checks the probability rule and accepts the new solution if it fulfils the probability rule. The procedure proceeds until the stopping criterion is satisfied.Fig. 3shows the mechanism of the original PNN, where training data is used to train the PNN and then the test data is classified using the PNN. In this stage, the accuracy of the classified data is calculated by Eq. (7).Fig. 4shows the structure of the proposed SFA approach. The FA is invoked to adjust the weights of the PNN and the test data is classified, and then the accuracy of the newly classified data is calculated. The first phase of the proposed approach concerns the FA, which is used as an improvement algorithm. As mentioned above, the FA is based on the light density of fireflies: when fireflies are brighter they attract other fireflies to the locations of more efficient solutions and finally to the optimal solution. The FA has been applied successfully in solving many optimization problems because of its good exploration capability. Since the size of the search space in a PNN is very large, an efficient method is needed to find the optimal solution, hence we employ the FA to find optimal values for the weights of the PNN. In the FA, some candidate solutions are randomly created and spread in the search space, then the fitness value for each candidate is calculated. All the candidates start moving towards the better position and during this movement the search for the optimal solution is carried out. If the candidate reaches the best position, it is considered to be the best candidate.The second phase of the proposed approach relates to the SA (see Fig. 4), which is also used as an improvement algorithm. The best result obtained by using the FA to evalute the population is passed to the SA algorithm to generate a neighbour solution. The new solution is accepted if it is better than the current one. If the new solution is worse than the current one, the probability rule is checked and the new solution is accepted if it satisfies the probability rule (as in Eq. (4)). This process continues until the temperature reaches zero. After that, the best solution is sent to the FA to generate a new population based on the best candidate. During this process, the search for the optimal solution is incessant. The accuracy of the optimal solution is calculated at the end of the procedure. The SFA is summarized in the pseudocode shown in Fig. 5.As seen from Fig. 4, the procedure starts from an initial population of randomly generated individuals. The quality of each individual is calculated using Eq. (7) and the best solution among them is selected. The hybridization of a FA with a SA algorithm is employed to achieve a balance between exploration and exploitation in order to ensure efficient convergence and obtain an accurate solution. It is also used as an improvement algorithm because the best result using the FA after the optimal solution is calculated they passed to the SA algorithm to generate a neighbour solution, and the new solution is accepted if it is better than the current one.Concurrently, the FA randomly generates the initial population of candidate solutions for the given problem (here, the weights of the PNN). After that, it calculates the light intensity for all fireflies and finds the most attractive firefly (best candidate) within the population. Then, it calculates the attractiveness and distance for each firefly to move all fireflies towards the most attractive firefly in the search space. Next, the best solution among the population is passed to the SA as the initial solution, Sol. Then the SA generates a neighbourhood/new solution, Sol*. The SA accepts the new solution, Sol*, if it is better than the current one. If the new solution is worse than the current one, Sol, it checks the probability rule and accepts the new solution if it satisfies the probability rule, which is computed as follows, where temp is a current temperature:(4)exp(−f(Sol*)−f(Sol)temp)>random[0,1]At every iteration, the temp is decreased by α, as defined in Eq. (5). The algorithm stops when the maximum number of iterations, Iter_max, is reached or the penalty cost is zero.(5)α=(log(T0)−log(Tf)/Iter_max)If the termination criterion is not met, the solution is returned to the FA and the FA begins again.In the second proposed hybrid method, denoted as LFA, the FA is hybridized with Lévy flight to solve classification problems. Lévy flight is a random walk in which each step of the movement is independently drawn from a probability distribution that has a heavy power-law tail. However, this power-law tail means that sometimes a very large step is taken. The best application that has been suggested for Lévy flight is for determining the movements of fishing boats, where it acts as an indicator of susceptibility and might serve as a warning mechanism for fisheries management [43]. The behaviour of Lévy flight can also be applied to optimization and optimal searches [44]. The hybridization of FA with Lévy flight is employed to create a balance between exploration and exploitation in order to ensure efficient convergence and an accurate solution.Fig. 6shows the structure of the proposed LFA approach. In the FA, all the candidates start moving towards the best positions and Lévy flight is used to control the movement of each firefly towards the best candidate by controlling the random step inside the movement operation in the FA.LFA algorithms is a metaheuristic algorithm that was developed by combining Lévy flight with the search strategy of the FA [33], as represented by the following equation:(6)xi=xi+β0∗exp(−γτij2)*(xj−xi)+αsignrand−12⊕Lévywhere the first term is the current position of a firefly, the second term is used for the attractiveness of a firefly, and the third term is randomization using Lévy flight as the randomization parameter. The product ⊕ means entrywise multiplications. The sign (rand −½), where rand is between [0,1], essentially provides a random sign or direction while the random step length is drawn from a Lévy distribution. The LFA is summarized in the pseudocode shown in Fig. 7.As seen in Fig. 7, Lévy flight is used to control the movement of each firefly towards the best candidate by controlling the random step inside the movement operation in the FA in order to ensure efficient convergence and obtain an accurate solution.In this method, denoted as LSFA, the FA is hybridized with Lévy flight to evaluate the population in each iteration and the best result is passed to the SA to generate a neighbour solution to solve classification problems. The aim of this algorithm is to try to control the random step in the firefly mechanism.In order to improve the algorithm further, two particular issues need to be considered, which are related to the absorption coefficient and that affect the accuracy of the solution. In the first case, fireflies can clearly see the brighter fireflies because they emit the same amount of light at any distance. Therefore, some of the further-away fireflies bright fireflies attempt to advance towards these brighter fireflies by taking the largest possible steps to save time. Thus exploration and exploitation are out of balance because exploitation is maximal and exploration is normal. In the second case, fireflies cannot see any brighter fireflies so the movement of fireflies is in random steps, thus exploration and exploitation is out of balance because the fireflies only explore and do not exploit the search space. Therefore, our proposed LSFA attempts to address these imbalances between exploration and exploitation by ensuring that both sides of the current value are explored thoroughly to find the optimal value. In other words, the firefly mechanism starts searching the problem space at distances far from the current location (exploration capability), while the search process is restricted to a very small area around the current location as the iterations elapse (exploitation capability).

@&#CONCLUSIONS@&#
