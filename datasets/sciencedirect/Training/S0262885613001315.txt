@&#MAIN-TITLE@&#
Human activity recognition in videos using a single example

@&#HIGHLIGHTS@&#
A hierarchical structure for accurate video to video matching and event recognitionIncorporating contextual information to the “bag of video words” frameworkCoding spatio-temporal compositions of video volumes by a probabilistic framework

@&#KEYPHRASES@&#
Action recognition,Bag of video words,Hierarchical codebook,Spatio-temporal contextual information,Probabilistic modeling,Context,Ensemble of volumes,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Human activity analysis is required for video surveillance systems, human–computer interaction, sports interpretation, and video retrieval for content-based search engines [1,2]. Moreover, given the tremendous number of video data available online these days, there is a great demand for automated systems that analyze and understand the contents of these videos. Recognizing and localizing human actions in a video is the primary component of such a system, and also the most important, as it affects the performance of the whole system significantly. Although there are many methods to determine human actions in highly controlled environments, this task remains a challenge in real world environments due to camera motion, cluttered background, occlusion, and scale/viewpoint/perspective variations [3–6]. Moreover, the same action performed by two persons can appear to be very different. In addition, clothing, illumination and background changes can increase this dissimilarity [7–9].To date, in the computer vision community, “action” has largely been taken to be a human motion performed by a single person, taking up to a few seconds, and containing one or more events. Walking, jogging, jumping, running, hand waving, picking up something from the ground, and swimming are some examples of such human actions [1,2,6]. In this paper, our main goal is to address the problem of action recognition and localization in real environments using a hierarchical probabilistic video-to-video matching framework. This problem is also referred to as action spotting[10]. To achieve this, we have developed a fast data-driven approach, which finds similar videos in a “target” set to a single labeled “query” video. Assuming that the latter contains an action of interest, e.g., walking, we find all videos in the target set that that are similar to the query, which implies the same activity. This video-to-video comparison also makes it possible to label activities, the so-called action classification problem. An overview of the algorithm is presented in Fig. 1. The major benefit of our approach is that it does not require long video training sequences, object segmentation, tracking or background subtraction. The method can be considered as an extension to the original bag of video words (BOV) approach for action recognition.Although an initial spatio-temporal volumetric representation of human activity may eliminate some pre-processing steps, for example background subtraction and tracking, it suffers from some major drawbacks. For example, in general, BOV-based approaches for activity recognition in the literature involve salient point detection. They usually ignore the geometrical and temporal structure of these visual volumes, as they store STVs in an unordered manner. Also they are unable to handle scale variations (spatial, temporal, or spatio-temporal) because they are too local, in the sense that they consider just a few neighboring video volumes (e.g., five nearest neighbors in [11] or just one neighbor in [4]). To overcome these issues, we have developed a multi-scale, hierarchical codebook of BOVs for densely sampled videos, which incorporates spatio-temporal compositions and their uncertainties. This permits the use of statistical inference to recognize the activities. We also note that, in order to measure similarity between a query and a target dataset, it is necessary to use information regarding the most informative spatio-temporal video volumes (STVs) in the video, i.e., the salient foreground objects. To select these space-time regions, we use the information obtained from our hierarchical BOV method, which in a sense, can be viewed as being a context-based spatio-temporal segmentation method.In this paper we present a hierarchical probabilistic codebook method for action recognition in videos, which is based on STV construction. The method uses both local and global compositional information of the volumes, which are obtained by dense sampling at various scales. Similar to other volumetric methods, we do not require background subtraction, motion estimation, or complex models of body configurations and kinematics. Moreover, the method tolerates variations in appearance, scale, rotation, and movement.As shown in Fig. 1, the proposed algorithm consists of two main components, hierarchical codebook construction of salient STVs and an inference mechanism for measuring the similarity between salient STVs of the query and target videos. Hierarchical codebook construction consists of four steps: coding the video to construct STVs and low-level probabilistic codebook formation while considering the uncertainties in the STVs; constructing ensembles of video volumes for each pixel in a video frame containing a large number of STVs and probabilistic models of their spatio-temporal compositions; high-level codebook construction of the ensembles; and finally, analyzing codewords as a function of time in order to construct a codebook of salient regions. The inference mechanism is based on a set of codewords constructed for each query video. It determines the most similar compositions of STVs in the target videos that match the query video. There are two important differences between our proposed hierarchical approach and previously reported ones. First, the latter are unable to handle both local and global compositional information. Second, they always select the informative regions at the lowest level of the hierarchy.The main contributions of this paper are as follows:•We introduce a hierarchical codebook structure for action detection and labeling. This is achieved by considering a large volume containing many STVs and constructing a probabilistic model of this volume to capture the spatio-temporal configurations of STVs. Consequently, similarity between two videos is calculated by measuring the similarity both between spatio-temporal video volumes and their compositional structures.We select the salient pixels in the video frames by analyzing codewords obtained at the highest level of the hierarchical codebook's structure. This differs from conventional background subtraction and salient point detection methods.In order to evaluate the capability of our approach for action matching and classification we have conducted experiments using three datasets: KTH [12], Weizmann [13] and MSR II [14].22http://research.microsoft.com/en-us/um/people/zliu/ActionRecoRsrc/.Three types of experiments were performed: action matching and retrieval, single dataset video classification, and cross-dataset action recognition. A preliminary version of this paper appeared in the International Conference on Computer and Robot Vision [15]. This paper is different from [15] in the following respects: 1) we provide more detailed descriptions of how the proposed algorithm learns visual context, 2) we have formulated the contextual graphs and similarity measurement in a spatio-temporal context, 3) a multiscale approach is implemented to deal with large variations in the scale of the actions, and 4) effects of different parameters has been evaluated by conducting extensive experiments. The rest of this paper is organized as follows. Section 2 reviews recent work on action recognition. Section 3 describes the proposed approach for hierarchical codebook construction and the steps of the algorithm. Section 4 describes the action matching algorithm. Section 5 then presents the experimental results and finally, Section 6 concludes the paper.

@&#CONCLUSIONS@&#
