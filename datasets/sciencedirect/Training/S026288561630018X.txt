@&#MAIN-TITLE@&#
Action recognition via spatio-temporal local features: A comprehensive study

@&#HIGHLIGHTS@&#
A timely review of action recognition based on spatio-temporal local features.Transfer techniques in image domain to video domain for action recognition.Evaluation of representation methods for action recognition.

@&#KEYPHRASES@&#
Action recognition,Spatio-temporal local features,Feature coding,Bag-of-words,Sparse coding,Fisher kernel,VLAD,NBNN,Match kernels,Performance evaluation,

@&#ABSTRACT@&#
Local methods based on spatio-temporal interest points (STIPs) have shown their effectiveness for human action recognition. The bag-of-words (BoW) model has been widely used and dominated in this field. Recently, a large number of techniques based on local features including improved variants of the BoW model, sparse coding (SC), Fisher kernels (FK), vector of locally aggregated descriptors (VLAD) as well as the naive Bayes nearest neighbor (NBNN) classifier have been proposed and developed for visual recognition. However, some of them are proposed in the image domain and have not yet been applied to the video domain and it is still unclear how effectively these techniques would perform on action recognition. In this paper, we provide a comprehensive study on these local methods for human action recognition. We implement these techniques and conduct comparison under unified experimental settings on three widely used benchmarks, i.e., the KTH, UCF-YouTube and HMDB51 datasets. We discuss insightfully the findings from the experimental results and draw useful conclusions, which are expected to guide practical applications and future work for the action recognition community.

@&#INTRODUCTION@&#
Human action recognition as an active topic in the computer vision community has been extensively researched in the last decades. Most of the existing methods, including both low-level feature extraction and high-level representations, in action recognition are extended from the text and image domains, i.e., the bag-of-word (BoW) model [1]. Local features have shown increasing effectiveness in visual recognition, and local methods based on spatio-temporal local features, e.g., three-dimensional histogram of oriented gradients (HOG3D) [2] and HOGHOF [3], become popular in action recognition since the inventions of spatio-temporal interest points detectors [4–7]. In contrast to holistic representations [8,9], local methods enjoy many advantages such as 1) avoidance of some preliminary steps, e.g., background subtraction and target tracking required in holistic methods, and 2) resistance to background variation and occlusions.The most widely used local methods, e.g., the bag-of-word (BoW) model [1] and sparse coding (SC) [10–12], have obtained remarkable performance in image and object classification. Recently, refinements of BoW and SC as well as alternative techniques including the soft assignment coding (kernel codebooks) [13], Triangle assignment coding [14], localized soft-assignment coding (LSC) [15] and locality linear-constrained coding (LLC) [16], have been developed to forward the state-of-the-art. However, these developments mostly remain in the image domain, which makes transferring them to the video domain an urgent and promising task.A simple non-parametric nearest neighbor (NN) based classifier, naive bayes nearest neighbor (NBNN) [17], was proposed in recently. By computing the ‘ Image-to-Class' rather than ‘ Image-to-Image’ distance, NBNN is able to avoid quantizing local features in the BoW model. In contrast to learning-based classifiers, the non-parametric NBNN classifier requires no training phase thus no risk of overfitting the parameters. Recently, enhanced versions of NBNN, including the NBNN kernels [18] and the local NBNN [19], have also been developed. The NBNN family have shown excellent effectiveness in image and object recognition.The Fisher kernel (FK) has recently drawn increasingly attention in the image domain and produced remarkable results for image classification [20–22]. It is shown in a recent study on feature coding [23] that the improved Fisher kernel (IFK), which is also called Fisher vector (FV), outperforms all the other encoding methods on several image datasets. Another important encoding method is the vector of locally aggregated descriptors (VLAD) introduced by Jégou et al. [24,25]. VLAD can be regarded as a simplified non-probabilistic version of Fisher vector and has shown comparable results with IFK.Match kernels between sets of local features have long been exploited in visual recognition [26,27]. Without relying on any mid-level feature representations, match kernels are able to compute the similarity between sets of unordered local features and have shown the effectiveness in image and object recognition. More importantly, match kernels provides a basic formulation of measuring two sets of local features, based on which local methods are connected. The newly proposed feature coding techniques have been widely used and demonstrated their effectiveness in the image domain, however, their performance on action recognition has not been comprehensively evaluated and compared. Motivated by this, in this paper, we transfer these prevailing techniques from the image domain to the video domain and put them under a unified evaluation framework with the same experimental settings. In contrast to the previous evaluations [5,28–30], we focus on the evaluation of state-of-the-art local methods, e.g., the BoW model, sparse coding, Fisher kernels, VLAD, NBNN and match kernels, based on spatio-temporal local features for human action recognition.Recently, methods using tracking of trajectories has been used for action recognition which can always outperform those based on STIPs while requiring higher computational complexity [31]. In addition, it is found by Reddy and Mubarak [32] that motion based descriptors are not scalable with respect to the number of action categories, which can be reasonably assumed to also hold for trajectory-based sampling of descriptors. As we concentrate on the comparison of representation methods rather than the overall performance, we follow a standard paradigm for action recognition using local features [28,29], and apply the same feature detection and description steps to all the methods to be evaluated.We systematically evaluate the performance of representative local methods, some of which have not been used for action recognition yet. Extensive experimental results have been reported on three widely used benchmark action datasets, i.e., KTH, UCF-YouTube and HMDB51. To the best of our knowledge, we, for the first time, pull local methods under a unified setting and conduct a comprehensive study both theoretically and experimentally for action recognition.The main contributions of this paper lie in the following three aspects: 1) we have conducted a comprehensive study on state-of-the-art local methods for human action recognition, which serves as a baseline for research in this field; 2) we provide in-depth analysis and draw impartial conclusions from the findings in the experiments, which offers an important guide for further work on human action recognition; 3) we provide a timely review on the recent advancement of local methods based on spatio-temporal local features, which can be used as an up-to-date reference for the community of action recognition.During the last decade, action recognition with local spatio-temporal interest points (STIPs) have been extensively explored. To give an overview of the advancement of local features for human action recognition, we will provide a review of recently developed local methods both within and beyond the BoW model. In the following, we will give a more detailed description of these methods.The BoW model is a widely used algorithm for local representations and has proven to be successful in many action recognition tasks. However, local representations also suffer from many limitations. One of the most notorious deficiencies is that it fails to capture adequate structural and temporal information. In order to compensate for the loss of structures in local representations, a lot of methods try to improve local representations by exploring spatio-temporal structural information [33], including context information of each interest point [34,35], relationships between/among spatio-temporal interest points [36–39] and neighborhood-based features [40]. The relationship among visual words in the BoW model and their semantic meaning have also been explored to encode higher-level features [15,41–43]. New local descriptors have also been developed [44,45] to improve the performance of local methods.Sun et al. [34] proposed to model the spatio-temporal context information in a hierarchical way by exploiting three levels of context, namely, point-level, intra-trajectory and inter-trajectory context. In their work, trajectories are first extracted using Scale Invariant Feature Transform (SIFT). The point-level context is the average of SIFT descriptors extracted at the salient points on the trajectory. Intra-trajectory and inter-trajectory context is modeled by the transition matrix of a Markov process and encoded as the trajectory transition and trajectory proximity descriptors.In order to capture the most informative spatio-temporal relationship between local descriptors, Kovashka and Grauman [40] proposed to learn a hierarchy of spatio-temporal neighborhood features. The main idea is to construct a higher-level vocabulary from new features that consider the hierarchical neighboring information around each interest point.Matikainen et al. [36] proposed to express pair-wise relationships between quantized features by combining the power of discriminative representations with key aspects of naive Bayes. The relationship between local features is modeled as the distribution of quantized location differences between each pair of interest points. Two basic features namely STIP-HOG and quantized trajectories are considered.Gaur et al. [33] modeled the activity in a video as a “string of feature graphs” (SFGs) by treating a video as a spatio-temporal collection of primitive features (e.g., STIP features). They divide the features into small temporal bins and represent the video as a temporally ordered collection of such feature-bins, each bin consisting of a graphical structure representing the spatial arrangement of the low-level features. A video then becomes a string of such graphs and comparing two videos is to match two strings of graphs.Claiming that the higher-order semantic correlation between mid-level features (e.g., from the BoW representation) is useful to fill the semantic gap, Lu et al. [42] proposed novel spectral methods to learn latent semantics from abundant mid-level features by spectral embedding with nonparametric graphs and hypergraphs. A new semantics-aware representation (i.e., histogram of high-level features) is derived for each video from the original BOW representation, and actions are classified by a SVM with a histogram intersection kernel based on the new representation.Wang et al. [38] presented a novel local representation by augmenting local features with contextual features, which capture the interactions between interest points. Different from previous work on mining contextual information is considered as spatio-temporal statistics in the 3D neighborhood of each interest point. Multi-scale channels of contextual features are computed and, for each channel, a regular grid is used to encode spatio-temporal information in the local neighborhood of an interest point. Multiple kernel learning is employed to integrate the contextual features from different channels.Aiming to encode rich temporal ordering and spatial geometry information of local visual words, Zhang et al. [41] proposed to model the mutual relationships among visual words by a novel concept named the spatio-temporal phrase (ST phrase). A ST phrase is defined as a combination of k words in a certain spatial and temporal structure including their order and relative positions. A video is represented as a bag of ST phrases which is shown to be more informative than the BoW model.In order to capture the geometrical distribution of interest points, Yuan et al. [39] applied the 3D R transform on the interest points based on their 3D locations. The 3D R-transform is invariant to geometrical transformation and robust to noise. (2D)2 PCA is then employed to reduce the dimensionality of the 2D feature matrix from the 3D R transform, obtaining the so-called R features. To encode the appearance features, they combined the R features with the BoW representation. Finally, they proposed a context-aware fusion method to efficiently fuse these two features. Specifically, one feature is used to compute the context of each video and the other to calculate the context-aware kernel for action recognition.In the BoW model, mid-level features are obtained by k-means clustering which however is unable to capture the semantic relation between low-level features due to that only appearance similarity is used. Liu et al. [15] proposed to use diffusion maps to automatically learn a semantic visual vocabulary from abundant quantized mid-level features. Each mid-level feature is represented by the vector of point-wise mutual information (PMI). Diffusion maps can capture the local intrinsic geometric relations between the mid-level feature points on the manifold.With the argument that visual words from video sequences belonging to the same class in the BoW model are correlated and jointly reflect a specific action type, Wang et al.[43], by assuming that visual words share a common structure in a low-level space, presented a framework named semi-supervised feature correlation mining (SFCM) to exploit the shared structure. A discriminative and robust classifier for action annotation is trained by taking into account the global and local structural consistency.Shapovalova et al. [46] proposed to model a video using a global bag-of-words histogram based on local features, combined with a bag-of-words histogram focused latent regions of interest. The latent regions of interest are spatio-temporal sub-regions of a video. The model parameters are learned by a similarity constrained latent SVM, in which the constraint is to enforce that the latent regions chosen across all videos of a class are coherent.Le et al. [44] introduced an unsupervised deep learning algorithm, named Independent Subspace Analysis (ISA), which learns spatio-temporal features of interest points from unlabeled videos. Convolution and stacking are adopted in the deep learning model to scale the algorithm to large images and learn hierarchical representations.As indicated by Wang et al. [28] that dense sampling tends to produce better results than sparsely detected spatio-temporal interest points. Wang et al. [35] presented an approach by dense trajectories. Dense points are sampled from each frame and tracked based on displacement information from a dense optical flow field. A novel descriptor based on motion boundary histograms was introduced in their work to encode the trajectory information. The remarkable performance of dense trajectories is largely due to the rich description of scene and contextual information of dense sampling, and the robust extraction of motion information of trajectories.Also based on dense trajectories, Jiang et al. [47] presented a new video representation that integrates trajectory descriptors with the pair-wise trajectory locations as well as motion patterns. Global and local reference points are adopted to characterize motion information with the aim to be robust to camera movements.Aiming to alleviate the quantization errors in the BoW model, sparse coding has also been introduced to action recognition to learn more compact and richer representations of human actions [12,48,49].Rather than using the BoW model, Dean et al. [48] presented a new approach using the sparse coding algorithm to learning sparse, spatio-temporal features for activity recognition. A multi-stage approach is used to learn spatio-temporal features that can discriminate different actions.In order to obtain a more accurate and discriminative representation, an approach by encoding local 3D spatial–temporal gradient features was proposed by Zhu et al. [49] in which the sparse coding framework is used for the final action representation. A local spatial-temporal feature is transformed to a linear combination of a few atoms in a trained dictionary. They also investigated the construction of the dictionary with a scenario of transfer learning.Guha and Kreidieh [12] comprehensively explored sparse representations for human action recognition in video. Overcomplete dictionaries are learned from a set of local spatio-temporal descriptors in the training set. It is claimed that the obtained representation based on the dictionaries learned by sparse coding is more compact compared with the BoW model involving clustering and vector quantization. Three options of dictionaries, namely, shared, class-specific and concatenated, were investigated.Recently, Fisher kernels have been applied to the video domain for human action recognition based on local features. Oneata et al. [50] evaluated the use of Fisher vectors as an alternative to the BoW model to aggregate a small set of low-level descriptors, in combination with linear classifiers for both action recognition and localization. Kantorov and Laptev [51] developed highly efficient video features called the MPEG flow video descriptor using motion information in video compression and represented actions by Fisher vectors. The method improves the speed of video feature extraction, feature encoding and action classification. Peng [52] proposed the two-layer stacked Fisher vectors (SFV) for action recognition. In the first layer, large subvolumes are densely sampled from input videos, from which local features are extracted and encoded using Fisher vectors (FVs). The second layer compresses the FVs of subvolumes obtained in the previous layer, and then encodes them again with Fisher vectors. Compared with standard FV, SFV allows refining the representation and abstracting semantic information in a hierarchical way.Motion is regarded as the most reliable source of information for human action recognition, as it is related to the regions of interest. Jain et al. [45] introduced the Divergence–Curl–Shear (DCS) descriptor to encode scalar first-order motion features. This descriptor contains the motion divergence, curl and shear, which capture physical properties of the flow pattern. To handle the noisy motion from background and the unstable camera, an affine model is employed for motion compensation to improve the quality of descriptors. Dense trajectories are also used and the vector of locally aggregated descriptors (VLAD) is adopted for the final encoding of local features which is shown to be better than a standard BoW model. Although densely sampling shows increasing performance with the decrease of the sampling step size, it does not scale well with a large number of local patches and becomes even computationally intractable for large-scale video datasets. Vig et al. [53] proposed to select informative regions and descriptors by saliency-mapping algorithms. These regions are either used exclusively or given greater representational weights. By using the saliency-based pruning, up to 70% of descriptors can be discarded, while maintaining high performance on the Hollywood2 dataset.Beyond the BoW, sparse coding, FV and VLAD frameworks, many new methods have also been proposed from action representation and recognition including multiple feature fusion, matching kernels and deep learning based features.Cai et al. [54] propose Multi-View Super Vector (MVSV) for global action representation, which is composed of relatively independent components derived from a pair of descriptors. They develop a generative mixture model of probabilistic canonical correlation analyzers (M-PCCA), and utilize the hidden factors and gradient vectors of M-PCCA to construct MVSV for video representation. MVSV has outperformed FV and VLAD with descriptor concatenation and kernel fusion.To encode the relationships among local feature descriptors, Wu at al. [55] construct a two-graph model based on the 3D SIFT descriptor to represent human actions by recording the spatial and temporal relationships among local features. A novel family of context-dependent graph kernels (CGKs) are further proposed to measure similarity between graphs. Finally, a generalized multiple kernel learning algorithm with a proposed ℓ1,2-norm regularization is applied to combine these CGKs optimally together and simultaneously train a set of action classifiers.Yang and Tian [56] introduce an effective coding scheme to aggregate low-level descriptors into a super descriptor vector (SDV). In order to incorporate the spatio-temporal information, a novel approach of super location vector (SLV) was proposed to model the space-time locations of local interest points in a much more compact way compared to the spatio-temporal pyramid representations.Sun et al. [57] propose to combine SFA with deep learning techniques to learn hierarchical representations from the video data itself. A two-layered SFA learning structure with 3D convolution and max pooling operations is used to scale up the method to large inputs and capture abstract and structural features from the video. The method shows 1% improvement in comparison to state-of-the-art methods even without supervision or dense sampling on the KTH dataset.Recently, Lan et al. [58] propose a novel feature enhancing technique called Multi-skIp Feature Stacking (MIFS), which stacks features extracted using a family of differential filters parameterized with multiple time skips and encodes shift-invariance into the frequency space.Yang et al. [59] propose a multi-feature max-margin hierarchical Bayesian model (M3HBM) for action recognition. M3HBM jointly learns a high-level representation by combining a hierarchical generative model (HGM) and discriminative max-margin classifiers in a unified Bayesian framework.

@&#CONCLUSIONS@&#
