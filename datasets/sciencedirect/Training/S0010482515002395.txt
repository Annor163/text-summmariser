@&#MAIN-TITLE@&#
A wrapper-based approach for feature selection and classification of major depressive disorder–bipolar disorders

@&#HIGHLIGHTS@&#
We used the combination of support vector machine (SVM) and improved Ant Colony Optimization (IACO).We used an improved version of standard Ant Colony Optimization.We used coherence as a biomarker.We reduced the feature set from 48 to 22 using IACO.We modeled an SVM model using QEEG coherence values.We increased overall accuracy from 62.37% to 80.19% and AUC from 0.631 to 0.793 while decreasing the computational complexity and the number of features.

@&#KEYPHRASES@&#
Artificial intelligence,Support vector machine,Improved Ant Colony Optimization,Major depressive disorder,Bipolar disorder,Coherence,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Advances in computer science and data acquisition systems simplified collecting and storing large data sets with long time series. These data sets have found increasingly frequent and various application fields, such as astronomy [1], biology [2], finance [3], marketing [4] medicine [5], data mining and knowledge discovery purposes [6]. Intrinsically, the evaluation process of large data is a valuable process and recent studies underline the use of FS methods with their promising outcomes [7–10].Large scale of datasets with high feature dimensionality may state high classification accuracy and over-fitting performance without using proper validation methods. In order to both overcome that biased error estimate and remove the irrelevant and noisy features that mislead or impede early diagnose and effective treatment process, nested-CV is used. With nested-CV, an inner-CV loop is used for model selection while an outer-CV is used to compute an estimate of the error with a completely new dataset. Through the use of nested FS, such ubiquitous problems can be automatically detected and removed, resulting in more reliable subset or pattern discovery in many fields [11–15]. Within this context, a wrapper-based system is generally used combining a classifier and a meta-heuristic algorithm to identify the best subset of features without sacrificing prediction accuracy.A common way to describe meta-heuristic algorithms is that they combine randomness, probability and mathematical equations to imitate natural phenomena. These phenomena include the biological evolutionary process like genetic algorithm (GA) [16] and the differential evolution (DE) [17], animal behaviour like particle swarm optimization (PSO) [18], ACO [19], and the physical annealing process like simulated annealing (SA) [20]. Many meta-heuristic algorithms and their improved modifications have also been successfully applied to various optimization problems in recent studies [21–24]. Compared to conventional numerical methods, those algorithms have outperformed on generating better solutions [25]. Among these meta-heuristic algorithms, ACO is a stochastic search method based on observations of social behaviors of real insects or animals and it has been shown to be an efficient algorithm for FS problems [15,26–30]. However there are still some weaknesses of ACO in practice. The probability to get trapped in local optimal solution, high computational time and system resources requirement to obtain the optimal solution and the difficulties to set the heuristic parameters to achieve the good performance are prominent considerations of the algorithm. So, in order to avoid the potential weaknesses, similar improved ACO methods were proposed in recent studies [31–35].In the last decade, there has been an upsurge of interest within the neuroscience community in the use of AI methods. One such method is supervised machine learning (ML), in the area of AI, can automatically detect patterns in the existing training data and then use the detected patterns to make prediction on future data [36]. Compared to conventional methods, the advantages of applying supervised ML could be underlined twice. Supervised ML methods address individual differences, rather than considering group differences as most traditional statistical comparisons do, and classify subjects in order to contribute to clinical decision process. Those methods generate a model using training set that includes input and output data. Following the classification process, the model is tested using external test data to estimate prediction capability of the model. Those methods are also sensitive to spatially distributed and subtle effects in the brain that would otherwise be indistinguishable applying traditional univariate methods which focus on gross differences at group level [37]. SVM is a specific type of supervised ML method based on the structural risk minimization (SRM) principle. SVM is used to solve classification problems by maximizing the margin between the two opposing classes separated through a hyperplane. SVMs are widely used in order to solve the problem of model selection, over-fitting, nonlinear, the curse of dimensionality and local minimum in a better way [38–39], and have promising outcomes in regression tasks [40] and also are widely used in the classification of psychiatric disorders [41–45]. Some psychiatric disorders are frequently misdiagnosed which ultimately lead to suboptimal treatment and poor outcomes. One good example for such diagnostic dilemma is the difficulties in discriminating depressive episodes in patients with BD and patients experiencing MDD, a clinical term which has been used for clinical cases experiencing depression without any lifetime presence of mania [46]. Discriminating MDD and BD at earlier stages of illness could therefore help to facilitate efficient and specific treatment. This is because, bipolar disorder is linked with poorer functioning and the highest rates for committed suicides, and using specific treatments such as mood stabilizers may be crucial for the treatment of bipolar patients [47]. In addition, receiving antidepressants for patients with BD may induce a manic episode, characterized by elevated mood, agitation, grandiose delusions and a marked increase in goal-directed behaviour may result with inappropriate risk-taking [48]. Recent studies have utilized neuroimaging methods between MDD and BD to reveal discrete patterns of functional and structural abnormalities in neural systems and found some potential [42–44]. In some other studies, conventional statistical techniques were used and those methods rely on the basic assumption of linear combinations which may have well-known inadequacies for discriminating heterogeneous symptom based psychiatric diagnosis [45]. Over the past decade, machine learning methods have been used increasingly in the study of affective disorders and in comparisons of these patients to those with other psychiatric disorders [49].Thus, classifying MDD and BD at earlier stages of illness could therefore help to facilitate efficient and specific treatment. Some studies used neuroimaging methods for BD and MDD to reveal discrete patterns of functional and structural abnormalities in neural systems critical for emotion regulation [49–51]. In some other studies, traditional statistical techniques were used and those methods rely on the basic assumption of linear combinations only, so may not be appropriate for such tasks [52]. Over the past decade, machine learning methods have been used increasingly in the study of affective disorders and in comparisons of these patients to those with other psychiatric disorders [53].A recent study used SVM to compare the diagnostic performance of BD and MDD and classified the subjects with 54.76% accuracy [54]. In a similar study pattern recognition analysis was applied using subdivisions of anterior cingulate cortex (ACC) blood flow at rest. SVM classified MDD and BD subjects using subgenual ACC blood flow 81% accuracy [55]. Another study employed multivariate pattern classification techniques using brain morphometric biomarkers and yielded up to 79.3% accuracy by differentiating the 2 depressed groups [56]. In another study, the epileptic seizure detection for multichannel EEG signals based on the automatic identification system is presented. Considering both MDD and BD EEG signals approximate entropy and statistic values were used for feature extraction. The seizure detection accuracy of SVM with various kernel functions was also tested in this study using MDD and BD EEG signal. The prediction accuracy is given as 97.17% for RBF type kernel SVM model [57]. Beside high classification accuracy contribution to the diagnosis process, computerized feature extraction methods have been used increasingly in the study of affective disorders and in comparisons of psychiatric disorders. Depressive episode in BD is regarded among the most wearing psychiatric disorders with a lifetime prevalence of up to 4–5% [58]. Although BD and MDD have been considered as distinct clinical cases, treated with specific therapeutic methods, former studies revealed that 60% of BD cases were incorrectly diagnosed as UD, and were consequently treated inappropriately [49]. Thus, it is critically important to determine the biomarkers reflecting distinctive pathophysiologic processes in BD and MDD [59]. EEG coherence is one of those potential neurophysiological biomarkers reflecting brain dynamics. Coherence describes relationship between signals in a given frequency band and various spatial coherence signals are gathered over long distances as parallel processing [60–62]. EEG coherence is a remarkable large scale measure of functional relationships or synchronized functioning between cortical regions pairs, therefore coherence appreciated as a biomarker representing the brain׳s functional connectivity [63–65]. Recent studies underline the clinical contribution of coherence as biomarker in the classification of psychiatric disorders [45,66–68].Through above analysis, this paper aims to reveal the discriminating features without sacrificing the classification accuracy of MDD and BD subjects using SVM and IACO. The coherence, a measure of functional connectivity in the brain, values were first calculated using a previously defined method [69]. Subjects, EEG recordings and coherence calculation steps are given in Section 2. FS methods and the hybrid structure with SVM are described in Section 2.4. Computational experiments of proposed approach are reported in Section 3 and finally the outcomes in terms of engineering and psychiatry perspective are discussed in Section 4.We conducted a retrospective investigation in 1977 patients who applied to the Neuropsychiatry Istanbul Hospital Department of Psychiatric Outpatient Clinic between January 2010 and April 2015. Among these patients, 101 patients receiving the diagnose of BD and MDD on admission were recruited for this study. 46 bipolar disorder patients in depressive episode (17 males and 29 females) and 55 patients with MDD (23 males and 32 females) were matched. Eligible subjects were outpatients suffering from a depressive episode of BD or MDD, diagnosed according to Diagnostic and Statistical Manual of Mental Disorders (DSM)-IV criteria for either a primary diagnosis of bipolar affective disorder depressive episode of major depressive episode on the Structured Clinical Interview for Axis I Disorders (SCID-I). We included subjects with a diagnosis of MDD who received at least the scores of 8 on the Hamilton Depression Rating Scale-17 item version (HDRS) or subjects with a diagnosis of BD depressive episode and scoring higher than 13 points in Young Mania Rating Scale (YMRS) [70]. We excluded the subjects with first depressive episode, episode with current psychotic features, history of rapid cycling (≥4 cycles during a year), history of mixed episodes, current psychiatric comorbidity on axis I, serious unstable medical illness or neurologic disorder (e.g., epilepsy, head trauma with loss of consciousness), alcohol or substance abuse within 6 months preceding the study and patients treated by electroconvulsive therapy within 3 months before their participation to the study. We also excluded subjects with less than four psychiatric admissions. This criterion was set to ensure the longitudinal reliability of the diagnosis. All patients were evaluated by four experienced psychiatrist with a clinical expertise of at least 5 years. Inter-rater reliability was not evaluated. However, we also excluded subjects with less than four psychiatric admissions. This criterion was set to ensure the longitudinal reliability of the diagnosis. As a note of caution, none of the patients were under antidepressant medication at the time of the EEG recording. This is because before QEEG recording is a routine procedure conducted before the planning of treatment for all patients who applied to Neuropsychiatry Istanbul Hospital. However, BD subjects were receiving a mood stabilizer or a combination of mood stabilizers (25% lithium, 57.5% sodium valproate, 17.5% quetiapine and olanzapine). Participants met the routine laboratory studies (complete blood count, chemistry, thyroid stimulating hormone), urine toxicology screen, and electrocardiogram were performed at study screening, and subjects were required to be medically stable before enrollment to the study.For all the patients, EEGs were recorded for five minutes of eyes-closed resting state condition. Patients were instructed to avoid medication for 12h before the EEG recording took place. In order to observe and reveal the efficacy of coherence, quantitative EEG (QEEG) data were collected from 101 subjects who were seated in a sound-attenuated, electrically shielded room in a reclining chair with eyes closed (wakeful–resting condition). The technicians monitored the QEEG data during the recording and re-alerted the subjects every minute as needed to avoid drowsiness. Electrodes were placed with an electrode using 19 recording electrodes distributed across the head according to the international 10–20 system arrangement. Three minutes of eye-closed EEG at rest were acquired using Scan LT EEG amplifier and electrode cap (Compumedics/Neuroscan, USA) with the sampling rate of 250Hz. 19 sintered Ag/AgCl electrodes positioned according to the 10/20 International System with binaural reference. For each individual, intra-hemispheric coherence was measured across electrode pairs F3–C3, F3–P3, F3–T5, C3–P3, C3–T5, P3–T5 on the left hemisphere, and F4–C4, F4–P4, F4–T6, C4–P4, C4–T6, P4–T6 on the right hemisphere. Inter-hemispheric coherence was measured across electrode pairs F3–F4, C3–C4, P3–P4, and T7–T8. Raw EEG signal was filtered through a band-pass filter (0.15–30Hz) before artifact elimination and EEG segments with obvious eye, head movements and muscle artifacts were manually removed. The data analysis of the EEG was accomplished with the Neuroguide Deluxe 2.5.1 software (Applied Neuroscience, St. Petersburg, FL).Classical EEG spectral analysis was implemented using magnitude-squared coherence, as a function, is based on Fourier transform, of the frequency f, value. Coherence is defined as the normalized power spectrum per frequency of two signals recorded simultaneously at different sites on the scalp. The magnitude-squared coherence Cxy(f) is calculated for every pair of electrodes as the square of the modulus of the mean cross power spectral density (PSD) normalized to the product of the mean auto PSD. The coherence value for an electrode pair wave-forms x and y is calculated as(1)Cxy(f)=|Pxy(f)|2|Pxx(f)||Pyy(f)|wherePxy(f)is the cross PSD estimate ofxandy,Pxx(f)andPyy(f)are the PSD estimates ofxandy, respectively. The power spectrum (periodogram) and cross-power spectrum are defined as(2)Pxx(f):=|x¨(f)|2=x¨(f)x¨(f)¯,(3)Pxy(f):=ÿ(f)x¨(f)¯where x¨ is complex conjugate of x and(4)x¨(f):=∫−∞∞x(t)e−iwtdtis the Fourier transform that generates the information about frequencies occurring in signals and the dominant frequency for those signals.During the calculation process, raw EEG signal was divided into periods of 650ms with a 50% overlap and each period was windowed with a Hanning window. The Matlab program was then employed for coherence analysis by using 10–14 artifact free epochs for each subject. The scan numbers were set same randomly between the target and non-target stimulation conditions. Coherence values were calculated for the target and non-target stimuli for long-range intra-hemispheric and inter-hemispheric pairs for delta, theta and alpha frequency bands. Finally, distribution of coherence values was normalized using Fisher׳s Z transformation [68].FS process is a commonly used technique for decreasing dimensionality of the data and increasing efficiency of learning algorithm. For FS process the whole search space covers all possible subsets of features and the number of subsets is calculated as given in Eq. 5:(5)∑s=0n(ns)=(n0)+(n1)+…(nn)=2nwhere n represents the number of features and s is the size of the current feature subset [71]. FS methods usually require heuristic or random search strategies causing high complexity, so the degree of optimality of the final subset is generally reduced [15]. FS methods could be grouped into three major classes based on their evaluation procedure [72]. If an algorithm performs FS independent of a learning algorithm, then it is called as filter approach and mostly includes selecting feature subsets based on the inter-class separability principle. Due to its computational efficiency, the filter approach is popular while using high-dimension data. If the evaluation step is processed with a classification algorithm, the FS algorithm is called wrapper approach. In the wrapper approach, selected features are fed into a preset learning model to measure performance of the subset. Compared to filters, in wrappers, the predictive performance of the final selected subset is correlated with the chosen relevance measure while dealing with large dataset may increase the complexity due to the use of learning algorithms in the evaluation of feature subsets [73]. Finally, in embedded approach the FS and learning algorithm are interleaved similar to wrapper methods and the link between the FS and the classifier is stronger, nevertheless the wrapper methods has a better coverage of the search space [6]. Wrappers are constituted by three components; learning machine, feature evaluation criteria; and a FS method as shown inFig. 1.Because a large number of features may cause high complexity meta-heuristic search methods seem to be prominent due to their flexibility in random searching to contribute to FS process. Recent studies underline nature inspired methods such as: particle swarm optimization (PSO), genetic algorithm (GA)-based attribute reduction and gravitational search algorithm (GSA). Besides these methods, attempt to achieve better solutions by application of knowledge from previous iterations, ACO is another auspicious approach to solve the combinational optimization problems and has been widely employed in FS [29].ACO is a stochastic algorithm that mimics real ant colonies to construct a solution by a sequence of probabilistic decisions. The probability matrix is initialized randomly to enable variety for each ant and is expanded by adding a solution component, pheromone, after each probabilistic decision step. The sequence of decisions taken by each ant during the searching forms a pheromone trail and the density of the pheromone of the path is [74,75]. At the end of each iteration, ants deposit pheromones on the path they have visited and depending on the solution performance, pheromone density varies. The pheromone density on the trail evaporates with time so that the new comer ants can find alternative paths. Besides, at the end of each tour the pheromone density on the path generated by the best ant and worst and is also updated to follow award and penalty strategy in order to strengthen the search guide of the region near the optimal solution. The iterative process continues till the stopping criterion is reached. The stopping criterion may be either a number of iterations or a solution of desired quality [15]. ACO algorithms make probabilistic decision in terms of the artificial pheromone trails and the local heuristic information expressing desirability of the next node. These two factors are combined to form the so-called probabilistic transition rule and are expressed as given in Eq. 6:(6)puk={τ⋅uσηuυ∑cu∈N(sp)τ⋅uσηuυifcu∈N(sp)0otherwisewhereτuis a pheromone level of the edge from any feature to feature(u), indicating how informative the feature is andη(u)is a heuristic desirability of choosing feature(u)reflecting the desirability of choosing the next edge. For an ant k,N(sp), represents the set of all possible features to be connected from the current feature andCurepresents the selected feature for each ant. With the use ofpukprobability function, the probabilities of all possible next weighted features are calculated where high probability means more attractive alternative for the ant.σandυare constants to trade off the relative importance of the pheromone,τuand the heuristic information,ηu. Ifυ=0the search process will only utilize the pheromone value it will cause to the rapid emergence of stagnation situation and the ants will not be able to find any good solution at all [76]. At the end of each iteration, the pheromone values, associated with the edge of joining features are updated using Eq. 7. That step of the algorithm is called local pheromone update process.(7)τu←(1−ρ)⋅τu+∑k=1mΔτukwhere ρ pheromone trail decay coefficient, m is the number of ants andΔτukis the laid pheromone on the edge of feature(u)by ant k, where(8)Δτuk={δ⁎Jkifantusedfeature(u)initstour,0otherwisewhereδis a constant andJkis the fitness value of selected feature set by ant k[77]. By using this rule, the pheromone level of the features with highest fitness value will increase frequently, which will make that set more inclined to be selected during subsequent iterations by the ants. At the end of each tour, following the local pheromone update process, global pheromone update process starts. Pheromones of the paths belonging to the best and worst of the tour are updated as given in the following Eqs. 9 and 10 respectively:(9)τubest=τubest+θ⁎Jbest(10)τuworst=τubest−0.1⁎θ⁎Jworstwhereτubestandτuworstare the pheromones of the paths followed by the ant in the tour with the lowest(Jworst) and highest fitness values(Jbest) in one iteration respectively. With the contribution of pheromone evaporation (λ) given in equation, the pheromone density of the visited paths is reduced to avoid from the features with higher pheromone to be chosen so that the ants could explore the features which have never been chosen:(11)τu=τuλ+[τubest+τuworst]In ACO based FS, the ants search the feature space to construct each candidate subset using the probability transition rule. Since there is not the certain mechanism to predetermine invalid subsets, it is possible for ACO to consider any combination of the features as a candidate so that lots of invalid candidates could be produced, especially in the initial phase. These invalid candidates feature subsets could then impede the algorithm to converge the optimal solution and therefore reduce the performance of the algorithm. One of the other severe weaknesses of conventional ACO algorithm is stagnation. Stagnation is experienced if the ants generate the same solution without any improvement therefore causing the algorithm to be trapped in a local optimal solution [78]. In order to address the problem of conventional ACO algorithms, an improvement technique is added in order to progress the ACO method so that diversity of ants׳ solution set is assured. Recent studies also focus on the noteworthy results of improved optimization algorithms versions [79–81]. In classical ACO, the parametersσandυin Eq. 6 are static and all ants use the same values in the running of FS process. Thus the relative importance of each is constant during the FS process for each ant disregarding the phase of ACO. At the beginning of the FS process the ants have little information about alternative paths, therefore the distance has stronger impact on routing compared to pheromone. On the other hand after the algorithm runs for a long time the impact of pheromone is to be reinforced, because more information is stored in pheromone about better paths. A dynamic process was therefore replaced with standard ACO to converge to the solution fast and robust instead of using static routing bias parameters. Because the pheromone calculation process given in Eqs. 7–10 is employed to evaluate the performance of current ant and the pheromone update process, as given in Eq. 11, is a static process used to update pheromone table after each visit, they are all independent of bias parameters. In the experiments, we used an experimental rule of adjusting parameters, under which the algorithm finds the best solution faster in average [82]. Dorigo [83] recommended thatσ=1andυ=5are reasonable for many cases. So, we set our parameters as 1 and 5 respectively at the very beginning of FS process. The values are updated at the end of each tour according to the accuracy values of overall ants in the tour. If a shorter path is detected at the end of the tour theσparameter is increased by 0.25 and the value ofυis decreased by 0.25 in order to increase relative importance of pheromone to the distance. The process is repeated till the values are 5 and 1 respectively. With IACO, the effectiveness of the improved ACO was underlined in terms of time complexity. The design of the designed IACO algorithm is as follows.Initialize ACOrepeatset heuristic parametersσandυfor ant k ϵ{1,2,…,m}choose a feature set using probabilistic rulecalculate fitness valuelocal pheromone updateend-forevaluate solutions generated by all ants in the touridentify the best and the worst of the tourglobal pheromone updatedecrease pheromone density by evaporation rateif a shorter path is detected at the end of the tourupdate heuristic parametersσandυend-ifuntil stopping criterion is reachedSelect the features with highest pheromone valuesA fitness function is used to put forth the degree of goodness of selected subset. For a classification problem, if two subsets with different number of features present quite similar performance, the subset with less number of features comes into prominence. Therefore, the evaluation of fitness function regards two concerns: the classification accuracy and the number of features in the subset. In order to satisfy these concerns, the fitness function is designed in terms of both accuracy and number of features as(12)f(xj)=m⁎J(Xj)+n⁎(1/|Xj|)whereXjis the subset constituted by jth ant,J(Xj)is the classification accuracy usingXj, |Xj| is the number of features ofXj,m∈ [0,1] andn∈ [0,1] are the two coefficients used assign relative importance to classification accuracy and number of the selected subset parameters [84]. In our study, because the classification accuracy is relatively important compared to the number of features we set m as 0.92 and n as 0.78. Substituting the coefficients in the equation, the fitness values are calculated as given in Table 3.SVM is a widely used method for classification and regression problems mapping the training samples from the input space into a higher dimensional feature space. Because SVMs are based on linear or nonlinear RBF kernels, they are preferred to improve correlation of data with nonlinear nature. SVMs map raw data into a high-dimensional feature space using a nonlinear mapping function(φ)first, and construct the optimal separating hyperplane just on the base of support vectors to do linear regression in this space. The basic classification principle could be depicted as shown inFig. 2.With a training set given asS={(xi,yi)|xi∈H,yi∈{1−+},i=1,2,…l},wherexiare the input vectors andyithe labels ofxi,the target function is calculated as(13){minφ(W)=12W.W+C∑i=1lδisubjecttoyi(W.φ(xi))+b≥1−δi,δi≥0i=1,2,…lwhereWrepresents the hyperplane normal vector,Cis a penalty coefficient, which controls the trade-off between maximization of the margin width and minimizing the number of misclassified samples in the training set is set as 10.δiis another hyper-parameter and controls the width of kernel is set as 0.2. Finally, optimal hyperplane is transformed into the following quadratic equation:(14){maxL(α)=∑i=1lαi−12∑ijαiαiyjyjK(xixj)subjectto∑i=1lαiyi=0,0≤αi≤C,i=1,2,…l.And the output function could be expressed as(15)f(x)=sign[∑i=1lyiαiK(xi.x)+b]Depending on the data, various kernel functions could be used in decision function. Linear, kernel and radial basis function (RBF) are mostly used kernel functions and the functions are given in Eq. 16 respectively [86]:(16)K(x,xi)=〈x.xi〉,K(x,xi)=(〈x.xi〉+c)dK(x,xi)=exp(−〈x−xi2〉/2σ2)In this study 3 types of kernels were employed and RBF kernel was selected with its comparatively better performance as given inTable 1.This study adopts IACO approach to present a novel IACO–SVM model for parameter optimization problem of SVM. In order to classify validation samples or unknown data correctly a training setS={(xi,yi)|xi∈H,yi∈{1−+},i=1,2,…l},and a decision functionfis employed to map the input vectorsxonto the outputsy∈{−1,1}. In the proposed model, the inputs are processed as the selected feature subset by IACO while the outputs are considered as psychiatric disorder type. To do this, in this context, all features are fed into the FS step from alpha, delta and theta coherence values and more informative features are selected by implementing meta-heuristic process of artificial ants. The performance of selected feature subset is then evaluated by the fitness function over the SVM classifier. According to the fitness value, the pheromone update process is initiated and heuristic parameters are also modified subsequently. The modeling process loops till the stopping criterion is satisfied. At the end of each modeling process, the performance of the classifier is tested using an external test data that is new to the model. Through the nested-CV process the model with the highest performance is selected. The overall process of the proposed method is illustrated inFig. 3.As shown in Fig. 3, nested-CV consists of two nested, outer and inner, cross validations Firstly; the dataset is split into six parts, 6-fold outer-CV, using stratified sampling. While one fold is reserved for outer-CV test, remaining five folds are held out for training process in inner-CV cycle. In inner-CV cycle, 5-fold CV process is applied and 5 models are generated finally. The models with the selected feature subsets are then sorted according to their classification accuracies. In order to eliminate the leakage from test and training data the best of those five models is tested with reserved outer-CV test fold that is completely non-familiar to the models. Following the inner CV loop, reserved outer CV test fold is swapped with one fold of five training folds so that the models generated by inner-CV process are tested by completely new test data for each outer CV cycle. In inner CV, features are selected with IACO using inner-CV training data. With IACO method, four folds are used for training and one for validation subsequently. As we splitted the data into six parts in outer-CV, the aforementioned steps are repeated five more times to determine the local best model for each loop. Finally, to discover the global best of six locally best models with their optimized feature subset, the array is sorted according to the classification accuracy.In this study, a hybrid approach, combining IACO and SVM methods, was employed in order to generate a model using fewer but more informative features. The computational complexity of the proposed model in terms of time can be computed considering the parameters given in Eq. (17):(17)Niterations×(Nants×(Tfeatureselection+TSVMtraining)+Tpheromneupdating)where NIterationsis the number of iterations and NAntsis the number of ants in each iteration. Tfeature selectionis the runtime for an ant to generate a feature subset, TSVM trainingis the runtime for an ant to train SVM classifier using selected features which scales cubically (O(n3)) with the size of the training set (n), and finally TPheromoneUpdatingis the runtime to update pheromone table after a model is generated [87]. In order to reduce the complexity, the high dimensional data should be transformed into low dimensional data which includes extracting the essential information from the data. Transforming the data to a more condensed form not only improves the classification accuracy but also reduces the computational complexity. In this study dimensionality reduction was realized as FS using IACO which reduced the number of variables and reduced SVM training runtime while improving the overall performance of the system. Besides, application of an IACO version reduced the number of iterations(Niterations)from 48 to 33 while increasing the fitness value as shown inFig. 4. The tests were performed on a Windows 7 Professional operating system installed computer with Intel(R) Core (TM) i5-3470 CPU @3.20GHz processor, 4.00GB of physical memory and 500GB Seagate disk drive hardware configuration.As it is seen in Fig. 4, fitness value calculation process, given in Eq. 12, is repeated for all iterations. As the number of iteration increases the performance of the algorithms improves. In order to investigate the convergence performance of both ACO and IACO approach, algorithms were executed 10 times. The mean value of the minimum fitness functions of those 10 sessions versus the number of iteration is plotted in Fig. 4. It is possible to deduce that the proposed approach, IACO, converges faster and accurate than the standard ACO algorithm. Besides, 20 trials were performed to compare the average running time of standard ACO, PSO, GA and IACO, and the results are given inTable 2.Space complexity of an algorithm is expressed in terms of memory space consumed throughout the overall process. The space occupied by the algorithm is calculated using fixed and variable amount of memory. Fixed amount is occupied by the variables used in the program while variable amount is occupied by the component whose size is dependent on the iterations and recursive procedures. In order to determine the overall amount of memory used by the algorithm two space parameters are taken into consideration. With data space, the amount is expressed with the variables, data structures, allocated memory and other data components. If the scale of our problem is p, and the number of ants is q, then the space complexity is calculated as given in Eq. (18):(18)S(p)=S(p)=O(4p2)+O(2pq).For the IACO, the scale of the problem is the same therefore the space complexity is same with simple ACO.

@&#CONCLUSIONS@&#
