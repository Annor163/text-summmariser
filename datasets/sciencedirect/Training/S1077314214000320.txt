@&#MAIN-TITLE@&#
GLocal tells you more: Coupling GLocal structural for feature selection with sparsity for image and video classification

@&#HIGHLIGHTS@&#
GLSS utilizesℓ2,p-norm that is capable of selecting discriminative features by adjusting value of p.GLSS is built on global and local data structures that help boosting the efficacy of feature selection.We conduct extensive experiments on different datasets to evaluate our proposed method.

@&#KEYPHRASES@&#
Global and local,Feature selection,ℓ,2,,,p,-norm,Image and video classification,

@&#ABSTRACT@&#
The selection of discriminative features is an important and effective technique for many computer vision and multimedia tasks. Using irrelevant features in classification or clustering tasks could deteriorate the performance. Thus, designing efficient feature selection algorithms to remove the irrelevant features is a possible way to improve the classification or clustering performance. With the successful usage of sparse models in image and video classification and understanding, imposing structural sparsity in feature selection has been widely investigated during the past years. Motivated by the merit of sparse models, in this paper we propose a novel feature selection method using a sparse model. Different from the state of the art, our method is built uponℓ2,p-norm and simultaneously considers both the global and local (GLocal) structures of data distribution. Our method is more flexible in selecting the discriminating features as it is able to control the degree of sparseness. Moreover, considering both global and local structures of data distribution makes our feature selection process more effective. An efficient algorithm is proposed to solve theℓ2,p-norm joint sparsity optimization problem in this paper. Experimental results performed on real-world image and video datasets show the effectiveness of our feature selection method compared to several state-of-the-art methods.

@&#INTRODUCTION@&#
Many applications in computer vision and multimedia, such as image and video annotation, require images and videos be represented by low-level features. If some of these features are irrelevant or redundant, this could be deleterious for the performance of classification or clustering tasks. The main idea of feature selection is to choose a subset of input variables by eliminating the features with little or no predictive information. By removing such features from the original feature representation, feature selection could speed up the learning process, enhance model generalization capability and alleviate the effect of curse of dimensionality.In the past several years, there have been many feature selection methods proposed in the computer vision, pattern recognition and multimedia communities. Typically, feature selection algorithms fall into two categories: feature ranking and subset selection. Feature ranking ranks the features by a metric and eliminates the features that do not achieve an adequate score. Subset selection searches the set of possible features for the optimal subset. Feature selection can significantly improve the comprehensibility of the resulting classifier models and often build a model that generalizes better to unseen points. Further, it is often the case that finding the correct subset of predictive features is an important problem in its own right. For example, a physician may make a decision based on the selected features whether a dangerous surgery is necessary for treatment or not.In computer vision and multimedia areas, feature selection based on subset selection has drawn more attention. The classical Fisher Score [1] feature selection method evaluates the relevance of features according to the label distribution of the data points. Minimum-Redundancy–Maximum-Relevance [2] feature selection method selects useful features which have the strongest correlation with a classification variable based on mutual information. Several previous works have shown the effectiveness of these methods. However, these traditional algorithms usually evaluate features one by one, which is not computationally efficient and ignores the correlation between different features.Recently, sparse models have been successfully used in the multimedia and computer vision tasks such as image classification [3–5], headpose estimation [6], face recognition [7,8] and action recognition [9]. The sparse model has also been widely investigated in feature selection. The intuition for this type of approaches is that many real-world data are often sparse which means feature selection can be achieved by searching the sparse representation of the data. In contrast with the traditional feature selection approach by using the sparse model, we are able to select features jointly in a batch mode and meanwhile to leverage the correlation between different features. Nie et al. [10] leverage jointℓ2,1-norm minimization on both loss function and regularization for feature selection. Yang et al. [11] have proposed anℓ2,1-norm regularized feature selection method for unsupervised learning considering the manifold structure of data representation. Yang et al. [12] proposed a semi-supervised algorithm called ranking with Local Regression and Global Alignment (LRGA) to learn a robust Laplacian matrix for data ranking. Gao et al. [13] proposed a method which simultaneously utilized both visual and textual information to estimate the relevance of user tagged images.The sparse model for feature selection has proved to be effective but most existing methods have two limitations. On one hand, the widely usedℓ1-norm andℓ2,1-norm are not flexible enough to control the sparseness of feature representation. Hence, useful features could be neglected or noisy features could be selected. In other words, they may not find out the optimal subset of the original features. On the other hand, many of them only consider global information of the data representation but ignore the local structure of data distribution which also has significantly useful information for selecting more discriminating features. Locality Preserving Projections (LPPs) [14] searches for an embedding space in which the similarity among the local neighborhoods is preserved. However, LPP has two disadvantages: Firstly, LPP does not take the label information into consideration which is crucial for classification tasks; Secondly, like most graph-based methods, graph construction of LPP is sensitive to noise and outliers. To address these issues, in this paper we propose a novel and robust feature selection method by employing aℓ2,p-norm based sparse model and meanwhile considering both global and local data structures. We name our method GLocal Structural feature selection with Sparsity (GLSS). Instead of using the traditionalℓ1-norm orℓ2,1-norm, we propose to exploit theℓ2,p-norm based sparse model. Since we can adjust the value of p in our framework, our algorithm is more flexible to control the sparseness of the feature representation, thus resulting in a better subset of the original feature set. To use the information of both global and local data structure, we build two regression models in a joint framework: one for all the data points and one for the local neighboring data points. Fig. 1illustrates the overview of our feature selection method. All training and test samples are represented by low-level feature vectors. Our GLSS method considers global and local information with sparsity. Then the selected features are fed into a classifier to do image or video classification.The main contributions of our work are as follows:•Our method GLSS utilizes theℓ2,p-norm based sparse model for feature selection. This model is more capable of selecting discriminative features by adjusting the value of p.GLSS is built upon both global and local data structures. Exploring the GLocal information helps boosting the efficacy of feature selection.This paper is the extension of our conference paper [15]. The extension includes both the theoretical principle and the applications. We would like to highlight them as follow:•We add more details about the intuitive of our objective function and the formulation derivative.We used our method for two more applications, which are video concept detection and image annotation. Our method shows promising performance and is especially competitive when few labeled samples are available, which makes it attractive for large scale multimedia data understanding.We conducted more complementary analyzing experiments on the 6 datasets to assess the overall performance of our method for different applications. These include studies which aim to understand the influence of the sparse level, the influence of the unlabeled data, the influence of the local set and parameter sensitivity studies which demonstrates how the parameters affect the performance.We compared our method with anotherL2,1-norm feature selection method. The experiment results show great improvement overL2,1-norm feature selection method which prove the advantage of ourL2,p-norm sparsity level adjusted method.For practical applications it is interesting how fast our algorithm converges. Therefore, we also conducted a newly added experiment which studies the convergence of our method.The rest of paper is organized as follows. In Section 2, we illustrate the formulation of our framework and propose an algorithm for solving the objective function. Experiments are given in Section 3 and Section 4 draws the conclusion of this paper.The low-level features of images or videos incorporate different information, either globally or locally. Intuitively, effective analysis on both levels would boost the feature selection efficacy. In this section, we propose our GLocal Structural feature selection with Sparsity (GLSS) algorithm and derive an efficient solver for the problem.We first explore the local data structure to help the selection of discriminating features from the original representation. Inspired by previous works [16,17], we build a local setNifor each datumxi.Ni={xi,xi1,…,xik-1}and it consists ofxiand itsk-1nearest neighbors. For each local set, a local prediction functionfiis defined to correlate the data within the set with their predicted labels and we can obtainfithrough the following objective function:(1)∑xk∈Niℓfi(xk),qk+αΩ(fi)whereℓ(·)is the loss function andΩ(·)is the regularizer.xk∈Niandqkis the predicted label forxk.αis a regularization parameter.Globally, we also define a prediction function f to correlate all the n data points with their predicted labels as follows:(2)∑i=1nℓf(xi),qi+γΩ(f)whereqiis the predicted label forxiandγis a regularization parameter.Suppose the feature dimension is d and there are c classes. We apply linear regression model and obtainfi(x)=WiTx+biandf(x)=WTx+bwhereWi∈Rd×candW∈Rd×care two projection matrices. We aim to leverage both the global and local information [15]. Hence, we propose the following joint framework:(3)minW,Wi,qi,qj,b,bi∑i=1n∑xj∈NiWiTxj+bi-qjF2+αWiF2+β∑i=1nWTxi+b-qiF2+αΩ(W)whereβis a parameter and·Fis the Frobenius norm of matrix. As W is used for feature selection, a sophisticated regularizer is needed to make W able to reflect the importance of different features. Previous work has shown that sparse models are useful for feature selection by eliminating redundancy and noise [18–20]. The sparse models are used to make some of the feature coefficients shrink to zeros. As a result, W can be regarded as the combination coefficients for the most discriminative features to achieve feature selection.Specifically, we propose to minimizeW2,p=∑i=1d(∑j=1cWij)p21pto achieve that goal.·2,pdenotes theℓ2,p-norm (0<p<2). p is used to control the degree of sparseness. The lower p is, the more sparse W is or in other words, more rows of W would shrink to zeros. A merit of usingℓ2,p-norm, compared to usingℓ2,1-norm orℓ1-norm, is that we can adjust the value of p to search for the optimal subset from the original features. Consequently, Eq. (3) can be rewritten as:(4)minW,Wi,qi,qj,b,bi∑i=1n∑xj∈NiWiTxj+bi-qjF2+αWiF2+β∑i=1nWTxi+b-qiF2+αW2,ppLetXi=[xi,xi1,…,xik-1]∈Rd×k,Qi=[qi,qi1,…,qik-1]T∈Rk×c, Eq. (4) can be rewritten as:(5)minW,Wi,Q,Qi,b,bi∑i=1nXiTWi+1kbiT-QiF2+αWiF2+βXTW+1nbT-QF2+αW2,ppwhere1k∈Rkand1n∈Rnare two vectors with all ones. Next, we build up the connection between the predicted labelsQ∈Rn×cand the ground truth labelsY∈Rn×c. Q is supposed to be consistent with Y and we propose to minimizeTr(Q-Y)TU(Q-Y)inspired by [21–24]. U is a diagonal matrix whose diagonal elementUii=∞ifxiis labeled andUii=1otherwise. To this end, we propose the following objective function for feature selection based on both local and global data structure:(6)minW,Wi,Q,Qi,b,bi∑i=1nXiTWi+1kbiT-QiF2+αWiF2+βXTW+1nbT-QF2+αW2,pp+Tr(Q-Y)TU(Q-Y)After W is learned, we can see that many rows of the optimal W shrink to zeros (close to zeros). We rank each feature according to‖W‖Fin descending order and select the top ranked features.In this subsection, we present our solution for Eq. (6). Since it involves theℓ2,p-norm which is non-smooth and cannot be solved in a closed form, we adopt the alternating minimization algorithm to optimize the objective function with respect tobi,Wi,b,Qi,Qand W respectively in five steps as follows:Step 1: FixWi,b,Qi,Q,W, Optimizebi.By setting the derivative of Eq. (6)w.r.t.bito zero, we have:bi=1k(QiT1k-WiTXi1k)Step 2: Fixbi,b,Qi,Q,W, OptimizeWi.By setting the derivative of Eq. (6)w.r.t.Wito zero, we have:Wi=(XiHkXiT+αI)-1XiHkQiwhereI∈Rd×dis an identity matrix andHk=I-1k1k1kTis a locally centering matrix.Step 3: Fixbi,Wi,Qi,Q,W, Optimizeb.By setting derivative of Eq. (6)w.r.t. b to zero, we have:b=1n(QT1n-WTX1n)Step 4: Fixbi,Wi,b,W, OptimizeQiand Q.DenoteGi=Hk-HkXiT(XiHkXiT+αI)-1XiHkand define a selection matrixS∈{0,1}n×k, whereSij=1ifxiis thejthelement inNiandSij=0, otherwise. Consequently,Qi=STQand we can obtain:∑i=1nTr(QiTGiQi)=TrQT∑i=1nSGiSTQ=Tr(QTLlQ)whereLl=∑i=1nSGiSTreflects the exploration of local data structure. In this way,Qiabsorbed by Q which means we only need to optimize Q. Then Eq. (6) becomes:minQTr(QTLlQ)+βXTW+1nbT-QF2+Tr(Q-Y)TU(Q-Y)By setting the derivative w.r.t. Q to zero, we have:Q=(Ll+βH+U)-1(UY+βHXTW).whereH=I-1n1n1nTis the global centering matrix.Step 5: Fixbi,Wi,Qi,Q,b, OptimizeW.DenotingW=[w1,…,wd]T, we define a diagonal matrix D with its diagonal elementsDii=12pwi22-p. The objective is then equivalent to:minWXTW+1nbT-QF2+αTr(WTDW)By setting the derivative w.r.t. W to zero, we obtain:W=(A+αβD)-1BwhereA=XHβI-β2(Ll+U+βH)-1HXTandB=βXH(Ll+U+βH)-1UY.According to the optimization, we propose Algorithm 1 to solve the objective function of Eq. (6). The detailed convergence analysis of the algorithm is provided in Appendix A.Algorithm 1The Algorithm for GLocal Structural Feature Selection with Sparsity (GLSS).

@&#CONCLUSIONS@&#
In this paper, we have proposed a novel feature selection method for different multimedia applications, i.e., image and video annotation and 3D motion data analysis. Our method proposes two advances over the state of the art: (1) theℓ2,p-norm based sparse model; (2) the exploitation of both the global and local structures of data distribution. By using theℓ2,p-norm based sparse model, our method is able to jointly select features across all data points and has more flexibility in choosing the discriminating subset from the original features. Meanwhile, by considering both the global and local structures of data distribution, the feature selection can be boosted as the two structures are both critical in classification tasks. Experimental results performed on real-world image and video datasets show the efficacy of our feature selection method compared to several state-of-the-art feature selection methods.