@&#MAIN-TITLE@&#
Using IDS fitted Q to develop a real-time adaptive controller for dynamic resource provisioning in Cloud's virtualized environment

@&#HIGHLIGHTS@&#
In this paper, a fast fuzzy solution is proposed to enable application of reinforcement learning in continuous domains of state-action pairs.In this paper, the proposed adaptive control mechanism is used to provide a goal driven solution to dynamic resource provisioning in Cloud's virtualized environment.IDSFQ is presented to provide adaptive control scenario for dynamic resource provisioning inside Cloud's spot market.

@&#KEYPHRASES@&#
Q-learning,Active learning method (ALM),IDS fitted Q (IDSFQ),Real-time reinforcement learning in continuous space,

@&#ABSTRACT@&#
Reinforcement learning (RL) is a powerful solution to adaptive control when no explicit model exists for the system being controlled. To handle uncertainty along with the lack of explicit model for the Cloud's resource management systems, this paper utilizes continuous RL in order to provide an intelligent control scheme for dynamic resource provisioning in the spot market of the Cloud's computational resources. On the other hand, the spot market of computational resources inside Cloud is a real-time environment in which, from the RL point of view, the control task of dynamic resource provisioning requires defining continuous domains for (state, action) pairs. Commonly, function approximation is used in RL controllers to overcome continuous requirements of (state, action) pair remembrance and to provide estimates for unseen statuses. However, due to the computational complexities of approximation techniques like neural networks, RL is almost impractical for real-time applications. Thus, in this paper, Ink Drop Spread (IDS) modeling method, which is a solution to system modeling without dealing with heavy computational complexities, is used as the basis to develop an adaptive controller for dynamic resource provisioning in Cloud's virtualized environment. The performance of the proposed control mechanism is evaluated through measurement of job rejection rate and capacity waste. The results show that at the end of the training episodes, in 90 days, the controller learns to reduce job rejection rate down to 0% while capacity waste is optimized down to 11.9%.

@&#INTRODUCTION@&#
In current distributed computing world, Cloud's virtualized environment is known as the enabling technology of “on-demand”, “pay as you go” and “resource pool” paradigms in the context of computational resources provisioning. Recently, adaptive resource provisioning mechanisms, which are assisted by intelligent systems, have gained wide popularity in the context of Cloud computing. However, in current Cloud's spot market, which is one of the most challenging environments to deal with, facing with the dynamic scale of requests for computational resources requires designing dynamic resource management scenarios [1–4]. Thus, Dynamic Resource Provisioning (DRP) is essential to assure efficient resource utilization in the Cloud's infrastructure systems. Elastic resource scaling, the ability of the Cloud to scale its resources to the volume of incoming demands, is an embeded concept for dealing with dynamicity in Cloud's virtualized environment. Elasticity allows for dynamic resource provisioning when facing with dynamic demands [5].Dynamic resource provisioning in the Cloud's spot market can be formulated as an adaptive control problem. Adaptive control approaches have been applied successfully to provide dynamic resource allocation solutions in distributed physical servers [6–9]. Because of high amount of uncertainty, along with the lack of explicit model for Cloud's resource management systems, classical adaptive control mechanisms are not applicable to establish high performance dynamic resource provisioning. Nonetheless, artificial intelligence (AI) methods provide facilities to introduce model free solutions through intelligent approaches [10–12]. Model-free techniques like RL provide the adaptive controller with autonomous data driven solutions which are organized gradually to such uncertain environments [13].Recently, RL methods have gained wide popularity in adaptive control problems with successful applications [14–19]. In engineering applications, RL is known as a bio-inspired machine learning technique used to solve sequential decision problems [20–22]. In RL, the major part of the efforts is concentrated on learning by interacting with the environment. An agent learns through evaluative feedback, as the environment pays for the action that the agent performs at each state [23]. Through an RL process, the agent learns the optimal policy Q for receiving better rewards (as the feedbacks of the environment). Among the RL methods, Q-Learning [24] has proved its capabilities when employed in real-time applications [25–27]; Q-learning is commonly used for solving finite Markov Decision Process (MDP) problems, one of the most promising solutions in engineering applications [20–22,24].RL problems are commonly solved by dynamic programing (DP) techniques. However, in continuous domains, due to the curse of dimensionality [28] together with the continuity of the domain, the state space of the RL problem cannot be modeled by conventional DP or finite state space formulation techniques. Therefore, a solution is required to generalize the experiences of the RL algorithm to unseen conditions. In such situations, general approximation techniques, like artificial neural networks (ANNs), are used to approximate the reward function of the RL problem [29].One of the most promising approximation-based approaches is the neural fitted Q (NFQ) technique [30]. Fitted Q provides ability to remember the experiences of interactions with the environment as beneficial knowledge revealed though exploration, provided that the agent can reflect on past experiences in the similar conditions to improve long term (cumulative) reward [23,31,32]. Therefore, convergence to the optimal policy of the RL algorithm can be guaranteed.However, when the decision time is important and the number of experiences increases, approximation techniques, which are employed as fitted Q techniques, commonly fail to provide acceptable response time. In essence, popular methods used for approximation in fitted Q approaches like ANNs, Takagi–Sugeno–Kang Fuzzy Inference System (TSK-FIS), Radial Basis Function Networks (RBFNs) and Neuro-Fyzzy (NFs) suffer from considerable computational complexities which impose a meaningful delay to the RL process. On the other hand, incremental methods like recurrent neural networks (RNNs) face with the possibility of divergence.In this paper, a novel use of IDS modeling method [33], which is a solution to modeling multi-dimensional systems without dealing with computational complexities, is proposed to model the Q-values in real-time constrains. IDS has shown its real-time capabilities for modeling multiple-inputs, single-output (MISO) approximation problems [34]. Thus, IDS properly suits the Q-learning problem that is an MISO problem (there is only Q-value as the output of approximation procedure). The IDS model is trained via a recursive partitioning algorithm, called the active learning method (ALM) [35].This paper proposes a novel fitted Q method to accelerate the modeling procedure inside Q-learning for continuous RL problems. Since Q-learning is one of the most practical solutions to adaptive control in real-time applications, this paper studies employment of Q-learning to provide an adaptive control solution to DRP problem in Cloud's virtualized environment. The proposed method is supposed to learn how to avoid job rejection while gradually leads the management system to optimal resource usage (which is tightly geared with energy consumption in Cloud's environment).The rest of the paper is organized as follows: the next section describes the Q-learning method as the engine of the adaptive controller proposed in this paper. Section 3 is an overview on the DRP problem. In Section 4, the proposed method is explored to design an adaptive controller for the DRP problem in Cloud's virtualized environments. Experimental results are presented in Section 5. And finally, Section 6 concludes the paper.Due to the fact that an explicit model of the environment is required for obtaining optimal policies in DP solutions, classical DP methods commonly face with difficulties for providing explicit model in addition to corresponding computational costs [20]. Nonetheless, in continuous domains, RL methods are addressed as approximate DP (ADP), regarding less computational costs and no assumption on having explicit models of the environment [36]. ADP is a perspective of RL which introduces approximation solutions to deal with the curse of dimensionality, continuous domain requirements and uncertainty in the environment of the problem.Among model-free ADP methods, Q-learning is guaranteed to converge [37]. Q-learning is adaptive, can be real-time and works based on a single value reflected on pairs (state, action), denoted as cumulative reward, so as to derive the optimal policy. Q-learning starts from random initial Q-values and updates values online via the observed transitions (st, at, st+1, rt+1) [24,38] (where stis the current state, atis the chosen action, st+1 is the state after performing atat st, and rt+1 is the reward reflected on performing atat st); after each transaction, the Q-value of the corresponding (state, action) pair is updated with:(1)Q(st,at)←Q(st,at)+α(rt+1+γmaxaQ(st,a)−Q(st,at)),where α is the learning rate, γ is the discount factor that determines the impact of future reward, rt+1 is the immediate reward reflected on (st, at, st+1) tuple, andmaxaQ(st,a)is the maximum cumulative reward which can be received through choosing an action in state st.For each experiment, when the agent performs an action atin state st, the Q-entry belonging to (st,at) is updated depending on the utility of the chosen action in the corresponding state. As mentioned earlier, Q-values represent the optimal policy for achieving the goal of the learning process [22,24].This paper offers that the problem of DRP in Cloud's virtualized environments can be solved via adaptive control on dynamic scaling of resources to maximize the revenue function. Optimal resource provisioning in the Cloud's environment is a crucial task for Cloud infrastructure providers; they should provide hosts according to the Service Level Agreements (SLAs) at the proper level of Quality of Service (QoS) for client satisfaction. Cloud providers should also optimize the DRP task so as to reduce extra costs usually imposed by wastage in energy consumption in active servers. Therefore, regarding both client satisfaction and resource optimization is required to gain better revenue for Cloud providers.Regarding client satisfaction, such solutions should avoid job rejection, which is expected to persuade the clients to submit their requests to other Cloud providers. In this paper, two major reasons are considered for job rejection, including long response time and inadequate capacity. The controller should avoid job rejection by providing enough capacity for the given jobs as well as reducing the response time. Due to the fact that the response time is more crucial in spot markets, jobs are considered to be presented to the Cloud as spot requests which are required to be responded in real-time manner; otherwise, the job is rejected. To guarantee client satisfaction, the proposed adaptive controller should consider higher priority for job rejection accusation in comparison to resource usage optimization.The DRP problem is a proper suit for RL as an optimal control mechanism [39]. Optimal controllers are commonly addressed as adaptive controllers. In the literature, the most practical architecture for adaptive control is the Model Reference Adaptive Control (MRAC) mechanism [40]. Trying to formulate the DRP problem as an adaptive control problem, from the MRAC controllers’ point of view, no reference generator exists to optimize the control signal, accordingly. Hence, performance metrics such as job rejection and energy consumption would be appropriate alternatives for corresponding non-parameterized output error, as the control basis in the MRAC control architecture. The adaptation mechanism of an adaptive controller, using performance metrics as reference signals, is shown in Fig. 1.In Fig. 1, while Adjustable Controller tries to control the Plant in an optimal way, the Adaptation Mechanism tunes the Adjustable Controller's parameters incrementally in order to improve the performance of the controller. However, in comparison to the MRAC mechanism, no reference signal generator is employed in Fig. 1. Therefore, the performance of the controller is measured by some performance metrics in the Performance Measurement component. Finally, the measured metrics provide the Adaptation Mechanism with decision through comparison of the measured metrics with the desired ones using the Comparison Decision component.The problem of DRP in Cloud's virtualized environments can be considered as a sequential decision making problem which requires finding a proper policy for managing the resource allocation task in a dynamic way. When a continuous set of control signals is required for such problems, including how to find a proper host for the given virtual machine (VM) or how many active machines should be managed to host the incoming tasks, and on the other hand, the uncertain response of the system cannot be modeled in a real-time manner, the RL based control mechanism can benefit from interaction based solutions.The uncertainty existing inside the Cloud's environments (when the state is changed as the result of performing an action) can be overcome by Q-learning which is adapted to the optimal solution independent of the action selection mechanism. Though, Q-learning is shown to have the ability of convergence to the optimal policy in uncertain environments. Action independency in the convergence procedure of Q-learning requires making decisions based on information gained in exploratory efforts rather than proceeding based on the probability of transition tuple P(s, a, s′), a semi-greedy solution; the agent is recommended to choose the action that is expected to return the greatest discounted cumulative reward. However, due to working in continuous domains, modeling the probability of transition from state s to state s′ by taking action a is not practically affordable in our RL based adaptive controller.The goal of the controller proposed in this paper is to generalize the discounted cumulative reward to unseen (state, action) pairs. To provide the required generalization ability, the proposed method benefits from the advantages of the IDS method. The IDS method is proven to have the ability of modeling MISO systems resolving the issue of computational complexities [43]; in this way, the IDS method can be introduced as an appropriate solution for real-time applications. The Q-value reflected on a (state, action) pair through Q-learning is in fact a single value which introduces the approximation function of the continuous space as an MISO system. Thus, IDS method can be introduced as a proper solution to continuous Q-learning in real-time applications.The architecture of the proposed RL based adaptive controller is illustrated in Fig. 2.In the proposed architecture in Fig. 2, there are three major components: the Cloud (cluster of servers), the scheduler and the adaptive controller. The Cloud is our plant to be controlled through a wisely designed mechanism so as to obtain the highest performance in terms of the given performance metrics. The scheduler participates in taking some different characteristics of the actions (control signals) and performing actions on the Cloud. And, the adaptive controller is responsible for observing the status of the entire system and making control decisions.In Fig. 2, there are also some other boxes including Performance Measurement, Reward Calculation and State Generation. Since our control system does not have access to a proper supervisor, some specific performance metrics showing some arbitrary aspects of the system performance are measured in the Performance Measurement component. Then, based on the measured metrics, the status of the Cloud (before taking actions) and the current status of the system, an overall view of the system performance is computed in the Reward Calculation procedural component. In the State generation component, the state of the system is interpreted through Temporal monitor memory and Prediction to an understandable and useful knowledge for being used in our adaptive controller.The adaptive controller also contains three components: Action Recommender, IDSFQ (IDS fitted Q) and Action Selection. IDSFQ is responsible for observing the exploratory actions and their outcomes (given by reward) to extract experimental rules for the controller. These rules (which are adaptively tuned during the learning process) are then used to make better decisions based on the gathered knowledge (exploiting the past experiences and using them for making better decisions) rather than deciding according to some constant, pre-identified views of the entire system.The Action Recommender is responsible for exploiting the rules stored inside IDSFQ and introducing some actions which are expected to have the highest performance. A set of actions are introduced to enable the controller, by employing a non-observed action in an exploratory manner, to improve its performance through examining actions outside the experienced borders. Choosing between the introduced actions is made through considering a tradeoff between exploration and exploitation in the Action Selection component.In the following sections, first, Section 4.A describes IDSFQ as the approximation engine of the proposed controller. Then, in Section 4.2, formulation of the DRP problem in Cloud to an RL based adaptive controller is presented.Projection of the MISO system into some single-input, single-output (SISO) sub-systems to deduce the output model is the major part of complexity reduction task inside the IDS method (Fig. 3). Then, via an interpolation mechanism, the IDS method aggregates the output model of the projected systems to convolve the final model.The implementation of the IDS modeling method is explored in [34] which is the basis of the complexity reduction in the proposed controller. Since the IDS method is a solution to generalize the model based on the general behavior of the system output with respect to the system inputs, the IDS method actually performs the role of an inference engine. As mentioned before, a recursive partitioning method called ALM is introduced in [37] which uses the IDS method in partitioned inputs–output domains so as to produce localized sub-models of the given system. The localized sub-models are then stored as localized fuzzy rules in a regenerative neural network (NN) to construct the final model.Despite the regenerative NN is able to remind the localized rules, the procedure of training a regenerative NN is still suffers from complexities [34]. Therefore, in this paper a hierarchical fuzzy rule base is introduced to store the rules. Considering that the main operators of the IDS method are the IDS and center of gravity (COG) operators (which will be described later), the architecture of the ALM proposed in this paper is illustrated in Fig. 4.Based on Fig. 4, the learning process is performed by ALM which stores the approximation rules in the rule base. Then, the knowledge learned by ALM is accessible via the Estimation Interface. In ALM, rules are defined as Narrow Paths (NPs) which are representatives for IO behaviors in SISO systems. Each NP is a candidate model for the MISO system. NPs are then unified through interpolation to express the behavior of the MISO system. In the process of ALM, the modeling procedure is localized so as to extract local behaviors instead of general behaviors represented by primary NPs. These localized behaviors improve the generalization ability of the entire process. Inside ALM, localization is performed through recursive partitioning of data domains. These localized areas are stored in the hierarchical fuzzy rule base in terms of NPs and are accessible via the Estimation Interface. The process of NP extraction is described in the following subsection.NP is the main concept inside the IDS method that is considered as the general behavior of the output with respect to the input in an SISO system. The IDS method extracts the NP for each SISO sub-system via applying IDS and COG operators, consequently. The IDS operator is a fuzzification mechanism which is used as a thickening operator inside the IDS modeling process. The COG operator is a defuzzification mechanism that is used as a thinning operator in the IDS modeling process. The main idea behind extraction of NPs in SISO systems through IDS and COG operators lies behind the fact that since the given SISO system consists of some discrete points (Fig. 5), the general behavior of the output with respect to the input which is a function alike continuous NP, should be extracted via applying a fuzzy thickening operation (continuity propagation via fuzzification) and a thinning operation (defuzzification), consequently.The IDS operator can be implemented through a d% margined M×M board [34]. Since the domain of the ith SISO sub-system, i.e. Xi−Y, is defined as:(2)DXi:x|min(Xi)<x<max(Xi)DY:y|min(Y)<y<max(Y).The domain of d% margined domain is formulated as:(3)DX⌢i:x|min(Xi)−marginXid%<x<max(Xi)+marginXid%DY⌢:y|min(Y)−marginYd%<y<max(Y)+marginYd%,wheremarginZd%is calculated by:(4)marginZd%=max(Z)−min(Z)×d100.Therefore, the size of each unitUX⌢iM,UY¯Min the ith SISO sub-system for an M×M board is calculated by:(5)UX⌢iM=max(X⌢i)−min(X⌢i)MUY⌢M=max(Y⌢)−min(Y⌢)M.Now, the board required for implementing the IDS method is introduced. To apply IDS method, first the data points of the SISO system are projected to the corresponding coordinates on the 2D board of the IDS method (Fig. 6a). The IDS operator aims at converting a discrete spread of data points in a 2D plane to a continuous area so as to extract the general behavior of the output with respect to the input. Hence, the IDS operator considers the current data points as actual observations and propagates the membership values from their coordinates to their neighborhood in order to form a fuzzy continuous area. The propagation of fuzzy membership is attenuated by distance to devote more memberships to closer neighbors. Considering a linear implementation for attenuation function, the received membership value μ for a point, coordinated at the distance of (u,v) with respect to the actual sample (xs,ys) on the board, can be calculated by:(6)τ=R−Δd(xs+u,ys+v)+1;−R≤u,v≤R,μ=τ;ifτ>00;otherwise,where R is the radius of the IDS operator and:(7)Δd(xs+u,ys+v)=u2+v2.When the IDS operator is applied to the discrete data of the SISO sub-system, the result is a fuzzy continuous plane called the IDS plane. Then, the COG operator is applied to the IDS plane to extract the NP, denoted by ψ(x):(8)ψ(x)=∑j∈Y(x)μjYj∑j∈Y(x)Yj,Extraction of an NP through the IDS method is illustrated in Fig. 6.Now, having a single NP for each SISO sub-system, a simple fuzzy rule is presented as:IFxisX1THENyisψ1;IFxisX2THENyisψ2;…IFxisXNTHENyisψN;where Xi(i=1…N) is the ith input parameter.Modeling a MISO system as the combination of some SISO sub-systems also requires using an aggregation mechanism; this paper defines aggregation as an interpolation mechanism described in the following subsection.After extraction of NPs, the IDS method devotes confidence levels to these NPs in order to use them in an interpolation mechanism. The confidence level of the ith NP is computed as the reciprocal value of the data spread inside the SISO sub-system around its NP. The spread of N data points around an NP is calculated as:(9)Sψi=1N∑j=1N(ψi(xj)−yj)2Thus, the confidence level of the ith SISO sub-system is defined as:(10)Ci=1SψiTherefore, the simulation of the output with respect to the input in our mechanism is finally calculated as:(11)Out(x1,x2,…,xn)=∑i=1NCiψi(xi)∑i=1NCiwhere x1, x2, …, xnare the input parameters of the given MISO system.To improve the generalization ability, ALM also regards an additional procedure which hierarchically localizes the modeling procedure inside the data domains described above in this section. Hierarchical localization inside a data domain is performed through a recursive partitioning process which is described in the following section.At each level of recursion, the domain of the given MISO is divided into two partitions based on which SISO sub-system that has the greatest confidence level. In [41] the midpoint is used as a heuristic to divide a plane. Although, the heuristic does not guarantee to converge to an optimal solution, it may reach satisfactory solutions [42]. Therefore, based on the coordinates of the division point, a rule is defined as:IFX1iL<x1<X1iUANDX2iL<x2<X2iUANDX3iL<x3<X3iUTHENIFxisX1THENyisψ1IFxisX2THENyisψ2…IFxisXNTHENyisψNENDwhereXjiUandXjiLare the upper and the lower bounds of the jth input domain of the ith rule, respectively.While the localization is used to improve the generalization accuracy of the IDS method, over-partitioning, due to overfitting, might reduce the performance of the proposed method. As a recursive partitioning structure, the procedure has a tree structure like a decision tree. So, early stopping as the overfitting solution of decision trees can be used to avoid overfitting. Early stopping considers a threshold for the generalization error as the condition of branching in decision trees. Thus, partitioning is performed (like branching in tree) when the generalization error is smaller than the considered threshold. Fig. 7illustrates the branching conditions of the ALM decision process.In Fig. 7, at each state (recursion), if the generalization error of the current sub-domain is greater than the threshold (like node B), the localization procedure continues through branching. Otherwise, if the generalization error of the current state is less than the threshold (like node C), the recursive localization procedure is terminated for the current node (labeled as “terminating node” like node C).In the parameter space of Q-learning, three main parameters are included: state, action and reward. Therefore, true declaration of the state space, the action and the reward parameters is very important to guarantee the best performance.In the proposed method in this paper, some state parameters for observing the status of DRP in Cloud's environment are considered in the reactional form, and some others are proactional. Reactional parameters refer to those observed only in the current status of the system. On the other hand, proactional parameters inspire future expectations acquired by anticipation. In the status parameters (both in reactional and proactional forms), two categories of parameters are employed, i.e., collectivity and granularity, to capture information from the system. As two parameters in the collectivity category, we intend to regard the wholeness of the capacity and demand. On the other hand, the granularity category, which is regarded by its reciprocal value, includes two parameters as follows:•Capacity granularity which recognizes how much continuous capacity with respect to the total capacity is available to host big requests. The main idea behind measuring granularity lies behind the fact that the fine grained capacity may not provide enough space for hosting a big request (which leads to rejecting the request or turning on extra physical machines), while the total free capacity is adequate to host the request.Demand granularity that determines how much the controller is likely to face with big requests with respect to the total demands in the future control period.The above mentioned parameters of the state are elaborated as follows:✓ Current status (reactional):1The current workload (in the form of collectivity) which is the sum of the entire resources consumed inside the Cloud infrastructure.(12)EtRes=∑j∈PMsLoadjReswhere PMs is the set of Physical Machines (PMs), andLoadjResis the load of Res∈{RAM, CPU, BandWidth, Storage} in the jth PM.The reciprocal value of granularity seen from the viewpoint of the capacity where the greater value corresponds to a better ability of the controller to prevent from resource wastes.✓ Future demand (proactional)1Anticipation of the future demand (in the form of collectivity) that enables the provider to plan on how much resources should be provided for future resource availability.(14)Ct+1Res=∑k∈AllDemands(t,t+1)demandkReswheredemandkResis a demand for Res∈{CPU, RAM, Bandwidth, Storage} submitted to the Cloud in the future control period, and AllDemands(t, t+1) includes the indexes of demands in the future control period.Anticipation of the reciprocal value of demand granularityLikeFtRes,Dt+1Resis also a metric for granularity; a greater value forDt+1Resindicates that the Cloud will face with big requests, and therefore, the Cloud provider should provide enough continuous space to host the incoming requests. For the Cloud provider, a great value ofDt+1Resis undesired and prevents the resource provisioning mechanism from using chopped capacity, imposing activation of extra physical servers.Using Eqs. (12)–(15) which describe the state of the system, the controller is able to draw a proper picture of the system, leading to generate sophisticated control signals.The next section describes prediction, an urgent requirement for proactional decisions, in our control scheme.In this paper, ANN, which is a popular prediction tool in engineering applications, is employed to provide parameter estimation. The ANN should predictDt+1ResandCt+1Res, as the proactional state parameters. The prediction process can be defined as a sequential supervised learning (SSL) problem which deals with time series of historical observations onDtResandCtRes.Since ANNs are able to solve Classical Supervised Learning (CSL) problems, the sliding window is used to convert an SSL problem into a CSL problem [43–45]; see Fig. 8.As Fig. 8 illustrates, the sliding window converts the time series ofDtResandCtResinto the format of a CSL problem. Through the sliding window, the window of N observations is separated into a single value for prediction purpose (Dt+1ResandCt+1Res), and N−1 previous observations(DiResandCiReswherei=t−(N−1),t−(N−1)+1,…,t). Then, a general approximator, like an ANN, solves the resulting CSL problem. The time series include the amount of usages for four types of the resources (i.e., CPU, RAM, Band width and Storage) in the future periods. Fig. 9illustrates the architecture of the ANN used as the general approximator for prediction ofDt+1ResandCt+1Res.In this paper, the problem of predictingDt+1ResandCt+1Rescan be described as a 24 inputs–8 outputs system. Thus, the architecture of the corresponding ANN (Fig. 9) should model a 24 inputs–8 outputs system. Hence, inputs to the ANN are prepared via a sliding window with size of 4 that uses three recent observations (amount of demand, demand collectivityCtand reciprocal value of demand granularityDt) for four types of resources to predict the future value. The process of training in ANN is performed through the Levenberg–Marquardt algorithm which is provided by the MATLAB software.Control signals in controllers are addressed as actions in RL mechanisms. The mechanism of controlling the plant for introducing proper actions is described in the following section.The goal of the controller is to learn how to prevent from job rejection and also to optimize the resource usage. Regarding the client satisfaction, prevention from job rejection reduces missing clients. On the other side, from the provider's point of view, reduction in the number of active servers prevents from wastages in energy consumption, which corresponds directly to the number of extra active servers.Therefore, the controller has two major tasks:1.Determining the total number of active servers to host the requests in the next period from time t to t+1,Determining those physical servers that should host new VMs in the next period of time.Thus, the action of the controller is defined as:(16)at=ΔNS,where atis the action (control signal) at time t that is a duplex containing change in the overall number of active servers (ΔN) and the vision of VM placement (S).The main part of the VM placement vision is its strategy that can be selected from [First Fit (FF), Worst Fit (WF) and Best Fit (BF)]; based on the FF strategy, the current VM is assigned to the first Physical Machine (PM) from the set of available PMs that can host the current VM. According to the WF strategy, the PM with the biggest available capacity among those PMs that have enough space to host the current VM is devouted. By the BF strategy, the current VM is assigned to that PM that has the least capacity among those PMs which can host the current VM. S also involves fitness for each resource type in the VM placement vision. The fitness is measured by:(17)fVMcurrent=∑res∈ResourcesαresVMrescurrent∑res∈ResourcesαresfPMi=∑res∈ResourcesαresCapacityresPMi∑res∈Resourcesαres;∏res∈Resources1+CapacityresPMi−VMrescurrentCapacityresPMi>0wherefVMcurrentis the resource fitness value of the current request,fPMiis the capacity fitness value of the ith PM, αres is the weight of resource res∈Resources;Resources={CPU, RAM, Storage, BandWidth},VMrescurrentindicates the resource requirements of the current VM for resource res, andCapacityresPMiis the capacity of the ith PM for resource res.In Eq. (17),∏res∈Resources1+CapacityresPMi−VMrescurrentCapacityresPMi>0is used to guarantee that the required resources forVMrescurrentare satisfied with the capacity of PMi.In summary, S consists of the placement strategy and the fitness coefficients of every resource type:(18)S=Strategy∈{FF,WFandBF}αres;res∈ResourcesTherefore, the controller interacts with the environment (resource provisioning in the Cloud) until it learns which control action atis proper at state stin order to obtain the maximum profit.The reward, which is the response of the environment to the action taken in the current state, is the final metric for evaluating the overall performance of the system. In Q-learning, the reward is included in the learning procedure by means of discounted cumulation of reward values during a sliding window of the decision period boundaries.In essence, the definition of the reward should reflect the goal of the control mechanism inside the RL process. In this paper, two major goals contribute to compute the reward: prevention from job rejection and optimization of energy consumption (which is tightly geared with the optimization of resource usage). Thus, the immediate reward for resource Res at time “t” is defined by:(19)RT=RJT×[−RJT]+(1+[−RJT])FTReswhereFtResis the reciprocal value of capacity granularity (see Eq. (13)), and RJtis the job rejection rate formulated as follows:(20)RJt=∑i∈Rejected(t)demandi∑j∈AllDemands(t)demandjwhere Rejected(t) is the set of indexes corresponding to rejected demands at time “t”, and AllDemands(t, t+1) includes the indexes of demands in the future control period.In Eq. (19),−RJtis used for devoting priority to two major measurements included in the formula. More precisely, Eq. (19) has two major parts:1.RJt×−RJt(1+−RJt)FtRes.As the first priority, if job rejection occurs,−RJtwill be −1 eliminating participation of the second part of the equation, i.e., Rt=−RJt. In other words, when facing with job rejection, capacity granularity has no contribution in the immediate reward, and less job rejection results in better reward. On the other hand, if no job rejection occurs, the first part of the equation will be equal to zero, i.e.,Rt=FtRes. In this case, less capacity granularity (higherFtRes) results in better reward. Therefore, according to Eq. (19), the controller first tries to prevent from job rejection, and then plans to optimize the resource usage. This enables the controller to regard higher priority for prevention from job rejection since job rejection has more negative consequences.In summary, the input–output parameters of the RL system can be described by Table 1.Due to the need for exploration, RL based controllers perform actions regarding a tradeoff between exploration and exploitation. In this paper, the tradeoff is determined through an action recommendation mechanism described in the following section.Action selection in continuous Q-learning with real-time constraints is a difficult task. On one side, Q-learning works based on an action-independent learning procedure that requires finding max Q (see Eq. (1)) by examining the entire action set through an explorative policy. On the other side, exploration of the continuous action-space domain is not affordable in real-time constraints. Thus, in this paper, a population based optimization method is used to findmaxaQ(st,a); this also enables the action selection procedure to follow some explorative policies by having chance to choose from a set of discrete actions. In fact, a sort of discretization is performed through the population based optimization method so as to prevent from exploration in continuous space.Another issue in Q-learning is to balance the action selection mechanism with an exploration policy so as to enable the agent to improve its current sub-optimal policy. This paper offers recommending a set of actions to the action selection mechanism through the discretization mechanism. In this paper, the genetic algorithm (GA) is used as the population based optimization mechanism for action recommendation. The schematic view of the GA based action recommendation mechanism is illustrated in Fig. 10.In the GA algorithm, since the initial population is chosen randomly, the cross-over is performed by choosing three random positions in the middle of chromosome at each iteration, and 0.05 mutation possibility is considered for each gene in the chromosome. The desirability of the population is just dependent upon the maximum utility (cumulative reward) among the population of the solutions (actions described in Section 4.2.3) at a certain number of iterations.In this paper, the encoding of a single solution inside the GA population considers binary values as the GA formulation requirements. According to the formulation of actions mentioned before, the properties of a solution (i.e., the selected action) include the number of changes in the active servers (ΔN), the strategy of VM placement (Strategy), and the coefficients of VM's fit into the resources (αres); ΔN ranges from −63 to +64, and fitness coefficients for every resource type, i.e., αres, ranges from 0 to 31. Moreover, three types of strategy [FF, WF and BF] are considered. While the ranges of the encoded parameters could be determined in wider areas, we chose theses limited ranges so as not to impose the optimizer with a wide (and extreme time consuming) search area.Therefore, the domain of ΔN contains 26 choices, each resource type inside αresincludes 25 choices (totally 220 choices for αresconsidering four resource types), and the Strategy contains three choices (which is expressed as 22 bound to 3). Thus, the entire chromosome (a single solution) contains 28 genes which can be used to encode 228=26×22×220 solutions. The encoding of the action domain for GA population is illustrated in Fig. 11.In Fig. 11, each property of the action is characterized as a binary valued field inside the chromosome declaration. Therefore, each solution for the action can be processed by cross-over and mutation operators which are commonly designed to deal with binary values. Also, the strategy of VM placement is bound to three choices so that the operators are limited to choose between three choices.At each period, the goal of GA is to find the action which is expected to return the maximum cumulative reward from the interface of IDSFQ. Though, instead of a single action a group of actions is returned to provide exploratory abilities for the controller. Since real-time optimization is not guaranteed by a raw GA process, to overcome real-time constraints included in the resource provisioning scenario, a threshold (1000 iterations) is considered for maximum number of iterations when seeking for more optimal solutions inside the problem space. Therefore, while reaching the global optimum is not guaranteed, the near optimal solution is expected to be reached, although, GA is not required to explore all 228 possible choices.In order to regard a chance for exploration, the population based optimizer introduces a set of actions to the action selection component. Then, an action is selected regarding an exploration policy. The controller devotes probabilities to each member of the recommended action set to determine its proper chance for being chosen as the final decision. Therefore, first the actions are sorted decreasingly by their fitness values, and then probability for each action is determined with Eq. (21):(21)P(ai)=e−ilog(t)∑j=0N−1e−iog(t):i=0:N−1,where t is the learning episode, i is the index of the action inside the sorted action set (actions with better expected cumulative reward are placed in lower indices), N is the size of the action set, aiis the ith action, and log(t) is used to reduce the speed of linear increment of t.Fig. 12illustrates the probability distribution of action selection over the sorted actions for t=0 according to Eq. (21).Therefore, the action is selected as the output of the controller randomly regarding the mentioned probability distribution (Eq. (21)).This section evaluates the proposed RL based adaptive controller for DRP in Cloud's virtualized environment. IDSFQ is initialized by random Q-values on random (state, action) pairs. Then, the dataset, which contains records of 90 days requests submitted to the Cloud, is tested on the proposed controller. Each day, the control signal is generated in two minutes intervals (each day, 720 control signals). Each request to the Cloud, which corresponds to a single VM, is a vector consisting of demands for four types of resources (i.e., CPU, RAM. Storage and Bandwidth); requests (4×1 vectors) are generated in a random fashion. Distribution of jobs over the days is performed by determining a pattern formed by combination of three normal distributions over a day. The distribution of workload versus resource demand for CPU cores in a single day is illustrated in Fig. 13.Performance metrics used to evaluate the prediction mechanism are reliability and accuracy. Reliability which is measured through regression R, ranges from −1 to 1 and is an index for determining the degree of linear relationship between the measured and the simulated values in a test set. R=0 is an indication for no linear relationship; however, if R=1 or −1, a perfect positive or negative linear relationship exists, respectively. The desired value for R is 1 reflecting the perfect reliability for the modeling method. R is calculated by:(22)R=(1/N)∑i=1N(Yi−T¯)(Ti−Y¯)(1/N)∑i=1N(Yi−T¯)2×(1/N)∑i=1N(Ti−Y¯)2Accuracy is commonly measured via normalized mean squared error (nMSE). The error is squared in order to penalize the large values of error along with eliminating the effect of the positive and negative values of differences. The nMSE value is calculated by:(23)nMSE=∑i=1N(Yi−Ti)2∑i=1N(Yi−Y¯)2,To evaluate the performance of the ANN, used for predicting proactional state parameters prediction (i.e.,Dt+1ResourceandCt+1Resource), 15 experiments in different random training and test sets are performed. The training and the test sets include 80% and 20% of the entire data set which include the records of 20 days workload observations. Each day, 91,309 jobs are submitted to the Cloud which are spread over the day and incorporated with 10% noise without historical effects.The average prediction performance of 15 experiments, when the size of sliding window is 4, is illustrated in Table 2.Results in Table 2 demonstrate that the performance of the ANN for predicting resource usage is appropriate, and the NN is configured suitably.Fig. 14illustrates R for a single experiment of predicting the total demand of CPU cores in the next control period.Convergence of the proposed RL based adaptive controller for producing immediate reward is illustrated in Fig. 15(the Fig. depicts the minimum immediate reward).The reward illustrated in Fig. 15 is the minimum immediate rewards for the resources, i.e.,minres∈ResourcesRtresthe controller achieved by choosing actions at each day (Episode), while γ is set to 0.9, and α in Eq. (1) is equal to 0.8. Results report that at the end of the training episodes (90 days), the controller is able to choose actions which produce minimum of 0.63 for the immediate reward. The primary goal of the reward production is to avoid job rejection. Convergence of the controller for preventing from job rejection is illustrated in Fig. 16.Fig. 16 illustrates that after 70 days, the controller learned to prevent from job rejection in a stable manner. Even though, at first, the controller could prevent from job rejection in day 54, however the stability is achieved in 70th day.The convergence process that enables the controller to avoid generating non-optimal control signals is illustrated in Fig. 17.Results in Fig. 17 show that at the 90th day, the controller performs actions which only impose 11.9% resource waste to the Cloud infrastructure system. Thus, the proposed controller is converged to a proper policy for resource allocation.To evaluate the performance of the proposed IDSFQ method, in this paper, some other general approximation approaches including Neural Networks (N FQ), Neuro-Fuzzy (NF FQ), Recurrent Neural Network (Recurrent NFQ) and TSK-FIS (Fuzzy FQ) are employed for implementing the proposed control architecture. The mentioned approximation methods are provided by the MATLAB toolbox as black box general approximators. Comparison between the proposed controller using IDSFQ and some other methods for fitting Q-values on (state, action) pairs is illustrated in Fig. 18.As illustrated in Fig. 18, the proposed IDSFQ controller shows better performance in comparison with the other general approximators. While at the end of control episodes (90th day), NFQ, Recurrent NFQ, NFFQ and Fuzzy FQ reached to the position of choosing actions with positive immediate rewards, the proposed controller learned to choose actions that produce minimum of 0.63 for the immediate reward.

@&#CONCLUSIONS@&#
