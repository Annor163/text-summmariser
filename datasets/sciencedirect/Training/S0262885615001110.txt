@&#MAIN-TITLE@&#
Learning to detect video events from zero or very few video examples

@&#HIGHLIGHTS@&#
We deal with the challenging problem of high-level event detection in video.We build event detectors based solely on textual descriptions of the event classes.We also learn event detectors from very few positive and related training samples.We present results and comparisons on a large-scale TRECVID MED video dataset.

@&#KEYPHRASES@&#
Video event detection,Textual event description,Zero positive examples,Few positive examples,Related videos,

@&#ABSTRACT@&#
In this work we deal with the problem of high-level event detection in video. Specifically, we study the challenging problems of i) learning to detect video events from solely a textual description of the event, without using any positive video examples, and ii) additionally exploiting very few positive training samples together with a small number of “related” videos. For learning only from an event's textual description, we first identify a general learning framework and then study the impact of different design choices for various stages of this framework. For additionally learning from example videos, when true positive training samples are scarce, we employ an extension of the Support Vector Machine that allows us to exploit “related” event videos by automatically introducing different weights for subsets of the videos in the overall training set. Experimental evaluations performed on the large-scale TRECVID MED 2014 video dataset provide insight on the effectiveness of the proposed methods.

@&#INTRODUCTION@&#
High-level (or complex) video event detection is the problem of finding, within a set of videos, which of them depict a given event. Typically, an event is defined as an interaction among humans or between humans and physical objects [1]. Some examples of complex events are those defined in the Multimedia Event Detection (MED) task of the TRECVID benchmarking activity [2,3]. For instance, in MED 2014 [2], the defined complex events include Attempting a bike trick, Cleaning an appliance, or Beekeeping, to name a few.The detection of such events in video has recently drawn significant attention in a wide range of applications, including video annotation and retrieval [1], video organization and summarization [4], or surveillance applications [5]. In [6], Brown studies how high-level events play a substantial role in the mechanism of structuring memories and recalling past experiences. This leads to the expectation that event-based organization of video content can significantly contribute to bridging the existing semantic gap between human and machine understanding of multimedia content.There are several challenges associated with building an effective detector of video events. One of them is finding a video representation that reduces the gap between the traditional low-level audio–visual features that can be extracted from the video and the semantic-level actors and elementary actions that are by the definition the constituent parts of an event. In this direction, several works have shown the importance of using simpler visual concepts as a stepping stone for detecting complex events (e.g. [7,8]). Another major challenge is to learn an association between the chosen video representation and the event or events of interest; for this, supervised machine learning methods are typically employed, together with suitably annotated training video corpora. While developing efficient and effective machine learning algorithms is a challenge in its own right, finding a sufficient number of videos that depict the event so as to use them as positive training samples for training any machine learning method is also not an easy feat. In fact, video event detection is even more challenging when the available positive training samples are limited, or even non-existent; that is, when one needs to train an event detector using only textual information that a human can provide about the event of interest.In this work we study the problems of i) learning an event detector solely from a textual description of the event, without using any positive video examples, and ii) learning from very few positive training samples together with a small number of “related” videos. The paper is organized as follows. In Section 2, related work in video event detection using zero or a few positive examples is reviewed. In Section 3, we present and examine different design choices for a framework that learns video event detectors based solely on textual information for training, while in Section 4 the combination of the above methods with learning from a few positive examples, as well as from related training examples, is examined. Results of the application of the proposed techniques to the TRECVID MED 2014 dataset are provided in Section 5. Finally, conclusions are drawn and discussed in Section 6.

@&#CONCLUSIONS@&#
