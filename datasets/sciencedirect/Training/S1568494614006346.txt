@&#MAIN-TITLE@&#
Moving object classification using local shape and HOG features in wavelet-transformed space with hierarchical SVM classifiers

@&#HIGHLIGHTS@&#
An integrated system that segments and classifies four moving objects.A weight mask is proposed to enhance the distinguishing pixels in a segmented object.A new classification feature vector extracted from a wavelet-transformed space.A hierarchical linear support vector machine classification configuration is proposed.

@&#KEYPHRASES@&#
Pedestrian detection,Car detection,Multi-class object classification,Haar wavelet transform,Histogram of oriented gradients (HOG),Support vector machines,

@&#ABSTRACT@&#
This paper proposes an integrated system for the segmentation and classification of four moving objects, including pedestrians, cars, motorcycles, and bicycles, from their side-views in a video sequence. Based on the use of an adaptive background in the red–green–blue (RGB) color model, each moving object is segmented with its minimum enclosing rectangle (MER) window by using a histogram-based projection approach or a tracking-based approach. Additionally, a shadow removal technique is applied to the segmented objects to improve the classification performance. For the MER windows with different sizes, a window scaling operation followed by an adaptive block-shifting operation is applied to obtain a fixed feature dimension. A weight mask, which is constructed according to the frequency of occurrence of an object in each position within a square window, is proposed to enhance the distinguishing pixels in the rescaled MER window. To extract classification features, a two-level Haar wavelet transform is applied to the rescaled MER window. The local shape features and the modified histogram of oriented gradients (HOG) are extracted from the level-two and level-one sub-bands, respectively, of the wavelet-transformed space. A hierarchical linear support vector machine classification configuration is proposed to classify the four classes of objects. Six video sequences are used to test the classification performance of the proposed method. The computer processing times of the object segmentation, object tracking, and feature extraction and classification approaches are 79ms, 211ms, and 0.01ms, respectively. Comparisons with different well-known classification approaches verify the superiority of the proposed classification method.

@&#INTRODUCTION@&#
The detection and classification of moving objects from video sequences have become active research topics and are widely used today in various circumstances, such as human motion capture [1], home care systems [2], and intelligent transportation systems [3]. Applications in intelligent transportation systems include pedestrian detection [3–5], car detection [6], and traffic flow estimation [7] and monitoring [8,9], among others. The detailed classification of moving objects into categories of pedestrians, cars, motorcycles, or bicycles can aid in the accurate analysis and monitoring of traffic conditions or the retrieval of a specific object from a video sequence. The objective of this paper is to develop classification schemes for these four types of moving objects. This paper proposes a new set of features and a hierarchical classification approach intended to increase the classification rate. Moving object classification procedures usually are used in conjunction with an integrated system, which includes pre-processing steps to obtain the initial candidate objects and post-processing to integrate the classification results. Therefore, in the red-green-blue (RGB) color space, a moving object segmentation approach is incorporated into the classification system. In particular, a shadow removal (SR) technique is incorporated into the hierarchical classification approach to increase the classification rate.The feature extraction of candidate objects and the classifier design are the two most important factors affecting the performance of an overall classification system. In recent years, many studies have been proposed for the detection of pedestrians, cars, or bicycles/motorcycles [10–18]. The features used in these studies include Haar wavelets [10,11], Haar-like features [12], local binary patterns (LBP) [19], and histograms of oriented gradients (HOG) [14,15]. In [10], modified Haar wavelet features were extracted from a Haar wavelet framework with an overcomplete dictionary. The results using the Haar wavelet features were presented for people, face, or car detection problems. The LBP is a texture descriptor that has been widely used in various applications, such as pedestrian detection [13] and face recognition [20]. The LBP operator first assigns a label to every pixel of an image and then uses a histogram of the labels as the texture descriptor. In [14], the HOG features have been shown to outperform the Haar wavelet, scale-invariant feature transform with principal component analysis, and shape contexts for pedestrian detection in complex backgrounds. In recent years, the HOG features have been successfully applied to different detection/classification problems, such as car detection [21,22], motorcycle detection [21,23], and face recognition [24,25].Different features have their own strengths in describing an object. The HOG well captures local shape characteristics of the targeted object based on directions and magnitudes of image derivatives. The LBP is a texture descriptor, which is computationally efficient, invariant to monotonic gray level changes, and convenient in multi-scale extension. It can filter out noises using the concept of uniform pattern. Some studies on combinations of two sets of feature descriptors also have been proposed [4,26,27], such as the combination of the HOG with the LBP (HOG-LBP) [26] or with the local receptive fields [4]. Such combination takes advantage of each feature descriptor and, therefore, improves the detection performance. However, such a combination is at the cost of increasing feature dimensions and computational loads. This paper proposes a new feature descriptor, which extracts the local shape and the histogram of oriented gradients (HOG) from the level-two and level-one sub-bands, respectively, in the wavelet transformed space (LSHOG-W). The experimental results of the four-moving-object classification problem show that the proposed feature descriptor is characterized by the advantages of lower feature dimensions and better classification performance in contrast to the modified Haar wavelet coefficients, the LBP, the HOG, and the HOG-LBP features.This paper proposes a four-class object segmentation and classification approach for use in a video sequence with a fixed camera. The extraction of a high-dimensional feature descriptor, such as the HOG and LBP, is time consuming, especially when the operation is applied to a whole image with an exhaustive block-based search [13–15]. For the problem presented in this paper, an automatic moving-object segmentation approach is proposed so that detection/classification is performed only on the minimum enclosing rectangle (MER) window of a segmented object. This approach helps reduce the computational load especially when it is of interest to carry out real-time operations. Unlike previous object detection approaches, which detected a specific object with a fixed block size [10–15], the sizes of the four types of classified objects are different. To obtain the fixed feature dimensions for the different objects, this paper proposes a window scaling operation followed by an adaptive block-shifting operation. To apply feature extraction to the MER window, a weight mask is used to enhance the distinguishability of pixels, which is then followed by the Haar discrete wavelet transform (DWT). Finally, the new feature LSHOG-W is extracted in the wavelet transformed space.For the classifier design, neural fuzzy classifiers (NFC) [1,2,16], support vector machines (SVMs) [10,14,18], and boosted decision trees [12] have been employed in object detection/classification problems. For high-dimensional and large-scale classification problems, NFC training is time consuming and typically makes use of parallel processing techniques [28]. In light of their training efficiency and good performance, SVMs and boosted decision trees have become the leading classifiers for object detection problems with high-dimensional feature descriptors. Combinations of SVMs with wavelet coefficients [10], LBP [13,19], HOG [14,21], and HOG-LBP [26] for object detection/classification have been proposed. Unlike previous studies, which have largely focused on the detection of single objects using one SVM, this paper considers the four-class classification problem and addresses this problem using multiple linear SVMs. In particular, a new hierarchical SVM classification approach, which incorporates the SR technique, is proposed. Experiments on the combination of the LSHOG-W feature with the hierarchical SVM with the SR technique are conducted to verify the performance of the proposed approach.The remainder of this paper is organized as follows. Section 2 introduces the histogram- and tracking-based segmentation approach used to detect moving objects. Section 3 introduces the proposed feature extraction process. Section 4 introduces the proposed hierarchical SVM classification configuration. Section 5 presents the experimental results of the proposed detection and classification approach. Comparisons with various well-known features are also conducted in this section. Finally, Section 6 presents the conclusions of this study.Several object segmentation algorithms with a fixed camera have been proposed [29–35]. The proposed segmentation method is implemented in the RGB color space with the consideration of shadow removal. It consists of five steps. The first three steps are not only color-based modifications of those used in gray images [32] but are also characterized with the new function of registering a new object as a background region. Detailed implementation algorithms are introduced as follows.Step 1: This step finds the frame difference between the current and previous frames to identify the moving pixels. The frame difference is given by the following equation:(1)FDn(x,y)=maxRn(x,y)−Rn−1(x,y),Gn(x,y)−Gn−1(x,y),Bn(x,y)−Bn−1(x,y)where (Rn, Gn, Bn) and (Rn−1, Gn−1, Bn−1) represent the RGB color channels of the current and previous frames, respectively. If the FDn(x, y) is greater than a threshold value ThFD, then the pixel with position (x,y) is identified to be a changing pixel; otherwise, it is identified as a stationary pixel. The ThFDis set to a small value to find the change in a pixel but the value should not be too small to reduce the influence of noise. This paper sets ThFDto 10 according to this principle and experimental result in a training video.For the application considered in this paper, moving objects such as the pedestrian in Fig. 1usually appear in the scene at the start of the system. The frame difference is used to identify the moving objects while registering the initial background. Additionally, after the registration of the background, the frame difference is used to register a new object such as a newly parked car as the new background region if it is identified as a stationary region for a successive period of time, as introduced in the next step.Step 2: This step registers a pixel as part of the background region if it is identified as a stationary pixel for a succession of Fth (which is set to 50 in this paper) frames, regardless the time at which the condition is met. Once a pixel has been registered as a background region in frame n, the initial RGB components in the background are assigned as follows:(2)μRn(x,y)=Rn(x,y),μGn(x,y)=Gn(x,y),μBn(x,y)=Bn(x,y).For the registered background regions, this step updates the composing pixels using the following equation:IFRn(x,y)−μRn(x,y)<2σRn(x,y)andGn(x,y)−μGn(x,y)<2σGn(x,y)andBn(x,y)−μBn(x,y)<2σBn(x,y)THEN(3)μRn(x,y)=αμRn−1(x,y)+(1−α)Rn(x,y)σRn2(x,y)=ασRn−12(x,y)+(1−α)(Rn(x,y)−μRn(x,y))2μGn(x,y)=αμGn−1(x,y)+(1−α)Gn(x,y)σGn2(x,y)=ασ2Gn−1(x,y)+(1−α)(Gn(x,y)−μGn(x,y))2μBn(x,y)=αμBn−1(x,y)+(1−α)Bn(x,y)σBn2(x,y)=ασBn−12(x,y)+(1−α)(Bn(x,y)−μBn(x,y))2,where σRn(x, y), σGn(x, y), and σBn(x, y) are the standard deviations of each color component of a pixel with position (x, y) in the n-th frame, and α (0<α<1) is a predefined constant used for temporal averaging. This paper sets α to 0.7. Fig. 1 shows the registered background in different frames by using the background registration and update step. For a slow moving object such as the pedestrian in the first column of Fig. 1, its composing pixels do not satisfy the background registration constraint and are, therefore, not registered as the background. A pixel that is not registered as a background region is marked by white color in the second row of Fig. 1. The area of the white regions decreases as more pixels are registered as background regions with the increase of time.Step 3: This step uses the following background difference equation to distinguish moving objects from the background:(4)BDn(x,y)=Rn(x,y)−μRn(x,y)σRn(x,y)+Gn(x,y)−μGn(x,y)σGn(x,y)+Bn(x,y)−μBn(x,y)σBn(x,y).If the background difference BDn(x, y) is greater than the threshold value ThBD, then the pixel at position (x, y) is identified as a moving object region. When the background difference of each of the three color components is greater than its standard deviation, the value of BDn(x, y) is greater than three. The ThBDvalue is selected to be greater than three and is experimentally set to four according to the performance in a training video. After segmentation, the algorithm uses a morphological operator, which performs one erosion operation followed by one dilation operation, each of which has 3×3 structuring features, to eliminate small noise.Step 4: The fourth step aims to reduce the influence of shadow on the segmented object. Several studies on shadow detection have been proposed [34,35]. This step uses a simple but effective shadow removal (SR) technique to reduce the shadow effects. In the RGB color space, let vectorsVo⇀andVb⇀denote the RGB color information of a pixel in the segmented object and the background buffer, respectively. In [34], shadow is detected by using a brightness distortion, α, which is a scalar value that brings the observed colorVo⇀close to the expected chromaticity lineVb⇀in the RGB color space. That is, α is obtained by minimizingVo⇀−αVb⇀2. Unlike [34], this paper simply uses the included angle θ between two color vectors in the RBG color space to measure their difference in brightness. A pixelVo⇀in the RGB-segmented region is classified as a shadow if cos(θ)≥cth andVo⇀−Vb⇀2≤Dth. This case indicates that the foreground pixelVo⇀and the background pixelVb⇀at the same frame position have very similar RGB color component values (i.e., Dth should not be too large); however, their difference in lightness is not highly significant (i.e., cth should be close to 1). The value of each color component in a vector is within [0,255], and so the distanceVo⇀−Vb⇀2ranges from 0 to 442. When the difference in each of the three color components is greater than 50, thenVo⇀−Vb⇀2is greater than 87. Therefore, Dth is suggested to be not greater than 87. IfVo⇀is different fromVb⇀only in the lightness, then the two vectors should have almost the same direction in the color space. That is, their included angle θ should be very close to zero, i.e., cos(θ) is very close to 1. This explains the reason why the threshold cth should be close to 1. This paper experimentally sets the threshold values of cth and Dth to 0.995 and 75, respectively, according the segmentation performance in a training video. Fig. 2shows the segmentation result of a motorcycle using the SR technique.Step 5: The position of the minimum enclosing rectangle (MER) window of an object is determined by the horizontal and vertical histograms of the segmented image, as shown in Fig. 3. A complete object appears when the two horizontal boundary coordinates are available (a zero histogram value), as shown in Fig. 3(b). If either the width or the length of the MER window is smaller than the minimum interested number of pixelsn¯, then the MER window is ignored; otherwise, the MER window is sent to the classification stage and is used in the subsequent tracking-based segmentation approach. This paper setsn¯to 22 under the consideration of object distinguishability and the block size for feature extractions. Detailed explanation of this setting is given in Section 5. This operation helps to eliminate small objects, such as those in the right side of Fig. 2(b) and (d), in a segmented image.Once a complete object is segmented, the corresponding complete MER window is obtained. Afterward, the MER window of the moving object is obtained by using a tracking algorithm. The histogram-based segmentation approach is no longer applied to this tracked object to avoid overlapping the histograms of existing and new objects. Given the object coordinate (xn−1, yn−1), the tracking algorithm first predicts the next coordinate(x′n,y′n). The prediction method in [18] is used because it predicts the next coordinate using both estimated velocities and accelerations from previous frames and is efficient in computation. The predicted coordinate can be obtained by using the more complex Kalman filter algorithm in which the influence of noise is considered for tracking performance improvement [36]. Based on the predicted coordinate(x′n,y′n), the correct coordinate(x′n+dx,y′n+dy), is then obtained by identifying the dx and dy that minimize the correlation coefficient r(dx, dy), which is defined as follows:(5)r(dx,dy)=∑(x,y)∈MEROn−1(x,y)−O¯n−1On(x′+dx,y′+dy)−O¯n∑(x,y)∈MEROn−1(x,y)−O¯n−12∑(x,y)∈MEROn(x′+dx,y′+dy)−O¯n212,where 0≤dx≤10, 0≤dy≤10, On−1(x, y) is the grayscale intensity of the pixel (x, y) in frame n−1, andO¯n−1is the average intensity of the pixels in the MER window of frame n−1. Fig. 4shows examples of the tracking-based segmentation results.Fig. 5shows the flowchart of the LSHOG-W feature extraction process. Pre-processing handles the variations in object sizes and image brightness. The weight mask of a window is proposed to enhance the feature distinguishability among the four classes of objects. The two-level Haar wavelet transformation is first performed. Then, both the local shape features and the HOG are obtained from the transformed space. Detailed introductions of each operation are as follows.The image in the MER window is converted into a grayscale image because the use of grayscale is favorable for later feature extraction. To obtain a fixed feature dimension for the classification of various objects with different sizes, the MER window of an object is resized. The maximum size of a rescaled MER (RMER) is 128×64 and is obtained by applying the same scaling factor to the MER width and length. If the original window satisfies this constraint, then no resizing is performed. In the scaling process, a bilinear interpolation is used to find the new grayscale levels in the RMER window. Finally, an alpha-gamma equalization with parameters α=75 and γ=0.8 is applied to the RMER window to reduce the effect of lightness [37].A weight mask is proposed to enhance the distinguishing region of an object in the RMER window. The size of the mask is M×M pixels, where M is set to 64. To find the weights in the mask, one thousand segmented patterns are collected from each of the four classes of pedestrians, bicycles, motorcycles, and cars. That is, there are a total of 4000 patterns. For each pattern, the silhouette of a segmented object is found, as shown in Fig. 6. The minimum enclosing square of each silhouette is identified and then scaled to the size of the weight mask. The histograms of all 4000 silhouette images in the mask are obtained by counting the number of silhouette pixels in each of the M×M mask bins. Fig. 6 shows the intensities of each class of histogram. Because the histogram is computed from a total of 4000 patterns, the maximum histogram value in each bin is thus 4000. A bin with a small histogram value close to zero indicates that most objects do not appear at this position, and therefore, it contains little distinguishing information. Similarly, a bin with a large histogram value close to 4000 indicates that most objects appear at the corresponding position, and therefore, it also contains little distinguishing information. A bin with half of the maximum histogram value (i.e., 2000) indicates that half of the objects appear at the position while the others do not. This bin thus contains important distinguishing information and is set to have the maximum weight mask value. Based on this observation, the histogram-based weight values W(x,y) in the mask are obtained as follows:(6)W(x,y)=H(x,y)/2000,H(x,y)≤2000(4000−H(x,y))/2000,H(x,y)>2000,1≤x≤M,1≤y≤Mwhere H(x, y) denotes the histogram value at position (x, y). The weigh values are then normalized to obtainW¯(x,y), where0≤W¯(x,y)≤1, by using the following equation:(7)W¯(x,y)=W(x,y)−WminWmax−Wmin,Wmax=maxx,yW(x,y),Wmin=minx,yW(x,y),where Wmin and Wmax denote the minimum and maximum weight values of W(x, y), respectively. These two values are selected so thatW¯(x,y)ranges from 0 to 1. Fig. 6 shows the normalized weight values in the weight mask, from which it can be observed that the center and corner parts are assigned with lower weight values because they contain less distinguishing information for classification.The image in the RMER window is multiplied with the weights in the weight mask. In the multiplication process, the center of the RMER window coincides with that of the weight mask, as shown in Fig. 7. Additionally, the weight mask is resized so that the two horizontal or vertical boundaries within it coincide with the RMER window. The weights in the resized mask are computed using the bilinear interpolation. The gray value of a pixel is multiplied with the weight value at the same position. Fig. 7 shows the window images of three objects before and after the window masking operation; in this figure, it can be observed that the regions with less distinguishing information show smaller gray levels after masking.Among the various wavelets, this paper uses Haar DWT because of its efficiency in computation. The mother wavelet function ψ(t) of the Haar DWT is given as follows:(8)ψ(t)=10≤t<12−112≤t<10otherwiseThe scaling function ϕ(t) of the Haar wavelet transform is(9)ϕ(t)=10≤t<10otherwise,The scaling and wavelet basis functions can decompose an image into four square sub-bands, including the average sub-band LL1, horizontal sub-band LH1, vertical sub-band HL1, and diagonal sub-band HH1. The average sub-band LL1 can be further decomposed into another four components, LL2, LH2, HL2, and HH2, which result in a multi-level resolution as shown in Fig. 8.The motivation for using the DWT is described as follows. The wavelet transform shows the advantage of multi-resolution analysis and local analysis in the spatial domain. The horizontal, vertical, and diagonal sub-bands in level two represent the horizontal, vertical, and diagonal edges in an image, respectively, and can be used to find the local shape feature. Based on the consideration of both the edge resolution and the feature dimension, the sub-bands in level two instead of level one are used. The local object shape can be described rather well by the result of the three sub-bands in level two with the advantage of a low-dimensional descriptor. The LH1 and HL1 sub-bands in level one can be directly used to compute the HOG descriptor in the wavelet space (HOG-W). Unlike the shape-accumulated feature, the HOG-W is extracted from the sub-bands in level one instead of level two of the Haar DWT. This is because the number of features in the HOG-W is fixed at nine regardless of the number of coefficients in each sub-band. The sub-bands of level one provide higher image resolution for computing the histogram and are therefore used in the HOG-W.For each image in a weighted RMER window, this paper applies the Haar DWT to overlapping blocks in the image rather than to the whole image, as in the traditional Haar DWT, to obtain a finer spatial resolution of the edges. Additionally, the traditional Haar DWT fails to find a fixed feature dimension for windows with different sizes. The proposed approach addresses this problem by changing the shift size between neighboring blocks. Fig. 9shows locations of the overlapping blocks, which scan the image from left to right and top to bottom. The size of each block is set to 16×16 pixels. To obtain a fixed feature dimension, the total number of overlapping blocks should be fixed. The maximum shift size between two overlapping blocks is set to be half the size of a block, i.e., 8 pixels. Because the maximum size of a RMER window is 128×64 pixels, the number of overlapping blocks is set to 15 for the larger boundary length and 7 for the shorter. That is, there are a total of 15×7 blocks for each RMER window. Toward this end, the shift size automatically varies with the RMER window size. For each 16×16 block, the image is decomposed by using the two-level 2-D Haar DWT, as shown in Fig. 9. The local shape features (LSFs) are extracted from the three sub-bands, LH2, HL2, and HH2, each of which measures 4×4 pixels. The sub-bands in the second-level are used based on the consideration of both the edge resolution and the feature dimension. The local object shape can be described rather well by the result of the three sub-bands in the second-level. The number of coefficients in each sub-band is a moderate value of 16. The LSF,F⇀Shapei, extracted from block i is defined as the absolute accumulation of the three sub-band coefficients and is given as follows:(10)FShapei(x,y)=LH2i(x,y)+HL2i(x,y)+HH2i(x,y),1≤x≤4and1≤y≤4,where (x, y) is the position of a pixel in each sub-band. For example, Fig. 8 shows the 16 feature values in one block. The LSFs from the same block are normalized by using the L2-norm normalization method. For each masked RMER window, the whole shape feature vectorF⇀Shapeallcomposed ofF⇀Shape1,…,F⇀Shape15×7is obtained, and the total dimension is 15×7×16=1680.For block i in the wavelet-transformed space, in addition to the shape featureF⇀Shapei, the HOG descriptor is used as another classification feature and is denoted as HOG-W. The horizontal and vertical gradients of a pixel in block i are obtained from theLH1iandHL1isub-bands, respectively, in the first-level of the Haar DWT, as shown in Fig. 8. Both gradients at the same position (x, y) are used to obtain the gradient magnitude Mi(x, y) and orientation θi(x, y) via the following equations:(11)Mi(x,y)=LH1i(x,y)2+HL1i(x,y)2,(12)θi(x,y)=tan−1LH1i(x,y)HL1i(x,y).To obtain the HOG-W on the edge orientation, the unsigned orientation (0°∼180°) is divided into nine bins with each covering 20 degrees. That is, there are a total of nine HOG-W features for each block. The HOG-W features within the same block are normalized by using the L2-norm normalization method. Because there are a total of 15×7 overlapping blocks, the total number of HOG-W features from a RMER window is 15×7×9=945.The LSHOG-W feature that combines the LSF and the HOG-W by concatenation is proposed to classify the moving object in the RMER. As a result, the total number of LSHOG-W features in each block is 16 (from the LSF) +9 (from the HOG-W)=25. The dimension of the LSHOG-W feature vector is 15×7 (blocks)×25=2625. These features are sent to the hierarchical linear SVM classifier for object classification.This paper uses hierarchical linear SVM classifiers to classify pedestrians, cars, motorcycles, and bicycles. Because of the high-dimensional features, this paper uses linear instead of nonlinear classifiers to ease the computational load. A hierarchical SVM classification configuration consisting of two classification stages is proposed. Fig. 10shows the proposed classification configuration. In the first stage, the motorcycles and bicycles are assigned to the same class of two-wheel objects because of their high similarity in shapes. That is, this stage classifies the three classes of pedestrians, cars, and two-wheel objects. This is a multi-class classification problem, and the one-against-all technique is used [38]. In this technique, there are three linear SVMs. The output of the ith linear SVM is denoted as yiand is given as follows:(13)yi(x⇀)=∑k=1Nwkixk+b,wherex⇀=[x1,...,xN]is the feature vector, N is dimension of the feature, andwkiand b are coefficients trained through the SVM. The outputs of the three SVMs in the first stage are denoted as y1, y2, and y3, as shown in Fig. 10. There are a total of N training samples for each of the pedestrians, cars, motorcycles, and bicycles. That is, there are a total of 2N training samples for the two-wheel class. A linear SVM is trained using all of the 4N samples collected for training. For training the first linear SVM, the desired output is “1” for the N training samples belonging to the pedestrian class, and the desired output is “−1” for the remaining 3N samples belonging to the other two classes. The other two linear SVMs are trained in the same way. To test the performance of the classification system, another set of data different from the training samples are used. For each test sample, if the maximum output of the three linear SVMs is from the first, second, or third linear SVM, then the unknown object is classified as a pedestrian, a car, or a two-wheel object, respectively.In the second stage, if the object in the RMER window is classified as a two-wheel object, further classification of the object into a motorcycle or a bicycle is performed. Because only two classes with similar shapes are classified in this stage, the weight mask is not applied to the RMER in this stage. Additionally, segmentation of the object without applying the SR technique is employed. The reason for this is that a motorcycle generates a larger shadow area than a bicycle, as shown in Fig. 11; hence, the shadow region is invoked as a clue for distinction among these two. This is different from the first stage in which the shadow removal technique is applied to segment a moving object before classification. This SR technique helps distinguish among the three classes of objects in the first stage, especially the cars and motorcycles, which both generate large shadow areas. A linear SVM is used to classify the two classes in the second stage and its output is denoted as y4, as shown in Fig. 10. There are a total of 2N training samples for training the linear SVM. During testing, if the output, y4, of the linear SVM is greater than zero, then the object is classified as a motorcycle; otherwise, it is classified as a bicycle.

@&#CONCLUSIONS@&#
This paper proposes a combination of the new feature descriptor LSHOG-W with the hierarchical SVM classification configuration for the classification of four types of moving objects. Instead of conducting detection/classification through the exhaustive search of a whole image, application of the proposed classification approach to a segmented object significantly reduces the processing time and makes the classification approach feasible for real-time operations. The LSHOG-W feature descriptor, which extracts the local shape and HOG-W features from the Haar wavelet-transformed space, demonstrates the advantages of satisfactory classification performance and relatively smaller dimensionality. Additionally, the effects of the preprocessing techniques of shadow removal and window mask in addition to the hierarchical SVM classification configuration on improvement of the performance have been verified through experimental results. All of the proposed techniques have been integrated into a user-friendly classification system.