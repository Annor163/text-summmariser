@&#MAIN-TITLE@&#
Direct model based visual tracking and pose estimation using mutual information

@&#HIGHLIGHTS@&#
We tackle the 3D model based tracking using the entire image and a textured 3D model.We propose a nonlinear optimization of the problem based on mutual information.Mutual information deals with the different modalities of real and virtual images.Our proposed method withdraws feature detection and matching issues.Results show the success of the approach and its precision.

@&#KEYPHRASES@&#
Tracking,3D pose estimation,Mutual information,

@&#ABSTRACT@&#
This paper deals with model-based pose estimation (or camera localization). We propose a direct approach that takes into account the image as a whole. For this, we consider a similarity measure, the mutual information. Mutual information is a measure of the quantity of information shared by two signals (or two images in our case). Exploiting this measure allows our method to deal with different image modalities (real and synthetic). Furthermore, it handles occlusions and illumination changes. Results with synthetic (benchmark) and real image sequences, with static or mobile camera, demonstrate the robustness of the method and its ability to produce stable and precise pose estimations.

@&#INTRODUCTION@&#
Camera tracking and pose estimation are critical for robotic applications such as localization, positioning tasks or navigation. The use of a monocular vision sensor in these contexts is full of potential since images bring very rich information on the environment. The problem of camera pose estimation is equivalent to camera localization. We aim to design a new camera pose estimation method that uses the mutual information as a visual feature.Camera localization has received much interest in the last few years. Visual Simultaneous Localization And Mapping [1–3] or, in the computer vision community, Structure From Motion with bundle adjustment optimization [4,5] are common ways of estimating the camera pose, or relative pose. These approaches reconstruct the environment and estimate the camera position simultaneously but are prone to drift (although a loop can be detected). Visual odometry is another way to retrieve the relative pose of the camera [6] but estimations drift irremediably.However, if a 3D model on the environment is already available to the robot, drift, exploration and loop closure issues can be withdrawn. In [7], it has been shown that the use of 3D information on the environment ensures a better precision in pose estimation. This 3D information allows the camera pose estimation, when a unique camera is embedded on a mobile platform, precise and without drifting, if the robot moves near referenced [8], or even georeferenced [9] landmarks. This is ensured since pose estimation is essentially a mono image problem.For a few years, 3D models of cities or urban environments have been made available through various digitized town projects over the world. The French National Institute of Geography (IGN) digitalized streets and buildings of the XIIth arrondissement of Paris in France (Fig. 1(a)). Hence, we aim to exploit this textured 3D model to localize a vehicle using vision, i.e. to estimate the pose of the camera in the real scene merging the information brought by the real image (Fig. 1(b)) and the virtual world (Fig. 1(c)) in a multi-modality scheme.Model-based pose estimation is a problem tackled since several years working with various feature types: points [10,11], lines [12], both [13] or wireframe models [14–16]. These works dealt with geometrical features but only a few other works take into account the photometric information explicitly in the pose estimation and tracking. Some of them mix geometric and photometric features [17,18]. Photometric features (image intensity) can directly be considered to estimate the homography and then the relative position between a current and a reference image [19]. A more recent approach proposes to estimate such transformation using information theoretic approaches. Mutual information shared by a planar textured model and images acquired by the camera are used to estimate an affine transformation [20], or a homography [21].The contribution of this paper is to generalize the latter work to general 3D models defined by a textured mesh, since this is a common way in computer vision or computer graphics, to represent a virtual scene. Hence, this work formulates the pose optimization problem as the maximization of the mutual information shared by a real image and a virtual view rendered from a given pose.Even if we try to maximize the mutual information between the current image and a model of the scene projected or transferred in this image, as in [21], the model of the scene in the latter work is just a reference template that is warped within the current one in order to estimate the parameters of a homography. Therefore, with the mutual information as a similarity function, the optimization process presented in [21] is inspired from the inverse compositional approach introduced by Baker and Matthew [19] (a sequel of the famous KLT) and adapted to consider the specificities of the mutual information. Parameters to be estimated in [21] belong to sl(3), the special linear group of dimension 3.In this paper, as in [21], we try to maximize the mutual information between the current image and a model of the scene. Theoretical background is then indeed very similar. Nevertheless, in our case the model of the scene is a 3D model that is projected in the image according to the current estimated camera pose (using the GPU). In this paper, the parameters to be estimated are then a pose (that belongs to se(3), the special Euclidian group). Optimization space and optimization techniques are then very different.The proposed method for pose estimation using a virtual reference scene is close to the work of Dame and Marchand [22], where a real camera is moved to a desired pose in a visual servoing control law, except that:•in our case, the camera is virtually moved to its optimal pose, corresponding to the real image whereas in [22] they physically move the camera using a robot and real images only.in [22], this last real desired image is furthermore acquired by the same camera as the one that is used to control the robot and, thus, no camera geometric or photometric difference exists between them, contrary to the real/virtual camera case as ours. This emphasizes the importance of using a robust similarity measure such as MI.[22] use only a 2D image as reference and consider a fronto-parallel desired planar scene whereas the current paper deals with any scene structure.Despite these differences, some theoretical aspects of the current paper are shared with [22], but differences and asset are highlighted in next sections.The remainder of the paper is organized in three main parts. First, the general formulation of the model based on visual pose estimation as a non linear optimization problem is introduced in Section 2. Then, in Section 3 the maximization of the mutual information to optimize the pose is detailed. Finally, results are presented, in Section 4, the behavior of the proposed pose estimation method, its precision and its robustness, before the conclusion.Pose estimation is considered in this work as a full-scale non linear optimization problem. Hence, for a new image, the pose is computed by minimizing the error between measurements in the image and the projection of a 3D model of the scene for a given pose. Since camera motion between two images is assumed to be small the pose obtained for the previous image is a good initial guess for the pose of the new image. The initialization problem is only encountered for the first image acquired by the camera. This issue is more of a detection, matching and recognition problem and is out of the scope of this paper, even if an obvious solution is mentioned in the last experiment (Section 4.2: GPS initial guess at the entrance of city, for the localization experiment).Visual pose estimation has mostly been known through feature based approaches. Considering r is a vector representation of the three translations and three rotations pose (r=[tX, tY, tZ, θX, θY, θZ]), the camera pose r⁎must satisfy some properties measured in the images. Considering s(r), the projection of 3D scene features for the pose r, the camera pose r⁎is the pose ensuring that the error between s(r) and s⁎(observation in the image) is minimal. The optimization problem can thus be written:(1)r^=argminr∥sr−s∗∥.The 3D model is classically made with geometrical features such as point [23] and line [14]. In that case, the main issue is to determine in each frame the correspondences between the projection of the model and features extracted from the image s⁎and to track them over frames.Errors or imprecision in the low level tracking leads to important error in the tracking and pose estimation process.To avoid these geometrical features tracking and matching issues, and also the loss of precision that these approaches introduce, other formulations that use images as a whole need to be proposed. It has to be noted that such direct approach has been widely considered for 2D tracking or motion estimation [19]. In such approach the idea is directly to minimize the error, the sum of squared differences (the SSD), between an image template I⁎and the current image I transferred in the template space using a given motion model (usually a homography).Theoretically, assuming that a 3D model of the scene is available, this process can scale to the pose estimation process. Indeed, in that case, the pose can be determined by minimizing the error between the image acquired by the camera I⁎and the projection of the scene for a given pose I(hbfr). The cost function could be written as:(2)r^=argminr∑xIrx−I∗x2.In Eq. (2), I(r,x) can be obtained using a rendering engine. The latter virtual model, even mapped with photorealistic textures is rendered through any 3D engine (such as openGL) and the obtained image is nothing but a synthetic image. Hence, even if the cost function of Eq. (2) is free from geometric feature tracking or matching, illumination variation or occlusions highly affect the cost function causing the visual tracking to fail.We propose to formulate another optimization criterion directly comparing the whole current and desired images. Rather than using a difference based cost function as the SSD, we define an alignment function between both images as the Mutual Information (MI) between I(r) and I⁎[24,25]. MI is a measure of the quantity of information shared by the two images [24]. When MI is maximal, then the two images are registered. The MI similarity measure has been used for registration works [25] and more recently to track planes in image sequences [21] and visual servoing [22]. This feature has shown to be robust to noise, specular reflections and even to different modalities between the reference image and the current one. The latter advantage is particularly interesting in our work since we want to align a synthetic view with a real image.We then propose an extension of [21] to the case of non planar model based pose estimation and tracking. This extension adapts the use of MI over SL(3) to the SE(3) space. It means the parameter space is the full six 3D pose parameters (three translations and three rotations).As stated in Section 2, more or less classical cost functions for pose estimation (Eqs. (1) and (2)) have to be reformulated. The goal is to perform the registration of the model with respect to the image and it can be formulated as the optimization of the mutual information shared between the input real image I⁎and the projection of the model. If r is the pose of the calibrated camera, the pose estimation problem can be written as [26]:(3)r^=argmaxrMII∗,Ir.Virtual image I(r) is resulting from the projection of the model at given pose r.MI is defined in [24] by the entropy H of images I and I⁎and their joint entropy:(4)MIII∗=HI+HI∗−HII∗.Entropies H(I) and H(I⁎) and joint entropy H(I,I⁎) are a variability measure of a, resp. two, random variable I, resp. I and I⁎. For H(I), if i are the possible values of I(x) (i∈[0, Nc] with Nc=225) and pI(i)=Pr(I(x)=i) is the probability distribution function of i (obtained from image histogram), then the Shannon entropy H(I) of a discrete variable I is given by the expression:(5)HI=−∑i=0NcpIilogpIi.In a similar way, we obtain the joint entropy expression:(6)HII∗=−∑i=0Nc∑j=0Nc∗pII∗ijlogpII∗ij.To determine the solution of Eq. (3), we consider a Newton's optimization method. To consider such an approach we have to exhibit the Jacobian and Hessian related to the mutual information. The Jacobian links the variation of the mutual information feature to the pose variation.To solve Eq. (3) for the pose r, a textured 3D model of the object to track is necessary and it has to be projected for each camera pose r. To generate images of the 3D model, we used OpenGL as a 3D renderer and more particularly the Ogre3D library.11Ogre3D, Open Source 3D Graphics Engine, http://www.ogre3d.org.OpenGL allows to generate not only intensity images but also deepness images. More precisely, we obtain an image where each pixel contains the Z coordinate of the 3D point projected in this pixel. This is particularly interesting since the Z of each visible point appears in the Jacobian linking mutual information and pose variations as shown in Section 3.3.Camera rotations and translations are highly correlated, as obviously X translation and Y rotation axes, for instance.Hence, a simple steepest descent optimization approach using the direction given by the image Jacobian related to MI would not provide an accurate estimation of the optimum of MI. Therefore, a second order optimization approach as a Newton's like method is necessary.Using a first order Taylor expansion of the MI similarity function at the current pose rkin the non linear pose estimation gives:(7)MIrk+1≈MIrk+LMITr˙Δt.∆tis the period of time necessary to transform rkinto rk+1 using the pose variationr˙(which can be seen as the virtual camera velocityv=r˙). The pose is updated thanks to e[v], the exponential map on SE(3):(8)rk+1=evrk.LMI (Eq. (7)) is the image Jacobian related to MI, i.e. the Jacobian matrix linking the variation of MI and the pose variation. This leads to:(9)LMITrk+1≈LMITrk+HMIrkvΔt,where HMI(rk) is the MI Hessian matrix. The goal is to maximize the MI so we want the system to reach the pose rk+1 where the variation of MI with respect to the pose variation is zero: LMI(rk+1)=0. Setting ∆t=1 in Eq. (9), the approximated increment that leads to a null MI variation is:(10)v=−HMI−1rkLMITrk.As demonstrated in [22], in order to have a good estimation of the Hessian after convergence, rather than using the Hessian HMI−1(rk), we useHMI∗−1estimated at the desired position r⁎HMI∗−1=HMI−1r∗:(11)v=−HMI∗−1LMIT.LMI refers to the interaction matrix related to MI computed at current position rk. Of course, the optimal pose r⁎is unknown but the Hessian matrix at the optimumHMI∗−1can be estimated without knowing r⁎, considering Z=Z⁎(each image point has its own Z), since consecutive poses are close (see the end of Section 3.3). LMI is recomputed with current Z of each image point at each iteration of the optimization process.Knowing entropy and joint entropy expressions (Eqs. (5) and (6)), MI (Eq. (4)) is developed as:(12)MIII∗=∑i,jpII∗ijlogpII∗ijpIipI∗j.From Eq. (12) and simplifications allowed by the chain derivation rule [27], the interaction LMI and Hessian matrices HMI are expressed as:(13)LMI=∑i,jLpII∗1+logpII∗pI∗and(14)HMI=∑i,jLpII∗TLpII∗1pII∗−1pI∗+HpII∗1+logpII∗pI∗,where the set of possible values is not mentioned for clarity. The MI measure imposes the complete computation of the interaction matrix (no approximation as it is usually done with standard features) [21].To face the derivation rules of the MI function, probabilities PI(i) are interpolated by B-spline functions, written ϕ. Thus, the final analytical formulation of PI(i), that can be considered as a normalized image histogram, becomes:(15)pIi=1Nx∑xϕi−I¯x,where the possible gray values are nowI¯x∈0,Nc−1. Hence, this expression allows to reduce the histogram number of bins [28] in order to decrease the dimensionality of the problem but also to smooth the MI cost function profile [22].Thus, from the joint probability:(16)pII∗ijr=1Nx∑xϕi−I¯xrϕj−I∗x,we deduce its variations with respect to the camera pose, that is the interaction matrix (Eq. (13)) and the Hessian (Eq. (14)) :(17)LpII∗ijr=1Nx∑xLϕi−I¯xrϕj−I∗x(18)HpII∗ijr=1Nx∑xHϕi−I¯xrϕj−I∗x.The variation of ϕ is got by applying the chain derivation rule:(19)Lϕi−I¯xr=−∂ϕ∂iLI¯(20)Hϕi−I¯xr=∂2ϕ∂i2LI¯TLI¯−∂ϕ∂iHI¯.Assuming a Lambertian scene, at least for short displacements, the interaction matrix related to the intensity at a pointLI¯and its HessianHI¯is obtained as follows [29]:(21)LI¯=∇I¯LxandHI¯=LxT∇2I¯Lx+∇xI¯Hx+∇yI¯Hy,where∇I¯=∇xI¯,∇yI¯are the image gradients,∇2I¯are the gradients of image gradients and Lxis the Jacobian of a point that links its displacement in the normalized image plane to the camera velocity. Hxand Hyare the Hessians of the two point coordinates with respect to the camera velocity [30]. The Jacobian hbfLxis given by [31]:(22)Lx=−1/Z0x/Zxy−1+x2y0−1/Zy/Z1+y2−xy−x.The Jacobian depends on both the position (x, y) of the point in the normalized image plane and its depth Z in the camera frame. Z is obtained from the 3D engine rendering our textured 3D model, using the Z-buffer. Therefore, the Jacobian and Hessian can be exactly computed [22]. During the iterative process of the optimization, the virtual camera moves, causing the depth of each point to change. The Jacobian and Hessian matrices are therefore changing at each iteration. Since Z⁎is needed (Eq. (11)), we assume that the depth of points between current and desired poses is not so different and fix, for each point, Z⁎=Z, since consecutive poses are close. Therefore, at convergence, the estimation of H⁎will be accurate.The algorithm presented in Fig. 2sums up all the processes of the mutual information based pose estimation and tracking approach.To illustrate the convergence and behavior of the MI optimization, an initial pose distant from 2.5cm and 3.3° from the optimal one is set, for a real image of the “tea box” sequence (see Section 4.2.1 for more detailed results). Then, it is interesting to see the evolution of MI over iterations of pose optimization (Fig. 3) as it is smooth and reaches logarithmically its maximum value.

@&#CONCLUSIONS@&#
