@&#MAIN-TITLE@&#
FCA based ontology development for data integration

@&#HIGHLIGHTS@&#
A formal and semi-automated method is proposed to support ontology integration.The method is designed to deal with data exhibiting implicit and ambiguous information.Case studies have been carried out on several non-trivial industrial datasets.Resultant ontologies better fit and respect underlying knowledge structure of the domain.

@&#KEYPHRASES@&#
Ontology development,Formal concept analysis,Data integration,Information sharing,

@&#ABSTRACT@&#
Graphical abstractImage, graphical abstract

@&#INTRODUCTION@&#
Business productivity and competitiveness are increasingly being driven by the effective access and use of data. Data provides a mine of information that can help us spot undiscovered patterns of business importance and to create the knowledge that will be needed to tackle the challenges of future. However with data becoming available and growing at unprecedented rates, organisations struggle to take full advantage of valuable data. One main reason for this is that data is usually created and maintained by a range of organisations. This results in mismatch between datasets, i.e., datasets differ from one organisation to another not only in what is encoded but also in how it is encoded.In order for organisations to use and digest heterogeneous data and uncover the untold business patterns, there is a growing interest to develop techniques that investigate complex data phenomena and facilitate better data interoperability (Doan, Halevy, & Ives, 2012; Doan, Noy, & Halevy, 2004; Duckham & Worboys, 2005; Huang, Lin, & Chan, 2012; Jiang, Zhang, Tang, & Nie, 2015; Lenzerini, 2002). Among various techniques developed, ontology research is one discipline that can deal with data heterogeneity and improve data sharing (Kalfoglou & Schorlemmer, 2003; Mate et al., 2015; Noy, 2004). Ontology-based integration systems are usually characterised by a global ontology which represents a reconciled, integrated view of the underlying data sources. Systems taking this approach usually provide users with a uniform interface—all queries made to source data are expressed in terms of a global ontology, as are the query results. This frees the user from the need to understand each individual data source. Unfortunately, in many domains one faces the problems of either having no established ontology that can be readily employed in the integration work, or existing ontologies do not fit for the purpose (e.g., not consisting of knowledge that sufficiently captures the semantics of the information under investigation).In this paper we contribute a formal and semi-automated approach for ontology development. Rather than starting from scratch, we build an ontology by effective discovering and use of the knowledge that is buried in the datasets to be integrated. The method is based on Formal Concept Analysis (FCA) (Ganter & Wille, 1999; Ganter, Stumme, & Wille, 2005), which is a mathematical approach for data analysis. FCA supports ontology development by abstracting conceptual structures from attribute-based object descriptions, and it enables considerable ontology development activities automated.Our research extends classical FCA theory to support ontology development for integrating datasets that exhibit implicit and ambiguous information. Implicit information is caused by the fact that some organisations tend to take some domain knowledge as granted, and do not explicitly specify it in their design documents or datasets. This can lead to an ontology that is ill-formed, and does not correctly capture critical concepts and the semantics of the domain. Ambiguous information is due to the fact that organisations differ from each other in culture, conventions and requirements in system development, hence they may vary in how they choose to represent a business object, and at what levels of granularity such information is encoded. This causes inconsistencies between the datasets of different organisations.We consider that overcoming this implicity and ambiguity is an important step in ontology development. The work reported here is a follow on research of Beck et al. (2013), Fu and Cohn (2008a) and Fu and Cohn (2008b). In this paper we report further technical advances we have made. To restore implicit information, we introduce a rule based method. We discuss how rules are derived and deployed for recovering implicit information. To resolve disambiguate information, we define a set of primitive operations to deal with simple matches in data alignment. These operations are then composed to deal with more complicated matches. Finally, we report on our experiments that are carried out to construct an ontology for integrating non-trivial datasets from several UK water companies. We measure the quality of the developed ontology by utilising the metrics of classical information theory and also in terms of its fitness to the application domain. Our experimental results demonstrate that techniques described in this paper provide an effective mechanism for reconciling and harmonising heterogeneous data from disparate sources, and they support development of ontologies that better fit and respect the underlying knowledge structures of domains.The remaining part of the paper is organised as follows. Section 2 reviews related research. Section 3 recalls relevant notions of FCA and briefs our framework for ontology development. Sections 4 and 5 present techniques that deal with implicit and ambiguous information. Section 6 discusses how to derive an ontology by using results generated from Sections 4 and 5. Section 7 reports our experimental results. Section 8 concludes the paper and suggests future research.Several areas of research are interesting to this work. Firstly, integration techniques investigated in database and information integration are quite relevant. Various topics have been studied by these communities and the ones that are the most interesting here are mapping discovery and schema integration, and techniques have been developed to support these (Bahga & Madisetti, 2015; Do & Rahm, 2002; Doan et al., 2004; Lenzerini, 2002; Liu & Zhang, 2014; Madhavan & Halevy, 2003; Pedersen, Pedersen, & Riis, 2013; Rahm & Bernstein, 2001). Mapping discovery takes two or more database schemas as input and produces a mapping between elements of the input schemas that correspond semantically to each other. Many of the early as well as current mapping solutions employ hand-crafted rules or heuristics to match schemas (Madhavan, Bernstein, & Rahm, 2001; Rahm & Bernstein, 2001). Examples of such heuristics include linguistic matching of schema element names, detecting similarity of structures of schema elements, and considering the patterns in relationships of the schema elements. Techniques have also been proposed to use learning based methods (Doan, Domingos, & Halevy, 2001; Neumann, Ho, Tian, Haas, & Meggido, 2002).Schema integration constructs a global schema based on the inter-schema relationships produced in mapping discovery. Each mapping element is analysed to decide which representation of related elements should be included in the global schema. When a mapping describes the corresponding schema elements as identical, their integration is straightforward—simply includes one of schema elements into the global schema. More frequently, the corresponding schema elements are not the same but are mutually related by some semantic properties, and schema merging is performed manually or semi-automatically with the assistance of domain engineers to guide the designers in their resolution.Ontology research is another discipline that deals with data integration. A common definition of an ontology is that it is a formal, explicit specification of a domain of discourse (Gruber, 1993). As it provides a shared understanding and explicit specification of a domain, an ontology is considered to have a key role to play in data integration (Bakhtouchi, Bellatreche, & Ait-Ameur, 2011; Bian, Zhang, & Peng, 2011; Noy, 2004; Uschold & Grüninger, 2004; Yu et al., 2012). Unfortunately, for many domains one faces the need to develop ontologies from scratch (as there is no existing ontology that can be used readily), and a growing number of methods have been proposed in recent years to address the issues of ontology design and development. Most methods are based on the traditional knowledge engineering approach (Brockmans et al., 2006; Pinto & Martins, 2004; Sure, Tempich, & Vrandecic, 2006). These methods usually start with defining the domain and scope of ontologies. This is followed by a data acquisition process: important concepts are collected; a concept hierarchy is derived, and properties and semantic constraints are attached to concepts.As developing ontologies from scratch is an expensive process to perform, there has been increasing interest in reusing or merging existing ontologies (or other knowledge structures such as thesauri) that are developed independently in different applications (Duong, Truong, & Nguyen, 2012; Truong & Nguyen, 2012; Xie, Liu, & Guan, 2011; Yang, 2011). Central to these studies is research on ontology mapping and ontology integration. Approaches to ontology mapping are similar to ones for matching database schemas and other structured data, and they use lexical and structural components of definitions to find correspondences. However, as an ontology captures richer data semantics than traditional database schemas, the methods for finding mappings tend to exploit these extra data semantics (Kalfoglou & Schorlemmer, 2003; Nguyen, 2007; Rodriguez & Egenhofer, 2003; Truong & Nguyen, 2012). For example, in Noy and Musen (2000) a tool has been developed to use linguistic similarity matches between concepts for initiating mappings, and then use the underlying ontological structures (classes, slots, facets) to suggest a set of heuristics for identifying further matches between the ontologies. In Duong and Jo (2012), a method has been proposed to mapping ontological concepts using propagating Priorly Matchable Concepts. The method exploits information such as concept types, relations and constraints to provide suggestions for possible concept matches. The method guilds on how to priorly check the similarity between concepts and it reduces computational complexity by avoiding checking similarity among unmatchable concepts. In Nguyen (2006), an approach has been proposed to resolve three levels of ontology conflicts: instant level, concept level and relation level, using consensus method. The techniques developed in Doan, Madhavan, Domingos, & Halevy (2003) and Spohr, Hollink, and Cimiano (2011) employs learning based techniques to find ontology mappings. They exploit information in data instances and taxonomic structure of ontologies, and then uses a probabilistic model to combine results of different learners.Based on the inter-ontology mappings derived in mapping discovery, a merging process integrates the source ontologies and generates a global ontology. However, deriving a meaningful ontology is a hard problem even with the ground set of inter-ontology mappings provided, and most methods that support the merging process are performed in an interactive manner with the assistance of human users, as is done in database and information integration research.Another branch of research studies ontology development and integration with formal methods. Of particular interest here is research based on Formal Concept Analysis (FCA) (Ganter & Wille, 1999; Ganter et al., 2005; Wille, 1982). FCA is a formal method for concept classification and conceptual structure derivation. FCA related tools enable considerable knowledge processing activities to be automated, particularly concept generation and hierarchy derivation. As a result, FCA has been attracting great interest to support systematic, semi-automated development and integration of ontologies (Bai & Zhou, 2011; Formica, 2006; He & Wang, 2011; Nanda, Simpson, Kumara, & Shooter, 2006; Xia, 2013). For example, inRouane, Valtchev, Sahraoui, & Huchard (2004) ontological hierarchy merging is studied in the framework of FCA by taking into account of both taxonomic and other semantic relationships of ontologies. A method FCA-MERGE has been developed in Stumme and Maedche (2001) to use FCA to support ontology integration. FCA_MERGE takes as input the two ontologies and a set of natural language documents, and computes a concept lattice from two source ontologies using FCA techniques. The concept lattice is then exploited by domain experts to derive a merged ontology. In Zhao, Wang, and Halang (2006) a similarity method has been introduced to map ontology concepts basing on Rough Set and Formal Concept Analysis theory. The idea is to construct from two source ontologies a concept lattice with FCA and similarity measure of two concepts are then computed using Rough Set theory. In Chen, Bau, and Yeh (2011) authors proposed a method that combines WordNet and Fuzzy Formal Concept Analysis techniques for merging ontologies. WordNet is firstly used to align concepts from a source ontology to concepts in a base ontology, and the remaining unmapped concepts are then aligned to the base ontology using a similarity measure based on fuzzy FCA.Our approach is in line with FCA based research. Yet it differs from previous studies in several aspects. Firstly, while most research focusing on similarity measure of ontology concepts, we contribute an integrated framework that offers a structural and systematic description of ontology merging process. Secondly, with FCA as backbone we investigate how to resolve implicit and ambiguous information. Previous research is either implicit on how these problems are resolved, or only address particular types of these problems. For example, in Rouane et al. (2004) there is an interesting discussion on attribute conflicts, but the authors do not address in detail how these problems are resolved. Thirdly, while most previous research considers one to one mapping between concepts, our method is able to deal with more complicated issues, i.e., an ontology concept may have multi-mappings from another ontology, which has not been investigated sufficiently in literature. Finally, we applied the proposed techniques to non-trivial industrial datasets, and examined how effectively the proposed method can help with improving data interoperability. This has rarely been reported in other FCA-based works.In this section, we introduce the basic concepts of FCA and brief our framework for ontology development. We will use data and examples from water infrastructure domain to present techniques developed in this research.FCA theory was developed in Wille (1982) and a typical task that FCA can perform is data analysis, making the conceptual structure of the data visible and accessible (Ganter & Wille, 1999; Ganter et al., 2005). Central to FCA is the notion of formal context, which is defined as a triple K:=〈G, M, I〉, where G is a set of objects, M is a set of attributes, and I⊆GXM is a binary relation between G and M. A relation 〈g, m〉 ∈ I is read as “object g has the attribute m”. A formal context can be depicted by a cross table as shown in Fig. 1(a), where the elements on the left side are objects; the elements at the top are attributes; and the relations between them are represented by the crosses.A formal concept of a context K:= 〈G, M, I〉 is defined as pair (A, B), where A⊆G, B⊆M, A´= B and B´=A. A´ is the set of attributes common to all the objects in A and B´ is the set of objects having the attributes in B. The extent of the concept (A, B) is A and its intent is B. The formal concepts of a context are ordered by the sub- and super-concept relations. The set of all formal concepts ordered by sub- and super-concept relations forms a concept lattice. Fig. 1(b) shows the concept lattice for the context in Fig. 1(a), where a node represents a concept labelled with its intensional and extensional description. The links represent the sub- and super-concept relations.The formal contexts introduced above are not the ones that occur most frequently in applications of FCA. Most often data is encoded in many valued contexts. A many valued context K:= 〈G, M, W, I〉 consists of a set of objects G, a set of attributes M, a set of attribute values W, and a set of ternary relations I⊆G × MXW. A relation <g, m, w> ∈ I is read as “object g has the attribute m and its value is w”. Fig. 2(a) shows a many valued context which lists different water pipes having different attribute values. In order for FCA theory to be applied to a many valued context, it needs to be unfolded into a one valued context through conceptual scaling (Ganter & Wille, 1999). Fig. 2(b) shows the one valued context for the many valued context in Fig. 2(a) after conceptual scaling.As the extent and intent of a concept overlaps with those of its super- and sub-concepts, redundancy exists in a concept lattice. To prevent this, reduced labelling is introduced. A lattice with reduced labelling is obtained by replacing each concept (A,B) with (N(A),N(B)), where N(A) contains the non-redundant elements in A, and N(B) contains the non-redundant elements in B. An object o will appear in N(A) if the corresponding concept is the greatest lower bound of all concepts containing o. An attribute a will appear in N(B) if the corresponding concept is the least upper bound of all concepts containing a. Fig. 3(a) shows the lattice derived from the one in Fig. 1(b) with reduced labelling. Furthermore we can eliminate in a lattice the concepts which do not possess their own attributes or objects. This leads to a structure called a Galois Sub Hierarchy (GSH). A GSH only consists of so called attribute concepts and object concepts. An object concept represents the smallest concept with this object in its extension, and an attribute concept represents the largest concept with this attribute in its intension. The GSH of the lattice in Fig. 3(a) is depicted in Fig. 3(b), where concepts 1, 5, 8 and 12 are removed due to the empty N(A) and N(B). Concept 2 is an attribute concept and concept 9 is an object concept.With the FCA theory as the backbone, we have developed a framework to support ontology development. The framework essentially consists of three components: Context Formation, Context Composition and Ontology Derivation, as illustrated in Fig. 4. To generate an integrated ontology for two datasets, Context Formation takes the datasets as inputs and generates a one valued context for each of them. The generated contexts are then fed to Context Composition to produce an integrated GSH. Ontology Derivation takes the GSH generated in Context Composition and generates an integrated ontology as well as concept mappings between two datasets. We will describe Context Formation in Section 4, and elaborate on Context Composition and Ontology Derivation in Sections 5 and 6.Fig. 5shows the components of Context formation. Given a dataset, Data Acquisition derives concepts encoded in the dataset as well as their attribute definitions, and the result is a many valued context for the dataset. The component looks at sources where various feature types (concepts) and their definitions can be extracted. The most common sources here are text/web documents created by system designers/developers for specifying system requirements and design. Other important sources are conceptual/logical data models of the concerned dataset. The generated context is then fed to the Information Explication component to restore implicit information. The component Conceptual Scaling transforms a many valued context into a one valued context, in order for classic FCA techniques to be applicable.The main challenge here is to deal with implicit information. Implicit information is caused by several factors. As an example in water infrastructure domain, when defining a feature type, organisations tend to explicitly state specific properties, but leave common ones unarticulated in their design documents. For instance, a sewer pipe is characterised by how it conveys sewage: either by gravity or by pressure, with the gravity distribution employed more often than the pressurised form. Most water companies explicitly specify the pressurised characteristic of a sewer pipe, but not the gravity one. Furthermore, many organisations take some domain knowledge as granted, and do not encode it explicitly. For example, a sludge sewer is usually pressurised rather than gravity. As this is well understood in the domain, many water companies choose not to encode this information explicitly. Table 1shows a portion of a many valued context that is generated for a sewerage dataset, where many blank cells exist due to implicit or unarticulated domain knowledge.The main consequence of this is that it can lead to an ontology that is ill-formed, and does not correctly capture critical concepts and semantics of the domain. Fig. 6shows the GSH for the context in Table 1. Due to implicit information, many important concepts, such as gravity sewer and underground sewer, are missing from the hierarchy and therefore from the resultant ontology. Furthermore, different organisations may choose what not to articulate in their datasets. We believe this hidden knowledge is one of main reasons that hinder data compatibility or interoperability across organisations.We classify implicit information into two groups: attribute-specific and object-specific. Attribute-specific implicit information is concerned with a particular attribute, and is applicable to all objects having that attribute. Object-specific implicit information is concerned with an attribute of particular objects only. An example of former is with the how attribute in Table 1. The unarticulated domain knowledge here is that a sewer pipe carries sewage by gravity if not explicitly specified, and this applies to all sewerage pipes having how attribute. An example of object-specific implicit information is with the how attribute of pipeType3. The implicit information here is that if a pipe carries sludge sewage, by default it carries it by pressure. This is relevant to the how attribute, but applies to pipeType3 only (pipes that carry sludge) and therefore is classified as object-specific implicit information.We use a rule based approach to recover implicit information. As implicit information is largely unarticulated domain knowledge, we need to work closely with domain experts to acquire these rules. We have two types of rules, attribute rules dealing with attribute-specific implicit information, and object rules dealing with object-specific implicit information.To elicit attribute rules, we iterate each attribute. An attribute has implicit information if it has missing values for some objects. Each attribute with implicit information in a context table incurs a rule. Involvement of domain experts is required at this point to generate such a rule. For example, Rule 1 in Fig. 7is collected for the how attribute in Table 1.To elicit object rules, we iterate each object in the context, and examine each of its attributes that do not have a value. If an attribute has implicit information which cannot be recovered with an attribute rule, an object rule is elicited to recover implicit information with the help of domain experts. For example, for object pipeType3, the attribute how has implicit information. As the implicit information for how in this case is pressurised, it cannot be recovered with Rule 1 discussed above. An object rule, Rule 4 in Fig. 7, is acquired in this case for pipeType3.Fig. 7 shows a set of rules elicited for the context table in Table 1, where Rule 1, 2 and 3 are attribute rules. Rule 4 is an object rule, which works for the how attribute of the object pipeType3 only.This step is concerned with how a context table can be manipulated to restore implicit information. To recover implicit information for an object, we first identify a set of rules applicable to it. This includes all relevant attribute rules and object rules for the object. Each attribute of the object is examined to see if it has implicit information. If the answer is yes, the relevant attribute rule is identified. The identification of an object rule is straightforward as it is linked to the concerned object directly. For an object, if both an attribute rule and an object rule are identified as relevant to an attribute, the object rule overrides the attribute rule when restoring implicit information. For example, for PipeType3 (in Table 1), both Rule 1 and 4 (in Fig. 7) deal with how attribute, but only Rule 4 is applied when restoring implicit information for how of this object.Once applicable rules have been identified, we generate new objects by applying different combination of the rules. This allows objects with different combination of attributes to be identified. Each derived object retains the existing object attribute relationships of the original object and derives new ones (for attributes having missing values) by applying corresponding rules. For example, for sewerPipeType1, there are two attributes that have implicit information, what and location. Accordingly, two attribute rules are identified: Rule 1 for what attribute and Rule 2 for location attribute. There is no object rule identified for pipeType1. By applying different combination of the rules, three new objects are derived from pipeType1, pipeType1_object1 by applying Rule 1, pipeType1_object2 by applying Rule 2, and pipeType1_object3 by applying Rule 1 and 2. All new objects retain existing object attribute relationships of pipeType1, and with different relationships derived due to the different rules applied. Depending on the number of rules applicable, each original context object derives different number of new objects. For example, there are 2 applicable rules for PipeType1, PipeType2 and PipeType3. The combination of these rules generated 3 derived objects for each original object. PipeType4 has 3 applicable rules and 7 new objects have been derived.Table 2lists the many valued context after implicit information has been restored with rules. This many valued context is then fed to Conceptual Scaling component (as shown in Fig. 6) to generate a one valued context table. Table 3lists the one valued context table after the conceptual scaling of the context in Table 2.Context composition takes two formal contexts as input, and generates an integrated GSH. The main components of Context Composition are Context Integration and Hierarchy Generation, as shown in Fig. 8.The main challenge here is to deal with ambiguous information during context integration, i.e., different terms may be employed to refer to the same attribute, and attributes may be modelled at different levels of granularity. An example here is that one dataset may model a sewerage pipe as either main or lateral and another may classify it as trunk main, non-trunk main, or private pipe. Attribute disambiguation is a process to match attributes from different datasets. In this research we use a pre-defined data dictionary developed in Fu and Cohn (2008a) to disambiguate attributes. The data dictionary maintains a set of terms that describe concepts in a domain, as well as their terminological relationships, e.g. BT/NT (Broader/Narrower Term) etc. Using the data dictionary, we can decide semantic relationships of two attributes. In what follows, we will use the context tables K1 and K2 shown in Table 4and 5to illustrate the context integration process.Given two contextsK1:=〈G1,M1,I1〉andK2:=〈G2,M2,I2〉, the integrated contextK:=〈G,M,I〉is computed by first performing a disjoint union of object sets of two contexts, that is,(1)G=G1∪*G2M and I are assigned M1 and I1 from K1 at this stage, i.e,M=M1andI=I1. Table 6shows the context K after the above operations.The next step identifies the semantic relationship between an attribute in M2 and an attribute in M. For each attribute Ai∈ M2 of K2, we perform a semantic mapping operation with attributes in K. Based on the mapping identified for Ai, we derive the new attributes and relationships to be added to the context table K. We use attExt to denote the set of attributes to be added to M, and use relExt to denote the set of relationships to be added to I. After each round of mapping, the attribute set M and relationship set I of K are calculated as:(2)M=M∪attExt(3)I=I∪relExtAn attribute could find mappings of different types, including 1 to 1 mapping, or 1 to many mappings, and accordingly different operations for context table manipulation. In what follows, we first describe primitive operations, which deal with 1 to 1 mappings, and we will then discuss how the primitive operations can be composed to deal with 1 to many mappings.For a given attribute Ai∈ M2 of K2, four types of mapping can be identified from M of K. Table 7summarises types of mapping, as well as attributes (i.e. attExt) and relationships (i.e. relExt) to be added to K for each mapping type, where Ajdenotes the match from M of K.IAifinds an equivalent attribute Aj∈ M of K. In this case, Aiwill be unified with Aj. The context table K is expanded with relationships between Ajand objects that have relationships with Aiin K2. For example for the context K2 in Table 5, the attribute live finds an equivalent attribute operational in K. They are unified as operational in K, i.e.,attExt=Φ(no new attribute to be added to K). Since there exists a relationship <K2O2, live> in K2, new relationships are established in K, i.e., relExt={<K2O2, operational>}. Similarly we found an equivalent match main in K for attribute main in K2. The resultant context table is shown in Table 8, where the newly added relationships are shaded for the purpose of readability.Aifinds a match Ajthat is more generic to it. In this case, the resulting context K is expanded with attribute Aiand relationships between Aiand objects from K2. New relationships are established in K between those objects having attribute Aiand attribute Aj. The theory is that if Aiis a more specific feature of Aj, then any object which has attribute Aishould also have attribute Aj. For example, the closest match for abandoned destroyed in K2 is abandoned which is a broader term to it. Then attribute abandoned destroyed is added to K, and following two relationships are added to K:<K2O1,abandoneddestroyed><K2O1,abandoned>where the first is originated from K2 due to the existence of <K2O1, abandoned destroyed> in K2. The second is derived due to the fact that abandoned destroyed is a more specific feature to abandoned, and therefore the existence of <K2O1, abandoned destroyed> derives <K2O1, abandoned>. The result of these context extensions is highlighted in Table 9.Aifinds a match Ajthat is more specific to it. In this case the context K is expanded with Aiand existing relationships between Aiand objects from K2. New binary relationships are established in K between those objects having relationships with Aj(originally from K1) and attribute Ai(which is originally from K2). The theory is that if Aiis a more generic feature of Aj, then any object which has attribute Ajshould also have attribute Ai. For example, if the closest match for the attribute proposed in K2 is proposed recommission in K which is a narrower term to it, then attribute proposedis added to K. The following two relationships are added to K,<K2O3,proposed><K1O2,proposed>where the first is originated from K2 due to the existence of <K2O3,proposed> in K2. The second is established due to the existence of <K1O2,proposed recommission> in K as well as the fact that proposed is a more generic feature to proposed recommission. The result of this mapping operation is shown in Table 10.Aifinds no match in K. In this case the context K is simply expanded with Aiand existing relationships between Aiand objects originating from K2. For example there is no semantic match in K for the attribute standby of K2. In this case, K is extended with attExt ={standby} and relExt={<K2O4,standby>}, as shown in Table 11.In many situations, an attribute may have multiple matches and each match is of different type, e.g. having both an equivalent and a broader match at same time. The primitive operations discussed above can be composed to deal with these complex cases. For an attribute A ∈ M2 of K2, if a set of matches {A1, A2, …, An} are identified from M of K, the context K is extended as follows:(4)M=M∪j=1nattExtAj(5)I=I∪j=1nrelExtAjwhereattExtAjandrelExtAjrespectively denote the attribute and relationship sets that are derived when A is matched to Ajwith the primitive operations discussed in Section 5.1.For example, for the attribute abandoned intact, two matches are found from K, the equivalent match abandoned intact and the generic match abandoned. For equivalent match abandoned intact, the following are generated:attExt=ΦrelExt={<K2O5,abandonedintact>}For the generic match abandoned, the following are generated byattExt={abandonedintact}relExt={<K2O5,abandonedintact>,<K2O5,abandoned>}Adding these into K results in the formal context shown in Table 12, which is also final integrated context table. The GSH constructed from this integrated context is illustrated in Fig. 9.Ontology derivation component of our framework takes the GSH generated in Section 5 and generates an ontological structure11The term ontological structure is used here to mean that the derived conceptual structure only contains limited data semantics, i.e. only concepts, attributes and is-a relationships are identified. Further development is still required to capture other data semantics to generate a full ontology.. Fig. 10shows the components of ontology derivation. The GSH is exploited to derive several types of information, including ontological concepts, subsumption relationships between concepts, and attributes of concepts. The information identified forms an ontological structure from which a full ontology can be developed. The mapping between concepts of different datasets can also be identified from the GSH.This subcomponent derives mappings between concepts of two datasets. Given a formal concept in a GSH, if its extent contains more than one objects (e.g. the extent of node 3 is {K1O1, K2O2} in Fig. 9), then it indicates a potential mapping between these source concepts. The validation of domain engineers is requested at the evaluation stage to judge whether a mapping identified is correct. If the answer is negative, features need to be identified to differentiate one concept from another. This often involves the identification of new attributes or relationships of concerned concepts. The existence of incorrect matches triggers the need to iterate context composition or integration operations.As we employ a GSH in the research, intermediate, abstract concepts are reduced in the context integration step and the resulting hierarchy consists only of object concepts and attribute concepts. Object concepts have to be kept in the resultant ontological hierarchy as they correspond to the initial concepts (either explicit or implicit) of datasets and therefore need to remain in ontological structure to respect the initial class specification of the datasets. For an attribute concept, the assistance of domain engineers is required to decide whether it should be kept or discarded by taking into account its significance or interest to the application. When an attribute concept is discarded in a GSH, all elements in its intent are passed on to its sub concepts, and super-/sub-concept relationships are established between its super-concepts and sub-concepts.After a decision has been made on which concepts are to be kept in the resultant ontology, the rules for identifying relationships and attributes of a concept are straightforward:•All elements in the intent of a formal concept are declared as attributes of the ontological concept.Sub/super relations between two formal concepts are identified as is-a relationships between the corresponding ontological concepts.An evaluation of the proposed techniques has been performed on several industrial datasets. We first describe the experimental setup and the ontology similarity measures employed in the evaluation. We then report on the evaluation results.Datasets we used for performing our experiments were sourced from four UK water companies. These datasets essentially encode same types of information, including various water pipes, metering and treatment facilitates for transporting freshwater/wastewater for customers across the UK. However each organisation records its information with little thought towards interoperability with others. This results in data heterogeneities. Due to data confidentiality agreement we have with our industrial partners, we cannot publish these datasets. Nevertheless we have list in Table 13the statistics on the datasets.The mapping and integration was carried out in a semi-automated manner, where data acquisition and attribute disambiguation were conducted manually, the open source tool Galicia (Valtchev et al., 2003) was employed for context manipulation and GSH generation, and all other processes such as information explication and conceptual scaling were completed with Java and SQL codes. The evaluation was performed in three phases.•Phase I experiments constructed local ontologies for each dataset involved. Pairwise comparison was conducted to measure the similarity of these local ontologies, and the results were then served as benchmarks for the subsequent evaluation.Phase II experiments studied how implicit information impacts on ontology interoperability, and demonstrated how information explication can help with ontology alignment.Phase III compared an ontology developed in this research with a handcrafted ontology developed with traditional knowledge engineering approach. The performance of two ontologies was evaluated by studying how best the two ontologies fit and respect the knowledge structures of datasets to be integrated.Evaluation was performed at 2 levels: lexical level and taxonomic level. Lexical level evaluation reflects how well the lexical terms of a source ontology cover those of a target ontology. Taxonomic level evaluation examines how well the conceptual hierarchy of a source ontology resembles that of a target ontology. We employ the ontology measures proposed in Dellschaft and Staab (2006) and Maedche and Staab (2002) in our experiments. Lexical precision and recall of a source ontology OSagainst a target Ontology OTare computed as:(6)LP(OS,OT)=|CS∩CT||CS|(7)LR(OS,OT)=|CS∩CT||CT|where CS(or CT) is the set of terms describing concepts in Os(or OT).Lexical F-measure, LF, is used for balancing the precision and recall values, and is calculated as harmonic mean of LP and LR.(8)LF=2·LP(OS,OT)·LR(OS,OT)LP(OS,OT)+LR(OS,OT)Taxonomic level measures are divided into local and global measures. Local measures compare the similarity of hierarchical positions of two concepts in the source and the target ontologies. For local taxonomic precision, the similarity of two concepts is computed based on the common semantic cotopies from the concept hierarchies. The common semantic cotopies includes all the common super- and sub-concepts of a concept pair. Given such a semantic cotopy ce, the local taxonomic precision tp and recall tr of two concepts c1 ∈ CSand c2 ∈ CTis defined as(9)tp(c1,c2,OS,OT)=|ce(c1,OS)∩ce(c2,OT)|ce(c1,OS)|(10)tr(c1,c2,OS,OT)=|ce(c1,OS)∩ce(c2,OT)|ce(c2,OT)|Sincetp(c2,c1,OT,OS)=|ce(c1,OS)∩ce(c2,OT)|ce(c2,OT)|, we have(11)tr(c1,c2,OS,OT)=tp(c2,c1,OT,OS)Global taxonomic precision and recall are defined by summing up local taxonomic precision and recall of common concepts in two ontologies.(12)TP(OS,OT)=1|CS∩CT|∑c∈(CS∩CT)tp(c,c,OS,OT)(13)TR(OS,OT)=1|CS∩CT|∑c∈(CS∩CT)tr(c,c,OS,OT)SinceTP(OT,OS)=1|CS∩CT|∑c∈(CS∩CT)tp(c,c,OT,OS)andtp(c,c,OT,OS)=tr(c,c,OS,OT)due to Eq. (11), we have(14)TR(OS,OT)=TP(OT,OS)Taxonomic F-measure TF is used to balance TP and TR to generate a combined taxonomic measure.(15)TF(OS,OT)=2*TP(OS,OT)*TR(OS,OT)TP(OS,OT)+TR(OS,OT)A combined measure GF, which balances the lexical and taxonomic measures, is used to give a summarising overview of the similarity of OSagainst OT, and is computed as the harmonic mean of LF and TF:(16)GF(OS,OT)=2*LF(OS,OT)*TF(OS,OT)LF(OS,OT)+TF(OS,OT)

@&#CONCLUSIONS@&#
The availability of vast quantities of data presents organisations with both opportunities and challenges. Data integration techniques offer a promising way for addressing the issue of data heterogeneities and promoting data sharing and interoperability across organisations. In this paper we present a formal and semi-automated method for ontology development, with the aim to reconcile heterogeneous data and support data integration. The research extends classical FCA theory to address the issues of implicit and ambiguous information, which, we consider, are important but have not been sufficiently investigated by previous studies. The research enables considerable ontology engineering activities automated, including concept derivation and hierarchy generation. In contrast to studies that draw upon either small or simplified datasets, we evaluate the proposed techniques on non-trivial industrial datasets. Our experimental results demonstrate the techniques described in this paper can help curate and fuse data from disperse sources, and support the development of ontologies that better fits and respects the underlying knowledge structure of domain. There are a number of works which we plan to undertake in the future, including developing techniques to deal with incomplete information in data integration, and validating the proposed techniques on datasets in other application domains.