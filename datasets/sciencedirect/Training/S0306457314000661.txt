@&#MAIN-TITLE@&#
Signaling sarcasm: From hyperbole to hashtag

@&#HIGHLIGHTS@&#
The use of hashtags such as #sarcasm reduces the further use of linguistic markers of sarcasm in tweets.Hashtags such as #sarcasm appear to be the extralinguistic equivalent of non-verbal expressions in live interaction.Sarcastic hashtags are 90% appropriate.Sarcastic tweets without hashtags are hard to distinguish from non-sarcastic hyperbolic tweets.In French tweets, the hashtag #sarcasme conveys a polarity switch less frequently than in Dutch.

@&#KEYPHRASES@&#
Social media,Automatic sentiment analysis,Opinion mining,Sarcasm,Verbal irony,

@&#ABSTRACT@&#
To avoid a sarcastic message being understood in its unintended literal meaning, in microtexts such as messages on Twitter.com sarcasm is often explicitly marked with a hashtag such as ‘#sarcasm’. We collected a training corpus of about 406 thousand Dutch tweets with hashtag synonyms denoting sarcasm. Assuming that the human labeling is correct (annotation of a sample indicates that about 90% of these tweets are indeed sarcastic), we train a machine learning classifier on the harvested examples, and apply it to a sample of a day’s stream of 2.25 million Dutch tweets. Of the 353 explicitly marked tweets on this day, we detect 309 (87%) with the hashtag removed. We annotate the top of the ranked list of tweets most likely to be sarcastic that do not have the explicit hashtag. 35% of the top-250 ranked tweets are indeed sarcastic. Analysis indicates that the use of hashtags reduces the further use of linguistic markers for signaling sarcasm, such as exclamations and intensifiers. We hypothesize that explicit markers such as hashtags are the digital extralinguistic equivalent of non-verbal expressions that people employ in live interaction when conveying sarcasm. Checking the consistency of our finding in a language from another language family, we observe that in French the hashtag ‘#sarcasme’ has a similar polarity switching function, be it to a lesser extent.

@&#INTRODUCTION@&#
In the general area of sentiment analysis, sarcasm is a disruptive factor that causes the polarity of a message to flip. Unlike a simple negation, a sarcastic message often conveys a negative opinion using only positive words – or even intensified, hyperbolic positive words. Likewise, but less frequently, sarcasm can flip the polarity of an opinion with negative words to the intended positive meaning. The detection of sarcasm is therefore important, if not crucial, for the development and refinement of sentiment analysis systems, but is at the same time a serious conceptual and technical challenge.In this article we introduce a sarcasm detection system for tweets, messages on the microblogging service offered by Twitter.1http://www.twitter.com.1In doing this we are helped by the fact that sarcasm appears to be a commonly recognized concept by many Twitter users, who explicitly mark their sarcastic messages by using hashtags such as ‘#sarcasm’ or ‘#not’. Hashtags in tweets are explicitly marked keywords, and often act as categorical labels or metadata in addition to the body text of the tweet (Chang, 2010). By using the explicit hashtag any remaining doubt a reader may have is taken away: the message is not to be taken literally; it is sarcastic.While such hashtags primarily function as conversational markers of sarcasm, they can be leveraged as annotation labels in order to generate a model of sarcastic tweets from the text co-occurring with these hashtags. A clear advantage of this approach is the easy acquisition of a vast amount of training data. On the other hand, its performance is dependent on the correctness of two assumptions: first that users who include one of the selected hashtags in their tweet actually intended to convey sarcasm and indeed intended to flip the polarity of the message, and second that the pattern of sarcasm in a tweet still holds when the hashtag is excluded from it as a training label. We set out to test these assumptions along with the quality of the resulting sarcasm detection system by applying it on a realistically large and unbiased sample of tweets (of which the vast majority is non-sarcastic) posted on the same day.The hashtag as a marker of sarcasm has been leveraged in previous research to detect sarcasm in tweets (González-Ibánez, Muresan, & Wacholder, 2011; Reyes, Rosso, & Buscaldi, 2012). One contribution of this paper to the existing body of work is that a sarcasm classifier is trained on several markers of sarcasm in tandem, the most frequent being ‘#not’, and performance is assessed on a realistically large and unbiased sample of tweets. Furthermore, we provide insight into the role of hyperbole in sarcastic tweets, and we perform a cross-lingual comparison of the use of sarcasm in Twitter by annotating French tweets ending with ‘#sarcasme’.This paper is structured as follows. In Section 1.1 we discuss the concepts of sarcasm, the broader category of verbal irony, and their communicative function according to the literature. In Section 2 we offer a brief survey of related work on the development of automatic detectors of polarity in social media. Our experimental setup is described in Section 3. We report on the results of our experiments in Section 4 and analyse our results in view of the theoretical work discussed earlier in Section 5. We summarize our results, draw conclusions, and identify points for future research in Section 6.Twitter members mark their sarcastic messages with different hashtags. As described in more detail in Section 3.1, we find that four words tend to be used as hashmarks in sarcastic posts: ‘#sarcasm’, ‘#irony’, ‘#cynicism’ and ‘#not’. Although sarcasm, irony and cynicism are not synonymous, they have much in common. This is especially true for sarcasm and irony; many researchers treat those phenomena as strongly related (Attardo, 2007; Brown, 1980; Gibbs & O’Brien, 1991; Kreuz & Roberts, 1993; Mizzau, 1984; Muecke, 1969), and sometimes even equate the terms in their studies in order to work with a usable definition (Grice, 1978; Tsur, Davidov, & Rappoport, 2010). Cynicism is more mocking and tells us more about human beliefs than irony and sarcasm (Eisinger, 2000), but there is a close correlation between these concepts (Yoos, 1985). The hashtag ‘#not’ is not the name of a rhetorical device or trope such as sarcasm, irony and cynicism, but it is a conventionally used meta-communication marker to indicate the message contains a shift in evaluative valence.In psycholinguistics and cognitive linguistics sarcasm has been widely studied, often in relation with concepts such as cynicism, and with verbal irony as a broader category term. A brief overview of definitions, hypotheses and findings from communication studies regarding sarcasm and related concepts may help clarify what the hashtags convey.In this study, we are interested in sarcasm as a linguistic phenomenon, and how we can detect it in social media messages. Yet, Brown (1980) warns that sarcasm ‘is not a discrete logical or linguistic phenomenon’ (p. 111), while verbal irony is. Indeed, Reyes and Rosso (2012) see sarcasm ‘as specific extension[s] of a general concept of irony’ (p. 755). In line with the extensive use of #sarcasm in tweets to mark verbal irony, we take the liberty of using the term sarcasm while verbal irony would be the more appropriate term. Even then, according to Gibbs and Colston (2007) the definition of verbal irony is still a ‘problem that surfaces in the irony literature’ (p. 584).There are many different theoretical approaches to verbal irony. It should (a) be evaluative, (b) be based on incongruence of the ironic utterance with the co-text or context, (c) be based on a reversal of valence between the literal and intended meaning, (d) be aimed at some target, and (e) be relevant to the communicative situation in some way (Burgers, Van Mulken, & Schellens, 2011). Although it is known that irony is always directed at someone or something (the sender himself, the addressee, a third party, or a combination of the three, see Burgers et al. (2011) and Livnat (2004)) and irony is used relatively often in dialogic interaction (Gibbs, 2007), these two elements of irony are hardly examinable in the case of Twitter: the context of the Twitter messages is missing and it is inconvenient to investigate interaction. Therefore, it is hard to interpret the communicative situation and the target of the message. However, it is possible to analyse texts, such as tweets, on their evaluative meaning and a potential valence shift in the same way as Burgers et al. (2011) did. Burgers et al.’s own definition of verbal irony is ‘an utterance with a literal evaluation that is implicitly contrary to its intended evaluation.’ (p. 190).Thus, a sarcastic utterance involves a shift in evaluative valence, which can go two ways: it could be a shift from a literally positive to an intended negative meaning, or a shift from a literally negative to an intended positive evaluation. Since Reyes, Rosso, and Veale (2013) also argue that users of social media often use irony in utterances that involve a shift in evaluative valence, we use the definition of verbal irony of Burgers et al. (2011) in this study on sarcasm, and we use both terms synonymously. The definition of irony as saying the opposite of what is meant is commonly used in previous corpus-analytic studies, and is reported to be reliable (Kreuz, Roberts, Johnson, & Bertus, 1996; Leigh, 1994; Srinarawat, 2005).In order to ensure that the addressees detect the sarcasm in the utterance, senders use markers in their utterances. Attardo (2000) states that those markers are clues a writer can give that ‘alert a reader to the fact that a sentence is ironical’ (p. 7). The use of markers in written and spoken interaction may be different (Jahandarie, 1999). In spoken interaction, sarcasm is often marked with a special intonation (Attardo, Eisterhold, Hay, & Poggi, 2003; Bryant & Tree, 2005; Rockwell, 2007), air quotes (Attardo, 2000) or an incongruent facial expression (Attardo et al., 2003; Muecke, 1978; Rockwell, 2003). In written communication, authors do not have such clues at their disposal. Since sarcasm is more difficult to comprehend than a literal utterance (Burgers, Van Mulken, & Schellens, 2012a; Gibbs, 1986; Giora, 2003), it is likely that addressees do not pick up on the sarcasm and interpret the utterances literally. To avoid misunderstandings, writers use linguistic markers for irony (Burgers, Van Mulken, & Schellens, 2012b): tropes (a metaphor, hyperbole, understatement or rhetorical question), schematic irony markers (repetition, echo, or change of register), morpho-syntactic irony markers (exclamations, interjections, or diminutives), or typographic irony markers (such as capitalization, quotation marks and emoticons). Thus, besides hashtags to mark the opposite valence of a tweet, Twitter members may also use linguistic markers. A machine-learning classifier that learns to detect sarcasm should in theory be able to discover at least some of the features that Burgers et al. (2012b) list, if given sufficient examples of all of them in a training phase. While metaphor and understatement may be too complex to discover, exclamations and typographical markers should be easy to learn. Hyperbole, or the ‘speaker overstating the magnitude of something’ (Colston, 2007, p. 194), may be discovered by the classifier by the fact that it is often linked to words that signal intensity, as we now analyse in more detail.Especially in the absence of visual markers, sarcastic utterances need strong linguistic markers to be perceived as sarcastic (Attardo et al., 2003), and hyperbole is often mentioned as a particularly strong marker (Kreuz & Roberts, 1995). It may be that a sarcastic utterance with a hyperbole (‘fantastic weather’) is identified as sarcastic with more ease than a sarcastic utterance without a hyperbole (‘the weather is good’). While both utterances convey a literally positive attitude towards the weather, the utterance with the hyperbolic ‘fantastic’ may be easier to interpret as sarcastic than the utterance with the non-hyperbolic ‘good’. Hyperbolic words carry intensity. Bowers (1964) defines language intensity as ‘the quality of language which indicates the degree to which the speaker’s attitude toward a concept deviates from neutrality’ (p. 416). According to Van Mulken and Schellens (2012), an intensifier is a linguistic element that can be removed or replaced while respecting the linguistic correctness of the sentence and context, but resulting in a weaker evaluation. Intensifiers, thus, strengthen an evaluative utterance and could make an utterance hyperbolic. Typical word classes of intensifiers used for hyperbolic expressions, inter alia, are adverbs (‘very’, ‘absolutely’) and adjectives (‘fantastic’ instead of ‘good’), in contrast to words which leave an evaluation unintensified, like ‘pretty’, ‘good’ and ‘nice’. According to Liebrecht (in preparation), typographical elements such as capitals and exclamation marks are also intensifying elements which can create hyperbolic utterances. So, there is an overlap between linguistic elements to intensify and linguistic elements to overstate utterances. It may be that senders use such elements in their tweets to make the utterance hyperbolic, in order to signal sarcasm.The automatic classification of communicative constructs in short texts has become a widely researched subject in recent years. Large amounts of opinions, status updates, and personal expressions are posted on social media platforms such as Twitter. The automatic labeling of their polarity (to what extent a text is positive or negative) can reveal, when aggregated or tracked over time, how the public in general thinks about certain things. See Montoyo, Martínez-Barco, and Balahur (2012) for an overview of recent research in sentiment analysis and opinion mining.A major obstacle for automatically determining the polarity of a (short) text are constructs in which the literal meaning of the text is not the intended meaning of the sender, as many systems for the detection of polarity primarily lean on positive and negative words as markers. The task to identify such constructs can improve polarity classification, and provide new insights into the relatively new genre of short messages and microtexts on social media. Previous works describe the classification of emotions (Davidov, Tsur, & Rappoport, 2010a; Mohammad, 2012), irony (Reyes et al., 2013), sarcasm (Davidov, Tsur, & Rappoport, 2010b; González-Ibánez et al., 2011; Tsur et al., 2010), satire (Burfoot & Baldwin, 2009), and humor (Reyes et al., 2012).Most common to our research are the works by Reyes et al. (2013), Tsur et al. (2010), Davidov et al. (2010b) and González-Ibánez et al. (2011). Reyes et al. (2013) collect a training corpus of ironic tweets labeled with the hashtag ‘#irony’, and train classifiers on different feature sets representing higher-level concepts such as unexpectedness, style, and emotions. The classifiers are trained to distinguish ‘#irony’-tweets from tweets containing the hashtags ‘#education’, ‘#humour’, or ‘#politics’, achieving F1-scores of around 70. Tsur et al. (2010) focus on product reviews on the World Wide Web, and try to identify sarcastic sentences from these in a semi-supervised fashion. Training data is collected by manually annotating sarcastic sentences, and retrieving additional training data based on the annotated sentences as queries. Sarcasm is annotated on a scale from 1 to 5. As features, Tsur et al. infer patterns from these sentences consisting of high-frequency words and content words. Their system achieves an F1-score of 79. Davidov et al. (2010b) apply a comparable system on a small set of tweets manually annotated on sarcasm, and achieve an F-score of 83. When testing the system on tweets marked with ‘#sarcasm’, the F-score drops to 55. They state that apart from indicating the tone of a tweet, ‘#sarcasm’ might be used as a search anchor and as a reference to a former sarcastic tweet, adding a fair amount of noise to the data. González-Ibánez et al. (2011) aim to distinguish sarcasm from literally positive and negative sentiments in tweets. Tweets belonging to all three categories were collected based on hashtags describing them (‘#sarcasm’ and ‘#sarcastic’ for sarcastic tweets) and tested through 5-fold cross-validation on a set comprising 900 tweets for each of the three categories. As features they make use of word unigrams and higher-level word categories. While the classifier achieves an accuracy of only 57%, it outperforms human judgement. González-Ibánez et al. (2011) conclude that the lack of context makes the detection of sarcasm in tweets difficult, both for humans and for machines.In the works described above, a system is tested in a controlled setting: Reyes et al. (2013) compare irony to a restricted set of other topics, Tsur et al. (2010) take from the unlabeled test set a sample of product reviews with 50% of the sentences classified as sarcastic, and González-Ibánez et al. (2011) train and test in the context of a small set of positive, negative and sarcastic tweets. In contrast, we apply a trained sarcasm detector to a real-world test set representing a realistically large sample of tweets posted on a random day, the vast majority of which is not sarcastic. Detecting sarcasm in social media is, arguably, a needle-in-a-haystack problem: of the 2.25 million tweets we gathered on a single day, 353 are explicitly marked with the #sarcasm or its pseudo-synonyms. It is therefore only reasonable to test a system in the context of a typical distribution of sarcasm in tweets.As argued in Section 1.1, while ‘#sarcasm’ (‘#sarcasme’ in Dutch – we use the English translations of our hashtags throughout this article) is the most obvious hashtag for sarcastic tweets, we base our training set on an expanded set of pseudo-synonymous hashtags. We also found empirical evidence that we need to expand the set of hashtags. Liebrecht, Kunneman, and Van den Bosch (2013) reported that training a classifier solely on ‘#sarcasm’ as a training label resulted in high weights for hashtags that have the same function as ‘#sarcasm’: to switch the evaluative valence or give a description of the type of tweet. While Qadir and Riloff (2013) expand sets of hashtags denoting the emotion of a tweet by bootstrapped learning, this approach does not seem appropriate for the more subtle rhetorical instrument of sarcasm. We decided to extract all hashtags from the ranked list of features from the (Liebrecht et al., 2013) study and manually examine the tweets accompanying them by means of twitter.com. From this examination, we selected the hashtags that almost unambiguously denoted sarcasm in a tweet in addition to ‘#sarcasm’: ‘#irony’, ‘#cynicism’, and ‘#not’. The former two denote tropes comparable to sarcasm, while the latter is also typically used to switch the evaluative valence of a message. Hashtags that only partly overlap in function such as ‘#joke’ or ‘#haha’ were not included due to their ambiguous usage (either shifting the evaluative valence of a message or simply denoting a funny tweet).For the collection of tweets we made use of a database provided by the Netherlands e-Science Centre consisting of IDs of a substantial portion of all Dutch tweets posted from December 2010 onwards (Tjong Kim Sang & Van den Bosch, 2013).2http://twiqs.nl/.2From this database, we collected all tweets that contained the Dutch versions of the selected hashtags ‘#sarcasm’, ‘#irony’, ‘#cynicism’, and ‘#not’ until January 31st 2013. This resulted in a set of 644,057 tweets in total. Following Mohammad (2012) and González-Ibánez et al. (2011), we cleaned up the dataset by only including tweets in which the given hashtag was placed at the end or exclusively followed by other hashtags or a url. Hashtags placed somewhere in the middle of a tweet are more likely to be a grammatical part of the sentence than a label (Davidov et al., 2010b), and may refer to only a part of the tweet. Additionally, we discarded re-tweets (repostings of an earlier tweet by someone else). Applying these filtering steps resulted in 406,439 tweets in total as training data. Table 1offers more details on the individual hashtags; ‘#not’ occurs a factor more frequently than ‘#sarcasm’, which in turn occurs a factor more frequently than ‘#irony’;‘#cynicism’ again occurs a factor less frequently.We trained a classifier on sarcastic tweets by contrasting them against a background corpus. For this, we took a sample of tweets in the period from October 2011 until September 2012 (not containing tweets with any of the sarcastic hashtags). To provide the classifier with an equal number of cases for the sarcasm and background categories and thus produce a training set without class skew, 406,439 tweets were selected randomly, equal to the amount of sarcastic tweets. Again, we did not include re-tweets in the sample.To test our classifier in a realistic setting, we collected a large sample of tweets posted on a single day outside the time frame from which the training set is collected, namely February 1, 2013. After removal of re-tweets, this set of tweets contains approximately 2.25 million tweets, of which 353 carry one of the sarcasm hashtags at the end.While the distribution of sarcastic versus other tweets on the test day is highly imbalanced, we chose not to copy this distribution to the training stage. As pointed out by Chawla, Japkowicz, and Kotcz (2004), in an imbalanced learning context classifiers tend to be overwhelmed by the large classes and ignore the small ones. To avoid the influence of class size, we decided to completely balance the tweets with and without ‘#sarcasm’ in the training set. This is likely to drive the classifier to overshoot its classification of tweets as sarcastic in the testset, but we consider only the top of its confidence-based ranking; in our evaluation of the ability of the classifier to detect sarcasm in tweets that lack an explicit hashtag, we evaluate the ranking of the classifier with precision atn<250.All collected tweets were tokenized.3Tokenization was carried out with Ucto, http://ilk.uvt.nl/ucto.3Punctuation, emoticons, and capitalization information were kept, as these may be used to signal sarcasm (Burgers et al., 2012b). We made use of word uni-, bi- and trigrams as features (including punctuation and emoticons as separate words). User names and URLs were normalized to ‘USER’ and ‘URL’ respectively. We removed features containing one of the hashtags from the training set. Finally, we removed terms that occurred three times or less or in two tweets or less.As classification algorithm we made use of Balanced Winnow (Littlestone, 1988) as implemented in the Linguistic Classification System.4http://www.phasar.cs.ru.nl/LCS/.4This algorithm is known to offer state-of-the-art results in text classification, and produces interpretable per-class weights that can be used to, for example, inspect the highest-ranking features for one class label. The α and β parameters were set to 1.05 and 0.95 respectively. The major threshold (θ+) and the minor threshold (θ-) were set to 2.5 and 0.5. The number of iterations was bounded to a maximum of three.

@&#CONCLUSIONS@&#
