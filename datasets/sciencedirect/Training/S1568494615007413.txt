@&#MAIN-TITLE@&#
A nonlinearly-activated neurodynamic model and its finite-time solution to equality-constrained quadratic optimization with nonstationary coefficients

@&#HIGHLIGHTS@&#
For the first time, a novel finite-time convergent neurodynamic model is proposed.The conventional GD model and the recently-proposed ZD model are developed.Our proposed neurodynamic model can outperform the existing neural dynamical models.

@&#KEYPHRASES@&#
ZD model,GD model,Finite-time convergence,Nonstationary quadratic optimization,Nonlinear activation function,

@&#ABSTRACT@&#
The recently-proposed Zhang dynamics (ZD) has been proven to achieve success for solving the linear-equality constrained time-varying quadratic program ideally when time goes to infinity. The convergence performance is a significant improvement, as compared to the gradient-based dynamics (GD) that cannot make the error converge to zero even after infinitely long time. However, this ZD model with the suggested activation functions cannot reach the theoretical time-varying solution in finite time, which may limit its applications in real-time calculation. Therefore, a nonlinearly-activated neurodynamic model is proposed and studied in this paper for real-time solution of the equality-constrained quadratic optimization with nonstationary coefficients. Compared with existing neurodynamic models (specifically the GD model and the ZD model) for optimization, the proposed neurodynamic model possesses the much superior convergence performance (i.e., finite-time convergence). Furthermore, the upper bound of the finite convergence time is derived analytically according to Lyapunov theory. Both theoretical and simulative results verify the efficacy and superior of the nonlinearly-activated neurodynamic model, as compared to these of the GD and ZD models.Quadratic optimization arises in diverse fields of science and engineering including communication processing [1], image processing [2], nonlinear control [3], motion planning and obstacle avoidance in robotics [4,5], etc. In addition, various realistic issues can be addressed by turning initial problems into quadratic optimization problems subject to equality constraints. For instance, the least square problem with linear-equality constraints can be regarded as a basic analytical form that is extensively exploited for system design and modeling [6,7]. It is evident that the quadratic optimization has great importance both from the mathematical and practical viewpoints. This is reflected in the large number of approaches that have been proposed for solving quadratic optimization problems in real-time [6–11].A well-accepted way for real-time solution of quadratic optimization problems is iterative algorithms which can be performed on digital computers [6,7]. While the minimum-arithmetic operations of an iterative algorithm for a quadratic optimization are relative to the third power of the dimension number of Hessian matrix. Therefore, these iterative algorithms might not be effective enough for real-time applications of nonstationary problems. However, these quadratic optimization problems possess a nonstationary character in many real-time applications and have to be tackled in real time. One typical application of real-time quadratic optimization problems solving in robotics is for robot motion planning and control [4,5].Compared with conventional iterative methods for quadratic optimization problems solving, neural-dynamic approaches have some potential superiority in real-time applications [12–17]. First, neural dynamics (ND) possesses some outstanding natures such as distributed storage, self-adaptation, and parallel processing. Second, ND can solve various engineering and mathematical problems with large-scale nonstationary coefficients. Third, the structure of ND can be implemented by using very large scale integration technology. Therefore, ND is mostly viewed as one of influential computational methods for tackling many difficult problems in real time. At present, there are many gradient-based ND models proposed and investigated for solving quadratic optimization problems [6–8]. These methods use the norm of the error vector as the performance indicator and develop a neural dynamics changing along the negative gradient-descent direction to make the error norm vanish to zero with time in the time-invariant case. For the nonstationary case, the error norm cannot converge to zero even after infinitely long time due to the lack of velocity compensation of time varying parameters. Especially, Zhang dynamics (ZD) has been presented for solving online nonstationary quadratic optimization problem effectively [9,10]. The convergence performance is a significant improvement, as compared to gradient-based dynamics (GD) that cannot make the error converge to zero even after infinitely long time. That is to say, by making good use of the time-derivative information of the nonstationary coefficients of the nonstationary quadratic optimization problem, the ZD model can eliminate effectively the delay errors produced by the GD model and can reach the theoretical solution of the nonstationary quadratic optimization problem exponentially.As research continues, we found that the resultant ZD model with the suggested activation functions cannot reach the optimal theoretical solution in finite time, which may limit its applications in real-time calculation [9,10,20,21]. Although better convergence can be obtained by using Zhang dynamics, it needs infinitely long time to achieve the theoretically solution of the nonstationary quadratic optimization instead of the numerical algorithm, which is able to find the theoretically ideal solution within finite steps for the constant quadratic optimization problem. This sharp contrast motivates us to think about designing a novel strategy to tackle the nonstationary quadratic optimization problem in finite time.It has long been realized that different selections of the activation function can result in different convergence results for feed-forward neural networks. As to recurrent neural networks, a well-defined activation function is beneficial to the convergence rate of the dynamic system. Inspired by the investigation on the finite-time convergence of recurrent neural networks [22–29], we present the new activation function, which is named weighted sign-bi-power activation function, to accelerate Zhang dynamics to finite-time convergence to the theoretical optimal solution.Compared with existing neurodynamic models (specifically the GD model and the ZD model) for optimization, the proposed neurodynamic model possesses much superior finite-time convergence. Furthermore, the upper bound of the finite convergence time is derived analytically according to Lyapunov theory. Now, in order to better show the novelty of this paper, the main differences between this paper and Ref. [29] are compared and listed in Table 1. As seen from the table, it can be concluded that only the weighted sign-bi-power activation function is an improved version from the original sign-bi-power activation function. As for the remainder of this paper, they are different from those of [29]. Therefore, this is the first time to present and investigate a finite-time convergent model for solving the equality-constrained quadratic optimization with nonstationary coefficients as well as provides a much less conservative upper bound of the convergence time.The remainder of this paper is divided into five parts. Section 2 presents the problem formulation and then transforms it equivalently as a nonstationary linear system. In Section 3, for comparative purposes, we present the two of the most relevant works: the GD and ZD models. Section 4 proposes a weighted sign-bi-power activation function and a finite-time convergent neurodynamic model for nonstationary quadratic optimization. Besides, the upper bound of the finite convergence time is derived analytically according to Lyapunov theory. The superiority of the proposed neurodynamic model is verified by simulative results in Section 5. Section 6 is conclusion. Besides, it is worth pointing out that the main novelties and contributions of this paper are summarized and listed as follows.(1)For the first time, a novel finite-time convergent neurodynamic model together with a specially-constructed activation function is proposed and exploited for tackling nonstationary quadratic optimization problem in real-time.For comparison purposes, the conventional GD model and the recently-proposed ZD model are developed and applied to nonstationary quadratic optimization. Furthermore, the differences and links among such three neurodynamic models are summarized.Our proposed neurodynamic model can outperform the existing neural dynamical models, e.g., the conventional GD model and the recently-proposed ZD model, with superior convergence performance (i.e., finite-time convergence) achieved.An illustrative example is provided, which can verify the efficacy and superiority of our proposed neurodynamic model for solving nonstationary quadratic optimization problem in real-time.Let us discuss the following nonstationary quadratic optimization which is subject to the nonstationary equality constraint f(x(t), t)=0∈Rm:(1)minimize12xT(t)Q(t)x(t)+pT(t)x(t),(2)subjecttof(x(t),t)=0,where variable x(t)∈Rnis unknown at time instant t∈[0, +∞) and needs to be found [with xT(t) denoting the transpose of x(t)]. In nonstationary quadratic optimization depicted in Eqs. (1) and (2), Hessian matrix Q(t)∈Rn×nand coefficients p(t)∈Rnare smoothly nonstationary. Besides, f(·) denotes a mapping function which can be linear or nonlinear. This paper aims at proposing a nonlinearly-activated neurodynamic model for solving the above nonstationary quadratic optimization problem and finding the finite-time solution x(t) in real time t. In order to illustrate the complexity of the interesting nonstationary quadratic optimization (1) and (2) as well as for better readability, the following nonstationary quadratic optimization is taken as an example:(3)minimize18(sint)+8x12(t)+18(cost)+8x22(t)+costx1(t)x2(t)+sin5tx1(t)+cos5tx2(t),subjecttosin2tx12(t)+cos2tx2(t)=cos4t.Fig. 1shows the three-dimension snapshots of the example at different time instants. As observed from the figure, we know that the objective-function surface and the nonlinear-constraint plane are both “moving” with time t. That is to say, the nonstationary optimization problem is a quite difficult problem because the optimal solution is also “moving” with time t due to the “moving” effects of the nonlinear-constraint plane and the objective-function surface.Therefore, for the purposes of simplicity and clarity, and also to guarantee the solution uniqueness, we limit the discussion to the situation where f(x(t), t)=A(t)x(t)−b(t)=0 and Hessian matrix Q(t)∈Rn×nis positive-definite, although the extension for more general nonstationary quadratic optimization is possible. Thus, the above nonstationary quadratic optimization can be rewritten as below:(4)minimize12xT(t)Q(t)x(t)+pT(t)x(t),(5)subjecttoA(t)x(t)−b(t)=0,where coefficient A(t)∈Rm×mand b(t)∈Rmare smoothly nonstationary.It is well known that the most common approach for solving equality-constrained QP problems in the real domain is to use a Lagrange multiplier and minimize a suitable cost function [6–10]. Thus, for solving nonstationary quadratic optimization (4) and (5)[9,10], its related Lagrangian is presented as follows:(6)H(x(t),λ(t),t)=xT(t)Q(t)x(t)/2+pT(t)x(t)+λT(t)A(t)x(t)−b(t),where λ(t)∈Rmdenotes the multiplier variable.It is well-known that solving the quadratic optimization (4) and (5) could be achieved by zeroing the following equations:(7)∂H(x(t),λ(t),t)∂x(t)=Q(t)x(t)+p(t)+AT(t)λ(t)=0,∂H(x(t),λ(t),t)∂λ(t)=A(t)x(t)−b(t)=0.LetC(t):=Q(t)AT(t)A(t)0m×m∈R(n+m)×(n+m),y(t):=x(t)λ(t)∈Rn+m,d(t):=−p(t)b(t)∈Rn+m.In addition, the above linear equation system is further rewritten as a matrix-vector form as below:(8)C(t)y(t)=d(t).As Q(t) is the positive-definite matrix and A(t) is the full-row-rank matrix, C(t) is nonsingular, which ensures the uniqueness of the solution to Eq. (8). In addition, for comparison purposes, the nonstationary theoretical solution of (8) can be presented as below:(9)y*(t)=[x*T(t),λ*T(t)]T:=C−1(t)d(t)∈Rn+m.Note that, for a small dimensional matrix C(t), the theoretical solution y*(t) of (8) can be exploited to verify the efficacy of the nonlinearly-activated neurodynamic model. However, for a large-scale dimensional matrix C(t), the theoretical solution y*(t) of (8) is not easy to be obtained, so we can also use ||C(t)y(t)−d(t)||2 as a standard to judge the effectiveness of the proposed neurodynamic model.In this section, for completeness of this paper and for comparative purposes, we present the two of the most relevant works: the GD and ZD models. The GD method is widely used to solve constant problems and the ZD method is recently proposed to solve nonstationary problems. Next, such two methods are developed and exploited to solve the equality-constrained quadratic optimization with nonstationary coefficients.In this subsection, the GD model is developed for solving online nonstationary quadratic optimization (4) and (5), and its design process is provided as below [6–10].According to the GD method, a norm-based energy function is design as follows:(10)E(t)=12‖C(t)y(t)−d(t)‖22.Evidently, its minimum nonstationary trajectory can be achieved if and only if the neuro-solution y(t) of nonstationary quadratic optimization (4) and (5) is equal to y*(t).Then, a calculation strategy could be developed to evolve along a descent way of‖C(t)y(t)−d(t)‖22/2until the minimum nonstationary trajectory is reached. The common descent direction is the negative gradient of this energy function. Thus, we have(11)−∂E(t)∂y=−12∂‖C(t)y(t)−d(t)‖22∂y=−CT(t)C(t)y(t)−d(t).Finally, according to the above negative gradient, we can obtain the following gradient-based explicit dynamics of a GD model:(12)y˙(t)=−γ∂E(t)∂y=−γCT(t)C(t)y(t)−d(t),where y(t)∈Rn+m, from an initial state y(0) to start, is the neural state corresponding to y*(t), and γ>0 is exploited to adjust the convergence rate of GD model (11).As discussed before, during the processes of solving nonstationary quadratic optimization (4) and (5), GD model (11) cannot make the error converge to zero even after infinitely long time. Therefore, the ZD method has been proposed recently and has been proven to converge ideally when time goes to infinity. In this subsection, the ZD model is developed for solving online nonstationary quadratic optimization (4) and (5), and its design procedure is also presented as below [9,10].First, to monitor and control the process of nonstationary quadratic optimization (4) and (5) solving, an indefinite vector-valued error function E(t) is defined as follows (instead of the norm-based energy function in the GD model):(13)E(t)=C(t)y(t)−d(t).Then, the ZD formula is adopted so that E(t) decreases to zero as time goes to infinity. That is(14)dE(t)dt=−γΨE(t),where γ>0 is used to adjust the convergence rate of the ZD formula. Besides, Ψ(·): Rn+m→Rn+mdenotes a vector array of activation functions.Finally, expanding Eq. (14) and in view ofE˙(t)=C˙(t)y(t)+C(t)y˙(t)−d˙(t), we have the following implicit dynamics of a ZD model:(15)C(t)y˙(t)=−C˙(t)y(t)+d˙(t)−γΨC(t)y(t)−d(t),where y(t)∈Rn+mdenotes the neural state of ZD model (15). Furthermore, we have the following lemma to ensure the global and exponential convergence of ZD model (15) for nonstationary quadratic optimization (4) and (5) solving [9,10].Lemma 1Consider nonstationary equality-constrained quadratic optimization(4) and (5)and the corresponding nonstationary linear system(8). If monotonically-increasing odd activation functions Ψ(·) is used, starting from randomly-generated initial state y(0)∈Rn+m, state vector y(t) of ZD model(15)always converges to y*(t) of nonstationary linear system(8), of which the first n elements constitute the nonstationary theoretical solution of nonstationary quadratic optimization(4) and (5).□Because of the in-depth study on neural dynamics, we found that the convergence rate of neural-dynamic models can be thoroughly improved by an elaborate design of the activation function Ψ(·). In addition, taking advantage of the nonlinearity, a properly-designed nonlinear activation function often outperforms the linear one in convergence rate. Therefore, in this section, we aim at developing a nonlinear activation function, which can endow ZD model (15) with a finite-time convergence for solving nonstationary quadratic optimization (4) and (5). Motivated by the investigation on finite-time control of autonomous systems and finite-time convergence of recurrent neural networks [22–25], we propose a weighted sign-bi-power activation function to accelerate ZD model (15) to finite-time convergence to the theoretical solution of nonstationary quadratic optimization (4) and (5). The weighted sign-bi-power activation function wspb(·) is defined as follows:(16)wspb(z)=12k1sgnr(z)+12k2sgn1/r(z)+12k3z,where k1, k2, k3 and r∈(0, 1) are tunable positive parameters. In addition, sign-bi-power function sgnr(·) has the following expression:(17)sgnr(u)=|u|r,ifu>0;0,ifu=0;−|u|r,ifu<0;where u∈R and |u| denotes the absolute value of u. Thus, we can obtain the following finite-time convergent ZD (FTCZD) model for nonstationary quadratic optimization (4) and (5):(18)C(t)y˙(t)=−C˙(t)y(t)+d˙(t)−γWSBPC(t)y(t)−d(t),where WSBP(·) denotes the array of the weighted sign-bi-power activation function with each element defined as wsbp(·).Now, three different models for nonstationary quadratic optimization (4) and (5) are presented. In this subsection, the following facts are provided for comparing GD model (11), ZD model (15) and FTCZD model (18).(1)A specially-constructed nonlinear function (i.e., the weighted sign-bi-power activation function) is developed and exploited in FTCZD model (18), which makes its form totally different from these of GD model (11) and ZD model (15).The design of ZD model (15) and FTCZD model (18) is ground on the removal of every entry of a vector-valued indefinite error function. By contrast, the design of GD model (11) is based on the removal of a scalar-valued nonnegative energy function.ZD model (15) and FTCZD model (18) adopt an exponential-design formula [i.e., design formula (14)] for the nonstationary quadratic optimization problem solving. Thus, they can possess the exponential convergence performance. In contrast, GD model (11) adopt a negative-gradient descent rule, which is intrinsically for handling the quadratic optimization problem with static/constant coefficients only.Similar to ZD model (15), FTCZD model (18) is depicted in implicit dynamics that coincides better with the systems in practice and in nature. By contrast, GD model (11) is depicted in an explicit dynamics that is generally related to classic Hopfield neural networks.ZD model (15) and FTCZD model (18) systematically use the time-derivative information of nonstationary coefficients [i.e.,Q˙(t),p˙(t),A˙(t)andb˙(t)]. This is one main reason why the state vector of ZD model (15) and FTCZD model (18) can exactly converge to the nonstationary theoretical solution of the nonstationary quadratic optimization. In contrast, GD model (11) does not use this important information, and thus is not efficient enough for solving online the nonstationary quadratic optimization.It can be theoretically proved that FTCZD model (18) achieves superior finite-time convergence performance as compared to GD model (11) and ZD model (15). It is worth pointing out that convergence rate of ZD model (15) is related to design parameter γ, while GD model (11) cannot guarantee its convergence theoretically.Before ending this subsection, we would like to mention that the implicit dynamical systems usually arise in analog circuit systems based on Kirchhoff's rules. In addition, implicit dynamical systems have greater abilities in representing dynamical systems due to preserving physical parameters by using matrix coefficients, e.g., C(t) on the left-hand side of FTCZD model (18) and ZD model (15). Furthermore, the implicit dynamical systems can be transformed mathematically to explicit dynamical systems. From this point of view, owing to the merits of implicit dynamical systems, the proposed FTCZD model (18) and ZD model (15) are superior to GD model (11).As we know, the applications of neural networks rely generally on the dynamical performance of their models. Therefore, in this subsection, we present the main theorem to reveal the finite-time convergence performance of FTCZD model (18) by using the weighted sign-bi-power activation function.Theorem 1Consider nonstationary quadratic optimization(4) and (5)and the nonstationary linear system(8). From randomly-generated initial vector y(0)∈Rn+mto start, neural state y(t) of FTCZD model(18)globally converges to y*(t) of nonstationary linear system(8)within finite time, where the first n elements of y*(t) are the nonstationary theoretical solution x*(t) to nonstationary quadratic optimization(4) and (5). Besides, the upper bound of convergence time is derived as(19)tc⩽2ln1+k1k3e+(0)1−rγk3(1−r),if|e+(0)|<1;2rlnk2+k3k3e+(0)1−rr+k2γk3(1−r)+2rln1+k3k1γk3(1−r),if|e+(0)|⩾1;where e+(0)=max{|ej(0)|} for all possible j.ProofLety˜(t)=y(t)−y*(t)denote the difference between the nonstationary solution y(t) obtained by the proposed FTCZD model (18) and the nonstationary theoretical solution y*(t) of nonstationary linear system (8). Thus, we can obtain(20)y(t)=y˜(t)+y*(t)∈Cn+m.Substituting the above equation to FTCZD model (18); and in view of equation C(t)y*(t)−d(t)=0 andC(t)y˙*(t)+C˙(t)y*(t)−d˙(t)=0, we further know thaty˜(t)is the solution to the following dynamics:(21)C(t)y˜˙(t)=−C˙(t)y˜(t)−γWSBP(C(t)y˜(t)).On the other hand, sinceE(t)=C(t)y(t)−d(t)=C(t)[y˜(t)+y*(t)]−d(t)=C(t)y˜(t), Eq. (21) can be expressed equivalently as(22)dE(t)dt=−γWSBPE(t).Entry-wisely, we have(23)e˙j(t)=−γ12k1sgnrej(t)+12k2sgn1/rej(t)+12k3ej(t),where ej(t) is the jth entry of E(t).Since e+(0)=max{|ej(0)|}, −e+(0)⩽ej(0)⩽e+(0). Note that every ej(t) in E(t) have identical dynamicse˙j(t)=−γwsbp(ej(t))and the same the weighted sign-bi-power activation function. Then, according to the Comparison Lemma, we know that −e+(t)⩽ej(t)⩽e+(t) with t⩾0 for all possible j. This means that ej(t) converges to zero for all possible j when e+(t) reach zero. In other words, the convergence time of FTCZD model (18) is bounded bytc+of the dynamics of e+(t), wheretc+represent the convergence time of the dynamics of e+(t).To estimatetc+, we can begin with the following formula:(24)e˙+(t)=−γwsbp(e+(t))withe+(0)=max{E(0)},where wsbp(·) is the weighted sign-bi-power activation function.Then, by defining a Lyapunov function candidatev(t)=|e+(t)|2, the time-derivative ofv(t)is thus obtained:(25)v˙=−2γe+wsbp(e+(t))=−γk1|e+(t)|r+1+k2|e+(t)|1r+1+k3|e+(t)|2=−γk1v(t)r+12+k2v(t)1+r2r+k3v(t).For such a differential equation, ifv(0)=|e+(0)|2⩽1, according to the Lemma 3 of [30], there existstc+satisfying(26)tc+⩽2ln1+k1k3v(0)1−r2γk3(1−r),such that all ej(t)=0 whentc>tc+.Ifv(0)=|e+(0)|2⩾1, according to the Lemma 3 of [30], there existstc+satisfying(27)tc+⩽2rlnk2+k3k3v(0)1−r2r+k2γk3(1−r)+2rln1+k3k1γk3(1−r),such that all ej(t)=0 whentc>tc+.The above results mean that, if the weighted sign-bi-power activation function (16) is adopted, neural state y(t) of FTCZD model (18) converges to the theoretical solution y*(t) of nonstationary linear system (8) in finite time tc, of which the first n elements of y*(t) are the nonstationary theoretical solution to nonstationary quadratic optimization (4) and (5). □From the above comparison and analysis results, it can be concluded that GD model (11) always has a lagging error for solving nonstationary quadratic optimization (4) and (5), and that ZD model (15) converges exponentially to the nonstationary theoretical optimal solution of quadratic optimization (4) and (5) when time goes to infinity. However, they never converge to the theoretical optimal solution of quadratic optimization (4) and (5) within finite time, which may limit their applications in real-time processing. In contrast, FTCZD model (18) possesses the desired finite-time convergent performance. Evidently, FTCZD model (18) is superior to GD model (11) and ZD model (15), which theoretically ensures that FTCZD model (18) can realize superior convergence property.In the above section, FTCZD model (18) is proposed for solving nonstationary quadratic optimization (4) and (5) by adding a specially-constructed nonlinear activation function. In addition to detailed design process, the excellent finite-time convergence performance is analyzed in details. Besides, two of the most relevant works (i.e., the GD model and ZD model) are presented for comparative purposes. In this section, one illustrative example is provided for substantiating the efficacy and superiority of FTCZD model (18), as compared to GD model (11) and ZD model (15). It is worth pointing out that, in this paper, the abscissa axis denotes the x-axis and the vertical axis denotes y-axis for all 2-D figures.Now, without loss of generality, the following nonstationary coefficients for nonstationary quadratic optimization (4) and (5) are selected:Q(t)=0.5sin(t)+2cos(t)cos(t)0.5sin(t)+2,p(t)=sin(3t)cos(3t),A(t)=sin(4t),cos(4t),andb(t)=cos(2t).It follows from the definitions of equation (8) that P(t) is equal to0.5sin(t)+2cos(t)sin(4t)cos(t)0.5sin(t)+2cos(4t)sin(4t)cos(4t)0,andd(t)=−sin(3t),−cos(3t),cos(2t)T.It is evident that the nonstationary theoretical solution y*(t) of (8) in this situation can be given as0.5sin(t)+2cos(t)sin(4t)cos(t)0.5sin(t)+2cos(4t)sin(4t)cos(4t)0−1−sin(3t)−cos(3t)cos(2t).Since we have got the nonstationary theoretical solution of (8), we can use it as a criterion to verify the effectiveness of such three neurodynamic models. Simulative results are discussed as follows.As shown in Fig. 2(a), from 20 randomly-generated initial states y(0)∈R3 to start, state vectors y(t)∈R3 of GD model (11) do not meet well with the nonstationary theoretical optimal solution of nonstationary linear system (8). In addition, Fig. 2(b) presents the corresponding transient convergence behavior of ||C(t)y(t)−d(t)||2 synthesized by GD model (11) for real-time solution of nonstationary linear system (8). From the subfigure, we see that ||C(t)y(t)−d(t)||2 is rather large, and more importantly, does not converge to zero. The results verify that GD model (11) is not effective for real-time solution of nonstationary linear system (8) and nonstationary quadratic optimization (4) and (5).While ZD model (15) is applied to real-time solution of nonstationary linear system (8) under the same conditions, state vectors y(t) of ZD model (15) can converge to the nonstationary theoretical solution of nonstationary linear system (8) about 5s, which is demonstrated in Fig. 3(a). In addition, it is observed from Fig. 3(b) that ||C(t)y(t)−d(t)||2 synthesized by ZD model (15) decreases to zero within 5 s. Thus, the efficacy of ZD model (15) solving for nonstationary linear system (8) and nonstationary complex quadratic optimization (4) and (5) is verified primarily.Now, in order to verify the efficacy and superiority of FTCZD model (18) for solving nonstationary linear system (8) and nonstationary quadratic optimization (4) and (5), under the same conditions, Fig. 4provides the simulative results of FTCZD model (18). It is seen from Fig. 4(a) that, from 20 randomly-generated initial states y(0)∈R3 to start, state vectors y(t)∈R3 of FTCZD model (18) converges directly to the nonstationary theoretical solution of (8) within a rather short time. In addition, Fig. 4(b) shows the transient convergence behavior of ||C(t)y(t)−d(t)||2 synthesized by FTCZD model (18). As presented in Fig. 4(b), ||C(t)y(t)−d(t)||2 synthesized by FTCZD model (18) decreases to zero within 0.2 s, which is about 25 times faster than that of ZD model (15).On the whole, from the above simulative results depicted in the example, we draw a conclusion that ZD model (15) and FTCZD model (18) are both efficacious for real-time solution of nonstationary linear system (8) and nonstationary quadratic optimization (4) and (5); and that GD model (11) is not suitable for solving online nonstationary linear system (8) and nonstationary quadratic optimization (4) and (5) because the change of nonstationary coefficients is hard to track for it. More importantly, by comparison, FTCZD model (18) has a superior convergence performance to ZD model (15) for real-time solution of such a nonstationary quadratic optimization problem. In a word, the effectiveness and superiority of FTCZD model (18) are substantiated in this section.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
In this paper, by adopting a specially-constructed nonlinear activation function, a finite-time convergent neurodynamic model (i.e., the FTCZD model) has been proposed and studied for real-time solution of nonstationary quadratic optimization problems. The finite-time convergence performance of the proposed model has been analyzed and presented with the convergence upper bound also estimated. We not only have made a comparison among the GD model, the ZD model and the proposed FTCZD model theoretically, but also made a comparison among them using illustrative examples. Computer-simulation results have further verified and demonstrated the theoretical analysis, efficacy and superiority of the proposed FTCZD model for real-time solution of the nonstationary quadratic optimization problem, as compared to the GD and ZD models. Future work may lie in the study of ADMM for solving time-varying optimization problems.