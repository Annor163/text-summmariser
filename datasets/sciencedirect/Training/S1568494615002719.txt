@&#MAIN-TITLE@&#
Incremental learning with partial-supervision based on hierarchical Dirichlet process and the application for document classification

@&#HIGHLIGHTS@&#
Partial-supervision: use available knowledge to guide model learning process for better accuracy.Incremental learning: adjust parameters and model structure to the latest information.Introduce granular computing idea to achieve better accuracy and detect new emergent categories.

@&#KEYPHRASES@&#
Hierachical Dirichlet process,Semi-supervised learning,Partial-supervision incremental learning,Text classification,Natural language processing,

@&#ABSTRACT@&#
Hierarchical Dirichlet process (HDP) is an unsupervised method which has been widely used for topic extraction and document clustering problems. One advantage of HDP is that it has an inherent mechanism to determine the total number of clusters/topics. However, HDP has three weaknesses: (1) there is no mechanism to use known labels or incorporate expert knowledge into the learning procedure, thus precluding users from directing the learning and making the final results incomprehensible; (2) it cannot detect the categories expected by applications without expert guidance; (3) it does not automatically adjust the model parameters and structure in a changing environment. To address these weaknesses, this paper proposes an incremental learning method, with partial supervision for HDP, which enables the topic model (initially guided by partial knowledge) to incrementally adapt to the latest available information. An important contribution of this work is the application of granular computing to HDP for partial-supervision and incremental learning which results in a more controllable and interpretable model structure. These enhancements provide a more flexible approach with expert guidance for the model learning and hence results in better prediction accuracy and interpretability.

@&#INTRODUCTION@&#
Hierarchical Dirichlet Process (HDP) [1–5] was proposed as an improvement to latent Dirichlet allocation (LDA) [6]. It is an unsupervised learning method widely applied in the field of natural language processing, topic extraction and text mining [7–9], as well as video/audio and image classification [10–12]. HDP was proposed to model more complex topics with inherent hierarchical structures and has been applied to hierarchical topic modeling and text mining [13–15] and video recognition [16]. The advantages of HDP over LDA are: (1) HDP is able to model a more complex corpus with intrinsic hierarchical logic structure; (2) HDP has an inherent mechanism to determine the total number of clusters/categories without user input.Although HDP is able to automatically detect the nature of statistical distribution over topics and words and the total number of categories, it is an unsupervised model and thus expert knowledge on partially known labels and structures cannot be incorporated into its model building procedure. HDP usually results in more clusters/categories, which makes the topic hard to interpret, hence is not applicable for most real world applications. In addition, there is a large amount of new information available every day and the topics we are interested in may change frequently, therefore we need to ensure the model can adapt to the changing environment.To address these issues, Mao et al. [17] proposed a semi-supervised model for HDP; they used the defined labels to assist the construction of the hierarchical topic structure meanwhile retaining the flexibility of introducing new topics/nodes. However their work neither covered how to deal with multiple defined labels for a single document, nor included how to adapt the model to the latest information. Karthikeyan and Aruna [18] proposed a probability-based topic-oriented method with semi-supervision for text and image clustering and classification, and the result demonstrated improved accuracy in comparison to the DBSCAN and K-means algorithms. Their approach of probability calculation by using counts of words within documents is similar to the fundamentals of LDA, however, they did not address incremental adjustment of the model to make it adapt to the latest information. Andrei and Florin [19] presented a model extending existing clustering algorithms for incrementally quickly organizing web-history in a hierarchal structure. However their work only covered parameter incremental learning (i.e., assign a new document to an existing cluster, and adapt its parameters), not the structure incremental learning (i.e., new categories emergence, old topic disappearance, etc.). Hoffman et al. [20] proposed online learning for Latent Dirichlet Allocation, which is used for LDA, but not for complex problems with inherent hierarchical structures. Gao et al. [21] proposed incremental HDP, which realizes the incremental learning by splitting and merging topics based on pre-defined thresholds. They introduced three sampling steps: ‘prediction sampling’, ‘HDP sampling’ and ‘rejuvenation sampling’. The ‘rejuvenation sampling’ is an incremental learning approach but it needs to retrieve historical data within the latest time window. This means, their proposed incremental HDP still needs to retrieve some historical data and re-train the model, although, at least only a part of the historical data within the time windows is needed. On the other hand, our proposed method completely realizes online incremental learning, without requiring using any historical data to re-train the model. Lis [22] proposed the incremental learning framework for generic methods for automatic online picture collection. Their proposed method makes use of a sample set of new image data with high likelihood and high entropy for model incremental learning. However, this mechanism of sampling a subset, selectively loses important information which covers multiple topics, including both existing topic(s) and new topic(s). Their proposed incremental learning scheme works well for binary image classification (background and foreground), but is not suitable for text classification with many documents covering multiple topics.In this paper, we propose the incremental learning for HDP with partial supervision. The proposed method incorporates the advantages of flexible structures, partial-supervision (dealing with the mixture of documents with and without labels), a solution for sampling multiple predefined labels for a single document and self-adaptation of the model (in both parameters and structure) using the latest data. Two motivations of the proposed incremental learning with partial supervision are: (1) on one hand we encounter a large amount of new information every day, which is constantly changing; the topics we are interested in today might be totally different from last month; the old pre-trained model might be completely unsuitable for the new environment; and on the other hand it is not feasible to regularly re-train the model from the combination of historical data and new data; however, incremental adaptive learning enables the model to adapt itself (both model parameters and model structure) to the latest information. (2) Given the large amount of information, it is impossible for anyone to have knowledge of all documents; the user might have some knowledge about a small portion of documents (which he has read), and assigned topic(s) to the documents he knows; the user might have some idea about the structure (or expected resulting structure) of the model which can be incorporated into the model learning process. Partial supervision of the learning process helps increase the interpretability and accuracy of the resulting model. In this paper, we also enhance the formulas for sampling for multiple defined labels for a single document which reflects the real world situation. A practical application of our proposed system could be academic/research papers/documents hierarchical classification from the online IEEE and ACM databases and incrementally sorting new papers in new issues for all journals and conferences, meanwhile detecting newly emergent areas/topics in research. The document classification results then can be used for dynamic academic recommendations and reporting new trends in different research areas. It could also be part of a recommender system. The application needs more development work to acquire online updates from the defined academic websites and convert PDF files and html files into text files which can then be analyzed by our proposed system. However, in this paper we are using a ‘news’ dataset which is a more complicated task to analyze than academic document classification. We have obtained promising result in news classification and we expect the accuracy to improve further when applied to the academic domain.The advantages of our proposed incremental learning with partial supervision for HDP compared with existing supervised/semi-supervised HDP are:•We enable partial-supervision to incorporate available incomplete expert knowledge to guide the model learning process. Our partial-supervision mechanism is able to automatically deal with a mixture of documents with and without tags, which is why we call it partial supervision. When all the documents are assigned with labels, our proposed learning becomes supervised learning for HDP; whilst when no documents are assigned with labels, our proposed learning becomes traditional unsupervised HDP learning. In addition, the proposed method also allows the user to define the structure or partial structure for the model. All these enhancements provide more control and flexibility to the user and permit the use of limited available expert knowledge to deliver a better model with improved accuracy.Our proposed method supports a multiple-label sampling constraint (i.e., multiple defined labels for one single document), which is a realistic representation. We define formulas for sampling for multiple defined labels obeying Dirichlet distribution, which is exactly the same sampling process as the sampling process without defined labels; sampling with defined label(s) is made within the sub-space constrained by the defined labels; sampling without defined label(s) is made from the whole space.We propose the incremental learning method with model decay; the model is able to forget the out-of-date information and better adjust its parameters and structure to the latest information. The advantages of doing incremental learning are (1) the dynamic self-adaptation allows the model to represent the latest information better and results in better accuracy; (2) in our fast changing world some topics emerge, some topics die; the dynamic self-structure evolution helps detect the changes of existing topics and emergence of new topics.We introduce the idea of granular computing to the incremental learning for HDP model to represent new additional information. Granular computing is an idea for processing complex information entities called information granules. Generally speaking, information granules are collections of entities that arrange information together due to their similarity or coherence. Granular computing is an approach to recognize and exploit the knowledge at various levels of resolutions or scales. When applied to our method, the idea is, one topic can be represented by multiple nodes with high similarity or coherence, and each additional node is considered as a new granule, either representing part of one existing topic, or a potentially new topic.The paper is organized as follows. We give a brief introduction of the HDP in Section 2, which is the basis of our proposed incremental learning with partial supervision for HDP. The proposed partial supervision for HDP is given in Section 3. The proposed incremental learning scheme is explained in Section 4. The performance comparison and analysis for benchmark datasets are given in Section 5. Finally, conclusions and a discussion are presented in Section 6.Hierarchical Dirichlet process (HDP) was proposed after LDA by Blei [1] to enable the investigation of and representation of more complex corpus/data. HDP is a stochastic process that can be used to define a nonparametric distribution on an assortment of mixtures model. That is, each grouping of data is drawn from a mixture model, and the mixture components are shared among the different groups. Using a hierarchy of Dirichlet processes enables the number of mixture components to be inferred from the data.In the HDP the assumptions are: corpus is a collection of documents and each document is a collection of words. The HDP generates a hierarchical (tree-like) model with each tree path representing one topic hierarchy and with each leaf node representing one topic.Given a collection of documentsD, the HDP is to determine to what degree each document d∈Dcovers various topics at the leaf level and to what degree each word in the document belongs to or highlights which level it is at in the category hierarchy in the sampled path. The final objective of the HDP is to find the topic distributions P(z|d) for each topic z at the leaf level for each document d, given an observed word set in this document. Collapsed Gibbs sampling [9] can be used to estimate these parameters for the HDP.Unlike LDA, the HDP introduces a hierarchical structure for the topic (sub-topic) distribution. HDP also differs from LDA in that it does not need the number of topics K to be defined in advance and assumes that there is always a possibility that there exists new topic(s) for one document. HDP introduces a new parameter γ, the number of the imaginary documents belonging to a topic which does not exist in the current model. This indicates a possibility of introducing new topic(s).In the HDP, the collapsed Gibbs sampling process for one document d is divided into two iteratively cooperative sampling stages: sampling for the path (down to the leaf node) and sampling levels for each word along the sampled path.The path sampling for document d is based on Bayesian theorem: Eq. (1). Given the parameter of γ (the chance a word in a document belongs to a new topic (or new sub-topic) which does not exist in current model and the Dirichlet distribution parameter for words η, the probability of sampling document d on pathcdis proportional to the prior probability for pathcd: P(cd|c−d, γ), and the occurrence probability of the wordwd:P(wd|c,w−d,z,γ); assumecd=node1(root node)→node2→…→nodeLand L is the number of levels for the HDP.(1)P(cd|w,c−d,z,η,γ)∝P(cd|c−d,γ)×P(wd|c,w−d,z,γ)Then the prior probability P(cd|c−d, γ) for pathcdin Eq. (1) is calculated by Eq. (2), where niis the number of documents having a path going through nodei, and γ is the parameter indicating how many imaginary documents do not belong to any existing nodes.c–dandw–din the equations means that the influence for document d has been removed from the model and it needs to be re-sampled.(2)P(cd|c−d,γ)α∏i=1L−1ni+1ni+γif all nodes alongcdis existing nodes∏i=1pni+1ni+γ×γnp+1+γif firstpnodes alongcdis existing nodesEq. (2) is a direct calculation of the path prior probability; this calculation has its various transformations (e.g. exponential and logarithmic transformations for computation issues).The probability ofP(wd|c,w−d,z,γ)in Eq. (1) is calculated by Eq. (3) obeying the Dirichlet distribution, wherenl,−d,wis the count of word w at level l without considering this word's count for document d;nl,wis the number of word w at level l, and W is the total number of vocabulary in the dictionary, and η is the Dirichlet distribution parameter for words.(3)P(wd|c,w−d,z,η)∝∏l=1LΓ∑wnl,w−d,w+W×η∑∏wΓ(nl,−d,w+η)×∏wΓ(nl,w+η)Γ∑wnl,w+W×η∑The stochastic explanation for Dirichlet distribution for Eq. (3) can be found in the reference [1] (and other literature). In Eq. (3)Γ is standard gamma function defined as Γ(z+1)=z×Γ(z). In the implementation, the Γ can be calculated regressively and reused iteratively.Given a path sampled through Eq. (1), word level sampling (allocating each word w in document d to one of the levels z: wz) is performed based on Eq. (4),(4)P(wz|ld,−w,cd,η,α)∝P(ld,z,−w|ld,−w,α)×P(wz|ld,z,−w,cd,η)whereP(ld,z,−w|ld,−w,α)is the topic distribution over levels and calculated by Eq. (5);(5)P(ld,z,−w|ld,−w,α)∝nld,z,−w+αandP(wz|ld,z,−w,cd,η)is the probability for word w based on the statistical assignment and is calculated by Eq. (6).(6)P(wz|ld,z,−w,cd,η)∝η+ncd,l,wW×η+∑wncd,l,wwherenld,z,−wis the word count in document d at level z along the pathcdandncd,l,wis the word count for word w at level z in all documents along the pathcd, and W is the total number of words in the vocabulary in the dictionary.The Gibbs sampling for the HDP learning process is shown in Fig. 1.In this section we introduce the partial-supervised learning for HDP. The proposed partial-supervised learning has the advantages of: model structure partial definition; learning from corpus with partial defined documents; and a consistent sampling process within the sub-space constrained by the defined labels. We will discuss these aspects in the following sections.HDP does not require the number of topics to be defined by the user, because HDP introduces an extra parameter γ, which is the number of imaginary documents belonging to the topic/sub-topic which does not exist in the current model. The introduction of the parameter γ enables HDP to detect the number of topics automatically based on the nature of word-topic distributions. However this automatic determination of the number of topics is purely determined by the word-topic distribution amongst the topics in the training corpus. Although it is able to detect the latent word-topic distribution, the resulting model is usually incomprehensible. The user ideally wants to have some control over the model training process and expects to achieve a resulting model that can be interpreted. In this section, we introduce a mechanism to control the structure for the proposed HDP to guide the model structure building process.We can see from Section 2, in the model learning process, whether a new node/topic/sub-topic is added to the existing model depends on the path sampled in one document and is controlled by Eq. (2). In Eq. (2), if not all nodes on the sampled path are existing nodes; then a new tree branch is created and added to the existing model, with all new nodes placed on the newly created branch. The path sampling is done step by step from the root node (highest level) down to the leaf node (lowest level). Each lower level node (say child node) on the path is sampled based on its higher level node (parent node). At each level, say the (i+1)th level sampling, there is a chance of γ/(ni+γ) to sample out of the existing nodes/topics/sub-topics. The way to stop more new nodes/topics/sub-topics sampling is by setting the parameter of γ as zero. Then the chance of sampling out of the existing nodes/topics/sub-topics isγni+γ=0In the HDP, the step-by-step path sampling process, Eq. (2), can be written as Eqs. (7) and (8):(7)Pchild_nodei|parent_node,γ=nchild_nodeinparent_node+γ(8)Pnew_child_node|parent_node,γ=γnparent_node+γTo control the number of children nodes for each parent node; we need to reset the parameter of γ as zero when the number of children nodes (for one parent node) reaches its defined maximum number. Eqs. (7) and (8) can then be rewritten as Eqs. (9) and (10);(9)P(child_nodei|parent_node)=nchild_nodeinparent_node(10)P(new_child_node|parent_node)=0Hence to control the model structure (number of nodes/topics/sub-topics), automatic monitoring and intervention is added so that either Eqs. (7) and (8) or Eqs. (9) and (10) are used for the next level path-node sampling. Assume we define the hierarchical level as L and the maximum number of children nodes for each level is defined as {m1, m2, …, mL−10} from the highest level (root node) to the lowest level (leaf node); the maximum number of child nodes for the lowest level (leaf level) is zero because leaf nodes do not have further child nodes. The path node sampling process for one document for the proposed model structure definition is illustrated in Fig. 2.The structure control can be done by setting the maximum number of child nodes. If the maximum number of child nodes is unlimited, the proposed model decays to the traditional HDP without structural control.Note that the maximum number of child nodes cannot constrain the creation of a new node with a new defined topic; even if it has reached the maximum number of child nodes.In the HDP model, each topic path/hierarchy is represented by a series of nodes from the root node down to the leaf node: {node1, node2, …, nodeL}, where L is the number of levels for the HDP model and each nodeirepresents one sub-topic among its upper-level parent topic. Given the leaf topic nodeL(which is the most detailed category), we can trace it back to its parent topic nodes up to the root node due to the tree-structure of the HDP model. Hence in the implementation, the leaf node information can be used for (partial) supervised learning, which is represented asTik.We consider the task as: given a collection of M documents,D={d1, d2, …, dM}, which is composed of MLlabeled documentsDL={d1,d2,…,dML}, and MUunlabelled documentsDU={d1,d2,…,dMU}, where MU+ML=M. Assume we have K defined topicsT={T1, T2, …, TK} in the current model (and initially K=0), each document diinDLhas one or more assigned topicsTi={Ti1,Ti2,…,Tik}, where eitherTik∈TorTikis a new topic to be added to the existing model. In the Gibbs sampling process, the target topic for any labeled document diinDLis restricted to belong to the set of labelsTi={Ti1,Ti2,…,Tik}, and the target topic for any unlabelled document djinDUbelongs to the whole topic domainT={T1, T2, …, TK, …} and a possible new topic.For the documents with defined label(s), to guarantee that we always get a valid label constrained to the defined labels, Eqs. (7) and (8) for path sampling is modified to Eqs. (11) and (12). If the defined topic/sub-topic has been included in the existing model then Eq. (11) is used to sample the path; else new nodes (new path branch) are created and added to the model according to Eq. (12).(11)P(child_nodei|parent_node)=nchild_nodei∑child_nodej∈definedpath(nchild_nodej)0otherwise(12)P(new_child_node|parent_node)=1For the documents without defined labels, the Gibbs sampling process follows Eqs. (7)–(10).Given the sampled pathc={c1, c2, …, cL}, the word level sampling process is exactly the same with HDP: Eqs. (4)–(6) as discussed in Section 2.The algorithm for Gibbs sampling for our proposed partial supervised learning is shown is Fig. 3.Vast quantities of information are being generated every day with new emerging or changing concepts and terminologies. Any existing statistical model might not be suitable for the latest information. However it is not feasible to re-train the model by using all historical data and the newly collected data regularly. Incremental learning allows the model to maintain the old information in the model and adjust the model parameters and structure with bias toward the newly collected data. The key points for incremental learning are: only the newly collected data are used for the incremental learning without retrieving the historical data; the updated model should shift toward the updated information whilst maintaining the robustness and consistency of the existing model; and the model should remain constrained to a manageable size with the incremental learning (i.e., the model size cannot be increasing exponentially).The proposed incremental learning for HDP is composed of parameter incremental learning and structural incremental learning. We will discuss these in the following sections.The parameter incremental learning for HDP with partial supervision deals with the combination of the corpus, with and without known labels. For each newly collected document, HDP infers which topic this document belongs to by iterative sampling. When it arrives at some convergence on this new document, the probability distribution indicates to what degree this new document belongs to each topic. For a document with defined topic(s), the sampling is only done on the sub-space within the pre-defined topic(s), hence the conclusion is drawn within the constriction of the pre-defined topic(s). For a document without any defined topic, the sampling is done on the whole space for all topics and the conclusion is drawn on the word-topic distribution among all topics. Through this iterative re-sampling process, the word-topic distribution is updated automatically. So no additional computation is required.We introduce the decay factor for the proposed incremental learning in order that the model is able to forget the out-of-date information. The document/word counts for each topic are decaying based on an exponential decay function through time, as shown in Eq. (13).(13)nit+1=nit×e−1/λwhere λ is the decay constant; the larger λ is, the slower the decay. Fig. 4shows the decay by setting λ as 25 and 2.5 respectively. When λ=25, one document's contribution to the model decays to 50% after 20 time-window slices and decays to almost zero after 100 time-window slices. When λ=2.5 one document's contribution decays to zero after 10 time-window slices. The parameter λ can be changed based on the size of the defined time slice and the expected topics change rate (topic decay, drift and emergence) overtime. The nodes representing fewer ‘effective number of documents’ are non-representative and get less weights in the sampling process. ‘Effective number of documents’ means that we consider the decay through time and give less weight to older documents. All nodes which represent documents less than a threshold (e.g., one) will be removed from the model to keep the model concise and efficient.The HDP is able to build up the model automatically constrained by the defined number of maximum child nodes for all levels and the defined labels/topics. Each leaf node is considered as a granular node in the HDP model. Some leaf nodes have labels learned from the partial supervised learning from documents with defined labels, which are called defined granular nodes; other leaf nodes do not have the defined labels, and these are known as undefined granular nodes. In the partial supervised learning, the user might want to check the frequently occurring words in the undefined granular node to either define a new topic or relate them to an existing topic using expert knowledge. This process is considered to be part of partial supervised learning. Otherwise, the undefined granular nodes are connected to the most similar defined granular nodes. The Kullback–Leibler divergence is used to calculate the distance from one granular node to another granular node which is defined by Eqs. (14) and (15).(14)DKL(P||Q)=∑iP(i)×lnP(i)Q(i)(15)P(i)=n(i)∑in(i)where n(i) is the word counts for the ith word in the alphabet.In the implementation, we assume a granular node X is a node with no label, we then calculate the Kullback–Leibler divergence of all the defined granular leaf nodes from node X, and select the node Y with the smallest Kullback–Leibler divergence from node X and apply node Y's label to node X. That is node X represents a similar topic as Y. Note that, the comparison and re-assignment is done at the end of each incremental learning operation, and this assignment for the undefined granular nodes is changing with the adaptation of the parameters/word-topic distributions. The undefined granular nodes are always mapped to their closest defined granular nodes.The Kullback–Leibler divergence of node Q (represented by word probability distribution in node Q) from node P (represented by word probability distribution in node P) is a measure of the information lost when node Q is used to approximate node P. The assumption for Kullback–Leibler divergence is that the accumulation for both P and Q is one. In addition, from Eq. (14), we can see, both P(i) and Q(i) cannot be zero for the validity of the calculation and comparison. We introduce a small adjustment value ɛ to constrain the value of DKL(P||Q) to be larger than zero and smaller than positive infinity. So we add a small value ɛ to adjust Eq. (15) and valid the computation. Eq. (15) can be rewritten as Eq. (16).(16)P(i)=n(i)+εsizealphabet×ε+∑in(i)Lemma 1By using Eq.(16), The minimal value for Kullback–Leibler divergence, DKL(P||Q), is 0 when the probability distributions for P and Q are exactly the sameP(i)=Q(i) for all i. thenDKL(P||Q)=∑iP(i)×lnP(i)Q(i)=0Lemma 2The approximated maximum value of Kullback–Leibler divergence of one granular node from another granular node is: −ln(ɛ), where ɛ is the small smooth factor to validate the calculation.Now the approximated maximum value is estimated for: DKL(P||Q). The probability distributions are described as three groups:(1)for all distributions with index i group, np(i)>0 and nq(i)=0, then P(i)=np(i)/∑allnpand Q(i)=ɛ/∑allnq≈0for all distributions with index j group, np(i)=0 and nq(i)>0, then P(j)=ɛ/∑allnp≈0 and Q(j)=nq(j)/∑allnqfor all distributions with index k group, np(i)=0 and nq(i)=0, then P(k)=ɛ/∑allnp≈0 and Q(k)=ɛ/∑allnq≈0;where ∑allnp=∑inp(i)+∑jnp(j)+∑knp(k) and ∑allnq=∑inq(i)+∑jnq(j)+∑inq(k)DKL(P||Q)=∑iP(i)×lnP(i)Q(i)+∑jP(j)×lnP(j)Q(j)+∑kP(k)×lnP(k)Q(k)where∑iP(i)×lnP(i)Q(i)=∑iP(i)×lnnq(i)∑allnp×∑allnqε≈−∑iP(i)×ln(ε)∑jP(j)×lnP(j)Q(j)=∑jP(j)×lnε∑allnp×∑allnqnq(j)≈∑jP(j)×ln(ε)∑kP(k)×lnP(k)Q(k)=∑kP(k)×ln∑allnq∑allnp≈0DKL(P||Q)≈−∑iP(i)×ln(ε)+∑jP(j)×ln(ε)=−ln(ε)×∑iP(i)−∑jP(i)≤−ln(ε)Hence, the approximated maximum value of Kullback–Leibler divergence of one granular node from another granular node is: −ln(ɛ), where ɛ is the small smooth factor to validate the calculation. In our experiment, we set ɛ as e−100=3.72E−44 for easy calculation, then the maximum value for: DKL(P||Q) is 100. When the distance of one granular undefined node from all defined granular nodes are larger than a threshold (which is defined by ɛ): d×(−ln(ɛ)), saying ifDKL(P|Q)d×(−ln(ε)), then we consider that undefined granular node as different enough from all defined granular nodes and consider that unknown node as a new detected topic.The algorithm for the proposed incremental learning for HDP with partial supervision is shown in Fig. 5(a)–(d).In this section, we investigate the performance of the proposed partial supervised learning. We firstly apply the HDP with our proposed partial supervised learning to the benchmark 20 news data [23] for document classification, with full supervision and compare the result with bench mark methods, Support Vector Machine (SVM) and Naïve Bayes (NB) [24] to make sure the proposed method with full supervision has better accuracy than the other methods. To ensure a fair comparison we use a 10-fold cross validation which is consistent with [24]. We can see that the proposed HDP achieves better accuracy than SVM and NB (Table 1).Next we apply the proposed partial supervision to the 20 news dataset using a 10-fold cross validation for training and testing whilst varying the label percentage for the training data from 0.01 to 1.0. The results are visualized in Fig. 6. From Fig. 6, we see only 1% labeled training data provides an accuracy of 64% for testing; increasing this tagged document percentage helps increase both training and testing accuracy. The accuracy increase follows a log-like curve with dramatic accuracy improvement prior to 20% labeled documents. In addition, we note that partial supervision with only a tiny portion of labeled documents helps map the model topics to the real world topics automatically which saves the significant effort required to map the model topics to the real world topics by trial and error passes (20! trials and error passes).The 20 news data are labeled data with only one topic assigned to each document. To investigate how the proposed partial supervision works for a corpus with multiple labels using our proposed multiple-label sampling mechanism, we applied the proposed method to the Reuters data [25]. The Reuters dataset is a collection of real world news through 6 months from October 1996 to March 1997. Each article belongs to one or more conceptual categories; and there is high concept overlap between different defined topics, which represents the real world dataset. To determine the criterion to calculate the accuracy for multiple labeled documents is not easy. In this section, we define the criterion calculated by the probability overlap between the objective/defined topic(s) and the model predicted topic(s). In this section, we set the overlap probability threshold (called “match threshold in the tables”) as low value (0.3) and high value (0.8). If the probability overlap between the target/defined topic(s) and the model predicted topic(s) is larger than the defined overlap probability threshold, we increment a correction counter. The training/testing accuracy is calculated by the total number of corrections divided by the total number of articles for training/testing. The high probability overlap threshold (0.8) is stricter than the low probability overlap threshold (0.3). So it is expected that the lower the probability overlap threshold is, the less strict the criterion is and the higher the accuracy we expect. There are news data for 6 months in Reuters data. We divide the whole dataset into two parts: first 3 months articles for training and the remaining 3 months articles for testing and the result is shown in Table 2. The accuracy for a ‘match threshold’ of 0.3 is better than the accuracy of results observed by setting the ‘match threshold’ to 0.8. We use the stricter ‘match threshold’ of 0.8 in the following experiment and discussion. The ‘match threshold’ of 0.8 is a very strict criterion. If we assume that we have more than 10 categories in total and if the correct prediction probability is 0.7 and all the remaining categories prediction probability is around 0.02–0.03, this is still considered a prediction failure under the criterion of ‘match threshold’=0.8. However in reality, the prediction probability of 0.7 for the correct category is a very good result and works well for inference for articles with multiple defined topics. Our current criterion for accuracy calculation might not be the best one to judge the accuracy performance for multi-label predictions. How a more intelligent and smart criterion for accuracy judgment should be defined, is not the key focus of our discussion. We are more interested in how the relative performance of how the proposed partial supervision compares to the traditional HDP.The accuracy results are shown in Fig. 7. We can see a consistent increase for both training accuracy and testing accuracy when we increase the percentage of labeled documents. In this partial supervised model, whether label(s) is considered for document training is randomly generated based on the ‘Tagged percentage’. Partial expertise (corpus with partial tagged documents) does help to increase the accuracy; and the more expertise we apply, the better accuracy we can obtain. Another point we want to highlight is that, in addition to the performance increase in accuracy in general, a small portion of tagged documents helps to define/assort the categories: mapping the model topics to the real world topics. From Fig. 7 we conclude that the proposed partial supervised learning works well for the corpus with multiple defined labels for a single document.When we talk about incremental learning, we aim to adapt the model with the latest data/documents in two aspects: adapt the word/topic distribution, and adapt the model structure including old topic(s) fading out and new topic(s) creation. In this section, we firstly investigate accuracy improvement by the incremental learning in terms of word/topic distribution. The decay factor λ is set as 25.We are assuming that at the beginning we have no data and the model starts from scratch, which is the real world scenario most of the time. In the following experiment, we set the percentage of documents with tagged label(s) as 5% which is a small percentage but realizable in practice. We select one month's data for incremental learning performance testing, from 1st November to 30th November 1996. On the first day, 1st November, we have no model for prediction; the initial model is trained from the data/documents from 1st November. The assumption of incremental learning is that we cannot afford to re-train on the combination of all historical data and the latest data. Thus we have three options for the following experiment: (1) We can fix the model obtained from the data/documents on 1st November and use this fixed model for testing the following days, (the “base model”); (2) we can train a new model every day based on the previous day's data and do the testing on the current day (“pervious day model”); (3) our proposed incremental learning; we always incrementally adapt the model with the latest data/documents from the current day: that is, on 2nd November, we can use the model we obtained on 1st November to do the prediction and meanwhile make use of the data/documents on 2nd November for incremental learning. This testing and incremental learning process continues in the following days. We always use the model we derived from the previous incremental learning for testing and meanwhile incrementally adapt the model based on today's data/documents. When we compare the computation complexity, no more computation is needed if we use the base model. The computational complexity for the previous day model and incremental model are exactly the same (see the algorithm in Section 4.4). However, because the model structure during the incremental training is slightly more complicated than the previous day model (which is always trained from scratch based on previous day data), the training time for incremental training is longer than the previous day model. However this training time increase is not exponential but constrained by the defined maximum model structure. We will compare the training time later in this section. The testing of accuracy is shown in Fig. 7. With only 5% labeled documents, we can get accuracy around 0.25–0.3 by using our proposed incremental learning (as we discussed in Section 4.1, this is not a good accuracy criterion for documents with multiple labels). In addition 5% labeled documents is not sufficient for training. By increasing the tagged document percentage, the accuracy will be improved accordingly (as shown in Fig. 7). However 5% tagged data might be realizable given the fact that we are facing a huge volume of new data every day. From Fig. 8, we can see that incremental learning gets the best accuracy compared to using the base model and previous day model. The average accuracies for base model, previous day model and incremental model are 0.22, 0.21 and 0.29 respectively. In addition, the results for base model (with the deviation of 0.02) and incremental model (with the deviation of 0.03) are comparatively more stable the previous day model (with the deviation of 0.05). The reason is the loss of model robustness from the cumulative historical data by training new model(s) using only the previous day's data. The base model obtains a comparatively stable accuracy through time; that means the data/documents are comparatively stable on its topics and on words they are using on each topic within one month (word/topic distribution is stable). That is what we expect. When we consider a longer time scale saying a couple of years, we expect the accuracy to decay when using the base model due to the change of topics and words in each topic. Even in a comparatively stable environment, we can still obtain a better accuracy by using our incremental learning as shown in Fig. 8.To illustrate the advantages of the proposed incremental learning for topic changing through time, we artificially rearranged the documents into three groups, the first group consists of all documents purely relevant to category C, the second group consists of all documents relevant to both C and E or purely category E (where the topic E comes up), and the third group consists of all the remaining documents (where the topic G comes up). The categories of C, G and E are high level categories; there are sub-categories under those high level categories, e.g. C_1. The accuracy is calculated based on the lowest level (most exact) categories. Then we segment all documents into groups with 100 documents per group and incrementally train the model every 100 documents (each group) each time. The comparison of results is shown in Fig. 9. Fig. 9(a)–(c) shows the accuracy for the base model, previous day model and the proposed incremental model through time respectively. The x-axis is arranged by every 100 documents for each group (“round” in the figure). There are about 72,000 documents altogether which goes 720 rounds with 100 documents for each single round of incremental learning. We can see a general and stable accuracy increase after 200 rounds by using our proposed incremental learning from Fig. 9(c). The initial 200 rounds can be considered as a starting up phase for stabilization. The accuracy from the base model decays to zero at the time topic E comes up, which is what we expected. When we use the previous day model, we get very good accuracy of 100% sometimes and very bad accuracy of 0% at the other time points which shows the instability of the previous day model. For this experiment, we surprisingly achieve the best average accuracy (0.42) by using the previous day model for the artificial corpus with sharp topic changes at points in time and the topics remain stable after each sharp topic change. This contributes to the achievement of the best accuracy in the experiment. The average accuracy for the base model and our proposed incremental model are 0.1 and 0.37 respectively. The deviations for base model, previous day model and the proposed incremental model are 0.14, 0.29 and 0.19 respectively. The base model has the smallest deviation because all accuracy is approaching zero after the new categories (E and G) are introduced (after 280 rounds). We can see large deviations before 280 rounds.The time comparison between previous day model and incremental model (training and testing) is shown in Fig. 10. The x-axis is arranged by every 100 documents for each “round”, and the y-axis is the total training and testing time (in s) for every additional 100 documents. For the base model we do not need to incrementally train (or re-train) the model after we derive the base model in the first round, and this base model is kept constant in the following testing stages. We can see from Fig. 10(a), for the incremental model, the CPU time used for training and testing suddenly increases at the point of round 280 and the point of round 510 where new categories are introduced, which adds to the complexity of the model. A more complicated model needs longer training time to adjust the model. The previous day model does not need to introduce structure complexity through time and hence it does not add more computational complexity. For the previous day model (as shown in Fig. 10(b)), a completely new model was trained from scratch based on the previous day data only (batch of 100 documents at previous round). So the training time for the previous day model is on average faster than the incremental model. This is because there is no structural complexity accumulated from the previous training (as is the case for the incremental model). However the CPU time for the previous day model training is independent and is based on the complexity of the local batch of 100 training documents. That is why we see the time increase at round 610 to round 630.In addition to the smooth parameters α, β and γ, other parameters introduced to our proposed incremental learning with partial supervision are the decay factor λ, and the predefined maximum number of child nodes for each level {m1,m2, …,mL−1, 0} wheremiis the maximum number of child nodes for level i, the last zero means at the leaf level no child node is allowed to be added. Some nodes have labels learned from the partial supervised learning, and some nodes have not. Nodes without a defined label are mapped to one defined node (with a label) by using the smallest Kullback–Leibler divergence. If the smallest Kullback–Leibler divergence is smaller than the threshold dis×(−ln(ɛ)) (see Section 4.3); else a potentially new topic is marked for this node.γ is the imaginary number of documents which belong to a new unknown topic in the sampling process and is fixed as one in much of the literature. This value is relevant to the valid number of documents in total. ‘Valid number’ means the number of documents after decay at each time point. The number of imaginary documents which belongs to a new unknown topic should be proportional to the ‘valid number’ of total documents; that is the higher number of total documents we have, the more chance that one of the them belongs to a new unknown topic. In our proposed incremental method, we dynamically change this value according to the current ‘valid number’ of total documents. In order to control the size of the model, we also force this value back to zero when the maximum number of child nodes reaches its constraintmifor the ith level in the model. That means {m1,m2, …,mL−1, 0} has a higher priority to control the size of the model. We will investigate how maximum number of child nodes {m1,m2, …,mL−1, 0} influences the resulting model and accuracy.In extreme case, if all values ofmiare unlimited, then there is no structural constraints and the model structure learning becomes the traditional HDP. Experimentally the traditional HDP usually results in too many nodes and an unmanageable model size. However, such large models have no advantages in accuracy improvement compared to a concise model.On the contrary, if we set allmi=0, then new nodes/topics are only added when it is defined through the partial supervision (some of the documents are labeled). In this scenario, all nodes have a defined label through partial supervised learning. No additional node/topic is needed. The disadvantage for this constraint is that we cannot detect any new topics and all unknown documents are forced to be matched to one of the existing topics (through partial supervised learning), which dramatically reduces the accuracy of the model.We compare the accuracy performance and model complexity by setting different values for the maximum number of child nodes {m1,m2, …,mL−1, 0}.Fig. 11shows the accuracy comparison when setting the number of child nodes as unlimited, zeros and {3, 15, 0}. Other parameters are set the same as in previous testing: λ=25, tagged percentation is 5% for training. From Fig. 11 we can see, when we constrain the maximum number of child nodes as {3, 15, 0}, it gives us enough granular nodes (resulting in 32 granular leaf nodes) to represent the knowledge (topics/sub-topics) and better accuracy by about 0.3. When we constrain the model as allmi=0, which means the new nodes are only added when defined by the partial supervised learning, the number of leaf nodes equals the defined number of topics, then we get a worse accuracy at 0.15 because there are not enough granular leaf nodes to represent the corpus model. When we set the number of numbermi=unlimited, for all i, we get a model that is worse than the model with structural constraint of {3, 15, 0}. There are about 400 granular leaf nodes in the unlimited model which results in slightly worse accuracy; meanwhile, it slows down the learning process dramatically due to the huge size of the model.Another parameter we introduce in our proposed incremental learning is the decay factor λ. The contribution of one document decays at a rate based on Eq. (13). The larger λ is, the slower the decay is. Topic nodes are removed from the model when its number of valid documents decays below a threshold (we use 1 as the threshold). A quicker decay (with smaller value of λ) increases the adaptability however it also reduces the robustness of the model. In addition, a quicker decay (with smaller value of λ) helps the model to maintain a concise structure, however it does result in worse accuracy because of losing its robustness from the historical data. The comparison of different values of λ is shown in Fig. 12(a)–(d). From Fig. 12 we can see a gentle decay, with λ=25 results in the best accuracy (Fig. 12(a)); too quick decay with λ=2.5 (Fig. 12(b)) results in an unstable model and worse accuracy; the model with slow decay (Fig. 12(c)) and without any decay (Fig. 12(d)) loses the adaptability and again results in worse accuracy. Actually, the curve with small decay in Fig. 12(c) is quite similar to the curve without any decay in Fig. 12(d).

@&#CONCLUSIONS@&#
In this paper, we propose the incremental learning with partial supervision for the HDP. There are two aspects to this: partial supervision and incremental learning.The motivation for partial supervision is the challenge of a huge volume of new daily information (e.g. media, documents, comments and reviews from the Internet) and only having a very limited knowledge about a small portion of the information whilst wishing to incorporate the partial knowledge to guide the model learning process from all the documents. The experiment results show that, partial supervision with only a small portion of expert knowledge makes a significant contribution to improving the model accuracy and transparency (which makes the model easy to understand and interpret). It should be noted that this partial supervision also benefits the automatic mapping from the model topics to the real world topics. Without this automatic mapping we need to try to map all combinations of all model topics with all real world topics and find the best matching as the final model. However this process grows exponentially with the number of topics and quickly becomes an unrealistic task.The motivation for incremental learning is the challenge of information constantly changing with the result that the model becomes out-of-date and may not be suitable for the latest information. This in turn may result in a decline in model accuracy. The proposed incremental learning is able to reduce the contribution from out-of-date information and adjust itself to the latest information automatically. The experiment results show the advantages of the incremental learning.The key contributions of the proposed incremental learning with partial supervision are: (1) the proposed methods provide a solution for constrained sampling of multiple defined labels for a single document, which makes the sampling process for multiple-labeled documents consistent with the whole sampling process without defined labels for HDP; (2) as shown experimentally, only a small portion of supervision (labeled documents) makes a significant contribution to the resulting model in terms of both accuracy and interpretability; (3) the introduction of the concept of granular computing to the HDP model solves the interpretability problem for temporarily unknown nodes/topics and theoretically validates how to decide on the creation of a new topic.