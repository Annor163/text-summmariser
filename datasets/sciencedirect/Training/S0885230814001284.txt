@&#MAIN-TITLE@&#
Unsupervised segmentation of the vocal tract from real-time MRI sequences

@&#HIGHLIGHTS@&#
Vocal tract segmentation considering sequential nature of the RT-MRI data.Explicit consideration of vocal tract configurations with an open and closed velum.Single, high level segmentation initialisation per speaker, unsupervised operation thereafter.Small set of images for training: small manual annotation overhead.Evaluation of precision and accuracy over large image set and considering annotated images by four observers.

@&#KEYPHRASES@&#
Vocal tract,Segmentation,Real-time MRI,

@&#ABSTRACT@&#
Advances on real-time magnetic resonance imaging (RT-MRI) make it suitable to study the dynamic aspects of the upper airway. One of the main challenges concerns how to deal with the large amount of data resulting from these studies, particularly to extract relevant features for analysis such as the vocal tract profiles. A method is proposed, based on a modified active appearance model (AAM) approach, for unsupervised segmentation of the vocal tract from midsagittal RT-MRI sequences. The described approach was designed considering the low inter-frame difference. As a result, when compared to a traditional AAM approach, segmentation is performed faster and model convergence is improved, attaining good results using small training sets. The main goal is to extract the vocal tract profiles automatically, over time, providing identification of different regions of interest, to allow the study of the dynamic features of the vocal tract, for example, during speech production. The proposed method has been evaluated against vocal tract delineations manually performed by four observers, yielding good agreement.

@&#INTRODUCTION@&#
During speech production, the vocal tract configuration continuously changes over time. This dynamic nature has long been recognised as of paramount importance to study speech production (Saltzman and Munhall, 1989) and should cover the characterisation of the position and movement of the different articulators involved, such as the tongue, lips and velum (Fig. 1).Several techniques have been proposed and used to gather data to enable the study of these dynamic aspects such as electromagnetic midsagittal articulography (EMMA) (e.g., Oliveira and Teixeira, 2007) or ultrasound (e.g., Wrench et al., 2011). Even though they provide high frame rates, they present some limitations by focusing in a restricted set of regions of the vocal tract. In recent years, real-time magnetic resonance imaging (RT-MRI) has been used for speech studies (e.g., Narayanan et al., 2011; Teixeira et al., 2012), allowing enough frame rate to provide useful data regarding the position and coordination of the different articulators, over time, while potentially avoiding the hyperarticulation effect observed in sustained productions (i.e., the speaker sustains vowel production while a single static image is acquired) (Engwall, 2003). Although typically acquired at the midsagittal plane, any other plane of interest may be considered (Silva et al., 2012).A wide range of applications can profit from these dynamic studies (Höwing et al., 1999), such as the assessment of swallowing disorders (Kumar et al., 2013) or the characterisation of the articulatory properties of speech (Oliveira et al., 2012; Shosted et al., 2012; Silva et al., 2013). A notable example of the latter is the study of nasal and oral vowels (for example, considering European Portuguese, the second sound in “canto” ([I] sing) and “cato” (cactus)). These have traditionally been considered to differ essentially on the lowering of the velum, for nasal vowels, without any additional articulatory adjustment, but a few studies have recently shown evidence of modifications occurring also at the tongue and lips (Engwall et al., 2006; Carignan, 2011; Shosted et al., 2012a).The articulation of European Portuguese (EP) nasal vowels has been addressed by the authors and colleagues (Martins et al., 2008; Oliveira et al., 2009), focusing on velum dynamics or using limited tongue information obtained from EMMA studies. Real-time MRI has been acquired (Teixeira et al., 2012) to extend these studies with a characterisation of the oral configuration of EP nasal vowels, important, for example, for articulatory synthesis (Teixeira et al., 2005; Birkholz et al., 2011).The often used visual comparison among different image frames is of limited use due to the subjective nature of the comparison and the difficulty to cross compare among multiple images. Therefore, relevant data regarding the vocal tract and different articulators must be extracted for analysis. Real-time MRI studies rapidly result in several thousands of images and one of the main challenges concerns how to deal with such a large amount of data in order to extract relevant features and provide researchers with the materials and visualisations that allow systematic analysis (e.g., Silva et al., 2013).In this scenario, manual segmentation of each image is not only unthinkable, given the large number of images, but also inadvisable in a scenario where a large number of observers might be available to perform the annotation, since the noisy nature of the images makes it difficult to maintain consistency intra- and inter-observer. Therefore, a systematic (semi-)automatic method to extract the relevant data should be used.

@&#CONCLUSIONS@&#
In this paper we propose a model based method to perform the segmentation of the vocal tract from midsagittal RT-MRI image sequences. While segmentation of this kind of data has been previously addressed, that was performed without attending to the sequential nature of the data, i.e., by processing each image without accounting for its neighbours.The proposed method allowed tackling the segmentation of a large RT-MRI database by building a model-based approach using a small set of annotated images. Although not specifically included in the training set, the vocal tract configurations along the sequences, such as those exhibiting tightly closed lips, were no particular challenge to the proposed method. This was possible by exploring the small inter-frame variability, in each image sequence, using the segmentation of one image as starting point for the next. Evaluation results, comparing the segmentations provided by the proposed method with those performed by four observers, show that the presented approach performs well, providing good levels of precision, accuracy and performance. Our approach, using two different models (nasal and oral) to address the different velum configurations (open and closed), allowed accurate segmentations that provide important extra data regarding velum aperture assessment, whether it is performed visually, by a phoneticist, or using computational tools. This is also a notable difference to the works of Proctor et al. (2010), Vasconcelos et al. (2011) and Raeesy et al. (2013). One of the main positive implications resulting from the proposed method is that it allows innovative approaches regarding automatic quantitative analysis of vocal tract data, as presented by the authors in Silva et al. (2013), considering the whole database available instead of a few chosen occurrences.The image processing is performed sequentially, hindering parallel processing of the images inside a sequence which, as advocated by Bresch and Narayanan (2009), might improve performance on a cloud computing scenario. Nevertheless, we do not consider it a problem since parallel processing can still be performed for multiple sequences at once. The dominant factor in our database is not the size of each sequence, but their number. Besides, although, at this moment, we do not aim for a scenario where on-the-fly segmentation must be performed, the performance of the current implementation (in Matlab) is fast enough to provide, upon request, the segmentation of a particular image sequence (75 images) in less than four minutes.Considering the different characteristics of the images gathered in RT-MRI studies of the upper airway, due to the different acquisition protocols used, the specific AAM models created for our database are most probably not directly applicable to other upper airway RT-MRI databases. Nevertheless, we consider that the proposed methodology is general enough to be used to deal with any upper airway RT-MRI database. The only requirement is that the models are retrained using a new training set, chosen according to the same criteria used here, i.e., selecting the frames for the most distinctive vocal tract configurations in the database. This, of course, is not limited to oral and nasal vowels as is the case of our database. For example, if an RT-MRI study of the upper airway is to include the articulation of lateral sounds (Teixeira et al., 2012) (e.g., /l/ as in “sal” (salt)), since the configuration of such sounds presents very distinctive characteristics on the range of movements of the tongue tip, when compared to vowels, frames showing that configuration should also be included in the training set.