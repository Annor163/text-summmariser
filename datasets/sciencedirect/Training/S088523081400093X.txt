@&#MAIN-TITLE@&#
A survey on the application of recurrent neural networks to statistical language modeling

@&#HIGHLIGHTS@&#
We explain in detail the different steps in computing a language model based on a recurrent neural network.We survey the applications and findings based on the current literature.We survey the methods for reducing computational complexity.

@&#KEYPHRASES@&#
Recurrent neural networks,Natural language processing,Language modeling,Speech recognition,Machine translation,

@&#ABSTRACT@&#
In this paper, we present a survey on the application of recurrent neural networks to the task of statistical language modeling. Although it has been shown that these models obtain good performance on this task, often superior to other state-of-the-art techniques, they suffer from some important drawbacks, including a very long training time and limitations on the number of context words that can be taken into account in practice. Recent extensions to recurrent neural network models have been developed in an attempt to address these drawbacks. This paper gives an overview of the most important extensions. Each technique is described and its performance on statistical language modeling, as described in the existing literature, is discussed. Our structured overview makes it possible to detect the most promising techniques in the field of recurrent neural networks, applied to language modeling, but it also highlights the techniques for which further research is required.

@&#INTRODUCTION@&#
Statistical language modeling (SLM) amounts to estimating the probability distribution of various linguistic units, such as words, sentences, and whole documents (Rosenfeld, 2000). Applications of statistical language modeling include, but are not limited to, speech recognition (Jelinek, 1998; Schwenk, 2010), spelling correction (Ahmed et al., 2009), text generation (de Novais et al., 2010), machine translation (Brown et al., 1993; Och and Ney, 2002; Kirchhoff and Yang, 2005), syntactic (Huang et al., 2014) and semantic processing (Deschacht et al., 2012), optical character recognition and handwriting recognition (Vinciarelli et al., 2004).A traditional task in SLM is to model the probability that a given word appears next after a given sequence of words. From a purely linguistic point of view this task is ill-defined, since the term ‘word’ has no unique meaning (Manning and Schuetze, 1999). In fact there are at least four nonequivalent, definitions of this concept.11http://www.sussex.ac.uk/english/documents/essay—what-is-a-word.pdf.Nevertheless, the theoretical debate about the term ‘word’ has not worried machine learning researchers, who have developed practical models to deal with language. N-gram models were among the earliest techniques to model the probability of observing a given word after some previous words (Bahl et al., 1983; Jelinek, 1998; Church, 1988). The N in N-gram refers to the number of considered previous words plus the next word in the sequence. N−1 is the context length, that is the number of words that the model takes into account to estimate the probability of the next word. The estimations by N-gram models result from word co-occurrence frequencies. Basically, the probability that a certain word appears next after a given sequence of words is estimated as the number of times that the given sequence augmented with the given word appears in the training data divided by the number of times that the given sequence is present in the training data (the training data is typically a large amount of plain text in this case). In practice, the probability distributions are smoothed by assigning non-zero probabilities to words that are not present in the training data. One reason for smoothing is to compensate the very small fraction of all proper names that are mentioned in any given training data set (Kneser and Ney, 1995; Chelba et al., 2010; Moore, 2009).Soon more advanced language models were developed, including models based on decision trees (Potamianosa and Jelinek, 1998; Heeman, 1999) and maximum entropy based techniques (Rosenfeld, 1994; Peters and Klakow, 1999; Wang et al., 2005). These models allowed the incorporation of various features (e.g., part of speech tags, syntactic structure) into the language models, rather than having to rely on the words alone. Then with the widely cited work of Bengio et al. (2003), artificial neural networks found their way into the domain of SLM. Bengio et al. (2003) applied a feedforward neural network (FNN) to a training set consisting of a sequence of words, showing how the neural model could simultaneously learn the probability that a certain word appears next after a given sequence of words and at the same time learn a real valued vector representation for every word in a predefined vocabulary. Thus the FNN model learned an appropriate set of features while it was learning how to predict the next word in a sentence. In contrast, the decision tree and maximum entropy language models typically required the features to be manually engineered before any model could be trained (Heeman, 1999; Peters and Klakow, 1999).It is this ability to infer a real valued vector for each word in a vocabulary that has driven much of the recent interest in using neural networks for language modeling. Recent research suggests that such vector representations carry important linguistic information. That is, the vector representation is not just a placeholder for the corresponding word, but the different components encode useful pieces of meaning (Turian et al., 2010). For example, Collobert et al. (2011) show that using a single set of vector representations, neural networks perform well on several natural language processing tasks in the absence of any other features. Mikolov et al. (2013, 2013) show that the vector representations are at least partially compositional, for example, given the vector for King, if you subtract the vector for Man and add the vector for Woman, you end up with a vector similar to that of Queen. Recent work by Chen et al. (2013) demonstrates that the vector representations also capture relations like synonymy, antonymy and regional spelling variations.Stated in a more general way, neural networks have the capability to project the vocabulary into hidden layers, so that semantically similar words are clustered. This explains why neural networks can provide better estimates for N-grams which have never been seen during training, offering an answer to the problem of data sparseness (Bengio et al., 2003; Deoras et al., 2011; Kombrink et al., 2011).The FNNs of Bengio et al. (2003) have an important drawback: only a fixed number of previous words can be taken into account to predict the next word. This limitation is inherent to the structure of FNNs, since they lack any form of ‘memory’: only the words that are presented via the fixed number of input neurons can be used to predict the next word and all words that were presented during earlier iterations are ‘forgotten’, although these words can be essential to determine the context and thus to determine a suitable next word.Ingenious solutions were proposed to introduce some memory in the FNN architecture to overcome the limited context length. Noteworthy variants of the original FNN are the Jordan (1986) and Elman (1990) networks where extra neurons are incorporated that are connected to the hidden layer like the other input neurons. These extra neurons are called context neurons and hold the contents of one of the layers as it existed when the previous pattern was trained. This model allows a sort of short term memory. Training can still be done in the same way as for FNNs. Fig. 1is a graphical example of a Jordan network, showing the network output fed back into the context unit, which in turn sends it to the hidden layer in the next iteration. In an Elman network a context neuron is fed by a hidden layer.The context length was extended to indefinite, one could even say infinite, size by using a recurrent version of neural networks, conveniently called recurrent neural networks (RNN), which can handle arbitrary context lengths. Initial enthusiasm about RNN as statistical language modelers, mainly driven by their abilities to learn vector representations for words (as in FNN) and to handle arbitrarily long contexts, in addition to the fact that they are universal approximators (Schäfer and Zimmerman, 2007), was quickly tempered by their extremely slow learning and by the observation that the theoretical incorporation of arbitrarily long context lengths does not manifest itself in practice. Consequently, many variations on the original RNN have been developed to cope with these limitations and, at the same time, to specialize their structure towards SLM (Kombrink et al., 2011).This survey presents the main RNN architectures that resulted from the attempts to turn the original, theoretically well founded RNN with its limited practical usefulness into more practically oriented structures, where researchers were willing to exchange the theoretical soundness with heuristic reasoning (at least to some degree), but without giving up the aforementioned strengths. Each RNN architecture is compared to the original structure, and empirical evaluations, as done by researchers in the field, are presented.Section 3 describes the most commonly used measures to evaluate statistical language models and outlines some important deficiencies of these measures. In Section 4, we give a general description of basic recurrent neural networks, i.e. the RNNs as they were originally developed. The description is general in the sense that it is independent of the intended task, although we will restrict attention to supervised RNNs, since supervised machine learning techniques are much more prevalent in SLM than their unsupervised counterparts. In Section 5, we provide an in-depth account on the application of basic RNN on the task of modeling the probability of observing a word after a given sequence of words. It is shown how basic RNNs can be applied to perform this task and how these models perform in comparison to other models. This section ends with three main limitations of a basic RNN: long training times, the need to choose a fixed number of hidden neurons in advance, and the observation that in practical applications the context length that is taken into account is small. Sections 6–8 discuss the most important methods that have been developed to overcome these limitations. Although the paper focuses on the training of recurrent neural networks in the context of language modeling, Section 9 describes some important approaches for decoding when using RNN based language models. In Section 10 we discuss two extensions of the basic RNN that came into existence to further increase the performance of RNN. Finally, Section 11 concludes the paper and provides some further research directions.The most commonly used measures to evaluate language models are perplexity (PPL) and, for speech recognition systems, word error rate (WER) (Jelinek, 1998). The PPL measure is the inverse of the geometric average probability assigned by the model to each word in a test data set, given some sequence of previous words. The popularity of this measure for evaluating language models is due to the fact that it allows an easy comparison between different models. WER directly measures the quality of the speech recognition system, by counting the number of mistakes between the output of the system and the reference transcription which is provided by a human annotator.As these measures are widely used, they will pop up frequently in the evaluation of RNNs below. However, one should be aware that these measures have been widely criticized. For example, numerous examples are known where language models provide a large improvement in perplexity over some baseline model, but with no or little improvement in the word error rate (Clarkson and Robinson, 1998; Iyer et al., 1997; Martin et al., 1997). Furthermore, comparisons between different models in terms of perplexity requires great care, since the perplexity depends not only on the language model but also on the training data and on the underlying vocabulary. Consequently, comparisons are only meaningful when the same data and the same vocabulary are used, a fact which is often overlooked. The WER, on the other hand, puts an overemphasis on frequent, but uninformative words, and some techniques can yield large WER improvements when applied to simple systems, while showing no improvement at all for more complex systems. Comparison of relative WER reductions when applying different techniques to different systems is practically useless (Mikolov, 2012), again a crucial aspect that is all too often not taken into account by researchers in the field.These critiques have motivated researchers to develop new evaluation measures. In Clarkson and Robinson (2001), the authors point out that the calculation of perplexity is based solely on the probabilities of words contained within the test text, thereby disregarding the probabilities of alternative words which will be competing with the correct word within the decoder. They show that by considering the probabilities of the alternative words it is possible to derive measures of language model quality which are better correlated with word error rate than perplexity is. It is argued that optimizing language model parameters with respect to these new measures leads to a significant reduction in the word error rate. Another attempt to extend perplexity to take more information into account is described in Chen et al. (1998). Unfortunately, these more advanced evaluation measures haven’t been picked up by language modelers yet, despite the fact that these measures compensate for some essential drawbacks of PPL and WER. While WER measures the dissimilarity between a system's output and the expected ground truth, the BLEU (Bilingual Evaluation Understudy) metric measures the similarity between a system's output and the ground truth (Papineni et al., 2002). The latter metric is widely used in machine translation. Another error metric is TER (Translation Error Rate) that measures the number of edits required to change a system's output into the ground truth output (Snover et al., 2006).Recurrent neural networks are trained via a sequence of training examples ((x(1), y(1)), (x(2), y(2)), …, (x(m), y(m))) withx(t)∈ℝnI,y(t)∈ℝnJfor 1≤t≤m. The vectors x(t) are given as inputs to the network, while the vectors y(t) denote the target output. A subscript will be used to refer to a specific component of a vector, e.g. xi(t) refers to the ith component of x(t). The network calculates output valueso1(t),…,onJ(t)in several steps:(1)aj(t)=∑i=1nIαjixi(t)+∑i=1nHρjihi(t−1),j=1,…,nH(2)hj(t)=F(aj(t)),j=1,…,nH(3)bj(t)=∑i=1nHβjihi(t),j=1,…,nJ(4)oj(t)=G(bj(t)),j=1,…,nJwhere αji, βji, ρjiare parameters, also called weights, to be learned by the system. It is customary to view such a network as consisting of neuron-like units, called neurons, that receive inputs, perform some transformation, and either send the result to other neurons or present the result as the output of the network. Especially noteworthy are the so-called hidden neurons that receive the input vector x(t), calculate a linear combination of the individual components, followed by a nonlinear transformation F and send the result to the so-called output neurons that will calculate the output values oj(t), j=1, …, nJ. The functions F and G are (typically) nonlinear functions to be determined by the user. The function F is often chosen as the sigmoid:(5)F(x)=11+e−xor as the hyperbolic tangent:F(x)=ex−e−xex+e−xTo speed up computation time, the tangent hyperbolic can be approximated by the hard tangent hyperbolic (Collobert et al., 2011):F(x)=−1ifx<−1=xif−1≤x≤1=1ifx>1A popular choice for the output function G, especially in the context of statistical language modeling, is the softmax function:(6)G(bj(t))=ebj(t)∑q=1nJebq(t)The number of hidden neurons nHis determined in advance by the user and is fixed during training. Unfortunately, there does not exist any generally accepted rule of thumb, let alone a general theory. In Sheela and Deepa (2013), a review is presented on methods that heuristically determine the number of hidden neurons for neural networks. Some simple heuristics are described in Panchal et al. (2011):1.The number of hidden neurons should be between the number of input components and the number of output components.The number of hidden neurons should be 2/3 of the number of the input components, plus the number of output components.The number of hidden neurons should be less than twice the number of the input components.A graphical representation of a basic RNN is given in Fig. 2(a). This figure shows that the hidden neurons receive input values from both the input neurons and the hidden neurons. This is in contrast to feedforward neural networks that only receive input values from the input neurons, as shown in Fig. 2(b).We focus our attention on the SLM task of modeling the probability of observing a word after a given sequence of words. The reasons for this restriction are (1) that this task is a basic one in the domain of SLM and (2) we think that a detailed account on the application of basic RNN to one specific task will provide more insight into this model than providing a shallow description on a range of tasks (space constraints prevent the detailed description of basic RNN on all SLM tasks where they can be useful). For convenience, we refer to this task as the basic SLM task.Training data for SLM are usually taken from corpora, i.e. large and structured sets of texts. Frequently used corpora are the Reuters corpus22http://about.reuters.com/researchandstandards/corpus/.(containing Reuters News stories), the Wikicorpus33http://www.lsi.upc.edu/∼nlp/wikicorpus/.(with large portions of Wikipedia), the Brown corpus44http://icame.uib.no/brown/bcm.html.(consisting of 500 samples of English-language text, distributed across 15 genres, totaling roughly one million words), The Wall Street Journal (WSJ) corpus (distributed by LDC, that consists of read speech with texts drawn from a machine-readable corpus of Wall Street Journal news texts), and English Gigaword55http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2011T07.(containing over ten million documents from seven news sources).The text documents contained in corpora need some preprocessing before they can be used by neural networks. It is customary to construct a fictitious vocabulary V containing all distinct words that are present in a considered corpus (or part of a corpus), in an arbitrary but fixed order. This requires the splitting of the corpus into individual words. One widely used tool to perform this task is the Stanford Parser.66http://nlp.stanford.edu/software/lex-parser.shtml.Depending on the application, punctuation symbols may or may not be removed. To this vocabulary one adds a placeholder word, often denoted as the word UNK (unknown word), to represent all words that are seen in a test data set, after training, but that are not contained in the vocabulary. It is convenient to give the placeholder word UNK the labelw1and all the other words labelsw2,…,wN. Thus we have thatV=(w1,…,wN).Each word in the vocabulary is then represented as a vector with N components via the following function τ:τ(w1)=(1,0,0,…,0)τ(w2)=(0,1,0,…,0)⋮τ(wN)=(0,0,…,0,1)This function implements the so-called 1-of-N coding which provides a suitable representation of words that can be processed by a neural network. It is clear that τ is invertible, which allows us to designate a 1-of-N coding vector with the corresponding word, for convenience. Thus we feel free to use such expressions as ‘a word is given as input to the network’ as shorthand for ‘a 1-of-N coding vector of a word is given as input to the network’.The corpus itself can, after tokenization and defining the vocabulary, be represented as a sequence of vectors, by replacing each word in the text by its corresponding 1-of-N coding vector. We denote this sequence as D=(ω1, …,ωn), where n is the number of words in the corpus.For the described task the training examples are all pairs (ωi,ωi+1), i=1, …, n−1. The vectorωi, corresponding to the ith word in the considered corpus, is given as input, whileωi+1 serves as target output. Providing the network with training examples of this form and choosing the output function as the softmax function (6) ensures that the network will learn the task of modeling the probability of a given word as the next word after a given sequence of previous words. Although the input of each training example contains only one word, the recurrent structure of the network implies that the network does not only use the previous word, but also incorporates some other contextual information, to construct the required probability distributions. This will be made clear below.As the training examples are presented to the network in a definite order, we can denote the training data set asT=((x(1),y(1)),…,(x(|T|),y(|T|))), with|T|denoting the number of training instances inT. Notice that we denote the ith training example by (x(i), y(i)) and not by (ωi,ωi+1), since it is not required that words are presented to the network in the same order as they appear in the corpus. We come back to this point in the next section.If all instances belonging toThave been presented to the network and the weights have not yet converged to final values, we continue training by presenting (x(1), y(1)) again to the network, followed by (x(2), y(2)), and so on. The term epoch is used to denote the presentation of all examples inTonce to the network.Training is stopped when a certain stopping criterion is met. For example, training can be stopped when the decrease in training error is smaller than a certain threshold. A subset of the training data called validation set is usually reserved for validating the model by measuring the training error on this data.The parameters of a neural network are updated each time a certain, not necessarily constant, number of training instances have been presented to the network. We refer with the term batch to the presentation of such a number of training instances. Thus each time a batch has been processed, the network parameters are updated. This implies that the training data setTcan be considered as consisting of batches B(1), …, B(L), i.e.T=(B(1),…,B(L))where the ith batch B(i) is of the form(x(j),y(j)),(x(j+1),y(j+1)),…,(x(|B(i)|+j−1),y(|B(i)|+j−1))and where |B(i)| denotes the number of training instances in B(i). A batch can be as small as one training instance or as large as the training data itself.Although a batch can contain arbitrarily selected training examples fromT, an obvious choice is to let a batch consist of training instances corresponding to a fixed number of consecutive sentences in the given text. This is referred to as the consecutive scheme. If the batches B(1), B(2), …B(L) are also in the same order as the sentences in the text, then this is referred to as the genuine consecutive scheme. Alternatively, batches can consist of training examples coming from randomly selected sentences in the given text. This is referred to as the random scheme. Some researchers claim that randomizing the order of the sentences results in faster training (Bengio et al., 2003; Mikolov et al., 2011). Note that only in the genuine consecutive scheme it is true that (x(i), y(i))=(ωi,ωi+1).It is convenient to extend the notation for a training instance to incorporate the batch to which it belongs. Thus we will use the notation (x(k, t), y(k, t)) to refer to the tth training instance of the kth batch.Given that a training example is of the form (ωi,ωi+1), see Section 5.3, withωiandωi+1 1-of-N coding vectors of words from the vocabulary, we have that nI=N=|V| and nJ=N=|V|, where nIand nJdenote the number of input and output components of an RNN (see Section 4). Since we use the notation (x(k, t), y(k, t)) to denote an arbitrary training example, see Section 5.4, the description of an RNN (1)–(4) takes the following form when applied to the basic SLM task:(7)aj(k,t)=∑i=1|V|αji(k)xi(k,t)+∑i=1nHρji(k)hi(k,t−1),j=1,…,nH(8)hj(k,t)=F(aj(k,t)),j=1,…,nH(9)bj(k,t)=∑i=1nHβji(k)hi(k,t),j=1,…,|V|(10)oj(k,t)=G(bj(k,t)),j=1,…,|V|The parameters to be learned are αji, j=1, …nH, i=1, …, |V|, ρji, j=1, …, nH, i=1, …, nHand βji, j=1, …, |V|, i=1, …, nH. These parameter values are typically randomly initialized. A common choice is the Gaussian distribution. For example, in Mikolov (2012) a normal distribution with mean 0 and variance 0.1 is used. Another distribution that is sometimes chosen to initialize the parameter values is the uniform distribution over some interval [−δ, δ], δ>0 (Rojas, 1996). More sophisticated initialization methods can be found in, for example, Fernández-Redondo and Hernández-Espinosa (2000) and Marichal et al. (2007). A lot of practical ‘tricks’ in applying back-propagation learning can be found in LeCun et al. (1998). As outlined above, the parameter values are updated every time a batch has been processed.The number of hidden units nHmust be chosen by the user, as discussed in Section 4.It is customary to define hi(1, 0)=0 for all i=1, …, nH. For any other batch B(k), k>1, the initial values depend on the scheme that is used.First, if the genuine consecutive scheme is used it is natural to define hi(k, 0)=hi(k−1, |B(k−1)|), since in this case the order in which words are presented to the network equals the order in which they appear in the given text. Loosely speaking, in this scheme all words from the given text are supplied to the system, one by one, in the order in which they appear in the given text, and by defining hi(k, 0)=hi(k−1, |B(k−1)|), the network will calculate the output based on the given word and all previous words in the text.Second, if the consecutive scheme is used, sentences contained in a batch correspond to consecutive sentences in the text, but the last sentence of a certain batch and the first sentence of the next batch are not consecutive. Thus when a new batch is started, its context cannot be considered as contained in the previous batches. In this case it is natural to define hi(k, 0)=0. This implies that when a word is given as input to the network, only the previous words contained in the same batch are taken as context into account.Third, if the random scheme is used, we also define hi(k, 0)=0. In this case, it is even natural that when a training example (x(k, t), y(k, t)) is given to the network for which x(k, t) corresponds to the first word of a sentence in the text, to define hi(k, t−1)=0 for all i.A natural question is why one would choose the consecutive scheme or random scheme, given the fact that in these schemes the context that is taken into account is limited, and does not go all the way back to the first word of the text as in the genuine consecutive scheme? An important reason is that by defining hi(k, 0)=hi(k−1, |B(k−1)|) rounding errors are carried over batches and get accumulated. The same applies to other types of errors, for example, errors in the tool that is used to split the given text into individual words. Furthermore, the fact that in the genuine consecutive scheme all previous words are taken into account in calculating the output should not be interpreted as saying that all previous words are given as input. Rather, one word is given as input and there is only an influence of all previous words. Thus the context size that is effectively taken into account in the genuine consecutive scheme does not necessarily exceed the context size in the other two schemes in a significant way. We continue this discussion in the following section. For convenience, we assume the genuine consecutive scheme in the remainder of this section, since in this scheme the words are presented to the network in the order in which they appear in the corpus, which is intuitively appealing.Using the softmax (6) as output function ensures that the outputs o1(k, t), …, o|V|(k, t) can be interpreted as the probabilities that each of the words in the vocabulary V appears next after a certain sequence of previous words. However, since the input to an RNN is limited to one word and since the recurrent term hi(k, t−1) in Eq. (7) only ensures that nonlinear transformations of previous words are cycled through the network, we can informally state that the context that is taken into account is something between the previous word and all previous words. More formally, when a trained network has been presented a sequence of words(τ(wj1),…,τ(wjm−1)), withwji∈V, and it is now given the wordτ(wjm), the produced output is not to be interpreted asP(Wjm+1=w1|wj1…,wjm),…,P(Wj+1=w|V||wj1,…,wjm), where Wj+1 represents the word to appear next after the given sequence as a random variable. Rather, there exist functions f1, …, f|V| such that the outputs can be interpreted asPWj+1=wi|fi(wj1,…,wjm−1),wjm,i=1,…,|V|The error function that is widely used on the basic SLM task is cross-entropy (Frinken et al., 2012). For a given training example (x(k, t), y(k, t)), this criterion defines the error between the target output y(k, t) and the produced output (o1(k, t), …, o|V|(k, t)) as:(11)E(k,t)=−∑p=1|V|lnop(k,t)yp(k,t)Since y(k, t) is the 1-of-N coding vector of a certain word in V, only the component corresponding to the wordw∈Vfor whichτ(w)=y(k,t)is one, and all other components are zero. Thus we can write E(k, t) also as(12)E(k,t)=−∑p=1|V|lnop(k,t)Δp(k,t)where(13)Δp(k,t)=1ifyp(k,t)=1(14)=0otherwiseThe error associated with a batch B(k) is then defined as E(k)=∑tE(k, t) and the error associated with an epoch is analogously defined as E=∑kE(k). When referring to ‘error’ or ‘training error’ further in this paper, we specifically mean the error associated with a batch, i.e. E(k), since it is after having processed the training examples in a batch that the parameter values are updated. After having processed a batch we typically want to know the current error in training the system and naturally we then take E(k) to mean this current error.If we have a validation data set, we can evaluate the generalization capability of the network at any appropriate moment during training. The validation error is typically calculated after some batch B(k) has been processed, by giving all examples fromVto the network, and then by calculating the associated errorEv(k)made by the network in the same way as E(k) is calculated:(15)Ev(k)=−∑p=1Mv∑j=1|V|lnoj(k,p)Δj(k,p)whereEv(k)thus denotes the validation error and where it is implicitly understood that the network is applied on the examples inVwith parameter values αji(k), βji(k) and ρji(k). The importance of the validation error is that it can be considered as an approximation for the generalization error.Many methods have been developed to update the parameters in such a way that the training error is minimized. Update rules that are extensions of gradient descent can be considered as the most basic. In gradient descent a parameter p is updated as(16)p(k+1)=p(k)−λ∂E(k)∂p(k)where p(k+1) is the value of parameter p that will be used by the network to process training examples from batch B(k+1), and where p refers to either αjior βjior ρji. The parameter 0<λ<1 is called the learning rate and can be chosen as, for example, 0.1 (Mikolov, 2012). The gradients ∂E(k)/∂p(k) are derived in Appendix A.Although the update rule above is often used in RNN and other types of neural networks, it is well known that a fixed learning rate tends to make the learning process inefficient (Polak, 1997). Too low of a learning rate makes the network learn very slowly, while too high of a learning rate makes the weights and objective function diverge, implying that there is no learning at all. This can be solved by adapting the learning rate during training, for example by reducing λ as 1/t (Seung, 2002).Alternatively, the learning rate is kept fixed, but the update rule (16) is extended with the inclusion of a momentum term μ∈[0, 1], such that a parameter p is updated asp(k+1)=p(k)+μp(k)−p(k−1)−λ∂E(k)∂p(k)It has been found that such a momentum term increases the rate of convergence dramatically (Rumelhart et al., 1986). Typical choices for λ and μ in the above update rule are λ=0.1, μ=0.9 (Matignon, 2005).Many other advanced update rules have been developed. As an illustration, ALAP1 (Almeida et al., 1998) updates a parameter asp(k+1)=p(k)−λ(k)∂E(k)∂p(k)with variable learning rate λ(k). After all parameters have been updated, the learning rate is updated via the ruleλ(k+1)=λ(k)+γ<∇E(k),∇E(k−1)>where <.,.> denotes the inner product and γ denotes a positive constant, e.g. γ=0.1.In all the update rules above, the derivative∂E(k)∂p(k)is needed. This derivative with respect to αji(k) requires one to calculateδjh(k,t)=∂E(k,t)∂aj(k,t), which in turn requires one to calculateδph(k,t+1)=∂E(k,t+1)∂ap(k,t+1),p=1,…,nH(see Eqs. (A.9) and (A.3) in Appendix A). Consequently, parameters can only be updated after all subsequent training examples from the current batch are processed by the system.More concretely, parameters are updated in two passes, a forward pass and a backward pass. In the forward pass, the values aj(k, t), hj(k, t), bj(k, t) and oj(k, t) are calculated according to (7)–(10) and this over the training examples (x(k, t), y(k, t)), t=1, …, |B(k)|. This provides all necessary values to calculateδjh(k,t), as defined in (A.3), recursively, starting fromδjh(k,|B(k)|), then calculatingδjh(k,|B(k)|−1), and so on, up toδjh(k,1). Each valueδjh(k,t)uses the valuesδjh(k,t+1),j=1,…,nH, which is why this pass is performed backwards. Notice that to calculateδjh(k,|B(k)|)we need to defineδjh(k,|B(k)|+1). These values are typically defined as zero. The valuesδjh(k,t)are then used to update ρji(k) and αji(k), which follow from Eqs. (A.8) and (A.9) in Appendix A. Eq. (A.7) shows that to update βji(k) only a forward pass is needed.Training is stopped when a certain stopping criterion is met. For example, training can be stopped when the decrease in training error is smaller than a certain threshold. This condition can be made more severe by requiring that the training error is smaller than a certain threshold for some predetermined number S of consecutive batches. However, to avoid overfitting it is necessary to incorporate the validation error into the stopping criterion. Training is then typically stopped whenever the validation error appears to start increasing, for example if the validation error is larger than the previous validation error. The rationale is that at the start of the training, the network is not yet adapted to the intended task, which will result in large errors onTandV. These errors will decrease as the network is trained. While the training error keeps decreasing, the validation error will start to increase after some learning period, when overfitting occurs.In Prechelt (1997), it is rightly pointed out that the validation error does not necessarily show a smooth decrease-increase behavior. The fact that the validation error may show a jagged pattern can be accounted for by continuing training until the training error meets some stopping criterion, but to select as optimal parameter values those values for which the corresponding validation error is the smallest among all calculated validation errors. One implementation of this strategy is to define a threshold ϵ and to stop training when E(k)/E(k−1)<ϵ. The optimal parameter values are then taken as the parameter values corresponding to the batch κ for which it holds thatκ=argminkEv(k), where k runs over all batches that were processed during the training process. This strategy requires one to keep in memory all parameter values that were calculated during training.We provide a training algorithm for the basic recurrent neural network described above applied to the basic SLM task. To be concrete, we choose as update rule gradient descent with momentum (17) and initialize the parameter values according to the normal distribution with mean 0 and variance 0.1. Furthermore, the batches are assumed to be arranged according to the consecutive scheme (see Section 5.4; if another scheme is used, Section 5.7 should be taken into account).This is not to say that this is the best training algorithm. More advanced training algorithms could result in better optima (although often at the cost of increased training time), but the algorithm below is advanced enough to be useful in practice, and at the same time it is basic enough to provide insight into the implementation of a host of more advanced extensions on it.Algorithm 11.Decide on the following:•A corpus (see Section 5.1).The value for nH, i.e. the number of hidden neurons.The values for λ and μ in Eq. (17).A value ϵ>0 that is used as threshold on the training error to stop training.A value forS∈ℕthat indicates the number of consecutive times that the training error should be below the threshold before training is stopped. We can choose S=1, but because in practice the training error will show a jagged behavior, it is advised to consider a larger S.The number of consecutive sentences contained in a batch.Preprocess the data set. Split the text into words(w1,…,wN), construct the vocabulary V, and convert the wordswito the corresponding 1-of-N codingωi=τ(wi), as outlined in Section 5.2.Split the data into training data and validation data, and divide the training data into batches, as outlined in Sections 5.3 and 5.4.Initialize the parameter values. Define αji(1), βji(1) and ρji(1) as random values from the normal distribution with mean 0 and variance 0.1. Using gradient descent with momentum, we also need to initialize the parameters for the second batch. It is common to initialize them to be equal to the parameter values for the first batch, i.e. αji(2)=αji(1), βji(2)=βji(1) and ρji(2)=ρji(1).Initialize k=1, where k is the index that keeps track of the batches and initialize s=0 which will be used to keep track of the current number of consecutive times that the training error is below the threshold.If no batch has been selected before or if the previously selected batch was the last batch from the training data set, define B(k) as the first batch from the training data set. Otherwise, define B(k) as the next batch from the training data set. Initializehi(k,0)=0,i=1,…,nHδih(k,|B(k)|+1)=0,i=1,…,nHt=1Consider training example (x(k, t), y(k, t)) from B(k). Do the following calculations:aj(k,t)=∑i=1|V|αji(k)Δi(k,t)+∑i=1nHρji(k)hi(k,t−1),j=1,…,nHhj(k,t)=F(aj(k,t)),j=1,…,nHbj(k,t)=∑i=1nHβji(k)hi(k,t),j=1,…,|V|oj(k,t)=G(bj(k,t)),j=1,…,|V|δjo(k,t)=oj(k,t)−Δj(k,t),j=1,…,|V|E(k,t)=−∑j=1|V|lnoj(k,t)Δj(k,t)Keep in memory the following values:hj(k,t),j=1,…,nHoj(k,t),j=1,…,|V|δjo(k,t),j=1,…,|V|E(k,t)If t=|B(k)|, go to the next step. Otherwise, increase t by 1 and go to step 1.Calculate the training error corresponding to the current batch B(k):(17)E(k)=∑t=1|B(k)|E(k,t)Keep in memory the following values:αji(k),j=1,…,nH,i=1,…,|V|ρji(k),j=1,…,|V|,i=1,…,nHβji(k),j=1,…,|V|,i=1,…,nHCalculate the error on the validation set,Ev(k), as given by Eq. (15). Keep this value in memory.Check whether E(k)/E(k−1)<ϵ. If so, increase s by 1. If not, set s=0.If s=S, stop training. Find the minimum validation errorEv(k). Return the corresponding parameter values αji(k), βji(k) and ρji(k) as optimal parameter values.If s<S go to the next step.Perform the following calculations:δjh(k,t)=hj(k,t)(1−hj(k,t))∑p=1|V|δpo(k,t)βpj(k)+∑p=1nHδph(k,t+1)ρpj(k)for j=1, …, nH. Keep the valuesδjh(k,t)in memory.Reduce t by 1.If t=0 go to the next step. Otherwise, go to step 15.If k>2, calculate ∂E(k)/βji(k), ∂E(k)/ρji(k) and ∂E(k)/αji(k), as given by Eqs. (A.7)–(A.9).If k>2 update the parameter values asβji(k+1)=βji(k)+μβji(k)−βji(k−1)−λ∂E(k)∂βji(k),j=1,…,|V|,i=1,…,nHρji(k+1)=ρji(k)+μρji(k)−ρji(k−1)−λ∂E(k)∂ρji(k),j=1,…,nH,i=1,…,nHαji(k+1)=αji(k)+μαji(k)−αji(k−1)−λ∂E(k)∂αji(k),j=1,…,nH,i=1,…,|V|Otherwise defineβji(k+1)=βji(k)ρji(k+1)=ρji(k)αji(k+1)=αji(k)Go to step 6.In Mikolov (2010), several configurations of the basic recurrent neural network (e.g. different numbers of hidden neurons) are trained to perform several speech recognition tasks. Training data was derived from the NYT section of English Gigaword. Several baseline models were considered, but only the modified Kneser-Ney smoothed 5-gram model (Kneser and Ney, 1995), hereafter KN5 for short, is compared to the basic RNN. The other baseline models are compared to more advanced RNNs, which we consider below. PPL and WER were used to evaluate the performance. It was found that the basic RNN outperforms the KN5 model in terms of these measures. The values of the evaluation measures for the worst performing RNN (containing the smallest amount of hidden neurons, namely 60) were PPL=229 and WER=13.2, while the KN5 model had values PPL=221, WER=13.5. The best configuration (with the largest amount of hidden neurons, namely 400) significantly outperformed the KN5 model: PPL=171, WER=12.5.In Mikolov et al. (2011) and Mikolov (2012), an extensive experiment was performed on the Penn Treebank portion of the WSJ corpus. RNN were compared to several N-gram models and to more advanced models, namely the maximum entropy model (Rosenfeld, 1994), the random clusterings language model (LM) (Emami and Jelinek, 2005), the random forest LM (Xu, 2005), the structured LM developed by Filimonov and Harper (2009), and the within and across sentence boundary LM (Momtazi et al., 2010). The authors observed that the basic RNN had a lower perplexity than all other models, except for the within and across sentence boundary LM which had a perplexity of 116.6, while the basic RNN had a perplexity of 124.7.An experimental evaluation in the domain of machine translation was performed in Le et al. (2012). It was observed that a FNN with context length 10 performed slightly better in terms of PPL than a RNN on a large-scale English to French translation task. Performance in terms of BLEU was identical for both network architectures.

@&#CONCLUSIONS@&#
