@&#MAIN-TITLE@&#
Robust combinatorial optimization with variable cost uncertainty

@&#HIGHLIGHTS@&#
We provide a new bound for Value-at-Risk combinatorial optimization problems.The new bound is compared to the classical one theoretically and numerically.Different solution algorithms are proposed for the resulting problem.

@&#KEYPHRASES@&#
Combinatorial optimization,Robust optimization,Dynamic programming,Price of robustness,Budgeted uncertainty,

@&#ABSTRACT@&#
We present in this paper a new model for robust combinatorial optimization with cost uncertainty that generalizes the classical budgeted uncertainty set. We suppose here that the budget of uncertainty is given by a function of the problem variables, yielding an uncertainty multifunction. The new model is less conservative than the classical model and approximates better Value-at-Risk objective functions, especially for vectors with few non-zero components. An example of budget function is constructed from the probabilistic bounds computed by Bertsimas and Sim. We provide an asymptotically tight bound for the cost reduction obtained with the new model. We turn then to the tractability of the resulting optimization problems. We show that when the budget function is affine, the resulting optimization problems can be solved by solvingn+1deterministic problems. We propose combinatorial algorithms to handle problems with more general budget functions. We also adapt existing dynamic programming algorithms to solve faster the robust counterparts of optimization problems, which can be applied both to the traditional budgeted uncertainty model and to our new model. We evaluate numerically the reduction in the price of robustness obtained with the new model on the shortest path problem and on a survivable network design problem.

@&#INTRODUCTION@&#
LetX⊂{0,1}nbe the feasibility set of a combinatorial optimization problem and c be a cost vector inRn. We study in this paper optimization problems of the form(1)CO≡minx∈XcTxin the situation where the cost c is uncertain. If the coefficients of c are described by random variables, a classical approach replaces CO by its Value-at-Risk version:(2)minx∈XVaR∊(cTx),whereVaR∊(cTx)=inf{t|P(cTx⩽t)⩾1-∊}. Hence, (2) tries to find the best solution cost that is guaranteed with a probability of1-∊. WhileVaRis very popular in the financial industry (Cornuejols & Tutuncu, 2006), it suffers from several drawbacks, among which: (i) the probability distributions of the random parameters are often impossible to describe with precision and (ii) the resulting optimization problems are very hard to solve exactly unless strong assumptions are made on the probability distributions.In contrast, robust optimization relaxes the knowledge about the components of c by only assuming that c belongs to a convex, closed, and bounded setU⊂Rn. The problem turns then to looking for a feasible solution that minimizes its worst-case cost:(3)minx∈Xmaxc∈UcTx.This approach has several advantages, the first of which is its numerical complexity. Given a positive parameterΓand boundsc¯iandc¯i+cˆifor eachi=1,…,n, Bertsimas and Sim (2003) consider the budgeted uncertainty polytope(4)UΓ≔c∈Rn:ci=c¯i+δicˆi,0⩽δi⩽1,∑δi⩽Γ,and prove that problem(5)COΓ≡minx∈Xmaxc∈UΓcTxpertains to the same complexity class as CO. For general uncertainty polytopesU, the linear programming relaxation of problem (3) can be reformulated as a linear program, while the discrete problem (3) can be reformulated as a mixed-integer linear program. These approaches have made it possible to solve a large variety of robust combinatorial optimization problems. Recent works (Büsing & D’Andreagiovanni, 2012; Mattia, 2012) have independently extended the properties ofUΓto richer classes of uncertainty polytopes.Another advantage of robust optimization lies in its less specific assumptions on the uncertainty. In many applications, it is not realistic or not possible to describe the uncertainty on the parameters by unique probability distributions. In these settings, it is sometimes more convenient to restrict our knowledge to the description of the possible outcomes by providing an uncertainty setUwithout providing any probability weight. There exist also intermediary models that assume partial knowledge on the distribution of the uncertain parameters. These models are usually cited as ambiguous chance constraints (Erdogan & Iyengar, 2006) or distributionally robust optimization problems (pioneered by Z̆ác̆ková (1966)).Among the large literature on robust optimization, researchers have proposed uncertainty sets that allow a robust constraint to approximate a chance constraint or an ambiguous chance constraint in the following sense: any solution to the robust constraint will be feasible for the original chance constraint or ambiguous chance constraint. In particular, Bertsimas and Sim (2004) have proved that uncertainty setUΓapproximates an ambiguous chance constraint where the coefficients are described by bounded random perturbations that are only assumed to be symmetrically and independently distributed around their means. Their result yields the following relation between problems (2) and (3): the optimal solution of problem (3) is an upper bound for the optimal solution of problem (2) when the coefficients of c can be any random variables symmetrically and independently distributed around their means.In a recent work, Poss (2013) mentions that the bound provided by Bertsimas and Sim is too conservative for feasible solutions with few non-zero components. In fact, for problems whose optimal solutions have small cardinalities compared to n, the probabilistic bounds from Bertsimas and Sim can be meaningless because the bounds would prescribe values forΓthat are greater than the solutions cardinalities. This motivates the introduction by Poss (2013) of a more general uncertainty model where the amount of uncertainty is not bound by a static number but by a non-negative functionγ(x)defined on the feasibility region X. Namely, the author defines variable budgeted uncertainty as multifunctionUγ:⇉Rn, defined as follows:(6)Uγ(x)≔c∈Rn:ci=c¯i+δicˆi,0⩽δi⩽1,∑δi⩽γ(x).The uncertainty model (6) is tested on the robust knapsack problem showing a reduction of the price-of-robustness by an average factor of 18% for little increase in computational time.In this paper, we apply uncertainty modelUγto optimization problems with uncertain costs, yielding(7)COγ≡minx∈Xmaxc∈Uγ(x)cTx.We study the properties and the computational complexity ofCOγand present numerical examples showing the cost reduction obtained when using modelCOγinstead of modelCOΓ.In the rest of the paper,‖x‖=∑i=1n|xi|denotes the usualℓ1-norm. We also denote byz(COγ)andz(COΓ)the optimal solutions costs of problemsCOγandCOΓ, respectively. Finally, we assume throughout that functionγsatisfies the following:1.The functionγis non-decreasing.0⩽γ(‖x‖)⩽‖x‖for eachx∈X.For anyx1,x2∈{0,1}nsuch that‖x1‖=‖x2‖,γ(x1)=γ(x2).In view of the third property above, we sometimes commit an abuse of notation and denoteγ(k)withk∈Z. These assumptions are natural in our context since the introduction of the functionγshould modulate the size of the uncertainty set according to the cardinality of x. In particular, the functions prescribed by the probabilistic bounds from Bertsimas and Sim (2004) and introduced in Section 2.1 satisfy these assumptions.Our contributions lie in the study of the properties and solution methods of problemCOγas well as on its numerical assessment on two combinatorial optimization problems from the literature. Section 2 starts with the study of two important properties of modelCOγ. Section 2.1 applies the techniques from Poss (2013) to explain how we can defineγto obtain a better bound forminx∈XVaR∊(cTx)than the classical budgeted uncertainty model. We provide then in Section 2.2 an asymptotically tight bound (asymptotic with respect to n) for the maximum cost reduction one can obtain when solvingCOγinstead ofCOΓ. Up to our knowledge, Monaci and Pferschy (2013) is the only prior work where a theoretical study of price of robustness is proposed.In Sections 3 and 4, we turn to the study of the computational complexity ofCOγ. In Section 3, we reduce problemCOγto solving a sequence of problems CO or its cardinality-constrained version, which depends on whetherγis affine or not. In Section 4, we focus on problems CO for which efficient dynamic programming algorithms are available. We adapt these algorithms to solve the robust versionsCOΓandCOγ. The contributions of Sections 3 and 4 are summarized in Table 1where we provide the complexity ofCOγdepending on the existence of a dynamic programming algorithm for CO and on the type of functionγ(affine or integer-valued). The values reported in the column “Complexity” assume thatγhas been chosen in accordance with the simplest probabilistic bound from Bertsimas and Sim (2004);τandτ⩽denote the times for solving CO and its cardinality-constrained version, respectively. Recalling from Bertsimas and Sim (2003) thatCOΓcan be solved inO(nτ), we see from Table 1 that whenγis affine the solution times ofCOΓandCOγare of the same order of magnitude.In addition to these results, our dynamic programming approach from Section 4 also applies to the classical robust modelCOΓstudied by Bertsimas and Sim (2003). We show that whenΓ∈Z, problemCOΓcan be solved inO(n1/2τ), improving over the solution time ofO(nτ)from Bertsimas and Sim (2003). Hence, our result extends to a large class of dynamic programming algorithms the ideas proposed by Klopfenstein and Nace (2008) and Monaci, Pferschy, and Serafini (2013) for the robust knapsack problem, by using a more general description of dynamic programming.In Section 5, we present a numerical comparison of modelsCOΓandCOγon the shortest path problem and on the hop-constrained path diversified network design problem. The main objective of our experiments is to evaluate the cost reduction obtained by using the new model. Finally, we conclude the paper in Section 6, and present a MILP formulationCOγand technical proofs in Appendices A–D.The main motivation of modelUγcomes from the probabilistic bounds computed by Bertsimas and Sim (2004) and extended to variable uncertainty by Poss (2013). We recall below how the strongest of these bounds applies to the problem of minimizing the value-at-risk. Namely, we show that for a properly chosen function, denotedβin what follows, the optimal solution of robust modelCOβprovides a less conservative solution for the stochastic problem (2) than modelCOΓ. We sketch next how to construct functionβ, which does not have an analytical expression. The full details of the construction can be found in Poss (2013, Section 3), where the author also introduces another function,α, which has an analytical expression but leads to a more conservative model thanβ.Letc̃i=c¯i+ηicˆibe a random variable associated with parameterciand suppose thatηi,i=1,…,n, are arbitrary random variables independently and symmetrically distributed in[-1,1]. Bertsimas and Sim (2004) and Poss (2013) show that any vector x that satisfies the robust constraintcTx⩽tfor allc∈UΓ, satisfies(8)P(c̃Tx>t)⩽B(‖x‖,Γ)=12‖x‖(1-μ(x))‖x‖⌊ν(x)⌋+∑l=⌊ν(x)⌋+1‖x‖‖x‖l,whereν(x)=(Γ+‖x‖)/2andμ(x)=ν(x)-⌊ν(x)⌋. Inequality (8) implies that satisfying probabilistic constraint(9)P(c̃Tx>t)⩽∊with∊∈(0,1)can be done by choosing a proper value ofΓ, which depends on‖x‖. Namely, when‖x‖are∊are given, the smallestΓthat ensures the satisfaction of inequality (9) is given by(10)minΓs.t.B(‖x‖,Γ)⩽∊.Problem (10) may not always have a solution for small values of‖x‖and∊. This is not an issue in practice because one can see (Poss, 2013) that the problem has a solution for∊equal to 0.01 and 0.05 as soon as‖x∗‖is greater than or equal to 8 and 5, respectively. We define functionβas follows(11)β(x)=solutionto(10)iftheproblemisfeasible‖x‖otherwise.The key property ofCOβis presented next.Theorem 1Letβ(x)be as in(11)and∊∈(0,1)be a probability level. It holds that(12)minx∈XVaR∊(c̃Tx)⩽minx∈Xmaxc∈Uβ(x)cTx.Inequality (12) also holds for the classical model of budgeted uncertainty whereΓis fixed and equal toβ(n). Sinceβ(x)⩽Γfor allx∈X, then(13)minx∈Xmaxc∈Uβ(x)cTx⩽minx∈Xmaxc∈UΓ(x)cTxholds and the approximation of problem (2) provided byCOβis tighter than the one provided byCOΓ. In fact, for problems for which the solution has a small cardinality compared to n, modelCOΓprovides a very crude approximation of problem (2) because the value ofΓgiven byβ(n)holds independently of the cardinality of any solution. In the extreme situation,Γmay even lie above the cardinality of the optimal solution, regardless to the value of∊. This is likely to happen, for instance, in network design problems defined onG=(V,E)where the cardinality of the solutions is typically inO(|V|)while the number of variables is inO(|E|)=O(|V|2). This situation is illustrated in our numerical experiments. In contrast to this, modelCOβprovides a tighter bound by reducing the value ofΓas the cardinality of x decreases.Choosing functionγaccording to the probabilistic guarantees computed by Bertsimas and Sim (2004) leads to using non-linear functions such asβandαdescribed in Poss (2013). There exist different ways to avoid this non-linearity. Poss (2013) suggests to replace a non-linear functionγby a piece-wise upper approximation. A better idea, which is used in our numerical experiments, relies on solving first modelCOΓwithΓ=γ(n)and then solving modelCO∂γwhere∂γis the affine function that over-approximatesγas a function of k at the optimal solution ofCOΓ.We study in the next section a theoretical upper bound on the cost reduction provided by modelUγ.Once we have established that modelUγprovides the same probabilistic guarantee as modelUΓ, it is natural to wonder how much cheaper the solution provided by modelUγcan be. We show below that when a technical condition holds, the ratioz(COγ)z(COΓ)is never smaller thatγ(⌈Γ⌉)-1⌈Γ⌉and the bound is asymptotically tight with respect to n. The next result requires thatγsatisfies the following condition:(14)thereexistsapositiveintegerk̲suchthatγ(k)-1kisnon-increasingforallk⩾k̲.One can check numerically that functionβintroduced in Theorem 1 satisfies condition (14). The proof of the next theorem is provided in Appendix B.Theorem 2Let CO be any combinatorial optimization problem andΓ=γ(n). Suppose thatγsatisfies property(14)and that the optimal solution ofCOγhas a cardinality greater than or equal tok̲. It holds that:(15)z(COγ)z(COΓ)⩾γ(⌈Γ⌉)-1⌈Γ⌉.Bound(15)is asymptotically tight with respect to n forγ=(-2ln(∊)‖x‖)12,c¯=0,cˆi=1for eachi=1,…,n, andX⊂{x,‖x‖⩾k̲}.Function(-2ln(∊)‖x‖)12mentioned in Theorem 2 has been introduced in Poss (2013) where the author shows how the function can also be used to decrease the conservatism ofUΓ. However, it is not an easy task to verify analytically the tightness of (15) for functionβgiven that the function does not have an analytical expression. Hence, we illustrate numerically in Fig. 1the bounds provided by (15) forβ. Fig. 1 suggests that bound (15) is very tight already for relatively small values of n. One can hardly expect to witness such cost reductions with practical problems since the theoretical bounds consider extreme cases wherec¯=0andcˆi=1for eachi=1,…,n. Our numerical experiments from Section 5 realized on two combinatorial optimization problems with hundreds of variables exhibit cost reduction ranging from less than 1% up to 40%.In the following two sections, we study the computational complexity of problemCOγ.We study in this section the computational complexity ofCOγ. We show that whenγis affine,COγbelongs to same complexity class as the original problem CO. We show then that for more general functionsγ,COγbelongs to same complexity class as the cardinality-constrained version of CO.We first consider the case whereγis an affine function of x:(16)γ(x)=γ0+∑i=1nγixi.We see next that surprisingly, the computational complexity ofCOγfor an affine functionγis essentially the same as the complexity of CO. The result below is an extension of Bertsimas and Sim (2003, Theorem 3) to the case of variable budgeted uncertainty. The proof is omitted because it is almost identical to the proof of Bertsimas and Sim (2003, Theorem 3). Without loss of generality, we suppose below that the coefficients of the variables are ordered by decreasing order ofcˆiand we use the notationcˆn+1=0.Theorem 3Whenγis an affine function,COγcan be solved by solving then+1nominal problems:(17)γ0cˆl+minx∈X∑i=1n(c¯i+γicˆl)xi+∑i=1l(cˆi-cˆl)xi,forl=1,…,n+1and taking the cheapest optimal solution.The only difference between problem (17) and the problems solved in Bertsimas and Sim (2003, Theorem 3) lies in the costs coefficients of x. In problem (17), these coefficients depend on the componentsγiof the budget functionγwhile they are independent ofΓin Bertsimas and Sim (2003, Theorem 3). We mention that whenΓis integer, Álvarez-Miranda, Ljubic, and Toth (2013) and Lee, Lee, Park, and Park (2012) show how the number of problems solved in Bertsimas and Sim (2003, Theorem 3) can be reduced ton-Γ+2. The reduction is not useful in our context because, even if we assume thatγis integer-valued, the reduction would lead to solvingn-γ(k)+2problems where k is the cardinality of the optimal solution. Since we do not know that cardinality, we should use a conservative approach, leading ton-γ(0)+2problems to be solved whereγ(0)=0.We turn now to non-affine budget functions, such as the functions used in Theorem 1. We show below thatCOγis strongly related to the cardinality-constrained version of CO(18)COk⩽≡minx∈X,‖x‖⩽kcTx.For a large class of combinatorial optimization problems, such as the knapsack problem or the shortest path problem, the original problem and its cardinality constrained version stay in the same complexity class. We show in the next results how solvingCOγamounts to solve robust versions ofCOk⩽.Theorem 4ProblemCOγcan be solved by solving the n nominal problems:minx∈X,‖x‖⩽kmaxc∈Uγ(k)cTx,fork=1,…,nand taking the cheapest optimal solution.We must prove that following equality holds(19)minx∈Xmaxc∈Uγ(x)cTx=mink=1,…,nminx∈X,‖x‖⩽kmaxc∈Uγ(k)cTx.⩽: For everyk=1,…,n, the following holds:(20)minx∈X,‖x‖⩽kmaxc∈Uγ(k)cTx⩾minx∈X,‖x‖⩽kmaxc∈Uγ(x)cTx⩾minx∈Xmaxc∈Uγ(x)cTx,where inequality (20) follows from the fact thatγis non-decreasing.⩾: Letx∗be the optimal solution of the left-hand-side of (19) and denotek∗=‖x∗‖. The following holds(21)minx∈Xmaxc∈Uγ(x)cTx=minx∈Xmaxc∈Uγ(x∗)cTx⩾minx∈X,‖x‖=k∗maxc∈Uγ(k∗)cTx⩾minx∈X,‖x‖⩽k∗maxc∈Uγ(k∗)cTx,whereUγ(k∗)=Uγ(x∗)was used to obtain inequality (21). □Applying sequentially Theorems 4 and 3, we obtain thatCOΓis polynomially solvable whenCOk⩽is polynomially solvable.Corollary 1ProblemCOγcan be solved by solving then(n+1)nominal problems:γ(k)cˆl+minx∈X,‖x‖⩽k∑i=1n(ci+γicˆl)xi+∑i=1l(cˆi-cˆl)xi,fork=1,…,n,l=1,…,n+1and taking the cheapest optimal solution.We show below that a cardinality constrained optimization problem can also be solved as a problemCOγ. Hence, polynomially solvable optimization problems that becomeNP-hard when adding a cardinality constraint, such as the minimum cut problem (Bentz, Costa, Derhy, & Roupin, 2009), lead toNP-hard problemsCOγin general.Theorem 5ProblemCOk⩽can be solved by solvingCOγwith(22)γ(x)=0if‖x‖⩽k1if‖x‖>k,andcˆi=∑ici+1for eachi=1,…,n.Consider an instance ofCOk⩽and create an associated instance ofCOγby settingcˆi=∑ici+1for eachi=1,…,n, and definingγas in (22). Letx∗andc∗be, respectively, the optimal solution and the optimal solution cost to the instance ofCOγ. Ifc∗⩽∑ici, thenx∗is also an optimal solution for the instance ofCOk⩽. Ifc∗>∑ici, this means that the original instance of (18) is infeasible. □Corollary 1 and Theorem 5 show that, given a combinatorial optimization problem defined by optimizing a linear cost function over the feasibility set X, problemsCOγandCOk⩽belong to the same complexity class. This equivalence does not hold ifγis restricted to affine functions since Theorem 3 states that the resulting problemCOγbelongs to the same complexity class as CO.We propose in this section an alternative method for solving problemsCOΓandCOγ. Given a combinatorial optimization problem CO and a dynamic programming (DP) algorithm A to solve CO, our objective is to modify A to solve problemsCOΓandCOγby taking into account theminmaxstructure of these problems inside algorithm A. Hence, unlike the results of Section 3, the results of this section also provide faster algorithms for solving the classical robust optimization problemCOΓ. Notice that not all DP algorithms can be used in our framework and, in fact, DP is a methodology that goes far beyond the scope of the static combinatorial optimization problems studied in this paper (Sniedovich, 2011). We define precisely in Section 4.1 the type of functional equations we consider in this paper, which include many well-known DP algorithms such as the Bellman–Ford algorithm and the DP approach to the knapsack problem. We show then in Sections 4.2 and 4.3 how the aforementioned DP algorithms extend naturally to robust problemsCOΓandCOγ.Consider an optimization problem CO with cost function c and suppose that there exists a DP algorithm A to solve CO. Hence, CO can be reformulated over a finite state spaceS, which contains at least two special states: 0 is the initial state and N is the final state. For some problems, there exists several initial states, in which caseO⊂Sdenotes the set of initial states. The main idea of many DP algorithms is to fix variablesxito 1 iteratively, using a recurrence loop. The optimal cost of state s is denoted byF(s)whileF(N)denotes the optimal solution cost of CO. We denote byq(s)the set of variables that can be fixed to 1 at state s and byp(s,i)∈Sthe previous state from s when choosing variablei∈q(s). Setq(s)may also include a dummy element d which means that the optimal choice at state s is that no variable is fixed to 1. Algorithm A solves problem CO by computingF(N)via the functional equation below:(23)F(s)=mini∈q(s){F(p(s,i))+ci},s∈S⧹OF(s)=0,s∈O,and all states that are not inOare initialized with the value+∞. LetQ=maxs∈S|q(s)|. In the worst-case, functional Eq. (23) requires to visit all spaces ofS, yielding a total time ofO(Q|S|), thus yielding a non-polynomial running-time in general. This approach leads to an intuitive interpretation in term of shortest path in a directed and acyclic graph. LetG=(V,A)be a directed graph withV=SandA={(p(s,i),s):s∈S,i∈q(s)}where the cost of arc(p(s,i),s)is equal toci. Clearly, the value of the shortest path from the states inOto state N is equal to the optimal solution of problem CO. The interpretation of CO as a shortest path problem in the directed and acyclic graph defined by the state space is used in the next two sections to extend functional Eq. (23) to the robust cases.We provide below a classical example of DP algorithm that follows Eq. (23): the Bellman–Ford algorithm. Other applications can be found in the literature, such as the constrained shortest path problem (Righini & Salani, 2008), the traveling salesman problem (Held & Karp, 1962), the capital budgeting problem (Cornuejols & Tutuncu, 2006), and scheduling problems (Brucker & Knust, 2006). Notice, however, that this is not the case for all DP algorithms used in combinatorial optimization, since, for instance, the Floyd–Warshall algorithm does not select individual arcs iteratively.Example 1Bellman–Ford algorithmThis algorithms looks for the shortest path between two nodes (denoted o and t) in a graphG=(V,A)that does not contain negative cycles. Here, the state space isS={(i,k):i∈V,k=0,…,|V|-1}, with initial state spaceO={(o,k):k=0,…,|V|-1}and the final state N is(t,|V|-1). Then, the possible choices at each state areq(i,k)={(j,i)∈A}ifk>0where the predecessors arep((i,k),(j,i))=(j,k-1), andq(i,0)=∅.We show below that the solution cost ofCOΓcan be computed using a functional equation that is similar to Eq. (23). The proof is omitted and can be found in Appendix C.Theorem 6Consider an instance of problem CO that can be solved inO(τ)by using functional Eq.(23). Then, its robust versionCOΓcan be solved inO(Γτ).Let us illustrate Theorem 6 whenΓis chosen according to a probabilistic guarantee of1-∊. Theorem 2 from Bertsimas and Sim (2004) imposes thatΓ=(-2ln(∊)n)1/2. In this case, the solution time of Theorem 6 becomesO(n1/2τ). Using betters bounds forΓ, see Bertsimas and Sim (2004, Theorem 3), the solution time can be further decreased.Theorem 6 can be extended toCOγby keeping track of the size of the paths in G. Here, allowing fractional values ofγincreases the computational complexity of the algorithm, contrasting with the case ofCOΓ. In the next result, we use the notationΓ′=min(n,maxk=0,…,n⌊γ(k)⌋). The proof is omitted and can be found in Appendix D.Theorem 7Consider an instance of problem CO that can be solved inO(τ)by using functional Eq.(23). Ifγ(k)∈Zfor eachk=0,…,n, thenCOγcan be solved inO(nΓ′τ). Otherwise,COγcan be solved inO(n2Γ′τ).In this section, we assess numerically our new model on two combinatorial optimization problems from the literature: the shortest path problem and the survivable network design problem. The main objective of our computational experiments is to evaluate the reduction of the price of robustness obtained from usingCOβinstead ofCOΓwhereβis given by Theorem 1 andΓ=β(n)is chosen accordingly. This cost reduction is formally defined asCOΓ-COβCOΓ. As a byproduct, we compare numerically the efficiency of some of the solution methods proposed in Section 3 and the dualization proposed by Poss (2013) and recalled in Appendix A.We have seen in Section 3 thatCOγis much easier to solve whenγis an affine function. For this reason, our experiments with variable uncertainty focus on an approach in two steps. First, we solve modelCOΓwithΓ=β(n)and denote its optimal solution byx∗. Then, defining∂βas the affine function that approximatesβat‖x∗‖, we solve modelCO∂β. For the shortest path problem, we also evaluate the value of the bound provided byUB(COβ)equal tomaxUβcTx∗.The value ofβwas computed numerically with a precision of three decimals by solving problem (10) by enumeration. We could easily computeβfor k going up to 3000, which was enough for our computational experiments. These values are illustrated in Fig. 2for three choices of∊. The experiments have been run a computer equipped with a processor Intel Core i7-3520M at 2.90GHz and 8GB of RAM. All algorithms were coded in JAVA and CPLEX (2013) 12.4 was used as the MIP solver with standard parameter settings.We illustrate in this section the gain of the new modelCOβcompared to the classical modelCOΓon theo-tshortest path problem defined in graphG=(V∪{o,t},A)that may contain cycles and where all costs are positive. The problem can be modeled as as follows. Let|V|=nand|A|=m, andc:A→R+andcˆ:A→R+be two cost functions. Let alsob∈Rnbe such thatbo=-1,bt=1, andbi=0for eachi∈V,δ+(i)={j∈V:∃(i,j)=a∈A}, andδ-(i)={j∈V:∃(j,i)=a∈A}. The formulation follows:(24)mincTx(SP)s.t.∑a∈δ-(i)xa-∑a∈δ+(i)xa=bii∈V∪{o,t}xa∈{0,1}a∈A.The formulation above can be adapted to cost varying inUΓandUβby replacing objective function (24) withminmaxc∈UΓcTxandminmaxc∈UβcTx, respectively. Our experiments are based on four real road networks used in Zhan and Noon (1998), NE1, AL1, MN1, and IA1, and the main characteristics of the networks are presented in Table 2. We generatecˆas Bertsimas and Sim (2003): for eacha∈A,cˆais uniformly distributed in[0,8ca].While(SP)can handle relatively large deterministic problems, its dualized reformulations are ineffective for solving robust problems because the resulting matrix is not totally unimodular anymore and solving the problems enumerates many branch-and-bound nodes. Instead, using Bertsimas and Sim (2003, Theorem 3) and Theorem 3, problemsCOΓandCO∂βcan be solved by solvingn+1shortest path problems, respectively, which can in turn be solved by Dijkstra’s algorithm. Unreported results show that solving the robust versions of(SP)by dualizing the mathematical program is orders of magnitudes slower than using a sequence of Dijkstra’s algorithm. Using Theorem 4, problemCOβcan also be solved to optimality by solving n cardinality-constrained robust shortest path problems which are solved by using the robust version of the Bellman–Ford algorithm described in Section 4.2.We provide in Table 3the solution costs for each model, expressed as percentages of the deterministic solution cost. For each value of∊in{0.01,0.02,…,0.1}, we generate 5 instances and report the geometric averages. One can see from Table 3 that the cost of problemCOΓis identical for all∊⩽0.8. This is due to the fact thatβ∊(2684)is greater than the cardinality of the optimal solution for all∊⩽0.8. Hence, choosingΓ=β∊(2684), the optimal solution of problemCOΓis identical to the solution of the problem where all costs take simultaneously their maximum values with probability 1.The solution times are very similar for problemsCOΓandCO∂βand each instance could be solved in around 2.5seconds. Recall, however, that problemCO∂βrequires to solve first problemCOΓto obtain the vectorx∗at whichβis linearized. Hence, the total solution time for problemCO∂βaround 5seconds. ProblemCOβis much harder to solve since several minutes are needed to solve the problem to optimality. Moreover, we see from Table 3 that the additional gain of problemCOβover problemCO∂βis very little in comparison to the gain of problemCO∂βover problemCOΓ. For this reason, we restrict our attention to the comparison of problemsCOΓandCO∂βin what follows. We report in Fig. 3the cost reduction when using problemCO∂βinstead of problemCOΓ, expressed in percentage of the optimal solution cost of problemCOΓ.Given an undirected graphG=(V,E)with nonnegative edge costs and node pairs Q, we consider the problem of selecting a minimum cost set of edges so that the induced subgraph contains at least K edge-disjoint paths containing at most L edges between each pair in Q. The problem is motivated by the need of developing survivable networks without loss of service quality, even in case of network failure (link or node failure). The problem has been a very active source of investigation in the last 10years and it is only recently that researchers have introduced formulations valid for any positive integersK,L, see Botton, Fortz, Gouveia, and Poss (2013), Mahjoub, Simonetti, and Uchoa (2013). These formulations use a small set of binary design variables, x, denoting which edges belong to the optimal solution, and a larger set of real flow variables defined on layered graphs. We denote byX′⊆{0,1}|E|the set of binary vectors x that define a subgraph feasible for the problem. Denoting byc:E→R+the cost of using the different edges of E, the deterministic version of the problem can thus be stated asminx∈X′cTx.The robust versions of the problem can be further defined asminx∈X′maxc∈UΓcTxandminx∈X′maxc∈UβcTx, respectively. Notice that other versions of robust network design problems have been studied in the literature, see Koster, Kutschka, and Raack (2013) and the references therein.In the following, we compare problemsCOΓandCO∂βon two instances taken from Botton et al. (2013), Mahjoub et al. (2013): TC-21-5 and TC-41-10. Both instances are defined by complete undirected graphs with 21 and 41 nodes, respectively, where 5 and 10 node pairs must be connected, respectively. We further suppose thatK=2andL=3and consider two values of∊:0.05and 0.10. We define the extreme cost functioncˆ:E→R+as follows. For eachδ∈{0.25,0.5,1,2,4,8}, we create 5 instances wherecˆeis randomly and uniformly generated in[0,δce]for eache∈E.For these two instances, the comparison from Mahjoub et al. (2013) and unreported results show that the fastest solution method for the deterministic problem is the formulation from Botton et al. (2013), without using the Benders’ decomposition algorithm. Based on this, we use the formulation from Botton et al. (2013) to compare the solution times required by the dualization of Theorem 8 and those required by the method that consists in solving several deterministic problems, see Theorem 3. The deterministic version of the problem is solved in 0.34seconds and 13seconds, respectively. For each of the robust instances, we set a time limit of 5000 multiplied by the deterministic solution time. We report in Table 4the geometric averages of these solution times expressed as multiples of the deterministic solution times. Average solution times exceeding the time limit are denoted by T. Solution times for problemCO∂βdo not include the time needed to compute∂βby solving problemCOΓand approximatingβat its optimal solution. Results from Table 4 show that the solution times for solving problemCO∂βby the dualization are strongly impacted by the value ofδ. Still, the dualization approach is faster than the method that solvesn+1deterministic problems for all our instances, the speed-up factor being as large as 171 on average for instance TC-41-10 withδ=0.25and∊=0.05.We report in Fig. 4the cost reduction obtained when using problemCO∂βinstead of problemCOΓ, expressed in percentage of the optimal solution cost of problemCOΓ. Results for instance TC-41-10 withδ=8are not available because the instances could not be solved within the time limit.

@&#CONCLUSIONS@&#
