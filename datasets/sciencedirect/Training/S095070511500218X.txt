@&#MAIN-TITLE@&#
Bilinear low-rank coding framework and extension for robust image recovery and feature representation

@&#HIGHLIGHTS@&#
We mainly explore the low-rank image recovery problem.A bilinear low-rank image coding framework is proposed.For recovery, TLRR preserves both column and row information of given data.Out-of-sample extension of TLRR is presented for handling outside data.We propose two local and global low-rank subspace learning methods for feature learning.

@&#KEYPHRASES@&#
Image recovery,Bilinear low-rank coding,Image representation,Subspace learning,Out-of-sample extension,

@&#ABSTRACT@&#
We mainly study the low-rank image recovery problem by proposing a bilinear low-rank coding framework called Tensor Low-Rank Representation. For enhanced low-rank recovery and error correction, our method constructs a low-rank tensor subspace to reconstruct given images along row and column directions simultaneously by computing two low-rank matrices alternately from a nuclear norm minimization problem, so both column and row information of data can be effectively preserved. Our bilinear approach seamlessly integrates the low-rank coding and dictionary learning into a unified framework. Thus, our formulation can be treated as enhanced Inductive Robust Principal Component Analysis with noise removed by low-rank representation, and can also be considered as the enhanced low-rank representation with a clean informative dictionary via low-rank embedding. To enable our method to include outside images, the out-of-sample extension is also presented by regularizing the model to correlate image features with the low-rank recovery of the images. Comparison with other criteria shows that our model exhibits stronger robustness and enhanced performance. We also use the outputted bilinear low-rank codes for feature learning. Two unsupervised local and global low-rank subspace learning methods are proposed for extracting image features for classification. Simulations verified the validity of our techniques for image recovery, representation and classification.

@&#INTRODUCTION@&#
Vision data (e.g., images) and non-vision data in the real-world emerging applications, such as face recognition [13,21,27,39], robust alignment of images [26], and document retrieval, can usually be characterized by using high-dimensional attributes or features. Also, plenty of real-world multimedia data, including images, videos and documents, can also be characterized by low-rank subspaces, so recent decade has witnessed lots of efforts and increasing attention on the research of recovering low-dimensional or low-rank structures from high-dimensional data with important information in data preserved by feature learning or low-rank coding. Representative works dedicated to these topics include [1–11,15,26,33–38,41–46,50,56]. In this paper, we mainly focus on the study on the bilinear low-rank coding for image recovery, error correction and image representation.One most representative low-rank recovery criterion is named Robust Principal Component Analysis (RPCA) [3,8,9,16]. For a given observed data matrixX=x1,x2,…,xN∈Rn×Ncorrupted by certain sparse errorsE0, RPCA recoversX0(X=X0+E0)by solving the following nuclear norm minimization problem:(1)MinY,EY∗+γEℓ,SubjX=Y+E,where·∗is the nuclear norm of a matrix, i.e., the sum of singular values of the matrix,·ℓisl1-norm (·1)orl2,1-norm (·2,1)to characterize the sparse errors, andγis a positive weighting parameter. The minimizerY∗corresponds to the principal components of X and is also the low-rank recovery toX0. Note that RPCA can well address gross corruptions with large magnitude if only a fraction of entries are corrupted [3,6]. But RPCA is a transductive model, so it cannot handle new data [6]. Besides, RPCA implicitly assumes that the underlying data structures lie in or lie near a single low-rank subspace, but most real data are described by using a union of multiple subspaces [1,2], so the recovery of RPCA may be inaccurate in reality. To enable RPCA for including outside data, Inductive Robust Principal Component Analysis (IRPCA) [6] was recently proposed by seeking a low-rank projectionU=u1,u2,…,un∈Rn×nto deal with outside data. IRPCA solves the projection U and the principal componentsY=y1,y2,…,yNfrom the following convex nuclear norm based problem:(2)MinU,EU∗+γEℓ,SubjX=Y+E,Y=UX.The original data can be recovered asU∗X(orX-E∗)by IRPCA. Based on the learnt U, given data can be mapped onto the underlying subspaces and the possible corruptions can be efficiently removed [6]. Note that IRPCA performs recovery along column direction of X, so row information of data is lost by IRPCA.To well address mixed data with (grossly) corrupted observations, another low-rank criterion called Low-Rank Representation (LRR) [1,2] was also recently proposed for subspace recovery, clustering and segmentation. For subspace segmentation, LRR aims at computing a low-rank representationV=v1,v2,…,vN∈RN×Namong all candidates that represent all data vectors as the linear combination of bases in a given dictionary D. By setting X itself as the dictionary (i.e.,D=X), the convex optimization criterion of LRR is defined as(3)MinV,EV∗+γEℓ,SubjX=DV+E,D=X.After obtaining the optimal solutionV∗,E∗, the original data is recovered asX-E∗(orXV∗). Different from IRPCA, LRR recovers or segments given data along row direction, but column information of given matrix is similarly lost by LRR. Note that LRR is also a transductive criterion as RPCA, so it cannot handle new points efficiently. As a result, both LRR and RPCA are inappropriate for the practical applications requiring fast online computation [6]. For image recovery and subspace segmentation, LRR applies the matrix X itself as dictionary, so it requires that sufficient noiseless data is available in dictionary (i.e., only a part of D is corrupted). But most real data are contaminated by various errors, e.g., corruptions and noise, so directly setting X itself as dictionary may be invalid and may depress the robustness performance for subspace recovery and segmentation [11,23].To overcome the shortcomings of LRR and IRPCA for image recovery, we incorporate the concept of tensor representation [12,28,30] into the low-rank recovery and present a bilinear coding criterion, Tensor Low-Rank Representation (TLRR), for enhancing the robustness of image recovery to noise, corruptions or missing values in data. Compared with the existing studies, the contributions of this paper are summarized as follows. First, to enhance the robustness to noise and corruptions, and to well handle data with missing values, our TLRR aims to reconstruct given images along row and column directions at the same time by embedding data onto a low-rank tensor subspace spanned by seeking a pair of low-rank matrices alternately from a nuclear norm minimization problem. Besides, TLRR exhibits a strong generalization power. The modeling of TLRR seamlessly integrates the low-rank representation and dictionary learning into a unified framework, that is, it can perform simultaneous subspace recovery, error correction and dictionary learning. As a result, when learning a low-rank projection to construct a clean informative dictionary, TLRR is considered as an “enhanced” version of IRPCA based on the noise and corruptions removed data. Similarly, when learning the low-rank representation for image recovery, TLRR is regarded as enhanced LRR learning with trained low-rank informative dictionary. As a consequence, the image recovery performance and the robustness against noise, corruptions and missing values can be greatly improved by our proposed bilinear TLRR model theoretically. Second, we present an out-of-sample extension of TLRR for deal with the outside images, since TLRR is essentially a transductive criterion as LRR and RPCA. To enable such capability, we add a Least Square (LS)-style [49] regularization term into the objective function of TLRR to compute a projection for correlating features with the low-rank recovery of images so that the bilinear low-rank recovery of new test images can be directly obtained by embedding them onto the projection. Third, we propose two similarity preserving and global structure preserving low-rank subspace learning methods by using the outputted bilinear low-rank codes of TLRR as inputs for image feature extraction and classification.The paper is summarized as follows. Section 2 briefly reviews the related work. Section 3 proposes the TLRR algorithm mathematically. Subsequently, in Section 4 we present the out-of-sample extension of bilinear TLRR for including outside images. We in Section 5 discuss bilinear low-rank coding for subspace learning. Section 6 shows the settings and evaluates our methods. Finally, the paper is concluded in Section 7. For easy to follow the work, we first present the important notations and abbreviations of algorithms in Table 1.

@&#CONCLUSIONS@&#
