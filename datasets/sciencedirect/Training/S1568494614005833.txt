@&#MAIN-TITLE@&#
Flow-based tolerance rough sets for pattern classification

@&#HIGHLIGHTS@&#
This paper proposes a flow-based tolerance rough set using the flow, which represents the intensity of preference for one pattern over another, to measure the similarity between two patterns.The proposed flow-based similarity measure represents a complete departure from the traditional distance measure rather than modifies simply the traditional distance measure.The proposed method has been tested on several real-world data sets. Its classification performance is comparable to that of other rough-set-based methods.

@&#KEYPHRASES@&#
Tolerance rough set,Classification,Preference information,Pairwise comparison,MCDA,

@&#ABSTRACT@&#
Rough set theory is a useful mathematical tool for pattern classification to deal with vagueness in available information. The main disadvantage of rough set theory is that it cannot handle continuous attributes. Although various discretization methods have been proposed to deal with this problem, discretization can result in information loss. It has been found that tolerance rough sets with a tolerance relation can operate effectively on continuous attributes. A tolerance relation is related to a similarity measure which is commonly defined by a simple distance function to measure the proximity of any two patterns distributed in feature space. However, for a simple distance measure, it oversimplifies the criteria aggregation resulting from not considering attribute weights, and it is not a unique way of expressing the preference information on each attribute for any two patterns. This paper proposes a flow-based tolerance rough set using flow, which represents the intensity of preference for one pattern over another, to measure similarity between two patterns. To yield high classification performance, a genetic-algorithm-based learning algorithm has been designed to determine parameter specifications and generate the tolerance class of a pattern. The proposed method has been tested on several real-world data sets. Its classification performance is comparable to that of other rough-set-based methods.

@&#INTRODUCTION@&#
Pawlak [39,40] introduced rough set theory to approximate a vague concept (e.g., X) in the universe U in terms of a pair of precise sets which are known as upper and lower approximations. Undoubtedly, rough set theory is a useful technique for analysis of vague concepts in the field of multiple attributes decision analysis (MCDA) [6–8,41,42,52]. However, traditional rough set-based methods are restricted by the requirement that all quantitative attributes must be discrete [38]. Discretization is usually performed before these methods are used. However, the main disadvantage of discretization is information loss. The tolerance rough set (TRS), which was further developed on the basis of rough set theory, was found to handle continuous attributes effectively [24,38,50]. A number of researchers [24–26,29,35,38,50,58] have addressed applications of TRS to pattern classification, such as handwritten numeral characters, remote sensing data and land cover, by treating each class in a classification problem as a concept in a given decision table. TRS indeed plays an important role in pattern recognition. In a traditional TRS, the tolerance classes are determined using a tolerance relation, which is commonly defined by a simple distance measure [47] which indicates the proximity of any two patterns distributed in feature space. Moreover, a similarity threshold must be specified to determine the required level of similarity between any two patterns.The problem addressed by this paper is that, although the use of a simple distance measure for estimating the similarity is simple enough for the traditional TRS, it oversimplify the criteria aggregation resulting from not considering attribute weights, and it is not a unique way of expressing the preference information on each attribute for any two patterns. In other words, the simple distance measure may not be an appropriate choice to measure the similarity for TRS. For MCDA classification methods, the outranking relation theory (ORT), first established by Roy [45], with pairwise judgments on each attribute have gained more attention [14,15]. The outranking relations can provide the preference information among patterns by using pairwise comparisons. This provides a motivation for using preference for one pattern over another to measure the similarity instead of a distance function.Among outranking methods, the well-known Preference Ranking Organization METHods for Enrichment Evaluations (PROMETHEE) methods introduced by Brans, Marechal and Vincke [3,4,10,11,51] is an effective method that can be used to measure the strength of the preference for one pattern over another by estimating the outranking (leaving flow) and the outranked character (entering flow) of each pattern. This paper contributes to propose a novel flow-based TRS (FTRS) which uses preference information expressed by flows among patterns to measure the similarity between any two patterns. For the FTRS, when the net flow of one pattern is sufficiently close to that of another pattern to fall below a given similarity threshold, the former (latter) can be included within the tolerance class for the latter (former). After the tolerance classes for all patterns have been determined, a classification procedure can be used to assign each pattern to a class. To construct a classifier with high classification performance, because genetic algorithms (GA) are a powerful search and optimization method [17,31,44], a genetic-algorithm-based method has been developed here that automatically determines the relative weight of each attribute and a similarity threshold that yields high classification performance.The rest of the paper is organized as follows. Sections 2 and 3 briefly introduce rough sets and TRS with a traditional similarity measure respectively. Section 4 presents the proposed FTRS. Section 5 describes the GA-based learning algorithm for the proposed FTRS-based classifier (FTRSC). Section 6 reports the experimental results of the application of the proposed method to some real-world data sets. Several rough-set-based classification methods presented by Skowron et al. [49] are taken into account. The results show that the proposed FTRSC with subset and concept approximations performs well in comparison with traditional TRSC. Section 7 presents the discussion and conclusions.Rough set theory can deal with vagueness and uncertainty in decision making. Let S=(U, A∪D) be a decision table, where U is a non-empty set of finite elements, A is a non-empty set of finite attributes, and D is a non-empty set of finite decision classes. Each attribute a∈A defines an information function fa: U→Va, where Vais the set of values of a. For any P⊆A, an indiscernibility relation Ind(P) can be defined as follows:(1)Ind(P)={(xi,xj)∈U2|fi(a)=fj(a),∀a∈P}where xiand xjare indiscernible when (xi, xj)∈Ind(P). Some equivalence classes or elementary sets are generated by Ind(P). The elementary set of a pattern x is represented by [x]P. Any finite union of elementary sets is called a P-definable set [19]. For pattern classification, a concept X consists of elements that have the same class label, so that X∈U/D.Sometimes, X⊆U is not P-definable. In other words, there exist elements in the same elementary set which have different class labels, so that X is a vague concept. In this case, X can be approximated by a pair of precise concepts [39,52] using the P-upper approximation,P¯X, and the P-lower approximation,P_X, as follows:(2)P¯X={x|x∈U,[x]P∩X≠ϕ}(3)P_X={x|x∈U,[x]P⊆X}whereP_X⊆P¯XandP_Xconsists of elements that certainly belong to X, whereasP¯Xconsists of elements that possibly belong to X. The tuple〈P_X,P¯X〉composed of the lower and upper approximations is called a rough set.P_XandP¯Xare so-called traditional singleton approximations. WhenP¯X=P_X, X is precise with respect to P (i.e., X is definable); whenP¯X≠P_X, X is rough with respect to P (i.e., X is undefinable). Moreover, the vagueness of X can be described by the accuracy of the rough set representation of X:(4)αP(X)=P_XP¯Xwhere 0≤αP(X)≤1. αP(X) provides an indication of how closely the rough set approximates X. αP(X)=1 means that this concept can be approximated without any uncertainty using the granulation of rough set theory. A vague concept has the boundary region BNDP(X), consisting of elements that cannot be categorized into the concept with certainty, where BNDP(X) is defined as:(5)BNDP(X)=P¯X−P_XThe degree of inclusion of x within X with respect to P can be defined by a rough membership function as:(6)μXP(x)=[x]P∩X[x]PwhereμXP(x)∈[0,1]and |[x]P| denotes the cardinality of [x]P. Undoubtedly, the value of the rough membership function of each pattern inP_Xis 1, that of patterns inP¯Xlies in the interval (0, 1], and that of patterns in BNDP(X) lies in the interval (0, 1). Decision rules induced from the lower approximation of the concept are called certain rules, whereas those induced from the upper approximation of the concept are called possible rules [18]. The set of rules can be used for classification, but this is beyond the scope of this paper.Because rough set theory is unable to deal with real-valued data, a discretization procedure is usually performed before using it. Discretization is the process of converting continuous attributes into discrete attributes. Although many discretization methods have been proposed [5,12,20], the use of such methods can result in information loss. Furthermore, there is no optimal discretization method for all decision problems [20]. Attention has also been focused on TRS because a TRS can handle real-valued attributes by defining a suitable similarity relation for each attribute.In this section, the TRS with a traditional similarity measure and a classification procedure proposed by Kim and Bang [25] for TRS are introduced. xiRaxjdenotes that xiand xjare similar with respect to attribute a, where Rais a tolerance relation with respect to attribute a. A standard similarity measure Sa(xi, xj) with respect to Racan be defined by a simple distance function in [47]:(7)Sa(xi,xj)=1−|a(xi)−a(xj)|maxa−minawhere a(xi) and a(xj) are attribute values of xiand xjrespectively in Va, and maxaand minadenote the maximum and minimum values respectively of the domain interval of attribute a. Of course, the same definition can be used for all attributes [38]. The relation between Raand Sa(xi, xj) is as follows:(8)xiRaxj⇔Sa(xi,xj)≥τawhere τa∈[0,1] is the similarity threshold of attribute a. For A, an overall similarity measure SA(xi, xj) can be defined as:(9)SA(xi,xj)=∑a∈ASa(xi,xj)AThe global tolerance relation RAis related to SA(xi, xj) as:(10)xiRAxj⇔SA(xi,xj)≥τwhere τ∈[0,1] is a global similarity threshold based on all attributes. In contrast to Ind(P), which is an equivalence relation, a tolerance relation has the reflexive and symmetric properties but not the transitivity property.A tolerance class TC(xi) of xican be generated for a certain τ by considering the patterns that have a tolerance relation with xias:(11)TC(xi)={xj∈U|xiRAxj}X can be approximated by the lower approximationAτ_Xand the upper approximationAτ¯X. As in the traditional rough set,Aτ_XandAτ¯Xcan be defined by singletons as follows:(12)Aτ_X={x|x∈U,TC(x)⊆X}(13)Aτ¯X={x|x∈U,TC(x)∩X≠ϕ}The tuple〈Aτ_X,Aτ¯X〉is known as a tolerance rough set.Next, instead of using singletonsAτ_XandAτ¯X, subset and concept approximations introduced by Grzymala-Busse and Siddhaye [19] for the traditional rough set can be further incorporated into TRS. The subset and concept approximations are defined for TRS, since the type of approximations may have impact on classification performance. For subset approximations,Aτ_XandAτ¯Xcan be defined by the union of tolerance classes as follows [25,26]:(14)Aτ_X=∪{TC(x)|x∈U,TC(x)⊆X}(15)Aτ¯X=∪{TC(x)|x∈U,TC(x)∩X≠ϕ}As for concept approximations,Aτ_XandAτ¯Xcan be defined by modifying subset approximations (i.e., subsetsAτ_XandAτ¯X) as follows:(16)Aτ_X=∪{TC(x)|x∈X,TC(x)⊆X}(17)Aτ¯X=∪{TC(x)|x∈X,TC(x)∩X≠ϕ}Some characteristics of these approximations are summarized as follows:1.When x∉X, TC(x) cannot be a subset of X since x∈TC(x). This leads to that the subsetAτ_Xis identical to the conceptAτ_X. Also, it does not guarantee that the subsetAτ¯Xand the conceptAτ¯Xare the same.The singletonAτ_Xis a subset of the subsetAτ_X.Both the singleton and conceptAτ¯Xare subsets of the subsetAτ¯X.It is interesting to compare the classification performances of these three types of approximations.For a TRS-based classifier (TRSC) using a traditional similarity measure with a certain τ, a classification procedure as presented in [25] and [26] for pattern x is as follows:Step 1. Determine〈Aτ_TC(x),Aτ¯TC(x)〉The reason for using TC(x) is that,Aτ_TC(x)consists of patterns that are certainly similar to x, whereasAτ¯TC(x)consists of patterns that are possibly similar to x. Classification information with respect to x can be further derived fromAτ_TC(x)andAτ¯TC(x). As mentioned above, for the subset and concept approximations, it is certain thatAτ¯TC(x)is the same (i.e., TC(x)) butAτ¯TC(x)is not.Step 2. Compute the relative frequency of each decision class forAτ_TC(x)IfAτ_TC(x)consists of at least two patterns, then the relative frequency of each decision class can be determined byAτ_TC(x)−{x}. Then, x can be assigned to the class that has the largest relative frequency. The procedure can be terminated if the largest relative frequency is unique; otherwise, the class label of x can be determined by the boundary region BNDA(TC(x)) of x (i.e.,Aτ¯TC(x)−Aτ_TC(x)).Step 3. Determine the class label by the boundary regionBecause the patterns inAτ¯TC(x)have been considered in the previous step, only the patterns in BNDA(TC(x)) contribute to the classification in this step. Let Xidenote a set consisting of patterns that belong to the decision class Ci. For a pattern y in BNDA(TC(x))≠ϕ, the rough membership functionμCi(y)for TRS with respect to A can be defined as [25]:(18)μCi(y)=TC(y)∩XiTC(y)whereμCi(y)∈[0,1]and |TC(y)| denotes the cardinality of TC(y). Then the average rough membership function of x with respect to Cican be computed as:(19)μ¯Ci(x)=1m∑y∈BNDA(TC(x))μCi(y)where m is the number of patterns in BNDA(TC(x)). x can then be assigned to the class that has the largest average rough membership function. The class cannot be determined when BNDA(TC(x))=ϕ.From the perspective of MCDA, preference information among patterns should be considered. ORT-based techniques provide preference information by performing pairwise comparisons between alternatives. Given xi∈C1 and xj∈C2, the ordering of the classes for C1≻C2 in the outranking relation theory (ORT) implies that xiis at least as good as xj[30]. The preference information involving the strength of the preference for one pattern over another can be estimated by flows as in PROMETHEE methods, which have been extensively used in MCDA [1,14,15,21,57,60].Flows with respect to a pattern are usually computed using preference indices. To compute a preference index, decision-makers are often asked to select one preference function and to specify certain preference parameters for the selected preference function. Among various preference functions, the Gaussian preference function is chosen here, not only because of its simplicity, but also for its usefulness. It is worth noting that Olson [36] has already verified the applicability of the Gaussian preference function and used the major league professional baseball data to examine the differences between actual rankings and rankings using PROMETHEE. The results for PROMETHEE II using a Gaussian preference function were significantly accurate.Let each alternative be evaluated by a vector of n attributes. Fig. 1shows the Gaussian preference function H(sk) (1≤k≤n) for xi=(xi1, xi2, xin) and xj=(xj1, xj2, xjn) for attribute k, which can be defined as follows:(20)H(sk)=1−e−dk2/2σk2where σk>0 is a preference parameter that can be determined by decision-makers and dk=xik−xjk. H(sk) will approach one for a certain σkwhen dkis sufficiently large. However, it is cumbersome for decision-makers to specify a suitable value for σk. Fig. 1 shows that a partial preference index pk(xi, xj)=H(sk) for xik≥xjk, where pk(xi, xj)∈[0,1] is a measure of the intensity of the preference for xiover xjfor attribute k. pk(xi, xj)=0 when xik<xjk. The greater the value of pk(xi, xj), the stronger is the preference for xiover xjfor attribute k. Similarly, pk(xj, xi)=H(sk) for xjk≥xik, whereas pk(xj, xi)=0 when xjk<xik. pk(xi, xj) represents the leaving flow from xito xj, whereas pk(xj, xi) represents the entering flow from xjto xifor attribute k. When pk(xi, xj)>0, pk(xj, xi)=0, and vice versa.Let T be a set consisting of all training patterns, and let |T| denotes the cardinality of T. The net leaving flowϕk+(xi)for attribute k is defined by summing the intensity of the preference for xiover the training patterns in T for attribute k as follows:(21)ϕk+=1T∑xj∈Tpk(xi,xj)In a similar manner, the net entering flowϕk−(xi)for attribute k is defined by summing the intensity of the preference of all patterns in T over xifor attribute k as follows:(22)ϕk−=1T∑xj∈Tpk(xj,xi)The net flow ϕk(xi) for attribute k, which is used to indicate the intensity of the preference for xiover the other patterns for attribute k[2,32], can be further defined as follows:(23)ϕk=ϕk+(xi)−ϕk−(xi)Becauseϕk+(xi)∈[0,1]andϕk−(xi)∈[0,1], ϕk(xi)∈[−1, 1].An overall preference index p(xi, xj) can be further derived using the weighted average of pk(xi, xj) (1≤k≤n):(24)p(xi,xj)=∑k=1nwkpk(xi,xj)where 0≤w1, w2, …, wn≤1 such that w1+w2+…+wn=1. p(xi, xj) represents the flow from xito xj. The greater the value of p(xi, xj), the stronger is the preference for xiover xj. The leaving flow ϕ+(xi) and the entering flow ϕ−(xi) of xican then be formulated as follows:(25)ϕ+(xi)=1T∑xj∈Tp(xi,xj)(26)ϕ−(xi)=1T∑xj∈Tp(xj,xi)where ϕ+(xi)∈[0,1] and ϕ−(xi)∈[0,1]. ϕ+(xi) represents the outranking character of xiover all training patterns, and ϕ−(xi) represents the outranked character of xiby all training patterns. The net flow ϕ(xi) for xican be computed using ϕ+(xi) and ϕ−(xi) as:(27)ϕ(xi)=ϕ+(xi)−ϕ−(xi)where ϕ(xi)∈[−1,1]. It is easy to show that ϕ(xi) is the weighted average of ϕk(xi) (1≤k≤n):(28)ϕ(xi)=∑xj∈Twkϕk(xi)The FTRS can be introduced by defining a flow-based similarity measure.xiRafxjdenotes that xiand xjare similar with respect to attribute a, whereRafis a flow-based tolerance relation with respect to attribute a. A flow-based similarity measureSaf(xi,xj)with respect toRafcan be defined as:(29)Saf(xi,xj)=ϕa(xi)−ϕa(xj)The same definition can be used for all attributes. The relation betweenRafandSaf(xi,xj)is as follows:(30)xiRafxj⇔Saf(xi,xj)≤τafwhereτaf∈[0,2]is a flow-based similarity threshold for attribute a. For A, an overall flow-based similarity measureSAf(xi,xj)could be defined as(31)SAf(xi,xj)=ϕ(xi)−ϕ(xj)The global relationRAfis related toSAf(xi,xj)as:(32)xiRAfxj⇔SAf(xi,xj)≤τAfwhereτAf∈[0,2]is a global flow-based similarity threshold based on all attributes.τAfcan therefore be treated as a cutoff point. A flow-based tolerance class FTC(xi) of xican be generated by considering those patterns that have a flow-based tolerance relation with xias:(33)FTC(xi)={xj∈U|xiRAfxj}.The lower and upper approximations of X, denoted byAτf_XandAτf¯Xrespectively, can be determined by various approximations as described in the previous section by replacingAτ_,Aτ¯, TC(x) and TC(x) withAτf_,Aτf¯and FTC(x) respectively. The tuple〈Aτf_X,Aτf¯X〉is used to denote a FTRS. Similarly to a TRS with a traditional similarity measure, a flow-based tolerance relation has the reflexive and symmetric properties, but not the transitivity property. A novel classification method is proposed by combining the proposed FTRS with the classification procedure introduced in the previous section.To illustrate the operation of FTRS using a flow-based similarity measure instead of the traditional similarity measure, a small decision table with two classes (i.e., C1 and C2) is shown in Table 1, consisting of four real-valued conditional attributes and a single decision attribute is taken into account. Assume that σkis equal to 1 for the Gaussian preference function H(sk) and attributes are of equal weight (i.e., wk=¼, 1≤k≤n). For attribute 1, p1(x1, x3) represents the entering flow from x1 to x3 and can be computed as follows:(34)p1(x1,x3)=1−e−d12/2=0.454where d1=51.1−50.0=1.1. In a similar manner, p2(x1, x3), p3(x1, x3) and p4(x1, x3) can be computed as 0.992, 0.865 and 0, respectively. Therefore, p(x1, x3) can be computed as follows:(35)p(x1,x3)=¼(0.454+0.992+0.865+0)=0.578.As shown in Table 2, because p(x1, x1)=0, p(x1, x2)=0, p(x1, x4)=¼, p(x1, x5)=¼, p(x1, x6)=¼, ϕ+(x1) =(p(x1, x1)+p(x1, x2)+p(x1, x3)+p(x1, x4)+p(x1, x5)+p(x1, x6))/6=0.221. Whereas ϕ−(x1)=(p(x1, x1)+p(x2, x1)+p(x3, x1)+p(x4, x1)+p(x5, x1)+p(x6, x1))/6=0.432. As shown in Table 3, the net flow ϕ(x1) for x1 can be derived as ϕ+(x1)−ϕ−(x1)=−0.211.LetτAfbe equal to 0.488. Thus, the flow-based tolerance classes, FTC(x1)={x1, x2, x3, x4, x6}, FTC(x2)={x1, x2, x3, x4, x6}, FTC(x3)={x1, x2, x3}, FTC(x4)={x1, x2, x4, x5, x6}, FTC(x5)={x4, x5} and FTC(x6)={x1, x2, x4, x6} can be obtained. It can be seen that a pattern (e.g., x1, x2) is possible to belong to more than one tolerance sets. The singleton approximations for each flow-based tolerance class are summarized in Table 4. For instance, because only FTC(x3)⊆FTC(x3), singletonAτf_FTC(x3)is equal to {x3}. Undoubtedly, singletonAτf¯FTC(x3)={x1,x2,x3,x4,x6}because every flow-based tolerance class except FTC(x5) has an intersection with FTC(x3).The class labels of x1, x2 and x4 can be determined directly byAτf_FTC(x1),Aτf_FTC(x2), andAτf_FTC(x4)respectively because the largest relative frequencies between C1 and C2 for x1, x2 and x4 inAτf_FTC(x1),Aτf_FTC(x2), andAτf_FTC(x4)respectively are unique. Evidently, the class labels of x1, x2, and x4 are 1, 1, and 2 respectively. In contrast, the class labels of x3, x5, and x6 can be further determined byAτ¯FTC(x3)−Aτ_FTC(x3),Aτ¯FTC(x5)−Aτ_FTC(x5), andAτ¯FTC(x6)−Aτ_FTC(x6)respectively, since the largest relative frequencies between C1 and C2 for x3, x5, and x6 inAτf_FTC(x3)−{x3},Aτf_FTC(x5)−{x5}, andAτf_FTC(x6)−{x6}, respectively, are not unique.For x3, because the boundary region of x3 is {x1, x2, x4, x6}, x3 can be correctly assigned to C1 by comparing1/4(μC1(x1)+μC1(x2)+μC1(x4)+μC1(x6))=0.525with1/4(μC2(x1)+μC2(x2)+μC2(x4)+μC2(x6))=0.475. In a similar manner, both x5 and x6 can be incorrectly assigned to C1 by the boundary regions {x1, x2, x4, x6} and {x1, x2, x3, x4, x5}, respectively. The procedure of determining class labels for the subset and the concept approximations is the same as that for the singleton approximations.The proposed classification method does not involve any complex mechanisms for tuning its parameter specifications. Its construction involves basic genetic operations including selection, crossover, and mutation. To construct a FTRSC with high classification power, A real-valued GA is used to determine the relative weights of the respective attributes (i.e., w1, w2,…, wn), the preference parameter for each attribute (i.e., σ1, σ2, …, σn) and a global flow-based similarity threshold (i.e.,τAf). In other words, 2n+1 parameters can be determined by the GA. It is assumed that the preference parameter for each attribute lies in the interval of (0, 5). These parameter specifications cannot be easily pre-specified by decision-makers.Step 1. Population initializationGenerate an initial population of nsizechromosomes. nsizechromosomes make up a population, and 2n+1 real-valued parameters constitute a chromosome. Randomly assign a real value ranging from zero to one to each parameter in a chromosome.Step 2. Chromosome evaluationEach chromosome corresponds to a FTRSC. The fitness function f(FTRSC) for evaluating a chromosome can be simply defined as follows:(36)f(FTRSC)=NC(FTRSC)where NC(FTRSC) denotes the number of correctly classified training patterns. For a chromosome, the procedure for obtaining NC(FTRSC) is:(1)Compute the net flow for each pattern using the relative weights of the respective attributes and the preference parameter on each attribute in this chromosome;Compute the overall flow-based similarity measure between any two patterns;Determine a flow-based tolerance class for each pattern;Select one type of approximation (i.e., singleton, subset, or concept) to obtain a flow-based tolerance rough set for each flow-based tolerance class;Perform the classification procedure presented by Kim and Bung. This will yield the number of correctly classified training patterns.Step 3. New chromosome generationGenerate new chromosomes in the next generation by selection, crossover, and mutation.Step 4. Elitist strategyRandomly remove ndel(0≤ndel≤nsize) newly generated chromosomes from the newly generated population. Insert ndelchromosomes with the maximum fitness from the previous generation.Step 5. Termination testTerminate the algorithm when nmaxgenerations have been created; otherwise, return to Step 2.When the stopping condition has been satisfied, the algorithm is terminated, and the best chromosome with maximum fitness value among all successive generations serves as the desired solution to examine the generalization ability of the proposed classification method.Let Pkdenote the population generated in generation k (1≤k≤nmax). Chromosome i(1≤k≤nsize) in Pkis represented bywi1kwi2k…winkσk1iσk2i…σkniτik. By evaluating the fitness value of each chromosome in Pi, the genetic operations, including selection, crossover, and mutation [17,31,44], are iterated until nsizenew chromosomes have been generated in Pk+1.After the fitness values of individual chromosomes have been obtained, members of the next generation are selected by a binary tournament. Two chromosomes are randomly drawn from the current population, and the one with the higher fitness is placed in a mating pool. This process can be repeated nsizetimes until there are nsizechromosomes in the mating pool. Thus, nsizepairs of chromosomes can be selected for mating. Next, apply crossover and mutation to a selected parent to reproduce children by altering the chromosomal makeup of both parents.After selecting two chromosomes,wi1kwi2k…winkσi1kσi2k…σinkτik(i.e., chromosome i) andwj1kwj2k…wjnkσj1kσj2k…σjnkτjk(i.e., chromosome j), where 1≤i, j≤nsize, the crossover probability Prcdetermines whether or not the crossover operation will be performed on any two real-valued parameters in selected parents. A larger crossover probability enables a larger solution space to be explored. Two new chromosomes are generated to replace their parent strings by inserting these two new chromosomes into Pk+1. The operation is performed as follows: For chromosomes i and j, two new chromosomeswi1k′…wink′σi1k′σi2k′…σink′τik′andwj1k′wj2k′…wjnk′σj1k′σj2k′…σjnk′τjk′can be generated, wherewiwk′=awwiwk+(1−aw)wiwk,wiwk′=(1−aw)wiwk+awwiwk(1≤w≤n),σiwk′=bwσiwk+(1−bw)σjwk,σjwk′=(1−bw)σiwk+bwσjwk(0≤w≤n),τik′=cτik+(1−c)τjk,τjk′=(1−c)τik+cτjk. aw, bw, c are all random numbers selected from the interval [0,1].The mutation operation with probability Prmis performed for each real-valued parameter of the newly generated chromosomes generated by the crossover operation. To avoid generating too much random perturbation, use a low mutation rate. If a mutation occurs, the affected gene will be changed by adding a number randomly selected using Prmfrom a specified interval (e.g., [−0.01, 0.01]) to individual parameters. After crossover and mutation, ndel(0≤ndel≤nsize) chromosomes can be randomly removed from newly generated chromosomes to make room for the best chromosomes with the maximum fitness value in the current generation.Through computational experiments on several real-world data sets summarized in Table 5, the generalization ability of the proposed classification method (i.e., FTRSC) was examined for three types of approximations. The data sets used in the computer simulations are available from the UCI machine learning repository at http://www.ics.uci.edu/∼mlearn/MLRepository.html. The computer programs are coded in Delphi 7.0 on a personal computer with a Pentium dual CPU, Microsoft Windows XP, 2 GB RAM, and a clock rate of 3GHz. It is noteworthy that one advantage of the proposed classification method is that it is simple enough to implement as a computer program without any statistical assumptions.This section is organized as follows. Section 6.1 presents the parameter specifications used by the genetic-algorithm-based learning algorithm. Section 6.2 reports the performance of different classification methods on real-world data sets.A number of factors can influence GA performance, including the size of the population and the probability of applying the genetic operators. Unfortunately, there is no optimal set of parameter specifications. Based on the principles suggested by Osyczka [37] and Ishibuchi et al. [23], the factors used in each experiment are determined as follows:(1)nsize=50: The most common population size varies from 50 to 500 individuals. Hence, 50 individuals is an acceptable minimum size.nmax=500: The stopping condition is specified according to the available computing time.ndel=2: To generate less perturbation in the next generation, only a small number of elite chromosomes are used.Prc=1.0, Prm=0.01: Because a larger Prcenables wide exploration of the solution space, a larger Prcis usually specified as large. However, to avoid generating excessive perturbation, a smaller Prmshould be specified.Although these parameter specifications are somewhat subjective, experimental results show that they are acceptable.To examine the performance of the proposed method, five-fold cross-validation (5-CV) is performed ten times independently for each data set. In practice, 5-CV divides all patterns into five equally sized and disjoint subsets. For a classification method, four subsets serve as training patterns, and the single remaining subset serves as test data. This procedure is repeated until each of the five subsets has been tested. In particular, the distribution-balanced stratified CV (DBSCV) [59] is used to estimate the generalization accuracy of a classifier. For stratified CV (SCV), each class is uniformly distributed among the five folds such that the class distribution in each fold is similar to that in the original data set. It was shown that that SCV performs better than typical CV [27]. In addition to SCV, DBSCV further considers the distribution in feature space for each class. In other words, if there are multiple clusters within a class, DBSCV uniformly partitions each cluster into five folds. The main reason for using DBSCV is that Zeng and Martinez [59] showed that DBSCV outperforms SCV for estimating the generalization accuracy of a classifier.Past classification studies involving recognition of handwritten numeral characters [25], iris data [26], remote sensing data [29], and land cover [58], have focused more on TRSC with subset approximations (TRSC-SU). Nevertheless, comparisons for TRSC with different approximations using different data sets are needed. Using 5-CV, the classification performances of various classification methods on the data sets are further examined, including TRSC with singleton approximations (TRSC-SI), TRSC-SU, TRSC with concept approximations (TRSC-CO), FTRSC with singleton approximations (FTRSC-SI), FTRSC with subset approximations (FTRSC-SU), and FTRSC with concept approximations (FTRSC-CO).In addition, the classification performance of the proposed methods was compared with that of several representative classification methods presented by Skowron et al. [49]. These classification methods, which are mainly rough-set-based methods, are briefly introduced as follows:HLM: The lattice machine (LM) is a machine learning paradigm that can be used to construct a model in the form of hypertuples. HLM is a hierarchical version of the LM [54] on the basis of a weighting measure. HLM uses the CaseExtract algorithm to construct a hierarchy of hypertuples.RSES-O: RSES-O, which is implemented in the Rough Set Exploration System (RSES) [6–8], is a rule-based method with shortening optimization. An optimal threshold for the positive region is used to shorten all decision rules with the minimal number of descriptors [8]. Note that, the computational kernel of the RSES is the RSES library (RSESlib) [56].RSES-H: RSES-H, which was proposed by Skowron et al. [48], is a hierarchical version of RSES-O. In other words, RSES-H constructs a hierarchy of rule-based classifiers. The levels of the hierarchy are defined by different levels of minimal rule shortening [8]. A new pattern can be classified by one hierarchy of the classifier.RIONA: RIONA is a classification algorithm implemented in RSES [9]. It uses the k-nearest neighbor method to induce distance-based rules. For a given pattern, the patterns that are most similar to this given pattern can vote for its decision, but those patterns that do not match any rule are excluded from voting.The details of these methods can be referred to [49]. The classification performances of these methods on ten data sets are summarized in Table 6.The test results from variants of TRSC and FTRSC are summarized in Table 7. It can be seen that FTRSC-CO is superior to TRSC-CO except for the vehicle data. FTRSC-CO outperforms RSES-H except for the Tic-Tac-Toe Endgame (TTT) and the voting data and outperforms RIONA except for the sonar and the wine data. FTRSC-SU outperforms RSES-H except for the statlog heart and TTT data and outperforms RIONA except for the sonar and the wine data.ANOVA is commonly used to test the differences between more than two related sample means [46]. ANOVA assumes that the samples are drawn from normal distributions and that the homogeneity of variance holds. Unfortunately, these two assumptions are most probably violated when analyzing the performance of classification methods [13]. Therefore, the non-parametric Friedman test [16] with the post hoc test is used to perform statistical analysis of the classification methods described above over the ten data sets. The Friedman test ranks the classification methods for each data set separately, with the best-performing method obtaining the rank of 1, the second-best 2, and so on. In case of ties, average ranks can be assigned. Let rj, k1, and k2 denote respectively the average rank of classification method j, the number of classification methods, and the number of data sets used. Let classification methods 1, 2, 3, 4, 5, 6 7, 8, 9 and 10 denote TRSC-SI, TRSC-SU, TRSC-CO, FTRSC-SI, FTRSC-SU, FTRSC-CO, HLM, RSES-H, RSES-O and RIONA respectively. In Tables 6 and 7, it can be seen that, for instance, r1=7.95 for TRSC-SI, r2=6.45 for TRSC-SU and r3=5.95 for TRSC-CO.Under the null hypothesis that the average ranks of the classification methods are equal, the FFstatistic distributed as the F distribution with k1−1 and (k1−1)(k2−1) degrees of freedom can be formulated by the Friedman statisticχF2[22]:(37)FF=(k2−1)χF2k2(k1−1)−χF2whereχF2is defined as(38)χF2=12k2k1(k1+1)∑j=1k1rj2−k1(k1+1)24Because k1=10 and k2=10,χF2=30.40. These parameters lead to FF=4.590. Because FFis greater than the critical value F(9, 81) (i.e., 1.998) at the 5% level, the null hypothesis is rejected.It would be interesting to detect any significant differences among these ten classification methods. A post hoc test, the Nemenyi test [33], is therefore used to detect any significant differences among the classification methods. The classification performance of two classification methods is significantly different if the difference inf their average ranks is not below the critical difference CD at the α level:(39)CD=qαk1(k1+1)6k2CD is equal to 3.95 because q0.10=2.92 (i.e., α=0.10) for k1=10. Therefore, the results can be summarized as follows:(1)FTRSC-CO is significantly superior to TRSC-SU (6.45−2.40=4.05), TRSC-SI (7.95−2.40=5.55), HLM (6.85−2.40=4.45), and RSES-O (6.50−2.40=4.10). Although FTRSC-CO was not shown to be significantly superior to TRSC-CO, the difference between FTRSC-CO and TRSC-CO is slightly smaller than CD (5.95−2.40=3.55). It can therefore be concluded that the experimental results show that FTRSC-CO seems to outperform TRSC-CO. As mentioned above, FTRSC-CO performs better than TRSC-CO on nine out of ten data sets.There is no significant difference between FTRSC-CO, FTRSC-SU (2.65−2.40=0.25) and FTRSC-SI (5.10−2.40=2.70). Moreover, there is no significant difference between TRSC-CO, TRSC-SU (6.45−5.95=0.50) and TRSC-SI (7.95−5.95=2.00). Even so, it can be seen that FTRSC-CO performs better than FTRSC-SU and FTRSC-SI on seven out of ten data sets, and that TRSC-CO performs better than TRSC-SU on six out of ten data sets and better than TRSC-SI on eight out of ten data sets.As for FTRSC-SU, it is significantly superior to TRSC-SI (7.95−2.65=5.30) and HLM (6.85−2.65=4.20). The difference between FTRSC-SU and TRSC-SU is slightly smaller than CD (6.45−2.65=3.80). Also, the difference between FTRSC-SU and RSES-O is just below CD, but close to it (6.50−2.65=3.90). It can therefore be concluded that the experimental results show that FTRSC-SU seems to outperform TRSC-SU and RSES-O.Although FTRSC-CO was not shown to be significantly superior to RSES-H (5.45−2.40=3.05) and RIONA (5.70−2.40=3.30), FTRSC-CO performs better than RSES-H and RIONA on eight out of ten data sets. Similarly, FTRSC-SU performs better than RSES-H and RIONA on eight out of ten data sets.

@&#CONCLUSIONS@&#
Contrary to the traditional tolerance rough set (i.e., TRS), which uses a simple distance measure to evaluate the proximity of any two patterns, this paper proposes a flow-based tolerance rough set (i.e., FTRS), which incorporates flows among patterns into the similarity measure. As in PROMETHEE, a flow represents the intensity of preference for one pattern over another pattern. FTRS not only deals with continuous attributes but also uses a flow-based similarity measure that considers preference information among patterns using pairwise comparisons. The contributions of this paper are to present a novel FTRS and a classifier using FTRS (i.e., FTRSC). A GA-based learning algorithm is used to determine the optimal global flow-based similarity threshold. After the optimal threshold value has been obtained, the preference index is used to determine the net flows and to generate the flow-based tolerance class for each pattern. The lower and upper approximations of each pattern can be further determined by its flow-based tolerance class. Then the classification procedure provided in [25,26] is used to determine the class label for each pattern.The process of a FTRSC consists of the determination of a net flow, a flow-based tolerance class, a FTRS, and a class label for each pattern. For |T| patterns and n attributes, the time complexity of the determination of net flows is O(n|T|2). Because the time complexity of the determination of a flow-based tolerance class, a FTRS, and a class label for each pattern is O(|T|), the time complexity of these operations for all patterns is O(|T|2). This indicates that the time complexity of the proposed method is mainly dependent on the number of pairwise comparisons and the number of attributes. To measure the computation time, the proposed methods are further performed by producing 100 generations for each data set by using all the patterns in the set as training patterns. From Table 8, the results can be summarized as follows:(1)The proposed methods require less computation time on the hepatitis, the iris and the wine data. Because each of these three data sets has relatively smaller sample size, it is reasonable that our methods require less computation time to perform pairwise comparisons.The variants of FTRSC require more computation time on the Australian credit approval, diabetes and TTT data. Because each of these three data sets has relatively larger sample size, it is reasonable that the proposed methods require much more computation time to perform pairwise comparisons. Nevertheless, it seems that the computation time is tolerable for each of these three data sets.The computation time of FTRSC-CO is greater than that of FTRSC-SI and FTRSC-SU for each data set.Although the above results reveal that sample size has a great impact on computation time, it seems that this is not a serious problem.Compared to the well-known HLM, RSES-H, RSES-O, RIONA, and TRSC, the experimental results on real-world data sets are encouraging in terms of FTRSC classification performance. It seems that FTRSC-CO and FTRSC-SU give satisfactory performance compared to the other classification methods for all data sets. Experimental results indicate that FTRSC-CO significantly outperforms TRSC-SU, TRSC-SI, HLM and RSES-O and seems to outperform TRSC-CO. FTRSC-SU is significantly superior to TRSC-SI and HLM, and seems to outperform TRSC-SU and RSES-O. These results show that (1) FTRSC-CO and FTRSC-SU have shown their superiority in classification performance on pattern classification problems in comparison with TRSC. With the subset and concept approximations, the performance of a classification method using a tolerance rough set can be improved by considering preference information from pairwise comparisons instead of using a standard distance measure. This reflects that FTRS is more capable than TRS of finding relevant neighboring samples. (2) The classification performances of variants of FTRSC are comparable to those of HLM, RSES-H, RSES-O, and RIONA. Note that it is not possible to conclude which classification method is best because there is no such thing as the “best” classifier [28].On the basis of the strict preference relation, Hu [61] used the rough-set-based rule classifier (RSRC) [6] with pairwise-comparison-based tables, generated by pairwise comparisons on each attribute, to determine decision rules instead of using the original decision tables. Compared to the proposed method using FTRS and the net flow for each pattern (e.g., ϕ(x)), Hu [61] used the RSRC and decision tables generated by using the net flow for each attribute (e.g., ϕk(x)). Using 5-CV, the results obtained by this classification method on Australian credit approval, Diabetes, Glass, Iris, Statlog Heart, and sonar data are 87.7, 74.6, 69.7, 96.5, 84.0, and 74.5% respectively. It can be seen that FTRSC-SI, FTRSC-SU, and FTRSC-CO are comparable to RSRC with pairwise-comparison-based tables. Although the characteristics of the proposed method and the RSRC using pairwise-comparison-based tables are quite different from each other, both methods have demonstrated the potentiality of improving classification performance by applying the strict preference relation to the rough set.In this paper, FTRS has been used to construct a classifier with supervised learning. Nevertheless, it would be interesting to extend FTRSC to unsupervised clustering problems. Comparisons of classification performance between FTRSC and TRSC on unsupervised learning tasks remain to be performed. Moreover, as mentioned above, discretization is not necessary for the proposed method. However, TRS with appropriate discretization may be a valuable alternative to TRSC and FTRSC. Whether discretization such as soft cuts [34] can improve classification performance remains to be studied in future work. Besides, an interesting topic for TRS is to consider incomplete information in a decision table [62–64]. However, this is beyond the scope of this paper. The extension of the FTRS to process incomplete information is left to follow-up study.