@&#MAIN-TITLE@&#
A CFD-based high-order Discontinuous Galerkin solver for three dimensional electromagnetic scattering problems

@&#HIGHLIGHTS@&#
We develop a new high-order curved mesh generation method for high-order DG methods.Intersecting elements can not be observed even in highly-stretched cases.The quality of the generated mesh can be controlled by choosing proper modulus of elasticity.The method introduced in this paper is very easy to implement and to be extended to 3d case.

@&#KEYPHRASES@&#
Time-domain Maxwell’s equations,Discontinuous Galerkin method,Computational efficiency,Radar cross-section,Quadrauture-free implementation,Parallel computing,

@&#ABSTRACT@&#
In this paper, a CFD (Computational Fluid Dynamics) based DG (Discontinuous Galerkin) method is introduced to solve the three-dimensional Maxwell’s equations for complex geometries on unstructured grids. In order to reduce the computing expense, both the quadrature-free implementation method and the parallel computing based on domain decomposition are employed. On the far-field boundary, the non-reflecting boundary condition is implemented. Numerical integration rather than the quadrature-free implementation is used over the faces on the solid boundary to implement the electromagnetic solid boundary condition for perfectly conducting objectives. Both benchmark examples and complex geometry case are tested with the CFD-based DG solver. Numerical results indicate that highly accurate results can be obtained when using high order even on coarse grid and the present method is very suitable for complex geometries. Furthermore, the costs of CPU time and the speedup of the parallel computation are also evaluated.

@&#INTRODUCTION@&#
In the last few decades, the Finite Difference Time Domain (FDTD) method has been widely used to solve the Maxwell’s equations in the time domain [1–3]. Finite element methods [4] have also been tried for solving the Maxwell’s equations, which is usually limited from using high order schemes due to its continuity requirement over the element interfaces. The Finite Volume (FV) method [5,6] is also an option, whose main disadvantage is the low order property.Recently, Discontinuous Galerkin (DG) methods [7] have been trying to solve the time-domain Maxwell’s equations. In [8], DG was used to solve the dispersive lossy Maxwell’s equations in PML (Perfectly Matched Layer) regions. High-order DG methods were employed to solve the Maxwell’s equations on hexahedral and hybrid grids in [9,10]. In [11], the Petrov–Galerkin and DG methods for both time-domain and frequency-domain electromagnetic calculations were compared. A non-conforming multi-element DG for irregular geometries was introduced in [12]. Two hybridizable DG methods for time-harmonic Maxwell’s equations were developed in [13]. The convergence and super-convergence of staggered DG for Maxwell’s equations on Cartesian mesh were discussed in [14]. The DG solutions of the Maxwell’s equations in meta-materials and anisotropic materials were discussed in [15–17]. A Schwarz-type domain decomposition method and an hp-adaptivity method were introduced in [18,19] for large scale electromagnetic simulations.In this paper, we aim to apply a CFD-based high-order Discontinuous Galerkin (DG) method to solve the 3D Maxwell’s equations and test the efficiency of the numerical simulation when using quadrature-free implementation and parallel computing. The paper is organized as follows. In Section 2, the three-dimensional Maxwell’s equations are displayed. The CFD-based high-order DG discretization is described in Section 3. The methods for improving the efficiency of the numerical simulation are given in Sections 4 and 5. The entire computational procedure is displayed in Section 6. Section 7 demonstrates the numerical results and the conclusions are given in Section 8.The time-dependent Maxwell’s equations read:(1)∂B∂t+∇×E=0,(2)∂D∂t-∇×H=-J,whereBandDare the magnetic flux density and the electric displacement, respectively.Jis the electric current density.EandHare the electric field intensity and the magnetic field intensity, which are related toBandDwith the electric permittivity∊and the magnetic permeabilityμ:(3)B=μH,(4)D=∊E.The three-dimensional Maxwell’s equations in the conservation form [18] can be written as:(5)∂U∂t+∇·F(U)=S(U),whereU=Bx,By,Bz,Dx,Dy,DzT,F(U)=Fx,Fy,Fzand(6)Fx=0-Dz/∊-Dy/∊0Bz/μ-By/μ,Fy=Dz/∊0-Dx/∊-Bz/μ0Bx/μ,Fz=-Dy/∊Dx/∊0By/μ-Bx/μ0.In free space,S=0,0,0,0,0,0T,∊andμare constants.High-order DG schemes have been widely used in CFD for Euler and N–S equations. The Maxwell’s equations in Eq. (5) share the same conservative form with the Euler equations, which makes the application of high-order DG to the Maxwell’s equations quite straightforward.After multiplying a test function V, integrating over the computational domain and performing an integration by parts, the following weak form is obtained:(7)∫ΩV∂U∂tdΩ+∫∂ΩVF(U)·ndδ-∫Ω∇V·F(U)dΩ=0,∀V,where∂Ωis the boundary ofΩ. By subdividing the computational domainΩinto the non-overlapping elementsΩe, the semi-discrete system is written as:(8)∂∂t∫ΩeVhUhdΩe+∫∂ΩeVhF(Uh)·ndδ-∫Ωe∇Vh·F(Uh)dΩe=0,∀Vh,whereUhandVhare the high-order approximations toUand V:(9)Uh(x,y,t)=∑j=1N(p)uj(t)ϕj(x,y),(10)Vh(x,y,t)=∑j=1N(p)vj(t)ϕj(x,y).In the above equations,ϕj(x,y)are the basis functions of degree p andN(p)is the number of the basis functions. Note that in Eq. (9)Uh(x,y,t)has 6 components, which means that there aref6N(p)unknowns for each element.In each element, Eq. (8) must be satisfied for anyVhwhich is a linear combination of the basis functionsϕj(x,y). Then, Eq. (8) can be written as:(11)∂∂t∫ΩeϕiUhdΩe+∫∂ΩeϕiF(Uh)·ndδ-∫Ωe∇ϕi·F(Uh)dΩe=0,1⩽i⩽N(p).The flux functionF(Uh)·nis replaced by a numerical flux functionH(Uh-,Uh+,n):(12)∂∂t∫ΩeϕiUhdΩe+∫∂ΩeϕiH(Uh-,Uh+,n)dδ-∫Ωe∇ϕi·F(Uh)dΩe=0,1⩽i⩽N(p).where theUh-is the internal interface state and theUh+is the neighboring element interface state. The widely used numerical fluxes in CFD include LLF [20], Roe [21], etc. For the convenience of using the quadrature-free implementation, the simple LLF numerical flux is employed here:(13)H(Uh-,Uh+,n)=12F(U-)·n+F(U+)·n+αmax(U--U+),whereαmax=max1μ∊,-1μ∊,1μ∊,-1μ∊,0,0is the maximum local eigenvalue of∂(F(U)·n)∂U6×6andn=(nx,ny,nz)Tis the unit vector pointing fromU-toU+.On the far-field boundary, the non-reflecting boundary condition is used, where the incoming wave is simply set to be zero. On the solid boundary, the following boundary condition is employed:(14)n×Dt=0,n·Bt=0,n·∇n·Dt=0,n·∇n×Bt=0,where t refers to the total value which includes the incoming and the scattering parts. From Eq. (14), the scattering unknownsUbon the solid boundary can be evaluated. Then the integrals over the solid boundary faces in Eq. (8) can be calculated as:(15)∫∂ΩeVhF(Uh)·ndδ=∫∂ΩeVhF(Ub)·ndδ.After the spatial high-order DG discretization, the following system is obtained:(16)Mdudt=R,whereMis the mass matrix anduis the collection of all of the unknowns in Eq. (9). The 4-stage Runge–Kutta time-stepping is used to obtain the scattering unknownsu.The numerical evaluation of the integrals in Eq. (12) over the elements and the element interfaces dominates the CPU cost [20]. In order to accelerate the numericial simulation, the quadrature-free implementation [20] is adopted to convert the numerical integration to matrix–vector multiplication.First, for each elemente,Ue,Fxe,FyeandFzeare written in the following high-order form:(17)Ue=∑j=1N(p)ujeϕj,(18)Fxe=∑j=1N(p)fx,jeϕj,(19)Fye=∑j=1N(p)fy,jeϕj,(20)Fze=∑j=1N(p)fz,jeϕj.The unknownsujeandfje=(fx,je,fy,je,fz,je)Tare stored as a 2D array of size6×N(p)and a 3D array of size3×6×N(p). Note that the entire memory cost is dominated by the storage ofujeandfjeover the computational domain.Then the following matrices can be pre-computed over the reference elementΩreffor the evaluation of∫ΩeϕiUhdΩeand∫Ωe∇ϕi·F(Uh)dΩe:(21)MΩref=∫ΩrefϕiϕjdΩrefN(p)×N(p),(22)MΩrefx=∫Ωref∂ϕi∂xϕjdΩrefN(p)×N(p),(23)MΩrefy=∫Ωref∂ϕi∂yϕjdΩrefN(p)×N(p),(24)MΩrefz=∫Ωref∂ϕi∂zϕjdΩrefN(p)×N(p).Then,(25)∫ΩeϕiUhdΩe=MΩrefue|Je|,(26)∫Ωe∇ϕi·F(Uh)dΩe=MΩrefxfxe|Je|+MΩrefyfye|Je|+MΩrefzfze|Je|,whereJeis the Jacobian of the mapping from the reference element to the element e.When the LLF numerical flux is used,(27)∫∂ΩeϕiH(Uh-,Uh+,n)dδ=∫∂Ωeϕi12F(U-)·n+F(U+)·n+αmax(U--U+)dδ,only the matrices such as(28)M∂Ωref-=∫∂Ωrefϕiϕj-dδN(p)×N(p)and(29)M∂Ωref+=∫∂Ωrefϕiϕj+dδN(p)×N(p)need to be pre-computed. Then the integrals over element interfaces in Eq. (27) can be evaluated as:(30)∫∂Ωeϕi12F(U-)·n+F(U+)·n+αmax(U--U+)dδ=12M∂Ωref-(fx-nx+fy-ny+fz-nz)|J∂Ωe|+12M∂Ωref+(fx+nx+fy+ny+fz+nz)|J∂Ωe|+12αmax(M∂Ωref-u--M∂Ωref+u+)|J∂Ωe|,whereJ∂Ωeis the Jacobian of the mapping from the 2D reference element to the element interface∂Ωe.Since the above matrices need to be computed only once, repeatedly evaluating the integrals at each time step is avoided and the entire cost of the CPU time can be significantly reduced. Note that the integrals on the solid boundary Eq. (15) are computed via numerical integration, where the solid boundary conditions are imposed.One of the significant advantages of the high-order DG scheme is its suitability for parallel computation. In this paper, the parallel computing based on the domain decomposition is employed.The computational domain is subdivided into sub-domains using the widely used partitioning tool “Metis” [22,23]. “Metis” partitions the computational mesh based on “Graph partitioning”, which can ensure the number of the elements assigned to each processor is nearly the same and the number of adjacent elements assigned to different processors is minimized. Then the computing costs among the processors can be well balanced and the data transferring between the sub-domains can be minimized. Fig. 1shows a two-dimensional mesh partitioning result, where the entire domain is partitioned into to 4 sub-domains with almost the same number of elements.It can be observed from Eq. (30) that onlyu+,fx+,fy+,fz+from the neighboring sub-domains need to be transferred to the current sub-domain when∂Ωeis located on the partitioning boundary. For an example shown in Fig. 2, before evaluating the integral over∂Ωein Eq. (30), theu+,fx+,fy+,fz+from the element f locating in sub-domain B need to be transferred to sub-domain A. On the other hand, the evaluation of the integral over∂Ωfrequiresu-,fx-,fy-andfz-from the element e.Based on the details discussed previously, the computational procedure can be summarized as below:1.Generate the unstructured computational grid “Grid-master.txt” for a given geometry. The file “Grid-master.txt” contains the information of the elements, the nodes and the faces of the grid over the entire domain.Partition the “Grid-master.txt” into N sub-domains using “Metis”, which generates a text file “partition-infor.txt” as shown in (31) where thePion the ith row indicates that the ith element is located in the sub-domainPi.The “Grid-slave-i.txt”, which includes the local mesh information of the ith sub-domain itself and the connections between its neighbors, can be generated based on “Grid-master.txt” and “partition-infor.txt”.Set the number of the iterative steps and the order of DG.Start the 4-stage Runge–Kutta time-stepping to obtain the time-dependent solution as discussed in Section 3, where the time-step△T⩽Hmin2p+1μ∊. Once the unknowns U are updated after each time-step, the data transferring between sub-domains will be conducted, as displayed in Fig. 2.Post-process the obtained high-order DG solution.(31)P1P2P3…Pi…In this section, the present CFD-based high-order DG is employed to simulate the scattering fields of three geometries: the perfectly conducting sphere, cube and missile. Both accuracy and efficiency are analyzed. For all of the simulations, the following dimensionless TM incoming wave is used:(32)Hxi=0,Hyi=-cos[2π(x-t)],Hzi=0,Exi=0,Eyi=0,Ezi=cos[2π(x-t)].As a benchmark problem, the wave scattering by a perfectly conducting sphere in free vacuum space, is first considered here. The numerical simulation is performed with the DG method of different orders on unstructured grids of various mesh size. Fig. 3displays the used computational grids which are marked asMeshsphere1,Meshsphere2andMeshsphere3, where the numbers of the element are 3362, 12,276 and 40,810 respectively. The order of DG scheme varies from 1 to 4.TheEzcontours on theZ=0profile att=10.0are demonstrated in Figs. 4 and 5. The results in Fig. 4 are obtained using the 3rd-order DG onMeshsphere1,Meshsphere2andMeshsphere3respectively, where theEzcontours become smoother with the increasing number of elements. Fig. 5 displays the results computed with the DG method of various orders onMeshsphere2, which indicate that the accuracy can be significantly improved by using higher order.The convergence in accuracy can also be observed from the RCS (Radar Cross-Section) results in Fig. 6, where the RCS results obtained by using various orders on a series of grids are compared with the exact series solution.The CPU costs of 5000 time-steps onMeshsphere3with 16 partitions are presented in Fig. 7(a). The CPU time significantly increases with increasing order. The speedup of the parallel computing is displayed in Fig. 7, where the CPU time of 100 time-steps is recorded for evaluating the speedup and the 4th-order DG is employed onMeshsphere2andMeshsphere3. Note that the difference between the actual speedup ratio and the ideal one becomes more and more significant with increasing number of cores because the CPU costs of the data exchanging between partitions become relatively more and more expensive compared to those of the local computations performed on each sub-domains (see Fig. 8).In order to make comparisons between the present DG method and the widely used FV (Finite Volume) method [24] in both efficiency and accuracy, a series of FV results are provided here. The FV simulations are performed on four different unstructured grids in Fig. 9, which have 40,810, 204,171, 889,282, 3,543,847 elements respectively. The comparison between the RCS results obtained by the FV method and the exact series solution is given in Fig. 10. The RCS distributions obtained by the FV method are approaching the exact series solution with increasing number of elements. However, significant difference can still be observed even when the finest mesh is used. By comparing Figs. 6 and 10, it can be clearly observed that the accuracy of the FV simulation on finer mesh is lower than that of the high-order DG simulation on relatively coarse mesh.Fig. 11displays the CPU time costs of the FV simulations, where all of the FV simulations are also performed with 16 partitions. From Figs. 7 and 11, it can be observed that the CPU time costs of the FV simulations on the mesh with 889,282 and 3,543,847 elements are more expensive than the most time-consuming DG case (the 4th-order simulation onMeshsphere3).Another frequently used test case is the perfectly conducting cube. The computational mesh is demonstrated in Fig. 12, which contains 10,386 elements in total. TheEzdistributions att=10.0obtained with different orders are given in Fig. 13. TheEzcontours become smoother with increasing order. Only slight difference between the RCS results obtained by the DG method of different orders can be observed in Fig. 14, which indicates that the used mesh is fine enough for RCS evaluation when high-order is employed.In order to demonstrate the ability of the present method to handle complex geometries, a perfectly conducting missile model is also tested here. Fig. 15(a) presents the size of the geometry. The computational mesh in Fig. 15(b) contains 59,762 elements which is partitioned into 32 sub-domains here. The mesh size varies significantly near the solid boundary due to the complex geometry and the mesh size near the far-field boundary is relatively large compared to that near the solid boundary.Fig. 16displays theEzdistributions att=20using the orders from 1 to 4. The 1st-order DG and the 2nd-order DG can not give smoothEzcontours on the used mesh in Fig. 15, but the 3rd-order DG and the 4th-order DG produce smoother results. The RCS results obtained with orders from 1 to 4 are compared in Fig. 17, where the convergence in accuracy can be observed with increasing order. The CPU costs when using various orders are displayed in Fig. 18(a) and the speedup for the 4th-order case is demonstrated in Fig. 18(b).

@&#CONCLUSIONS@&#
