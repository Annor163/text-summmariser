@&#MAIN-TITLE@&#
Action recognition using global spatio-temporal features derived from sparse representations

@&#HIGHLIGHTS@&#
A spatiotemporal feature detector (for human actions) based on sparse representations.Features are obtained by ranking the most salient regions.Descriptors are used in a bag-of-features based classification framework.The performance is evaluated on three standard human action datasets.We report very competitive performance using our proposed approach.

@&#KEYPHRASES@&#
Global spatio-temporal features,Action classification,Activity recognition,

@&#ABSTRACT@&#
Recognizing actions is one of the important challenges in computer vision with respect to video data, with applications to surveillance, diagnostics of mental disorders, and video retrieval. Compared to other data modalities such as documents and images, processing video data demands orders of magnitude higher computational and storage resources. One way to alleviate this difficulty is to focus the computations to informative (salient) regions of the video. In this paper, we propose a novel global spatio-temporal self-similarity measure to score saliency using the ideas of dictionary learning and sparse coding. In contrast to existing methods that use local spatio-temporal feature detectors along with descriptors (such as HOG, HOG3D, and HOF), dictionary learning helps consider the saliency in a global setting (on the entire video) in a computationally efficient way. We consider only a small percentage of the most salient (least self-similar) regions found using our algorithm, over which spatio-temporal descriptors such as HOG and region covariance descriptors are computed. The ensemble of such block descriptors in a bag-of-features framework provides a holistic description of the motion sequence which can be used in a classification setting. Experiments on several benchmark datasets in video based action classification demonstrate that our approach performs competitively to the state of the art.

@&#INTRODUCTION@&#
The increasing popularity of video hosting websites such as Youtube and Dailymotion has brought out a deluge of video data, demanding smart algorithms for their efficient processing. Among several potential applications that video data could facilitate, an important one that has witnessed significant interest from the computer vision community has been that of action recognition in video sequences. Although conceptually similar to static image data, action classification in videos pose several challenges. While issues such as intra-class variations, scale, affine transformations, noise, and clutter are common to videos and image data, we also need to tackle additional difficulties due to object/camera motion, dynamic backgrounds, and diversity in the speed at which actions are performed. Extra computational and storage resources needed for processing videos is another important impediment for developing explicit data-driven approaches.Although there has been plenty of research in this area, not many approaches take advantage of the complexity or lack there-of (redundancy) in the video data. Laptev [1] introduced the space time interest point detector approach to finding the unique spatio-temporal features in a manner similar to finding Harris corners in images. They model significant local phenomena in the video based on spatio-temporal scale-space extrema. Alternatively, one could think of the “interesting” regions as globally significant, i.e., over the entire video sequence. The main intuition of our approach is to exploit the structural redundancy (for example, dynamic but periodic backgrounds, moving camera, etc.) in the video; avoiding such non-informative regions of the video will leave us with portions with significant foreground motion that is different from the spatio-temporal variations in the rest of the video. Liu et al. [24] describe a feature selection approach to address this issue which involves computing multiple static and motion features followed by rule based pruning to select useful static features and a page rank metric on similarity graphs of motion features. This is followed by mutual information based optimization of action class vocabularies. Consequently, the vocabularies are discriminatively trained. The corresponding histogram features are then classified using a heterogeneous adaboost training of static and motion features. Whereas Liu et al. compute known static and motion features and prune them, we propose a new feature detection approach which optimally selects spatio-temporal features based on minimum description length principles. Additionally, we do not aim to optimize the individual class based vocabularies to keep the framework as generalizable as possible as such an effort can become daunting with a large number of action classes such as in the UCF 50 dataset.In order to determine which spatio-temporal variations are informative, we take inspirations from information theory by modeling the complexity of each spatio-temporal patch (a sub-window of the image sequence). Towards this end, we would like to estimate the Kolmogorov complexity[2], which defines the minimum length of a code that can represent the given data; the more the redundancy in data, fewer bits are enough to represent it. Even though this complexity cannot be computed in practice, we can attempt to approximate it using the idea of minimum description length (MDL); in which the encoding scheme is restricted to only a predefined family of encoding models. We propose to use a dictionary learning and sparse coding model for MDL, which we show affords a simple and efficient algorithm for estimating the self-similarity of video data. In fact, we can observe in practice that many signals are redundant and it is fair to assume that such sparsity exists if a suitable basis dictionary is chosen. Most patches can potentially be represented sparsely with a suitable basis, however despite efforts there might be a few patches which can not be represented sparsely. Such patches might require additional representation “budget” or in other words their description lengths are longer than expected. These patches are likely to be more informative (less-redundant). The exact values of the estimates are not important since our interest is to rank the topM%salient patches, for a relatively small M.This paper is organized as follows. In the next section, related work for the problem of human action classification based on spatio-temporal features is discussed. In Section 3, we provide the theoretical motivation for measuring the complexity (description length) of spatio-temporal patches. We formulate the problem of measuring the complexity as the representation error (ℓ2norm distortion) based on dictionary learning and sparse representation. In Section 3.4, we describe our algorithm for classifying human actions using our global spatio-temporal descriptors in a bag-of-features framework. Finally in Section 4, we present experimental results on several benchmark datasets for video based action recognition.

@&#CONCLUSIONS@&#
