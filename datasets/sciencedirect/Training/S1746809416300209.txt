@&#MAIN-TITLE@&#
Medical image fusion using discrete fractional wavelet transform

@&#HIGHLIGHTS@&#
The paper presents a novel image method based on discrete fractional wavelet (DFRWT) for fusing the multimodal medical images.The proposed method makes the traditional time–frequency domain wavelet image fusion method extended to time-fractional frequency domain, and the method provides alternative results by adjusting the sparsity of subband mode coefficients in different p orders.The proposed method can effectively suppress the pseudo-Gibbs effect, and can get better visual effect and higher values of objective evaluations compared to others.

@&#KEYPHRASES@&#
Medical image fusion,Fractional wavelet transform,Wavelet,Multiresolution analysis,

@&#ABSTRACT@&#
A multimodal medical image fusion method based on discrete fractional wavelet (DFRWT) is presented in this paper. With a change in p order in domain (0,1], source medical images are decomposed by DFRWT in different p order. The sparsity character of the mode coefficients in subband images changes. According to the method, to enhance the correlation between subband coefficients, the non-sparsity character of the mode coefficients in low p order should be utilized. The coefficients of the all subbands are fused using the weighted regional variance rule. Finally, inverse DFRWT is applied to obtain a fused image. Subjective and objective analyses of the results and comparisons with other multiresolution domain techniques show the effectiveness of the proposed scheme in fusing multimodal medical images.

@&#INTRODUCTION@&#
Medical images in different modality display different characteristic information of human viscera and diseased tissue. Anatomical images, such as computerized tomography (CT) and magnetic resonance imaging (MRI), provide high-resolution human anatomical information but do not reflect the function of organ metabolic information. Functional medical images such as positron emission tomography (PET) provide functional information on organ metabolism and blood flow but do not provide the focus positional information because these images are of low spatial resolution [1,2]. Therefore, studying how complementary information from different modalities can be combined to obtain more useful information through image fusion has important significance for clinical use.Image fusion can be performed in three different levels [3,4], i.e., from low to high: pixel level, feature level, and decision level. Pixel-level image fusion is the primary method, and its special features are its large amount of information and high precision. At present, pixel-level image fusion is the main method in medical fusion research [5]. In the past decades, people have proposed many pixel-level approaches, such as intensity–hue–saturation (IHS) [6], principal component analysis (PCA) [7], and multiresolution-analysis-based methods [8]. Among these methods, the most widely used methods are multiresolution analysis (MRA) techniques such as Laplacian pyramid (LP) and discrete wavelet transform (DWT) [9–11] because of the influence of spectral degradation in IHS and PCA. In the LP method, each pyramid level generates only one bandpass image, and the method fails to introduce any spatial orientation selectivity in the decomposition process and, hence, often causes blocking effects. The DWT has good time–frequency localization characteristics and multiresolution characteristic, and it can offer the information in horizontal, vertical, and diagonal directions and low-pass components. However, the result of the DWT fusion method has a pseudo-Gibbs effect because of the down-sampling process at each DWT decomposition stage. To overcome the limitations of the lack directionality of the DWT methods, curvelets are considered an effective model in capturing curvilinear properties [12], such as lines and edges. Contourlet transform is derived from the anisotropic scaling relations of curvelet transform, which, in a certain sense, is a form of application of the curvelet transform [13]. The contourlet transform can provide a multiscale and directional decomposition of images, which is more suitable for catching complex contours, edges, and textures. Because of the down-sampling process in the coefficient decomposition step, the contourlet and, curvelet transforms are shift variant, and the image fusion quality is affected by it. The nonsubsampled contourlet transform (NSCT) [14,15] is a shift-invariant version of the contourlet transform, which uses nonsubsampled pyramids (NSPs) and nonsubsampled directional filter banks (NSDFBs) to decompose source images in a multidirectional way, and provides an effective way to obtain a more accurate analysis of multimodality images. Because transform coefficients of the NSCT in each subband are same as the original image size, its redundancy results in increase in computational complexity and longer fusion time.In recent years, a novel time–frequency analysis theory, namely fractional wavelet transform (FRWT) [16,17], has been proposed, and the fractional wavelet transform has extended the analysis method of wavelet transform from the time–frequency domain to time–fractional-frequency domain. The time–frequency analysis theory can characterize signal features in time and fractional-frequency domain [18]. The FRWT is more flexible for image processing and provides a new approach to image fusion because of fractional frequency, which is a new concept.To solve the aforementioned problems, we present a new multimodal medical image fusion method using discrete fractional wavelet (DFRWT), which is based on a multiresolution principle in low p order [19,20]. The DFRWT performs multilevel fusion over two sets of multimodal medical images using a self-adaptive weighted scheme. The proposed fusion method is compared with other multiresolution methods such as LP, DWT, curvelet and contourlet transforms, and NSCT. The superiority of the proposed fusion method is validated through subjective vision and objective fusion metrics for medical image fusion.The rest of this paper is organized as follows. In the next section, we explain the concept of the FRWT and how the DFRWT is realized. Section 2 provides the analysis of the histogram distribution form and sparsity of subband mode coefficients in different p orders under a 2-D DFRWT. Section 3 describes an image fusion algorithm based on the DFRWT. Section 4 gives the experimental results and evaluates the performance of various methods. Conclusions are summarized at the end of this paper.The FRWT was first introduced by Shijun in 2012 as a generalization of the wavelet transform. The FRWT was derived from fractional convolution [16,17,21]. The pth-order continuous FRWT can be defined as(1)Wp,x(α,a,b)=∫−∞+∞f(t)ψ¯p;a,b(t)dt=∫−∞+∞f(t)⋅eit2−b22cotα1aψ¯t−badt.hereψp;a,b(t)=1ae−it2−b22cotαψt−ba=e−it2−b22cotαψa,btis the kernel of the fractional wavelet. Inα=pπ/2, p denotes the order of the fractional Fourier transform (FRFT), and α indicates the rotation angle of the transformed signal for the FRFT. For p∈(0, 1], kernelψa,btis a continuous affine transformation of mother wavelet ψ(t), where a is called the scaling parameter and b is a translation parameter, which determines the time location of the wavelet. Note that when p=1, the FRWT coincides with the WT. In literature [16,17] each fractional wavelet component is essentially a differently scaled bandpass filter in the fractional Fourier domain. The analysis of the signal equivalent to multiresolution analysis (MRA) process in the time–fractional domain plane and a novel signal processing tool has time–fractional frequency domain localization characteristics.To establish a relationship between the FRWT and the MRA, literature [17] has given the definition of the MRA concept of the FRWT. The series of approximation subspace sequencesVjαis composed of fractional scale function ϕp;j,k(t), namely,(2)Vjα=spanϕp;j,k(t)ϕp;j,k(t)=e−it2/2−1/2(k2j)2cotα×ϕj,k(t),j,k∈zandVjα⊂Vj−1α.The series of subspace sequencesWjαis composed of fractional wavelet function ψp;j,k(t), namely,(3)Wjα=spanψp;j,k(t)ψp;j,k(t)=e−it2/2−1/2k2j2cotα×ψj,k(t),j,k∈z,whereWjαis the orthogonal complement ofVjαinVj−1α.Literature [19] analyzed conditions of the construction orthonormal basis of fractional wavelet in time and fractional domains and gave a new transitive relation on adjacent scales space function ϕp;j,k(t), ψp;j,k(t), ψp;j−1,k(t). If function spacesVjα,Wjα, andVj−1αmeet the conditions of orthogonality discussed in literature [16], then direct sum decompositionL2(R)=⊕j∈zWjαcan be obtained fromVj−1α=Wjα⊕Vjα.Specific derivation of the implementation procedure on the DFRWT coefficient decomposition and reconstruction was proposed in literature [19]. Fig. 1shows the flowchart of the coefficient decomposition of one layer of the DFRWT. Discrete sequencesc′njare the j-th layer coefficients in the decomposition of a continuous function f(t) of baseϕp;j,k(t)inVjα. Discrete sequencesc′kj+1andd′kj+1are the (j+1)-th layer coefficients in the decomposition of a continuous function f(t) of baseϕp;j+1,k(t)inVj+1αandψp;j+1,k(t)inWj+1α, respectively. First,c′njis multiplied with discrete chirp signal sequences, i.e.,cnj=c′nj×ei1/2n×2j2cotα. Then a DWT is performed on the discrete sequences ofcnjto obtain coefficientsckj+1anddkj+1in the wavelet domain. Finally,ckj+1anddkj+1are multiplied with discrete sequencese−i1/2(2k)×2j2cotαto obtain the results of approximate coefficientsc′kj+1and detail coefficientd′kj+1in the fractional wavelet domain in the (j+1)-th layer. The obtained values are complex. The decomposition into approximations and details at coarser resolutions can be continued as far as wanted. The procedure can be iterated as many times as wanted.The block diagram shown in Fig. 2illustrates the reconstruction algorithm, i.e., how to restructure j-th layer coefficientc′njfrom (j+1)-th layer coefficientsc′nj+1n∈zandd′nj+1n∈z. First,c′nj+1n∈zandd′nj+1n∈zare multiplied with the discrete sequencese−i1/2(2n)×2j2cotαto obtain coefficientscnj+1n∈zanddnj+1n∈z. Then inverse discrete wavelet transformation (IDWT) is performed to obtain coefficientckjk∈zin the wavelet domain. Finallyckjk∈zis multiplied with the discrete sequencee−i1/2k×2j2cotαto obtain upper layer coefficientc′njin the fractional wavelet domain. The process can continue until original sequencec′n0is reconstructed.According to literature [21], the DFRWT is the fractional Fourier convolution process in different scales. The main process of its decomposition and reconstruction adds coefficient modulation steps based on Mallat algorithm. Similar with the DWT, the number of its decomposition coefficients is half of that of the upper layer because of down sampling. Therefore, the data processing efficiency is high. One-dimensional DFTWT extension is performed to two dimensions to obtain a 2-D DFTWT. Similar with the 2-D DWT, f(x, y)∈L2(R2) is supposed to derive the FRWT of 2-D image f(x,y), i.e.,(4)Wp1,p2;x,y(a,b1,b2)=∫−∞+∞∫−∞+∞f(x,y)ψ¯p1,p2;a,b2b2(x,y)dxdywhereψp1,p2;a,b1,b2(x,y)is a separable fractional wavelet orthonormal basis of L2(R2), namely,(5)ψp1,p2;a,b1,b2(x,y)=ψp1;a,b1(x)ψp2;a,b2(y).Then, Eq. (4) becomes(6)Wp1,p2;x,ya,b1,b2=e−i1/2b12cotα1+b22cotα2∫−∞+∞∫−∞+∞fx,yei1/2x2cotα1+y2cotα21aψ¯t−b1aψ¯t−b2adxdywhereα1=p1π/2,α2=p2π/2, and α1 and α2 indicate the rotation angles of the transformed signal for the 2-D FRFT. [p1,p2]denote the order of the 2-D FRFT [19,22], a is the scale and b1,b2 are the time-shifted factors [15,18]. When p1=p2=1 andα1=α2=π/2,Wp1,p2;x,ya,b1,b2is the 2-D FRWT, which coincides with the 2-D DWT.By extending the flowchart of the 1-D DFRWT coefficient decomposition and reconstruction to 2-D, we obtain the flowcharts of the 2-D DFRWT in a single layer. Figs. 3 and 4depict the process of the 2-D coefficient decomposition and reconstruction, respectively. In the figures,En1n2jis an expression ofei1/2n12j2cotα1+n22j2cotα2, which denote the modulation process ofc′n1n2jin the horizontal and vertical directions to obtaincn1n2j. Then, a 2-D WT is performed on the discrete matrix ofcn1n2jto obtain coefficientsck1k2j+1,dk1k2j+1,γk1k2j+1, andβk1k2j+1⋅E¯k1k2j+1is an expression ofe−i1/22k12j2cotα1+2k22j2cotα2, which denotes the modulation process ofck1k2j+1,dk1k2j+1,γk1k2j+1, andβk1k2j+1to obtain coefficients ofc′k1k2j+1,d′k1k2j+1,γ′k1k2j+1, andβ′k1k2j+1in the fractional wavelet domain. Among them,c′k1k2j+1is the approximation subband coefficient, andd′k1k2j+1,γ′k1k2j+1, andβ′k1k2j+1are the coefficients in the vertical, horizontal, and diagonal directions, respectively. | | denotes the process of the coefficient modulo. The explanation for Fig. 4 is the same.The grayscale histogram of an image represents the statistical characteristics of the image gray-level scale. We can analyze the frequency of the appearance of the different gray levels contained in the image by viewing the image histogram. By studying the grayscale histogram of subband mode coefficients of a medical image the DFRWT in different p orders, we can clearly find a sparse case of the mode coefficients of the subband images and apply medical image fusion research based on the DFRWT.One layer of the DFRWT is used as the medical image (Fig. 5). The input image is decomposed into four non-overlapping multiresolution subbands by filters, namely, LL (approximation coefficients), LH (vertical details), HL (horizontal details), and HH (diagonal details). To have an idea about the sparse case of mode coefficients of the subband images, we use p=1, 0.9, 0.7, 0.5, 0.3, 0.1, 0.008 and give the four subband mode coefficients images grayscale histogram charts (Fig. 6), the corresponding mean values of subband mode coefficients, respectively. The expression of mean for subband mode coefficients is as following(7)mean=1MN∑i=1M∑j=1Nabs(x(i,j))where x(i,j) is subband decomposition coefficients at position (i,j), M and N are the number of rows and columns of the subband, respectively. The brighter the image pixel, the higher value of the mode coefficient.When p=1, the result of image DFRWT is the same as that of the DWT. The LL subband image represents the coarse-scale DWT coefficients, whereas the LH, HL, and HH subbands represent the fine-scale DWT coefficients. Energy is concentrated in a relatively low-frequency image of the LL subband, and the other three subband coefficients (high frequency) have lower energy concentration. From its grayscale histogram, the histogram of the LL subband image shows a wide distribution between 0 and 255. Because the majority of the coefficients are of very small magnitudes and, thus, low energy; the histogram of the detail subband images shows a narrow distribution in the left side near 0. When p=0.9, the magnitudes of the detailed subband image coefficients and histogram distribution changed slightly. When p=0.7, they changed significantly, the detailed subband image coefficients have non-sparse feature, and the histogram distribution is wider. When p=0.5 or less, close interference fringes appear in all subband images because the modulation frequency gradually increases; The change mean values of all subband mode coefficients along with the p from 1 to lower order are displayed in Table 1, the mean value of LL band mode coefficients relatively high near p=1 and the high frequency subbands mean value are low because of sparsity. With the p value gradually decreased, the mean value of all subband mode coefficients are tend to equalize; the energy of the four subband images and the histogram are closely distributed. All the subband images do not show sparsity, and correlation between pixels in the subband images is increased. In low p order, therefore, low-p-order 2-D DFRWT proved to be a new method for medical image fusion.Although the DWT can provide better spatial and spectral localization of image information, it has no shift invariant property because of the down-sampling operation. Thus, the fusion result of the DWT method cannot efficiently preserve the salient features of the source images and produce pseudo-Gibbs effect and some artifacts in details. A step is added to the DFRWT medical image fusion method, i.e., selecting a p value based on multiresolution analysis framework of the traditional DWT image fusion method. A schematic of the DFRWT fusion algorithm of two registered images, i.e., A and B, is depicted in Fig. 7. The source images are first decomposed into low-frequency subbands and a sequence of high-frequency subbands in different scales and orientations and in different p orders. Then, at each position in the transformed subbands, the coefficients of both the low-and high-frequency bands are performed with a certain fusion rule. Finally, the fused image is obtained by applying inverse DFRWT transform on the fused subbands. From analysis in Section 2, the decomposition coefficients are a variety of different p orders; the fusion effects are different. This characteristic adds additional flexibility for the DFRWT fusion method and enhances the selection space for the fusion results [20].In the wavelet medical image fusion method, because of different decomposition coefficients feature in the low- and high-frequency bands, the LF- and HF-subbands are fused using the different combinations of fusion rules. However, in low p order, the decomposition coefficients of the DFRWT in each subband has the same histogram distribution and shows the same non-sparse characteristics; hence, we use identical fusion rule in the LF- and HF-subbands.Step 1: Take l layer DFRWT of both source images in the same p order and obtain 3l+1 pieces of the subbands coefficients.Step 2: Suppose that the region has the size of M×N (which is generally 3×3, 5×5, and 7×7; here, take 3×3), and take p(x,y) as the center. The local varianceVarlk(x,y)is defined as follows:(8)Varlτ(x,y)=1M×N∑i=1M∑j=1Nabsflτ(x+i,y+j)−μ2whereflτ(x,y)is the subband coefficient at position (i,j) at the l-th scale and τ-th direction. μ is the average of the coefficients in the current local area, M and N are the number of rows and columns of the local area, respectively. Each directionτ(τ=a,v,d,h)local variance of the related pixel is record asVarl⋅Aτ(x,y),Varl⋅Bτ(x,y).Step 3: According to the regional variance, the weight of current fusion coefficient is determined as [15,23,24](9)ωl,Aτ=Varl⋅Aτ(x,y)Varl⋅Aτ(x,y)+Varl⋅Bτ(x,y),ωl,Bτ=Varl⋅Bτ(x,y)Varl⋅Aτ(x,y)+Varl⋅Bτ(x,y).whereωl,Aτandωl,Bτare the weights between the DFRWT coefficients of the source images at the l-th scale and τ-th direction, respectively. Supposing thatSl,Fτare the fusion coefficients, we use the following formula to compose the pixel:(10)Sl,Fτ=fl,Aτ(x,y)×ωl,Aτ+fl,Aτ(x,y)×ωl,AτFrom formula (9) and (10),Sl,Fτas the fusion coefficients are dynamically and adaptively fused with the significant features of the source images. This rule can automatically allocate the right weight for each pixel and prominent target combination information according to the characteristics of image.Step 4: Perform inverse DFRWT on the fused low- and high-frequency subband coefficients, which are reconstructed into a fused image.To evaluate the performance of the proposed image fusion approach, three groups of different modes of human brain medical images are considered. These images are characterized in three different groups: (1) CT-MRI, (2) MR-T1-MR-T2, and (3) MR-PET. The size of all the images used in the test is 256×256. The simulation is conducted in MATLAB R2008 on a personal computer with Intel Core 2/1.8 G/1 G, and the computed results are subjectively compared in terms of visual quality and using some metrics in fusion. We chose p=1, 0.7, 0.5, 0.1 and a random low p value for the test. In these experiments, two-level decompositions are performed for all transform-based approaches. DB3 filters are used for the DFRWT algorithm. The results of the proposed fusion framework are compared with the LP-, curvelet-, contourlet-, and NSCT-based methods. The original images and fusion results are displayed in Figs. 8–10.Fig. 8(a) is a CT image that shows bone structures, and Fig. 8(b) is an MRI image that shows the soft tissue at the same place in brain. The fused image of the CT and the MRI can provide good general information of the bone structure and the soft tissue. In the test, because of redundancy, the LP, curvelet, contourlet, and NSCT methods have a good image of the CT edge, but the gray value is higher than the CT source image [the indicator point 1 in Fig. 8(l)]. Because of the down-sampling process, a ring shadow is generated in Fig. 8(c) (curvelet method) and pseudo-Gibbs effect is produced in Fig. 8(d) (contourlet method). The fusion results of the LP and NSCT methods are better than that of the curvelet and contourlet methods, but the brightness of indicator point 2 in Fig. 8(l) is slightly inadequate.The DFRWT is an orthogonal transformation without redundancy. When p=1, the fusion image is the DWT method result, and when the p value is comparatively large, around 1 in the domain because of sparsity in high-frequency bands, the relevance of coefficients are very poor. The block effect is obviously generated in the edge of the bone and soft tissue [the place of indicator point 3 in Fig. 8(l)]. As the p value gradually reduces, the coefficients of all the subbands have no sparsity; therefore, the relevance of the coefficients are enhanced and the extent of the block effect becomes smaller. In very low p value, the whole quality of the fused image is superior to the aforementioned methods.Fig. 9(a) and (b) are the MRI-T1 and MRI-T2 source images of test 2, respectively. MRI-T1 proves the structure of the anatomical information, and MRI-T2 displays the state of tissue lesions. According to fusion results in Fig. 9, the LP, curvelet, contourlet, and NSCT methods are not enough to express information completely [indicator point 1 in Fig. 9(l)]. In addition, a clear black stripe emerges in the fusion image at the place of the junction of the two source images [indicator point 2 in Fig. 9(l)]. The fused images of the DFRWT in high p value have strong block effect, whereas in low p value block effect is significantly reduced. The information of MRI-T2 is expressed clearly. The source images shows a natural connection [indicator point 2 in Fig. 9(l)]. The whole subjective effect is better than the aforementioned methods.Test 3 positron emission tomography (PET) images provide functional information with low spatial resolution. However, functional images do not have detailed anatomical information. The technology of PET-MRI image fusion allows the information from two different examinations to be correlated and interpreted on one image, which lead to more precise information and accurate diagnoses. From the results of image fusion in Fig. 10, in some key information points, the curvelet, contourlet, and NSCT methods lack a sense of depth [indicator point 1 in Fig. 10(l)], and the effect of LP is better than the above methods, but it cannot provide the smooth brink of PET [indicator point 2 in Fig. 10(l)].From the aforementioned experiments, we can conclude that the DFRWT in low p-order fusion method effect is obviously superior to that of the LP, curvelet, contourlet, and NSCT methods. The main reason is that in low p-order non-sparse characteristic of subband coefficients strengthen the correlation among pixels, thereby reducing the pseudo-effect in the DWT fusion method, preserving the texture and edge features of the original image clearly, and eliminating the block artifacts in fused image. In different low p order, the subjective vision of the fusion image has little difference but in a slightly different objective indicators.At present, no objective assessment standard exists. We use mutual information (MI),QAB/F, standard deviation (STD) as the fusion metrics.(a)MI. For input images A and B and fused image F, the MI between image X and fused image F (X can be A or B) is defined as(11)MI=IAF+IBFwhere IAFand IBFare the MI between the source images and fused image. The MI measures the information transferred from the source images to the fused image. It measures the degree of dependence of the two images. The high value of fusion factor implies that the fused image is more informative.QAB/F. We use the definition of objective image fusion performanceQAB/Fdirectly from [24,25]. TheQAB/Fmetric evaluates the level of the fusion algorithm in transferring input gradient information into the fused image, which indicates how much edge information is preserved in the fused image. This metric represents the edge information associated with the fused image and visually supported by human visual system.QAB/Fis given by(12)QAB/F(m,n)=∑i=lm∑j=lnQA/F(i,j)gA(i,j)+QB/F(i,j)gB(i,j)∑i=lm∑j=lngA(i,j)+gB(i,j)and(13)QA/F=QgA/F(n,m)QαA/F(n,m),QB/F=QgB/F(n,m)QαB/F(n,m)whereQA/FandQB/Freflect edge information preservation values respectively, gAand gBare the weights for edge preservation valuesQA/FandQB/F, respectively.QAB/Franges from 0 to 1. The higher value of metricQAB/Fimplies that the fused image displays better edge information.The STD of an M×N fusion image is given by(14)STD=1M×N∑i=1M∑j=1Nf(i,j)−μ2where f(i, j) is the pixel value of the fused image, and μ is the mean value of the fused image. The STD reflects the discrete image gray scale relative to the mean gray value, if STD is large, then the image gray level distribution of dispersion, image contrast is large, which can see more information. So the higher value of the STD indicates the high quality of the fused image.The objective evaluations of the fused results of the proposed method and other comparable methods for the medical images are listed in Table 2. Table 2 shows the advantage of the proposed fusion scheme in low p order. The comparison of the proposed method with other multiresolution-based fusion methods (LP, curvelet, contourlet, and NSCT) shows that the DFRWT approach has high values of STD and fusion factor with the decrease in the p order. Similarly, the results prove that the proposed method is advantageous, as compared with the other methods. The value ofQAB/Fshows that the proposed algorithm could satisfactorily extracts the image details from the source images and maintain clarity in the final fusion image. The value of the MI indicates that the proposed method in low p order acquires more information from the source images. The statistical assessment findings agree with the subject visual analysis. In low p order, the numerical variation of the objective evaluations is very small, and it shows that the proposed method has better selectivity and flexibility. To be clear, because of the low resolution of the PET image in test 3, a part of the edge details of the source MRI image is concealed; therefore, theQAB/Fvalue is lower than the counterpart fusion methods, but this value does not affect the subjective effects. The computational time for all the methods is reported in Table 2. The LP, curvelet, contourlet, and DFRWT methods are different form multiresolution analysis approaches because these methods use a down-sampling process in coefficient decomposition; thus, the time consumptions are close. NSCT is based on an NSP structure and nonsubsampled directional filter banks; therefore, the NSCT method is the slowest. Considering the objective and subjective evaluations, the proposed method is more effective for clinical use than the other methods.Because the main content of the fusion images in different p orders are roughly the same and the difference are mainly reflected in the detail changes, so the relevant index values of the DFRWT fusion method have no significant change. When p orders close to 1, due to downsampling in course of fractional wavelet transform, the high subband decomposition coefficients have weak association because of sparsity, which make Gibbs phenomenon visually more prominent in fusion image. But in low p order, the non-sparsity made the close relationship between all subband coefficients, which can effectively reduce pseudo Gibbs effect. In low p order, the subjective vision of the fusion images are difficult to distinguish, as there are a lot of uncertainty and randomness to fusion image, objective evaluations ofQAB/Fand STD are slightly different, but on the whole the values are remain at a high level.

@&#CONCLUSIONS@&#
