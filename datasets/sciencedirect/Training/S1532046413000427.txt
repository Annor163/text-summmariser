@&#MAIN-TITLE@&#
EXpectation Propagation LOgistic REgRession (EXPLORER): Distributed privacy-preserving online model learning

@&#HIGHLIGHTS@&#
EXPLORER handles learning from distributed sources without sharing raw data.EXPLORER allows client sites dynamically shift from online to offline modes.EXPLORER offers online learning capability for efficient model update.EXPLORER provides high estimation accuracy and strong privacy protection.

@&#KEYPHRASES@&#
Clinical information systems,Decision support systems,Distributed privacy-preserving modeling,Logistic regression,Expectation propagation,

@&#ABSTRACT@&#
We developed an EXpectation Propagation LOgistic REgRession (EXPLORER) model for distributed privacy-preserving online learning. The proposed framework provides a high level guarantee for protecting sensitive information, since the information exchanged between the server and the client is the encrypted posterior distribution of coefficients. Through experimental results, EXPLORER shows the same performance (e.g., discrimination, calibration, feature selection, etc.) as the traditional frequentist logistic regression model, but provides more flexibility in model updating. That is, EXPLORER can be updated one point at a time rather than having to retrain the entire data set when new observations are recorded. The proposed EXPLORER supports asynchronized communication, which relieves the participants from coordinating with one another, and prevents service breakdown from the absence of participants or interrupted communications.

@&#INTRODUCTION@&#
Frequentist logistic regression [1] has a long and successful history of useful applications in biomedicine, including various decision support applications, e.g., anomaly detection [2] survival analysis [3], and early diagnosis of myocardial infarction [4]. Despite its simplicity and interpretability, the frequentist logistic regression approach has limitations. It requires training data to be combined in a centralized repository and cannot directly handle distributed data (the scenario in many biomedical applications [5]). It has been shown in last decade that data privacy cannot be maintained by simply removing patient identities. For example, Sweeney showed that a simple combination of [date of birth, sex, and 5-digit zip code] was sufficient to uniquely identify over 87% of US citizens [6]. Due to privacy concerns, training data in one institute cannot be exchanged or shared with other institutes directly for the purposes of global model learning. To address such a challenge, many privacy-preserving models have been studied [7–10]. Among the most popular ones, privacy-preserving methods based on secure multiparty computing (SMC) [11–15] (i.e., building accurate predictive models without sharing raw data) do not change the results and seem practical compared to solutions based on data generalization and perturbation [16–19] that change results.For distributed model learning with multiple sites, a common scenario is that each site has a subset of records with the same fields, which is usually referred to as horizontally partitioned data. In this paper, we focus on the horizontally partitioned data for distributed logistic regression learning in Bayesian paradigm. During the past decade, numerous privacy-preserving/secure distributed frequentist regression models for horizontally partitioned data [20–26] have been studied. For example, the DataSHIELD framework [20] provides a secure multi-site regression solution without sacrificing the model learning accuracy. However, in the above multi-site regression frameworks, the information matrix and score vector exchanged among multiple sites may result in information leakage during each learning iteration [27,28]. To mitigate privacy and security risks, Karr and Fienberg et al. studied numerous SMC based distributed regression model [21–25]. Unfortunately, as mentioned by El Emam et al. in [26], aforementioned approaches can still potentially leak sensitive personal information. Therefore, the authors [26] proposed a secure distributed logistic regression protocol to offer stronger privacy/security protection. The computational complexity of the above protocol grows exponentially with the increase of site number.The closest work for the method presented here is the Grid LOgistic REgression (GLORE) model [29] and the Secure Pooled Analysis acRoss K-site (SPARK) protocol [26], which train frequentist logistic regression model in a distributed, privacy-preserving manner. GLORE leverages non-sensitive decomposable intermediary results (i.e., calculated at an individual participating site) to build an accurate global model. However, as GLORE does not use any SMC protocol, there is no provable privacy guarantee. SPARK protocol uses secure building block (e.g., secure matrix operation, etc.) to develop a secure distributed logistic regression protocol. However, SPARK will not scale well for a large distributed network, as its complexity grows exponentially with the network size. Both GLORE and SPARK require synchronized communication among participants (i.e., all parties had to be simultaneously online for multiple iterations of training until convergence). Additionally, the frequentist logistic regression approach is inefficient in learning data that are frequently being updated, because the model needs to be completely retrained when they receive any additional observations.We propose a Bayesian alternative for the distributed frequentist logistic regression model, which we call EXpectation Propagation LOgistic REgRession (EXPLORER). EXPLORER offers distributed privacy-preserving online model learning. The Bayesian logistic regression model was described previously by Ambrose et al. [30], who compared it with the frequentist logistic regression model in terms of performance. Marjerison also discussed Bayesian logistic regression [31] and suggested a Gibbs sampling based optimization, which unfortunately is very time-consuming operation. However, both papers assumed a centralized computation environment, and privacy was not taken into consideration. In comparison, EXPLORER focuses on privacy preservation and it is based on an efficient state-of-the-art inference technique (i.e., expectation propagation [32]). To the best of our knowledge, EXPLORER is the first paper addressing distributed logistic regression in the Bayesian setting. EXPLORER handles some shortcomings of the frequentist logistic regression approach and other frequentist SMC models, as illustrated in Table 1.The major contributions of this paper are as follows: we propose a Bayesian approach for logistic regression that takes the privacy issue into account. Just like GLORE and SPARK, the EXPLORER model learns from distributed sources, and it does not require access to raw patient data. In addition, it provides online learning capability to avoid the need for training on the entire database when a single record is updated. Furthermore, EXPLORER supports asynchronous communication so that participants do not have to coordinate one another. This prevents service breakdowns that result from absence of participants or communication interruptions. Finally, we introduced Secured Intermediate iNformation Exchange (SINE) protocol to enhance the security of the proposed EXPLORER framework, in order to further reduce the risk of information leak during the exchange of unprotected aggregated statistics. The proposed SINE protocol offers provable security and light-weighted computation overhead to ensure the scalability of the EXPLORER framework.

@&#CONCLUSIONS@&#
In summary, EXPLORER offers an additional tool for privacy-preserving distributed statistical learning. We showed empirically on two relatively data sets that the results are very similar to those of ordinary logistic regression. These promising results warrant further validation in larger data sets and further refinement of the methodology. Inability to openly share (i.e., transmit) patient data without onerous processes involving pair-wise agreements between institutions may significantly slow down analyses that could produce important results for healthcare improvement and biomedical research advances. EXPLORER provides a means to mitigate this problem by relying on multiparty computation without need for extensive re-training of models, nor reliance on synchronous communications among sites.Assumed-density filtering (ADF) method is a sequential technique for fast computing an approximate posterior distribution in Bayesian inference. However, the performance of ADF technique depends on the data order. Expectation propagation (EP) algorithm [53], as an extension of ADF, exploits a good approximation to the posterior by incorporating iterative refinement on the solution produced by ADF. Thus, EP is usually much more accurate than ADF. EP works by approximating each likelihood term through minimizing Kullback Leibler (KL) divergence between true posterior and approximate posterior within a tractable distribution (e.g., distributions in exponential family). Then by iteratively performing this approximation process, the approximate distribution will finally reach a fixed point [32].In our Bayesian logistic regression problem, parameterβis associated with a Gaussian prior distribution asp(β)=N(β,0,V).Given a training dataset D={(x1,y1),(x2,y2),…,(xN,yN)}, the likelihood function for parameterβis written asp(yi|β)=σ(yiβTxi))=11+exp(-yiβTxi).Then let us denote the true posterior distribution ofβbyp(β|y)∝p(β)∏ip(yi|β)=p(β)∏ifi(β)and approximate posterior byq(β|y)∝p(β)f̃i(β). It is mathematically convenient to choose a Guassian distribution for approximation termf̃i(β)such that the resulted approximate posterior will also be a Gaussian. To perform an efficient EP process,f̃i(β)can be parameterized asf̃i(β)=Ziexp-12vi(yiβTxi-mi)2.The procedure to obtain the posterior approximation through EP algorithm is shown as follows [54]:1.Initialize the prior distribution:f̃0(β)=N(β,0,v0I),setm0=0,V0=v0IZ0=12π, where v0 is a hyper prior.Initialize the term approximationsf̃i(β)to 1:f̃i(β)=Ziexp-12vi(yiβTxi-mi)2set mi=0, vi=∞ and Zi=1,Initialize the posterior probability distribution q(β):set mnew=m0, Vnew=V0 and Z=Z0.Until all (mi, vi, Zi) converge:for i=1,…,N:(a)Removef̃i(w)from the posterior q(β)V⧹i=(Vnew)-1-vi-1xixiT-1,=Vnew+(Vnewxi)(vi-xiTVnewxi)-1(Vnewxi)T,m⧹i=mnew+(V⧹ixi)vi-1(xiTmnew-mi).Update mnew and Vnew according to ADFmnew=m⧹i+V⧹ixiα1,Vnew=V⧹i-(V⧹ixi)α2(V⧹ixi)T,whereα1=yiσ(-z)1+yi2(π/8)xiTV⧹ixi;α2=α1α1+yi2(π/8)xiTmnew1+yi2(π/8)xiTV⧹ixiUpdate the approximated terms fi(w)vi=α2-1-xiTV⧹ixi,mi=xiTm⧹i+α1α2,Zi=Z1+vi-1xiTV⧹ixiexpα122α2.In this section, we will introduce the details of the factor graph construction and message passing in the EXPLORER. Let us write down the factorization of the posterior distribution of each site as follows:(B.1)p(βj|Y1Y2Y3)=p(βj)p(Y1Y2Y3|βj)p(Y1Y2Y3),(B.2)=p(βj)p(Y1|βj)p(Y1Y2Y3)p(Y2Y3|βj),(B.3)=p(βj)p(Y1|βj)p(Y1Y2Y3)p(βj|Y2Y3)p(Y2Y3)p(βj),(B.4)=p(βj)p(Y1|βj)p(Y1Y2Y3)p(Y2Y3)∫p(βjβ|Y2Y3)dβp(βj),(B.5)=p(Y1|βj)p(Y2Y3)p(βj)p(Y1Y2Y3)p(βj)∫p(βj|β)p(β|Y2Y3)dβ,(B.6)=p(Y2Y3)p(Y1Y2Y3)p(Y1|βj)∫δ(βj,β)p(β|Y2Y3)dβ,(B.7)=1Z∏i=mj-1mjp(yi|βj)∫δ(βj,β)p(β|Y2Y3)dβ,whereZ=p(Y2Y3)p(Y1Y2Y3)is a normalization constant. In the above equation, p(βj∣Y1Y2Y3) is equivalent to belief b(βj) in the BP algorithm, which is captured by the variable node Bj. Therefore, in the context of factor graph and BP algorithm, we can interpret the product term∏i=1mp(yi|βj)as the message collected from all the factor nodesfijand the integral term∫δ(βj,β)p(β|Y2Y3)as the message sent from factor node hj. Moreover, according to the factor node update rule in BP algorithm, we can identify the delta function δ(βj,β) as the factor function and p(β∣Y2Y3) as the message sent from server node B. In practice, there are many ways to select factor functions to reflect the contribution of p(βj∣β). In this paper, we follow the suggestion in Loeliger [34] and choose δ(βj,β) to represent the probability p(βj∣β) for mathematical convenience, because of∫δ(βj,β)p(β|Y2Y3)dβ=p(βi|Y2Y3).See Tables C.13–C.16.For all 2-site EXPLORER setups of datasets 3–11, the difference between means (DBM) of class distribution and covariates of the 2 sites has been shown in Figs. D.6–D.14in this section, which offers an intuitive sense about how heterogeneous these sites are.Supplementary material associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2013.03.008.