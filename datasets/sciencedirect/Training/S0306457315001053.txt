@&#MAIN-TITLE@&#
DeASCIIfication approach to handle diacritics in Turkish information retrieval

@&#HIGHLIGHTS@&#
Risk-sensitive evaluation of approaches for handling diacritics in Turkish information retrieval.Application of diacritics restoration to Turkish information retrieval.Investigation of the diacritics sensitivity of stemming algorithms.

@&#KEYPHRASES@&#
Accents,DeASCIIfier,Diacritics restoration,Risk-sensitive evaluation,Stemming,Turkish information retrieval,

@&#ABSTRACT@&#
The absence of diacritics in text documents or search queries is a serious problem for Turkish information retrieval because it creates homographic ambiguity. Thus, the inappropriate handling of diacritics reduces the retrieval performance in search engines. A straightforward solution to this problem is to normalize tokens by replacing diacritic characters with their American Standard Code for Information Interchange (ASCII) counterparts. However, this so-called ASCIIfication produces either synthetic words that are not legitimate Turkish words or legitimate words with meanings that are completely different from those of the original words. These non-valid synthetic words cannot be processed by morphological analysis components (such as stemmers or lemmatizers), which expect the input to be valid Turkish words. By contrast, synthetic words are not a problem when no stemmer or a simple first-n-characters-stemmer is used in the text analysis pipeline. This difference emphasizes the notion of the diacritic sensitivity of stemmers. In this study, we propose and evaluate an alternative solution based on the application of deASCIIfication, which restores accented letters in query terms or text documents. Our risk-sensitive evaluation results showed that the diacritics restoration approach yielded more effective and robust results compared with normalizing tokens to remove diacritics.

@&#INTRODUCTION@&#
American Standard Code for Information Interchange (ASCII)11http://tools.ietf.org/search/rfc4949is a scheme that encodes 128 specified characters into 7-bit binary integers. ASCII contains English alphabet letters (a–z and A–Z), numbers from 0 to 9, and some other special characters. However, a number of languages use characters outside the ASCII range and they have letters with diacritics in their alphabet, such as Czech, Danish, Finnish, French, Greek, Hungarian, Icelandic, Latvian, Lithuanian, Norwegian, Polish, Romanian, Swedish, Spanish, and Turkish. The absence of diacritics in digitally stored text is a severe obstacle to natural language processing (NLP) and information retrieval (IR) for languages with alphabets not covered by the standard ASCII range.Turkish is an agglutinative and morphologically complex language, where a relatively small set of distinct stems is expanded by rich combinations of derivational and inflectional suffixes to create new meanings.The Turkish alphabet is a Latin alphabet that comprises 29 letters. It has all the letters of the English alphabet, except “q,” “w,” and “x.” In addition, it has the local characters “ç,” “ğ,” “ı,” “ö,” “ş,” and “ü” with diacritic symbols, which have been modified from their Latin originals to meet the phonetic requirements (to distinguish different sounds) of the language.Diacritics are also referred to as accent marks, which are defined as: “A mark placed above, below, or to the side of a character to alter its phonetic value.”22http://www.unicode.org/glossary/#accent_markTable 1shows Turkish accented letters and their ASCII equivalents. Due to these non-ASCII letters, Turkish users experience many IR problems on the Internet (Aytaç, 2005).With regard to accents and diacritics, Manning et al. stated that: “Nevertheless, the important question is usually not prescriptive or linguistic but is a question of how users are likely to write queries for these words. In many cases, users will enter queries for words without diacritics, whether for reasons of speed, laziness, limited software, or habits born of the days when it was hard to use non-ASCII text on many computer systems” (Manning et al., 2008).However, Turkish texts written in the English alphabet may have different meanings that cannot be distinguished even by a human. For example, an interesting news story by Diaz (2008) described how a Turkish SMS written in English letters resulted in a completely twisted meaning that resulted in homicides.Turkish users have a tendency to write Turkish search queries without using accented Turkish letters due to the reasons explained above. Therefore, there is a need for an ability to search with and without accents in Turkish IR. For instance, in diacritic insensitive IR, the words resume and résumé should be treated as if they are the same word.ASCIIfication, also referred to as latinization, is the normalization of tokens to reduce all accented letters to their base character. ASCIIfication is a common practice for achieving accent-insensitive IR; however, this may result in a change in meaning because certain words are distinguished only by their accents. Table 2shows a list of examples that would retrieve false matches.Furthermore, the ASCIIfication process yields synthetic words, which can negatively affect downstream morphological analysis components (such as stemmers or lemmatizers) in the processing pipeline. The transformation of the word hastalığının into hastaliginin is an example. In this case, hastaliginin is not a legitimate Turkish word, so it is not recognized by morphological analyzers. In this study, we propose and evaluate an alternative solution based on the application of deASCIIfication for restoring diacritics in query terms or text documents.The remainder of this paper is organized as follows. Section 2 describes related research. In Section 3, we explain the two deASCIIfication algorithms used in this study. In Section 4, we provide intrinsic evaluation results obtained using the two different deASCIIfication systems. Section 5 describes the experimental setup. In Section 6, we present the experimental results and a discussion. In Section 7, we give robustness results obtained from a comparative risk-sensitive evaluation of different approaches for handling diacritics in Turkish IR. In Section 8, we give our conclusions and suggestions for future research.

@&#CONCLUSIONS@&#
In this study, we proposed the use of a DR (deASCIIfication) technique in IR and we compared its effectiveness and robustness with a latinization (ASCIIfication) technique, which is the traditional method for addressing the problems caused by search queries and documents written without diacritics. Based on our results, we reached the following conclusions.•ASCIIfication is preferable (especially if documents have diacritic mistakes) when no stemmer or an accent-insensitive stemmer (simple truncate stemming) is employed during text analysis.The ASCIIfication approach should be avoided if we want to take advantage of accent-sensitive stemmers.DeASCIIfication is preferable if we employ an NLP-based stemmer that expects to operate on valid Turkish words.DeASCIIfication is more flexible in category A settings (documents are written well in terms of diacritics) because it does not require a change in the index. This method is applied during the retrieval time only, so it can be enabled or disabled on the fly.In our practical experiments, we demonstrated how open source solutions can be used to set up a diacritic-insensitive Turkish retrieval system. To facilitate repeatability, reproducibility, and open-source sharing, the custom token filter implementations, Solr field type definitions (used in this study), and the script file used to obtain the evaluation metrics have been made publicly available to other researchers (and anyone else who is interested) under Apache License, Version 2.0. They are available on GitHub in the public repository: http://github.com/iorixxx/lucene-solr-analysis-turkishThe evaluation methodology used in the present study measures the success and robustness of search systems when handling diacritic-less query terms and content. The experimental methodology employed for evaluating the diacritics sensitivity of retrieval systems could also be applied to other languages with accented letters in their alphabets (e.g., Czech, Danish, Finnish, French, Greek, Hungarian, Icelandic, Latvian, Lithuanian, Norwegian, Polish, Romanian, Swedish, and Spanish).In future research, we will combine the outputs of the Zemberek DeASCIIfier and Turkish DeASCIIfier to overcome the shortcomings of each. It should be noted that the Turkish DeASCIIfier produces a single output for a given word, whereas the Zemberek DeASCIIfier can produce multiple outputs. In addition, we will try to develop a system that selects the best candidate from the united output of combined deASCIIfiers to determine whether this approach significantly enhances the effectiveness of IR, or if it simply adds redundant computations and run-time complexity overheads.In our future research, we also plan to investigate the accent sensitivity of successor variety stemming (Stein & Potthast, 2007), which is a special form of truncate stemming.Supplementary material associated with this article can be found, in the online version, at 10.1016/j.ipm.2015.08.004