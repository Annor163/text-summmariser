@&#MAIN-TITLE@&#
Incremental learning to segment micrographs

@&#HIGHLIGHTS@&#
An incremental approach to annotation of images is demonstrated for segmentation.Density functions from training mimic those output by traditional annotation methods.Less burdensome for microscopists and allows correction and control on the result.The rationale of manual annotation is more easily accepted with this method.

@&#KEYPHRASES@&#
Image analysis,Segmentation,Incremental learning,Image labelling,Microscopy,Microstructure,

@&#ABSTRACT@&#
Supervised learning approaches to image segmentation receive considerable interest due to their power and flexibility. However, the training phase is not painless, often being long and tedious. Accurate image labelling can take several hours of expert operators’ valuable time. User interfaces are often specifically designed to assist the user for the task at hand. This is clearly unfeasible for most application domains.We propose a simple segmentation framework based on classification and supervised incremental learning. A statistical model of pixel classes is learnt by incrementally adding new sample image patches to automatically-learned probability functions. Learning is iterated and refined in a number of steps rather than being executed in a one-shot training phase. We show that one-shot training and incremental labelling tend to produce similar statistical models, as the number of iterations grows. Comparable classification results are thus obtained with considerably less human effort.

@&#INTRODUCTION@&#
Segmentation of biological imagery is mostly concerned with the macroscopic world, often in the context of medical applications [20,6,7,2]. The lower extent of published work about segmentation of microscopic imagery can be justified by a number of problems in microscopy, such as: the large diversity of imaging approaches, the lack of a priori knowledge on the image content, the lack of common evaluation procedures, and the large variety of imaged objects for different disciplines [15]. Most of the existing literature on automatic segmentation of microscopic imagery is dedicated to biomedical data (e.g., analysis of cells; see [14,17] for a review) or to the recognition of particles, such as bacteria in biological images. Segmentation of micrographs of compact structures has attracted less interest so far. This is demonstrated by the structure and content of existing benchmark databases [3].The large variety of materials and microstructure images makes it hard to devise a general segmentation approach. General-purpose segmentation methods fail on most classes of microscopy images due to several reasons, the most important being the image formation mechanism of microscopes. In the case of Scanning Electron Microscopes (SEMs), for example, the thin lens model for optical sensors turns out to be a reasonable approximation of electron imaging physics [9,5]. However, SEM imagery violates most of the assumptions at the basis of standard segmentation methods. In fact, pixel intensity is influenced, in the sampled point, by the orientation of the specimen surface with respect to the electron detector, as well as by the extent of the interaction volume, directly connected to the accelerating voltage [9]. Composition of the specimen at that location also plays an important role. As a result, pixel intensities alone do not contain rich and reliable enough information for segmentation. Usual image cues do not suffice for high quality segmentation. As a result, in most disciplines the segmentation of micrographs is mostly carried out with time-consuming user interaction [1,18], by means of techniques as simple as thresholding and basic morphological operators [11,10].On the other hand, studying specific approaches for every application would be infeasible. More general approaches to manage images from different sources rely on unsupervised data mining. Successful examples are reported for cell segmentation [14,17,4]. More complex structures like those shown in this paper, however, present challenges that rule out most unsupervised approaches. The cheese micrograph in Fig. 3(a) exhibits several holes in the compact matrix, mainly due to whey pockets or to fat droplets, as shown in the manual annotation in Fig. 3(b). Without a proper training, most people would fail to tell the two apart and most microscopists do not even agree on some controversial cases. In our experience, unsupervised techniques generally output three main classes: compact matrix, highlight and crevices, and pores (i.e., large or regular holes). This is probably due to the well-distinguishable response of the most popular texture filters of the three classes. Unfortunately, fat holes and whey pockets often exhibit similar feature responses. Supervised learning approaches try to overcome this limitation by incorporating domain knowledge, with the help of annotated images (e.g., [12]). The statistical distribution of image features can be exploited to segment new images generated by a source with similar statistical properties. This pattern recognition approach has been fruitfully applied in several different contexts. An important property of statistical pixel classification is its ability to measure uncertainty (i.e., classification probability). Uncertainty scores are intuitive and easily related to background knowledge of microscopy experts. They can be used to reject classified pixels whose associated probability falls below a user-specified threshold (useful, e.g., in quantitative image analysis).The main problem of classification approaches is the long, tedious, human-intensive training phase. In our experience, an accurate image labelling can take several hours for an expert operator to label each image [12]. Whatever user interface assisting the user and to reduce training time, should be specifically designed for the task at hand. This is clearly infeasible for most application domains. Still, the labelling burden remains.We propose a simple segmentation framework based on Bayesian classification and supervised incremental learning. Conditional probabilities are learned by incrementally adding new sample image patches to automatically-learned probability functions. Supervised learning and image classification are iterated until satisfactory results are obtained.We tested our method on SEM images of biological samples. We show that one-shot and incremental labelling produce similar statistical models. Thus, comparable classification results are obtained with considerably less human effort.Although this work is motivated by the search for high-quality segmentation of biological specimens in the context of scanning electron microscopy, the general framework proposed in this paper virtually applies to every supervised segmentation algorithm in every context, irrespective of the imaging technology used.The basic idea of the proposed framework is to organise the training procedure into two phases: unsupervised bootstrap and iterative supervised model refinement. In the bootstrap phase, training images are independently segmented using an unsupervised segmentation method. A statistical model of the training images is generated using the segmented images as labellings to infer pixel classes. In the iterative phase, the model is refined by letting the user correct segmentations. This process is iterated until acceptable classification results are obtained.Fig. 1shows a graphical description of the whole process. A number of images from the training dataset are automatically segmented using a clustering algorithm. Segmentations are employed as labellings for automatic learning of pixel statistics. The learned statistical model is used to segment again training images. The output segmentations are used to guide the user in order to suggest improvements to the model. A new model is then built by merging the current model with information learnt at each supervised step. This process is iterated until the user is satisfied with the classification results, or until no further improvement is gained. Notice that unsupervised segmentation is executed over one image, whereas supervised segmentation employs the merged statistics of the whole training dataset. The graphical separation into two blocks in Fig. 1 is intended to highlight the fact that the first learning phase is unsupervised or weakly-supervised (we will discuss this issue below), whereas successive steps require user intervention.In our implementation, supervised learning is executed by image labelling. Classifications are presented in colour to the operator together with input images (see Fig. 2). The user can select misclassified regions and associate them with a different label. Image regions as well as single pixels can be selected, using whatever tool is available in the labelling software. We implemented our framework as a plugin for ImageJ [16]. The user can thus take advantage of all selection tools available in that software, plus a few custom ones. Each selection can be attached one of the labels defined in the initial step. The model is updated by merging new statistics extracted from user-selected patches. In this incremental approach, the operator is only asked to give exemplar and even imprecise labellings. Consequently, the training phase is less burdensome and time consuming for the user. Moreover, overfitting can be avoided by adding new labellings from time to accommodate for new misclassified images, during the natural life of the tool.In the following pages, a fully-operational algorithm is presented. Notice, however, that our objective is not to propose a new segmentation algorithm. Rather, we simply provide a new framework to ease the training burden for whatever learning-classification algorithm one wants to use. In this paper, we choose a series of features to cope with the peculiarities of image datasets used in our experiments. Nonetheless, other features can be used for whatever application. The same is true for pre-processing and post-processing. Our experiments aim at showing that there is no remarkable difference if a classifier is trained in a single shot, or in a more distributed iterative fashion.Image acquisition is an inherently noisy process in microscopy. Most of the acquisition noise is automatically removed by the device software by averaging over multiple acquisitions of the same sample. However, some noise is still present in micrographs that can affect the results of feature extraction.Median filter followed by Gaussian blur is used to attenuate speckle noise as well as white noise. Although median filtering can remove image features, we experimentally found that filtering with a small window slightly improves segmentation results with our images. Then, the morphological minimum filter is applied to smooth out small irregularities around object borders. This improves classification quality at blob borders. Processed images are used for feature extraction.Feature vectors are built from image pixels using whatever image filters one may choose for her application. The whole algorithm is then executed in the feature space. Hence, feature vectors can be computed beforehand and stored. Input images are normalised before filtering, to have zero mean and unit standard deviation to accommodate for varying illumination conditions.In our implementation, we employ ten features: Maximum-Response (MR8) filter bank (8 responses), plus pixel brightness and a radial symmetry detector. The MR8 filter bank consists of 38 filters, but only 8 filter responses are used [19]. Filter responses are taken from two isotropic filters (a Gaussian and a Laplacian of a Gaussian (LoG), both at scaleσ=10), an edge (first derivative) filter, and a bar (second derivative) filter (both at 6 orientations and 3 scales(σx,σy)={(1,3),(2,6),(4,12)}). The response of the isotropic filters are used directly, whereas the oriented filters (bar and edge) contribute with one response for each scale, collapsed by using only the maximum filter response across all orientations. This gives 8 filter responses and guarantees rotational invariance.The radial symmetry transform (RST) computes, for each pixel, the influence of nearby pixels along the gradient direction [13]. Pixels on radial boundaries receive high scores (in magnitude). Sign encodes the gradient direction. Scores are computed at various scales (ri={1,2,⋯,6}pixels) to take into account contributions of pixels in various neighbourhoods. RST is very effective in capturing borders of radially-symmetric objects. We use RST to characterise points with high radial symmetry, such as globular pores in a compact matrix. Globular structures are present in micrographs related to most disciplines, ranging from food samples to porous rocks. We employ large radii (ri={8,11,15,19,23,27,32,36,42}pixels) to characterise the inner part of pores, not only their borders.Finally, the intensity feature is just the normalised input.Since MR8 filters are normalised using the L1 norm, filter responses are of the same magnitude. Contrarily to what suggested in [19], we do not apply the final Weber-like normalisation. RST is normalised as to have zero mean and unit standard deviation. This normalisation ensures that responses in the feature space have the same magnitude along all axes.Each image is independently segmented using an unsupervised or weakly-supervised segmentation method. Segmentations are used as labellings to bootstrap the iterative procedure.In our implementation, an initial labelling is automatically obtained from the feature vectors using clustering. Principal components are computed from the set of image feature vectors and only the most significant are retained. In our experiments, we retain only the principal components whose normalised energy sums to 0.8. In most of our experiments, three principal components suffice. In two cases, four components were needed. Clustering is executed in the Principal Component space using k-means. Initial centroids are provided by the user by selecting one pixel sample for each class, only for one training image. This is a straightforward operation for a domain expert, since classes represent morphological structures in the sample. Alternatively, the operator might provide only the number of classes. This, however, would not ensure fast convergence to the desired classes. Our earlier experiments showed that the learning rate is slower, as one might expect. Our microscopists think that this slight extra burden is a price worth to pay for faster convergence and, most of all, it gives the feeling of having more control on the system.We model the statistical source using a simple Bayesian framework. Let us denote with ckthe kth class,k=1,⋯,K, and letD(x)={Fm(x)}m=1,⋯,Mbe the feature vector for pixel x, where K is the number of classes and M is the number of features. We learn the model for the posterior probabilityP(ck|D)from the likelihoodsP(D|ck)using Bayes’ theorem. Likelihood functions are simply modelled as normalised histograms over the feature space.We assume that the priors P(ck) are equiprobable to avoid any bias on the data. These probabilities might be different for each class since, for example, the image area occupied by one class can be substantially higher than for other classes. Indeed, that is the case for many applications. Priors can be easily computed from training sets simply by counting the number of pixels in each class, provided that the training set is representative enough of the dataset.In our framework, however, the user only selects small image samples as needed. There is no way to tell if the selected class is common or rare, from these samples alone. Priors could be computed from initial labellings, again by counting labelled pixels. However, it would not be wise to try to infer any crucial information from automatic labellings of a few images. Finally, too many uncontrolled elements can affect the percentage of pixels in one class over the total volume, especially when microstructure exhibits high variability, such as in natural materials. Consequently, equiprobability of priors, P(ck), is a reasonable choice.Once we get the posterior probabilitiesp^k(x)=P(ck|D), we choose the class maximisingp^k(x)(MAP classification rule) as the new label for pixel x.We adopt an incremental approach to learning mainly due to its versatility and its ability to adapt to new images. Following the definition in Giraud-Carrier [8], a learning algorithm is named incremental if, for any given samples1,⋯,sn, it produces a sequence of hypothesesh0,⋯,hn, such that hidepends only onhi−1and si. In the proposed framework, the initial hypothesis, h0, is given by the unsupervised or weakly-supervised segmentation obtained in the initial step (k-means clustering, in our implementation). Samples siare given as likelihood functions obtained from user re-labellings.Images that have been classified at stepi−1are used as reference labellings during step i. Basically, label images are presented to the operator together with input micrographs for further refinement of learned statistics (Fig. 2). Image regions can be easily selected in label images and re-labelled. Based on the idea that re-labelling occurs mainly at misclassified regions, feature vectors in the misclassified region act as new samples to update the learned model in the most critical regions of the corresponding likelihood functions.After the operator is satisfied with the labelling, statistics are gathered from the re-labelled regions alone and integrated into the current model. LetpFmi(x)be the likelihood of the feature Fmcomputed at pixel x and step i, andNFmibe the number of samples used to learnpFmi(x). The update is executed by merging old and new statistics with the expressions:(1)NFmi+1=αNFmi+N¯Fm(2)pFmi+1(x)=αNFmipFmi(x)+N¯Fmp¯Fm(x)NFmi+1wherep¯Fm(x)is the likelihood of Fmat pixel x, computed from the re-labelled data, andN¯Fmis the number of corresponding samples. With reference to the definition above, all the likelihood functionsp¯Fm(x)at step i constitute the new sample si. The aging factorα∈[0,+∞[reduces the incidence of previously acquired data (for α < 1) thereby giving more importance to new samples. The extreme values of α represent the cases in which the old model is completely discarded (α=0) or no new information is taken into account (α→+∞). In our experiments, we empirically setα=0.01. Such a low value causes our method to strongly rely on newcomers. Hence, the whole algorithm is sensitive to wrong labellings, possibly leading to catastrophic forgetting of peculiarities of the previously learned model. This is a risk. However, our main objective is a fast learning rate. Most microscopists might be willing to risk occasional loss of data, if they are rewarded with faster learning rates. Moreover, the training software can be equipped with an undo function to go one step backward in the iterative process. Hence, possible damages to the model trained so far can be undone at the cost of the time wasted in the last iteration.The underlying idea to this incremental strategy is that, whatever the initial shape of the density functionspFmi(x), the process should ideally converge towards the perfect statistical model for the imaging system and samples at hand. In particular, the model generated by our framework should be close to likelihood functions learned from an exact pixel-by-pixel labelling, provided that the latter is a good model of the data.The important point here is that the user does not have to be precise. In fact, rough region selections should suffice for successful learning, since misclassifications can be corrected in further refinement learning steps. Provided that partial and incorrect labellings are acceptable, this allows for a speed-up in the manual labelling process. As a side effect, re-labelling previous classifications is much easier than labelling images from scratch since, for example, they allow the selection of irregularly-shaped regions with one mouse click as well as connected regions of pixels sharing the same classification. Again, this helps to speed-up labelling.Notice that classes are chosen by to the microscopist in the initial step, and input to k-means or whatever segmentation algorithm one might choose. In the framework discussed so far, new classes appearing after the initial training cannot be added. However, this is a limitation of our implementation, not of the framework itself. A new class can be processed by making room for a new likelihood function in the model. Its shape is learned from the given examples as soon as they are provided by the user. From the user perspective, it suffices to add a “new class” button to the user interface in the program. Selection and labelling can be done in the very same way as iterative training.Although each pixel is classified using structured neighbourhood information, its classification is independent from classifications of neighbouring pixels. That is, pixels are assumed statistically independent. This makes our method intrinsically pixel-wise. As a result, some pixels can get a different classification from their surrounding, resulting in isolated (often misclassified) points. In our implementation, a simple re-classification procedure is adopted to smooth out those misclassified pixels. Letp¯be a pixel with classificationC(p¯), and letΩ(p¯)be its neighbourhood. IfC(p)0.25em0ex≠0.25em0exC(p¯),∀p∈Ω(p¯),p¯takes the classification shared by the majority of its neighbouring pixels. This simple procedure assures the absence of isolated pixel classifications, thus reducing the probability of misclassifications.

@&#CONCLUSIONS@&#
