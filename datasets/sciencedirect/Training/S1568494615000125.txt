@&#MAIN-TITLE@&#
Investigating a hybrid of Tone-Model and Particle Swarm Optimization techniques in transcribing polyphonic guitar sound

@&#HIGHLIGHTS@&#
Each Tone-Model is constructed from monophonic training examples.Plausible sounding pitches can be guessed from polyphonic input using Tone-Models.PSO finds the optimal Tone-Models combinations that best describes the audio input.The approach gives a better transcription accuracy than the competing NMF technique.The approach performs well when the Tone-Models are stable (synthesized audio case).

@&#KEYPHRASES@&#
A hybrid of Tone-Model and Particle Swarm Optimization,Transcribing polyphonic note-events,Non-negative matrix factorization,

@&#ABSTRACT@&#
In this article, we describe a novel polyphonic analysis that employs a hybrid of Tone-Model (TM) and Particle Swarm Optimization (PSO) techniques. This hybrid approach exploits the strengths of model-based and heuristic-search approaches. The correlations between each monophonic Tone-Model and the polyphonic input are used to predict relevant pitches such that the aggregations of the pitches’ Tone-Models are able to describe the harmonic contents of the polyphonic input. These aggregations are then refined using PSO. PSO heuristically searches for a local optimal aggregation in which some Tone-Models suggested earlier may be excluded from the final best aggregation. We present and discuss the design of our approach. The experimental results from the proposed hybrid approach are compared and contrasted with the non-negative matrix factorization (NMF) technique. A performance comparison between synthesized guitar sound and acoustic guitar sound is discussed. The experimental results confirm the potential of TM–PSO in polyphonic transcription task.

@&#INTRODUCTION@&#
Polyphonic transcription attempts to extract note event information from a polyphonic audio stream. Early work by [1] viewed this task from a source separation perspective. Recovering original sources (i.e., note events) from a single channel mixed-source signal is possible, only if some specific constraints to the signal [2] are known. From the literature, harmonic structures of notes have been widely employed as a constraint for this purpose. One of the early demonstration of this concept was from [3], where a two-part polyphonic audio stream was transcribed using harmonic information devised as comb filters.We may describe the exploitation of the harmonic structure along the spectrum in which at one end are a knowledge-based approaches and at the other end, soft computing approaches. A knowledge-based approach was attempted by [4,5] with some success. However, the two major bottlenecks in the knowledge-based approach, i.e., knowledge elicitation and a long inference chain, might discourage many researchers from pursuing this direction.In [6], a Bayesian probability network model was employed in a music scene analysis. The Bayesian network allowed prior knowledge to be encoded in the model. Kashino et al., modeled their Bayesian networks based on the backboard system of [4]. In [7], a dynamic Bayesian network model was explored as a generative model of polyphonic notes. Conceptually speaking, the Bayesian approach is attractive since it allows both expert knowledge and evidential knowledge from the data to be encoded in the system. However, the approach is subject to intractability issues; so the model is usually abstracted with a tradeoff against its performance.Many researchers approached the polyphonic transcription task using various supervised learning techniques, for example, artificial neural networks [8], support vector machines and hidden Markov model [9]. It is important for the supervised models to learn suitable harmonic information and the main drawback of this approach is its impracticality. Recent works by [10–14] offered new perspectives in approaching the polyphonic transcription task. It was suggested by [10] that NMF can be employed in transcribing polyphonic notes. However, there were still issues of source permutations (i.e., the monophonic lines appear in random order) and cleanliness of source separation (i.e., separated source might still be polyphonic). The issues arose from the fact that there were many local optima that could satisfy the factoring of a given matrix V. In [15,16], the Tone-Model concept was proposed to tackle both source permutation and cleanliness of source separation issues. The concept of Tone-Models may be seen as a form of dictionary vectors implemented in [11]. However, while the dictionary vector may learn the characteristics of the harmonic structure of the underlying note or notes (in unsuccessful cases), our Tone-Model learn a harmonic structure of a single note.A complete set of dictionary vectors (or Tone-Models) could be constructed from either theoretical knowledge or learned from training examples. It is, however, not practical to theoretically enumerate nor learn all the possible combinations of polyphonic notes due to the exponential explosion of combinations. Here, we have decided to abstract the elemental component at the level of harmonic structure of a monophonic note. In our approach, the system learns monophonic Tone-Models. This results in a much smaller model size than learning all the possible polyphonic Tone-Model combinations.Inspired by the additive synthesis perspective, the harmonic characteristics of an observed input can be described in terms of an aggregation from the elemental components of monophonic Tone-Models. In brief, for each time frame, the system estimates a plausible polyphonic transcription output from the correlations between polyphonic input and the plausible combinations of monophonic Tone-Models. This preliminary transcription is fuzzy, as the Tone-Models might not successfully represent all the variations in the harmonic structures due to timbre variations introduced from note executions, etc. All plausible aggregations will be further fine-tuned using PSO [17,18]. The PSO searches for the most likely weighted linear aggregation of the Tone-Models that would best estimate the input harmonic structure. The rest of the paper is organized as follows: Section 2 presents related works, discusses our motivations and explains the problem formulation; Section 3 presents the important concepts of the proposed approach; Section 4 presents the experimental design and evaluation criteria; Section 5 discusses the experimental results; and finally Section 6 concludes this research work.In this work, our task concerns identifying simultaneous sound pitches from a single input channel. Separating simultaneous sound pitches from a single channel input of mixed signals is a formidable task. However, this is possible if knowledge of the basic components of the polyphonic sound is known. From the literature, the exploitation of knowledge in terms of Tone-Models was common in most polyphonic transcription reports, although it was manifested in different variants. For example, in [19] the authors employed spectral parameters to separate melody and bass line in an attempt to describe a music audio scene. In [20], prior knowledge of fundamental frequency and harmonic structures of pitch was encoded in a Bayesian network. In [11], the authors exploited Independent Component Analysis (ICA) and the idea of sparse decomposition of the Fourier spectra to identify polyphonic notes. The sparse decomposition of the Fourier spectra is the group of components that described atomic sound components. In [21], Independent Subspace Analysis (ISA) and Hidden Markov Model (HMM) were employed to model chord spectra and note duration. Here the ISA or HMM model learned spectra information (i.e., information of Tone-Models). In a recent work by [22], the authors exploited Tone-Models in the form of monophonic constrained non-negative sparse coding to transcribe polyphonic notes.In [10], the authors discussed the potential of NMF in a polyphonic transcription task. The authors showed that NMF could extract the note events from polyphonic audio if the number of unique events (i.e., the factoring parameter r of Vm×n≈Wm×rHr×n) was properly set. In [16], a Tone-Model was employed as a basis vector W in NMF with some degree of success. The Tone-Models used in [16] were spectra components that described the harmonic structures of each pitch. To date, NMF has attracted considerable interest from researchers and the application of spectral information as constraints to NMF has received much interest.In the NMF technique discussed above, if the matrix W successfully represents the underlying harmonic structures of the polyphonic notes, then the matrix H has an interpretation of the polyphonic notes being sound or silent. Employing Tone-Models as a basis vector W in NMF can be seen as providing NMF with a guide to search for the matrix H (V≈WH). We can search for the matrix H using techniques drawn from the swarm intelligence approach.Let us pose the same problem using a Bayesian approach, let X denote the input spectra derived from monophonic input generated from the pitches in a specific range, e.g., E2 to G5; if we have the likelihood of each Tone-Model p(X|M), where M∈{E2, …, G5}, then it is possible to infer the sounding pitch using the Bayesian decision rule [23]:(1)P(M|X)=p(X|M)P(M)p(X)However, extending the concept to handle all polyphonic combinations would require 2|M|−1 models. It is impractical to learn all the possible combinations of all pitches. Instead of learning all the possible combinations, our system uses the Tone-Model information of each pitch to guess the relevant pitches. A relevant pitch is a plausible candidate justified by the correlation of a monophonic Tone-Model to the polyphonic input.Due to the approximate nature of the suggested relevant pitches, they are likely to contain true positive, false positive and false negative cases. We employ a heuristic search to find a good solution from these relevant pitches. Our implementation can be described in a search perspective as follows: (i) the application of Tone-Models prunes the unfruitful part of the search space and then (ii) PSO refines the solution obtained from the first stage.A guitar produces sounds from its vibrating strings. There are six strings in a standard guitar. Fig. 1shows a range of notes (shown as MIDI note numbers) on the guitar finger board. The following observations are made from the domain:1.The maximum number of simultaneous pitches is limited to six. This constraint is from the physics of the instrument.A total of 40 different pitches could be produced from the 15 fret positions (see Fig. 1, MIDI note numbers 40–79 correspond to pitches E2–G5, respectively). Possible combinations of pitch patterns equal 40C1 + 40C2 + 40C3 + 40C4 + 40C5 + 40C6, wherenCkcan be written using the factorial as n!/((n−k)!k!). It should be noted that some of these combinations would not be possible due to the physics of the instrument as well as the anatomy of our hand. Nevertheless, the amount of possible combinations is very large.It is impractical to create a dictionary of all possible pitch combinations. Hence, we propose to abstract the domain to the level of Tone-Model of each note. Harmonic characters of polyphonic notes are approximated as weighted aggregations of these monophonic Tone-Models.The aggregation of the different Tone-Models can be approximated by searching for the most likely combination. However, it is also impractical to approach the problem with exhaustive search techniques. Hence, we propose to search for the optimal Tone-Model aggregation using PSO.Let a be a sequence a=〈a1, a2, …, an〉 where anrepresents a time domain data sampled from polyphonic audio wave at fssamples per second. A sequence of these time domain samples can be used to estimate the frequency components in the polyphonic audio. The frequency component Xk, estimated from a sequence a of length N using Fourier transform, is(2)Xk=∑n=0N−1hnane−j2πkn/Nwhere hnis the hamming window defined as 0.54−0.46cos(2π(n/N)) and k=0, 1, …, N/2. In our implementation, N is set at 8192 samples. Xkis further binned according to the musical pitches. The binning transforms Xkcoefficients into a piano roll of activation strength with the center frequency fcof pitch p, p∈{40, …, 99}11The range of pitches on the guitar is limited to the region between E2 and G5. Therefore, it is decided to create 60 bins of pitches ranging from E2 to D♯7 for a piano roll representation.at:(3)fc(p)=440×2(p−69)/12The activation magnitude of the pitch p is the average of the magnitude of Xkcoefficients in the range of kl=0.99fc(p) to ku=1.01fc(p). Hence, the magnitude of Xpcan be calculated using the equation below:(4)Xp=1ku−kl+1∑k=klku|Xk|where |·| denotes the magnitude of Xk, kland kuare the lowermost and the uppermost k index for the pitch p. The observed input spectra at time frame τ, X(τ) is constructed by concatenating Xptogether:(5)X(τ)=〈Xp〉where 〈Xp〉 denotes a sequence of Xp, p∈{40, …, 99}.In this implementation, all audio data are recorded with 44.1KHz sampling rate, 16 bits, mono channel. The 40 Tone-Models, xm, m∈{40, …, 79}, are learned in an offline mode from monophonic pitches ranging from E2 to G5 (this corresponds to MIDI note numbers 40–79). The monophonic pitch is produced from a built-in standard MIDI synthesizer device and from an acoustic guitar. The input wave file is transformed into its corresponding frequency domain representation using short time Fourier transform (STFT, see Eqs. (2)–(5)). The Tone-Model xmis the mean calculated from many STFT frames since the number of STFT frames depends on the length of the input wave file.We denote the concept of relevant pitch using the vectorr∈R40×1. Each entry rm, m∈{40, …, 79} is the activation strength of the corresponding Tone-Model m. The activation strength is computed using the similarity between the input polyphonic spectral X and the Tone-Model xm.The r is estimated frame by frame (see Fig. 2). For a given time frame τ, a strong correlation between xmand the input harmonic structure X of the frame is expected if xmis one of the sounding notes of that frame. In this implementation, the correlation is estimated from the overlap in the overtone series between the input and the Tone-Model. Six relevant pitches are determined at each time frame by computing the degree of relevance between the input X and each Tone-Model xm(based on the fact that there could only be six maximum sounding pitches at any given time):Estimate relevant pitches r// Input: X, Output: rfunctionrelevantPitch(X) return r// Initializer∈R40×1with zeroes in all entriesfor each Tone-Model xm, m∈{40, …, 79}// max(a, b):= if a ≥ b, → a; else → brm=∥xm−max(xm−X,0)∥∥xm∥endforreturn rendEach entry rmhas an interpretation of how relevant the model m is to that particular frame. PSO employs this information to finetune the final aggregations (see Fig. 2).The PSO [17] algorithm is a heuristic search which does not require derivative objective functions. A typical PSO implementation exploits two main sources of knowledge derived from each member of the swarm (i.e., self learned experience) and from all members of the swarm (i.e., group culture). Standard PSO search concepts can be expressed using the following equations:(6)xij(t+1)=xij(t)+vij(t+1)wherevijis the velocity of particle i in dimension j.(7)vij(t+1)=wvij(t)+c1r1j(pij(t)−xij(t))+c2r2j(gij(t)−xij(t))where xijandvijdenote the position and velocity of the particle i in the j dimension. Thewis the inertia weight, and c1r1jand c2r2jare the weight parameters that combine the influence of the local best position pijand the global best position gijto determine the velocities of the particles in the next time step. Table 1shows the PSO parameters settings. These parameters are empirically determined. Recommended values according to [24] were used as a starting point, and then the values were empirically modified to locate the operating region for our task.Let pidenote a vector of pijwhere pijrepresents the activation strength of the Tone-Model j of the particle i. Information from the relevant vector r determines the selected Tone-Models and their activation strength. The objective function of each particle in the swarm is to minimize:(8)fobj=∥X(τ)−X{mr}pi∥whereX{mr}denotes a matrix constructed from |{mr}| Tone-Models of the suggested relevant pitches r. Take note that, we use |{mr}| to denote the cardinality of {mr}.Let g represent the most probable activation strength of the pitches of the whole swarm, and g will take the value of the best piat each step. The final best g suggests the optimum aggregation of the Tone-Models which is actually the transcribed notes, i.e., for each time step τ,X(τ)40×1≈X{mr}40×|{mr}|g|{mr}|×1. The pseudo code in Table 2gives the implementation details of the hybrid of Tone-Model and PSO.The returned g (see Table 2) is the activation strength of Tone-Models which are the sounding pitches. These values are normalized and then converted to binary note on/off events. In this experiment, the activation strength values above 10% are considered as note on, otherwise they are considered as note off.We wish to highlight the following aims in our experiments. In the first experiment, we compared the performance of the proposed hybrid TM–PSO with the competing NMF technique (implementation follows [25,16]). The same synthesized guitar sound files were employed in the experiment. In the second experiment, we compare the transcription performance of TM–PSO on two data sets drawn from synthesized guitar sounds with acoustic guitar sounds.Our system was implemented in MATLAB. All synthesized guitar sound files were generated using a nylon string sound font. This is a common sound font available in all sound cards that support general MIDI. The acoustic sound files were obtained from two sources: (i) a recording from an acoustic guitar model Yamaha CG 40 using a microphone with the following specifications: frequency response: 20–16KHz, sensitivity: −58 ± 3dB, S/N ratio: 40dB; and (ii) from public sound samples provide by the university of IOWA.We prepared MIDI files of the following six pieces: (1) Speak Softly Love, a theme song from the movie God Father; (2) All My Loving, one of the Beatles’ hits; (3) Yellow Bird, a Jamaican folk song; (4) Bach Chorale titled Aus tiefer Not schrei’ ich zu dir (This chorale is indexed as number 10 in the Riemenschneider edition [26], here onwards referred to as R10); (5) Bach Chorale (R26) titled O Ewigkeit, du Donnerwort; and (6) Bach Chorale (R28) titled Nun komm, der Heiden Heiland. These pieces were rearranged for the guitar by the author (except for Bach Chorales which were from [26]. The MIDI files can be downloaded from http://www.jsbchorales.net/bwv.shtml).22Last access 1 May 2013: http://www.jsbchorales.net/bwv.shtml.These MIDI files provided ground truth references in the evaluation stage.We generated synthesized sound files from the MIDI files by playing-back using a synthesized nylon string option on the PC sound card. The same six pieces were played using a classical acoustic guitar (model: Yamaha CG 40) and recorded through a microphone. The guitarist was instructed to play all the songs according to the MIDI files (he would listen to the MIDI files using a headphone while recording). This helped to minimize discrepancy in tempo between the synthesized sound files and the performed sound files. All wave files were recorded using 16 bits, 44.1KHz sampling rate, with a mono channel. It is always difficult to find public data sets for research in polyphonic transcriptions. The above data were prepared and also made available via Google Docs.33https://docs.google.com/folder/d/0B0ZSUio2YfcAY1diaFdfbnhOSVU/edit?usp=sharing (You need a Google account to access this.).Recently, the electronic music studios at the University of Iowa included guitar samples in their sound database.44Last access 1 July 2013: http://theremin.music.uiowa.edu/MIS.html.The guitar samples in the databases included both 16 bits, 44.1KHz sampling rate and 24 bits, 96KHz sampling rate. The data set employed in this experiment was from the 16 bit, 44.1KHz sampling rate data set. The data set included 117 note events played on six guitar strings (from E2 to B3 on the low E string, from A2 to E4 on the A string, from D3 to G#4 on the D string, from G3 to C#5 on the G string, from B to F#5 on the B string and from E4 to B♭5 on the high E string). Further information of the data set is available at the website http://theremin.music.uiowa.edu/MIS.html.These 117 note events, covering the pitches ranging between E2 and B♭5, were used to train the Tone-Models. Since the data set was a collection of single note events, we decided to randomly pick note events from the available 117 note events and concatenate them to form a random note sequence. To simulate polyphonic notes, up to six note events might sound simultaneously. The note events information was converted to MIDI files which were then used to generate synthesized guitar sounds as well as acoustic guitar sound files.55In order to generate acoustic sound files, we wrote a program to extract note events information from MIDI files and concatenate corresponding acoustic sound samples to generate acoustic guitar sound files.We created 32 random note sequences. There were eight different sequences for each of the following duration types: 0.25, 0.5, 0.75 and 1s.66The duration of 0.25s was chosen as the minimum duration since our STFT window size was set at 8192 samples. (There were 11,025 samples in 0.25s for a signal sampling at 44.1KHz. Therefore, we decided to set the minimum duration at this value.) The duration of 1s was chosen as the maximum duration because from our observation of all the samples from IOWA, the activities were low after 1s.. Each random sequence contained an average of 100 note events. There were a total of 64 test files for both synthesized sounds and acoustic sounds with four duration types.To avoid subjectivity on counting the number of correctly transcribed notes when the duration of a transcribed note might be either slightly longer or shorter than the actual duration, we counted the number of pixels. This was also the approach taken by [11,16]. We represented the ground truth, G40×n1, Gij∈{0, 1} and the transcription output T40×n2, Tij∈{0, 1} as images of note on/off (see Fig. 3, white pixels denote note on and black pixels denote note off). Since the ground truth G of each song was computed from a reference MIDI file and the transcribed notes T were computed from a wave file, the number of columns n1 might not be equal to n2. In such a case, the smaller matrix was linearly scaled up, so that both matrices had the same number of column n. Overlaying the transcribed notes T40×non the ground truth G40×nrevealed pixels corresponding to true positive (tp: correctly transcribed), false positive (fp: transcribed as notes while they were not notes) and false negative (fn: not transcribed as notes while they were notes).More specifically, let TP, FP and FN be matrices of size 40×n initialized with all entries = 0, their entries are determined as follows:(9)∀i,jIFGij=1∧Tij=1THENTPij=1∀i,jIFGij=0∧Tij=1THENFPij=1∀i,jIFGij=1∧Tij=0THENFNij=1In this manner, tp=∑ijTPijcorresponded to the correctly transcribed notes, fp=∑ijFPijcorresponded to the spurious notes and fn=∑ijFNijcorresponded to the notes that were undetected. The results were evaluated based on the standard precision and recall measures where(10)Precision=tptp+fp(11)Recall=tptp+fn(12)F=2×Precision×RecallPrecision+Recall

@&#CONCLUSIONS@&#
