@&#MAIN-TITLE@&#
A new method for optimizing a linear function over the efficient set of a multiobjective integer program

@&#HIGHLIGHTS@&#
We propose a new exact algorithm for solving multi-objective integer programs.The algorithm can optimize a linear function over the set of efficient solutions.The algorithm is efficient and it is faster than the state-of-the-art algorithms.The algorithm can be modified easily to efficiently compute the nadir point.

@&#KEYPHRASES@&#
Multiobjective integer programming,Nondominated points,Extension of the L-shape search method,Optimizing over the efficient set,Nadir point,

@&#ABSTRACT@&#
We present a new algorithm for optimizing a linear function over the set of efficient solutions of a multiobjective integer program (MOIP). The algorithm’s success relies on the efficiency of a new algorithm for enumerating the nondominated points of a MOIP, which is the result of employing a novel criterion space decomposition scheme which (1) limits the number of subspaces that are created, and (2) limits the number of sets of disjunctive constraints required to define the single-objective IP that searches a subspace for a nondominated point. An extensive computational study shows that the efficacy of the algorithm. Finally, we show that the algorithm can be easily modified to efficiently compute the nadir point of a multiobjective integer program.

@&#INTRODUCTION@&#
Many real world-problems involve multiple objectives. Due to conflict between objectives, finding a feasible solution that simultaneously optimizes all objectives is usually impossible. Thus, generating many or all efficient solutions, i.e., solutions in which it is impossible to improve the value of one objective without a deterioration in the value of at least one other objective, is usually the goal in multiobjective optimization.We focus on multiobjective integer programs (MOIPs). Exact algorithms for MOIPs can be divided into decision space search algorithms, i.e., methods that search in the space of feasible solutions, and criterion space search algorithms, i.e., methods that search in the space of objective function values. During the last decade, several criterion space search methods have been developed for biobjective, triobjective, and general MOIPs, and, currently, they appear to be more effective than decision space search algorithms. Therefore, our focus will be on criterion search space methods.Despite the recent progress in the development of exact algorithms for MOIPs, computing the efficient frontier is still not practical for many real-world problems. Furthermore, some researchers argue (see for instance Jorge, 2009) that presenting (too) many efficient solutions to a decision maker may, more than anything, confuse the decision maker and may make selecting a preferred solution almost impossible. An approach that alleviates this issue is finding the most desirable solution among the efficient solutions (preferably without enumerating all of them). This approach is known as optimizing over the efficient set, which is a global optimization problem (Benson, 1984).Many researchers have addressed the problem of optimizing a linear function over the efficient set of a multiobjective linear program (MOLP). We refer the interested readers to the studies by Benson (1991, 1992, 1993, 1984), Dauer (1991), Ecker and Song (1994), Sayın (2000) and Yamamoto (2002). However, there are only a few studies that address the problem of optimizing a linear function over the efficient set of a MOIP. There are two main reasons for that:1.The set of (feasible) solutions to a MOLP, in the decision space as well as in the criterion space, is convex (assuming that the problem is feasible). Therefore, all efficient solutions to a MOLP are supported, i.e., they can be obtained by optimizing a weighted combinations of objective functions. Unfortunately, in general, this is not the case for a MOIP. As a consequence, determining the set of efficient solutions of a MOIP is (far) more challenging.The natural algorithm for optimizing a linear function over the efficient set of a MOIP, which enumerates the nondominated points of the MOIP and uses lower and upper bounds on the optimal value of the linear function to curtail the enumeration, requires the efficient enumeration of the nondominated points and the computation of effective bounds. Even though progress has been made on the former in recent years, little is known about how to accomplish the latter.To the best of our knowledge, Abbas and Chaabane (2006) are the first to develop a method for optimizing a linear function over the efficient set of a MOIP. Their algorithm is a decision space search method. In each iteration, the method adds (different types of) cuts to guarantee an improvement in the value of the linear function. Unfortunately, Abbas and Chaabane do not report any computational results, which makes it difficult to assess the efficacy of their method.Jorge (2009) is the first to develop a (simple) criterion space search algorithm. His approach is a variant of the method of Sylva and Crema (2004). Chaabane, Brahmi, and Ramdani (2012) and Chaabane and Pirlot (2010) present minor variations to Jorge’s algorithm. For instance, the algorithm in Chaabane et al. (2012) differs primarily in the scalarization technique used to determine whether a solution is efficient. Unfortunately, in both studies, no computational comparison with Jorge’s algorithm is included. As a result, it is not possible to determine whether the changes lead to improved performance.A related problem, and a special cases of optimizing a linear function over the efficient set, is finding the nadir point, i.e., the point in criterion space given by the worst value of each of the objective functions over the set of efficient solutions. Recently, Köksalan and Lokman (2014) developed a new algorithm to find the nadir point for a MOIP. They showed that their approach is significantly faster than using Jorge’s algorithm. The main reason that their algorithm performs better than Jorge’s algorithm is that their algorithm decomposes the criterion space rather than adding more and more binary variables and disjunctive constraints to the single-objective IPs as is done in Jorge’s algorithm (and Sylva and Crema’s algorithm). Kirlik and Sayın (2014) also present an algorithm for finding the nadir point. Unfortunately, they did not compare their approach to the one of Köksalan and Lokman.The main contribution of our research is the development of a new algorithm for enumerating the nondominated points of a MOIP and showing how the algorithm can be modified to optimize a linear function over the set of efficient solutions of a MOIP. The algorithm is an extension of the L-shape search method for triobjective integer programs (Boland, Charkhgard, & Savelsbergh, 2015) that works in the full-dimensional criterion space rather that a projected lower-dimensional criterion space. The algorithm has the following desirable characteristic: after finding a nondominated point, it adds at most one new subproblem to the list of subproblems still to be solved. As a consequence, the algorithm can be modified to efficiently optimize a linear function over the set of efficient solutions of a MOIP.All criterion search space methods, except for the method of Sylva and Crema (2004), decompose the criterion search space into smaller subspaces (or subproblems) after finding a nondominated point; one of which can be deleted as it is dominated by the newly found nondominated point, but the others, which may still contain as-yet-unknown nondominated points, have to be searched at some future point in time (and possibly further decomposed). By maintaining a best known efficient solution, i.e., a known efficient solution with the best value of the linear function, and by computing a bound on the value of the linear function for a subspace, it may be possible to discard that subspace from further consideration. However, because computing such bounds is time consuming, the efficiency of such an approach depends strongly on the (total) number of subspaces created. The method of Sylva and Crema (2004) does not decompose the search space, but, instead, “eliminates” the dominated part of the search space by adding sets of disjunctive constraints (and associated binary variables) to the IP that searches for a nondominated point. As a result, searching for nondominated points becomes more and more time consuming, because the IPs get harder and harder to solve. This, of course, also implies that computing a bound on the value of the linear function becomes more and more time consuming.By developing a new method for enumerating the nondominated points of a MOIP which, when decomposing the search space, never increases the number of subspaces that still need to be explored by more than one, and does not increase the complexity of the subspaces that still need to be explored, we are able to limit the time spent on computing bounds on the value of the linear function, because fewer bounds are computed (and, thus, there are fewer wasted bound computations), and each bound computation does not require the addition of a large number of sets of disjunctive constraints (and, thus, bound computations do not become more time consuming over time).The proposed algorithm has two additional desirable properties: (1) because it maintains a lower and an upper bound on the value of the linear function at any point in time, it can be used to quickly generate a provably high-quality approximate solution, and (2) it can easily be modified to efficiently compute the nadir point of an MOIP.An extensive computational study demonstrates that the proposed algorithm clearly outperforms Jorge’s algorithm (Jorge, 2009), the current state-of-the-art, on a variety of benchmark and randomly generated instances, and that it is competitive with customized algorithms for computing the nadir point.The rest of paper is organized as follows. In Section 2, we introduce important concepts and notation. In Section 3, we present an introductory example. In Section 4, we detail the logic of the new algorithm. In Section 5, we review Jorge’s algorithm and show how this algorithm as well as our algorithm can be modified to compute the nadir point efficiently. In Section 6, we present the results of a comprehensive computational study. Finally, in Section 7, we give some concluding remarks.A multiobjective optimization problem can be stated as follows:(1)minx∈Xz(x):={z1(x),⋯,zp(x)},wherezj(x):Qn→Qfor allj∈{1,⋯,p},X⊆Qnrepresents the feasible set in the decision space and the imageYofXunder vector-valued functionz={z1,...,zp}represents the feasible set in the criterion space, i.e.,Y:=z(X):={y∈Qp:y=z(x)for somex∈X}. For convenience, we also use the notationQ≥p:={y∈Qp:y≥0}for the nonnegative orthant ofQp,andQ>p:={y∈Qp:y>0}for the positive orthant ofQp.WhenXis defined by a set of affine constraints andz1(x),…,zp(x)are linear functions, then (1) is a multiple objective linear program (MOLP) and a multiple objective integer program (MOIP) whenX⊆Zn. In this study, we focus on MOIPs and we denote byXLPthe linear programming relaxation ofX.Definition 1A feasible solutionx′∈Xis called efficient or Pareto optimal, if there is no otherx∈Xsuch that zk(x) ≤ zk(x′) fork=1,…,pand z(x) ≠ z(x′). If x′ is efficient, then z(x′) is called a nondominated point. The set of all efficient solutionsx′∈Xis denoted byXE. The set of all nondominated pointsy′=z(x′)∈Yfor somex′∈XEis denoted byYNand referred to as the nondominated frontier or the efficient frontier.z(x)∈YNif there is no othery′∈Ysuch that y′ ≠ z(x) andz(x)∈y′+Q≥p.The pointzN∈Qpis the nadir point ifziN=maxx∈XEzi(x)for alli∈{1,⋯,p}.For the remainder, we will assume thatXLPis compact and that we want to solve(2)minx∈XEf(x),where f(x) is a linear function. To ensure that the problem cannot be solved straightforwardly, we assume thatX≠XE,that|YN|≥2and that f(x) is not a strictly positive linear combination ofz1(x),z2(x),…,zp(x).Lety∈Qpbe an arbitrary point in criterion space. We find an efficient solutionxn∈XEwith z(xn) ≤ y, if it exists, by solving the following optimization problem, which is a special case of the hybrid scalarization method of Guddat, Vasquez, Tammer, and Wendler (1985):xn∈arg min{∑j=1pzja:x∈X,zja=zj(x)∀j∈{1,⋯,p},zja≤yj∀j∈{1,⋯,p},zja∈Q∀j∈{1,⋯,p}}.We denote this search byFind-NDP(y). (We introduce auxiliary variableszja∈Qfor∀j∈{1,⋯,p}representing the values of the objective functions to reduce the number of non-zeros in the coefficient matrix.)Proposition 4If xn exists, then z(xn) is a nondominated point, i.e.,z(xn)∈YN.If xnexists, butz(xn)∉YN,then there must exist somey^∈YNwhich dominates z(xn), i.e. there must existx^∈Xwithz(x^)=y^≤z(xn)andy^≠z(xn). Thus,zk(x^)=y^k≤zk(xn)fork={1,2,⋯,p}and∑k=1pzk(x^)=∑k=1py^k<∑k=1pzk(xn),contradicting the optimality of xn.□Any efficient solutionxn∈XEis optimal toFind-NDP(y)for somey∈Qp.Sety=z(xn).□The new algorithm for solving MOIPs works with objects in criterion space defined by sets of disjunctive constraints. We denote each object byO(Y˜,y,yL)whereY˜={y1,⋯,yt}⊆Y,andy,yL∈Y. We refer toY˜,y, and yLas the upper bound envelope, the search guide point, and the lower bound point, respectively. Each object is defined as follows:O(Y˜,y,yL)={za∈Qp:zja≥yjL∀j∈{1,⋯,p},zja≤(yji−ϵ−Mj)bij+Mj∀i∈{1,⋯,t},∀j∈{1,⋯,p},∑j=1pbij=1∀i∈{1,⋯,t},zja≤(yj−ϵ−Mj)bj′+Mj∀j∈{1,⋯,p},∑j=1pbj′=1,bij∈{0,1}∀i∈{1,⋯,t},∀j∈{1,⋯,p},bj′∈{0,1}∀j∈{1,⋯,p}},where ϵ is a small positive constant, and Mjis an appropriately chosen large constant, e.g.,maxx∈XLPzj(x)for allj∈{1,⋯,p}. To find an efficient solution xnin an objectO(Y˜,y,yL),we solve the following optimization problem:xn∈arg min{∑j=1pzja:x∈X,zja=zj(x)∀j∈{1,⋯,p},zja≤(yji−ϵ−Mj)bij+Mj∀i∈{1,⋯,t},∀j∈θ(yi),∑j∈θ(yi)bij=1∀i∈{1,⋯,t},zja≤(yj−ϵ−Mj)bj′+Mj∀j∈θ(y),∑j∈θ(y)bj′=1,zja∈Q∀j∈{1,⋯,p},bij∈{0,1}∀i∈{1,⋯,t},∀j∈θ(yi),bj′∈{0,1}∀j∈θ(y)},whereθ(yi)={j∈{1,⋯,p}:yji>−∞}andθ(y)={j∈{1,⋯,p}:yj>−∞}. We denote this search byFind-NDP-Obj(Y˜,y). The sets θ(yi) for allyi∈Y˜and θ(y) are introduced for computational reasons only, and avoid the generation of redundant variables and constraints.Proposition 6If xn exists, then z(xn) is a nondominated point, i.e.,z(xn)∈YN.Similar to the proof of Proposition 4.□Any efficient solutionxn∈XEis optimal toFind-NDP-Obj(Y˜,y)for someY˜⊂Qpandy∈Qp.SetY=∅,yj=zj(xn)forj∈{1,…,p}∖{k},andyk=zk(xn)+ϵ.□The set of disjunctive constraints associated with a point y in the above optimization problem, i.e.,zja≤(yj−ϵ−Mj)bj+Mj∀j∈θ(y),∑j∈θ(y)bj=1,bj∈{0,1}∀j∈θ(y),ensures that the point za, if it exists, is not dominated by y. Observe that no constraints related to the lower bound point yLof the object are incorporated in the optimization problem. Consequently, we may find a point that lies outside the object. Note that whenY˜=∅andy=null,Find-NDP-Obj(Y˜,y)reduces toxn∈arg min{∑j=1pzja:x∈X,zja=zj(x)∀j∈{1,⋯,p},zja∈Q∀j∈{1,⋯,p}}.Letxn∈XEbe an efficient solution. To find an efficient solutionxf∈XEwith z(xf) ≤ z(xn) and minimum value of f(x), we solve the following optimization problem:xf∈arg min{f(x):x∈X,andzi(x)≤zi(xn)∀i∈{1,⋯,p}}.We denote this search byFind-Best(xn).Observation 8The solution xfexists and z(xf) is a nondominated point, i.e.,z(xf)∈YN.We denote by fland futhe best known (global) lower and upper bound on the optimal objective value of (2). We denote a solution corresponding to flby xland to fuby xu. LetY˜N:={y1,⋯,yt}⊆YN,then an improved lower bound, if it exists, on the optimal objective value of (2) can be found by solvingxl∈arg min{f(x):x∈X,zja=zj(x)∀j∈{1,⋯,p},zja≤(yji−ϵ−Mj)bij+Mj∀i∈{1,⋯,t},∀j∈{1,⋯,p},∑j=1pbij=1∀i∈{1,⋯,t},zja∈Q∀j∈{1,⋯,p},bij∈{0,1}∀i∈{1,⋯,t},∀j∈{1,⋯,p}}.We denote this search byFind-LB(Y˜N). It is the optimization problem at the heart of the method of Jorge (2009).Observation 9If xlexists, then f(xl) is a lower bound for the optimal objective value of (2). If xldoes not exist or if fu≤ f(xl) withfu=minx∈XY˜Nf(x)withXY˜N:=∪yi∈Y˜N{x∈X:z(x)=yi},then fuis the optimal objective value of (2).Next, we observe that we can compute a lower bound on the optimal objective value of (2) over the set of efficient solutions with their image in the objectO(Y˜,y,yL)by solving the following optimization problem:xol∈arg min{f(x):x∈X,zja=zj(x)∀j∈{1,⋯,p},zja≥yjL∀j∈{1,⋯,p},zja≤(yji−ϵ−Mj)bij+Mj∀i∈{1,⋯,t},∀j∈θ(yi),∑j∈θ(yi)bij=1∀i∈{1,⋯,t},zja≤(yj−ϵ−Mj)bj′+Mj∀j∈θ(y),∑j∈θ(y)bj′=1,zja∈Q∀j∈{1,⋯,p},bij∈{0,1}∀i∈{1,⋯,t},∀j∈θ(yi),bj′∈{0,1}∀j∈θ(y)}.We denote this search byFind-LB-Obj(Y˜,y,yL).We first illustrate informally how the new algorithm computes the nondominated frontier of a MOIP on an instance with two objective functions. Suppose that the nondominated frontier contains the five points {z1, z2, z3, z4, z5} shown in Fig. 1a.The idea underlying the new algorithm is to find, in each iteration, an as-yet-unknown nondominated point and to then decompose the criterion space. The algorithm maintains a priority queue of objects, each representing a (different) part of the criterion space that still needs to be explored. When decomposing an object, no more than two new objects are created and added to the priority queue. The priority queue is initialized with the object O(∅, null, null), i.e., the object representing the part of the criterion space that contains all nondominated points.In the first iteration (see Fig. 1b), the algorithm finds nondominated point z3 by calling Find-NDP-Obj(∅, null), i.e., by solvingxn∈arg min{∑j=12zja:x∈X,zja=zj(x)∀j∈{1,2},zja∈Q∀j∈{1,2}}.No other nondominated points exist in the green area, i.e., inz3+Q≥2. Therefore, we add an object corresponding to the white area to the list, i.e.,L=O(∅,(z13,z23),null).In the second iteration (see Fig. 1c), the algorithm finds the nondominated point z1 by calling Find-NDP-Obj(∅,(z13,z23)),i.e., by solvingxn∈arg min{∑j=12zja:x∈X,zja=zj(x)∀j∈{1,2},zja≤(zj3−ϵ−Mj)bj′+Mj∀j∈{1,2},∑j=12bj′=1,zja∈Q∀j∈{1,2},bj′∈{0,1}∀j∈{1,2}}.No other nondominated points exist in the new green area, i.e., inz1+Q≥2. Therefore, as is shown in Fig. 1d, we add two objects to the priority queue,L1=O(∅,(z11,z23),null)andR1=O({(z13,−∞),(−∞,z21)},null,(z11,z23)).In the third iteration (see Fig. 1e), the algorithm searches L1 and finds the nondominated point z5 by callingFind-NDP-Obj(∅,(z11,z23)),i.e., by solvingxn∈arg min{∑j=12zja:x∈X,zja=zj(x)∀j∈{1,2},zja≤(min(zj3,zj1)−ϵ−Mj)bj′+Mj∀j∈{1,2},∑j=12bj′=1,zja∈Q∀j∈{1,2},bj′∈{0,1}∀j∈{1,2}}.No other nondominated points exist in the new green area, i.e., inz5+Q≥2. Therefore, we add two objects to the priority queue,L2=O(∅,(z11,z25),null)andR2=O({(z15,−∞),(−∞,z23)},null,(z11,z25)).In the fourth iteration (see Fig. 1f), the algorithm searches L2 by callingFind-NDP-Obj(∅,(z11,z25)),i.e., by solvingxn∈arg min{∑j=12zja:x∈X,zja=zj(x)∀j∈{1,2},zja≤(min(zj3,zj1,zj5)−ϵ−Mj)bj′+Mj∀j∈{1,2},∑j=12bj′=1,zja∈Q∀j∈{1,2},bj′∈{0,1}∀j∈{1,2}},but does not find a new nondominated point, and no new objects are created.Note that when the algorithm searches R1 and callsFind-NDP-Obj({(z13,−∞),(−∞,z21)},null),i.e., solvesxn∈arg min{∑j=12zja:x∈X,zja=zj(x)∀j∈{1,2},zja≤(z13−ϵ−Mj)b11+M1,b11=1,zja≤(z21−ϵ−Mj)b22+M2,b22=1,zja∈Q∀j∈{1,2},b11,b22∈{0,1}}.the constraints enforce thatz1(x)<z13andz2(x)<z21. Also note that at the beginning of every iteration, the set of objects plus any dominated space that has been identified form a partition of the initial object.The algorithm maintains a priority queue of objects in the criterion space, each of which still has to be explored, i.e., may still contain as-yet-unknown nondominated points. Each object in the priority queue is characterized by a set of pointsY˜defining its upper envelope, a point y to “guide” the search of the object, and a point yLdefining its lower bound. The priority queue is initialized with the object defined byY˜=∅,y=null,andyL=null. The algorithm also maintains a list of known efficient solutions and their corresponding nondominated points, denoted by LE(initially the list is empty).For each objectO(Y˜,y,yL),we can compute the minimum p-dimensional hypercube, denoted by B(bl, bu) withbl,bu∈Qpand bl≤ bu, that contains it:•bl=yL(ifyL=null,thenbl=(−∞,⋯,−∞));Define{r1,⋯,rt+1}witht=|Y˜|as followsrji={yjiifθ(yi)={j},Mjif|θ(yi)|≠1,fori=1,⋯,tandrjt+1={yjifθ(y)={j},Mjif|θ(y)|≠1.bju=min(rj1,⋯,rjt+1)for allj=1,⋯,p. (IfY˜=∅andy=null,thenbu=(M1,⋯,Mp).)The algorithm maintains the objects in nonincreasing order of∑i=1pbiu. In case of ties, objects will be maintained in nondecreasing order of∑i=1pbil. (Maintaining the elements in this way is helpful for the enhancements of the algorithm that will be described later in this section.)In each iteration, the algorithm pops an objectO(Y˜,y,yL)from the priority queue. If the priority queue is empty, then the algorithm terminates. Otherwise, the algorithm callsFind-NDP-Obj(Y˜,y)to find a nondominated point xn. Ifxn=null(i.e., the search failed to find a nondominated point), then the exploration of the object is complete. Otherwise, xnis added to LEand the object is decomposed into one or two new objects (and dominated space).To perform the decomposition, four pointsy^L,y′,y′′,y^∈Qpare determined. For each component j (j=1,⋯,p):•y^jL=max(yjL,min(yj,zj(xn)));yj′={zj(xn)ifzj(xn)>yjandzj(xn)>y^jL,−∞otherwise;yj′′={yjifzj(xn)<yjandy^jL<yj,−∞otherwise;y^j={y^jLify^jL>yjL,−∞otherwise.Ify′≠(−∞,⋯,−∞)andy′′≠(−∞,⋯,−∞),then we addO(Y˜∪{y′,y′′},null,y^L)to the priority queue, and ify^≠(−∞,⋯,−∞),then we add(Y˜,y^,yL)to the priority queue. Note that (for computational reasons, i.e., avoiding the generation of redundant variables and constraints) before addingO(Y˜∪{y′,y′′},null,y^L)to the priority queue, we remove any pointsyi∈Y˜with y′ ≤ yior y′′ ≤ yi. Similarly, before addingO(Y˜,y^,yL),we remove any pointsyi∈Y˜withy^≤yi. Algorithm 1 shows a precise description of the algorithm.To illustrate the decomposition scheme, we show and discuss the four possible situations, given in Figs. 2–5, that can arise during the exploration of an objectO(Y˜,y,yL). In each situation, we assume thatFind-NDP-Objreturns an efficient solution xn≠ null (and, for ease of presentation, thatp=3). Furthermore, for simplicity, we assume that the object defined by the upper bound envelope and the lower bound point is a box in the criterion space, which implies that if there is no search guide point, i.e.,y=null,the object is a box in criterion space. (It may seem that assuming the object is a box is restrictive, but that is not the case; these situations provide meaningful insights in the workings of the algorithm.)In Fig. 2, there is no search guide point (y=null) and the nondominated point z(xn) is found inside the object. In this case, only one object will be added to the priority queue, namelyO(Y˜,y^,yL)withy^=z(xn).In Fig. 3, there is no search guide point, and the nondominated point z(xn) is found outside the object. In this case too, only one object will be added to the priority queue, namely(Y˜,y^,yL),buty^≠z(xn),because{z(xn)+Q≥p}⊆{y^+Q≥p}.In Fig. 4, there is a search guide point (y ≠ null) and the nondominated point z(xn) is found inside the object. In this case, two objects will be added to the priority queue, namelyO(Y˜∪{y′,y′′},null,y^L)andO(Y˜,y^,yL)withy^=y^L. Note that ify∈{y^L+Q≥p},then y is inO(Y˜∪{y′,y′′},null,y^L),otherwise it is inO(Y˜,y^,yL). Moreover, the reason that the algorithm uses two points y′ and y′′ to defineO(Y˜∪{y′,y′′},null,y^L)is that{z(xn)+Q≥p}⊆{y′+Q≥p}and{y+Q≥p}⊆{y′′+Q≥p}.In Fig. 5, there is a search guide point, but the nondominated point z(xn) is found outside the object. In this case, two objects will be added to the priority queue, namelyO(Y˜∪{y′,y′′},null,y^L)andO(Y˜,y^,yL),buty^≠y^L,because{y^L+Q≥p}⊆{y^+Q≥p}.We will now present a proposition which is essential for showing that Algorithm 1 decomposes the criterion space properly. Its proof is given in Appendix A. We use notation ∨ for the logical disjunction, ∧ for the logical conjunction, and, when ejforj=1,⋯,pis a logical expression,⋁jejfore1∨e2∨⋯∨ep.Proposition 10LetY⊆Qp,a,y∈(Q∪{−∞})pandg∈Qp. Define the vectorsy′,y′′,y^∈(Q∪{−∞})pbyyj′={gj,ifgj>yjandgj>aj,−∞,otherwise,yj′′={yj,ifyj>gjandyj>aj,−∞,otherwise,andy^j={min{yj,gj},ifmin{yj,gj}>aj,−∞,otherwise,for allj=1,⋯,p. Define setsA={z∈Y:⋁j(zj<yj)∧⋁j(zj<gj),z≥a},B={z∈Y:⋁j(zj<yj′)∧⋁j(zj<yj′′),zj≥max{aj,min{yj,gj}},∀j},andC={z∈Y:⋁j(zj<y^j),z≥a}.ThenA=B∪C.The next proposition shows that the decomposition scheme partitions an object, which implies the correctness of the algorithm.Proposition 11Let xn be the nondominated point found when exploringO(Y˜,y,yL)and letO(Y˜∪{y′,y′′},null,y^L)andO(Y˜,y^,yL)be the objects created by the algorithm. ThenO(Y˜,y,yL)∖{z(xn)+Q≥p}=O(Y˜∪{y′,y′′},null,y^L)∪O(Y˜,y^,yL).Take g ≔ z(xn), a ≔ yLandY:={x∈Qp:⋁j(zj<wj),∀w∈Y˜}.Then observe thatO(Y˜,y,yL)∖{z(xn)+Q≥p}={z∈Y:⋁j(zj<yj)∧⋁j(zj<gj),z≥a}.Also, note thaty^jL=max{yjL,min{yj,zj(xn)}}=max{aj,min{yj,gj}},∀j,soO(Y˜∪{y′,y′′},null,y^L)={z∈Y:⋁j(zj<yj′)∧⋁j(zj<yj′′),zj≥max{aj,min{yj,gj}},∀j}.Furthermore, for each j, by its definition,yj′=zj(xn)=gjif and only ifzj(xn)=gj>yjandzj(xn)=gj>y^jL,(andyj′=−∞otherwise). But if gj> yj, thengj>y^jLif and only if gj> aj. So it must be thatyj′={gj,gj>yjandgj>aj−∞,otherwise∀j.Similarly, for each j, by its definition,yj′′=yjif and only ifzj(xn)=gj<yjandy^jL<yj,(andyj′′=−∞otherwise). But if gj< yj, thenyj>y^jLif and only if yj> aj. So it must be thatyj′′={yj,yj>gjandyj>aj−∞,otherwise∀j.Finally, observe thatO(Y˜,y^,yL)={z∈Y:⋁j(zj<y^j),z≥a}and thaty^j={y^jLify^jL>yjL,−∞otherwise.={min{yj,gj}ifmin{yj,gj}>aj,−∞otherwise.,∀j.We may thus apply Proposition 10 to obtain the required result.□If the number of nondominated points is finite, then Algorithm1explores a finite number of objects.Algorithm 1, after finding a nondominated point z, removes the portion of the criterion space dominated by z, including z itself, and adds at most two new objects to the priority queue. Consequently, the objects created and explored by Algorithm 1 can be represented in a binary tree, in which any path from the root to a leaf node cannot have more than|YN|+1nodes, because along such a path, a nondominated point can only be found once (and a leaf node contains no nondominated point, i.e., it is infeasible). Therefore, Algorithm 1 explores at most2|YN|+1−1objects.□It is worth mentioning that a more careful analysis reveals that Algorithm 1 must do better than2|YN|+1−1. Specifically, in a worst case scenario, the number of objects explored is bounded above byϕ|YN|+3−1,whereϕ=1+52(the golden ratio). There are two reasons that this is true:1.if after exploring an object, Algorithm 1 adds two new objects to the priority queue, then there is no search guide point in one of them, andafter exploring an object without a search guide point, Algorithm 1 adds at most one object to the priority queue.Thus, Algorithm 1 creates a Fibonacci search tree and not a binary search tree, i.e., the number of nodes in each level of the search tree corresponds to the Fibonacci numbers.To enhance the performance of the algorithm, we try to avoid callingFind-NDP-Objby recognizing situations in which we can determine the nondominated point it will find beforehand. Specifically, we perform the following steps before callingFind-NDP-Obj.•Step 1 (searching for a feasible solution): We search LEfor a feasible solution. If we find one, then we skip solvingFind-NDP-Obj(Y˜,y),because the solution will be efficient, otherwise we go to Step 2. To make scanning for a feasible solution as efficient as possible, we sort the efficient solutions in nondecreasing order of their objectives’ values. (We first sort them based on their first objective value, then, in case of ties, we sort them based on their second objective value, and so on.)Step 2 (determining infeasibility): We maintain a list of infeasible objects, denoted by LI, i.e., objectsO(Y˜,y,yL)for which callingFind-NDP-Obj(Y˜,y)has returned null.Suppose the algorithm is exploring objectO(Y˜={y1,…,yt},y,yL)and LIcontains an objectO(Y′˜={y′1,…,y′s},y′,y′L)such thatS(Y˜,y)⊆S(Y˜′,y′),whereS(Y˜,y):={za∈Qp:zja≤(yji−ϵ−Mj)bij+Mj∀yi∈{1,⋯,t}∀j∈θ(yi),∑j∈θ(yi)bij=1∀i∈{1,⋯,t},zja≤(yj−ϵ−Mj)bj′+Mj∀j∈θ(y),∑j∈θ(y)bj′=1,bij∈{0,1}∀i∈{1,⋯,t},∀j∈θ(yi),bj′∈{0,1}∀j∈θ(y)},andS(Y˜′,y′):={za∈Qp:zja≤(yj′i−ϵ−Mj)bij+Mj∀i∈{1,⋯,s}∀j∈θ(y′i),∑j∈θ(y′i)bij=1∀i∈{1,⋯,s},zja≤(yj′−ϵ−Mj)bj′+Mj∀j∈θ(y′),∑j∈θ(y′)bj′=1,bij∈{0,1}∀i∈{1,⋯,s},∀j∈θ(y′i),bj′∈{0,1}∀j∈θ(y′)},thenFind-NDP-Obj(Y˜,y)will return null, and we do not have to callFind-NDP-Obj(Y˜,y). Checking whether or notS(Y˜,y)⊆S(Y˜′,y′)can be done efficiently as follows1.Let B(bl, bu) and B(b′l, b′u) be the minimum hypercubes that containO(Y˜,y,yL)andO(Y˜′,y′,y′L),respectively. Clearly, ifS(Y˜,y)⊆S(Y˜′,y′)then bu≤ b′u. Therefore, bu> b′uimmediately implies thatS(Y˜,y)¬⊆S(Y˜′,y′);LetW:=Y˜∪yandW′:=Y˜′∪y′,thenS(Y˜,y)⊆S(Y˜′,y′)if and only if for each w′ ∈ W′ there exists a w ∈ W such that w ≤ w′.Note that it is not difficult to modify Algorithm 1 so that it solves at most2|YN|+1IPs for any MOIP with p objective functions. To do so, we make sure that each nondominated point is found exactly once during the course of the algorithm. This can be done by checking, after finding a nondominated point z(xn), if z(xn) is feasible forFind-NDP-Obj(Y˜,y)for all objects in the priority queue, and, if so, adding z(xn) toY˜. With this modification, exactly|YN|IPs have to be solved to find all nondominated points. Furthermore, because after finding a nondominated point, the number of objects in the priority queue increases by at most one, we have that after finding all nondominated points, there are at most|YN|+1objects in the priority queue. Therefore, at most|YN|+1infeasible IPs have to be solved before the algorithm terminates. Thus, at most2|YN|+1IPs are solved in total. However, this modification increases the complexity of the single-objective IPs that need to be solved, because more disjunctive constraints and binary variables are added. This is exactly what we want to avoid to ensure that we have a practically efficient algorithm. Our computational experiments suggest that the number of IPs solved by the algorithm as presented is at mostp|YN|+1.In this section, we start with a review of Jorge’s method (Jorge, 2009). We then show how the algorithm presented in the previous section can be modified to optimize a linear function over the set of efficient solutions. Finally, we show how these methods can be used to compute the nadir point.Jorge’s algorithm (Jorge, 2009) maintains a list of nondominated points, denoted withY˜N. At the start of the algorithmY˜N=∅and the bounds on the optimal value of the linear function over the set of efficient solutions are set asfl=−∞andfu=∞. (Due to our assumption thatXLPis compact, in our implementation, we usefu=maxx∈XLPf(x)andfl=minx∈XLPf(x).) Let 0 < ϵ1 and 0 ≤ ϵ2 be two small constants. The algorithm terminates when the relative gap between fuand flis less than or equal to ϵ2, i.e., assuming f(x) ≥ 0 for allx∈X,the algorithm terminates whenfu−flfu+ϵ1≤ϵ2(note that the inclusion of ϵ1 ensures that the denominator is always positive), or whenFind-LB(Y˜N)returns null, i.e., the problem has become infeasible.In each iteration, the algorithm tries to improve the lower bound by callingFind-LB(Y˜N). If the problem has become infeasible, i.e., null is returned, the algorithm terminates. Otherwise, the lower bound flis updated, i.e.,fl=f(xl)and the search for an efficient solution xnwith z(xn) ≤ z(xl) continues by callingFind-NDP(z(xl)). Ifz(xn)=z(xl)then xlis efficient by definition, and the upper bound is updated, i.e.,xf=xl. However, if z(xn) ≠ z(xl), thenFind-Best(xn)is called to find an efficient solution xfwithz(xf)=z(xn)and with minimum value of f(x); the corresponding nondominated point z(xf) is added toY˜N. If f(xf) is better than the best upper bound found so far, i.e., fu> f(xf), then the upper bound is updated, i.e.,fu=f(xf)andxu=xf.A precise description of Jorge’s algorithm (JA) can be found in Algorithm 2. Because commercial IP solvers can effectively exploit known feasible solutions to speed up the solution of an IP, in our implementation of JA, we provide xlas a feasible solution to the IP solved byFind-NDP(z(xl)). Similarly, we provide xnas a feasible solution to the IP solved byFind-Best(xn).The algorithm presented in Section 4 is easily modified to optimize a linear function over the set of efficient solutions. For each object, we compute a lower bound on the optimal value of (2) over efficient solutions with an image in the object and its corresponding solution, i.e.,folandxol,and store these with the objects. Furthermore, the objects in the priority queue are maintained in nondecreasing order of the values of the associated lower boundsfol.At the start of the algorithm, we compute an upper and lower bound on the optimal value of (2), i.e.,fu=maxx∈XLPf(x)andfl=f(xol)withxol=Find-LB-Obj(∅,null,null). Ifxoldoes not exist, then the algorithm terminates, because the problem is infeasible. The priority queue is initialized with(O(∅,null,null),(xol,f(xol))).In each iteration, the algorithm pops an objectO(Y˜,y,yL)and its associated lower bound(xol,fol)from the priority queue. Obviously, if the priority queue is empty, the algorithm terminates. Otherwise, the object has the minimum lower bound flamong the objects in the priority queue, so the algorithm sets fltofol,and checks the relative gap between fuand fl. Iffu−flfu+ϵ1≤ϵ2,then the algorithm terminates. Otherwise, the algorithm callsFind-NDP(z(xol))to find an efficient solution xn. Ifz(xn)=z(xol),i.e.,xol∈XE,then xfis set toxol. Otherwise, the algorithm callsFind-Best(xn)to find an efficient solution xfsuch thatz(xf)=z(xn)and has a minimum value of f(x). If f(xf) < fu, then the global upper bound is updated, i.e.,xu=xfandfu=f(xf).If fl≥ futhen xuis optimal. If fl< fu, we decompose the object into one or two smaller objects, in a way that is similar to what was described above. We first computey^L,y′,y′′,y^∈Qp. Ify′≠(−∞,⋯,∞)andy′′≠(−∞,⋯,∞),then we callFind-LB-Obj(Y˜∪{y′,y′′},null,y^L)to compute a lower boundxol,and ifxol≠nullandf(xol)<fu,we add(O(Y˜∪{y′,y′′},null,y^L),(xol,f(xol)))to the priority queue. Next, ify^≠(−∞,⋯,∞),then we callFind-LB-Obj(Y˜,y^,yL)to compute a lower boundxol,and ifxol≠nullandf(xol)<fu,we add(O(Y˜,y^,yL),(xol,f(xol))to the priority queue.A precise description can be found in Algorithm 3. To remove any redundant calculations, we maintain a list of the solutions that resulted in improved upper bounds during the execution of the algorithm, i.e., a list with solutions xf. Before callingFind-NDP(z(xol)),we search the list to see if it contains a solution xfthat is feasible for the IP solved byFind-NDP(z(xol)). If successful, then that solution must be optimal for bothFind-NDP(z(xol))andFind-Best(xn),because xfis an efficient solution. If unsuccessful, we providexolas a feasible solution to the IP solved byFind-NDP(z(xol)). Similarly, we provide xnas a feasible solution to the IP solved byFind-Best(xn).Proposition 13If the number of nondominated points is finite, Algorithm3is finite and finds an optimal solution to(2).Algorithm 3 is built on top of Algorithm 1, which enumerates all nondominated points. Consequently, Algorithm 3, in the worst-case, examines all nondominated points and is, thus, guaranteed to find an optimal solution to (2). When the number of nondominated points is finite, Algorithm 1 is finite (Proposition 12), and, thus, Algorithm 3 is finite as well.□To compute the jth component of the nadir point of a MOIP, one can simply optimize the linear functionf(x)=−zj(x)over the set of efficient solutions of the MOIP. However, a slightly different approach is more efficient. Ehrgott and Tenfelde-Podehl (2003) observe that the jth component of the nadir point of a MOIP can also be computed as follows. First, remove zj(x) from the set of objective functions and compute the nondominated frontier of the resulting(p−1)-dimensional MOIP. Second, translate back each of the nondominated points to the p-dimensional criterion space (this can be done by solving a single IP for each of the obtained nondominated points in the(p−1)-dimensional criterion space minimizing zj(x)). Third, find the maximum value of zj(x) over the translated points.Note that the problem solved in(p−1)-dimensional space is not exactly that of optimizing a linear function over the efficient set, but it is close:maxx∈XE{minzj(x)}. This problem can be solved using our proposed algorithm by making two modifications (related to finding an efficient solution xfwith minimum value of f(x)):•The linear function f(x) that is minimized in the IP inFind-Best(xn)is set to zj(x); andEven if, for an efficient solution xn, we havez(xol)=z(xn),Find-Best(xn)is called.Similar changes can be made to Jorge’s algorithm.To improve the performance of the method even further, we also propose to keep a list of upper bounds obtained during the search, and use this list to initialize fuin the search for each component of the nadir point.We have performed a computational study to assess the performance of the new algorithm for solving MOIPs with an arbitrary number of objective functions, and to assess the efficacy of the modified algorithm that can optimize a linear function over the efficient solution of a MOIP.Our algorithms as well as Jorge’s algorithm have been implemented in C++ and use CPLEX 12.6 as the integer programming solver. Interested readers can find the sources of our algorithms at http://hdl.handle.net/1959.13/1062187. All computational experiments have been carried out on a Dell PowerEdge R710 with dual hex core 3.06 gigahertz Intel Xeon X5675 processors and 96 gigabyte RAM, with the RedHat Enterprise Linux 6 operating system, using a single thread.The goal of our research is to develop a new and effective algorithm for optimizing a linear function over the set of efficient solutions of a MOIP. However, in order to do so, we have developed a new method for enumerating all nondominated points of a MOIP (with some desirable characteristics). Therefore, even though it is not our main focus, we start analyzing the performance of Algorithm 1.Three sets of instances, taken from Özlen, Burton, and MacRae (2013), are used to assess the performance of the new algorithm for solving a MOIP: one set of instances of the triobjective 3-dimensional knapsack problem (3DKP), one set of instances of the triobjective assignment problem (AP ), and one set of instance of the triobjective traveling salesman problem (TSP). Note that these instances have been used in several other studies, e.g., Boland et al. (2015).Detailed performance statistics of the proposed algorithm and the L-shape search method (LSM) of Boland et al. (2015) can be found in Table 1, where the number nondominated points (#NDP), the number of single-objective IPs solved (#IP), the time taken to generate the nondominated frontier (Time (seconds)), the maximum number of disjunctive sets added to any single-objective IP solved during the execution of the algorithm (#MaxDS), and the maximum number of big-M constraints to any single-objective IP solved during the execution of the algorithm (#MaxMC) added to a model are reported. Where appropriate, the number of IPs that were infeasible is shown in parentheses. We note that LSM is one of fastest algorithms for generating the nondominated frontier of a triobjective integer program.We observe that the proposed algorithm is slower than LSM, even though it solves a smaller number of IPs. This is because the algorithm solves more infeasible IPs and adds more sets of disjunctive constraints to the IPs being solved (LSM adds at most one set of disjunctive constraints to the IPs being solved). However, we also see that the maximum number of disjunctive sets added to an IP by the proposed algorithm is very small, which was one of the main goals.We assess the performance of the proposed algorithm for optimizing a linear function over the efficient set of a MOIP on well-known publicly available instances of triobjective integer programs and on randomly generated instances of integer programs with more than three objectives.We investigate the performance of the proposed algorithm for optimizing a linear function over the efficient set of a MOIP by comparing it to the performance of JA on the triobjective instances introduced in the previous section. We impose a runtime limit of 5 hours for the experiments in this section.We first perform experiments with the linear functionf(x)=−∑i=13zia,which we expect to be challenging for both algorithms, because the MOIP seeks to minimizeziafori=1,2,3,but the linear function seeks to maximize∑i=13zia. Detailed performance statistics for the proposed algorithm as well as for JA with this linear function can be found in Table 2. The number of nondominated points computed in the process of finding the optimal solution is given in columns with heading #FNDP. Columns with heading %FNDP give the fraction#FNDP×100#NDP. Finally, the total number of objects created by the proposed algorithm is given in columns with heading#Objects. Where appropriate the number of objects explored is given in parentheses.The results show that both algorithms only generate about 15 percent (on average) of all nondominated points before finding an efficient point that optimizes the linear function. The results also demonstrate that the proposed algorithm is much more efficient than JA. (JA was unable to solve most AP instances and the largest KP instance within the time limit.) Comparing the runtime of the proposed algorithm (Table 2) with the runtime of LSM to generate all nondominated points (Table 1) reveals that optimizing the linear function over the set of efficient points is faster for KP and TSP instances, but slower for AP instances. (Note that after generating all nondominated points by LSM, the minimum value of f(x) needs to be computed for each, but this can be done by solving a single IP for each nondominated point, and the cost of doing only increases the runtime by about 5 percent). The likely reason for the difference in performance is the existence of many feasible solutions in the AP instances. This results in slow improvement of the lower bound in each iteration of the proposed algorithm. This can be seen in Table 3where we show the relative optimality gap of the proposed algorithm in the first 50 iterations. The results in the table clearly highlight the difference between AP instances and KP and TSP instances.Next, we perform a similar experiment, but this time with a random linear function. To generate a random function for an instance, we compute the minimum and maximum coefficients among all objective functions, denoted by cland cu, randomly choose 10 percent of variables and set their coefficients in the linear function to zero, and draw the coefficients of the other variables randomly from the discrete uniform distribution [cl, cu]. (Note that since we set 10 percent of the variables to zero, it is unlikely that f(x) is a linear combination of the objective functions.) The results of the experiment can be found in Table 4. We see, as before, that the proposed algorithm is significantly faster than JA on all instances, but that its performance on AP instances is still poor (compared to LSM). However, the runtimes for the 3DKP and TSP instances have improved by a factor of 3.25 compared to the case with linear functionf(x)=−∑i=13zi(x). A more in-depth analysis reveals that the runtime of the proposed algorithm depends highly on operationFind-LB-Obj. This can be seen clearly in Fig. 6, where the ratio of time spent inFind-LB-Objto the total runtime of the proposed algorithm is given forf(x)=−∑i=1pziaand for f(x) a random linear function. On average, more than 80 percent of the runtime is spent inFind-LB-Obj. The results in Fig. 6 suggest that withf(x)=−∑i=1pzia,the single-objective IPs associated with operationFind-LB-Objare harder to solve than with random linear function. One reason for that can be the number of sets of disjunctive constraints added. On average, whenf(x)=−∑i=1pzia,the number of sets of disjunctive constraints added is 3.58, 3.09, and 2.67 for AP, 3DKP, and TSP instances, respectively. However, with a random linear function, these numbers are 3.13, 2.95, and 2.10, respectively.Next, we focus on computing the nadir point. Kirlik and Sayın (2014) and Köksalan and Lokman (2014) compare the performance of their algorithms with Jorge’s algorithm when it computes the jth component of the nadir simply by settingf(x)=−zj(x)(and works in p-dimensional criterion space). They show that their algorithms are significantly faster than this basic version of Jorge’s algorithm for computing the nadir point (which is unable to find the nadir point within the imposed time limit for many instances), sometimes by a factor of more than 1000. In Table 5, we report results for the basic version of Jorge’s algorithm, the modified version of Jorge’s algorithm (which works in(p−1)-dimensional criterion space), and the modified version of our algorithm.We observe that the modified version of Jorge’s algorithm is much faster than its basic version. In fact, the nadir point cannot be computed within five hours (the time limit) for several instances with the basic version, whereas the nadir point can be computed for all instances with the modified version in less than an hour (much less in most cases). Furthermore, we see that our proposed algorithm is up to 6 times faster than the modified version of Jorge’s algorithm. Even for the largest instances, our proposed algorithm computes the nadir point in about 5 minutes. A proper comparison of the performance of our proposed algorithm with the methods of Kirlik and Sayın (2014) and Köksalan and Lokman (2014) is only possible when we are able to run all codes on the same instances and on the same computer. However, a rough comparison (based on published results) suggests that our proposed algorithm is somewhat slower, maybe by a factor of about 3. However, our proposed algorithm was designed to solve the more general problem of optimizing a linear function over the set of efficient point of a MOIP, whereas the methods of Kirlik and Sayın (2014) and Köksalan and Lokman (2014) are specifically designed to compute the nadir point of a MOIP.In order to further test the performanceof the proposed algorithm, a total of 480 random MOIP instances were generated. The generated instances are similar to those used by Jorge (2009), but we tried to make them harder by changing some of the parameters.More specifically, we assume that set of feasible solutionsXis given by Ax ≤ b andx∈Z≥0n. The sparsity of matrix A is set to 75 percent. The components of the vector b and the entries of the matrix A are drawn randomly from discrete uniform distributions [50, 200] and [1, 30], respectively. The objective functions coefficients, i.e., the components of vector ciwithzi(x)=cixfor alli=1,⋯,p,are drawn randomly from discrete uniform distribution[−10,0]. (Note that the components of vector ciare nonpositive, because the constraints of the problem are in less-than-or-equal form and the objective functions are to be minimized.) Moreover, the coefficients of the linear function, i.e., the components of vector cfwithf(x)=cfx,are drawn randomly from discrete uniform distribution[−10,5]. In Jorge (2009), the components of the vector b are drawn from [50, 100]. Consequently, our generated instances may have a larger feasible set. Moreover, in Jorge (2009), the vectors ciand cfare drawn randomly from[−10,10]. Therefore, our generated instances tend to be more difficult, because in our generated instances, on average, one third of the coefficients of the vector cfhave positive sign (i.e., the opposite of the sign of the same variable in vectors ci).The instances are divided into three classes based on the number of objective functions, each with 160 instances. The instances of class I, II, and III have 4, 5, and 6 objective functions, respectively. Each class has 16 subclasses, based on the dimensions of the matrix Am × n(20 × 25, 20 × 30, 20 × 35, 20 × 40, 40 × 50, 40 × 60, 40 × 70, 40 × 80, 80 × 100, 80 × 120, 80 × 140, 80 × 160, 100 × 125, 100 × 150, 100 × 175, and 100 × 200), and each subclass contains 10 instances.A runtime limit of 300 seconds was imposed for the solution of each instances. We find that both the proposed algorithm and Jorge’s algorithm are able to solve almost all instances in subclasses 20 × 25, 20 × 30, 20 × 35, and 20 × 40, and cannot solve most instances in the other subclasses (in which case they terminate with a feasible, but not necessarily optimal solution). More specifically, the proposed algorithm solved 193 instances and Jorge’s algorithm solved 159 instances.Detailed results for the algorithms on instances in subclasses with 20 constraints can be found in Table 6. In this table, values represent averages over 10 instances. Column %Imp(JA) shows the percentage of improvement of the proposed algorithm over JA in terms of runtime. Even though JA solves fewer IPs, and identifies fewer NDPs, its runtime is significantly higher than the proposed algorithm.A runtime performance profile of the two algorithms on the instances in subclasses with 20 constraints which are solved to optimality within the time limit by both algorithms is given in Fig. 7 (per class). A performance profile (Dolan & Moré, 2002) presents cumulative distribution functions for a set of algorithms being compared with respect to a specific performance metric. The runtime performance profile for a set of algorithms is constructed by computing for each algorithm and for each instance the ratio of the runtime of the algorithm on the instance and the minimum of the runtimes of all algorithms on the instance. The run time performance profile then shows the ratios on the horizontal axis and, on the vertical axis, for each algorithm, the fraction of instances with a ratio that is greater than or equal to the ratio on the horizontal axis. This implies that values in the upper left-hand corner of the graph indicate the best performance. The runtime performance profile demonstrates that the proposed algorithm outperforms Jorge’s algorithm. In fact, the proposed algorithm is more than 2.6 times faster in more than 40 percent of instances.A performance profile of the relative optimality gap over all instances is given in Fig. 8(per class). We have added 0.1 to the optimality gap of all instances in order to be able to handle instances which are solved to optimality by one or both of the algorithms and give an optimality gap of zero. We observe that the proposed algorithm achieves a better optimality gap within the runtime limit of 300 seconds.

@&#CONCLUSIONS@&#
