@&#MAIN-TITLE@&#
Comparing the acoustic expression of emotion in the speaking and the singing voice

@&#HIGHLIGHTS@&#
Acoustic analyses of emotional singing show significant differences between 10 emotions.Loudness, tempo, spectral balance and perturbation differentiate emotions most.Patterns for singing are very similar to those found for actors’ speech portrayals.Singers tend to rely more than actors on the use of voice perturbation, specifically vibrato.

@&#KEYPHRASES@&#
Vocal expression,Emotional interpretation in singing,Comparison between emotion expression in speech and singing,Acoustic analyses of emotion,

@&#ABSTRACT@&#
We examine the similarities and differences in the expression of emotion in the singing and the speaking voice. Three internationally renowned opera singers produced “vocalises” (using a schwa vowel) and short nonsense phrases in different interpretations for 10 emotions. Acoustic analyses of emotional expression in the singing samples show significant differences between the emotions. In addition to the obvious effects of loudness and tempo, spectral balance and perturbation make significant contributions (high effect sizes) to this differentiation. A comparison of the emotion-specific patterns produced by the singers in this study with published data for professional actors portraying different emotions in speech generally show a very high degree of similarity. However, singers tend to rely more than actors on the use of voice perturbation, specifically vibrato, in particular in the case of high arousal emotions. It is suggested that this may be due to by the restrictions and constraints imposed by the musical structure.

@&#INTRODUCTION@&#
Affect bursts (short, spontaneous, nonverbal expressions of emotion in voice, face, and body) can be considered as “living fossils” of early human affect expression that may have served as precursors of parallel evolution of speech, song, music and dance. Affect expression is likely to have played a special role because (1) innate mechanisms for spontaneous expression in face, voice, and body may have been the earliest communicative mechanisms in place, rapidly followed for control structures for learned behaviors, (2) both affect bursts and controlled vocalizations are widely shared across many species, and (3) the production mechanisms, at least for spontaneous expressions, are located in the subcortical regions of the mammalian brain (see Scherer, 2013a,b). This assumption is compatible with other suggestions concerning the origin of speech and music (such as the role of gestures or phonetic symbolism) and does not address the issue of their respective evolutionary priority (in fact, the common production substrate does not disambiguate between different evolutionary scenarios). The parallelism of human emotion expression in speech and music has been demonstrated by a comprehensive review of empirical studies on patterns of acoustic parameters in these two forms of human affect communication (Juslin and Laukka, 2003). The assumption of powerful “affect primitives” in speech and language is also supported by research on the recognition of emotion in speech (Bryant and Barrett, 2008; Laukka et al., 2013b; Pell et al., 2009; Sauter et al., 2010; Scherer et al., 2001) and music (Laukka et al., 2013a). This research has generally shown the existence of both a fairly high degree of universality of the underlying expression and recognition mechanisms and of sizeable differences between cultures, especially for self-reflective, social, and moral emotions.The emotional power of the speaking voice, taken for granted ever since the ancient works of rhetoric (Cicero, Quintilian), has been frequently studied in empirical research (Scherer, 1986, 2003). The emotional power of the singing voice, although frequently acknowledged (Scherer, 1995; Sundberg, 1989), has only rarely been studied in an experimental fashion. Yet, in performing vocal music in Western music traditions (liturgical works, opera, and different kinds of song), professional singers have to be able to produce an extraordinary range of emotional meanings. A wide variety of means help to achieve such interpretation, such as gestures and facial expression, but the central instrument to evoke the subtle shadings of emotion is the human voice. In fact, singers are often judged in terms of their ability to produce the emotional modulation of their vocal performance that is considered appropriate to nature of the emotion to be portrayed (as implied by the text of a song or the libretto of an opera). In addition, like actors in spoken theater, they are expected to express the required emotions in a credible, authentic fashion, giving the impression of “inhabiting” the emotional feelings of the character they are performing (Scherer, 2013a).Previous studies that have been devoted to the understanding of the emotional power of the singing voice tried to identify the acoustic cues used by listeners to recognize the emotional meaning in singing voice. A standard method for this purpose has been to correlate acoustic profiles of different emotions portrayed by professional singers with the listeners’ judgments of emotions perceived (Kotlyar and Morozov, 1976) as well as with the level of expressiveness (Sundberg et al., 1995) or strength of the perceived emotion (Jansens et al., 1997). Results have agreed in that performances characterized by higher arousal levels (as joy and anger) tend to show higher average sound pressure levels and fast tempi than performances characterized by lower arousal levels (as sadness). Also a clear association between anger and the presence of vibrato, and sadness and the absence of vibrato has been found (Jansens et al., 1997). Another method used to investigate how the emotional meaning is conveyed in singing voice has been to compare recordings of a same song performed by various singers and analyze listeners’ judgments of emotions perceived for each particular performer. Siegwart and Scherer (1995) and Howes et al. (2004) found that listeners’ preferences and emotion judgments were indeed associated with specific acoustics characteristics. Correlations between different acoustic parameters and listeners’ perception of emotions in singing voice (as well as in music in general) can also be studied by investigating the listeners’ emotion judgments of sounds that had each of different parameters systematically and independently manipulated (Scherer and Oshinsky, 1977; Kotlyar and Morozov, 1976). Procedures of synthesis and resynthesizes have also been used to systematically manipulate acoustics parameters, in order to investigate the effects and relevance of each of the parameters for listeners emotion judgment (e.g., Goto et al., 2012; Fonseca, 2011; Kenmochi and Ohshita, 2007; Risset, 1991; Sundberg, 1978). A comparison between acoustic patterns that characterizes both expressive speech and expressive singing suggests a striking parallel between the expression of emotions in the speaking and the singing voice between. For instance, both in speech and singing anger is associated with high F0 variability (assuming that F0 variability in speech is translated into vibrato extent in singing) and with high vocal intensity, while sadness is associated with slow speech rate (and low tempo) as well as with low vocal intensity.In consequence, we expect that emotion expression is similar in speaking and singing voice – because of the evolutionary origin of the expression mechanisms and the need for authenticity (Maynard Smith and Harper, 2003; Mortillaro et al., 2013). Obviously, there may well be important differences across languages and cultures due in large part to language characteristics such as phonemic structure or intonation rules. Yet, given the stability of findings across music and speech (Juslin and Laukka, 2003), one can expect similarities across studies in different languages and cultures between the expression of emotion in speech and singing.Unfortunately, this issue not well researched. Compared to the study of facial expression of emotion, research on vocal expression is relatively rare, particularly with respect to the comparison between languages and cultures (see a recent review by Scherer et al., 2011). To the best of our knowledge, there have been no systematic, empirical attempts to compare the acoustic patterns of vocal emotion expression in speech and singing. This article is one of the first attempts to throw some light on the hypothesis, based on evolutionary considerations (see above), that the expression of emotion in speech and singing have evolved in parallel and thus share many features. The approach chosen here is to compare the results of an acoustic analysis of emotion portrayals in neutral phrases sung by three professional opera singers with evidence from recent studies on the vocal expression of emotion (Goudbeek and Scherer, 2010; Patel et al., 2011; Sundberg et al., 2011) in order to examine the plausibility of the hypothesis of isomorphism.When studying vocal expression through acoustic parameters it is important to distinguish pure vocalization such as affect bursts or interjections and regular speech-like utterances, because of formant structure of vowels, intonation and other factors. This is similar for “vocalises” and text based singing. In consequence, we examine nonsense text and /a/ vocalizations in both speech utterances and sung phrases. To examine the isomorphism hypothesis, we will report the results of the acoustic analysis of the phrases and vocalises sung by the three opera singers using ANOVAs to test for emotion and vocal production differences as well as post hoc comparisons of emotion groups and compare the results systematically to the published results from the Goudbeek and Scherer (2010) and Patel et al. (2011), Sundberg et al. (2011) studies using profile correlations (see also Goudbeek and Scherer, 2010, Table 2, for comparison between two studies using this approach).

@&#CONCLUSIONS@&#
The results of the acoustic analyses are very clear-cut: there are significant main effects of the enacted emotions on all of the parameters measured and for both types of sung material, reaching very respectable, in some cases extremely strong, effect sizes. This can be interpreted as signifying that the singers, despite the absence of emotionally meaningful lyrics, are able to modify their voice quality and the dynamic aspects of the musical rendering for expressive purposes. In addition, we found some significant main effects for the type of sung material. Thus, the presence of different vowels in the nonsense phrases seem to have affected the relative strength of the lower partials (which are, due to the formant structure), stronger in the case of the /aa/ vowel used for the vocalises. In addition, probably because of the presence of vowel–consonant transitions in the phrases, there was a larger amount of phonation irregularity for the phrases (see Fig. 1). Importantly, we found only one Emotion×Material interaction for autocorrelation (the phrase/vocalise differences being particularly prominent for anger and panic fear).We had selected a number of acoustic parameters that are frequently employed in studies of vocal expression. As in earlier studies, we found several of these parameters highly intercorrelated because they are assessing similar dimensions in slightly different measurement operations. A principal components analyses revealed a factor structure that is quite similar to the ones found earlier (Goudbeek and Scherer, 2010; Patel et al., 2011). Because of this stability over different studies, speakers/singers, and materials, we decided to combine some of the parameters to composite scales – dominance of low partials, spectral balance, and perturbation (keeping loudness level and tempo as separate variables) to simplify the interpretation of the data. As these composite scales, particularly the first two, were strongly correlated with overall loudness level (SIL), we decided to partial out the latter superfactor and work with residual scores to obtain the specific effect of the respective dimension without the effect of loudness.The ANOVAs showed significant emotion effects for the composite scales spectral balance and perturbation but not for low partials dominance. The effects for the individual parameters averaged for the latter composite scale had in fact been weaker in the individual analyses, resulting in their common denominator not allowing to strongly differentiate the different emotions. Thus, despite the overall concordance of findings for both types of singing material, the vowel /aa/ may be more appropriate in future studies because fixed formant structure allows a higher degree of replicability, compared to varying formant frequencies in material with greater variety of vowels.In addition to the overall ANOVAs we computed post hoc comparison to determine the specific effects of particular parameters on specific emotions. The results show a clear differentiation between some emotions at the extreme poles of the respective dimension with less clear-cut distinctions (with respect to differential levels of the parameter) for emotions situated in the mid range. We replicate the frequently reported finding that anger has relatively strong energy in the higher frequency range (higher spectral balance). Whereas low arousal emotions (like sadness) show little waveform irregularity, have a lower loudness level, and are sung with a slower tempo, perturbations, loudness, and tempo are increased for high arousal emotions like anger and fear.While these results are interesting in their own right, especially as there is little in terms of comparable data in the literature, the main aim of this paper was to compare the findings with comparable data from actor-produced vocal emotion expressions in speech-like portrayals. In order to examine the degree of similarity between the two expression modalities, we ran profile correlations for the individual variables that had been measured in the respective studies over the shared set of emotions. In addition, we computed post hoc comparisons with homogeneous subgroups for the speech data in two different studies and compared the respective groups. Overall, we found a rather striking degree of agreement with somewhat of an exception for the perturbation dimension. It seems that singers make general more use of a specific type of phonatory irregularity in the form of vibrato (possibly because of the classic significance of this parameter in the domain of operatic singing). The difference is particularly pronounced in the case of anger. While singers use strong vibrato for high arousal emotions including anger, actors express anger generally with firm, regular phonation. Some actors tend to show perturbations in sadness (possibly tremor) whereas this is rarely the case for singers. One important factor involved in this dimension may be that singers, given the constraints of musical structure and composition, have fewer acoustic parameters to work with and thus privilege vibrato as a mechanism of choice.Despite these very encouraging results, we need to mention a number of limitations in the current study. It would clearly be advantageous to have a larger number of singers representing a wider range of tessitura. However, based on pilot studies with singing students, we decided that the approach chosen could only be used with excellent singers benefitting from extensive experience on major opera stages. Clearly, this population is extremely difficult to contact and it is even more difficult to obtain their collaboration on what is an arduous and time consuming task. Therefore, we privileged expertise and experience over numbers for this initial study. There is no doubt that our results require future replication with a larger set of different voice types but numbers are always likely to be fairly small. Furthermore, from the standpoint of acoustic analysis, the duration of approx. 8s for a voice sample is too short to expect a high degree of spectral stabilization, especially for the parameters based on the long-term spectrum. At the same time, it is difficult to see how this limitation can be overcome given the ecological constraints – emotions are short-lived episodes and strong expressions in voice and face often occur only at the apex of emotional arousal. Also, for reasons of comparison with the published results on vocal emotion expression in speech, a similar format for the enacted expressions needed to be adopted.As a general conclusion we can retain that overall there are many similarities in vocal emotion expression across speech and singing, confirming theorizing that suggests a parallel evolution of speech and music from primitive affect bursts, sharing similar codes for the signaling of affect (Scherer, 1991, 2013b). It is to be hoped that future work will extend this type of inquiry to the physiological underpinnings of phonation and articulation in speech and singing in emotion expression, leading to a better understanding of the utility of different parameters by linking parameters directly to underlying production mechanisms. Another avenue for further development, especially when data for larger groups of singers and actors become available, is the use of machine learning to assess the degree of generalizability of the underlying acoustic profiles. A recent study by Weninger et al. (2013) shows that algorithms trained on emotional music are quite successful on emotional speech and vice versa, suggesting that this may be a very viable approach. Weniger et al. show that the effect generalizes, to some extent, even to environmental sounds. This suggests that the similarity of emotion coding in the audio domain may be quite robust and that, inconsequence, the results we obtained with recordings under studio conditions, would generalize, at least in large part, to real life recordings of many different kinds of music. Obviously, it is to be expected that noise and degraded acoustic conditions (low bandwidth channel, background noise, reverberation) as well as genre-specific vocal effects (e.g., belting), may affect the degree of generalizability.Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.csl.2013.10.002.The following are the supplementary data to this article: