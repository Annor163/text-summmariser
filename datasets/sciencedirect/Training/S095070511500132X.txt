@&#MAIN-TITLE@&#
Unsupervised video categorization based on multivariate information bottleneck method

@&#HIGHLIGHTS@&#
Novel multivariate IB model is proposed for unsupervised video categorization.Effective solution is designed to integrate multiple features simultaneously.Information-theoretic optimization is constructed to alleviate the semantic gap.

@&#KEYPHRASES@&#
Video categorization,Unsupervised learning,Multivariate information bottleneck,Multiple features,Mutual information,

@&#ABSTRACT@&#
The integration of multiple features is important for action categorization and object recognition in videos, because single feature based representation hardly captures imaging variations and individual attributes. In this paper, a novel formulation named Multivariate video Information Bottleneck (MvIB) is defined. It is an extensional type of multivariate information bottleneck and can discover categories from a collection of unlabeled videos automatically. Differing from the original multivariate information bottleneck, the novel approach extracts the video categories from multiple features simultaneously, such as local static and dynamic feature, each type of feature is treated as a relevant variable. Specifically, by preserving the relevant information with respect to these feature variables maximally, the MvIB method is able to integrate various aspects of semantic information into the final video partitioning results, and thus captures the complementary information resided in multiple feature variables. Extensive experimental results on five challenging video data sets show that the proposed approach can consistently and significantly outperform other state-of-the-art unsupervised learning methods.

@&#INTRODUCTION@&#
With the continuing rapid growth of personal video recordings, online video data and broadcast news, unsupervised action categorization and object recognition in video clips have been an active and challenging research area. However, there are two key issues in the task of discovering categories automatically from a collection of unlabeled videos: (1) Because of the cluttered background, camera motion, occlusion, changes of view point and variances of the geometric distribution in videos, robust feature representation extraction remains a difficult problem. Moreover, single feature based representation can hardly capture imaging variations and individual attributes. (2) Automatical method for differentiating videos is also a quite challenging task due to the lack of the ground-truth label information.For the first key issue, robust feature representation should be achieved before discovering categories from a collection of unlabeled videos. Recently, researchers have found that the quality of feature representation is of great importance to action categorization and object recognition in videos. Therefore, large amounts of feature extraction techniques were proposed. The well-known feature representations include static based on edges and limbs [1–3], shape or form features [4,5]; interest point based representation [6,7]; motion or optical flow patterns [8,9]; nontensor product wavelet filter banks [10]. However, the capability of single feature is not enough to capture discriminative information, which will make the representations include prejudices caused by single feature. Besides, most of feature extraction approaches described above only consider single aspect of information for the task of video action classification. For instance, certain actions, such as hand clapping, produce a small number of dynamic features since most body parts remain static. While some other actions, such as cycling and horseback riding, are similar in motion features. Therefore, it is difficult to distinguish action categories in the task of unsupervised video categorization only based on single feature. So we strongly feel that various kinds of features are mutually complementary for video action categorization.A reliable mechanism to learn the action and object categories based on the visual features is the second key issue associated with unsupervised category discovery in videos. However, most current learning approaches are supervised methods, which need the ground-truth label information. Labeling the videos manually is a labor intensive and time consuming process, which often invites subject biases or mistakes by human labelers. So some research efforts have been dedicated to the task of unsupervised object category discovery, such as k-means, PLSA [11], LDA [12], AP [13], SC [14]. These existing unsupervised methods try to learn the object categories from the visual contents in a two-step manner: (1) Building an affinity matrix to reflect the video relations based on visual features. (2) Partitioning the videos into different groups by considering the affinity values. This assumption always limits the performance of aforementioned methods due to the semantic gap between the visual features and their high-level semantic concepts. Besides, most unsupervised approaches used in the domain of video categorization can only cope with single feature.In this paper, a novel unsupervised video categorization method called Multivariate video Information Bottleneck (MvIB) is proposed, which can partition video clips from multiple cues. Like original multivariate information bottleneck (multivariate IB) model, the novel clustering approach treats pattern structure extraction as data compression. In the video categorization procedure, the proposed method conserves the relevant information from multiple feature cues rather than only one source of feature information. Besides, a information-theoretic optimization is adopted to learn the latent semantic correlations between the videos and their constructive visual words automatically, which can relieve the semantic gap between the visual features and their high-level semantic concepts.The major contributions of this study are summarized as follows:•A novel and effective multivariate information bottleneck model is proposed, which extends the original multivariate IB to the task of unsupervised video category discovery.The MvIB method can incorporate multiple information cues into the clustering process, which provides an effective solution to integrate multiple features simultaneously.An effective information-theoretic optimization method is designed to learn the latent semantic correlations between the videos and their low-level visual features, which alleviates the semantic gap in the current unsupervised learning techniques.The rest of this paper is organized as follows. In Section 2, the related work is introduced. In Section 3, the basic knowledge is presented about multivariate IB method. In Section 4, details of the MvIB approach are described. In Section 5, extensive experimental results are presented to demonstrate the performance of MvIB. Finally, conclusions are given in Section 6.

@&#CONCLUSIONS@&#
In this paper, we proposed a novel formulation named Multivariate video Information Bottleneck (MvIB) method for unsupervised video categorization, which is an extensional type of original multivariate IB. Differing from the original multivariate information bottleneck, the MvIB method extracts the video categories from multiple feature variables simultaneously, while source variable X is compressed to compression variable T. Therefore, the compression results capture the complementary information resided in multiple feature variables. Besides, the information-theoretic optimization is adopted to learn the latent semantic correlations between the videos and their constructive visual words automatically, which can relieve the semantic gap between the visual features and their high-level semantic concepts. The experiments on five challenging benchmark video data sets confirm the effectiveness of the MvIB algorithm. In the future, we will try to incorporate more features into the MvIB approach and apply it to more difficult tasks such as action and scene recognition on large-scale data.Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.knosys.2015.03.028.