@&#MAIN-TITLE@&#
Reconstruction of 3D surface maps from anterior segment optical coherence tomography images using graph theory and genetic algorithms

@&#HIGHLIGHTS@&#
A method to produce 3D maps of the anterior segment is presented.Graph theory is used to segment 2D OCT images.Genetic algorithms are used to align 2D images to a 3D map.Good repeatability is achieved using a data set covering 17 eyes.

@&#KEYPHRASES@&#
Optical coherence tomography (OCT),Anterior segment,Cornea,Dynamic programming (DP),Graph cut and genetic algorithms (GA),

@&#ABSTRACT@&#
Automatic segmentation of anterior segment optical coherence tomography images provides an important tool to aid management of ocular diseases. Previous studies have mainly focused on 2D segmentation of these images. A novel technique capable of producing 3D maps of the anterior segment is presented here. This method uses graph theory and dynamic programming with shape constraint to segment the anterior and posterior surfaces in individual 2D images. Genetic algorithms are then used to align 2D images to produce a full 3D representation of the anterior segment. In order to validate the results of the 2D segmentation comparison is made to manual segmentation over a set of 39 images. For the 3D reconstruction a data set of 17 eyes is used. These have each been imaged twice so a repeatability measurement can be made. Good agreement was found with manual segmentation for the 2D segmentation method achieving a Dice similarity coefficient of 0.96, which is comparable to the inter-observer agreement. Good repeatability of results was demonstrated with the 3D registration method. A mean difference of 1.77pixel was found between the anterior surfaces found from repeated scans of the same eye.

@&#INTRODUCTION@&#
Optical coherence tomography (OCT) is capable of producing high resolution images of the human eye. These images are extensively used for diagnostics looking at the posterior segment of the eye [1]. OCT images of the anterior segment have also been used in several medical applications including contact lens fitting, ectasia diagnosis, surgical planning and monitoring of patients with eye pathologies [2–5]. One of the challenges faced is the automated segmentation of anterior segment images.Commercially available OCT devices have different inbuilt segmentation methods. Currently only CASIA (Tomey, Tokyo, Japan) is able to both image and automatically segment the entire anterior segment [6]. However, we do not know the proprietary details of the segmentation approach used. The use of proprietary software only coming with the device will also limit the access for many applications. Other commercially available devices are either limited to only imaging a small section of the cornea or are unable to automatically segment the entire anterior segment like the Visante OCT system (Zeiss Meditec Inc., Dublin, California) [7]. Previous segmentation techniques have used a variety of techniques to achieve results but have mainly focused on 2D segmentation only [8–12].2D segmentation techniques of the anterior segment have included those that use threshold methods [9], level set based techniques [11], graph cut techniques [12] and dynamic programming [8]. La Rocca et al. used graph theory and dynamic programming for the segmentation of spectral domain OCT (SD OCT) images [8]. These images only included the central region of the cornea. The method used the image gradient to identify edges on the images; Dijkstra's algorithm [13] was then used to find the shortest path through the image based on the image gradient. Three boundaries on the cornea to be segmented; their results were in good agreement with manual observers. The central corneal region covered in this work had the highest signal to noise ratio in the image, and regions further away from the center were approximated by employing polynomial curves to extend the anterior and posterior surfaces. However, extending the surfaces in this form could affect the method's reliability in estimating the tomography of the peripheral cornea.Notable efforts at 3D segmentation of cornea images include a study by Robles et al. on images of mice corneas [14]. Their technique was adapted from the graph cut techniques that had been developed for retinal OCT images. There was no discussion of issues of inter-image misalignment in their paper and their validation only considered the 2D segmentation step. Another group has carried out construction of a 3D surface using segmentation of SD OCT comprising multiple 2D images of the cornea [15]. This group used peak detection and a generalized Hough transform to segment 2D images. The images used included both perpendicular and parallel sets with equal spacing in the two directions. The group then minimized the distance between the 2D cornea edges at the cross over points of the images to align the images and create a 3D reconstruction of the cornea. The technique was demonstrated on only 3 image sets from 3 healthy subjects and the results were not validated.More work has been carried out on the registration of posterior segment OCT images to produce 3D maps of the retina. A commonly used method is to exploit blood vessels which cross the retina and can be seen on multiple 2D cross sections of the images [16,17]. The cornea and retina have a different structure. The cornea is avascular and healthy corneas do not contain any defining features other than their uniform shape. This means that techniques developed for retinal OCT images cannot be easily transferred to images for use on the cornea.In this paper we propose a novel method for segmenting time domain OCT (TD OCT) images of the anterior segment. TD OCT produces lower resolution images compared to SD OCT but is currently the only widely used OCT technique to cover the entire anterior segment. An example 2D image to be segmented is show in Fig. 1. In order to generate a 3D image of the cornea, 16 2D OCT images of the cornea were acquired. These were taken in a radial manner so all were expected to pass through the cornea's center. These images can individually be segmented on a 2D basis using the method described here. The challenge in segmenting these 2D images is the lower signal to noise regions that make the posterior surface hard to detect. We propose to use a shape term based on our successful detection of the anterior surface to overcome this problem, this is the first time segmentation using a shape term and graph theory segmentation has been used on anterior segment images. The challenge in producing reliable 3D segmentation is mainly caused by difficulties in alignment of 2D images. The possible involuntary eye movement could cause misalignment between 2D scans as illustrated in Fig. 2, and make the resulting 3D surfaces unreliable. We propose to use genetic algorithms to solve this problem. The main advantage of GAs is their ability to globally minimize a function that may contain many local minima. They have been used in other image registration contexts with significant success [18–20].The images for this study were all acquired from ocular healthy patients. For the 3D study artificial tears were instilled in eyes in order to obtain good quality images.The remainder of the paper is structured as follows. Section 2 will present a novel 2D segmentation which uses graph theory and a shape constraint term to segment the images. Section 3 presents a novel approach to aligning segmented 2D images to produce a 3D map of the anterior segment. Section 4 presents the validation methods used and the sources of images used in the study. Section 5 presents the results of the validation analysis. Section 6 discusses the results and conclusions.Our 2D segmentation algorithm consists of five major steps. Two surfaces are detected: the boundary between the corneal epithelium and air which we define the anterior surface and the boundary between the stroma and the corneal endothelium which we define as the posterior surface. First a pre-processing step is carried out to locate the iris. An initial segmentation of the anterior surface is carried out with the iris removed from the image. This segmentation is used to create a shape function. Final segmentation of the anterior and posterior surfaces is then carried out. Fig. 3illustrates these major steps. Each step will be discussed in more detail. Fig. 4shows the progress after each of the described steps.Anterior segment OCT images contain both the iris and the cornea. These are both structures that go across the image so our segmentation must distinguish between them. The first step in our segmentation program is therefore to identify the region that contains the iris to prevent it being incorrectly identified as the cornea. This is done by checking the mean intensity of the horizontal rows in the image. The iris will correspond to the row with be the maximum intensity since it is a flat horizontal structure. Once the iris is identified that region is removed from the image for the initial segmentation. In Fig. 4 the image centre left shows the image after this step.The remaining part of the image is then segmented using graph theory. Each pixel of the image is regarded as a node of a graph which represents the image. Now the segmentation problem becomes to find the shortest path from the left to the right of the image. More specifically, the following energy term is used to define the weighting of each node on the image.(1)C1(x,z)=w1×∂I(x,z)∂z+w2×∂I(x,z)∂x,where w1 and w2 are weighting coefficients,∂I(x,z)/∂zis the vertical gradient of the image at point (x,z) and∂I(x,z)/∂xis the horizontal gradient of the image. The gradient was calculated using the Matlab gradient function which calculates the gradient using a central difference approximation. The reason for using a combination of the two gradients is the curvature of the cornea. In the central region the vertical gradient is at correct orientation to find the boundary. For the horizontal gradient the weighting of the central quarter of the image was set to zero. As the cornea curves towards the edges of the image the cornea edge is orientated towards the corners of the image so using both horizontal and vertical gradient is important. w1 was set to 1 across the image and w2 was set to zero in the central region and 1 elsewhere. One potential variation would be to alter the values of w1 and w2 using an estimation of the shape of the cornea. Results of this idea were not included since tests varying these values found that the results of the segmentation were not strongly related to the values of the weighting coefficients. In Fig. 4 the image bottom left corresponds to the result of this step.When the iris is removed, parts of the sclera are also removed with it. In order to segment the full width of the image the segmentation is repeated over the original full image. The result of the previous segmentation is used to guide the segmentation to avoid false detection of the iris. This gives a new cost function of(2)C2(x,z)=w1×∂I(x,z)∂z+w2×∂I(x,z)∂x+w3×ϕa(x,z),where ϕais a distance function each point corresponds to the distance to the line detected in the previous step. This encourages the line to follow the path of the previous segmentation but allows it to extend into regions that were not part of the previous image. The three weighting terms were kept equal in this step, w1=1, w2=1, and w3=1. The image top right in Fig. 4 shows the segmentation at this stage.The next step of the segmentation is to detect the posterior surface. This is harder to detect than the anterior surface since the edge is not as well defined on this boundary. In order to aid detection of this boundary two strategies were introduced, one was to flatten the cornea using the anterior surface and the other is to use a shape guiding term constructed from the anterior surface. Flattening the cornea in the image aids segmentation since graph theory segmentation favors straight lines over curved lines in segmentation. This is done by shifting each column in the image so the anterior surface previously detected becomes a straight line on the image. The flattened image is shown middle right in Fig. 4.To calculate the shape term the central corneal width is calculated using an established technique where peaks in the image are found [21]. The corneal thickness on either side of the center increases. A quadratic term was used to make an estimate of the thickness at each point along the cornea. The estimated thickness was given by(3)ct(x)=ctcentral+c1x−xt2+c2x−xt,where ct(x) is the estimated corneal thickness at point x, ctcentral is the central corneal thickness, xtis the location of the centre of the cornea and c1, c2 are coefficients that determine the strength of the quadratic terms. The strength of these coefficients was determined by empirical testing. The values used where c1=0.00033 and c2=0.0077. This enabled the generation of an estimated location of the posterior surface. A distance function was generated and used to guide the segmentation of the posterior surface.The cost function for the posterior surface is given by(4)C3(x,z)=w1×−∂I(x,z)∂z+w2×−∂If(x,z)∂z+w3×ϕp(x,z),wherew1,w2,w3are weighting functions to determine the strength of the different terms,∂I(x,z)/∂zis the gradient in vertical direction and∂If(x,z)/∂zis the gradient in the vertical direction after a median filter was applied to the image. This filter was used to reduce to effect of noise on the segmentation; the shape of the filter was a vertical line of 5pixel. ϕp(x, z) is the shape term determined from the anterior surface through the method discussed above. The negative gradient is assumed as the posterior surface is a boundary from light to dark unlike the anterior surface. Horizontal gradient is no longer used since the cornea has been flattened. The strength of the weighting functions used was w1=1, w2=1, and w3=0.5.Fig. 4 shows the progress of the segmentation following conduct of the steps discussed above.The determination of the weighting coefficients for the different stages was carried out using a leave one out technique. A range of values was tested for each weighting term. For each of the 39 images the values of the coefficients that gave the best results for segmentation over the other 38 images was used. It was found that the same values were found to work best for all the different images. The values used for the coefficients are stated in each of the sections.After determining the weighting function C1,2,3(x, z) the shortest path for each function was found using the dynamic programming technique. This works by calculating the cumulative cost function for each point using the following expression(5)t(x,z)=∞z<1,z>mC(x,z)x=1p=z−2:z+2mint(x−1,z−3)+C(x−1,z−1)+C(x,z),t(x−1,p)+C(x,z),t(x−1,z+3)+C(x−1,z+1)+C(x,z)otherwise,where t(x, z) is the cost to reach point (x, z), C(x, z) is the weight function giving the cost at node (x, z), x is the x direction index, z is the z direction index and m is the image height. The cost function used depends on which surface is being found. The shortest paths for C1,2,3(x, z) defined in previous equations are all found using this method. This minimization method controls how steep a line can be found in the image. Each pixel can be connected to one of closest 7pixel from the previous column. If it is one of the 5 closest only the values at the previous point are taken into account. If there has been a move of 3pixel up or down then an additional cost is added from a point in the same column. The choice of an additional point from the previous column was made since the boundary is considered to pass through this point. This allows for steep lines to still be detected while preventing too much movement in the vertical direction without a cost penalty occurring. Only one point is found for each point on the x axis. Fig. 5displays the results of this segmentation on an image also containing manual segmentation results.In order to generate a 3D surface from the 2D segmentation results a suitable method for alignment must be developed since errors may develop in 2D image position and orientation due to possible eye motion and incorrect camera positioning. Fig. 6shows an example image of the anterior surface generated when our alignment process is not used. The need for this registration step means that direct 3D segmentation is not practical.In the 2D segmentation we have identified the anterior and posterior surfaces as well as an estimate of the location of the iris. In order to carry out a course alignment of 2D images, 3 points are identified on each image; the apex of the cornea and both junctions of the iris and cornea. The apex of the cornea can be identified by finding the point with the minimum z coordinate on the anterior surface, where z is the vertical axis on the image with the zero point at the top of the image. The two junction points can be identified as where the approximate iris location crosses the posterior surface, Fig. 7. Once these three points are identified on all the images they can be used to align the images.The vertical distance from the apex of the cornea to the line connecting the points at the edge of the iris is used as a measure of the anterior chamber depth, which should remain constant or vary within very tight limits in 2D images taken for the same eye. Images that are not aligned with the center of the cornea will likely have a smaller depth. A linear relation between the anterior chamber depth and the distance between the image and the cornea's center was assumed due to the expected small values of the misalignment distances. A quadratic relation was tested but this was found to be less accurate than using the linear method. This is probably due to the errors in the calculation of the chamber depth which would be magnified when using a quadratic relation compared to a linear one. Further, a vertical displacement was implemented to each image based on the mean height of the two iris points on each image. These heights should be the same since the iris is a relatively flat structure, and hence any variations are assumed to result from the eye's rigid motion in the anterior-posterior direction. Finally any displacement in the horizontal plane of the image is eliminated by aligning the apex points on each image so they are all have the same x position.The results of the preliminary alignment were reasonable but still contained errors in alignment, which made it necessary to carry out a further step. It was decided to use genetic algorithms (GA) since it is a technique that is able to effectively minimise a function with many local minima. The results obtained when only the preliminary alignment was used are included in the analysis for comparison. GA is a technique with a trial and improvement basis, which can be used to minimize an energy function [22]. With GA, a series of parameters are randomly initialized within a range. These solutions are ranked using a fitness function. New solutions are generated by combining the best solutions of each step as well as adding in a random element. This technique is good for finding a global minimum for demanding applications, where more conventional minimization techniques may end up finding only local minima. The challenge in using GA is the selection of a suitable energy function.Zernike polynomials have been previously used to model the surface of the cornea [23], defining it by the following expression(6)C(r,θ)=∑p=1PapZp(r,θ)+ε,where C(r, θ) is a mathematical representation of the corneal surface, p is the polynomial ordering index, Zp(r, θ) the pth order Zernike polynomial with associated coefficient apand ε is the experimental and modeling error.A model cornea was created by fitting Zernike polynomials up to the fifth order to a surface generated from the preliminary step. This process created a smooth surface in the expected shape of the cornea. The mean distance between the surface generated from the images and this model surface was used as the energy function, which took the form:(7)E=∑i=1:Nabszisurface−zizernike,wherezisurfaceis a point on the test surface,zizernikeis a point with same x and y coordinates on the polynomial surface and N is the total number of points on the test surface. The points on the test surface were all points on the individual slices no smoothing or fitting was applied at this stage. GA was used to minimize this energy function over 64 parameters corresponding each of the 16 2D images being free to move in all 3 directions and rotate in the image plane.Points on the same B scan were not allowed to move relative to each other. This means all points on each of these scans were always moved the same amount.The Matlab implementation of the GA was used with a total population of 100, of which 4 are elite, 80 are crossover offspring and 16 are mutation offspring. Mutation offspring are produced by randomly generating a direction and step length that satisfies the boundary condition. Crossover offspring are created by taking a weighted average of the parents. The process was run until it showed no improvement over a period of 100 generations.In order to improve the results the preliminary alignment discussed above was used as a constraint on the range of the parameters. The range of possible displacements and rotations for each image was limited to values that were close to the result of the preliminary alignment. The displacement in the x and y directions was limited to 20pixel in either direction and a maximum rotation of 10 degrees was allowed. If much wider limits were used then this would reduce the effect of the preliminary alignment on the final result. Increasing these values would reduce the effect of the preliminary step, results are presented for alignment both with and without this preliminary step for comparison. Once the registration of the images was completed a 5th order Zernike polynomial was fit to the results in order to generate a surface including points not on the original slices. This was done separately to the anterior and posterior surfaces.A systematic approach was adopted to evaluate the performance of the proposed segmentation technique.The automatic 2D segmentation technique was compared to the output of manual observers using three measures. The first measure is Dice Similarity Coefficient (DSC) which is a measure of what proportion of the segmentation is shared between the two images [24]. DSC is defined by the following equation.(8)DSC=2P∩QP+Qwhere P and Q are the two segmentations to be compared, in this case the manual and automated segmentation results. DSC has a range between 0 and 1. The higher the DSC value, the more similar the two segmented regions are.The second measure is the unsigned mean surface position error (MSPE) which is a measure of the distance between the boundaries from two different segmentation techniques at each point across the image.Thirdly, the 95% Hausdorff distance which is a measure of the maximum distance between two matching points in the results of two different segmentation techniques [25]. The Hausdorff distance from set A to set B is defined as(9)HD(A,B)=maxa∈Aminb∈B|a−b|,where A and B are sets of boundary points from the two images to be compared. The 5% largest distances were removed then the maximum of HD(A,B) and HD(B,A) was determined for each image [26]. This step was performed to remove the effect of a small number of badly located points from the analysis. Hausdorff distance is the measure of the misalignment of the worst point. Perfect alignment is represented by a Hausdorff distance of 0.Three different automatic 2D segmentation methods were compared in the study, including our novel method, a previously published level set method [11] and the graph theory method without a shape term (which is equivalent to a previously published method [8]). Inter and intra observer differences were also compared. Our method without shape term differs from the method used by La Rocca et al. in the preprocessing steps used. The preprocessing steps used by that group were implemented but it was found the preprocessing steps used here worked better on our data set.The validation of the 3D technique was carried out by comparing the results of segmentation of two different sets of images of the same eye. The mean difference between the anterior surfaces generated was used as a measure of similarity. This distance was calculated between the surfaces after the Zernike polynomial had been fitted to generate smooth surfaces.For the 2D segmentation work, 39 anterior segment OCT images were taken through the center of the cornea from healthy eyes (one per subject). The study was approved by the institutional review board and undertaken while following the tenets of the Helsinki Declaration. The images were acquired by the Visante AS-OCT system at the Wenzhou Medical University, China. The Visante system is a time domain system that uses 1300nm infrared light to obtain cross-sectional images of the anterior segment with a scanning rate of 2000 axial scans per second. Each B scan image contains 256 A-scans in 16mm with 1024 points per A scan to a depth of 8mm. The images have a transverse resolution of 60μm and an axial resolution of 18μm. These images were manually segmented by two expert ophthalmologists (FB and MS) marking on the locations of the anterior and posterior boundaries.In validating the 3D segmentation method, images of 17 eyes were acquired using the Visante AS-OCT system. These were taken at the Western University of Health Sciences, California, United States. A total of 17 healthy eyes were imaged for this study. 5 eyes were from volunteers who had only one eye imaged and 12 eyes were from volunteers who had images taken of both eyes. For each eye, two 3D images were recorded. Two sets of images were taken per eye with a short period of rest to enable a repeatability test to be carried out of our 3D alignment technique. Alignment was carried out using three techniques, namely the initial identification of landmarks step, using GA without the initial step and using both GA and the initial step. These techniques were compared to assess their relative effectiveness.All work was carried out using a Win7 PC with Intel Core i5-2320 CPU @3.00GHz and 4.00GB RAM. Statistics calculations were made using SPSS version 20 (IBM, Armonk, NY, USA).

@&#CONCLUSIONS@&#
