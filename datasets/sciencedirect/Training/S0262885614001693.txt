@&#MAIN-TITLE@&#
How to use Bag-of-Words model better for image classification

@&#HIGHLIGHTS@&#
We empirically study coding and pooling under a large range of vocabulary sizes.We provide detailed application guidelines to use BoW in practical applications.Combined with average pooling, most coding methods are comparable.The best performance of max pooling is better than the one of average pooling.A saturant point exists in maximum pooling.

@&#KEYPHRASES@&#
Image classification,Bag-of-Words,Visual vocabulary,Coding,Pooling,

@&#ABSTRACT@&#
The Bag-of-Words (BoW) framework is well-known in image classification. In the framework, there are two essential steps: 1) coding, which encodes local features by a visual vocabulary, and 2) pooling, which pools over the response of all features into image representation. Many coding and pooling methods are proposed, and how to apply them better in different conditions has become a practical problem. In this paper, to better use BoW in different applications, we study the relation between many typical coding methods and two popular pooling methods. Specifically, complete combinations of coding and pooling are evaluated based on an extremely large range of vocabulary sizes (16 to 1M) on five primary and popular datasets. Three typical ones are 15 Scenes, Caltech 101 and PASCAL VOC 2007, while the other two large-scale ones are Caltech 256 and ImageNet. Based on the systematic evaluation, some interesting conclusions are drawn. Some conclusions are the extensions of previous viewpoints, while some are different but important to understand BoW model. Based on these conclusions, we provide detailed application criterions by evaluating coding and pooling based on precision, efficiency and memory requirements in different applications. We hope that this study can be helpful to evaluate different coding and pooling methods, the conclusions can be beneficial to better understand BoW, and the application criterions can be valuable to use BoW better in different applications.

@&#INTRODUCTION@&#
Image classification is a fundamental problem in computer vision. It plays a key role in many applications such as image analysis and visual surveillance. In recent years, the Bag-of-Words (BoW) model has been widely used on many popular datasets and competitions, e.g., 15 Scenes [1], Caltech 101 [2], Caltech 256 [3], PASCAL VOC [4] and ImageNet [5]. In BoW, local features are first extracted to construct image representation, which is then fed into a classifier, as shown in Fig. 1. Specifically, the representation is an essential part, which includes two steps:Coding:Coding means that local features are encoded by a vocabulary and the response of the feature on the vocabulary is generated. The probabilistic strategies [6–9] describe the distribution of local features, while sparse coding methods [10–15] better reconstruct the features. Recently, superior performance has been obtained by some high-dimensional coding methods [16–19].Pooling transforms the response of all local features on a vocabulary into image representation, which is fed into a classifier. Average pooling [6] and maximum pooling [10] are widely used. Recently, weighted average pooling [17] and local pooling [20] have shown better results.Although many coding and pooling methods have been proposed, there are limited guidelines about how to use them in different applications [21–24]. Boureau et al. [21,22] analyze theoretically how coding and pooling are related based on sample cardinality (the number of local features) under small vocabulary sizes; Chatfield et al. [23] and Huang et al. [24] evaluate typical coding methods under relatively larger vocabulary sizes, but without considering different pooling schemes. Besides, all studies do not evaluate coding and pooling on large-scale datasets for generalization, such as the ImageNet database [5]. Different from the previous studies, in this paper, we consider four aspects:•To provide systematical user guidelines, the complete combinations of more popular coding methods [15,14] and two popular pooling methods (average, maximum) under an extremely large range of vocabulary sizes (16 to 1M) are considered. The maximum vocabulary size (1M) is 1000 and 40 times larger than 1024 in [21,22] and 25k in [23] respectively.Given the fact that large-scale image classification has become much more active in recent years [25–27], we consider two large-scale datasets, namely Caltech 256 [3] and ImageNet [5]. Furthermore, combined with three typical ones including 15 Scenes, Caltech 101 and PASCAL VOC 2007, the evaluation on these primary datasets can provide strong support and generalization for the conclusions and guidelines.Based on experimental results, the relation between coding and pooling is analyzed from different regimes of classification performance. In different regimes, the combinations of coding and pooling have different influence on the classification performance. Besides, these conclusions and guidelines are validated on various vocabulary construction methods for their strong generalization.To use the BoW model conveniently in practical applications, we provide detailed application criterions by selecting the appropriate pairs of coding and pooling methods. These criterions are given based on precision, efficiency and memory requirements, and we summarize these three factors as guidelines for some typical applications.There are three contributions in this paper:•Systematic evaluation. In this paper, complete combinations of more coding and pooling methods, an extremely large range of vocabulary sizes, primarily typical and large-scale datasets constitute a systematic evaluation. This evaluation compares many coding and pooling methods on primary datasets, and it is convenient to use appropriate methods.Interesting conclusions. In this paper, we draw some conclusions about coding and pooling. Some of them are different from the previous viewpoints [21–23], while some have never been found before but have shown importance in better applying the BoW model in practice. Particularly, extremely large sizes and large-scale datasets are important to draw these conclusions and improve the generalization ability.Application criterions. In this paper, based on the conclusions, the detailed application criterions of the BoW model are provided based on precision, efficiency and memory requirements. These criterions can be helpful for researchers and industry community to use appropriate coding and pooling methods in different applications.The rest of this paper is organized as follows. Section 2 first introduces the related work on coding and pooling. Then, detailed experimental setups are presented in Section 3, and conclusions are drawn in Section 4. Besides, application criterions are provided in Section 5. Finally, Section 6 gives conclusive remarks.

@&#CONCLUSIONS@&#
