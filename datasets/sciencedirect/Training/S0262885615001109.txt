@&#MAIN-TITLE@&#
Accurate abandoned and removed object classification using hierarchical finite state machine

@&#HIGHLIGHTS@&#
We propose a novel and accurate ARO classification method.We propose a hierarchical FSM consisting of pixel-, region-, and event-layers.State transition is done by the pre-trained SVM using 7 different input features.The proposed ARO method shows higher classification and low false alarm.The proposed ARO method can be applied to many practical applications.

@&#KEYPHRASES@&#
Support vector machine,Hierarchical finite state machine,Pixel classification,Region classification,Event classification,

@&#ABSTRACT@&#
The ability of most existing approaches to classify abandoned and removed objects (AROs) in images is affected by external environmental conditions such as illumination and traffic volume because the approaches use several pre-defined threshold values and generate many falsely-classified static regions. To reduce these effects, we propose an accurate ARO classification method using a hierarchical finite state machine (FSM) that consists of pixel-layer, region-layer, and event-layer FSMs, where the result of the lower-layer FSM is used as the input of the higher-layer FSM. Each FSM is defined by a Mealy state machine with three states and several state transitions, where a support vector machine (SVM) determines the state transition based on the current state and input features such as area, intensity, motion, shape, time duration, color and edge. Because it uses the hierarchical FSM (H-FSM) structure with features that are optimally trained by SVM classifiers, the proposed ARO classification method does not require threshold values and guarantees better classification accuracy under severe environmental changes. In experiments, the proposed ARO classification method provided much higher classification accuracy and lower false alarm rate than the state-of-the-art methods in both public databases and a commercial database. The proposed ARO classification method can be applied to many practical applications such as detection of littering, illegal parking, theft, and camouflaged soldiers.

@&#INTRODUCTION@&#
Identification of abandoned and removed objects (AROs) in videos is used to identify illegal activities. An abandoned object is a new object which has been brought into a scene and remains there for more than a given time; a removed object is an object that has disappeared from the scene, so that its former location remains empty for more than a given time [1]. Classification of AROs has been intensively studied since the PETS 200611http://www.cvg.rdg.ac.uk/PETS2006/.and AVSS 200722http://www.eecs.qmul.ac.uk/andrea/avss2007.html.challenges.One of the most important problems in this research is to accurately determine static regions, which means both of abandoned object and removed object, because these are often falsely classified due to the existence of temporarily-stationary objects and to changes in illumination, or both [2]. Most existing ARO classification methods use either intensity or motion cues and many pre-defined threshold values to find static regions; these approaches result in poor classification accuracy and a large false alarm rate when traffic is heavy and the background is complex.Existing ARO classification methods can be divided into feature-based methods and the finite state machine (FSM)-based methods. Feature-based methods use the temporal pattern of intensity or motion to find static regions. Beynon [3], Miguel [4], and Hassan [5] used the mixture-of-Gaussian (MoG) background model to classify the foreground regions, computed an association cost function based on position, size, color and edge information to track the foreground region, and identified the foreground region with the smallest cost value as the foreground region in the next frame. They classified the static regions by selecting the foreground regions with no motion during a pre-defined time. Liao [6] and Chang [7] used the foreground mask sampling method to find static regions. They sub-sampled six frames evenly during the previous 30s and generated six foreground images by taking the frame differences between the sub-sampled frame and its preceding frame. They obtained the static regions by taking the intersection of the foreground regions over six foreground images. Bayona [8] combined the foreground mask sampling method with the traditional foreground classification method to reduce the number of falsely classified static regions. They obtained the foreground images by subtracting the background for 30s and used the morphological operation to integrate the foreground images for 900 frames. They used the same morphological operation to identify static regions by re-combining the integrated foreground image with the resultant image. Porikli [1] used two MoG background models (short-term and long-term background models) with different learning rates to extract two different types of foreground images. They obtained the static regions by selecting the foreground regions that were not extracted from the short-term background model but were extracted from the long-term background model. Tian [9] used three MoG background models, assuming that the second dominant Gaussian represented the static regions. If the weight of the second dominant Gaussian for a pixel was larger than a threshold value, then the pixel was defined as a static pixel and the static pixels were grouped using the connected component analysis (CCA) [10,11]. They also introduced a simple tracking method that used the overlap ratio between two foreground regions during consecutive frames to reduce the number of false static regions.However, heavy occlusion by other moving objects severely degrades the tracking accuracy of feature-based methods that use motion features, so these methods are ineffective in crowded environments, especially for objects that have similar color or texture. Although Ortego [12] proposed a multi-feature framework robust to crowded environments, most of feature-based methods that use intensity features are strongly dependent on many pre-defined threshold values, and therefore generate many falsely-classified static regions.The FSM-based method uses a FSM to find static regions. Fujiyoshi [13] presented a method that used the stability of color information to find the static regions. They classified each pixel as stationary or transient according to its color history, and used CCA to define the static region by clustering the stationary pixels. Their method could successfully identify overlapped static regions but generated many static region classification errors due to illumination changes and noise. Mathew [14] suggested a FSM with five states: creation, deletion, foreground Gaussian (FG), background Gaussian (BG) and background dominant Gaussian (BDG), to classify objects which have appeared and remained stationary in the scene. If a new object was present in the same location for a pre-defined time, its state was changed from FG to BG and from BG to BDG (static region) over time. Evangelio [15] proposed using a short-term and a long-term background model to classify foreground objects, and classified each pixel into either background, foreground, static or uncovered background states by using dual background models to combine foreground classification results. Their method generated static regions by grouping static pixels, which are foreground pixels that are obtained from only the long-term background model. Then they used the edge information of their boundary regions to classify static regions as abandoned or removed objects. Fan [16] used the FSM to define the life cycle of a temporally static region (TSR), i.e., a foreground region whose bounding box does not move over a pre-defined time. The overlap ratio with other foreground regions was used to classify the state of the TSR into visible, occluded, or healed states. If the maximum overlap ratio was less than the pre-defined threshold value, then the state of the TSR was changed from ‘occluded’ to ‘visible’, which represented a true static region. If the TSR with the visible state was absorbed into the background model, then the state was changed from ‘visible’ to ‘healed’.However, the existing FSM-based methods consider only pixel-level FSM, cluster the connected pixels to find the static region, and use intensity or edge-based features to classify a static region as an abandoned or a removed object; they do not consider higher-level FSMs such as region-layer or event-layer FSM systematically. Also, their ARO classification accuracies are largely dependent on the environmental conditions such as illumination, traffic and background complexity because their state transitions are based on many non-trained pre-defined threshold values.To solve these problems, we propose a novel ARO classification method that uses a hierarchical FSM that consists of pixel-layer, region-layer and event-layer FSMs, where the decision results from the lower layer FSM are fed forward into the inputs of the next higher layer FSM. We also propose using SVM classifiers to determine the state transitions of the pixel-layer, region-layer and event-layer FSMs, because SVM classifiers can be trained to determine the state transitions optimally from a large number of training samples. The trained SVM removes the burden of requiring users to set pre-defined threshold values experimentally, and reduces the effect of the pre-defined threshold values on the ARO classification accuracy. The proposed ARO classification method consists of three stages: pixel classification, region classification and event classification, performed sequentially by the pixel-layer FSM, the region-layer FSM, and the event-layer FSM, respectively, as shown in Fig. 1.The main contributions of this paper are summarized as follows. First, we propose the hierarchical structure of FSMs consisting of low-level, middle-level and high-level processing that describes the entire ARO classification very systematically and accurately. Second, we propose the state transitions using SVM classifiers instead of a lot of pre-defined threshold values, which make the proposed ARO classification method robust to the change of environmental conditions such as light, traffic volume, and background complexity. Third, we propose a number of new features that reduce false static regions and improve ARO classification performance greatly.This paper is organized as follows. Section 2 explains the structure of the proposed ARO classification method, the features and SVM classifiers for state transitions that are used in the pixel-layer, the region-layer and the event-layer FSMs, respectively. Section 3 summarizes how the proposed ARO classification method works. Section 4 presents extensive experimental results using the LVSN, Wallflower, PETS2006, AVSS2007, CAVIAR and ObjectVideo databases to evaluate the true classification accuracy and false classification ratio. Section 5 concludes the paper.We propose a novel ARO classification method based on the hierarchical FSM; all state transitions of each FSM layer are determined by SVM classifiers. The structure of the proposed ARO classification method is shown in Fig. 2. The pixel-layer FSM consists of three pixel states (background pixel b, foreground pixel f and static pixel s); its initial state is b. The region-layer FSM consists of three region states (background region B, foreground region F and static region S); its initial state is the region state (F or S) that is obtained from the pixel-layer FSM. The event-layer FSM consists of three event states (withholding event w, abandonment event a and removal event r); its initial state is w. Final states of this structure are a and r. Altogether, the three FSMs have 20 possible transitions (see Table 1), where each FSM uses a distinct set of features to determine its state: pixel-layer FSM uses intensity (Cd,Cp,Cs) and time duration (Tf), the region-layer FSM uses area (AF,AS), intensity (I), motion (M), shape (S) and time duration (TS), and the event-layer FSM uses color (DC) and edge (DE).The proposed ARO classification method works as follows. First, we prepare a background image and accept an input image. Second, we perform a pixel-layer processing on each pixel to determine its pixel state by the pixel-layer features and the pixel-layer FSM. Third, we obtain the background, foreground, and static regions using the connected component analysis and morphological operations. Fourth, we perform a region-layer processing on each region to determine its region state by the region-layer features and the region-layer FSM. Fifth, we perform an event-layer processing on each true static region to determine its event state by the event-layer features and the event-layer FSM. Finally, we update the background image by absorbing the classified ARO into the background. We continue the above ARO classification method on the next input image.In the pixel-layer processing, the pixel-layer FSM determines the state of each pixel by using the intensity and time duration features defined below.Among the many existing methods [17–23] for moving object detection under a fixed camera, we adopted three different intensity features Cd, Cpand Csto determine the status of each pixel; these denote distance-based confidence value, probability-based confidence value, and similarity-based confidence value, respectively [23]. The background of each pixel is independently modeled using a mixture of at most K Gaussian models during some initial frames. Let ηlbe the kth background Gaussian model with normalized weight wl, mean μl, and variance σl2, i.e. ηl~G(wl,μl,σl2), l=1,⋯,K. If an observation I(Xt) of pixel X at current time t has the dominant background Gaussian model ηk, k∈[1,K], the relevant model parameters are updated as(1)wk←1−αwk+α,(2)μk←1−αμk+αIXt,(3)σk2←1−ασk2+αIXt−μk2,where a is the learning rate.The confidence value Cd′(Xt) measures the distance between I(Xt) and the mean of the dominant background Gaussian model as(4)Cd′Xt=IXt−μk255∈0,1,where μkis the mean of the kth background Gaussian model at the pixel Xtand the confidence value has a real number between 0 and 1.The confidence value Cp′(Xt) measures the probability that the pixel belongs to the background as(5)Cp′Xt=∑∀kwkPηkPIXt|ηk∑∀kwkPηk∈0,1,where wkis the normalized weight of the k th background Gaussian model at pixel Xt, P(I(Xt)|ηk) is the Gaussian probability of the observation I(Xt) with respect to ηk, and P(ηk) is the probability that ηkbelongs to the background as(6)PI(Xt|ηk=12πσke−IXt−μk2/2σk2,(7)Pηk=11+e−awk/σk+b,where μkand σkare the mean and the standard deviation of the kth background Gaussian model at pixel Xtand a and b are the sigmoid parameters.The confidence value Cs′(Xt) measures the similarity between observation I(Xt) and the mean μkof the dominant background Gaussian model as(8)Cs′Xt=e−|IXt−μk|/20∈0,1.We define three different intensity features as the average value of confidence values within the neighbor region as(9)CdXt=1NXt∑∀X∈NXtCd′X∈0,1,(10)CpXt=1NXt∑∀X∈NXtCp′X∈0,1,(11)CsXt=1−1NXt∑∀X∈NXtCs′X∈0,1,where N(Xt) is the 8-neighborhood of the pixel Xtand |N(Xt)| is the neighborhood size.We compute the time duration Tf′(Xt) which defines the number of consecutive frames in which Xtis classified as a foreground pixel [24]. We count the number of consecutive frames in which each pixel belongs to the foreground as(12)Tf′Xt=Tf′Xt−1+δsXt=f,where Tf′(Xt) and Tf′(Xt−1) are the accumulated frame count of position X at frames t and (t−1) respectively, s(Xt) denotes the status of pixel X in frame t, s(Xt)∈{b,f,s}, and the δ function increments the frame count by 1 when s(Xt)=f. Whenever s(Xt)≠f, Tf′(Xt) returns to zero. Then, we define a normalized time duration(13)TfXt=Tf′Xt2×Tmin,where Tminis the minimum time duration that is needed to change from a foreground pixel to a static pixel.Generally, transition of a Mealy state machine from a current state to a next state occurs when a given condition (current state, input data) is satisfied. Because the given condition includes many threshold values, they affect the accuracy and robustness of ARO classification. Therefore, the FSM must be devised such that state transitions do not require comparison to threshold values. To meet this requirement, we use SVM classifiers to identify state transitions. For each different pixel state, the SVM is trained differently as detailed in the following paragraphs. In the background pixel state, the background SVM classifier SVMbdetermines one of two states (b, f) according to intensity features Cd, Cpand Cs. In the foreground pixel state, the foreground SVM classifier SVMfdetermines one of three states (b, f, s) according to Cd, Cp, Csand time duration feature Tf. In the static pixel state, the static SVM classifier SVMsdetermines one of two states (b, s) according to Cd, Cpand Cs.We collected a dataset of 10,000 frames including background, foreground, and static pixels from the personally collected database in real situations, which is disjoint with the benchmark databases for testing and randomly selected 10% of the data set to use to train each pixel-layer SVM as follows (see Table 2). SVMbwas trained using 1000 frames including background and foreground pixels, where each pixel is represented by three intensity features. SVMfwas trained using 1000 frames including background, foreground and static pixels, where each pixel is represented by three intensity features and one time duration feature, and the background and foreground pixels are assumed to have zero time duration. SVMswas trained using 1000 frames including background and static pixels, where each pixel is represented by three intensity features. Each SVM classifier in the pixel-layer FSM was trained using a radial basis kernel function (RBF) whose parameter set including a constant for error penalty and covariance matrix was optimized in preliminary trials.After each pixel is classified as either a foreground, background or static pixel, CCA is used to group all pixels with the same state into a region, and regions that consist of foreground or static pixels are treated as static region candidates. During region-layer processing, the region-layer FSM uses features such as area, intensity, motion, shape, and time duration to classify static region candidates as true or false static regions.We compute the areas of the foreground and static regions AF′(Rt) and AS′(Rt) by counting the number of the pixels within the foreground and static regions, respectively. Then, we define a normalized foreground and static region AF(Rt) and AS(Rt) as(14)AFRt=AF′Rt2×AF,min,(15)ASRt=AS′Rt2×AS,min,where AF,minand AS,minare the minimum areas required before a background (or foreground) region can be changed to a foreground (or static) region. In this paper, we set AF,minand AS,minto 150, which provides the best region classification accuracy.We consider two intensity features: intensity dissimilarity (ID) and intensity stability (IS). IDof region Rtmeasures the dissimilarity between the input image and the background image as(16)IDRt=1∑k=1KH¯k∑k=1KHk−H¯k2H¯k∈0,1,where K is the number of bins, H(k) is the kth bin value in the input intensity histogram andH¯kis the background intensity histogram of static region candidate Rt. IDof a static region candidate increases as the intensity difference between the input image and the background image increases; this correlation implies that high IDcorresponds to a high probability that a static region candidate is a true static region.ISof a static region candidate Rtmeasures the normalized variance of the intensity over a fixed number of frames as(17)ISRt=1ASRt×T∑∀Xt∈Rt∑s=0T−1IXt−s−I¯Xt2552∈0,1,where AS(Rt) is the area of the static region candidate Rt, T is the time duration, IS(Xt−s) is the intensity value of X in the region Rtat the (t−s) th frame and Ī(Xt) is the mean of the intensity values of Xtduring the T frames. In this paper, we set the value of T to 30 frames due to a trade-off between many false static regions by a small value of T and many missing static regions by a large value of T. Static region candidates with low ISvalues have a high probability of being true static regions. The two intensity features are concatenated into an intensity feature vector IT=[IDIS].We propose three motion features: degree of overlap (MO), degree of area change (MA) and degree of position change (MP) as bounding boxes that surround the static region candidates. In this paper, we manage the bounding boxes for each region in the following manner: (1) two bounding boxes overlap considerably in consecutive frames, the bounding box in the current frame replaces the bounding box in the previous frame, and (2) if the bounding box in the current frame is significantly displaced from the corresponding bounding box in the previous frame, we add a new bounding box in the current frame. Fig. 3illustrates how a static region candidate changes over time, where the gray and white regions in the bounding boxes represent the foreground region and the static region, respectively, and a static region (a) starts to appear, (b) is growing, (c) is fully grown and (d) is left alone and reaches steady state.MOof a static region candidate Rtat the tth frame measures the overlap ratio as(18)MORt=∑k=1KARt∩RtkARt∈0,1,where K is the number of foreground regions that intersect the static region candidate, A(Rt) is the bounding box area of the static region candidate and A(Rt∩Rtk) is the common bounding box area between the static region candidate and the kth intersecting foreground region. If a foreground region covers the entire static region candidate, MO=1 (see Fig. 3-(b)). If a foreground region partially overlaps the static region candidate, 0<MO<1 (see Fig. 3-(c)). If the static region candidate is not surrounded by a moving (foreground) object, MO=0 (see Fig. 3-(d)).MAof a static region candidate Rtin the tth frame measures the change of the bounding box area between two consecutive frames as(19)MARt=1−minARt,ARt−1maxARt,ARt−1∈0,1,where A(Rt) and A(Rt−1) are the area of the bounding box of the static region candidate in frames t and (t−1), respectively. If a static region candidate starts to appear, MA=1 (see Fig. 3-(a)). If a static region grows over consecutive frames, 0<MA<1 (see Fig. 3-(b)). If the area does not change in two consecutive frames, MA=0 (see Fig. 3-(c) and (d)).MPof static region candidate Rtin the tth frame measures the change of bounding box position between two consecutive frames as(20)MPRt=maxΔX¯maxWtWt−1ΔY¯maxHtHt−1∈0,1,whereΔX¯=|X¯t−X¯t−1|andΔY¯=|Y¯t−Y¯t−1|represent the displacement of the bounding box center positions between two consecutive frames along the X and Y axes, respectively, andX¯tY¯t, Wtand Htdenote the bounding box center position, width and height of the static region candidate in the tth frame, respectively. If a static region candidate starts to appear, MP=1 (see Fig. 3-(a)). If some displacement occurs along the X or Y axes between two consecutive frames, 0<MP<1 (see Fig. 3-(b)). If no displacement occurs along the X or Y axes between two consecutive frames, MP=0 (see Fig. 3-(c) and (d)).We consider both MAand MPto analyze motion cues because the bounding boxes of static region candidates in two consecutive frames can have the same areas but different positions, or different areas but the same positions. The motion features MO, MAand MPenable to identify static regions completely without using any tracking method. Existing static region classification methods based on the object tracking often failed to detect static regions under heavy occlusion by other moving objects because they usually use the template matching technique to identify the same objects between two consecutive frames. However, the proposed method classifies the static region candidates into true static region when the changes of motions (MO, MAand MP) of static region candidates are near zero values irrespective of heavy occlusion. Therefore, the proposed motion feature is more discriminative to find the static regions under heavy occlusion than the object tracking approaches. The motion features are concatenated with a motion feature vector MT=[MOMAMP].Fig. 4shows how MO, MAand MPof the region that includes a bag change over time, where four columns from the top represent the input sequence with region-layer classification results, MO, MA, and MP, respectively. Fig. 4-(a) corresponds to the input image at frame 1700 that has MT=[1.00,0.01,0.00] and shows that the static region candidate has not yet appeared because the man is carrying the suitcase; Fig. 4-(b) corresponds to the input image at frame 2000 that has MT=[0.00,0.05,0.08] and shows that the static region candidate has not been determined as a true static region because the man is wandering around the suitcase; Fig. 4-(c) corresponds to the input image at frame 2300 that has MT=[0.00,0.02,0.02] and shows that the static region candidate is determined as a true static region because the man has left the suitcase.In this work, we designed shape features because temporarily stopped moving objects with a negligible local motion change (a person, a group of people) often generate false static regions and people are the most dominant factor for generating false static regions as known from the paper [2]. We observed that most of abandoned or removed objects such as bags, satchels, suitcases, and trunks have simple and smooth contours (see Fig. 5). In contrast, most of foreground objects such as people or groups of people have more complex contours than do abandoned or removed objects. To classify true (abandoned or removed objects) or false (people or groups of people who are standing or sitting temporarily) static regions, we propose three shape features: degree of unevenness (SU), degree of symmetry (SS) and degree of filling (SF) as follows.Given a foreground (or static) region Rtthat has a center positionX¯Rtand a boundary contour C(Rt) whose contour length along C(Rt) is L(Rt), we divide L(Rt) into N equal intervals counter-clockwise along the boundary contour C(Rt), starting from the X1 that intersects with the horizontal positive axis; this process generates a sequence of positions X1, X2,⋯,XN(see Fig. 6-(c)). Then, we define a sequence of lengths l1,l2,⋯,lNthat connect the center positionX¯Rtand the positions X1, X2,⋯,XN, respectively, and a sequence of angles θ1,θ2,⋯,θNthat are the angles between the horizontal positive axis and the positions X1, X2,⋯,XN, respectively.SUof Rtquantifies the degree of unevenness by quantifying the unevenness of C(Rt) as(21)SURt=12∑j=1Nlj+1%N−ljmaxlj+1%NljN+∑j=1Nθj+1%N−θjmaxθj+1%NθjN∈0,1,where (j+1)%N denotes a modular operation, i.e., (N+1)%N=1. If contour C(Rt) is simple and smooth, SUis small, and approaches zero in the case of a circle. If C(Rt) is complicated and uneven, SUis large, and approaches one in the case of a star.SSof Rtquantifies the degree of symmetry by quantifying the symmetry of the vertically (or horizontally) half-folded subregions as(22)SSRt=12AL∩ARmaxALAR+AT∩ABmaxATAB∈0,1,where AL∩AR(or AT∩AB) is the overlapped area between the left and right (or the top and bottom) subregions of region Rtand ALand AR(or ATand AB) are the areas of the vertically (or horizontally) half-folded subregions, respectively (see Fig. 6-(d)). If the region is a rectangle or circle, SS=1. Because a person or group of people is usually non-symmetric along the vertically or horizontally half-folded directions, the value of SSis small.SFof Rtquantifies the degree of filling by calculating the proportion of the bounding box that is filled by the region as(23)SFRt=ARtARt′∈0,1,where A(Rt) is the area of the region (computed by counting the number of foreground or static pixels) and A(Rt′) is the area of the corresponding bounding box. If the boundary contour of the region is complicated, SFis low due to the large empty space in the bounding box (see Fig. 6-(e)). The three shape features are concatenated into a shape vector ST=[SUSSSF].We compute the time duration TS′(Rt) which defines the number of consecutive frames in which region Rtis classified as a static region as(24)TS′Rt=TS′Rt−1+δsRt=S,where Tf′(Xt) and Tf′(Xt−1) are the accumulated frame counts of region R in frames t and (t−1), respectively, s(Rt) is the status of region R in frame t, s(Rt)∈{B,F,S}, and δ function increments the frame count by 1 when s(Rt)=S. Whenever s(Rt)≠S, TS′(Rt) returns to zero. Then, we define a normalized time duration TS(Rt) as(25)TSRt=TS′Rt2×Tmin,where Tminis the minimum time duration that is needed to change from a foreground region to a static region.SVMs were trained differently in each region state. In the background region state, the background SVM classifier SVMBdetermines one of two states (B, F) according to one area feature AF. In the foreground region state, the foreground SVM classifier SVMFdetermines one of three states (B, F, S) according to two area features AFand AS, intensity features IDand IS, motion feature vector M and shape feature vector S. In the static region state, the static SVM classifier SVMSdetermines one of three states (B, F, S) according to the area feature AS, intensity features IDand IS, motion feature vector M, shape feature vector S and time duration feature TS.We collected a dataset of 10,000 frames including background, foreground, and static regions from the personally collected database in real situations, which is disjoint with the benchmark databases for testing and randomly selected 10% of the data set to use to train each region-layer SVM as follows (see Table 3). SVMBwas trained using 1000 frames including background and foreground regions, where each region is represented by one area feature. SVMFwas trained using 1000 frames including background, foreground and static regions, where each region is represented by one area feature, two intensity features, three motion features and three shape features. The collected personal database includes a cup on the desk or luggage on the table in the very ideal and simple experimental condition, where the objects are not included in the test databases such as the public databases and a commercial database. Therefore, users can reproduce the training databases easily.Here, the background region is assumed to have zero ASand to have the same intensity, motion and shape features as the foreground region. SVMSwas trained using 1000 frames including background, foreground and static regions, where each region is represented by one area feature, two intensity features, three motion features, three shape features and one time duration feature. Here, the background region is assumed to have the same area, intensity, motion and shape features as the static region, and the foreground region is assumed to have zero time duration. Each SVM classifier in the region-layer FSM was trained using a radial basis kernel function (RBF) whose parameter set including a constant for error penalty and covariance matrix was optimized in preliminary trials.During event-layer processing, the event-layer FSM uses color and edge features to classify true static regions as either abandoned or removed objects. These features are defined in this section.We use the change of the color richness of a true static region to discriminate between abandoned or removed objects [25]. We expand the static region by increasing its width and height by a factor of 1.5. Then we compute the color richness of the expanded static region in the input image and the color richness of the region that corresponds to the expanded static region in the background image. For abandoned objects, the color richness is higher in the expanded static region in the input image than in the corresponding region in the background image; for removed objects the opposite relationship is true.To measure the color richness of the expanded bounding box St, we divide each color channel (R,G,B) into 8 bins and compute the sum of the top-N most populated bin values as(26)CRSt=∑i=1NHiRGB,where Hi(R,G,B) is the top ith most populated bin value among the 83 bins. In this paper, we set the value of N to 10 experimentally. Then, we compute the difference DCof color richness of static region Stas(27)DCSt=CRISt−CRBStmaxCRISt,CRBSt∈−1,1,where CRI(St) and CRB(St) denote the color richness of frame t in the input and background images of the expanded static region St, respectively. Therefore, DC(St) approaches 1 in the case of an abandoned object, and approaches −1 in the case of a removed object.We also accept the change of the edge strength of true static region to discriminate the abandoned or removed objects [15] (see Fig. 7). We expand the static region by increasing the width and height by a factor of 1.5. Then, we use morphological operations to extract the boundary region StBof the static region as(28)StB=StD∩StEC,where StDand (StE)Cdenote the dilated region and the complement of the eroded region of the true static region, respectively. Then, we compute the edge strength of the static region by adding the gradient magnitude |G| on the pixels within the boundary region StBas(29)ESSt=∑X∈StBGx2X+Gy2X,where Gxand Gyrepresent the gradient component along the x and y directions at pixel position X, respectively. Then we compute the difference DEof edge strength of the static region Stas(30)DESt=ESISt−ESBStmaxESISt,ESBSt∈−1,1,where ESI(St) and ESB(St) are the edge strength of the boundary region StBin the input image and the background image, respectively. When ESI=ESB, we do not perform the abandoned or removed object discrimination until the next frame image. DE(St) approaches 1 in the case of abandoned objects and approaches −1 in the case of removed object.SVMs were trained differently for each event state. In the withholding event state, the withholding SVM classifier SVMwdetermines one of three states (w, a, r) according to one color feature DCand one edge feature DE. Abandonment and removal event states have self-transition because they are the final states.We collected a dataset of 10,000 frames including withholding, abandonment and removal events from the personally collected database in real situations, which is disjoint with the benchmark databases for testing and randomly selected 10% of the data set to use to train each event-layer SVM as follows (see Table 4). SVMwwas trained using 1000 frames including withholding, abandonment and removal events, where each event is represented by one color feature and one edge feature. Each SVM classifier in the event-layer FSM was trained using a radial basis kernel function (RBF) whose parameter set including a constant for error penalty and covariance matrix was optimized in preliminary trials.The proposed ARO classification method is executed in five stages (see Algorithm 1). In the initialization stage, it applies MoG to the initial 100 frames to generate the background image and then (line 1) sets the states of all pixels in this image to the background state (line 2).In the pixel-layer classification stage, it represents each pixel with its position and the pixel state as Xt=(xt,yt,s(Xt))Tat the tth frame, where s(Xt)∈{b,f,s} (line 3). Then, for all pixels, it computes the necessary features among {Cd,Cp,Cs,Tf} according to the pixel state s(Xt) (line 4) and uses the corresponding pixel-layer SVM classifier according to the pixel state s(Xt) to determine the next pixel state s(Xt+1) (line 5). Then it uses the next pixel state that is determined from the pixel-layer SVM classifier to update the next state of each pixel by Xt+1=(xt,yt,s(Xt+1)) (line 6).In the region-layer classification stage, it uses CCA to obtain the foreground and static regions, and sets these regions as static region candidates (line 7). Then, it represents each static region candidate with its boundary positions and the region state as Rt={xtLT,ytLT,xtRB,ytRB,s(Rt)}, where xtLT,ytLT,xtRB,ytRBdenote the x and y positions of the left top point and the right bottom point of the bounding box respectively, and s(Rt)∈{B,F,S} (line 8). Then, for all static region candidates, it computes the necessary features among {AF,AS,I,M,S,TS} according to the region state s(Rt) (line 9) and uses the corresponding region-layer SVM classifier according to the region state s(Rt) to determine the next region state s(Rt+1) (line 10). Then, it uses the next region state that is determined from the region-layer SVM classifier to update the next state of each static region candidate by Rt+1={xt+1LT,yt+1LT,xt+1RB,yt+1RB,s(Rt+1)} (line 11) and sets the static region candidates whose next region states are static region states S as true static regions St(line 12).In the event-layer classification stage, it represents each true static region with its boundary positions and the event state as St={xtLT,ytLT,xtRB,ytRB,s(St)} (line 13). Then, for all true static regions, it computes the features {DC,DE} according to the event state s(St) (line 14) and uses the corresponding event-layer SVM classifier according to the event state s(St) to determine the next event state s(St+1) (line 15). Then, it uses the next event state that is determined from the event-layer SVM classifier to update the state of each event by St+1={xt+1LT,yt+1LT,xt+1RB,yt+1RB,s(St+1)} (line 16).In the background image update stage, it uses Eqs. (1), (2) and (3) to update the MoG's parameters for the pixels with the background pixel states (line 17) and absorbs the true static regions into the background image after a user-defined time (line 18).Algorithm 1Algorithm procedure of the proposed ARO classification method.We used public databases and a commercial database to evaluate the performance of the proposed ARO classification method. The public databases were Laboratoire de Vision et Systemes Numerique (LVSN)33http://vision.gel.ulaval.ca/en/Projects/Id-283/projet.php., Wallflower44http://research.microsoft.com/en-us/um/people/jckrumm/wallflower/testimages.htm., PETS200655http://www.cvg.rdg.ac.uk/PETS2006/data.html., AVSS2007 (i-LIDS)66http://www.eecs.qmul.ac.uk/andrea/avss2007.html.and CAVIAR77http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1.databases and the commercial database was ObjectVideo (OV) database. The proposed ARO classification method was applied to each frame independently and was implemented in Visual C++ on a Windows PC platform with a 2.94-GHz Intel Core i7 CPU.To evaluate the pixel-layer classification, we used the precision (P) and the recall (R) as(31)P=TPPTPP+FPP,(32)R=TPPTPP+FNP,where TPP, FPPand FNPdenote the number of correctly classified foreground pixels (true positives), the number of the falsely classified foreground pixels (false positives) and the number of missed foreground pixels (false negatives) in all frames, respectively.To evaluate the region-layer classification, we used the true classification accuracy (TCA) and the false classification ratio (FCR) as(33)TCA=TPRTPR+FNR,(34)FCR=FPRTPR+FPR,where TPR, FNR, FPRand GTRdenote the number of correctly classified static regions (true positives), the number of missed static regions (false negatives), the number of falsely classified static regions (false positives) and the total number of true static regions (GTR=TPR+FNR) in all frames, respectively. To evaluate TCA and FCR, we consider the PASCAL measurement (PC) which is an overlap ratio as(35)PC=AGT∩ARtAGT∪ARt,where A(GT) and A(Rt) are the bounding box areas of the ground-truth static region and the observed true static region at the tth frame, respectively. If there exists more than one true static region with PC≥0.5 in the specific frame, then the frame is included to compute TPR; similarly, if the specified frame includes more than one true static region with PC<0.5, the frame is included to compute FPR. When the specific frame has ground-truth data GT but no true static region with PC≥0.5, the frame is included to compute FNR.To evaluate the event-layer classification, we used the static region discrimination accuracy (SRDA) as(36)SRDA=TPEGTE,where TPEis the number of the correctly classified events (true positives) and GTEis the total number of events.In this experiment, we consider the foreground pixel classification performance of the proposed pixel-layer FSM because the LVSN and Wallflower public databases provide the ground-truths for foreground pixels. LVSN consists of two image sequences (Highway I and Highway III) including 20 images. Wallflower consists of seven sequences of seven images; we used the WavingTrees image sequence.Among three different pixel SVM classifiers, we consider the SVMbclassifier because it determines whether a pixel is a background or foreground. Table 5compared the precision and recall in the foreground pixel classification of the proposed ARO classification method (H-FSM) with those of the multi-hypothesis method [23] over all image sequences with ground-truth. From Table 5, we know that the proposed ARO classification method had 5% higher precision and 2% higher recall than the multi-hypothesis method over all image sequences; this means that the proposed ARO classification method has fewer number of falsely classified foreground pixels (false positives) and fewer number of missed foreground pixels (false negatives) than the multi-hypothesis method.Fig. 8shows the examples of the foreground pixel classification results with (a) Highway I, (b) Highway III and (c) Wallflower public database, where the first, second, third and fourth rows represent the input image sequence, the ground-truth sequence of the foreground regions, the results of the multi-hypothesis method [23] and the results of the proposed ARO classification method, respectively.In this experiment, we consider the true static region classification performance of the proposed region-layer FSM using the public database built by i-LIDS (AB-Easy, AB-Medium, AB-Hard, ABTEA101a, PV-Easy, PV-Medium, PV-Hard, and PV-Night), PETS (Cam3) and CAVIAR (LeftBag), which include ten video clips and a total of twenty-eight static regions.Table 6compared TPRand FPRin the region classification of the proposed ARO classification method (H-FSM) with those of other methods [15,9,26,27], where Kim [27] is our previous method that uses the intensity, motion and shape features and one SVM classifier, over all image sequences with ground-truth and the number within parenthesis following the name of databases is the number of true static regions. From Table 6, we know that (1) all methods classified all true static regions on their test databases, (2) the proposed ARO classification method has the least number of falsely classified static regions and (3) the number of falsely classified static regions was largely dependent on the scene complexity such as traffic volume, and the existence of moving objects in the initial frames. The proposed ARO classification method rejects false static regions more effectively than other methods because it uses together multiple features that define the characteristics of true static regions accurately. Especially, the shape feature classifies well the temporarily stopped objects such as a person and a group of people.In this experiment, we consider the true static region classification performance of the proposed region-layer FSM using the OV commercial database, which includes 22 video clips with a total of 62 static regions. The video clips are divided into three classes according to the discrimination difficulty levels: eight low, seven medium and seven high, where the high difficulty level consists of high traffic volumes and static regions that are frequently occluded by other foreground regions. To evaluate the true static region classification performance, we used the same SVM classifiers SVMB, SVMFand SVMSwhich are used in the performance evaluation of the public database.Tables 7 and 8compared TPR(TCA) and FPR(FCR) in the region classification of the proposed ARO classification method (H-FSM) with those of other methods (OV's commercial software) [28] and Kim [27] over all image sequences with ground-truth, and the number within parenthesis following difficulty levels is the number of true static regions. From Tables 7 and 8, we know that (1) TCA of the proposed ARO classification method was 32% higher than that of OV's method at all difficulty levels, (2) FCR of the proposed ARO classification method was 14% smaller than that of OV's method at all difficulty levels, (3) FCR of OV's commercial software was largely dependent on the scene complexity and increased in proportion to the traffic volume and (4) all methods show high average values of FCR because the relatively long playing time of the video clips causes a high probability that foreground regions will be falsely classified as true static regions (false positives). The proposed ARO classification method shows a stable true static region classification among different difficulty levels due to using the trained SVM classifiers for the state transitions. This stability makes the proposed ARO classification method applicable to various real situations. (See Fig. 11.)Fig. 9shows the examples on how a true static region is obtained over time, where the first, second and third rows represent the input image sequence, the foreground region sequence and the true static region sequence, respectively, and (a) one person approaches the fence (1700th frame), (b) he leaves a bag near his leg (1900th frame), (c) he departs from the bag (2100th frame) and (d) he disappears from the scene and the bag is abandoned (2300th frame).We compared the proposed ARO classification method to OV's commercial software in two cases, one in which OV falsely classified a static region but the proposed ARO classification method did not (see Fig. 10-(a) and (b)); and one in which OV missed a static region but the proposed ARO classification method found it (see Fig. 10-(c) and (d)). OV's commercial software does not consider the shape information in the region-layer classification, so it falsely classified a static region in a group of two persons who did not move for a short time (see Fig. 10-(a)); but the proposed ARO classification method did not make this error (see Fig. 10-(b)). OV's commercial software considers only pixel-layer processing and can therefore be misled by rapid illumination change, so this algorithm missed a static region in an illegal parking scene due to the blinking of the car headlight (see Fig. 10-(c)); but the proposed ARO classification method did not make this error (see Fig. 10-(d)).In this experiment, we used the OV commercial database to evaluate the abandoned or removed event classification accuracy of the proposed event-layer FSM. This database includes 22 video clips with a total of 62 static regions.Table 9compared SRDA in the event classification of the proposed ARO classification method (H-FSM) with those of other methods (OV's commercial software) [28] and Kim [27] over all image sequences with ground-truth, and the number within parenthesis following difficulty levels is the number of ARO events. From Table 9, we know that (1) OV's software and the Kim [27] method gave almost same SRDA because they use edge strength for event classification, (2) the proposed ARO classification method obtained perfect SRDA because this method uses color richness and edge strength together for event classification, and (3) the proposed ARO classification method had 4–5% higher SRDA than the other methods.When processing pixels individually, average computation time was 50ms; the pixel (P)-, region (R)-, and event (E)-layer processing and background image update required 38, 6, 0.1 and 6ms, respectively. When processing blocks of 4×4pixels, average computation time was 18ms; the pixel-, region-, and event-layer processing and background image update required 10, 6, 0.1 and 2ms respectively (see Table 10), where the values in parenthesis are for the block-based processing.The proposed ARO classification method successfully detected several real cases such as the detection of abandonment (littering, illegal parking (staying), camouflaged soldiers) and detection of removal (illegal parking (leaving), and theft of small and large objects) (see Fig. 11).

@&#CONCLUSIONS@&#
