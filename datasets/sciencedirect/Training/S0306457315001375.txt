@&#MAIN-TITLE@&#
Transforming LSA space dimensions into a rubric for an automatic assessment and feedback system

@&#HIGHLIGHTS@&#
We model how to implement a rubric in latent semantic analysis.The proposed method change abstract dimensions into meaningful dimensions.The method allows to detect easily written contents.Inbuilt rubric method has been used to give feedback to 924 university students.

@&#KEYPHRASES@&#
LSA,Rubric,Lexical descriptors,Automatic evaluation,Summary,

@&#ABSTRACT@&#
The purpose of this article is to validate, through two empirical studies, a new method for automatic evaluation of written texts, called Inbuilt Rubric, based on the Latent Semantic Analysis (LSA) technique, which constitutes an innovative and distinct turn with respect to LSA application so far. In the first empirical study, evidence of the validity of the method to identify and evaluate the conceptual axes of a text in a sample of 78 summaries by secondary school students is sought. Results show that the proposed method has a significantly higher degree of reliability than classic LSA methods of text evaluation, and displays very high sensitivity to identify which conceptual axes are included or not in each summary. A second study evaluates the method's capacity to interact and provide feedback about quality in a real online system on a sample of 924 discursive texts written by university students. Results show that students improved the quality of their written texts using this system, and also rated the experience very highly. The final conclusion is that this new method opens a very interesting way regarding the role of automatic assessors in the identification of presence/absence and quality of elaboration of relevant conceptual information in texts written by students with lower time costs than the usual LSA-based methods.

@&#INTRODUCTION@&#
It is a well-known fact that novice writers pay little attention to revision processes. It is also well known that even when revisions are made these are often superficial or mechanic (Fitzgerald, 1987; Graham, 2006). There is also evidence that even university students spend little time on revision processes (Pianko, 1979); in fact, it can be stated that one of the characteristics of more competent writers is spending more time on revision and applying it to a conceptual and structural level (Hayes & Flower, 1986) instead than to superficial text traits (spelling, errors, etc.) which is the common strategy in novice writers. The importance of improving the quality of students' writing cannot be overstated, and the same can be said about the relevance of revision processes to this purpose. Precisely in order to aid and motivate writers in the process of revision of academic texts, special emphasis has been placed in recent years on the creation of automatic assessors and tutors that provide help in instruction tasks (Foltz, Laham, & Landauer, 1999b; Foltz, Streeter, Lochbaum, & Landauer, 2013; Magliano & Graeser, 2012; Shermis, Koch, Page, Keith, & Harrington, 2002). Some of these automated assessors have made use of engines based on Latent Semantic Analysis (LSA) and its ability to identify concepts (Foltz, Gilliam, & Kendall, 2000; Graesser et al., 2000; Kintsch, Caccamise, Franzke, Johnson, & Dooley, 2007). Part of the assessors in this group are aimed at offering ongoing online conceptual feedback when a student delivers a text or changes part of a text already delivered (Franzke, Kinstch, Caccamise, Johnson, & Dooley, 2005). The aim of these assessors is to provide support to the cyclical process by which students enter the summary of a previously-read text. The system provides information about the concepts that are included or not with respect to those which should be included, and the student re-revises his text (reviewing even the basic text to obtain information). Then the student enters another summary that will usually be based on the first one.As was previously stated, Latent Semantic Analysis (LSA) is one of the techniques that can identify conceptual information and is widely used for this reason. There is no need to specify the basics and details of this technique, which is thoroughly documented (Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990; Landauer & Dumais, 1997). There is extensive documentation too about the educational aspect of this technique (Haley, Thomas, Petre, & De Roeck, 2007; Kakkonen, Sutinen, & Timonen, 2005; Millis, Magliano, Wiemer-Hastings, Todaro, & McNamara, 2007). However, some essential LSA concepts should be described to understand the logic underlying the methodology proposed in this paper. LSA starts by analyzing an extensive set of documents (the linguistic corpus), where, to begin with, a matrix X of occurrences is generated, with n rows and m columns. The number n of rows corresponds to the number of different terms analyzed in this first step and the number m of columns refers to the number of documents used to train the LSA system. Each cell in X contains the frequency of the term in each document. It is known that this LSA based X matrix does not contain a useful representation of the semantics because, among other things, it has a huge dimensionality, and contains the subjective use made by the authors of the terms. After softening the X matrix by means of a function such as entropy (i.e. Nakov, Popova, & Mateev, 2001) the algebraic technique known as singular value decomposition, which defines the essence of LSA, is applied. This technique yields a US matrix with n x k dimensions, where k is now a small number of dimensions, usually ranging between 250 and 350. This US matrix is known as a latent semantic space because it contains an efficient representation of the terms in the corpus (i.e. Landauer & Dumais, 1997). The term “latent” is due to the fact that the semantic space has an eminently abstract nature. The k dimensions that represent terms and texts do not correspond to discernible concepts or specific episodes with which to label the dimensions. The k dimensions do not have thus psychological verisimilitude, yet they have proven very useful to simulate semantic judgments and many tasks involving text meaning (i.e. evaluation, predication, diagnosis, telephone call routing, etc.) The concept of semantic space is thus very important, as it is the basis onto which the texts to be semantically interpreted are projected, and thus is also very important for educational assessment applications.The way in which texts written by students have been usually assessed and rated has been to compare the vector representation of those texts in the semantic space with the vector representation of one or more texts that serve as the standard criterion and have been written by experts in the discipline (usually teachers). There are various techniques in this regard, from the simplest ones, based on a single standard summary (golden summary, see Landauer, Foltz, & Laham, 1998 or more recently Klein, Kyrilov, & Tokman, 2011), to more complex ones, such as the technique that makes use of a sample of previously rated standard summaries so that the scale is more plausible from the point of view of a human judge (grading techniques, see Burstein, Kukich, Wolff, Lu, & Chodorow, 1998; Dronen, Foltz, & Habermehl, 2015; Kakkonen & Sutinen, 2004). To the extent that the student's summary approaches a pre-rated standard summary, it will get the same grade as the standard summary. In any case, in both techniques and some intermediate ones, a single grade is obtained, which is a function of the semantic distance between the student's text and those standard criteria. Dronen et al. (2014) describe the high costs of this type of study and propose methods for pre-grading to be as efficient as possible.One of the ways in which these techniques have been implemented to assess student texts has implied providing not only a single grade, but the information of what contents are missing in a summary to reach an acceptable standard as well. This is basically a rubric-based content detection task (see, for instance, Summary Street by Franzke et al., 2005). This is done in the same way as when giving a single grade, that is, by taking text samples (or partial golden summaries) from the concepts to inform about (e.g. sentences extracted from the book to be read), project them onto the semantic space as a vector, and compare them to the vector that represents the student's text. The feedback for each concept will depend on whether the distance between vectors (those of the students and those of the partial golden summaries) is large or small. Another example is Apex (Dessus & Lemaire, 1999), an interactive learning environment, in which the teacher must identify short passages of the instructional text that contains a key concept and the topic or topics to which each concept belongs. In Magliano and Graesser (2012) it is shown that automatic systems involve the creation of a number of answers (which they call expectations) that represent either the different levels of students, or else concepts, inferences, or usual misconceptions among students. These answers are real language samples. Obviously, the preparation of these instruments (golden summaries, graded summaries or partial golden summaries) is time consuming and requires effort. In particular the elaboration of partial golden summaries involves a more time and complexity, besides problems regarding the quantity and sampling of the text to be collected in order to represent each concept. Moreover, golden summaries, graded summaries and partial golden summaries are all based on what may be considered to be a blind or meaningless vector space, so we will name them a “latent space instruments”, in contrast with the meaning “loaded” vector space instrument we present in this paper, the Inbuilt Rubric.Along these lines, developing new procedures to go beyond mere measurements of similarity between texts and refine the techniques to capture the basic conceptual axes in a written essay in a nuanced and efficient way is certainly a challenge (see Dronen et al., 2014; Kontostathis & Pottenger, 2006; Magliano & Graesser, 2012; McNamara, 2011 for possible LSA developments).Facing this challenge, in this paper we propose the use of a new strategy to identify contents by means of a rubric. This is a strategy that has already been described as a general application to provide some of the k dimensions in the semantic space with a meaning imposed a priori (Olmos, Jorge-Botana, León, & Escudero, 2014). Briefly put, the strategy of this study is based on choosing descriptors (words) for each of the concepts to be established as a criterion. Once each defined concept has its descriptors, each concept becomes a vector in the semantic space and a change of basis is applied so that words are expressed in a new space with those vector-concepts that are part of the new basis. In order to avoid distorting the distances and positions in the former semantic space, a mathematical technique is used to force orthogonalization once again in a controlled and monitored manner. The final product of this strategy is a new semantic space in which the p first dimensions have the meaning of the concepts that were imposed on them a priori. More specifically, in this paper we propose that those p first dimensions be structured as the main concepts of a rubric aimed at assessment and feedback. This can make the design processes for LSA-based evaluation objects more agile. This method will be called Inbuilt Rubric, by contrast to the classic “latent space instruments” mentioned above. In this paper we will provide a detailed description of the Inbuilt Rubric method, as well as two empirical studies. In the first study, evidence of the validity of the methodology proposed in a controlled context of evaluation on a sample of 78 summaries by secondary school students is sought. In the second we present a pilot study where the method has been used in a distance education online system, which was accessed by 864 university students taking a third-year course in the Degree in Psychology. Participants use profiles of the automated assessment and feedback in this pilot study will be described, as well as routes for improvement in the quality of written answers.The general procedure to transform the latent semantic space into a new, meaningful space can be found, together with all the technical details, in Olmos et al. (2014). The description of the Inbuilt Rubric provided here is shorter and simplified, and is accompanied by a specific application: a study of rubric-based evaluation of an expository text, The strangler tree, 500 words long, conceived for secondary school students. The method has three distinct steps.(1)Rubric: This is the first step. It involves establishing the evaluation rubric by extracting the basic conceptual axes in the text. The text used describes a type of jungle trees that survive by being parasites on other trees. In order to survive, many plants compete to reach sunlight in an area of thick vegetation. These trees use their roots to first surround their host trees, until they create a thick web around them, and finally strangle them and remain as an independent tree. In a previous paper, six experts (León & Grupo de Investigación Santillana, 2004) discussed the essential conceptual axes of this text. These conceptual axes were four: (1) Suitable contextualization of the text. This involves presenting strangler trees and locating them (jungle areas, tropical areas, etc.) (2) Secondly, it was regarded as essential to describe the process of strangulation by means of the roots as a gradual process with a lethal ending for the host tree, which is clearly a parasitism strategy. (3) Another conceptual axis of the text is the immediate goal of this parasitism process, namely the struggle and competition to reach sunlight. (4) Finally, the fourth conceptual axis of the text is the ultimate goal described in it: a general strategy of survival in difficult adaptation conditions. So the conceptual axes were four: contextualization of the strangling tree, the process of strangulation and asphyxiation, the immediate goal of the struggle to reach sunlight, and the ultimate goal which is a survival strategy.Lexical descriptors: The second step required by the method is the insertion of the rubric into the LSA. This is done by choosing lexical descriptors that capture the four conceptual axes described in the rubric in the best way possible. Because the next step is transforming the latent semantic space in such a way that the first dimension captures the meaning of the first conceptual axis (strangler trees, humid jungle areas, etc.), the lexical descriptors that best express these ideas must be chosen. The procedure is the same for the second conceptual axis. The lexical descriptors that best describe the conceptual axes must be found. The second step ends when lexical descriptors are chosen for all the conceptual axes imposed in the rubric.The lexical descriptors chosen for the four conceptual axes were the following: for the first dimension, strangle, tree, rain-forest, jungle and tropical were chosen. For the second dimension, to kill, to asphyxiate, to strangle and roots were chosen. For the third dimension, competition, sun and light were chosen. Finally, for the fourth dimension adaptation, survival and survive were used.Change of basis: Finally, the third step, once the rubric and its conceptual axes have been completed and once the lexical descriptors that best represent them have been chosen, consists in transforming the meaningless latent semantic space into a new one whose first p dimensions signify the conceptual axes of the rubric. Employing the usual LSA notation (Deerwester et al., 1990) and, as presented in the introduction to the paper, we will call the non-transformed meaningless semantic space, US, a matrix with n x k dimensions, where n is the number of terms and k the number of latent dimensions. In our study, the number of dimensions k with which LSA was trained was 250 and the number of terms n was 13,389. The term vectors in US are referenced in the canonical (standard) basis. The k dimensions are actually the vectors in the canonical basis, and every n vector is represented by means of the mentioned basis.Let us call this new basisβ={VAXIS1, VAXIS2, VAXIS3, VAXIS4, VAXIS5, …, VAXISK}, where Vk is a vector in the new basis. The first p vectors inβwill represent the conceptual axes created in the rubric. In addition, each vector representing each conceptual axis is the vector sum of all its descriptors. For example, VAXIS1 = VSTRANGLE + VTREE + VRAIN-FOREST + VJUNGLE + VTROPICAL, that is to say, the sum of the vectors that represent the descriptor terms in the original semantic space US. The new basis must have the same number of vectors, k, as the original canonical basis. To this end, in addition to having the p vectors in the rubric axes, the remaining vectors in the new basis {Vp+1, …, Vk} are created using vectors from the canonical basis, until a basis of k vectors is completed. The only requirement is that all vectors must be linearly independent.Following this procedure, we can express the terms of the matrix US in the new basisβ, obtaining a new term matrix C whose p first dimensions are “meaning loaded” (as meaningful as the vectors of the axes VAXIS1, VAXIS2, VAXIS3, VAXIS4 in the new basisβ) . Each term vectorciin the new term matrix C can be calculated by multiplying each vectorusiin US byβ–1. This possibility was originally suggested by Hu, Cai, Wiemer-Hastings, Graesser, and McNamara (2007):(1)C=(US)(β−1)This method is direct and has no mathematical complexity. However, it should be pointed out that the other matrixβwhich serves as the basis will in all likelihood not be orthogonal. Forβto constitute a new basis, it must meet the requirement that the k vectors which it contains must be linearly independent. This is guaranteed, unless the words chosen to form two concepts are exactly the same. The problem is that inβvectors can be (and will probably be) correlated, as a null correlation is a very particular case. Soβlacks orthogonality, or, put otherwise,βis an oblique basis. Generating a new C matrix withβbeing a non-orthogonal matrix makes later interpretation of the coordinates difficult. In fact, a non-orthogonal basis often creates a space that is hardly usable, as the original distances between the term vectors are not preserved (Olmos et al., 2014; Visinescu & Evangelopoulos, in press). To ensure the orthogonality of the basis, we applied Gram–Schmidt before using Formula 1 (see for example, Schneider, Steeg, & Young, 1987). Gram-Schmidt ensures that the vectors inβare orthogonal. Basically, what it does is transform the vectors inβinto similar ones (which constitute theβ’ basis) while preserving orthogonality. The requirement imposed is that theβvectors must be similar to theβ’ vectors, so that correlation between a vector and its orthogonalized version must be greater than or equal to .70. The reason is that .702 ≈ .50, so both vectors must share at least 50% of variance (see details in Olmos et al., 2014). In the study performed, it was found that the correlation between the vector in the first conceptual axis inβand its orthogonalized correction inβ’ was: rβ1β’1 = 1; the second correlation between the vector in the second conceptual axis inβwith that inβ’ was rβ2β’2 = .987. In turn, rβ3β’3 = .962 y rβ4β’4 = .890, so in all cases the requirement rβiβi’ > .70 was met. Experience tells us that by choosing words that are not really redundant and when the number of meaningful dimensions (i.e. conceptual axes) is not high, there will be no reliability problems preserving the orthogonal basis. In the specific example of the expository text, the vectors that constitute basisβ’ are very similar to those inβbefore orthogonalization was forced. Thus, in order to obtain the new semantic space C’ the following was used:(2)C′=(US)(β′−1)At this point we have a new meaningful semantic space in C’. The first four dimensions now bear the meanings that we have given to them, in such a way that a term whose coordinate is high in the first dimension will be related to the first conceptual axis (strangler trees in humid, tropical, jungle areas, etc.), a word whose coordinate is high in the second dimension will convey abundant information about the second conceptual axis (the asphyxiation and strangulation process), etc. The benefits of having a meaningful semantic space are obvious: in C' we can interpret the coordinates of the terms or summaries (at least in the p meaningful dimensions), and yet this is not the case in the original space US, given that the k dimensions are meaningless.Projection of the summaries ontoC'. The rest of the method is simple. Because the four first dimensions reflect the conceptual axes of the text (although then it is necessary to verify a posteriori whether those new dimensions really capture well the intended meaning), by projecting a summary onto the new semantic space C' we will have what that summary saturates in each of the four dimensions (note that the projection of the summaries is in the k-dimensions of C', so the first four dimensions are meaningful and the remaining k – p are abstract/meaningless dimensions). Thus we will be able to see how much each summary bears of each conceptual axis of the rubric. The global score of a summary is a linear combination of the mentioned dimensions. In the specific case of this study, the same weight was given to each of the four conceptual axes, so the total score of the quality of the summary is the sum of all four dimensions. There is the possibility of previously standardizing the dimensions, turning them into standardized scores for those coordinates.

@&#CONCLUSIONS@&#
We have presented a brief description and some promising results on the use of a new method called Inbuilt Rubric, based on LSA, for assessing students' summaries and giving feedback about them. This method involves a transformation (via a change of basis and reorthogonalization) of the original latent semantic space into a new one that is non-latent in nature. This new space is non-latent because its first dimensions are meaningful. In particular, such dimensions represent the conceptual axis of an academic rubric.Inbuilt Rubric has been shown to probably be a more reliable (more comparative studies in addition to this one are required) and an economic assessment method for the kind of summarization task used in our studies than the traditional Latent Space Instruments (e.g., golden summary, partial golden summaries methods or pregraded summaries as an external measure of the summary quality). Its relationship to human judges’ scores of student produced summaries is higher (.82 as compared to .66); this is also shown by our regression equations. This has been further proved through the results of the scatter plots comparing human judges, Golden Summary and Inbuilt Rubric. As for the ability to provide feedback about the conceptual axes in a summary, this method displays significant equivalence with the experts' judgments.One of the main implications of the method process is that it makes it much easier to identify contents. Classic procedures, such as Summary Street (Kintsch et al., 2007), require comparing the student's summary to partial golden summaries or statements that express those conceptual axes. May be, this involves longer writing times and problems regarding the quantity and sampling of the text to be collected in order to represent each concept. In Inbuilt Rubric this task is much simplified, as those axes are captured in the semantic space coordinates, given that the new semantic space contains a rubric in some of its dimensions. In this way, deciding whether the summary has sufficiently covered a conceptual axis or not is a simple task. Because every axis in the rubric is represented in a dimension in the summary vector, it suffices to compare that vector to another one representing a golden summary. This golden summary in turn contains the ideal rubric in its dimensions.This idea was implemented in the second study by means of an online system in which 864 university students summarized a textbook chapter. The system offered visual feedback in which students can see whether their summary correctly captures each conceptual axis in the rubric with respect to an ideal range imposed by a golden summary. In this second study, students had the possibility of rewriting their summary as many times as they wished, and the visual feedback was provided again in each attempt. When they believed that the summary they had written was good enough, they would submit it. The results regarding use showed that students naturally performed the task, as their performance displayed an upward trajectory, which was expected if the feedback system helped to improve students' summaries. Finally, when asked about the degree in which this task had helped them to learn about the subject, 85% of students answered that it had helped in a high or very high degree. This is a valuable result given that at university, high school or in massive open online courses there exists a strong demand by teachers for automatic scoring or giving feedback on student essays (Balfour, 2013).As a final conclusion, it should be pointed out that both the Inbuilt Rubric system and the system applied to a real situation are easy to implement, as the procedure does not require time to extract partial text samples to provide feedback, and has proven reliable to identify content quality. However, two things should be taken into account: a) firstly, the descriptor selection task for each axes requires practice; b) secondly, because the rubric is included in the space dimensions, every object of assessment must have its own semantic space, and thus take up more RAM memory in the computer equipment. In any case, current computer equipment has sufficient capacity to host many spaces coexisting simultaneously. Generally, automatic assessment methods work in expository texts. Inbuilt Rubric method would be especially sensible to narrative texts. Building a rubric in narrative text and implementing it in LSA model is undoubtedly a challenge. In the future, it may be necessary to consider two main aspects: firstly, it is important to standardize the parameters to create the rubric into the LSA model (e.g., number of descriptors per conceptual axe, take into account the length of the summaries or essays, or to relate the rubric with golden summaries as in the study 2). Secondly, it is also important to incorporate psychometric properties in automatic open-ended assessment (i.e., reliability, content validity or, fundamentally, study the factor structure via confirmatory factor analysis to validate measurement models).