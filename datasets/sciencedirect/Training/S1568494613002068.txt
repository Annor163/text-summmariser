@&#MAIN-TITLE@&#
Building the fundamentals of granular computing: A principle of justifiable granularity

@&#HIGHLIGHTS@&#
Introduction of the principle of justifiable granularity.Comprehensive algorithmic framework supporting the design of information granules.Discussed are two essential requirements of information granularity.Provided is a series of experimental studies.

@&#KEYPHRASES@&#
Granular computing,Information granules,Design of granules,Justifiable granularity,Fuzzy sets,Intervals,

@&#ABSTRACT@&#
The study introduces and discusses a principle of justifiable granularity, which supports a coherent way of designing information granules in presence of experimental evidence (either of numerical or granular character). The term “justifiable” pertains to the construction of the information granule, which is formed in such a way that it is (a) highly legitimate (justified) in light of the experimental evidence, and (b) specific enough meaning it comes with a well-articulated semantics (meaning). The design process associates with a well-defined optimization problem with the two requirements of experimental justification and specificity. A series of experiments is provided as well as a number of constructs carried for various formalisms of information granules (intervals, fuzzy sets, rough sets, and shadowed sets) are discussed as well.

@&#INTRODUCTION@&#
Granular computing [1,2,24,25] embraces a spectrum of concepts, methodologies, algorithms and applications, which dwell upon information granules and their processing. Information granularity is a fundamental concept that permeates this entire area. Information granules are building blocks using which a problem is represented, its model is constructed and ensuing decisions are constructed. Granular computing concentrates on processing information granules and constructively develops a holistic view at the discipline and incorporates the exisiting technologies and formalisms of sets (interval analysis), fuzzy sets [7,14,15], rough sets [11–13], probabilistic granules, shadowed sets [16–18] and alike. The fundamental quest arising in granular computing concerns a systematic way of forming information granules. With this regard, it becomes ultimate to develop a general way of designing information granules irrespectively from the formalism within which information granules are expressed. In brief, the crux of the problem can be highlighted as follows: given a collection of pieces of data or information granules (pieces of experimental evidence), form a representative information granule, which reflects the nature of the available experimental evidence. The ultimate objective of this study is to introduce a concept of justifiable granularity supporting a way of realizing an information granule. In addition to the concept itself, we form the underlying optimization problem in which the requirements of the proposed construct are expressed as well-defined optimization objectives so that a solution can be formed in a formal fashion. The formulation of the problem is independent from the way in which information granules are presented (say, as fuzzy sets or intervals).In retrospect, while granular computing has enjoyed a significant growth, a comprehensive discussion on the realization of information granules is evidently lacking. Needless to say that there have been a lot of studies in which information granules were constructed in conjunction with some specific applications and in the context of the use of a certain formal vehicle of information granules, say fuzzy sets or rough sets. With this regard, different techniques of clustering are often studied. Clustering is viewed as a means of building a collection of information granules – clusters [18,20]. Depending on the formal mechanisms of clustering (where we encounter clustering, fuzzy clustering, rough clustering), formed is a collection of information granules such as sets, fuzzy sets or rough sets. The realization of information granules is associated with the underlying performance index (objective function) used in the clustering method. In contrast with the concept and the ensuing methodology introduced here, clustering lacks this unified approach (so finally information granules are reflective of quite diversified rationale behind a certain method being used). Furthermore in clustering we form a collection of information granules; in this study we are concerned with a formation of a single information granule. Likewise the existing collection of user-driven methods aimed at the design of information granules exhibits a visible diversity and the method there do not offer a unified treatment of the concept of information granularity and support a general way of their construction.In a nutshell, it is intuitively appealing to assume that an essence of the collection of experimental data (evidence) can be captured in a form a certain representative whose nature should be more general (abstract) in comparison with the experimental data to be dealt with. In case D is a collection of numeric data, the representative has to be granular (set, fuzzy set, etc.). In situation when D comprises information granules D={G1, G2, …, GN}, one envisions that the representative of D is elevated at the higher level of abstraction than the original entities. Symbolically, we can use here the notation G2 (GG) to describe this construct of interest. For instance, we anticipate that in case of D coming as a collection of fuzzy sets, the resulting granular representative could be formed as a fuzzy set of higher type, say interval-valued fuzzy set, type-2 fuzzy set, probabilistic set, etc.In a formal manner, we can pose the problem as follows:Given some experimental evidence (typically, of numerical nature), construct a single information granule, which is (a) experimentally justifiable, and (b) exhibits a significant level of specificity.This formulation of the problem is general in many different ways and when moving with an algorithmic realization of the principle, one has to express the underlying optimization objectives (what the requirements (a)–(b) are) and decide upon the formalism using which information granules are expressed.The study is arranged in a top–down fashion. We start with the essence of the concept (Section 2). A series of illustrative experiments is reported in Section 3; here we show how information granules are designed in presence of discrete numeric data as well as data governed by some probability density functions. Next, we discuss a realization of information granules in the form of rough sets, look at the parametric aspects of the performance index and look at the weighted data. In Section 7, we show how with the aid of the representation theorem, the parameterized family of intervals can be arranged together to form a fuzzy set and a type-2 fuzzy set. A construction of a multidimensional information granule is presented in Section 8 while in Section 9 we elaborate on the realization of the principle of justifiable granularity in the presence of information granules rather than numeric evidence.As noted above, we are concerned with a development of a single information granule Ω based on some experimental evidence (data) coming in a form of a collection of a one-dimensional (scalar) numeric data, D={x1, x2, …, xN} where xk∈R. In what follows, to focus the discussion, all information granules will be defined in R. This information granule is expressed in a certain formal framework of granular computing by being formed as an interval (set), fuzzy set, rough set, shadowed set and alike [22]. The essence of the principle of justifiable granularity is to form a meaningful information granule Ω based on available experimental evidence (data) D where we require that such a construct has to adhere to the two intuitively compelling requirements:(i) Experimental evidence: The numeric evidence accumulated within the bounds of Ω has to be as high as possible. By requesting this, we anticipate that the existence of the information granule is well motivated (justified) as being reflective of the existing experimental data. For instance, if Ω is a set (interval) then the more data are included within the bounds of Ω, the better – in this way the set becomes more legitimate. Likewise in case of a fuzzy set, the higher the sum of membership degrees of the data in Ω, the higher the justifiability of this fuzzy set is.(ii) Semantic meaning: At the same time, the information granule should be as specific as possible. This request implies that the resulting information granule comes with a well-defined semantics (meaning). In other words, we would like to have Ω highly detailed, which the information granule semantically meaningful (sound). This implies that the smaller (more compact) the information granule (lower information granule) is, the better. This point of view is in agreement with our general perception of knowledge being articulated through constraints (information granules) specified in terms of statements such as “x is A”, “y is B” where A and B are constraints quantifying knowledge about the corresponding variables. Evidently, the piece of knowledge coming in the form “x is in [1,3]” is more specific (semantically sound, more supportive of any further action, etc.) than another piece of knowledge where we know only that “x is in [0,10]”.While these two requirements are appealing from an intuitive perspective, they have to be translated into some operational framework in which the formation of the information granule can be realized. This framework depends upon the accepted formalism of information granulation, viz. a way in which information granules are described as sets, fuzzy sets, shadowed sets, rough sets, probabilistic granules and others.For the clarity of presentation and to focus on the nature of the construct, let us start with an interval (set) representation of information granule Ω. The requirement of experimental evidence is quantified by counting the number of data falling within the bounds of Ω. When a finite set of experimental data D is provided, we determine cardinality of elements falling within the bounds of Ω, namely card{xk|xk∈Ω}. More generally, we may consider an increasing function of this cardinality, say f1(card{xk| xk∈Ω}) where f1 is an increasing function of its argument. The simplest example is a function of the form f1(u)=u.The specificity of the information granule Ω associated with its well-defined semantics (meaning) can be articulated in terms of the length of the interval. In case of Ω=[a, b], any continuous nonincreasing function f2 of the length of this interval, say f2(m(Ω)) where m(Ω)=|b−a| can serve as a sound indicator of the specificity of the information granule. The shorter the interval (the higher the value of f2(m(Ω))), the better the satisfaction of the specificity requirement. It is evident that two requirements identified above are in conflict: the increase in the values of the criterion of experimental evidence (justifiable) comes at an expense of a deterioration of the specificity of the information granule (specific). As usual, we are interested in forming a sound compromise between these requirements.Having these two criteria in mind, let us proceed with the detailed formation of the interval information granule. We start with a numeric representative of the set of data D around which the information granule Ω is created. A sound numeric representative of the data is its median, med(D). Recall that the median is a robust estimator of the sample and typically comes as one of the elements of D. Once the median has been determined, Ω (the interval [a, b]) is formed by specifying its lower and upper bounds, denoted here by “a” and “b”, respectively; refer also to Fig. 1.The determination of these bounds is realized independently. Let us concentrate on the optimization of the upper bound (b). The optimization of the lower bound (a) are carried out in an analogous fashion. For this part of the interval, the length of Ω or its nonincreasing function, as noted above. In the calculations of the cardinality of the information granule, we take into consideration the elements of D positioned to the right from the median, that is card{xk∈D| med(D)<xk≤b}. Again, in general, we can compute f1(card{xk∈D| med(D)<xk≤b}), where f1 is an increasing function. As the requirements of experimental evidence (justifiable granularity) and specificity (semantics) are in conflict, we can either resort ourselves to a certain version of multi-objective optimization and look at the resulting Pareto front or consider a maximization of the composite multiplicative index that is realized independently for the lower and upper bound of the interval, that is(1)V(b)=f1(card{xkD|med(D)<xk≤b})*f2(|med(D)−b|)(2)V(a)=f1(card{xk∈D|a≤xk<med(D)})*f2(|med(D)−a|)We obtain the optimal upper bound bopt, by maximizing the value of V(b), namely V(bopt)=maxb>med(D)V(b). In the same way, constructed is the lower bound of the information granule, aopt, that is V(aopt)=maxa<med(D)V(a).Among many possible design alternatives regarding functions f1 and f2, we consider them coming in the following forms(3)f1(u)=u(4)f2(u)=exp(−αu)where α is a positive parameter delivering some flexibility when optimizing the information granule Ω. Its essential role is to calibrate an impact of the specificity criterion on the constructed information granule. Note that if α=0 then f2(u)=1 and thus the criterion of specificity of information granule is completely ruled out (ignored). In this case, b=xmax with xmax being the largest element in D. Higher values of α stress the increasing importance of the specificity criterion. Sufficiently high values of α promote very confined, numeric-like information granules.As indicated, there is a substantial flexibility in defining f1 and f2. For instance a power function f1(u)=uβ, β>0 offers a higher level of flexibility in quantifying an impact of the first component on the values of the performance index (1)–(2).Alluding to (1), it could be worth noting that the expression links to the concept of quantiles [8] used in statistics but the construct investigated here becomes semantically richer. The first factor shown in this expression where f1(u)=u could be regarded as a mass of probability associated with the interval (as links to the property of the sufficient experimental evidence). The second factor underlines the need for the well-stressed semantics of the constructed information granule. In this way, we intend to capture two equally important features of the information granule being formed.In what follows, we present a series of numeric experiments, in which we use data come as finite collections of numeric data as well as those being drawn from some probability density functions. We concentrate only on the optimization of the upper bound of the interval (b).Finite sets of numeric dataNow let us consider an example involving a finite number of one-dimensional data D={3.1, 2.3, 1.7, 0.4, 1.9, 3.6, 4.0}. The median is equal to 2.3. The plots of the performance index are shown in Fig. 2. A clearly delineated maximum of V(b) is present. The form of this dependency is associated with the discrete values of the elements in D hence a collection of decreasing segments of V. The relationship is reflective of the distribution of the discrete data. The segments of the curve are of decreasing character as we are moving away from the discrete point (as the value of f1 is kept is unchanged however f2 decreases). The corresponding intervals for selected values of α are the following:α=0.0 [0.40 4.00] α=1.0 [1.69 3.61] α=1.5 [1.69 2.32] α=4.0 [1.88 2.32]Data with underlying probability density functionIf the data are governed by some probability density function (pdf), then the optimization criterion takes this into consideration leading to the following expressionV(b)=∫0bp(x)dx*exp(−αx). Note that here we use f1(u)=u. In the sequel we obtain the optimal value of the bound by plotting this performance index V(b) regarded as a function of “b”. An example of V(b) for several types of pdfs, namely uniform, Gaussian, and exponential ones is shown in Fig. 3. We note that with the increase of the values of α, the specificity of the resulting information granule becomes higher (namely, the interval tends to be narrow). For instance, in the case of the Gaussian distribution, we see that bopt=1.0 and bopt=0.5 for the values of α equal to 0.6 and 2.0, respectively.For comparison, we include the plots in which for the same pdfs we specify f1 to be in the form f1(u)=u2 and f1(u)=u0.5, see Figs. 4 and 5. Note that the optimal values of “b” are shifted toward higher values for f1(u)=u2 and lower values for f1(u)=u0.5.Another comprehensive view at the resulting construct can be obtained when plotting the values of f1 versus f2, see Fig. 6. It forms another presentation of the results contained in Fig. 4. For the lower values of α there is a section of the curve where f2 changes leaving f1 almost unchanged. The tendency changes with an increase of the values of α; a symmetrical relationship is observed for α=0.4, see Fig. 6(c).In spite of their fundamental differences, the concepts of shadowed sets [17] or rough sets [11] share some conceptual similarity in the sense that these information granules identify three regions in the universe of discourse: (a) full membership (complete belonginess), (b) full exclusion (lack of belonginess), and (c) ignorance about membership (no knowledge about membership of elements located in this region is available) shown as two shaded regions; see Fig. 7.Shadowed sets are isomorphic with three-valued logic and help model ignorance about membership of elements localized within certain regions of the universe of discourse.The objective function takes into account the different nature of the regions by bringing them into the overall expression using a certain weighting scheme, which looks differently at the region of full membership and ignorance. More specifically, we admit that the region characterizing a lack of membership knowledge should contribute to a different extent when counting elements falling within this region (this count has to be discounted). Likewise we discount the length of the information granule when considering this region. Considering the optimization of the bounds “b” and “db” (and effectively b+db) we have(5)V(b,db)=f1(card{xk∈D|med(D)≤xk≤b}+γcard{xk∈D|b<xk≤b+db})**f2(|med(D)−b|+γ|b+db−b|))where γ denotes a weight factor assuming values lower than 1; in particular we can set its value to ½. The maximization of (5) gives rise to the optimal values of “b” and “c” (=b+db) or alternatively “b” and “db”.As an example, we consider data governed by the uniform pdf over [−4,4] for which we form a shadowed set around the numeric representative equal to zero, see Fig. 8. The intent is to maximize V(b,db) with “b” and “db” being the parameters of the shadowed set. As before we optimize only the upper bound of the shadowed set with its two parameters. The plots of V treated as a function of “b” and “db” for selected values of γ=0.2 and 0.5 are shown in Fig. 8 with the optimal values of b=0.368, db=0.66 and b=0. 205, db=0.59.Alluding to the form of the maximized multiplicative objective function used in the optimization problem, it is insightful to elaborate on the choice of the numeric values of the parameter of this construct, namely α. Note that for α=0.0, we have f2(u) that is equal identically to 1 and only the first component of V(b) is used in the formation of the information granule. In this case the resulting interval consists all experimental data. When increasing the values of α, more attention is paid to the specificity of the resulting interval.As before let us concentrate on the data positioned to the right from the median of D and arrange them in an increasing order, x1<x2<….xMwhere M is the number of these data, see Fig. 9. In light of the increasing specificity for higher values of a, we request that the maximum of V(b) is attained for x1 and this occurs for all data in this ordered set of data. Considering the form of the maximized objective function we have a series of relationships resulting from the requirements imposed on the location of the maximum at x1,(6)f1(card{xk∈D|med(D)<xk≤x1})exp(−α|x1−med(D)|))>>f1(card{xk∈D|med(D)<xk≤x2})exp(−α|x2−med(D)|))f1(card{xk∈D|med(D)<xk≤x1})exp(−α|x1−med(D)|))>>f1(card{xk∈D|med(D<xk≤x3})exp(−α|x3−med(D)|))⋮f1(card{xk∈D|med(D)<xk≤x1})exp(−α|x1−med(D)|))>>f1(card{xk∈D|med(D)<xk≤xM-1})exp(−α|xM−1−med(D)|))For each of the above inequalities we determine minimal values of a which satisfy the individual inequalities shown above. Denote them by α1, α2, …, αM−1. Subsequently αmax is computed by taking the maximal value in this set, αmax=max{α1, α2, …, αM−1}.Note that in all the inequalities shown above we have card {xk∈D| med(D)<xk≤x1}=1 and in case of f1(u)=u, f1(1)=1 so the inequalities can be written down as(7)exp(−α|x1−med(D)|))>2exp(−α|x2−med(D)|))exp(−α|x1−med(D)|))>3exp(−α|x3−med(D)|))⋮exp(−α|x1−med(D)|))>(M−1)exp(−α|xM−1−med(D)|))The determination of the maximal value of α to be used in the construction of the lower bound of the interval is realized in the analogous manner. The maximal values of α could be different for the bounds and this fact is underlined by the use of the notation αmax(b) and αmax(a). All in all, let us underline that the range of the values of a directly links with the ranges of the values assumed by the bounds of the interval. For instance the upper bound (b) is positioned in-between x1 and xM.The above construct can be augmented to cope with situations where the individual data are associated with some weights (which could quantify their quality which may vary from one element to another). Given the data in the form (xi, wi) where the weights wiassume values located in the [0,1] interval, w=[w1, w2, …, wN] we reformulate the maximized performance index to be in the form(8)V(b)=f1∑k=1xk:med<xk<bNwkf2(|b−med(D,w)|)where med(D, w) is a weighted median (whose computing uses the weighted data). Recall that the weighted median is found as a solution to the minimization problem(9)med(D,w)=argminy∑k=1Nwk|xk−y|Fuzzy clustering such as e.g., Fuzzy C-Means (FCM) [4,20] forms a partition matrix U=[uik], i=1, 2, …, c; k=1, 2, …, N with “c” being the number of clusters and N denoting the number of data. Consider the i-th cluster for which the membership grades are shown as the i-th row of U. Those grades are organized in a single vector w, dim(w)=N. The formation of the interval information granule is completed for the individual coordinates of the data being clustered with the weights being the corresponding rows of the partition matrix.As an illustrative example, we consider two data sets shown in Fig. 10. There are two clusters in both of them however in the first data set the clusters are very condensed (visible) while in the second one there is a significant level of scattering.After running the FCM algorithm and clustering the data into c=2 clusters, the obtained prototypes are quite similar, v1=[0.51 0.57] v2=[2.00 1.96] for (a) and v1=[0.45 0.65] v2=[2.03 1.99] for (b). The obtained information granules (each data point is projected on x1 and x2 coordinates and weighed by the corresponding membership grade present in the partition matrix) are as follows (α=1.0)(a) x1 [0.398 0.71], [1.78 2.1] x2 [0.50 0.71], [1.88 1.96](b) x1 [0.199 0.91], [1.58 2.20] x2 [0.60 1.01], [1.59 2.20],It is worth stressing a key difference between clustering (say, fuzzy clustering) and the principle of justifiable granularity. In clustering we form a collection of information granules at the same time. The principle of justifiable granularity is concerned withy the formation of a single information granule – a situation with which clustering has nothing to do with. Nevertheless there is an interesting relationship between these two approaches in the sense that the results of clustering can be further described by forming a information granule and in this way characterize a cluster in a more comprehensive manner. As clustering applies broadly to real-world problems involving experimental data, one can anticipate a two-level description of structure; first by forming a collection of information granules and subsequently describing each cluster by the ensuing information granule.The parameter α used in the construction of the interval information granule plays a pivotal role in the formation of a fuzzy set as a justifiable information granule. As we have noted in Sections 2 and 3, there is a range of feasible values the parameter α can assume. The values αmax(a) and αmax(b) are linearly normalized to the [0,1] range, [0,αmax(a)]→[0,1] and [0,αmax(b)]→[0,1]. In light of the inclusion relationship (lower values of a produce broader intervals), by sweeping through the values of α in [0,1], we construct the corresponding intervals. In virtue of the representation theorem [13], these intervals give rise to a fuzzy set. More specifically, we have(10)Ω(x)=supα[min(α,χΩ(α)(x)]where χΩ(α) denotes a characteristic function of Ω(α) (viz. describing the interval obtained for the given value of α).The following example illustrates the essence of the development of the fuzzy set as an outcome of the use of the principle of justifiable granularity. As an illustrative brief example, consider a set of one-dimensional data X={2.5 1.95 0.20 2.00 −0.50 1.70 1.90 1.10 0.90 0.10 1.75 2.25 2.30 −0.65 −0.32}. Its median is 1.7. We arrange the data in an increasing order and then determine the maximal value of α, αmax. For the upper bound (b) of the interval we haveαmax(b)=max (4.620982, 5.493060, 5.545177, 3.218876, 3.257745)=5.545177.For the lower bound of the interval (a) we obtain the maximal value of αmax(a)=max (3.465735, 1.220680, 1.386294, 1.133407, 1.119850)=3.465735Using linearly normalized values of α, α*αmax(a) and α*αmax(b) we form at the family of intervals −α-cuts as belowα=0.1[0.0962.504]α=0.2[0.0962.300]α=0.3[0.8952.000]⋮α=0.9[0.8952.000]α=1 [1.1 1.75] (by default, for α=1, we form an interval based on the two data points being the closest to the median and positioned on its both sides, that is 1.1 and 1.75). The step-wise membership function A can be described in the following formA(x)=0.1ifx∈[0.0962.504]0.3ifx∈[0.8952.00]0.0ifx∈[0.0962.504]1ifx∈[1.11.75]Fuzzy sets are described by numeric membership functions. In contrast, probabilistic sets, type-2 fuzzy sets, interval-valued fuzzy sets or granular fuzzy sets, in general, generalize fuzzy sets. There is a significant conceptual departure: granular fuzzy sets with membership grades modeled as information granules are more in rapport with reality by departing from sometimes too demanding requirement of reliance on numeric membership grades. We can witness a great deal of studies devoted to type-2 fuzzy sets [5,6,9] (which are one of the visible realizations of granular fuzzy sets). There is a wealth of their applications, see [10]. Interestingly enough, these studies do not raise and solve a central problem of forming granular values of membership, which brings some difficulty at the application end of the spectrum. Let us note that having a collection of fuzzy sets of type-1 (viz. with numeric membership functions) defined in the same universe of discourse, by applying the principle of justifiable granularity we can easily determine the granular membership grades, say intervals thus forming an interval-valued fuzzy set.As an example, let us consider that for each element of the universe of discourse X={x1, x2, x3, x4} we are provided with 11 membership gradesx1{0.20.10.450.320.400.270.050.000.480.150.51}x2{0.200.400.450.600.340.400.500.320.290.100.25}x3{0.400.550.620.700.810.740.790.650.670.500.69}x4{1.000.950.860.930.950.700.750.680.600.910.90}Considering that α=0.6 the resulting interval-valued fuzzy set comes with the following lower and upper bounds [0.15 0.51] [0.29 0.50] [0.62 0.70] [0.68 0.95]. By forming a collection of intervals (α-cuts) for several values of α, we obtain a type 2 fuzzy set as shown in Table 1.The realized information granule (either in a form of an interval or a fuzzy set) exploits one-dimensional numeric data. The extension to the multidimensional case is straightforward and can be done by constructing a Cartesian product of the information granules formed for the individual variables (viz. the construct is realized for the individual variables of the multidimensional data). For instance, given an information granule A defined in X1, B arising at X2 and C at X3, the result is in the form A×B×C. In case of the interval information granules, we take the minimum of the successive characteristic functions (A×B×C) (x1, x2, x3)=min(A(x1), B(x2), C(x3)). Proceeding with the example presented in Section 6.1, we form the Cartesian product of the information granules producing “boxes” over the two-dimensional universe of discourse, which are viewed as granular prototypes.Now the advantage of these constructs is apparent: they offer another more comprehensive, in-depth view at the structure of the data that was not feasible in case of relying only on numeric prototypes obtained through fuzzy clustering. We see that character of the data sets in Fig. 11is very different yet the numeric prototypes are very close to each other and the granular prototypes help reveal the striking differences between the data. As the hyperboxes (boxes) depend on the values of a, we can choose a range of feasible values of α by allowing for the maximal value of α for which the resulting hyperboxes do not overlap in this multidimensional space. The hyperboxes themselves could be regarded as a granular signature of the underlying data, see [3,19].It is worth stressing that the principle of justifiable granularity covers a broad spectrum of scenarios-all of them can be arranged along the two coordinates. The first one is concerned with the formal environment of information granules. The second one points at the nature of available experimental evidence used in the realization of the information granule (sets, fuzzy sets, etc).In our considerations so far we have focused on the use of numeric evidence while constructed were interval like information granules and those expressed by fuzzy sets. The developed principle covers other cases; some modifications of the criteria pertinent to the specificity of the contemplated realization are to be envisioned. Information granules can be formed for the data which themselves are granular, say intervals, fuzzy sets, etc. The main flow of the construction is the same as before however the objective function needs to be carefully revisited to reflect the essence of the information granules one has to deal with. As the data, let us consider intervals, fuzzy sets, and probability density functions, see Fig. 12and look for the information granule described in terms of a certain interval.The second component (quantifying specificity of the information granule) of the performance index (1) remains the same. The first component of the objective function V is modified as follows (in the following formulas we are concerned with the upper bound of the interval):-for intervals(11)V(b)=f1∑i=1N∫XX(x)Xi(x)dx-for fuzzy sets(12)V(b)=f1∑i=1NPoss(X,Xi)where Poss(X, Xi) denotes a possibility measure [14] of X with regard to Xi, which expresses a degree of overlap of these two information granules. Proceeding with the detailed calculations, we have Poss(X, Xi)=supx[min(X), Xi(x))]-for probability density functions(13)V(b)=f1∑i=1N∫XX(x)pi(x)dxwhere pi(x) is a pdf of the i-th probabilistic information granule and the i-th integral is the expected value of X taken with respect to the i-th probabilistic information granule described by its pdf (which results in the probability of the event X).We can envision a hierarchical nature of forming information granules, which is of particular relevance when dealing with several sources of experimental evidence. For instance, one starts with a number of sources of numeric experimental evidence, for each of them constructs an information granule and subsequently develops an information granule on a basis of the granular experimental evidence. Schematically, we can describe the situation as follows R→G(R)→G2(R) where G2(R) stands for the information granule produced at the higher level of the hierarchy. The detailed illustration in case of interval information granules is illustrated in Fig. 13.

@&#CONCLUSIONS@&#
Granular computing is still a rapidly growing research area with a broad agenda calling for solid fundamentals associated with an efficient algorithmic layer. In this study, we have introduced a concept of justifiable granularity, which delivers a badly needed fundamental background by delivering a way of forming legitimate information granules.In virtue of the universal nature of the underlying concept, which is applicable to various formalisms of information granularity, the construction of information granules can be completed in various setups irrespectively from the format of the experimental evidence (which could be either numeric or granular). Likewise hierarchical structures of information granules are well supported.The developed framework provides an important, tangible, and applicable theoretical and algorithmic support to realize various ongoing investigations completed in the realm of fuzzy sets, especially the studies carried out for type-2 or interval-valued fuzzy sets. It is worth emphasizing that the systematic design of granular fuzzy sets (forming the pillar of constructs of type-2 fuzzy models including fuzzy controllers and classifiers) becomes crucial in this development context. Information granules facilitate interpretation of numeric findings; this effect is demonstrated in case of fuzzy clustering: we showed that granular prototypes (which are built on a basis of numeric prototypes and partition matrices) bring a more detailed and complete insight into the results produced by fuzzy clustering.Further investigations, which are worth pursuing could focus on the re-visiting the concepts of justifiable information granularity from the perspective of mereology and rough mereology and its formal setting [13,21,23] where the available experimental evidence (numeric of granular) could be investigated as a problem of synthesis of complex information granules based on the collection of elementary objects (information granules).It could be of interest to investigate the minimum description length (MDL) principle in guiding a construction of information granules in case when encountering their realization involving a number of parameters. There is also another important direction in system modeling, where the principle of information granularity can contribute to the quantification of performance (quality) of the numeric model. For instance, in collaborative modeling (where each model is treated as a source of knowledge producing a numeric outcome for a given input), these numeric outcomes can be looked at globally and represented as a single information granules and quantifying in this way an existing diversity of sources of knowledge involved.