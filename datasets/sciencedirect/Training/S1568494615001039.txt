@&#MAIN-TITLE@&#
A framework for multi-document abstractive summarization based on semantic role labelling

@&#HIGHLIGHTS@&#
We have proposed a framework for multi-document abstractive summarization based on semantic role labeling (SRL). To the best of our knowledge, SRL has not been employed for abstractive summarization.The integration of genetic algorithm with SRL based framework for abstractive summarization results gives improved summarization results.My study focus on two highlights and discussion is based on these two highlights.

@&#KEYPHRASES@&#
Abstractive summary,Semantic role labeling,Semantic similarity measure,Language generation,Genetic algorithm,

@&#ABSTRACT@&#
We propose a framework for abstractive summarization of multi-documents, which aims to select contents of summary not from the source document sentences but from the semantic representation of the source documents. In this framework, contents of the source documents are represented by predicate argument structures by employing semantic role labeling. Content selection for summary is made by ranking the predicate argument structures based on optimized features, and using language generation for generating sentences from predicate argument structures. Our proposed framework differs from other abstractive summarization approaches in a few aspects. First, it employs semantic role labeling for semantic representation of text. Secondly, it analyzes the source text semantically by utilizing semantic similarity measure in order to cluster semantically similar predicate argument structures across the text; and finally it ranks the predicate argument structures based on features weighted by genetic algorithm (GA). Experiment of this study is carried out using DUC-2002, a standard corpus for text summarization. Results indicate that the proposed approach performs better than other summarization systems.

@&#INTRODUCTION@&#
The information on Web is growing at exponential pace. In the current era of information overload, multi-document summarization is an essential tool that creates a condensed summary while preserving the important contents of the source documents. The automatic multi-document summarization of text is a major task in the field of natural language processing (NLP) and has gained more consideration in recent years [1]. One of the problems of information overload is that many documents share similar topics, which creates both difficulties and opportunities for natural language systems. On one hand, the similar information conveyed by several different documents, causes difficulties for the end users, as they have to read the same information repeatedly. On the other side, such redundancy can be used to identify accurate and significant information for applications such as summarization and question answering. Thus, summaries that synthesize common information across many text documents would be useful for users and reduce their time for finding the key information in the text documents. Such a summary would significantly help users interested in single event described in many news documents [1]. In this paper, we propose a framework that will automatically fuse similar information across multiple documents and use language generation to produce a concise abstractive summary.Two approaches are employed to multi-document summarization: extractive and abstractive. Most of the studies have focused on extractive summarization, using techniques of sentence extraction [2], statistical analysis [3], discourse structures and various machine learning techniques [4]. On other hand, abstractive summarization is a challenging area and dream of researchers [5], because it requires deeper analysis of the text and has the capability to synthesize a compressed version of the original sentence or may compose a novel sentence not present in the original source. The goal of abstractive summarization is to improve the focus of summary, reduce its redundancy and keeps a good compression rate [6].Past literature shows that there have been a few research efforts made toward abstractive summarization. Many researchers have tried to generate abstractive summaries using various methods. These abstractive methods can be grouped into two categories: Linguistic (Syntactic) based approach and Semantic based approach. Linguistic (Syntactic) based approach employs syntactic parser to analyze and represent the text syntactically. Usually, in this approach, verbs and nouns identified by syntactic parser are used for text representation and further processed to generate the abstractive summary. On other hand, semantic based approach aims to produce abstractive summary from semantic representation of document text. Different semantic representations of text used in the literature are ontology based and template based representation. Titov and Klementiev [7] made a distinction between syntactic and semantic representation of sentence. They addressed that syntactic analysis is far away from representing the meaning of sentences. In particular, syntactic analysis does not define who did what to whom (and how, when…).All the linguistic based approaches [1,6,8,9] proposed for the abstractive summarization rely on the syntactic representation of source document. These approaches employ syntactic parser to represent the source text syntactically. The major limitations of these approaches is the lack of semantic representation of source text. Since abstractive summarization requires deep analysis of text, therefore, semantic representation of source text will be a more suitable representation.On other hand, a few semantic based approaches have also been proposed for abstractive summarization and are briefly discussed as follows. A multi-document summarization system, GISTEXTER, presented in [10] exploits template based method to produce abstractive summary from multiple newswire/newspaper documents depending on the output of the information extraction (IE) system. This approach use template for topic representation of document. The major limitation observed in this approach was that linguistic patterns and extraction rules for template slots were manually created by humans, which is time consuming. Moreover, this method could not handle or capture the information about similarities and differences across multiple documents.A fuzzy ontology based approach [11] was proposed for Chinese news summarization to model uncertain information and hence can better describe the domain knowledge. This approach has several limitations. First, domain ontology and Chinese dictionary has to be defined by a domain expert which is time consuming. Secondly, this approach is limited to Chinese news, and might not be applicable to English news.The methodology proposed in [12] generates short and well-written abstractive summaries from clusters of news articles on same event using abstraction schemes. The abstraction scheme uses a rule based information extraction module, content selection heuristics and one or more patterns for sentence generation. The drawback of the methodology was that information extraction (IE) rules and generation patterns were written by hand, which was extremely time consuming.A framework proposed by [13] generates abstractive summary from a semantic model of a multimodal document. The semantic model is constructed using knowledge representation based on objects (concepts) organized by ontology. The important concepts represented by semantic model are rated based on information density metric and expressed as sentences using the available phrasings stored for each concept in a semantic model. The limitation of this framework was that it relies on manually built ontology, which is time consuming. Secondly, it is manually evaluated by humans i.e. human judges were asked to assess the quality of system summary by comparing it with other summary generated by traditional extraction methods.The abstractive approach presented by [14] summarizes a document by creating a Rich Semantic Graph (RSG) for the source document. Rich semantic graph is an ontology based representation i.e. graph nodes are the instances of ontology noun and verb classes. Like other approaches, the limitation of this approach was that it also relies on manually built ontology, which is time consuming.The major limitation of all the semantic based approaches for abstractive summarization is that they are mostly dependent on human expert to construct domain ontology and rules; which is a drawback for an automatic summarization system. Our work, in contrast, aims to treat this limitation by using semantic role labeling (SRL) technique to build semantic representation from the document text automatically.SRL has been widely applied in text content analysis tasks such as text retrieval [15], information extraction [16], text categorization [17] and sentiment analysis [18]. In the area of text summarization, [19] introduced a work that combined semantic role labeling with general statistic method (GSM) to determine important sentences for single document extractive summary. At first, they employed SRL and semantic similarity measure to compute the sentence similarity score. Secondly, the general statistic method was used to computes the sentence score based on features, without taking into account their weights. Finally, the sentence scores obtained from both methods are combined to assign the overall score to each sentence in the document and the top ranked sentences are extracted according to 20% compression rate. However, our work is different from [19] in the following manner. We focus on multi-document abstractive summarization while [19] focus on single document extractive summarization. We employ SRL in second phase of the framework, for semantic representation of text in the document collection. On other hand, [19] employed SRL and semantic similarity measure to compute sentence semantic similarity score. To the best of our knowledge, semantic role labeling (SRL) technique, which exploits semantic role parser, has not been employed for the semantic representation of text in multi-document abstractive summarization.Therefore, this study proposes a framework that will employ SRL for semantic representation of text in order to generate a good abstractive summary. The framework for multi-document abstractive summarization presented in this study, is different from previous abstractive summarization approaches in a few aspects. First, it employs semantic role labeling to extract predicate argument structure (semantic representation) from the contents of input documents. Secondly, it analyzes the text semantically by utilizing semantic similarity measure in order to cluster semantically similar predicate argument structures across the text; and finally it ranks the predicate argument structures based on the features weighted and optimized by genetic algorithm; since text features are sensitive to the quality of the generated summary.The rest of this paper is organized as follows: Section 2 outlines the proposed framework. The evaluation of the framework is given in Section 3. Finally we end with conclusion in Section 4.The framework of our proposed approach is illustrated in Fig. 1. Given a document collection that need to be summarized, first of all, we split the document collection into sentences in such a way that each sentence is preceded by its corresponding document number and sentence position number. Next, SENNA semantic role labeler [20] is employed to extract predicate argument structure from each sentence in the document collection. In semantic similarity matrix computation phase (as discussed in Section 2.3), the similarities of predicate argument structures (PASs) are computed by comparing them pair wise based on Jiang's semantic similarity measure [21] and edit distance algorithm. Once the similarity matrix for PASs is obtained, we perform agglomerative hierarchical clustering (HAC) algorithm based on average linkage method to cluster semantically similar predicate argument structures. The number of clusters will depend on compression rate of summary. Section 2.4 will describe this phase in detail. The PASs in each cluster are scored based on features, weighted and optimized by genetic algorithm and the top ranked predicate argument structures are selected from each cluster (as described in Section 2.5). Finally, the SimpleNLG realisation engine [22] is employed to generate sentences from the selected predicate argument structures. The generated sentences will form the final abstractive summary (as discussed in Section 2.6).The aim of semantic role labeling (SRL) is to determine the syntactic constituents/arguments of a sentence with respect to the sentence predicates, identify the semantic roles of the arguments such as Agent, Patient and Instrument, and the adjunctive arguments of the predicate such as Locative, Temporal and Manner [23]. The primary task of SRL is to identify what semantic relation a predicate holds with its participants/constituents. As abstractive summarization requires deeper semantic analysis of text, therefore, this study employs semantic role labeling to extract predicate argument structure from sentences in the document collection. The framework uses SENNA [20] tool distributed under open source and non-commercial license, and yields a host of natural language processing (NLP) predictions: semantic role labeling (SRL), part-of-speech (POS) tags, named entity recognition (NER) and chunking (CHK). We employ SENNA tool in our framework for SRL, POS tags and NER.At first, we decompose the document collection into sentences in such a way that each sentence is preceded by its corresponding document number and sentence position number. Next, SENNA semantic role labeler is employed to parse each sentence and properly labels the semantic word phrases. These phrases are referred to as semantic arguments. The semantic arguments can be grouped in two categories: core arguments (Arg) and adjunctive arguments (ArgM) as shown in Table 1. In this study, we consider A0 for subject, A1 for object, A2 for indirect object as core arguments, and ArgM-LOC for location, ArgM-TMP for time as adjunctive arguments for predicate (Verb) V. We consider all the complete predicates associated with the single sentence structure in order to avoid loss of important terms contributing to the meaning of sentence, and the actual predicate of the sentence. We assume that predicates are complete if they have at least two semantic arguments. The extracted predicate argument structure is used as semantic representation for each sentence in the document collection. A sentence containing one predicate is represented by simple predicate argument structure while a sentence containing more than one complete predicate is represented by a composite predicate argument as illustrated below.Example 1Consider sentence S: Tropical Storm Gilbert formed in the eastern Caribbean and strengthened into a hurricane Saturday night.After applying semantic role labeling to sentence S, the corresponding two predicate argument structures are obtained as follows:P1: [A0: Tropical Storm Gilbert] [V: form] [ArgM-LOC: in the eastern Caribbean]P2: [A0: Tropical Storm Gilbert] [V: strengthen] [A2: into a hurricane Saturday night]Both predicate argument structures P1 and P2 are associated with a single sentence S and hence the sentence S is represented by a composite (more than one) predicate argument structure. Both incomplete predicate argument structures (PASs) and the PASs that are nested in a larger predicate argument structure are ignored.Example 2Consider the following two sentences represented by simple predicate argument structures.S1: Eventually, a huge cyclone hit the entrance of my house.S2: Finally, a massive hurricane attack my homeThe corresponding simple predicate argument structures P1 and P2 are obtained after applying semantic role labeling to sentences S1 and S2:P1: [AM-TMP: Eventually] [A0: a huge cyclone] [V: hit] [A1: the entrance of my house]P2: [AM-DIS: Finally] [A0: a massive hurricane] [V: attack] [A1: my home]Once the predicate argument structures (PASs) are obtained, they are split into meaningful words or tokens, followed by removal of stop words. The words in PASs are stemmed to their base form using porter stemming algorithm [24]. Next, SENNA POS tagger [20] is employed to label each term of semantic arguments (associated with the predicates), with part of speech (POS) tags or grammatical roles. The POS tags NN stands for noun, V for verb, JJ for adjective and RB for adverb, etc. This step is required, as semantic arguments of the predicates will be compared based on grammatical roles of the terms. In this study, we compare only terms of the semantic arguments of the predicates which are labeled as noun (NN) and the rest are ignored as discussed in Section 2.3. After POS tagging, the two predicate argument structures P1 and P2 in example 2 are as follows:P1: [A0: a massive (JJ) hurricane NN] [V: attack] [A1: my home (NN)]P2: [AM-TMP: Eventually (RB)][A0: a huge (JJ) cyclone (NN)] [VBD: hit][A1: the entrance (NN) of my house (NN)]We also employ SENNA NER [20] to identify named entities such as person names (cabral), and organization names (civil defense) in the semantic arguments of predicate. These named entities are stored for each predicate argument structure (PAS) and is required in later phase for scoring the PAS based on proper noun feature.This study compares predicate argument structures based on noun–noun, verb–verb, location–location and time–time arguments. Therefore, the framework extracts only tokens from predicate argument structure, which are labeled as noun, verb, location, and time as identified in previous steps. All the PASs associated with the sentence will be included in comparison. Once the nouns, verbs, and other arguments (time and location) if exist, are extracted, the predicate argument structures obtained in example 2 after further processing will becomeP1: [A0: hurricane NN] [V:attack] [A1: home (NN)]P2: [AM-TMP: Eventually (RB)] [A0: cyclone (NN)] [VBD: hit] [A1: entrance (NN), house (NN)]In next phase, the noun ‘home’ in the semantic argument A1 of PAS P1 will be compared with both of the nouns ‘entrance’ and ‘house’ in the semantic argument A1 of PAS P2. The temporal (time) semantic argument ‘Eventually’ in P2 is skipped from comparison as there is no corresponding temporal argument in P1.The objective of this phase is to build matrix of semantic similarity scores for each pair of predicate argument structure. In this phase, similarity of the predicate argument structures (PASs) is computed pair wise based on acceptable comparisons of noun–noun, verb–verb, location–location and time–time. Based on experimental results in the literature [25], Jiang and Conrath measure has the closest correlation with human judgment amongst all the semantic similarity measures. Therefore, this study exploits Jiang's semantic similarity measure [21] for computing semantic similarity between each pair of PASs. Jiang's measure is information content based measure and consider that each concept in the WordNet [26] hold certain information. According to this measure, the similarity of two concepts is dependent on the information that the two concepts share.Given two sentences SiandSj, the similarity score between predicate argument structure (PAS) k of sentence Si (vik) and PAS l of sentence Sj (vjl) is determined using Eq. (5), wheresimp(vik,vjl)is the similarity between predicates (verbs) determined using Eq. (2),simarg(vik,vjl)is the sum of similarities between the corresponding arguments of the predicates determined using Eq. (1). Both equations (1) and (2) exploit Jiang's semantic similarity measure for computing similarity between noun terms in the semantic arguments of the predicate argument structures and the verbs of predicate argument structures, respectively. Similarity between corresponding temporal arguments i.e.simtmp(vik,vjl)is computed using Eq. (3) and the similarity between corresponding location arguments i.e.simloc(vik,vjl)is calculated using Eq. (4). Since Jiang's measure is based on WordNet, the temporal and location arguments may not be found in the WordNet, therefore we use edit distance algorithm instead of Jiang's measure in Eqs. (3) and (4) for computing possible match/similarity between temporal and location arguments of the predicates.The similarity score between the two predicate argument structures is computed using Eqs. (1)–(5).(1)simarg(vik,vjl)=sim(A0i,A0j)+sim(A1i,A1j)+sim(A2i,A2j)(2)simp(vik,vjl)=(sim(Pi,Pj))(3)simtmp(vik,vjl)=(sim(Tmpi,Tmpj))(4)simloc(vik,vjl)=(sim(Loci,Locj))Eqs. (1)–(4) are combined to give Eq. (5) as follows(5)sim(vik,vjl)=simp(vik,vjl)+[simarg(vik,vjl)+simtmp(vik,vjl)+simloc(vik,vjl)]The semantic similarity computation of the two predicate argument structures discussed in example 2 is depicted in Fig. 2.In order to compute the similarity score of the given two terms/concepts in the semantic argument ‘A0’ of predicates P1 and P2; let concept C1=“hurricane” and concept C2=“cyclone” as shown in Fig. 2. First, Jiang's measure [21] uses WordNet to compute the least common subsumer (lso) of two concepts, then determines IC(C1), IC(C2), andIC(lso(C1, C2)). The information content (IC) of concept is achieved by determining the probability of occurrence of a concept in a large text corpus and quantified as follows:(6)IC(C)=−logP(C)where P(C) is the probability of occurrence of concept ‘C’ and is computed as follows:(7)P(C)=Freq(C)Nwhere Freq (C) is the number of occurrences of concept ‘C’ in the taxonomy and N is the maximum number of nouns.Jiang's measure [21] calculates the semantic distance to obtain semantic similarity between any two concepts as follows:(8)Jiangdist(C1,C2)=IC(C1)+IC(C2)−2×IC(lso(C1,C2))Based on WordNet, Jiang’ measure determines ‘cylone’ as a least common subsumer of the given two concepts ‘hurricane’ and ‘cyclone’ as shown below.<entity<physical_entity<process<phenomenon<natural_phenomenon<physical_phenomenon<atmospheric_phenomenon<storm<windstorm<cyclone_<entity<physical_entity<process<phenomenon<natural_phenomenon<physical_phenomenon<atmospheric_phenomenon<storm<windstorm<cyclone_<hurricanelso(C1,C2)=CycloneIC(C1)=11.0726IC(C2)=10.6671IC(lso(C1,C2))=10.6671According to Eq. (8), the similarity between concepts C1=“Hurricane” and C2=“Cyclone” is computed as follows:Jiangdist=11.0726+10.6671−2×10.6671=0.4055The similarity of other concepts/terms is determined in the same manner. However,simtmp(vik,vjl)andsimloc(vik,vjl)are set 0 as there are no temporal and location arguments for comparison in both predicate argument structures. According to Eq. (5), the similarity score of the given two predicate argument structures is computed as follows:sim(vik,vjl)=0.8571+[0.4055+0.29+1+0+0]sim(vik,vjl)=2.5526In order to normalize the result in the range of [0,1], we use a scaling factore−αSim(vik,vjl)[27], Where α is constant set to 0.05 (optimal value).=e−0.05X2.5526So, the above result will become assim(vik,vjl)=0.8801Once the semantic similarity score for each pair of predicate argument structure (PAS) is achieved, then semantic similarity matrix Mi,jis constructed from the similarity scores of the predicate argument structure Pi and Pj. Mi,jis defined as follows:(9)Mi,j=Msim(Pi,Pj)ifi≠j0otherwisewhere Msim(Pi, Pj) refers to semantic similarity score of predicate argument structure Pi and Pj in the Matrix Mi,j.Agglomerative hierarchical clustering is well-known technique in the hierarchical clustering method, which is quite old but has been found useful in the range of applications [28]. There are five well-known linkage methods of agglomerative hierarchical clustering (HAC) [29] i.e. single linkage, complete linkage, average linkage, ward and centroid method. Based on different measures (Entropy and F-Score and Kendall W test), it was found from the literature studies in [30–32] that average linkage is the most suitable method for document clustering. Therefore, this study exploits HAC algorithm based on average linkage method. This phase takes semantic similarity matrix as input from previous phase in which the value at position (i, j) is the semantic similarity between ith and jth predicate argument structures. We consider the value at position (i, j) in the semantic similarity matrix as semantic similarity between ith and jth clusters, assuming that the construction of similarity matrix begins with each predicate argument structure as a single cluster. The pseudo code for clustering similar predicate argument structures is given below.Pseudo code for agglomerative clustering algorithmInput: Semantic Similarity MatrixOutput: Clusters of similar predicate argument structuresa. Merge the two clusters that are most similarb. Update the semantic similarity matrix to represent the pair wise similarity between the newest cluster and the original cluster based on average linkage methodc. Repeat step 1 and 2 until the compression rate of summary is reachedIn this study, we consider 20% compression rate of summary.The aim of this phase is to select high ranked predicate argument structures from each cluster based on features weighted and optimized by genetic algorithm (GA). At first, all the sentences in the document collection are represented by corresponding predicate argument structure (PAS) extracted through SENNA SRL. The following features are extracted for each predicate argument structure (PAS) in the document collection and hence each PAS is represented by a vector representing the weight of these features P={P_F1, P_F2,…P_F10}.This feature is determined by counting the number of matching contents words in the predicate argument structure (PAS) and the title of a document [33].(10)P_F1=Number of title words in PASNumber of words in document titleWe use the normalize length of the PAS, which is the ratio of number of words in the PAS over the number of words in the longest PAS of the document [2].(11)P_F2=Number of words occuring in the PASNumber of words occuring in the longest PASFor each predicate argument structure P, the semantic similarity between P and other predicate argument structures in the document collection is computed using Eq. (5). Once the similarity score for each predicate argument structure (PAS) is achieved, then the score of this feature is obtained by computing the ratio of sum of similarities of PAS P with all other PASs over the maximum of summary in the document collection [34].(12)P_F3=∑sim(Pi,Pj)Max∑(sim(Pi,Pj))Position of PAS [33] gives importance of the PAS in the text document and is equivalent to position of sentence from which PAS is extracted. Consider 10 sentences in the document, the score of position feature is 10/10 for the first sentence, 9/10 for the second sentence and so on. The score of this feature is computed as follows:(13)P_F4=Length of document−PAS Position+1Length of documentThe predicate argument structure that contains more proper nouns is considered as significant for inclusion in summary generation. This feature identifies proper nouns as words beginning with a capital letter. The score of this feature is computed as the ratio of the number of proper nouns in the PAS over the length of PAS [33]. Length of PAS is the number of words/terms in the PAS.(14)P_F5=Number of proper nouns in the PASLength of PASThe predicate argument structure containing numerical data such as number of people killed, is regarded as important for inclusion in summary generation. The score for this feature is calculated as the number of numerical data in the PAS over the length of the PAS [33].(15)P_F6=Number of numerical data in the PASLength of PASSome sentences may have more than one predicate argument structure associated with them, represented by a composite predicate argument structure and considered important for summary. The score of this feature [34] is computed as follows:(16)P_F7=Total number of nouns and verbs in the PASLength of PASThe predicates argument structure containing time and date information for an event is considered as important for summary generation. The score of this feature is computed as ratio of the number of temporal information (time and date) in the PAS over the length of PAS [35].(17)P_F8=NumberoftemporalinformationinthePASLengthofPASFrequent terms are most probably related to the topic of document and in this study we consider top 10 as maximum number of frequent semantic terms. Nouns and verbs are considered as frequent semantic terms in the predicate argument structure. The score of this feature is calculated as the ratio of number of frequent semantic terms in the PAS over the maximum number of frequent semantic terms [23].(18)P_F9=NumberoffrequentsemantictermsinthePASMax(Numberoffrequentsemanticterms)The score of important term Wican be determined by the TF-IDF method [36]. We apply TF-IDF method to the predicate argument structures in the document collection and consider the term weights for semantic terms i.e. nouns and verbs in the predicate argument structure.The weight of semantic term is calculated as follows:(19)Wi=TfixIdfi=TfixlogNniwhere Tfiis the term frequency of the semantic term i in the document, N is the total number of documents, and ni is the number of documents in which the term i occurs. This feature is computed as the ratio of sum of weights of all semantic terms in the PAS over the maximum summary of the term weights of PAS in the document collection [33].(20)P_F10=∑ki=1Wi(P)Max(∑ki=1Wi(P))where k is the number of semantic terms in the PAS P.In automatic text summarization, the quality of the summary is sensitive to the text features i.e. not all features have same relevance with respect to summary. Therefore, feature weighting is crucial for summary generation. In this work, we will employ genetic learning algorithm (GA) to obtain optimal feature weights for ranking predicate argument structures for summary generation. GA is chosen since it is a robust and a well-known adaptive optimization technique used in various fields of research and applications [37]. This particular GA based experiment is evaluated against multi-documents in DUC 2002 data set [38]. The GA procedure is shown in Fig. 3.In the first step of GA, a real valued population consisting of 50 chromosomes is randomly produced. The chromosomes are randomly initialized with values between 0 and 1. Each individual chromosome represents weights of features in the form of (w1,w2,…,w10). To produce a random population of N chromosomes, we have for instance:Population=0.250.080.740.530.260.320.420.730.210.62←Chromosome10.770.670.030.220.310.010.020.730.510.42←Chromosome2⋮0.540.870.520.450.330.710.390.490.830.18←ChromosomeNwhere each row represents the chromosome and each column value correspond to the weight of each feature.The fitness evaluation function in GA determines the best chromosome in the population based on its fitness value. The chromosome having highest fitness value has more chances to survive and continue in next generation.We define fitness function F(x) as the average recall obtained with each chromosome when summarization process is applied on multi-documents and is given in Eq. (21).(21)F(x)=∑S∈{ReferenceSummaries}∑gramn∈SCountmatch(gramn)∑S∈{ReferenceSummaries}∑gramn∈SCount(gramn)where n is the length of the n-gram, Countmatchis the number of n-grams shared between system generated summary and reference summaries, and Count is the total number of n-grams in the reference summaries.This operation determines the chromosome that will continue to survive in next generation. Stochastic universal sampling (SUS) method [39] has been used to select parents from the current population. This method uses M equally spaced steps in the range [0,Sum], where M is the number of required selections and Sum is the sum of the scaled fitness values over all the individual chromosomes in the current population.These operations describe how GA creates its next generation. In this study, we use two reproduction operations i.e. cross over and mutation. The cross over operation produces new chromosomes from the two parent chromosome by exchanging information in the parent chromosomes. Mutation operation is performed by altering gene of chromosome by another to generate a new genetic structure.This process will continue to produce new generations and evaluate its fitness until maximum limit of generations is reached. In this study, we used 100 maximum generations. When the process finishes, the individual chromosome with the best fitness value is chosen as the optimal feature weights. The obtained optimal feature weights are given in Section 3.2. Once the feature weights are obtained, then Eq. (22) is used to determine the score of predicate argument structures in each cluster.(22)Score(Pi)=∑k=110WkxP_Fk(Pi)where P_Fk(Pi) is score of feature k for predicate Pi, Wkis the weight of feature k. The top scored predicate argument structures are selected from each cluster and are given as input to the summary generation process in next phase.This phase demonstrates how arguments of the predicates will be combined, transformed and realize them as summary sentences. This phase takes top scored predicate argument structures (PASs) from previous phase, employ SimpleNLG [22] and a simple heuristic rule implemented in SimpleNLG, to generate summary sentences from PASs. The heuristic rule states that if the subjects in the predicate argument structures (PASs) refer to the same entity, then merge the predicate argument structures by removing the subject in all PASs except the first one, separating them by a comma (if there exist more than two PASs) and then combine them using connective “and”.SimpleNLG is an English realisation engine which provides simple interfaces to produce syntactical structures and transform them into sentences using simple grammar rules. The realisation engine has excellent coverage of English syntax and morphology, and at the same time offers API to users, which aim to provide a programming control over the realisation process.Moreover, the significant advantage of this engine is its robustness i.e. the engine will not crash when the input syntactical structures are incomplete or ill-formed but typically will yield an inappropriate output. The process of SimpleNLG engine consists of three main steps as depicted in Fig. 4.The first step in SimpleNLG engine requires defining the basic syntactic constituents that will construct a sentence. The second step assigns features to the constituents. The different features that can be set for constituents are: subject, object verb, tense, etc. In the last step, the SimpleNLG realiser combines the syntactic constituents using simple grammar rules and realize them as syntactically correct sentence.As discussed in Section 2.2, we consider specific arguments i.e. A0 for subject, A1 for object, A2 for indirect object as core arguments, and ArgM-LOC for location, ArgM-TMP for time as adjunctive arguments for predicate (Verb) V while the rest of the arguments are ignored. Thus, the final summary sentences generated from the predicate argument structures will be compressed version of the original source sentences in most cases. During summary sentence generation process through SimpleNLG, the simple heuristic rule implemented in SimpleNLG combine the predicate argument structures that refer to the same subject (entity). The following example demonstrates how we generate abstractive summary from the given source input sentences. For instance, the following source input sentences:S1: Hurricane Gilbert claimed to be the most intense storm on record in terms of barometric pressure.S2: Hurricane Gilbert slammed into Kingston on Monday with torrential rains and 115 mph winds.S3: Hurricane Gilbert ripped roofs off homes and buildings.After applying SENNA SRL, the corresponding three predicate argument structures P1, P2 and P3 are obtained as follows:P1: [A0: Hurricane Gilbert] [V: claimed] [A1: to be the most intense storm on record]P2: [A0: Hurricane Gilbert] [V: slammed] [A1: into Kingston] [AM-TMP: on Monday]P3: [A0: Hurricane Gilbert] [V: ripped] [A1:roofs off homes and buildings]We assume that P1, P2 and P3 are the top scored predicate argument structures selected from different clusters in previous step. According to the rule stated above, the subject A0 is identified as repeated in the above example and is eliminated from all predicate argument structures except the first one. The SimpleNLG applies the heuristic rule on the above three predicate argument structure and form the summary sentence that is compression version of the original source sentences.Hurricane Gilbert claimed to be the most intense storm on record, slammed into Kingston on Monday and ripped roofs off homes and buildings.The next example demonstrates how the SimpleNLG compresses a single sentence from constituents (semantic arguments) of the sentence predicate.For instance the source sentence “Floods prevented officials from reaching the hotel zone in Cancun and there were no relief efforts under way by late Wednesday”.After applying SENNA SRL, the corresponding two predicate argument structures P1 and P2 obtained from the above sentence are as follows:P1: [A0: Floods] [V: prevented] [A1: officials] [A2: from reaching the hotel zone in Cancun]P2: [A0: officials] [V: reaching] [A2: the hotel zone in Cancu]The second predicate argument structure P2 is ignored as it is subsumed by the first predicate argument structure P1. Hence, the input source sentence is represented by one representative predicate argument structure P1 as discussed in Section 2.2. The SimpleNLG will take the semantic arguments (A0, A1, A2) of predicate (V: prevented) in P1 as constituents to form the resultant sentence as shown in the sample code below.Sample Code Listing1.Lexicon lexicon = Lexicon.getDefaultlexicon();NLG Factory nlg Factory = new NLG Factory(lexicon);Realiser realiser = new Realiser (lexicon);SPhraseSpec p=nlgFactory.createClause();p.setSubject (“Floods”);p.setVerb (“prevented”);p.setObject (“officials”);p.setIndirect Object (“from reaching the hotel zone in Cancun”);String output=realiser.realsieSentence(p);The stepwise detail of the above code is as follows:Step 1. Simple NLG comes with a built in lexicon that contain information (part of speech) about words, and accessed via:Lexicon lexicon=Lexicon.getDefaultLexicon();Step 2. Once we get a lexicon, we can create an NLGFactory object to create SimpleNLG structures.NLGFactory nlgFactory=new NLGFactory(lexicon);Step 3. This step creates a realiser object that will transform/combine SimpleNLG structures (constituents of sentence) into text.Realiser realiser=new Realiser(lexicon);Step 4: This step creates a simple NLG construct SPhraseSpec by calling a method of nlgFactory object, which allows us to define the syntactic constituents that will form a sentence.SPhraseSpec p=nlgFactory.createClause();Step 5: This step defines the constituent “Floods” along with the specification of subject feature.p.setSubject(“Floods”);Step 6: This step defines the constituent “prevented” along with the specification of verb feature.p.setVerb(“prevented”);Step 7: This step defines the constituent “officials” along with the specified object feature.p.setObject(“officials”);Step 8: This step defines the constituent “from reaching the hotel zone in Cancun” along with the specified indirect object feature.p.setIndirectObject(“from reaching the hotel zone in Cancun”);Step 9: This step uses realiser object created in step3, which takes different constituents stored in object p obtained in steps 5,6 7 and 8, assemble them using simple grammar rules embodied in simple NLG, and finally the sentence is realized to make the result syntactically and morphologically correct:String output=realiser.realiseSentence(p);The output variable will contain the generated sentence i.e.Floods prevented officials from reaching the hotel zone in Cancun.Once the summary sentences are generated from predicate argument structures, they are re-arranged based on the position of predicate argument structures. The position of predicate argument structure (PAS) is determined from the position of source sentence in the document collection, from which this PAS is extracted. In case, the summary sentence is generated from more than one predicate argument structures then its probable position in the summary is determined based on the position of the first predicate argument structure.

@&#CONCLUSIONS@&#
