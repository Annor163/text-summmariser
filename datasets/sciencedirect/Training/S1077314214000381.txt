@&#MAIN-TITLE@&#
An integrated neuromimetic architecture for direct motion interpretation in the log-polar domain

@&#HIGHLIGHTS@&#
A bio-inspired vision system for motion interpretation in cortical domain.A population of motion energy neurons for the computation of optic flow.Relationships between the log-polar and Cartesian affine description of optic flow.An architecture that exploits multi-core CPU and GPU for real-time processing.Benchmarking on synthetic and real-world sequences.

@&#KEYPHRASES@&#
Bio-inspired vision systems,Space-variant mapping,Cortical representation,Optic flow,Population coding,Affine description,Time-to-contact,Surface orientation,

@&#ABSTRACT@&#
A hierarchical vision system, inspired by the functional architecture of the cortical motion pathway, to provide motion interpretation and to guide real-time actions in the real-world, is proposed. Such a neuromimetic architecture exploits (i) log-polar mapping for data reduction, (ii) a population of motion energy neurons to compute the optic flow, and (iii) a population of adaptive templates in the cortical domain to gain the flow’s affine description. The time-to-contact and the surface orientations of points of interest in the real-world are computed by directly combining the linear description of the cortical flow. The approach is validated through quantitative tests in synthetic environments, and in real-world automotive and robotics situations.

@&#INTRODUCTION@&#
Motion interpretation aims to relate spatio-temporal variations of the image sequences to motion events in the 3D space for gaining useful information about the observed scene as required in different application domains, such as autonomous navigation, robot manipulation tasks, and 3D dynamic scene understanding.Any artificial agent acting in a real-world scenario should perform the analysis and interpretation of visual motion in a fast and reliable way. This task is effectively solved by the primate visual system. In the visual cortex of mammals, indeed, the analysis of motion and of the 3D spatial relationships occur jointly in the dorsal cortical pathway [1], which is specialized for spatial localization and action planning. Such a pathway can be seen as an “action stream” in the visual cortex for guiding in real-time the actions that we direct at objects in the world. The dorsal pathway is organized in a hierarchical structure of layers that process the visual signal to extract simple image features that become more informative and complex in higher layers. In particular, the visual signal is acquired and spatially transformed by the retina, then it reaches the primary visual cortex (V1), whose neurons are sensitive to the elemental visual attributes, such as orientation, motion and disparity. This early visual processing is projected first to the V2 cortical area where cells are sensitive to relative disparity and to more sophisticated feature representation, then to the Middle Temporal (MT) extra-striate area that provides estimates of visual motion. Finally, MT neurons project to the Medial Superior Temporal (MST) area where an estimation of visual features related to 3D events in the real-world, such as heading, surface orientation and time-to-contact, takes place. The overall architecture is characterized by deep hierarchies that exhibit advantages from the computational point of view and for their generalizations properties, with respect to the standard Computer Vision approaches [2].In this paper, to fully exploit such an effective hierarchical approach adopted by the mammals’ visual system, we propose a bio-inspired vision system that mimics the first stages of the dorsal cortical pathway. Indeed, the integrated neuromimetic architecture we propose is an instance of some of the principles described in [2]. Nevertheless, the proposed neuromimetic vision system does not aspire to model and to replicate with accuracy all the primate electrophysiological and human psychophysical data, i.e. our neural algorithm is not a neural model of the visual pathway, but it is a bio-inspired architecture that implements, at a functional level, some neural paradigms, only. In particular, our aim is (1) to capture in the simplest possible way the essential aspects of the neural computations that occur in the cortical motion pathway (namely V1-MT-MST) and lead to the estimations of visual motion features in real-world situations, and (2) to quantitatively assess the results of our neural architecture with respect to the performances of other state-of-the-art vision algorithms (either bio-inspired or not, available in the literature). In particular, we have considered the following processing stages.The front-end stage of the proposed bio-inspired vision system performs a space-variant image acquisition. This accounts the fact that the distribution of the photoreceptors in the mammals’ retina is denser in the central region, the fovea, whereas it is sparser in the periphery. Accordingly, the projection of the photoreceptor array into the primary visual cortex can be well described by a log-polar mapping [3]. Such a mechanism simultaneously provides a wide field of view, a high spatial resolution in the region of interest, and a significant reduction of the amount of data to be processed.The second stage is the computation of the optic flow through a distributed neural architecture that mimics the mechanisms underlying motion analysis in the areas V1 and MT [4]. In distributed representations, or population codes, the information is encoded by the activity pattern of a network of neurons that are selective to elemental vision attributes, e.g. oriented edges, direction of motion and texture [5].The final stage of the proposed vision system performs motion interpretation through a first order analysis of the optic flow. Indeed, as it is reported in neuroscience [6] and psychophysics [7] studies, many cells in MT and MST cortical areas are sensitive to speed gradients, thus to the orientation of the surfaces. This supports the view that orientation of surfaces and time-to-contact (TTC) estimation are represented in the dorsal stream of the visual pathway, and suggests that the first-order representation is instrumental for the visual understanding of the 3D scene [8–10].The main contributions of this paper are:–A complete integrated neuromimetic architecture that models the pathway from the acquisition stage to motion interpretation. Through a cortical representation of the visual signal, which allows feature extraction and interpretation directly in the log-polar domain (see Section 3.1), the system gains real-time performances in real-world situations (see Section 4).A distributed neural architecture based on a population of motion energy neurons for the computation of optic flow in the log-polar domain (see Section 3.3). The neural architecture described in this paper extends a preliminary work presented in [11]. The main differences are that now it entirely works in the cortical domain and it exploits a fast implementation that can run both on CPU and on Graphics Processing Unit (GPU) (see Section 4.1).The relationships between log-polar and Cartesian affine descriptions of the optic flow that allow us to recover the 3D motion parameters, such as time-to-contact and surface orientation, expressed in real-world coordinates, directly from the cortical representation of the visual motion (see Section 3.5).

@&#CONCLUSIONS@&#
In this paper, we have proposed a vision system that mimics the first processing stages of the visual cortex for motion interpretation directly in the cortical domain. Such a neuromimetic architecture and its hybrid implementation that exploits both multi-core CPU and GPU allows us to reach real-time performances in real-world situations. Each frame is processed in about 50ms: on the multi-core CPU the log-polar mapping is performed in about 1ms and the adaptive pattern matching in about 5ms (i.e. it is possible to compute up to 9 points of interest), and on the GPU the optic flow is computed in about 50ms, thus the global execution time is kept below the 50ms.In the proposed approach, the input Cartesian image sequences are first transformed into the log-polar domain, thus achieving data reduction, then the cortical optic flow is computed through a distributed population of motion energy neurons. This stage of processing yields results that are comparable to those of other state-of-the-art bio-inspired approaches that work in the Cartesian domain, if we limit our analysis to an area near the fovea. Results are worse by considering the whole image, and this is due to the consistent data reduction achieved by the log-polar transform. Though, we show that motion interpretation is still possible, with reliable results. At this point, by mimicking the processing of the MST cortical area, the vision system performs a first order analysis of the cortical optic flow through an adaptive pattern matching operation, to obtain the affine cortical description of the flow.In order to exploit such a cortical description to visually guide the actions of an agent in the real 3D world, we have devised the relationships between the cortical affine description of the optic flow and the 3D Cartesian parameters of the motion interpretation (e.g. time-to-contact and orientation of the surfaces). A quantitative evaluation shows that an accurate description of affine Cartesian quantities can be obtained from the linear description of the corresponding cortical optic flow.Finally, we have shown the effectiveness of the developed vision system in computing the time-to-contact and the normal vectors of the surfaces for some points of interest, by considering synthetic and real-world sequences. The quantitative evaluation shows that the system is able to compute reliable estimates of the TTC both in an automotive context, where two cars are approaching at high velocity, and in a robotic situation, where objects are located at small different distances from a slowly moving robot. A dataset of the used sequences and the related estimates of TTC and surface orientation are available for the Computer Vision community at the web page http://www.pspc.unige.it/Research/Neuromimetic_vision_system.htmlIn [50], to recover the elementary flow components with an adaptive pattern matching operation, we adopt an approach based on Kalman filter (KF) [73]. In order to use the EFCs (see Section 3.4) as “models” for the KF, they should be recursively generated, this is achieved through a lattice network in the velocity space:(A.1)v[k]=Φ[k,k-1]v[k-1]+s[k-1]+n1[k-1],which describes the temporal evolution, from the previous time stepk-1to the current time step k, of the relationships among motion features over a fixed small spatial region according to specific rules embedded in the transition matrixΦ. The driving inputs[k]is evaluated at each time step, by computing the average of the optic flow velocity components at the patch’s borders, whereasn1[k]represents the noise.The output of the KF will be the estimate of the affine description of the optic flow on the basis of the spatial contextual information described by the generative models (Eq. (A.1)) of the EFCs. In this way, we perform an adaptive pattern matching capable of tracking the coefficients of a linear description of the optic flow. Formally, given the process equation in Eq. (A.1) and the measurement equation:(A.2)v[k]=C[k]v[k]+n2[k],wherev[k]is a noisy measure, at current time k, of the optic flowv[k],n2[k]is the uncertainty of the measure, andCis a modified unitary operator for discarding the image points where the optic flow is not available or not reliable, the output of the KF will be:(A.3)vˆ[k|Vk]=vˆ[k|Vk-1]+G[k]ν[k],wherevˆ[k|Vk-1]is the a priori velocity state estimate,vˆ[k|Vk]is the a posteriori state estimate,Vkrepresent all the measurements until time steps k,ν[k]=v[k]-vˆ[k|Vk-1]is the innovation andG[k]is the Kalman gain.In order to have a statistical measure of the discrepancy between predictions and observations, as an indication of the filter’s consistency, it is frequently used the Normalized Innovation Squared (NIS) [74]:(A.4)NISk=νT[k]Σ-1[k]ν[k],defined on the basis of the innovation and on its covarianceΣ. Since the covariance of the innovation depends on the estimate of the measure noisen2, it is important to have a reliable estimate of the noise in the measure.To measure the structural properties of the input optic flow, a multiple model KF [74] is used. The NIS value is used to compute, for each model, the likelihood of the measurements, on which to base the selection among the different models. Such models are obtained from Eq. (11), by incorporating the pure translations into the corresponding deformation components, thus obtaining generalized deformation components:(A.5)vxx=a1αx+a2dxx=defm1vyx=a3αx+a4dyx=defm2vxy=a5αy+a6dxy=defm3vyy=a7αy+a8dyy=defm4.In the multiple model approach it is assumed that the system obeys one of a finite number of modelsmiwithi=1,2,…,r(withr=4, in our case). The likelihood of the measurementvgiven a modelmiat time step k is given by:(A.6)f(v|mi)=|2πΣmi|-12e-12νmiTΣmi-1νmi,wheremiis the considered model. The probability that the candidate modelmiis the correct one is given by the following equation:(A.7)pmi[k]=f(v|mi)∑j=1rf(v|mj),withpmi[0]=1/r,i=1,2,…,rand∑i=1rpmi[k]=1at each time step k.The final model-conditioned estimate of the statevis computed as a weighted combination of the a posteriori states of each candidate filter:(A.8)vˆ[k]=∑i=1rpmi[k]vˆmi[k].For the four models considered in the proposed approach (see Eq. (A.5)), we obtain:(A.9)vˆ=pm1vˆxx+pm2vˆyx+pm3vˆxy+pm4vˆyy,wherepm1,pm2,pm3,pm4are the probabilities related to each model andvˆxx,vˆyx,vˆxy,vˆyyare the state estimates for each KF. Combining Eqs. (A.5) and (A.9) we have:(A.10)vˆxvˆy=pm1aˆ1+pm2aˆ3pm3aˆ5+pm4aˆ7+pm1aˆ2pm2aˆ4pm3aˆ6pm4aˆ8xy,from which it is possible to derive the estimated coefficients of the affine model:(A.11)cˆ1=pm1aˆ1+pm2aˆ3,cˆ2=pm1aˆ2,cˆ3=pm2aˆ4cˆ4=pm3aˆ5+pm4aˆ7,cˆ5=pm3aˆ6,cˆ6=pm4aˆ8.In the text (see Section 3.5) the notation·̃, here used to indicate the KF estimates, is omitted for notational simplicity.Here, we summarize the problem of motion interpretation [45,75–77] from optic flow. In [50] we have previously formulated the problem of motion interpretation without simplifying assumptions (e.g. on the motion of the observer), thus obtaining a complete description that allows us to cope with real-world complex situations.The relative motion between an observer and the scene can be described at each instant t as a rigid-body motion, by means of the translational velocityT=(TX,TY,TZ)Tand of the angular velocityΩ=(ΩX,ΩY,ΩZ)T. By considering a viewer-centered coordinate frame, i.e. fixed on the observer, each pointX=(X,Y,Z)Tin the 3D space has an apparent motion given by:Ẋ=-(T+Ω∧X). In a pinhole camera model, the perspective projectionx=(x,y)Tof the pointXis defined asx=fX/Zandy=fY/Z. The image motion fieldẋ=(ẋ,ẏ)T=(vx,vy)Tis expressed as a function of image position and surface depthZ=Z(x,y), in the following way:(B.1)vxvy=f-TX/Z-ΩY-TY/Z+ΩX+TZ/ZΩZ-ΩZTZ/Z·xy+1fxyΩX-x2ΩYy2ΩX-xyΩY.The motion field is defined in terms of distinct depth values at each image point, and this makes estimation of 3D parameters ill-conditioned [75]. One way to overcome this problem is to pool measurements on the basis of assumptions about scene structure, e.g. by assuming a smooth surface structure in order to use locally linear depth variation. Let us assume that locally about a generic pointX0=(X0,Y0,Z0)T, the surface is approximately planar, with the orientation defined by the unit surface normalN=(nX,nY)T. Points on the surface aroundX0therefore have depth values given byZ=D+nXX+nYY,;, where D is the distance from the origin of the point at which the surface tangent cuts the optical axis Z. Using the perspective projection model, the inverse depth is:(B.2)1Z=1D1-nXxf-nYyf,Thus, it is possible to combine Eqs. (13), (B.1) and (B.2) in order to compute the motion-structure relationships under linear approximation.The process of inferring the time-variant geometry of a scene from the corresponding visual motion is carried out locally. The reason for this is that it is impractical to represent arbitrary shapes and motions of surfaces in the scene by a global parameterization scheme. Restriction to local analysis facilitates working in a smaller parameter space. A patching together of this local 3D information is necessary to obtain a global description of the scene.The orientation of a surface (with respect to the line of sight) is usually represented by two angles called slant,σ, and tilt,τ, as described in [78]. By definingA=(TX/D,TY/D)as the viewer translation,F=(nX,nY)=(Zx,Zy)/D=∇Z/Das the surface orientation (represented by the depth gradient, scaled by depth), we can obtain direct relationships between the affine coefficients (see Eq. (13)) and behaviorally-relevant quantities for motion interpretation and scene understanding:(B.3)divv=c2+c6=2TZD+F·Acurlv=c3-c5=-2ΩZ+|F·A|defv=((c2-c6)2+(c3+c5)2)1/2=|F||A|.The orientation of the axis of expansion,θ, bisectsAandF:θ=(∠A+∠F)/2. It is worth noting that the magnitude of the depth gradient is the tangent of the slantσof the surface (|F|=tanσ) and its direction corresponds to the tiltτof the surface tangent plane (∠F=τ). From that it is possible to derive the normal to the surfacen=(n1,n2,n3), by using the following relationships:(B.4)n1=sinτcosσn2=sinτsinσn3=cosτ.The TTC is a parameter (tc) that describe how long it takes for a moving observer to reach an object in the environment [76], and it ca be defined as:(B.5)tc=2η(x,y)(divv-F·Acos(τ-∠A)).This formulation is valid in general and it also takes into account the relative motion among the observer and other objects in the scene.