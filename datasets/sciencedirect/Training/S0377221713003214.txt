@&#MAIN-TITLE@&#
Zero duality gap in surrogate constraint optimization: A concise review of models

@&#HIGHLIGHTS@&#
The paper is a review of different models that close surrogate duality gaps.We identified five research streams that close surrogate duality gaps. Then we gave a review of each model.Relationship between these models when exist discussed.Future research directions are identified and suggested.

@&#KEYPHRASES@&#
Surrogate constraint relaxation,Mathematical programming optimization,Duality gap,

@&#ABSTRACT@&#
Surrogate constraint relaxation was proposed in the 1960s as an alternative to the Lagrangian relaxation for solving difficult optimization problems. The duality gap in the surrogate relaxation is always as good as the duality gap in the Lagrangian relaxation. Over the years researchers have proposed procedures to reduce the gap in the surrogate constraint. Our aim is to review models that close the surrogate duality gap. Five research streams that provide procedures with zero duality gap are identified and discussed. In each research stream, we will review major results, discuss limitations, and suggest possible future research opportunities. In addition, relationships between models if they exist, are also discussed.

@&#INTRODUCTION@&#
Consider the optimization problem (P) given as follows where f and each component gi(x) of the vector function g(x) are real-valued functions defined on X. No specific characteristics of these functions or of X are assumed unless otherwise specified.(1)P:Minx∈Xf(x):st.g(x)⩽0.Let X(F) be the set of all feasible points in (P) defined by(2)X(F)={x∈X:g(x)⩽0}.No distinction is made between the row and column vectors, and all vector products are dot products in the usual sense and conformable dimensions are taken for granted. Glover (1965) introduced the surrogate constraint relaxation as an alternative to the Lagrangian relaxation. A surrogate constraint for problem (P) is a linear combination of the component constraints of g(x)⩽0 that associates a multiplier uiwith each gi(x)⩽0 to produce a single inequality ug(x)⩽0, where u=(ui). This inequality is implied by g(x)⩽0 whenever u⩾0 (Glover, 1975). Given a multiplier vector u⩾0, the surrogate problem is then defined by(3)SP(u):Minx∈Xf(x):st.ug(x)⩽0.Let the optimal objective function value for SP (u) be s(u) defined by(4)s(u):infx∈X(u)f(x):whereX(u)={x∈X:ug(x)⩽0}.Note that SP(u) is a relaxation of (P), for u⩾0, s(u) cannot exceed the optimum objective function value of (P). It approaches this value more closely as ug(x)⩽0 becomes a more ‘faithful’ representation of the constraint g(x)⩽0. Also, we know that X(F)⊆X(u) and thus a faithful representation of g(x)⩽0 by ug(x)⩽0 depends upon how large the set X(u) is compared to the set X(F). Choices of the vector u that improve the proximity of SP (u) to (P), which provide the greatest values of SP (u), yield the strongest surrogate constraints. Based upon these facts, the definition of the surrogate dual SD is as follows:(5)SD:Maxu⩾0s(u).Since X(F)⊆X(u), the optimal value of the surrogate dual SD is smaller than or equal to the optimal value of problem (P) and the amount of difference is called the surrogate duality gap. The smaller is the gap the more faithfully the single inequality ug(x)⩽0 represents the system of inequalities g(x)⩽0.Note that except for non-negativity, there is no restriction on the values of the multiplier vectors u and components uiof u can be any real number, i.e., integer, rational or irrational (Glover, 1965).Greenberg and Pierskalla (1970) observed that the optimal value of the SD problem is always as good as the optimal value of the Lagrangian dual (LD) defined by(6)LD:Maxu⩾0L(u).In (6), L(u) is the function defined by(7)L(u):infx∈X{f(x)+ug(x)}.Thus, the surrogate constraint duality gap is always as small or smaller than the Lagrangian duality gap.Over the years researchers have introduced a variety of different methods for finding the multiplier vectors u that yield the strongest surrogate constraints (see for example, Glover, 1965; Dinkel & Kochenberger, 1978; Dyer, 1980; Karwan & Rardin, 1984; Austin & Ghandforoush, 1985; Gavish, Glover, & Pirkul, 1991; Kim & Kim, 1998; Narciso & Lorena, 1999 & Glover, 2003). An important issue in surrogate constraint optimization is the way that constraints are aggregated to create a single constraint. If it is beneficial to the search process, several constraints may be created instead of a single constraint; however, without the loss of generality we concentrate on the creation of a single constraint. Most constraint aggregation schemes for integer programing, group two constraints into one and then sequentially aggregate each of the remaining constraints with the newly formed constraint. Refer to, Rogers, Plante, Wong, and Evans (1991), for a comprehensive survey of aggregation and disaggregation in optimization. Methods for simultaneously combining constraints also have been introduced by several researchers (Rogers et al., 1991 & Glover, 2003). In all of these methods, the concern is to find a set of multipliers to aggregate multiple constraints into a single constraint. If a solution to the single constraint problem also satisfies all constraints of the problem (P) then we have an optimal solution to the original problem which will imply that we have a zero duality gap. An early result to close the duality gap is given by Luenberger (1968) who showed that any quasi-convex programming problem could be solved exactly if the surrogate multipliers are correctly chosen.Based on the aforementioned discussion, an important research topic is to find a multiplier (or a set of multipliers) u such that the set of solutions to the single constraint ug(x)⩽0 is the same as the set of solutions to the system of constraints g(x)⩽0. This guarantees X(F)=X(u). Clearly, in order to have the zero surrogate duality gap, it is not necessary to have X(F)=X(u), however, it is a sufficient condition. To the best of our knowledge, there are at least five research streams that attempted to close the duality gap in the surrogate constraint methods. These approaches are categorized as follows: (1) Aggregation of Diophantine equations; (2) Irrational multipliers method; (3) Maximum entropy method; (4) P-norm method; and (5) Slicing algorithm. Rogers et al. (1991) provide a comprehensive discussion regarding surrogate constraint optimization but does not cover several models we discuss here.In the following sections, major results in each of the five models are reviewed, relationships between models are discussed, limitations are discussed, and possible future research opportunities are suggested.Finding an aggregation of a given set of equalities to create a single one (or several equalities) with the same set of solutions as the original system of equations possess has been an important research topic over one century. The seminal paper by Mathews (1896), is likely the first publication to provide a solution to this problem. Consider the system of two equations given by (8) where xj⩾0, for j=1, …, n, are unknown integers, and aijand bj, for j=1, …, n, and i=1, 2, are given positive integers.(8)∑j=1na1jxj=b1,∑j=1na2jxj=b2,Mathews (1896) showed that the systems of Eq. (8) possess the same set of solutions as the single Eq. (9) provided that u1 and u2 are suitably chosen relatively prime integers (whose greatest common divisor is one). Furthermore, Mathews extended the result to more than two equations.(9)∑j=1n(u1a1j+u2a2j)xj=u1b1+u2b2.The aggregation of a system of two equations into a single equation is based upon the following property (Anthonisse, 1973).Proposition 1If u1and u2are two relative prime integers, ui≠0, then all integer solutions of the equation(10)u1y1+u2y2=0,are of the form y1=tu2, and y2=−tu1where t is any integer.Define:(11)y1=∑j=1na1jxj-b1,y2=∑j=1na2jxj-b2.Elmaghraby and Wig (1970), used this method for the first time and applied to optimization by adding slack variables to inequalities, making them equalities. Other successful implementations of the method to optimization have been reported in (Glover & Babayev, 1995; Babayev, Glover, & Ryan, 1997).During the last several decades researchers have put major emphasis on creating a set of multipliers of u to create a single equation where the set of solutions to the single equation is the same as the set of solutions to the original set of equations. In all such attempts the authors imposed many limiting assumptions, such as: variables and equations must be bounded, variables must be non-negative and integer valued, problem data must be non-negative and integer, equations must be linear with bounded constants (Rogers et al., 1991).Several successful implementations of the aggregation of equations have been recorded in the literature. Considerable amount of difficulties have also been reported. The difficulties stem from the fact that coefficients of the single constraint can grow very large in practice as the number of equalities increases (Glover & Woolsey, 1972; Kannan, 1983; Fishburn & Kochenberger, 1985 & Khurana & Murty, 2012). It has been shown that the computation required to aggregate constraint equations in a linear integer program with non-negative variables and non-negative coefficients, is polynomial time bounded (Kannan, 1983). Glover and Woolsey (1972), noted that Mathews’ method of aggregating sequentially applied to m equations, yields a greater than exponential growth of the coefficients in the resulting constraint.In the light of these limitations, attempts have been made to generate multipliers that create small coefficients in the equivalent knapsack problem (Rogers et al., 1991). One such attempt is the log prime method, proposed by Ram, Karwan, and Babu (1988) for linear equalities. In the log prime method, values of multipliers are specific irrational numbers. Alidaee and Wang (2012) have recently proposed a generalization of log prime method. The irrational multipliers method will be discussed in the next section. Now we discuss a successful implementation of Diophantine equations with rational multipliers. This is achieved by transforming the surrogate problem to maximum consistency problem (MCP) introduced by (Glover & Babayev, 1995 and Babayev et al., 1997). The MCP implementation can be explained as follows (Babayev, 2008). Consider the general linear integer problem given by,(12)Max∑j=1ncjxj,(13)s.t.∑j=1najixj⩽Bi,i=1,…,m,(14)xj⩾0,j=1,…,n,integer.Coefficients aji⩾0, cj, and Bi⩾0, are non-negative integers. After converting this problem to a knapsack problem similar to SP (u), problem (3), and adding a slack variable xn+1 to the inequality then we can consider the following surrogate problem:(12)Max∑j=1ncjxj,(15)s.t.∑j=1n+1ajxj=b,(16)xj⩾0,j=1,…,n+1,integer.Let the value of the optimal solution be c∗ with lower and upper limits of c andc¯, respectively. Now the problem is to find a value of c∗ in the interval[c̲,c¯]. Note that, c=c(b) is a non-decreasing function of b. The search can be carried out by testing values of c from[c̲,c¯]using some strategy. Thus, the original knapsack problem is reduced to the following MCP. Here we want to find integer values xj, for which the system of two Diophantine equations given below, is consistent for the maximum value of c in the interval[c̲,c¯](Babayev et al., 1997; Glover et al., 1997 and Babayev, 2008).(17)∑j=1ncjxj=c,(15)∑j=1n+1ajxj=b,(16)xj⩾0,j=1,…,n+1,integer.Readers are referred to Alidaee and Wang (2012) for several different examples that illustrate applications of aggregation of Diophantine equations and irrational multipliers method in optimization. Example 1 presented below illustrates the MCP procedure.Example 1(see Parker & Rardin, 1988):(P)Maxc=x1+2x2,s.t.3x1+2x2⩽9,x1+4x2⩽8,0⩽x1,x2⩽5,andInteger.One method that closes the surrogate duality gap is known as the log prime method (Ram et al., 1988). Rogers et al. (1991) considered the log prime method as ‘a methodological breakthrough’ in surrogate optimization. The method considerably reduces the values of coefficients for the aggregated knapsack problem. In the log prime method the generated multipliers are specific irrational numbers chosen as ui=ln (pi) where piare distinct prime numbers. Several computational experiments of the log prime method are promising (Lee & Luebbe, 1987; Lee & Luebbe, 1988, & Ram et al., 1988). Ram et al., 1988 and Rogers et al. (1991) pointed out several advantages of the log prime method over other existing ones. These authors indicated that: (1) the method will give rise to numerically small coefficients in the equivalent knapsack problem compared to other methods, (2) the computation involved in generating the equivalent single constraint is very little, (3) the choice of multipliers is independent of the initial set of constraints, and (4) the result does not require any bound on the variables or on the equations. It should be noted that these authors also imposed several restrictive assumptions such as all entry data and variables needed to be non-negative integers. The log prime method is based on the following theorem (Ram et al., 1988).Theorem 1The system of Eq.(18)in non-negative integer variables xj, j=1, …, n, is equivalent to Eq.(19).(18)∑j=1naijxj=bi,fori=1,…,m,(19)∑i=1mln(pi)∑j=1naijxj=∑i=1mln(pi)bi,where aij, biare non-negative integer constants and piare distinct prime numbers.In Theorem 1, multipliers are ui=ln (pi) where piare distinct prime numbers. Ram et al. (1988) noted that this method however does give rise to computational problems in solving the equivalent problem. They stated “due to the irrational nature of the multiplier set and finite word length on any computer, the evaluation of these multipliers on the computer will not be exact. This will result in the original system and the single-constraint on the computer not being exactly equivalent.” To deal with such deficiencies, the authors proposed a bounding result and showed its effectiveness in computational implementation. Recently, Alidaee and Wang (2012) presented a general irrational multipliers method that includes log prime method as a special case.While the log prime method originally was presented for non-negative integer programs with all positive integers entry data, the general irrational multipliers method is applicable to any integer or mixed integer programming problem, and can be easily extended to non-linear programs. The only assumption imposed was that all functions in the constraints must be rational-valued (Alidaee & Wang, 2012). Results in the general irrational multipliers method follow from the properties of rational and irrational numbers given below (Apostol, 1974).Property 1Given two irrational numbers u1and u2where for all rational numbers, z, u1≠zu2, then the only solution to equation u1y1+u2y2=0 where y1and y2are restricted to be rational numbers is y1=y2=0.Earlier, in regard to Proposition 1, we noted that in order to have only zero solutions to the Eq. (10), any favorable assumption imposed on the equations must satisfy that the value of y2for all feasible x cannot become a multiple of u1for an appropriate choice of u1. Property 1 exactly satisfies such assumption. Let uk(k=1, …, m) be a set of irrational numbers with the assumption of property 1 satisfied for each pair (ui,uj) and i≠j, and consider an equation∑k=1mukyk=0where ykfor all k are restricted to be rational numbers. It follows from property 1 that yk=0 for all k is the only solution to this equality. Note that there is an infinite number of possibilities for the choices of ukthat are easily obtainable which satisfy property 1. For example, consider2k, or,3k, for k=2, 3, 4, …. The following theorem includes the result of theorem 1 as a special case (Alidaee & Wang, 2012).Theorem 2Let components gi(x) of g(x) be rational-valued for i=1, 2, …, m, and assume g(x)=0 has a solution. Given any set of irrational numbers ui, for i=1, 2, …, m, where property 1 is satisfied for each pair (ui,uj) then the set of solutions for the single equation ug(x)=0 is the same as the set of solutions for the system of equations g(x)=0, and thus the surrogate duality gap is zero for the corresponding optimization problem (P).Thus, when g(x)⩽0, we can add slack variables to inequalities in problem (P) and apply theorem 2. Using theorem 2, it was shown that restrictions on the values of variables, the values of multipliers and the values of constraints each can profoundly affect the existence or non-existence of the duality gap (Alidaee & Wang, 2012). The authors showed that: (1) if each function gi(x) is rational-valued, and the components of the multiplier vector u are allowed to be any non-negative real numbers then the surrogate primal–dual gap is always zero; (2) if each function gi(x) is rational-valued, and the components of u are also restricted to rational values then a non-zero surrogate duality gap can exist; (3) if each function gi(x) is real-valued, and the components of u are restricted to rational values, then a non-zero surrogate duality gap can exist. We refer the readers to Alidaee and Wang (2012) who provided several examples to illustrated the above points.Ram et al. (1988), presented a bounding result for the log prime method, for the cases when the equations are bounded and showed its effectiveness in implementation. The result was generalized by (Alidaee & Wang, 2012). Ram et al. (1988) noted that the constraint constants in knapsack are irrational numbers with an inexact representation on the computer. As a result in computational implementation, the knapsack problem may not appear to be equivalent to the original problem (P). This may cause any of the following to happen: (i) some incumbents obtained in a branch-and-bound procedure to find the optimal solution may not be feasible to (P) and (ii) some feasible solutions to (P) may be detected as infeasible to the knapsack problem. To remedy this problem the authors presented a theorem that provides a tolerance, which when used in testing feasibility in the branch-and-bound procedure for the knapsack problem, will ensure that situation (ii) does not arise. If, in addition, incumbent solutions to the knapsack problem are tested explicitly for feasibility to the original problem (P), before they are accepted as incumbent solutions, then situation (i) will also be avoided. The theorem was extended to more general case (Alidaee & Wang, 2012). Assume that irrational constants uiare replaced by real numbersuˆi=ui+Δifor i=1, …, m, where −Δ⩽Δi⩽Δ. Here, Δifor each i=1, …, m, is chosen appropriately to representuˆias a good approximate of ui. In the branch-and-bound procedure for the knapsack problem, the tolerance t, is computed from the constraint constants in (P) and a suitable value of Δ (Ram et al., 1988). The value of Δ, used will depend on the computer used and the precision of computation specified in the program. Let Uibe a value such that Ui⩾maxx{∣gi(x)∣} for all i, andt=∑i=1mΔUi. In many problems choosing Uiis easy to obtain, e.g., when we deal with integer linear programs having positive entry data and bounded variables.Theorem 3Consider the problem (P) and its equivalent knapsack formulation SD. Assume the irrational number uiin the knapsack constraint is replaced with real numbersuˆi=ui+Δifor all i. If the feasibility for the knapsack problem is tested as follows, then an x feasible for the knapsack must be declared feasible for the original problem:(1)If|uˆg(x)|⩽tthen declare x feasible; andIf|uˆg(x)|>tthen xis infeasible.As mentioned earlier, there are some limitations in using irrational numbers for multipliers in the context of log prime method. We also mentioned that several successful implementations of log prime method have been reported, however, in regard to general irrational numbers for multipliers no implementation has been published yet. This provides a fertile area of research to identify appropriate irrational multipliers that can work well in practice. This may be especially useful when applied with a heuristic such as those used in (Sarin, Karwan, & Rardin, 1988; Van De Velde, 1993; Combettes, 2003; Gomes da Silva, Clímaco, & Figueira, 2004; Parra-Hernández & Dimopoulos, 2005; Alidaee, Kochenberger, & Wang, 2008; Tavares, Pereira, & Costa, 2008; Chen et al., 2010; Ablanedo-Rosas & Rego, 2010 & Li et al., Li, Liang, & Chang, 2012). We refer to Silver (2004) for a comprehensive discussion of heuristic methods including heuristics using surrogate relaxations. Another area that needs to be investigated is the use of the method to generate good bounds in the context of exact procedures such as those used in (John & Kochenberger, 1988; Sarin et al., 1988; Van De Velde, 1993; Martello, Pisinger, & Toth, 1999; Hooker & Osorio, 1999; Vairaktarakis & Winch, 1999; Shen & Jong, 2000; Karabati, Kouvelis, & Yu, 2001; Hooker, 2002; Patel & Chinneck, 2007; Muritiba, Lori, Malaguti, & Toth, 2010 and Üster & Kewcharoenwong, 2011).In the next two sections, two different models with similarities are discussed. Multipliers in these models are also irrational. One is based on maximum Entropy principle, and the other is called P-Norm method.Templeman and Li (1987), and Li (1991) introduced the maximum entropy model for problem (P). Later Li (1992) and Li and Fang (1997) extended the method to min–max optimization problem. Further extension of the model has been considered (Yu, Feng, & Zhang, 2001; Su, Yu, & Wang, 2010, and Tan, Li, & Li, 2011). In this model, the multipliers are calculated each time a new points x is computed, and the multipliers are irrational numbers. Successful implementations of the method have been reported (Li, 1991; Li, 1992; Chen, Matoba, Inabe, & Okabe, 1998; Hu & Fang, 1999; Chen, Fang, & Tsaoc, 2002; Xiao & Yu, 2010; Li, Tan, & Li, 2010). The surrogate problem for (P) can also be stated as follows:(4’)SD:Maxu∈Λs(u),whereΛ=u⩾0,∑i=1mui=1.Li (1991) showed that problem (P) is equivalent to problem (B).(1’)B:Minx∈Xf(x):st.γ(x)⩽0,whereγ(x)=max1⩽i⩽m{gi(x)}.In (B) γ(x) is a uniform L∞ approximation to the original constraint set. Using this result, the constraint (20) holds.(20)ug(x)⩽γ(x),∀u∈Λ.In order to approach γ(x), using (20) we can solve problem (C).(21)C:Maxu∈Λug(x):x∈X.A direct solution to (C) consists of solving a sequence of surrogate problems SP (u) with different values of u. Instead, the author resorted to the maximum entropy principles and formulated an entropy-augmented maximization problem (D), for a given integer parameter p.(22)D:maxu∈Λgp(x,u)=ug(x)+H(x)/p,where in (D) we haveH(u)=-∑i=1muiln(ui).Problem (D) is an approximation of problem (C). Advantage of using (D) is that we can easily find close formulation of surrogate values of u given as follows.(23)ui=exppgi(x)∑i=1nexppgi(x),i=1,…,m.Upon substitution of u in (D) we have(24)gp(x)=(1/p)ln∑i=1mexppgi(x).Here, the positive values of p tend to infinity. Thus, instead of solving problem (P) we solve problem (E) which has a single constraint. In theorem 4, Li (1991) showed that the two problems are equivalent for large enough values of p.(25)E:Minx∈Xf(x):st.gp(x)⩽0.Theorem 4Problems (E) and (P) are equivalent if p tends to infinity. Furthermore, accuracy of gp(x) is as follows.(26)0⩽gp(x)-γ(x)⩽ln(m)/p.Some of the difficulties with solving non-linear programming are due to the fact that there are too many inequalities compared to equalities, where most of the inequalities are non-binding. However, the efficiency of an algorithm mainly depends on manipulations of inequalities. This is especially true for the large scale systems (Templeman & Li, 1987; Li, 1991; and Chen et al., 1998). In fact, the parameters uiindicate how “active” the corresponding inequalities are in the calculation process. The value of uican be considered as the probability for which the corresponded constraint is active at the solution. Assume gi(x), for i=1, …, m, is rational valued, and consider the problem (P) in the following form where si⩾0, for i=1, …, m, is slack variable:(1)P:Minx∈Xf(x):st.G(x)=g(x)+s=0.Proposition 2 follows from Property 1.Proposition 2Consider problem (P) where gi(x), for i=1, …, m, is rational valued. For any value of x if gi(x)≠gj(x), for i,j=1,…,m, then udefined by(23)satisfiesProperty 1. In addition, the set of solutions for the single equation uG(x)=0 is the same as the set of solutions for the system of equations G(x)=0, and this indicates the surrogate duality gap is zero for the corresponding optimization problem (P).Given x if condition of the proposition is satisfied for two irrational numbers uiand ujdefined by (23) we have ui≠zuj, for all rational values of z. Now, the proof follows from the Theorem 2.Since multipliers in the entropy approach are irrational, the limitations that we mentioned earlier regarding inaccuracy of use of irrational numbers in computers apply here too. However, Li (1991) presented an algorithm that solves the single constrained problem (E). As explained below, their algorithm solves the unconstrained problem (F) where α is the Lagrange multiplier associated with the constraint of problem (E) and c is a penalty parameter.(27)F:Minx∈Xϕ(x)=f(x)+αgp(x)+c(gp(x))2/2.The value of αkin each iteration is calculated by(28)αk+1=αk+cgp(xk)It has been proved that ϕ(x) in (27) is locally convex in some neighborhood of optimal solution x∗ for a sufficiently large c (Arrow, Gould, & Howe, 1973). Further research in this area may concentrate on combining the method with other methods, such as slicing algorithm (Nakagawa, James, Rego, & Edirisinghed, 2011). To illustrate the entropy method we consider problem (P) from Example 1. □Considering problem (P) of Example 1, Proposition 2 emphasizes that for any x if g1(x)≠g2(x) then values of u1 and u2, from (23), satisfy condition of proposition 1 and thus solves the problem. Thus, calculating u1 and u2 by (23), and substituting in the following surrogate problem, and using excel it gives optimal solution x=(0, 2) and c∗=4.Li (1999) proposed a surrogate relaxation model, called p-norm. The method is non-linear. It closes the duality gap as the value of p exceeds some threshold. Generalization of the p-norm method to Lagrangian relaxation has also been considered (Li & White, 2000; Li & Sun, 2001; and Li & Sun, 2000). Li (1999) considered the problem (P) in the following format:(1)P:Minx∈Xf(x):st.g(x)⩽b.Here, without loss of generality, we assume that gi(x), for i=1, …, m, and f(x) are strictly positive for all x∈X, otherwise we can add a large enough number to both sides of an inequality and force the new left-hand side to be positive (Li, 1999; Li & White, 2000; Li & Sun, 2000; Xu, Liu, & Li, 2005; Li & Sun, 2006; and Wang & Li, 2007). Define the set of feasible region by:(29)X(F)={x∈X:g(x)⩽b}.For a given function ψ(x) with components ψi(x), i=1, …, m, define:(30)‖Λψ(x)‖p=∑i=1m[uiψi(x)]p1/p,‖Λψ(x)‖∞=maxi{biψi(x)}.The p-norm surrogate problem is defined by problem (G):(31)G:Minx∈Xf(x):st.‖Λg(x)‖p⩽‖Λb‖p.Multipliers uiin (G) must satisfy the following equation:(32)u1b1=u2b2=⋯=umbm.Let B be a constant that satisfies(33)uibi=B,fori=1,…,m.For a value of u that satisfies (33), and a given integer 1⩽p⩽∞, the feasible region for problem (G) is denoted by:(34)Fp(u)={x|‖Λg(x)‖p⩽‖Λb‖p;x∈X}.Li (1999) showed for a multiplier u,X(F)⊆Fp(u) for 1⩽p⩽∞, and that X(F)=F∞(u) which proves that p-norm surrogate problem closes the gap when p tends to infinity. Note that if 1⩽p<∞ with or without (33) satisfied, problem (G) still is a relaxation of the problem (P). For positive multipliers u satisfying (33), define the feasible region, in g space, as follows:(35)Ap(u)=g∈Rm|g=g(x)forsomexsatisfying‖Λg(x)‖p⩽‖Λb‖p;x∈X.Theorem 5 shows that the duality gap can be closed if p-norm is used (Li, 1999).Theorem 5For q<p<∞, we have Ap(u)⊆Aq(u) where u satisfies(32). For the set of multipliers u satisfying(32), there exists a finite q such that X(F)=Fp(u) for all p>q. Therefore, for all p>q, the duality gap is zero.Some of the limitations of the method stem from that fact that the p-norm surrogate problem is highly non-linear. However, several authors have reported successful applications of the method (Li, Sun, & Wang, 2006; Wang & Li, 2007; Li & Sun, 2007; Thakur, Fawaz, & M’edard, 2011). The approach is similar to the Entropy method. Some of these similarities have been pointed out (Li & White, 2000; and Li & Sun, 2000). Li and Sun (2000) gave an elegant revised formulation of p-norm and used Lagrangian relaxation to present an algorithm that solves the problem. In fact problem (P) can always be converted to an equivalent form with b1=b2=⋯=bm=B>0. Using the revised formulation, Li and Sun (2000) showed that for large enough p>0, problem (P) is equivalent to problem (H).(36)H:Minx∈Xf(x):st.Gp(x)⩽B,whereGp(x)=‖g(x)‖p/mp, and m is number of constraints. This was shown by use of theorem 6.Theorem 6The feasible set of problem (P) is the same as the feasible set of problem (H) for all p>p1where p1=ln (m)/ln[(1+B)/B].Using the revised formulation the authors then presented a Lagrangian algorithm to solve to solve problem (H). To illustrate the P-norm procedure again we solve problem (P) of Example 1.Example 3Consider problem (P) from Example 1. Given an arbitrary value of 10 for B, using (33) we have u1=1.111 and u2=1.25. Now, we can solve (G) instead of (P) as follows:Minc=-x1-2x2,st.‖Λg(x)‖p={[3x1+2x2]p+[x1+4x2]p}1/p⩽‖Λb‖p=102p,0⩽x1,x2⩽5,andInteger.Of course, this is a non-linear optimization. Solving this problem using excel for p=1, 2, 3, 4 we get c=−5. However as p reaches 5, excel will provide an optimal solution of x=(0, 2) with c∗=−4.Nakagawa (2003) presented a surrogate constraints model for solving separable non-linear integer programming, called slicing algorithm (SA). The method solves a succession of target problems that enumerates all solutions hitting a particular target. The target problems are produced by using an optimal surrogate multiplier vector. For the problem (P) consider the surrogate dual problem of (3’) and (5’).(1)P:Minx∈Xf(x):st.g(x)⩽b,(3’)SP(u):Minx∈Xf(x):st.ug(x)⩽ub,(5’)SD:Maxu∈Λs(u).SA is based on the following property (Nakagawa, 2003):Property 2Let xqbe an optimal solution of a problem SP (u) for a surrogate multiplier vector uq∈Λ. For any u∈Λ such that ug(xq)⩽ub, it holds that SP(u)⩽f(xq).An obvious consequence of this property is that we can remove the region {u∈Λ: ug(xq)⩽ub} from the set Λ. Nakagawa, Hikita, and Kamada (1984) used this property to solve exactly the surrogate dual problem SD, by solving succession of single constrained problems. At the completion the algorithm covers the whole region Λ. Denote the final optimal multiplier vector by u∗. Now, if an optimal solution xSDof the problem SP(u∗) is feasible to the original problem (P), then xSDis also optimal for (P). However, there is generally a gap, i.e., xSDwhich is not feasible for problem (P). In that case, the value of f(xSD) is used as a lower bound to the objective value of the original problem (P). To close the gap, Nakagawa (2003) presented a series of target problems (TP) to be solved exactly by complete enumeration. The target problem is stated as follows: In this formulation, u∗ is an optimal multiplier vector of a surrogate dual problem SP (u) corresponding to the original problem (P). The feasible solutions hitting the target are called target solutions.(37)TP(fT,u∗):Enumerateallsolutionsxhitting,atargetf(x)⩽fT,st.u∗g(x)⩽u∗bT,x∈X.Property 3If fT⩾f(xOPT) when xOPTis an optimal solution of problem (P), then all exact optimal solutions of (P) are target solutions to the target problem TP(fT,u∗).Using Property 3, if fT⩾f(xOPT) and if we can enumerate every target solution of problem TP(fT, u∗), then we have all the exact optimal solutions of (P). If we have a heuristic method that produces near-optimal solutions, xNear, of good quality, then we can use the near-optimal value as the target value fTof TP(fT, u∗). One possible position to choose fTis the interval [f(xSD), f(xNear)]. Thus, repeated application of Property 2 and 3 shrinks the duality gap to zero and solves optimally problem (P) (Nakagawa, 2003).Some of limitations of the method are as follows: In solving the target problem a good target value fTis needed. If such good target value is not available then a good amount of computer time and memory will be wasted (Nakagawa, 2003). Furthermore, a target problem is solved using enumerative procedures. Depending on the problem at hand the target problem could be a complex problem to solve. Thus, complexity of the target problem needs to be investigated. Even with these limitations, several successful application of SA has been reported (Nakagawa, Namikawa, Kimura, & Ohtagaki, 2005; Isada, Y, James, & Nakagawa, 2005; Onishi, Kimura, James, & Nakagawa, 2007). Example 4 illustrates procedure of the slicing method.Example 4Consider problem (P) from Example 1. Note that in the entropy procedure at each iteration we need to solve for optimal multipliers u1 and u2 for surrogate dual, s(u). Given uk=(u1, u2)k=(1−u, u)kat iteration k we need to solve the following problem.Maxx∈Xx1+2x2,st.(1-u)(3x1+2x2)+u(x1+4x2)⩽(1-u)9+u8,0⩽x1,x2⩽5,andInteger.Then we need to solve SD: Maxu⩾0s(u). To find optimal value of u we use a procedure given by Nakagawa and Miyazaki (1981). For the sake of completeness we state the procedure here, given below. Note that in this procedure we haveg1(x)=3x1+2x2,g2(x)=x1+4x2,andr(x)={b1-g1(x)}/{b1-g1(x)-b2+g2(x)}.1.Set u1=0, u2=1, k=1.Obtain the exact solution of x1 of s(u1). If g2 (x1)⩽b2, then go to Step 6. Otherwise, set k←2.Obtain the exact solution of x2 of s(u2). If g1 (x2)⩽b1, then go to Step 6. Otherwise, calculate r(x1) and r(x2), and setu̲←r(x1),u¯←r(x2)and k=3.Setuk←(u̲+u¯)/2, and obtain the exact solution xkof s(uk). If gj(xk)⩽bj, for j=1,2, then go to Step 6. Otherwise, do (a) or (b).a.When g1(xk)>b1, setu¯←r(xk).When g2(xk)>b2, setu←r(xk).Ifu¯>u̲, set k←k+1, and return to Step 4. Otherwise, Stop: s(u) has a gap.Stop: xkis the exact solution of the original problem P.Now, we can solve the problem (P) using slicing procedure as follows.k=1,u1=0,x1=(0,4),s(u1)=8,g1(x1)=8<9,g2(x1)=16>8,k=2,u2=1,x2=(4,1),s(u2)=6,g1(x2)=14>9,g2(x2)=8=8,r(x1)=0.11111,r(x2)=1.00,k=3,u̲=0.11111,u¯=1.00,u3=(u̲+u¯)/2=0.55556,x3=(1,2),s(u3)=5,g1(x3)=7<9,g2(x3)=9>8,u̲=r(x3)=0.666667,k=4,u̲=0.666667,u¯=1.00,u4=(u̲+u¯)/2=0.83333,x4=(3,1),s(u4)=5,g1(x4)=11>9,g2(x4)=7<8,k=5,u̲=0.666667,u¯=0.666667,s(u5)=5,Stop,thereisagap.For a target value fT, e.g., 3, we can enumerate all feasible solutions of the problem (P) that hit TP(fT,u∗) given below. Of course, here there are 7 feasible solutions where two, x=(2, 1), and x=(2, 1) are optimal.TP(fT,u∗):Enumerateallsolutionsxhittingatarget,x1+2x2⩾fT,st.(1-u)(3x1+2x2)+u(x1+4x2)⩽(1-u)9+u8,0⩽x1,x2⩽5,andInteger.

@&#CONCLUSIONS@&#
