@&#MAIN-TITLE@&#
Local part model for action recognition

@&#HIGHLIGHTS@&#
We propose a new local part model for action recognition.A feature sampling strategy with high feature density is used.We explore and prove the benefits of using accurate optical flow algorithm for action recognition.High performance and fast action recognition are achieved.

@&#KEYPHRASES@&#
Bag-of-features (BoF),Action recognition,Random sampling,Local part model,Multi-channel SVM,

@&#ABSTRACT@&#
This paper introduces an action recognition system based on a multiscale local part model. This model includes both a coarse primitive level root patch covering local global information and higher resolution overlapping part patches incorporating local structure and temporal relations. Descriptors are then computed over the local part models by applying fast random sampling at very high density. We also improve the recognition performance using a discontinuity-preserving optical flow algorithm. The evaluation shows that the feature dimensions can be reduced by 7/8 through PCA while preserving high accuracy. Our system achieves state-of-the-art results on large challenging realistic datasets, namely, 61.0% on HMDB51, 92.0% on UCF50, 86.6% on UCF101 and 65.3% on Hollywood2.

@&#INTRODUCTION@&#
The recognition of human actions in videos remains a very active field of research which has a significant impact on a wide range of applications such as intelligent video surveillance, video retrieval, human-computer interaction and smart home systems. Over the last decade, the advances in the area of computer vision and pattern recognition have fuelled a large amount of research with great progress in human action recognition. Much of the early progress [1–3] has been reported on atomic actions with several categories based on staged videos captured under controlled settings, such as KTH [3] and Weizmann [1]. More recently, there are emerging interests for sophisticated algorithms in recognizing actions from realistic video. Such interests involve two prospects: 1) In comparison to image classification evaluating millions of images with over one thousand categories, action recognition is still at its initial stage. It is important to develop reliable, automatic methods which scale to large numbers of action categories captured in realistic settings. 2) With over 100h of video uploaded to YouTube every minute,11http://www.youtube.com/yt/press/statistics.htmland millions of surveillance cameras all over the world, the need for efficient recognition of visual events in videos is crucial for real world applications.In this paper, we address the problem of fast human action recognition from uncontrolled, realistic video. We propose a solution that achieves both accurate recognition performance and high computational efficiency.Recent studies [4,5] have shown that low-level local spatio-temporal features and bag-of-features(BoF) can achieve remarkable performance for action recognition on realistic videos. Such approaches have several advantages, such as simplicity, compact video representation, relatively independent representation of events, and better tolerance to illumination, occlusion, deformation and multiple motions etc. However, there are still a number of challenges that need to be addressed for action recognition applied in large scale real-world videos.First, the bag-of-features model only contains statistics of unordered features, and any information related to temporal ordering and spatial structure is lost. In consequence, such approaches have difficulty to discriminate between actions characterized by their structure and event-orderings, such as “stand up” and “sit down”. A more discriminative method should include global structure information and ordering of local events.Most interest point detectors used for action classification have been extended from the 2D spatial domain. They were originally designed for feature matching, not for selecting the most discriminate patches for classification. Interest point detectors [6] or selected features [7] by unsupervised learning have been shown to be very useful for simple KTH dataset [3] with single, staged human actions and uncorrelated backgrounds. We argue that it is more suitable to include the background information for real-life challenging datasets [8–11] because some of their background features are highly correlated with the foreground actions (e.g. diving with water background and skiing with snow background), and thus provide discriminative information for the foreground categories.It should also be noted that most existing action recognition methods use relatively expensive feature extractor, which could constitute a limiting factor considering the huge amount of data to be processed. In particular, the use of dense trajectories, which is the secret sauce in most state-of-the-art methods, imposes a costly preprocessing step that prevents these methods to be used in real-time scenarios. Moreover, sparse interest point representations may miss important aspects of the scene and therefore do not generate enough relevant information for classification. In contrast, dense sampling methods can provide a very large number of feature patches and thus can potentially produce excellent recognition performance; better results are generally observed as the feature density increases [5,6]. However, the increase in the number of processed points adds to the computational complexity even if simplifying techniques, such as integral video and approximative box-filters, are used.To overcome these challenges, we proposed a local part model (LPM) to better represent spatio-temporal activities. Our local part model includes both a coarse root ST patch covering local content statistics and finer overlapping part ST patches integrating local structure and temporal relations. To further improve the efficiency of the approach, we use random sampling for feature extraction. An important contribution of this paper resides in the high efficiency of the approach while still producing competitive performances. Our method indeed runs at 30 to 70 fps, depending on the feature used for recognition. This gain in efficiency is achieved by having recourse to two main strategies. First the use of random sampling and integral video for feature extraction. Second by avoiding costly dense trajectory computations and instead relies on global optical flow estimation. We demonstrate in this paper that the use of accurate flow fields is beneficial for action recognition in real-life applications.The paper is organized as follows: The next section reviews the related works. Section 3 describes the details of our methods. Section 4 introduces different descriptors we used. In Section 5, we present the experimental setup and datasets we tested on. Section 6 summarizes our results and the comparison of our method with other approaches. In terms of recognition accuracy, this is a significant improvement over the state-of-the-art, as well as over our two previous conference publications [12,13]. This paper is built upon these two previous publications. It includes the following additions: 1) an analysis of the impact of dimensionality reduction for efficient action recognition; 2) an improvement of Local Part Model through the use of multiple channels resulting in better performance; 3) an evaluation of the use of more accurate optical flow estimation on performance; and 4) an experimental analysis on different components of the Local Part Model as well as additional experiments on datasets with large numbers of action categories captured in realistic settings. The code to perform random sampling with our Local Part Model is available on-line.22http://www.site.uottawa.ca/laganier/projects/actionLPM/index.html

@&#CONCLUSIONS@&#
This paper introduces an action recognition system based on a multiscale local part model. High performance is obtained by including both a coarse global local root model and high resolution part model made of overlapping patches. Our system still exhibits excellent performance even after a significant dimensionality reduction to 96 (root 32+part 64) dimensions, i.e., of the same size as a single original HOF/HOG/MBHx/MBHy vector. We introduced the idea of using very high sampling density for accurate classification. A random sampling strategy is also proposed for efficient action recognition. Our system experiments showed state-of-the-art results on realistic large scale datasets.The most notable conclusion is that, without losing efficiency, random sampling with very high density can generate larger numbers of patches, and therefore achieves good performance. Another very important fact for the performance resides in the local motion descriptors, which ideally should accurately encode both local structure and motion information. As one of such descriptors, MBH has been shown to outperform other descriptors by encoding motion boundary and suppressing camera motion. Yet, a more accurate optical flow estimation can significantly improve the MBH performance.Our method is capable of recognizing realistic human action on large scale video. It could also be applied in the context of action localization, abnormal detection and video retrieval. In the future, we would like to apply salient feature sampling. We also want to explore the different combinations of parts or deeper part hierarchies (i.e., parts with parts).