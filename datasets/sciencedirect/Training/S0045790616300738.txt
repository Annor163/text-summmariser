@&#MAIN-TITLE@&#
A novel feature extraction method based on late positive potential for emotion recognition in human brain signal patterns

@&#HIGHLIGHTS@&#
The use of stimulation strategy may help to enhance the emotion recognition from human brain signals.The late positive potential (LPP) was analyzed in order to select the features for emotion classification.The LPP based electroencephalography (EEG) features were selected under multiple frequency bands.The emotion classification was performed by using support vector machine (SVM) and k nearest neighbors (KNN).These findings offer experimental evidence that the LPP components may be possible features for emotion recognition.

@&#KEYPHRASES@&#
EEG pattern recognition,Late positive potential,EEG feature extraction,EEG emotion recognition,

@&#ABSTRACT@&#
Graphical abstractImage, graphical abstract

@&#INTRODUCTION@&#
Qualitative developments in the assistance given by human–machine interactions (HMI) have become an essential subject in the computing field [1]. Advancements in HMI now provide the means for users to improve the quality of their lives using intelligent systems. Emotion detection and understanding play critical roles in emotional intelligence. Several studies in affective computing have used visual stimuli to induce human emotions [2,3], whereby the subject may experience an emotional response while receiving a visual emotional stimulus. The emotional response corresponds to the subject's understanding of the current stimulus and situation [4,5]. Furthermore, several studies by cognitive scientists, neuroscientists, and psychologists have shown that emotion plays an important role in intelligent and rational thinking [6–8].Although the emotional behavior of a subject can be analyzed by neurophysiological processing, emotion recognition is a sophisticated problem for which only simple statistical methods have been employed. Emotion classification using a combination of complex classifiers and event-related potential (ERP) features may help to improve the recognition rate. Recently, researchers have used classifiers with ERP features for emotion detection. These studies have focused only on a limited number of channels (anterior-posterior), early ERP amplitude, and few signal frequencies (delta, theta, and alpha). Although the emotion recognition rate >70%, the emotion recognition results were dependent on each arousal−valence coordinate [9,10].EEG is a common noninvasive method for recording electrical activity in the human brain. EEG signals generate specific patterns according to the subject's states, such as valance, arousal, and concentration. Today, the emotion-based EEG feature extraction and classification of brain signal patterns have become interesting research topics. The EEG patterns generated from emotional tasks are also applicable. These patterns are useful for disabled people to express their emotions with the help of EEG-based brain−computer interface (BCI) systems. In addition, it has potential for application in a range of activities from ordinary human life to the needs of brain-disorder patients, including driving a car, controlling a cursor, playing a game, communicating with autistic patients, and so on [11].In general, the implementation of EEG-based emotion recognition has great benefits, including, for example, producing signals from the central nervous system, low intrusiveness, and high temporal resolution. While emotion recognition methods perform effectively, there are several problems with respect to the HMI. For example, the machine must be capable of recognizing emotion in a noisy environment or the subject must be looking directly at the camera. The independent nervous system can be used to overcome the abovementioned issues. Currently, emotion recognition using EEG has been used in several studies and may overcome the problems associated with existing emotion recognition systems.With these considerations, the most prominent techniques currently employ feature extraction methods such as common spatial patterns (CSP), wavelet transform, higher order crossings (HOC), event-related potential (ERP) amplitude, and statistical-based analysis. These methods typically use EEG features in the time and frequency domains, including gamma event-related dynamics (ERD), frequency bands, and ERP. The support vector machine (SVM) and k-nearest neighbor (KNN) seem to be the most common classifiers employed in these studies. Konstantinidis et al.[9], used the ERP components N100 and N200 for emotion recognition in the arousal−valence domain. Another study by Frantzidis et al.[10], proposed the P100, N100, P200, and P300 ERP components for emotion classification. Both of these studies extracted features from early components of ERP and their subjects were all adults. Frantzidis et al., presented a feature set that contained only three central electrodes (Cz, Fz, and Pz) and they used delta, theta, and alpha frequency filters for the EEG data. The SVM results obtained by Frantzidis et al., showed high accuracies of 85.71%, 85.71%, 71.43%, and 82.14% for high arousal and high valence (HAHV), high valence and low arousal (HVLA), low valence and high arousal (LVHA), and low valence and low arousal (LVLA), respectively. This method shows the emotion recognition for each coordinate separately in the arousal−valence model.Despite the number of emotion recognition approaches to the classification process [9,10,12,13], more research is required to increase recognition accuracy and to determine unknown areas of emotional behavior in the human brain. In affective computing, one study used event-related properties as neural markers for the early detection of emotion [14], and introduced the LPP, which shows the response to emotional stimulus. Our previous study [15] also showed the great potential of adopting LPP-based features for emotion classification. However, there are missing features in the existing ERP-based feature extraction method [10] such as the beta and gamma frequency bands, EEG channels (from temporal and occipital brain regions), and classification in the arousal−valence domain. We propose a method of feature extraction that can recognize the combined set of emotions present in the arousal and valence domains, rather than the single coordinate in the arousal−valence domain of the IAPS. We propose emotion recognition for four emotions in the arousal−valence domain—happy, calm, sad, and scared. We also considered the EEG channels from all important brain regions, including the frontal, central, temporal, parietal, and occipital. Initially, the EEG signals were produced with a single-frequency filter and an output sampling rate of 128Hz. As such, the recorded EEG data contained a lot of noise from the wireless sensors and other environmental sources. Therefore, we chose to cut off the EEG data from 0 to 50Hz. Further, the proposed feature set includes the LPP features for each frequency band, i.e., delta, theta, alpha, beta, and gamma. This frequency domain analysis method helps to identify the optimal frequency band for the LPP features. Here we hypothesize that the LPP features from human brain signals may achieve better classification accuracy than previous methods. To the best of our knowledge, there have been no studies of the LPP feature extraction method for children's EEG-based emotion recognition. This study represents the first time implementation of a methodology for emotion classification based on a combination of LPP features.The remainder of this paper is organized as follows. Section 2 provides background information on the EEG experiment settings, stimulation process, and preprocessing of the EEG data. The ERP analysis and the proposed method of feature extraction are described in Section 3. Section 4 contains our results and we discuss the experiment in Section 5. Finally, we present our conclusions in Section 6.The goal of this stimulation experiment was to extract the features of various emotional responses from human subjects while they were given visual emotional stimuli. We used the IAPS database in this experiment, which was mainly developed in the arousal−valence domain for emotion-based experiments [16]. We separately define four emotion-related states as sad, scared, happy, and calm. On the basis of these ratings, we selected 180 pictures (45 pictures x 4 states) from equally distributed groups along the arousal−valence axes from the IAPS database.For practical EEG applications, costs, placement, and connectivity are major factors to be considered as minimal. The EEG signals were recorded with the Emotiv-EPOC System. The sensors are polycarbonate and the device includes 14 electrodes with two reference channels that offer accurate spatial resolution. The device has an internal sampling rate of 2048Hz before filtering. The output sampling rate is 128 samples per second. We selected a 10/20 electrode placement, which is the most effective international standard for capturing reliable EEG recordings with the least number of electrodes [17]. The 10/20 EEG electrode placement is the commonly used standard by most researchers for EEG-based emotion recognition through audio and visual stimuli. This system is based on the relationship of different electrode positions located on the scalp and the primary side of the cerebral cortex [18]. Fig. 1shows the 16 electrodes (AF3, F7, F3, FC5, T7, CMS, P7, O1, O2, P8, DRL, T8, FC6, F4, F8, and AF4), which were inserted for recording EEG signals. Average reference channels (CMS/DRL) were placed in the P3/P4 locations.In this experiment, a total of 21 subjects (09 male and 12 female) participated. The subjects were students of the same school, aged from 12 to 14 years. They were informed of the purpose of our research and experiment, and each filled out a consent form after being briefly introduced to our research and the stages of visual simulation.We recorded the EEG signals of each subject as the designated pictures were presented randomly for 1.5 s, interspersed with 0.5-s intervals showing a black image. The black image is used to release the subject's emotional feeling or brain activity generated by the previous stimulus. We also projected a fixation mark (cross) for 4.0 s exactly in the center of the screen to focus the attention of the subject toward his/her upcoming stimulus. Fig. 2shows the timing diagram of this experiment, in which the total EEG-recording collection time was 368 s for each subject. This procedure was repeated separately for each subject.The recorded EEG signal patterns were preprocessed using a toolbox called EEGLAB provided by SCCN Lab [20], which runs as a MATLAB plugin. This software includes various functionalities, including data retrieval, channel and event information management, and data visualization. In the preprocessing phase, we used ICA. We also manually rejected artifacts such as eye movements, muscle movements, bad channels, and eye blinks. Fig. 3a shows some eye blinks through the frontal channels, which are marked with black boxes. During preprocessing, we easily removed these artifacts by applying our selected artifact rejection method, as shown in Fig. 3b. We adopted the newly developed band-pass filtering method pop_eegfiltnew() rather than the less effective EEGLAB method pop_eegfilt(). To reject artifacts, we processed the EEG data of all subjects through frequency filters from 0 to 50Hz. This method filters EEG signal data using a Hamming window [21].We compared our EEG-based emotion recognition results using two existing feature extraction methods. These methods are based on statistical calculations and power-spectrum features in the frequency domain [22–24]. A statistical feature vector (FV) is composed of six kinds of features in the time domain. The frequency domain features are composed by processing the frequency bands in each epoch. We obtained five frequency and six time domain features, and obtained a total of 84 statistical features (14 channels x 6 statistical features). Similarly, we estimated 490 (5 frequencies x 14 channels x 7 sub-windows) features as frequency domain features. Wang et al., and other researchers implemented both of the above methods in different experimental environments. In our experimental settings, we also implemented and analyzed both existing feature extraction methods to compare their results with those of our proposed feature extraction method.The primary goal of our research was to explore ERP-based features in emotion recognition from EEG brain signals. LPP is a type of ERP that indicates the attention paid by a subject to visual stimuli. Considering that different individuals have slightly different emotional response patterns to the same situation, each subject participating in the experiment reflected their own unique physiological responses for each affected state.Fig. 4shows the average ERP of all subjects between 0 to 50Hz. At the bottom of each image, the color legends show the different emotions. The horizontal axis is the timeline of each epoch and the vertical axis is the ERP amplitude. We used the ANAOVA method to compute the ERP, as displayed by the black box above the timeline of each chart, where p <0.01. AF3, AF4, F3, F7, F8, FC5, FC6, and T7 show the LPP values between 600 and 1000 milliseconds with p <0.01. The remaining channels (O1, O2, P7, and P8) show the LPP values between 300 and 600 milliseconds with p <0.01.Fig. 4 shows the modulation of the grand average ERP amplitude of the four emotion-related interpretations. Before further analysis, it is important to observe the emotion regulation of a subject's emotional activity. Therefore, we first performed ERP analysis to verify the emotion regulation of all subjects. ERP analysis allows for the examination of the physiological behavior associated with a subject's emotion regulation. It shows the existence of LPP in the early, middle, and late windows in spatial-temporal resolution. The frontal region of the brain shows the middle-window LPP, which indicates the late emotion regulation or response from subjects while watching emotional stimuli. The occipital and parietal brain regions show the early window LPP, which corresponds to an early response from a subject to emotional stimuli. The existence of an LPP indicates emotional changes in spatial-temporal resolution. Therefore, we selected the LPP feature set for emotion recognition from brain signals.Fig. 5shows a diagram detailing the proposed EEG-based emotion recognition process. This system process flow diagram shows the single input connection containing the signals of the EEG brain device. The data preprocessing unit filters the signal's data through frequency filters from 0 to 50Hz. It includes five frequency filters—delta, theta, alpha, beta, and gamma. We applied the selected frequency filters to each brain signal separately. We also carried out ICA and manual artifact rejection during this process. The cleaned data was then forwarded to the feature extraction phase. We employed three kind of features: 1) the proposed three-LPP feature sets (early, middle, and late), 2) six statistical feature sets, and 3) five frequency-based feature sets. Further, we extracted these features from brain signals and processed them into the KNN (K=5) and SVM classifiers with 10-fold cross-validation. The classification process was performed using Waikato Environment for Knowledge Analysis (WEKA) software [25]. Both classifiers were trained and tested over the group of four emotions. We used the default settings available in WEKA for both classifiers.The mathematical formulation of our proposed method describes the EEG-based feature extraction computation process as follows:(1)T(t)∈RT{RTdenotesthevectorofthetimeseriesofsingleelectrode(t),TisnumberoftimesamplesinT(t),trepresentssingleelectrodeandt:=[1,2,…,14]}(2)EpochM(subject,T(t),ec)=extractEpocht=1,subject=114,21(subject,T(t))where, “ec” is an epoch counter that corresponds to 180 epochs of the four emotions for each “subject.” The method “extractEpoch” returns the matrix “EpochM” of all channelsT(t)with the epoch length (stimulus-time x sample-rate). If we consider the case of a single subject, we compute the frequency band filtering by passing the raw signals of each epoch “EpochM(subject,T(t),ec)” into the following computational process. The method “computeFreq” generates a matrix “E”of similar size for the current epoch of a given subject:(3)Efr(subject,T(t),ec)=computeFreqfr=16(EpochM)(4)fr∈{δ[0.5,4]θ[4,8]α[8,13]β[13,30]γ[30,50]=Γ[δ,θ,α,β,γ]}where, “fr” has six different types of frequency settings. Furthermore, four types of LPP features are described in the following equations:(5)V(ec,i,k)={LPPe(fri=16(Ei))orLPPm(fri=16(Ei))orLPPl(fri=16(Ei))orLPP(e,m,l)(fri=16(Ei))or}(6)LPPk∈{[LPPe]300600,[LPPm]6001000,[LPPl]10001500,[LPPe,LPPm,LPPl]}where, “k” represents the type of LPP from the given set of options in Eq. (6). Eq. (5) computes the single-feature vector for the selected LPP type (LPPk) for every filtered epoch (Efr(subject,T(t),ec)). As described by Eq. (4), we employed six types of frequency bands (delta=δ,theta=θ,alpha=α,beta=β,gamma=γ,(δ,θ,α,β,γ)==Γ) in this study. Eqs. (5) and (6) define the four kinds of LPP, which are further explained in Fig. 6, showing the proposed feature sets for the emotion classification. The selected LPP windows are denoted by  LPPe, LPPm, LPPl, andLPP(e, m, l), which correspond to the early, middle, late, and combined-LPP windows, respectively. We can extract 24 kinds of feature vectors by combining the abovementioned LPPs and frequency bands. The selected LPP windows then extract the data samples for 300 to 600, 600 to 1000, and 1000 to 1500 milliseconds of the early, middle, and late LPPs, respectively.

@&#CONCLUSIONS@&#
