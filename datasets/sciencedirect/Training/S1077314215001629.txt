@&#MAIN-TITLE@&#
Human–computer interaction based on visual hand-gesture recognition using volumetric spatiograms of local binary patterns

@&#HIGHLIGHTS@&#
Hand-gesture recognition system based on color imagery for HCI.Design of a novel spatio-temporal descriptor with a high discriminative power.Sensible combination of spatial (local and global) and temporal information.Obtained results outperform other relevant works using depth and color imagery.

@&#KEYPHRASES@&#
Recognition,Hand gestures,Image descriptor,Video descriptor,Patterns,Segmentation,Spatio-temporal,LBP,SVM,Classification,

@&#ABSTRACT@&#
A more natural, intuitive, user-friendly, and less intrusive Human–Computer interface for controlling an application by executing hand gestures is presented. For this purpose, a robust vision-based hand-gesture recognition system has been developed, and a new database has been created to test it. The system is divided into three stages: detection, tracking, and recognition. The detection stage searches in every frame of a video sequence potential hand poses using a binary Support Vector Machine classifier and Local Binary Patterns as feature vectors. These detections are employed as input of a tracker to generate a spatio-temporal trajectory of hand poses. Finally, the recognition stage segments a spatio-temporal volume of data using the obtained trajectories, and compute a video descriptor called Volumetric Spatiograms of Local Binary Patterns (VS-LBP), which is delivered to a bank of SVM classifiers to perform the gesture recognition. The VS-LBP is a novel video descriptor that constitutes one of the most important contributions of the paper, which is able to provide much richer spatio-temporal information than other existing approaches in the state of the art with a manageable computational cost. Excellent results have been obtained outperforming other approaches of the state of the art.

@&#INTRODUCTION@&#
Recently, hand-gesture recognition systems based on vision have undergone an increasingly popularity due to their wide range of potential applications in the field of Human–Computer Interaction (HCI), such as multimedia application control [1], video-games [2], and medical systems [3]. These interfaces are considered more natural, intuitive, friendly, and less intrusive for the user than traditional HCI devices (mouse, keyboard, remote control, etc.). Although the use of keyboard and mouse can be still very useful for some applications, there are situations/applications where hand-based interfaces can be a key advantage. On the other hand, the fact that most of consumer devices are supplied with color cameras has also motivated the growth of HCI systems based on hand-gesture recognition and the design of color-based approaches.In spite of the great body of works in hand gesture recognition, there are still some challenges affecting its performance. Recognizing a hand, and characterizing its shape and motion in images or videos is a complex task. The hand dynamics is highly complex because of its articulable nature with more than 25 degrees of freedom. This fact makes to model the different poses and motions very difficult. In addition, the appearance of a hand can change dramatically because of illumination changes, scaling, blurring, orientations, and occlusions. On the other hand, intraclass and interclass variance of the gestures are very high. The same action performed by the same individual several times is slightly different, and this problem gets worse if the same action is performed by two different individuals. Finally, since gestures typically appear within a continuous stream of motion, a temporal segmentation for determining when they start and end is necessary.Two of the most popular approaches for hand-gesture recognition are based on machine learning approaches and template matching, using both color-based and depth-based imagery. They are used together with feature descriptors, in order to perform the recognition task. The works in [4] and [5] describe the hand pose by its contour shape, and then they perform the gesture classification by using template matching through a shape distance metric called Finger-Earth Mover’s Distance (FEMD). In [6] the depth map of a hand pose is transformed to a point cloud, which is characterized by the Ensemble of Shape Function (ESF) descriptor, and then it is classified by multi-layered random forest (MLRF). The work in [7] presents an interactive finger-spelling graphical user interface based on American Sign Language (ASL). The hand shape features are based on Gabor filters of the intensity and depth images, and the classification task is carried out by multi-class random forest. Finally, the work in [8] presents a framework based on a 2D volumetric shape descriptor that is delivered by a SVM for hand posture classification using depth imagery.A common issue of all the previous works is the use of descriptors, which must be able to represent the image region in a reliable way independently of the scene conditions. For this purpose, they should be invariant to translations, rotations, scale changes, and dramatic illumination changes. On the other hand, it is desirable that they have a reduced dimension in order to achieve a high computational efficiency. Therefore, it is necessary to find a good trade-off between recognition accuracy and computational efficiency.On the other hand, hand gestures are intrinsically dynamic, i.e. they vary with the time dimension. For this reason, a hand gesture descriptor should take this information into account. For this purpose, some video descriptors have been developed [9], however most of them are focused on human action recognition [10,11]. Using these techniques for hand gesture recognition is not totally suitable because the durations of gestures are much shorter than human activities, which negatively impacts their effectiveness. Furthermore, the extraction of these features is generally slow with a reduced amount of global spatial information, and they do not offer a scalable solution for efficient matching when the database is large. Other descriptors include motion trajectories [12], spatio-temporal gradients [13], and global histograms of optical flow [14]. However, the comparison of existing methods is often limited given the different range of used experimental settings.Therefore, a novel and highly discriminative video descriptor, which is called Volumetric Spatiograms of Local Binary Patterns (VS-LBP), has been designed for hand-gesture recognition in color imagery. It is not only computationally efficient and robust to dramatic illumination changes, but also provides much richer spatio-temporal information (at local and global levels) than other descriptors. This video descriptor has been integrated into a hand-gesture recognition system to provide a more natural and intuitive Human–Computer Interaction (HCI) interface. The proposed recognition framework has been tested to simulate a mouse-like pointing device to interact with a computer as an example of application. For this purpose, a new database has been created, which contains specific hand gestures to control the different mouse functionalities. Excellent results have been obtained regarding other approaches based on depth only or both color and depth.The rest of the paper is structured as follows. Section 2 explains the designed video descriptor VS-LBP. Section 3 describes the proposed hand-gesture recognition system, explaining in detail each one of its stages: hand pose detection, hand pose tracking, and hand gesture recognition. Section 4 describes the proposed database and presents the experimental results obtained for the VS-LBP descriptor and the global system. Finally, Section 5 summarizes the conclusions of this work.In the last years, the LBP descriptor [15] has been successful in several applications, such as texture recognition [16], face detection/recognition [17], and facial expression recognition [18], due to its powerful characteristics. However, the fact that the LBP descriptor does not consider any global spatial information is a disadvantage in the case of hand gestures. Textures can be seen as a set of patterns that recur several times. As for faces, they are formed by a uniform surface (forehead skin, cheeks, and chin), and by four patterns that do not greatly change their relative positions (two eyes, a nose, and a mouth). In all these cases, the global spatial information is not very determining since the appearance only can change in a limited and controlled way. However, a hand is a deformable object with more than 25 degrees of freedom, and its appearance can change largely. The hand patterns do not spread out uniformly as in textures, nor they are located in specific areas of the image as in faces. For hand poses, knowing what part of the image the patterns come from is as important as knowing the type of patterns, and the number of times that they appear.Regarding the LBP extensions to include temporal information, there are two main ones: VLBP and LBP-TOP descriptors [19]. They essentially present the same problem as the LBP descriptor presents for describing static hand poses: they do not consider global spatial information. This information is vital since now, the hand appearance not only changes dramatically in the spatial domain, but also changes in the temporal domain. In addition, a hand gesture performed by an individual can differ significantly from the same hand gesture performed by other different individual. Therefore, the lack of localized patterns (global spatial information) turns into a more complex problem than before. On the other hand, the feature extraction process in VLBP and LBP-TOP produces already high dimensional feature vectors, and therefore adding spatial information to these descriptors to obtain more discriminative features, would be prohibited in terms of computational cost, and memory requirements.The VS-LBP descriptor is a major extension of the LBP descriptor [15] to achieve reliable and compact representations from video sequences containing hand gestures. It includes global spatial information to be more discriminative by identifying from what part of the image the local binary patterns come, and temporal information to deal with dynamic hand gestures. The algorithm to compute the VS-LBP can be divided into the three following steps: Multi-scale LBP computation, S-LBP computation and Temporal sampling, which are described in the following sub-sections.The first step consists of computing the Multi-scale LBP descriptor [15] from every image region. This descriptor is a variation of the LBP operator that was originally designed for texture description [20]. The LBP operator thresholds a 3 × 3 neighborhood by the intensity value of the center pixel in order to extract local spatial structures from an image region. The thresholded values are concatenated in an 8-bit binary number, and converted to decimal for a more compact representation. Finally, they are used to generate a histogram of28=256labels. Fig. 1summarizes the computation of the LBP. The Multi-scale LBP descriptor extends the capabilities of the LBP to deal with patterns at different scales by using neighborhoods of different sizes. The new neighborhood pattern is defined as a set of sampling points evenly spaced on a circle centered at the pixel to be labeled, as shown in Fig. 2.The notation for defining this operator isLBPP,R,where P means number of sampling points on a circle of radius R. The mathematical expression to obtain a label fromLBPP,Ris:(1)LBPP,R=∑p=0P−1s(gp−gc)2p,where gccorresponds to the gray value of the center pixel of the local neighborhood,gp(p=0,…,P−1)corresponds to the gray values of the P equally spaced sampling points on the circular neighborhood, and s(x) is the sign function defined as:(2)s(x)={1,x≥00,x<0.Bilinear interpolation is used whenever a sampling point does not fall in the center of a pixel.As a result, an image of local binary patterns is obtained, as shown in Fig. 3. Then, the Histogram of Local Binary Patterns (H-LBP) is computed.The second step consists of extracting spatial information from the image of LBPs, as shown Fig. 4. First, the coordinates of all the LBP patterns that have contributed to a specific bin in the H-LBP histogram (representing a specific LBP type) are computed. From the algorithmic viewpoint, this computation is not necessary as it is previously performed during the multi-scale LBP computation. Second, a uniform sub-sampling of the image region coordinates is carried out, obtaining a total of M × N sampled coordinates, defining M as the number of rows, and N as the number of columns. The set of coordinates of each LBP bin contributes to one histogram of M × N sampled coordinates, which are called S0, S1,…,SM×N−1in Fig. 4, using a bilinear interpolation. This way, a histogram of spatial coordinates is generated per each LBP bin of the computed H-LBP (spatial histograms). As a result, we obtain 2Pspatial histograms whose length is M × N, where P was the number of neighbors in the LBPP, R. The H-LBP itself and the set of spatial histograms are all concatenated to form a super-descriptor called Spatiogram of Local Binary Patterns (S-LBP), whose dimension is2P+[2P×(M×N)].The S-LBP descriptor is highly discriminative since it contains both local (the H-LBP) and global spatial information (histograms of spatial coordinates of all the LBP patterns). The uniform sub-sampling of the image coordinates allows to shorten the histograms length and keep the computational cost manageable, establishing a trade-off between the computational cost and the discrimination ability. On the other hand, the bilinear interpolation approach increases the robustness against slight image translations, and the grid effect.The last step consists of adding temporal information to the S-LBP framework by carrying out a randomly and quasi-equally temporal sampling scheme in the video sequence. Close images in time hardly change their appearance, containing redundant information to identify the action that is being performed. This strategy also allows to deal with variations in the execution speed of the hand gestures by considering several sampling steps.The randomly and quasi-equally spaced sampling is carried out as follows. An additive random shift is applied to those images corresponding to an equally spaced sampling in the temporal dimension defined by Δe, as shown in Fig. 5.The random shifting is performed following a discrete uniform distribution over the considered maximum interval Δmax. Once all the sampled images have been obtained, the S-LBP descriptors from those selected images are concatenated to form Volumetric Spatiograms of Local Binary Patterns.Therefore, VS-LBP descriptor includes spatio-temporal information in an efficient and compact way. On the one hand, local and global spatial information is added by means of S-LBP, which increases the discriminative power. On the other hand, the temporal sampling strategy allows to consider a smaller number of frames for computation and also reduce the computational cost. Moreover, this is a versatile approach since it can be used together with any image descriptor to compute the spatial features. But, we have to keep in mind that the overall length of the final video descriptor depends on the length of the image descriptor and the number of sampled images.The proposed HCI interfacehas been tested to simulate an example of application, in particular, a mouse device to interact with a computer by means of hand gestures. In this sense, a robust hand-gesture recognition system has been implemented, and a new database has been created, which contains hand gestures based on mouse functionalities. Nevertheless, the presented vision-based recognition system can be integrated into other multimedia devices provided with a color camera, such as smartphones and televisions, and extended to other applications by increasing the database to consider new hand gestures, and therefore, new functionalities.The proposed database contains a set of hand gestures that represent the main functions of a mouse device, such as cursor, left click and right click; and two additional functions from the viewpoint of the application, such as mouse activation and mouse deactivation. As a result, five different hand gestures are considered to simulate a mouse device and interact with a computer, which can be seen in Fig. 6. Notice that the hand gestures proposed to activate and deactivate the mouse application are static, since they keep the same appearance, orientation and position along the time. However, they are considered as dynamic taking into account that they have to persist a determined period of time to be recognized.On one hand, the proposed hand-gesture recognition system is based on machine learning techniques and feature extraction methods. In particular, SVM classifiers have been used because they work very well with high-dimensional data, and are capable of delivering high performance in terms of classification accuracy. On the other hand, they allow us to deal with non-linear boundaries by means of different kernels, which makes it more adaptable.The system is composed of three stages: detection of hand poses, tracking of detected hand poses (Fig. 9), and recognition of dynamic hand gestures. The detection phase employs the LBP descriptor [15] and a binary SVM classifier to detect specific hand poses in every frame. The tracking phase uses those detections as input for a multiple object tracker that estimates potential trajectories of hand poses along time. These trajectories contain an ordered set of hand poses that performs different dynamic hand gestures. Finally, the recognition phase analyzes those trajectories by computing high efficient spatio-temporal features called VS-LBP, which are delivered to a bank of SVM classifiers for gesture recognition. A block diagram of the system can be seen in Fig. 7.The aim of the detection stage is to detect those hand poses that can be part of the considered dynamic hand gestures. These specific hand poses (positive class) must be recognized among other irrelevant hand poses and background (negative class).To detect hand poses in different spatial locations and scales, every frame is scanned by a spatial sliding window at multiple scales. Fig. 8 shows this strategy. The multi-scale analysis is carried out by generating a multiresolution pyramid, which contains different scales of the frame that is being processed. Then, a fixed rectangular window is slid along each scale so that the multiple windows are overlapped. The overlapping magnitude between consecutive windows is determined by the spatial step of the sliding window. The choice of this parameter is a trade-off between the computational cost and the accuracy of the detection. This strategy has been chosen instead of sliding several windows with different sizes applied to an only scale, because the computational cost is lower.This way, the LBP image descriptor [15] is applied to every spatial window for feature extraction. It computes a feature vector that represents the image region, being robust against dramatic illumination changes, and very computationally efficient. Then, every feature vector feeds a classifier, which determines if that image region is a positive or a negative sample.Since this stage has to deal with two classes, hand poses (positive class) and background (negative class), a binary SVM classifier is used for detection. In order to achieve a higher performance, a Hellinger kernel, more commonly known as Bhattacharyya coefficient [21], has been used. It allows to learn non-linear decision boundaries by projecting the features in a higher dimensional space, where linear boundaries can be computed to separate the different classes. It can be mathematically represented as:(3)k(f,f′)=∑if(i)f′(i),where f and f′ are normalized feature vectors.At this point, potential hand poses are detected, however they need to be filtered because of windows overlapping. Since several overlapping windows belonging to one specific scale could contain a significant fraction of a relevant hand pose, all of them could be labeled as positive samples. In order to select the one that better represents the hand pose, a non-maxima suppression technique is applied [22] in every scale. However, the problem persists among different scales of the multiresolution pyramid. For this reason, another non-maxima suppression technique based on an overlapping criterion is applied to normalized windows from different scales: if the overlapping of two normalized windows (i.e. normalized to a common scale) is bigger than a specific threshold, the window that presents the highest classification score is labeled as a positive sample, and the other one as a negative sample.Finally, a set of filtered detections is obtained in every frame, which is used as input of the tracking phase.The goal of the tracking phase is to estimate temporal hand trajectories from the detected hand poses. When the first frame is processed in the detection phase, the obtained detections are used as input of the tracker, which will create as many trajectories as the number of detected hand poses in the frame. This way, every time a frame is processed, new detected hand poses are associated to their corresponding trajectories according to the location of detections in previous frames.Since there can be missing detections due to occlusions, strong changes in the hand appearance, and also false detections generated by background structures, the estimation of the hand locations can be inaccurate. The hand-detection identities can be interchanged, as well, due to erroneous associations between detections and trajectories. To deal with these problems, a multiple object tracker which is robust to erroneous, distorted, and missing detections [23] has been used. This tracker is based on a constant velocity model for predicting the object locations, and a soft probabilistic data association that recursively estimates the best correspondence between measurements/detections and existing objects/trajectories.As a result, one or several trajectories are generated depending on the number of detected hand poses in every frame. Every trajectory can be seen as a cropped video sequence, which only contains hand poses. This volume of ordered hand poses is used as input of the recognition phase to be analyzed.The goal of the recognition stage is to temporally segment the trajectories that contain an ordered set of hand poses, and recognize the dynamic hand gestures that are executed in every temporal segment. In this case, five different dynamic hand gestures (five classes) must be distinguished, therefore a set of five SVM classifiers is used for recognition, where everyone is trained to recognize a specific dynamic hand gesture. The same kernel as in the detection phase is used to learn non-linear decision boundaries (see Eq. (3)). The VS-LBP descriptor is used for feature extraction from every temporal segment. It is robust against dramatic illumination changes, and variations in the execution of the dynamic hand gestures. In addition, it contains spatio-temporal information in an efficient and compact way that makes it highly discriminative regarding other video descriptors. Fig. 10 shows a block diagram of this phase.In order to determine where a specific dynamic hand gesture starts and ends, that is, in which temporal segments is performed, a sliding temporal window scans every trajectory by overlapping consecutive temporal segments. The overlapping between consecutive temporal windows or segments is a trade-off between the computational cost and the accuracy of the recognition. On the other hand, the size of the temporal window has to be fixed according to the average number of frames of the different dynamic hand gestures.While processing frame by frame, once a trajectory contains the same number of cropped hand poses as the size of the temporal window, the sliding of the temporal window starts. For every temporal window, several VS-LBP feature vectors are computed. Every feature vector is slightly different because of the random temporal sampling scheme (see Section 2), which allows to adapt the system to different slight variations in the hand gesture execution, increasing the recognition accuracy.This set of feature vectors, all associated to the same temporal window, are individually classified as belonging to a specific hand gesture class. Ideally, all of them should belong to the same class, but in practice there can be different recognized gestures due to the intra-class and inter-class variability. For this reason, a voting scheme is used to label the dynamic hand gesture as the most recognized class, as shown Fig. 11. This process is repeated for each trajectory that has at least the same size as the temporal window, discarding erroneously short trajectories.The final step is a temporal validation of the predicted dynamic hand gesture, as shown in Fig. 12. The condition that the same prediction should be consistent over a determined number of consecutive windows is imposed. The reason is that if the step size is enough small, the windows will differ in a few frames, and should contain the same dynamic hand gesture. This strategy solves potential errors due to gestures transitions and erroneous estimated trajectories.Both detection and recognition phases have to be trained to estimate the optimal parameters for a binary SVM and for five SVM classifiers, respectively. The proposed database contains 30 video sequences for training (see Section 4.1), which are used to train both detection and recognition stages, but in different ways.In the training stage of the detection phase, image regions containing hand poses that are part of the five dynamic hand gestures are used as positive samples. They are independently extracted from every frame of the video sequences, without any consideration if they belong to a specific gesture or another since all of them are just hand poses that we are interested in detecting. On the other hand, image regions containing background and other irrelevant hand poses are used as negative samples to train the classifier. Spatial windows that tightly surround the object are used to train the classifier, this way a small spatial step for sliding must be considered.In the recognition stage, training samples from every gesture are the own video sequences, where every video sequence contains several repetitions of a hand gesture. Every training video sequence is spatially and temporally segmented, so that, training samples from every gesture are cropped video sequences that perform one repetition of the hand gesture, whose frames are image regions containing only the hand poses. The way of training the SVM classifiers is following a one-vs-all strategy, where each classifier is trained by considering as positive samples all the sequences from the class that is being trained, and as negative samples sequences belonging to the rest of classes.In order to generate a larger number of training samples, the VS-LBP descriptor can be applied to every training sequence several times, taking advantage of its random temporal sampling scheme (see Section 2). Every computation of the video descriptor produces different feature vectors that should be strongly correlated, that is, they should theoretically belong to the same cluster in the feature space.

@&#CONCLUSIONS@&#
A more natural, intuitive, user-friendly, and less intrusive Human–Computer interface for controlling an application by executing hand gestures has been developed. In particular, a mouse-like pointing device has been evaluated as an example of HCI application, where different mouse functions are triggered depending on the recognized hand gesture.For this purpose, a robust hand-gesture recognition system has been designed and implemented. The system has been divided into three stages: detection, tracking, and recognition. The detection stage processes a video sequence frame by frame, and uses a binary SVM classifier together with an image descriptor in order to detect potential hand poses. These detections are employed as input of a multiple object tracker to generate a spatio-temporal trajectory of hand regions. Finally, the recognition stage segments the video sequence using the trajectory, then computes a video descriptor from the segmented video sequence, and lastly delivers the video descriptor to a set of classifiers to carry out the recognition task. The key contribution of the system has been the design of a novel and highly discriminative video descriptor for the recognition stage, which is called Volumetric Spatiograms of Local Binary Patterns. It has proven to be more discriminative and computationally efficient than other methods in the state of the art.The final obtained recognition accuracy of the global system is quite high, allowing to use the developed hand-gesture recognition system for real HCI applications.