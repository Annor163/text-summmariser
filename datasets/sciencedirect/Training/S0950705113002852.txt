@&#MAIN-TITLE@&#
Sparse signal reconstruction using decomposition algorithm

@&#HIGHLIGHTS@&#
This paper proposes a decomposition algorithm for sparse signal reconstruction.A small quadratic programming problem is solved in each iteration.The convergence of the decomposition algorithm is also shown in this paper.The decomposition method can get a fast convergence for small regularization values.

@&#KEYPHRASES@&#
Compressed sensing,Sparse signal reconstruction,Quadratic programming,Decomposition algorithm,Quadratic programming,

@&#ABSTRACT@&#
In compressed sensing, sparse signal reconstruction is a required stage. To find sparse solutions of reconstruction problems, many methods have been proposed. It is time-consuming for some methods when the regularization parameter takes a small value. This paper proposes a decomposition algorithm for sparse signal reconstruction, which is almost insensitive to the regularization parameter. In each iteration, a subproblem or a small quadratic programming problem is solved in our decomposition algorithm. If the extended solution in the current iteration satisfies optimality conditions, an optimal solution to the reconstruction problem is found. On the contrary, a new working set must be selected for constructing the next subproblem. The convergence of the decomposition algorithm is also shown in this paper. Experimental results show that the decomposition method is able to achieve a fast convergence when the regularization parameter takes small values.

@&#INTRODUCTION@&#
In recent years, compressed sensing (CS) has received a lot of attentions in signal processing, computer vision and machine learning. CS theory has shown that a sparse signal can be reconstructed with far fewer samples or measurements than that used by traditional methods [1–4]. It is well known that the number of measurements required by traditional methods must follow the Shannon-Nyquist Theorem: the sampling rate (or Nyquist rate) must be at least twice the bandwidth of a signal. However, the Nyquist rate would lead to a large number of measurements, which must be compressed before they can be transmitted. As a result, most of the measurements can be thrown away. Fortunately, CS goes against traditional methods. In CS, there are three basic tasks: how to sparsely represent a given signal, how to design a good sampling matrix, and how to recover the signal represented sparsely. Here, we only focus on the last task, or the problem of sparse signal reconstruction.The problem of sparse signal reconstruction can be formulated as three different optimization formulations [5,6]: the ℓ2−ℓ1 problem (called basis pursuit denoising (BPDN) in [7] or ℓ1-regularized least squares problem in [8,9]), the least absolute shrinkage and selection operator (LASSO) in [10], and the quadratically constrained linear problem (QCLP). Actually, the first two formulations are quadratic programming problems. A lot of methods have been presented for solving one of them. For example, matching pursuit (MP) and orthogonal matching pursuit (OMP) which are greedy methods have been applied to approximately solve QCLP [11–14]. In [15], NESTA (short for Nesterov’s algorithm) was also developed for solving QCLP. The methods in [16,17] were presented for solving the LASSO formulation. In [18], both LASSO and the sphere code algorithm have been applied to array signal processing.It is known that solving an unconstrained problem is generally much easier than solving a constrained one. Since the ℓ2−ℓ1 formulation is an unconstrained problem, many researchers have paid attention to it and done great work. Methods commonly used for solving the ℓ2−ℓ1 problem include the gradient projection for sparse reconstruction (GPSR) method [5], the ℓ1 regularized least squares (ℓ1-LS) method [8], the sparse reconstruction by separable approximation (SpaRSA) method [6], the fixed point continuation (or FPC) method and its improved version [19], the fixed point continuation active set (FPC-AS) method [9], and other iterative shrinkage/thresholding (IST) methods [20,21]. Figueiredo et al. proposed an expectation–maximization (EM) algorithm to recover a sparse signal in an iterative way [20] and described a new bound optimization algorithm (BOA) to reconstruct a sparse signal [21]. The FPC, FPC-AS and SpaRSA are also IST methods.ℓ1-LS [8] is a specialized interior-point method, which uses the preconditioned conjugate gradients algorithm to compute the search direction. This method can deal with large-scale sparse signal reconstruction problems. GPSR [5] is based on the well-known projected gradient step technique. In GPSR, the ℓ2−ℓ1 problem is recast into a bound-constrained quadratic programming (BCQP), and scheme with different step sizes would lead to two algorithms which are GPSR-Basic and GPSR Barzilai-Borwein (GPSR-BB). The experimental comparison between GPSR and ℓ1-LS shows that GPSR is better. SpaRSA [6] is an algorithm to minimize composite functions composed of a smooth term and a separable non-smooth term. In general, the composite functions are used much wider than the ℓ2−ℓ1 formulation is. Compared with other IST methods, SparRSA has a flexible selecting scheme for the step size, which improves practical performance. In [6], experimental results show that SpaRSA is faster than GPSR, FPC, and ℓ1-LS.It has been observed that the performance of GPSR, SpaRSA and other methods degrades when the regularization parameter takes small values. Thus a continuation procedure is introduced into these methods. The ℓ2−ℓ1 problem is solved for a decreasing sequence of values of the regularization parameter, using the computed solution for the current parameter value as the initial solution for the next smaller parameter value. We also observe that the reconstruction performance is good but the optimization speed is relatively slow when the continuation procedure is used in the case of small values of the regularization parameter.In this paper, a method based on a decomposition algorithm is provided for sparse signal reconstruction, which is almost insensitive to the regularization parameter. This method is to decompose the original quadratic programming (QP) problem into a sequence of smaller QP ones. In each iteration, it is easy to solve such a subproblem. The optimal solution is found when the current solution satisfies the optimality conditions. Otherwise, we need to update the working set which consists of the indexes of variables violating the optimality conditions. The convergence of the decomposition algorithm is also proved here.The rest of this paper is organized as follows. Section 2 describes the problem of sparse signal reconstruction and some classical methods including GPSR, SpaRSA and FPC. In Section 3, we present the sparse signal reconstruction based on decomposition algorithm (DA-SSR) and prove its convergence. We compare DA-SSR with other classical methods in Section 4 and provide conclusions of this paper in Section 5.

@&#CONCLUSIONS@&#
