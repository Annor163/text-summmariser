@&#MAIN-TITLE@&#
ObjectPatchNet: Towards scalable and semantic image annotation and retrieval

@&#HIGHLIGHTS@&#
We build ObjectPatchNet (OPN) from large-scale loosely annotated Internet images.OPN preserves regionlevel semantic labels and contextual clues for image annotation.OPN could be utilized as visual words with semantics for scalable image retrieval.Though not exhaustively tuned, OPN performs decently in these two applications.OPN could be an open platform for different applications and further improvements.

@&#KEYPHRASES@&#
Visual vocabulary,Large-scale image retrieval,Image annotation,

@&#ABSTRACT@&#
The ever increasing Internet image collection densely samples the real world objects, scenes, etc. and is commonly accompanied with multiple metadata such as textual descriptions and user comments. Such image data has potential to serve as a knowledge source for large-scale image applications. Facilitated by such publically available and ever-increasing loosely annotated image data on the Internet, we propose a scalable data-driven solution for annotating and retrieving Web-scale image data. We extrapolate from large-scale loosely annotated images a compact and informative representation, namely ObjectPatchNet. Each vertex in ObjectPatchNet, which is called as an ObjectPatchNode, is defined as a collection of discriminative image patches annotated with object category labels. The edge linking two ObjectPatchNodes models the co-occurrence relationship among different objects in the same image. Therefore, ObjectPatchNet models not only probabilistically labeled image patches, but also the contextual relationship between objects. It is well suited to scalable image annotation task. Besides, we further take ObjectPatchNet as a visual vocabulary with semantic labels, and hence are able to easily develop inverted file indexing for efficient semantic image retrieval. ObjectPatchNet is tested on both large-scale image annotation and large-scale image retrieval applications. Experimental results manifest that ObjectPatchNet is both discriminative and efficient in these applications.

@&#INTRODUCTION@&#
With the far reach of Internet and the popularity of digital cameras, digital camcorders, and camera cellphones, etc., the amount of user generated Internet photos has been increasing explosively. The explosive growth of these visual data has brought new challenges to multimedia research community. However, it is necessary to note that such ever increasing Internet image collection densely samples the real world objects, scenes, etc., thus has potential to serve as a knowledge source for large-scale image applications.For example, Torralba et al., verify that 80 million tiny loosely labeled Internet images can be used as a reference dataset for image annotation, and recognition, etc., in simple data-driven ways [1]. Similarly, with the recent development of large-scale image datasets containing lots of image categories, such as ImageNet [2], a naïve strategy for image annotation would be running K Nearest Neighbor (KNN) tagging. More specifically, given an image, a set of image labels can be collected by retrieving its KNN Internet images. This image could hence be annotated by the most frequent labels collected. However, the efficiency and effectiveness of such data-driven strategy are inevitably confronted by the large memory and computation demand to store and process these large-scale image data. Moreover, the cluttered backgrounds and noisy tags in Internet images may also result in inaccurate annotation. Consequently, large-scale loosely annotated Internet images provide valuable information, but more effective and more efficient utilization of such Internet data is highly desired.To achieve both scalable and accurate image annotation and semantic image retrieval, we propose to build the ObjectPatchNet from large-scale loosely annotated Internet images. As illustrated in Fig. 1, ObjectPatchNet is composed of vertices and edges linking them. We call each vertex as an ObjectPatchNode. Each ObjectPatchNode contains visually similar image patches and is labeled with probabilistic object tags. The edges model the contextual relationships between pairs of ObjectPatchNodes. More specifically, in ObjectPatchNet, we define the contextual relationship as the co-occurrence probability between two objects in an image. As shown in Fig. 1, the thickness of the edge represents the strength of the contextual relationship. From the figure, it can be observed that, the object “Bear” co-occur frequently with objects “Grass” and “Creek” in the same image.As shown in Fig. 1, with ObjectPatchNet we can largely reduce the negative effects of image background clutter, since only discriminative and semantically meaningful patches are kept and noisy patches are discarded. Moreover, since annotation information is assigned to image patches and extra contextual clues between objects are captured, ObjectPatchNet would be more accurate in annotating new images than the raw image dataset. In addition, we can easily remove information redundancy in each ObjectPatchNode based on the visual similarity of image patches it contains. It is easy to infer that, ObjectPatchNet would be more discriminative and compact than the original image dataset, and image applications based on it would be more efficient and accurate.Based on ObjectPatchNet, both image annotation and retrieval can be carried out in efficient and effective ways. As shown in Fig. 1, images to be annotated can first be divided into patches; KNN ObjectPatchNodes of each patch can hence be retrieved. The labels of these retrieved ObjectPatchNodes can be collected, and then refined with the help of the contextual clues to be the final image labels. Experiments show that ObjectPatchNet based image annotation achieves satisfactory accuracy with high efficiency.In large-scale near-duplicate image retrieval, the Bag-of-visual-Words (BoWs) image representation and inverted file indexing have shown very good efficiency [3]. Inspired by this, we propose to use ObjectPatchNet as a visual vocabulary with semantic labels. As shown in Fig. 1, each image is first represented as a Bag-of-ObjectPatchNodes representation, and then indexed with inverted file. During retrieval, images containing same ObjectPatchNodes and similar semantics with the query will be returned. Therefore, large-scale image retrieval combining both visual and semantic clues can be achieved with high efficiency. More details about our image annotation and image retrieval strategies will be given in Section 4.Our work is partially similar to the recent image-web [4], which also explores the connectivity between image regions in a large-scale image dataset. However, image-web [4] is designed for photo collection exploration and does not consider semantic clues. ObjectPatchNet also differs from Object bank [5] and Object-graph [6], which are designed for specific visual tasks at limited data scale, i.e., visual recognition and unsupervised category learning, respectively. Some works also utilize loosely annotated images for image annotation [7,8]. For example in [8], a sparse graph is constructed among loosely annotated images. A label refinement strategy is then developed to effectively filter the noisy tags and update this sparse graph. The noisy images could hence be utilized to infer the concepts of unlabeled images. Our work differs from them in that, our work mines the concepts in finer scale, i.e., image patch level and captures extra contextual clues among patches, thus is potential to extract more valuable clues. Moreover, the meaningful patches could be regarded as semantic-preserving visual words in large-scale image search.Compared with the existing image annotation algorithms [9–19], ObjectPatchNet based image annotation is data-driven, scalable, and efficient. Indeed, each ObjectPatchNode can be regarded as an attribute [20–23]. However, their differences are: (1) ObjectPatchNode preserves both visual and semantic clues, but attribute most often conveys semantic clues; (2) ObjectPatchNode can be directly used as a knowledge source in data-driven ways, thus is scalable and valid for different tasks; and (3) extra contextual clues between ObjectPatchNodes are conveniently modeled, which are proven helpful to improve the performance of ObjectPatchNet.Mining contextual information to improve theaccuracy of semantic annotation has been done for a while in the field ofmultimedia research [24–26]. For instance, Qi et al. propose a Correlative Multi-Label framework (CML) [24] to learn the correlations between labels for video annotation. The CML approach shows satisfying performance on a dataset containing 61,901 sub video shots and 39 concepts. The flicker distance also utilizes the co-occurrence clues of different labels in the identical images to measure the semantic similarities [25]. However, the difference between our work and the previous ones are: (1) we mine both contextual clues and useful visual information from a large-scale loosely annotated image dataset and (2) contextual clue is stored in a compact ObjectPatchNet, thus can be efficiently applied for different applications in flexible data-driven ways. Capturing extra contextual clues is also one of the major differences between this work and our previous work in [27].The contributions of this work are summarized as follows:(1)We propose to construct ObjectPatchNet from the publically available and ever-increasing loosely labeled Internet images, which results in more effective and more efficient utilization of such Internet image data.ObjectPatchNet preserves useful information for image annotation, i.e., both region-level semantic labels and contextual clues between objects.Each vertex in ObjectPatchNet, i.e., ObjectPatchNode, could be utilized as a visual word with semantic labels. Based on the corresponding BoWs model, scalable semantic image retrieval can be achieved.ObjectPatchNet could be an open platform which allows for different applications and further improvements on top of it. In this paper, ObjectPatchNet is not exhaustively tuned for image annotation and semantic image retrieval. However, it still achieves decent performance in these two applications.The remainder of this paper is organized as follows. Section 2 reviews the related work on image annotation and semantic image retrieval. Section 3 introduces our proposed ObjectPatchNet construction strategy. Section 4 presents ObjectPatchNet based applications. Section 5 analyzes the experimental results, followed by the conclusions and future work in Section 6.

@&#CONCLUSIONS@&#
In this paper, we propose to build ObjectPatchNet from loosely labeled Internet Images, which results in more effective and more efficient utilization of such Internet image data. ObjectPatchNet is composed of ObjectPatchNodes, each of which is a collection of discriminative image patches with label information and contextual information. ObjectPatchNet can be seen as a compact and informative knowledge source. Based on ObjectPatchNet, we propose novel image annotation and image retrieval algorithms such as ContextRank and inverted file indexing with Semantic Weighted TF weighting. Experimental results reveal that the generated ObjectPatchNet and the proposed algorithms are both efficient and effective in large-scale image annotation and large-scale semantic image retrieval applications.The goal of our work is to utilize large-scale loosely labeled images for scalable image applications. This goal is different from the ones of most related image annotation works, which are based on small-scale training data, complicated machine learning models, and are tuned for specific tasks. Our work utilizes large-scale training data and simple data-driven strategy for different image applications. Compared with related works, it is data-driven, scalable, and efficient.There are still many interesting problems that are worthy of further study in ObjectPatchNet construction. For example, feature weighting strategies, distance metric learning, or multi-kernel learning can be adopted to make the visual similarity computation between image patches and image patch groups more semantically reasonable. More clues and algorithms such as relevance modeling [10], sparse coding [15], semi-supervised learning [40] can be considered to compute the relevance degrees between groups and categories in Eq. (5). Visually similar images for each image patch can also be collected and utilized for estimating the semantics of image patches in Eq. (5). The semantic relationships defined by the WordNet [28] can also be taken into consideration to guide the group labeling. In addition, ObjectPatchNet can be constructed from multi-million-scale datasets and can be tested in more image annotation tasks such as tag-to-region and tag ranking and datasets like the NUS-WIDE [41], which contains a large number of loosely annotated images. We will continue to improve ObjectPatchNet in our future work.