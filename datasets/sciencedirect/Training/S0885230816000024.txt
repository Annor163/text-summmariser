@&#MAIN-TITLE@&#
Sparse kernel machines with empirical kernel maps for PLDA speaker verification

@&#HIGHLIGHTS@&#
Incorporate empirical kernel maps into relevance vector machines (RVMs).Report the extensive analyses on the behaviors of RVMs.Provide insight into the properties of RVMs and their applications in i-vector/PLDA speaker verification.Compare PLDA–RVM with conventional PLDA and PLDA–SVM.

@&#KEYPHRASES@&#
Relevance vector machines,Empirical kernel maps,Probabilistic linear discriminant analysis,I-vectors,NIST SRE,

@&#ABSTRACT@&#
Previous studies have demonstrated the benefits of PLDA–SVM scoring with empirical kernel maps for i-vector/PLDA speaker verification. The method not only performs significantly better than the conventional PLDA scoring and utilizes the multiple enrollment utterances of target speakers effectively, but also opens up opportunity for adopting sparse kernel machines in PLDA-based speaker verification systems. This paper proposes taking the advantages of empirical kernel maps by incorporating them into a more advanced kernel machine called relevance vector machines (RVMs). The paper reports extensive analyses on the behaviors of RVMs and provides insight into the properties of RVMs and their applications in i-vector/PLDA speaker verification. Results on NIST 2012 SRE demonstrate that PLDA–RVM outperforms the conventional PLDA and that it achieves a comparable performance as PLDA–SVM. Results also show that PLDA–RVM is much sparser than PLDA–SVM.

@&#INTRODUCTION@&#
Nowadays, utilizing i-vectors (Dehak et al., 2011) as features and probabilistic linear discriminant analysis (PLDA) (Kenny, 2010; Garcia-Romero and Espy-Wilson, 2011; Prince and Elder, 2007) as back-end classifiers are the most popular strategies in speaker verification. Likelihood ratio (LR) scores from two hypotheses are used as verification decisions in i-vector/PLDA systems. Given a test i-vector and a target-speaker i-vector, the two hypotheses are that the test i-vector and the target-speaker i-vector are from the same speaker and that these two i-vectors are from two different speakers. Accordingly, no other i-vectors are involved in the computation of the LR score. this scoring method implicitly uses background information through the universal background model (UBM) (Reynolds et al., 2000), total variability matrix (Dehak et al., 2011), and PLDA's factor loading matrix. Although this LR scoring method is computationally efficient, the implicit use of background information is a drawback.To address the limitation of these scoring methods, PLDA–SVM equipped with empirical kernel maps (EKMs) and support vector machines (SVMs) was proposed to take the background speaker information explicitly into consideration during the scoring process (Mak and Rao, 2013; Rao and Mak, 2014a). This method captures the discrimination between a target-speaker and non-target-speakers in the SVM weights. Specifically, for each target speaker, an empirical score space with dimension equal to the number of enrollment i-vectors of this target speaker is defined by using the idea of empirical kernel maps (Scholkopf et al., 1999; Xiong et al., 2005; Zhang and Mak, 2011). Given an i-vector, a score vector living in this space is formed by computing the LR scores of this i-vector with respect to each of the enrollment i-vectors. A speaker-dependent SVM – referred to as PLDA–SVM – can then be trained using the training score vectors. During verification, given a test i-vector and the target-speaker under test, the LR scores are mapped to a score vector, which is then fed to the target-speaker's SVM to obtain the final test score. The empirical kernel map presented in this paper is related to the anchor model (Sturim et al., 2001; Collet et al., 2006; Noor and Aronowitz, 2006; Mami and Charlet, 2006). However, in the anchor model, a test utterance is projected onto a space represented by a set of reference speakers unrelated to the target-speakers, whereas in the empirical kernel map, the test utterance is projected onto an empirical feature space specific to the claimed speaker.Compared with previous speaker recognition evaluations (SRE), NIST 2012 SRE (NIST, 2012) introduces some new protocols that help researchers to enhance the performance of speaker verification systems. One of the new protocols is that some target speakers have multiple enrollment utterances. PLDA–SVM with empirical kernel maps is not only a novel way of incorporating multiple enrollment i-vectors in the scoring process, but also opens up opportunity for adopting sparse kernel machines in PLDA-based speaker verification systems. Accordingly, this paper proposes to incorporate the empirical kernel maps into a sparse kernel machine known as the relevance vector machine (RVM) (Tipping, 2001). The main difference between SVM and RVM lies in the learning methods. The former is based on structural risk minimization, whereas the latter is based on a fully probabilistic framework. RVMs do not suffer from the limitations of SVM (Tipping, 2001), but can obtain a comparable performance as SVM.RVMs have been applied to speaker identification. For example, Tang et al. (2008) compared the performance of GMM-UBM, SVM, and RVM for text-independent speaker identification under adverse far-field recording conditions with extremely short utterances. The input features of the RVMs in Tang et al. (2008) are MFCC, whereas the input to the RVM in this paper is PLDA score vectors.Comparing with our earlier work (Rao and Mak, 2014b), we provide additional experiments and analyses in this paper. In summary, this paper has three objectives:1.utilizing speaker-dependent score spaces as opposed to the fixed speaker-independent score space used by the conventional anchor model;investigating the property of empirical kernel maps in SVMs, RVM regressions, and RVM classifications;comparing PLDA–RVM with PLDA–SVM from three perspectives: evaluation performance, sparsity, and actual computation time.The paper is organized as follows. Section 2 describes the idea of empirical kernel maps in PLDA speaker verification. Section 3 presents the PLDA–SVM scoring with empirical kernel maps. Section 4 introduces RVM regression and RVM classification using empirical kernel maps. In Sections 5 and 6, we report results based on NIST 2012 SRE. Section 7 summarizes the findings of this work.Given a length-normalized (Garcia-Romero and Espy-Wilson, 2011) test i-vector xtand target-speaker's i-vector xs, the Gaussian-PLDA likelihood ratio score can be computed as follows (Kenny, 2010; Garcia-Romero and Espy-Wilson, 2011; Mak and Rao, 2013):(1)SLR(xt,xs)=P(xt,xs|same speaker)P(xt,xs|different speakers)=const+xtTQxt+xsTQxs+2xtTPxs,where(2)P=Λ−1Γ(Λ−ΓΛ−1Γ)−1;Λ=WWT+ΣQ=Λ−1−(Λ−ΓΛ−1Γ)−1;Γ=WWT.In Eq. (2), W is the factor loading matrix andΣis the covariance of the PLDA model. Eqs. (1) and (2) suggest that PLDA-LR scoring uses the information of background speakers implicitly through W andΣ. To make better use of multiple enrollment utterances of target speakers and to explicitly use the information of background speakers, we have recently proposed a speaker-dependent discriminative model that incorporates the empirical kernel maps for scoring (Mak and Rao, 2013; Rao and Mak, 2014a,b). We refer to the mapping from i-vectors to PLDA score vectors as empirical kernel maps (EKMs).Assume that target-speaker s has Hsenrollment utterances and that each enrollment utterance leads to one i-vector. Then, Hsi-vectors will be obtained. In case the speaker provides one or a very small number of enrollment utterances only, we can apply an utterance partitioning technique (Rao and Mak, 2013) to produce multiple i-vectors from his/her enrollment utterance. Denote these i-vectors as:(3)Xs=xs,1,…,xs,j,…,xs,Hs.Let us denote the set of non-target-speaker i-vectors as11It is not necessary to apply utterance partitioning to non-target speakers because non-target i-vectors are abundant.:(4)Xb=xb,1,…,xb,i,…,xb,B.Therefore,X={Xs,Xb}is the training set for target-speaker s. Because target speakers have different numbers of enrollment utterances, the dimension of the resulting PLDA score vectors is different for different speakers.The empirical kernel map is defined as:(5)S→LR(x,Xs)=SLR(x,xs,1)SLR(x,xs,2)⋮SLR(x,xs,Hs)where SLR(x, xs,j) is defined in Eq. (1). Therefore, the PLDA score space is defined by target-speaker's i-vectors through the PLDA model. Because Hsis typically small, the dimension ofS→LR(x,Xs)is low.Support vector machine (Kung et al., 2005) (SVMs) are well-known supervised learning method used for classification and regression. Assume that we are given N training vectors {x1, …, xN} with labels yn∈{+1, −1}, n=1, …, N. Using the pairs{xn,yn}n=1N, an SVM can be trained (Kung et al., 2005). Given a test vector xt, the SVM's output is written as(6)f(xt;w)=∑i=1NwiK(xt,xi)+w0wherew=[w0,…,wN]are the weights determined by minimizing the error on the training set while maximizing the margin between the two classes,w0is a bias term, and K(xt, xi) is a kernel function. This paper uses PLDA score vectors (via the empirical kernel maps) as the input to the SVMs and applies the speaker-dependent SVMs for i-vector/PLDA speaker verification. Specifically, Eq. (6) is rewritten as:(7)SSVM(xt,Xs,Xb)=∑j∈Ssαs,jK(xt,xs,j)−∑i∈Sbαb,iK(xt,xb,i)+w0whereSsandSbcontain the indexes of the support vectors corresponding to the speaker class and impostor class, respectively. αs,jand αb,iare the Lagrange multipliers of the SVM. The relationship between w and α can be expressed aswn=αnyn. K(xt, xs,j) is a kernel function with the form:(8)K(xt,xs,j)=KS→LR(xt,Xs),S→LR(xs,j,Xs)whereK(·,·)is a standard SVM kernel, e.g., linear or RBF. Only RBF kernelK(x,y)=exp−∥x−y∥22γ2was adopted in this paper. K(xt, xb,i) can be obtained by replacing xs,jin Eq. (8) with xb,i.While our earlier studies (Mak and Rao, 2013; Rao and Mak, 2014a) have demonstrated that PLDA–SVM scoring (Eq. (7)) performs better than simple PLDA scoring (Eq. (1)), the SVMs in Eq. (7) still has some limitations (Tipping, 2001).1.Although SVM is a sparse model, the number of support vectors increases linearly with the size of the training set. In our case, this property limits the value of B (Eq. (4)) for training the SVMs.The SVM scores in Eq. (7) are not probabilistic, meaning that score normalization may be needed to adjust the score range of individual SVMs.To achieve the best performance, it is necessary to strike a compromise between the training error and the margin of separation through adjusting the penalty factor for each target speaker during SVM training. Given the limited number of enrollment utterances for some speakers, this is not easy to achieve.To overcome the first and third limitations of SVMs mentioned in Section 3, this paper proposes incorporating the empirical kernel maps into another sparse kernel machine known as the relevance vector machine (RVM) (Tipping, 2001), which leads to the PLDA–RVM scoring. To overcome the first limitation of SVM, PLDA–RVM makes use of the property of RVM to ensure that the number of relevant vectors does not grows linearly with the number of training vectors. To overcome the third limitation, PLDA–RVM takes the advantage of RVM by noting that there is no penalty factor in RVM training. As a result, only one hyper-parameter (the RBF width) needs to be adjusted.In terms of output scoring, RVM (Tipping, 2001) and SVM have the same form (Eq. (6)). However, their learning methods are very different. The training of SVMs is based on structural risk minimization (Vapnik, 1998), whereas RVM training is based on Bayesian relevance learning (Tipping, 2001) so that it provides a Bayesian treatment of Eq. (6). RVMs have two modes of operations: regression and classification. They are elaborated in the following subsections.Assume that for target speaker s, we have a set of training i-vectorsX={Xs,Xb}as in Eqs. (3) and (4) and that yn=1 whenxn∈Xsand yn=−1 whenxn∈Xb. When an RVM is applied to regression, the targets yn's are assumed to be sampled from the following model22To simplify notations in subsequence equations, we drop the subscripts s and b that annotate the target speaker and background speakers, respectively.:yn=f(xn;w)+ϵn,n=1,…,N,whereN=|Xs|+|Xb|, f(xn;w) is given by Eq. (6), and ϵnfollows a Gaussian distribution with zero mean and variance σ2. This is equivalent to say thatp(yn|xn)=N(yn|f(xn;w),σ2).Assume also that yn's (n=1, …, N) are independent, the likelihood of the training data set can be written as:(9)p(y|w,σ2)=(2πσ2)−N2exp−12σ2∥y−Φw∥2=Ny|Φw,σ2Iwhere(10)y=[y1,…,yN]T;w=[w0,…,wN]T;Φ=[ϕ(x1),ϕ(x2),…,ϕ(xN)]Tϕ(xi)=[1,K(xi,x1),K(xi,x2),…,K(xi,xN)]T.To avoid over-fitting, RVM defines a zero-mean Gaussian prior distribution over w:(11)p(w|α)=∏i=0NN(wi|0,αi−1)=N(w|0,A−1)whereα=[α0, α1, …, αN]T, αiis the hyperparameter associated with weightwiand A=diag(α0, α1, …, αN).Given the distribution of y in Eq. (9) and the prior distribution of w in Eq. (11), we can use the formula of conditional Gaussians (Eq. (2.116) in Bishop, 2006) to obtain the posterior distribution over the weights as follows33To use Eq. (2.116) of Bishop (2006), we consider x and y in Eq. (2.116) as our w and y, respectively. Also,Λand L in Eqs. (2.113) and (2.114) are our A and σ−2I, respectively. Moreover,μand b in Eqs. (2.113) and (2.114) are zero vectors in our case.:(12)p(w|y,X,α,σ2)=N(w|μ,Σ)where(13)μ=σ−2ΣΦTyandΣ=(σ−2ΦTΦ+A)−1.The optimal value ofαand σ2 can be obtained by maximizing the following marginal likelihood with respect toαand σ2:(14)p(y|X,α,σ2)=∫p(y|X,w,σ2)p(w|α)dw=∫N(y|Φw,σ2I)N(w|0,A−1)dw=N(y|0,σ2I+ΦA−1ΦT).Setting∂lnp(y|X,α,σ2)∂αi=0and∂lnp(y|X,α,σ2)∂σ−2=0,we obtain the following updated formulae for αiand σ2 (see Appendix A for the derivations):(15)αinew=γiμi2and(σ2)new=∥y−Φμ∥2N−∑i=0Nγiwhere μiis the ith component ofμin Eq. (13) and γi=1−αiΣiiwith Σiibeing the ith diagonal element ofΣin Eq. (13). During the optimization, many of the hyperparameters αitend to infinity and the corresponding weightswibecome zero; the vectors xicorresponding to the non-zero weights are considered as relevance vectors.By considering w probabilistic and using the notion of conditional independence (Bishop, 2006), the predictive distribution of ytgiven a test vector xtis(16)p(yt|y,xt,X)=∫σ2∫α∫wp(yt|xt,w,α,σ2)p(w,α,σ2|y,X)dwdαdσ2where(17)p(yt|xt,w,α,σ2)=p(yt|xt,w,σ2)(18)p(w,α,σ2|y,X)=p(w|y,X,α,σ2)p(α,σ2|y,X).Instead of computing the posterior p(α, σ2|y) in Eq. (17), Tipping (2001) used a delta function at the most probable values ofαand σ2 as an approximation. Therefore, using Eq. (18) and assuming uniform priors forαand σ2, Eq. (16) reduces to(19)p(yt|y,xt,X)=∫σ2∫α∫wp(yt|xt,w,αMP,σMP2)p(w|y,X,αMP,σMP2)p(αMP,σMP2|y)dwdαdσ2=∫wp(yt|xt,w,αMP,σMP2)p(w|y,X,αMP,σMP2)dw=∫wN(yt|ϕ(xt)Tw,σMP2)N(w|μMP,ΣMP)dwwhere(20)(αMP,σMP2)=argmaxα,σ2p(α,σ2|y,X)=argmaxα,σ2p(y|α,σ2,X)p(α)p(σ2)=argmaxα,σ2∫p(y|w,σ2,X)p(w|α)dw=argmaxα,σ2∫N(y|Φw,σ2I)N(w|0,A−1)dw=argmaxα,σ2N(y|0,σ2I+ΦA−1ΦT).and(21)μMP=σMP−2ΣMPΦTyandΣMP=σMP−2ΦTΦ+A−1.Because both terms in the integrand of Eq. (19) are Gaussians, the predictive distribution is also a Gaussian:44Again, we make use of the marginal and conditional Guassian formulae in Eqs. (2.113)–(2.115) of Bishop (2006) to derive Eq. (23). Specifically, in Eqs. (2.113)–(2.115) of (Bishop, 2006), we substitute x by w, y by yt,μbyμMP,Λ−1 byΣMP, L−1 byσMP2, b by 0, and A by ϕ(x)T.(22)p(yt|y,X,xt,αMP,σMP2)=N(yt|g(xt),σt2)with(23)g(xt)=μMPTϕ(xt)andσt2=σMP2+ϕ(xt)TΣMPϕ(xt).To use RVM regression for PLDA-based speaker verification, we train one RVM regressor for each target speaker, i.e., each speaker has his/her ownμMP in Eq. (21). During verification, given a test i-vector xtand a target speaker s, we use the posterior mean g(xt) in Eq. (23) as the verification score. More specifically,(24)SRVM-R(xt,Xs,Xb)=g(xt)=μMP,sTϕ(xt,Xs,Xb)where(25)ϕ(xt,Xs,Xb)=[1,K(xt,xs,1),…,K(xt,xs,Hs),K(xt,xb,1),…,K(xt,xb,B)]TWhen RVM is applied to classification, the target conditional distribution p(y|x) is assumed to follow a Bernoulli distribution. Assume a set of training i-vectorsX={Xs,Xb}as in Eqs. (3) and (4) and yi=1 whenxi∈Xsand yi=0 whenxi∈Xbfor target speaker s, the likelihood of the training data set can be written as (Tipping, 2001):(26)p(y|w)=∏i=1Nσf(xi;w)yi1−σf(xi;w)1−yi,yi∈{0,1}.where(27)N=|Xs|+|Xb|;y=[y1,…,yN]T;w=[w0,…,wN]Tand σ{·} is the logistic sigmoid link functionσz=11+e−z. Similar to RVM regression, RVM classification also introduces a zero-mean Gaussian prior distribution over w as defined in Eq. (11).Using Eqs. (26) and (11), we can obtain the posterior distribution of w:(28)p(w|y,α)=p(y|w)p(w|α)∫p(y|w)p(w|α)dw=g(w)p(y|α),where we have defined g(w)≡p(y|w)p(w|α). Taking logarithm of g(w), we have(29)logg(w)=logp(y|w)p(w|α)=∑i=1Nyilogσ(f(xi;w))+(1−yi)log1−σ(f(xi;w))−12wTAw+const=∑i=1Nyilogσϕ(xi)Tw+(1−yi)log1−σϕ(xi)Tw−12wTAw+const,where we have used f(xi;w)=ϕ(xi)Tw. Note that because p(y|w) in Eq. (26) is not a Gaussian, we cannot analytically perform the integration in Eq. (28) to obtain a closed-form solution for p(w|y,α). One possible solution is to use the Laplace's method (Bishop, 2006) to approximate p(w|y,α) by a Gaussian distribution. The idea is to find a Gaussian approximation q(w) with mean w0 equals to a mode of p(w|y,α). This can be achieved by approximating logg(w) to a Taylor expansion around w0:(30)logg(w)≈logg(w0)−12w−w0THw−w0,where H is a Hessian matrix(31)H=−∇∇wlogg(w)|w=w0(32)=∂∂w∂wTlogg(w)|w=w0=∑i=1Nσϕ(xi)Tw01−σϕ(xi)Tw0ϕ(xi)Tϕ(xi)+A=ΦTBΦ+Awhere B is an (N+1)×(N+1) diagonal matrix with diagonal elements(33)bii=σϕ(xi)Tw01−σϕ(xi)Tw0,andΦandϕ(xt) are defined in Eq. (10).The value of w0 can be obtained by using iterative reweighted least squares (IRLS) (Bishop, 2006) as follows:(34)w0new=w0old−(Hold)−1∇wlogg(w)|w=w0oldwhere∇wlogg(w)=ΦTy−σ(ϕ(x1)Tw),…,σ(ϕ(xN)Tw)T−Aw.At convergency, the gradient is zero and therefore we havew0→A−1ΦTy−σ(ϕ(x1)Tw0),…,σ(ϕ(xN)Tw0)TTaking exponential of Eq. (30) and noting that q(w)∝g(w), we have(35)q(w)=|H|1/2(2π)(N+1)/2exp−12w−w0THw−w0=N(w|w0,H−1),which is a Gaussian distribution with mean w0 and covariance matrix H−1.We then use q(w) to approximate the posterior p(w|y,α) around the mode w0. Comparing the covariance matrix H−1 in Eq. (31) with that in Eq. (2.117) of Bishop (2006) reveals that B is the precision matrix of p(y|w) (see Eq. (2.114) of Bishop, 2006). As a result, using Eq. (2.116) of Bishop (2006), we obtain the posterior mean of the weights in p(w|y,α) as(36)wMP=H−1ΦTBy.Given Eqs. (15), (31), (33), (34), (36), we may proceed the estimation ofαas follows. First, we initializeαto obtain A. Then, we initialize w and use Eq. (34) to estimate w0. We then plug this w0into Eq. (31) and Eq. (33) to obtain H and B, respectively, followed by estimating wMP using Eq. (36). A new estimation ofαis then obtained by maximizing the likelihood p(y|α), i.e., using Eq. (15) without σ2. Then, the cycle is repeated.To apply RVM classification for PLDA-based speaker verification, we train one RVM classifier for each speaker, i.e., each speaker his/her own wMP in Eq. (36). During verification, given a test i-vector xtand a target speaker s, we useσwMP,sTϕ(xt)as the score. More precisely,(37)SRVM-C(xt,Xs,Xb)=σwMP,sTϕ(xt,Xs,Xb)where(38)ϕ(xt,Xs,Xb)=[1,K(xt,xs,1),…,K(xt,xs,Hs),K(xt,xb,1),…,K(xt,xb,B)]Tand K(xi, xj) is defined in Eq. (8). For the convenience of plotting DET curves, this paper uses the linear function (σ(z)=z) instead of logistic sigmoid link function in Eq. (37).The core set of NIST 2012 Speaker Recognition Evaluation (SRE) (NIST, 2012) was used for performance evaluation. This paper focuses on the phonecall speech of the core task, i.e., Common Evaluation Conditions 2, 4, and 5. Hereafter, we use “CC” to denote common evaluation conditions. In the evaluation dataset, no noise was added to the test segments of CC2, whereas noise was added to the test segments of CC4 and test segments in CC5 were collected in a noisy environment. All of these conditions contain training segments with variable length and variable numbers of training segments per target speaker. We removed the 10-second utterances and the summed-channel utterances from the training segments of NIST 2012 SRE but ensured that all target speakers have at least one long utterance for enrollment. The speech files in NIST 2005–2010 SREs were used as development data for training the UBM, total variability matrix, LDA-WCCN, PLDA models, SVMs and RVMs.We used our voice activity detector (Yu and Mak, 2011; Mak and Yu, 2014) to detect the speech regions of each utterance. 19 MFCCs together with energy plus their 1st- and 2nd-derivatives were extracted from the speech regions, followed by cepstral mean normalization (Atal, 1974) and feature warping (Pelecanos and Sridharan, 2001) with a window size of 3 seconds. A 60-dim acoustic vector was extracted every 10ms, using a Hamming window of 25ms.To improve noise robustness, we followed the suggestions in Leeuwen and Saeidi (2013) to add noise to the training files. To this end, we constructed a noise dataset comprising 13 real crowd noise files and 17 heating, ventilation, and air conditioning (HVAC) noise files from Freesound (2005) and 10 artificial crowd noise files generated by summing 441 utterances from male and female speakers in pre-2012 NIST SRE. For each training file with SNR above 15dB, we generated two noisy speech files at an SNR of 6dB and 15dB by randomly selecting two noise files from the noise dataset. For each training file with SNR between 6dB and 15dB, we produced a noisy speech file at 6dB. The SNRs of test files were estimated by the speech voltmeter function in FaNT http://dnt.kr.hsnr.de/download.html and the VAD decisions. Specifically, we used the VAD (Yu and Mak, 2011; Mak and Yu, 2014) to determine speech and non-speech regions in a speech file. Then, energies of the speech and non-speech parts as determined by the voltmeter function of FaNT were used for estimating the SNR.Because the test conditions involve phonecall speech only, only telephone utterances were selected as enrollment utterances for matching the channel between enrollment and test sessions. Although many target speakers in NIST 2012 SRE have multiple training segments, some of them have a few training segments only. More precisely, after removing the 10-second segments and summed-channel segments, 50 out of 723 male target speakers and 65 out of 1095 female target speakers have one long training segment only. To provide more speaker-class i-vectors for creating the empirical kernel maps and for training the SVMs/RVMs for these speakers, we used a technique called utterance partitioning with acoustic vector resampling (UP-AVR) (Mak and Rao, 2011; Rao and Mak, 2013). Specifically, for each conversation, a sequence of acoustic vectors is extracted. Then, the sequence is partitioned into N equal-length segments, and an i-vector is estimated from each segment. If more i-vectors are required, the acoustic vectors in the sequence are randomly reshuffled and the partitioning process is repeated to produce another N vectors. If this partitioning-randomization process is repeated R times, (RN+1) i-vectors can be obtained from a single conversation, where the additional one is obtained from the entire acoustic sequence.Our earlier studies (Rao and Mak, 2011, 2013; Mak and Rao, 2011) suggest that to facilitate the SVM training algorithm to find a good decision boundary, it is necessary to minimize the imbalance between the numbers of speaker-class and impostor-class training vectors. To this end, we propose the following rule to produce Hsspeaker-class training i-vectors for speaker s:(39)Hs=17,|Us|<17|Us|,|Us|≥17whereUsis the set of full-length enrollment utterances and|Us|represents its cardinality. To make the full use of the enrollment utterances of a target speaker, UP-AVR (N=4 and R=4) was performed on each full-length enrollment utterance to produce a sub-utterance set, one for each utterance. Then we selected sub-utterances from these sets uniformly to make the total number of enrollment utterances equal to 17. For example, if|Us|of a target speaker is equal to 5, 5 sub-utterances sets are produced from these 5 full-length enrollment utterances by UP-AVR (N=4 and R=4). Each sub-utterance set contains 16 sub-utterances. To fully use the enrollment utterances of this target speaker, 3 sub-utterances were extracted from each of two sub-utterances sets and 2 sub-utterances were extracted from each of other three sub-utterances sets. Eventually, we have 12 sub-utterances and 5 full-length utterances for this target speaker.We adopted known non-targets (Rao and Mak, 2014a) to train SVMs and RVMs in this paper. For each target speaker, 500 competing known non-target speakers were randomly selected and each known non-target speaker provides one utterance. Therefore, 500 utterances were used to train his/her SVM/RVM for all common conditions. Note that RVM classification applies a logistic link function (Eq. (37)) to compute the probabilistic outputs (posterior probabilities of the target-speaker class) (Tipping, 2001). While probabilistic outputs are desirable when the classification task involves one RVM only, in NIST SRE, we have one RVM per target speaker and the performance indexes (EER, minDCF, and DET) are based on the scores of all true-speaker trials and impostor attempts. This will lead to two skewed score-distributions with modes close to 1 and 0 for true-speaker trials and impostor attempts, respectively. Although these skewed distribution do not hurt the performance of SRE, we only apply the logistic sigmoid function during the training of RVM classifiers and dropped the function during scoring so that the score distribution of RVM classification is consistent with that of other methods. More precisely, Eq. (6) was used for computing the verification scores in the classification mode of RVMs and SVMs in our experiments.The i-vector systems are based on a gender-dependent UBM with 1024 mixtures. 3500 microphone utterances and 3501 telephone utterances from NIST 2005–2008 SREs were used for training the male UBM. 4177 microphone utterances and 4178 telephone utterances from NIST 2005–2008 SREs were used for training the female UBM. We selected 14,875 telephone and interview conversations from 575 speakers in NIST 2006–2010 SREs to estimate male total variability matrix with 400 total factors and 20,656 telephone and interview conversations from 889 speakers in NIST 2006–2010 SREs to estimate female total variability matrix with 400 total factors.According to Lei et al. (2012), adding noisy data to train the UBM and total variability matrix receives very small gains. Hence, we followed the steps in Lei et al. (2012) and only applied noisy data to train the LDA and PLDA parameters. For the common condition without added noise (CC2), we used clean utterances from NIST 2006–2010 SREs to train the PLDA models. For the common conditions with added noise (CC4 and CC5), we pooled clean utterances, utterances at 6dB SNR, and utterances at 15dB SNR to estimate the loading matrix.We used within-class covariance normalization (WCCN) (Hatch et al., 2006) for whitening (McLaren et al., 2012) the i-vectors, followed by vector-length normalization (Garcia-Romero and Espy-Wilson, 2011). Then, we performed linear discriminant analysis (LDA) (Bishop, 2006) and WCCN on the resulting vectors to reduce the dimension to 200 before training the PLDA models with 150 latent variables.

@&#CONCLUSIONS@&#
This paper investigates the property of empirical kernel maps in SVM and RVM and compares the performance among these three classifiers in PLDA-based speaker verification. Experimental results show that PLDA–RVM is more sparse than PLDA–SVM, but it achieves comparable performance as PLDA–SVM. In addition, this paper also provides one way to speed up the processing time of sparse kernel machines with EKMs. The idea of combining RVM with PLDA can be further explored in future work. For example, it is interesting to exploit the property that the kernel function used in RVM do not need to fulfill the Mercer's condition.