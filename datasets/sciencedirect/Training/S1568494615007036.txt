@&#MAIN-TITLE@&#
Three-way decision reduction in neighborhood systems

@&#HIGHLIGHTS@&#
We define the three-way decision reducts of positive region preservation, boundary region preservation and negative region preservation with the neighborhood rough set model.Three condition entropy measures are constructed based on three-way decision regions by considering variants of neighborhood classes.The monotonic principles of entropy measures are proved, from which we can obtain the heuristic reduction algorithms in neighborhood systems.

@&#KEYPHRASES@&#
Rough set theory,Three-way decisions,Attribute reduction,Neighborhood systems,

@&#ABSTRACT@&#
Rough set reduction has been used as an important preprocessing tool for pattern recognition, machine learning and data mining. As the classical Pawlak rough sets can just be used to evaluate categorical features, a neighborhood rough set model is introduced to deal with numerical data sets. Three-way decision theory proposed by Yao comes from Pawlak rough sets and probability rough sets for trading off different types of classification error in order to obtain a minimum cost ternary classifier. In this paper, we discuss reduction questions based on three-way decisions and neighborhood rough sets. First, the three-way decision reducts of positive region preservation, boundary region preservation and negative region preservation are introduced into the neighborhood rough set model. Second, three condition entropy measures are constructed based on three-way decision regions by considering variants of neighborhood classes. The monotonic principles of entropy measures are proved, from which we can obtain the heuristic reduction algorithms in neighborhood systems. Finally, the experimental results show that the three-way decision reduction approaches are effective feature selection techniques for addressing numerical data sets.

@&#INTRODUCTION@&#
Feature reduction is a quite useful data preprocessing technique, aiming to determine a minimal feature subset from a problem domain while retaining a suitably high classification accuracy of decision systems. It is also mentioned as a semantic-preserving dimension reduction [15,7], attribute reduction [47] and feature selection [8], applied in many areas including pattern recognition [36], machine learning [46], data mining [26] and big data [32], etc. As to the machine learning problems, feature reduction is an important preprocess to achieve the essence by deleting noisy, irrelevant or misleading features.Rough set theory (RST) [29], proposed by Pawlak, has been used successfully as a reduction tool to discover data dependencies by reducing the redundant features that contained in a data set. In the last two decades, many methods for feature reduction have been developed in researches of rough set theory [30,33,28,34,18,38]. According to the different knowledge representations, methods of rough set reduction are mainly classified into two categories: algebra methods and information entropy methods [27]. The methods of algebra representation include positive region approaches and discernibility matrix approaches. In the algebra representations, a reduct is defined by a positive region preservation or a discernibility matrix function. Pawlak in [30] proposed a reduct of positive region preservation that does not vary the positive region or the quality of classification. Skowron and Rauszer in [33] introduced a new approach to knowledge reduction by providing a function of discernibility matrix that can obtain all reducts of a given data set. In the information entropy representations, a reduct is defined by Shannon's entropy and its extensions. Miao and Hu in [28] proposed an entropy measure for information systems and a mutual information measure for decision systems. Furthermore, they presented two heuristic reduction algorithms based on the information entropy and mutual information respectively. Slezak in [34] discussed the applications of feature reduction in data mining area by an entropy measure and extracted decision rules from big data sets. Liang et al. in [18] introduced a new information entropy to incomplete data reduction process for measuring the uncertainty of incomplete information systems. Wang et al. in [38] developed a novel reduction algorithm based on the condition entropy of a decision system.Since the real world existing massive uncorrect, uncertain and noisy data, the Pawlak rough set theory is extended by introducing probabilistic theory. The three-way decision theory proposed by Yao in [44,45] comes from Pawlak rough set theory and probabilistic theory [24,27,40,41]. Its main purpose is to interpret the positive, negative and boundary regions of rough sets as three decisions outcomes: acceptance, rejection and uncertainty (or deferment) in a ternary classification respectively [9]. In recent years, many researchers mainly focus on model extensions and practical applications of three-way decisions. The one category is model researches on three-way decisions. It mainly contains the extension models of rough sets, such as decision-theoretic rough sets [2,42], variable precision rough sets [17,52], Bayesian rough sets [35], fuzzy rough sets and rough fuzzy sets [3], interval-valued fuzzy rough sets [5,10] and dominance-based fuzzy rough sets [4]. The another category is practical applications of three-way decisions, such as government decisions [25], text classification [19], information filtering [20], email filtering [16], investment decisions [23], cluster analysis [22] and attribute reduction of three-way decisions [27].However, the Pawlak rough set reduction and three-way decision reduction are also established on the equivalence approximate space and only compatible for categorical data sets. They need to scatter the records when processing continuous numerical data, this will lead to losing of information (including the neighborhood structure information and order structure information in real spaces) [15,11], so the reducts of numerical data sets are strongly related with the methods of scatting. To overcome this drawback, many extensions of Pawlak rough set theory and their corresponding definitions on attribute reduction have been presented, such as fuzzy rough sets [7,43,12], tolerance approximate models [31], similarity rough approximate models [37], dominance approximation relation models [6], covering approximation models [49–51] and neighborhood granular models [21,39]. Among all the extensions, the neighborhood rough set model [13,14] can be regarded as a specified implementation of the neighborhood granular model. The neighborhood rough set model can process both numerical and categorical data sets via the δ-neighborhood set, which will not break the neighborhood structure and order structure of data sets in real spaces.However, there are some premature theories and disadvantages about three-way decision reduction. The monotonicity of decision regions (positive region, boundary region or negative region) preservation reductions no longer holds in the three-way decision systems. The monotonicity is very important for constructing heuristic reduction algorithms, which can quickly converge to be an attribute reduct. Monotonously heuristic information that can guide searches for the attribute reduction in the three-way decision model is unavailable. Therefore, monotonous measure functions need to be developed to design heuristic reduction algorithms in the three-way decision model. In addition, few studies have been published on three-way decision reduction in neighborhood systems. In this paper, we propose a novel method for the attribute reduction of neighborhood decision systems based on three-way decision theory. We review some concepts related to attribute reductions with neighborhood rough sets and focus on the definitions of attribute reducts based on decision region preservation of three-way decisions in neighborhood decision systems. As monotonously heuristic information to design attribute reduction algorithms of the three-way decisions in neighborhood systems is lacking. Hence, we propose three monotonic measure functions by considering variants of the condition information entropy. Finally, heuristic computing methods of three types of decision region preservation reducts are also given.The rest of this paper is structured as follows. Section 2 describes Pawlak rough set theory, three-way decision theory and the neighborhood rough set model. Section 3 introduces the positive region preservation reduct, the boundary region preservation reduct and the negative region preservation reduct based on a three-way decision model in neighborhood systems, and gives the computing methods for three types of decision region preservation reducts by introducing three novel monotonic measures. Furthermore, we develop heuristic algorithms to obtain three types of decision region preservation reducts. Section 4 provides an experimental analysis, including the theoretic analysis and the effectiveness of the proposed attribute reduction methods. Section 5 concludes the paper, and proposes further work in this area.In this section, we recall the basic notions related to Pawlak rough sets [30], three-way decisions [44,47] and neighborhood rough sets [14].Definition 1[29] A decision system is a four-tuple: S=(U, C∪D, V, f), where U={x1, x2, …, xn} is a finite non-empty set of objects called universe, C is a non-empty finite set of condition attributes, D is a finite set of decision attributes, C∩D=∅; Vais a non-empty set of values of a∈(C∪D), and f:U×(C∪D)→Vais an information function that maps an object in U to exactly one value in Va.For brevity, a decision system is denoted by S=(U, C∪D).Definition 2[29] Given a decision system S=(U, C∪D), for a subset B⊆C, an indiscernibility relation is defined by:(1)IND(B)={(x,y)∈U×U|∀b∈B,f(x,b)=f(y,b)}.Obviously, IND(B) is an equivalence relation, which is reflexive, symmetric and transitive. The family of all equivalence classes of IND(B) will be denoted by U/IND(B), or simply U/B; an equivalence class of IND(B) containing x will be denoted by [x]B.[29] Given a decision system S=(U, C∪D), for an attribute subset B⊆C and an object subset X⊆U, the lower and upper approximations of X with respect to B are defined by:(2)B*(X)={x∈U|[x]B⊆X}=∪{[x]B|[x]B⊆X};(3)B*(X)={x∈U|[x]B∩X≠∅}=∪{[x]B|[x]B∩X≠∅}.The ordered pair 〈B*(X), B*(X)〉 is called a Pawlak rough set of X with respect to the equivalence relation IND(B). According to the lower and upper approximations, one can obtain the positive, boundary and negative regions [29]:(4)POSB(X)=B*(X);(5)BNDB(X)=B*(X)−B*(X);(6)NEGB(X)=U−POSB(X)∪BNDB(X)=U−B*(X).The positive region POSB(X) consists of all objects that are definitely contained in the set X. The negative region NEGB(X) consists of all objects that are definitely not contained in the set X. The boundary region BNDB(X) consists of all objects that may be contained in X. Because approximations are from equivalence classes, inclusion into the boundary region reflects uncertainty about the classification of objects.A classical attribute reduct in Pawlak rough set model is a relative reduct with respect to the decision attribute D, which is defined by requiring that the positive region of the decision attribute D is unchanged.Definition 4[29] Given a decision system S=(U, C∪D), and U/D={D1, D2, …, Dn}, an attribute set R⊆C is a Pawlak reduct of C with respect to D if it satisfies the following two conditions:(1)POSR(D)=POSC(D);for any attribute a∈R, POSR−{a}(D)≠POSC(D),In this definition, condition (1) is also called a positive preservation condition and condition (2) is called a set independency condition.In this subsection, we introduce the basic concepts related to the three-way decision theory proposed by Yao in [44]. There are only two states and three actions (accept, defer and reject). The state set Ω=(X, ⌝X) indicates that an element is in X and not in X, and the action set is A={aP, aB, aN}, where aP, aBand aNrepresent the three actions of deciding that an object is in the sets POS(X), BND(X) and NEG(X), respectively. Moreover, when an object belongs to X, let λPP, λBPand λNPdenote the costs of taking the actions aP, aBand aN, respectively; when an object does not belong to X, then let λPN, λBNand λNNdenote the costs of taking the same three actions, respectively. The loss functions regarding the states X and ⌝X can be expressed by a 2×3 matrix, as follows:aPaBaNXλPPλBPλNP⌝XλPNλBNλNNGiven a decision system S=(U, C∪D), for a subset B⊆C, [x]Bdenotes the equivalence class of x with respect to IND(B), and the probabilities for the two complement states are denoted byP(X|[x]B)=|X∩[x]B||[x]B|and P(⌝X|[x]B)=1−P(X|[x]B).From the matrix, the expected loss associated with taking different actions can be expressed by:R(aP|[x])=λPPP(X|[x]B)+λPNP(⌝X|[x]B);R(aB|[x])=λBPP(X|[x]B)+λBNP(⌝X|[x]B);R(aN|[x])=λNPP(X|[x]B)+λNNP(⌝X|[x]B).IfR(aP|[x]B)≤R(aB|[x]B)andR(aP|[x]B)≤R(aN|[x]B), then decide x∈POSB(X);IfR(aB|[x]B)≤R(aP|[x]B)andR(aB|[x]B)≤R(aN|[x]B), then decide x∈BNDB(X);IfR(aN|[x]B)≤R(aP|[x]B)andR(aN|[x]B)≤R(aB|[x]B), then decide x∈NEGB(X).By the reasonable loss conditions, λPP≤λBP<λNP, λNN≤λBN<λPN, the minimum-risk decision rules (P)–(B) can be written as:(P1)If P(X|[x]B)≥α and P(X|[x]B)≥γ, then decide x∈POSB(X);If P(X|[x]B)≤α and P(X|[x]B)≥β, then decide x∈BNDB(X);If P(X|[x]B)≤β and P(X|[x]B)≤γ, then decide x∈NEGB(X),α=λPN−λBN(λPN−λBN)+(λBP−λPP)=λ(P−B)Nλ(P−B)N+λ(B−P)P;β=λBN−λNN(λBN−λNN)+(λNP−λBP)=λ(B−N)Nλ(B−N)N+λ(N−B)P;γ=λPN−λNN(λPN−λNN)+(λNP−λPP)=λ(P−N)Nλ(P−N)N+λ(N−P)P.Rule (B1) indicates that 0≤β<γ<α≤1. Thus, the following three-way decisions are obtained:(P2)If P(X|[x]B)≥α, then decide x∈POSB(X);If β<P(X|[x]B)<α, then decide x∈BNDB(X);If P(X|[x]B)≤β, then decide x∈NEGB(X).Hence, the thresholds α, β are determined by the loss functions λ, and the three regions are established:POSBα,β(X)={x|P(X|[x]B)≥α};BNDBα,β(X)={x|β<P(X|[x]B)<α};NEGBα,β(X)={x|P(X|[x]B)≤β}.The two-category case is a basic item for the three-way decision model in which only the three-way regions (POS, NEG and BND) can fully describe the region structures. The two-category case can be generalized to the multi-category case and can also be used to conduct relevant degeneration detection.Definition 5[48] Given a decision system S=(U, C∪D), for a subset B⊆C, and the thresholds α, β, the positive, boundary and negative regions of D with respect to B in three-way decisions are defined by:POSBα,β(D)=∪X∈U/DPOSBα,β(X);BNDBα,β(D)=∪X∈U/DBNDBα,β(X);NEGBα,β(D)=U−POSBα,β(D)∪BNDBα,β(D).The quantitative preservation reducts of decision regions in the three-way decisions are that decision regions with respect to the decision partition U/D remained quantitatively unchanged. The quantitative preservation reducts of decision regions are defined as follows.Definition 6[48] Given a decision system S=(U, C∪D), for a subset R⊆C, and the thresholds α, β,(1)R is a quantitative positive region preservation reduct of S if it satisfies:(I)|POSRα,β(D)|=|POSCα,β(D)|;∀B⊂R,|POSBα,β(D)|≠|POSRα,β(D)|.R is a quantitative boundary region preservation reduct of S if it satisfies:(I)|BNDRα,β(D)|=|BNDCα,β(D)|;∀B⊂R,|BNDBα,β(D)|≠|BNDRα,β(D)|.R is a quantitative negative region preservation reduct of S if it satisfies:(I)|NEGRα,β(D)|=|NEGCα,β(D)|;∀B⊂R,|NEGBα,β(D)|≠|NEGRα,β(D)|.In this definition, conditions (I) of the three types of reduct definitions are also called the region preservation conditions, and conditions (II) of the three types of reduct definitions are called the independency conditions.In general, a neighborhood decision system can be denoted by NS=(U, C∪D, δ), where U={x1, x2, …, xn} is a nonempty finite set of objects called the universe, C={a1, a2, …, am} is a condition attribute set, D is a decision attribute set, and δ is a neighborhood parameter (0≤δ≤1).The neighborhood rough set model is established on the neighborhood relation which uses a distance function. Neighborhood relation is a similarity relation based on a neighborhood parameter for estimating two real numbers.Definition 7[14] Let a neighborhood decision system NS=(U, C∪D, δ) and a distance function f:U×U→[0, 1], for a non-empty subset B⊆C and a neighborhood parameter δ∈[0, 1], B determines a similarity relation denoted by NRδ(B) in the following,(7)NRδ(B)={(x,y)∈U×U|fB(x,y)≤δ}.The neighborhood classnBδ(x)of x∈U in the subspace B is defined by:(8)nBδ(x)={y|x,y∈U,fB(x,y)≤δ}.The fB(x, y) is a distance function, which satisfies:(1)fB(x, y)≥0: Distances cannot be negative;fB(x, y)=0: If and only if x=y;fB(x, y)=fB(y, x): Distance is symmetric;fB(x, y)+fB(y, z)≥fB(x, z): Triangular inequality.A distance function is a metric on a set of points, mapping pairs of points into the non-negative real numbers. In general, there are three metrics those are widely used.[14] Let x and y be two objects in N-dimensional feature space C={a1, a2, …, aN},v(x,ai)denotes the value of sample x in the ith dimension ai, then a general metric, named Minkowsky distance, is defined by:(9)fC(x,y)=(∑i=1N|v(x,ai)−v(y,ai)|p)1/p,where (1) it is called Manhattan distance, if p=1; (2) Euclidean distance, if p=2; (3) Chebychev distance, if p=∞.Obviously, neighborhood relation is a similarity relation, which satisfies reflexivity and symmetry. Specially,nBδ(x)is an equivalence class and NRδ(B) is an equivalence relation if δ=0, this case is applicable to categorical data.Definition 9[14] Let NS=(U, C∪D, δ) be a neighborhood decision system and a subset B⊆C, for any object set X⊆U, the neighborhood lower and upper approximations of X on B, are defined by:(10)B*(X)δ={x∈U|nBδ(x)⊆X},(11)B*(X)δ={x∈U|nBδ(x)⋂X≠∅}.And the lower and upper approximations of decision attribute D can be defined in the following.Definition 10[14] Let NS=(U, C⋃D, δ) be a neighborhood decision system, and U/D={D1, D2, …, Dn} be equivalence classes constituted by decision attribute set D on the universe U, for the condition attribute subset B⊆C, the neighborhood lower and upper approximations of D with respect to B are defined by:(12)B*(D)δ=⋃i=1nB*(Di)δ=POSB(D)δ,(13)B*(D)δ=⋃i=1nB*(Di)δ=U,whereB*(Di)δ={xi|nBδ(xi)⊆Di,xi∈U},B*(Di)δ={xi|nBδ(xi)∩Di≠ϕ,xi∈U}.Similar with the definition in rough sets, the lower approximation of the decision is defined by the union of the lower approximation of each decision class. The neighborhood lower approximation of the decision is also called the neighborhood positive region of the decision, denoted by POSB(D)δ, which is the subset of records whose neighborhoods consistently belong to one of the decision classes.[1]Let NS=(U, C⋃D, δ) be a neighborhood decision system. For P, Q⊆C and xi∈U, we have:1.ifQ⊆P, thennQδ(xi)⊇nPδ(xi);if0≤γ≤δ≤1, thennPγ(xi)⊆nPδ(xi);nPδ(xi)⊆⋂p∈Pnpδ(xi);nPδ(xi)≠∅and⋃xi∈UnPδ(xi)=U.Suppose a neighborhood decision system NS=(U, C⋃D, δ) shown in Table 1, where U={x1, x2, x3, x4, x5, x6}, C={a, b, c} and D={d}. It has three real-valued condition attributes and a single categorical decision attribute. In the table, all the value of condition attributes are normalized between 0 and 1. For all the examples in this paper, the Euclidean distance is used and the neighborhood parameter is employed with δ=0.3.The equivalence classes of the decision attribute are calculated by U/D={D1, D2}, where D1={x1, x3, x6} and D2={x2, x4, x5}. The neighborhood classes of objects with respect to the condition attribute set C are generated as follows:nCδ(x1)={x1},nCδ(x2)={x2},nCδ(x3)={x3},nCδ(x4)={x4,x5},nCδ(x5)={x4,x5,x6},nCδ(x6)={x5,x6}.Suppose A={a}, B={b} and P={a, b}, the neighborhood classes of objects with respect to the condition attribute sets A, B and P are generated as follows:nAδ(x1)={x1,x2,x3},nAδ(x2)={x1,x2,x3},nAδ(x3)={x1,x2,x3},nAδ(x4)={x4,x5,x6},nAδ(x5)={x4,x5,x6},nAδ(x6)={x4,x5,x6}; andnBδ(x1)={x1,x3,x4,x5},nBδ(x2)={x2},nBδ(x3)={x1,x3,x4},nBδ(x4)={x1,x3,x4,x5},nBδ(x5)={x1,x4,x5,x6},nBδ(x6)={x5,x6}; andnPδ(x1)={x1,x3},nPδ(x2)={x2},nPδ(x3)={x1,x3},nPδ(x4)={x4,x5},nPδ(x5)={x4,x5,x6},nPδ(x6)={x5,x6}.Obviously, A⊆P⊆C, for ∀xi∈U, thennAδ(xi)⊇nPδ(xi)⊇nCδ(xi). The neighborhood positive regions of D with respect to A,B,P and C are obtained:POSA(D)δ=POSA(D1)δ∪POSA(D2)δ=∅,POSB(D)δ=POSB(D1)δ∪POSB(D2)δ={x2},POSP(D)δ=POSP(D1)δ∪POSP(D2)δ={x1,x3}∪{x2,x4}={x1,x2,x3,x4},POSC(D)δ=POSC(D1)δ∪POSC(D2)δ={x1,x3}∪{x2,x4}={x1,x2,x3,x4}.The reduction process of classical Pawlak rough sets described in [29] can only operate effectively with data sets containing categorical values. As most data sets contain real-valued features, it is necessary to perform a discretization step beforehand. However, the discretization will bring about information loss in the process of dimensionality reduction. By using neighborhood rough sets, it is possible to use this information for better guiding feature reduction.In general, a three-way decision neighborhood system can be denoted by TS=(U, C∪D, δ, α, β), where U={x1, x2, …, xn} is a nonempty finite set of objects called the universe, C={a1, a2, …, am} is a condition attribute set, D is a decision attribute set, δ is a neighborhood parameter (0≤δ≤1), and α, β (0≤β<α≤1) are thresholds for the three regions pair-wise disjoint.Definition 11Let TS=(U, C∪D, δ, α, β) be a three-way decision neighborhood system, and U/D={D1, D2, …, Dm} be an equivalence class set constituted by decision attribute D on the universe U. For the condition attribute subset B⊆C, the positive, boundary and negative regions of D with respect to B in the three-way decision system are defined by:(14)POSB(α,β)(D)δ=⋃i=1m{x|x∈Di,α<p(Di|nBδ(x))≤1},(15)BNDB(α,β)(D)δ=⋃i=1m{x|x∈Di,β<p(Di|nBδ(x))≤α},(16)NEGB(α,β)(D)δ=⋃i=1m{x|x∈Di,0<p(Di|nBδ(x))≤β},wherep(Di|nBδ(x))=|nBδ(x)∩Di||nBδ(x)|. The union ofPOSB(α,β)(D)δandBNDB(α,β)(D)δis called the non-negative region and denoted by⌝NEGB(α,β)(D)δ.In this example, we adopt the same neighborhood system and neighborhood parameter used in Example 1. Suppose TS=(U, C∪D, δ, α, β) be a three-way decision neighborhood system, where δ=0.3, α=0.8 and β=0.6. For condition attribute set C={a, b, c}, we have the following results by Definition 11:p(D1|nCδ(x1))=|nCδ(x1)∩D1||nCδ(x1)|=|{x1}∩{x1,x3,x6}||{x1}|=1,p(D1|nCδ(x3))=|nCδ(x3)∩D1||nCδ(x3)|=|{x3}∩{x1,x3,x6}||{x3}|=1,p(D1|nCδ(x6))=|nCδ(x6)∩D1||nCδ(x6)|=|{x5,x6}∩{x1,x3,x6}||{x5,x6}|=0.5,p(D2|nCδ(x2))=|nCδ(x2)∩D2||nCδ(x2)|=|{x2}∩{x2,x4,x5}||{x2}|=1,p(D2|nCδ(x4))=|nCδ(x4)∩D2||nCδ(x4)|=|{x4,x5}∩{x2,x4,x5}||{x4,x5}|=1,p(D2|nCδ(x5))=|nCδ(x5)∩D2||nCδ(x5)|=|{x4,x5,x6}∩{x2,x4,x5}||{x4,x5,x6}|=0.67.Then, we obtain the positive, boundary and negative regions of D with respect to C as follows:POSC(α,β)(D)δ=⋃i=1m{x|x∈Di,0.8<p(Di|nCδ(x))≤1}={x1,x2,x3,x4},BNDC(α,β)(D)δ=⋃i=1m{x|x∈Di,0.6<p(Di|nCδ(x))≤0.8}={x5},NEGC(α,β)(D)δ=⋃i=1m{x|x∈Di,0<p(Di|nCδ(x))≤0.6}={x6}.Given a three-way decision neighborhood system TS=(U, C∪D, δ, α, β) and the condition attribute subset B⊆C, then:(1)POSB(α,β)(D)δ∪BNDB(α,β)(D)δ=POSB(β,0)(D)δ;BNDB(α,β)(D)δ∪NEGB(α,β)(D)δ=NEGB(1,α)(D)δ;POSB(α,β)(D)δ∪BNDB(α,β)(D)δ∪NEGB(α,β)(D)δ=U;POSB(α,β)(D)δ∪BNDB(α,β)(D)δ=⌝NEGB(α,β)(D)δ;BNDB(α,β)(D)δ∪NEGB(α,β)(D)δ=⌝POSB(α,β)(D)δ;POSB(α,β)(D)δ∪NEGB(α,β)(D)δ=⌝BNDB(α,β)(D)δ;POSB(α,β)(D)δ∩BNDB(α,β)(D)δ=∅;BNDB(α,β)(D)δ∩NEGB(α,β)(D)δ=∅;POSB(α,β)(D)δ∩NEGB(α,β)(D)δ=∅.(1) According to Definition 11, we havePOSB(α,β)(D)δ∪BNDB(α,β)(D)δ=(⋃i=1m{x|x∈Di,α<p(Di|nBδ(x))≤1})∪(⋃i=1m{x|x∈Di,β<p(Di|nBδ(x))≤α})=⋃i=1m{x|x∈Di,(α<p(Di|nBδ(x))≤1)∪(β<p(Di|nBδ(x))≤α)}=⋃i=1m{x|x∈Di,β<p(Di|nBδ(x))≤1}=POSB(β,0)(D)δ.The proofs of (2), (3), (4), (5), (6), (7), (8) and (9) are similar to that of (1).□The reducts of decision regions in the three-way decision model are those decision regions with respect to the decision partition U/D remained unchanged. The region preservation reducts of three-way decision neighborhood systems are defined in the following.Definition 12Given a three-way decision neighborhood system TS=(U, C∪D, δ, α, β), R⊆C,(1)R is a positive region preservation reduct of TS if it satisfies:(I)|POSR(α,β)(D)δ|=|POSC(α,β)(D)δ|;∀B⊂R,|POSB(α,β)(D)δ|≠|POSR(α,β)(D)δ|,R is a boundary region preservation reduct of TS if it satisfies:(I)|BNDR(α,β)(D)δ|=|BNDC(α,β)(D)δ|;∀B⊂R,|BNDB(α,β)(D)δ|≠|BNDR(α,β)(D)δ|,R is a negative region preservation reduct of TS if it satisfies:(I)|NEGR(α,β)(D)δ|=|NEGC(α,β)(D)δ|;∀B⊂R,|NEGB(α,β)(D)δ|≠|NEGR(α,β)(D)δ|.In this definition, conditions (I) of the three types of reduct definitions are called the region preservation conditions, and conditions (II) of the three types of reduct definitions are called the independency conditions.In the same neighborhood decision system described in Example 1, suppose δ=0.3, α=0.8 and β=0.6. For condition attribute set P={a, b}, we have the following results by Definition 11:p(D1|nPδ(x1))=|nPδ(x1)∩D1||nPδ(x1)|=|{x1,x3}∩{x1,x3,x6}||{x1,x3}|=1,p(D1|nPδ(x3))=|nPδ(x3)∩D1||nPδ(x3)|=|{x1,x3}∩{x1,x3,x6}||{x1,x3}|=1,p(D1|nPδ(x6))=|nPδ(x6)∩D1||nPδ(x6)|=|{x5,x6}∩{x1,x3,x6}||{x5,x6}|=0.5,p(D2|nPδ(x2))=|nPδ(x2)∩D2||nPδ(x2)|=|{x2}∩{x2,x4,x5}||{x2}|=1,p(D2|nPδ(x4))=|nPδ(x4)∩D2||nPδ(x4)|=|{x4,x5}∩{x2,x4,x5}||{x4,x5}|=1,p(D2|nPδ(x5))=|nPδ(x5)∩D2||nPδ(x5)|=|{x4,x5,x6}∩{x2,x4,x5}||{x4,x5,x6}|=0.67.Then, we obtain the positive, boundary and negative regions of D with respect to P as follows:POSP(α,β)(D)δ=⋃i=1m{x|x∈Di,0.8<p(Di|nPδ(x))≤1}={x1,x2,x3,x4},BNDP(α,β)(D)δ=⋃i=1m{x|x∈Di,0.6<p(Di|nPδ(x))≤0.8}={x5},NEGP(α,β)(D)δ=⋃i=1m{x|x∈Di,0<p(Di|nPδ(x))≤0.6}={x6}.For condition attribute set A={a}, we have following results by Definition 11:p(D1|nAδ(x1))=|nAδ(x1)∩D1||nAδ(x1)|=|{x1,x2,x3}∩{x1,x3,x6}||{x1,x2,x3}|=0.67,p(D1|nAδ(x3))=|nAδ(x3)∩D1||nAδ(x3)|=|{x1,x2,x3}∩{x1,x3,x6}||{x1,x2,x3}|=0.67,p(D1|nAδ(x6))=|nAδ(x6)∩D1||nAδ(x6)|=|{x4,x5,x6}∩{x1,x3,x6}||{x4,x5,x6}|=0.33,p(D2|nAδ(x2))=|nAδ(x2)∩D2||nAδ(x2)|=|{x1,x2,x3}∩{x2,x4,x5}||{x1,x2,x3}|=0.33,p(D2|nAδ(x4))=|nAδ(x4)∩D2||nAδ(x4)|=|{x4,x5,x6}∩{x2,x4,x5}||{x4,x5,x6}|=0.67,p(D2|nAδ(x5))=|nAδ(x5)∩D2||nAδ(x5)|=|{x4,x5,x6}∩{x2,x4,x5}||{x4,x5,x6}|=0.67.Then, we obtain the positive, boundary and negative regions of D with respect to A as follows:POSA(α,β)(D)δ=⋃i=1m{x|x∈Di,0.8<p(Di|nAδ(x))≤1}=∅,BNDA(α,β)(D)δ=⋃i=1m{x|x∈Di,0.6<p(Di|nAδ(x))≤0.8}={x1,x3,x4,x5},NEGA(α,β)(D)δ=⋃i=1m{x|x∈Di,0<p(Di|nAδ(x))≤0.6}={x2,x6}.For condition attribute set B={b}, we have following results by Definition 11:p(D1|nBδ(x1))=|nBδ(x1)∩D1||nBδ(x1)|=|{x1,x3,x4,x5}∩{x1,x3,x6}||{x1,x3,x4,x5}|=0.5,p(D1|nBδ(x3))=|nBδ(x3)∩D1||nBδ(x3)|=|{x1,x3,x4}∩{x1,x3,x6}||{x1,x3,x4}|=0.5,p(D1|nBδ(x6))=|nBδ(x6)∩D1||nBδ(x6)|=|{x5,x6}∩{x1,x3,x6}||{x5,x6}|=0.5,p(D2|nBδ(x2))=|nBδ(x2)∩D2||nBδ(x2)|=|{x2}∩{x2,x4,x5}||{x2}|=1,p(D2|nBδ(x4))=|nBδ(x4)∩D2||nBδ(x4)|=|{x1,x3,x4,x5}∩{x2,x4,x5}||{x1,x3,x4,x5}|=0.5,p(D2|nBδ(x5))=|nBδ(x5)∩D2||nBδ(x5)|=|{x1,x4,x5,x6}∩{x2,x4,x5}||{x1,x4,x5,x6}|=0.5.Then, we obtain the positive, boundary and negative regions of D with respect to B as follows:POSB(α,β)(D)δ=⋃i=1m{x|x∈Di,0.8<p(Di|nBδ(x))≤1}={x2},BNDB(α,β)(D)δ=⋃i=1m{x|x∈Di,0.6<p(Di|nBδ(x))≤0.8}=∅,NEGB(α,β)(D)δ=⋃i=1m{x|x∈Di,0<p(Di|nBδ(x))≤0.6}={x1,x3,x4,x5,x6}.According to the above calculations, the equationPOSP(α,β)(D)δ=POSC(α,β)(D)δis satisfied, butPOSP(α,β)(D)δ≠POSA(α,β)(D)δandPOSP(α,β)(D)δ≠POSB(α,β)(D)δare obtained. So, P={a, b} is a positive region preservation reduct of TS. Similarly, P={a, b} is also a boundary region preservation reduct and a negative region preservation reduct.Definition 13Given a three-way decision neighborhood system TS=(U, C∪D, δ, α, β), where U/D={D1, D2, …, Dm} is an equivalence class set. For a subset B⊆C, the positive dependency, boundary dependency and negative dependency of D with respect to B in the three-way decision system are defined by:(17)μPOSB(α,β)(D)δ=|POSB(α,β)(D)δ||U|,(18)μBNDB(α,β)(D)δ=|BNDB(α,β)(D)δ||U|,(19)μNEGB(α,β)(D)δ=|NEGB(α,β)(D)δ||U|.Proposition 3Let TS=(U, C∪D, δ, α, β) be a three-way decision neighborhood system. For a subset B⊆C, we haveμPOSB(α,β)(D)δ+μBNDB(α,β)(D)δ+μNEGB(α,β)(D)δ=1.ProofIt is straightforward from Proposition 2 and Definition 13.□Example 4In Example 3, for a subset P={a, b}⊆C, the positive dependency, boundary dependency and negative dependency of D with respect to P are calculated by:μPOSP(α,β)(D)δ=|POSP(α,β)(D)δ||U|=|{x1,x2,x3,x4}||U|=2/3,μBNDP(α,β)(D)δ=|BNDP(α,β)(D)δ||U|=|{x5}||U|=1/6,μNEGP(α,β)(D)δ=|NEGP(α,β)(D)δ||U|=|{x6}||U|=1/6.Thus,μPOSP(α,β)(D)δ+μBNDP(α,β)(D)δ+μNEGP(α,β)(D)δ=2/3+1/6+1/6=1.In Pawlak rough set model, the positive preservation reduct is monotonic, while the condition attribute set increases. However, the monotonicity of each region preservation reduct does not hold in three-way decision neighborhood systems. To develop heuristic algorithms to obtain region preservation reducts for the three-way decision neighborhood systems, three types of monotonic measures are constructed using variants of the condition entropy, and their monotonicity are proven in this subsection.Definition 14Given a three-way decision neighborhood system TS=(U, C∪D, δ, α, β), where U/D={D1, D2, …, Dm} is an equivalence class set. For a subset B⊆C, we can define three coverings of U:(20)U/RPOSB(α,β)={POSB(α,β)(D1)δ,POSB(α,β)(D2)δ,…,POSB(α,β)(Dm)δ,U}(21)U/RBNDB(α,β)={BNDB(α,β)(D1)δ,BNDB(α,β)(D2)δ,…,BNDB(α,β)(Dm)δ,U}(22)U/RNEGB(α,β)={NEGB(α,β)(D1)δ,NEGB(α,β)(D2)δ,…,NEGB(α,β)(Dm)δ,U}Given a three-way decision neighborhood system TS=(U, C∪D, δ, α, β), where U/D={D1, …, Dj, …, Dm} and B⊆C. For ∀xi∈U, we can define three neighborhood classes generated by the three coveringsU/RPOSB(α,β)(D)δ,U/RBNDB(α,β)(D)δandU/RNEGB(α,β)(D)δ, respectively, as follows:(23)nRPOSB(α,β)δ(xi)={POSB(α,β)(Dj)δ|xi∈Djandxi∈POSB(α,β)(Dj)δ}{U|xi∈Djandxi∉POSB(α,β)(Dj)δ};(24)nRBNDB(α,β)δ(xi)={BNDB(α,β)(Dj)δ|xi∈Djandxi∈BNDB(α,β)(Dj)δ}{U|xi∈Djandxi∉BNDB(α,β)(Dj)δ};(25)nRNEGB(α,β)δ(xi)={NEGB(α,β)(Dj)δ|xi∈Djandxi∈NEGB(α,β)(Dj)δ}{U|xi∈Djandxi∉NEGB(α,β)(Dj)δ}.In the same neighborhood decision system described in Example 1, suppose δ=0.3, α=0.8 and β=0.6. For the condition attribute set P={a, b}, according to the calculations of Example 3, we have the following results:POSP(α,β)(D1)δ={x|x∈D1,0.8<p(D1|nPδ(x))≤1}={x1,x3},POSP(α,β)(D2)δ={x|x∈D2,0.8<p(D1|nPδ(x))≤1}={x2,x4}.We have D1={x1, x3, x6} and D2={x2, x4, x5}. According to the Definition 15, we can obtain the results as follows:nRPOSP(α,β)δ(x1)={x1,x3},nRPOSP(α,β)δ(x2)={x2,x4},nRPOSP(α,β)δ(x3)={x1,x3},nRPOSP(α,β)δ(x4)={x2,x4},nRPOSP(α,β)δ(x5)={x1,x2,x3,x4,x5,x6},nRPOSP(α,β)δ(x6)={x1,x2,x3,x4,x5,x6}.For the condition attribute set B={b}, from the results of Example 3 and Definition 15, we have the following results:POSB(α,β)(D1)δ={x|x∈D1,0.8<p(D1|nBδ(x))≤1}=∅,POSB(α,β)(D2)δ={x|x∈D2,0.8<p(D1|nBδ(x))≤1}={x2},nRPOSB(α,β)δ(x1)={x1,x2,x3,x4,x5,x6},nRPOSB(α,β)δ(x2)={x2},nRPOSB(α,β)δ(x3)={x1,x2,x3,x4,x5,x6},nRPOSB(α,β)δ(x4)={x1,x2,x3,x4,x5,x6},nRPOSB(α,β)δ(x5)={x1,x2,x3,x4,x5,x6},nRPOSB(α,β)δ(x6)={x1,x2,x3,x4,x5,x6}.Given a three-way decision neighborhood system TS=(U, C∪D, δ, α, β) and condition attribute subset B⊆C. For xi∈U, thenRPOSB(α,β)δ(xi),nRBNDB(α,β)δ(xi)andnRNEGB(α,β)δ(xi)are neighborhood classes generated byU/RPOSB(α,β)(D)δ,U/RBNDB(α,β)(D)δandU/RNEGB(α,β)(D)δ, respectively. The information entropies ofRPOSB(α,β)(D)δ,RBNDB(α,β)(D)δ,RNEGB(α,β)(D)δand B are defined by the follows:(26)(1)H(RPOSB(α,β)(D)δ)=−1n∑i=1nlog|nRPOSB(α,β)δ(xi)|n.(27)(2)H(RBNDB(α,β)(D)δ)=−1n∑i=1nlog|nRBNDB(α,β)δ(xi)|n.(28)(3)H(RNEGB(α,β)(D)δ)=−1n∑i=1nlog|nRNEGB(α,β)δ(xi)|n.(29)(4)H(B)=−1n∑i=1nlog|nBδ(xi)|n.According to the results of Example 5 and Definition 16, we can calculate the information entropies as follows:H(RPOSP(α,β)(D)δ)=−1n∑i=1nlog|nRPOSP(α,β)δ(xi)|n=−164*log26+2*log66=0.3181,H(RPOSB(α,β)(D)δ)=−1n∑i=1nlog|nRPOSB(α,β)δ(xi)|n=−161*log16+5*log66=0.1297.According to the results of Example 1 and Definition 16, we can obtain the results:H(P)=−1n∑i=1nlog|nPδ(xi)|n=−164*log26+log16+log36=0.4979,H(B)=−1n∑i=1nlog|nBδ(xi)|n=−163*log46+log16+log26+log36=0.3474.Given a three-way decision neighborhood system TS=(U, C∪D, δ, α, β), P, Q⊆C.(1)ThenRPOSP(α,β)δ(xi)andnRPOSQ(α,β)δ(xi)are neighborhood classes generated byU/RPOSP(α,β)(D)δandU/RPOSQ(α,β)(D)δ, respectively. The joint entropy ofRPOSP(α,β)(D)δandRPOSQ(α,β)(D)δis defined by(30)H(RPOSP(α,β)(D)δRPOSQ(α,β)(D)δ)=−1n∑i=1nlog|nRPOSP(α,β)δ(xi)∩nRPOSQ(α,β)δ(xi)|n.ThenRBNDP(α,β)δ(xi)andnRBNDQ(α,β)δ(xi)are neighborhood classes generated byU/RBNDP(α,β)(D)δandU/RBNDQ(α,β)(D)δ, respectively. The joint entropy ofRBNDP(α,β)(D)δandRBNDQ(α,β)(D)δis defined by(31)H(RBNDP(α,β)(D)δRBNDQ(α,β)(D)δ)=−1n∑i=1nlog|nRBNDP(α,β)δ(xi)∩nRBNDQ(α,β)δ(xi)|n.ThenRNEGP(α,β)δ(xi)andnRNEGQ(α,β)δ(xi)are neighborhood classes generated byU/RNEGP(α,β)(D)δandU/RNEGQ(α,β)(D)δ, respectively. The joint entropy ofRNEGP(α,β)(D)δandRNEGQ(α,β)(D)δis defined by(32)H(RNEGP(α,β)(D)δRNEGQ(α,β)(D)δ)=−1n∑i=1nlog|nRNEGP(α,β)δ(xi)∩nRNEGQ(α,β)δ(xi)|n.According to the results of Example 1, Example 5 and Definition 17, we can calculate the joint entropy as follows:H(BRPOSP(α,β)(D)δ)=−1n∑i=1nlog|nBδ(xi)∩nRPOSP(α,β)δ(xi)|n=−16(log|{x1,x3,x4,x5}∩{x1,x3}|6+log|{x2}∩{x2,x4}|6+log|{x1,x3,x4}∩{x1,x3}|6+log|{x1,x3,x4,x5}∩{x2,x4}|6+log|{x1,x4,x5,x6}∩{x1,x2,x3,x4,x5,x6}|6+log|{x5,x6}∩{x1,x2,x3,x4,x5,x6}|6)=0.5273.Given a three-way decision neighborhood system TS=(U, C∪D, δ, α, β), B⊆C.(1)ThenBδ(xi)is a neighborhood class generated by U/B and thenRPOSC(α,β)δ(xi)is a neighborhood class generated byU/RPOSC(α,β)(D)δ. The condition entropy ofRPOSC(α,β)(D)δconditioned to B is defined by(33)H(RPOSC(α,β)(D)δ|B)=−1n∑i=1nlog|nBδ(xi)∩nRPOSC(α,β)δ(xi)||nBδ(xi)|.ThenBδ(xi)is a neighborhood class generated by U/B and thenRBNDC(α,β)δ(xi)is a neighborhood class generated byU/RBNDC(α,β)(D)δ. The condition entropy ofRBNDC(α,β)(D)δconditioned to B is defined by(34)H(RBNDC(α,β)(D)δ|B)=−1n∑i=1nlog|nBδ(xi)∩nRBNDC(α,β)δ(xi)||nBδ(xi)|.ThenBδ(xi)is a neighborhood class generated by U/B and thenRNEGC(α,β)δ(xi)is a neighborhood class generated byU/RNEGC(α,β)(D)δ. The condition entropy ofRNEGC(α,β)(D)δconditioned to B is defined by(35)H(RNEGC(α,β)(D)δ|B)=−1n∑i=1nlog|nBδ(xi)∩nRNEGC(α,β)δ(xi)||nBδ(xi)|.According to the results of Example 1, Example 5 and Definition 18, we can calculate the condition entropy as follows:H(RPOSP(α,β)(D)δ|B)=−1n∑i=1nlog|nBδ(xi)∩nRPOSP(α,β)δ(xi)||nBδ(xi)|=−16(log|{x1,x3,x4,x5}∩{x1,x3}||{x1,x3,x4,x5}|+log|{x2}∩{x2,x4}||{x2}|+log|{x1,x3,x4}∩{x1,x3}||{x1,x3,x4}|+log|{x1,x3,x4,x5}∩{x2,x4}||{x1,x3,x4,x5}|+log|{x1,x4,x5,x6}∩{x1,x2,x3,x4,x5,x6}||{x1,x4,x5,x6}|+log|{x5,x6}∩{x1,x2,x3,x4,x5,x6}||{x5,x6}|)=0.1779.Given a three-way decision neighborhood system TS=(U, C∪D, δ, α, β), B⊆C, then:(1)H(RPOSC(α,β)(D)δ|B)=H(BRPOSC(α,β)(D)δ)−H(B).H(RBNDC(α,β)(D)δ|B)=H(BRBNDC(α,β)(D)δ)−H(B).H(RNEGC(α,β)(D)δ|B)=H(BRNEGC(α,β)(D)δ)−H(B).(1) According to Definitions 16 and 17, we haveH(BRPOSC(α,β)(D)δ)−H(B)=−1n∑i=1nlog|nBδ(xi)∩nRPOSC(α,β)δ(xi)|n−(−1n∑i=1nlog|nBδ(xi)|n)=−1n∑i=1nlog|nBδ(xi)∩nRPOSC(α,β)δ(xi))||nBδ(xi)|. From the Definition 18, we haveH(RPOSC(α,β)(D)δ|B)=−1n∑i=1nlog|nBδ(xi)∩nRPOSC(α,β)δ(xi)||nBδ(xi)|. Therefore,H(RPOSC(α,β)(D)δ|B)=H(BRPOSC(α,β)(D)δ)−H(B).The proofs of (2) and (3) are similar to that of (1).□From the results of Examples 6, 7 and 8, we can obtain that H(B)=0.3474,H(BRPOSP(α,β)(D)δ)=0.5273,H(RPOSP(α,β)(D)δ|B)=0.1779. Thus,H(RPOSP(α,β)(D)δ|B)=H(BRPOSP(α,β)(D)δ)−H(B)is found.Given a three-way decision neighborhood system TS=(U, C∪D, δ, α, β). If Q⊆P⊆C, then:(1)H(RPOSC(α,β)(D)δ|Q)≥H(RPOSC(α,β)(D)δ|P).H(RBNDC(α,β)(D)δ|Q)≥H(RBNDC(α,β)(D)δ|P).H(RNEGC(α,β)(D)δ|Q)≥H(RNEGC(α,β)(D)δ|P).According to Definition 15, ifxi∈POSB(α,β)(Dj)δ, thennBδ(xi)∩nRNEGC(α,β)δ(xi)=nRNEGC(α,β)δ(xi); ifxi∉POSB(α,β)(Dj)δ, thennBδ(xi)∩nRNEGC(α,β)δ(xi)=nBδ(xi)∩U=nBδ(xi). Hence,|nBδ(xi)∩nRNEGC(α,β)δ(xi)||nBδ(xi)|=|nRNEGC(α,β)δ(xi)||nBδ(xi)|or 1. According to Proposition 1, for Q⊆P⊆C, we havenQδ(xi)≥nPδ(xi). Hence,|nQδ(xi)∩nRNEGC(α,β)δ(xi)||nQδ(xi)|≤|nPδ(xi)∩nRNEGC(α,β)δ(xi)||nPδ(xi)|. Then, we have−1n∑i=1nlog|nQδ(xi)∩nRNEGC(α,β)δ(xi)||nQδ(xi)|≥−1n∑i=1nlog|nPδ(xi)∩nRNEGC(α,β)δ(xi)||nPδ(xi)|. Therefore,H(RPOSC(α,β)(D)δ|Q)≥H(RPOSC(α,β)(D)δ|P).The proofs of (2) and (3) are similar to that of (1).□In this example, we adopt the same neighborhood system and neighborhood parameter used in Example 1. Suppose B={b}, P={a, b} and C={a, b, c}, it is obvious that B⊆P⊆C. By computing the condition entropies among them, one can obtain thatH(RPOSC(α,β)(D)δ|B)=0.1799,H(RPOSC(α,β)(D)δ|P)=0.0502.According to the monotonicity of the proposed measures, we define three attribute significances for the region preservation reduction process in three-way decision neighborhood systems. Heuristic computing methods of three types of region preservation reducts are also given in this subsection.Definition 19Given a three-way decision neighborhood system TS=(U, C∪D, δ, α, β), where C is the condition attribute set and D is the decision attribute. For B⊂C, ∀a∈C−B, the three attribute significances of a in B in three-way decision region reducts can be defined by the follows:(1)SIG(a,B,RPOSC(α,β)(D)δ)=H(RPOSC(α,β)(D)δ|B)−H(RPOSC(α,β)(D)δ|B∪{a})SIG(a,B,RBNDC(α,β)(D)δ)=H(RBNDC(α,β)(D)δ|B)−H(RBNDC(α,β)(D)δ|B∪{a})SIG(a,B,RNEGC(α,β)(D)δ)=H(RNEGC(α,β)(D)δ|B)−H(RNEGC(α,β)(D)δ|B∪{a})Similarly, the three attribute significances can be used to evaluate the significance of attribute a in the three region preservation reducts of a three-way decision neighborhood system. Consequently, the three algorithms for attribute reduction can be described as follows.Input: a three-way decision neighborhood system TS=(U, C∪D, δ, α, β)Output: a positive region preservation reduct BStep 1:Let B=∅, initialize the parameters α, β, and δ;For every attribute b∈C, compute the condition entropy of attribute b,H(RPOSC(α,β)(D)δ|{b});Select the attribute which maximize theH(RPOSC(α,β)(D)δ|{b}), record it as bmax;B⟵B∪{bmax};For every attribute a∈C−B, compute the significance of condition attribute a,SIG(a,B,RPOSC(α,β)(D)δ);Select the attribute which maximize theSIG(a,B,RPOSC(α,β)(D)δ), record it as amax;B⟵B∪{amax};If|POSB(α,β)(D)δ|≠|POSC(α,β)(D)δ|, then goto Step 5, else goto Step 9;The set B is the selected positive region preservation reduct.Input: a three-way decision neighborhood system TS=(U, C∪D, δ, α, β)Output: a boundary region preservation reduct BStep 1:Let B=∅, initialize the parameters α, β, and δ;For every attribute b∈C, compute the condition entropy of attribute b,H(RBNDC(α,β)(D)δ|{b});Select the attribute which maximize theH(RBNDC(α,β)(D)δ|{b}), record it as bmax;B⟵B∪{bmax};For every attribute a∈C−B, compute the significance of condition attribute a,SIG(a,B,RBNDC(α,β)(D)δ);Select the attribute which maximize theSIG(a,B,RBNDC(α,β)(D)δ), record it as amax;B⟵B∪{amax};If|BNDB(α,β)(D)δ|≠|BNDC(α,β)(D)δ|, then goto Step 5, else goto Step 9;The set B is the selected boundary region preservation reduct.Input: a three-way decision neighborhood system TS=(U, C∪D, δ, α, β)Output: a negative region preservation reduct BStep 1:Let B=∅, initialize the parameters α, β, and δ;For every attribute b∈C, compute the condition entropy of attribute b,H(RNEGC(α,β)(D)δ|{b});Select the attribute which maximize theH(RNEGC(α,β)(D)δ|{b}), record it as bmax;B⟵B∪{bmax};For every attribute a∈C−B, compute the significance of condition attribute a,SIG(a,B,RNEGC(α,β)(D)δ);Select the attribute which maximize theSIG(a,B,RNEGC(α,β)(D)δ), record it as amax;B⟵B∪{amax};If|NEGB(α,β)(D)δ|≠|NEGC(α,β)(D)δ|, then goto Step 5, else goto Step 9;The set B is the selected negative region preservation reduct.The running time of algorithm POSA is mainly consumed in the calculation of the attribute significance. The attribute significance is determined by the condition entropy. The computation of condition entropy involves the construction of the neighborhood class, while the time complexity of computing neighborhood class is 0(|C||U|2). In the Step 2 of algorithm POSA, each attribute loops exhaustively, so the time complexity of the algorithm POSA is 0(|C|2|U|2). In the other algorithms BNDA and NEGA, the time complexities are the same as the algorithm POSA.

@&#CONCLUSIONS@&#
