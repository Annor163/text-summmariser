@&#MAIN-TITLE@&#
Measuring inconsistency in probabilistic logic: rationality postulates and Dutch book interpretation

@&#HIGHLIGHTS@&#
Consistency, independence and continuity are incompatible postulates.Minimal inconsistent sets are not suitable to analyze probabilistic incon-sistencies.Independence can be weakened considering the underlying consolidation process.Inconsistency and incoherence measures based on distances and Dutch books coincide.

@&#KEYPHRASES@&#
Probabilistic reasoning,Probabilistic logic,Inconsistency measures,

@&#ABSTRACT@&#
Inconsistency measures have been proposed as a way to manage inconsistent knowledge bases in the AI community. To deal with inconsistencies in the context of conditional probabilistic logics, rationality postulates and computational efficiency have driven the formulation of inconsistency measures. Independently, investigations in formal epistemol-ogy have used the betting concept of Dutch book to measure an agent's degree of incoherence. In this paper, we show the impossibility of joint satisfiability of the proposed postulates, proposing to replace them by more suitable ones. Thus we reconcile the rationality postulates for inconsistency measures in probabilistic bases and show that several inconsistency measures suggested in the literature and computable with linear programs satisfy the reconciled postulates. Additionally, we give an interpretation for these feasible measures based on the formal epistemology concept of Dutch book, bridging the views of two so far separate communities in AI and Philosophy. In particular, we show that incoherence degrees in formal epistemology may lead to novel approaches to inconsistency measures in the AI view.

@&#INTRODUCTION@&#
“when you can measure what you are speaking about, you know something about it; but when you cannot […] your knowledge is of a meagre and unsatisfactory kind;”— Lord Kelvin [45]Measuring has been a prominent activity in advancing scientific and technological development. Not all measures are alike and good measures express intuitive notions in a useful way. In the field of deductive logical reasoning, one usually has an intuition expressing that one theory is more inconsistent than other, capturing the idea that the “effort” to restore consistency is greater in one case than the other. Also, no effort is required to restore the consistency of a consistent theory.Based on those intuitions, there are several proposals for measuring inconsistency in knowledge bases over purely logical languages [19]. Some of these proposals involved attaching probabilities to formulas [29], or the combination of inconsistency factors [20]. Some of these measures are discrete or even qualitative, while others are more like distances, but all these measures have to behave like an information measure[6]. And to adhere to certain intuitions, a series of postulates for inconsistency measures for purely logical knowledge bases were proposed [21,22]; for example, the consistency postulate states that the inconsistency measure of a consistent base is 0.Purely logical bases are known to be expressively limited in representing uncertainty required for real-world applications. In this work, we are interested in measuring the inconsistency of knowledge bases over logical probabilistic languages, which combine the deductive power of logical systems with the well-founded theory of probability. This kind of extension of purely logical systems can be traced back to the work of Boole [2], but has gained attention of AI researchers since the work of Nilsson [33], and has been extended to conditional probabilistic logic [37].In AI, one of the main uses of measuring inconsistency in a knowledge base is to guide the consolidation of inconsistent pieces of information. Within propositional logic, Grant and Hunter [13] showed how inconsistency measures can be used to direct the stepwise resolution of conflicts via the weakening or the discarding of formulas.In probabilistic bases, inconsistencies are rather common, specially when knowledge is gathered from different sources. To fix these probabilistic knowledge bases, one can, for instance, delete pieces of information, or change the probabilities' numeric values (or intervals). In this case, an inconsistency measure helps one to detect if a change approximates consistency or not. In other areas, inconsistency measures for probabilistic logic have found applications in merging conflicting opinions, leading to an increased predictive power [47,25], and in quantifying the incoherence of procedures from classical statistical hypothesis testing [41].Example 1.1Consider we are devising an expert system to assist medical diagnosis. Suppose a group of experts on a disease D is required to quantify the relationship between D and its symptoms. Suppose three conditional probabilities are presented:•the probability of a patient exhibiting symptomS1given he/she has disease D is50%;the probability of a patient exhibiting symptomS2given he/she exhibits symptomS1and has disease D is80%;the probability of a patient exhibiting symptomS2given he/she has disease D is30%.The issue of measuring inconsistency in probabilistic bases has more recently been tackled by Thimm [44], Muiño [31] and Potyka [34], who developed measures based on distance minimization, tailored to the probabilistic case. Potyka focused on computational aspects, looking for efficiently computable measures [34]. Muiño was driven by the CADIAG-2 knowledge base, presenting its infinitesimal inconsistency degree, however based on a different semantics [31]. Thimm [44] adapted Hunter and Konieczny's [22] desirable properties for inconsistency measures to the probabilistic setting, developing measures that satisfy a set of rationality postulates.It was Thimm [44] who realized the importance of continuity as a Postulate for the probabilistic case, namely the property that a small change in the probability associated to formula (absent in the purely logical case) should lead only to small changes in the inconsistency measure. It was just natural that, (conditional) probabilistic logic being an extension of the classical cases, the continuity postulate was simply added to the postulates defining classical inconsistency measures.In this work, we argue that continuity cannot hold together with classical postulates such as consistency and independence, and some of these postulates must be abandoned or exchanged for other ones that restore joint satisfiability. So the first contribution of this work is that we identify and fix the possible problem with the postulates proposed by Thimm [44].Another contribution lies in showing that these measures of inconsistency have a direct counterpart in formal epistemology research over the coherence of an agent's degrees of belief. It is known that inconsistent probabilistic beliefs correspond to a set of bets with guaranteed loss to the agent, which is called a “Dutch Book” [8,27]. This agent's incoherence has been measured by formalizing the intuition that the greater the inconsistency the greater the corresponding sure loss, and vice versa [40,43]. Thus we interpret these incoherence measures via guaranteed losses as inconsistency measures, showing that existing measures based on distance minimization correspond to guaranteed losses that quantify an agent's incoherence. To the best of our knowledge, no clear link has been shown between these two areas.Here is a bird's-eye view of how we achieve these goals.After introducing probabilistic knowledge bases in Section 2, this paper develops three main contributions, in three different sections, dealing closely with three other works. In the following, we overview such contributions, together with the organization of the paper and their relation to the existing literature.Inconsistency measures for probabilistic knowledge bases were analyzed via rationality postulates by Thimm [44]. In Section 3, we argue for the incompatibility of such desirable properties. Firstly, we introduce the problematic postulates: consistency, independence and continuity. The independence postulate claims that a free conditional — a (conditional) probability assignment that does not belong to any minimal inconsistent set — can be rule out without changing the degree of inconsistency. We also present the MIS-separability property, which deals with decomposability (through Minimal Inconsistent Sets) and implies independence. As Thimm's work regards precise probabilities, these four concepts are then introduced in this way. In a second step, Section 3 brings the first contribution of this work: the presented postulates are shown to be incompatible.In Section 4, we search for a reasonable way to reconcile the incompatible postulates. First of all, we argue that independence is the requirement to be weakened, together with the stronger property of MIS-separability. Afterwards, the concept of free conditional is analyzed, for independence is based on it. We find that free conditional is a notion linked to classical consolidation and contraction (i.e., discarding formulas to reach consistency), and it is not suitable for probabilistic bases. The innocuous conditional concept is introduced, by investigating a natural consolidation procedure for probabilities: through widening their intervals, instead of ruling them out. The i-independence postulate is put forward based on innocuous conditionals. In a similar way, we observe that MIS (minimal inconsistent set) is a notion that fails to capture all causes of inconsistency in the probabilistic knowledge bases. We define the alternative concept of inescapable conflict, which yields the IC-separability property. We show that innocuous conditionals are to inescapable conflicts as free conditionals are to minimal inconsistent sets. At the end of Section 4, the second main contribution of this paper emerges when i-independence and IC-separability are shown to be compatible with consistency and continuity, besides other desirable properties from the literature that are then presented.Once a consistent package of postulates is laid out, a myriad of inconsistency measures can still be considered rational. Hence, in Section 5, further criteria to evaluate inconsistency measures are discussed: computational efficiency and meaningful interpretation. On the one hand, two inconsistency measures computable through linear programs are adapted to imprecise probabilities from the work of Potyka [34]. It is shown that both measures satisfy the core postulates, and we present the additional desirable properties each one satisfies. On the other hand, we review two measures from Schervish, Kadane and Seidenfeld that quantify the incoherence of an agent using the betting concept of Dutch book [40], under the operational interpretation that her degrees of belief (probabilities) determine her gambling behavior. The third main contribution of this paper lies in showing the connection between inconsistency measures for probabilistic knowledge bases and incoherence measures for agents from formal epistemology. We prove that the two measures adapted from Potyka that are computable through linear programs are equivalent to two measures from Schervish et al. — that is, these two measures are rather efficient and have a meaningful interpretation. The final part of the section reviews other measures from the work of Schervish et al., showing that they also satisfy the postulates and are computationally feasible.As several families of inconsistency measures are discussed throughout the paper, Table 1compiles their notation, the section in which they are defined and a short explanation of each.A propositional logical language is a set of formulas formed by atomic propositions combined with logical connectives, possibly with punctuation elements (parentheses). We assume a finite set of symbolsXn={x1,x2,x3,…,xn}corresponding to atomic propositions (atoms). Formulas are constructed inductively with connectives (¬,∧,∨,→) and atomic propositions as usual. The set of all these well-formed formulas is the propositional language overXn, denoted byLXn. Additionally, ⊤ denotesxi∨¬xifor somexi∈Xn, and ⊥ denotes ¬⊤.Given a signatureXn, a possible world w is a conjunction of|Xn|=natoms containing eitherxior¬xifor eachxi∈Xn. We denote byWXn={w1,…,w2n}the set of all possible worlds overXnand say aw∈WXnentails axi∈Xn(w⊨xi) iffxiis not negated in w. This entailment relation can be extended to allφ∈LXnas usual.A probabilistic conditional (or simply conditional) is a statement of the form(φ|ψ)[q_,q¯], with the underlying meaning “the probability that φ is true given that ψ is true lies within the interval[q_,q¯]”, whereφ,ψ∈LXnare propositional formulas andq_,q¯∈[0,1]are real numbers. Note that we do not assumeq_≤q¯, since we are going to measure inconsistency. If ψ is a tautology, a conditional like(φ|ψ)[q_,q¯]is called an unconditional probabilistic assessment, usually denoted by(φ)[q_,q¯]. We say a conditional in the format(.)[q,q]is precise and denote it by(.)[q].A probabilistic interpretationπ:WXn→[0,1], with∑jπ(wj)=1, is a probability mass over the set of possible worlds, which induces a probability measurePπ:LXn→[0,1]by means ofPπ(φ)=∑{π(wj)|wj⊨φ}. A conditional(φ|ψ)[q_,q¯]is satisfied by π iffPπ(φ∧ψ)≥q_Pπ(ψ)andPπ(φ∧ψ)≤q¯Pπ(ψ). Note that whenPπ(ψ)>0, a probabilistic conditional(φ|ψ)[q_,q¯]is constraining the conditional probability of φ given ψ; but any π withPπ(ψ)=0trivially satisfies the conditional(φ|ψ)[q_,q¯](this semantics is adopted by Halpern [15], Frisch and Haddawy [11] and Lukasiewicz [30], for instance). A knowledge base is a finite set Γ of probabilistic conditionals such that, if(φ|ψ)[q_,q¯],(φ|ψ)[q_′,q¯′]∈Γ, then[q_,q¯]=[q_′,q¯′]. That is, for each pairφ,ψ, only one probability interval can be assigned to(φ|ψ)in a knowledge base.33Note that this requirement is not too restrictive. Since nothing was said about logically equivalent propositions, a knowledge base may contain different probability intervals assigned to φ andφ∧⊤, for instance.A knowledge base Γ is consistent (or satisfiable) if there is a probability mass satisfying all conditionals(φ|ψ)[q_,q¯]∈Γ. It is precise if all intervals are singletons.The problem of verifying the consistency of a knowledge base is called probabilistic satisfiability (or PSAT) [12]. Probabilistic satisfiability has been rediscovered several times, and an analytical and unconditional version was actually proposed by Boole [2]. Hailperin [14], Bruno and Gilio [3], and Nilsson [33] suggested solutions via linear programs. This linear programming approach can be easily extended to handle conditional probabilities under the semantics we are using [16]. Recent advances in algorithms for PSAT solving can be found in [17,9,28].If any probability mass π satisfying(φ|ψ)[q_,q¯]impliedPπ(ψ)>0, in an alternative semantics, the latter restriction could be added to the program, although losing the linear program standard format; this is the semantics adopted by Muiño [31], for instance. De Finetti proposed an alternative setting in which the conditional probability is fundamental [8] and the satisfaction of probabilistic conditionals does not trivialize when the conditioning event has null probability. In such scenario, the consistency is called coherence, and its checking demands solving a sequence of linear programs [5].When all interval bounds are rational numbers, PSAT is an NP-complete problem [12]; if there is a solution, there is a solution with onlym+1possible worlds receiving positive probability mass, where m is the knowledge base size. Nevertheless, column generation methods can handle large problems [26,24], and several approaches have recently appeared [28,9,17,7]. Note that this linear programming approach can be applied to other probabilistic logics (see, for instance, [1] and [23]).Approaches to measuring inconsistency in probabilistic knowledge bases have been put forward by Muiño [31], Thimm [44] and Potyka [34], with different semantics for the conditionals. We follow the one adopted by Thimm and Potyka, in which a conditional is also satisfied by any measure assigning null probability to the conditioning formula. Thimm has done a groundlaying work [44], extending Hunter's postulates for inconsistency measures to the probabilistic case, which is our starting point. Potyka suggests feasible measures [34] we will review in Section 5.1, after investigating carefully the postulates. In this section, we begin with some desirable properties proposed by Thimm and then argue against their joint satisfiability.LetK(Kprec) be the set of all (precise) knowledge bases. An inconsistency measure for knowledge bases is a functionI:K→[0,∞). Thimm's investigation is restricted to measuresI:Kprec→[0,∞)over knowledge bases with precise probabilities, to what we narrow our focus in this section. The author proposes some desirable properties such a function should satisfy, following Hunter and Konieczny's work for classical logic [20]. Although Thimm investigates a total of ten postulates, we describe in this section only four of these properties that we consider problematic. The first one claims that an inconsistency measure must at least discriminate between consistent and inconsistent bases:Postulate 3.1ConsistencyI(Γ)=0iff Γ is consistent.A second desirable property has to do with probabilistic conditionals one can ignore while measuring inconsistency, since they are not involved with the unsatisfiability, in some sense. Some notation is needed to formalize it.Definition 3.2A set Γ of probabilistic conditionals is a minimal inconsistent set (MIS) if Γ is inconsistent and every setΓ′⊊Γis consistent.Minimal inconsistent sets can be considered the purest form of inconsistency [21], capturing its causes. The focus on MISes is derived from the seminal work of Reiter [36] on the diagnosis problem. Reiter investigated how formulas from a base could be ruled out in order to restore consistency, by choosing at least one element from each MIS, computing thusly a hitting set of their collection.LetMIS(Γ)denote the collection of all MISes in Γ. Now we can define the central concept of free probabilistic conditional, following Thimm [44]:Definition 3.3A free probabilistic conditional of Γ is a probabilistic conditionalα∈Γsuch that, for allΔ∈MIS(Γ),α∉Δ.Analogously, a free probabilistic conditional of Γ is in all its maximal consistent subsets. The postulate of independence then claims that ruling out a free probabilistic conditional from a knowledge base should not change its inconsistency degree [44].Postulate 3.4IndependenceIf α is a free probabilistic conditional of Γ, thenI(Γ)=I(Γ∖{α}).A stronger condition, also introduced by Hunter and Konieczny and adopted by Thimm, deals with a sort of decomposability of the inconsistency measure through its minimal inconsistent sets. We call it a property, saving the name “postulate” to the most basic properties required from every measure. The version we present is tailored from Hunter and Konieczny's work [20]:Property 3.5MIS-separabilityIfΓ=Δ∪Ψ,Δ∩Ψ=∅andMIS(Γ)=MIS(Δ)∪MIS(Ψ), thenI(Γ)=I(Δ)+I(Ψ).The idea behind this property is that the inconsistency of the whole knowledge base should be the sum of the inconsistency of its parts, whenever the partition does not break any minimal inconsistent set. For instance, considerΔ={(x1)[0.5],(¬x1)[0.6]},Ψ={(x2)[0.7],(x2∧x3)[0.8]}andΓ=Δ∪Ψ. It is clear that Δ and Ψ are the only minimal inconsistent sets in Γ. MIS-separability posits that the measure of inconsistency of Γ is obtained by summing the measures of Δ and Ψ; formally,I(Γ)=I(Δ)+I(Ψ). MIS-separability is stronger than independence [44]:Proposition 3.6IfIsatisfies MIS-separability, thenIsatisfies independence.These properties can be found in Hunter and Konieczny's work [21], in the definition of a “MinInc” separable basic inconsistency measure for knowledge bases over classical propositional logic. The measures they introduce are shown to fit such desiderata. Thimm revises the adaptation of these classical inconsistency measures to the probabilistic case and convincingly argues that they are not suitable to the quantitative nature of probabilities, since classical logic is qualitative.To motivate the search for new inconsistency measures for probabilistic knowledge bases, while dispensing with measures from classical logic, Thimm puts forward the postulate of continuity. Intuitively, one expects that small changes in the probabilities of a knowledge base yield small changes in its degree of inconsistency. To formalize the continuity concept in precise knowledge bases, we introduce some notation, following Thimm [44].That work studies precise knowledge bases of the formΓ={(φi|ψi)[qi]|1≤i≤m}. For each precise knowledge base Γ, there is a characteristic functionΛΓ:[0,1]|Γ|→Kprecthat, roughly speaking, changes the probabilitiesqiin the base; i.e.,ΛΓ(〈q1′,q2′,…,qm′〉)={(φi|ψi)[qi′]|1≤i≤m}. To handle the (consistent) empty knowledge base, we defineΛ∅:{∅}→{∅}. Thimm imposes some order on the set Γ, building a sequence, for the functionΛΓbe unique and well-defined. For simplicity, we just suppose there is some order (say, lexicographic) over the probabilistic conditionals used to uniquely specifyΛΓ.44Technically, we could use the lexicographic order over the pairs(φi|ψi)to construct a function Lex taking each set Γ to the corresponding sequenceΨ=Lex(Γ), uniquely specifying a functionΛΨ′that changes the probabilities of the sequence Ψ. Then it could be definedΛΓ(q)=Lex−1(ΛΨ′(q)).Now the continuity postulate can be enunciated, with ∘ denoting function composition:Postulate 3.7Continuity (for precise probabilities)For allΓ∈Kprec, the functionI∘ΛΓ:[0,1]|Γ|→[0,∞)is continuous.To find inconsistency measures holding the desirable properties, including continuity, Thimm introduces a family of measures based on distance minimization, taking into account the numerical value of the probabilities. The basic idea is to quantify the inconsistency through the minimum changes, according to some distance, one has to apply on the probabilities to make the base consistent. The compatibility of consistency, independence and continuity is implicitly stated when it is proved that this whole family of inconsistency measures based on distance minimization satisfies them; and another family is proved to hold MIS-separability as well [44].The work done by Thimm [44] has carefully analyzed the problem of measuring inconsistency in knowledge bases over probabilistic logic. Desirable properties were borrowed from classical logic [20], and the crucial postulate of continuity was added. To attend these properties, measures based on distance minimization were introduced and some important results were proved. However, under a closer examination, the proposed postulates are incompatible.Theorem 3.8There is no inconsistency measureI:Kprec→[0,∞)that satisfies consistency, independence and continuity.To prove by contradiction, suppose there is a measureIsatisfying consistency, independence and continuity. Consider the following knowledge bases:(1)Γ={(x1∧x2)[0.5+ε],(x1∧¬x2)[0.5]}for some0<ε≤0.1(2)Δ=Γ∪{α},α=(x1)[0.8]We are going to useIto measure the inconsistency of Δ whenε→0. To apply independence, we are going to show that α is free in Δ; we prove that Γ is the only MIS in Δ. Note that{(x1∧x2)[0.5+ε],(x1)[0.8]}is consistent for anyε∈(0,0.1], for such set is satisfied by the probability measure induced by the following probability mass:π1(x1∧x2)=0.5+ε,π1(x1∧¬x2)=0.3−ε,π1(¬x1∧x2)=π1(¬x1∧¬x2)=0.1. To prove that{(x1∧¬x2)[0.5],(x1)[0.8]}is consistent, consider the following probability mass:π2(x1∧x2)=0.3,π2(x1∧¬x2)=0.5,π2(¬x1∧x2)=π2(¬x1∧¬x2)=0.1. Hence, all MISes of Δ must containΓ={(x1∧x2)[0.5+ε],(x1∧¬x2)[0.5]}, for other subsets are all consistent. Furthermore, note that Γ is inconsistent and minimal, so it is a MIS. We can conclude that Γ is the only MIS in Δ, for any value of0<ε≤0.1. As α is a free probabilistic conditional of Δ, we can apply independence:I(Δ)=I(Γ),for any0<ε≤0.1.To exploit the continuity ofI, we need the characteristic function of Δ,ΛΔ:[0,1]3→Kprec, to be well-defined; so, we need an order over the probabilistic conditionals. Suppose that Γ and Δ are ordered as they were defined in (1) and (2). Letq⁎be the vector〈0.5,0.5,0.8〉. It follows thatΛΔ(q⁎)differs from Δ only in its first conditional, which becomes(x1∧x2)[0.5]. Now we prove thatΛΔ(q⁎)is inconsistent. For any probability measurePπ,Pπ(x1∧x2)=Pπ(x1∧¬x2)=0.5impliesPπ(x1)=1, contradictingα={(x1)[0.8]}. AsIsatisfies consistency,(3)I∘ΛΔ(q⁎)>0.By the continuity ofI, the functionI∘ΛΔ:[0,1]3→[0,∞)must be continuous, so there must be a limit at the pointq⁎, and such limit must be unique for any path approachingq⁎:limq→q⁎⁡I∘ΛΔ(q)=limε→0+⁡I∘ΛΔ(〈0.5+ε,0.5,0.8〉)=limε→0+⁡I(Δ).By independence, we also have:limε→0+⁡I(Δ)=limε→0+⁡I(Γ).AsIsatisfies continuity and{(x1∧x2)[0.5],(x1∧¬x2)[0.5]}is satisfiable, the consistency ofIimplies(4)limε→0+⁡I(Γ)=I({(x1∧x2)[0.5],(x1∧¬x2)[0.5]})=0=limq→q⁎⁡I∘ΛΔ(q).The continuity ofIrequires thatI∘ΛΓ(q⁎)=limq→q⁎⁡I∘ΛΓ(q), which by (3) and (4) is a contradiction, finishing the proof.  □Corollary 3.9There is no inconsistency measureI:Kprec→[0,∞)that satisfies consistency, MIS-separability and continuity.Looking at the counterexample given in the proof of Theorem 3.8 may shed some light on what is the cause of such conflict among the desirable properties. The only minimal inconsistent set in Δ is Γ, and so independence forces the degree of inconsistency of Δ to be the same as that of Γ, but this is not generally the case when inconsistency is measured via probability changing. This happens due to the fact that changing the probabilities in Γ to some consistent setting does not in general imply that Δ becomes consistent. Although Γ is the only minimal inconsistent set of Δ, there is another way to prove the contradiction. Note that Γ implies(x1)[1+ε], withε>0, which contradicts a probability axiom, but also contradictsα=(x1)[0.8]. Whileε=0consolidates Γ, consolidating Δ requires a bigger change in probabilities, which is ignored by independence. By demandingI(Δ)=I(Γ)forε>0, the postulate of consistency forces a discontinuity onε=0. Whenε→0, the inconsistency degree of Γ tends to zero (by continuity), and independence requires the same from Δ. But this contradicts continuity, given consistency, for{(x1∧x2)[0.5],(x1∧¬x2)[0.5]}would still contradict(x1)[0.8], and Δ would be inconsistent.The findings from the previous section suggest that in order to drive the rational choice of an inconsistency measure for knowledge bases, we must abandon at least one postulate among consistency, independence and continuity. We claim that a weakening of the desired properties can restore their compatibility, and in this section we investigate paths to achieve that goal. After reconciling the problematic postulates, we review other proposed properties for inconsistency measures and extend them to the general case of knowledge bases with imprecise probabilities, showing some measures to satisfy them.The consistency postulate seems to be indisputable, since the least one can expect from an inconsistency measure is that it separates inconsistent from consistent cases, or some inconsistency from none. The answer to the question of which property we should relax to restore compatibility is thus reduced to either independence or continuity. Hunter and Konieczny have already noted problems with independence in knowledge bases over classical logic, proposing to relax it [22]. Intuition shall be inclined towards keeping continuity, for it reflects the particular quantitative nature of probabilistic reasoning. A pragmatic reason to give up independence (and so MIS-separability) is simply to keep continuity, given consistency, to save inconsistency measures based on distance minimization. In the sequel, the withdrawal of independence within probabilistic logic is argued for in a more compelling way.The notion of free conditional and the postulate of independence are strongly related to the idea that minimal inconsistent sets are the causes of inconsistencies, as suggested by Hunter and Konieczny [20]. Thimm says that free conditionals are “harmless”, in some sense, to the consistency of a knowledge base [44]. What is behind these notions is the classical way of handling inconsistency through ruling out formulas, as Reiter proposed in his diagnosis problem [36] and as the standard AGM paradigm of belief revision defines base contraction (see [18] for a general view of the AGM paradigm). Reiter's hitting sets technique views a repair of some inconsistency set of formulas as giving up of at least one element from each minimal inconsistent set. For such repair to be minimal, no free formula should be discarded. In the AGM paradigm, the consolidation process of a belief base can be interpreted as the contraction of ⊥, the contradiction. The inclusion postulate claims that the result of a contraction is a subset of the belief base in question, and the relevance postulate forces the contraction of ⊥ to contain all free formulas of the base.When we move from classical to probabilistic logic, there is a natural way to relax formulas without completely losing their information. Note that ruling out a probabilistic conditional(φ|ψ)[q_,q¯]is semantically equivalent to changing it to(φ|ψ)[0,1], so it is a particular (and extreme) case of widening the probability interval. If we need to give up the belief on(φ|ψ)[q_,q¯]to restore consistency, perhaps there are someq_′≤q_andq¯′≥q¯such that(φ|ψ)[q_′,q¯′]can still be consistently believed. When inconsistency is measured continuously, through changes in probabilities, it is this more general kind of consolidation process that is being suggested. As it is indicated in the proof of Theorem 3.8, consolidating all minimal inconsistent sets (Γ) through probability changing does not imply consolidating the whole base (Δ). We can conclude that the concepts of free conditional and minimal inconsistent set are not suitable to analyze continuous inconsistency measures based on distance minimization.Furthermore, it seems that the definition of free conditional, and so independence, can be refined to be suitable for analyzing continuous measures, while continuity is a harder definition to be contrived to be compatible with independence. Hence, we can try to weaken independence, and perhaps MIS-separability, by modifying the notion of free conditional, instead of fully forgetting this postulate.As both independence and MIS-separability are defined via minimal inconsistent sets, in order to weaken these properties to reach compatibility with consistency and continuity, it seems reasonable to replace MIS by an alternative concept that could reconcile the desirable properties altogether. However, to do it in a principled way, we first analyze the concept of free probabilistic conditional as to the corresponding consolidation procedure and then modify it to save independence. Afterwards, a related notion of conflict that also fixes MIS-separability is introduced.A weaker form of independence has already been suggested in the literature. Thimm [44] defines a safe conditional as one whose atomic propositions are disjoint from those in the rest of the base. We also demand that the conditional be satisfiable in order to be safe.55Thimm [44] only considers conditionals(φ|ψ)[q_,q¯]such thatφ∧ψand¬φ∧ψare (classically) satisfiable, so the conditional is also satisfiable.The weak independence postulate then posits that ruling a (satisfiable) safe conditional out should not change the inconsistency measure of a base. Hunter and Konieczny have suggested the same weakening for independence, in the classical setting, when they acknowledge that independence may be too strong a property to require [22]. Weak independence is compatible with consistency and continuity, since Potyka's measures satisfy them [34]. Although safe conditionals are easily recognizable, we expect that they be rare in practice, due to the natural logical dependencies among propositions within a base. We are looking for a stronger, more useful notion of independence, between the safe-based and the free-based ones, hence we look for a concept between safe and free.Besides defining free probabilistic conditional through minimal inconsistent sets, one could equivalently do it via the notion of consolidation as giving up conditionals to restore consistency. Let us formalize this concept.Definition 4.1Let Γ be a knowledge base inK. An abrupt repair of Γ is any setΔ⊆Γsuch thatΓ′=Γ∖Δis consistent — we callΓ′an abrupt consolidation. If an abrupt repair Δ is such that, for everyΨ⊊Δ,Γ∖Ψis inconsistent, Δ is a minimal abrupt repair — andΓ′=Γ∖Δis a maximal abrupt consolidation.We can now prove66Long proofs of technical results are in a separate Appendix A.a result that states different ways to define a free probabilistic conditional, as being part of no minimal abrupt repairs (of all maximal consistent sets) or being consistent with any abrupt repair. We say a conditional α is consistent with a knowledge base Γ if there is a probability mass π that satisfies α and Γ.Theorem 4.2Consider a knowledge baseΓ∈Kand a probabilistic conditionalα∈Γ. The following statements are equivalent:1.There is no minimal abrupt repair Δ of Γ such thatα∈Δ.For all maximal abrupt consolidationΓ′of Γ,α∈Γ′.IfΓ′=Γ∖Δis an abrupt consolidation of Γ (equivalently, Δ is an abrupt repair of Γ), then α is consistent withΓ′.There is no minimal inconsistent setΔ⊆Γsuch thatα∈Δ.Note that the fourth statement above is the definition of free probabilistic conditional given in Section 3.1. The first and the second statements are clearly dual to each other, so we have presented two new ways of equivalently defining a free probabilistic conditional without mentioning minimal inconsistent sets, but using abrupt repair and abrupt consolidation. As it is suggested in the previous section, ruling a conditional out is equivalent to widening the corresponding interval to[0,1]— that is why we call it an abrupt repair. However, a probabilistic logic allows for a more general notion of consolidation, formalized below. To save notation, we write(φ|ψ)[q_,q¯]⊆(φ|ψ)[q_′,q¯′]ifq_′≤q_andq¯′≥q¯; and ⊊ is defined from ⊆ as usual.Definition 4.3Let Γ be a knowledge base inK.Γ′∈Kis a widening of Γ if there is a bijectionf:Γ→Γ′such thatα⊆f(α)for allα∈Γ; furthermore, if a wideningΓ′is consistent, we say it is a consolidation of Γ.In other words, a consolidation of Γ is the result of widening the probability intervals of its conditionals to a consistent setting. Analogously to the maximal abrupt consolidation, related to a minimal abrupt repair, we can define a sort of consolidation with minimal changes, we call dominant.Definition 4.4A consolidationΓ′of Γ is a dominant consolidation (or simply a d-consolidation) of Γ if, for all consolidations Ψ of Γ, ifΓ′is a widening of Ψ, thenΓ′=Ψ.A d-consolidationΓ′of Γ is such that if some probability interval of Γ were less widened, fixing the others, the resulting base would not be consistent. In other words, it is not possible to give up strictly less information than a d-consolidation while restoring consistency; for an interval to be less widened, another must be more enlarged. In these sense, the changes in the probability bounds are minimal, and the consolidation is maximal.From these concepts, two new definitions for free probabilistic conditional could be derived: a conditional is free if it is in any d-consolidation; or a conditional is free if it is consistent with any consolidation. We can prove these definitions are actually equivalent:Lemma 4.5Consider a knowledge baseΓ∈Kand a probabilistic conditionalα∈Γ. The following statements are equivalent:1.For all d-consolidationsΓ′of Γ,α∈Γ′.IfΓ′is a consolidation of Γ, then α is consistent withΓ′.A modification of the free probabilistic conditional concept is suggested by the comparison of Lemma 4.5 with Theorem 4.2, which would yield a different postulate of independence. To not overload the concept of free conditional, we say these probabilistic conditionals are innocuous, for they are consistent with any consolidation of the knowledge base.Definition 4.6An innocuous probabilistic conditional of Γ is a probabilistic conditionalα∈Γsuch that, for every dominant consolidationΓ′of Γ,α∈Γ′.The difference between free and innocuous conditionals can be seen in the knowledge base from the proof of Theorem 3.8, as the following example shows.Example 4.7Consider the following knowledge base:Δ={(x1∧x2)[0.6],(x1∧¬x2)[0.5],(x1)[0.8]}.As it was claimed in the proof of Theorem 3.8,{(x1∧x2)[0.6],(x1∧¬x2)[0.5]}is the only minimal inconsistent set of Δ; soα=(x1)[0.8]is a free probabilistic conditional. Nonetheless, Δ has no innocuous probabilistic conditional. This can be noted through the following dominant consolidation of Δ:Δ′={(x1∧x2)[0.55,0.6],(x1∧¬x2)[0.45,0.5],(x1)[0.8,1]}.Δ′is consistent and any consolidationΨ≠Δ′has at least one wider probability interval; soΔ′is dominant. But no original conditional of Δ is inΔ′, so none is innocuous. Equivalently, anyβ∈Δis inconsistent withΔ′. An example of innocuous conditional can be given in the knowledge baseΨ=Δ∪{(x2)[0.3,0.8]}, since(x2)[0.3,0.8]would be consistent with any consolidation of Ψ.An innocuous probabilistic conditional of Γ is consistent with any abrupt consolidation of Γ, since it is semantically equivalent to a consolidation with[0,1]probability intervals; furthermore, a safe conditional of Γ is clearly consistent with any consolidation of Γ:Proposition 4.8Consider a probabilistic conditionalα∈Γ. If α is safe, it is innocuous; if α is innocuous, it is free.As to the independence postulate, we modify it in a corresponding way:Postulate 4.9i-IndependenceIf α is an innocuous probabilistic conditional of Γ, thenI(Γ)=I(Γ∖{α}).From Proposition 4.8 follows the relation among weak independence, i-independence and independence:Corollary 4.10IfIsatisfies independence, thenIsatisfies i-independence. IfIsatisfies i-independence, thenIsatisfies weak independence.To redefine MIS-separability, we need a new notion of minimal conflict, related to the consolidation we introduced. Note that the union of minimal inconsistent sets is equal to the union of minimal abrupt repairs of a knowledge base, so that it forms the complement of the set of free probabilistic conditionals. To be consistent, we should provide a definition of conflicting sets such that their union is complementary to the set of innocuous conditionals. A set with all probabilistic conditionals that are not innocuous would be inconsistent when not empty, but would not have the minimality we are looking for. Such a set would be analogous to the union of all minimal inconsistent sets, but we search for a more fundamental notion of conflict, that can be derived by analyzing the consolidation properties of minimal inconsistent sets.A minimal inconsistent set is minimal regarding set inclusion, and this is related to the abrupt consolidation:Proposition 4.11A knowledge base Γ is a minimal inconsistent set iff Γ is inconsistent and there are noΔ1,…,Δk⊊Γ, withk≥1, such that:1.⋃i=1kΔi=Γ;For everyΓ′⊆Γ, ifΓ′∩Δiis an abrupt consolidation ofΔifor all1≤i≤k, thenΓ′is an abrupt consolidation of Γ.Intuitively, a minimal inconsistent set Γ is a conflict that cannot be analyzed in smaller subsets such that abruptly consolidating them implies abruptly consolidating Γ. Starting with a single inconsistent base Γ, we can find smaller subsetsΔisatisfying both items of 4.11. We can do this recursively on the inconsistent setsΔiuntil we reach unanalyzable conflicts, which happens to be minimal inconsistent sets. So, abruptly consolidating these sets is abruptly consolidating Γ. Substituting consolidation for abrupt consolidation, we have an analogous definition of conflict:Definition 4.12A knowledge base Γ is an inescapable conflict if Γ is inconsistent and there are noΔ1,…,Δk⊊Γ, withk≥1, such that:1.⋃i=1kΔi=Γ;IfΔi′is a consolidation ofΔifor all1≤i≤kand⋃i=1kΔi′is a widening of Γ, then⋃i=1kΔi′is a consolidation of Γ.The extra condition in the second item of Definition 4.12 forces consolidations of different knowledge basesΔi,Δj⊊Γwith some probabilistic conditional in common to agree in that probability interval; otherwise,⋃i=1kΔi′would not be a knowledge base. In other words, the second item says that if we widen the probability intervals of Γ making eachΔiconsistent, then Γ becomes consistent. Inescapable conflicts could equivalently be defined in an alternative way:Lemma 4.13A knowledge base Γ is an inescapable conflict iff there is a wideningΓ′of Γ such thatΓ′is a minimal inconsistent set.Lemma 4.13 captures the intuition behind the proof of Theorem 3.8, where there is a widening that consolidates any proper subset of the knowledge base without consolidating the whole base. As it happens with abrupt consolidation and MISes, to consolidate Γ, one only needs to widen its probability intervals in such a way that each inescapable conflict is solved.Corollary 4.14Consider two knowledge basesΓ,Γ′∈Ksuch thatΓ′is a widening of Γ. If for every inescapable conflictΔ⊆Γits widening{β∈Γ′|α∈Δandα⊆β}is consistent, thenΓ′is a consolidation of Γ.As all abrupt consolidations can be viewed as consolidations and each knowledge base is a widening of itself, an inescapable conflict is something weaker than a minimal inconsistent set:Corollary 4.15If Δ is a minimal inconsistent set, then Δ is an inescapable conflict.Example 4.16Consider again the knowledge base from Example 4.7:Δ={(x1∧x2)[0.6],(x1∧¬x2)[0.5],(x1)[0.8]}.As it was already shown,{(x1∧x2)[0.6],(x1∧¬x2)[0.5]}is the only minimal inconsistent set of Δ — and, by Corollary 4.15, it is an inescapable conflict. Nevertheless, it can be proved that the whole Δ is an inescapable conflict as well.Suppose, by contradiction, there areΔ1,…,Δk⊊Δsuch that⋃i=1kΔi=Δand, ifΔi′is a consolidation ofΔifor all1≤i≤kand⋃i=1kΔi′is a widening of Δ, then⋃i=1kΔi′is a consolidation of Δ. To build⋃i=1kΔi′, we pick a consolidationΔi′for eachΔi⊊Δ. There are two cases: (a)(x1∧x2)[0.6]∈Δi; and (b)(x1∧x2)[0.6]∉Δi. In case (a), we constructΔi′by widening the probability interval of the conditional(x1∧x2)[0.6]to(x1∧x2)[0.5,0.6]; formally,Δi′=(Δi∖{(x1∧x2)[0.6]})∪{(x1∧x2)[0.5,0.6]}. In case (b), we choose the trivial consolidationΔi′=Δi. Even though the proof is omitted, we claim that eachΔi′is consistent. Consider then the following knowledge base:Δ′=⋃i=1kΔi′={(x1∧x2)[0.5],(x1∧¬x2)[0.5],(x1)[0.8]}.By the premises,Δ′is a consolidation of Δ, but it is inconsistent, sinceΔ′∖{(x1)[0.8]}implies(x1)[1](as shown in Section 3.2). Finally, there cannot exist suchΔ1,…,Δk⊊Δ, and Δ is an inescapable conflict.We can now change MIS-separability to respect inescapable conflicts (IC) instead of minimal inconsistent sets. LetIC(Γ)denote the collection of all inescapable conflicts of Γ.Property 4.17IC-separabilityIfΓ=Δ∪Ψ,Δ∩Ψ=∅andIC(Γ)=IC(Δ)∪IC(Ψ), thenI(Γ)=I(Δ)+I(Ψ).As inescapable conflict is a weaker concept than MIS, MIS-separability is stronger than IC-separability.Corollary 4.18IfIsatisfies MIS-separability, thenIsatisfies IC-separability.Recall that a free probabilistic conditional is defined in the standard way as not belonging to any minimal inconsistent set. We prove the analogous result for innocuous conditionals and inescapable conflicts, linking all concepts introduced in this section.Theorem 4.19The following statements are equivalent:1.For all d-consolidationΓ′of Γ,α∈Γ′.IfΓ′is a consolidation of Γ, then α is consistent withΓ′.There is no inescapable conflict Δ in Γ such thatα∈Δ.α is an innocuous probabilistic conditional in Γ.A result analogous to Proposition 3.6 follows:Corollary 4.20IfIsatisfies IC-separability, thenIsatisfies i-independence.As already mentioned, inescapable conflicts are to consolidations as minimal inconsistent sets are to abrupt consolidations. If consolidation via conditionals withdrawal, as in Reiter's and AGM approaches, can focus on the collection of minimal inconsistent sets (ignoring free conditionals), consolidation through widening probability intervals can be done by watching only for the inescapable conflicts (ignoring innocuous conditionals). All these relations among free and innocuous probabilistic conditionals, minimal inconsistent sets and inescapable conflicts argue in favor of the new proposed postulates, whose compatibility with consistency and continuity we will prove.To replace the postulate of independence and the property of MIS-separability, we propose the weaker pair of i-independence and IC-separability towards building a compatible package together with consistency and continuity. Before proving such compatibility, the postulates have to be generalized to imprecise knowledge bases. To generalize consistency, i-independence and IC-separability is straightforward, we just enlarge their intended scope from knowledge bases inKprecto bases inK, but the continuity postulate demands some notation.LetΓ={(φi|ψi)[q_i,q¯i]|1≤i≤m}be a knowledge base. The characteristic function of Γ can be generalized as a functionΛΓ:[0,1]2m→Kthat changes both upper and lower bounds of each probabilistic conditional in Γ; formally,ΛΓ(〈q_1′,q¯1′,…,q_m′,q¯m′〉)={(φi|ψi)[q_i′,q¯i′]|1≤i≤m}. Now the continuity postulate can be generalized, with ∘ denoting function composition:Postulate 4.21ContinuityFor allΓ∈K, the functionI∘ΛΓ:[0,1]2|Γ|→[0,∞)is continuous.Note that the postulate above implies Postulate 3.7, which defines continuity for precise probabilities. Given a base Γ of size m, Postulate 3.7 considers a functionf:[0,1]m→Kprec(the characteristic function when probabilities are precise) such thatf(〈q1′,q2′,…,qm′〉)=ΛΓ(〈q1′,q1′,q2′,q2′,…,qm′,qm′〉)and requires thatI∘fbe continuous. But note that, ifI∘ΛΓis continuous, so isI∘f. Therefore, Theorem 3.8 and Corollary 3.9 also hold within the imprecise probability framework.Hunter and Konieczny proposed another basic postulate for inconsistency measures [20] that was also adopted by Thimm [44].Postulate 4.22MonotonicityFor any knowledge basesΓ,(Γ∪{α})∈K,I(Γ∪{α})≥I(Γ).Thimm actually suggests a stronger principle, super-additivity, which implies monotonicity. Since super-additivity is incompatible with normalization [44] — as also is IC-separability —, we state them as properties, and not postulates.Property 4.23Super-additivityFor any knowledge baseΓ∪Δ∈K, ifΓ∩Δ=∅, thenI(Γ∪Δ)≥I(Γ)+I(Δ).Property 4.24NormalizationFor any knowledge baseΓ∈K,I(Γ)∈[0,1].To attend the desirable properties, we generalize the inconsistency measures based on distance minimization proposed by Thimm [44] to the case of imprecise probabilities. Muiño introduced similar ideas under a different semantics for conditional probabilities [31]. Firstly, we define a family of p-norms.Definition 4.25Consider a (positive)m∈N>0and ap∈N>0∪{∞}. Given a vectorq=〈q1,q2,…,qm〉over the real numbers, the p-norm of q is‖q‖p=∑i=1m|qi|ppif p is finite; otherwise it is‖q‖∞=maxi⁡|qi|.Thimm defines a familyIpof inconsistency measures based on the p-norms, which we modify to also considerp=∞and handle the empty base.Definition 4.26Consider ap∈N>0∪{∞}and aΓ∈K. The functionIp:K→[0,∞)is thedp-inconsistency measure, defined asIp(Γ)=min⁡{‖q−q′‖p|ΛΓ(q)=ΓandΛΓ(q′)is consistent},for any non-empty Γ, andIp(∅)=0.Finally, we are in a position to show inconsistency measures satisfying the wanted properties. We extend Thimm's results to prove that alldp-inconsistency measures satisfy the reconciled postulates and that some of them hold additional properties. Muiño has similar results, though under a different semantics [31].Theorem 4.27For anyp∈N>0∪{∞},Ipis well-defined and satisfies the postulates of consistency, continuity, i-independence and monotonicity.The compatibility of IC-separability and super-additivity with consistency, continuity, monotonicity and i-independence is confirmed by theI1measure:Lemma 4.28Ipsatisfies super-additivity and IC-separability iffp=1.If normalization is required, we can use the following result due to Muiño [31]:Lemma 4.29Ipsatisfies normalization iffp=∞.Although we have compatible postulates to drive the rational choice of inconsistency measures, these desirable properties are satisfied by a myriad of functions. We may use other arguments to pick some particular inconsistency measures among those obeying the postulates. This section investigates computational aspects of measuring inconsistency through distance minimization, reviewing and generalizing measures proposed by Potyka [34] that can be handled via linear programming. In a second moment, we show how the concrete measures introduced can be justified by means of Dutch books, displaying the maximum guaranteed loss an agent would be exposed to, if stakes are limited. We also show that Dutch books offer other interesting measures.In order to better understand the connection between Potyka's inconsistency measures and incoherence measures based on Dutch books, it is worth detailing the construction of the corresponding linear programs. Furthermore, due to such link, every property we prove for Potyka's inconsistency measures shall be inherited by the equivalent Dutch book measures presented in the next section.To check the consistency of a knowledge base, one can use the well-known formulation of PSAT as a linear program [16]. Consider a knowledge baseΓ={(φi|ψi)[q_i,q¯i]|1≤i≤m}. Under the semantics adopted, each assessment(φi|ψi)[q_i,q¯i]is equivalent to the pairPπ(φi∧ψi)−q_iPπ(ψi)≥0andPπ(φi∧ψi)−q¯iPπ(ψi)≤0of restrictions onPπ. The knowledge base is consistent iff these 2m restrictions can be jointly satisfied by a probability measurePπinduced by a probability mass π. Consider two (m×2n)-matrices,A=[aij]andB=[bij], withaij=Iwj(φi∧ψi)−q_iIwj(ψi)andbij=Iwj(φi∧ψi)−q¯iIwj(ψi), in whichIwj:LXn→{0,1}is the indicator function of the set{φ∈LXn|wj⊨φ}—Iwjis the valuation relative to the possible worldwj. The knowledge base Γ is satisfiable iff there is a(2n×1)-vector π satisfying the system:(5)Aπ≥0(6)Bπ≤0(7)∑π=1(8)π≥0.Restrictions in (5) correspond toPπ(φi|ψi)≥q_i, and those in (6) codifyPπ(φi|ψi)≤q¯i; Constraints (7) and (8) force π to be a probability mass over the possible worldsw1,w2,…,w2n. As all constraints are linear, this system can be solved by linear programming techniques as Simplex. Despite the exponential number of columns, column generation methods can be used to handle them implicitly [26,24], keeping the computation efficient enough to solve large knowledge bases (thousands of probabilities in [17,10]).To measure inconsistency using distance minimization withIp, we can add to the system variablesε_i≥0(ε¯i≥0) corresponding to decrements (increments) in lower (upper) bounds of each probability interval. Any conditional(φi|ψi)[q_i,q¯i]yields a pair of restrictionsPπ(φi∧ψi)−q_iPπ(ψi)≥−ε_iPπ(ψi)andPπ(φi∧ψi)−q¯iPπ(ψi)≤ε¯iPπ(ψi). Computing theIpmeasure is thus reduced to minimizing the p-norm of the vector〈ε_1,ε¯1,…,ε_m,ε¯m〉.77Note that if we allowε_i<0(andε¯i<0), it would represent the tightening of a bound, useless when searching for consistency, and the minimization would avoid it anyway.Nonetheless, the constraints contain non-linear terms (fromε_iPπ(ψi)andε¯iPπ(ψi)), and Potyka points out that these programs have (non-global) local optima [34], so convex optimization techniques cannot be directly applied. Thus, computingIpis typically less efficient than deciding PSAT, as empirical results indicate [34].Potyka emphasizes this impracticability and suggests a new family of inconsistency measures, the minimal violation measures[34], which we adapt here to the case of imprecise probabilities. In order to keep constraints linear, “violation” variablesε_i,ε¯i≥0are inserted in the right-hand side ofPπ(φi∧ψi)−q_iPπ(ψi)≥0andPπ(φi∧ψi)−q¯iPπ(ψi)≤0, yieldingPπ(φi∧ψi)−q_iPπ(ψi)≥−ε_iandPπ(φi∧ψi)−q¯iPπ(ψi)≤εi¯. Potyka's minimal violation measures are obtained when the p-norm of〈ε_1,ε¯1,…,ε_m,ε¯m〉is minimized with such constraints. We denote byIpεthe optimal value from the following program, whereε_=[ε_i]andε¯=[ε¯i]are (m×1)-vectors:(9)min⁡‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖psubject to:(10)min⁡‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖pAπ≥−ε_(11)min⁡‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖pBπ≤ε¯(12)min⁡‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖p∑π=1(13)min⁡‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖pπ,ε_,ε¯≥0.The restrictions are all linear, and non-linear terms may appear only within the objective function. We can ignore the monotone function.pwithin the p-norm definition, applying it only after the minimization stops. The degree of each term in the new objective function is p, and forp=1a linear program is recovered, since‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖1=∑i=1mε_i+ε¯i. Hence, one can apply the standard Simplex and column generation methods to computeI1εwith practically the same efficiency as deciding PSAT [34].For any finite p different from 1, the system (9)–(13) has non-linear terms in its objective function, but this is not the case when we considerp=∞. The ∞-norm is equivalent to take the maximum of the vector〈ε_1,ε¯1,…,ε_m,ε¯m〉, but this is the same as considering allε_i,ε¯iequal to a single scalarε≥0. The measureI∞εis the solution of the following program [34], in whichε_=ε¯=[εε…ε]Tare (m×1)-vectors:(14)min⁡εsubject to:(15)min⁡εAπ≥−ε_(16)min⁡εBπ≤ε¯(17)min⁡ε∑π=1(18)min⁡επ,ε≥0.The system (14)–(18) is also a linear program, like (9)–(13) whenp=1, but has a lesser number of variables. However, Potyka remarks that the variable ε in (14)–(18) is involved in 2m restrictions, while each variableε_i,ε¯iappears in only one constraint in (9)–(13); therefore, the computation ofI∞εmay in practice be slightly less efficient than computingI1ε[34].For sets of unconditional probabilistic assessments, when all conditioning eventsψiare equivalent to ⊤, the inconsistency measuresIpandIpεare extensionally identical for all p. The reason is that the restriction onPπandε_i,ε¯icorresponding to a conditional is the same when computing both measures. For instance, any constraintPπ(φi∧ψi)−q¯iPπ(ψi)≤0becomes equivalent toPπ(φi)−q¯i≤0whenψiis a tautology, and inserting an error to the probability bound,Pπ(φi)−(q¯i+ε¯i)≤0, is the same as placing it in the right-hand side.Potyka has proved that these measures,Ipεwithp∈N>0∪{∞}, besides being computable via linear programming, satisfy the postulates of consistency, continuity and monotonicity for the case of precise probabilities [34]:Proposition 5.1For anyp∈N>0∪{∞},Ipε:Kprec→[0,∞)is well-defined and satisfies consistency, continuity, weak independence and monotonicity.I1εalso satisfies super-additivity.We can generalize the result above to encompass probability intervals and the new postulates we introduced:Theorem 5.2For anyp∈N>0∪{∞},Ipε:K→[0,∞)is well-defined and satisfies consistency, continuity, i-independence and monotonicity.I1εalso satisfies super-additivity and IC-separability; andI∞εsatisfies normalization.Now we have a set of compatible postulates for inconsistency measures and two particular measures satisfying them that can be computed rather efficiently using linear programming techniques. On the one hand,I1εalso satisfies super-additivity and IC-separability; on the other hand,I∞εadditionally satisfies normalization. Nonetheless, one can argue that these measures lack some proper justification, despite satisfying some postulates and being feasible, as Capotorti, Regoli and Vattari did [4]. They claim that distances between conditional probabilities are meaningless, being only geometrical measures. This might be the case, but it would only undermine theIpfamily. ForIpεmeasures, distances between probabilities are computed weighting by the probabilities of the conditioning formulas, so to speak, allowing some operational interpretation. In the next section, we provide a rationale forI1εandI∞εbased on Dutch books.In formal epistemology, there is an interest in measuring the incoherence of an agent whose beliefs are given as probabilities or lower previsions over propositions or random variables — a Bayesian agent. If we have propositions from classical logic, the formalized problem at hand is exactly the one we are investigating. When the agent's degrees of belief are represented by a knowledge base, to measure the agent's incoherence is to measure the inconsistency of such knowledge base. Schervish, Kadane and Seidenfeld [39,40,38] have proposed ways to measure incoherence of an agent based on Dutch books.Dutch book arguments are based on the agent's betting behavior induced by her degrees of belief, typically used to show their irrationality. To introduce the concept of Dutch book, we start with the unconditional case. Dutch book arguments rely on an operational interpretation of (imprecise) degrees of belief, in which their lower/upper bounds are defined through bet buying/selling prices. Suppose an agent believes that the probability of propositionφibeing true lies within[q_i,q¯i], for1≤i≤m. Consider a bet ticket on the propositionφithat returns a prize (from the ticket seller) ofλi≥0ifφiis the case, and is worthless ifφiis not the case. Dutch book arguments generally use a willingness-to-bet assumption that this agent is willing to buy such a bet ticket onφiforq_iλi≥0and to sell it forq¯iλi≥0, for anyλi≥0. Then, if a bettor can buy and/or sell a set of bet tickets from/to the agent that will cause her a sure loss no matter which possible world is the case, we say she is exposed to a Dutch book. This set of bet tickets that causes a guaranteed loss to the agent is called a Dutch book.Example 5.3Alice (the agent) and Bob (the bettor) are flying to the beach. To spend the time on the plane, they discuss and gamble on the destination weather, to be checked on arrival — will it be sunny and/or hot (say, at least20∘C)? Alice assigns probability intervals for three propositions, formed by the atomsx1=“the weather is sunny” andx2=“the weather is hot”:•she believes the probability of the weather being hot is between70%and80%; which we represent by the conditional(x1)[0.7,0.8];she thinks the probability of the weather being sunny is between50%and60%; which is represented by(x2)[0.5,0.6];she also says that the probability of the weather being both hot and sunny is at most10%; formalized into(x1∧x2)[0,0.1].He sells to Alice a bet ticket that pays back $10 if the weather is hot (x1is true); and 0 otherwise. Alice pays$10×0.7=$7for it, which she considers fair;Bob also makes Alice buy for$10×0.5=$5a bet ticket that will return $10 to her only if the weather is sunny (x2is true);Finally, Bob buys from Alice a bet ticket whose prize $10 is paid back only in case the weather is hot and sunny (x1∧x2is true); and Alice sells it for$10×0.1=$1.Note that, no matter how is the weather when they arrive and pay the prizes, the total quantity Alice can receive from Bob is at most $10. Since she was losing $11 before landing and checking the weather, she will lose at least $1 in the end. Given this sure loss scenario, this set of three bets is said to be a Dutch book.Instead of the agent (the bettor) paying for a bet ticket and eventually getting its prize back from the bettor (the agent), we can view this whole operation as a single contract between these two players.Definition 5.4A gamble onφ∈LXnis an agreement between the agent and the bettor with two parameters, the stakeλ∈Rand the relative priceq∈[0,1], stating that:•the agent paysλ×qto the bettor if φ is false;the bettor paysλ×(1−q)to the agent if φ is true.A gamble on φ with stakeλ≥0and relative price q is equivalent to the agent buying from the bettor a bet ticket forλ×qthat returns λ only if φ is the case; if the stake λ is negative, the gamble is equivalent to the bettor buying the same ticket from the agent. The willingness-to-bet assumption translates to gambles in the following way: if an agent believes that the probability of a proposition φ being true lies within[q_,q¯], she finds acceptable gambles on φ with any stakeλ≥0and relative priceq_and gambles with any stakeλ≤0and relative priceq¯. In Example 5.3, the tickets trading is equivalent to a set of three gambles: a gamble onx1with stake $10 and relative price 0.7; a gamble onx2with stake $10 and relative price 0.5; and a gamble onx1∧x2with stake$−10and relative price 0.1.A gamble on φ can be generalized to consider a conditioning event ψ. Consider a bet ticket that, when ψ is true, pays a prize of λ if φ is the case and returns 0 if φ is false. In other words, this bet ticket works as a gamble on φ when ψ is the case. However, suppose this bet ticket pays back to the agent the same amount that was spent in its buying if ψ is false — that is, the gamble is canceled. The following generalization of gambles capture these “conditional bets”:Definition 5.5A (conditional) gamble onφ|ψ∈LXn|LXnis an agreement between the agent and the bettor with two parameters, the stakeλ∈Rand the relative priceq∈[0,1], stating that:•the agent paysλ×qto the bettor if ψ is true and φ is false;the bettor paysλ×(1−q)to the agent if ψ is true and φ is true;the gamble is called off, causing neither profit nor loss to the involved parts, if ψ is false.Accordingly, we generalize the willingness-to-bet assumption: if an agent believes that the probability of a proposition φ being true given that another proposition ψ is true lies within[q_,q¯], she finds acceptable gambles onφ|ψwith stakeλ≥0and relative priceq_and gambles with stakeλ≤0and relative priceq¯. A Dutch book is a set of (conditional) gambles that the agent sees as fair, under the willingness-to-bet assumption, that causes her a guaranteed loss no matter which possible world is the case. We assume Dutch books contain exactly two gambles on(φi|ψi)per each conditional(φi|ψi)[q_i,q¯i]∈Γ, the base formalizing the agent's beliefs: one with stakeλ_i≥0and the other with stake−λ¯i≤0. This is not restrictive, since gambles on the same(φi|ψi)with the same relative price can be merged by summing the stakes, and the absence of a gamble is equivalent to a stake equal to zero. We can thus denote a Dutch book simply by the absolute value of its stakesλ_1,λ¯1,…,λ_m,λ¯m≥0, wherem=|Γ|. Actually, any set of gambles involving an agent whose epistemic state is represented by Γ can be represented by these 2m (absolute value of) stakes, since the relative prices are set in Γ.If the set of probabilistic conditionals that represents an agent's epistemic state turns out to be inconsistent, then she is exposed to a Dutch book, and vice-versa [32]. In other words, an agent sees as fair a set of gambles that causes her a guaranteed loss if, and only if, the knowledge base codifying her (conditional) degrees of belief is inconsistent. We can check this connection in Example 5.3:(x1)[0.7,0.8]and(x2)[0.5,0.6]imply a probability of at least 0.2 forx1∧x2, which Alice violates. Consequently, she is exposed to a Dutch book, for her three probability interval assignments are not jointly satisfiable. In this way, Dutch book arguments were introduced to show that degrees of belief must obey the axioms of probability and are a standard proof of incoherence (introductions to Dutch books and their relation to incoherence can be found in [42] and [8]). Hence, a natural approach to measuring an agent's degree of incoherence is through the magnitude of the sure loss she is vulnerable to. The intuition says that, the more incoherent an agent is, the greater the guaranteed loss that can be imposed on her through a Dutch book. Nevertheless, with no bounds on the stakes, such loss would also be unlimited for incoherent agents. For instance, in Example 5.3, if stakes were $100, $100 and$−100, Alice would have a net loss of at least $10, instead of $1, regardless of the weather on arrival. To better understand the loss a Dutch book causes to an agent, we formalize it in the following.Consider the knowledge baseΓ={(φi|ψi)[q_i,q¯i]|1≤i≤m}representing an agent's epistemic state. Letλ_i,λ¯i≥0denote gambles on(φi|ψi), the first with relative priceq_iand stakeλ_i≥0, the second with relative priceq¯iand stake−λ¯i≤0, for1≤i≤m. A set of gambles can then be represented by the vector〈λ_1,λ¯1,…,λ_m,λ¯m〉. If a possible worldwjis the case, the net profit for the agent regarding a bet onφ|ψwith stake λ and relative price q can be computed viaλ(Iwj(φ∧ψ)−qIwj(ψ)),in whichIwj:LXn→{0,1}is the indicator function of the set{φ∈LXn|wj⊨φ}— a valuation. For a gamble on(φi|ψi)with stakeλ_i(or−λ¯i), the agent's net profit in a possible wordwjisλ_i(Iwj(φi∧ψi)−q_iIwj(ψi))(or−λ¯i(Iwj(φi∧ψi)−q¯iIwj(ψi))). Recall (from (5)–(8)) thataij=Iwj(φi∧ψi)−q_iIwj(ψi)andbij=Iwj(φi∧ψi)−q¯iIwj(ψi). If a given possible worldwjis the case, the set of gambles〈λ_1,λ¯1,…,λ_m,λ¯m〉gives the agent a profit of∑i=1maijλ_i+∑i=1m−bijλ¯i. Let ℓ be the sure loss (−ℓ is profit) a set of gambles yields to the agent; i.e., no matter which possible world is the case, the agent loses at least ℓ. Thus, ℓ is such that∑i=1maijλ_i+∑i=1m−bijλ¯i≤−ℓfor all possible worldswj. When there is no restriction on the stakes, to find the set of gambles〈λ_1,λ¯1,…,λ_m,λ¯m〉that maximizes the sure loss is to solve the following linear program:(19)max⁡ℓsubject to:(20)max⁡ℓ[1a11…am1−b11…−bm11a12…am2−b12…−bm2⋮⋮⋱⋮⋮⋱⋮1a12n…am2n−b12n…−bm2n][ℓλ_1⋮λ_mλ¯1⋮λ¯m]≤[00⋮0](21)max⁡ℓλ_1,λ¯1,…,λ_m,λ¯m≥0.The linear program above can be viewed as the dual of that in lines (5)–(8), which checks the consistency of Γ, if we consider that 0 is the function being minimized in the latter, since we are interested only in its feasibility (for duality theory in linear programing, see, for instance, [46]). Note that, in (5)–(8),Bπ≤0is equivalent to−Bπ≥0and∑π=1can be inserted into A as a line of 1's. By duality theory, as the program above is feasible, it is unbounded iff (5)–(8) is infeasible. That is, if Γ is inconsistent, sure loss via Dutch book is unlimited.Different strategies to circumvent this in order to measure incoherence as a finite loss are found in the formal epistemology literature. Schervish et al. propose a flexible formal approach to limiting these stakes generating a family of incoherence measures for upper and lower previsions on bounded random variables [40]. In this section, we are interested in two of them, which we simplify to our case.Their whole family of incoherence measures is based on the maximum guaranteed loss an agent is exposed to via a Dutch book, varying only on how stakes are limited. The first incoherence measure Schervish et al. introduce that concerns us is when the sum of the absolute values of the stakes is lesser than or equal to one,∑iλ_i+λ¯i≤1. The second incoherence measure we investigate is defined as the maximum guaranteed loss when each stake have absolute value no greater than one, orλ_i,λ¯i≤1.88Schervish et al. [40] actually measure the incoherence as maximum rates between the guaranteed loss and the sum (the maximum) of the stakes' absolute values. Clearly, this is equivalent to maximizing the guaranteed loss when the sum of the stakes' absolute values is no greater than 1 (or these absolute values are in[0,1]).We define the inconsistency measuresISSKsum:K→[0,∞)andISSKmax:K→[0,∞)on knowledge bases as these two incoherence measures on the corresponding agents represented by these knowledge bases. That is, we equateISSKsum(Γ)(andISSKmax(Γ)), for anyΓ∈K, to the maximum sure loss an agent whose epistemic state is represented by Γ is exposed to through a Dutch book when the sum (maximum) of the stakes' absolute values is at most one.Example 5.6Recall Example 5.3, in which there are three gambles, with stakes $10, $10 and$−10. These gambles guarantee a loss of at least $1 to Alice. But now suppose that Bob, while choosing the gambles, must do it so that the absolute values of the stakes sum up to one. He could so arrange the same gambles but changing the stakes to 1/3, 1/3 and−1/3. In this new scenario, Alice would have a sure loss of 1/30. Similarly, if the absolute value of each stake is limited to the interval[0,1], stakes could be 1, 1 and −1, yielding a guaranteed loss of 1/10 to Alice. In fact, it can be checked (by solving the linear programs) that 1/30 and 1/10 are the greatest amount one can take for sure from Alice via Dutch book if stakes have absolute values summing up to one or are all in[0,1], respectively. Formalizing, withΓ={(x1)[0.7,0.8],(x2)[0.5,0.6],(x1∧x2)[0,0.1]}codifying Alice's epistemic state, we haveISSKsum(Γ)=1/30andISSKmax(Γ)=1/10.Even though incoherence measures based on Dutch books from the formal epistemology community and inconsistency measures based on distance minimization from Artificial Intelligence researchers may seem unrelated at first, they are actually two sides of the same coin. The programs that compute the maximum guaranteed loss an agent is exposed to are technically dual to those that minimize distances to measure inconsistency. Nau has already investigated this matter, mentioning results similar to the following [32]:Theorem 5.7For anyΓ∈K,ISSKsum(Γ)=I∞ε(Γ).Just add the constraintλ_1+λ¯1+⋯+λ_m+λ¯m≤1to the linear program (19)–(21). The dual of this new program would become the program (14)–(18), which computesI∞ε(Γ). So, by the strong duality theorem,ISSKsum(Γ)=I∞ε(Γ), for both programs are always feasible.  □Recall thatI∞εis exactly one of the two feasible measures proposed by Potyka [34]. Far from meaningless, such measure quantifies the maximum sure loss an agent is exposed to when the sum of the stakes is no greater than one — or, equivalently, fixed at one.As to Potyka's other feasible proposal,I1ε, duality in linear programming provides a correspondence with the second incoherence measure we presented from Schervish et al.:Theorem 5.8For anyΓ∈K,ISSKmax(Γ)=I1ε(Γ).Similarly to the proof of Theorem 5.7, insert the constraintsλ_i,λ¯i≤1, for1≤i≤m, into the linear program (19)–(21). The dual of this new program would become the program (9)–(13), withp=1, which computesI1ε(Γ). Again, by the strong duality theorem,ISSKmax(Γ)=I1ε(Γ), since both programs are always feasible.  □Theorem 5.8 states the extensional identity betweenI1εandISSKmax. Within the unconditional probabilities scenario, this means that the Manhattan distance between the agent's probabilities and the closest consistent probabilities is equal to the maximum sure loss she is exposed to when stakes' absolute values are not higher than one.Theorem 5.7 and Theorem 5.8 give an operational interpretation for the inconsistency measuresI∞εandI1εbased on betting behavior. It was remarked in Section 5.1 thatIpεandIpgive the same inconsistency degrees to unconditional knowledge bases. Thus, Dutch books with limited stakes (λ_i,λ¯i≤1or∑iλ_i+λ¯i≤1) can be used to rationalize alsoI1andI∞in the unconditional setting. However, when we take into account conditional probabilities, onlyI1εandI∞εmeasure the maximum guaranteed loss an agent would be exposed to, when stakes are limited viaλ_i,λ¯i≤1or∑iλ_i+λ¯i≤1, respectively.Different strategies for bounding stakes can lead to different inconsistency measures, but our motivation in this section was not to use Dutch books to determine which measures should be adopted — that is the reason of the postulates. The point here is that these two measures (I1εandI∞ε), besides satisfying the postulates and being computable through linear programs, have a meaningful interpretation. In the next section, we show that other measures based on Dutch books have these qualities as well.In order to measure incoherence as the greatest guaranteed loss in a Dutch book, Schervish et al. have firstly proposed two different ways of normalizing such loss: by limiting either the agent's or the bettor's resources [39]. The authors introduce the concept of escrow as the amount committed into a gamble by the agent (or the bettor). For instance, consider a gamble onφi|ψiwith stakeλ_i≥0and relative priceq_i. The agent might loseq_iλ_iwith this gamble, while the bettor is exposed to a loss of(1−q_i)λ_i. Now consider a gamble on the same conditional with stake−λ¯i≤0and relative priceq¯i. The agent might have to pay(1−q¯i)λ¯ito the bettor, whilst the bettor might loseq¯iλ¯i≥0to the agent. Schervish et al. call these quantities the agent's and the bettor's escrows. Equivalently, the agent's (or bettor's) escrow for a gamble is how much she (he) has to commit from her (his) resources to cover an eventual loss.Instead of bounding the sum of the stakes, an agent's degree of incoherence can be measured, as the maximum guaranteed loss in a Dutch book, by limiting the agent's (or the bettor's) total escrow to one.99Again, this is equivalent to measuring incoherence as the maximum ratio between the sure loss and the agent's (the bettor's) total escrow.In other words, we are limiting how much the agent (or the bettor) could lose in case that every gamble resolves unfavorably, inflicting a loss to her (him). Schervish et al. give market situations that justifies these choices [39]. We denote byISSKa,sum:K→[0,∞)andISSKb,sumK→[0,∞)∪{∞}1010For reasons that will be clear soon, we relax in this section the definition of inconsistency measures, allowing their range to include ∞.the inconsistency measures corresponding to these two incoherence measures, when the agent's or the bettor's total escrow is at most one, respectively.Formally, starting with the linear program of lines (19)–(21),ISSKa,sum(Γ)andISSKb,sum(Γ)are obtained via the maximization of ℓ by adding further constraints. LetΓ={(φi|ψi)[q_i,q¯i]|1≤i≤m}be a knowledge base. To computeISSKa,sum(Γ), one need to insert the restriction∑i=1mq_iλ_i+(1−q¯i)λ¯i≤1into (19)–(21). Similarly,ISSKg,sum(Γ)is the solution (on ℓ) of the program (19)–(21) incremented with the constraint∑i=1m(1−q_i)λ_i+q¯iλ¯i≤1.The fact thatISSKg,summay be unbounded is acknowledged by Schervish et al. [39]. For instance, consider an agent whose belief state is given byΓ={(φ)[1],(¬φ)[1]}. The agent finds acceptable pairs of gambles on (φ and ¬φ) in which the bettor has escrows equal to zero (λ_i(1−q_i)=0, forq_i=1), and sure loss can be scaled arbitrarily up. In such cases, we defineISSKb,sum(Γ)=∞.Example 5.9Recall Example 5.3, its three gambles, with stakes $10, $10 and$−10, and the implied loss of at least $1 to Alice. But now suppose that Bob has to choose gambles in such a way that his (or Alice's) total escrow sum up to 1. Note that, with stakes $10, $10 and$−10, his total escrow is$10×(1−0.7)+$10×(1−0.5)+$10×0.1=$9(Alice's is$10×0.7+$10×0.5+$10×(1−0.1)=$21). He could then arrange the same gambles but changing the stakes to 10/9, 10/9 and−10/9(or 10/21, 10/21 and−10/21) in order to his (Alice's) total escrow be equal to one. In this new scenario, Alice would have a sure loss of 1/9 (or 1/21). Once again, one could verify (by solving the linear programs) that 1/9 and 1/21 are the greatest amount one can take for sure from Alice via Dutch book if Bob's or Alice's total escrow is no greater than 1, respectively. Formalizing, withΓ={(x1)[0.7,0.8],(x2)[0.5,0.6],(x1∧x2)[0,0.1]}codifying Alice's epistemic state, we haveISSKb,sum(Γ)=1/9andISSKa,sum(Γ)=1/21.Schervish et al. contemplate in detail a whole spectrum of ways to bound the agent's escrows, the bettor's, or their sum in order to measure the maximum sure loss [40]. For each of these three quantities, the author note that the two extreme functions in their framework used to normalize the guaranteed loss are the maximum and the sum, from which six different inconsistency measures arise [38]. Note that the sum of the agent's and the bettor's escrows for a single gamble is equal to the absolute value of its stake, soISSKsumandISSKmaxare two inconsistency measures from this same framework. To build the remaining two measures, escrows could be bounded via their maximum, instead of their total. Intuitively, this corresponds to limiting the quantity the agent (or the bettor) accepts to eventually lose in each individual gamble.We define the inconsistency measureISSKa,max:K→[0,∞)(andISSKb,maxK→[0,∞)∪{∞}) on knowledge bases as the degree of incoherence of the corresponding agents measured via the maximum sure loss she is exposed through a Dutch book if the agent's (the bettor's) escrow for each gamble in no greater than one. To computeISSKa,max(Γ), forΓ={(φi|ψi)[q_i,q¯i]|1≤i≤m}, we may again use the linear program (19)–(21) and compute the maximum value of ℓ with extra constraintsq_iλ_i≤1and(1−q¯i)λ¯i≤1, for1≤i≤m. Similarly,ISSKb,max(Γ)is the solution (on ℓ) to the program formed by inserting the restrictions(1−q_i)λ_i≤1andq¯iλ¯i≤1, for1≤i≤m, into (19)–(21). As withISSKb,sum, we defineISSKb,max(Γ)=∞when such program is unbounded.Example 5.10Remember the scenario from Example 5.3, in which three gambles are considered, with stakes $10, $10 and$−10, and Alice has a guaranteed loss of at least $1. Now suppose that Bob can only choose gambles in which his (Alice's) eventual loss — the escrow — is lesser than or equal to one. In other words, his (her) maximum escrow is no greater than 1. With these constraints, Bob can choose the same three gambles, but with stakes 2, 2 and −2: his escrows are2×(1−0.7)=0.6,2×(1−0.5)=1and2×0.1=0.2(with stakes 10/9, 10/9 and−10/9, Alice's escrow are(10/9)×0.7=7/9,10/9×0.5=5/9and10/9×(1−0.1)=1). Note that Bob (Alice) can eventually lose at most 1 in a single gamble. In this new setting, Alice would have a guaranteed loss of 1/5 (or 1/9). By solving the corresponding linear programs, we would find that 1/5 and 1/9 are the greatest amount one can take for sure from Alice via Dutch book if Bob's or Alice's maximum escrow is no greater than 1, respectively. Formalizing, withΓ={(x1)[0.7,0.8],(x2)[0.5,0.6],(x1∧x2)[0,0.1]}codifying Alice's epistemic state, we haveISSKb,max(Γ)=1/5andISSKa,max(Γ)=1/9.These four inconsistency measures (ISSKa,sum,ISSKb,sum,ISSKa,maxandISSKb,max) based on limiting the escrows have most of the desirable properties we presented.Theorem 5.11ISSKa,sum,ISSKb,sum,ISSKa,maxandISSKb,maxare well-defined and satisfy consistency, i-independence and monotonicity.ISSKa,maxandISSKb,maxalso satisfy super-additivity and IC-separability.Lemma 5.12ISSKa,sum,ISSKa,max,ISSKb,sumandISSKb,maxare continuous for probabilities within(0,1).Lemma 5.13ISSKa,sumsatisfies normalization.Altogether,ISSKa,sum,ISSKb,sum,ISSKa,maxandISSKb,maxare all computable through linear programs, have the core desirable properties and can be given an operational interpretation.ISSKa,maxandISSKb,maxalso satisfy super-additivity and IC-separability, whileISSKa,sumis normalized. These measures can be good alternatives, depending on the context, as the market scenarios described by Schervish et al. [39].

@&#CONCLUSIONS@&#
Handling inconsistency has been receiving increased attention in the AI community since most inference methods rely on the consistency of the premises; and such requirement is commonly violated in large bases of probabilistic knowledge. A reasonable start point to deal with the inconsistency in probabilistic bases is to know how severe it is, and how this severity changes with the probabilities. In this work, we studied different ways of measuring inconsistency in probabilistic knowledge bases. Three aspects were discussed: postulates the measures should satisfy, the efficiency of the methods used to compute the measures, and possible meaningful interpretations for them. As it was argued for, the independence postulate shall be abandoned in favor of continuity. The causes of such incompatibility were analyzed, and a modification of independence was proposed to restore compatibility. Inconsistency measures that can be computed using linear programs were reviewed and proved to satisfy the postulates, and we gave them a rational by means of Dutch books. Finally, we showed that other measures in the literature based on Dutch books, and computable through linear programming, also satisfy the postulates.By restoring the compatibility of the postulates for measuring inconsistency in probabilistic knowledge bases, we put forward a new pair of properties one can use to formulate or evaluate measures: i-independence and IC-separability. These desirable properties are based on two new concepts: innocuous conditional and inescapable conflicts. Besides measuring inconsistency, these concepts may be useful for formalizing inference from inconsistent bases or performing probabilistic belief revision/update, for instance. Both concepts are derived from a specific consolidation procedure — the widening of probability intervals. If other consolidation methods are considered, one can define analogous concepts, even in a different logical formalism.In AI, inconsistency measures for probabilistic knowledge bases have been based on distance minimization, while in Formal Epistemology incoherence measures for Bayesian agents were focused on Dutch books vulnerability. The connections here established can help both communities to investigate their corresponding problems under a different angle.The introduced concepts of innocuous conditional and inescapable conflict might have practical use in measuring inconsistency only if their instances are recognizable in a reasonable time. Nothing was said here about the complexity of the computational task of finding innocuous conditionals and inescapable conflicts within a knowledge base, but they are clearly very hard problems. Thus, future work includes investigating these problems aiming at devising algorithms to solve them. It would also be interesting to propose concrete procedures to consolidate knowledge bases, as done in [35] for instance. To achieve that, one could rely on the same triplet: rationality postulates, efficiency of computation and meaningful interpretation. Another intended continuation of this work is to study principled ways of inferring probabilistic conclusions from inconsistent bases, using the ideas here presented. For instance, this could be done by defining the set of models of an inconsistent base as the set containing all models of each closest consistent base, construed as the consolidations corresponding to some inconsistency measure here studied.Proposition 3.6IfIsatisfies MIS-separability, thenIsatisfies independence.Let Γ be a knowledge base andα∈Γa free conditional. By MIS-separability, as α is free and all MISes of Γ are inΓ∖{α}, we haveI(Γ)=I(Γ∖{α})+I(α).  □There is no inconsistency measureI:Kprec→[0,∞)that satisfies consistency, MIS-separability and continuity.It follows directly from Theorem 3.8 and Proposition 3.6.  □Consider a knowledge baseΓ∈Kand a probabilistic conditionalα∈Γ. The following statements are equivalent:1.There is no minimal abrupt repair Δ of Γ such thatα∈Δ.For all maximal abrupt consolidationΓ′of Γ,α∈Γ′.IfΓ′=Γ∖Δis an abrupt consolidation of Γ (equivalently, Δ is an abrupt repair of Γ), then α is consistent withΓ′.There is no minimal inconsistent setΔ⊆Γsuch thatα∈Δ.The first two items are clearly dual, and the fourth one is the definition of free conditional. Suppose α is free in Γ. Note that all abrupt consolidationsΓ′of Γ are consistent with α. AsΓ′is consistent, it has no MIS, and adding α cannot create a MIS, for it is free. Thus, if an abrupt consolidation does not contain α, it is not maximal. Now suppose there is a maximal abrupt consolidationΓ′such thatα∉Γ′. ForΓ′is maximal, α cannot be consistent with it. AsΓ′is consistent, it has no MIS, and adding α creates a MIS (that contains α), which also is a MIS of Γ — hence, α cannot be free.  □Consider a knowledge baseΓ∈Kand a probabilistic conditionalα∈Γ. The following statements are equivalent:1.For all d-consolidationΓ′of Γ,α∈Γ′.IfΓ′is a consolidation, then α is consistent withΓ′.Suppose all d-consolidations of Γ contain α. For any consolidation Ψ, there is a d-consolidationΨ′such that, for eachβ′∈Ψ′, there is aβ∈Ψsuch thatβ′⊆β. Therefore, any probability mass π satisfyingΨ′must also satisfies Ψ, andα∈Ψ′implies π satisfies α as well. Now suppose there is a d-consolidation Ψ that does not contain α. Asα∈Γ, there is aβ∈Ψsuch thatα⊊β. For Ψ is dominant,(Ψ∖{β})∪{α}cannot be a consolidation and thus is inconsistent. Finally, α is not consistent with Ψ.  □Consider a probabilistic conditionalα∈Γ. If α is safe, it is innocuous; if α is innocuous, it is free.IfΓ={α}, then α is safe, innocuous and free iff it is satisfiable, thus we focus onΓ≠{α}. Let Γ be built over the set of atomsXn={x1,…,xn}. Suppose α is safe and, without loss of generality, the set of atoms appearing in α isXα={x1,…,xm}, for somem<n. As α is satisfiable, there is a probability massπα:WXα→[0,1]satisfying it, whereWXαis the set containing the2mpossible worlds with atoms fromXα. The baseΓ′=Γ∖{α}is built over the set of atomsXΓ′=Xn∖Xα. Any consolidation Ψ ofΓ′must also be formed by atoms inXΓ′. If Δ is a consolidation of Γ, there is a consolidation Ψ ofΓ′such thatΔ=Ψ∪{β}, for some β such thatα⊆β. LetπΨ:WXΓ′→[0,1]be the probability mass satisfying Ψ, whereWXΓ′is the set containing the2n−mpossible worlds with atoms fromXΓ′. Consider the probability massπ:WXn→[0,1]such thatπ(wi∧wj)=πα(wi)×πΨ(wj)for any pair(wi,wj)∈WXα×WXΓ′. Note that π satisfies Ψ and α, thus π satisfies Ψ and β. Therefore, α is consistent with any consolidationΔ=Ψ∪{β}of Γ and is innocuous by Lemma 4.5.Now suppose α is innocuous. Any abrupt consolidationΔ⊆Γis equivalent (and equisatisfiable) to a consolidationΔ′∈Γsuch thatΔ′=Δ∪{(φ|ψ)[0,1]|(φ|ψ)[q_,q¯]∈Γ∖Δ}. As α is innocuous, it is consistent with any consolidationΔ′and, consequently, any abrupt consolidation Δ. Finally, by Theorem 4.2, α is free.  □IfIsatisfies independence, thenIsatisfies i-independence. IfIsatisfies i-independence, thenIsatisfies weak independence.It follows directly from the definitions and Proposition 4.8.  □Proposition 4.11A knowledge base Γ is a minimal inconsistent set iff Γ is inconsistent and there are noΔ1,…,Δk⊊Γ, withk≥1, such that:1.⋃i=1kΔi=Γ;For everyΓ′⊆ΓifΓ′∩Δiis an abrupt consolidation ofΔifor all1≤i≤k, thenΓ′is an abrupt consolidation of Γ.(→) Suppose Γ is a MIS and there areΔ1,…,Δk⊊Γsatisfying both items. For any1≤i≤k, asΔi⊊Γis consistent,Γ∩Δiis an abrupt consolidation ofΔi. Thus, by the second item, Γ is an abrupt consolidation of itself, which contradicts the fact that Γ is inconsistent.(←) Now suppose Γ is inconsistent but not a MIS. LetMIS(Γ)={Δ1,…,Δm}be the set of MISes in Γ, for somem≥1. LetΔm+1denote the set of free formulas in Γ. Clearly,⋃i=1m+1Δi=Γ. Now consider a setΓ′⊆Γsuch thatΓ′∩Δiis consistent for any1≤i≤m+1. IfΓ′was inconsistent, it would contain a MISΔi∈MIS(Γ)andΓ′∩Δiwould be inconsistent — a contradiction. ThusΓ′is an abrupt consolidation of Γ.  □A knowledge base Γ is an inescapable conflict iff there is a wideningΓ′of Γ such thatΓ′is a minimal inconsistent set.(←) Consider a minimal inconsistent setΓ′that is a widening of Γ. To prove by contradiction, suppose Γ is not an inescapable conflict. As its wideningΓ′is inconsistent, Γ also is, for each conditional inΓ′is implied by a conditional in Γ. Hence, as Γ is not an inescapable conflict, there must beΔ1,…,Δk⊊Γsuch that⋃i=1kΔi=Γand, ifΔi′is a consolidation ofΔi, for all1≤i≤k, and⋃i=1kΔi′is a widening of Γ, then⋃i=1kΔi′is a consolidation of Γ. Consider such collectionΔ1,…,Δk⊊Γ. Note that, for eachΔi, there is a wideningΨi⊊Γ′, defined viaΨi={β∈Γ′|α∈Δiandα⊆β}, for1≤i≤k. Furthermore anyΨi⊊Γ′is consistent, forΓ′is a minimal inconsistent set. As⋃i=1kΨiis equal toΓ′, it is a widening of Γ. As eachΨiis consistent,Γ′must be a consolidation of Γ and, thus,Γ′is consistent. This is a contradiction, which proves that Γ is an inescapable conflict.(→) LetΓ={α1,α2,…,αm}be an inescapable conflict and defineΔi=Γ∖{αi}, for1≤i≤m. Note thatΔ1,…,Δm⊊Γsuch that⋃i=1mΔi=Γ. LetΔi′denote an arbitrary consolidation ofΔi, for1≤i≤m. If every set{Δ1′,Δ2′,…,Δm′|⋃i=1mΔi′is a widening of|Γ}is such that⋃i=1mΔi′is a consolidation of Γ, Γ would not be an inescapable conflict. So, there are consolidationsΔi′for eachΔi(≤′i≤m) such that⋃i=1mΔi′=Γ′is widening of Γ but is not consistent. AsΓ′is a widening of Γ,Γ′={α1′,α2′,…,αm′}for someαi⊆αi′for1≤i≤m. AsΔi=Γ∖{αi},Δi′=Γ′∖{αi′}, for1≤i≤m. Hence,Δ1′,…,Δm′are the maximal proper subsets of Γ, and every proper subset ofΓ′is consistent. Thus,Γ′is a minimal inconsistent set.  □Corollary 4.14Consider two knowledge basesΓ,Γ′∈Ksuch thatΓ′is a widening of Γ. If for every inescapable conflictΔ⊆Γits widening{β∈Γ′|α∈Δandα⊆β}is consistent, thenΓ′is a consolidation of Γ.We will prove via the contrapositive: given Γ and its wideningΓ′, ifΓ′is not a consolidation of Γ, then there is an inescapable conflictΔ⊆Γsuch that the set{β∈Γ′|α∈Δandα⊆β}is inconsistent.IfΓ′is not a consolidation of Γ,Γ′is inconsistent and must contain at least one minimal inconsistent set, that we denote byΔ′. Let Δ be the set{α∈Γ|β∈Δ′andα⊆β}— that is,Δ′⊆Γ′is a widening ofΔ⊆Γ. By Lemma 4.13, Δ is an inescapable conflict.  □Corollary 4.15If Δ is a minimal inconsistent set, then Δ is an inescapable conflict.Just note that Δ is a widening of itself. By Lemma 4.13, Δ is an inescapable conflict.  □IfIsatisfies MIS-separability, thenIsatisfies IC-separability.If follows directly from the definitions and Corollary 4.15.  □Theorem 4.19The following statements are equivalent:1.For all d-consolidationΓ′of Γ,α∈Γ′.IfΓ′is a consolidation of Γ, then α is consistent withΓ′.There is no inescapable conflict Δ in Γ such thatα∈Δ.α is an innocuous probabilistic conditional in Γ.By the definition of innocuous conditionals and Lemma 4.5, the first, the second and the fourth statements are equivalent. It remains to prove that α is innocuous iff there is no inescapable conflict Δ in Γ such thatα∈Δ.(→) Let α be innocuous in Γ. Suppose there is an inescapable conflictΔ⊆Γsuch thatα∈Δ. Consider the baseΨ=Δ∖{α}. LetΨ′be a consolidation of Ψ. Thus,Γ′=Ψ′∪{(φ|ψ)[0,1]|(φ|ψ)[q_,q¯]∈Γ∖Ψ}is consistent and it is a consolidation of Γ. Due to the fact that α is innocuous, α is consistent withΓ′(by Lemma 4.5) and, therefore, withΨ′. Consequently,Ψ′∪{α}is a consolidation of Δ for any consolidationΨ′of Ψ. Furthermore, if{β}is a consolidation of{α}(i.e.,α⊆β),Ψ′∪{β}is a consolidation of Δ. AsΨ,{α}⊊Δare such thatΨ∪{α}=Δ, and any consolidationsΨ′and{β}of theirs are such thatΨ′∪{β}is a consolidation of Δ, Δ is not an inescapable conflict, which is a contradiction.(←) Suppose there is no inescapable conflict Δ in Γ such thatα∈Δ. Consider the baseΨ=Γ∖{α}. Every consolidationΓ′of Γ can be written asΓ′=Ψ′∪{β}, whereΨ′is a consolidation of Ψ andα⊆β. As all inescapable conflicts of Γ are in Ψ, by Corollary 4.14,Ψ′∪{α}is consistent. Hence, α is consistent with any consolidationΓ′=Ψ′∪{β}and α is innocuous by Lemma 4.5.  □Corollary 4.20IfIsatisfies IC-separability, thenIsatisfies i-independence.Let Γ be a knowledge base andα∈Γan innocuous conditional. As α is innocuous, all inescapable conflicts of Γ are inΓ∖{α}by Lemma 4.19. By IC-separability, we haveI(Γ)=I(Γ∖{α})+I(α).  □For anyp∈N>0∪{∞},Ipis well-defined and satisfies the postulates of consistency, continuity, i-independence and monotonicity.To show thatIpis well-defined, we use results from the proof of Theorem 1 in [44]. For anyΓ={(φi|ψi)[q_i,q¯i]|1≤i≤m}, Thimm shows that the setQΓ={〈q1,…,qm〉∈Rm|ΛΓ(〈q1,q1,…,qm,qm〉)is consistent}is compact and closed, whereΛΓ:[0,1]2m→Kis the characteristic function of Γ. Leth:R2→Rbe a function such thath(a,b)=max⁡(0,a−b)for anya,b∈R. The measureIpis the minimum of‖fq_,q¯(q)‖pwithq∈QΓ, wherefq_,q¯:Rm→R2mis a function such thatfq_,q¯(〈q1,…,qm〉)=〈h(q_1−q1),h(q1−q¯1),…,h(q_m−qm),h(qm−q¯m)〉. Intuitively,fq_,q¯(q)measures, for each pointqi, how much the lower and the upper bounds have to change for we haveqi∈[q_i,q¯i]. Finally,Ipis well defined, forQΓis closed and compact [44].Consistency: By definition, a p-norm is never negative, thusIp(Γ)≥0. SupposeΓ=ΛΓ(q)is consistent. A vectorq′=qis such that‖q′−q‖p=0for anyp∈N>0∪{∞}, thusIp(Γ)=0. Now supposeΓ=ΛΓ(q)is inconsistent. For everyq′∈QΓ,q′≠q, then‖q′−q‖p>0andIp(Γ)>0for anyp∈N>0∪{∞}.Continuity: Given a baseΓ={(φi|ψi)[q_i,q¯i]|1≤i≤m}, its characteristic functionΛΓ:[0,1]2m→Kand a fixedq∈QΓ, define the functiongq:R2m→Rsuch thatgq(〈q_1,q¯1,…,q_m,q¯m〉)=‖fq_,q¯(q)‖p. Note thatIp∘ΛΓ(〈q_1,q¯1,…,q_m,q¯m〉)is computed as the minimum of{gq(〈q_1,q¯1,…,q_m,q¯m〉)|q∈QΓ}. Eachgqis continuous, and the minimum of continuous functions is continuous, henceIp∘ΛΓis continuous.Monotonicity: LetΛΓ(q′)be a consolidation ofΓ=ΛΓ(q)such that‖q′−q‖pis minimized, for ap∈N>0∪{∞}, andIp(Γ)=‖q′−q‖p. To prove by contradiction, supposeI(Γ∪{α})<Ip(Γ), for someΨ=Γ∪{α}∈K. Hence, there is a consolidationΨ′=ΛΨ(r′)ofΨ=ΛΨ(r)such that‖r′−r‖p<‖q′−q‖p. Consider the baseΓ′=Ψ′∖{β}, such thatα⊆β. AsΨ′is consistent,Γ′=ΛΓ(q″)also is, and it is a consolidation of Γ. Since q andq″are projections (subsets, in sense) of r andr′,q″−qis a projection ofr′−rand‖q″−q‖p≤‖r′−r‖p<‖q′−q‖p. Finally, it would follow thatIp(Γ)≤‖q″−q‖p<‖q′−q‖p=Ip(Γ), which is a contradiction.i-Independence: Consider the basesΓ=ΛΓ(r)andΨ=Γ∖{α}inK, whereα=(φ|ψ)[q_,q¯]is innocuous in Γ. We are going to prove thatIp(Γ)≤Ip(Ψ), and the desired result follows from monotonicity. LetΨ′=ΛΨ(q′)be a consolidation ofΨ=ΛΨ(q)such that‖q′−q‖pis minimized, for ap∈N>0∪{∞}, andIp(Ψ)=‖q′−q‖p. Note thatΓ′=Ψ′∪{(φ|ψ)[0,1]}is a consolidation of Γ. Asα=(φ|ψ)[q_,q¯]is innocuous, α is consistent withΓ′andΨ′. Hence,Ψ′∪{α}=ΛΓ(r′)is a consolidation of Γ. Note thatr′−risq′−qwith two extra 0's (from alpha). Finally,Ip(Γ)≤‖r′−r‖p=‖q′−q‖p=Ip(Ψ).  □Lemma 4.28Ipsatisfies super-additivity and IC-separability iffp=1.(→) To note that super-additivity and IC-separability do not hold ifp>1, consider the basesΨ={(⊤)[0.9]},Δ={(⊥)[0.1]},Γ=Ψ∪Δ. By the definition of d-consolidation, ifIp(Γ)=d, then there is d-consolidationΛΓ(q′)ofΓ=ΛΓ(q)such that‖q′−q‖p=d. The only d-consolidations ofΨ,Δ,ΓareΨ′={(⊤)[0.9,1]},Δ′={(⊥)[0,0.1]},Γ′=Ψ′∪Δ′, for changing the lower bound in Ψ and the upper bound in Δ is useless to reach consistency. For any finite p,Ip(Ψ)=Ip(Δ)=0.1pp=0.1, andIp(Γ)=0.1p+0.1pp=0.12p. Forp=∞,Ip(Ψ)=Ip(Δ)=max⁡〈0.1〉=0.1andIp(Γ)=max⁡〈0.1,0.1〉=0.1. Therefore, for anyp>1∈N∪{∞},Ip(Γ)<0.2=Ip(Ψ)+Ip(Δ), and both super-additivity and IC-separability fail.(←) Now fixp=1. To prove that super-additivity holds, suppose there are basesΨ,Δ,Γ=Ψ∪ΔinKsuch thatΨ∩Δ=∅. LetΨ′=ΛΨ(q′),Δ′=ΛΔ(r′),Γ′=ΛΓ(s′)be d-consolidations ofΨ=ΛΨ(q),Δ=ΛΔ(r),Γ=ΛΓ(s)that minimize‖q′−q‖1,‖r′−r‖1,‖s′−s‖1, corresponding toI1(Ψ),I1(Δ),I1(Γ). Clearly,Γ′can be partitioned intoΨ″∪Δ″in such a way thatΨ″=ΛΨ(sΨ′),Δ″=ΛΔ(sΔ′)are consolidations ofΨ,Δ. By the construction ofsΨ′andsΔ′,‖s′−s‖1=‖sΨ′−q‖1+‖sΔ′−r‖1. Hence, forI1(Ψ)≤‖sΨ′−q‖1andI1(Δ)≤‖sΔ′−r‖1, it follows thatI(Γ)=‖s′−s‖1≥I1(Ψ)+I1(Δ).To prove that IC-separability holds, suppose there are basesΨ,Δ,Γ=Ψ∪ΔinKsuch thatΨ∩Δ=∅,IC(Γ)=IC(Ψ)∪IC(Δ). LetΨ′=ΛΨ(q′),Δ′=ΛΔ(r′), be consolidations ofΨ=ΛΨ(q),Δ=ΛΔ(r)that minimize‖q′−q‖1,‖r′−r‖1, corresponding toI1(Ψ),I1(Δ). AsΓ′=Ψ′∪Δ′=ΛΓ(s′)is a widening ofΓ=ΛΓ(s)such that, for eachΦ∈IC(Γ)=IC(Ψ)∪IC(Δ), the base{β∈Γ′|α∈Φandα⊆β}is consistent (all inescapable conflicts are solved),Γ′is a consolidation of Γ by Corollary 4.14. As‖s′−s‖1=‖q′−q‖1+‖r′−r‖1=I1(Ψ)+I1(Δ), it follows thatI1(Γ)≤I1(Ψ)+I1(Δ). By super-additivity,I1(Γ)≥I1(Ψ)+I1(Δ), thusI1(Γ)=I1(Ψ)+I1(Δ).  □Lemma 4.29Ipsatisfies normalization iffp=∞.(→) To note that normalization does not hold if p is finite, consider the baseΓ={(⊤)[0],(⊥)[1]}. The only d-consolidation of Γ isΓ′={(⊤)[0,1],(⊥)[0,1]}, for changing the lower bound in(⊤)[0]and the upper bound in(⊥)[1]is useless to reach consistency. For any finite p,Ip(Γ)=1p+1pp=2p>1, and normalization fails.(←) By definition,I∞(Γ)is the minimum of‖q′−q‖∞subject toΓ=ΛΓ(q)andΛΓ(q′)being consistent. As the vectorsq,q′are in[0,1]2|Γ|,‖q′−q‖∞∈[0,1], since|qi′−qi|∈[0,1]for all elementsqi,qi′ofq,q′.  □For anyp∈N>0∪{∞},Ipε:K→[0,∞)is well-defined and satisfies consistency, continuity, weak independence and monotonicity.I1εalso satisfies super-additivity.See Section 4 in [34].  □Theorem 5.2For anyp∈N>0∪{∞},Ipε:K→[0,∞)is well-defined and satisfies consistency, continuity, i-independence and monotonicity.I1εalso satisfies super-additivity and IC-separability; andI∞εsatisfies normalization.For well-definedness, consistency, i-independence, monotonicity, super-additivity and IC-separability, see the proof of Theorem 5.11. For continuity, see Lemma 5.12.For normalization, we note thatI∞ε(Γ)=ISSKsum(Γ)for anyΓ∈K, by Theorem 5.7. When we are computingISSKsum, we limit the sum of the absolute values of the stakes to one. As the agent cannot lose more than the absolute value of the stake in each gamble in a Dutch book, and they sum up to one,ISSKsum=I∞εsatisfy normalization.  □Theorem 5.11ISSKa,sum,ISSKb,sum,ISSKa,maxandISSKb,maxare well-defined and satisfy consistency, i-independence and monotonicity.ISSKa,maxandISSKb,maxalso satisfy super-additivity and IC-separability.LetΓ={(φi|ψi)[q_i,q¯i]|1≤i≤m}be an arbitrary knowledge base inKandγ_i,γ¯ibe non-negative real parameters, for1≤i≤m. Consider the following program, withp∈N>0∪{∞}, whereε_γ(ε¯γ)is a (m×1)-vector whose elements areε_iγ_i(ε¯iγ¯i), for1≤i≤m:(A.1)min⁡‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖psubject to:(A.2)min⁡‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖pAπ≥−ε_γ(A.3)min⁡‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖pBπ≤ε¯γ(A.4)min⁡‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖p∑π=1(A.5)min⁡‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖pπ,ε_1,ε¯1,…,ε_m,ε¯m≥0.Define the inconsistency measureIpγ:K→[0,∞)∪∞in such a way thatIpγ(Γ)is the minimum of the objective function of the program above; or ∞ if it is infeasible. Ifγ_i=γ¯i=1for all1≤i≤m,Ipγ(Γ)=Ipε(Γ), for anyp∈N>0∪{∞}.Whenp=1, ifγ_i=q_iandγ¯i=1−q¯ifor1≤i≤m, the program above is the dual of that formed by adding the constraintsq_iλ_i≤1and(1−q¯i)λ¯i≤1, for1≤i≤m, into the program (19)–(21). That is,I1γ(Γ)=ISSKa,max(Γ). Analogously, ifγ_i=1−q_iandγ¯i=q¯ifor1≤i≤m,I1γ(Γ)=ISSKg,max(Γ).Whenp=∞, in all restrictions we can replaceε_i,ε¯iby a single scalar ε, as it was done in (14)–(18), creating an equivalent new linear programPthat minimizes ε, computingI∞γ(Γ). Ifγ_i=q_iandγ¯i=1−q¯ifor1≤i≤m, the programPis the dual of that formed by adding the constraints∑i=1mq_iλ_i+(1−q¯i)λ¯i≤1into (19)–(21). That is, P computesI∞γ(Γ)=ISSKa,sum(Γ). Analogously, ifγ_i=1−q_iandδ¯i=q¯ifor1≤i≤m, P computesI∞γ(Γ)=ISSKg,sum(Γ).Note that the linear restrictions in the program (A.1)–(A.5), when it is feasible, define a convex, closed region of feasible points (a simplex). The p-norm is a continuous function, so the minimum of the objective function in (A.1) is well-defined for anyp∈N>0∪{∞}. If the program (A.1)–(A.5) is infeasible for someΓ∈K,Ipγ(Γ)is (well-)defined as ∞.Consistency: Note that a p-norm is never negative. The base Γ is consistent iff the program (5)–(8) is feasible; and such program is feasible iff the program (A.1)–(A.5) has a feasible solution with〈ε_1,ε¯1,…,ε_m,ε¯m〉=〈0,0,…,0〉; which is the case iff‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖p=0is the minimum of the objective function in (A.1).Monotonicity: Consider the programPfrom lines (A.1)–(A.5), corresponding to the computation ofIpγ(Γ), for someΓ∈K. LetΨ=Γ∪{α}be a knowledge base. For anyp∈N>0∪{∞}and parametersγ_1,γ¯1,…,γ_m,γ¯m≥0, the program (A.1)–(A.5) whose solution givesIpγ(Ψ)has two extra constraints in comparison withP. Thus, the program that computesIpγ(Ψ)cannot reach a smaller value for‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖p, the objective function being minimized byP. Furthermore,‖〈ε_1,ε¯1,…,ε_m+1,ε¯m+1〉‖p≥‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖p, for anyp∈N>0∪{∞}and parametersγ_1,γ¯1,…,γ_m,γ¯m≥0. Hence,Ipγ(Γ∪{α})≥Ipγ(Γ), for anyp∈N>0∪{∞}.i-independence: LetΓ={(φi|ψi)[q_i,q¯i]|1≤i≤m}be a knowledge base inKandα=(φm|ψm)[q_m,q¯m]be an innocuous conditional in Γ, and defineΨ=Γ∖{α}. SupposeIpγ(Ψ)is finite. The solution on〈ε_1,ε¯1,…,ε_m−1,ε¯m−1〉to the program (A.1)–(A.5) that computesIpγ(Ψ)corresponds to a consolidation of Ψ given byΨ′={(φi|ψi)[q_i−γ_iε_i,q¯i+γ¯iε¯i]|1≤i≤m−1}. For α is innocuous in Γ, it is consistent withΨ′∪(φm|ψm)[0,1](a consolidation of Γ) andΨ′∪{α}is a consolidation of Γ. Hence,〈ε_1,ε¯1,…,ε_m−1,ε¯m−1,0,0〉corresponds to a feasible solution to the program (A.1)–(A.5) computingIpγ(Γ). As‖〈ε_1,ε¯1,…,ε_m−1,ε¯m−1〉‖pis equal to‖〈ε_1,ε¯1,…,ε_m−1,ε¯m−1,0,0〉‖pfor anyp∈N>0∪{∞},Ipγ(Γ)≤Ipγ(Ψ). By monotonicity,Ipγ(Γ)=Ipγ(Ψ).Now supposeIpγ(Ψ)is infinite. Thus, the program (A.1)–(A.5) that computesIpγ(Ψ)is infeasible. Constraints in such program are inherited by the program that computesIpγ(Γ)=Ipγ(Ψ∪{α})together with the infeasibility, henceIpγ(Γ)=∞by definition.Super-additivity: Suppose there are basesΨ,Δ,Γ=Ψ∪ΔinKsuch thatΨ∩Δ=∅. Without loss of generality, letΨ={(φi|ψi)[q_i,q¯i]|1≤i≤k},Δ={(φi|ψi)[q_i,q¯i]|k+1≤i≤m}andΓ={(φi|ψi)[q_i,q¯i]|1≤i≤m}. IfI1γ(Γ)=∞, super-additivity trivially holds, then considerI1γ(Γ)is finite. Let〈ε_1,ε¯1,…,ε_m,ε¯m〉be part of a solution (that includes π) to the program (A.1)–(A.5) that computesI1γ(Γ), minimizing the objective function. AsΓ′={(φi|ψi)[q_i−ε_iγ_i,q¯i+ε¯iγ¯i]|1≤i≤m}is consistent, so areΨ′={(φi|ψi)[q_i−ε_iγ_i,q¯i+ε¯iγ¯i]|1≤i≤k}andΔ′={(φi|ψi)[q_i−ε_iγ_i,q¯i+ε¯iγ¯i]|k+1≤i≤m}, which are consolidations of Ψ and Δ. Thus,〈ε_1,ε¯1,…,ε_k,ε¯k〉and〈ε_k+1,ε¯k+1,…,ε_m,ε¯m〉correspond to feasible solutions to the programs that computeI1γ(Ψ)andI1γ(Δ), respectively. It follows thatI1γ(Ψ)≤‖〈ε_1,ε¯1,…,ε_k,ε¯k〉‖1andI1γ(Δ)≤‖〈ε_k+1,ε¯k+1,…,ε_m,ε¯m〉‖1. Finally,I1γ(Δ)+I1γ(Ψ)≤(∑i=1kε_i+ε¯i)+(∑i=k+1mε_i+ε¯i)=∑i=1mε_i+ε¯i=I1γ(Γ).IC-separability: To prove that IC-separability holds, suppose there are basesΨ,Δ,Γ=Ψ∪ΔinKsuch thatΨ∩Δ=∅,IC(Γ)=IC(Ψ)∪IC(Δ). Without loss of generality, letΨ={(φi|ψi)[q_i,q¯i]|1≤i≤k},Δ={(φi|ψi)[q_i,q¯i]|k+1≤i≤m}andΓ={(φi|ψi)[q_i,q¯i]|1≤i≤m}. IfI1γ(Ψ)=∞orI1γ(Δ)=∞, thenI1γ(Γ)=∞by monotonicity, and IC-separability holds, considering that ∞ plus any non-negative number yields ∞; thus, we assumeI1γ(Ψ),I1γ(Δ)<∞. Let〈ε_1,ε¯1,…,ε_k,ε¯k〉and〈ε_k+1,ε¯k+1,…,ε_m,ε¯m〉be solutions (onε_,ε¯) to the programs in the form (A.1)–(A.5) that computeI1γ(Ψ)andI1γ(Δ), respectively, minimizing their objective functions. As all inescapable conflicts of Γ are either in Ψ or in Δ, the union of consolidations of Ψ and Δ is a consolidation of Γ, by Corollary 4.14. Hence,〈ε_1,ε¯1,…,ε_m,ε¯m〉correspond to a feasible solution to the program in the form (A.1)–(A.5) that computesI1γ(Γ)andI1γ(Γ)≤‖〈ε_1,ε¯1,…,ε_m,ε¯m〉‖1=(∑i=1kε_i+ε¯i)+(∑i=k+1mε_i+ε¯i)=I1γ(Ψ)+I1γ(Δ). By super-additivity,I1γ(Γ)=I1γ(Ψ)+I1γ(Δ).  □Lemma 5.12ISSKa,sum,ISSKa,max,ISSKb,sumandISSKb,maxare continuous for probabilities within(0,1).Consider the inconsistency measureIpγdefined in the proof of Theorem 5.11, the knowledge baseΓ={(φi|ψi)[q_i′,q¯i′]|1≤i≤m}and the vectorq=〈q_1,q¯1,…,q_m,q¯m〉. Note that, for any measureIpε, the parametersγ_1,γ¯1,…,γ_m,γ¯mare positive (forISSKa,sum,ISSKa,max,ISSKb,sumandISSKb,max, they are positive ifq∈(0,1)2m). When these parameters are positive, every probability massπ:WXn→[0,1]defines a vectorεπ(q)=〈ε_1,ε¯1,…,ε_m,ε¯m〉for each q in the following way:ε_i=−min⁡{0,(1/γ_i)(Pπ(φi∧ψi)−q_iPπ(ψi))}andε¯i=max⁡{0,(1/γ¯i)(Pπ(φi∧ψi)−q_iPπ(ψi))}for every1≤i≤mandq∈[0,1]2m(orq∈(0,1)2m). Note that the pairπ,επ(q)is a feasible solution to the program (A.1)–(A.5) that computesIpγ(ΛΓ(q))for anyq∈[0,1]2m(orq∈(0,1)2m), sincePπ(φi∧ψi)−q_iPπ(ψi)≥−ε_iγ_iandPπ(φi∧ψi)−q¯iPπ(ψi)≤ε¯iγ¯ifor all1≤i≤m. Thus, every π yields a value for the objective functionhπ(q)=‖επ(q)‖of the program (A.1)–(A.5), for anyq∈[0,1]2m(orq∈(0,1)2m). Asγ_1=γ¯1=⋯=γ_m=γ¯m=1forIpεand anyq∈[0,1]2m,επ(q)is continuous onq∈[0,1]2m, and as any p-norm is a continuous function,hπ:[0,1]2m→[0,∞)also is for any π. (ForISSKa,sum,ISSKa,max,ISSKb,sumandISSKb,max,q∈(0,1)2mimplies positive parametersγ_i,γ¯i; furthermore, such parameters change continuously — linearly — withq∈(0,1)2m. Thus,hπ:(0,1)2m→[0,∞)is continuous onq∈(0,1)2m). To computeIpγ(ΛΓ(q))for a particular q, one needs to take the minimum in π of{hπ(q)|π:WXn→[0,1]is a probability mass}. As the minimum of continuous functions is continuous,Ipγ∘ΛΓ:[0,1]2m→[0,∞)∪{∞}(orIpγ∘ΛΓ:(0,1)2m→[0,∞)∪{∞}) is continuous for anyp∈N>0∪{∞}.  □ISSKa,sumsatisfy normalization.When we are computingISSKa,sum, the maximum sure loss when limiting the agent's total escrows to one. As the agent cannot lose more her total escrow in a Dutch book,ISSKa,sumis trivially normalized.  □