@&#MAIN-TITLE@&#
Design space exploration using Self-Organizing Map based adaptive sampling

@&#HIGHLIGHTS@&#
It can sample from the input region that satisfies certain objective value.It can sample from non-convex and multiple discrete regions.It does not require “positive” and “negative” samples from the beginning.Good scaling property to higher dimensional problems.

@&#KEYPHRASES@&#
Optimization,Simulated-annealing,Self-Organizing Map,Density learning,Feasible region,

@&#ABSTRACT@&#
In engineering design, a set of potentially competitive designs is conceived in the early part of the design process. The purpose of this research is to help such a process by investigating algorithm that enables approximate identification of a set of inputs of real variables that return desired responses from a function or a computer simulation. We explore sequential or adaptive sampling methods based on Self-Organizing Maps (SOM). The proposed method does not rely on parameterized distributions, and can sample from multi-modal and non-convex distributions. Furthermore, the proposed merit function provides infill characteristics by favoring sampling points that lay farther from existing points.The results indicate that multiple feasible solutions can be efficiently obtained by the new adaptive sampling algorithm. The iterative use of the SOM in density learning to identify feasible or good designs is our new contribution and it shows a very rapid increase in number of feasible solutions to total number of function evaluation ratio. Application examples to planing hull designs (such as in powerboats and seaplanes) indicate the merits of the feasible region approach to observe trends and design rules. Additionally, the well distributed sampling points of the proposed method played favorable effect in improving the prediction performance of a classification problem learned by Support Vector Machine.

@&#INTRODUCTION@&#
In today's engineering, it is common practice to use computer simulations to understand the behavior of complex systems and optimize their parameters to obtain satisfactory designs before building actual physical prototypes. The goal of an engineer is not only to optimize the systems but also to understand what makes a design good. Engineers may also need to deal with simulation models that may not represent all the effects necessary to make a right decision. The question – particularly in the early stage of the design process – is often not about finding the best parameter values, but establishing what parameter values would generate a satisfactory design. At a more pragmatic simulation level, engineers often confine the simulation runs to parameter settings for which results are trustworthy [1]. For example, a simulation may crash or its solution may not converge. Such information may not be available until the simulation is running. Furthermore, there are widespread incentives to reduce the number of simulation runs since high fidelity simulations often require a lot of time and computational resources as evidenced in the research of surrogate model based optimization [2–4] and multifidelity optimization [5,6] methods. Our research is motivated by situations in which efficient identification of diverse designs satisfying minimum specifications and understanding about the problem are more important than finding an accurate single optimal solution.We propose a new adaptive sampling algorithm that uses a Self-Organizing Map (SOM) [7,8] to perform importance sampling: Self-Organizing Map Based Adaptive Sampling (SOMBAS). The fundamental idea is an algorithm that learns to select new samples in the region of interest, using the density learning mechanism in SOM. It is similar to Monte Carlo Optimization methods such as Probability Collectives [9] and Cross-Entropy Methods [10] or Subset Optimization methods [11,12]. However, we do not use parameterized probability density functions to represent solutions. Instead, SOM is used to obtain a set of solutions as represented by the weight vectors in each cell of the map. SOM represents a densely sampled region as a larger area on the map than a sparsely sampled region. Therefore, a weight vector can be considered as being analogous to an instance of a random vector drawn from a probability density distribution. Furthermore, these vectors are mutated to improve diversity. The training set can be iteratively enriched with, or replaced by, new samples that exhibit desirable responses (or objective values). SOM is retrained in each iteration with the updated training set. To enhance uniform sampling in the region of interest, a new merit function is also proposed. The flowchart of SOMBAS is shown in Fig. 1.The idea of using the weight vectors as candidate solutions can also be seen in Liukkonen et al. [13]. They train SOM from a set of experiments and look for a best candidate solution from the SOM weight vector. The chosen weight vectors are taken as representing an average solution in their respective neighborhoods of good solutions. However, their method does not entail further sampling and does not have the density learning notion. Our new method substantially extends the idea by making the search process adaptive (i.e. iterates to further explore good solutions).A preliminary version of SOMBAS was presented in Ito et al. [14]. Current work investigates the scalability of SOMBAS to high-dimensional problems and application to a conceptual design example giving extra insights of the parameters on the results.SOMBAS does not entail any modification of SOM just as in Kita et al. [15]. Therefore, different implementations of SOM or other density learning algorithms can be used instead. It focuses on interesting regions of the input space by modifying the sample densities. While Kita et al. use SOM to do clustering (niching), we use SOM to generate new samples according to the density of its training samples. Their paper shows its advantage in multimodal functions and relative weakness in non-separable unimodal functions. Our algorithm, on the other hand, shows no such tendency.Couckuyt et al. [16] and Gorissen et al. [17] have proposed a sequential sampling approach that samples uniformly from the region of interest specified by upper and lower bounds on the output. They extended the Efficient Global Optimization (EGO) [18] and used prediction variances to determine new sampling points that would likely produce an output in the desired range and away from existing samples. The fundamental difference between their work and this paper is that we do not fit interpolating surrogate models that require optimization of the surrogate model hyperparameters. In SOMBAS, no optimality on SOM training is imposed and a user can specify the number of training iterations. Our novelty is in the application of SOM in adaptive sampling scheme. This enables us to sample from distributions without the need to parameterize them. Furthermore, SOM is scalable to high-dimensional input space.Emmerich et al. [19] and Ulrich and Thiele [20] have proposed algorithms with identical objectives as SOMBAS. They propose diversity measures in their evolutionary algorithms and explicitly optimize for this measures. However, their methods involve multidimensional integrations or matrix inversions that would make the algorithms difficult to apply in high-dimensional problems. In SOMBAS, diversity is kept by a simple merit function that takes the distance to the nearest-neighbor into account and a mutation algorithm.We use Self-Organizing Maps’ (SOM) weight vectors as a representation of a sample distribution. Typically, SOM is represented as a two-dimensional array of cells (be it hexagonal or square shaped). Each of these cells has a weight vector associated with it. In this work, the weight vector is a set of continuous design variables that represents an instance of a possible new solution. We initially assign random numbers to the vector elements. Then, the weight vectors are learned from a given set of training samples. The trained weight vectors can be considered to be a finite sample representation of the training sample distribution. The weight vectors wjare updated using the following equation for a given training sample t.(1)wj(k+1)=wj(k)+hcj(k)[t(k)−wj(k)],where j is a spatial index that identifies the cells in SOM, k is the training iteration index, hcjis a neighborhood function that depends on the distance between wcand wjon the map where wcis the closest weight vector to the training sample t(k) in the Euclidean sense. The neighborhood function decreases as the distance between the cells becomes far apart on the map. Thus, given a training sample and the closest matching weight vector, the farther cells on the map receive less influence of the weight update. The shape and magnitude of hcj(k) are changed as k increases in such a way that the second term (the weight update term) on the right-hand side of Eq. (1) reduces the radius and magnitude of influence.Algorithm 1 shows a high-level description of the Self-Organizing Map Based Adaptive Sampling. In each iteration, the trained Self-Organizing Map (SOM) produces new solution candidates and their corresponding objective values. Weight vector selection is based on these estimated values. Perturbations are applied to these selected vectors, and their objective values are computed, replacing the estimated values. Updating of the training set is performed and a subset of these selected samples are included in the training set to train the SOM of next iteration.Algorithm 1SOM BASED ADAPTIVE SAMPLING1:Generate N samples to create initial training set2:while Termination condition not met do3:Train SOM using the normalized training set4:for all cells satisfying SELECTION CONDITION do5:Perturb the weight vectors of the selected cells according to MUTATION6:end for7:Un-normalize the perturbed samples8:Evaluate true output of the perturbed and unperturbed samples9:UPDATE TRAINING SET10:end whileThe probability of a weight vector being selected depends on how close its objective value estimate is to the known smallest value. Note that the objective values in the weight vectors of SOM are estimates. The selection condition is(2)r<expymin−yˆT,where 0≤r<1 is a random number drawn from a uniform distribution, ymin is the smallest output in the training sample, andyˆis the estimated objective value from the weight vector. The temperature 0.01≤T≤10 defines how selective the condition is and a smaller value of T results in fewer new samples added to the training data set. The pseudocode of this selection condition is given in Algorithm 2.Algorithm 2SELECTION CONDITION1:Let ymin be the smallest output in the training set X,yˆbe output from a cell weight vector, and T be the selectivity parameter (or Temperature)2:Generate a uniform random number 0≤r<1 and check the following:3:ifr<expymin−yˆTthen4:Corresponding weight vector is selected5:end ifWe consider a case in which we seek to minimize an objective value y below certain threshold L. Below this threshold, diversity of solutions is sought. In this paper, we will call such a search as feasible region identification or feasible region search. One idea is to use a merit function similar to those described in Torczon and Trosset [21]. One could give a better chance of being selected to points (i.e. cell weight vector) that are distant from existing training samples regardless of y value. To achieve this, we propose the following formula for the merit function.(3)F=max(L,y)−ρmin(∥s−ti∥2),i=1,2,…,Nwhere s is the input vector for which F needs to be minimized, tiis a set of target samples from which minimum distance to the input vector s is calculated, N is the number of such target vectors, and ρ is a weight constant. Unlike Torczon's merit function, our merit function incorporates a “truncation” value L below which only the separation from other target vectors timatters. To minimize this merit function, one needs y<L and maximize the distance to the nearest target vector min(∥s−ti∥2). In our case, target vectors are the training set and the input vector s is the selected weight vector from SOM. The algorithm to replace the output with this merit function is described in Algorithm 3. Ifyˆis greater than the threshold L, bothyˆand the new weight vector's distance from the training set are taken into account. Ifyˆis less than L, then the distance to the nearest training vector is the only term affecting the objective value and smaller F is obtained when the weight vector's distance to the nearest neighbor is larger. The ρ in Eq. (3) is a positive weighing constantAlgorithm 3MERIT FUNCTION1:Let L denote the value below which objective or output y is considered to be “good enough”, s denote a weight vector(xT,yˆ)from SOM, and ti=1,2,…,Ndenote the training samples2:if Trunc is specified then3:Normalize L (s, and tiare already normalized)4:yˆ←max(L,yˆ)−ρmin(s−ti2)5:Normalizeyˆ6:end if7:Use thisyˆin SELECTION CONDITIONMutation, as described in Algorithm 4, is applied to the selected weight vectors. We use the weight vectors as the centers of multivariate Gaussian distributions. The covariance matrix is obtained from the selected weight vectors. We use an idea similar to CMA-ES [22] to update the covariance matrices. The covariance matrix in the current iteration is combined with the covariance matrix computed in the previous iteration: 0.2C+0.8Cold. This is to avoid adapting too quickly to a local minimum. On top of that, we multiply a factor which is different whether the previous iteration produced a new minimum or not. If the previous iteration achieved a new minimum, we apply an expansion factor Fe, to which we assign a real value larger than 1. On the other hand, if the previous iteration did not produce a new minimum, we multiply a contraction factor Fc, to which we assign a real value between 0 and 1. The covariance matrix is the same for all the selected weight vectors. Each weight vector is perturbed by sampling from the multivariate Gaussian distribution. Mutation is very important to avoid premature convergence in SOMBAS.Algorithm 4MUTATION1:Let Fcbe contraction factor and Febe expansion factor2:Let Pmbe mutation probability3:Given training samples, compute covariance matrix C, and let the covariance matrix from previous iteration be Cold4:if current ymin < previous yminthen5:C=Fe(0.2C+0.8Cold)6:else7:C=Fc(0.2C+0.8Cold)8:end if9:For each selected sample, perturb it by sampling from multivariate normal distribution with center at the selected sample with covariance C.10:Replace a parameter in the selected vectors with the mutated one at probability of PmAfter the perturbation of new samples, the training set is updated. Algorithms 5 and 6 are two such methods. Algorithm 5 has a faster convergence but is more prone to lose diversity in the training set prematurely compared to Algorithm 6. In the latter method, if max(L, y) of the new perturbed sample and that of the randomly selected training sample are the same, the replacement of the selected training sample takes place only if the new perturbed sample has a larger distance to its nearest neighbor than the distance of the training sample to its nearest neighbor. Otherwise, the new perturbed sample replaces the training sample when the new sample has a smaller objective value. The nearest neighbors are searched among all the sampled points. In the next section, we will use Algorithm 6.Algorithm 5UPDATE TRAINING SET 11:Add the perturbed samples to the training set2:if Training set sample size larger than maximum sample size then3:Sort the training set with respect to output value4:Remove the worst samples to make the training sample size equal to maximum sample size5:end ifUPDATE TRAINING SET 21:for all perturbed weight vectors’ response ypdo2:Randomly pick one of the training samples, and obtain its response yt3:Obtain dp, the nearest neighbor distance of perturbed sample to sampled points thus far and dtthe nearest neighbor distance of the training sample to sampled points thus far4:if max(L, yp)=max(L, yt) and dp>dtthen5:Replace the training sample with the perturbed weight vector6:else ifyp<ytthen7:Replace the training sample with the perturbed weight vector8:end if9:end for

@&#CONCLUSIONS@&#
SOMBAS is able to select samples in the design space below a given threshold value and, in addition, it is able to do so in a space-filling way. Our approach to feasible region identification is different from binary classification methods in Machine Learning. Classification methods require both positive and negative samples from the outset of the learning iteration. SOMBAS, on the other hand, will search for feasible regions, even if all of the initial training samples were unfeasible.SOMBAS’ efficient acquisition of feasible solutions in higher dimensions, namely for the 30 and 100-dimensional Rastrigin function and Rosenbrock function, was shown to be superior to DE. It can be argued that feasible region identification becomes identical to optimization when the feasible region becomes infinitesimally small. For example, we could set f≤10−6 as the feasible region in the 30-dimensional Rastrigin function. In this case, DE would be a better choice. Further research is needed to understand the relationship between accurately finding an optimum point and efficiently identifying a feasible region.In the engineering example, we identified input values that generate satisfactory designs. By looking at multiple solutions, it enabled the extraction of design knowledge regarding how the design parameters interact under certain stability criteria. This is a significant advantage with respect to standard optimization techniques.In the application SOMBAS in Machine Learning example, in which Support Vector Machine was used to learn a binary classification model from the sampled data, the accuracy of prediction improved as the number of samples increased and the number of feasible samples for a given function evaluation budget was substantially higher than the random sampling. On the other hand, DE achieved a steady increase in the proportion of number feasible samples (feasible rate Ns/Nf) while the accuracy of prediction (F1 score) stagnated as the number of data increased. It would be beneficial to investigate the merit of applying SOMBAS to different Machine Learning tasks and methods.