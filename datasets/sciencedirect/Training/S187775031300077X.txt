@&#MAIN-TITLE@&#
Open-source tools for dynamical analysis of Liley's mean-field cortex model

@&#HIGHLIGHTS@&#
A parallel open-source implementation of Liley's mean-field cortex model, in PETSc.We implement fully implicit time integration of nonlinear and variational equations.We perform equilibrium continuation, with computation of inhomogeneous eigenmodes.We compute periodic solutions with Newton–Krylov iteration.

@&#KEYPHRASES@&#
Mean-field modelling,Hyperbolic partial differential equations,Numerical partial differential equations,35Q92,65Y05,

@&#ABSTRACT@&#
Mean-field models of the mammalian cortex treat this part of the brain as a two-dimensional excitable medium. The electrical potentials, generated by the excitatory and inhibitory neuron populations, are described by nonlinear, coupled, partial differential equations that are known to generate complicated spatio-temporal behaviour. We focus on the model by Liley et al. (Network: Computation in Neural Systems 13 (2002) 67–113). Several reductions of this model have been studied in detail, but a direct analysis of its spatio-temporal dynamics has, to the best of our knowledge, never been attempted before. Here, we describe the implementation of implicit time-stepping of the model and the tangent linear model, and solving for equilibria and time-periodic solutions, using the open-source library PETSc. By using domain decomposition for parallelization, and iterative solving of linear problems, the code is capable of parsing some dynamics of a macroscopic slice of cortical tissue with a sub-millimetre resolution.

@&#INTRODUCTION@&#
Models of cortical dynamics come in two main families: neuronal network models and mean-field models. The former describe many interacting neurons, each with their own dynamical rules, while the latter describe electrical potentials, generated collectively by many neurons, as continuous in space and time. These potentials can be thought of as averages over a number of macrocolumns, groups of hundreds of thousands of neurons in columnar structures at the surface of the cortex. Both of these modelling approaches can be classified as forward: they attempt to predict the future state of the cortex, given the current state and a set of physiological parameter values. A complementary approach, that can be called backward, is to divide the cortex into interacting components that can be regarded as functional units, and compute the strength of coupling between these units. The backward approach is often based on functional Magnetic Resonance Imaging (fMRI) experiments. A promising modelling strategy is to describe each functional component with a neuronal network or mean-field model, and then have them interact according to empirically determined coupling, thus combining the forward and backward approaches [1].When considering the forward modelling of a macroscopic piece of cortical tissue, a fundamental difference between the neuronal network and mean-field families is apparent. A model of the former kind should describe billions of neurons, and many times more connections between them. As demonstrated by recent publications, such as by Izhikevich and Edelman [2] or by the Blue Brain team [3], progress in super computing allows for the simulation of ever larger neuronal networks, that reflect actual brain dynamics. However, it is hard to see how the output of such models can be analyzed, other than by purely statistical techniques. Models of the latter kind, in contrast, can be analyzed as smooth, infinite-dimensional dynamical systems.An added advantage of the mean-field approach is that the electrical potentials, which appear as dependent variables, are observable, macroscopic quantities. An indirect measurement of these fields is provided by the electroencephalograph (EEG) [4]. The EEG is usually measured with electrodes on the scalp or, in exceptional circumstances, directly on the surface of the brain. In either case, the measured signal is not that of individual neurons, but that of many neurons, spread out over a few square centimetres or millimetres. Thus, the way the signals of individual neurons are smeared out by the spatial averaging of mean-field modelling is similar to the way they are mixed up in EEG measurements. Because of the link between the local mean potential and the EEG, mean-field models are sometimes called EEG models (e.g. [5,6]). The geometry of the cortical surface, however, is not taken into consideration in doing so. This surface is folded, and electrocortical activity will result in different EEG signals depending on the location and orientation of the generating tissue. A more direct link between the model variables and measurements may be given by the Local Field Potential (LFP), which is typically measured in vitro or under anaesthesia.The origin of mean-field modelling lies in the 1970s, when pioneers like Freeman [7], Wilson and Cowan [8] and Lopes da Silva et al. [9] started to model components of the human cortex with continuous fields. Over the past four decades, mean-field models have been used to study a range of open questions in neuroscience, such as the generation of the alpha rhythm, 8–13Hz oscillations in the EEG (see, e.g., [9,5]), epilepsy (see, e.g., [10–12]) and anaesthesia [6]. In a different context, they are used in models for sensorimotor control, pattern discrimination and target tracking [13]. As discussed above, mean-field models also appear as components of combined forward-backward models that aim to capture the functioning of the cortex as a whole, such as in Honey et al. [14].Although mean-field models have been used in all these settings, little analysis has been done on their behaviour as spatially extended dynamical systems. In part, this is due to their staggering complexity. The Liley model [15] considered here, for instance, consists of fourteen coupled Partial Differential Equations (PDEs) with strong nonlinearities, imposed by coupling between the mean membrane potentials and the mean synaptic inputs. The model can be reduced to a system of Ordinary Differential Equations (ODEs) by considering only spatially homogeneous solutions, and the resulting system has been examined in detail using numerical bifurcation analysis (see [16] and references therein). In order to compute equilibria, periodic orbits and such objects for the PDE model, we need a flexible, stable simulation code for the model and its linearization that can run in parallel to scale up to a domain size of about 2500cm2, the size of a full-grown human cortex. We also need efficient, iterative solvers for linear problems with large, sparse matrices. In this paper, we will show that all this can be accomplished in the open-source software package PETSc [17]. Our implementation consists of a number of functions in C that are available publicly [18].The goal of this computational work is to parse the spatio-temporal dynamics of a full-fledged mean-field model. We will present the numerical implementation of algorithms for the computation of equilibria and time-periodic solutions and study their stability and parameter dependence. Thus, our goal is similar to that of Coombes et al., who analyzed “spots”: rotationally symmetric, localized solutions in a model of a single neuron population in two dimensions [19]. The challenge lies in giving up the restriction to a single population, a single space dimension, or solutions with a fixed spatial symmetry.The model we use was first proposed by Liley et al. [15]. The dependent variables are the mean inhibitory and excitatory membrane potential, hiand he, the four mean synaptic inputs, originating from either population and connecting to either, Iee, Iei, Iieand Iii, and the excitatory axonal activity in long-range fibres, connecting to either population, ϕeeand ϕei. The model equations are:(1)τk∂hk(x→,t)∂t=hkr−hk(x→,t)+hekeq−hk(x→,t)hekeq−hkrIek(x→,t)+hikeq−hk(x→,t)hikeq−hkrIik(x→,t)(2)∂∂t+γek2Iek(x→,t)=eΓekγek{NekβSe[he(x→,t)]+pek+ϕek(x→,t)}(3)∂∂t+γik2Iik(x→,t)=eΓikγik{NikβSi[hi(x→,t)]+pik}(4)∂∂t+vΛ2−32v2∇2ϕek(x→,t)=Nekαv2Λ2Se[he(x→,t)](5)Sk[hk]=Skmax1+exp−2hk−μkσk−1where index k={e, i} denotes excitatory or inhibitory. The meaning of the parameters, along with some physiological bounds and the values used in our tests, are given in Table 1. A detailed description of these equations can be found in Refs. [15,16]. Here, we will focus on the aspects of the model most relevant for the numerical implementation.There are two sources of nonlinearity, related to the coupling of the synaptic inputs to the membrane potential and vice versa. The former connection is quadratically nonlinear, while the latter is given by the sigmoidal function Sk, which describes the onset of firing as the potential exceeds the threshold value μk. These nonlinearities tend to form sharp transitions of the potentials across the domain. That is one reason why we opted for a finite-difference discretization over a pseudo-spectral approach. Spectral accuracy would be of limited value in the presence of steep gradients and the finite-difference scheme can be parallelized much more efficiently. The second reason is that we would like to be able to change the geometry of the domain and the boundary conditions in future work. The finite-difference scheme is more flexible in that respect.The only spatial derivatives in the model are those in the equations for the long-range connections. These are damped wave equations. We will discretize the Laplacian using a five-point stencil on a rectangular grid. In previous work, Bojak and Liley chose a second-order centred difference scheme for the time derivatives [6]. A disadvantage of this approach is that the stability condition of this scheme dictates that we set the time step inversely proportional to the grid spacing. In practice, they used a time step of 0.05ms. To avoid this obstacle, we want to use implicit timestepping, and have currently implemented the unconditionally stable implicit Euler method, as described in Section 3.Following earlier work on this model (e.g. [6,20]), we adopt periodic boundary conditions in both dimensions. This is a common choice in the study of mean-field models, and is partially justified by the observation, that each part of the cortex is connected to each other part. A discussion of this argument can be found in chapter 11 of Nunez and Srinivasan [4]. A result of this choice is that the model PDEs will be equivariant under translations and reflections. This equivariance has consequences for the behaviour of the model. An in-depth discussion of these consequences is beyond the scope of the current paper, but in Section 6 we will decribe how to compute periodic solutions for the equivariant system.Other authors have used this model with an additional diffusive term in the equations for the membrane potentials to model gap junctions [21]. Inclusion of these terms can drastically change the bifurcation behaviour, as they can cause Turing transitions to space-dependent equilibria. Without the additional terms, a Hopf bifurcation from a spatially homogeneous equilibrium to a space dependent periodic orbit or a saddle-node bifurcation of the equilibrium often appear to be the primary instability. The gap junction terms can readily be included in our implementation, and in Section 5 we will describe how to solve for equilibrium states that may depend on space.We will test our implementation by comparing to, and extending, the computations of oscillations with a 40Hz component by Bojak and Liley [20]. Oscillations with this frequency are called gamma oscillations, and have been hypothesized to aid in the communication between groups of neurons [22]. Both simulations and experiments indicate that gamma oscillations occur in subjects performing cognitive tasks (see, e.g. [23] and references therein). Gamma band activity was found in the Liley model despite the fact that it was in no way tuned or formulated to produce this behaviour.The parameter values for this experiment are listed in Table 1. The 40Hz oscillations arise spontaneously if the number of local inhibitory-to-inhibitory connections is changed slightly. We introduce a scaling parameter r by replacingNiiβ→rNiiβ. This is the only parameter that will be varied in our tests.Rather than creating our code from scratch, we opted to work with the Portable, Extensible Toolkit for Scientific Computation (PETSc): an open-source, object oriented library that is designed for the scalable solution and analysis of PDEs [24,17]. PETSc is written in the C language, and is usable from C/C++ as well as Fortran and Python. We use PETSc in conjunction with the Scalable Library for Eigenvalue Problem Computations (SLEPc) [25], for the computation of eigenspectra of equilibrium and periodic solutions. Since our implementation uses some features of PETSc that are recent additions and are still being modified, we use the development version of both projects.PETSc is split up into multiple components to address the various problems associated with solving PDEs numerically. For our purposes, we treat the DM component, which handles the topology of the discretization, as the most fundamental, from which we can easily derive memory allocation and communication for distributed vectors (Vec) and matrices (Mat). With vectors and matrices, we can now solve linear systems, such as those that arise in Newton iteration for implicit time-stepping and the computation of equilibria and periodic orbits. PETSc's component for this is called KSP, and it has numerous iterative solvers implemented, as well as preconditioners, (PC), to increase convergence rates. For implicit time-stepping, for example, we use GMRES, preconditioned with incomplete LU (ILU) factorization, combined with the block Jacobi method [26,27]. On top of the linear solvers come the nonlinear solvers, PETSc's SNES component, which implements a few different methods, such as globally convergent Newton iteration with line search [28]. Finally, PETSc provides a timestepping component, TS, to obtain time dependent solutions. Implemented here are numerous explicit and implicit schemes such as adaptive stepsize Runge–Kutta and implicit Euler. The implicit schemes make use of the SNES component. A schematic of the hierarchy discussed here can be found in Fig. 1.For our dynamical systems calculations we will frequently need to compute specific eigenvalues and eigenvectors for system-sized matrices. For this end, we use SLEPc, which implements iterative eigenvalue solvers using PETSc Vec and Mat distributed data structures. The component of SLEPc that we use is EPS, which has a few algorithms for iteratively solving eigenproblems. Its default algorithm is Krylov–Schur iteration.Following earlier work by Bojak and Liley (e.g. [6,20]) we consider the PDEs on a rectangular domain with periodic boundary conditions. On this domain, we use a rectangular grid of Nxby Nypoints. In the tests presented in Section 7, the domain and the grid are square. PETSc allows for more complicated domain shapes and grids, that can be encoded in the DM component, independent of the higher-level components. This choice of periodic boundary conditions is one of computational convenience. As we have no formulation of boundary conditions that come from the physiology, we adopt periodic boundaries and look at phenomena that are on length scales below that of the system size. A brief demonstration of this can be seen in Fig. 2and its caption.Within DM, PETSc provides a simpler subcomponent, DMDA, for working with finite differences on structured grids such as our rectangle. If we specify a stencil to use for the spatial derivatives in the DMDA, PETSc will automatically handle numerous things for parallel execution, such as memory allocation and the communication setup for distributed vectors and for the distributed Jacobian matrix.To make use of PETSc's solvers, the model must be written as a system of equations that is first order in time. This we achieve by introducing new states Jjkand ψekaccording to(6)∂Ijk∂t=Jjk−γjkIjk(7)∂Jjk∂t=eΓjkγjk{NjkβSj[hj]+ϕjk+pjk}−γjkJjk(8)∂ϕek∂t=ψek−v2Λ2ϕek(9)∂ψek∂t=v2Λ2NekαSe[he]+32v2∇2ϕek−v2Λ2ψek,with indices j, k={e, i}.We opted to use a struct, seen in Code 2.1, to store the fields, rather than a triply indexed array.Code 2.1Struct for the fields.typedef struct _Field{PetscReal h_e, h_i,I_ee, J_ee,I_ie, J_ie,I_ei, J_ei,I_ii, J_ii,phi_ee, psi_ee,phi_ei, psi_ei;} Field;All of the model parameters are stored in a struct designated as the application context. The application context is how PETSc gets problem related parameters into the user-defined functions needed by its solvers.Code 2.2Application context struct with the model parameters.typedef struct _AppCtx{PassiveReal hr_e, hr_i,tau_e, tau_i,heq_ee, heq_ie,heq_ei, heq_ii,Gamma_ee, Gamma_ie,Gamma_ei, Gamma_ii,gamma_ee, gamma_ie,gamma_ei, gamma_ii,Nalpha_ee, Nalpha_ei,Nbeta_ee, Nbeta_ie,Nbeta_ei, Nbeta_ii,v, Lambda,Smax_e, Smax_i,mu_e, mu_i,sigma_e, sigma_i,p_ee, p_ei,p_ie, p_ii;…} AppCtx;Similar to the fields, this allows readable code for the parameters. For example, one accesses the Γieparameter as user->Gamma_ie, if user is defined as the pointer AppCtx *user;. How the parameters show up in our struct for the application context is shown in Code 2.2.In addition to the structs listed above, we need to provide PETSc with (at least) a C function that computes the vector field for a given state. We call this function FormFunction, and from this PETSc is capable of approximating the Jacobian with various finite difference methods. However, we also supply a C function to explicitly compute the Jacobian, named FormJacobian, because this allows for more efficient calculations, especially when looking at stepping the variational equations in Section 4.We currently use the implicit Euler method to time-step the discretized equations. As mentioned in Section 1.1, this allows us to take larger time steps than feasible with explicit methods. Since we are aiming to compute periodic orbits, rather than to generate long time series, the first order accuracy of the method is not an issue. Once a periodic orbit is computed, the time-step size can be reduced to increase accuracy. Another option is to use Richardson extrapolation to increase the order of accuracy, using the same nonlinear solving as described below.We symbolically write the dynamical system as(10)u˙=f(u),f:ℝN→ℝNwhere N is the total number of unknowns after discretization, in our case 14×Nx×Ny. The implicit Euler scheme for time integration is given by(11)un+1=un+dtf(un+1)where the subscript represents the step number, dt the step size, and u0 the initial conditions. This nonlinear equation is solved by Newton iteration:(12)un+1k+1=un+1k+duk,where the superscript denotes the Newton iterate, and dukis the solution to the linear system(13)I−dt∂f∂uun+1kduk=dtf(un+1k)−un+1k+unk,where ∂f/∂u denotes the N×N Jacobian matrix. Provided that the initial approximation,un+10, is close enough to the actual solution of Eq. (11), this iteration should converge quadratically. This is achieved by making the initial approximation the result of an explicit Euler step(14)un+10=un+dtf(un).As we scale up the size of our problems, it becomes the linear solve in Eq. (13) that takes most of the time. This problem is handled by using GMRES to solve the linear system. For large time steps, the spectrum of the matrix in Eq. (13) is spread out, and we need to precondition it for iterative solving. We make use ILU, which has shown to be reliable for this type of problem [29,30]. If we use more than one processor, PETSc uses distributed storage for the matrix, and combines ILU with block Jacobi preconditioning.

@&#CONCLUSIONS@&#
