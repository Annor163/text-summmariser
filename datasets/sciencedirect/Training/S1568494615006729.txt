@&#MAIN-TITLE@&#
Extracting classification rules from modified fuzzy min–max neural network for data with mixed attributes

@&#HIGHLIGHTS@&#
Proposed method modifies the fuzzy min–max neural network to handle both continuous and discrete attributes effectively.It prunes the trained network to reduce the number of hyperboxes created.Rules are extracted from the pruned network which justifies its classification results.

@&#KEYPHRASES@&#
Fuzzy min–max neural network,Classification,Hyperbox,Continuous attributes,Discrete attributes,Rule extraction,

@&#ABSTRACT@&#
This paper proposes the modified fuzzy min–max neural network (MFMMN) classification model to perform the supervised classification of data. The basic fuzzy min–max neural network (FMMN) can only be applied to the continuous attribute values and cannot handle the discrete values. Also justification of the classification results given by FMMN required to be obtained to make it more applicable to real world applications. These both issues are solved in the proposed MFMMN. In the MFMMN, each hyperbox have min–max values defined in terms of continuous attributes and a set of binary strings defined for discrete attributes. Bitwise ‘and’ and ‘or’ operators are used to update the discrete values associated with each hyperbox. The trained network is pruned to remove the less useful hyperboxes based on their confidence factor. The proposed model is applied to nine different datasets taken from the University of California, Irvine (UCI) machine learning repository. Finally the case study of a real time weather data is evaluated using MFMMN. The experimental results show that the proposed model has given very good accuracy. In addition to accuracy, the number of hyperboxes obtained after pruning are very less which lead to less number of concise rules and reduced computational complexity.

@&#INTRODUCTION@&#
Classification of data is one of the most frequent decision making tasks performed by human. A classification problem occurs when an object needs to be assigned to a predefined class based on attributes values related to that object [1]. Artificial neural network (ANN) is one of the most commonly used classifier technique [2]. The reason for being commonly used is that they have the universal approximation property, they can learn from examples and also have capability of generalization beyond the training data [3–5]. But the problem with traditional neural network techniques is that they take more training time because of scanning the input data for many times [6]. This problem motivates the use of FMMN proposed by Simpson [7,8] where input data need not be scanned many times. The design of FMMN focuses on following properties:•Online adaptation: It should be able to learn new classes and refine the old ones. Also for the new incoming information, trained network need not be retrained for both old and new information thus reducing the huge demand on memory requirements and training time.Nonlinear separability: It should be able to build decision regions that separate classes of different shapes and sizes.Overlapping classes: It should be able to form a decision boundary that reduces the amount of misclassification for the patterns falling in the overlapping region.Training time: It should be able to learn in a short time.Soft and hard decisions: It should be able to provide both hard and soft decisions.Verification and validation: It should be able to provide a mechanism to verify and validate its performance.Tuning parameters: The number of tuning parameters should be as few as possible.Nonparametric classification: It should not depend on a priori knowledge of the input patterns, as this information is not available many times.Gabrys et al. have extended this basic version of FMMN for a number of improvements like generalized task of both classification and clustering, adaptive value of maximum hyperbox size, etc. [9]. This FMMN is useful for variety applications including image processing, speech processing, etc. [10,11]. In [12], fuzzy hyper-line segment neural network (FHLSNN) is proposed and its performance is found superior than the FMMN algorithm. In [13], fuzzy hypersphere neural network is presented wherein hypersphere instead of hyperbox is used to represent the fuzzy subset in the n-dimensional pattern space. Also the FMMN is extended by many researchers to remove its loopholes in the expansion–contraction process [14–16].In [17], an agglomerative learning scheme for FMMN is proposed and has shown the robust behaviour in the presence of noise or outliers and it is insensitive to the order of training patterns presentation. In [18], FMMN is optimized to avoid the over fitting problem. A fuzzy min–max neural network classifier with compensatory neuron architecture given in [19] is a novel reflex mechanism inspired from human brain is used to solve the problem of class overlaps.Data-core-based fuzzy min–max neural network (DCFMN) given in [20] is based on the effect of data core and noise. In [21], a multi-level fuzzy min–max (MLF) classifier is proposed which employs smaller hyperboxes in subsequent levels to handle the overlapping area problem. In [22,23], new learning algorithms for fuzzy min–max neural classifier based on adaptive resolution classifier technique are proposed. In machine learning, support vector machines (SVMs) are supervised learning models that are widely used for classification and regression analysis [24]. Although SVMs have good generalization performance, they cannot process discrete data and have the limitation of speed and size [25].The drawback of all these neural networks is that they can handle only continuous data. But many real world applications have the data of both continuous and discrete type in nature. So the fuzzy min–max neural network should be extended to handle both continuous and discrete data. In [26], the method that extends the FMMN input to discrete variables by introducing the distance between categories of a discrete variable is given. For this the relation between the discrete variable and classification variable is considered.In addition to the drawback of processing only continuous attributes, the basic FMMN and its extended versions lack explanation capability and considered as black boxes [27]. The justification of the results given by FMMN is not available. It motivates the need of rule extraction from FMMN that provides the justification of the results. One of such methods is given in [28] and another in [29] that extracts the fuzzy if – then rules from trained FMMN. In [30], genetic algorithm based rule extractor is proposed. But both of these methods can process only the continuous attributes and cannot operate on the data having discrete attributes. So rules extracted from these methods are expressed only in terms of continuous attributes.The proposed MFMMN can process both continuous and discrete data and it also extracts the rules which will justify the classification decision given by the network. These rules are expressed in terms of both continuous and discrete attributes. For this the membership function and expansion condition given in [9] are modified to process the discrete attributes. After training, the pruning based on the confidence factor of hyperboxes is done to improve accuracy and to reduce the number of hyperboxes. Also pruning gives the less number of efficient rules associated only with the hyperboxes of higher confidence factors. There are number of pruning algorithms available in the literature which tells the importance of pruning but they focus only on traditional ANN [31–33].Proposed MFMMN extracts the high quality rules measured in terms of the following [34]:•Rule accuracy: The extent to which the rule set is able to classify a set of previously unseen examples from the problem domain correctly.Rule fidelity: The extent to which the rule set mimics the behaviour of the ANN from which it was extracted.Rule consistency: The extent to which, during different training sessions, the ANN generates rule sets which produce the same classifications of testing examples.Rule comprehensibility: The number of rules in the rule set and the number of antecedents per rule.The organization of the paper is as follows. Section 2 describes the fuzzy min–max neural network. Section 3 gives the proposed MFMMN model. Experimental results and discussions are given in Section 4. Finally, the concluding remarks are given in Section 5.Following subsections describe the general fuzzy min–max neural network used for classification proposed by Gabrys et al. [9].The fuzzy min–max classification neural network builds the decision boundaries by creating fuzzy subsets of the n-dimensional pattern space as given in Fig. 1. Each of this fuzzy subset is called the hyperbox which is characterized by a pair of min–max points and the membership function. The 3-dimensional hyperbox structure with its min–max points is shown in Fig. 2.Mathematically each hyperbox Bjis defined by(1)Bj={Xh,Vj,Wj,f(Xh,Vj,Wj)}where Xh=(x1, x2, …, xn) is the hth input pattern, Vj=(vj1,vj2,…,vjn)is the min point, Wj=(wj1,wj2,…,wjn)is the max point and f(Xh, Vj, Wj) is the membership function. The membership function f(Xh, Vj, Wj) for hyperbox Bjis represented as bj(Xh) and its value is in the range 0≤bj(Xh)≤1, measures the degree to which Xhfalls outside of hyperbox Bj. Its value is always one when pattern Xhfalls inside or on boundaries of Bjand decreases as the pattern goes away from that hyperbox. Due to the fact that the fuzzy membership function proposed by Simpson and used in FMMN algorithms can assign a relatively high membership value to an input pattern which is quite far from the class prototype, Gabrys et al. have proposed new membership function that is defined as [9](2)bj(Xh)=mini=1,…,n(min([1−f(xhiu−wji,γi)],[1−f(vji−xhil,γi)]))wheref(r,γ)=1ifrγ>1rγif0≤rγ≤10ifrγ<0whereXh=[Xhl,Xhu]is the hth input pattern in a form of lower,Xhl, and upper,Xhu, limits vectors contained within the n-dimensional unit cube,Vj=(vj1,vj2,…,vjn)is the min point for Bj,Wj=(wj1,wj2,…,wjn)is the max point for Bj, and γiis the sensitivity parameter that controls how fast the membership values decreases as the distance between Xhand Bjincreases.The architecture of FMMN consists of three layers of nodes, as shown in Fig. 3. This architecture grows adaptively to meet the demands of the problem. It consists of an input layer (FA), hyperbox layer (FB) and the output layer (FC). The input layer has 2*n-input nodes, two for each of the n-dimensions of input pattern. The output layer contains nodes equal in number to the number of classes. The hyperbox layer which is also called the hyperbox layer wherein, each node represents a hyperbox fuzzy set. All FAto FBconnections are the min–max points. The FBlayer transfer function is the hyperbox membership function defined in (2). The min and max points are stored in matrices V and W, respectively. The min point matrix V is applied to the first n-input nodes representing the vector of lower bounds of the input pattern and the max point matrix W is applied to the other n-input nodes representing the vector of upper bounds of the input pattern. The connections between the FBand FCnodes are binary-valued and are stored in matrix U. The equation for assigning the values from FBto FCconnections is(3)uji=1IfBjis a hyperbox of classci0otherwisewhere Bjis the jth node in the hyperbox layer with j=(1, 2, …, m) and ciis the ith node in the output layer with i=(1, 2, …, k). The output of the FCnode represents the degree to which input pattern Xhfits for class ci. The transfer function of each FCnode performs the fuzzy union operation on the appropriate hyperbox fuzzy set values which is defined as(4)ci=maxj=1mbjujiwhere bjis the membership value of Xhto the jth hyperbox and ujiis the binary value as defined in (3). Eq. (4) gives the membership value of pattern Xhto each of the ith class. This is the soft classification decision given by FMMN and can be used wherever required. If hard decision is required then the maximum value among all values of ciis set to unity.Learning in FMMN creates the collection of hyperboxes. Prior to learning only input and output layer exists and the hyperbox layer is constructed during learning by adding hyperboxes one by one. When each input pattern Xhis presented, it is checked for the possible expansion of the existing hyperbox of the same class as that of pattern Xh. If expansion is not possible then new hyperbox is created. This continues till all patterns are finished. So the learning process consists of four steps: initialization, expansion, overlaps test and contraction. These are summarized as below:1Initialization: Before training starts Vjand Wjare initialized with(5)Vj=0andWj=0When jth hyperbox Bjis adjusted for the first time using the input pattern, the min and max points of this hyperbox would be(6)Vj=XhlandWj=XhuExpansion: In this step the membership values of input pattern Xhto all the hyperboxes of same class as that of Xhare calculated. The hyperbox with the highest membership value, Bj, is selected and tested for possible expansion. The expansion test is given by(7)∀i=1,…,n(max(wji,xhiu)−min(vji,xhil))≤θwhere 0≤θ≤1 is the user defined threshold value that determines the maximum size of the hyperbox. The larger value of θ gives the small number of hyperboxes and small value gives a large number of hyperboxes.If the expansion condition for hyperbox Bjis satisfied then it is expanded by adjusting min and max points in each dimension i by using the equations(8)vjinew=min(vjiold,xhil)∀i=1,2,…,n,(9)wjinew=max(wjiold,xhiu)∀i=1,2,…,n.If neither of the existing hyperboxes can expand to include the input pattern, then a new hyperbox is created.Overlap test: After expansion of the hyperbox Bj, its overlap is tested with each different class hyperbox Bk. For every dimension i, if any one of the following four case is satisfied then overlap exists between two hyperboxes. With δold=1 initially, the four overlap test cases and the corresponding minimum overlap value for the ith dimension are as follows.(10)Case1:vji<vki<wji<wki,δnew=min(wji−vki,δold).(11)Case2:vki<vji<wki<wji,δnew=min(wki−vji,δold).(12)Case3:vji<vki<wki<wji,δnew=min(min(wki−vji,wji−vki),δold).(13)Case4:vki<vji<wji<wki,δnew=min(min(wji−vki,wki−vji),δold).If δold−δnew>0, then Δ=i and δold=δnew. If not, the testing stops and the minimum overlap index variable is set to indicate that the next contraction step is not necessary, i.e. Δ=−1.Contraction: If overlap between the hyperboxes exists and Δ be the selected dimension for contraction, then by using the minimal disturbance principle these hyperboxes are adjusted by the following four cases corresponding to the overlap occurred.(14)Case1:vjΔ<vkΔ<wjΔ<wkΔ,wjΔnew=vkΔnew=wjΔold+vkΔold2.(15)Case2:vkΔ<vjΔ<wkΔ<wjΔ,wkΔnew=vjΔnew=wkΔold+vjΔold2.(16)Case3a:vjΔ<vkΔ<wkΔ<wjΔand(wkΔ−vjΔ)<(wjΔ−vkΔ),vjΔnew=wkΔold.(17)Case3b:vjΔ<vkΔ<wkΔ<wjΔand(wkΔ−vjΔ)>(wjΔ−vkΔ),wjΔnew=vkΔold.(18)Case4a:vkΔ<vjΔ<wjΔ<wkΔand(wkΔ−vjΔ)<(wjΔ−vkΔ),wkΔnew=vjΔold.(19)Case4b:vkΔ<vjΔ<wjΔ<wkΔand(wkΔ−vjΔ)>(wjΔ−vkΔ),vkΔnew=wjΔold.In contrast to the earlier fuzzy min–max neural networks, the proposed MFMMN have the following additional properties:1Unlike FMMN, the MFMMN can process discrete attributes. For this purpose the membership function, architecture and the learning algorithm are modified.Earlier fuzzy min–max neural networks test the overlap only after the input pattern has expanded the existing hyperbox and not when it has created the hyperbox inside the different class hyperbox. However MFMMN tests the overlap even after creation of new hyperbox by the input pattern. The overlap and contraction test cases are modified to detect and remove such overlap.In MFMMN, the justification of its classification decision in terms of rules can be obtained. So the drawback of being black box is removed and thus it can be used in a wide range of applications including sensitive applications.The extracted rule conditions are formed by using linguistic terms for the continuous attributes and by the values of the discrete attributes. Due to this these rules are more interpretable.In order to extract more efficient and compact rule set, MFMMN is pruned which also helps to filter the noise in the data.In MFMMN, very less number of efficient hyperboxes are left after pruning which gives rise to better computational complexity.In earlier fuzzy min–max neural networks, even after contraction hyperboxes are overlapping on edges. But this is not the case with MFMMN where after contraction, hyperboxes are not overlapping.Thus MFMMN can process both continuous and discrete attributes effectively and also gives the justification of its classification decision in terms of the rules. For this, MFMMN is first trained using modified membership function and then pruned to reduce the number of hyperboxes created. After pruning, MFMMN is ready for classification of test data followed by the extraction of rules which justifies the classification results given for the test data. Following subsections give the details of the proposed MFMMN including its input, architecture, learning, pruning and rule extraction.MFMMN can process the input data comprising of both continuous as well as discrete attributes. Also it can be applied to the dataset with either of the continuous or discrete attributes only. If the input comprises of either the continuous or discrete attributes then the effect of others is nullified by initializing the discrete attribute strings to 1s or continuous attribute values to 0s.In MFMMN each input pattern is represented as an ordered pair(20)[X,C]where X is the input pattern comprising of both continuous and discrete attributes and C∈{c1, c2, …, ck} is the class label of X.Input pattern X is represented as(21)X=[Xh,Dh]where Xhand Dhare the set of continuous and discrete attributes respectively.Let X is given by(22)X={xh1l,xh2l,…,xhncl,xh1u,xh2u,…,xhncu,dh1,dh2,…,dhnd}where nc and nd are the total number of continuous and discrete attributes respectively.Xhis given by(23)Xh={xh1l,xh2l,…,xhncl,xh1u,xh2u,…,xhncu}.Here Xhcan be represented as(24)Xh=[Xhl,Xhu]whereXhlandXhuare the upper and lower limits vectors contained within n-dimensional unit cube Inand are given as(25)Xhl={xh1l,xh2l,…,xhncl},(26)Xhu={xh1u,xh2u,…,xhncu},and Dhis given by(27)Dh={dh1,dh2,…,dhnd}.The architecture of MFMMN consists of three layers: input layer, hyperbox layer and output layer. Input layer accepts the input, that is processed during the training and hyperboxes are added in the hyperbox layer which is empty initially and constructed during the training. Output layer contains the class nodes to which input pattern is to be classified. In the proposed MFMMN as given in Fig. 4, in addition to the min–max points and membership function, each hyperbox Bjis specified by set of binary strings Dj=(dj1, dj2, …, djnd) where each binary string djiin the set is for ith discrete attribute. Each binary string djiis formed by using 1-of-m coding technique [35]. If the domain of ith discrete attribute consists of m values then m-bit binary string is used and for each value of that discrete attribute the corresponding bit position is 1 and remaining bit positions are 0s.Thus each hyperbox Bj, as given in Fig. 5is defined by(28)Bj={X,Vj,Wj,Dj,f(X,Vj,Wj,Dj)},where X is the input pattern,Vj=(vj1,vj2,…,vjnc)andWj=(wj1,wj2,…,wjnc)are the min–max points of jth hyperbox respectively, Dj=(dj1, dj2, …, djnd) is the set of binary strings, one for each discrete attribute and f(X, Vj, Wj, Dj) is the membership function.Membership function for Bjis given as(29)bj(X)=12(mini=1,…,ncmin1−fxhiu−wji,γi,1−fvji−xhil,γi+1nd∑i=1ndg(dhi,dji))wheref(r,γ)=1ifrγ>1rγif0≤rγ≤10ifrγ<0andg(dhi,dji)=1if bitand(dhi,dji)>00if bitand(dhi,dji)=0where γ=[γ1, γ2, …, γnc] are the sensitivity parameters regulating how fast the membership values decrease, bitand(dhi, dji) is the logical bitwise ‘and’ operation performed on two binary strings dhiand dji. The dhiis the value of the ith discrete attribute of input X and djiis the value of ith discrete attribute associated with hyperbox Bj. This membership function has two parts, one for continuous attributes and other for discrete attributes and the average of these is taken as a membership value of X to Bj. On presentation of each input pattern X, bitwise ‘and’ operation is performed on ith discrete attribute string of input pattern, dhi, and ith discrete attribute string, dji, of set Dj, associated with hyperbox Bj. Its value is 1 if the pattern lies within the hyperbox and its all discrete attributes values are matching with the hyperbox discrete values. This membership value goes on decreasing with the increase violations of continuous as well as discrete attributes.Learning in MFMMN takes place in four steps:1Initialization: Before training starts Vj, Wjand Djare initialized as(30)Vj=0,Wj=0andDj=ϕ.When jth hyperbox, Bj, is adjusted for the first time using the input pattern,X=[Xhl,Xhu,Dh], the min–max points and set Djof this hyperbox would be(31)Vj=Xhl,Wj=XhuandDj=Dh.Creation/expansion of the hyperboxes: The membership values of X to all the hyperboxes of the same class as that of X are calculated by using the modified membership function as given in (29). These hyperboxes are arranged in descending order of their membership values. After this, the following steps are carried sequentially for possible inclusion of the input pattern X.Step 1: Determine whether the pattern X is contained by any one of the hyperboxes. This can be verified by using fuzzy hyperbox membership function defined in (29), i.e. if(32)bj(X)=1then the input pattern X is included in hyperbox Bj. Therefore in the training process all the remaining steps are skipped and training is continued with the next training pattern.Step 2: If the pattern X falls outside the hyperbox Bj, then the hyperbox is expanded to include the pattern provided the expansion criterion is satisfied. For this the hyperbox with the highest membership value, Bj, is selected and tested against continuous as well as discrete attributes for possible expansion. So for the proposed classification model two expansion conditions are defined one for continuous and other for discrete attributes.Expansion condition for continuous attributes is(33)∀i=1,…,nc(max(wji,xhiu)−min(vji−xhil))≤θ,and expansion condition for discrete attributes is(34)∑i=1ndg(dhi,dji)≥β,where g(dhi, dji) is as defined in (29) and β is the user defined threshold of minimum number of discrete attributes of input X that must agree for inclusion of X into Bj.If the expansion conditions for both continuous and discrete attributes are satisfied for hyperbox Bj, min–max points and each string in the set Djof Bjare adjusted using the equations(35)vjinew=min(vjiold,xhil)∀i=1,…,nc,(36)wjinew=max(wjiold,xhiu)∀i=1,…,nc,(37)djinew=bitor(djiold,dhi)∀i=1,…,nd.Step 3: If the pattern X is not included by any one of the above steps, then a new hyperbox is created for that pattern. The min–max points and set Djof this hyperbox would beVj=Xhl,Wj=XhuandDj=Dh.Overlap test: All the earlier versions of FMMN have the drawback that they can only eliminate the overlap created due to the expansion as specified in step 2 but cannot eliminate the overlap created due to hyperbox creation specified in step 3. But the proposed MFMMN is capable to test and eliminate these both type of overlaps. Also unlike the earlier versions of FMMN, overlap test cases given below does not allow the hyperboxes of different classes to overlap even on boundaries. This ensures that no pattern even falling on boundaries will have the full membership for different class hyperboxes.After expansion or creation of the hyperbox Bj, its overlap is tested with each different class hyperbox Bk. For every dimension i, if any one of the following four cases is satisfied then the overlap exists between two hyperboxes. By assuming δold=1 initially, the four test cases and the corresponding minimum overlap value for the ith dimension are as follows:(38)Case1:vji<vki≤wji<wki,δnew=min(wji−vki,δold).(39)Case2:vki<vji≤wki<wji,δnew=min(wki−vji,δold).(40)Case3:vji≤vki≤wki≤wji,δnew=min(min(wki−vji,wji−vki),δold).(41)Case4:vki≤vji≤wji≤wki,δnew=min(min(wji−vki,wki−vji),δold).If δold−δnew>0, then Δ=i and δold=δnew. If not, the testing stops and the minimum overlap index variable is set to indicate that the next contraction step is not necessary, i.e. Δ=−1. These overlaps are eliminated using hyperbox contraction as described in the next subsection.Contraction: In the earlier versions of FMMN, contraction removes the overlap but after the contraction, overlap on boundaries still exists. Due to this, patterns falling on overlapping boundaries of different class hyperboxes have the full membership for both the hyperboxes. To avoid this, the contraction step of the proposed MFMMN is modified which ensures that after contraction of overlapping hyperboxes, minimum distance of α=0.001 is maintained between the boundaries of hyperboxes.If overlap between the hyperboxes exists and Δ be the selected dimension for contraction, then by using the minimal disturbance principle these hyperboxes are adjusted by the following four modified cases corresponding to the overlap occurred between the overlapping hyperboxes Bjand Bkare given below.(42)Case1:vjΔ<vkΔ≤wjΔ<wkΔ,vkΔnew=wjΔold+vkΔold2+α2,wjΔnew=wjΔold+vkΔold2−α2.(43)Case2:vkΔ<vjΔ≤wkΔ<wjΔ,vjΔnew=wkΔold+vjΔold2+α2wkΔnew=wkΔold+vjΔold2−α2.(44)Case3a:vjΔ<vkΔ≤wkΔ<wjΔand(wkΔ−vjΔ)<(wjΔ−vkΔ),vjΔnew=wkΔold+α2.(45)Case3b:vjΔ<vkΔ≤wkΔ<wjΔand(wkΔ−vjΔ)>(wjΔ−vkΔ),wjΔnew=vkΔold−α2.(46)Case4a:vkΔ≤vjΔ≤wjΔ≤wkΔand(wkΔ−vjΔ)<(wjΔ−vkΔ),wkΔnew=vjΔold−α2.(47)Case4b:vkΔ≤vjΔ≤wjΔ≤wkΔand(wkΔ−vjΔ)>(wjΔ−vkΔ),vkΔnew=wjΔold+α2.In Fig. 6, each of the possible overlap and its contraction is illustrated. Fig. 7illustrates the formation of hyperboxes and their adjustment to remove the overlap. This is illustrated with a example of five four-dimensional patterns out of which two dimensions are continuous and other two are discrete.An effective pruning algorithm is a crucial component of any neural network rule extraction algorithm [36]. By removing the inputs that are not needed for solving the problem, the extracted rule set will be more concise. In addition, pruned network also serves to filter the noise that might be present in the data. In the proposed classification model the trained network has been pruned to remove the less useful hyperboxes based on their confidence factor to improve the overall system performance. The confidence factor identifies good hyperboxes that are frequently used and generally accurate, as well as that are rarely used but extremely accurate. The confidence factor for each hyperbox node is defined in terms of its usage frequency and its predictive accuracy on the training set [35] is given by(48)CFj=(1−α)Uj+αAjwhere Uj∈[0, 1] is the usage of jth hyperbox, Aj∈[0, 1] is its accuracy, and α∈[0, 1] is a weighting factor. The value of Ujis defined as the number of patterns in the training set classified by any hyperbox j, divided by the maximum number of patterns in the training set classified by any hyperbox with the same classification class [28,36] and is given by(49)Uj=Cj/max{Cf:hyperboxfpredicts classk}where Cjis the number of patterns classified by hyperbox j for class ck, Cfis the number of patterns classified any hyperbox f for class ck. On the other hand, the value of Ajis defined as the number of correctly classified patterns by any hyperbox j, divided by the maximum correctly classified patterns with the same classification class [28,36] and is given by(50)Aj=Pj/max{Pf:hyperboxfcorrectly predicts classk}where Pjis the number of patterns correctly classified by hyperbox j for class k, Pfis the number of patterns correctly classified by any hyperbox f for class k. The hyperboxes with a confidence factor less than or equal to some user defined threshold are pruned. Also the confidence factor of each hyperbox is tagged with the if–then rule formed for that hyperbox. The confidence factor associated with each rule is the combined measure of the accuracy and usage of that rule. The rules with confidence factor 1 are highly accurate and most useful.The rule extraction techniques are grouped in three categories based on the granularity of the underlying ANN: pedagogical, decompositional and eclectic [31,34]. The pedagogical group of techniques are of the maximum granularity and considers the ANN as a black box and extracts global relationships between the inputs and the outputs of the ANN. Decompositional techniques view the ANN at its finest level of granularity i.e., at the level of the individual hyperbox and output units and extracts the rules from these units. These subsets of rules are then aggregated to form global relationships. But not all rule extraction techniques map neatly into the pedagogical or decompositional category. Therefore new category called eclectic was devised to accommodate essentially hybrid techniques which analyze the ANN at the individual unit level but which extract rules at the global level.The proposed classification model aims to extract the rules from the MFMMN using eclectic category where each hyperbox layer node is analyzed and the rules stating the global relationships between input and output of the MFMMN are extracted. As the MFMMN is trained using continuous and discrete attributes, the rule antecedents are formed by using the conditions for both continuous and discrete attributes. For each hyperbox Bj, the min–max values are used to form the rule conditions of continuous attributes and set of discrete attribute strings, Dj, obtained during training is used to form the rule conditions of discrete attributes. The rule conditions of all attributes are connected by logical ‘and’ operator to form an antecedent of if–then rules. Rule conditions of continuous attributes are formed by quantization of the min and max values as given in [35]. Due to quantization the rules can be described in words rather than real numbers and such rules are more readable. A quantization level Q is defined as the number of feature values in the quantized rules.For example, with Q=3, feature values are described as low, medium or high in the fuzzy rules. Quantization by round-off distributes, Q quantization points evenly, with one at each end point given by(51)Vq=q−1Q−1where q=1, 2, …, Q. This method then rounds off min–max values to the nearest Vqvalue.Rule conditions for discrete attributes are formed for each string in the set Djof hyperbox Bj. As 1-of-m coding is used for the discrete attributes, presence of 1's at different bit position in the string representing a discrete attribute diϵDj, for i=1, 2, …, nd, means presence of that respective value. By connecting all these values with logical ‘or’ and all discrete attributes by logical ‘and’ these rule conditions can be combined with rule conditions of continuous attributes.For example, for hyperbox Bjof class k, Dj={d1, d2, d3}={110, 101, 010}.For simplicity, assume that all discrete attribute have three possible values-high, medium and low and suppose first bit position is for high, second for medium and third for low then rule condition using only discrete attributes will be:If d1=high or medium and d2=high or low and d3=medium then class=k.The effectiveness of the MFMMN is evaluated by using nine different benchmark datasets. For the experimentation with MFMMN, input dataset is partitioned into training set and testing set. The training set provides input for hyperboxes creation and provides an estimate of confidence factor for pruning based on usage and accuracy of each hyperbox. The testing set then measures the performance of the pruned network. In MFMMN, thresh, threshold for confidence factor based on which the hyperboxes are pruned and is the controlling parameter that considers both continuous and discrete values. Due to this instead of conducting the experiments for different θ values, as is the case for FMMN and GFMMN, MFMMN experiments are conducted for different thresh values.Weather dataset has 14 instances with 4 attributes out of which 2 are continuous and 2 are discrete as given in Table 1(a) Continuous attributes are normalized into the range of 0–1 and discrete attributes are coded by using 1-of-m coding as given in Table 1(b). Table 1(c) gives the result of MFMMN with different values of thresh ranging from 0 to 1, where PA is the percentage accuracy, H is the number of hyperboxes left after pruning. The PA value of 100% is obtained for thresh=0, thresh=0.1, thresh=0.2 and for thresh=0.3 with 11, 6, 6 and 6 hyperboxes left after pruning respectively. The rules are extracted for these 6 hyperboxes with thresh=0.3 and θ=0.2. These min–max values of these 6 hyperboxes are quantized with Q=3 and binary discrete attribute strings are represented using discrete values as in Table 2which can directly be converted into the rules, where column 2 to 5 form the antecedent of the rule and column 6 form the consequent. In addition each rule is tagged with the corresponding confidence factor given in column 7. Table 3lists the if–then rules extracted for the weather dataset by using the proposed MFMMN. Each of these rules represents one hyperbox and it classifies the samples classified by that hyperbox with the same accuracy. So these rules mimic the exact behaviour of the MFMMN and therefore they have good fidelity. Also the smaller size of the rule set makes these rules more comprehensible.To illustrate the robustness of the proposed MFMMN, second experiment is conducted on a credit approval data set [37]. There are altogether 690 samples and 15 attributes, 6 of which are continuous and the rest 9 are discrete. The continuous attributes are labelled as C1, C2, …, C6 and discrete attributes are labelled as D1, D2, …, D9. Each discrete attribute is coded by using 1-of-m coding, where m is the total number possible values for that attribute. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data. This dataset is a good mix of continuous and discrete attributes. It has discrete attributes with a small number of values, and discrete with larger number of values. There are also a few missing values in the dataset. Table 4gives the possible values for all attributes.Experiments are conducted for two different sizes of training and testing data. These are of 60% and 80% as the training data and the rest of 40% and 20% is taken as the test data. Selection of the training data is random in nature. The results are analyzed and compared with the classification results given by the basic FMMN [7] and GFMMN [9]. Both FMMN and GFMMN cannot process discrete values directly so they are converted into equivalent numeric representations and are treated like continuous values. The values of β=4, θ=0.1, and γ=4 are used for this experiment. The value of θ can be increased which gives rise in hyperbox size with the little loss in classification accuracy [7].Tables 5and 6depicts the experimental results with 60% and 80% training data respectively where the accuracy for test data increases with increase in size of train data. As in Table 5, for the size of 60% train–40% test, MFMMN has given the highest accuracy of 84.42% with only 8 hyperboxes. This is very good optimization on number of hyperboxes. As in Table 6, for the size of 80% train–20% test, MFMMN has given the best accuracy of 92.75% as compared to other two methods and this with only 12 hyperboxes. From these two different size experiments it can be noticed that original FMMN performed better than GFMMN and proposed MFMMN for small values of θ, but with a far more number of hyperboxes. However, the accuracy rates of original FMMN became inferior to that of MFMMN when θ ≥ 0.5. This can also be seen from Fig. 8, where the plots of θ/thresh vs. PA and θ/thresh vs. number of hyperboxes are given for each experiment. Due to the less number of accurate hyperboxes left after pruning, extracted rules from these hyperboxes are more comprehensible and accurate. Table 7summarizes the experimental results given in Tables 5 and 6 in terms of highest PA value and number of hyperboxes created.Proposed MFMMN has given the highest test accuracy of 92.75% with 12 hyperboxes for 80% train and 20% test data with thresh=0.5. For each of these 12 hyperboxes, min–max points and discrete attribute strings are expressed to form rule antecedents and consequent is formed by the class associated with that hyperbox. The rules extracted for these 12 hyperboxes are given in Table 8. These rules are given in tabular form and can easily be expressed in if−then structure. Each rule is tagged with the confidence factor of corresponding hyperbox. Rules are given in the decreasing order of their confidence factor. The number of test samples classified by each rule and corresponding error rates are summarized in Table 9and it depicts that there are 50% rules with 0% error and total error is of 7.25%.To test the applicability of MFMMN to the wide range of applications, in addition to the weather and credit dataset, 7 more datasets are used for experimentation. This includes iris, bupa liver disorder, heart disease, wine, thyroid, ionosphere and glass dataset.Total 121-MFMMN experiments are conducted on each of these datasets for θ=0–1 with the step size of 0.1 and for each θ value, thresh is varied from 0 to 1 with the step size of 0.1. Also for comparison, the results of FMMN and GFMMN are obtained for θ=0–1 with the step size of 0.1. These results are in terms of minimum, maximum and average percentage accuracies as recorded in Table 10and the minimum, maximum and average number of hyperboxes as given in Table 11. The average percentage accuracy given by MFMMN is higher for all the datasets except iris dataset. This higher accuracy is obtained with very less number of efficient hyperboxes as compared to other classifiers.In any fuzzy min–max neural network, hyperboxes are created during training and these created hyperboxes are ready to classify the incoming test patterns. The response time needed mainly depends on the number of hyperboxes created. Also the memory space is required for each hyperbox to store the min–max values and associated class value which is in total 2*n+1 where n is number of input dimensions. In MFMMN, the space required per hyperbox will be 2*nc+nd+1, where nc is the number of continuous attributes and nd is the number of discrete attributes. Thus in MFMMN, each discrete attribute is stored only once instead of storing twice as in the form of min–max values. Also as seen in Table 11, number of hyperboxes created in MFMMN is far less than other classifiers. This reduces the memory space requirement considerably.Tables 12and 13depicts the training and recall times for the FMMN, GFMMN and MFMMN. The training times for MFMMN are higher than others because of separate treatment done with discrete attributes in the form of bitwise and and or operations. But once the hyperboxes are created during training, they are ready to classify any new incoming input patterns. Recall time decides the response time and is the crucial requirement in case of real-time application. As compared to other classifiers, MFMMN has very less recall time except in credit and heart dataset.Results given by MFMMN are compared with the results of other classifiers like GFMMN [9], exclusion/inclusion fuzzy classification network (EFC) [16], FMCN [19], DCFMN [20] and MLF [21] and are given in Table 14. The results mentioned in [21] are used for comparison. These results show that MFMMN has given maximum accuracy or close to it with less number of hyperboxes. In addition to this MFMMN also has the additional ability to process discrete attributes and to extract rules.FMMN and its all extended versions including proposed MFMMN have the capability of online adaptation. As specified in Section 1, these classifiers are able to learn new classes and refine the old ones within few passes through the data, by making theta adaptive during the training, provided samples input in real time are labelled, since all are supervised classifiers. Initially, the MFMMN was trained offline, with the rainfall data of eleven years for the Kozhikode district in Kerala state of India consisting of 8030 data samples, to acquire past knowledge to predict the occurrence of rainfall before exposing it to online training. There are total 8 features out of which 2 are discrete and remaining are continuous. These features include different type of parameters like station level pressure, mean sea level pressure, temperature, dew point temperature, relative humidity, wind speed, month and wind direction. This data is divided into nine years data for training and two years data for testing and it has given the percentage accuracy of 100% and 89.16% for training and testing data respectively.Further, to demonstrate the applicability of the MFMMN for real time weather forecasting we have designed the data acquisition system with two separate software simulations for training and recall phases. The designed system contains a switch to toggle between training and recall phases whenever needed. The MFMMN trained offline is then exposed to online adaptation by applying real time samples that were input at the rate of one sample/hour. The input sample acquired was delayed by 1h to keep relation between the cause and effect. This means the samples with the desired output obtained after 1h was input during online adaptation. Thus during training, the MFMMN adapts to forecast rainfall intensity with a lead time of 1h using only the current sample as an input. At any moment one can toggle to recall phase to input the current sample and to forecast rainfall after 1h. Performance of MFMMN is recorded for one week continuously and it has predicted 88.34% of the time correctly. Time required for adapting each online pattern is 0.63s and recall time is of 0.52s.

@&#CONCLUSIONS@&#
