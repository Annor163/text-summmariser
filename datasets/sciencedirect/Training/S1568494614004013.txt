@&#MAIN-TITLE@&#
A multi-objective genetic algorithm with fuzzy c-means for automatic data clustering

@&#HIGHLIGHTS@&#
We deploy a multi-objective GA with the FCM algorithm to assign patterns into different clusters for soft partitioning.The approach is able to solve data clustering when prior knowledge on the number of clusters is not available.The two criteria: Jm and overlap-separation are simultaneously optimized.The overlap-separation measure is applicable to handle the overlapping characteristics of clusters.The algorithm is able to handle well-separated, compact and overlapped clusters.

@&#KEYPHRASES@&#
Clustering,Multiobjective optimization,Fuzzy clustering,Genetic algorithms,

@&#ABSTRACT@&#
This article presents a multi-objective genetic algorithm which considers the problem of data clustering. A given dataset is automatically assigned into a number of groups in appropriate fuzzy partitions through the fuzzy c-means method. This work has tried to exploit the advantage of fuzzy properties which provide capability to handle overlapping clusters. However, most fuzzy methods are based on compactness and/or separation measures which use only centroid information. The calculation from centroid information only may not be sufficient to differentiate the geometric structures of clusters. The overlap-separation measure using an aggregation operation of fuzzy membership degrees is better equipped to handle this drawback. For another key consideration, we need a mechanism to identify appropriate fuzzy clusters without prior knowledge on the number of clusters. From this requirement, an optimization with single criterion may not be feasible for different cluster shapes. A multi-objective genetic algorithm is therefore appropriate to search for fuzzy partitions in this situation. Apart from the overlap-separation measure, the well-known fuzzy Jmindex is also optimized through genetic operations. The algorithm simultaneously optimizes the two criteria to search for optimal clustering solutions. A string of real-coded values is encoded to represent cluster centers. A number of strings with different lengths varied over a range correspond to variable numbers of clusters. These real-coded values are optimized and the Pareto solutions corresponding to a tradeoff between the two objectives are finally produced. As shown in the experiments, the approach provides promising solutions in well-separated, hyperspherical and overlapping clusters from synthetic and real-life data sets. This is demonstrated by the comparison with existing single-objective and multi-objective clustering techniques.

@&#INTRODUCTION@&#
Clustering is an important mechanism in data analysis to define or organize a group of patterns or objects into clusters. The objects in the same cluster share common properties and those in different cluster s have distinct dissimilarity [1,2]. This basic exploratory analysis provides meaningful information for many disciplines such as pattern classification, image segmentation, document retrieval, biology, marketing research, health psychology, etc. Most data in these disciplines are in multi-dimensional space which inherently creates difficulty for humans to capture or perceive information. Therefore, the ultimate goal of data clustering is to achieve unsupervised classification of complex data where there is little or no prior knowledge on those data.Consider a set of input patterns X={x1, x2, …, xM}, where xi=(xi1, xi2, …, xin)T∈Rn. Each xijis defined as a feature, attribute, dimension or variable. The aim is to partition the patterns into disjoint c subsets, C={C1, …, Cc}, such thatCi≠∅,⋃i=1cCi=X, and Ci∩Cj=∅ for i≠j. This partitioning problem can be considered as an NP-hard optimization problem [3].The most widely used algorithms to solve this problem are k-means [4] and fuzzy c-means (FCM) [5]. Both algorithms are centroid-based and require a fixed number of clusters beforehand. k and c denote the number of clusters and may be used interchangeably. The k-means algorithm starts from randomizing k centroids, one for each cluster. Each pattern is assigned into a cluster based on its nearest centroid, and then positions of the centroids are iteratively adjusted according to their corresponding members in order to minimize the sum of the squared error. This clustering method is considered as hard partitioning since k disjoint clusters are obtained. In FCM, soft partitioning is defined since every pattern is considered as a member of every cluster but different degrees of membership are assigned for different clusters. The aim of FCM is to minimize the generalized least-squares objective function, which the degree of membership plays an important role to optimize the data partitioning problem. However, the disadvantages of both algorithms are well known: a correct number of clusters is required beforehand and the algorithms are quite sensitive to centroid initialization [3,6]. In practice, many problems have no knowledge on the number of clusters a priori. Most research papers have proposed solutions for such problems by running an algorithm repeatedly with different fixed values of k and with different initializations [3]. However, this may not be feasible with large data sets and large k. Furthermore, the algorithm running only with a limited number of k may be inefficient or not attractive since a solution depends on a limited set of initializations. This is known as the “number of clusters dependency” problem [7]. In a problem like this, the optimization may get stuck in local minima which the search process identifies, rather than a global optimal solution.From the above reasons, evolutionary algorithms are shown to be alternative optimization methods using stochastic principles to evolve clustering solutions. In other words, they are based on probabilistic rules to search for a near-optimal solution from a global search space. Some evolutionary algorithms to optimize the number of clusters in data partitioning problems have been proposed in [8–17]. The approaches in [8,9] are based on swarm intelligence. In [8], simple and robust Artificial Bee Colony (ABC) algorithm which simulates the intelligent foraging behavior of honey bee swarms has been applied. Another proposed method in [9] has applied a hybrid of fuzzy c-means and PSO also known as FCM-FPSO.In [10–12], genetic algorithms (GAs) have been applied for automatic determination of the number of clusters, where variable-length strings have an important role to tackle the problem of the number of clusters. The proposed method in [10] apply a GA with the split-and-merge operator and the technique in [10] uses the k-means algorithm. In [13], the k-means clustering has been enhanced by a GA, which considers the affect of isolated points. The k-means based on GA has also been applied in [14]. The immune GA and dynamic chromosome coding has been proposed to improve the search and to increase the convergence. FCM is also incorporated in the evolutionary algorithm in [16] and its application for remote sensing imagery in [17]. To optimize the number of clusters in appropriate data partitioning, suitable cluster validities or objective functions must be identified. The aforementioned methods have deployed the objective function based on cluster compactness which is commonly applied in most clustering algorithms. In complex data sets where clusters are partitioned with different sizes or different geometric shapes, optimization with only a single criterion may not be applicable to solve these characteristics simultaneously [7,15,18].Multi-objective evolutionary algorithms (MOEAs) have been shown to deliver promising solutions for such problems with effective search performance over single-objective clustering algorithms [15]. Two or more conflicting (or complementary) objective functions are deployed in the evolutionary process. MOEAs for clustering have been proposed in [15,19]. In [15], the algorithm called MOCK is based on PESA-II with locus based chromosome encoding. It has shown to outperform single-objective clustering algorithms and ensemble techniques. However, it performs well for hyperspherical shaped or well-separated clusters but provides low performance on overlapping clusters [19]. Another disadvantage on the locus based encoding is the length of the string, which increases with the size of the data set. This imposes expensive computation when a large data set is analyzed.VAMOSA, developed in [19], is based on multi-objective simulated annealing with center-based encoding and the newly developed point symmetry based distance from [20]. The approach has been compared with MOCK and other algorithms in artificial and real-life data sets of varying shapes, sizes or convexity. It successfully determines the appropriate number of clusters and provides overall performance better than other algorithms. However, it fails to detect a cluster having non-symmetrical shapes.This article presents a multi-objective evolutionary approach to optimize data clustering with no requirement of the number of clusters. The clustering method considers fuzzy clusters, where data associating with degrees of membership contain more information than hard clusters. Fuzzy clustering is more naturally feasible in many real situations since data elements are not assigned to exactly one class. The discrete nature of hard clustering is not favorable if clusters are overlapped. On the other hand, fuzzy clustering has the potential to overcome this limitation. In our approach, the fuzzy functionality is used to capture the overall cluster structures through the simultaneous optimization of two objectives: the well-known fuzzy Jmindex [5] and the overlap-separation measure [21,22]. This approach called FCM-NSGA applies the non-dominated sorting genetic algorithm-II (NSGA-II) [23] as an underlying multi-objective optimization framework. The fuzzy c-means algorithm is utilized in the allocation of data points to fuzzy clusters. The problem encoding is represented by variable-length strings with real-coded values of cluster centers. This encoding enables searching for a proper number of clusters.In the remainder of this article, FCM-NSGA is described in Section 2 providing details for each part of the algorithm. Section 3 presents evaluation of solutions and how to select the most appropriate solution. Data sets applied in the experiment are detailed in Section 4. Thereafter, Section 5 describes the results of FCM-NSGA compared with those of other approaches. Section6 provides details of time complexity of the algorithm and Section7 concludes the paper.This section presents the overall mechanism of FCM-NSGA starting from a brief background of genetic algorithms (GAs) and NSGA-II. A center-based string encoding suitable for the clustering problem is then presented. The functionality of FCM is incorporated in the approach and two objective functions are deployed in the multi-objective optimization. In the final part, genetic operators responsible for the search process are detailed.Genetic algorithms (GAs) are search and optimization techniques based on the stochastic approach enhanced by the principles of biological evolution in nature [24]. They were invented to mimic natural evolution, using the idea of a chromosome which encodes the genetic information of an individual. The genetic information is decoded to determine the individual's fitness which depends on its interaction with an environment. In simulated evolution, individuals represent encodings of solutions in GAs’ problem space. The chromosome's performance, known as fitness, is then interpreted by decoding its representation according to an objective function (i.e., evaluation function, fitness function) for a particular problem. In the evolutionary process, reproduction, crossover and mutation operators play significant roles to produce better and better individuals until an optimal solution is obtained.A GA has been applied in the non-dominated sorting genetic algorithm (NSGA), which was originally proposed by Srinivas and Deb [23] to solve multi-objective optimization problems. Due to the evaluation by multiple objectives, a vector of decision variables is given, which produces a set of solutions called Pareto-optimal solutions or non-dominated solutions. The main mechanism of NSGA is a non-dominated sorting process to construct a non-dominated front. However, NSGA has high computational complexity with O(MN3), where M is the number of objectives and N is the population size. NSGA-II [25] was therefore developed from the first version to reduce the complexity from O(MN3) to O(MN2). NSGA-II uses a fast non-dominated sorting to search for a non-dominated front and a crowding distance to maintain diversity in population.In our work, NSGA-II has been applied as a framework for multi-objective optimization of appropriate fuzzy clusters. The overall procedure of our algorithm is presented in Fig. 1. In the first generation, the random operator initializes N individuals for an initial population by randomly choosing cluster center points from M data patterns. The initial numbers of clusters are generated from a uniform distribution over the range 2 toMwhich is recommended by [19]. The assignment of membership degree and the update of center values are based on the FCM algorithm further described in Section2.3. The values of fitness are then computed according to the two objectives detailed in Section2.4. The fast non-dominated sorting and crowding distance assignment of NSGA-II are then performed.After the first generation, the GA operators then produce children solutions in the evolutionary process through simulated binary crossover (SBX) and mutation operators. The parent and children solutions are then merged to 2N solutions but only N individuals are chosen after the non-dominated sorting and crowded comparison procedures have finished. The process runs and searches for non-dominated front solutions until it meets the termination criterion (e.g., maximum number of generations). The non-dominated solutions are finally obtained. On each solution, each data point is assigned into a cluster corresponding to its maximum degree of membership. Further details for each module of the algorithm are described in Sections2.2–2.5.The chromosome in the FCM-NSGA algorithm is represented by a string of real-coded values defining cluster centers in n-dimensional space. Fig. 2shows an example of a chromosome comprising four centers {z1, z2, z3, z4} in two dimensions. For example, a string representation can be {(6.01, 2.79), (4.99, 3.40), (5.49, 3.25), (6.88, 3.08)}. The number of clusters is therefore represented by the length of the string. Each chromosome in an initial population has different lengths varied over a range of 2 toM.The main procedures of the FCM algorithm are the calculation of membership degree and the update of cluster centers. The membership degree is used to indicate the extent to which each data point belongs to each cluster, and this information is also used to update the values of cluster centers.Let X={x1, x2, …, xM} be data points of M patterns, where each pattern xkis a vector of features in Rn(n-dimensional space). C is the number of clusters. A distance from a data point xkto a cluster centerviis calculated using the squared Euclidean distance as follows:(1)dik2=∑j=1n(xkj−vij)2,1≤k≤M,1≤i≤Cdik2denotes the squared Euclidean distance calculated in n-dimensional space. Thereafter, the distance is used in the calculation of membership degree in Eq. (2).(2)uik=1∑j=1C(dikdjk)2/(m−1),1≤k≤M,1≤i≤Cuikdenotes a degree of membership of xkin the ith cluster. m>1 is a parameter which controls a degree of fuzziness. This means that each data pattern has a degree of membership in every cluster.The values of centroids are then updated according to Eq. (3). Finally, the membership degree of each point is calculated once again using Eq. (2) taking the new centroid values.(3)vi=∑k=1M(uik)mxk∑k=1M(uik)m,1≤i≤CMost objective functions considered in the clustering problem have usually based on two indexes: compactness and separation. The compactness indicates variation between data within a cluster or between data and cluster centroids, and it must be kept small. The separation measures the isolation of clusters, which is preferred to be large.If only compactness or separation is considered in a single-objective optimization, some limitations may degrade the performance of the optimization. The drawback of compactness is well known that it suffers from a monotonic decrease with increasing number of clusters [26,22]. This is apparent from the experimental results in Section5.3. For the traditional separation, its disadvantage has been raised in [26]. Separation considering inter-distance measurement between cluster centroids cannot correctly detect geometric structures. As demonstrated in Fig. 3, the distance from centroids P to Q in the partition A compared to the distance between U and V in the partition B are equal. In terms of the traditional measure of inter-cluster distance, these two partitions have the same property of separation but intuitively partition B is shown to have more separation. It can be seen that the measurement of centroid distance only can misjudge the separation of clusters because the overall shape is not considered. This leads to limited information about cluster structures.In order to solve the aforementioned problems, two objectives are optimized simultaneously in the multi-objective optimization of FCM-NSGA. These objectives are based on compactness together with on overlap and separation measure. The compactness is formulated by the objective function Jmproposed by Bezdek [5] as shown in Eq. (4).(4)Jm(U,V)=∑k=1M∑i=1C(uik)mdik2uikand m are defined in accordance with Eqs. (2)–(3). The sum of the squared error is measured by the squared Euclidean distance from a pattern to each centroid with the weight (uik)mattached. The aim is to minimize Jmto optimize compactness taking into account distance and degree of membership.Overlap and separation measure (overlap-separation for short) is the second objective that has a functionality to tackle the overlapping problem and to solve the problem of centroid separation as shown in Fig. 3. This measure is based on an aggregation operation of fuzzy membership degrees. There are two parts for this index. The first part is the overlap measure proposed by [21,22], which computes an inter-cluster overlap using fuzzy degrees as shown in Eq. (5).(5)O⊥(uk(xk),C)=⊥l=2,C1(⊥i=1,Cluik)A pattern xkhas membership vector uk(xk)=(u1k, …, uck). The ambiguity measurement of several membership values requires an aggregation operator (AO). The AO applied here is based on triangular norms (t-norms) for l-order ambiguity measurement. In this aggregation, the standard t-norm and t-conorm are used. The standard t-norm has the basic property that a⊤b=min(a, b) and for the standard t-conorm a⊥b=max(a, b). With a single value⊥luk∈[0,1], the l-order fuzzy-OR operator (fOR-l) which is the combination of a dual couple (⊤⊥) [27] is associated with uk, defined by(6)⊥i=1,Cluik=⊤A∈Pl−1(⊥j∈C∖Auj)Pdenotes the power set of C={1, 2, …, c} andPl={A∈P:|A|=l}, where |A| is cardinality of the subset A. From Eq. (6), the sorting in decreasing order (u1≥⋯≥uC) is obtained, and then, the lth highest value is chosen. We apply l=2 (i.e., ambiguity measures between two classes) so that the second largest element of ukcan be used.For the separation measure [26], the maximum degree ofmaxi=1,Cuikis taken into account. Therefore, the overall overlap-separation (OS) measure for M patterns is defined as follows:(7)OS=1M∑k=1MO⊥(uk(xk),C)maxi=1,CuikIn the FCM-NSGA clustering method, solutions have to be optimized in continuous search space according to the real-coded string representation of cluster centers. The stochastic search for the real-coded string is made possible by the binary tournament selection [28], the simulated binary crossover (SBX) operator [28] and the polynomial mutation operator in [29].In NSGA-II, the binary tournament selection is used to select parents to create the new generation. Two individuals are randomly chosen to play a tournament and a winner is chosen by the crowded comparison operator (≺n). This operator considers two attributes which are non-domination rank (irank) and crowding distance (idist). Let two individuals be i and j, the crowded comparison operator ≺nis defined as:i≺njif(irank<jrank)or((irank=jrank)and(idist>jdist))The lower rank is preferred if two individuals are in different ranks. If both individuals are in the same front (same rank), the solution with lesser crowded region is chosen.The SBX operator [28] performs similarly to the search power of a single-point crossover on binary strings and maintains the interval schemata processing in continuous variables instead of discrete variables. To control how children are different from their parents, a spread factor β is defined under the probability distribution function:(8)C(β)=0.5(ηc+1)βηc,ifβ≤10.5(ηc+1)1βηc+2,otherwiseThe value of the distribution index ηcwhich is any nonnegative real number has an impact on the spread of children solutions from parent solutions. A large value of ηcgives a high probability of obtaining children solutions near to parent solutions whereas a small value of ηcallows children far from their parents. From the probability distribution in Eq. (8),β¯is a random variable which makes the area under the probability curve equal to a uniform random number u(0, 1), as follows:(9)β¯=(2u)1ηc+1,ifu≤0.512(1−u)1ηc+1,otherwiseTo obtain children solutions, two parent solutions P and Q are selected from a mating pool by the binary tournament selection. Thereafter, a random number u is generated andβ¯is calculated from Eq. (9). Consider P=(p1, …, pn) and Q=(q1, …, qn), where n is the length of strings. Two childrenci1andci2are calculated in Eqs. (10) and (11).(10)ci1=0.5[(1+β¯)pi+(1−β¯)qi](11)ci2=0.5[(1−β¯)pi+(1+β¯)qi]Since each chromosome in the population has different lengths, the chromosomes of two parents may have equal or different lengths. In case their lengths are equal, the crossover operation can be illustrated in Fig. 4. Let two parents P={p1, p2, p3, p4} and Q={q1, q2, q3, q4} denote parent solutions with four cluster centers, where each piand qiis a vector of features. Two children A and B are created. In uniform crossover, the decision to perform crossover on each pair of centers from parents is with a probability 0.5 [28]. In this example, the crossover operation does not performs on position 3, the value of p3 and q3 are directly copied to position 3 of strings A and B, respectively. Positions 1, 2 and 4 have new values a1, a2, a4 and b1, b2, b4 on A and B, respectively.The main loop to control the crossover operation between two strings of parents is described as follows:The new values of centers for two children are calculated from Eqs. (10) and (11) by the SBX() procedure as follows:In case string lengths of two parents are different, an example of this operation is illustrated in Fig. 5. The centroids considered to be crossed in the longer string are randomly chosen. p1, p2, p3, p5 are chosen and crossed with q1, q2, q3, q4, respectively. The values of the omitted centers (p4, p6) are directly copied to a child. The calculation of the new values of children then follows the same procedures as in the case of equal length.The polynomial mutation operator defined in [29,30] is applied with a low probability to perturb a solution for the new population. The result of mutation is controlled by a probability distribution:(12)P(δ)=0.5(ηm+1)(1−|δ|)ηmFrom the above distribution, the perturbation factorδ¯can be calculated according to a random number riin the range (0, 1) and the distribution index ηm, as follows:(13)δi¯=(2ri)1/(ηm+1)−1,ifri<0.51−[2(1−ri)]1/(ηm+1),ifri≥0.5One parent is chosen as xiwhich is the value of the ith cluster.xiLandxiUare the lower and upper bound of xi, respectively. The mutated value yiis thereafter calculated by(14)yi=xi+(xiU−xiL)δi¯In the final generation, the FCM-NSGA algorithm produces a set of non-dominated solutions whose number varies according to the population size. All the solutions are considered to be equal in terms of fitness values compromised by the two objectives. In most real-world problems, a single solution must be chosen out of this set.We deploy the selection mechanism presented in [19] where a semi-supervised method has been used. The class label of 10% of the whole data set is assumed to be known. The remaining 90% of the sample has no class label information provided and FCM-NSGA executes on these unknown label samples called test patterns. After the clustering procedure in the multi-objective optimization has finished, patterns are grouped in their corresponding clusters but class labels of clusters have not been defined. The class labels are later assigned by the following procedure. In [19], the assignment of class labels is based on the nearest center criterion. In FCM-NSGA, the fuzzy degree of membership is combined with the center-based criterion in the class label assignment. First, each known label sample is mapped to a cluster corresponding to the maximum degree of membership. Second, a frequency table is built by the frequency of the samples that fall in their mapped clusters. In the final step, the label of the cluster is chosen from the known label of the samples having the maximum frequency. If frequencies between the groups are equal, the cluster label is assigned by the label of the sample that has the maximum fuzzy degree with the corresponding cluster.After the label assignment procedure, the Minkowski score (MS) according to [19] is computed to measure the amount of misclassification as shown in Eq. (15). Consider the true solution set T and the solution set to be measured S. The measure is defined by the number of point-pairs assignments of data items between T and S. n11 denotes the number of point-pairs that are in the same class in both S and T. n01 and n10 represents the mismatched number of point-pairs between the two sets. n01 denotes the number of point-pairs that are in S only, and n10 denotes the number of point-pairs that belong to T only. A lower score indicates a better solution. After the scores of all non-dominated solutions have been calculated, the solution with the minimum score is chosen as the best solution.(15)MS(T,S)=n01+n10n11+n10For the purpose of comparison, data sets applied in [19] have been used in our study. There are two groups of data sets, artificial and real-life data sets. The five artificial data sets are AD_5_2 from [31], AD_10_2 from [32], Square1, Square4 and Sizes5 from [18]. The five real-life data sets which are Iris, BreastCancer, Newthyroid, Wine and LiverDisorders are obtained from the UCI repository [33]. These data sets are summarized in Table 1and have some characteristics as follows:(1)AD_5_2 contains four squared clusters with highly overlapped data.AD_10_2 has equal size of clusters and contains some overlapped clusters.Square1 consists of four separated clusters with equal size.Square4 contains the same number of data samples and the size of clusters as Square1 but the distance between individual clusters is closer. This increases overlap between clusters.Size5 has four clusters with unequal size.Iris comprises 50 instances from each of three species of Iris, namely Setosa, Versicolor and Virginica. Four attributes are collected from the length and the width of sepals and petals in centimeters. Two species, Versicolor and Virginica, are highly overlapped whereas Setosa is obviously separable from others.BreastCancer originated from the Wisconsin breast cancer database. Nine features are represented by clump thickness, uniformity of cell size, uniformity of cell shape, marginal adhesion, single epithelial cell size, bare nuclei, bland chromatin, normal nucleoli and mitoses. Each sample is categorized within two classes: benign or malignant, and these two classes are linearly separated.Newthyroid includes five laboratory tests which can identify whether patient's thyroid gland functions are in normal condition, hypothyroidism or hyperthyroidism.Wine data are obtained from a chemical analysis determining the quantities of 13 constituents found in three types of wines in the same region but from three different cultivars in Italy.LiverDisorders has six attributes including five blood tests and the amount of daily drinks. The data are divided into two sets which indicate whether an individual suffers from alcoholism.

@&#CONCLUSIONS@&#
