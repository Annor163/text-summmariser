@&#MAIN-TITLE@&#
Automatic cough segmentation from non-contact sound recordings in pediatric wards

@&#HIGHLIGHTS@&#
We develop an automated system to detect cough sounds from continuous sound recording.We design our algorithms to target the pediatric population (age <6 years), addressing a fundamental gap in current technology.Proposed technique is robust against the variation of cough sound intensity levels and waveform-shapes.Working on a clinical dataset, consisting of 14 pediatric subjects, proposed algorithm achieved a classification sensitivity of 93% and specificity of 97.5%.

@&#KEYPHRASES@&#
Pediatric respiratory diseases,Cough,Automatic cough segmentation,

@&#ABSTRACT@&#
Cough is a common symptom of almost all childhood respiratory diseases. In a typical consultation session, physicians may seek for qualitative information (e.g., wetness) and quantitative information (e.g., cough frequency) either by listening to voluntary coughs or by interviewing the patients/carers. This information is useful in the differential diagnosis and in assessing the treatment outcome of the disease. The manual cough assessment is tedious, subjective, and not suitable for long-term recording. Researchers have attempted to develop automated systems for cough assessment but none of the existing systems have specifically targeted the pediatric population. In this paper we address these issues and develop a method to automatically identify cough segments from the pediatric sound recordings. Our method is based on extracting mathematical features such as non-Gaussianity, Shannon entropy, and cepstral coefficients to describe cough characteristics. These features were then used to train an artificial neural network to detect coughs segment in the sound recordings. Working on a prospective data set of 14 subjects (sound recording length 840min), proposed method achieved sensitivity, specificity, and Cohen's Kappa of 93%, 98%, and 0.65, respectively. These results indicate that the proposed method has the potential to be developed as an automated pediatric cough counting device as well as the front-end of a cough analysis system.

@&#INTRODUCTION@&#
Cough is a defense mechanism of the body to clear the respiratory tract from foreign materials which are inhaled accidentally or produced internally by infections [1]. It is a common symptom appearing in early to mid stages of respiratory diseases such as pneumonia, the leading cause of death in children less than five years of age. It has been estimated that pneumonia causes over 1.6 million deaths in this group per year [2], with more than 97% of cases occurring in the developing countries [3]. The world health organization (WHO) also reported that in those countries, pertussis (whooping cough) has also become one of the major childhood morbidities with an estimated 50 million cases and 300,000 deaths every year [4].Even though cough is common in respiratory diseases and considered as an important clinical symptom, there is no gold standard to assess it. In a typical consultation session, physicians may listen to several episodes of spontaneous or voluntary coughs, to obtain qualitative information such as the “wetness” of a cough. Such qualitative information is extremely useful in diagnosis as well as the treatment of respiratory diseases.During consultation sessions physicians may also seek quantitative information on coughs, such as the frequency of occurrence of cough events over a given time interval. This information can be used to determine the nature (e.g., acute, chronic) and the severity of coughs as well as to monitor the efficacy of a treatment [5]. However, to obtain this information, physicians heavily rely on subjective reports of patients or their carers. There is a great need for an automated device capable of counting the number of coughs, especially in childhood diseases. More importantly, technology capable of automatically extracting cough events from pediatric recordings is urgently needed in order to facilitate the diagnosis of diseases such as pneumonia [6,7].Several approaches have been taken to develop automated cough counting systems (e.g., Hull Automatic Cough Counter (HACC), Leicester Cough Monitor (LCM), LifeShirt, VitaloJAK, and PulmoTrack). The performances of these devices are varied. The HACC claimed 80 percent sensitivity and 96 percent specificity [8]. The figures for LifeShirt, Pulmotrack, LCM, and Vitalojak are (78%, 99%), (94%, 96%), (85.7%, 99.9%), and (97.5%, 97.7%) respectively [9–12]. To extract the sound events, HACC computed the standard deviation of cough sound intensities [8], while VitaloJAK [13] and LCM [11,14] used sound intensities themselves. This makes these methods susceptible to variations in recording conditions, such as the distance from the microphone to the patient, sensitivity of the recording instruments and the sound level of the coughs being recorded. Both LifeShirt [15] and PulmoTrack [16] described cough as having a distinguishable humped structure in the waveform. Then they identified coughs by counting the number and/or measuring the slope of these humps. None of these commercial devices have been tested on pediatric populations or subjects with diseases such as pneumonia. In such subjects, the cough sound intensities and waveform-shapes may vary widely. Therefore intensity or simple waveform-shape based methods are unlikely to be optimal and are less likely to be robust in field use.Cough recording on children, especially the younger ones, pose several additional challenges. Younger children are unable to produce voluntary coughs upon request. Any method targeting pediatric populations should be capable of using spontaneous coughs recorded over a period of interest. In pediatric recordings, crying, vocalization, and grunting are found abundantly, intermixed with cough sounds. Consequently, technology developed for adults are unlikely to be optimal for use on children.Existing commercial cough counting devices such as LifeShirt, Vitalojak, and Pulmotrack employ contact sensors. While the use of contact sensors may have some advantages, they also carry several drawbacks. Contact sensors, compared to non-contact (free-air) microphones are robust against background sound propagated through air. However, they are more vulnerable to sound conducted through tissue and bones; spurious rubbing sounds due to sensor movement can also be an issue. In infectious diseases, elaborate efforts are needed to avoid cross contamination of patients through contact instrumentation. Furthermore, in pediatric subjects, contact sensors can also be difficult to attach because of patient discomfort. In this paper we address these issues and propose a novel technology for the automated segmentation of cough events from recordings obtained using non-contact microphones in a pediatric ward. In particular:•We design our algorithms to target the pediatric population (age <6 years), addressing a fundamental gap in current technology.We develop a method for the segmentation of cough events, with algorithms capable of discounting background sounds such as crying, vocalization, and grunting.We develop new techniques that are robust against the variation of cough sound intensity levels and waveform-shapes. This is an unprecedented approach in this field, to the best of our knowledge.The method has the potential to be developed as an automated cough counting device as well as the front-end of a cough based diagnostic system.The cough recording system consisted of a low-noise microphone having a cardioid beam pattern (Model NT3, RODE®, Sydney, Australia), followed by a pre-amplifier and an A/D converter (Model Mobile Pre-USB, M-Audio®, CA, USA). The output of the Mobile Pre-USB was connected to the USB port of a laptop computer. The nominal distance from the microphones to the mouth of subjects was 50cm. The actual distance could vary from 40cm to 100cm due to the subject movement. We kept the sampling rate at Fs=44.1 k samples/s and 16-bit resolution to obtain the best sound quality.The data for this work were recorded at Sardjito Hospital, Yogyakarta, Indonesia, from pediatric patients admitted on respiratory complaints. The inclusion criteria used in the recruitment was patients with at least two of the following symptoms: cough, sputum, breathlessness, and temperature higher than 37.5°C. We excluded patients having advanced disease where recovery is not expected, diseases with droplet precautions and patients undergoing ventilation treatment. The recordings were started after physicians had examined the subjects, begun the initial treatment, and informed consent had been completed. We acquired data in the natural hospital environment, without modifying it anyway, other than placing our sound recording system by the bed (see Fig. 1). The duration of recording for each subject was between four and 6h. The research protocol had received ethics clearances from Sardjito Hospital and The University of Queensland, Australia.In this paper, we used recordings from 24 pediatric subjects. We divided these subjects into two datasets namely:(i)Model design dataset (MDD): Dataset MDD denotes the training dataset that was used to train the neural network (NN) classifier. The main criterion in designing MDD is that it should contain the whole range of cough types and their variations as well as non-cough sounds expected in the practical setting. That way, the NN can learn the characteristics of the variety of coughs and learn to differentiate them from non-cough sounds. Data set MDD was designed by manually picking representative cough and non-cough sounds from each subject in the training dataset. All such sound events were then concatenated as a single data stream to form the set MDD. Thus, the dataset MDD is not a natural sequence of sound events, but the combination of a large number of handpicked cough and non-cough sounds. To develop MDD, we took cough and non-cough sound samples from D1=10 subjects. The overall length of MDD was 15min.Prospective study dataset (PSD): Dataset PSD denotes the testing dataset and contains the actual sound stream recorded in the hospital. This way, our results on PSD can be taken as a true indication of performance in the clinical environment. Testing set PSD included the first 60min of the sound streams recorded from D2=14 subjects in the testing set. The total duration of PSD was 840min.Subjects in dataset MDD and PSD were mutually exclusive. The division of the subjects in two datasets was based on the order of presentation to the respiratory clinic of the hospital.In both MDD and PSD, cough events were manually identified and marked. To define the beginning and end of cough segments, the scorer carefully listened to the sounds and simultaneously looked at the time domain waveform displayed on computer screen. Adobe Audition version 2.0 software was used in this process. This manual identification of cough events was used as the gold standard against which results of automatic classification were compared.Based on physiological considerations, cough sounds are often considered as consisting of four different phases [1]: inspiratory, contractive, compressive, and expulsive. The inspiratory phase is initiated by breathing in and is terminated by the closure of the glottis. In the contractive phase, groups of respiratory muscles contract leading to a marked elevation of alveolar, pleural, and subglottic airway pressures. In the expulsive phase, the glottis opens quickly followed by rapid exhalation of air under a large pressure gradient. The rapid movement of air expelled from the lung generates the cough sounds with contributions coming from different areas of the respiratory system. The mechanism of cough sound production shares some similarities to that of speech production.In this paper, inspired by human speech and snore processing techniques, we developed techniques for the analysis of cough sounds.A discretized sound recording s[n] in MDD and PSD, can be modeled by the following equation:(1)s[n]=scg[n]+snc[n]+b1[n]+b2[n]where scg[n] is the cough sounds and snc[n] is the non-cough sounds such as speech, cry, laughs, vocalization and sounds from hospital equipment. The quantity b1[n] represents the low frequency noise in the measurement (e.g., noise coming from the vibration of microphone's stands) while b2[n] is the Gaussian noise (mostly high frequency).In this paper, we propose an automated method to extract cough sounds scg[n] from the recording s[n]. The block diagram of the overall method is shown in Fig. 2. It comprises four main processes: noise reduction, feature extraction from sub-blocks of data, design of automatic classification model for classifying the sub-blocks into cough (CG) and non-cough (NC) classes, and identification of cough events by grouping CG/NC-classified contiguous sub-blocks of data.In Sections 2.4.1–2.4.4, we provide a detailed description of the method.To reduce the noise b1[n] and b2[n], we processed s[n] through two different filters: (a) a high pass filter (HPF), and, (b) a power spectral subtractions (PSS) filter. The HPF was implemented to reduce the low frequency noise b1[n]. We designed the HPF as a fourth-order Butterworth filter with cut off frequency fc=10Hz. The particular fcwas selected based on the low frequency noise profile in dataset MDD. The PSS filter was employed to reduce the Gaussian noise b2[n] [17]. After the HPF and PSS filters, the estimatesˆ[n]of the recording s[n] is given by:(2)sˆ[n]=sˆcg[n]+sˆnc[n]wheresˆcg[n]andsˆnc[n]represent the estimates of cough sounds and non-cough sounds respectively.Our signal of interest is the cough sound estimatesˆcg[n]. In the following we discuss the method used to extractsˆcg[n]from the estimatesˆ[n]. The approach we take here is to extract fromsˆ[n]feature characteristic ofsˆcg[n]and use them to differentiatesˆcg[n]andsˆnc[n].To obtain the features of the sound signal, we applied a rectangular sliding window wr[n] of length N (N=882 samples, equal to 20ms) tosˆ[n], generating data sub-blocks. Letsˆ[n]={|sˆ1[n]|,…,|sˆk[n]|,…|sˆK[n]|}represents the filtered sound recording wheresˆk[n]represents the kth (k=1, 2, …, K) sub-block insˆ[n]. For each sub-blocksˆk[n]we computed the following features.•Mel-frequency cepstral coefficients (MFCCs): MFCCs is widely used in speech processing [18]. It was found to be highly useful for snore analysis [19–22] as well as cough analysis [11]. MFCCs provide a compact representation of the upper airway acoustical properties and allow one to separate contributions from the airway cavity geometry and the source vibration sounds. The MFCCs (ϕk) of a sub-blocksˆk[n]can be computed using (3).(3)ϕk=∑c=1CLk(c)cosr(2c−1)π2CNote that in (3), Lkis log energy output of c Mel Filter banks (c=1, 2, …, 40) of a sub-blocksˆk[n]and r is the number of cepstral coefficients (r=0, 1, …, 13). The MFCCs (ϕk) of all sub-blocks insˆ[n]can be expressed in a matrix form as:(4)ϕ=[ϕ1,ϕ2,…,ϕk,…,ϕK]Formant frequency: In speech, Formant frequency shows the characteristics of vocal tract resonances; in snore sound analysis they indicate the resonance of the upper airway. We hypothesized that in cough/respiratory sounds, formant may carry resonances of the entire respiratory tract. For instance, wheezing sounds, which originates due to vibrations of the bronchioles of the lung, may contribute higher frequency formants (resonance frequencies) in the cough sounds. We used linear predictive coding (LPC) to estimate the formant frequency. To find the coefficients, LPC uses autocorrelation method of autoregressive (AR) model which can be determined by solving Yule–Walker equations via the Levinson–Durbin recursion [23].In this work, we computed five formant frequencies per sub-block. The formant frequency of a sub-blocksˆk[n]is denoted by ℜk. In a matrix notation, we write the formant frequencies ℜ of all K sub-blocks ofsˆ[n]as (5).(5)ℜ=[ℜ1,ℜ2,…,ℜk,…,ℜK]Zero crossing rates (ZCR): The ZCR, defined as the total time a signal crosses the zero axis, is a simple but useful method to detect the periodic nature of a signal regardless of its magnitude. It may represent the glottis vibrations and can be used to separate voice and unvoiced signal [24]. The ZCR (Zk) of a sub-blocksˆk[n]can be calculated using (6).(6)Zk=1N−1∑n=1N−1H{−sˆk(n)sˆk(n−1)},1≤n≤N−1where H{e} is Heaviside function (its value is 0 for negative argument and 1 for otherwise). The ZCR (Z) of all K sub-blocks ofsˆ[n]signal is given in (7).(7)Z=Z1,Z2,…,Zk,…,ZKNon-Gaussianity score (NGS): The NGS provides an easy method to quantify the deviation of a given signal from a Gaussian model. In our previous work on snore sound analysis [25], this feature showed a capability to screen obstructive sleep apnea/OSA (OSA snoring tends to produce non-Gaussian distributed sounds). In cough, the non-Gaussianity may arise when the glottis suddenly opens during the expulsive phase. The NGS (ψk) of a sub-blocksˆk[n]can be calculated using (8), where p and q are the normal probability plot of the reference normal data and analyzed data, respectively andq¯represents mean of q.(8)ψk=1−∑j=1N(qj−p)2∑j=1N(qj−q¯)2,1≤j≤NThe NGS (ψ) of all sub-blocks insˆ[n]is denoted in (9).(9)ψ=[ψ1,ψ2,…,ψk,…,ψK]Shannon entropy: Cough sound is a complex signal which represents contributions from various sub-structures of the respiratory tract. Some of these components display pseudo-periodic structures, while others have a random stochastic character. In this work, we computed the Shannon entropy to capture these features. The Shannon entropy (ηk) of a sub-blocksˆk[n]was obtained using definition in (10)[26].(10)ηk=−∑n=1N−1(sˆk(n)2)ln(sˆk(n)2),1≤n≤N−1We formed the matrix η of all sub-blocks insˆ[n]as follows.(11)η=η1,η2,…,ηk,…,ηKFor each sub-blocksˆk[n], compute all the features described in Section 2.4.2 and form a feature vector Fk=[ϕℜZψη]T. We extracted these feature vectors from both model design dataset (MDD) and the prospective validation set (PSD). Let the feature set extracted from the kth sub-block of MDD be denoted by Fk,M and that from the PSD be denoted Fk,P.Next we formed an overall feature matrix based on the feature vectors of sub-blocks as: GM={F1,M, F2,M, …, Fk,M, …, FK,M} and GP={F1,P, F2,P, …, Fk,P, …, FK,P}, where GMand GPrepresent feature matrices for MDD and PSD.Cough feature matrix estimated from the set MDD, GM, was then used to train the automatic classifier model to classify sound data in a sub-blocksˆk[n]into the classes cough (CG) and non-cough (NC). Details of this process are described in Section 2.4.3. The matrix GPfrom the set PSD was used for the prospective testing of the trained models.The automatic cough segmentation method is a two stage process: classification of sound features into cough (CG) and non-cough (NC) classes, and identification of cough events. The description of these processes is given in Section 2.4.3(A) and (B).(A) Design of neural network model to classify a sub-blocksˆk[n]into the cough (CG) and non-cough (NC) classesIn this paper, we investigate the use of an artificial neural network (ANN) as the CG/NC classifier at the sub-block level. We used the ANN inspired by the capability of the human brain to recognize different types of cough sounds regardless of their intensity, duration, or wetness. Moreover, ANN has the advantage of classifying data using non-linear decision boundaries, based on a process of supervised learning with a set of given examples. We compared the performance of the ANN with linear discriminant analysis (LDA) to justify the selection of the non-linear classifier for this work. In contrast to ANN, LDA searches for the linear combination of features [27] that give the best possible separation between cough and non-cough sounds.In this work, we used the particular form of an ANN known as a time delay neural network (TDNN) [28] that has been used in speech recognition applications. TDNN is capable of classifying sub-blockssˆk[n]discounting temporal translations of the input feature set [28].Our TDNN structure comprised of an input layer (Li), two hidden layers (Lh1 and Lh2), and an output layer (Lo). The number of neurons in Lidepends on the size of input feature vector to ANN which is defined in next paragraph. The number of neurons in Lh1, Lh2, and Lowere set to 20, 10, and 1 respectively. We used a linear activation function for neurons in Loand sigmoid activation functions for neurons in Lh1 and Lh2 layers. To determine initial weights and bias values, we used the Nguyen-Widrow initialization method [29]. For updating weights during the training process, we employed the resilient back propagation (RPROP) algorithm [30].We trained the TDNN to classify each sub-block ofsˆ[n]into CG/NC class. Let Fk−2Fk−1FkFk+1Fk+2 represent the feature vectors of sub-blockssˆk−2[n],sˆk−1[n],sˆk[n],sˆk+1[n],sˆk+2[n], respectively. Let Q=[Fk−2Fk−1FkFk+1Fk+2] be the input feature vector to ANN formed by concatenating individual feature vector from sub-blocks. Therefore number of neurons in Liof TDNN will be 110. We used Q as input to the TDNN to classify the kth sub-block ofsˆk[n]into CG/NC class. This process was repeated for k=3, 4,…, K−2 to cover the whole signalsˆ[n]represented bysˆ[n]={|sˆ1[n]|,…,|sˆk[n]|,…|sˆK[n]|}.In training and optimizing the TDNN, we used the matrix GMand adopted the leave-one out validation (LOOV) technique. This involves using feature matrices from all the subjects in MDD except one to train the TDNN model, and validate the model using the remaining subject. This process was systematically repeated 10 times such that each subject in MDD was used as the validation data once. This resulted in 10 neural network models.Let um[k], m=1, 2, …, 10, represents the output of the mth TDNN. In Section 2.4.3(B), we describe the method to identify the beginning and the end of a cough event using um[k].(B) Identification of the beginning and the end of cough eventsThe identification of the beginning and the end of cough events from the output of TDNN models um[k] was carried out by following steps (S1)–(S3):(S1)Smoothing process – Pass um[k] through a moving average filter with the tap length β. Let the output of this filter beu˜m[k].Thresholding – Apply a threshold value λ tou˜m[k]. The sub-blocks withu˜m[k]above the threshold λ were assigned the decision γ=1 otherwise γ=0. The sub-blocks with value γ=1 are candidate cough event sub-blocks.Cough event identification – A cough event is identified if ρ number of consecutive sub-blocks are candidate sub-blocks (i.e., γ=1) and if following conditions are satisfied:τmin≤τρ≤τmax, where τρis the total time duration of ρ consecutive sub-blocks with γ=1. τminand τmaxare the minimum and maximum cough sound durations computed from the cough events in MDD.δρ>δ, where δρis the root mean square (RMS) value of ρ consecutive sub-blocks inu˜m[k]with γ=1 and δ is the threshold root mean square value.Both Conditions 1 and 2 were adopted to improve the specificity of the automatic classifier by rejecting those episodes which are falsely identified as probable cough episodes after S2. While Condition 1 checks for the duration of events, removing lengthy episodes which are more likely to be non-cough sounds, Condition 2 checks for the RMS value of TDNN output. Cough events in MDD indicated that RMS value of TDNN output for cough sounds were higher than that of non-cough sounds.All the parameters including β, λ and δ were optimized using data in MDD for maximizing the classifier performance. The steps for optimizing these parameters are as follows:(P1)Optimizing β – set parameter λ and δ at certain values then vary β from 1 to 30. Select the optimum β values at the maximum performance in the validation set.Optimizing λ – use the optimized β and δ values from step (P1), and then vary λ from 0.001 to 1. Select the optimum λ at the maximum performance in the validation set.Optimizing δ – use the optimized β and λ values from steps (P1) and (P2), and then vary δ from 0.001 to 1. Select the optimum δ at the maximum performance in the validation set.To evaluate the performance of the proposed method, performance measures such as sensitivity, specificity, accuracy, and Cohen's Kappa statistic were computed. The performances were computed both at the sub-block level as well as the cough event level by comparing the output of the algorithm with the reference scoring from the human observer.From the ten TDNN models, we selected the model that gave us the best performance as obtained in our LOOV validation technique on MDD. Let ℵ represent the selected TDNN model with βs, λs, δsas its corresponding parameters. The implementation of ℵ on the prospective dataset is described in the following section.The process of cough segmentation in PSD is as follows. Let GPbe a feature matrix computed using sound data from a subject in dataset PSD, following the process described in Sections 2.4.1 and 2.4.2. Apply the model ℵ to GPand automatically classify the cough sounds into classes CG/NC at the sub-block level; identify cough events following the steps given in Section 2.4.3. Repeat this for all the patients (D2=14) in PSD. Compare the results of automatic segmentation with that of manual segmentation and evaluate the performance, at sub-block as well as event levels. In Section 3.5, we describe the performance of ℵ in segmenting coughs from the prospective study data set (PSD).

@&#CONCLUSIONS@&#
