@&#MAIN-TITLE@&#
Real time detection of driver attention: Emerging solutions based on robust iconic classifiers and dictionary of poses

@&#HIGHLIGHTS@&#
A real time ADAS, based on video cameras and iconic classifiers, is proposed.Iconic classifiers use simplified learning, based on a small dictionary of poses.On-board experiments demonstrate robustness and effectiveness of the approach.The system is almost independent from the actual user, requiring limited training.Various categories of users and adverse light conditions are easily managed.

@&#KEYPHRASES@&#
Automotive applications,Monitoring of driver attention,Driver assistance systems,Neural networks,

@&#ABSTRACT@&#
Real time monitoring of driver attention by computer vision techniques is a key issue in the development of advanced driver assistance systems. While past work mostly focused on structured feature-based approaches, characterized by high computational requirements, emerging technologies based on iconic classifiers recently proved to be good candidates for the implementation of accurate and real-time solutions, characterized by simplicity and automatic fast training stages.In this work the combined use of binary classifiers and iconic data reduction, based on Sanger neural networks, is proposed, detailing critical aspects related to the application of this approach to the specific problem of driving assistance. In particular it is investigated the possibility of a simplified learning stage, based on a small dictionary of poses, that makes the system almost independent from the actual user.On-board experiments demonstrate the effectiveness of the approach, even in case of noise and adverse light conditions. Moreover the system proved unexpected robustness to various categories of users, including people with beard and eyeglasses. Temporal integration of classification results, together with a partial distinction among visual distraction and fatigue effects, make the proposed technology an excellent candidate for the exploration of adaptive and user-centered applications in the automotive field.

@&#INTRODUCTION@&#
Since late 90s, undesirable or unusual driver conditions have been clearly identified as a primary cause of car crashes and road deaths (Kircher et al., 2002; European Transport Safety Council, 2001). This problem attracted the interest of the scientific community, which has begun to study the development of intelligent and adaptive systems, namely Advanced Driver Assistance Systems (ADAS), suitable to monitor the diver’s state of vigilance and give real-time support in accident avoidance (Liang et al., 2007; Batista, 2005).As pointed out by Liang and Lee (2014), the nature of driver inattention can vary: fatigue and related symptoms like drowsiness and frequent nodding are very common in real cases but distraction from safe driving can also have a visual or cognitive cause.Visual distraction has often to do with the on-board presence of electronic devices or tools like mobile phones, navigation and multimedia systems, requiring active control from the driver (for example pushing buttons or turning knobs); visual distraction can be also related to the presence of salient visual information away from the road, thus causing spontaneous off-road eye glances and momentary rotation of the head. Cognitive distraction happens whenever the mind of the driver is not sufficiently focused on the critical task of safe driving; symptoms of cognitive distraction are less apparent, and difficult to be detected or quantified by objective indicators. Most of times the analysis of cognitive distraction is therefore based on long behavioral patterns and sophisticated statistical techniques (Liang and Lee, 2014).Focusing on fatigue and visual distraction, the paper investigates the design and the development of a fully automated driver assistance system based on advanced techniques coming from image analysis and related fields like pattern recognition and biometrics (Zhao et al., 2003).In previous studies, computer vision techniques have been often proposed to detect driver attention (Batista, 2005; Singh and Papanikolopoulos, 1999) both by standard and day-night infrared cameras. In particular, these techniques have been adopted to detect signs of visual distraction, like off-road gaze direction and persistent rotation of the head, and changes in the facial features which characterize persons with reduced alertness due to fatigue: longer blink duration, slow eyelid movement, small degree of eye opening, nodding, yawns and drooping posture are among the most interesting conditions which has proved to be captured by vision-based approaches (Bergasa et al., 2008).A common processing scheme, well discussed in Senaratne et al. (2011, 2007) includes the following steps:•face localization;localization of facial features (e.g. eyes or mouth);estimation of specific cues related to fatigue or distraction;fusion of cues in order to determine the global attention level.Concerning face localization, very robust techniques based on neural networks have been developed in late 90s (Rowley et al., 1998; Sung and Poggio, 1998). In 2004 Viola and Jones (Viola and Jones, 2004) proposed a new high performance algorithm based on integral images and robust classification; this algorithm is a de facto standard for real-time applications. Both the above approaches belong to the image-based subclass of the face detection techniques. More recently also feature-based approaches demonstrated a reasonable level of efficiency. In particular, Particle Swarm Optimization (Kennedy and Eberhart, 1995) has been proposed for locating and tracking a limited number of facial landmarks.Research on facial features extraction mainly focused on eyes and mouth (Zhao et al., 2003); Gabor and SVM techniques have been successfully proposed to this aim (Senaratne et al., 2007). In order to work under low light conditions, researchers also proposed the use of infrared illuminators, exploiting high reflection of the pupils (Senaratne et al., 2011); as noted in Lenskiy and Lee (2012), however, IR based approaches show malfunctions during daytime and require the installation of additional hardware.It is worth noting here that most of the literature defines the PERCLOS as the main cue for the estimation of driver’s fatigue. PERCLOS is a measure of the time percentage during which eyes remain closed 80% or more; in order to compute this cue, every image frame is usually classified into two classes (closed eyes or open eyes): k-NN techniques, SVMs and Bayes approaches have been successfully applied to this purpose (Rowley et al., 1998). Other cues commonly used are head pose, eye blinking detection (Lenskiy and Lee, 2012), slouching frequency and postural adjustment. To the aim of this work the estimation of the head pose certainly represents the most interesting issue (Sung and Poggio, 1998); this information can be derived by applying both 2D and 3D approaches (Murphy-Chutorian and Trivedi, 2009).Overall, previous studies show that the problem of detecting visual distraction and fatigue can be faced with fairly good results in driving simulators or constrained conditions. However, the application on a real moving vehicle presents new challenges like changing backgrounds and sudden variations of lighting. Moreover, a useful system should guarantee real time performance and quick adaptability to a variable set of users and to natural movements performed during driving.In order to tackle the real problem and to reach a sufficient level of accuracy and performance, we propose here a driver assistance system based on robust iconic classifiers. Starting from a preliminary image data reduction step, and from a priori knowledge related to known head poses and known patterns (like, for instance, closed/open eyes), we show that iconic classifiers perform well with respect to changes in pose and facial features configuration, while ignoring unessential details like glasses, hairstyle and lighting conditions. As explained later in the text, the conceptual boundary between raw input data, feature extraction and classification can be somewhat arbitrary; moreover the proper classification of the input data can be heavily influenced by the collection of poses and patterns used in the learning phase. For this reason we propose a binary classification of poses and features, where the collection of possible configurations is simply categorized in “attentive” versus “inattentive” classes.Following sections are organized as follows: Section 2 briefly introduces the adopted attention model and the fundamental methods applied for the various processing steps; Section 3 details the experimental setup, the data collection phase and experimental results. Finally Section 4 draws some conclusions and analyses possible outcomes of this research.Even though the adoption and the fusion of different cues usually shows some increase of performance, recent work (Masala and Grosso, 2013) demonstrates that this approach can be efficiently replaced by an alternative “fully iconic” approach, based on a generalized model of the “inattentive driver”. This iconic generalization, derived by processing and classifying off-line a sequence or a selected set of images of a generic real user, is denoted here as “dictionary of poses” because it captures essential iconic information related to the position of the head and the state of the eyes of a driver both during attentive and distracted or fatigued driving. As in the Viola Jones face detector (Viola and Jones, 2004), this pre-learned pattern that can be usefully exploited for on-line processing, achieving high levels of accuracy and real time performance.Note that for the Driver Assistance Systems the distinction among visual, cognitive and fatigue effects is not unessential; having knowledge about the origin of the distraction can help the system to implement adaptive and more intelligent behaviors. For this reason, while we define in the following as “inattentive driver” a subject showing visual distraction, fatigue effects or both (which is totally coherent with the final goal to detect the diver’s state of vigilance), we also try to maintain some level of information about the type of processing which generates the inattentive classification.The proposed attention model is based on a two-layer classifier where the single frames are processed and associated to the “attentive” or “inattentive” state of the driver. The first layer is devoted to the detection of the head pose (then including drowsiness due to fatigue and visual distraction) while the second layer distinguishes between open/closed eyes, a measure strictly related to fatigue. A block diagram of the complete system is shown in Fig. 1.Note that the Viola Jones face detector (Viola and Jones, 2004) is preliminary applied to each frame in order to extract a small region of interest (ROI) containing face-candidates. The Viola Jones detector relies on a large set of simple Haar-like features, and uses the AdaBoost learning algorithm to reduce this over-complete set . The detector is applied to gray-scale images, producing fairly regular results; however it fails when the face of the driver is partly or totally out of the field view. It also fails in case of partial occlusion of the face and in case of manifest rotation of the head; all these cases conservatively bring to the immediate association of the frame to the “inattentive state”. (See Figs. 2 and 3)Both the following layers work on extracted ROIs: these ROIs are first scaled to a fixed dimension (280×280pixels), then are processed giving rise to the final classification. Note that the system knows about the origin of the classification; therefore it can distinguish between “inattentive” frames due to absence of face candidates (I1), “inattentive” frames due to inappropriate head pose (I2) and “inattentive” frames due to closed eyes (I3). This information is used by the final temporal integration block, deciding conveniently about the alarm state of the system.The first classification layer is specialized on the detection of a wrong head pose in a single frame. The input is the ROI extracted from the Viola Jones face detector. ROI are first processed by histogram equalization, then a binarization filter is applied. In order to reduce the dimensionality of the image, a Sanger neural network is used. Then a dissimilarity representation is computed taking as reference a small dictionary of poses. The final classifier is a feed forward neural network (FF-Bp) which processes the dissimilarity representation and decides about the attention state of the driver due to head movement. If the head of the driver has the correct pose (A) the original ROI is passed to the secondary layer, otherwise it is labeled as inattentive I2.The second layer of classification is specialized in detecting the state of the eyes in a single frame; only ROIs labeled as A from the first layer are considered. In this case, first a small image rectangle centered on the eyes region (220×120pixels) is extracted and processed by histogram equalization, then a Sanger neural network similar to the previous stage is used to reduce the dimensionality. Also in this layer a dissimilarity representation is used in order to improve the subsequent classification step. Only the frames of the previous dictionary where both eyes are clearly recognizable are used as reference. The final classifier is again a feed forward neural network (FF-Bp) which processes the dissimilarity representation and decides about the attention state of the driver derived from the condition of the eyes. If the driver has normally open eyes the frame is associated to an attentive state (A3); otherwise it is labeled as inattentive state (I3).One of the key issues related to the proposed approach concerns the adoption of two Sanger neural networks (one for each layer) in order to reduce the dimensionality of the images corresponding to face candidates (Sanger, 1989) . A Sanger neural network is a simple three-layer feed-forward unsupervised network (with linear transfer function in the hidden neurons) which develops an internal representation corresponding to the principal components analysis of the full input data set. The input and output layers have the same dimension of the input patterns while the dimension of the hidden layer, corresponding to the number of the principal components, is determined during the training phase. Each network is trained as an auto-encoder (Duda et al., 2001; Masala et al., 2007), in such a way to reproduce at the output the input data. Starting from a typical number of principal components (12) used in eigen-faces detection (Turk and Pentland, 1991) and using a small number of training frames (frames from the adopted dictionary of poses) we found the best configuration for 16 principal components. Only these values, representing the optimal reduction of the iconic data, are passed to the subsequent classifiers.Note that the use of a dictionary of poses to train the Sanger networks has some interesting consequences. First of all each Sanger network is trained once; this means that processing can be executed off-line and without any reference to effective users. Secondly, once fixed the weights of the Sanger networks, data reduction can be easily obtained by projecting each ROI in the final feature space (i.e. by product of the Sanger weight vector for the row data frames). This operation is very fast, giving as a result a very compact representation of the iconic image content both for the first and the second classification layer.Representation based on dissimilarity is a well-known concept in the pattern recognition literature (Pekalska and Duin, 2000; Bottigli et al., 2005; Kim and Duin, 2011) and it is a very good alternative to the traditional feature-based description whenever relations between objects must be captured (Pekalska and Duin, 2000). A dissimilarity value expresses the difference between two objects or features and becomes zero only when the two objects are identical. In general, dissimilarity measures are applied directly to raw data (for instance images or temporal signals) but it is not rare the use of pre-processing steps aimed at reducing the dimension of the feature space. A very powerful pre-processing method, well investigated by authors in Kim and Duin (2011) is based on principal component analysis. In particular, it has been shown that computing dissimilarity on principal eigenvectors helps to face intractable problems like distortion, illumination changes and noise.To construct a decision rule based on dissimilarity, a model reference setRwith r elements is commonly used:Rconsists of prototypes which are representatives of all involved classes. In the learning process, a training setTof t elements is then adopted to build the t×r dissimilarity matrixD(T,R)relating all training objects to all prototypes. The information on a setSof s new objects is provided in terms of their distances to R, i.e. as an s×r matrixD(S,R).In the above approach, a key factor is the discriminative power of the adopted measure of difference, but intrinsic properties of the adopted metric must be also considered. In fact, many traditional optimization methods are not appropriate for non-metric dissimilarities, as they often rely on the triangle inequality axiom.A final remark concerns the dimension of the feature space where measures are performed. In order to guarantee a good representation of the real data distribution, the number of samples must be much higher than the dimension n of the space; a reduction of the spatial dimensionality is therefore important to maintain a compact model reference set, and besides, to contain computational burden.In the proposed approach the dissimilarity measure is performed by traditional Euclidean metric. The model reference set R is composed of 72 images (r=72) for the first layer and 48 images (r=48) for the second layer while the training set T is composed of several thousand of images, depending on the layer and on the considered subject. We denote the set R as “dictionary of poses” because the set is composed of images of a real user during the driving. Images are taken during three different sessions, with different conditions of light and slightly different distance from the camera. The same user appears with glasses and without glasses; different wrong poses of the head are also simulated by asking the user to look at eight fixed markers around the car. Open/close condition of the eyes is finally simulated asking the user to close the eyes both for correct and wrong poses of the head and simulating nodding. Some example of the images of the dictionary are given in Fig. 4.The dissimilarity representation is computed over the Sanger components; the dimension n is therefore equal to 16.In summary, we have:(1)ti=(ti1,ti2,…,ti16)i=1,…,t(2)rk=(rk1,rk2,…rk16)k=1,…,rwhere tiand rkare generic frames of the training and reference set. The generic element of the dissimilarity matrix will be:(3)dik=‖ti-rk‖A single row of the dissimilarity matrix will express all the distances of the generic training elementti with respect to the reference set. As the final classes of the reference set are a priori known, these distances can be obviously grouped in a number of subsets equal to the total number of classes and used to feed the training stage of the classifiers.For the classification step we used a Feed Forward Back Propagation Neural Network (FF-Bp) (Duda et al., 2001; Haykin, 1999). FF-Bp provides a not algorithmic, but very efficient, approach. Back propagation is used for learning: for a supervised system, the network is trained by using samples of known classes. In our case, the classifiers are trained on a training set and tested on a validation set to determine the optimal parameterization. As detailed in the next section, the total number of images used in the training/validation stages depend on the subject but it is always significant, ranging from about 1800 to nearly 2300 images per session.Concerning the configuration of the classifiers, the following have been used:1.First layer: a FF-Bp with 72 input neurons and 2 output neurons; note that the input neurons correspond to the dimensionality of the dissimilarities representation of data, while output neurons correspond to the attention states considered in this layer (0-correct, 1-wrong pose).Second layer: a FF-Bp with 48 input neurons and 2 output neurons; in this case the input neurons correspond to a subset of the dictionary of poses where the eyes are clearly recognizable, while output neurons correspond to the attention states considered in this layer (0-open, 1-closed eyes).As detailed in the experimental section, the result of a binary classifier for a given condition can be easily defined in terms of true (correct) and false (wrong) rate of detected items.Denoting by p and (1−p) the probability of correct/wrong detection of a generic Bernoulli trial (representing the classification of a single frame for which the ground truth state is “inattentive”), the related binomial distribution B(n,p) defines the discrete probability of a number k of correct detections in a sequence of n independent trials. More precisely, if the X random variable follows the binomial distribution, we can write this probability as:P(X=k)=nkpk(1-p)n-kThe cumulative distribution of a random variable X following a binomial distribution is defined in turn as:P(X≤k)=∑i=1knipi(i-p)n-iThe meaning of the above expression has to do with the probability of having a number of correct detections less than or equal to k in a sequence of n independent trials where p denotes the probability of a correct detection for a single trial and provided that the truth state remains “inattentive”.It is well known, for the law of large numbers, that taking n sufficiently big and k=n/2, P (X⩽k) tends to one if p<0.5; conversely, P (X⩽k) tends to zero if p>0.5. Therefore P (X⩽k) is a robust estimate of the observed condition when independent measures of the same condition can be performed (and obviously the condition does not change during the measurement process).The temporal integration scheme, commonly used in recognition methods, exploits these theoretical considerations extending the output of the classifiers over a span of n consecutive frames; a majority voting is usually adopted in order to decide about the final classification.The temporal span must obviously respect the dynamic of the observed event; in particular the duration of the observation window:•cannot exceed the typical duration of the event (an event lasting typically t1 ms cannot be correctly detected with a larger temporal window because the measures would refer to different conditions);must be greater than the minimum duration for which the event is considered significant (if the event becomes significant after t2 ms the temporal window must be larger in order to avoid false alarms).In the proposed approach, we adopted identical temporal windows, 400ms long, for the first and second layer. Using a frame rate of 10Hz this corresponds to the integration of 5 frames and a majority voting of 3 over 5 consecutive frames.

@&#CONCLUSIONS@&#
Summarizing, the main contribution of this paper is the proposal of an novel method, based on binary iconic classifiers and achieving good levels of accuracy and real time performance, therefore particularly suitable for effective automotive applications. The paper explains how the adoption of complex cues or specific facial features can be efficiently replaced by adopting a generalized model of the inattentive drive, coming from a small dictionary of poses and totally independent from the actual user. With respect to previous work in the field (Masala and Grosso, 2013) several major improvements can be noted: first of all the extension of the database to multiple sessions/multiple users and to real on-board sequences allowed a thorough validation of the approach; secondly the adoption of a dictionary of poses in order to train the Sanger network makes the image-reduction task totally independent from the actual user. Moreover, the proposed method allows for a simple generalization of additional inattention states: yaws or drooping postures can be easily introduced by adding a limited number of new training samples in the dictionary.Concerning weak points, it is worth noting that for both the classification stages an initial training of the system is yet required for each new user; this procedure requires less than one minute of training, which is an acceptable duration, but also requires an active cooperation of the new user, who must simulate both attentive and inattentive states.Current research is devoted to the simplification of this remaining training phase, deriving from the dictionary of poses a generic model of attention, totally independent from the single user, and devising a minimal “user adaptation” procedure, of about 5s, during which the model is adjusted to the iconic appearance of the current user. The approach would also admit an easy extension to the biometric field, serving as a face recognition based security system for the vehicle. In fact the same adaptation procedure could be used to analyze and store peculiar biometric features of the actual user.First results in this sense are encouraging. In particular, it is now clear that an iconic generalization of attention states can be efficiently applied to a small population of users. However, the extension of this approach to very large sets of users requires further investigation.