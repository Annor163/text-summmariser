@&#MAIN-TITLE@&#
An incremental privacy-preservation algorithm for the (k, e)-Anonymous model

@&#HIGHLIGHTS@&#
An efficient algorithm is developed to prevent incremental privacy breach.Only the most recent previously-released data is required for privacy preservation.The solution can always be guaranteed the optimal result.

@&#KEYPHRASES@&#
Privacy preservation,Anonymity,Incremental algorithm,Privacy breach,

@&#ABSTRACT@&#
An important issue to be addressed when data are to be published is data privacy. In this paper, the problem of data privacy based on a prominent privacy model,(k,e)-Anonymous, is addressed. Our scenario is that when a new dataset is to be released, there may be, at the same time, datasets that were released elsewhere. A problem arises because some attackers might obtain multiple versions of the same dataset and compare them with the newly released dataset. Although the privacy of all of the datasets has been well-preserved individually, such a comparison can lead to a privacy breach, which is a so-called “incremental privacy breach”. To address this problem effectively, we first study the characteristics of the effects of multiple dataset releases with a theoretical approach. It has been found that a privacy breach that is subjected to an increment occurs when there is overlap between any parts of the new dataset with any parts of an existing dataset. Based on our proposed studies, a polynomial-time algorithm is proposed. This algorithm needs to consider only one previous version of the dataset, and it can also skip computing the overlapping partitions. Thus, the computational complexity of the proposed algorithm is reduced fromO(nm)to onlyO(pn3)where p is the number of partitions, n is the number of tuples, and m is the number of released datasets. At the same time, the privacy of all of the released datasets as well as the optimal solution can be always guaranteed. In addition, experiment results that illustrate the efficiency of our algorithm on real-world datasets are presented.

@&#INTRODUCTION@&#
Privacy preservation should be the first priority to be addressed for data sharing between business collaborators. Over the past decade, various privacy preservation techniques have been proposed [1–5]. When the data are to be shared, such techniques can be applied prior to sharing, and the privacy-preserved data can be used for such purposes. However, the data are often changed all the time. Applying the privacy preservation techniques to the data each time can result in different versions of privacy-preserved data. Comparing them can lead to a privacy breach, which is called an incremental privacy breach [6].In this paper, we present an algorithm for preserving the privacy of the data when the data are not static, i.e., when the records are appended continuously. The focused privacy preservation model is based on (k, e)-Anonymous [5], which is one of the most prominent models.Based on the(k,e)-Anonymous privacy preservation model, the data to be shared are composed of identifier attributes such as the ID or name, quasi-identifier attributes such as gender or age, and sensitive attributes such as salary or disease [3]. To preserve the privacy, all of the identifier attributes in the data must be removed first. This removal typically leaves only the quasi-identifiers and the sensitive data remain.Thus, the task is to prevent a linkage between the quasi-identifier attributes and the sensitive attributes [3,7–9]. The(k,e)-Anonymous model [5] breaks or un-links the associations by first partitioning the data. Then, the sensitive values are shuffled within each partition. The conditions for the partitioning of the (k, e)-Anonymous model are that the number of distinct sensitive values of each partition must be at least k, and the error of each partition must be at least e. The error is the difference between the maximum and the minimum of the sensitive values in each partition.An algorithm that generates optimal partitioning, i.e., minimizing the sum of the errors, has been proposed in [5]. The complexity of the data partitioning algorithm is O(n2), where n is the number of data tuples.To illustrate the effectiveness of the(k,e)-Anonymous model, let us consider the following example. The data in Fig. 1(a) has the attributes “Age” and “Gender” as the quasi-identifier attributes and the attribute “Salary” as the sensitive attribute. Suppose that the k value is set to 3 and the e value is set to 2000. Then, the data in Fig. 1(a) can be partitioned with respect to the k and the e values, as shown in Fig. 1(b). As a matter of fact, the number of distinct sensitive values in each partition is at least 3, and the difference between the maximum and minimum value of each partition is at least 2000. After the shuffling, it can be seen that the associations between the sensitive attribute and the quasi-identifier attributes are un-linked, and thus the privacy can be preserved.Once the data are to be queried or shared, the privacy-preserved data can be utilized safely. For example, suppose that a query to determine the sum of the salaries of the males is issued. The result of the query from the non privacy-preserved data in Fig. 1(a) is 341,000. At the same time, the result from the privacy-preserved data in Fig. 1c) is 340,000–343,000. It can be seen that there is only a slight difference between the exact answer and the privacy-preserved answer. This type of query can be considered to be an aggregation query, which is fundamental for any further data analysis, e.g., OLAP or machine learning algorithms. Retaining the ability to answer this type of query means that a high utility of the privacy-preserved data can be obtained. In [5], it is reported that the optimal solutions from the(k,e)-Anonymous model can achieve less than 10% of the relative error of the aggregation query.If we consider only an individual privacy-preserved dataset, privacy can be preserved with regard to the k and e values. However, considering two or more versions of the datasets from different points in time could lead to the incremental privacy breach. Let us consider the following example. Let k be 3 and let e be 2000. Suppose that the two versions of privacy-preserved datasets at timet1and timet2are given as in Fig. 2(a) and (b) respectively. (Note that the name attribute in the figure is only for referencing and that both datasets have been privacy-preserved optimally.) Suppose that the attacker knows that Bob’s data, i.e., for a 52-year old male, are collected and exist in the dataset at timet1. Obviously, his data are in partitionp1of thet1-released dataset.Subsequently, when the new version of this dataset is released at timet2, the attacker can observe that Bob’s data are not in partitionp1at this time. The attacker can investigate the partitions in both versions of the dataset and can identify the difference between partitionp1at timet1andt2. Unfortunately, the difference reveals a sensitive value, the salary, of “Bob”, as shown in Fig. 2(c). The other example is when the attacker intersects both versions of the dataset. Fig. 2(d) shows the result of the intersection between partitionp2of both datasets. It obviously can reveal that the sensitive attribute value of “Alice” and “Hank” might be 88,000–89,000. This causes a privacy which causes a privacy breach with regard to the k and e value of the (k, e)-Anonymous model.In this paper, an algorithm that is based on(k,e)-Anonymous to preserve the privacy against an incremental privacy breach is proposed. The proposed algorithm is based on our theoretical studies on the effects of multiple dataset releasing. Such studies can allow us to reduce the computational complexity enormously, especially because it requires considering only the previous version of the dataset against the dataset to be released and not all of the released datasets. At the same time, the privacy is not compromised, i.e., the result of the privacy-preserved dataset is exactly the same as considering all of the released dataset. Thus, the complexity reduces from O(nm) of the naive approach, where n is the number of tuples and m is the number of released datasets, toO(pn3)where p is the number of partitions in the previous dataset. We also present the experiment results to show the efficiency of the proposed algorithm on the real-world dataset, as well as the optimal result and the privacy that can always be guaranteed. Because this paper is an extended version of a previous conference paper in [10], we emphasize the additional contribution as follows:•Theoretical studies have been extended to describe the effects of multiple dataset releasing clearly.More thorough experiments have been conducted to illustrate the efficiency of the proposed algorithm.The organization of this paper is as follows. The related work is presented in the next section. Basic definitions and the problem statement are presented in Section 3. Section 4 presents the theoretical studies on privacy breaching in incremental scenarios. Subsequently, the proposed algorithm is presented in Section 5. The experiment results of our work are presented in Section 6. Finally, we present the conclusions and future work in Section 7.

@&#CONCLUSIONS@&#
In this paper, we address the incremental privacy breach problem based on the(k,e)-Anonymous model. The characteristics of the effects of multiple dataset release are observed theoretically. We propose that an incremental privacy breach occurs when there is overlap between a partition range of a new dataset and any partition of any existing dataset. Based on such studies, we propose a polynomial-time algorithm withO(pn3)in which n is the number of tuples. With the proposed algorithm, only the most-recent previously released dataset must be considered rather than all of the previous datasets. In addition, not all of the partitions in the previous dataset must be considered for an incremental privacy breach determination; some of them can be discarded from further computing by our proposed observation. Based on the fact that our proposed work always guarantees the optimal result, we can conclude that our algorithm is both effective and highly efficient, as is confirmed by the experiment results.In our future work, we will further investigate the systematic problem in which the subset of the datasets is to be queried, and the attackers can attempt to accumulate the knowledge from each query for the attack. Additionally, the concurrency control issue, in which multiple users might add, delete, or query the dataset at the same time, will be considered.