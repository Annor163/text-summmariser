@&#MAIN-TITLE@&#
Testing the accuracy of DEA estimates under endogeneity through a Monte Carlo simulation

@&#HIGHLIGHTS@&#
We define endogeneity as the correlation between one input and the efficiency term.We run a Monte Carlo experiment to measure how endogeneity affects DEA estimations.We find that DEA is robust to negative endogeneity.But, a moderate or high positive endogeneity biases DEA performance.

@&#KEYPHRASES@&#
Data envelopment analysis,Endogeneity,Monte Carlo experiments,

@&#ABSTRACT@&#
Endogeneity, and the distortions on the estimation of economic models that it causes, is a usual problem in the econometrics literature. Although non-parametric methods like Data Envelopment Analysis (DEA) are among the most used techniques for measuring technical efficiency, the effects of such problem on efficiency estimates have received little attention. The aim of this paper is to alert DEA practitioners about the accuracy of their estimates under the presence of endogeneity. For this, first we illustrate the endogeneity problem and its causes in production processes and its implications for the efficiency measurement from a conceptual perspective. Second, using synthetic data generated in a Monte Carlo experiment we evaluate how different levels of positive and negative endogeneity can impair DEA estimations. We conclude that, although DEA is robust to negative endogeneity, a high positive endogeneity level, i.e., the existence of a high positive correlation between one input and the true efficiency level, might bias severely DEA estimates.

@&#INTRODUCTION@&#
The evaluation of the technical efficiency of decision-making units (DMUs) is a crucial component of management decision-making for saving resources and for monitoring and evaluating DMU activity in order to detect better and worse performers and improving results. Public service providers are naturally interested in efficiency assessments since they manage a huge amount of funds and face both increasing demands and financial constraints. Completely unknown production technologies or the frequent use of multiple proxy variables to approximate the real output as a result of the special characteristics of public sector production, which is not fixed upon profit maximization, complicate the estimation of accurate efficiency measures (Bowlin, 1986). In these contexts, non-parametric techniques, and especially dataenvelopmentanalysis (DEA), are the most commonly applied methods for measuring technical efficiency relative to an estimate of an unobserved true frontier in multiple frameworks (Gattoufi, Oral, Kumar, & Reisman, 2004).Within this framework, selecting the appropriate input and output variables to include in the model specification is one of the most critical choices that practitioners will have to undertake in order to obtain reliable efficiency scores. This point have received a lot of attention in the DEA literature over the past decades, where several works have analyzed the effects of different model misspecifications that can be detrimental to DEA estimates (Dyson et al., 2001; Galagedera & Silvapulle, 2003; Natarajan & Johnson, 2011; Pedraja-Chaparro, Salinas-Jimenez, & Smith, 1999; Ruggiero, 2005; Smith, 1997).However, there is another major concern, namely, the presence of endogeneity in the production process, which is frequently overlooked when practitioners apply DEA, even though this issue has received plenty of attention in econometrics. There are several potential sources of endogeneity, including measurement errors, omitted relevant variables or the presence of simultaneity (see Wooldridge, 2012, pp. 519–520). The idea behind this concept is that some inputs are not exogenous to the production model, but are determined within the model. In statistical terms, this phenomenon implies the presence of correlation between the error term and at least one explanatory variable. In the estimation of technical efficiency with frontier techniques, this phenomenon implies that at least one input is correlated with the inefficiency term as pointed out by Peyrache and Coelli (2009).The distortions in the estimation of economic models potentially caused by such endogeneity have been widely studied in the econometrics literature. However, its effects on efficiency measures estimated through nonparametric techniques like DEA have not received much attention. There are only a handful of studies that have addressed this issue and tested the performance of DEA under some kind of endogeneity (Bifulco & Bretschneider, 2001, 2003; Orme & Smith, 1996; Ruggiero, 2003). Using alternative simulation strategies and data generation processes, almost all the above research concludes that DEA estimates can be biased if there is a certain level of correlation between one input and the true efficiency. However, these previous works do not allow to draw general conclusions about the potential distortions of the presence of endogeneity on DEA estimates.As the aim of this paper is to analyze whether endogeneity can bias the results of DEA in order to make practitioners using this technique aware of the accuracy of their estimates, we make an effort to generalize the effect of endogeneity on DEA estimations by expanding the analysis in various directions. In this sense, we simulate both the negative and the positive endogeneity and different intensities of the correlation between one input and the true efficiency. Secondly, we incorporate a more flexible translog production function in addition to the traditional Cobb–Douglas, which fails to capture the potential nonlinear effects of inputs on the output variable.11This can be a significant weakness in complex production frameworks such as education or health provision.Finally, we conduct our simulation by performing a Monte Carlo experiment to provide more robust results.The article is organized as follows. Section 2 introduces some basic concepts of the DEA estimation and the potential effects of endogeneity on its performance from a conceptual viewpoint. Section 3 describes the methodology used to generate the synthetic data in our Monte Carlo experimental design. Section 4 presents the main results of the analysis. Finally, the paper concludes with a discussion of the main implications of our findings for practitioners using DEA to measure technical efficiency in different contexts and some directions for future research.The analysis of data in the presence of endogeneity is one of the main recent contributions of econometrics to statistical science (Blundell & Powell, 2003). Consider the multiple-input single-output production function:(1)yi=f(xi)+ɛii=1,2,…,nwhereyiis the level of observed output for DMU i, f is an unknown production function to be estimated, xi∈ ℜmis the vector of observable inputs and ɛirepresents the unobservable error term, which can also be identified as the distance to the true productive frontier. In fact, if we limit the estimation of f to the non-stochastic frontier models, we can assume that all those deviations are due to technical inefficiency and therefore ɛi≤ 0 ∀ i = 1, …, n.In order to estimateEq. (1) using a regression model, some crucial assumptions are required, including that the error term is uncorrelated with all the observed inputs E(ɛ|X) = 0; i.e., all regressors must be exogenous. In this context, the presence of endogeneity implies that xiand ɛiare correlated, thus the latter assumption cannot be hold in practiceE(ɛ|X) ≠ 0. This phenomenon can arise as the result of multiple sources, such as measurement errors, the omission of relevant variables in the model specification, although perhaps the most common cause is the presence of simultaneity or two-way causal relationships between the dependent and independent variables (e.g. see Wooldridge, 2012).The education sector is a good example to illustrate this issue, since the endogeneity problem arises very frequently (Mayston, 2003). Actually, in this framework the presence of the self-selection problem has been argued to be the basis for multiple theoretical and empirical critiques of traditional findings using conventional econometric techniques. As a result, multiple methods have been developed in the literature to deal with this problem (Schlotter, Schwerdt, & Woessmann, 2011). In this context, it is claimed that schools with better academic outcomes tend to attract relatively more advantaged students from a high socio-economic background. If parents motivation is correlated with socio-economic level, such pupils (and thus the school they attend) will tend to obtain better academic results for two reasons. Firstly, socio-economic level is an essential input for producing educational output. Secondly, parents and students motivation (which is unobserved) has a positive effect on school efficiency. Consequently, we will observe that schools whose students are from a high socio-economic background are more prone to be fully efficient. This mechanism results in a positive correlation between socio-economic background (input) and technical efficiency: E(ɛ|X) > 0. The same reasoning can be applied for the selection of schools by teachers in many public education systems, because highly qualified and motivated teachers self-select into the best schools (those with a high percentage of students from higher income families a lower ratio of repeat students and disadvantaged pupils and better facilities) and also have a positive effect on student results at those schools.However, the endogeneity problem can also arise in the opposite direction when a direct negative feedback from outputs to resources is observed (simultaneity). This applies, for instance, when school funding systems operating compensatory policies allocate more resources to schools with poorer academic results in order to improve the performance of these schools (Orme & Smith, 1996). If poorer results are due to a high inefficiency, then the ‘reverse causality’ problem implies allocating more resources to inefficient schools causing a negative correlation between resources (input) and true efficiency: E(ɛ|X) < 0.The endogeneity problem has also been a factor considered recently in the estimation of technical efficiency with parametric frontier techniques. For example, Solis, Bravo‐Ureta, and Quiroga (2007) employ a switching regression model to handle the selection bias in hillside farmers under different levels of adoption of soil conservation in El Salvador and Honduras. Greene (2010) proposes a simple way to extend the Heckman sample selection model to stochastic frontier analysis and apply it to measure state health system performance. Perelman and Santín (2011) address the endogeneity problem of school choice in Spain with instrumental variables. Finally, Mayen, Balagtas, and Alexander (2010), Bravo-Ureta, Greene, and Solís (2012)22This paper also applies the Greene's (2010) procedure.and Crespo-Cebada, Pedraja-Chaparro, and Santín (2014) apply propensity score matching to American dairy farms, farmers in Honduras and schools in Spain, respectively.Nevertheless, the potential distortions that endogeneity may cause in the measurement of technical efficiency using nonparametric techniques have received much less attention in the literature. In principle, it might seem that DEA should not be influenced by this problem, since it constructs a boundary around feasible combinations of inputs and outputs without assuming a parametric functional form (Orme & Smith, 1996). However, if we apply insights from Kuosmanen and Johnson (2010) and interpret the DEA model as a constrained variant of the convex nonparametric least squares regression (Kuosmanen, 2008), we can derive straightforward that the same problems of bias caused by the presence of endogeneity explained above can also arise within this approach.According to Banker (1993), the variable returns to scale DEA estimator of a production function f can be formally defined as:(2)fDEA(x)=maxλ∈ℜ+n{y|y=∑i=1nλiyi;x≥∑i=1nλixi;∑i=1nλi=1;λi≥0∀i=1,…,n}Substituting f inEq. (1) by the fDEA estimator, we can observe that theɛiDEAefficiency scores for each unit can also be obtained as the optimal solution to the following LP problem (additive DEA efficiency measure):(3)ɛiDEA=minλ,ɛ{ɛ|y=∑i=1nλiyi+ɛ;x≥∑i=1nλixi;∑i=1nλi=1;λi≥0∀i=1,…,n}As Kuosmanen and Johnson (2010) set, in the single-output setting, formulation (3) is very similar to the multiplicative standard output-oriented DEA under VRS proposed by Banker, Charnes, and Cooper (1984):(4)ϕiDEA=maxλ,ϕ{ϕ|ϕyri≤∑i=1nλiyi;xki≥∑i=1nλixi;∑i=1nλi=1;λi≥0∀i=1,…,n}where xkdenotes input k, yrstands for output r and i represents the production units. Multipliers λiare referred as intensity weights of each DMU determined by the program solution. The technical efficiency score of the ith DMU is equal or greater than one, whereϕ^i=1represents an efficient unit, whereasϕ^i>1indicates that the ith DMU is inefficient.In the single-output setting formulations (3) and (4) are equivalent in the sense thatϕiDEA=1−ɛiDEA/yi≥1. In fact, the authors demonstrate in their work that the DEA problem can be interpreted as a nonparametric least-squares model under the assumption thatɛi≤0(Kuosmanen & Johnson, 2010, p. 152). This connection between the nonparametric regressions and the mathematical programming approaches contributes to developing the statistical foundation of DEA. As a result, we can derive that DEA estimators will be consistent if all the assumptions in the least-squares regression model are fulfilled. However, in the case that ɛiis correlated with at least one input, the assumption E(ɛ|X) = 0 cannot be hold and, therefore, efficiency estimatesϕ^ican be biased. To better understand these ideas hereinafter we graphically illustrate this problem.Fig. 1represents a single-input (x)/single-output (y) production setting in which true efficiency ϕiis exogenously distributed, i.e., E(ϕ|X) = 0. In this scenario, the frontier estimated by DEA is very similar to the true one for the entire data range. Fully efficient DMUs are correctly identified, and efficiency is spread along the production frontier.However, as noted above, we may well find, in real-world production processes, some kind of correlation between the true efficiency and the level of input that is significantly different from zero: E(ϕ|X) ≠ 0. This correlation can be either positive or negative, as mentioned previously in the examples of different educational settings. Fig. 2illustrates the situation where endogeneity is positive, E(ϕ|X) > 0.In this case, although microeconomic theory establishes that the input level and the true efficiency are independently distributed, the existence of this positive endogeneity can break this assumption. According to the true frontier, DMUs with higher levels of input (and outputs), e.g., dots C and D in Fig. 2, are fully efficient units, whereas DMUs with lower input levels are less efficient. However, DEA estimates relative efficiency scores based on observed data and therefore the frontier built by DEA will find and classify some DMUs that have a low input level and are really highly inefficient as efficient. This is the case of dots A and B in Fig. 2, which are actually very far away from the true frontier but are identified by DEA as efficient units. Consequently, the border estimated by DEA will be far removed from the true one in the lower input frontier region. This means that efficiency improvement targets will be more demanding for observations with a higher input level than for those with a low input level. For example, while unit E is clearly closer than unit F to the true frontier in terms of output, both units appear to have a similar estimated technical efficiency because the actual production frontier is wrongly identified. Since efficiency scores are relative measures, the misidentification of some DMUs distorts all efficiency estimates and the performance ranking. This result could have very important implications, particularly, if DEA is conducted for benchmarking and policy making.On the other hand, the existence of a significant negative correlation between the input level and the true efficiency seems to just slightly affect DEA estimates. Fig. 3illustrates this context where in terms of the true production frontier more efficient units show low input levels and more inefficient units are those with high input levels.It is worth to note, that at the region of high input level the estimated frontier shifts slightly down from the true one. But in this case the most inefficient DMUs (those with high input level) remain far enough away from the DEA estimated frontier to be still identified as the most inefficient producers compared with other DMUs. The main reason for mantaining the high relative distance to the frontier for high input level DMUs is the monotonicity assumption in DEA estimations.33Monotonicity in production frontiers implies that an efficient DMU cannot reduce (increase) the vector of outputs (inputs) holding the vector of inputs (outputs) fixed while it still belongs to the frontier.Monotonicity impedes DEA to pursue downwards inefficient DMUs to drawing the estimated production frontier. In addition, the negative correlation between the input and the technical efficiency provides more information to DEA in order to correctly identify and estimate DMUs efficiency. The reason is that negative endogeneity reinforces the major microeconomic assumption behind the measurement of efficiency, i.e. for a constant output level using higher level of inputs implies greater inefficiency. For instance, unit G is highly inefficient, and although the DEA frontier is closer to G than the true one, the distance between G and the estimated frontier is still large enough in terms of output to be recognized as one of the most inefficient producers. In general, inefficiency is correctly identified since all DMUs keep their relative position. Thus, it is expected that under the presence of negative endogeneity the efficiency scores estimated by DEA will better match the true relative positions, and thus the estimated ranking will not be significantly different from the true one. However, negative endogeneity could move upwards average efficiency scores because now the DEA frontier is closer to the most inefficient DMUs than the true one.Now, the potential implications of these problems in quantitative terms have to be measured from a theoretical point of view. On this ground, we test DEA performance under endogeneity of different signs and intensity in Section 3. To do this, we simulate endogeneity through a significant positive and negative correlation between the true technical efficiency and one input.In order to illustrate the ideas developed above, we perform a Monte Carlo experiment applied to seven scenarios. Firstly, we use a data generation process (DGP) to create a baseline dataset without endogeneity (the exogenous scenario). Secondly, we simulate six alternative settings taking into account the correlation between the true efficiency (θi) and one observed input (the endogenous scenarios). Results from each endogenous scenario are then compared to the baseline scenario in order to measure the effects that endogeneity causes on DEA estimations. All datasets were defined in a single output framework with three inputs.Almost all previous studies in the literature have simulated data using the Cobb–Douglas production technology. For the sake of comparability, we also draw data from a Cobb–Douglas function with a single output and three inputs:(5)lnyi=α1lnx1i+α2lnx2i+α3lnx3iwhere yirepresents the output, and x1, x2 and x3 are the observed inputs. The inputs weights assigned in this work where α1 = 0.3, α2 = 0.35 and α3 = 0.35, assuming constant returns to scale.44Similar results were obtained using increasing returns to scale and decreasing returns to scale.Although this functional form is the most commonly used in economics and operational research, the assumption of constant input–output elasticities probably is a significant restriction for real world estimations. This means that regardless the scale of production, the marginal effects of inputs on outputs are the same. Therefore, the Cobb–Douglas production function fails to capture potential nonlinear effects of those resources. Since the main aim of this work is to test the accuracy of DEA in an experimental setting that reproduces a more realistic context, we also consider a more flexible technology in our experimental design, namely, the translog production function introduced by Christensen, Jorgenson, and Lau (1971):(6)lnyi=β0+∑k=1Kβklnxki+12∑k=1K∑j=1Kβkjlnxkilnxjiwhere  y  denotes the output and xk(k = 1, 2, 3) are the three inputs. We assumeβ0=3.5;β1=0.5;β2=0.3;β3=0.5;β11=−0.1;β22=−0.05;β33=−0.1;β12=β13=β23=0.01. These parameters were defined in order to obtain a well-behaved production function within the bounds imposed by the inputs distribution that are uniformly distributed over the interval [5, 50]. Therefore, after having generated the data we checked for two desirable conditions at each simulated data point.Firstly, we verify the monotonicity condition, where in a single output case requires that all marginal products must be non-negative ∂y/∂xk≥ 0∀k. For the translog production function this implies:(7)∂y/∂xk=yxk·∂lny∂lnxk=yxk·{βk+∑j=1Kβkjlnxj}=yxk·sk≥0∀kwhere y/xkis the average product and skis the elasticity of y with respect to xk. As the average product y/xkis always positive, monotonicity implies that all input-output elasticities skmust be non-negative for all DMUs across all inputs range.Secondly, we checked for concavity in all inputs, which implies that all marginal products must be nonincreasing, i.e., the law of diminishing marginal productivity (Coelli, Prasada-Rao, O'Donnell, & Battese, 2005). For the translog production function, all inputs must satisfy the following expression across the entire simulated data range:(8)∂2y/∂2xk=yxk2[βkk+(βk−1+∑j=1Kβkjlnxj)×(βk+∑j=1Kβkjlnxj)]=yxk2[βkk+(sk−1)sk]<0∀kFinally, the selected parameters and the distribution of inputs define the production scale elasticity. We perform the simulation assuming decreasing returns to scale (DRS), where scale elasticity ranges from 0.56 to 0.97, with a mean value of 0.69. These results are consistent with most complex production processes that take place in the real world. In the field of education, for example, if the initial school input endowments are all doubled, it would be reasonable to expect a less-than-double increase in students’ test scores, particularly at high levels of educational attainment (Essid, Ouellette, & Vigeant, 2013).The baseline scenario represents the exogenous case, where all inputs are uncorrelated with the true technical efficiency, and it is simulated using the following procedure:1.Generate randomly and independently three input vectors x1, x2 and x3 using a uniform distribution over the interval [5, 50] for N DMUs, n = 1, 2, … , N.Calculate the efficient level of output as yi= exp (ln  yi) using ln  yi= f( · ), where f( · ) is Eq. (5) or Eq. (6) respectively.Draw a random error term νifrom a N(0; 0.04), representing the random statistical perturbation in the production function. Since the main aim of this research is to test the performance of DEA under endogeneity, we do not simulate different magnitudes of the random shocks. As it has been demonstrated in previous studies, the larger the measurement error the poorer the performance of DEA (e.g.Bifulco & Bretshneider, 2001). Therefore, we chose a small measurement error in order to have, as in the real world, some noise but not so large that distorts the analysis of endogeneity.Randomly and independently generate N values of uiusing a half-normal distributionui≈|N(0;0.25)|and compute the vector ϕi= exp (ui). Then, compute the true technical efficiency level for each DMU0≤θi=1ϕi≤1.Compute the observed output as:y^i=yi·exp(vi)·θi.The remaining six scenarios were developed through a similar DGP, but taking into account the existence of endogeneity, which was modeled through the Pearson's correlation coefficient between the true technical efficiency θiand one observed input. Therefore, in each dataset we substitute the exogenous input x3 by an endogenous input E. In order to compute the latter with the same distribution as the exogenous inputs (x1, x2, x3) and with a specific level of correlation with  θi, for each endogenous scenario we follow the next procedure:1.Select the desired Pearson's correlation coefficient ρE, θbetween the endogenous input E and the true technical efficiency θ.Draw a random matrix A = (a1, a2) from a multivariate normal distributionN(0;Σ),whereΣ=[1ρE,θρE,θ1].Compute an identification number variable (ID) from 1 to N.Match the ID with the vector a1 obtaining: B = [IDa1]. Sort B by a1 in an ascending order (the ID variable will be unsorted): B' = [IDa1a1].Generate an independent vector x(n × 1) from a uniform distribution over the interval [5, 50] and sort it in an ascending order obtaining xs.Compute a new C matrix by merging B′ with xs: C = [IDa1a1xs].Sort C by the ID variable in an ascending order:C′=[IDa1_IDxID].The latter vector of C′(xID), will be defined as the endogenous input, E = xID.Match ID with the vector a2 obtaining: D = [IDa2]. Sort D by a2 in an ascending order (the ID variable will be unsorted): D' = [IDa2a2].Randomly and independently generate N values of uiusing a half-normal distributionui≈|N(0;0.25)|. Then, compute the vector ϕi= exp (ui) and sort this variable in an ascending order obtaining ϕs.Compute a new H matrix by merging D′ with es: H = [IDa2a2ϕs].Sort H by the ID variable in an ascending order:H′=[IDa2_IDϕID].The latter vector of H′(ϕID), is used to computed the true technical efficiency level for each unit,θi=1ϕID. The generated average true efficiency in each experiment ranges from 0.828 to 0.859 with standard deviations values between 0.097 and 0.116.Using the exogenous inputs x1 and x2 generated in the baseline scenario and the endogenous input E, compute the efficient level of output as yiend= exp (ln  yiend) using ln  yiend= f( · ), where f( · ) is Eq. (5) or Eq. (6) respectively.Finally, calculate the observed output using the random term νicomputed in the baseline dataset and the true efficiency level  θicomputed in step 13:y^iend=yiend·exp(vi)·θi.Two factors were allowed to vary in order to generate the six endogenous settings: the sign (negative or positive) and the intensity (high, medium or weak) of the correlation coefficient between the true efficiency and the endogenous inputs (ρθ, E). Table 1 summarizes the main descriptive statistics of the correlation coefficients that have been actually obtained in each simulated scenario.55The correlation coefficients were computed in each experiment and then averaged to obtain the final measures presented in Table 1.All scenarios were replicated using the Cobb–Douglas and the translog technologies for a sample size of 100 DMUs. Finally, for each dataset, we estimate the efficiency scoresθ^iby running an output oriented DEA model under constant and variable returns to scale (CRS and VRS), as proposed by Charnes, Cooper, and Rhodes (1978) and Banker, Charnes, and Cooper (1984) respectively. As a result, 28 scenarios were analyzed (the exogenous scenario, six types of endogeneity, two production technologies and two types of return to scale). In order to make the results more reliable, we ran a Monte Carlo experiment, where B, the number of replicates, was 1000.66Simulations were carried out using MATLAB 7.6.0 software.Consequently, all measures were computed in each replication and, finally, averaged to yield the results reported in Section 4.

@&#CONCLUSIONS@&#
The endogeneity problem and the distortions in the estimation of efficiency and production models that it causes is a growing concern in the frontier analysis field. As a result, some research has started to use conventional econometric approaches to deal with endogeneity in the estimation of production frontiers using parametric techniques. Nevertheless, the effects of endogeneity on efficiency estimates obtained with nonparametric frontier methods like DEA have received less attention in the literature.In this paper, we analyze to what extent the presence of endogeneity in the production process can bias DEA estimates. For this purpose, we simulate different levels of negative and positive endogeneity through the correlation between one input and the true technical efficiency using synthetic data generated in a Monte Carlo experiment. We conduct the analysis using a Cobb–Douglas production technology as well as a more flexible translog specification.We conclude that DEA is robust to the presence of negative and low positive endogeneity. However, a high and medium positive endogeneity, i.e., a high or medium positive correlation between one input and the true efficiency level, significantly biases DEA performance. For example, while for the translog production function in the exogenous scenario DEA–VRS yields a Spearman's correlation coefficient of 0.729 between the estimated and actual efficiency, this correlation drops to 0.594 in the case of medium positive endogeneity and to 0.265 in the presence of high positive endogeneity. We find that this decline in DEA performance is further driven by the misidentification of the most inefficient DMUs. This evidence shows that it is necessary to be extremely cautious if DEA is applied for the purpose of performance-based reforms, because many units which should not be considered as benchmarks for comparison could be wrongly identified as such.Therefore, the main derivation of this paper is to caution DEA practitioners concerning the accuracy of their estimates when they suspect that there might be some significant positive endogeneity in real world problems. Defining a method to detect and to deal with the presence of significant endogeneity in empirical applications falls beyond the scope of the present research, although it remains as a logical next challenge for future research.