@&#MAIN-TITLE@&#
Local and global uncertainty in binary tomographic reconstruction

@&#HIGHLIGHTS@&#
In binary tomography the projection data might be insufficient for an accurate reconstruction.We proposed methods to measure the reliability of the pixels of the reconstructions.We described a method for measuring the global uncertainty of the reconstructed image.We gave additional validation of the results.We proposed some possible applications of the given methods as well.

@&#KEYPHRASES@&#
Binary tomography,Reconstruction,Optimization,Uncertainty,DART,Non-destructive testing,

@&#ABSTRACT@&#
In binary tomography the goal is to reconstruct the inner structure of homogeneous objects from their projections. This is usually required from a low number of projections, which are also likely to be affected by noise and measurement errors. In general, the distorted and incomplete projection data holds insufficient information for the correct reconstruction of the original object.In this paper, we describe two methods for approximating the local uncertainty of the reconstructions, i.e., identifying how the information stored in the projections determine each part of the reconstructed image. These methods can measure the uncertainty of the reconstruction without any knowledge from the original object itself. Moreover, we provide a global uncertainty measure that can assess the information content of a projection set and predict the error to be expected in the reconstruction of a homogeneous object. We also give an experimental evaluation of our proposed methods, mention some of their possible applications, and describe how the uncertainty measure can be used to improve the performance of the DART reconstruction algorithm.

@&#INTRODUCTION@&#
Tomography[11,14] is a collection of imaging and image processing techniques for discovering the inner structure of objects from their projections. In transmission tomography, projections are gathered by exposing the object of study to some kind of electromagnetic or particle radiation, and measuring the loss of energy of the beams passing through it. Provided that sufficient measured data is available, one can determine the attenuation coefficient at each point of the object.In discrete tomography[12,13] we assume that the object to be reconstructed consists of only few different materials with known attenuation coefficients. Moreover, in the special case called binary tomography our aim is to detect the presence or absence of one single material at each position. With this prior information, it is possible to accurately reconstruct objects from only few (usually not more than 10–12) projections.However, in practical applications, limitations on the number of projections (as acquiring them can have a high cost, or cause unwanted damaging effects on the object of study) can make it impossible to gather a sufficient amount of projection data, even for binary tomography. Furthermore, even if we have an adequate projection set, we can find ourselves against computational problems, since the general case of the binary reconstruction problem is proved to be NP-hard, if the number of projections is more than two [10]. Finally, projections are always affected by some type of stochastic noise, that should be considered in the reconstruction [1,2,6].Because of the problems mentioned above, the evaluation of the projection data can be beneficial in practical applications of binary tomography, and new algorithms are developed for this purpose. For example, in [3], authors gave an upper bound on the variability of binary reconstructions from a given projection set, that determines a bound for the expected accuracy of the reconstructed results, too.In this paper, we provide two methods to investigate the local uncertainty in binary reconstructions. They can measure whether a given projection set provides sufficient information for determining the pixel values of the binary image representing the object of study. We analyse the performance of these measurement techniques in a set of experimental tests. We also derive a formula to summarize the local uncertainties into a global measure, that describes the overall information content of a projection set. Finally, we give some possible applications of our proposed methods.This work is based on our previous contribution [21] which was significantly extended here by the global uncertainty measurement of the projection sets, additional validation of the results, and the description of possible applications.The structure of the paper is the following. In Section 2, we give a brief explanation of the reconstruction problem and its algebraic formulation, and describe the foundations of the uncertainty problem. Then, in Section 3, we describe the concept of non-discrete reconstruction and the entropy map, which we will use for approximating the uncertainty of reconstructions. We also provide two methods for calculating these measures. In Section 4, we outline a test environment for evaluating the uncertainty measures and present experimental results. Then, in Section 5, we propose a formula that summarizes the local entropy map into a global measure, that we call cumulated entropy, and also give an experimental validation of this measure. In Section 6, we briefly describe how the noise on the projection values affect the uncertainty measure. In Section 7, we mention some possible applications of our methods in non-destructive testing of objects, adaptive projection acquisition, and improving the DART algorithm [5]. Finally, Section 8 is for the conclusion.We present our results for the two-dimensional case of binary tomography. The described methods can be extended to higher dimensions in a straightforward way. We use the algebraic formulation of binary tomography (see, e.g., chapter 7 of [14]), and assume that the object to be reconstructed is represented in a two dimensional image. Without the loss of generality, we assume that the image is of sizen×n(as any rectangular image can be padded to a square shape), and assume that the projection values are given by a finite number of projection rays which interact with the reconstructed pixels. With these assumptions the noiseless binary reconstruction problem can be written in a form of a linear equation system(1)Ax=b,x∈{0,1}n2,where•xis the vector of alln2unknown image pixels,bis the vector of all m projection values,Ais a projection coefficient matrix of sizem×n2, that describes the projection geometry by allaijelements representing the intersection between the i-th projection line and the j-th pixel.With this formulation, the noiseless reconstruction problem gains a versatile description, since any projection geometry (e.g. the well-known parallel beam or fan beam, or even more complex ones) can be applied, as long as the relation between the projections and the image pixels is linear, i.e., a correspondingAprojection coefficient matrix exists.If there is a reconstruction satisfying the projections, then one can obtain a solution to the noiseless reconstruction problem by solving (1). Although this model is correct in the mathematical sense, it is hardly applicable in practice. The projection data might be distorted by measurement errors causing the equation system to be inconsistent. In this case, no exact reconstruction exists, and only approximate solutions of (1) can be found. A common way for coping with this problem is to minimize(2)‖Ax-b‖22,x∈[0,1]n,that is equivalent to finding the solution of (1) in the noiseless case, while in the noisy case it provides a reconstruction which satisfies the projections the best – in an Euclidean sense.A further problem could be if we cannot gain enough projections and the system of equations in (1) becomes underdetermined, meaning that many (severely different) reconstructions can satisfy the projections. This causes uncertainty in the reconstruction, where the projections do not determine the exact values of the pixels. In a practical application this also means that we cannot decide if a reconstructed image shows the real structure of the object of study or we got a false result due to the incomplete data. For such cases it would be beneficial to have methods for evaluating the reliability of the projection data.In this section, we define the uncertainty for the projections in binary tomography assuming that all the solutions are known. We demonstrate our definition in some well-defined cases, then we give two methods to approximate the uncertainty without determining the set of all solutions. For these methods, we also present the results on the well-defined cases.Assume that the set of binary solutions of the systemAx=bis not empty and can be determined for the given vectorbof projections. The probability ofxi=1can be written as(3)pi=NbA(xi=1)NbA,whereNbAdenotes the number of all binary solutions, andNbA(xi=1)denotes the number of binary solutions withxi=1for the given reconstruction problem determined byAandb.Clearly, ifpiis close to 1 then thexipixel is more likely to take value 1 than 0. The probability ofxi=0denoted byp¯ican be defined in the same way. By the definition of the entropy usingpiandp¯iwe can calculate the information content ofxias(4)H(xi)=-(pilog2(pi)+(p¯i)log2(p¯i)).The valueH(xi)shows us, how much the given pixel is determined or how uncertain it is (ifH(xi)=0then the pixel is fully determined by the projections, and pixels with anH(xi)=1value are maximally uncertain). The image composed of these entropy values for allxiis called the entropy map.In general, there may be exponentially many possible reconstructions satisfying the projections. Furthermore, it could be also NP-hard to find even just one possible reconstruction. Thus, it is often not feasible to compute the entropy maps. On the other hand, in some special cases, the entropy maps can be precisely determined. In Fig. 1we gave three special phantoms whose exact entropy maps can be determined if only the horizontal and vertical projections are used.In case of the phantom of Fig. 1a all the horizontal and vertical projection values are equal to one. It can be easily seen that the total number of possible reconstructions isn!. Furthermore, if we choose one pixel and fix its value to 1, then all other pixels in the row and column of the chosen pixel will be equally 0. By deleting this row and column, we can reduce the problem to reconstructing a binary matrix of size(n-1)×(n-1), having all horizontal and vertical projection values equal to 1. This problem has(n-1)!solutions. Hence the probability of one pixel taking a 1 value in a reconstruction is(n-1)!n!=1n. Thus, the entropy map is homogeneous with(5)H(xi)=-1n·log21n+1-1n·log21-1n.The second phantom in Fig. 1b shows the image of a square, that is much simpler to reconstruct. From the horizontal and vertical projections, this object can be uniquely reconstructed, hence all the pixels in its entropy map are equal to zero. The third special phantom, shown in Fig. 1c, is similar to the square, but its border holds four extra object points, which yield two switching components. As a consequence, there are four possible reconstructions satisfying the projections, and on the uncertainty maps all pixels have a zero value, except the eight pixels having an uncertainty of one. The uncertainty map of this image from two projections is shown in Fig. 2.As mentioned above, the determination of an exact entropy map by taking all the reconstructions into account is usually practically impossible, hence – instead of using (4) – the entropy map should be approximated. The idea of our uncertainty approximation is based on finding so-called non-discrete reconstructions for the given set of projections, and calculating the entropy of each pixel in these reconstructions.The concept behind the non-discrete reconstruction is based on the fact that we have two goals in binary reconstructions. On one hand, we want to find results which satisfy the projections. On the other hand, we try to find binary solutions. The non-discrete reconstruction will (approximately) satisfy the projections, but will also contain non-binary pixel values to represent how easy it is to move the pixel values from the binary domain. By this, as we will see, one can measure the vagueness of the pixel values and determine their uncertainty.In the ideal (noiseless) case, one possible way to get non-discrete reconstructions is to solve the following minimization problem(6)Minimize:x-12e22,Subject to:Ax=b,andx∈[0,1]n2,whereestands for a vector with alln2positions having value 1. If the projection set is inconsistent, then the conditionAx=bin (6) should be exchanged to minimizing‖Ax-b‖22. This approach examines the reconstruction that is as far from the binary domain as possible, and by this, measures how easy it is to move the pixel values from the binary domain.We approximated a solution of (6) with a bounded version of the Simultaneous Iterative Reconstruction Technique (SIRT) [11,14]. SIRT is an iterative process where we start out from a given initial image and approximate the correct reconstruction by iteratively subtracting the back-projected error of the intermediate state from itself. In general, this method gives a continuous reconstruction with real pixel values and produces an image which is the closest one (in the Euclidean sense) to the initial image. We slightly modified this algorithm by truncating the pixel values in each iteration, to the[0,1]interval. The pseudo code of this algorithm is given in Algorithm 1.Algorithm 1Bounded Simultaneous Iterative Reconstruction TechniqueInput:Aprojection matrix;bexpected projection values;x(0)initial solution;∊step size bound;kmaxmaximal iteration count1:k←02: repeat3:v(k)←(Ax(k)-b)4:for alli∈1,…,n2do5:yi(k+1)←xi(k)-1∑j=1maji∑j=1majivj(k)∑l=1najl6:xik+1←0,ifyik+1<0yik+1,if0⩽yik+1⩽11,if1<yik+17:end for8:k←k+19: until‖x(k+1)-x(k)‖22<∊ork>kmax10: returnx(k)We processed the least binary reconstruction by performing the bounded SIRT algorithm from the(0.5,…,0.5)Tstarting point. Then, from itsxˆoutput we produced the entropy maps by using the output pixel values as the probability values given in (3). That is, we calculated the approximate entropy maps as(7)Hˆ(xi)=-(xˆilog2(xˆi)+(1-xˆi)log2(1-xˆi)).To validate our method we investigated it on the special phantoms shown in Fig. 1. The calculated approximate entropy maps can be observed in Fig. 3. In case of Fig. 1a the width and height of the image wasn=32. The approximation of the entropy map can be seen in Fig. 3a, where all the pixel uncertainty are equal to-132·log2132+1-132·log21-132≈0.2006. That is exactly the same as the real, theoretically deduced entropy map. In case of phantom of Fig. 1b we got an approximate entropy map with all pixels having zero value, which, again, coincide with the theoretical entropy. Finally, for the phantom of Fig. 1c, our approximate uncertainty map again produced the theoretically correct uncertainties, by giving uncertainty values of 1 for the eight uncertain pixels, and zero values otherwise.Summarizing the results for the special phantoms, we can say that the uncertainty measure calculated by the proposed approximation was equal to the theoretical uncertainty, in all of our tests. Unfortunately, we could only validate the method for some special cases, and other structures might exist where our method does not work properly. Nevertheless, we must mention that we have not found such a counter example so far.An alternative to calculate non-discrete reconstructions is a random sampling of the set of possible reconstructions belonging to the same set of projections, and then averaging the intensity in all those samples, pixelwisely. A similar approach for the random sampling of the space of reconstructions were previously applied in [4], where the authors generated random blobs for initializing continuous SIRT reconstructions.We did the random sampling by using a Simulated Annealing based reconstruction method that is the slightly modified version of the algorithm described in [22]. It performs the reconstruction by minimizing an energy function of the form(8)C(x)=‖Ax-b‖22,x∈{0,1}n2,with Simulated Annealing [17]. The pseudo code of this method is given in Algorithm 2.Algorithm 2Reconstruction algorithm based on Simulated AnnealingInput:Aprojection matrix;bexpected projection values;Tstart,Tminstarting and minimum temperatures;Tfactormultiplicative constant for reducing temperature;Robjectivebound for stopping criteria based on the ratio of the final and starting energy function value1:x←(0,…,0)T{set an initial state}2:T←Tstart{set the starting temperature}3:Cstart←Cold←‖Ax-b‖22{calculate the energy of the starting state}4: repeat5:fori=0to ndo6: choose a random position j in the vectorx7:x̃←x{make a copy of the current state}8:xj̃←(1-xj){alter the randomly chosen state}9:Cnew←‖Ax̃-b‖22{calculate the energy after the modification}10:z←Random([0,1]){generate a random number from the[0,1]interval with uniform distribution}11:ΔC←(Cnew-Cold){calculate the change of the energy}12:ifΔC<0orexp(-ΔC/T)>zthen13:x←x̃{accept the new state with a probability based on the energy change and temperature}14:Cold=Cnew15:end if16:end for17:k←k+118:T←T·Tfactor{lower the temperature}19: untilT⩽TminorCnew/Cstart⩽RobjectiveDue to the stochastic nature of this process, with each run of Algorithm 2 we get a random element of the possible reconstructions. If we perform several reconstructions from the same (insufficient) projection data, each result of the algorithm might be different. Then, the same pixels in different results may also differ and an averaging of the pixel values through several reconstructions will describe the variability of the pixel values. Pixels with an average value close to 0 or 1 have the same value in most reconstructions, while pixels with a 0.5 average are uncertain. Now, we can use the averaged value of several different runs of Algorithm 2 in a similar way as in the case of non-discrete reconstructions, and calculate approximate entropy maps from them.As well as in the case of the non-discrete reconstructions, we present our results of the randomized approach on the entropy maps of the special phantoms in Fig. 1. Due to the stochastic nature of the Simulated Annealing we compared the uncertainty maps of the Simulated Annealing based method and the theoretically derived uncertainty maps in a statistical way.For the phantom in Fig. 1a, we measured the average and standard deviation of the pixel values. By a simple calculation we got that the theoretical uncertainty value is about0.2005, in case of a 32 by 32 sized image (i.e., withn=32). In our tests, in the experimentally gained entropy map the average pixel uncertainty value was0.1930, and the standard deviation was0.0078. Thus the difference is not significant. The image of the approximated entropies can be seen in Fig. 4a.In case of the phantom in Fig. 1b, the expected uncertainty value is 0 for each pixel, and the average of the calculated pixel entropies was 0, with a 0 valued standard deviation, that is an exact match (as it can be seen in Fig. 4b).With the phantom of Fig. 1c we made two types of statistics because there are two types of pixels on the theoretical uncertainty maps. In most of the pixels, the uncertainty value should be 0 (let us call these pixels fixed pixels). On the other hand, there are eight pixels, which should take uncertainty values of 1 (we will call these pixels vague pixels for now). As for the statistics, the average approximated values of the fixed pixels in our test was0.0096, with a0.0009standard deviation, and for the vague pixels, this average was0.9878, with a standard deviation of0.0002. This again shows a good approximation of the entropy map. Fig. 4c shows the uncertainty map generated in this case.In conclusion, our tests revealed that the results of both the Simulated Annealing based method and the bounded SIRT are in accordance with the theoretically derived entropy maps.In this section we will show, via experimental tests, that there is a strong relationship between the entropy maps gained from Algorithms 1 and 2.We took 22 phantom images, produced their projection sets with different numbers of projections, and computed the pixel entropies from the given data with Algorithms 1 and 2. Some of the sample images can be seen in Fig. 5. To perform the computation, the parameters of Algorithm 1 were set empirically. As mentioned before we needed to use the initialx0=(0.5,…,0.5)Tvector in the beginning of the process, and the process was stopped in the k-th iteration, if‖xk+1-xk‖2became less than0.001or k was equal to 5000.As for the parameters of the Simulated Annealing based method, we used the parameter settings as described in [22], except that we did not apply a smoothness regularization term in the process. More exactly, the parameter values wereTstart=4.0,Tmin=10-14,Tfactor=0.97,Robjective=10-5. Moreover, for each given projection set we averaged 100 runs of the optimization process to approximate the entropy maps explained in Section 3.2.2.In order to compare the outputs of Algorithms 1 and 2 we used the average pixel difference(9)D(x,y)=1n2∑i=1n2|xi-yi|,that is a measure for the difference of two imagesxandy. This measure takes values between 0 and 1. If the two images are identical, thenD(x,y)takes a value of 0, and this value increases as the difference between the two images gets more and more significant.

@&#CONCLUSIONS@&#
