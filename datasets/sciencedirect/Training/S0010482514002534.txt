@&#MAIN-TITLE@&#
The L

@&#HIGHLIGHTS@&#
We propose a L1/2 penalized accelerated failure time (AFT) model.A coordinate descent algorithm with renewed L1/2 threshold is developed.The L1/2 penalized AFT model is able to reduce the size of the predictor in practice.The classifier based on the model is suitable for the high dimension biological data.

@&#KEYPHRASES@&#
Survival analysis,Regularization,Variable selection,Accelerated failure time model,L,1/2 penalty,

@&#ABSTRACT@&#
The analysis of high-dimensional and low-sample size microarray data for survival analysis of cancer patients is an important problem. It is a huge challenge to select the significantly relevant bio-marks from microarray gene expression datasets, in which the number of genes is far more than the size of samples. In this article, we develop a robust prediction approach for survival time of patient by a L1/2 regularization estimator with the accelerated failure time (AFT) model. The L1/2 regularization could be seen as a typical delegate of Lq(0<q<1) regularization methods and it has shown many attractive features. In order to optimize the problem of the relevant gene selection in high-dimensional biological data, we implemented the L1/2 regularized AFT model by the coordinate descent algorithm with a renewed half thresholding operator. The results of the simulation experiment showed that we could obtain more accurate and sparse predictor for survival analysis by the L1/2 regularized AFT model compared with other L1 type regularization methods. The proposed procedures are applied to five real DNA microarray datasets to efficiently predict the survival time of patient based on a set of clinical prognostic factors and gene signatures.

@&#INTRODUCTION@&#
The analysis high-dimensional and low-sample size microarray data for survival analysis of cancer patients is an attractive problem. The identification of phenotype genes associated with tumors may be helpful to discover novel information and a new way for clinical research, diagnosis, choice of therapy, prognosis, and prediction of the survival time and even may be helpful to the search the new targets in the field of drug development. It is a huge challenge to select the significantly relevant bio-marks from microarray gene expression datasets, in which the number of genes is far more than the size of samples. In the last two decades, some dimensional reduction approaches have been proposed in the literature. For instance, the principal component regression [1] and the partial least square methods [2] optimize the tradeoff between bias and variances through the low-dimensional projections of the genes and thus minimize the mean squared errors (MSEs) [3]. On the other hand, regularization procedures, which combine the regression loss function and the penalty function together to optimize, have become widely studied and used tools for incorporating high-dimensional and low-sample size data. The advantage of the regularization methods is to achieve a minimization of the regression errors and to select the relevant variables simultaneously through generating sparse solutions. The extensively used regularized estimation methods include the ridge regression [4], the regularizing support vector machine [5], the Lasso [6], the gradient directed regularization method [7], the adaptive Lasso [8,9], the smoothly clipped absolute deviation (SCAD) [10], the minimax concave penalty (MCP) [11] and so on. Moreover, some efficient optimization algorithms for regularization implementations have been proposed, such as the gradient Lasso [12] and the least angle regression (LARS) [13].In practice for survival analysis, the Cox proportional hazards (Cox) model and the accelerated failure time (AFT) model are widely used to study the relationship between key risk predictors and survival time. Here the key risk predictors are only a small number of genes selected from microarray data and the survival time includes both complete and censored cases. The regularized Cox regression methods proposed by Tibshirani [14] and Gui and Li [15,16], which adds a L1 penalty function to the Cox partial likelihood. Regularized methods for combining AFT model to predict failure time outcomes are less well developed. However, the proportional hazards assumption for the Cox model may not be suitable for some particular applications. Meanwhile, the most important goal of the clinical research is to identify the pathogenic gene and to predict the patient’s survival time. The requirement for analyzing failure time data arises in investigating the relationship between a censored survival outcome and high-dimensional microarray gene expression profiles [17–19]. Therefore, the AFT model was studied extensively in recent years. To try to improve the available data size and get the more accurate result, there are two main methods are used for the AFT model, one of them is called Buckley–James estimator [20–22], which adjusts censored observations using the Kaplan–Meier estimator; another one is the rank based estimator which can be motivated from the score function of the partial likelihood [23–25]. However one of the main reason that the AFT model is not used as widely as the Cox proportional hazards model in survival analysis is the difficulties in computing the semi-parametric estimators of the afore mentioned methods, even if the number of covariates is very small [26]. Not to mention that now a lot of datasets has such characteristics of high-dimension and low sample size. It becomes more difficult to apply these methods, or their regularized versions, especially when variable selection is needed along with estimation. In order to simplify the method, we use Kaplan–Meier weights [27] to estimate the censored data in our AFT model.In this article, we proposed an innovative penalized AFT approach based on a L1/2 regularization, which was proposed by Xu et al. [28]. They made a comprehensive study for Lq(0<q<1) regularizations and exhibited the unusual importance and special role of the L1/2 regularization. They put forward that when 0<q<1/2, there is no significant difference in the performance of Lqregularization, but the computational complexity to solve the L1/2 regularization is much lower than that of the L0 regularization; while 1/2<q<1, the solutions of the Lqregularizations will become more sparse with the decline of the q value, that means L1/2 regularization will yield most sparse solutions when 1/2<q<1. Moreover, they also revealed that the convergence of the L1/2 regularization is not very difficult compared with that of the L1 regularization. Therefore, the L1/2 regularization can be seen as a significant representative in the Lq(0<q<1) regularizations. Meanwhile, in the literature [28], some attractive statistical characters of the L1/2 regularization have been shown to us, such as oracle properties, consistency of variable selection, and unbiasedness. For survival analysis, our previous research work [50] revealed the L1/2 regularization for variable selection in Cox model.In this paper, we used a coordinate descent algorithm to implement the AFT model with L1/2 regularization method and demonstrated that this model can be efficiently applied to the datasets with high dimension and low sample size, such as DNA microarray datasets. The rest of this article is arranged as following: In Section 2, we presented a new version of the AFT model with the L1/2 regularization method. In Section 3, we introduced a coordinate descent algorithm to implement this model. The simulation performance and comparison of the AFT model with the L1/2 regularization and other regularization methods were shown in Section 4. Finally, we gave the summarization for this article in Section 5.In this article, we consider the dataset include n samples to study the correlations between the gene expression levels X and the survival time Y. We use the data form of(yi,δi,xi)i=1nto represent the individual patient’s sample, where yi=min(ti, ci) is observed where tiis the survival time, ciis the time to the first censoring event (e.g., study conclusion, date of final follow up) for subject i, δ is the censoring indicator, ifδi=0,it represents the right censoring time andδi=1means the completed time,xi=(xi1,…xip)indicates the p-dimension covariates.The AFT model is treated as a linear regression between the logarithm of response yiand the covariates xi:h(yi)=β0+xiTβ+εi,i=1,…,n. whereh(.)is the log transformation or some other monotone function,εiare n independent random errors with a normal distribution function, andβ=(β1,β2…,βp)is the regression coefficient vector of p variables.Because of the censoring time in the datasets, the standard least squares approach is not allowed to directly compute the regression parameters of the covariates in AFT model. In order to simplify the method, we use Kaplan–Meier weights to estimate the censored data in the least squares criterion. Since for high dimensional and low simple size data, the Kaplan–Meier weights estimator is more efficient than the Buckley–James and rank based approaches. Moreover, it has a strong and strict theoretical support under some reasonable conditions [24]. The estimated valueh(yi)of the censoring survival time yiis given by:h(yi)=(δi)h(yi)+(1−δi){S^(yi)}−1∑t(r)>th(t(r))ΔS^(t(r))wheres^is the Kaplan–Meier estimator [29] of the survival function and where theΔs^(t(r))is the step ofs^at time t(r). Therefore, the least squares approach of AFT model is to minimizel(β), where each censored yiis replaced with the imputed valueh(yi).In practice, not all the genes in the datasets may associate with the forecast of patients’ survival times: some values of covariates β may be zero in the true model. If the sample size tends to infinity, an ideal prediction procedure for survival analysis should consistently and efficiently select the key risk gene with non-zero coefficients by probability one. In practice, the regularization methods are used to solve the problem in the high-dimensional and low-sample size datasets. The regularization term (also call the penalty function) was added, so the penalized AFT model can be written as:(1)β=argmin{l(β)+λ∑j=1PP(βj)}whereλ>0is a tuning parameter and the larger values of λ provide increased regularization producing more sparse estimates [7], andP(β)is the penalty function or the regularization term. Currently, there are some regularization methods implemented in the AFT model, for example, the most widely used regularization method Lasso, in which the regularization term isP(β)=Σ|β|. Through shrinking some regression coefficients of the covariates to zero, the regularization methods can select the important variables and estimate the regression coefficients simultaneously. The L1 type regularization is an equivalent convex quadratic optimization problem and it can be solved efficiently. However, while it is used to the variable selection, the L1 type regularization may yield a lot of inconsistent selections. Some of the results are the extra bias in the variable selection and the result cannot be seen as the signal with the least measurements. Therefore, in order to obtain a better sparse solution in the high dimensional and low sample size datasets, the Lq(0<q<1) type regularizationP(β)=Σ|β|qwith the lower value of q have been researched. Compared with L1 type regularization methods, Lqis a nonconvex, nonsmooth and non-Lipschitz optimization problem. Moreover, the other difficulty of Lqis how to determine the best q value and develop the corresponding efficient algorithm. Xu et al. [30] proposed only when q=1/2 and 2/3 these two values, we can get an analytically expressive like thresholding representation in the regularizations. It would be easy to find the performance of q=1/2 is better than that while q=2/3 because while q=1/2, the solutions are more sparse, so that the L1/2 regularization occupies the important position in the Lq(0<q<1) type regularization methods. In this paper, we proposed an efficient L1/2 penalized AFT model which is given by:(2)β1/2=argmin{l(β)+λ∑j=1P|βj|1/2}Some valuable statistical properties, such as unbiasedness, and oracle properties, were established in the L1/2 regularization method. Compared with some other regularization methods, the L1/2 regularization is a competitive method both in the theoretical and experimental analyses. Due to this fact, the L1/2 penalized AFT mode is naturally expected.In this section, we introduce a coordinate descent algorithm to evaluate L1/2 penalized AFT model. The idea of the coordinate descent algorithm is simple and efficient. The target function is optimized by respecting to one single coefficient βj. at one time, iteratively repeating many cycles with j from 1 to p until convergence is satisfied. The coordinate descent algorithm can be efficiently applied to the high dimensional and low sample size problems [31], and has been implemented in the L1 type penalized regression [32,33]. We assume that without loss of generality the features have been standardized, such thatΣi=1nxij=0,Σi=1nxij2=1. Considering the AFT models in which the expected value of the outcome depends on the covariates by the linear functionh(yi)=xijβj, our goal is to estimate the value of regression coefficients βj. Penalized regression methods accomplish this by minimizing the objective functionL(β), which is composed of a loss function with a penalty function. Therefore, the AFT linear regularization model with the L1 penalty functionL(β)is given by:(3)L(β)=argmin{1n∑i=1n(yi−XTβ)2+∑j=1pPλ(βj)}whereP(β)is the function of the coefficients indexed by a tuning parameter λ which is used to control the tradeoff between the loss function and penalty function. By this way, we can get a spectrum of solutions depending on the values of λ.Try to get the best value of β to optimize Eq. (3), the proposed algorithm will repeated iteratively until convergence for each value of βj. Eq. (3) also can be given as follow:(4)L(β)=argmin{1n(yi−∑k≠jxikβk+xijβj)2+∑k≠jPλ(βk)+Pλ(βj)}The first order derivative at βjcan be expressed as:(5)∂L∂βj=∑i=1n(−xij(yi−∑k≠jxikβk−xijβj))+Pλ(βj)′=0Definey˜ij=Σk≠jxikβkas the partial residual for fitting βjandωj=Σi=1nxij(yi−y˜ij),The coordinate descent algorithm applied to L1 type regularization by the soft-thresholding operator defined, forλ≥0[34]:(6)βj=S(ωj,λ)={ωj+λifωj<−λωj−λifωj>λ0if|ωj|<λIn this paper, the AFT regression model with the L1/2 regularization term function can be expressed as:(7)L(β)=argmin{1n∑j=1n(yi−XTβ)2+∑j=1pPλ|βj|1/2}When the regularization parameter λ is tuned by the cross-validation approach, based on the convergence analyses of the L1/2 regularization [30], we introduce the renewed half threshold function:(8)βj=Half(ωj,λ)={23ωj(1+cos(2π3−arccos(λ8(|ωj|3)−(3/2))3))if|ωj|>5434λ2/30otherwiseBased on this thresholding function, with the Newton–Raphson iterative procedure, the coordinate descent algorithm was applied to the AFT model with L1/2 regularization method immediately, and the detailed steps are given as follows:The coordinate descent algorithm for the AFT model with L1/2 regularizationStep 1: Initial all β=0(j=1,2,...,p) and λ;Step 2: Minimizel(β)=∑i(yi−βTxi)T(yi−βTxi), subject to the constraints of the L1/2 penalized AFT model;Step 3: Updateβ=Half(ω,λ);Step 4: Repeat Steps 2–3, subject to the constraints of the L1/2 penalized AFT model, until∑j=1p|βij−β(i−1)j|<10E−6,whereβij,β(i−1)jare the result of ith and (i−1)th iteration, respectively.The performance of the coordinate descent algorithm with the L1/2 penalty function is suitable for the sparse problem, because the algorithm does not need to change many irrelevant parameters during the running procedure. Therefore, recalculating partial residuals is not necessary for each update step in this coordinate descent algorithm.For purpose of evaluating various aspects of performance of the L1/2 penalized AFT model, we performed a number of simulation experiments under the different data condition using different regularization methods. The introduction of the tuning parameters and the evaluation parameters which were used to assess the accuracy of the prediction method is given in Section 4.1. The comparison results of the penalized AFT model with different regularization methods (Lasso, Elastic net, SCAD, MCP and L1/2) in the simulated and real cancer datasets are shown in Sections 4.2 and 4.3, respectively.In order to get the optimal tuning parameter λ by cross-validation, a special but practical approach was proposed by van Houwelingen et al. [35], its formula is given by:(9)CV(λ)=∑i=1k{l(β(−i)(λ))−l−i(β−i(λ))}whereβ(−i)(λ)represents the estimation of β which is computed by the regression method when the ith fold is left out. The terml(β)is the log partial likelihood, andl(−i)(β)is the log partial likelihood without the ith fold. The optimal tuning parameter λ is obtained by maximizingCV(λ). The value of k in the cross-validation is determined by the size of the dataset. In this paper, the value of k was set to 5.The accuracy evaluation of the predicted survival time by censoring survival data is quite complex. In this paper, the integrated Brier-Score (IBS) and Concordance Index (CI) [36] were used to compare the prediction accuracy of the different regularization methods.At first, the Brier Score (BS) function of time y>0 is given as follow:(10)BS(y)=1n∑i=11[S^(y|xi)2I(yi≤y)G^(yi)+(1−S^(y|Xi))2I(yi>y)G^(y)]whereG^(.)represents the Kaplan–Meier estimation of the censoring distribution, andS^(.|Xi)means the survival estimation for the patient i. The value of BS(y) is dependent on the time y, and its value is between 0 and 1. A smaller value means the better prediction accuracy of the method. The IBS is written as:(11)IBS=1max(yi)∫0max(yi)BS(y)dyThe IBS can be used to evaluate the performance of the predictd survival functions of all observations at every time from 0 to max (yi).The value of concordance index represents a percentage of the correct part in the whole predict survival time. We determine whileδi=1andti<tj, our predicted timeE(ti)<E(tj)is correct. Therefore the CI is given by:(12)CI=∑i∑j1(E(ti)<E(tj))∑i∑j1(ti<tj&δi=1)As the IBS, the value of CI is between 0 and 1, but the opposite is, the higher of the CI value represents the better performance for the predicted result.We conducted the simulation studies using the AFT method with different regularization methods (Lasso, Elastic net, SCAD, MCP and L1/2) to evaluate their prediction accuracy, respectively. The AFT model simulation schemes were modeled on Bender’s work [37]. Our simulation data were generated as follows:Step 1: We set the correlation coefficient ρ, generate an arrayγ0,γi1,…γip(i=1,...,n) of independent standard normal distribution, and set:xij=γij1−ρ+γi0ρ(j=1,...,p) [19].Step 2: The patient’s survival time yiis written as:yi=exp(∑j=1pβijxij)(j=1,...,p).Step 3: Censoring time pointsy′i(i=1,...,m, m is the number of the censored sample) are from an random distribution, where the number of censoring data is decided by censoring rate.Step 4: We defineyi=min(yi,y′i)and aδi=I(yi≤y′i), so that(yi,δi,xi)treated as the observed data used in the AFT model.In order to make the simulation data conforms to the characteristics of cancer data, in every simulation, we set the dimension of the predictor genes p=1000; and there are seven nonzero bio-marks: β1=3, β4=−2, β7=1.5, β10=−1, β13=1, β16=2, β19=−1.5, and the coefficients β of the remaining 993 genes are zeros. The value of right censored is set to 30%, the size of training sample is set to three conditions: n=50, 100, and 200. We set two kinds of correlation coefficients: ρ=0 and ρ=0.3. In order to minimize the error in the experiment, every AFT model with different regularization method is evaluated on a test set including 50 samples. The results of each procedure averaged over 200 repeats.Table 1 shows the average results contained the number of total features and correct features selected by the each regularization method over 200 repeated experiments, respectively. Overall, it is obvious that for the total number of features, Elastic net selected most and the L1/2 selected the least. When the number of training samples is very small (n=50), all of the methods were very difficult to select the correct genes. However, as the value of n is increased, they can select more correct nonzero features. And while ρ=0, all the methods can get a better performance than that while ρ=0.3. L1/2 regularization method obviously selected the smallest number of features in different data setting environment. After thatTable 2 gives the percent correct of features selected by the five different methods, we can see that with the sizes increase, or the correlation coefficients decrease, the percent correct becomes better. Moreover, the percent correct of L1/2 regularization obviously is the best one in different data setting environment.Fig. 1(a)–(e) display the coefficient paths corresponding to selected features by the five regularization methods, respectively. In each subfigure, the x-axis displays the operation steps corresponding to decreasing λ, the y-axis is the coefficient. The vertical dotted line is drawn at the optimal solution, which is determined by the value of minimal misclassification errors computed by the five-fold cross validation. From Fig. 1, it obviously shows the solution path of the L1/2 penalized AFT model are more sparse than the AFT models with other penalized methods. Therefore we thought the L1/2 penalized method can select almost all correct nonzero features while a large number of irrelevant features were abandoned.To evaluate the performance of different five penalized AFT models, the average values of IBC and CI on the simulated datasets among 200 repeated experiments were shown inTables 3 and 4. Overall, all five regularization methods can get more accurate results while ρ=0 compared to the results while ρ=0.3. For example, when n=100, the performance of the L1/2 regularization method at ρ=0 is more accurate than that at ρ=0.3, because its IBS increases from 0.1067 to 0.1075 and its CI decreases from 0.8233 to 0.8148. Moreover, when the training sample sizes increase, their performances are improving as expected. For example, when ρ=0.3, the prediction accuracy of the L1/2 penalized method when n=200 is better than that when n=100, because its IBS decreases from 0.1075 to 0.0962 and its CI increases from 0.8148 to 0.8851. On the other hand, the five regularization methods have similar performance at the same parameter settings both in IBC and CI, and their differences seem to be marginal. For example, when ρ=0.3 and n=200, SCAD is best and Elastic net is worst according to the IBC and CI values, but their differences are smaller than 5%. Combined with the result of Tables 1 and 2, our conclusion is that the L1/2 penalized AFT model is an efficient approach to select the smallest subset of the relevant genes, and it can still get the similar prediction performance compared to the other regularization methods.In order to further evaluate the performance of the five different regularization methods, they were applied to the five real gene datasets. A brief introduction and summary of the used datasets are given in the below andTable 5.This dataset were first made public by Rosenwald et al [38]. There are 240 lymphoma patients’ samples. Each patient’s data consists 7399 gene expression measurements, and its survival time, including censored or not.The DLBCL 2003 dataset was published by Rosenwald et al. [19]. Compared to the DLBCL(2002), only 92 lymphoma patients’ information were collected, but the number of gene expression measurements of each patient increased to 8810.The DBC dataset was made public by van Houwelingen et al. [39], and there were 295 breast cancer patients’ information collected in this dataset. Each patient’s data consist 4919 gene expression measurements.The lung cancer dataset was used from Beer et al. [40]. It contains 86 lung cancer patients’ information including 7129 gene expression measurements, survival time and whether the survival time is censored.The AML dataset was published by Bullinger et al. [41]. There are 116 patients’ data which contain 6283 gene expression measurements.In order to accurately assess the performance of the five different regularization methods, the real datasets were randomly divided into two pieces: two thirds of the patient samples were put in the training set used for estimation and the remaining one third of the patients’ data would be used to test the prediction capability. In the real data experiments, the value of k is set to 5 in the k-fold cross validation. For each real dataset, each different regularization procedure was repeated over 50 times, respectively.Table 6 shows the average results of different regularization methods applied to the five real datasets. It is obviously that the numbers of genes selected by Elastic net are much more than those of the other regularization methods (Lasso, SCAD, MCP and L1/2). Among the other four regularization methods, the L1/2 penalized method selected the least subset of genes, the number of selected genes by SCAD and MCP were very similar, and the Lasso choose the most in these four regularization methods.In order to test the prediction accuracy of the different regularization methods, their average values of IBS and CI were given inTables 7 and 8, respectively. In the observation of Table 7, we can find that for DLBCL (2002), the SCAD method obtained the best performance compared with the other four methods; for the DLBCL(2003) and DBC datasets, the MCP performed better than the other methods; on the dataset named Lung cancer, the L1/2 got the best result; and the Lasso performed a bit better than other regularization methods on the AML dataset. Nevertheless, we can find that for the same dataset, the differences of the results under the five methods are not big. The gap between them is very close to the each other. In the observation of Table 8, the prediction performances of all the methods are also not much different. Combined with the results reported in Tables 6–8, we can see that the prediction accuracies of the various regularization methods are similar, but the L1/2 penalized method can select the least subset of genes. This is one significant factor in clinical research which must be taken into account, where the goal is to get a more accurate result using the least amount of genes in order to reduce the research cost. Moreover, the experimental results show that the L1/2 regularization method may be the best and most suitable choice compared with the other different regularization methods for the selection of the prognostic genes.In this section, we give a brief biological analysis of the results for the Lung cancer dataset to prove the superiority of our proposed L1/2 regularization method.Table 9 lists the 15 top-ranked informative genes selected by different five regularization methods. We can find that almost half of the genes, which are selected by L1/2 regularization method, also appear in the results of other four regularization methods (WWP1, TRA2A, HUWE1, DOC2A, CCL21, ADM, RPS29). The WWP1 (WW domain containing E3 ubiquitin protein ligase 1), were selected at the No.1 of the queue by four methods (Lasso, SCAD, MCP, L1/2) and the No.2 by Elastic net. From the related literature, we can find that all the eukaryotes contain the WW domain-containing proteins, and the WW domain-containing protein plays a vital role in a lot of regulation of cellular functions, such as protein degradation, transcription, and RNA splicing. This gene encodes a protein which contains 4 tandem WW domains and a HECT (homologous to the E6-associated protein carboxyl terminus) domain. The key trafficking decisions, such as the lysosomes and targeting of proteins to proteasomes, was regulated by the encoded proteins. It is likely to impact on the cancer [42–44]. The TRA2A (transformer 2 alpha homolog), is also in the front of the five gene queues. TRA2A belongs to the family of the transformer 2 homolog. It encodes a protein with several RRM (RNA recognition motif) domains. This phosphorylated nuclear protein can bind to specific RNA sequences and plays an important role in the regulation of pre-mRNA splicing. In [45], we can find that this gene is strongly associated with cancer. The other common selected gene HUWE1 (HECT, UBA and WWE domain containing 1), encodes a protein containing a C-terminal HECT (E6AP type E3 ubiquitin protein ligase) domain that functions as an E3 ubiquitin ligase. This protein ubiquitinates the p53 tumor suppressor, core histones, and DNA polymerase beta [46].On the other hand, there are two genes, PRKCSH (protein kinase C substrate 80K-H) and MYOG, which were only selected by L1/2 regularization method. For PRKCSH gene, its mutations have been proved to be related to the autosomal dominant polycystic liver disease. In [47], PRKCSH is also proved to be associated with lung cancer. MYOG (myogenin), is a muscle-specific transcription factor, which is able to induce myogenesis in a variety of cell types in tissue culture. It is also mentioned in the cancer study [48,49].We also received the similar case in the other results of the real gene expression datasets. The biological analyses of the selected genes proved that the L1/2 regularized AFT model can identify the useful genes efficiently.

@&#CONCLUSIONS@&#
