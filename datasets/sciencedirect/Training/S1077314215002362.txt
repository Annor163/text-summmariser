@&#MAIN-TITLE@&#
Lazy Generic Cuts

@&#HIGHLIGHTS@&#
An efficient algorithm for inference in binary higher order MRF–MAP is proposed.Scalable to mid sized cliques which could not be done by state of the art.The algorithm is a flow based combinatorial algorithm based on Generic Cuts.In a typical inference problem minimum depends only on small set of constraints.The experiments validate the observation and show superiority over state of the art.

@&#KEYPHRASES@&#
Markov random fields,Higher order potential,Graphical models,MRF-MAP,Optimal inference,Generic cuts,

@&#ABSTRACT@&#
LP relaxation based message passing and flow-based algorithms are two of the popular techniques for performing MAP inference in graphical models. Generic Cuts (GC) (Arora et al., 2015) combines the two approaches to generalize the traditional max-flow min-cut based algorithms for binary models with higher order clique potentials. The algorithm has been shown to be significantly faster than the state of the art algorithms. The time and memory complexities of Generic Cuts are linear in the number of constraints, which in turn is exponential in the clique size. This limits the applicability of the approach to small cliques only. In this paper, we propose a lazy version of Generic Cuts exploiting the property that in most of such inference problems a large fraction of the constraints are never used during the course of minimization. We start with a small set of constraints (called the active constraints) which are expected to play a role during the minimization process. GC is then run with this reduced set allowing it to be efficient in time and memory. The set of active constraints is adaptively learnt over multiple iterations while guaranteeing convergence to the optimum for submodular clique potentials. Our experiments show that the number of constraints required by the algorithm is typically less than 3% of the total number of constraints. Experiments on computer vision datasets show that our approach can significantly outperform the state of the art both in terms of time and memory and is scalable to clique sizes that could not be handled by existing approaches.

@&#INTRODUCTION@&#
MAP inference in graphical models corresponds to finding the most likely joint assignment to the underlying variables. It is an important problem for a wide variety of applications including natural language processing, computer vision and biology. The problem is known to be NP-hard (in the number of variables) except for some special cases, such as when the clique potentials are submodular [1–4]. Irrespective of submodularity, the complexity of the algorithms suggested in computer vision has been exponential in the size of the clique. This makes inference intractable even for mid-sized cliques.Two of the popular approaches for solving the MAP problem include: LP relaxation based message passing algorithms [5–8] and flow based algorithms [1,9–12]. A recent comprehensive survey about the algorithms for MAP inference can be found in [13].In message passing based approaches, convergence to the optimal solution is defined only in the limit when the algorithm is allowed to run indefinitely. Even if the algorithm converges, it may not always lead to an integral solution. A popular approach is to first define an LP relaxation of the problem followed by message passing on the relaxed version [5,6,14]. Komodakis and Paragios [15] have proposed a dual decomposition framework which solves the dual relaxation of the general MRF–MAP problem. The original problem is first decomposed into a series of subproblems that are easy to optimize. The solution of the original hard problem is obtained by cleverly combining solutions from the subproblems. Getting the combined solution in a dual decomposition framework is a non-trivial task even when the optimal solutions to subproblems are given.Flow based algorithms solve a combinatorial optimization problem such as max-flow/min-cut and have been shown to give better performance in practice [16]. Ishikawa [17] has proposed to reduce the higher order potentials into binary ones and combine the reduction with existing flow based algorithms. However, this reduction itself is exponential in the clique size in most cases. The reduction method suggested by Fix et al. [18] has been shown to give better performance theoretically and experimentally. Their technique is to reduce a group of higher order terms at the same time instead of each term individually. A big shortcoming of reduction based approaches is that the reduced pairwise problem is often non-submodular hence hard to solve optimally. This is true even when the original higher order version was submodular. A different approach to reduction is by Kahl and Strandmark [19] using the generalized roof duality. Their method can find a submodular approximation to the original higher order function of clique size at most 4. Their approach produces better solutions in practice but is computationally expensive. Reduction based approaches do not guarantee optimal inference even when the potential function is submodular, for which theoretical algorithms for finding optimal solutions are known [20].Recently, Arora et al. [1] has proposed a max-flow/min-cut based approach to deal with higher order potentials directly. Their algorithm, called Generic Cuts (GC), can be seen as a combination of the LP relaxation based and the flow based approaches. They define a gadget based flow graph corresponding to Lagrangian dual of LP relaxation, over which running a modified max-flow algorithm results in the optimal solution when the potentials are submodular. Fix et al. [10] have replaced the augmenting path style flow algorithm used in GC with their improved IBFS, showing improvement in running time for clique size of 4. The worst case time complexity of the Generic Cuts is O(n|C|2k32k) where n, |C| and k are the number of variables, number of cliques and the size of max-clique, respectively. There are (2k) labelings on a clique and the higher order potential cost of each labeling contributes a constraint in the dual formulation. So the term (|C|2k) in time complexity captures the total number of constraints. Though the algorithm has been shown to significantly outperform existing state of the art approaches, it is still exponential in the clique size making it intractable for larger clique sizes.The hardness in MRF–MAP optimization problem is due to the number of possible labelings on a clique which increase exponentially with clique size. We observe that for many such inference problems in computer vision, many of these possible labelings are forbidden since they attract a high cost in the energy function. In the corresponding optimization problem as defined in Generic Cuts, where the labeling cost maps to the cost of cutting a few edges in a flow graph, the solution obtained depends only on a very small set of constraints defining the cut. To illustrate the point, consider creating a GC flow graph where all but a subset of the constraints are ignored. We call such a problem the relaxed problem. The corresponding flowgraph is referred to as the relaxed graph. The constraints which are ignored are called the inactive constraints and the remaining ones are referred to as the active constraints. As we show later in this paper, the flow in the relaxed graph is an upper bound for the flow in the original graph. We also show that a maximum flow in the relaxed graph which does not violate any flow constraint of the original graph is a maximum flow for the original graph as well.Motivated by the observation mentioned above, we propose Lazy Generic Cuts (LGC), which brings in the constraints lazily to the active set, and gradually learns the relaxation in which a maximum flow is consistent with the original graph. For finding a maximum flow in the relaxed graph, we use standard GC with the modification to calculate residual capacities using active constraints only. Since the number of active constraints are usually significantly smaller than the total number of constraints in a typical computer vision problem, this allows each iteration to run much faster as well and requiring only a fraction of the memory compared to that of original GC.We show that the LGC algorithm is guaranteed to terminate in a finite number of steps at the optimum when the clique potentials are submodular. Though there is no optimality guarantee for non-submodular clique potentials, our experiments show that the solutions inferred by our algorithm have good visual quality and their energy value is close to that obtained by GC. Note that the number of active constraints can be significantly less than the total number of constraints. This property is key to scalability of the LGC algorithm to larger clique sizes.LGC can also be seen in the light of cutting plane inference algorithms proposed earlier for graphical models [21,22]. One of the key characteristics of these algorithms is their ability to deal with large number of constraints by working with a relaxed problem which has only a small subset of constraints (i.e. the active set in our terminology). If at optimality all constraints in the original problem are satisfied, then the current solution is also optimal for the original problem. Otherwise the algorithm refines the active set by including the violated constraints which is then used to cut down the feasible space. In our approach, we use the simple but effective strategy of scanning through all the constraints and including all the violated constraints for the next iteration. After including violated constraints we solve the new relaxed problem. This process is repeated until there are no violated constraints. Exploring more efficient ways for refining the set of active constraints is part of the future work. It should be noted that despite the linear scan to refine the active set, LGC algorithm can still give substantial gains as such scans are only done at the beginning of each iteration. However this allows each flow augmentation of GC to run on a significantly smaller set of active constraints only.We evaluate our algorithm on binary denoising problem. Our algorithm is able to scale to clique sizes which none of the existing algorithms can handle. Even on problem sizes which earlier techniques could handle, we are significantly more efficient both in terms of running time and memory.The outline of this paper is as follows. We first present the required background on Generic Cuts [1] in Section 2. We then describe the LGC algorithm in detail in Section 3. We also give a proof of correctness and convergence for the proposed algorithm in this section. We describe our experimental evaluation in Section 4 and conclude the paper with directions for future work in Section 5.

@&#CONCLUSIONS@&#
