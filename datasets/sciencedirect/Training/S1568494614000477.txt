@&#MAIN-TITLE@&#
A novel hybrid KPCA and SVM with GA model for intrusion detection

@&#HIGHLIGHTS@&#
A novel method combination of KPCA and SVM is proposed for intrusion detection.KPCA is used as a preprocessor of SVM to extract features.A new radial basis kernel function (N-RBF) is developed for KPCA and SVM.GA is employed to optimize the parameters of SVM.Experimental results shown that the proposed method had more excellent performance.

@&#KEYPHRASES@&#
Intrusion detection,Kernel principal component analysis,Kernel function,Support vector machines,Genetic algorithm,

@&#ABSTRACT@&#
A novel support vector machine (SVM) model combining kernel principal component analysis (KPCA) with genetic algorithm (GA) is proposed for intrusion detection. In the proposed model, a multi-layer SVM classifier is adopted to estimate whether the action is an attack, KPCA is used as a preprocessor of SVM to reduce the dimension of feature vectors and shorten training time. In order to reduce the noise caused by feature differences and improve the performance of SVM, an improved kernel function (N-RBF) is proposed by embedding the mean value and the mean square difference values of feature attributes in RBF kernel function. GA is employed to optimize the punishment factor C, kernel parameters σ and the tube size ɛ of SVM. By comparison with other detection algorithms, the experimental results show that the proposed model performs higher predictive accuracy, faster convergence speed and better generalization.

@&#INTRODUCTION@&#
Intrusion detection is one of the most essential things for security infrastructures in network environments, and it is widely used in detecting, identifying and tracking the intruders [1]. Capabilities of intrusion detection technologies have great importance with the performance of intrusion detection system (IDS). Researches always want to find an intrusion detection technology with better detection accuracy and less training time.However, there are many problems in the traditional IDS, such as the low detection capability against the unknown network attack, high false alarm rate, and insufficient analysis capability and so on. In nature, intrusion detection can be seen as classification problem, to distinguish between the normal activities and the malicious activities. The concerned problems of machine learning are how the systems automatically improve the performance with the increase of experience, which is consistent with that of the IDS. Therefore, various machine learning methods are developed for intrusion detection, such as decision tree [1], genetic algorithm (GA) [2], neural network [3], principal component analysis (PCA) [4], fuzzy logic [5], K-nearest neighbor [6], rough set theory [7] and support vector machine (SVM) [8].Among the methods mentioned above, SVM is an effective one, the main reason is that the distribution of different types of attacks is imbalanced, where the learning sample size of the low-frequent attacks is too small compared to the high-frequent attack. SVM is a margin-based classifier based on small sample learning with good generalization capabilities, which is frequently used in real world applications of classification [9]. It realizes the theory of VC dimension and principle of structural risk minimum, thus it does not have the over-fitting problem that artificial neural network cannot overcome. SVM has manifested its robustness and efficiency in the network action classification, and it is widely used in IDS as a popular method [10]. Eskin [11] addressed an unsupervised anomaly detection framework, and applied it in three unsupervised learning algorithms, including clustering method, K-nearest neighbor and SVM. Shon et al. [12] employed genetic algorithm (GA) for feature selection, and used SVM for intrusion detection. Srinoy [13] proposed an intrusion detection model using SVM and particle swarm optimization (PSO) which used PSO to extract intrusion features and SVM to classify. Fei et al. [14] proposed an incremental clustering method based on the density. Horng et al. [15] used the hierarchical clustering algorithm to provide the SVM with fewer, abstracted, and higher qualified training instances. To overcome the problem of uncertainty in IDS, Kavitha et al. [16] adopted a new technique known as neutrosophic logic (NL). Wu and Banzhaf [17] referred to the review of computational intelligence in intrusion detection and applied numerical evaluation measures to quantify the performance of IDS. Kolias and Kambourakis [18] gave the survey of swarm intelligence in intrusion detection. Kuang et al. [19] proposed a SVM model based on kernel principal component analysis (KPCA) and GA, which used KPCA to extract intrusion features, and GA to optimize the parameter of SVM. Li et al. [20] put forward pipeline of data preprocess and data mining in IDS, and used gradually feature removal method to feature reduction and SVM to classify.However, standard SVM still has some limitations, the performance depends on its parameters selection, and when the differences between the attributes of the sample are very big, using RBF in the training process will produce a large number of support vectors and the training time will be longer too. And two main parts should be conducted which are detection model set-up and intrusion feature extraction to get better performance.To solve the above mentioned problems, we present a novel intrusion detection approach combining SVM and KPCA to enhance the detection precision for low-frequent attacks and detection stability. In the proposed method, KPCA maps the high dimension features in the input space to a new lower dimension eigenspace and extracts the principal features of the normalized data, and multi-layer SVM classifier is employed to estimate whether the action is an attack. In order to shorten the training time and improve the performance of SVM classification model, an improved radial basis kernel function (N-RBF) based on Gaussian kernel function is developed, and GA is used to optimize the parameters of SVM.The rest of this paper is organized as follows. In Section 2, the proposed SVM classification model is described in detail. The classification procedure is presented to illustrate how to use the proposed SVM model for intrusion detection in Section 3.The experimental results are discussed in Section 4. Section 5 presents conclusion and future work.Principal component analysis (PCA) [21] is a common method applied to dimensionality reduction and feature extraction. PCA method can only extract the linear structure information in the data set, however, it cannot extract this nonlinear structure information. KPCA is an improved PCA, which extracts the principal components by adopting a nonlinear kernel method [22,23]. A key insight behind KPCA is to transform the input data into a high dimensional feature space F in which PCA is carried out, and in implementation, the implicit feature vector in F does not need to be computed explicitly, while it is just done by computing the inner product of two vectors in F with a kernel function.Let x1, x2, …, xn∈Rdbe the n training samples for KPCA learning [19]. The ith KPCA-transformed feature tican be obtained by(1)ti=1λiγiT[k(x1,xnew),k(x2,xnew),…,k(xn,xnew)]T,i=1,2,…,pHere, Column vectors γi(i=1, 2, …, p; 0<p≤n) is the orthonormal eigenvectors to the p largest positive eigenvalues λ1≥λ2≥…≥λp, k(xi, xj) is the calculation of the inner product of two vectors in the hyper-dimensional feature space F with a kernel function.By using Eq. (1), the KPCA-transformed feature vector of a new sample vector can be obtained.After feature extraction using KPCA, the training data points can be expressed as (t1, y1), (t2, y2), …, (tp, yp), ti∈Rk(k<d) is the transformed input vector, yiis the target value [19]. In the ɛ-SVM classification [24], the goal is to find a function f(t) that has at most ɛ deviation from the actually obtained targets yifor all the training data, and at the same time, is as flat as possible. The ɛ-insensitive loss function denotes as follows:(2)e(f(t)−y)=0,f(t)−y≤εf(t)−y−ε,otherwiseFormally the optimization problem by requiring the follows:(3)minimize12w2+C∑i=1p(ξi+ξi∗)subject toyi−(w'Φ(ti)+b)≤ε−ξi(w'Φ(ti)+b)−yi≤ε−ξi*ξi,ξi*≥0,i=1,2,…,p;C>0where ξiand ξi*are slack variables, the punishment factor C is regularization constant, ɛ denotes the tube size of SVM. C and ɛ are both determined by users empirically, the constant C determines the trade-off between the flatness of f(t) and the amount up to which deviations large than ɛ are tolerated.At the optimal solution, the decision function takes the following form:(4)f(t)=sgn∑i=1p(αi−αi∗)K(ti,tj)+bwhere αiand αi*are the Lagrange multiplier coefficients for the ith training sample, and obtained by solving the dual optimization problem in support vector learning [24]. The training sample for which αi≠αi∗ is corresponded to the support vectors, K(ki, kj) is a kernel function, b is found by the Karush–Kuhn–Tucker conditions at optimality.In the SVM, there are some common kernels, shown as follows, and any of those can be chosen to achieve the boundary function. Their detailed usages and descriptions, including parameters definitions, can be found in [25,26].(1) Gaussian RBF kernel:K(ti,tj)=exp−ti−tj2σ2,σ∈R(2) Polynomial kernel: K(ti, tj)=(a(ti·tj)+b)d, a∈R, b∈R, d∈N(3) Sigmoid kernel: K(ti, tj)=tanh(a(ti·tj)+b), a∈R, b∈R(4) Inverse multi-quadric kernel:K(ti,tj)=1ti−tj2+σ2,σ∈RSVM always has good performance in classification when using RBF, which is an effective kernel function for fewer parameters set and an excellent overall performance. A network record contains dozens of attributes, and there may be significant differences between them. Therefore, when the differences between the attributes are very big, using RBF in the training process will produce a larger number of support vectors and the training time will be longer too.In order to shorten the training time and improve the performance of SVM, an improved kernel function N-RBF is developed by embedding the mean value and the mean square difference values of feature attributes in Gaussian RBF kernel function to normalize the attribute values. The N-RBF is defined as follows:(5)K(ti,tj)=exp(−(ti−m)/s−(tj−m)/s2σ2)where m=(m1, m2, …, mj, …, mk) and s=(s1, s2, …, sj, …, sk) are the mean value and the mean square deviation of attributes, respectively, k is the dimension of sample vectors, mjand sjis denoted as follows:(6)mj=1n∑i=1nLij,j=1,2,…,k(7)sj=1n−1∑i=1n(Lij−mj)2,j=1,2,…,kwhere Lijis the jth attribute of the ith sample and n is the number of training samples.According to the functional theory, as long as the function K satisfies Mercer's condition, it can be denoted as an inner product, and it should be a positive definite kernel. Obviously, N-RBF is a positive definite kernel, and is a kernel function.Consequently, the three positive parameters σ, ɛ and C are user-determined parameters in SVM classification model, the selection of the parameters plays an important role in the performance of SVM model. A better approach is to apply cross validation to select the best choices among some candidate parameters. Based on the idea, several disciplined approaches can be used to obtain the optimal parameters for SVM classification model, out of which, evolutionary method such as genetic algorithm (GA), simulated annealing algorithm and PSO algorithm, is one of the most widely used approaches. In this paper, GA is employed.GA is a search technique used in computing to find exact or approximate solutions to optimization and search problems. Genetic algorithms, as global search heuristics, are a particular class of evolutionary algorithms (EA) that use techniques inspired by evolutionary biology such as inheritance, mutation, selection, and crossover. GAs have been considered with increasing interest in a wide variety of applications [27]. These algorithms are used to search the solution space through simulated evolution of “survival of the fittest”. These are also used to solve linear and nonlinear problems by exploring all regions of state space and exploiting potential areas through mutation, crossover and selection operations applied to individuals in the population.Therefore, in this paper, genetic algorithms are used to optimize the parameters σ, ɛ and C of SVM. And a negative mean absolute percentage error (MAPE) is used as the fitness function for evaluating fitness [24–26]:(8)MAPE=1N∑i=1Nai−diai×100%where aiand direpresent the actual and forecast values, respectively. N is the number of classification periods. GA is used to yield a smaller MAPE by searching for better combinations of three parameters in SVM. The process of optimizing the SVM parameters with GA is shown in Fig. 1, which is described below.Step 1: Encode SVM parameters and initialization of population. The three free parameters σ, ɛ and C are encoded in binary numbers and represented by a chromosome. Each bit of the chromosome represents whether the corresponding feature is selected or not. ‘1’ in each bit means the corresponding feature is selected, whereas ‘0’ means it is not selected. Randomly generate an initial population of chromosomes which represent the values of parameters in SVM model.Step 2: Calculate the fitness function of each chromosome according to Eq. (8). It is evaluated by the cross-validated predictive accuracy of the SVM model.Step 3: GA operators, which are selection, crossover and mutation. Selection is performed to select excellent chromosomes to reproduce. Based on fitness functions, chromosomes with higher fitness values are more likely to yield offspring in the next generation by means of the roulette wheel. The single-point crossover principle is employed. Segments of paired chromosomes between two determined break-points are swapped. Mutations are performed randomly by converting a ‘1’ bit into a ‘0’ bit or a ‘0’ bit in to a ‘1’ bit. The rates of crossover and mutation are probabilistically determined. In this study, the probabilities of crossover and mutation are set to 0.8 and 0.05, respectively.Step 4: Generate a new population for the next generation. Offspring replaces the old population and forms a new population in the next generation by the three operations.Step 5: Obtain the parameters of SVM model. If one of the stopping criteria (Generally, a sufficiently good fitness or a given number of generations) is satisfied, then the best chromosomes are presented as a solution, else go to step 2.After these steps, the optimal parameters σ, ɛ and C of the KPCA SVM model are obtained.This paper takes the KDD CUP99 [28] as the datasets of the experiments. The datasets can be divided into five categories which are normal, denial of service (DoS), unauthorized access from a remote machine (Remote to Local, R2L), unauthorized access to local supervisor privileges (User to Root, U2R) and Probe. Each network record contains 41 attributes, of which 34 attributes are continuous and 7 attributes are discrete. Before the experiments, we need to deal with the discrete attributes by counting the frequency of their values and converting them to numerical attributes, and transform all attributes into the normalized format.Multi-SVM classifiers are applied to intrusion detection because of multi-types existing in network. ‘One-against-one’, ‘One-against-all’ and ‘Binary tree’ are the popular methods in SVM multi-class classification [24]. ‘Binary tree’ SVM classification algorithm needs only k−1 two-class SVM classifiers for a case of k classes, while ‘One-against-all’ SVM classification algorithm needs k two-class SVM classifiers where each one is trained with all the samples and ‘One-against-one’ SVM classification algorithm needs k (k−1)/2 two-class SVM classifiers where each one is trained on data from two classes [24,25]. Obviously less two-class classifiers help to expedite the rate of training and recognition. Thus, ‘Binary tree’ SVM classification algorithm is adopted to construct detection model for intrusion detection.Based on the characteristics of different intrusion detection types, four SVM classifiers are developed to identify the five states: normal state (Nc) and the four intrusion state (DoS, R2L, U2R, and Probing). With all the training samples of the five states, SVM1 is trained to separate the normal state from the intrusion state. When input of SVM1 is a sample representing the normal state, output of SVM1 is set to +1; otherwise −1. SVM2 is trained to separate the DoS from the other intrusion states. When the input of SVM2 is a sample representing DoS, the output of SVM2 is set to +1; otherwise −1. SVM3 is trained to separate R2L from U2R and Probing. When the input of SVM3 is a sample representing the R2L, the output of SVM3 is set to +1; otherwise −1. SVM4 is trained to separate Probing from U2R. When the input of SVM4 is a sample representing Probing, the output of SVM4 is set to +1; otherwise −1. Thus, the multilayer SVM classifier is obtained. The basic principle of intrusion detection model based on improved SVM classifiers by combining KPCA and GA is shown in Fig. 2.All the four SVMs adopt N-RBF function as their kernel function, and the parameters σ, ɛ and C are optimized with GA. The adjusted parameters with maximal classification accuracy are selected as the most appropriate parameters. Then, the optimal parameters are utilized to train the SVM model.Intrusion detection belongs to classification problems in essence, it distinguishes between the abnormal data and the normal data, and the intrusion data is of a high dimension and contains many noise attributes. Therefore, KPCA is used to extract the principal components, SVM classifiers are applied to intrusion detection. The proposed hybrid approach is composed of two stages: In the first stage, the principal components are achieved based on KPCA theory, which find an optimal subset of all attributes and delete irrelevant and redundant attributes. The second stage is to use this attribute subset as the training dataset and testing dataset of SVM to perform the classification, and N-RBF kernels are used for KPCA and N-RBF kernels are also adopted for SVM, GA is used to select the optimal parameter of SVM. Fig. 3shows the procedures of the proposed SVM classification model for intrusion detection.In this section, we selected samples from the subset of KDD to form the training and testing set. There are some performance indicators for the intrusion detection system as follows: TP, FP, TN and FN, where TP represents that the normal behavior is correctly forecasted, FP indicates that the abnormal behavior is judged as normal, FN denotes that the normal behavior is wrongly thought as abnormal, and TN represents the abnormal behavior is correctly detected [17].(1) Detection rate:DR=TNTN+FP(2) False alarm rate:FAR=FNFN+TP(3) Correlation coefficient:cc=TP×TN−FP×FN(TP+FN)(TP+FP)(TN+FP)(TN+FN)where DR denotes the detection rate and FAR denotes the false alarm rate. They are important to evaluate the performance of the intrusion detection system. In addition, we consider another indicator cc, which denotes the correlation between the forecast result and the actual situation. It ranges from −1 to 1, where 1 implies the forecast result is fully consistent with the actual situation and −1 is on behalf of a random prediction.The experiment was processed within a MATLAB R2009b environment, which was running on a PC powered by Pentium IV 3.0GHz CPU and 3.0 GB RAM.In this section, we selected samples from the subset of KDD to form the training and testing set. There were five data sets in Table 1. In order to verify the effectiveness of N-RBF, the percentage of the normal samples in each set was different. Adopt the SVM as the classifier. Because the choice of parameters would impact algorithms’ performance directly, we used the python in Libsvm as a supplementary tool. The parameter σ of RBF and N-RBF is set to be 0.00028.The parameters a, d and b of POLY are set to be 0.00028, 3 and 1, respectively. The penalty parameter C of SVM is set to be 1024. The experiment results are shown in Table 2.As shown in Table 2, the detection rates of N-RBF and POLY are higher, especially N-RBF. Due to the introduction of normalization, it reduces the noise among attributes, so the detection rate of N-RBF is higher than RBF. Secondly, the training time of RBF is dozens or even hundreds of times of N-RBF, while the training time of POLY is several or dozens of times of N-RBF, which indicate N-RBF has good performance in reducing the training time and only costs a few seconds; and the test time POLY and N-RBF cost is significantly less than RBF. In general, although the detection effect of N-RBF is not greatly improved in comparison to the other two kernel functions, it has saved lots of time in the training process, which lays the foundation for the following experiments of the SVM.The above experiments have not considered the attacks of different kinds, respectively. In order to further analyze the detection performance of N-RBF on unknown attacks, we gave the following experiments. First, regenerated a test set, containing more than 90% attacks of new categories. Then we compared SVM with N-RBF, SVM with RBF, and SVM with POLY in the experiments, and counted the detection rates of these methods on the attacks of all categories, the comparisons of experimental results in 30 simulation experiments are given in Table 3.As shown in Table 3, we can see that the three methods show high detection rates on forecasting normal behaviors, and SVM with N-RBF has the highest detection rates on predicting the attacks of Probe and DoS. However, the results for detecting attacks of U2R and R2L are all unsatisfactory. In general, the detection rate of SVM with N-RBF on attacks of all categories is better than the other two methods.The following experiments were done to verify the effectiveness of the novel KPCA-GA-SVM (N-KPCA-GA-SVM). In this section, firstly, the subset we obtained in Section 4.2 was randomly divided into two subsets, each subset contains both the data of normal and abnormal class, one was as the training set, and the other was as the test set. Secondly, randomly select 10 datasets from the training subset, named from F1 to F10, as the training set, and any two training sample sets did not intersect. Thirdly, from the test subset, selected the normal and attack records with the same number to form the test set.Now, we evaluated N-KPCA-GA-SVM by comparing it with KPCA-GA-SVM [19], PCA-GA-SVM, Single-SVM and radical basis function neural networks (RBFNN) on the detection rate (DR), false alarm rate (FAR), correlation coefficient (cc), and training time (TrD) and test time (TeD). We employed four SVMs for the 5-class classification problem including Section 3.2, and partitioned the data into the two classes of “Normal” and “Rest” (DoS, R2L, U2R, Probe) patterns, where the rest was the collection of four classes of attack instances in the dataset. The objective was to separate normal and attack patterns. Repeat this process for all classes. In this paper, we chose p eigenvectors by trial and error, which corresponded to the first p biggest eigenvalues, to form the sub-eigenspace, satisfying:(9)∑i=1pλi∑i=1nλi≥90%In N-KPCA-GA-SVM model, N-RBF kernels were used for KPCA and N-RBF kernels were also adopted for SVM, GA method was used to select the optimal parameter of SVM and KPCA. KPCA was applied to feature extraction, this method aimed to map the high dimensional original input data to a lower dimensional eigensapce, which held the principal features and abandoned the subordinate and noise data. In N-KPCA-GA-SVM model, by many experiments, the parameters of the models were chosen as follows: population size: 50, maximal iteration: 200, the probabilities of crossover and mutation were set to 0.8 and 0.05, respectively. Through 20 simulation experiments, parameters (C, σ, ɛ)=(83.5191, 0.0907, 0.0008) of SVM were obtained. In the other SVM model, all SVMs adopted RBF as their kernel function, and the parameters σ, ɛ and C were randomly selected. In RBFNN model, RBFNN had four-layer ANN, with 5 input neurons, with two hidden layers with 20 and 30 neurons each, and 5 output neurons. The experiment results among different algorithms are listed in Table 4.As shown in Table 4, the classification accuracies of the proposed SVM model are superior to those of SVM classifiers whose parameters are randomly selected, SVM classifier for intrusion detection by using PCA, KPCA to extract feature has a good performance in accuracy and runtime than that without feature extraction. The experimental results demonstrate that the features extracted by KPCA can provide more additional discriminatory information for improving classification performance than PCA. And dimension reduction can improve the generalization performance and running time of SVM classifier. Furthermore, results also show that KPCA is better than PCA. The reason lies in the fact that KPCA can explore higher order information of the original inputs than PCA. By using the kernel method to generalize PCA into nonlinear, KPCA implicitly takes into account higher order information of the original inputs. More number of principal components can also be extracted in KPCA, eventually resulting in better generalization performance.We can also see from Table 4 that the stabilities of the learning of N-KPCA-GA-SVM and KPCA-GA-SVM are better than Single-SVM. Compared to KPCA-GA-SVM, N-KPCA-GA-SVM is more effective in detecting, because DR and cc of N-KPCA-GA-SVM are higher than that of KPCA-GA-SVM. We can also see that Single-SVM needed longer training time, because it has to do cross-judging and more training, and the training time of KPCA-GA-SVM and PCA-GA-SVM is in the acceptable range. And N-KPCA-GA-SVM has obvious advantages in training time over KPCA-GA-SVM. It is apparent that N-KPCA-GA-SVM needed less test time than another three algorithms. RBFNN also obtains good classification accuracy, but RBFNN requires large amounts of training data, and needs to adjust the parameters of the hidden activation function, the parameters are determined by experience or by using the optimum method to tune the network parameters and connecting weights such as Genetic algorithm. In addition, this table can also see that the overall performance of N-KPCA-GA-SVM model is better than other four methods for intrusion detection.The above results show that N-RBF and KPCA play some role in saving the training and test time. N-KPCA-GA-SVM is more reliable, compared to KPCA-GA-SVM, PCA-GA-SVM, Single-SVM and RBFNN. And it does not cause large fluctuations in detection performance. Moreover, it can improve the detection performance.

@&#CONCLUSIONS@&#
