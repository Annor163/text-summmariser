@&#MAIN-TITLE@&#
Evolutionary undersampling boosting for imbalanced classification of breast cancer malignancy

@&#HIGHLIGHTS@&#
Automatic clinical decision support system for breast cancer malignancy grading.Different methodologies for segmentation and feature extraction from FNA slides.An efficient classifier ensemble for imbalanced problems with difficult data.Ensemble combines boosting with evolutionary undersampling.Extensive computational experiments on a large database collected by authors.

@&#KEYPHRASES@&#
Machine Learning,Classifier ensemble,Imbalanced classification,Evolutionary algorithms,Clinical decision support,Breast cancer,

@&#ABSTRACT@&#
In this paper, we propose a complete, fully automatic and efficient clinical decision support system for breast cancer malignancy grading. The estimation of the level of a cancer malignancy is important to assess the degree of its progress and to elaborate a personalized therapy. Our system makes use of both Image Processing and Machine Learning techniques to perform the analysis of biopsy slides. Three different image segmentation methods (fuzzy c-means color segmentation, level set active contours technique and grey-level quantization method) are considered to extract the features used by the proposed classification system. In this classification problem, the highest malignancy grade is the most important to be detected early even though it occurs in the lowest number of cases, and hence the malignancy grading is an imbalanced classification problem. In order to overcome this difficulty, we propose the usage of an efficient ensemble classifier named EUSBoost, which combines a boosting scheme with evolutionary undersampling for producing balanced training sets for each one of the base classifiers in the final ensemble. The usage of the evolutionary approach allows us to select the most significant samples for the classifier learning step (in terms of accuracy and a new diversity term included in the fitness function), thus alleviating the problems produced by the imbalanced scenario in a guided and effective way. Experiments, carried on a large dataset collected by the authors, confirm the high efficiency of the proposed system, shows that level set active contours technique leads to an extraction of features with the highest discriminative power, and prove that EUSBoost is able to outperform state-of-the-art ensemble classifiers in a real-life imbalanced medical problem.

@&#INTRODUCTION@&#
Based on the data provided by the National Cancer Registry, up to 2015 there were 17,144 diagnosed cases of breast cancer in Poland. This statistic makes the breast cancer the most often diagnosed type of cancer among middle-age women and the number of diagnosed cases is still increasing. For instance, between 2009 and 2012 there was an increase of 1280 diagnosed cases. Unfortunately, this fact is translated into a larger death rate, which was recorded to be 5651 deaths in 2012, 341 more cases than in 2009. However, most of them could have been fully recovered if the diagnosis would have been made in the early stage of the disease. This is because cancers in their early stages are vulnerable to treatment, while cancers in their most advanced stages are usually almost impossible to treat.In order to differentiate the stages of a cancer, during the diagnosis process a grade is assigned, which is then used to determine the appropriate treatment. Since successful treatment is a key to reduce the high death rate of breast cancer, so it is the appropriate grading of the cancer malignancy. For this purpose, screening mammography tests are performed and when a suspicious region is found a fine needle aspiration biopsy (FNA) is taken. This is an invasive method, which extracts a small sample of the questionable breast tissue that allows the pathologist to describe the type of the cancer in detail. Malignancy grading allows doctors to precisely estimate cancer behavior with or without undertaking treatment, and therefore is called a prognostic factor. It plays an important role in breast cancer diagnosis and the appropriate treatment is chosen accordingly to this factor.The determination of malignancy is performed by assigning a malignancy grade to the case. To help in this very difficult task, a grading scale was proposed by Bloom and Richardson [6]. The grading scheme proposed by the authors was derived to assess malignancy from histological slides and is now widely used among pathologists to grade not only histological but also cytological tissues. However, due to the large variation in cancer images and the large number of slides to be analyzed every day by a specialist, a need for automatic decision support system has arisen. Machine Learning is a popular tool for developing support software that ease the work of the specialists. Among a plethora of methods used in this domain, classifier ensembles stand as one of the most efficient solutions for image classification [15,39].Automatic breast cancer detection from medical images has been widely addressed in the contemporary literature [14,48]. There are numerous reports on applying different imaging techniques (such as microscopic analysis [19], mammography [61] or magnetic resonance [49]), segmentation methods [38] or classification approaches [20] for this task. However, not much attention was paid to the problem of designing a decision support system for breast cancer malignancy grading [36].Classification of malignancy grading suffers from a well-known difficulty in Machine Learning, the class imbalance problem [29,22]. This problems arises when one class appears much more often than the other (we have many more cases from medium malignancy that from high malignancy), which leads to an uneven distribution of examples in the training set. This is a challenging problem in Machine Learning [67], since it usually brings along a number of difficulties such as overlapping, small disjuncts and small sample size [46]. For these reasons, specific methods to address these types of problems are needed.Imbalanced classification must be carefully addressed in the context of breast cancer classification. Standard classification methods tend to get biased towards the majority class, ignoring the minority class or treating it as noise. However, the minority class is the most important one – in the discussed application it corresponds to the highest grade of malignancy posing a significant threat to the life of a patient. Therefore, it must be detected with the highest precision, as one cannot allow for such a severe case to be mistreated.In this paper, we discuss the application of Pattern Recognition and Image Processing methods to extract the information from the FNA slides and automatically assign a malignancy grade to the case. In order to do so, we consider three different methods for segmenting cytological images and extracting features from them. Then, we apply a highly efficient ensemble fitted for handling difficult imbalanced problems. This method is based on a combination of boosting method [21] with evolutionary undersampling [26]. This allows us to propose a complete, automatic and highly efficient clinical decision support system that can be used in a daily physician routine.The main contributions of this work are as follows:•An automatic and complete clinical decision support system for breast cancer malignancy grading is developed.Different methodologies for segmenting fine needle aspiration biopsy (FNA) slides and extracting meaningful features from them are examined.An efficient classifier ensemble, specifically designed for imbalanced problems with difficult data distribution is considered. Boosting scheme is combined with evolutionary undersampling to obtain both accurate and diverse base classifiers.An extensive experimental analysis is carried out on a large database collected by the authors, showing that the proposed evolutionary undersampling boosting can outperform state-of-the-art methods dedicated to binary imbalanced learning, and hence proving the usefulness of the designed approach to breast cancer malignancy grading.The remaining part of this paper is organized as follows. Section 2 gives essential background about the problem of breast cancer malignancy grading. In Section 3, we discuss the three algorithms used to segment FNA slides. Section 4 introduces the imbalanced classification domain and reviews current algorithms and measures used in this field. The proposed evolutionary undersampling boosting ensemble is presented in detail in Section 5. In Section 6 the set-up used for the experimental analysis (methods used and their parameters), the results obtained and the discussion can be found, whereas Section 7 concludes the paper.Malignancy grading is one of the most important steps during cancer diagnosis. Based on that grading doctors are able to determine the appropriate treatment and, what is even more important, predict if the undertaken treatment is going to be successful. This examination is performed when suspicious regions in the breast tissue are found. For this purpose a mammography examination is done. When a suspicious region is found, a fine needle aspiration biopsy is taken. This is an invasive procedure that involves the extraction of a breast tissue with a syringe with outer needle diameter smaller than 1mm (typically between 0.4 and 0.7mm). The tissue is then place on a glass slide, stained and examined under a light microscope.This examination is based on the well defined scheme given by Bloom and Richardson [6] and called accordingly the Bloom–Richardson grading scheme. This scheme has underwent many modifications and currently the modification proposed by Scarff, called a modified Scarff–Bloom–Richardson system, is used to grade the breast cancer malignancy [55]. The grading scheme describes several features that are divided into three groups known as factors that assesses features in a point based scale.1.Degree of structural differentiation (SD). This factor describes cells’ ability to form groups. In histopathological slides, for which the grading scheme is also used, this factor describes cell tendency to form tubules. In the cytological smears tubules are not preserved and therefore cells’ groupings are examined. This factor is visualized in Fig. 1, where intermediate malignancy case with one group (Fig. 1a) and high malignancy case with highly dispersed cells (Fig. 1b) are presented.Pleomorphism (P). This factor examines differences in size, shape and staining of the nuclei11Nucleus is a a central organelle of a cell that contains most of the cell's DNA.. This scoring is fairly straightforward because the greater the irregularity of the nuclei is, the worse the prognosis becomes, as it can be observed in Fig. 2a.Frequency of hyperchromatic and mitotic figures (HMF). This factor assesses the number of visible mitosis in the image. Mitosis is the process in the cell life cycle in which a mother cell is divided into two identical cells. From the Image Processing point of view, mitosis can be observed as a dark stain in the nucleus. Here, the more mitotic cells are, the worse the prognosis is. An example of the mitosis is shown in Fig. 2b.All three factors of the modified Scarff–Bloom–Richardson scheme are assessed in a three-point scale, where one point is assigned to the least malignant case and three points are given to the highest malignancy one. Furthermore, the final grade is obtained summing up the quantitative values assigned to the three features (Eq. (1)) and following the chart presented in Fig. 3.(1)G=SD+P+HMF.Nonetheless, even though these factors are well-explained, the determination of cancer malignancy is a very difficult task and depends not only on the experience of the pathologist but also on his/her mind. More experienced pathologists that have seen more cases are more reliable in their diagnosis. However, due to overwork and fatigue, seeing more similar cases may lead to misclassification of the malignancy grade. In order to address this problem we present an automated grading approach that is able to evaluate and assign a grade to a FNA biopsy tissue, that is, we translate the modified Scarff–Bloom–Richardson grading scheme into a classification problem.One of the objectives of this paper is to perform a comparative study of three segmentation techniques to determine which one is the best method for nuclei extraction with respect to the posterior best breast cancer malignancy classification process. In computer vision, segmentation is a very important task because it influences all the subsequent stages such as feature extraction and classification. This is why it is extremely important to find a technique capable of extracting the nuclei accurately.In the proposed framework, the segmentation of breast cancer cells and nuclei is divided into two parts. In the first part we segment a low magnification image, whereas in the second a high magnification image is segmented (please refer to the dataset description in Section 6.1 for more details about the type of images). According to the modified Scarff–Bloom–Richardson scheme, for the 100× magnification images, we only need information about cells’ groupings. This is because we are interested in structure of cells’ groups, not in their individual shapes. We have found that for this purpose, a simple automatic thresholding method is sufficient to achieve an accurate segmentation. The threshold level was determined based on the bimodal histogram of the image with the algorithm described by Riddler and Calvard [53]. The segmentation results are shown in the Fig. 4.Otherwise, for the high magnification images, a more advanced segmentation algorithm needs to be applied. This is due to the fact that these images are used for the determination of cellular features, which need to be as accurate as possible. This is why a precise representation of the nucleus is required. For this purpose, we carry out a comparison of three segmentation techniques: fuzzy c-means color segmentation [37], level set active contours technique [47] and grey-level quantization method that uses a texture information for segmentation [28].The first segmentation method implemented in this study is the fuzzy c-means approach (FCM) proposed by Klir and Yuan [37]. It is focused on dividing the data X={x1, x2, …, xn} into c clusters assuming that there is a known pseudo-partition P={A1, A2, …, Ac} and Aiis a vector of all memberships of xkto cluster i. Applying Eq. (2) we can calculate centers of the c clusters [60].(2)vi=∑k=1n[Ai(xk)]mxk∑k=1n[Ai(xk)]m,i=1,2,…,c,where m>1 is a weight controlling the fuzzy membership. If∥xk−vi∥2>0for all i∈{1, 2, …, c} then the memberships are defined by Eq. (3). If∥xk−vi∥2=0for some i∈I⊆{1, 2, …, c} the memberships are defined as a nonnegative real number satisfying Eq. (3) for i∈I.(3)Ai(xk)=∑j=1c∥xk−vi∥2∥xk−vj∥21m−1−1,where ∑i∈IAi(xk)=1.We have noticed that values of m between 2 and 4 did not have significant influence on the segmented nuclei. Therefore the value of m=2 was used for segmentation.In order to segment an image we look for a set P that minimizes the performance index Jm(P) (Eq. (4)). The optimization solution to this problem can be found in [4].(4)Jm(P)=∑k=1n∑i=1c[Ai(xk)]m∥xk−vi∥2.The clustering method based on level sets (LS) is the second segmentation algorithm considered to extract precise nuclear information. Level sets belong to active contours models because they change their shape according to the information in the image. This property makes them a very good choice in biomedical applications [47].Level sets were introduced by Osher and Sethian [50] as a method for capturing moving fronts. This method relays on the determination of a surface Γ(t) described by Eq. (5). The surface propagates along its normal direction and is embedded as a zero level of a time–varying higher dimensional function ϕ(x, t) [50].(5)Γ(t)={x∈R3/ϕ(x,t)=0}.The determination of Γ, which is a closed curve in R2, requires a definition of an evolution equation for ϕ. This can be expressed in general form as [57]:(6)∂ϕ∂t+F|∇ϕ|=0.The function ϕ describes a curve defined by ϕ(x, t)=d, where d is a signed distance between x and the surface Γ. If x is inside (outside) of Γ then d is negative (positive). Function F is a scalar speed function that depends on image data and the function ϕ.Here ϕ is initialized as a signed distance function before evolution and is periodically reshaped to be a signed distance function. This is due to the fact that during the evolution, ϕ can assume sharp or flat shapes [43].In this study we have applied a modified level set approach of Li et al. [43] that overcomes the ϕ reshaping problem according to the evolution equation of a form:(7)∂ϕ∂t=−∂E∂ϕ,where∂E∂ϕis a Gateaux derivative of the energy functionEand is represented by:(8)∂E∂ϕ=−μΔϕ−div∇ϕ|∇ϕ|−λδ(ϕ)divg∇ϕ|∇ϕ|−νgδ(ϕ),where Δ is the Laplacian operator, div is the divergence operator, μ>0 is a parameter controlling the effect of penalizing the deviation of ϕ from a signed distance function, g is an edge indicator function, λ>0 and ν are constants. In this study we have used μ=0.04, λ=5.0 and ν=1.0.The third segmentation technique is a grey-level quantization (GLQ) based on the image textural description. It uses a second order statistic for the generation of grey level co-occurrence texture features [28]. In this method, for a spatial window inside the image, the conditional joint probabilities Cijare calculated according to Eq. (9) for all pairwise combinations of grey levels. We assume that the distance between the pixels is known.(9)Cij=Pij∑i,j=0G−1Pij,where Pijis a frequency of occurrence of two grey levels i and j and G is the number of quantized grey levels.The probabilities are stored as a gray level co-occurrence matrix, where the (i, j) element of the matrix represents the probability Cij. To identify textures within an image four features from the dependency matrix were derived. These properties are described by the following equations:(10)Entropy=−∑i,j=0G−1CijlnCij,(11)Contrast=∑i,j=0G−1Cij(i−j)2,(12)Inertia=∑i,j=0G−1(i−μx)(j−μy)Cijσxσy,(13)Energy=∑i,j=0G−1Cij2,where σ is the standard deviation and μ is the mean.To assess the segmentation results, all the images were visually studied by an expert pathologist to check which algorithm provided the best results. In Fig. 5, an example of the results of all three segmentation algorithms are presented. It can be noticed that level sets method provides the most accurate boundary representation. The main drawback of this method is that it requires an initial boundary representation that is later reshaped (initial level set). To make the proposed framework fully automatic we have obtained the initial level set as a result of the automatic thresholding method. Both fuzzy c-means and grey-level quantization are able to efficiently segment nuclei without any prior information about the boundary. From Fig. 5 one can observe that although FCM provides better nuclei segmentation than GLQ, the level sets based method is the one with the best representation of the nuclear boundary, whereas GLQ algorithm loses a lot of nuclear information during the segmentation. In Section 6, we will show the influence of this observations on the classification of the breast cancer malignancy.As we have already mentioned, the breast cancer malignancy grading classification considered in this study is an imbalanced classification problem, where the number of instances from one class is greater than the number of instances of the other class. As we describe in Section 6.1, there are 137 examples of class G2 (intermediate malignancy) and 39 of class G3 (high malignancy), whereas there are no cases of low malignancy. Hence, we are dealing with a two-class imbalanced problem, which is challenging problem in Machine Learning [67].On this account, in this section, we recall the problems that the imbalanced distribution of instances can produce (Section 4.1). Afterwards, we present the evaluation criteria that need to be considered in this framework (Section 4.2). Finally, we review some of the approaches considered to tackle this problem in the specialized literature (Section 4.3).A two-class classification problem is said to be imbalanced whenever the number of instances from both classes are not nearly the same, that is, one of the classes is under-represented. This fact produces a number of difficulties in the learning process and usually worsens the recognition rate of the minority (also named as positive) class [34]. In fact, the minority class is usually the most interesting one from the point of view of the learning task [29].The problem with imbalanced data-sets comes from the fact that standard classifier learning algorithms usually fail because they are designed to maximize accuracy rate (the number of correctly classified examples). As a consequence, they are biased toward the majority (negative) class, because being easier to learn it has a greater impact on the accuracy [33]. As a result, positive instances might be treated as noise and ignored, since general rules predicting the majority class produce better accuracy rates. An imbalanced data-set does not imply an added difficulty by itself [59,29,22] (the classes can be easily separable). However, when dealing with real-world problems it usually implies the appearance of several difficulties that hinder the classifier learning. For instance, small sample size [34], overlapping [27] or small disjuncts [65], whose analysis can bring about new research directions on the topic [46]. These difficulties are amplified in high-dimensional problems [68]. Additionally, novel problems arise when dealing with class imbalance for data streams [51] in non-stationary scenarios [62].The evaluation of the performance of a classifier is a key issue both to guide its modeling and to properly assess its quality with respect to other classifiers dealing with the same problem. When addressing a two-class problem, the results of the correctly and incorrectly classified examples of each class can be stored in a confusion matrix (Table 1).Although historically accuracy rate has been the most commonly used measure to evaluate the performance of classifiers (Eq. (14)), it is not suitable when facing a problem with an imbalanced class distribution. This is due to the fact that accuracy rate weights the influence of the classes depending on the number of instances, and hence classes with more instances have more influence on it, which is undesirable in an imbalanced scenario.(14)Acc=TP+TNTP+FN+FP+TN.On this account, other measures need to be considered in this framework, which take the into account the performance for each class independently. From the confusion matrix (Table 1), different measures evaluating the performance over each class independently can be deduced:•True positive rate (also known as Sensitivity)TPrate=TPTP+FN.True negative rateTNrate=TNFP+TN.False positive rateFPrate=FPFP+TN.False negative rateFNrate=FNTP+FN.Geometric Mean (GM) [2]: It considers a balancing between the accuracy over the instances of the minority and majority classes at the same time (Eq. (15)) being appropriate to deal with the class imbalance problem [26,23,24].(15)GM=TPrate·TNrate.Area Under the ROC Curve (AUC) [32]: Receiver Operating Characteristic (ROC) graphic [7] combines the measures obtained for each class to produce a valid evaluation criterion. ROC graphic allows one to visualize the trade-off between TPrate(benefits) and FPrate(costs), evidencing that increasing the number of true positives without also increasing the number of false positives is not possible for any classifier. Area Under the ROC Curve (AUC) [32] corresponds to the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. AUC provides a scalar measure of the performance of a classifier and it has been widely used in imbalanced domains [27,26,54]. AUC measure is computed as the area of the ROC curve:(16)AUC=1+TPrate−FPrate2.Due to the importance of the imbalanced data-sets problem, many techniques have been developed aiming at overcoming it. These approaches can be categorized into four groups [22]:1Algorithm level approaches (internal): The methods in this category consist of adapting existing classifier learning algorithms to tackle the class imbalance by biasing the learning procedure towards the minority class [44,2]. Their drawback is that they require special knowledge of both the classifier and the application domain.Data level (external): These methods balance the class distribution by data resampling [3,18]. Hence, they try to avoid the effects caused by class imbalance using a preprocessing step, which makes them independent of the classifier used, that is, more versatile. Well-known methods in this category are random undersampling, oversampling and Synthetic Minority Oversampling Technique) (SMOTE) [11].Cost-sensitive learning: This category falls between data and algorithm level categories. It requires both data level transformations (adding costs to instances) and algorithm level modifications (by modifying the learning process to accept costs) [13,70]. The classifier is biased towards the positive class assigning higher misclassification costs to this class and trying to minimize the total cost of both classes. An important drawback of these methods is that they need to define the costs for each class, which are hardly ever available in classification data-sets.Ensemble-based approaches [66]: These techniques have recently arisen as a new solution for the class imbalance problem with good results [22]. They usually combine an ensemble learning algorithm with one of the previous techniques, and more specifically, data level and cost-sensitive ones. In the case of data level approaches, a selected pre-processing algorithm is used before training each classifier of the ensemble to balance the class distribution [12,56,23]. Cost-sensitive ensembles work on the basis of costs in the ensemble learning algorithm, cost-sensitive evaluation can be used locally for each base classifier [52] or globally as an evaluation metric [58].The last category is the one in which we have focused to tackle the problem of the breast cancer malignancy classification due to the good performance shown by ensemble-based approaches, which have shown to outperform other models such as the commonly used data level methods [22,56]. More specifically, we focus on EUSBoost model [23], which combines Evolutionary Undersampling [26] with AdaBoost.M2 algorithm [21] (explained in the next section).In this section, we explain our proposal to overcome the class imbalance problem in breast cancer malignancy grading with ensembles of classifiers. The most common approach to address the class imbalance with ensembles is the introduction of a data preprocessing step which balances the data distribution. These methods are more versatile than those based on cost-sensitive ensembles [58] because the setting of the costs is avoided. The preprocessing-based ensemble methods can be further divided into Bagging-, Boosting-, and Hybrid-based ensembles, depending on the ensemble learning algorithm in which they are based [22]. An extensive empirical analysis of ensemble solutions for class imbalance was carried out in [22], where both Boosting [21] and Bagging [8] in combination with preprocessing techniques achieved the best results. Among the methods studied in [22], RUSBoost [56] was shown to be one of the most accurate approaches (it is based on random undersampling in combination with Boosting). However, a modification of this method called EUSBoost [23] was able to outperform RUSBoost by introducing the usage of evolutionary undersampling (EUS) [26], and therefore improving the accuracy and diversity of the base classifiers. For this reason, we consider this method for our proposal and we recall its operating procedure in the following subsections.First in Section 5.1, we present EUSBoost algorithm as an extension of Boosting using EUS with a modified fitness function. Afterwards, we recall EUS algorithm in Section 5.2. Finally, the modification of the original fitness function used in EUS in order to promote diversity is presented in Section 5.3.Before introducing the hybridization of Boosting and EUS, we should recall how Boosting [21] algorithm works. In Boosting classifiers are learned serially using the whole training set in all the base classifiers. However, the instances are weighted (starting with equal weights in the first iteration) and more focus is give to difficult instances after each round, aiming at correctly classifying in the current iteration those examples that were incorrectly classified in the previous one. EUSBoost is based on AdaBoost.M2 [21], which has been widely employed in imbalanced domains in combination with data level techniques. The main advantage of this AdaBoost variant is that it takes advantage of the confidences given by the base classifiers in the weight update.The combination of EUS with Boosting algorithm is direct, yet effective. The idea of RUSBoost and other Boosting-based algorithms [22] is followed, introducing the undersampling process in each loop of AdaBoost.M2 algorithm in order to balance the class distribution. In the case of EUSBoost, as well as in the rest of the approaches, the weights of the instances are only used in the learning of the base classifiers, whereas the undersampling process (using EUS) is carried out independently of them. The complete EUSBoost algorithm is presented in Algorithm 1. Notice that the new steps with respect to AdaBoost.M2 are the 7th and 8th, while the 9th is modified.•Step 7 – EUS is introduced, returning a new data-set (S′) which considers all the minority class instances and the selected ones from the majority class.Step 8 – The weights for the new data-set are computed.Step 9 – The classifier is trained. Even though the original data-set is maintained, those instances not present in the undersampled data-set have no weight, being ignored in the learning of the classifier.Algorithm 1EUSBoost, EUS embedded in AdaBoost.M2Input: Training set S={xi, yi}, i=1, …, N; and yi∈{c1, c2}; T: Number of iterations; I: Weak learner1:D1(i)←1/N for i=1, …, N {D is the weight distribution for the instances}2:wi,y1←D1(i)for i=1, …, N, y≠yi{w, W and qtare weights computed from D that used along the algorithm}3:fort=1 to Tdo4:Wit←∑y≠yiwi,yt5:qt(i,y)←wi,ytWitfor y≠yi6:Dt(i)←Wit∑i=iNWit7:S′ = EvolutionaryUndersampling(S);8:Dt′(k)←Wit∑xi∈S′Witifxi∈S′0otherwise9:ht←I(S,Dt′)10:ϵt←12∑i=1NDt(i)1−ht(xi,yi)+∑i,y≠yiqt(i,y)ht(xi,y)11:βt=ϵt1−ϵt{βtis the weight assigned to the tthclassifier}12:wi,yt+1=wi,yt·βt12(1+ht(xi,yi)−ht(xi,y))for i=1, …, N, y≠yi13:end forOutput: Boosted classifier:H(x)=argmaxy∈ℂ∑t=1Tln1βtht(x,y), where ht, βt(with ht(x, y)∈[0, 1]) are the classifiers and their assigned weights, respectivelyThe usage of EUS in the imbalance framework when constructing the ensemble allows one to better control the randomness behind the ensemble in such a way that the accuracy over the minority class can be boosted. Due to the initial randomness of the solutions of EUS, the resulting data subsets usually differ from one execution to another. EUSBoost benefits from this instability, since it helps maintaining the diversity (classifiers trained with identical data-sets are not useful to construct ensembles). However, a key factor of EUSBoost is the definition of a new fitness function for EUS, which takes into account the diversity of the instance subset obtained with respect to the already used ones in such a way that the final diversity of the ensemble is improved. This procedure is explained in Section 5.3, after recalling EUS in Section 5.2.EUS [26] is an evolutionary prototype selection algorithm adapted to work in imbalanced domains (it uses an appropriate fitness function). Prototype selection [25] is a sampling process aiming at reducing the reference set for the nearest neighbor (1NN) classifier in order to improve its accuracy and reduce the storage necessity. However, in an imbalanced scenario the objective differs, since the balance of the data distribution gains importance. On this account, EUS tries to obtain a useful undersampled data-set whose search is guided by a genetic algorithm. Initially, several randomly undersampled data subsets are created, which are then evolved until the currently best undersampled data-set cannot be further improved in terms of the fitness function. This algorithm has already shown its usefulness in real-world applications [16].Likewise in every evolutionary method, the way in which the solutions are represented by means of chromosomes is an important issue. In EUS, a binary vector is used to represent each solution, where each gene (binary value) represents the presence or absence of the corresponding instance in the data-set. Although all of instances could be codified in the chromosome, the search space can be reduced by only considering the majority class instances, whereas all the minority class instances are always introduced in the undersampled data-set. Therefore, a chromosome is represented as follows:(17)V=(vx1,vx2,vx3,vx4,…,vxn−),wherevxitakes the values 0 or 1, indicating whether instance xiis included or not in the data-set (n− stands for the number of majority class instances).In the evolutionary process, chromosomes are ranked using a fitness function, which in the case of EUS takes into account the balancing between both classes and the expected performance with the selected data subset [26]. In order to estimate this performance a hold-one-out technique is used with 1NN classifier and it is measured by the GM. Finally, the fitness function of EUS is as follows:(18)fitnessEUS=GM−1−n+N−·PifN−>0GM−PifN−=0,where n+ is the number of minority class instances, N− is the number of majority class instances selected and P is the penalization factor accounting for the importance given to the balance between both classes (whose recommended value is 0.2).In order to perform the search, the well-known CHC algorithm [17] is used due to its good balance between exploration and exploitation. CHC is an elitist genetic algorithm using the heterogeneous uniform cross-over (HUX) to combine two chromosomes (exactly half of the different genes are interchanged). An incest prevention mechanism is also considered where two parents are only recombined if their Hamming distance is greater than the threshold (initially L/4, being L the length of the chromosome); the threshold is reduced by one when no parents are recombined. In this genetic algorithm no mutation is applied, but when the recombined chromosomes are not able to improve their parents and the threshold reaches zero, the whole population (except for the best chromosome) are reinitialized. Reinitialization consists of using the best chromosome as a template, randomly changing 35% of its genes.In the case of EUS, the original HUX is modified to decrease the probability of including instances in the data-set in such a way that a good reduction rate is reached. To do so, each time HUX switches a gene on, it is switched off with a certain probability (the recommended value is 0.25).In order to improve the diversity of the base classifiers in EUSBoost, the original fitness function of EUS is modified. Only considering the randomness in EUS as a source of diversity may not be enough to provide different subsets of instances in each iteration of Boosting. Hence, diversity among data-sets is promoted in a supervised manner, which is not usually done.Diversity in classifier ensembles refers to the fact that they should be composed of base classifiers giving different outputs so that their combination can lead to significant improvements. Diversity is a key factor in ensembles and has been widely studied in the literature [10,64] even though no direct relation has been found between diversity and accuracy. However, in the imbalanced scenario diversity gains importance. In [64] it was found that there exists a relation between diversity and single-class performance measures, having a positive impact on the minority class classification, but also on global performance measures such as AUC.Since EUS is used as a preprocessing algorithm, the diversity of the outputs cannot be directly promoted. For this reason, the diversity among solutions is considered, that is, chromosomes that are different from the best chromosomes in previous iterations are preferred. Hence, it is assumed that base classifiers learned from data-sets with more different instances are more diverse (which is also assumed in Bagging, but in this case it is forced). To do so, a new fitness function is introduced, modifying the original evaluation procedure of EUS (Eq. (18)).First, diversity between solutions should be measured, which is done using the Q-statistic [69], which has been widely applied in classifier ensembles [42,64]. Recall that this measure is applied to compute the diversity between two solutions (Eq. (17)). Having two binary vectors (Vi, Vj), the Q-statistic is computed as follows:(19)Qi,j=N11N00−N01N10N11N00+N01N10,where Nabstands for number of instances with value a in the first vector and with value b in the second (if a=b both data-sets agree including or not the instance). The value of the statistic ranges from −1 to 1. Lower values of Q indicates greater diversity (Qi,j=0 means that both vectors are statistically independent).The Q-statistic is a pairwise measure [41], but the diversity between a candidate chromosome and the previously used best ones are computed. In order to aggregate all the pairwise values the maximum of all Qi,jis considered. In this way, the candidate instance subset that is the most dissimilar with respect to all the previous data-sets is considered. Therefore, being Vjthe candidate solution to be evaluated, and Vi, i=1, …, t (recall that t is the current iteration) all the previously used solutions, we compute the global diversity Q as:(20)Q=maxi=1,…,tQi,j.Once the evaluation of the diversity has been defined, EUS's fitness function is modified as follows:(21)fitnessEUSQ=fitnessEUS·1.0β·10.0IR−Q·β,where fitnessEUS is the original fitness function (Eq. (17)), IR is the imbalance ratio (the number of negative class examples divided by the number of positive class examples), Q is the global Q-statistic and β is a weight factor changing in each iteration:(22)β=N−t−1N.Notice that in Eq. (21) the Q term is subtracted to maximize the diversity. Furthermore, in the first iteration of the ensemble (t=1), EUS is executed in its original form (using Eq. (18)), since there are no vectors to compare with the actual candidate solution (for more explanation on this fitness function we refer the reader to the original paper of EUSBoost [23]).In this section we develop an exhaustive experimental study to check the usefulness of EUSBoost ensemble for the classification of breast cancer malignancy grading. Moreover, we compare the three image segmentation techniques described in Section 3 and discuss which one is the most appropriate to deal with this problem. The experimental analysis has two main research objectives:•To compare EUSBoost to a number of state-of-the-art methods dedicated to the imbalanced classification problem and check whether it can be useful in the process of designing a clinical decision support system for real-life application.To investigate whether there is a difference in the discriminative power of the features obtained through the three different feature extraction methods considered, and to check which one performs better with the proposed ensemble classifier.In the following subsections, we will describe the used dataset, bring details about the selected classification methods included in the comparison and their parameters, discuss the set-up of the experiments and present the obtained results together with the corresponding discussion.In this study we have used a database of FNA slides. The data was collected at the Department of Pathology and Oncological Cytology, Medical University of Wrocław, Poland. Currently the dataset consists of 341 images with a resolution of 96 dots per inch (dpi) and a size of 764×572 pixels. All of the images were taken with Olympus BX 50 microscope with mounted CCD-IRIS camera connected to a PC computer with MultiScan Base 08.98 software. Prior digitalization, the FNA slides were stained with the Haematoxylin and Eosin technique (HE) which yielded purple and black stain for nuclei, shades of pink for cytoplasm and orange/red for red blood cells.The organization of the images in the dataset follows the requirements of the diagnostic process in which two types of slides are taken into consideration. The first type (167 images) are the slides recorded in low magnification (100×) and the second type (174 images) are the slides recorded in high magnification (400×). The low magnification images are used to define features related to the degree of structural differentiation (see Section 2), that is, cells’ ability to form groups. 400× magnification images are the base for the calculation of features that reflect cells’ polimorphy and mitotic count. An extensive description of these features can be found in Krawczyk et al. [40].During examination there might be more than one high magnification image taken from one slide. This is the reason why in the database we sometimes have more than one 400× image for a 100× image. From the diagnostic point of view, this is caused by the fact that there are more than one suspicious region in the 100× image. Here, for the purpose of this study we have separated this cases and treated them as different cancer occurrences. For the classification purposes we have used a pair of images that consisted of one 100× image and one 400× image and based on the features computed (for feature extraction, please refer to Section 6.2), the malignancy grade was automatically assigned.In the dataset there are occurrences of intermediate (G2) and high (G3) malignancy grades. There are no images of low malignancy because there were no such cases at the Medical University of Wrocław, Poland since 2004. Here, we collected 268 images belonging to the G2 class and 73 to the G3 class which means that there are 137 cases of intermediate malignancy and 39 cases of high malignancy. This imbalanced dataset makes the classification scheme more difficult and was the motivation to perform a series of studies to achieve the best classification possible using ensembles designed to tackle this problem.All the images in the database were segmented according to the description in Section 3 and based on these segmentations a set of features was calculated. In this paper, we extracted 32 features that consisted of three features calculated from the low magnification images (100× magnification features) and 29 features from high magnification images (400× magnification features). All the features were determined according to the description of Krawczyk et al. [40] and they are summarized in Table 2.For images recorded with 100× magnification we have calculated three features that represent cells’ tendency to form groups. These are very important features during cancer diagnosis and therefore are taken into consideration in this study. For 400× images we have calculated 29 features in 5 categories. First category corresponds to binary features that are calculated based on the segmented binary image. These features allow us to estimate the polymorphy of the nuclei. The next category is formed of RST-invariant features (φ1–φ7) that are calculated based on the normalized central moments. Other two groups of features are histogram based ones and color histogram features, which are based on image histogram. The last group encompasses textural features, which are computed from a gray-level co-occurrence matrix and represent changes in nuclei texture. Textural information is important as it describes a chromatin changes inside each nucleus.In order to put the results obtained into context, we compare our method with several state-of-the-art algorithms dedicated to binary and imbalanced classification [22]. The list of used learning algorithms is given in Table 3, while their parameters are given in Table 4. Each ensemble uses C4.5 classifier as base learner. The parameter values were established through a grid-search procedure so that their performance is expected to be the best possible one.All experiments were done with the usage of combined 5×2 cv F test [1], which allows one to simultaneously perform cross-validation and to assess the statistical significance of the obtained results. Statistical significance level α=0.05 is assumed.

@&#CONCLUSIONS@&#
In this paper, we proposed a complete, fully automatic and highly accurate clinical decision support system based on ensemble classification. We have dealt with an imbalanced problem in which the minority class corresponded to the highest (and thus most important to detect) malignancy grade. We discussed three different methods for FNA slides segmentation and feature extraction. On their basis, we trained a novel ensemble classifier called EUSBoost. It combined a boosting scheme with evolutionary undersampling. This allowed us to perform a guided undersampling of the majority class, selecting the most important objects for the classifier training step. Additionally, by incorporating a diversity measure in the evolutionary algorithm we were able to assure that the classifiers are mutually complementary. Our method obtained the best results when features extracted from level set active contours technique were used, returning 95.43% of sensitivity, GM equal to 95.73% and AUC equal to 96.38. These excellent results allowed EUSBoost to outperform 13 state-of-the-art ensemble classifiers, which was backed-up with a thorough statistical analysis of the results.The proposed clinical decision support system can be easily implemented on a standard computer and may be a valuable aid to the everyday physician's routine.