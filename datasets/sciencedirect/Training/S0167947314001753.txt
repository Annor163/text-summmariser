@&#MAIN-TITLE@&#
A novel relative entropy–posterior predictive model checking approach with limited information statistics for latent trait models in sparse

@&#HIGHLIGHTS@&#
A novel Relative Entropy–Posterior Predictive Model Checking method is proposed.RE–PPMC with limited information checks the model fitness effectively.Compared with the original PPMC with PPP value, RE value is better.Compared with parametric bootstrapping, RE–PPMC is more feasible.The univariate information is robust with dual characteristics.

@&#KEYPHRASES@&#
Goodness-of-fit,Latent trait model,Limited information statistics,Parametric bootstrapping,Posterior predictive model checking,Relative entropy,

@&#ABSTRACT@&#
Limited information statistics have been recommended as the goodness-of-fit measures in sparse2kcontingency tables, but thep-values of these test statistics are computationally difficult to obtain. A Bayesian model diagnostic tool, Relative Entropy–Posterior Predictive Model Checking (RE–PPMC), is proposed to assess the global fit for latent trait models in this paper. This approach utilizes the relative entropy (RE) to resolve possible problems in the original PPMC procedure based on the posterior predictivep-value (PPP-value). Compared with the typical conservatism of PPP-value, the RE value measures the discrepancy effectively. Simulated and real data sets with different item numbers, degree of sparseness, sample sizes, and factor dimensions are studied to investigate the performance of the proposed method. The estimates of univariate information and difficulty parameters are found to be robust with dual characteristics, which produce practical implications for educational testing. Compared with parametric bootstrapping, RE–PPMC is much more capable of evaluating the model adequacy.

@&#INTRODUCTION@&#
Dichotomous data withkitems are common in social sciences and are considered as2kcontingency tables in the context of statistics. Latent variable models (LVM; for an overview, see  Bartholomew et al., 2011) provide a unified framework for the analysis of the latent structure of complex data. In particular, when the observed variables are categorical with continuous latent variables, it is referred to as a latent trait model (LTM). Note that, the LTM is often otherwise known as the item response theory (IRT) model. This type of models has been widely applied in many fields, especially for educational testing (e.g., to measure students’ abilities or to estimate item difficulty). There are several parametric estimation methods for LTM, e.g., the expectation maximization (EM) algorithm (Bock and Aitkin, 1981; An and Bentler, 2012), and the Bayesian approach (Patz and Junker, 1999; Yuen and Kuok, 2011; Yuen and Mu, 2012).However, the goodness-of-fit problem, which is an important step in the model building sequence, has not been completely solved. Traditional approaches include the Pearson chi-squared(χ2)and the log-likelihood ratio(G2), but they degenerate when the number of data pointsNis not sufficiently large and the number of itemskis not small (e.g.,kis around 10 butNis about 1500). In this case, the contingency tables are very sparse, since the total number of cells increases exponentially askincreases. For example,2kis around 103 or 106 whenkis 10 or 20, respectively.To address this problem, Bartholomew and Leung (2002) suggested using test statistics(Y2)in which information was only the second-order margin, and its distribution was approximated via a linear transformation of the chi-squared random variable by matching moments under the simple null hypothesis. Cai et al. (2006) incorporated the Jacobian to construct the covariance matrix of residuals under the composite null hypotheses. These are called the limited information statistics, compared with full information such asχ2andG2, since only the low-order margins are utilized. Maydeu-Olivares and Joe (2005) provided the theoretical framework for limited and full information. The limited information statistics approach is suitable for handling the sparseness problem, as the low-order margins are less likely to be insufficiently filled than the response cells. Furthermore, the low-order margins are quite sufficient to identify a LTM (Maydeu-Olivares and Joe, 2006). Nevertheless, the inversion of the asymptotic covariance matrix of the low-order residuals imposes computational problems (Maydeu-Olivares and Joe, 2008).Instead of taking the asymptoticp-values of test statistics with complicated variance–covariance matrix and its inverse, the parametric bootstrapping (PB) produces an empiricalp-value by sampling from the parametric null distribution from a frequentist perspective. A number of researchers (Langeheine et al., 1996; Bartholomew and Tzamourani, 1999; Genest et al., 2009) recommended the use of a PB procedure for full information statistics. Tollenaar and Mooijaart (2003) and Mavridis et al. (2007) investigated the appropriateness of using limited information statistics as well as full information statistics, but found the asymptotic and PBp-values are different, especially in the case of full information statistics. Moreover, PB is time-consuming when the latent variable dimension is larger than one.Another alternative is Bayesian inference, which provides not only an optimal estimation of the parameters but also the associated uncertainty. One well-known approach is the posterior predictive model checking (PPMC). The aim is to compare the observed data with replicated samples generated from the posterior distribution using a number of discrepancy measures. If the posited model is appropriate, the generated data and associated statistics will be similar to the originals. This concept was originated by Rubin (1984), and was later extended to include general discrepancies by Gelman et al. (1996a). Constructing the graph of the realized and predictive distributions was recommended in the literature (Gelman et al., 1996a; Sinharay et al., 2006; Levy, 2011) to assess model fit, together with the posterior predictivep-value (PPP-value). Sinharay et al. (2006) and Levy (2011) applied PPMC to IRT in order to examine item and person fit instead of global fit. In the present paper, we propose the Relative Entropy–Posterior Predictive Model Checking (RE–PPMC) approach to complement the original procedure so that an arbitrary assertion of graphical comparison can be avoided. The discrepancy measures used in this paper are full and limited information statistics, which are designed to assess model fit at the test rather than the item- or person-levels. Moreover, multi-dimensional models as well as a unidimensional model are considered.The paper is organized as follows: Section  2 presents the theoretical framework of the LTM; Section  3 describes discrepancy measures including full and limited information statistics; Section  4 introduces the procedure of the proposed RE–PPMC method; Section  5 reports simulation studies to investigate the performance of RE–PPMC and compares it with the original PPMC and PB; Section  6 analyzes some previously studied real data sets to illustrate additional insights of the proposed method.LVM provides a unified framework for the interpretation of multivariate data, and it is applicable to various research disciplines. One of its purposes is for dimension reduction, i.e., to reduce a large number of observed variables or items(k)to a much smaller number(q)of latent variables. Although Leung (2008) conducted three-dimensional analysis(q=3),qwas in most cases either 1 or 2 for categorical manifest variables. A further purpose is to measure certain latent quantities, e.g., cognitive abilities, and another to achieve conditional independence in order to explain inter-dependence among allkitems.Both manifest and latent variables can be continuous and categorical. In this paper, we focus on cases with binary manifest variables and continuous latent variables. Because the major model development in this area has stemmed from the field of educational and psychological testing, where the latent variables are conceived as traits, they are referred to as latent trait models (LTMs), or more usually the IRT model. The emphasis of this study is on proposing a novel method of checking model fit for LTMs. We concentrate on models with one or two factors.The specifications of one- and two-factor LTMs for binary data are described as follows. The vector of manifest variables is denoted byx=(x1,x2,…,xk)′withxi=0or 1; and the vector of latent variables is denoted byz=(z1,…,zq)′. Letp(xi=1∣z,α)denote the probability of a positive response to itemi(i.e.,xi=1) given latent student ability valuezwith parametersα. According to Bartholomew et al. (2011), this probability is given by:(1)p(xi=1|z,α)=exp(αi0+∑j=1qαijzj)1+exp(αi0+∑j=1qαijzj),i=1,…,k,whereαi0is the ‘difficulty’ parameter of itemiin education testing; andαij(j=1,…,q)are the factor loadings, also known as ‘discrimination’ parameters. Define ak-vectorα0that contains allαi0(i=1,…,k); ak-vectorαj(j=1,…,q)that contains allαij(i=1,…,k); and thek×(q+1)-matrixαthat contains all model parameters, i.e.,α=(α0,α1,…,αq). For the one-factor model,q=1; and for the two-factor model,q=2. One of the objectives of this paper is to investigate the effects of under- and over-fitting, so both the one- and two-factor models are considered.The latent variableszare assumed to follow independent standard normal distributions. Then the (unconditional) probability of therth observed response patternxr=(xr1,xr2,…,xrk)′can be written as(2)p(xr∣α)=∫−∞+∞ϕ(z)∏i=1k[p(xri=1∣z,α)]xri[1−p(xri=1∣z,α)]1−xridz,r=1,2,…,2k,whereϕ(⋅)is the probability density function of the standard normal random variable. For convenience of notation, definepr(α)≡p(xr∣α). The likelihood function is then expressed as(3)p(X∣α)=∏r2kpr(α)Nfr,wherefris the observed frequency of therth response pattern.Forkdichotomous items, there are2kresponse patterns. The vector of the sample (observed) frequencies for all response patterns is denoted byf=(f1,f2,…,f2k)′. The corresponding vector of the true probabilities under the model is denoted byp(α)=(p1(α),p2(α),…,p2k(α))′. The elements of the vectorp(α)can be computed from the Eq. (2). Then the vector of the estimated probabilities under the fitted model is denoted byp(αˆ)=(p1(αˆ),p2(αˆ),…,p2k(αˆ))′, whereαˆis the estimator ofα. The global fit (or test-level model fit) is assessed through the discrepancy between the estimated probabilitiesp(αˆ)and sample frequenciesf.This paper focuses on the goodness-of-fit measures with limited information statistics based on low-order margins denoted asYl(l≤5), with full information statisticsχ2andG2provides for comparison purposes.The Pearson chi-squareχ2and the likelihood ratioG2compare the observed and estimated frequencies across all response patterns. They are defined as follows:(4)χ2=1N∑r=12k(Nfr−Npr(αˆ))2pr(αˆ)(5)G2=2N∑r=12kpr(αˆ)ln(NfrNpr(αˆ)).Their asymptotic distributions are the chi-square distribution with2k−k(q+1)−1degrees of freedom. However, the asymptoticp-value (thep-value calculated from the asymptotic distribution of the test statistics under the null hypothesis) fails to reveal the nature of the discrepancy in the case of the assumed model in sparse tables. To obtain reliablep-values, the strategy of generating the empirical sampling distribution of goodness-of-fit statistics (χ2andG2) using PB has been proposed (Langeheine et al., 1996; Tollenaar and Mooijaart, 2003).Limited information statistics are closely related to the orders of margins. Let the estimated univariate (i.e. first-order margin) probabilities beπ1(αˆ)={p(xi=1|αˆ),i=1,…,k}, and the estimated second-order margin probabilitiesπ2(αˆ)={p(xi=1,xj=1|αˆ),i,j=1,…,k,andi<j}and so on. Low-order probabilities can be obtained using a(kl)×2kindicator matrixMlsuch thatπl(αˆ)=Mlp(αˆ)(l=1,2,…)(Maydeu-Olivares and Joe, 2005). Taking an example ofk=3, this relationship can be written as follows:(6)wherep(αˆ)=(p(000),p(100),…,p(111))′is the 23-vector that contains the probabilities for all possible response patterns; e.g.,p(100)is the probability of responding to the first item positively and the other two negatively.The first-order margin contains no information about dependencies among the items, but the margins with second order or above are able to capture this information. The low-order marginal frequencies are seldom sparse even in small data sets. Bartholomew and Leung (2002) proposed theY2statistics on the second-order margin:(7)Y2=∑i=1k−1∑j=i+1k(Nfij−Npij(αˆ))2Npij(αˆ)(1−pij(αˆ)),wherefijis the sample proportion responding positively to both itemsiandj; andpij(αˆ)is the corresponding probability estimated from the fitted model.In this paper, limited information statisticsYlare considered and defined as follows:(8)Y1=N(fl−πl)′(diag(Ωl))−1(fl−πl),l=1,2,…,wherefl=Mlf; andΩl=Ml(diag(p(αˆ))−p(αˆ)p(αˆ)′)Ml′. The uncertainties of these statistics will be handled by the proposed RE–PPMC method, as described in the next section.Our objective is to develop a Bayesian procedure to quantify the discrepancy between a prescribed LTM and the observed data. In this approach, relative entropy based on posterior predictive model checking is proposed to assess the goodness-of-fit.Letp(θ)denote the prior distribution of the parameters; and letp(X∣θ)be the likelihood function of a parameter model, where the vector of the unknown parametersθcontains two groups of components: item parametersαand latent variables (nuisance parameters here)z. Then the posterior probability distribution ofθ, denoted byp(θ∣X), is proportional to the product ofp(X∣θ)andp(θ), according to the Bayes’ theorem. PPMC is a Bayesian method for assessing model fit by examining whether the posterior predictive distribution of replicated dataXrepis consistent with the observed dataX(Rubin, 1984). The replicated data is drawn from the posterior predictive distribution:(9)p(Xrep∣X)=∫p(Xrep∣θ)p(θ∣X)dθ.The difference between the replicated and observed data can be illustrated graphically, but only by the way of subjective judgment. To obtain an objective assessment, the PPP-value (Meng, 1994) is defined as the tail-area of the posterior predictive distribution for a selected discrepancy measureD(⋅):(10)PPP-value=p(D(Xrep,θ)≥D(X,θ)|X)=∫I(D(Xrep,θ)≥D(X,θ))p(Xrep∣θ)p(θ∣X)dXrepdθ,whereI(⋅)is the indicator function taking the value of 1 if its argument is true and 0 otherwise. Discrepancy measures should be capable of revealing any significant discrepancy between model prediction and data. In this study, we consider the full and limited information statistics described in Section  3 as the discrepancy measures. The difference between the realized discrepancy measureD(X,θ)based on the observed data, and the predictive measureD(Xrep,θ)based on the replicated data, is indicative of misfit.If the postulated model fits the data well, the replicated dataXrepwill be similar to the observed dataX, and hence the PPP-value will be around 0.5 (Bayarri and Berger, 2000). An extreme PPP-value (e.g., smaller than 0.05 or larger than 0.95) implies a poor fit (Sinharay et al., 2006). However, PPP-values are conservative in some cases, as it uses the indicator function, only counts number of data points whereD(Xrep,θ)≥D(X,θ), but not the magnitude.Gelman et al. (1996a) mentioned that the plot of the realized discrepancy compared with the posterior predictive distribution was far more valuable than the PPP-value itself in most cases. However, the graphical assessment is ambiguous about the magnitude of discrepancies. In this paper, a novel RE–PPMC method which utilizes the information of the whole distribution is proposed to measure the difference between the realized and predictive distribution using the relative entropy (RE):(11)DKL(P∥Q)=∫ln(p(D(X,θ))p(D(Xrep,θ)))p(D(X,θ))dXdθ,wherep(D(X,θ))andp(D(Xrep,θ))are the probability density functions (PDFs) ofD(X,θ)andD(Xrep,θ)respectively.RE–PPMC is based on the general concept of relative entropy (also ‘Kullback–Leibler divergence’;  Kullback and Leibler, 1951; Sweeting et al., 2006), which is an asymmetric measure of the difference between two probability distributions. Specifically, it takes the expectation of the differences between two densities with respect to the realized posterior. The RE will yield a high value if the predictive dataXrepis not close to the observed dataX. Therefore, the smaller the RE value, the better the model fits the data.In practice, direct calculation of the integral in Eqs. (10) and (11) is computationally prohibitive, but it is more easily accomplished via Monte Carlo simulation. In other words, the required computation is typically a by-product of the usual Bayesian simulation, such as Markov chain Monte Carlo (MCMC), which provides a set ofθdrawn from the posterior distributionp(θ∣X).The RE–PPMC procedure is introduced as follows.Step 1: estimate the model parameters by the MCMC algorithm, and generateMsamples in the parameter space to obtain the posterior distribution forθ. For more details, please refer to Appendix A.Step 2: for each pointθmin the posterior distribution in Step 1, draw a replicated data setXrep,m, and then compute the values of the realized and predictive discrepancy measuresD(X,θm)andD(Xrep,m,θm)respectively.Step 3: calculate the RE between the realized and predictive distributions, using the kernel density estimates to represent the PDFs ofD(X,θ)andD(Xrep,θ)respectively. This can be computed by using the function ‘ksdensity’ in MATLAB, with automatic bandwidth selection (Botev et al., 2010). The normal kernel function is used and evaluated at specified values in vector(D(X,θ1),…,D(X,θM))′and(D(Xrep,1,θ1),…,D(Xrep,M,θM))′respectively.Step 4: assess the global fit using RE values:(12)DKL(P∥Q)≈1M∑i=1Mln(pˆ(D(X,θm))pˆ(D(Xrep,m,θm))),wherepˆ(D(X,θm))andpˆ(D(Xrep,m,θm))are the kernel density estimates obtained in Step 3.While the PPP-value only calculates the proportion of data points whereD(Xrep,θ)bigger thanD(X,θ), the relative entropy value measures the magnitude of the discrepancy between two probability density functionsp(D(X,θ))andp(D(Xrep,θ)). The PPP-value utilizes the tail area probability for the Bayesian test, and the RE value quantifies the information loss.Though the PPP-value provides a straightforward criterion for comparing the realized discrepancyD(X,θ)and predictive discrepancyD(Xrep,θ), it fails in certain other situations. Even if the PPP-value is around 0.5, it may represent different scenarios.  Fig. 1shows a typical case of the realized and posterior predictive distributions for two different data sets in two different ways:x–yplots and frequency distributions. In the upper layer, twox–yplots of replicated against realized distributions are displayed. In the lower layer, correspondingly both realized and predictive frequency distributions are plotted in the same graphs for comparison.According to Eq. (10), the PPP-value is estimated as the proportion of points above the 45° line in the upper layer of Fig. 1. Hence the PPP-values of the two scatterplots are close to 0.5, implying good fit in both cases. However, the standard deviation of predictive measuresD(Xrep,θ)is much larger than that of realized measuresD(X,θ)in the left top sub-plot, compared with the right top sub-graph. These distinctions are clearer in the graphs of the corresponding probability density function given in the lower layer of Fig. 1. For the data set in the left sub-plot, the distribution of realized discrepancy is peaked while the predictive one is flat, indicating model misfit. This is typically the case for full information and is described below. For the data set in the right sub-graph, the two curves are virtually overlapping, implying a good fit. Again, this is typical of limited information, as will be explained later.The above-mentioned example shows that the PPP-value is insufficient to assess model fit, and that graphical assessment is therefore important. The PPP-value scale based on tail area probability has become ossified at cut-points that tend to exaggerate the evidence against the null (Berger and Sellke, 1987). In other words, this property of the PPP-value implies that the test could fail to reject an inadequate posited model. This typical conservatism was also noted by Meng (1994) and Rubin (1998). Robins et al. (2000) pointed out that the distribution of the PPP-value was concentrated around 0.5 but less dispersed than a uniform distribution, and thus had conservative performing characteristics.Graphical display is a useful supplement but ambiguous on occasions, and it is desirable to develop an objective indicator for this purpose. The proposed RE–PPMC approach fulfils the goal. As mentioned above, the PPP-values of Fig. 1 are both close to 0.5, but the RE values are 1.07 and 0.01 respectively, indicating substantial underlying differences.To verify the effectiveness and efficiency of the proposed method, simulated studies are used with different item numbers, factor dimensions and sample sizes. The true item parametersαof three models used for data generating in simulation studies are listed in Table 1. Based on Eq. (2), the true probabilities of each cell in Models 1, 2 and 3 are obtained. Sampling is accomplished by drawingNrandom numbers from a uniform distribution in the interval (0,1) and classifying numbers into the cumulative true probability vectors to determine the frequency of each cell. Therefore, the simulated data sets under each simulated scenario are generated seriatim. The computer programs were written in MATLAB, version 7.14 for Windows 7 on an Intel Core i5, 2.9 GHz with 4 GB RAM, and the following results were based on this configuration.In Model 1, there are five items with one factor (or latent variable), and only the first five items(k=5)forα0’s andα1’s are applicable. Theseα’s are almost the same as the maximum likelihood estimates (MLE) of the Social Life Satisfaction (SLF) data (Bartholomew et al., 2011, p. 113). Hence, if the SLF data and simulated data are both generated from the one-factor LTM with similar parameters, the corresponding statistics will follow similar patterns, and this will be reported later. In Model 2, there are ten items(k=10)with one factor, and allα0’s andα1’s in the second and third columns are used. Model 1 comprises the first five items of Model 2. In Model 3, there are ten items with two factors and allα’s (i.e.,α0’s,α1’s andα2’s) are needed, and the vectorsα1andα2are enforced to be orthogonal to each other so that a two-factor model is represented. These three models are selected to cover the cases of five and ten items with one and two factors.Consider five values ofN,500,1000,1500,3000and 10,000, to investigate the effect of sample size on the goodness-of-fit. The first four values are commonly encountered in practice, while the last serves to illustrate the limiting behavior.The above procedure simulates data from known null distribution and fits the same model. To demonstrate the effects of fitting an inappropriate model, some data sets are generated from alternative hypotheses. As described above, the main purpose of LTM is for data extraction and modeling of item interdependence, and the types of alternative considered are under- and over-fitting. Under-fitting refers to a fitted model that is insufficient to provide a reasonable representation of a data set, e.g. fitting the data drawn from Model 3 (a two-factor model) with a one-factor model in simulation studies. For over-fitting, a fitted model is over-fitted to the data with unnecessary excessive components to explain idiosyncrasies, e.g. fitting the data generated from Model 2 (a one-factor model) with a two-factor model in simulation studies.Tables 2–4report the PBp-values, PPP-values and RE values respectively, where both fitting and data-generating models are associated with the same factor dimension.In the case of PB, thep-value is defined as the tail area from the observed discrepancy measures to the end, and so ap-value <0.05 indicates statistical significance and hence poor fit. For all discrepancy measures and sample sizes, thep-values are substantially larger than the significant 0.05 value, and most of them are larger than 0.2, implying good fit.In the case of the PPMC method, the PPP-values around 0.5 indicate good fit. Results show that all PPP-values range from 0.4 to 0.6 for limited information statistics and all sample sizes, indicating good fit. The range of PPP-values based on the full information statistics is 0.2–0.9, showing a wide degree of goodness-of-fit, because of the vulnerability towards sparseness.With the proposed RE–PPMC approach, the relative entropy is incorporated with PPMC to assess the goodness-of-fit. The smaller the RE value, the better the model fits the data. When full information statistics are used as discrepancy measures, the RE values are mostly larger than 1.0 for Models 2 and 3. For instance, in the case of Model 2 withN=1500, the relative entropies of full information statistics (χ2andG2) are 0.771 and 1.071 respectively. On the other hand, when limited information statistics are used, the RE values are very small, with only three values larger than 0.1 but less than 0.2. Again, in the same example, the RE values of the limited information statistics (Y1toY5) are 0.010, 0.023, 0.043, 0.055 and 0.061 respectively. Hence, the RE value is much smaller in the case of limited information statistics.To verify the results by RE–PPMC, graphical displays are used to visualize the performance of each discrepancy measure. If the model is adequate for the data, two curves will be similar or overlapping; in this case, the PPP-value is close to 0.5, and the RE value is very small, e.g. smaller than 0.1. For economy of space, only certain typical cases are selected for illustration, but others behave in a similar way. Fig. 2compares the performance of graphical diagnostic assessment between full and limited information for Model 2 with 1500 examinees. The left top subplot shows that the predictive values ofχ2are mostly larger than the realized values. The PPP-values ofχ2andG2are 0.758 and 0.575 respectively. These non-extreme values, especially in the case ofG2, suggest good model fit. However, the top sub-graphs both demonstrate that the distribution of the realized discrepancy has an obvious sharp peak while the predictive discrepancy is much flatter, i.e. the standard deviation of the realized discrepancy is much smaller. The lower sub-plots provide graphical evidence of good fit when limited information statistics are used, where the predictive values are similar to the realized. Full information statistics are therefore not suitable to assess model fit using RE–PPMC, because full information statistics based on cells are more vulnerable than limited information in sparse2ktables.Overall, the proposed RE–PPMC with limited information statistics is preferable for checking model fit. From this point onwards, limited information statistics are considered to be the only discrepancy measures.Here, the data sets are generated from the two-factor model and fitted with the one-factor model. Results are given in Table 5. ForY1, regardless ofNand models, all indicators, PB p-, PPP- and RE-values, suggest good fit. With PB, thep-values ofY2,Y3andY4are all statistically significant (i.e. <0.05), indicating under-fitness, but marginally significant in the case ofY5withN=1000and 1500. Generally, thep-value indicates poor fit in under-fitting cases but with only minor irregularities.For small value ofN,500,1000or 1500, the PPP-values range from 0.4 to 0.6, and are not capable of indicating under-fitting. For example, whenN=1500, Fig. 3shows that the curves of realized and predictive discrepancies virtually overlapping. In particular, the corresponding PPP-values are around 0.5 and the RE values between 0.1 and 0.2 forY2toY4, indicating some degree of under-fitness. In contrast, no significant discrepancy is detected in the graphs.For larger value ofN,3000or 10,000, the PPP-values deviate significantly from 0.5, indicating under-fitting, especially for whenN=10,000. WhenN=3000, the RE values forY2toY5are 0.915, 1.296, 1.144, and 0.755, respectively, implying discrepancies between the posited model and the data. The graphs indicate the same conclusion, where the two curves start separating. From Fig. 4, the means of the realized distributions ofY2toY5are larger than the predictive distributions with a shift of location, suggesting that the realized discrepancies are larger than the predictive ones, and hence demonstrate under-fitness. Moreover, the RE value expands with an increasing value ofN, where the two corresponding curves are further apart from each other. A similar phenomenon can be observed in the analysis of real data below.The general pattern is that the larger the value ofNthe larger the RE value and the proposed method is more capable of detecting under-fitting. In the simulation studies, good fit is associated with RE values which are of good fit if less than 0.1, while moderate fit is associated with RE values between 0.1–0.2; otherwise, poor fit is the result of RE values larger than 0.2. This will be used as a reference point in analyzing real data below.Robustness of the first-order margins fitIn the simulation studies,Y1indicates good fit in all cases. In other words, the first-order margin is robust, and this is reflected in the difficulty parameterαi0’s. Table 6reports the true and estimated values ofα0, together with the first-order margins, and they are ordered according to the true value ofα0in the second column. The true and estimated values are similar for both the values ofα0and the first-order margins. The estimated difficulty parametersαˆ0and first-order margins not only have the same ranking, but are also virtually correlated (correlation coefficient =0.994). To conclude, the number of factors has no effect on difficulty parameters.The robustness ofY1based on the first-order margins has a dual character. On the positive side, the difficulty parameters estimated from one factor or one-parameter IRT, including Rasch models, can be trusted even though the underlying models are of two or more factors. On the negative side, one should be aware that the models used may not be sufficient to explain the dependency among items, even though the univariate margins are satisfactorily fitted.In this section, the data sets are simulated from the one-factor model but fitted with a two-factor model to study over-fitness (see Table 7). All empirical PBp-values, PPP-values and RE values suggest good fit. This is consistent with the graphs which are similar to Fig. 2, with realized and predictive distributions overlapping. This is because the criteria used are discrepancy measures based on accuracy, without penalizing unnecessarily complicated parameterization. Hence, further research using other criteria and purposes is needed, e.g. utilizing Bayesian model class selection (Yuen, 2010).Comparing the parameter estimates with the corresponding true values, the difficulty parametersα0are also found to be robust. In addition, the true discrimination parametersα1exhibit a linear relationship with the estimatesαˆ1andαˆ2under the posited model in an over-fitting case (correlation coefficients of 0.884 and 0.826 respectively). These results explain the way in which all proposed statistics using PPMC and PB indicate good fit.While PB uses the MLE for data generation, PPMC uses samplings from the posterior distribution of the parameters. Most computational effort in the case of PB is devoted to the MLE from each sample, e.g. using the EM algorithm. Furthermore, fitting a two-factor model is much more computationally expensive than a one-factor model, as it involves considerably more double integrals. The computational expense grows exponentially with the latent dimensions (or number of factors) and bootstrap-replicates. To the contrary, PPMC spends most of the computing time, around 85%, on building the posterior distribution through MCMC, and the rest 15% on computing the RE value. In this way, RE–PPMC is more efficient than PB, especially in the case of a two-factor model fit.For one-factor models, both PB and RE–PPMC require approximately 20 min with 1000 bootstrap replicates and 5000 Markov-chain samples respectively. However, for Model 3 with two factors andk=10, PB takes almost a whole day, while RE–PPMC takes only an hour. In a large-scale examination where the number of itemskis much larger than 10, assessing data-model fit via PB may take a month or a year, which is not feasible in practice. In short, RE–PPMC has critical advantages over PB in the terms of computational practicality for two or more factors and with largek.Previously studied real data sets are considered, in order to provide a clear exposition of the performance of the proposed method. The first is the SLF data described above, which has been analyzed in many studies. The second and third data sets consist respectively of the 10- and 8-item sexual attitude data (referred to ‘Sex10’ and ‘Sex8’) of the British Social Attitudes survey of 1990 (Bartholomew and Leung, 2002). The original data covered ten items (Sex10), but Bartholomew and Leung (2002) found that only eight items (Sex8) were lined up with one factor.The sparseness problem is not serious in SLF data, asN=1490andk=5, but some cells have a frequency of less than 5. Theχ2andG2of one-factor LTM yield values of 38.92 and 39.09 respectively, both with 21 degrees of freedom, indicating poor fit at the 1% significance level. Bartholomew and Leung (2002) investigated the asymptotic results of the full information statistics, as none of the contributions from second- and third-order margins was individually larger than the critical values at the 5% level. They usedY2to analyze and suggested good fit when parameters are fixed at MLE. Cai et al. (2006) produced opposite results when the effect of parameter estimation was taken into account.In fitting a one-factor model to SLF data, the MLE of parameter values are almost identical to those in Model 1 in this paper. Therefore, if the SLF data is generated from the one-factor LTM, one would expect the corresponding statistics and graphs to be very similar to those simulated from Model 1. Table 8, however, reports dissimilar results. The PBp-values indicate statistical significances which imply poor fit. Though the PPP values are close to 0.5, the graphs in Fig. 5reveal some departure of the predictive posterior distribution from the realized. As graphs are associated with subjective interpretation, RE–PPMC provides an objective reference. ExceptY1, all RE values are larger than 0.2, indicating poor fit and consistency with PB results. Moreover, the RE values suggest further departures of the predictive posterior distribution from the realized inY4andY5than those inY2andY3, and explain whyY2andY3do not produce very large departures.There has been no previous instance in the literature of a two-factor model for SLF data. When a two-factor model is used for data, goodness-of-fit as measured by traditionalχ2=29.32andG2=28.82, with 16 degrees of freedom, yields correspondingp-values of 0.022 and 0.025 respectively. Nevertheless, good fit is suggested through PB, PPMC and RE–PPMC, especially with the RE values in Table 8 and the graphs in Fig. 6displaying much better fit. This further supports the proposition that traditionalχ2andG2are not suitable for goodness-of-fit assessment. As a form of further explanation, the rotated standardized factor loadings are plotted in Fig. 7, and the exact wording of items appears in Appendix B. There are two clear-cut factors extracted from all items. Items 1 and 5 are loaded on to one factor, related to ‘any people’, while items 2–4 are loaded on to the other, related to ‘poor people’. These all demonstrate the power of LTM to identify latent variables when the observed variables are binary.The second example turns to a case of the problem of severe sparseness with 10 items and 1077 respondents. Traditionalχ2andG2with asymptotic distributions are not invalid. Previous studies have shown poor fit with the one-factor model by measuring the observed second- and third-order margins (De Menezes and Bartholomew, 1996; Bartholomew and Leung, 2002; Bartholomew et al., 2002). Results are shown in Table 9.ExceptY1, the extreme empiricalp-values calculated from 1000 bootstrap samplings, reported in Table 9, show that one latent variable fails to explain attitudes towards sexual behavior. This is consistent with the corresponding results produced by all PPP-values of discrepancy measures exceptY1, all far from 0.5. The RE values are much larger than 1.0 (RE ofY2is 4.49 and the others are all infinite). The predictive values of the test statistics exceptY1are very different from the realized ones (shown in Fig. 8), providing stark evidence of the inadequacy of one-factor fit. For all limited information statistics exceptY1, the realized distribution always has a larger mean than the predictive distribution, indicating that these realized discrepancies are always larger than the predictive ones. When compared with those in simulation studies withN=1500in the under-fitting case (i.e. Section  5.2), the one-factor model is obviously inadequate.When the second factor is included, the PPP-values are close to 0.5. Comparing the RE values shown in Table 9 and the graphs in Figs. 8 and 9, it is clear that fitting with a two-factor model is a considerable improvement. After checkingα, the first eight items are in one factor whilst the last two are in the other, the same conclusion reached by Bartholomew and Leung (2002).Although allp-values via PB and PPP-values indicate good fit, the RE values imply that there are some departures in the case of the two-factor model. The RE values ranging from 0.18 to 0.40, show a discernible improvement from a one-factor model, but they do not suggest a very good fit. One way to improve this is to remove items. Because the last two items, concerned with adoption by homosexual couples, give the major contribution to poor fit (Bartholomew and Leung, 2002; Bartholomew et al., 2002), further analysis is conducted with the last two items omitted and being denoted as Sex8.Fitting a one-factor model to Sex8, the PPP-values are close to 0.5, but the RE values ofY2toY5are 0.515, 1.048, 1.283 and 1.266, all larger than 0.2 and implying that the one-factor model is a poor fit. Thep-values based on PB are smaller than 0.05 (exceptχ2andY1), again indicating poor fit. Hence, the fit with the one-factor model is still questionable, even after removing two items. (see Fig. 10.)Bartholomew and Leung (2002) reported fitting with 8 items on one-factor only; the present study adds to the analysis with the two-factor model. After adding another factor, the RE–PPMC results listed in Table 9 and the graphical assessment in Fig. 11show that the two-factor model fits well and offers better interpretations in terms ofα.The rotated standardized factor loadings forα1andα2are plotted in Fig. 12with 8 items. Note that items 3 and 4 have low loadings on the first factor and high loadings on the second, while items 6–8 have high loadings in the second factor and low loadings on the first. These findings support a two-factor model. However, items 2 and 5 lie in between, making it difficult for analytical tools to distinguish first and second factor loadings. According to the item content (exact wording of items appears in Appendix B), the first factor is concerned with homosexuals teaching or holding public position (items 6–8), and the second factor can be interpreted as premarital and extramarital sexual relations (items 3 and 4). Items 2 and 5 focus on sexual discrimination.

@&#CONCLUSIONS@&#
