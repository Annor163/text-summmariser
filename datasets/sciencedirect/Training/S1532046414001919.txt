@&#MAIN-TITLE@&#
SAGA: A hybrid search algorithm for Bayesian Network structure learning of transcriptional regulatory networks

@&#HIGHLIGHTS@&#
We propose an effective hybrid method for inferring Bayesian Networks with priors.Our method, SAGA, performs better than its components on high dimensional datasets.SAGA infers Bayesian Networks (BNs) with higher sensitivities and specificities.Inference with BNs is enhanced by the inclusion of prior network relationships.

@&#KEYPHRASES@&#
Bayesian Network,Inference,Search algorithms,Transcriptional regulatory network,Microarray dataset,

@&#ABSTRACT@&#
Bayesian Networks have been used for the inference of transcriptional regulatory relationships among genes, and are valuable for obtaining biological insights. However, finding optimal Bayesian Network (BN) is NP-hard. Thus, heuristic approaches have sought to effectively solve this problem. In this work, we develop a hybrid search method combining Simulated Annealing with a Greedy Algorithm (SAGA). SAGA explores most of the search space by undergoing a two-phase search: first with a Simulated Annealing search and then with a Greedy search. Three sets of background-corrected and normalized microarray datasets were used to test the algorithm. BN structure learning was also conducted using the datasets, and other established search methods as implemented in BANJO (Bayesian Network Inference with Java Objects). The Bayesian Dirichlet Equivalence (BDe) metric was used to score the networks produced with SAGA. SAGA predicted transcriptional regulatory relationships among genes in networks that evaluated to higher BDe scores with high sensitivities and specificities. Thus, the proposed method competes well with existing search algorithms for Bayesian Network structure learning of transcriptional regulatory networks.

@&#INTRODUCTION@&#
Bayesian Networks (BNs) are useful for reverse-engineering gene networks from microarray data. BNs are probabilistic in nature. BNs are, therefore, becoming increasingly useful for the inference of cellular networks, modeling signaling pathways and analyzing genetic data [1–3]. Unlike other algorithms for inferring relationships among genes such as the ordinary differential equation (ODE)-based methods which generally involve time series data and low dimensional datasets, BNs (including dynamic BNs) can effectively infer relationships from both large steady-state expression measurements (and time series measurements) [4–8]. Furthermore, clustering algorithms and information-theoretic-based methods only establish the existence of relationships among variables in the dataset but do not describe the specific directionality of relationships [9]. On the other hand, BNs infer causal relationships (when the causal Markov assumption holds) making them more desirable than many inference algorithms.However, structure learning of BNs from data is an NP-hard problem and so many heuristic methods have been proposed to solve search problems [10,11]. Most of the proposed methods are local search methods. The number of possible direct acyclic graphs (DAGs) grows exponentially for increasing numbers of nodes [12]. For this reason, we review below a number of search approaches that have been proposed to search for BN structures among DAGs.Since finding the optimal BN is NP-hard, most researches have involved reducing the search space of DAGs for algorithms. In the Sparse Candidate Algorithm, the super-exponential search space of DAGs is restricted by limiting the parents of each variable to a smaller set using measures of dependencies between variables that are based on a refined Mutual Information measurement [13]. Though this method saves computational time, it does not guarantee an exhaustive search because of the restrictions in the search space. In another approach, the k-greedy equivalence search algorithm, different local optima are explored in repeated greedy searches. The randomness of the repeated greedy equivalence search in the k-greedy equivalence search algorithm enables it to search good local optima [14]. The approach by Gamez and Puerta restricted the set of parent nodes, took advantage of the properties of locally consistent metrics and removed some nodes from the possible parent nodes as the search proceeded [15]. At the start of the search, no parent set to any variable is restricted permitting all neighbors to be explored. However, some of the variables are removed from the candidate parents based on the metric leading to a restriction on the candidates to be explored. This method requires prior knowledge of the parent sets among the variables. Such prior knowledge is rarely available for prediction. On the other hand, the A∗search algorithm learns the optimal Bayesian Network by exploring only the most promising parts of the search space [16]. At each step of this search, the node with the smallest cost from the priority queue (OPEN list) is selected to expand its successor nodes before it is placed in a CLOSED list. Although this has improved search time and efficiency, it does not guarantee an exhaustive search as it explores only part of the search space.Furthermore, heuristic approaches such as the hill-climbing method of the greedy search methods are among the most widely used search strategies for BN structure learning. However, a greedy search is prone to convergence on a local optimal solution which might not necessarily be sufficiently close to the global optimum. Nevertheless, the Greedy search produces fairly good results when the search is repeated several times; starting the search each time with a different randomly generated initial solution [17]. Further, a hybrid approach that combines the greedy search with a reducing search space exists [18]. This involves progressively restricting candidate solutions by identifying a set of forbidden parents to each node, while performing an iterated hill-climbing search [18]. The authors add variables to forbidden nodes if the conditional independence between variables and their parent sets cannot be ignored because of differences in scores as in Gamez et al. [18]. Such an approach reduces the search space for subsequent iterations and hence minimizes the time of search while producing high quality networks.A version of Simulated Annealing (SA) has also been used to search for good BN structures [19,20]. In this method, the search starts at an initial temperature T0, perturbs the existing structure and evaluates it. The new structure is accepted if it improves the network score. However, if the new structure decreases the network score, it is accepted based on a functional value depending on the temperature of the system [19]. The initial temperature is lowered gradually until a putative temperature is reached to decide the termination of the search. Though SA performs well for large datasets, its performance is not improved with increasing time, thus limiting the scores of networks found [20]. Simulated Annealing with re-annealing (SAR) has sought to improve Simulated Annealing by permitting temperature to rise in later parts of the search. This strategy enables the method to escape local optimal solutions. All of these methods are sequential approaches to the search for solutions. However, Jaakkola et al. have also used linear programming techniques to infer Bayesian Network structures [21]. Wang et al. further presented a hybrid method that generates a skeleton BN based on a dependency analysis in a first phase. This is followed by a search that uses a scoring metric combined with the knowledge learned from the first phase [22].There are also sets of algorithms that mimic natural behavior as they search for solutions without any idea of the nature of expected solutions (such as are expressed as prior networks). These algorithms examine a collection of possible solutions from the search space at each step. One such algorithm, Ant Colony Optimization, as applied to learning BNs, proceeds with an empty graph, and incrementally adds edges. This improves the K2 metric score of the network until a desired stopping criterion is met [23]. Daly and Shen presented a new algorithm, called ACO-E, for Bayesian Network structure learning [24]. This method employed Ant Colony Optimization (ACO) to search the space of possible networks. The authors found the method to outperform the Greedy search and others, while searching in the space of equivalence classes. Larranaga et al. presented a review of evolutionary algorithms applied to Bayesian Network structure learning [25]. In particular, the most widely used of such methods namely, the Genetic Algorithm (GA), handles populations of candidate solutions to the problem (in contrast to the sequential approaches) across a number of generations in order to obtain better solutions. The algorithm selects a subset of fitter solutions from the population to be the parents to reproduce new offspring solutions via evolution operators. The fittest solutions are then selected as new population for subsequent operations [26,27]. GAs are successful when node ordering assumptions hold, enabling the method to generate acyclic networks. Lee et al. further present a version of GA for BN structure learning where a population of solutions, represented as matrices, are generated from upper and lower triangular matrices together with new genetic operators [28].The Particle Swarm optimization also considers populations of solutions and selects solutions based on their fitness score at each step. Unlike the GA, it does not require any evolution operators and the K2 metric is used to evaluate such networks [29]. Though these population-based algorithms are able to produce very good results, they do not require known prior relationships among variables at the start of search. Moreover, evaluating every member of a population at each step to determine their fitness is expensive. For these reasons, such algorithms are usually applied to the class of optimization problems where one has no idea about expected optimal solutions [30]. However, for purposes of inferring transcriptional regulatory networks such as is presented in Adabor et al. with BNs (where priors guide the search to find likely relationships among variables), those methods that depend on domain knowledge expressed in prior relationships are definitely preferred [31]. In particular, in the current study, we compare our proposed sequential method, which builds on prior domain knowledge solution, with existing high-performing search methods (which also use prior knowledge represented in non-empty networks to guide the search toward more likely realistic solutions).At the other end of the spectrum of BN structure learning methods are the constraint-based methods. These use statistical tests of pairs of variables to find conditional independence based on some threshold. The conditional independencies form the basis for the Bayesian Network structure of interest [32,33]. However, the constraint-based methods are sensitive to errors on independence tests. There are hybrid algorithms that aggregate both score-and-search-based and constraint-based approaches of structure learning [34,35]. The Max–Min Hill Climbing algorithm is also a hybrid method that infers a skeleton of Bayesian Network with the constraint-based approach. It then uses a Bayesian scoring Hill-Climbing search to determine the orientations of the edges in the skeleton [36]. This method has been shown to be computationally less expensive than the Sparse candidate algorithm.From the foregoing, several of the existing sequential approaches for performing BN structure learning basically focus on searching over a reduced search space. This tends to make the algorithms less exhaustive when applied to problems of higher dimensions. In view of this issue, we present in the current paper, a more exhaustive hybrid search method which combines Simulated Annealing with a Greedy Algorithm (SAGA) that thoroughly explores most of the search space by undergoing a two-phase search: first, with a Simulated Annealing search and then with a Greedy search. This approach guarantees near optimal solutions within a fixed time without degrading the quality of the true regulatory network achieved. The random restarts of the greedy algorithm executed in the final phase guarantee a quick convergence to an optimistic best local or near optimal network. The SAGA technique coerces the decomposability of the Bayesian Dirichlet Equivalence (BDe) score function to evaluate the changes in networks. It also assists in model selection, which enhances the time of the search process [37]. The quick convergence to the local optimal network is further enhanced by the transient and stochastic nature of the search process. Prior domain knowledge is used to guide SAGA so that the results have biological significance and usefulness. Furthermore, the two-phase SAGA search does not restrain the number of nodes for effective structure learning, thus giving equal chances for any given variable to relate to other variables.

@&#CONCLUSIONS@&#
