@&#MAIN-TITLE@&#
An approximate dynamic programming approach for improving accuracy of lossy data compression by Bloom filters

@&#HIGHLIGHTS@&#
Construction of an ILP for item selection of a more accurate Bloom Filter (BF): yes–no BF.Design of heuristics for efficient feasible solutions based on analysis of ILP.An ADP using one-step improvement scheme is developed to improve efficiency of DP.The effectiveness of ADP is justified in numerical tests and gives accurate separations.The resulting yes–no BF from ADP improves the current technology of BF designs.

@&#KEYPHRASES@&#
Lossy compression,Bloom filter,Integer linear program,Approximate dynamic programming,Heuristics,

@&#ABSTRACT@&#
Bloom filters are a data structure for storing data in a compressed form. They offer excellent space and time efficiency at the cost of some loss of accuracy (so-called lossy compression). This work presents a yes–no Bloom filter, which as a data structure consisting of two parts: the yes-filter which is a standard Bloom filter and the no-filter which is another Bloom filter whose purpose is to represent those objects that were recognized incorrectly by the yes-filter (that is, to recognize the false positives of the yes-filter). By querying the no-filter after an object has been recognized by the yes-filter, we get a chance of rejecting it, which improves the accuracy of data recognition in comparison with the standard Bloom filter of the same total length. A further increase in accuracy is possible if one chooses objects to include in the no-filter so that the no-filter recognizes as many as possible false positives but no true positives, thus producing the most accurate yes–no Bloom filter among all yes–no Bloom filters. This paper studies how optimization techniques can be used to maximize the number of false positives recognized by the no-filter, with the constraint being that it should recognize no true positives. To achieve this aim, an Integer Linear Program (ILP) is proposed for the optimal selection of false positives. In practice the problem size is normally large leading to intractable optimal solution. Considering the similarity of the ILP with the Multidimensional Knapsack Problem, an Approximate Dynamic Programming (ADP) model is developed making use of a reduced ILP for the value function approximation. Numerical results show the ADP model works best comparing with a number of heuristics as well as the CPLEX built-in solver (B&B), and this is what can be recommended for use in yes–no Bloom filters. In a wider context of the study of lossy compression algorithms, our research is an example showing how the arsenal of optimization methods can be applied to improving the accuracy of compressed data.

@&#INTRODUCTION@&#
In information technology, lossy compression is a data compression method that reduces the size of the representation at the cost of the loss of some accuracy at the decompression time. In exchange for losing accuracy in representation, lossy data structures not only store all information in constant space but also respond to membership queries in constant time. Examples of lossy data structures include skip lists (Pugh, 1989), lossy dictionaries (Pagh & Rodler, 2001) and several hashing techniques.The Bloom filter is one of lossy methods of storing compressed data, introduced in Bloom (1970). The kind of data that Bloom filter is especially suitable for are sets. Given a set, a Bloom filter can be produced which represents the set in a compressed form. It can then be queried in the sense that there is an algorithm which, given an object and a Bloom filter representing a set, decides whether the object is or is not an element of the set. The querying algorithm is very efficient and works extremely fast compared to standard algorithms of accessing compressed data (one of the reasons why this algorithm is fast is that it contains many operations which are performed in parallel and that it is easy to implement in hardware) (Tarkoma, Rothenberg, & Lagerspetz, 2012a). The size of the Bloom filter can be very small compared to standard ways of compressing data, which is a major advantage of Bloom filters. Nevertheless, there is also an important disadvantage: Bloom filters only represent data approximately, and frequently the querying algorithm gives an incorrect answer to the question about the membership of an object in the set represented by a Bloom filter.The broad area of applicability of Bloom filters, due to their excellent space and time efficiency, is either in low-performance hardware or for tasks which must be performed extremely fast and speed is slightly more important than accuracy. Bloom filters have a range of uses in information technology (Broder & Mitzenmacher, 2002; Tarkoma, Rothenbergand, & Lagerspetz, 2012b), from hardware implementations to software applications domain, where it was first conceived to perform space and time efficient dictionary lookups (Bloom, 1970). Broder and Mitzenmacher (2002) have coined the Bloom filter principle: ‘Whenever a list or set is used, and space is at a premium, consider using a Bloom filter if the effect of false positives can be mitigated’. To give just one example, Bloom filters can be used for routing in computer networks: in this application, a path which a message must follow is represented by a Bloom filter, namely, as a union of those links between computers which together constitute the path. It is appropriate to use Bloom filters in this application because each computer along the path must decide where to forward the message very quickly (literally with the speed of light, assuming that the links between computers are optical cables).Calculating a Bloom filter of one object is a preliminary stage before building or querying a Bloom filter representing a set of objects. Assume that there is an algorithm which takes any object as its input and produces a binary array of a fixed length G, in which H bits are equal to 1 and other bits are equal to 0. We refer to this array as the Bloom filter of the object. The purpose of the Bloom filters of objects is to serve as uniform labels for each object which may interest us. Informally speaking, the Bloom filters of objects may be likened to bar-codes glued to every object. We denote the Bloom filter of an object s by η(s).Given a setS,the Bloom filter ofScan be computed as the bitwise disjunction of the Bloom filters of the elements inS; in other words, the Bloom filter of the setSis defined as a binary array of length G, and for eachj=1,…,Gthe jth bit is calculated as follows: if in every η(s), wheres∈S,the jth bit is 0 then the jth bit of the Bloom filter is 0; otherwise, if in at least one η(s) the jth bit is 1 then the jth bit of the Bloom filter is 1. We denote the Bloom filter of a setSbyβ(S).Given an object s and a Bloom filterβ(S),querying it to determine whether s is an element ofSis done as follows. For eachj=1,…,Gif the jth bit of η(s) is less than or equal to the jth bit ofβ(S)then we say that s is recognized11It is also convenient to use the pure-mathematics term ‘covered’, that is, s is covered byβ(S),thus stressing thatβ(S)is a lattice join (or, in another interpretation, a set union) of the Bloom filters of the elements ofS.by the Bloom filterβ(S)as belonging to the setS. In an ideal world, one would like to be able to claim that s is recognized as belonging toSif and only if s is an element ofS. However, this is not so. Due to the definition of a Bloom filter, if s is an element ofSthen s is recognized byβ(S); but the converse is not true: not necessarily an object s recognized byβ(S)is an element ofS. This kind of error is called false positives, in the sense that the Bloom filter query recognizes the element as belonging to the set, but should not do it.A number of approaches have been proposed to reduce the number of false positives in Bloom filters. The number H of positions equal to 1 can be varied (Kirsch & Mitzenmacher, 2008). Generalizations of the standard Bloom filter have also been considered, such as the yes–no Bloom filter (Vernitski, Carrea, & Reed, 2015) that is further studied in this paper, the retouched Bloom filter (Donnet, Baynat, & Friedman, 2006), the counting Bloom filter (Guo, Liu, Li, & Yang, 2010), the power of two choices (Lumetta & Mitzenmacher, 2007), the optihash (Carrea, Vernitski, & Reed, 2014) or partitioned hashing (Hao, Kodialam, & Lakshman, 2007). Both the standard Bloom filter and its generalizations listed above can work in the use scenario in which some false positives and some false negatives are allowed. Nevertheless, in this paper we concentrate on the construction of yes–no Bloom filter under the standard use scenario in which we allow some false positives (trying to minimize their number) and do not allow any false negatives (for example, in the application to routing this approach means that the message will definitely be delivered to the right recipient but perhaps also sent to some other computers, thus creating some unnecessary traffic in the network).This paper studies a yes–no Bloom filter (Vernitski et al., 2015), which is our new generalization of the standard Bloom filter which actively reduces the number of false positives at the stage of building the Bloom filter. Let us start with a fictitious and simplified but realistic use scenario. Suppose the management of an airport installs CCTV cameras whose output is automatically compared with photographs of 100 known terrorist suspects. Suppose these photographs are stored in a compressed form as a Bloom filter. Due to Bloom filters’ efficiency, even low-performance hardware can effectively compare faces of people in the airport with the faces of the suspects. Then, if the Bloom filter recognizes a face, the security staff is called to look at the person and make a decision. As we have discussed, Bloom filters produce false positives; therefore, the security staff will be called more often than needed. In particular, suppose that out of all the employees working in the airport, 300 people have faces that trigger false positives. We can considerably reduce unnecessary checks and nuisance if we actively indicate to the Bloom filter that these specific objects are recognized incorrectly and should not be recognized.A yes–no Bloom filter consists of two Bloom filters, one called a yes-filter and the other called a no-filter.22Slightly more general approaches are also possible, which involve more than two Bloom filters, but this particular choice in the design of the structure is considered for the purposes of this paper. See the conclusion for suggestions of further research.Now we shall define the algorithms for building the yes–no Bloom filter of a set and for querying a yes–no Bloom filter. We assume that each object has two Bloom filters corresponding to it: the yes-filter of lengthG+and the no-filter of lengthG−. Given an element s, we shall denote its yes-filter byη+(s)and its no-filter byη−(s).Consider a setSand another setTsuch that that the two sets do not overlap. The setSis the one that we want to store in a compressed form, and the setTis a set whose elements are likely to be queried but should not be recognized as elements ofS. (In the example above,Sis the set of suspects andTis the set of airport employees.) Build the yes-filterβ+(S)ofSas the bitwise disjunction of allη+(s),wheres∈S; in other words,β+(S)is the standard Bloom filter built from all arraysη+(s). The Bloom filterβ+(S)will have false positives, including some which are contained inT; let us denote the subset ofTconsisting of false positives ofβ+(S)byF. (In the example above,Fis the set of those employees whose faces are recognized by the Bloom filter.) Then the second, more interesting stage begins: we build the no-filterβ−(S)ofSso thatβ−(S)recognizes as many as possible elements ofFbut none of the elements ofS. (As we shall see later in the paper, unlike building a standard Bloom filter or the yes-filter, this stage involves flexibility as to which elements ofFshould be included inβ−(S),and, therefore, turns into a meaningful optimization problem.)Fig. 1shows diagrammatically how the yes-filter and the no-filter are built. The yes-filter recognizes all elements ofS(the wave-patterned set) and also many other objects, probably including some elements ofT(the chequer-patterned set). In an ideal scenario (shown on the diagram) we manage to build the no-filter in such a way that it recognizes all elements ofTrecognized by the yes-filter but does not recognize any elements ofS.Then when we query a yes–no Bloom filterβ+(S),β−(S),given an object s we say that s is recognized byβ+(S),β−(S)ifη+(s)is recognized byβ+(S)butη−(s)is not recognized byβ−(S). As you can see, the querying algorithm is as fast and easy as the one of the standard Bloom filter.We have conducted extensive computational experiments (Vernitski et al., 2015) showing that the number of false positives of a yes–no Bloom filter is considerably less than the number of false positives of a standard Bloom filter of the same length (by the length we mean the total memory used up, that is,G++G−for the yes–no Bloom filter and G for the Bloom filter).As we have said earlier, in this paper we study the use scenario when no false negatives are allowed. However, yes–no Bloom filters can be also used when one allows some false negatives. Also it must be said that in this paper we concentrate on the use scenario in which the setSis fixed; accordingly, a yes–no Bloom filter representingSis built once and then only queried. IfSis changed, we simply assume that the yes–no Bloom filter of the updated set is recalculated again. Some other generalizations of the Bloom filter are designed to work with the use scenario in which a limited number of elements may need to be added toSor excluded fromSvery fast, without rebuilding the Bloom filter from the Bloom filters of individual elements ofS; such constructions include, for example, counting Bloom filters (Guo et al., 2010).In our computational experiments with yes–no Bloom filters (Vernitski et al., 2015) we were using a very simple heuristic for building the no-filterβ−(S),which is very fast but not necessarily picks the best combination of false positives for the no-filter. By using a more advanced optimization algorithm the accuracy of yes–no Bloom filters can be improved further. Indeed, the difference between the standard Bloom filter and the yes–no Bloom filter is not only that the yes–no Bloom filter immediately provides some improvement of accuracy. What is also important is that the standard Bloom filter is rigidly determined by the setS(as bitwise disjunction of Bloom filters of all its elements), whereas when one builds a yes–no Bloom filter for a pair of setsSandT,one can choose the no-filter in many different ways. Therefore, building a yes–no Bloom filter can and should be considered as an optimization problem. In particular, if we do not allow any false negatives and allow some false positives, we can consider an optimization problem of maximizing the number of false positives recognized by the no-filterβ−(S).At this point we also need to comment whether using a more complicated algorithm is feasible. Indeed, running a more complicated and more time-consuming algorithm at the stage of building a compressed set is usually possible in applications. For instance, if we refer back to our airport example, it is querying that must be done very fast and perhaps with low-performance hardware, but building a compressed representation of the photographs of suspects is done infrequently and probably on a big computer.The aim of this work is to develop an optimization model for building a no-filter, and discuss a number of approaches to solve the resulting integer program. The paper is organized as follows. In Section 2 we have a literature review on the optimization problems related to Bloom filters. In Section 3 we construct an optimization model for the no-filter, which maximizes the number of false positives included in the no-filter. Comments on the difficulty of the resulting pure integer program will be made in the end of Section 3. Section 4 is about the structural properties of the ILP model and possible simplifications. Specifically, a reduced ILP is introduced in this section by replacing the explicit cover of every single real positive by some statistical information and consequently ignoring the use of some integer variables in the initial model. Based on this simplification, an Approximate Dynamic Programming (ADP) model is then constructed in Section 5, which makes use of the simplified ILP in its value function approximation. This ADP model is compared with a number of heuristics and shown effective in producing good solutions on most of the testing examples in Section 6. In Section 7 we make conclusions and comment on future work.Various studies have been done on Bloom filters and data compression in general, while most of the optimization research that has been done in this context focuses on the discussion of how to select hash functions to minimize the rate of false positives. For example, Mitzenmacher (2002) introduced the idea of compressed Bloom filters which limits the size of transmission by proxies, optimizing the number of hash functions to minimize both the rate of false positives and the size for transmission. On the other hand, Chen, Jin, Chen, Liu, and Ni (2012) proved through statistical studies that the optimal setting of Bloom filter should be determined by the number of items involved in a peer-to-peer keyword searching rather than by minimizing the false positive rate. From the optimization point of view, all above works are related to nonlinear programming which aims to find the best parameters for data compression and transmission; thus, it falls not into the same category of optimization as the one used in this work.To the best of the authors knowledge, the works most related to integer programming are done by Bruck, Gao, and Jiang (2006) and Zhong, Lu, Shen, and Seiferas (2008). Both works aim to minimize the Bloom filter false positive probability for given object popularities by customizing the number of hashes. The former achieves integrality by allowing arbitrary solution and then rounding to the nearest integer value, while the latter proposes a constrained nonlinear integer programming model and develops two polynomial-time approximation algorithms based on dynamic programming.In contrast, our study focuses on how to separate real positives of a given Bloom filter from its false positives and exclude as many as possible false positives. As it deals with a specific set of elements and specific false positives rather than an estimated false positive rate, the optimization model we are addressing here definitely requires the usage of integer variables. As a result, a Mixed-Integer Linear Programming (MILP) (Nemhauser & Wolsey, 1988) model is developed; in this model, the objective is to maximize the number of false positives selected for the no-filter, and constraints are imposed to ensure that no false negatives are created through this selection process. We will show this problem shares some similarities with the Multidimensional Knapsack problem (Bertsimas & Demir, 2002; Fréville, 2004; Lorie & Savage, 1955) and therefore isNP-complete.The major methodology we use to solve the large scale combinatorial optimization problem is based on Approximate Dynamic Programming (Bertsekas, 2012; Powell, 2007). ADP is a powerful tool for handling the curse of dimensionality. There are several aspects of this research, most of which focus on the approximation of value-to-go function in traditional Dynamic Programming based on discrete state and action space. The last decade is the period ADP has grown most significantly. It has shown a number of empirical successes in large-scale real-world applications such as in revenue management (Yang, Strauss, Currieb, & Eglese, 2014; Zhang & Adelmany, 2009), job scheduling (Dong & Glazebrook, 2010), Internet traffic routing (Grothey & Yang, 2012; Yang & Grothey, 2012) and so on. In this work we will develop a one-step improving scheme based on a reduced ILP to solve the optimization model.In this section we shall discuss the optimization model for picking up elements for the no-filterβ−(S). As described in the Introduction, by using a Bloom filter we can compress data through representing a setSas the bitwise disjunctionβ+(S)of Bloom filters of its elementss∈S. Building a Bloom filter will likely introduce some false positives,f∈F,which are the objects that are incorrectly recognized byβ+(S). Ideally we hope to exclude all false positives while retaining all elements ofSby adding a no-filter,β−(S),based on a subset ofF. Therefore, any element that belongs toβ+(S)but is excluded byβ−(S)should be recognized as an element in setS.It is not difficult to imagine that in most cases, taking allf∈Fto createβ−(S)is not a good decision as such a Bloom filter may cover somes∈Sagain. To simplify the process, we start from the assumption that we allow some false positives inβ+(S)∖β−(S)with the hope of excluding as many of them out as possible, while retaining all elements inS. To this end, we will develop an integer programming model and the notation for it is presented as follows.•S,s∈S: The set of elements which the Bloom filter aims to represent.F,f∈F: The set of false positives which are incorrectly recognized by the yes-filterβ+(S).•N: The number of elements in setS.M: The number of false positives; that is the size of the setF.G: The total number of bits in the no-filter of each objectη−(s).H: The number of nonzero bits in the no-filter of each objectη−(s).A=(asj)N×G: Parameter matrix for setS,whereasj={1,ifelements"sbitjis10,otherwise,s=1,…,N;j=1,…,G.B=(bfj)M×G: Parameter matrix for false positive setF,wherebfj={1,iffalsepositivef"sbitjis10,otherwise,f=1,…,M;j=1,…,G.•xf={1,iffalsepositivefisselectedfordisjunctionβ−(S)0,otherwise,f=1,…,M.δj={1,ifbitjis1inthedisjunctionβ−(S)withselectedfalsepositives0,otherwise,j=1,…,G.Our aim here is to maximize the total number of false positives that are picked up for disjunctionβ−(S),while guaranteeing that none of elements ofSis recognized byβ−(S).(3.1)max∑f=1Mxf(3.2)s.t.∑f=1Mbfjxf≤Mδj,j=1,…,G(3.3)(ILP)∑j=1Gasjδj≤H−1,s=1,…,N(3.4)xf∈{0,1},f=1,…,M(3.5)δj∈{0,1},j=1,…,GConstraint (3.2) relates δ with decision x, which makesδj=1if bit j is 1 in any selected false positives. Constraint (3.3) guarantees no element ofSis excluded by saying, for each element inSthere is at least one bit that is not being covered by the selected false positives forβ−(S).This is a pure integer programming problem which containsM+Gbinary variables andN+Gconstraints. Although the size of the problem increases linearly with its components, due to its integrality nature the solution difficulty grows exponentially as to prove optimality, we need to examine objective value at a maximum of2M+Gnodes.It is not difficult to see that the above problem shares some similarities with the so-called Multidimensional Knapsack Problem (MKP) (Fréville, 2004):•The aim is to maximize the total value (number) of items picked for package.Decisions are, for each item type, whether to include it in the package.Constraint(s) restrict on the selection of items in such a fashion that if some items have been selected then some others must be excluded. The major difference is that Knapsack problems impose this restriction by limiting the total weights/volumes whereas our problem impose it by checking if the disjunction will cover any elements.Indeed, we can show that the above problem is reducible to a 0–1 MKP. To achieve this, let us reformulate the problem to interpret the fact that, if several false positives’ bitwise disjunction covers any element, we have to exclude the possibility of having all theirxf=1in the final solution. As for every element inSit is not difficult to find all h-degree (a combination of h ≤ H items) combinations from the limited number of false positives which fully cover this element, we can list all such combinations and restrict for each of them, the number of selected items cannot go higher thanh−1. For a fixed elements∈S,letLhsdenote the set of h-degree combinations, each of which fully covers s, then mathematically above requirement can be represented as:∑f=1Meflxf≤h−1,∀l∈Lhs,h=1,…,H,whereefl={1,iffalsepositivefisanelementofcombinationl0,otherwisef=1,…,M;l∈Lhs.,Then, the original model (ILP) is converted into an MKP with less variables but (in general) more constraints:(3.6)max∑f=1Mxf(3.7)(MKP)s.t.∑f=1Meflxf≤h−1,∀l∈Lhs,h=1,…,H,s=1,…,N(3.8)xf∈{0,1},f=1,…,MHere we consider an example where there are two elements inSand four false positives withH=2. Following the previous definition, assumeA=[01101100],B=[1010100101010011].The (ILP) of this easy example is given as:maxx1+x2+x3+x4s.t.x1+x2≤4δ1,x3≤4δ2,x1+x4≤4δ3,(ILP_eg)x2+x3+x4≤4δ4,δ2+δ3≤1,δ1+δ2≤1,xf∈{0,1},f=1,…,4δj∈{0,1},j=1,…,4To convert it into a corresponding (MKP), we have to go through the nonzeros of each element to check which false positive it relates to. For instance, the first element has its second and third bits nonzero, which does not allow the combination of false positives {1, 3} and {3, 4} to be considered in the optimal solution. Therefore we create constraints:x1+x3≤1,andx3+x4≤1.Similarly, considering the second element we obtain:x1+x3≤1,andx2+x3≤1.And thus the complete (MKP) turns out to be (removing repeated constraints):maxx1+x2+x3+x4s.t.x1+x3≤1,(MKP_eg)x3+x4≤1,x2+x3≤1,xf∈{0,1},f=1,…,4Solving both systems we will achieve the same optimal solutionx1=x2=x4=1which meansβ−(S)={1,2,4}. Note that this example shows just a special case of the problem for which the (MKP) is a smaller system comparing with (ILP). Actually in practice when the number of items for both sets increases, the (MKP) will become far larger than the (ILP) as it has to consider every single infeasible combination of false positives.It is well known that 0–1 MKP isNP-complete, therefore a polynomial time solution approach for our problem may not be available. Nevertheless, the Knapsack problem is amongst the most studied problems in combinatorial optimization on which much theoretical and empirical work has been done, including some clever heuristics and/or pseudo-polynomial approaches like the greedy method and Dynamic Programming. In this work we will be talking about a greedy heuristic based on Approximate Dynamic Programming.We now try to take the special structural properties of it to design a heuristic which is anticipated to provide reasonably good (nearly maximized), and always feasible solutions. Traditional optimization approaches for the pure integer programming problems are based on Branch-and-Bound. This methodology starts from finding the optimal solution of the LP relaxation of the corresponding ILP and branch on the fractional optimal solution observed to search for the optimal integer solutions. In the intermediate steps, a number of integer feasible solutions are observed, whose corresponding objective values serve as the lower bound of the final optimal integer solution we are looking for. Therefore, it is not difficult to find feasible integer solutions throughout this procedure, while the biggest difficulty is to improve on the current best solution and/or to prove optimality without going through the whole Branch-and-Bound tree explicitly.To achieve an acceleration for the solution approach let us firstly consider the partial LP relaxation of the (ILP), defined as (LPRpartial), which allows decision x to take fractional values between 0 and 1.(4.1)max∑f=1Mxf(4.2)s.t.∑f=1Mbfjxf≤Mδj,j=1,…,G(4.3)(LPRpartial)∑j=1Gasjδj≤H−1,s=1,…,N(4.4)xf∈[0,1],f=1,…,M(4.5)δj∈{0,1},j=1,…,GLemma 4.1The optimal solution of LPRpartial must havexf˜*=0orxf˜*=1,f˜=1,…,M.Let (x*, δ*) be the optimal solution of LPRpartial. Assumexf˜*=ϵand 0 < ϵ < 1. Asx*=(x1*,…,xf˜*,…,xM*)is a feasible solution, it must satisfy:∑f=1Mbfjxf≤Mδj,j=1,…,GTherefore for any j:•Ifbf˜j=0,modifyingxf˜value will not have any impact to this constraint. So if we increasexf˜value from ϵ to 1, the constraint is also satisfied.Ifbf˜j=1,to satisfy the constraint we must have0<ϵ=xf˜≤∑f≠f˜bfjxf+xf˜≤Mδj,which impliesδj=1and the constraint becomes:∑f≠f˜bfjxf+xf˜≤M. As∑f≠f˜bfjxf≤M−1,we can always increase the value ofxf˜from ϵ to 1 without violating the constraint.While in both situations, by increasingxf˜value from ϵ to 1 we can achieve an improvement on the objective value by1−ϵ. Therefore we proved in the optimal solutionxf˜*=0orxf˜*=1,f=1,…,M.□The optimal solution of LPRpartial must also be the optimal solution of ILP.This is a direct conclusion of Lemma 4.1 since if the optimal solution of an LP relaxation turns out to be integer, it must also be the optimal solution of the original IP problem.□Up until now we have reduced the problem into a Mixed-Integer Programming (MIP) problem, where the only integer restrictions are made on δ values. Now we will revise the constraints to see if we can simplify the problem further by reducing the number of variables that we need to consider in the optimization model.Remember that constraints (4.3), which are only relating to δ values, impose the restriction that for any element inSthere must be at least one bit to be uncovered by the selected false positives disjunctionβ−(S). As these constraints are somehow the “hard constraints” which cannot be further simplified, let us consider whether we can get rid of constraints (4.2) instead, and therefore remove x variables completely.Suppose we can split the problem into two steps according to the constraints, then:•The first step, corresponding to constraints (4.2), is to create a disjunctionβ−(S)by picking up a collection of false positives.The second step, corresponding to constraints (4.3), is to check whether the disjunctionβ−(S)fully covers any element inS.If we hope to do every step individually, in the second step we have to define an aim that we want to achieve, which somehow reflect our initial aim of picking up as many false positives as possible forβ−(S). Without the value of xs we cannot do this accurately, but it is not difficult to imagine that, potentially, the more ones in disjunctionβ−(S)(the more bits covered byβ−(S)), the more false positives can be selected. To make it more accurate, we can also consider for every single bit, how many false positives it overlaps with. Potentially, the more false positives one bit is covered by, the higher possibility to make more xs equal 1 by allowingβ−(S)contains this bit. Therefore let us define a cardinality, cj, for every single bit j, which indicates how many false positives having this bit equals 1:cj=∑f=1Mbfj,j=1,…,G,and then use it to define the objective in an approximated problem containing only second step variables:(4.6)max∑j=1Gcjδj(4.7)(APP)s.t.∑j=1Gasjδj≤H−1,s=1,…,N(4.8)δj∈{0,1},j=1,…,GThe resulting system (APP) is a reduced pure ILP with G variables and N constraints, which becomes a so-called Set Packing problem (Winston, 2003). Although the Set Packing problem is alsoNP-complete (Garey & Johnson, 1979), with significantly reduced size, the (APP) is more tractable than the original (ILP).By solving this problem we will get efficiently an integer solution δ* which indicates a disjunctionβ−(S)*. Then going through the first step is straightforward:if a false positive is completely covered byβ−(S)*we include it in the collection, otherwise we exclude it.Through the later tests with examples, we can see this method works very well and efficiently for applications having thousands of elements and false positives.One typical pseudo-polynomial method to solve the Knapsack Problems is Dynamic Programming (DP) (Kellerer, Pferschy, & Pisinger, 2004; Martello & Toth, 1990). Nevertheless DP suffers from the so-called “curse of dimensionality” for large size problems (Powell, 2007). In this section we will consider DP to solve the problem, while avoiding the curse of dimensionality by using an approximated value function.•Stage:k=1,…,M,consider every false positive in its natural order.State:Dk=(d1k,d2k,…,dGk), which denotes the bitwise disjunction of yet-selected items after considering false positives1,…,kanddjk={1,ifbitjiscoveredbyyet-selecteditemsfromfalsepositives1,…,k0,otherwise,j=1,…,G.Decision:uk={1,ifbringfalsepositivekintoβ−(S)0,otherwise,k=1,…,M.Recurrence:Vk(Dk)={−∞,ifDkfullycoversANYelementinSmax{1+Vk+1otherwise(Dk⊕bk·),Vk+1(Dk)},,k=1,…,M−1;VM(DM)={−∞,ifDMfullycoversANYelementinS0,otherwise,whereDk⊕bk·≡{djk+bkj(1−djk)|j=1,…,G},which denotes the state after bringing false positive k into consideration.If the DP can be solved exactly it will definitely find the optimal solution as it considers potentially all possible combinations of false positives. To make decision for stage k we have to evaluate function values for stagek+1at all possible states beforehand. So starting fromk=M,following a backward decision process we can obtain the optimal decision ukiteratively for all stages and states, which contain the optimal solution as the best path we ought to follow fromk=1to M.Nevertheless, it is not always realistic to solve the DP accurately in practice, especially when the number of states increases exponentially with the problem size. As an alternative, let us consider the approximation to the DP model which uses an approximated value function forVk+1that is easy to evaluate without going through all the later stages in detail.This section introduces an ADP heuristic, which takes use of the simplified model (APP) as we defined in Section 4.2 to approximate the value functionVk+1. Basically we will follow all the definitions of DP model in the ADP, besides the fact that we will step forward in stage to check in turn whether the current false positive k should be included in the final disjunctionβ−(S). As this “forward” process just needs to be carried out once, we could accelerate the whole solution process significantly by using ADP instead of DP.In the DP model, at stage k we need to solve the recurrence:(5.1)Vk(Dk)=max{1+Vk+1(Dk⊕bk·),Vk+1(Dk)}which results in the best action, uk, that is to take for false positive k. Instead of calculatingVk+1explicitly through the backward recurrence, we modify the (APP) problem to give quickly an approximation to theVk+1value at the current state Dkand Dk⊕bk ·, denoted asV¯k+1(Dk)andV¯k+1(Dk⊕bk·). This requires to solve 2 subproblems(APP0k)and(APP1k)defined as follows.(APP0k)is defined as the optimization problem associated with the decisionuk=0.(5.2)max∑j∉Dkc0jkδj(5.3)(APP0k)s.t.∑j∉Dkasjδj≤(H−1)−∑j∈Dkasj,s=1,…,N(5.4)δj∈{0,1},j∉Dkwherec0jk=∑k˜=k+1Mbk˜j,j∉Dk,which define the cardinality of bits according to their appearance in the false positives that have not been considered yet. Solving(APP0k)we will get the optimal solutionδ0k,in terms of which bit should be covered by the disjunction generated with false positivesk+1,…,M. AndV¯k+1(Dk)is calculated as the number of false positives ink+1,…,Mthat are fully covered by this disjunction.On the other hand,(APP1k)is defined as the optimization problem associated with the decisionuk=1. So in addition to Dkwhich is the current disjunction, we also need to exclude all bits that have been covered by the false positive k and consider the best disjunction that is to be generated with false positivesk+1,…,M.(5.5)max∑j∉Dk⊕bk·c1jkδj(5.6)(APP1k)s.t.∑j∉Dk⊕bk·asjδj≤(H−1)−∑j∈Dk⊕bk·asj,s=1,…,N(5.7)δj∈{0,1},j∉Dk⊕bk·wherec1jk=∑k˜=k+1Mbk˜j,j∉Dk⊕bk·. Solving(APP1k)we will get the optimal solutionδ1k,andV¯k+1(Dk⊕bk·)is calculated as the number of false positives ink+1,…,Mthat are fully covered by this disjunction. With the resultingV¯k+1(Dk)andV¯k+1(Dk⊕bk·)values, we can then solve problem (5.1) for an approximated decisionu¯k.Using this approximation scheme, to solve the entire ADP model we need to solve 2M binary subproblems. Nevertheless, as the size of problem decreases gradually with the increased number of bits that have already been covered by Dk, the solution process will get increasingly quicker as each next stage. Besides this, we will also take use of the special recursive manner of(APP0k)and(APP1k)to further simplify the solution process.To improve the efficiency of solving the ADP model, in this section we will discuss some additional simplifications around the binary subproblems, motivated by the fact that there are large similarities between the subproblems of stage k andk+1.•Firstly, observe that if the best action in stage k was decided asuk=0,we haveDk+1=Dkwhich means constraint set for(APP0k)is completely the same as constraint set for(APP0k+1). The only difference between these two problems is then the cardinality in the objective, for which(APP0k+1)has H variables possessing one unit lessc0jkthan(APP0k). As the cardinality is in general comparable to the number of false positives which is normally hundreds or thousands, it is reasonable to believe that in most cases this 1 unit less will not change the optimal solution significantly. Therefore we can useV¯k+1(Dk)−1to approximateV¯k+2(Dk+1)if no new false positive has been added to Dkin stage k (uk=0), and the latter could directly be used in recurrence for stagek+1.Secondly, if the best action in stage k was decided asuk=0and if by solving stage k’s subproblem(APP0k)we already haveuk+1=1(bits of false positivek+1are fully covered byδ0k), then the optimal solution of subproblem(APP1k+1)plusuk+1=1is identical to the optimal solution to(APP0k). Therefore we do not need to re-solve the former again in stagek+1but instead useV¯k+2(Dk+1⊕bk+1·)=V¯k+1(Dk)−1.Thirdly, if the best action in stage k was decided asuk=1and if by solving stage k’s subproblem(APP1k)we already haveuk+1=1(bits of false positive k are fully covered byδ1k), then the optimal solution of subproblem(APP1k+1)plusuk+1=1is identical to the optimal solution to(APP1k). Therefore we do not need to re-solve the former again in stagek+1but instead useV¯k+2(Dk+1⊕bk+1·)=V¯k+1(Dk⊕bk·)−1.By considering all above possibilities, lots of binary subproblems can be skipped for the optimization step. For example, let us consider Instance 5 as given in the numerical test (Section 6.2). Originally we need to solve2M=7442integer programs, while by applying the acceleration strategy we can remove 2712 of them with criterion 1, 745 with criterion 2, 246 with criterion 3 and end up with only 3739 integer programs to solve which is nearly half of the original number. As a result the running time of ADP algorithm reduces significantly (from 19.71 to 10.04 s for Instance 5), which creates better possibilities for managing larger size problems. Of course we can further the acceleration design by also considering some warmstart approaches according to the closest previous optimal solution as observed. But this is omitted in this work as the solution process takes only 8 minutes for instances with ten-thousand false positives (Instances 9 and 10), and performs much quicker than alternative methods like B&B. For more details on the running time information please refer to Section 6.2.To test our ADP model, we firstly develop several simple heuristics which are mainly designed to reflect the nature of the problem.Our aim is to select as many false positives as possible to generate the disjunctionβ−(S)without covering any elements in it. To achieve this aim, the simplest idea is to consider false positives in turn, following the order in which they are defined, to check if selecting the next item will cover any elements. If not, we select it forβ−(S); otherwise we exclude it. This is actually a naive method that has been used by Vernitski (2015) for testing the effectiveness of the yes–no Bloom filter when it had been firstly introduced. We will make use of this naive method as the first heuristic to compare our ADP model with.It is obvious that the naive heuristic as described above will not lead to a generally “good” solution, since following different sequences of considering items will definitely produces different solutions. Only if we can figure out the “optimal” sequence of considering items, we can build the optimal solution this way. Therefore, in this section we will develop some other rules, which are easy to achieve with simple calculations, for how the sequence can be identified. Our aim here is to define for every false positive a degree, which indicates how large the impact will be to the procedure of including other items, if we decide to include this item inβ−(S). Note that most of the following discussion are around the case whereH=2.Naturally, we assume that there are no duplicate items in both the element set and the false positive set. This assumption can easily be satisfied with a preliminary step removing all identical items from both set. Let us consider a single bit j. If this bit is contained in n elements inS,in order to make all of these njelements not covered byβ−(S),we have to make other njbits that are covered by this njelements not covered byβ−(S). This means, inβ−(S)we have to restrict njbits to 0 if we decide to select bit j. Therefore, the higher the value of njassociated with a bit, the less preferred to include this bit as potentially including it will lead to the exclusion of many others.Secondly, selecting any false positive f will make two bits covered byβ−(S). Suppose the two bits are j1 and j2, then including item f inβ−(S)will make at mostnj1+nj2other bits been restricted to 0. As the more zeros there are inβ−(S),potentially the less false positives we can select, therefore we treat this information as a measure of the degree of false positive f, which will be used to decide the sequence in which all the false positive s are considered:DGRfe=∑j=1Gbfjnj,f=1,…,M.So the smaller theDGRfevalue is, the earlier the corresponding item will be considered and the larger opportunity for it to be selected. As this method does not consider the detailed coverage of elements by false positives, it is a very efficient heuristic.In practice we may see lots of same values inDGRfeas defined above. If two items share the same degree, we hope to take more information into account to decide on their sequences. This information comes from considering the false positives in a similar fashion. If a bit j is covered, then potentially all false positives containing the same bit are easier to include than those which are not, as they will just bring one more bit to 1 in the disjunctionβ−(S),and the fewer ones we observe inβ−(S),the less chance there is of an element being covered by it. Therefore, it is better to cover earlier the bits j which are contained by more false positives, since this creates more opportunity for later selection.So for bit j, we denote by mjthe number of false positives containing this bit, and define an alternative degree value according to mjs:DGRfb=∑j=1Gbfjmj,f=1,…,M.Thus, if there are any ties when we just check theDGRfevalues, we takeDGRfbinto consideration and pick up items in a non-decreasing order ofDGRfb.Remember that our ultimate goal is to pick up as many as possible false positives without covering any elements by the resulting disjunctionβ−(S). In practice, selecting any false positive may result in the exclusion of some others. Suppose we can determine accurately how many false positives will be excluded after selecting a false positive f; let us denote this number byDGRfc. Intuitively we would like to consider those having smallerDGRfcs with priority.This method finds explicitly how many false positives will be excluded by selecting a specific false positive f. Consider every single false positive f; by including it we will make at most two more bits inβ−(S)be 1. Exactly how many bits inβ−(S)will be changed to 1 actually depends on what bits have already been covered by the false positives considered and selected before f. As here our aim is to choose a reasonable order of items, let us treat every single false positive f independently, or equivalently, assume item f is the first being considered, to see how many other false positives will be excluded by selecting item f.Suppose false positive f has two bits, j1 and j2, equal one. LetSj1,j2denote a subset ofSwhose elements having j1 or j2 bit equals 1. If item f is selected, all elements inSj1,j2will create a disjunctionβ−(Sj1,j2)and any false positive (except f) which overlaps withβ−(Sj1,j2)∖{j1,j2}are then needed to be excluded. The total number of such false positives are defined asDGRfc. As using this heuristic we have to consider in detail the coverage of all elements by item f, the resulting heuristic works less efficientlythan the two above.Table 1 lists the numerical results of testing above ADP and heuristics on some randomly generated examples (in each example, a certain number of no-filters are picked at random with equal probability from the set of all Bloom filters with given parameters), which are compared with the best solution that solving the ILP model with CPLEX can find within pre-specified amount of time. Note that here we restricted the CPLEX to go through the Branch-and-Bound step for just limited time as for large problems, although it can quickly find a feasible integer solution, it will take very long time to improve it or prove its optimality. The specified time below is the average time that we observed through tests as when a “quick initial improves” finishes. After the cut-off time as specified, most examples will just continue the branching step without improving the objective for hours.Table 1 summarizesnumerical results on 10 examples tested with four heuristic methods as proposed, compared with the solution given by APP, ADP models and ILP solved by CPLEX. Small examples, like Examples 1–4, can be solved efficiently to optimality (with gap 0) by CPLEX. At the same time, heuristic methods also provide very good feasible integer solutions which are quite close to the optimal for small problems. ADP works especially well for all small examples, picking just one or two less false positives than what can be found by CPLEX. Note also that the execution times for all simple heuristic algorithms are small comparing to ADP and CPLEX. But since they are generally worse and just served as benchmarks for optimality comparison, we exclude the execution time of these heuristic algorithms.In contrast, when the number of false positives go above 1000, achieving optimality by CPLEX takes unreasonable time (e.g. it costs another 1 hour to reduce the gap from 85.10 percent to 81.28 percent in Instance 5) and effort (computer memory to save the Branch-and-Bound tree). As CPLEX can still report the current best solution out of the existing Branch-and-Bound tree after the cut-off time, we put this solution in the corresponding column of CPLEX together with the gap between the current best integer solution and the optimal objective of its LP relaxation. Looking at the table we can see that for larger size problems, like Examples 5–10, simple heuristics perform much worse than APP and ADP. ADP in general is the best performance heuristic which can always give as good solutions as CPLEX, within roughly a half of the solution time. As CPLEX cannot give an optimal solution in general, in most cases ADP can do even better than CPLEX like for all examples except Example 9. In general, the larger the gap that CPLEX left until the cut-off time, the more ADP is improving on the CPLEX result.From above test examples it is clear that the simple heuristic methods as described in the beginning of this section have the worst performance in general. Therefore, in later tests we will just focus on the APP and ADP and compare them with CPLEX solutions (for limited running time) to see if we can improve on CPLEX best feasible solutions within shorter time. Following experiments are all run on 100 random examples (Table 2). Three typical problem sizes are considered.The T1 is on small examples again, which can be solved to optimality directly by CPLEX. We can see in general ADP perform quite close to it, which can pick up just slightly less (by 0.94 percent) false positives than the optimal solution and performs the same well as CPLEX on about one third examples (32 out of 100). The T2 is on medium sized examples. Here we allow CPLEX to run for 100 seconds which is believed to be long enough for the Branch-and-Bound method to get initial updates to a good enough node in most cases. We can see ADP works much better than CPLEX in average for this 100 random runs, which improves the CPLEX best feasible solution by 1.27 percent. As in average there is a relatively small gap (25.03 percent) between the best feasible integer solution and the LP relaxation observed in CPLEX, we have reason to believe the ADP model gives quite good sub-optimal solution (if not optimal) in most cases. Indeed, ADP works better than CPLEX on 80 percent of random examples as tested, which emphasizes on the strength of it from another point of view. On the other hand, the average running time for this sized ADP model is roughly 50 seconds, which proves the efficiency of ADP model. Similarly for large examples, T3, ADP improves the CPLEX best feasible solution by 1.29 percent, which picks up roughly 60 more false positives to make the final no-filter more accurate. In addition, ADP picks up more false positives on 96 out of 100 instances, which means ADP improves the CPLEX result almost certainly.

@&#CONCLUSIONS@&#
In this work we have discussed the optimization model for a new Bloom filter construction, a yes–no Bloom filter. The optimization problem is a pure integer program which is reducible to multi-dimensional Knapsack problems with special constraints. This points out the difficulty of solving the whole problem to integral optimality, especially considering the problem size grows exponentially with the number of items/bits. On the other hand, some of the separation variables in the pure optimization model can be relaxed to continuous ones without changing the final solution. Although the partial relaxed model does not make the solution of the problem easier, it gives us some insights on how to simplify the model in order to design efficient heuristics producing sub-optimal solutions. A number of such heuristics have been presented in this work which all outperform the naive method used in the original paper (Vernitski, 2015).In addition to this, as in general the Knapsack problem can be solved by Dynamic Programming techniques, we also build the DP model for this optimization problem which is then extended to ADP to deal with the dimensionality in its recurrences. The resulting ADP model uses a one-step improvement scheme, which approximates the value-to-go function at any specific state by solving a simplified ILP with largely reduced size in contrast to the original integer model. The resulting ADP model has been justified effective in its performance, which in general picks up more false positives in the Bloom filter to make the separation more accurate than any heuristics. As claimed by the author in Vernitski (2015), the yes–no Bloom filter as introduced outperforms the standard Bloom filter even if the naive method is used when picking up false positives, whereas the separation provided by the ADP model will definitely improve the current technology of Bloom filter designs. The ADP algorithm as proposed can also be extended to other applications, because the considered problem is similar to Multidimensional Knapsack problems, and the latter has many applications in a number of areas.As future work, we shall consider multiple no filters. Another, more challenging direction of future work is to evolve the yes–no Bloom filter into a more complicated data structure which would contain several yes-no parts consecutively. Either of these modifications will improve the accuracy but will make it necessary to consider more complicated optimization problems. All in all, there are a number of directions of interesting future work that we can do on this topic as we develop the concept of a yes–no Bloom filter.