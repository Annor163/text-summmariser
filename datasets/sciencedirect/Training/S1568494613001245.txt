@&#MAIN-TITLE@&#
A spiking neural network (SNN) forecast engine for short-term electrical load forecasting

@&#HIGHLIGHTS@&#
Implementation of SNN to STLF is proposed in this paper.Testing and validation is carried out on the load data for Victoria in Australia.The impact of weather variables on the accuracy of proposed model is discussed.The error indices obtained indicate that SNN is superior to ANN and Hybrid models.

@&#KEYPHRASES@&#
Spiking neurons,Spike response model (SRM),ANN,Hybrid model,

@&#ABSTRACT@&#
Short-term load forecasting (STLF) is one of the planning strategies adopted in the daily power system operation and control. All though many forecasting models have been developed through the years, the uncertainties present in the load profile significantly degrade the performance of these models. The uncertainties are mainly due to the sensitivity of the load demand with varying weather conditions, consumption pattern during month and day of the year. Therefore, the effect of these weather variables on the load consumption pattern is discussed. Based on the literature survey, artificial neural networks (ANN) models are found to be an alternative to classical statistical methods in terms of accuracy of the forecasted results. However, handling of bulk volumes of historical data and forecasting accuracy is still a major challenge. The development of third generation neural networks such as spike train models which are closer to their biological counterparts is recently emerging as a robust model. So, this paper presents a load forecasting system known as the SNNSTLF (spiking neural network short-term load forecaster). The proposed model has been tested on the database obtained from the Australian Energy Market Operator (AEMO) website for Victoria State.

@&#INTRODUCTION@&#
Power system operation, planning and setting up of power generation infrastructure in any nation depends upon the growth in the power consumption by its population. Due to the depletion of fossil fuels, increase in population and increase of per capita power consumption in the world, forecasting of electrical load has become one of the major areas of research. Load forecasting is generally carried out to assist planners in making strategic decisions with regards to unit commitment, hydro-thermal co-ordination, interchange evaluation, and security assessments. Electrical load forecasting, in general is classified into three categories; short-term, medium term and long-term forecasting [1]. The duration for these categories is not explicitly defined in the literature papers. Thus, different authors use different time horizons to define these categories. Short-term load forecasting, in general ranges from few minutes to seven days [2]. Since in power system, the next day's power generation must be scheduled every day, short-term load forecasting (STLF) is necessary for economic scheduling of generation capacity. Medium-term or intermediate load forecasting deals with predictions ranging from few weeks to several months [3]. Outage scheduling and maintenance of plants and networks come under in these types of forecasts. Long term forecasting on the other hand deals with forecasts longer than a year [3]. It is primarily intended for capacity expansion plans, capital investments, and corporate budgeting. These types of forecasts are often complex in nature due to several uncertainties such as political factors, economic situation, per capital growth. Planning of new networks and extensions to existing power system for both the utility and consumers require long-term forecasts.Short-term load forecasting is, however, considered as very a difficult task. First, because the load series is complex and exhibits several levels of seasonality: the load at a given hour is dependent not only on the load at the previous hour, but also on the loads at several past hours, even on the loads at past days. Secondly, the consumption pattern is dependent on the prevailing weather conditions [4]. Also, due to power system deregulation, it has become more important i.e., these forecasts are used by energy management system (EMS) to establish operational plans for power stations and to plan transactions in the energy market. A very good example about the importance of load forecasting accuracy is that an increase in 1% of forecasting error caused an estimated increase of ten million pounds of operating costs for one electrical utility in United Kingdom [5]. Thus, the accuracy of STLF reduces the operating costs of electrical utilities in many areas. However, the most important issue in STLF is to extrapolate past load behavior while taking into account effect of other influencing factors such as weather and day of the week. In a power system, temperature is usually the most dominant variable in driving the electricity demand. A sample scattered plot showing the variations in the consumption pattern with respect to average temperatures is shown in Fig. 1. It is obvious that the relationship between temperature and load demand is non-linear. Previous research work and their results are prone to large errors, when the load forecasting model is exposed to large weather forecast errors. Therefore, improving the quality of weather forecasting is an effective way to improve the load forecasting accuracy [6–10].As indicated above, STLF is an important area and this is reflected in the literature with many traditional and non-conventional techniques like regression analysis [11–13], time series approach [14,15], neural networks [16–22], fuzzy logic [23–26] and support vector machines [27,28] etc. Based on the literature survey, though various STLF have been proposed, the scope for developing more efficient and accurate models is still a major challenge.Section 1 gives an introduction about short-term load forecasting (STLF) and its importance in power system planning. The proposed work and the justification in applying SNN to STLF are discussed in Section 2. The details about SNN, the step by step training algorithm (spike propagation algorithm) and the setting of control parameters are given in Section 3. In Section 4, the proposed methodology for implementation of SNN to STLF is discussed. Section 5 investigates the results obtained during the training and testing of SNN. The conclusion and the scope for further research are presented in Section 6.The proposed work uses spiking neural networks for STLF. The SNNs have evolved due to the significant growth achieved in the field of neurophysiology and information processing in brain. It has been discovered that the neural processing inside the brain is carried out in the form of spike trains and not real valued signals. As a result, the artificial neuron that has found closeness to its biological counterpart is known as spiking neuron and the network based upon this type of neurons is known as spiking neural network (SNN). A typical circuit model of spiking neuron was first proposed in 1952 as Hodgkin–Huxley [29,30] model. The spiking neurons are generally divided into three types: threshold, compartment and conductance based [31]. The threshold model is again divided into two types. They are non linear leaky integrate and fire model; and the spike response neuron model (SRNM) [32].In the proposed work, the SRNM has been used in the feed forward SNN. However, an understanding of neural coding is necessary for encoding information in terms of spike trains which is given in [33,34]. The methods of encoding real values include rate encoding, population encoding and temporal encoding [35]. In this work, precise time of spikes in the temporal encoding scheme has been implemented. This scheme is chosen since temporally encoded neurons have better computational ability in spatio temporal context [35]. Temporal encoded SNN have been used for pattern recognition proposed by Bohte in [36]. In [35], authors have successfully tested the performance of temporal encoded SNN for electricity time series forecasting using evolutionary learning technique. However, in this work, an attempt has been made to forecast electrical load using gradient method. The spike propagation algorithm is similar to the gradient descent method used for feedforward backpropagation neural network. The real time dataset is obtained from Australian Energy Market Operator (AEMO) website [37]. The data consist of half hourly load of each day from January 2004 to December 2009. The data is pre-processed by normalizing the load between 0 and 1. The historical data of hourly temperature values are obtained from [38]. The historical data for daily solar radiation is obtained from [39].The input and the output of a spiking neuron are described by a series of firing times known as the spike train. One firing time means the instant the neuron has fired a pulse. The potential of a spiking neuron is modeled by a dynamic variable and works as a leaky integrator of the incoming spikes and the newer spikes contribute more to the potential than the older spikes. If this integrated sum of the incoming spikes is greater than a predefined threshold level, then the neuron fires a spike. This makes SNN a dynamic system, in contrast with the second generation sigmoid neural networks which are static, and enables it to perform computation on temporal patterns in a very natural biological manner. Though its likeness is closer to the conventional back propagation ANN, the difference lies in having many numbers of connections between individual neurons of successive layers as in Fig. 2. Formally between any two neurons ‘e’ and ‘f’, input neuron ‘e’ receives a set of spikes with firing times te. The output neuron ‘f’ fires a spike only when membrane potential exceeds its threshold value (ν). The internal state variable xf(t) is described by the spike response function ε (t) weighted by the synaptic efficacy. i.e., the weight between the two neurons is given in Eq. (1)(1)xf(t)=∑k=1mYekWefkwhere, m is the number of synaptic terminals between two neurons e and f.Where,Wefkis the weight of the sub-connection k, and Y gives the unweighted contribution of all spikes to the synaptic terminal and is given in Eq. (2) as(2)Yik(t)=ϵ(t−ti−dk)dkis the delay between the two nodes in the kth synaptic terminal. ε(t) is the spike response function shaping the post synaptic potential (PSP) of a neuron which is given in Eq. (3),(3)ϵ(t)=tτe1−t/τwhere τ is the membrane potential decay time constant.The SNN is a 3 layer network with a single hidden layer as shown in Fig. 3. The parameters which affect the performance of the proposed SNN are discussed in Section 3.2. The step by step procedure of the spike propagation algorithm [37] is given below.Step 1: Prepare an initial dataset. The dataset is normalized between 0 and 1.Step 2: Generate the weights to small random values. Initialize the SNN parameters such as membrane potential decay time constant (τ) and learning rate (α).Step 3: For the proposed SPNN architecture, input layer (i), hidden layer (h) and output layer (o), do the steps 4–8 ‘itt’ times (itt=no of iterations). The network indices represents the number of neurons where, p=number of neurons in the input layer; q=number of neurons in the hidden layer; n=number of neurons in the output layer; m=number of synaptic terminals between any two neurons of successive layers.Step 4: For each pattern in the training set apply the input vector to the network input.Step5: Calculate the internal state variable of the hidden layer neurons. It is given by Eq. (4).(4)xh(t)=∑i∑k=1mYikWihkStep 6: Calculate the network output which is given by Eq. (5)(5)xj(t)=∑h∑k=1mYhkWhjkStep 7: Calculate the error, the difference between the actual spike time and the desired spike time.(6)E=.5*∑j=1n(tja−tjd)2wheretjais the actual output spike andtjdis the desired output spike and n is the number of neurons in the output layer.Step 8: Adjust weights of the network in such a way that minimizes the training error. Then, repeat the steps 4–8 for each pair of input and output vectors of the training set until the error for the entire system is acceptably low. The equations for change in weights between the hidden and output layer and input and hidden layers are given below.Change in weights from hidden layer (h) to output layer (o) is given as(7)ΔWhjk=−αδhYihk(tha)(8)where,δj=tjd−tja∑k,hWhjk(∂Yk/hj(tja)/∂tja)Change in weight from input (i) to hidden layer (h) is given as(9)ΔWihk=−αδhYihk(tha)(10)whereδh=∑j=1nδj∑kWhjk(∂Yhjk(tja)/∂tha)∑k,iWihk(∂Yihk(tha)/∂tha)The proposed SNN has the following adjustable parameters: Number of hidden nodes (h), weights (Wihand Whj), learning rate (α) and decay time constant (τ). As with many other forecast methods, the accuracy of the proposed SNN is dependent on the appropriate adjustment of its parameters. Mean square error (MSE) is considered as the performance index for adjustment of the control parameters. The range for each of the parameter is selected as W∈[0.01, 3], number of hidden layer neurons h ε [12, 30], number of synaptic connections k∈[10, 20], decay time constant τ∈[4, 8], and learning rate α∈[0.0005, 0.01]. Initially, training samples are constructed for a period of 300 days. Keeping the other parameters constant, within the selected neighborhood, the weights from input to hidden layer and hidden to output layer are varied and the training process is stopped when MSE obtained is minimum. Now with the weights being set, and keeping all other parameters constant, the number of hidden layer neurons is varied. The number of neurons in the hidden layer determines the network's learning capabilities. The selection of the number of hidden neurons is an important issue in network design. So, optimal number of hidden layer neurons are selected and varied within the given neighborhood and trials are carried out till least MSE is obtained. The number of synaptic connections is another important parameter for obtaining reliable results from the network. The number of connections between two neurons is varied for each trial. The setting for which least MSE is obtained is selected. For the feed forward network used in this problem, the delay value in each synaptic connection is increased in steps of 2ms. It should also be noted that the delay in each terminal may not be same. The next parameter to be set with respect to the order of precedence is the decay time constant value (τ) which depends on the coding interval (Δt). The coding interval (Δt) is the difference between the minimum value and maximum value of the normalized input spikes which is problem dependent. τ is varied for each trial, but the best possible result is obtained when the coding interval is equal to the decay time constant. The last parameter to be tuned is the learning rate α. For high values of α, the MSE obtained after SNN training is very large and did not converge. Therefore, small values are considered.However, it should be noted that minimizing the MSE during training phase of the SNN may lead to over fitting problem. Hence, SNN begins to memorize the training samples instead of learning them. When over fitting occurs in a SNN, the MSE continues to decrease and it seems that the training process progresses, while in fact the generalization capability of the SNN degrades and it loses its forecasting ability for the unseen forecast samples. To overcome this problem, the generalization performance of the SNN should also be monitored during its training phase. Since the forecast error is not available in the training phase, validation error is used as an approximation. For SNN learning, validation samples are a subset of training period that are not used for the adjustment of the control parameters of the SNN. Thus, the validation samples can give a better estimate of the error for the forecast samples. Whenever the validation error increases, the generalization performance of the SNN begins to degrade. It indicates the occurrence of over fitting problem, and so the training process of the SNN by the spike propagation algorithm should be terminated at this iteration. Thus, the iteration with the minimum validation error gives the final results of the SNN training.The design of SNN model to perform load forecast includes determination of model structure, selection of training algorithm and input variables, which significantly influence the network performance. The architecture and the selection of input variables are discussed in Sections 4.1.1 and 4.1.2.The implementation of the proposed model is divided into two phases. The block diagram shown in Fig. 4consists of 2 SNN's in a sequential manner. E1 and E2 represent the training errors of SNN1 and SNN2 respectively. In phase two, the forecasted temperature values are given as input to SNN2 to forecast the load pattern.In Phase 1, using SNN1, a day ahead hourly temperature profile is forecasted. The input data consists of hourly temperatures of the previous 2 days (12 neurons), hourly humidity values of the previous 2 days (12 neurons), maximum solar radiation of the previous day (1 neuron) and the corresponding month encoded in binary form. The twelve months in the yearly calendar are encoded in binary form (0001 to 1100). For example, if the binary representation is 0010, it represents month of July (4 neurons). The target values consist of the corresponding hourly temperatures on the day of forecast (6 neurons). Therefore, SNN1 consists of 29 input nodes 6 output nodes, respectively. Training is carried out for 4 times with the input data divided into 4 parts of a day consisting of 24h. The first part consists of hourly weather variables from 12A.M to 5A.M. Similarly, the other 3 parts consist of data for the remaining time horizon. For each of the four input formats, the temperatures for 6h are forecasted. Therefore, all the 24 hourly temperatures are forecasted. The maximum and the minimum temperatures of the forecasted day is recorded and used as test input for SNN2. The above scheme of testing and training in Phase 1 is carried out for the city of Melbourne in Victoria. The same procedure is repeated for the city of Geelong in Victoria. The objective of the forecast engine is to forecast the electrical load for the state of Victoria in Australia. Therefore, to have an accurate forecasts result, the weather data for 2 cities are considered in Phase 1. Finally, the maximum and the minimum forecasted temperatures for the 2 cities is recorded which will be acting as 4 test inputs for SNN2 in Phase 2. The best combination of parameters for SNN1 is Wih∈(1, 2), Whj∈(2, 3), h=15, k=12, τ=7ms and α=0.001.The cross correlation analysis between temperature and humidity is shown in Fig. 5. It enables to choose the number of input variables with some time lag which affect the other variable. As seen in the figure, the cross correlation coefficients obtained for a time lag of 1–5h are highly negative. Therefore, humidity values for 6h are chosen as inputs to the forecasting model. A similar analysis performed on the maximum temperature and maximum solar radiation is shown in Fig. 6. It is observed that the coefficient obtained at zero lag is greater than any other coefficient. Therefore, solar radiation of the previous day is taken as an input variable for SNN1. The form of cross correlation function (ACF) adopted here follows that of Box, Jenkins, and Reinsel, specifically [40].In Phase 2, a day ahead half hourly load profile is forecasted. For example, if the load pattern of Monday is given as test input, the load pattern of Tuesday is forecasted. For producing accurate load forecasts, the best combination of input variables is required. Here in this paper, apart from historical load data, the factors which have significant impact on the consumption pattern like day of the week, weather variables, holiday effect, etc., are chosen as inputs to the network. The topology of SNN2 included the following variables as inputs:One day half hourly lagged load: (48 neurons)One day lagged temperature values (minimum and maximum values): (4 neurons)Forecasted temperature values obtained from Phase 1 (minimum and maximum values): (4 neurons)Maximum and minimum demand in the last 24h: (2 neurons)Average temperature value in the last seven days: (2 neurons)Day of the week is encoded in binary form: (001 to 111). For example, if the binary representation is 010, it represents Tuesday (3 neurons).Month number: (0001 to 1100) (4 neurons)Holiday effect: (0 – holiday; 1 – working day) (1 neuron)

@&#CONCLUSIONS@&#
