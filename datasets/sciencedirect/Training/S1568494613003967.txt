@&#MAIN-TITLE@&#
An accuracy-oriented self-splitting fuzzy classifier with support vector learning in high-order expanded consequent space

@&#HIGHLIGHTS@&#
A new fuzzy classifier (FC) with expanded high-order consequent space.FC design through self-splitting clustering and SVM in high-order consequent space.The FC achieves high classification accuracy with a small number of rules.

@&#KEYPHRASES@&#
Fuzzy classifiers,Fuzzy neural networks,Support vector machines,Clustering algorithms,

@&#ABSTRACT@&#
This paper proposes a self-splitting fuzzy classifier with support vector learning in expanded high-order consequent space (SFC-SVHC) for classification accuracy improvement. The SFC-SVHC expands the rule-mapped consequent space of a first-order Takagi-Sugeno (TS)-type fuzzy system by including high-order terms to enhance the rule discrimination capability. A novel structure and parameter learning approach is proposed to construct the SFC-SVHC. For structure learning, a variance-based self-splitting clustering (VSSC) algorithm is used to determine distributions of the fuzzy sets in the input space. There are no rules in the SFC-SVHC initially. The VSSC algorithm generates a new cluster by splitting an existing cluster into two according to a predefined cluster-variance criterion. The SFC-SVHC uses trigonometric functions to expand the rule-mapped first-order consequent space to a higher-dimensional space. For parameter optimization in the expanded rule-mapped consequent space, a support vector machine is employed to endow the SFC-SVHC with high generalization ability. Experimental results on several classification benchmark problems show that the SFC-SVHC achieves good classification results with a small number of rules. Comparisons with different classifiers demonstrate the superiority of the SFC-SVHC in classification accuracy.

@&#INTRODUCTION@&#
Many classification models have been proposed for pattern classification using numerical data. Examples of the classification models are neural networks (NNs) [1,2], fuzzy classifiers (FCs) [3], and statistical models [4,5], such as a mixture of Gaussian classifier [4] and support vector machines (SVMs) [5]. FCs are based on fuzzy if-then classification rules. Neural networks and evolutionary computation approaches are characterized with optimization ability and have been applied to solve different optimization problems [6–10]. These approaches have also been applied to automate the design of classification rules using numerical data [11–20]. One popular approach is to bring the learning ability of neural networks into a fuzzy system, and the model designed is usually called a fuzzy neural network (FNN) or a neural fuzzy system [11–15]. Another popular approach is to use the optimization ability of genetic algorithms (GAs) for fuzzy rule generation [16–20]. The NN- and GA-based approaches generate fuzzy rules based on empirical risk minimization, which does not account for small structural risk. The generalization performance may be poor when the FC is over-trained.In contrast to the NN- and GA-based design approaches, a relatively new learning method, the support vector machine (SVM), has been proposed based on the principle of structural risk minimization [5]. Several studies on introducing SVMs into fuzzy-classification-rule generation have been proposed to improve the generalization performance of an FC [21–25]. This paper proposes a self-splitting fuzzy rule-based classifier with support vector learning in expanded high-order consequent space (SFC-SVHC). Based on the self-splitting clustering algorithm in [25], the antecedent parameters in the SFC-SVHC are determined using a variance-based self-splitting clustering (VSSC) algorithm. The SFC-SVHC differs from the NN, GA, and SVM-based FCs above in rule form and consequent parameter learning. That is, contributions of the SFC-SVHC are twofold. First, FCs typically use zero- or first-order TS-type fuzzy rules [11–25], where the consequent of a fuzzy rule is a linear decision function and may restrict the rule discrimination capability. For regression problems, the use of different nonlinear functions in the consequent of a fuzzy rule for regression performance improvement has been recently proposed in [26,27]. This motivates the new idea of expanding the entire rule-mapped consequent space of a first-order TS-type fuzzy classifier, which is used in the SFC-SVHC. Different from the rule forms in previous FCs [11–25], the SFC-SVHC expands the entire rule-mapped consequent space of a first-order TS-type fuzzy system via trigonometric function transformations. The expanded rule-mapped consequent (ERMC) space can be regarded as the inclusion of high-order function terms for discrimination capability improvement. Second, the SFC-SVHC uses a linear SVM for consequent parameter optimization in the ERMC space. The cost function used in the optimization considers not only training error but also separation margin. The objective of using a linear SVM is to endow the SFC-SVHC with high generalization ability.The rest of this paper is organized as follows. Section 2 presents surveys on design of FCs. Section 3 introduces the SFC-SVHC structure. Section 4 describes the SFC-SVHC structure learning using the VSSC algorithm. Section 5 introduces SFC-SVHC parameter learning using a linear SVM. Section 6 demonstrates the SFC-SVHC classification performance by applying it to several benchmark classification problems. This section also compares the performance of the SFC-SVHC with those of different classifiers. Section 7 presents discussion. Finally, Section 8 presents conclusions.This section presents surveys of different data-driven FCs using NNs, GAs, and statistical learning. NN-based FCs are typically designed based on structure and parameter learning [11–15]. The neural fuzzy classifier in [11] uses k-means [28] for antecedent parameter initialization. The neural fuzzy classifier in [12] starts with a large rule base and a learning algorithm is used to prune the rules. The use of fuzzy C-means (FCM) [29] to determine the initial antecedent parameters is suggested in [12]. For the two clustering-based approaches in [11,20], the FC performance depends on the random distributions of the initial cluster centers. The neural fuzzy classifiers in [13,15] use rule-firing strength as a rule-generation criterion for automatic generation of the rules from training data. The maximizing-discriminability-based self-organizing fuzzy network (MDSOFN) in [14] also uses the same rule generation approach as in [13,15]. The characteristic of the MDSOFN is that the consequent parameters are mapped to a linear-discriminant-analysis space to improve the classification discriminability. Parameters in these neural fuzzy classifiers are all tuned using the gradient descent algorithm with the objective of training error minimization.GA-based FCs use GAs to optimize the structure and parameters in an FC. Several GA-based FCs have been proposed, such as a structural learning algorithm in a vague environment (SLAVE) [16], a fuzzy hybrid genetics-based machine learning (FH-GBML) [17], and a steady-state genetic algorithm for extracting fuzzy classification rules from data (SGERD) [18]. In the SLAVE, an iterative learning algorithm using GA is applied to find the number of rules and the parameters in rules. The FH-GBML uses the hybridization of Michigan and Pittsburgh approaches to optimize an FC. The SGERD uses a new method to extract a compact set of readable fuzzy rules from numerical data in much lower computational efforts than the SLAVE and FH-GBML. The performance of these GA-based FCs depends on the assignment of the initial rule base for selection. Many learning coefficients (more than five) in these GA-based FCs have to be properly assigned in advance for good classification performance. In addition, different runs generate different results due to the stochastic learning property of GAs.For statistical learning of FCs, the application of a statistical logitboost algorithm to the design of an FC (called LogitBoost) was proposed in [30]. For statistical SVM-based FCs, a positive definite fuzzy classifier (PDFC) was proposed in [21], where a support vector (SV) generates a fuzzy rule. Because the number of SVs in an SVM is usually very large, especially for complex classification problems, the number of rules in a PDFC is equivalently large. The support-vector-based fuzzy neural network (SVFNN) proposed in [22] also builds initial rules from SVs. Then, a learning algorithm is used to remove irrelevant fuzzy rules. However, this rule reduction approach does not maintain the generalization ability and thereby degrades the performance of the original classification model. A fuzzy system learned through fuzzy clustering and SVM (FS-FCSVM) was proposed in [23], where zero-order Takagi-Sugeno (TS)-type fuzzy rules were used. A self-organizing TS-type fuzzy network with support vector learning (SOTFN-SV) was proposed in [24], where first-order TS-type fuzzy rules were used. As in [13,15], these two FCs generate rules based on the rule firing strength of an input sample instead of SVs to achieve a small model size. This kind of structure learning approach generates a new rule and assigns its antecedent part parameters (i.e., center and width of a fuzzy set) according to the location of a single training sample, which does not consider input data distributions around a rule. The VSSC algorithm used in SFC-SVHC determines the antecedent part parameters according to the input data distributions around a rule.The SFC-SVHC is based on functional expansion of the ERMC space in a first-order TS-type fuzzy system. Each rule in a first-order TS-type fuzzy system is of the following form:Rule i: IF x1 is Ai1 and x2 is Ai2⋯and xnis Ain, then(1)yˆ=hi0+∑j=1nhijxjwhere x1, …, xnare inputs, Aijis a fuzzy set, and hijis a real number. Fig. 1shows the SFC-SVHC structure, which has a total of six layers. Detailed mathematical functions of each layer are introduced layer by layer as follows.Layer 1 (input layer): Each node in this layer corresponds to one input variable. The node first scales a real input variable to the range [−1,1] and then transmits the scaled value to the next layer. The training data is represented by a labeled set S with(2)S={(x⇀1,y1),(x⇀2,y2),…,(x⇀N,yN)},wherex⇀k∈ℝnand yk∈{+1, −1}.Layer 2 (fuzzification layer): Each node in this layer corresponds to a fuzzy set Aijand computes the degree to which an input value belongs to it. Fuzzy set Aijis employed with the following Gaussian membership function:(3)Mij(xj)=exp−(xj−mij)2di2where mijand didenote the center and width of the fuzzy set, respectively. Eq. (3) shows all fuzzy sets in rule i share the same width di. The SFC-SVHC uses a VSSC algorithm to automatically determine the values of mijand di, details of which are described in Section 4. The number of fuzzy sets in each input variable xiis equal to the number of fuzzy rules r. Therefore, this layer has a total of nr nodes.Layer 3 (rule layer): A node in this layer represents one fuzzy rule and performs antecedent matching of a rule. The number of nodes in this layer is equal to the number of rules r. Each node performs a t-norm operation on inputs from layer 2 using the algebraic product operation to obtain a firing strengthμi(x⇀). Thus, given an input data setx⇀=[x1,x2,…,xn], the firing strengthμi(x⇀)of rule i is determined by(4)μi(x⇀)=∏j=1nMij(xj)=exp−∑j=1n(xj−mij)2di2=exp−||x⇀−m⇀i||2di2where the same width diis assigned to all Gaussian fuzzy sets in the same rule.Layer 4 (consequent layer): The consequent part of each rule Riin a first-order TS-type fuzzy system is a linear combination of current inputs plus a constant, i.e.,∑j=1nhijxj+hi0. Based on this function, each node i in this layer multiplies the current input vector with its corresponding rule firing strength μifrom layer 3. That is, each node i in the consequent layer of SFC-SVHC maps current input vectorx⇀to a vectorg⇀iin the rule-mapped-consequent space using the following mapping function:(5)Ri:x⇀=[x1,x2,…,xn]∈ℝn→g⇀i=[μix0,μix1,…,μixn]∈ℝn+1,where x0≜1.Layer 5 (expanded consequent layer): To improve the mapping ability, each node i in this layer expands vectorg⇀iin the rule-mapped-consequent space to a new vectorϕ⇀iin the ERMC space using the trigonometric function transformation E described as follows:(6)E:g⇀i=[μix0,…,μixn]∈ℝn+1→ϕ⇀i=[μix0,sin(πμix0),cos(πμix0),…,μixn,sin(πμixn),cos(πμixn)]∈ℝ3(n+1).Because each scaled input xjis within [−1,1] and each rule firing strength μiis within [0,1], the factor π is used in the two sinusoidal functions so that the function inputs πμixjfall within the period [−π, π].Layer 6 (output layer): Each node in this layer determines the relative degree that the input data belong to a class. The node calculates a simple weighted sum for defuzzification. For the mth output node, the link weight between it and node i in layer 5 is a vectora⇀imdefined as follows:(7)a⇀im=[ai00m,ai01m,ai02m,…,ain0m,ain1m,ain2m]∈ℝ3(n+1),i=1,...,r,The mth output node integrates the output from each node in layer 5 plus a constant, and the SFC-SVHC output can be written as(8)yˆm=∑i=1r<a⇀im,ϕ⇀i(x⇀)>=∑i=1r∑j=0n[aij0mμixj+aij1msin(πμixj)+aij2mcos(πμixj)]+b=<a⇀ˆm,Φ⇀(x⇀)>+bwhere(9)a⇀ˆm=[a⇀1m,…,a⇀rm]=[a100m,a101m,a102m,…,a1n0m,a1n1m,a1n2m,…,ar00m,ar01m,ar02m,…,arn0m,arn1m,arn2m]∈ℝ3r(n+1)≜[aˆ1m,aˆ2m,…,aˆ3r(n+1)m]and(10)Φ⇀(x⇀)=[ϕ⇀1(x),…,ϕ⇀r(x)]=[μ1x0,sin(πμ1x0),cos(πμ1x0),…,μ1xn,sin(πμ1xn),cos(πμ1xn),…,μrx0,sin(πμrx0),cos(πμrx0),…,μrxn,sin(πμrxn),cos(πμrxn)]∈ℝ3r(n+1)≜[Φ1,Φ2,…,Φ3r(n+1)]The motivation of using the trigonometric function transformation in (6) is explained as follows. The two trigonometric functions sin(·) and cos(·) used are orthogonal. They expand each term μixjin the original rule-mapped-consequent space to a high-order polynomial in terms of μixjas follows:(11)sin(πμixj)=∑k=0∞(−1)k(2k+1)!(πμixj)2k+1=π(μixj)−(π)3(μixj)33!+(π)5(μixj)55!−⋯,and(12)cos(πμixj)=∑k=0∞(−1)k(2k)!(πμixj)2k=1−(π)2(μixj)22!+(π)4(μixj)44!−⋯.That is, each node performs a higher order expansion of its input vector in (5) by using the two trigonometric functions.The parameter vectora⇀ˆmin the ERMC space is tuned using a linear SVM, as introduced in Section 4. For a two-class classification problem, the number of output nodes in layer 6 is equal to one. The input datum is classified as class 1 ifyˆ1>0; otherwise, it is classified as class 2. For a multiclass classification problem, the one-against-the-rest classification approach [31] is used. The number of output nodes in layer 6 is equal to the number of classes CS(CS>2). To determine the class to which an input datum belongs, the winner-take-all technique is used. This technique finds the maximum output by taking(13)m*=argmax1≤m≤CSyˆm.The SFC-SVHC then classifies the input datum as the m*th class. The SFC-SVHC structure in Fig. 1 shows that only one network with multiple outputs is used for a multi-class classification problem. The number of antecedent parameters in the SFC-SVHC is r(n+1) and is independent of the class number CS. The number of consequent parameters in the SFC-SVHC is 3r(n+1)+1 when CS=2 and Cs[3r·(n+1)+1] when CS>2. This method is different than kernel-based SVMs using the same one-against-the-rest technique, where CSclassification models (SVMs) are created for a multi-class classification problem. Values of the parameters in the kernels and their weights are different for the CSkernel-based SVM models. The use of only one classification model in SFC-SVHC reduces the memory requirement for classifier storage and computation time.The proposed SFC-SVHC uses a VSSC algorithm to determine antecedent fuzzy set parameters mijand diin (4). A cluster in the input space corresponds to a rule in SFC-SVHC. There are no rules in SFC-SVHC initially. All rules (clusters) are generated by performing the clustering algorithm on the input datax⇀. In this algorithm, the varianceσ¯i2of cluster i is computed as follows:(14)σ¯i2=∑j=1nσij2,where σijis the standard deviation of the cluster in the jth input dimension, i.e.,(15)σij2=1Ni∑x⇀∈Clusteri(xj−mij)2wherex⇀∈Clusteridenotes the samples belonging to cluster i and Niis the number of samples in this cluster. The varianceσ¯i2is used to decide which cluster i should be split into two. For the existing r clusters, the algorithm finds(16)I=arg1≤i≤rmaxσ¯i2.If cluster splitting operation is performed, then cluster I is split into two clusters, resulting in a total of r+1 clusters. The center of a cluster i is denoted bym⇀i=[mi1,…,min]. The Euclidean distance between an input datum and a cluster with centerm⇀iis calculated as follows:(17)D(x⇀,m⇀i)=∑j=1n(mij−xj)2.The initial centers of the two newly split clusters are assigned as the two input data that are nearest to the original cluster I, i.e., with the smallest distanceD(x⇀,m⇀I). Fig. 2illustrates the case whenr¯this set to three. After splitting, new centers of these r+1 clusters are recomputed by the k-means algorithm. The entire process repeats until the maximum number of clusters is equal to a predefined numberr¯th. The entire VSSC algorithm is summarized in Appendix B.After the VSSC algorithm, the number of rules r in SFC-SVHC is determined as equal to the number of clusters. In addition, Step 1 in the VSSC algorithm determines the standard deviationσ¯iof cluster i using (14). Step 2 determines the centerm⇀iof cluster i. A cluster is regarded as a fuzzy rule. As a result, the centerm⇀iof the ith cluster is assigned as the centerm⇀iof the ith rule in (4). The standard deviationσ¯iof the ith cluster is assigned as the width diof the ith rule in (4), i.e.,di=σ¯i.The parameter vectora⇀ˆmin the ERMC space of an SFC-SVHC is determined by a linear SVM. In this section, basic concepts of classification by a liner SVM are first described. The technique of using a linear SVM for parameter learning in the ERMC space is then introduced.Suppose that a labeled training set S, as shown in (2), is given. The linear SVM learning approach attempts to find the hyperplane<w⇀,x⇀>+b=0defined byw⇀∈ℝnandb∈ℝ, such that the data are separated according to the decision function:(18)f(x⇀)=sign(<w⇀,x⇀>+b).Suppose that the training data is linearly non-separable. A hyperplane is designed such that(19)yk(<w⇀,x⇀k>+b)≥1−ξk,k=1,…,Nwhere ξk≥0 is a slack variable, with ξk>1 denoting data misclassification. The goal of the SVM is to find a separation hyperplane for which the misclassification error can be minimized while maximizing the margin of separation. Finding an optimal hyperplane requires the solution of the following constrained optimization problem:(20)Minw,ξ12<w⇀,w⇀>+C∑k=1NξkSubject toyk(<w⇀,x⇀k>+b)≥1−ξkwhere C is a user-defined positive parameter and ∑ξkis an upper bound on the number of training errors. Parameter C controls the tradeoff between error and margin, and a larger value of C corresponds to assigning a higher penalty to errors. By forming the Lagrangian, the prime problem in (20) can be converted into the following equivalent dual problem [5, Chap. 6]:(21)MaxαL(α)=∑k=1Nαk−12∑j,k=1Nαkαjykyj<x⇀k,x⇀j>Subject to∑k=1Nαkyk=0,0≤αk≤Cwhere αkis Lagrange multiplier and(22)w⇀=∑k=1Nαkykx⇀k.The final hyperplane decision function is determined by solving (21) and is given by(23)f(x⇀)=sign(<w⇀,x⇀>+b)=sign∑k=1Nykαk<x⇀,x⇀k>+b=sign∑k∈SVykαk<x⇀,x⇀k>+bwhere the training samples for which αk≠0 are called support vectors (SVs). The detailed derivation process can be found in [5].Eq. (8) shows that the output variableyˆmis a linear combination of variables μixj, sin(πμixj), and cos(πμixj) in the high-order ERMC space. This linear relationship motivates the use of linear SVM for optimizing the liner combination vectora⇀ˆm. For a binary class classification problem, the linear SVM is directly applied to finda⇀ˆ1. To finda⇀ˆmfor a multi-class classification problem, the original data are separated into two classes using the one-against-the-rest classification approach. In this approach, the training patterns that belong to class m form one class with label “+1”, and the rest of the training patterns form the other class with label “−1”. For a test pattern, the winner-take-all technique in (13) is applied to determine the class to which the input pattern belongs.Each input datumx⇀is transformed to the vectorΦ⇀(x⇀)=[Φ1,…,Φ3r(n+1)](see (10)) in the high-order ERMC space. Eq. (8) shows that the SFC-SVHC output is a linear function ofΦ⇀(x⇀)with the consequent parameter vectora⇀ˆmfunctioning as linear combination coefficients. Therefore, the linear SVM can be applied to determine the consequent vectora⇀ˆmin the ERMC space. In the ERMC space, the training data pairs in (2) are represented by(24)S={(Φ⇀1(x⇀1),y1),(Φ⇀2(x⇀2),y2),…,(Φ⇀N(x⇀N),yN)},where yk∈{+1, −1}. The objective function in obtaining the optimal consequent parameter vectora⇀ˆmis given as follows:(25)Mina⇀ˆm,ξk12a⇀ˆmTa⇀ˆm+C∑k=1NξkSubject toyk(a⇀ˆmTΦ⇀(x⇀)+b)≥1−ξk.The vectorΦ⇀(x⇀)is fed as input to a linear SVM, and the data pairs are classified by the linear SVM in (23). According to (23), the output function is(26)yˆm(x⇀)=∑k=1Nykαk<Φ⇀(x⇀),Φ⇀k(x⇀k)>+bmwhere αkis solved by the linear SVM in Section 5.1. Eq. (26) can be represented by(27)yˆm(x⇀)=∑k=1Nykαk∑i=13r(n+1)Φi(x⇀)Φi(x⇀k)+bm=∑i=13r(n+1)∑k=1NykαkΦi(x⇀k)Φi(x⇀)+bm.Because (27) is equivalent to (8), parametersaˆimin (8) can be easily found from these two equations and are given as follows:(28)aˆim=∑k=1NykαkΦi(x⇀k)=∑k∈SVykαkΦi(x⇀k),i=1,…,3r(n+1).The motivation of using the linear SVM instead of the widely used gradient descent algorithm for parameter vectora⇀ˆmdetermination is to improve the generalization ability. When the gradient algorithm is employed, the cost function to be minimized considers only the training error. When the linear SVM is employed, the cost function to be minimized considers not only the training error but also the classification margin as shown in (25). The incorporation of the margin helps reduce the bound on the generalization error and therefore improves the SFC-SVHC generalization ability. Details of the corresponding theorems can be found in [5, Sec. 4.3]. This advantage is verified through experimental results in the next section.The linear SVM is used to tune the rule-mapped-consequent parametersa⇀ˆmin (8). That is, the linear SVM, as well as the gradient descent algorithm, is used as a parameter learning algorithm, which does not change the original function of the SFC-SVHC. As shown in (28), the number of SVs influences the values of parametersaˆimrather than the structure of the model. If a nonlinear SVM with a nonlinear kernel function K is used, then the original function (8) of SFC-SVHC is changed to∑i∈SVsyiαiK(Φ⇀,Φ⇀i). That is, the original function is changed, where the size of the new model increases with the number of SVs which is usually very large for complex classification problems. Since the objective is to tune the parameters in the SFC-SVHC, this paper adopts the linear SVM instead of the nonlinear SVMs.Finally, the learning steps of the entire SFC-SVHC are listed as follows:SFC-SVHC learning algorithmStep 1: (Initialization) Set the rule numberr¯thand the coefficient C in (20).Step 2: (Antecedent parameter learning) Perform the VSSC algorithm described in Section 2 and given in Appendix B to determine the antecedent parameters mijand diin (3).Step 3: Find the ERMC vectorΦ⇀(x⇀)in (10).Step 4: (Expanded consequent parameter learning) Use the linear SVM solution in (28) to find the ERMC parametera⇀ˆmin (9), where m=1,…,Cs.End algorithm

@&#CONCLUSIONS@&#
