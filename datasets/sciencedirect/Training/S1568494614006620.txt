@&#MAIN-TITLE@&#
Implementation of the RBF neural chip with the back-propagation algorithm for on-line learning

@&#HIGHLIGHTS@&#
This paper presents the hardware implementation of the floating-point processor (FPP).Radial basis function (RBF) neural network is developed on FPGA.FPP is designed to implement the back-propagation algorithm in detail.The on-line learning process of the RBF chip is compared numerically with the results of the MATLAB program.The performance of the designed RBF neural chip is tested for the real-time pattern classification of the XOR logic.Performances are evaluated by comparing results from the MATLAB through extensive experimental studies.

@&#KEYPHRASES@&#
Floating-point processor,RBF neural network,Back-propagation algorithm,FPGA,Nonlinear classifier,

@&#ABSTRACT@&#
This article presents the hardware implementation of the floating-point processor (FPP) to develop the radial basis function (RBF) neural network for the general purpose of pattern recognition and nonlinear control. The floating-point processor is designed on a field programmable gate array (FPGA) chip to execute nonlinear functions required in the parallel calculation of the back-propagation algorithm. Internal weights of the RBF network are updated by the online learning back-propagation algorithm. The on-line learning process of the RBF chip is compared numerically with the results of the RBF neural network learning process written in the MATLAB program. The performance of the designed RBF neural chip is tested for the real-time pattern classification of the XOR logic. Performances are evaluated by comparing results from the MATLAB through extensive experimental studies.

@&#INTRODUCTION@&#
Intelligent electronics that process intelligent information are rapidly getting attraction of researchers in engineering and information fields. Not only software programming, but also intelligent hardware for intelligent information processing is increasingly demanded due to the development of hardware technology.The word of ‘intelligence’ has been frequently used in the area of machine learning, soft computing, or artificial intelligence [1,2]. Nowadays, the concept of intelligence is used in almost every research fields including signal processing, control, mechatronics, robots, materials, transportation and even construction. Typically, in the areas of dealing with physical systems such as dynamical systems, intelligent electronics are severely demanded to improve the system performance.A neural network mimicking a human brain is one of powerful and popular intelligent tools since it has learning capability, adaptation capability, and generalization capability. An ultimate goal of using neural network is to use those capabilities to improve the performance in various aspects.Specially, in the area of dynamical systems, neural network has been used as nonlinear controllers for robot manipulators to minimize the tracking errors [3–6]. With the help of hardware technology, neural network and fuzzy logic controllers are designed on a DSP chip along with FPGA for controlling nonlinear dynamical systems [7,8]. Multilayered neural networks are implemented with particle swarm optimization algorithms on FPGA for identifying dynamical systems [9]. Fuzzy inference modules are also implemented on FPGA [10,11]. Interval type-2 fuzzy systems for a real plant are implemented on FPGA [12] and particle swarm optimization of interval type-2 fuzzy systems is implemented on FPGA [13].Recently, a lot of attempts to design neural network hardware on a field programmable gate array (FPGA) chip as an intelligent electronics have been made [14–17]. In many cases of developing the neural network hardware, the forward propagation of neural processing is only designed on an FPGA chip for an offline learning and online control scheme although the complete neural network processing requires two stages: forward and backward propagation [18–21]. FPGA implementations on stochastic and adaptive control applications are used [22,23]. Designing the backward propagation process is relatively difficult since a lot of calculations are involved to update internal weights [24–26]. FPGA hardware is developed to implement on-line learning for shape recognition [27]. Multi-layered perceptron network (MLP) has been developed on FPGA with fixed point data notation and its performance is measured by hardware specifications such as the number of multiply–add operation and the number of weight updates per second [28]. The neural estimators were designed by high level programming language and downloaded on NI C-RIO using Labview software for speed estimation of two-mass drive system [29]. RBF neural network has been implemented in vision system for real-time face tracking and identity verification. Since the facial identification is focused, the detailed hardware implementation of RBF neural network was not elaborated [30]. Genetic algorithms are implemented on FPGA [31]. Design and implementation of a RBF neural network on FPGA with the detailed back-propagation algorithm for an on-line classifier or controller, of which learning can be done in on-line fashion, are relatively rare in the literature.In this paper, therefore, a hardware design and implementation of the radial basis function (RBF) neural network (NN) by designing the floating-point processor (FPP) using the hardware description language (HDL) is presented. Due to its nonlinear characteristics of the network, it is very difficult to implement the neural hardware with integer-based operations to satisfy the acceptable accuracy. To calculate nonlinear functions required in the neural network processing such as sigmoid functions or exponential functions, floating-point operations are required.Designing the floating-point processor allows us to implement nonlinear functions. Thus, the exponential function can be designed as the 32 bit single-precision floating-point format. In addition, to update weights in the network, the back-propagation algorithm can also be designed with the HDL and implemented in the FPGA hardware. Most operations are performed in the floating-point based arithmetic unit and accomplished sequentially by the order of instructions stored in a ROM.Finally, the RBF neural network hardware is implemented by writing the assembly codes running on the floating-point processor. The RBF network is tested on the FPGA for nonlinear classifications. Extensive experiments are conducted by comparing possible calculation deviation errors due to the functional approximation between the designed chip and the MATLAB to evaluate the feasibility of using the neural network chip.The radial basis function (RBF) network is one of neural network structures that learn by measuring the Euclidean distance of data. The RBF network is simple in the structure that there are one nonlinear hidden layer and a linear output layer as shown in Fig. 1. There are no weights between the input layer and the hidden layer. The linearity of the output layer becomes an advantage of analyzing the stability of the closed loop system in association with feedback controllers. This simple structure turns out to be advantages of RBF network over the multilayered perceptron (MLP) network for dynamical systems [32].Each neuron at the hidden layer has a Gaussian function described as(1)ϕj(x)=exp−∑i=1L(xi−μj)22σj2where xiis the ith input element, μjis the mean value of the jth hidden unit, σjis the covariance of the jth hidden unit, and L is the number of input elements.The output layer is linear so that the kth output is an affine function that can be obtained as(2)yk=∑j=1Mϕjwjk+θkwhere ϕjis the jth output of the hidden layer in (1) and wjkis the weight between the jth hidden unit and the kth output, θkis the bias weight of the kth output, and M is the number of the hidden units.To train the network shown in Fig. 1, the back-propagation algorithm is derived to update weights between the hidden layer and the output layer, and nonlinear function parameters of the hidden units. The output error for the kth output is defined as(3)ek=ydk−ykwhere ydkis the desired output value. The output error is propagated backward to adjust internal weight values, which is known as the back-propagation algorithm.Then, the objective function is defined to minimize the output errors as(4)E=12∑k=1Nek2where N is the number of the output units.The back-propagation algorithm searches the minimum value by calculating the gradient of Eq. (4) with respect to the weight. The gradient of Eq. (4) is calculated as(5)Δw=−η∂E∂wwhere w can be weights or Gaussian parameters and η is the learning rate. Substituting (4) into (5) and applying the chain rule yields(6)Δw=−η(1/2)∂(e12+⋯+ek2+⋯eN2)∂ek∂ek∂w=−ηek∂ek∂wTherefore, the function∂ek/∂wneeds to be obtained for each internal variable,(wjk,θk,μj,σj)∈wof the RBF network. One typical example for the weightwjk, we have(7)Δwjk=−ηekw∂ek∂wjk=−ηwek∂(ydk−yk)∂yk∂yk∂wjk=ηwek∂yk∂wjk=ηwekϕjwhereηwis the learning rate.In the same manners, we can have the detailed update equations as(8)Δwjk=ηwekϕj,Δθk=ηθek,Δμj=ημϕj∑i=1L(xi−μj)σj2∑k=1Nekwjk,Δσj=ησϕj∑i=1L(xi−μj)2σj3∑k=1Nekwjk,whereηw,ηθ,ημ,ησare learning rates. Finally, all weights are updated recursively.(9)w(t+1)=w(t)+ΔwTo implement the RBF network hardware, Eqs. (1), (2), (8), and (9) are the main algorithms. Eqs. (1) and (2) are implemented for the forward propagation, (8) and (9) for the back-propagation process. In the forward propagation, the Gaussian function has to be represented in the hardware description language (HDL).Unfortunately, there is no property of representing nonlinear functions in the HDL. Thus, the numerical approximation procedure of expressing the Gaussian function is required.The Gaussian function for the real value x is given by(10)f(x)=e−((x−μ)2/2σ2)One of approximating methods is to use the Taylor series expansion. For example, the exponential function can be expressed by the Taylor series as follows.(11)e−x=1−x+12!x2−13!x3+14!x4−15!x5+16!x6−17!x7+18!x8+⋯+HOTwhere the high order terms (HOT) are ignored. In the programming with the HDL for an FPGA chip, we use the following format instead of (11) for simplicity.(12)e−x=((⋯(18x−1)17x+1)16x−1)15x+1)14x−1)13x+1)12x−1)x+1+⋯+HOTTruncation of the HOT may yield the approximating error of nonlinear functions. Effects by the approximation process are analyzed and evaluated for possible applications in the later section.The floating-point based processor is designed. The processor can perform arithmetic calculations such as addition, subtraction, multiplication, and division. The architecture of the processor follows the Harvard reduced instruction set computer (RISC) architecture. The overall structure of the processor is described in Fig. 2.The processor has the basic 4 cycles of fetch, decode, execute, and write-back steps to execute an instruction. Each unit has the function as follows.•Program memory: it is a ROM that stores instruction codes. Its word length is 32 bit long and 12 bit address can access 4096 instructions. Thus, the size of the program memory is 16K bytes.Data memory: it is a RAM that stores data. Its word length is 32 bit long and the size is 16K bytes.Program counter: it contains the address of the next instruction in the program memory. It refers to the status register to execute Jump or conditional branches.Instruction register: it stores the instruction code from the program memory for one cycle for decoding.Instruction decoder: a 32 bit instruction code is separated into an op-code and operands and then decoded into control signals.Status register: it states conditions of the functional unit after each calculation.Register group: it has the master group and the slave group. It can be used as a source for operand 1 and operand 2 for the functional unit. It has 32 registers and the register size is 16 bit.Reference ROM: it contains frequently used constants for calculations such as π, a constant for exponential function, and coefficients for the Taylor series expansion of sinusoidal functions and other nonlinear functions.Functional unit: it performs the arithmetic calculation based on a 32 bit single precision format such as addition, subtraction, multiplication, and division. It also performs logical operation as well. It has the converter for changing the floating-point format to the fixed-point format or vice versa.Instruction sets such as LD, MOV, ADD, SUB, MUL, DIV are defined as op code. The instruction set is 32 bit long composed of 7 bit op code, 24 bit operand as shown in Fig. 3. Assembly codes by the sequence of instructions can represent neural network algorithms.To save the time for the instruction, instruction is executed by the pipelining process of fetch, decode, execute and write-back as shown in Fig. 3.The data is a 32 bit single floating-point based on IEEE format. Table Ishows the special cases of number representations.Addition/subtraction takes 4 steps of recognition, arrangement, operation, and rearrangement as shown in Fig. 4.The recognition module decides the case depending upon conditions of operand, exponent, and mantissa as listed in Table II. The arrangement module adjusts the number of bits and operation module executes addition/subtraction. The rearrangement module modifies the exponent and sends it out. All of processes are completed within one clock cycle.The multiplication module is relatively simple. Multiplication can be done by an exponent part and a mantissa part separately. Signs are determined by XOR operation, exponents by addition, and mantissa by regular multiplication. Conditions such as zero, overflow, underflow, de-normalized error are also checked during operation. Fig. 5shows the multiplication block module.In the same way of multiplication, each part is calculated separately as in Fig. 6. The exponent part can be modified with respect to the result of mantissa operation. This can be adjusted by considering several cases of operands. The flow of the mantissa division process is shown in Fig. 7. Two mantissa values are compared under different cases.The data format needs to be modified when different format data such as integer-based data to process data internally are obtained from the external devices. Fig. 8shows the format converter that converts integer to single precision and single precision to integer.Using the floating-point processor as a core unit, the RBF network can be designed. The flow chart of processing the RBF network is shown in Fig. 9. The procedure can be divided into three parts, initialization, forward calculation, and backward calculation. This iterative procedure can be repeated until the error converges to satisfy the specified tolerance.The block diagram of the RBF network design is shown in Fig. 10and the detailed design by HDL is shown in Fig. 11. It consists of the floating-point processor, memories, a boot module, and a status controller.The process of the RBF network can be programmed by the instruction designed in the floating-point processor. The assembly codes are written for calculating the forward process and the backward process of the RBF network. Fig. 12(a) shows the program for the forward process of calculating Eq. (2) and the outputs are calculated from the inputs via the hidden layer. Each operation of calculating ϕjis shown in Fig. 12(a). Then the output ykis calculated and compared with the desired value to yield the error. Next is the back-propagation algorithm. The error is used for the backward calculation. Four different parameters are updated, weights between the hidden and the output layer, the bias weight of the output layer, the mean value of the hidden units, and the covariance values of the hidden units. The assembly codes for updating weights are presented in Fig. 12(b).The hardware design of the RBF network is described in Fig. 11. It is composed of a ROM for booting, a ROM for storing instructions, a data memory, a core processor, a control unit, and a communication module. The communication module sends resultant data to the external device after calculation by other modules.•A: Boot_ROM: initialize and load parameters of RBF network such as learning rates, weights to the data memory.B: Instruction_ROM: holds instructions for calculating the RBF network. It has a size of 32 bit 16K byte words.C: data memory: stores resultants after calculating the RBF network. Parameters such as μ or σ are stored. It has a 16K byte memory.D: core module: calculates actual processes of the RBF network after 4 step pipelined structure depending upon instructions.E: control module: once the calculation of the RBF network is done, the value of the objective function is sent to the PC through serial communication. It controls where to send the data by specified address.Others: multiplexers: synchronize the data or signals if necessary.The performance of the designed neural chip is tested on Cyclone II EP2C70F672C8 FPGA board, which has 300,000 gates. The classification of the XOR logic is performed by the neural chip. The XOR logic is known to be the prototype example of the nonlinear classification example determining the performance of neural networks. The RBF network designed as in Fig. 11 is embedded into this FPGA chip.Initial weights and input patterns are given in Table III. Although the number of RBF units and good initial center values can be easily found, here nominal values are used. To examine the numerical accuracy of the neural chip, the same initial conditions are specified for the neural chip and the MATLAB simulation. The 50th order of the Taylor series approximation is used.Learning takes about 650 iterations until the error converges as shown in Fig. 13. Fig. 13 shows the comparison plots of the error convergence between the neural chip and the MATLAB simulation. Under the same conditions, the RBF network is tested for the neural chip and the MATLAB. Fig. 13 shows the similarity of the error convergence with a small deviation when the error starts converging at around the 650th iteration. The difference is minimal and can be improved by allowing the high order of the Taylor series since this deviation occurs due to the numerical approximation of the nonlinear functions.The learning stops when the error satisfies the tolerance specified in Table III. After the learning process, all patterns are tested for the classification. The resultant classification is given in Table IV. We clearly see that learning of the neural chip has been done correctly.After learning, the final parameter values are determined and given in Table V.Based on the data from Table V, the output value of the hidden units with respect to each input pattern is calculated. Table VIsummarizes the values.(13)φ1(x)=e−(x−μ02/2σ02)=e−(x−0.11095808/2(0.33398876)2),φ2(x)=e−(x−μ12/2σ12)=e−(x−0.901311162/2(0.23129223)2)Thus, we can draw the decision boundary for the two classes. Based on Eq. (13), the decision boundary can be drawn on the input space. We see from Fig. 14that two classes are separated by the hyper plane generated by the RBF neural network.

@&#CONCLUSIONS@&#
