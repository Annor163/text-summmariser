@&#MAIN-TITLE@&#
Link-topic model for biomedical abbreviation disambiguation

@&#HIGHLIGHTS@&#
We suggest the link topic model for disambiguating biomedical abbreviations.The model chooses the most probably sense that generates the entire document with its underlying topic.Two distinct modes for word generation are designed to explore semantic dependencies between words.A word is generated either dependently on a specific long form or independently.

@&#KEYPHRASES@&#
Topic model,Latent Dirichlet allocation,Biomedical abbreviation disambiguation,Semantic link,Global abbreviation,

@&#ABSTRACT@&#
IntroductionThe ambiguity of biomedical abbreviations is one of the challenges in biomedical text mining systems. In particular, the handling of term variants and abbreviations without nearby definitions is a critical issue. In this study, we adopt the concepts of topic of document and word link to disambiguate biomedical abbreviations.MethodsWe newly suggest the link topic model inspired by the latent Dirichlet allocation model, in which each document is perceived as a random mixture of topics, where each topic is characterized by a distribution over words. Thus, the most probable expansions with respect to abbreviations of a given abstract are determined by word-topic, document-topic, and word-link distributions estimated from a document collection through the link topic model. The model allows two distinct modes of word generation to incorporate semantic dependencies among words, particularly long form words of abbreviations and their sentential co-occurring words; a word can be generated either dependently on the long form of the abbreviation or independently. The semantic dependency between two words is defined as a link and a new random parameter for the link is assigned to each word as well as a topic parameter. Because the link status indicates whether the word constitutes a link with a given specific long form, it has the effect of determining whether a word forms a unigram or a skipping/consecutive bigram with respect to the long form. Furthermore, we place a constraint on the model so that a word has the same topic as a specific long form if it is generated in reference to the long form. Consequently, documents are generated from the two hidden parameters, i.e. topic and link, and the most probable expansion of a specific abbreviation is estimated from the parameters.ResultsOur model relaxes the bag-of-words assumption of the standard topic model in which the word order is neglected, and it captures a richer structure of text than does the standard topic model by considering unigrams and semantically associated bigrams simultaneously. The addition of semantic links improves the disambiguation accuracy without removing irrelevant contextual words and reduces the parameter space of massive skipping or consecutive bigrams. The link topic model achieves 98.42% disambiguation accuracy on 73,505 MEDLINE abstracts with respect to 21 three letter abbreviations and their 139 distinct long forms.

@&#INTRODUCTION@&#
As biomedical research has grown rapidly, text mining and automatic data construction systems based on text processing and machine learning techniques have become essential in the biomedical domain. In particular, many important terms including clinical diseases, procedures, genes, proteins, and chemicals in bio text are often expressed as abbreviations.1In general, abbreviations can be classified into (1) acronyms that refer to words formed from the initial letters or letters of successive or major parts of their long form, and (2) non-acronyms that do not follow particular lexical patterns of the long forms [1].1Furthermore, biomedical abbreviations are highly ambiguous than ordinary terms [2,3], e.g. “APC” can refer to different long forms such as “antigen presenting cell(s),” “activated protein C,” “atrial premature contraction,” “atrial premature complexes,” “antiphlogistic-corticoid,” “anaphase-promoting complex,” and “adenomatous polyposis coli”. In other words, homonyms of abbreviations that share the same spelling but refer to multiple long forms2The long form of an abbreviation can be variously represented as ‘full form,’ ‘sense,’ ‘reference,’ ‘definition,’ or ‘expansion.’2are prevalent in biomedical text. According to the study of Liu et al. [3], 81.2% of abbreviations listed in the Unified Medical Language System (UMLS) Metathesaurus3UMLS is a large, multi-purpose Metathesaurus containing millions of biomedical and health-related concepts, their synonyms, and interrelationships. Please refer to http://www.nlm.nih.gov/research/umls/knowledge_sources/.3[4] are ambiguous and each abbreviation has, on average, 16.6 multiple long forms [5]. In addition, the same abbreviation may be used to denote RNA, protein, disease, or gene across a variety of entity categories [5,6].A common issue with biomedical abbreviations is the multiplicity of lexical variants associated with a given abbreviation [7–10]. For instance, “CML” has several different long forms of the same sense: “chronic myelocytic leukemia,” “chronic myelogenous leukemia,” and “chronic myeloid leukemia”. Similarly the abbreviation “APC” can have many spelling variants, such as “Apc,” “aPC,” “ApC,” “aPC,” “APc,” “apc,” and “AP-C.” These term variations cause uncertainties regarding exact term identification/normalization when the terms in text are mapped to unique identifiers of a reference database [8,9].Furthermore, abbreviations usually appear without their definitions, which it makes the task of identification more difficult [10–12]. Based on the definition given by Gaudan et al. [11], these are known as global abbreviations, which are distinct from local abbreviations that are explicitly stated with their intended long forms in text. Genes and proteins are referred to in text in a range of different ways as full names and abbreviation symbols. In fact, a considerable number of gene names are expressed as global abbreviations [13–15]. Some studies have reported that only 30% of gene abbreviations are accompanied by their corresponding long forms in text [13,14]. In addition, disambiguation of global abbreviations with multiple definitions is essential in establishing relationships among entities such as protein interaction map or genetic network that require correct definition pairs to be recognized [8].Especially, gene terms may refer to general English words [3,5,14,16] and have different meanings depending on the context. Since common English words that are also gene names, such as “end”, “leg”, “white”, and “key” often generate false positives in NLP text-mining applications, one main issue is to resolve abbreviation ambiguities.Many studies have thus far been conducted on biomedical abbreviations in terms of (1) recognition and (2) disambiguation. An abbreviation recognition (extraction) task detects abbreviations and their long forms explicitly defined in biomedical texts [10]. It has been studied using various methods including: set of rules or patterns [17], co-occurrence statistics by neighboring words of an abbreviation [1,18], and machine learning [19–21]. The process of extracting abbreviation-long form pairs from a raw text is still a major issue because the pairs are not sufficiently listed in most databases and new abbreviations appear at a fast rate. In fact, some research has reported that the sense inventory generated from the UMLS Metathesaurus could cover only 35% of long forms for abbreviations, which showed a low coverage for abbreviations [5,14].On the other hand, abbreviation disambiguation has been studied to a lesser extent than the abbreviation-long form pair extraction [10]. As abbreviation disambiguation identifies its intended sense (long form) in a given context, it is, in effect, equivalent to word sense disambiguation (WSD), which has been studied extensively during the past years. Most approaches have focused on the neighboring words of ambiguous words because these contextual words can provide strong and consistent clues for determining the sense of an abbreviation [11,15,21,22]. Thus, the context of an abbreviation is usually compared with previously-learned contexts to disambiguate its sense by using a context similarity. The similarity has been often assessed by determining how many words are shared or overlap between the contexts. However, such word-based context similarity can be a problem due to the lack of consistent vocabularies and variations in lexical choice.In this study, we propose the novel link topic model based on the latent Dirichlet allocation (LDA) [23] for disambiguating biomedical abbreviation in MEDLINE abstracts. The model chooses the most probable sense that generates the entire document with its underlying topic structure and link dependency between words. In the model, each document exhibits multiple topics with different proportions and each word is generated from one of the topics and link information. The link can be interpreted as a topical and contextual relatedness between two words, particularly long form words of abbreviations and their sentential co-occurring words. The existence of a link between two words is inferred by the Gibbs sampling method. To model the link association with the topic model, a probabilistic distribution that corresponds to the strength of the associations is specified so that two words constitute a link if they tend to co-occur with one another and to have similar topics with high frequency. That is, a link dependency is parameterized by topics and co-occurrence frequencies of two words.As a consequence, the proposed model avoids the insensitivity to word order of previous topic models based on a bag-of-words assumption, and the representations extracted by the model capture a richer latent structure of a text in terms of unigrams, consecutive bigrams, skip bigrams, and semantic dependencies between words. Compared to previous studies such as bigram [24], n-gram [25], and collocation [26] topic models in which word associations are sequential dependencies based on word order rather than semantic context, the word associations represented by the link captures both sequential dependencies and semantic context well.An abbreviation is disambiguated by computing the likelihood of a document. That is, the long form among candidates that is most likely to be generated given underlying topic and link structures of a document is determined. We use Gibbs sampling for approximate posterior inference for topics and word link distributions that best explain a collection of documents (observations). Twenty-one abbreviations were studied, with more challenging rare sense tested in comparison to previous works.Most studies that disambiguate biomedical terms employ linguistic features commonly used in WSD for general text to reflect contextual usage for each term sense. These include unigrams, bigrams, surrounding words, word collocations, lexical or syntactic information, and additional domain-specific biomedical resources such as Concept Unique Identifiers (CUIs) and Medical Subject Headings (MeSH) terms [10,11,15,22].Pakhomov [22] suggested two types of contextual features based on maximum entropy models to disambiguate abbreviations. The local-context model, which is based on two preceding and two following words of an abbreviation’s long form, was compared with the combo model, which is based on a combination of sentence- and section-level contexts. He achieved an approximately 89% accuracy with respect to six frequently used abbreviations [22]. Stevenson et al. [15] suggested a vector space model based on linguistic context features, MeSH, and CUIs to identify long forms of abbreviations. The accuracy of abbreviation disambiguation was very high (nearly 99%) with respect to the relatively small number of word senses. Liu et al. [21] also disambiguated abbreviations by using various context features and classifiers.Gaudan et al. [11] distinguished global abbreviations from local abbreviations. They built a dictionary for local abbreviation resolution and disambiguated global abbreviations using support vector machines (SVMs) trained to recognize the contexts of each abbreviation-sense pair. They merged long forms by grouping morphologically similar forms based on the n-gram similarity and morphologically dissimilar forms based on the number of shared words in documents containing the long forms. They achieved a high performance (98.5% accuracy) on the abbreviation disambiguation. However, they considered only major senses of words that appeared in more than 40 documents, with each abbreviation having 3.4 senses on average. Okazaki et al. [10] consolidated term variants by combining various string similarity measures between long forms such as character n-gram similarity, normalized Levenshtein distance, SoftTFIDF, and Jaro-Winkler similarity.In addition, LDA topic models motivated by Blei’s research have been applied to WSD [27–29]. Zhang et al. [27] resolved problems associated with name variation and ambiguity of entities based on the context similarity between two named entities in the K-dimensional topic space with a Hellinger distance. Boyd-Graber et al. [28] assumed that words assigned to the same topic would have similar meanings and share the paths within WordNet.4http://wordnet.princeton.edu/wordnet/download/.4Based on this assumption, they suggested a probabilistic process of word generation according to hyponomy relationships in WordNet, called WordNet-WALK, to explore a general domain WSD problem. Stevenson [29] represented the context of an ambiguous biomedical word with terms associated with the topics of 20 neighboring words. The study also employed the LDA topic model.To enhance understanding of the proposed link topic model, we first briefly introduce the basic LDA topic model [23] and collocation topic model [26] that motivated this study. The topic model is a methodology to discover patterns of word use and connect documents that exhibit similar patterns. It assumes that the words of each document arise from a random mixture of topics, where each topic is characterized by a distribution over words [23]. Thus, the model can handle a document in a more intuitive way than other methods that assume each document exhibits exactly one topic. In the model, all documents in the collection share the same set of topics. Also, each document exhibits multiple topics with different proportions and each word is generated from one of the topics. The hidden topic structures of documents are inferred from the distributions of observed words in a completely unsupervised way without labeling or any special initialization. Only the number of topics should be specified in advance. As a result, documents that share similar words have a similar thematic structure. Another advantage of LDA is that straightforward inference procedures can be provided on previously unseen documents, which is in contrast to other semantic models.Following the notation of Blei [23], we define a document as a sequence of N words denoted byw=(w1,w2,…,wN), where wiis the observed word for each position i in the document, and a corpus as a collection of M documents denoted by D={w1,w2,…,wM}. A latent topic structure is defined asz={z1,z2,…,zk}. The generative process is represented by a probability distribution of wordswgivenz, p(w|z). Given a collection of documents, we can inferzparameters from observed wordswusing Bayes’ rule. Typically, LDA generates words for each document in a corpus according to the following process.(1)For each document d, draw a distribution over topics θdfrom a Dirichlet prior α.For each word wiin document d.(a)Draw topic zifrom the document-topic distribution ∼Multinomial(θd).Draw word wifrom the word distribution over the sampled topic∼Multinomial(ϕzi).Fig. 1(a) visualizes the graphical model of the LDA topic model, where nodes represent random variables, shaded nodes are observed variables, and edges are dependencies. Let us assume that the collection of documents is comprised of only two topics, such as “genetics,” and “virus.” For instance, we first choose a topic distribution θdfor each document that places probability on “genetics” and “virus” topics with different portions and then for each word, we choose a topic (z) from θd. If the “virus” topic for the word is chosen, then word “injection” is generated from a multinomial probability conditioned on the topic, ϕvirus. In other words, the “virus” topic has probabilities of generating various words. The word “infection” will have a high probability given that topic. In the model, ϕ denotes the matrix of topic distributions over words drawn from Dirichlet (β) prior, andθdenotes the matrix of document-specific topic distributions drawn from Dirichlet (α) prior.The parametersαandβare parameterized by K (number of topics) and K×V (number of vocabularies), respectively, where αkcorresponds to the prior observation count for the number of occurrences of topic k, and βlowcorresponds to the prior observation count for the number of times the word w is sampled from a topic k. However, for convenience, all different α1,α2,…αkand β11,β12,…βKVare assumed to be one fixed quantity with α (50/k) and β(0.01).Because the Dirichlet distribution is the conjugate prior of the multinomial distribution, this property facilitate the process for LDA inference and parameter estimation. Given fixed parameters α and β, the joint distribution of a topic mixtureθ, a set of wordswand a set of topicsz, is given by:(1)p(θ,z,w,ϕ|α,β)=∏j=1Mp(θj|α)∏n=1Np(zn|θj)(wn|zn,ϕzn)p(ϕ|β)The key inferential goal is to compute the following posterior distribution of the hidden topic variables in a given document such that(2)p(z,θ,ϕ|w,α,β)=p(w,z,θ,ϕ|α,β)p(w|α,β)Although the posterior distribution is intractable for exact inference, the hidden topic structurezis iteratively inferred using the following equation based on the Gibbs sampling algorithm [30].(3)P(zi=t|wi=w,z-i,w-i)∝p(wi=w|zi=t,w-i,z-i)p(zi=t|w-i,z-i)∝Cw,t-i+β∑w′Cw′,t-i+WβCt,d-i+α∑t′Ct′,d-i+TαThe Gibbs update contains two parts, one from the topic distribution and one from the word distribution. In (3),Ct,d-idenotes the number of topic t assignments in document d excepting the topic observation of the current word wiand∑t′Ct′,d-idenotes the total number of topics in d excepting the current observation. Similarly,Cw,t-irepresents the number of times that term w has been observed with topic t, and∑w′Cw′,t-irefers to all word counts of topic t assignments in the document collection. The current ith observation is excluded from the counts. In the estimation process, a topic for each word is randomly initialized to values between 1 and K at first and resampled iteratively from the distribution specified by (3) which is conditioned on topic assignments of other words except the current word. Consequently, the word-topic and document-topic counts change according to the sampled topics of words at each iteration and the masses of topic–word counts are propagated into document-topic probabilities.The basic topic model assumes the bag-of-words model where words are generated independently from one another [23]. The probability of a sequence of words is not affected by the word order.Therefore, the model can capture only rough contextual information such as the fact that related words tend to appear in thematically similar documents. However, word order or phrases are important to understand texts [26].Accordingly, the LDA topic model has been extended to bigram [24], n-gram [25], and collocation topic models [26] to capture word order or the rich semantic and syntactic structures of documents.For instance, the collocation topic model relaxes the bag-of-words assumption by incorporating bigram collocations-words that tend to follow one another with high frequency [26]. For this, an additional parameter x for the collocation status, to indicate whether a word forms a bigram collocation, is assigned to each word. If xiis 0, then word wiforms a unigram and the word is generated from the distribution associated with its topic assignment zi,p(wi|zi,xi=0) as the typical LDA model. Otherwise, wibecomes a part of a collocation with the previous word wi−1. At this time, the generation probability of widepends solely on its previous word and not on the topic p(wi|zi−1,xi=1) Fig. 1(b) visualizes the collocation topic model that considers additional dependencies, such as xi→wi, wi−1→wi, and wi−1→xi.In this section, we briefly describe the limitations of the collocation model and contributions that distinguish this work from others. In the collocation topic model, the collocation status for each word is parameterized only by its previous word [26]. For example, it is likely to be xi=1 if a previous word is “gene”. However, the assumption that bigram collocation is formed by occurrences of a previous word seems insufficient. In addition, if “expression” and “gene” form a collocation, then “expression” is generated solely on the basis that it follows “gene” and not on its topic. Thus, the model can predict some associations that are based on word order such as “united–kingdom,” “metric–system,” “main–street,” “stock–market,” “interstate–highway,” and “serial–number” rather than semantic context [26]. As a result, a discovered bigram collocation always has a collocation relation no matter what the nearby context is, as mentioned in the study of Wang et al. [25]. Only with the associations based on word order, it can fail to capture some meaningful semantic associations for the abbreviation disambiguation.Furthermore, the topics of two words that form a collocation can be different from one another because a topic of a current word is determined by the document-topic distribution regardless of the topic associated with its previous word, even though two words form a collocation. Another weakness is that collocation cannot be formed if two tokens are separated by other words because the model can consider only consecutive bigrams.In order to address the issues of previously developed topic models, we adopt a new notion of link. In this study, link can be interpreted as a contextual dependency between two words. For instance, when the word “acetaldehyde” appears, it is likely that a word such as “ethanol” will appear in the same context. We here try to manage contextual associations between words in terms of topic as well as co-occurrence frequencies. For instance, the word “blood” frequently appears with the word “acetaldehyde”. However, “blood” is not much effective for distinguishing senses of “ACE” because it is frequently encountered in the contexts of other senses such as “acetate,” “acetylcholinesterase,” or “acetone” besides “acetaldehyde”. Therefore, we restrict the linked words to share a similar topic. Namely, a word that is dependent on a specific long form is trained to have a similar topic distribution as that of the long form. In particular, the proposed model has a benefit that both skip bigrams (non-adjacent word pairs) and adjacent bigrams can be considered with the link parameter. The possibility of the presence of a link is restricted to the pairs of long-form words and their sentential co-occurring words instead of to all possible word pairs. Consequently, 435,943 pairs out of all possible 23,610,331 bigrams (approximately 1.85%) were considered.In the existing LDA topic models, commonly used words tend to dominate all topics [31]. For example, words such as “compound,” “comparison,” “investigate,” “evaluate,” “study,” and “results” appear in nearly all abstracts, but they are mostly irrelevant to the main topics of documents. These words appear across a wide range of topics. One advantage of link information is to lessen the influence of such non-topic words or unrelated words to a specific long form.Therefore, in the link topic model, the bag-of-words assumption can be relaxed by considering semantic dependencies between words represented with a link, despite words are generated independent of word order.In this section, we first describe the link topic model we propose, its generation process for each document, and procedures for parameter inferences within the model. We next explain the construction scheme of a data set, including annotation. We then detail the processes of grouping long term variants based on topic information and removing stop words from the data set. Finally, we examine how topic models including the link topic model predict the long forms on given target abbreviations in test abstracts. This is accomplished by the likelihood (probability) which the topic models assign to the words in held-out abstracts.The link model involves two word generation modes. The generation of a word is dependent on either a given long form word or on the topic distribution, regardless of the given long form. The link assignment for each word indicates whether the word is generated either from topical and contextual relatedness to a given long form word or independently. Thus, a word forms either a unigram or a skipping/consecutive bigram with a specific long form. Note that although a pair of words frequently co-occurs in some sentences or abstracts, the words may have no link relationship (dependency) in the link topic model. The link variable is parameterized by word topics as well as by the co-occurrences of two words. The existence of a link between two words, the x in Fig. 1(c), is inferred.As shown in Fig. 1(c), two latent structures, topiczand link statusxare responsible for generating an observed sequence of wordswin a document. L indicates the total numbers of long forms. The generative process can be defined as a probability distribution of words (w) by using two hidden structures,zandx. Two word generation processes can be described as follows:1.Draw discrete distributions ϕzfrom a Dirichlet prior β for each topic z.Draw binomial distributions ψw,lffrom a beta prior γ0,γ1 for each word w and a given long form lf.Draw discrete distributions σlffrom a Dirichlet prior δ with respect to lf.For each document d, draw a distribution of topics θdfrom a Dirichlet prior α.For each word wiin document d (and a given long form lfd).(a)Choose xifrom binomialψwi,lfd.Choose zifrom multinomialθdifxi=0;else choosezlfd.Choose wifrom multinomialϕziif xi=0; else choose wifrom multinomial σlfd.In this process, θidenotes the document-specific topic distribution, ϕzis the conditional probability of word wiin a given topic z, andψwi,lfdis the binomial distribution of a link status variable xi, which is conditioned on wiand a given long form word lfd. In Step 5 (a), xiindicates that a word wiis generated dependently (xi=1) or independently (xi=0) from lfd, and σifdrefers to the probability of a word wiconditioned on a given long form lfd.The key inferential problem to use the link topic model is that of computing the posterior probabilities of p(xi|wi) and p(zi|wi) for the topic and link assignments of each word. The probabilities are iteratively inferred by sampling each xiand zifrom the distribution specified by (4) and (5). As a result, the link value xiis assigned to each word based on the following.(4)p(xi=0|x-i,w,z,lfd)∝p(wi|xi,x-i,zi)×p(xi=0|x-i)=cwi,zi-i+β∑w′cw′,zi-i+Vβcxi=0,wi,lfd-i+γ0∑xcx,wi,lfd-i+γ0+γ1p(xi=1|x-i,w,z,lfd)∝p(wi|xi,x-i,lfd)×p(xi=1|x-i)=cwi,lfd-i+δ∑w′cw′,lfd-i+Vδcxi=1,wi,lfd-i+γ1∑xcx,wi,lfd-i+γ0+γ1In (4),Cxi=0,wi,lfdis the number of times that wiis generated independently of the given long form lfdandCwiziis the counts of widrawn from the sampled topic zi. In contrast,Cxi=1,wi,lfddenotes the counts that wiis generated depending on the given long form lfd. In addition,Cwi,lfdis the number of times that wioccurs with lfdin the same sentences or within the same abstract, on the condition that wiand lfdhave appeared together in the same sentence(s) within another abstract. The current ith observations are excluded from the counts of occurrences in documents, which is represented by −i. In general, although two words in the same abstract do not imply that they have a semantic association, we assume they can have a semantic association if they co-occur in the same sentence of another abstract. Thus, the proposed link topic model can retain a precise and effective parameter space since it does not consider links between all words. We here used our own sentence chunker to extract sentential co-occurring word pairs.Consequently, the probability that wiforms a link to lfdis determined by the sentential co-occurrence counts between wiand lfdand the counts of wigenerated based on lfd. Similarly, the probability that wihas no link to the long form is determined by the counts of widrawn from topic ziand the counts of wigenerated independently from lfdThe two probabilities are compared under the Gibbs sampling.In Step (5b), the posterior probability of ziwith respect to wiis drawn from the following probability:(5)p(zi|z-i,w,x,lfd)∝p(wi|zi,z-i,w-i,x,lfd)×p(zi|z-i,x,lfd)In here, wiis generated from the multinomial distributionϕziof zidrawn by Step (5b) if xiis 0 and from the multinomial distribution of topicϕzlfdif xiis 1. A long form word is always generated by its own topic distributionϕzlfd. Thus, the word-topic probability can be represented by the following equation, where V and T denote the vocabulary size and the number of topics, respectively.P(zi|wi,z-i,w-i,lfd,α,β)=Cwi,zi-i+β∑w′Cw′,zi-i+VβCzi,d-i+α∑z′Cz′,d-i+Kα(xi=0)(6)P(zi|wi,z-i,w-i,lfd,α,β)=cwi,zlfd-i+β∑w′Cw′,zlfd-i+Vβczlfd,d-i+α∑z′cz′,d-i+Kα(xi=1)In other words, a word is generated by the topic drawn from a document topic distribution such as the basic LDA if it has no link to a given long form. On the other hand, a word is generated by the long form’s topic drawn from a document topic distribution if the word is dependent on the long form. In our study, if several identical long-form words appear in an abstract, then the topic of the nearest long-form word to the current word is selected. As shown in (6), the topics of two words become the same if a semantic link exists between the two words. In other words, a word that is dependent on a specific long form is likely to have a similar topic distribution as that of the long form.Fig. 1(c) shows a graphical model representation of the link topic model. In this figure, the probability θ refers to the document-topic distribution, ϕ to the word-topic distribution, ψ to the link status distribution between a word and a specific long form, and σ to the bigram co-occurrence distribution. Each estimate can be computed as follows:ϕw|z=Cw,z+β∑w′Cw′,z+Vβ,θz|d=Cz,d+α∑z′Cz′,d+Kα(7)σw|lf=cw,lf+δ∑w′cw′,lf+δV,ψx|w,lf=cx,w,lf+γx∑x′cx′,w,lf+γ0+γ1The model is defined to capture the property that a word will form a semantic link to a specific long form if it: (1) frequently co-occurs with the long-form word in the same sentences/abstracts, and (2) has a similar topic distribution as that of the long-form word. For example, the links between the long form of “ACE”, “acetaldehyde” and “MPP+” will be counted because they often co-occur in some sentences or abstracts and share similar topics. Actually, both “acetaldehyde” and “MPP+” had high probabilities for the same topic category after the Gibbs sampling iterations. During the iterations, the link and topic counts keep changing to obtain a sequence of document observations based on Eqs. (4) and (6).Our model aims to determine the long form that is most likely to be generated with respect to a given document under inferred parameters of topic models when an abbreviation appears in a document. Based on the topic structure estimated by the basic LDA, we compute the long form word generation probabilities for the test abstract from the top 25 topics out of the document-topic probabilities. Given a test abstract d in which a target abbreviation appears, we predict the most probable long form word w∗ out of all possible long forms that maximizes the likelihood of the data with (9). We adopt Heinrich’s method to estimate parameters for the topic distribution of unseen (test) documents [32].(9)w∗=argmaxwi∈lfp(wi|d)=∑t∈Ttop25(d)p(wi|t)p(t|d)For instance, in order to disambiguate the abbreviation “ACE,” we compute the generation probabilities of its twelve possible long forms such as (aceton|d), p(angiotensin_converting_enzyme|d), and others. The generation probability depends only on topics, not on other words in the document.In contrast, in the link topic model, the generation probability of words in a specific document d is computed according to the top 25 topics as follows:p(w|lfd)=argmaxlfd∈lf∏wi∈d∑tj∈Ttop25p(wi|tj,lfd,xi=1)+p(wi|tj,lfd,xi=0)p(w|lfd)=argmaxlfd∈lf∑wi∈dlog∑tj∈Ttop25p(wi|tj,lfd,xi=1)+p(wi|tj,lfd,xi=0)(10)=argmaxlfd∈lf∑wi∈dlog∑tj∈Ttop25Cwi,tj+βC·,tj+βVCwi,lfd+αC·,lfd+KαCxi=1,wi,lfd+γ1C·,wi,lfd+γ0+γ1+Cwi,tj+βC·,tj+VβCtj,d+αC·,d+Kα1-Cxi=1,wi,lfd+γ1C·,wi,lfd+γ0+γ1In (10), p(wi|lfd,xi=1) denotes the probability that wiis generated dependently on lfd, whereas p(wi|lfd,xi=0) represents the probability that wiis generated independently from lfd. Therefore, p(w|dj,lfd) represents the likelihood of a sequence of wordswappearing in d. Consequently, the link topic model can consider all other words observed in a document as well as topic information. The most probable long form identified by the link topic model would be chosen for a given text by means of (10).Because no standardized dataset exists for biomedical abbreviation disambiguation, we compared our model with other research conducted on similar datasets [2,10,11]. We first used UMLS to gather abbreviations and their possible long forms as in other studies [2,10,11]. In contrast to general WSD, building a tagged corpus is relatively easy [2,11]. A corpus annotated by each abbreviation-long form pair can be automatically constructed using MEDLINE.5It comprises more than 21 million journal citations to biomedicine and life sciences publications (Approximately 60% of the MEDLINE citations have an English abstract).5In our study, a document collection consists of MEDLINE abstracts containing target abbreviations. Abstracts are retrieved by means of Entrez search using a query composed of a specific abbreviation and a possible long form. Other possible long forms are excluded from query terms with the Boolean operator “NOT.”For example, suppose that the abbreviation “CAT” has three possible long forms: “chloramphenicol acetyltransferase,” “catalase,” and “carboxyatractyloside.” In order to retrieve the abstracts containing “CAT” and its long form “catalase”, the query would be formed as: “CAT”[TIAB] “catalase”[TIAB] NOT “chloramphenicol acetyltransferase”[TIAB] NOT “carboxyatractyloside”[TIAB]. Accordingly, the abstracts that mention only the abbreviation “CAT” and its target long form “catalase” are retrieved.While the model learns the underlying topic and link structures from observed words in training abstracts, only the specific abbreviations being the disambiguation target are removed from the training data. On the contrary, long forms of target abbreviations are removed from the test data. In addition, words in a long form are concatenated by using underscores. In our experiments, we used a corpus of 73,505 abstracts (113MB) with 274,434 unique terms from MEDLINE regarding 22 three-letter abbreviations and their 189 long forms from UMLS.We first grouped long form term variants having an identical reference by using the LDA topic model. In other words, the long forms of an abbreviation that share very similar topics were grouped as variants. The possible topics for long forms are extracted by the word-topic probability ϕ as in Eq. (7). Table 1shows some long forms and their top-ranking topic IDs, in which the probabilities of ϕ are greater than 0.000004. The numbers in bold fonts shows the common topic IDs across similar long forms. According to Table 1, variants actually are similar to one another in terms of topics. The final variant grouping was done manually based on the probability of ϕ and the topics in common. Consequently, the data set was reduced to 21 three-letter abbreviations and 139 long forms after grouping the variants. In case of “EMG”, all senses were grouped into one. Table 2shows the list of target abbreviations and long forms that we considered.In addition, stop words were filtered out for performance improvement. In this study, besides PubMed stop words, commonly used domain-specific words were further removed from the data set by (8). For instance, words such as “activation,” “increase,” “reduce,” “induce,” and “measure” are frequently encountered in most biomedical abstracts. To eliminate such common words, the weightw,dstatistic, which is a modified tf-idf weighting scheme, was used to assess the contribution of the word w in distinguishing the document d from other documents in a collection. In the equation that follows, rtf corresponds to the relative term frequency, idf to the inverse document frequency, and max fwto the maximum frequency of the term w in any document.rtf=fw,dmaxfw,idf=log|D|fw,Dlog(|D|),(8)weightw,d=rtf∗idf

@&#CONCLUSIONS@&#
