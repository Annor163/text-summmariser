@&#MAIN-TITLE@&#
Swarm intelligence based techniques for digital filter design

@&#HIGHLIGHTS@&#
Two novel modifications to QPSO proposed.Time dependency of constriction factor taken care.Improved methods for IIR filter design proposed and validated.

@&#KEYPHRASES@&#
Digital filter design,Particle Swarm Optimization,IIR filter,

@&#ABSTRACT@&#
This paper deals with the problem of digital IIR filter design. Two novel modifications are proposed to Particle Swarm Optimization and validated through novel application for design of IIR filter. First modification is based on quantum mechanics and proved to yield a better performance. The second modification is to take care of time dependency character of the constriction factor. Extensive simulation results validate the superior performance of proposed algorithms.

@&#INTRODUCTION@&#
Studies by Fang et al. [1] conforms the findings by the researchers of [2–13]. This work can be treated as extension of the works done in [1]. For ease of reading, we have reproduced the work as first part of this article that paves a way for the next part of the article.The problem, design of IIR filters, error surface in most of the cases is non-quadratic and multi-modal [2]. In a multi-modal error surface, global minima can be achieved by a global optimization technique. Bio-inspired algorithm, Genetic Algorithm (GA) [2], and its hybrids [3–5] have been used for digital IIR filter design. Because of disadvantages of GA (i.e., lack of good local search ability and premature convergence) invited Simulated Annealing (SA) [6] for use in the digital IIR filter design. Again, standard SA is very slow and also requires repeated evaluations of cost function to converge to the global minima. Hence, Differential Evolution (DE) [7] finds a scope in digital IIR filter design. But, the problem with DE algorithm is that it is sensitive to the choice of its control parameters.Particle Swarm Optimization (PSO) [8,9] also used in the problem of digital IIR filter design. PSO can be easily implemented and also outperforms GA in many of the applications. But, as the particles in PSO only search in a finite sampling space, PSO may easily be trapped into local optima. One useful modification of PSO is Quantum-Behaved Particle Swarm Optimization (QPSO) [10–13]. QPSO uses the merits of quantum mechanics and PSO. The problem of digital IIR filter design is a minimization problem that can be solved by PSO and QPSO. In addition, the parameters of filter are real coded as a particle and then the swarm represents all the candidate solutions.In [1], Fang et al. proposed one modification to the standard version of QPSO. This modification, QPSO-M, is proposed by introducing a random vector in QPSO in order to enhance the randomness and global search ability. In this paper we propose a modification to PSO, where, the constriction factor is allowed to change in each of the iterations with a linearly decreasing K. Importance and utility of proposed modifications validated by novel application to design of IIR filter. Three examples for the purpose of filter design are tested and compared with PSO and QPSO.Rest part of the paper is organized as: problem of filter design is outlined in section ‘Problem statement’. PSO, QPSO and proposed modifications are discussed in section ‘Methodologies’ followed by simulation examples in section ‘Simulation results’. Finally, the paper is concluded in section ‘Conclusion’.In this paper, we have used the same system structure as that used in [1] and reproduced in this section. Here, the model of IIR filter is considered to be same as that of the autoregressive moving average (ARMA) model. The model can be defined by the difference equation [3]:(1)y(k)+∑i=1Mbiy(k−i)=∑i=0Laix(k−i)where x(k) represents input to the filter and y(k) represents output from the filter. M(≥L) is order of the filter, aiand biare the adjustable coefficients. The transfer function of this model takes following general form:(2)H(z)=A(z)1+B(z)=∑i=0Laiz−i1+∑i=1Mbiz−iThe problem of IIR filter design can be considered as an optimization problem, where, the mean square error (MSE) is taken as the cost function. Hence, the cost function is:(3)J(ω)=E[e2(k)]=E[{d(k)−y(k)}2]Here, d(k) is the desired response of the filter, e(k)=d(k)−y(k) is the error signal. Using two sets of coefficients,{ai}i=0Land{bi}i=1Mwe can get the composite weight vector for the filter. This is defined as:(4)ω=[a0,…,aL,b1,…,bM]TThe objective however is to minimize MSE (3). This is achieved through adjustment of ω. Because of difficulty in realizing the ensemble operation, the cost function (3) can be replaced by the time-averaged cost function:(5)J(ω)=1N∑k=1Ne2(k)Here, N represents the number of samples used.This section discusses on optimization algorithms used in this paper for the problem of digital IIR filter design.PSO [14,15] is a global search technique and based on social behavior of animals. In PSO, each particle is a potential solution to a problem. In M-dimensional space, particle position is defined as Xi=(xi1, …, xid, …, xiM) and velocity is defined asVi=(vi1,…,vid,…,viM). Particles can remember their own previous best position. The velocity and position updation rule for particle i at (k+1)th iteration uses the relations:(6)vijk+1=w⋅vijk+c1⋅r1jk⋅(Pijk−xijk)+c2⋅r2jk⋅(Pgjk−xijk);r1jk,r2jk∈U(0,1)xijk+1=xijk+vijk+1Here, c1 (cognitive) and c2 (social) are two positive constants that controls respectively the relative proportion of cognition and social interaction. Vector Pi=(pi1, …, pid, …, piM) is the previous best position (the position giving the best fitness value) of particle i, which is called pbest. And vector Pg=(pg1, …, pgd, …, pgM) is the best position discovered by the entire population, which is called gbest. Parameter w is the inertia weight and the optimal strategy to control it is to initially set to 0.9 and reduce it linearly to 0.4 [14]. QPSO is derived from fundamental theory of particle swarm having the merits of quantum mechanics. In the QPSO algorithm with M particles in D-dimensional space, the position of particle i at (k+1)th iteration is updated by:(7)xijk+1=pijk±α⋅MPjk−xijk⋅ln1uijk,uijk∈U(0,1)(8)pijk=φijk⋅Pijk+(1−φijk)⋅Pgjk,φijk∈U(0,1)(9)MPk=(MP1k,…,MPMk)=1M∑i=1MPi1k,…,1M∑i=1MPiDkHere, parameter α is called contraction–expansion (CE) coefficient. Piand Pghave the same meanings as those in PSO. MP is called Mean Best Position, which is defined as the mean of the pbest positions of all particles.Search space of QPSO is wider than that of PSO because of introduction of exponential distribution of positions. Also, unlike PSO, where each particle converges to the global best position independently, improvement has been introduced by inserting Mean Best Position in QPSO. Here, each particle cannot converge to the global best position without considering its colleagues because that the distance between the current position and MP determines the position distribution of the particle for the next iteration.This section reproduces the modification proposed in [1]. Though QPSO finds better global search ability than PSO, still may fall into premature convergence [16] similar to other evolutionary algorithms in multi-modal optimization like GA and PSO. This results in great loss of performance and yield sub-optimal solutions. In QPSO, in a larger search space, diversity loss of the whole population is also inevitable due to the collectiveness although. From Eq. (2), it is evident that whenMPj−xijis very small, the search space will be small and xijcannot go to a new position in the subsequent iterations. The explorative power of particles is also lost and the evolution process may become stagnate. This case even may occur at an early stage whenMPj−xijis zero. Also, in the latter stage, the loss of diversity forMPj−xijis a problem to be considered. In order to prevent this undesirable difficulty, we in this paper construct a random vector. This random vector will replaceMPj−xijwith a certain probability CR, according to the difference between two positional coordinates that are re-randomized in the problem space, and the value of the new position updating equation becomes:(10)δ→=mu_xk−mu_xs,xijk+1=pijk±α⋅δ→⋅ln1uijkwhere mu_xkand mu_xsare two random particles generated in the problem space. Because of introduction of the random vector, the particles may leave from the current position and may be located in a new search domain.The modified version of QPSO is termed here as QPSO-M. The procedure, used in this paper is outlined as:Step 1: Initialize particles with random position and set the control parameter CR.Step 2: For k=1 to maximum iteration, execute the following steps.Step 3: Calculate the mean best position MP among the particles.Step 4: For each particle, compute its fitness value f{xi(k)}. If f{xi(k)}<f{Pi(k)}, then Pi(k)=xi(k).Step 5: Select gbest position Pg(k) among particles.Step 6: Generate a random number, in the range [0,1].Step 7: If RN<CR then update the position according to (7)–(9), else according to (8) and (10).Recent works in [17,18] indicate that the use of a “constriction factor” may be necessary to insure convergence of the PSO. A simplified method of incorporating a constriction factor is illustrated in [17]. In [17], the performance of PSO using an inertia weight was compared with the PSO performance using a constriction factor. It was concluded that the best approach is to use a constriction factor while limiting the maximum velocityvmaxto the dynamic range of the variable xmax in each dimension. It was also shown in [17] that this approach provides a performance superior to any similar technique reported in the literature.Constriction factor-based PSO [9] motivated this work to propose a new modification. This is achieved by introducing a time-dependent and linearly-decreasing K, unlike that of a fixed K. Here, we adjust k at each of the iterations following the recursion:kn=kmin+(kmax−kmin)m−nm−1Here, m and n respectively represents the maximum number of iterations and the current iteration. Note here that the use of a constriction factor ensures computational stability of the PSO algorithm. This results in a stabilizing effect on the swarm and which therefore calls for the use of a lower value of the constriction factor.Performance of above mentioned methodologies were studied through simulations. With QPSO as reference, simulations were carried for three different cases. One hundred Monte Carlo simulations were undertaken for each case. Simulation parameters illustrated in Table 1.In this case, i.e., when the second-order system and first-order IIR filter, the problem is of falling to local minima. To test the effectiveness of proposed algorithms, we have chosen a plant same as that discussed in [6], having numerator coefficients {0.05,−0.4} and denominator coefficients {1,−1.1314,0.25}. The objective is now to find coefficients “a” and “b” of 1st order filter H(z)=a/(1+bz−1). A random Gaussian noise with zero mean and unit variance was chosen as system input, x(k). The error surface has a global minimum at {a, b}={−0.312, −0.816} and a local minimum at {a, b}={0.117, 0.576}. For all the five algorithms, the search space chosen was (±1). The fixed initial positions are chosen randomly as: {0.117, 0.576}, {0.8, 0} and {{0.9,−0.9}} [6]. A comparison among various algorithms in hitting the global and local minimum is outlined in Table 2. We found these results by 100 random simulations and with randomly chosen initial positions. It is evident from the table that PSO and QPSO jump to the global minimum valley in more number of cases and converge to the global minimum, but they may also jump to the local minimum and then converge to the local minimum. However, QPSO-M and PSO-LD have ability to converge in the global minimum in almost all the cases.In Tables 3 and 4, we demonstrate the mean values of filter coefficients along with the standard deviations along with simulation run time for each of the algorithms. It is evident from Table 3 that QPSO-M could find the global minimum with the least standard deviations as compared to other four algorithms. It is evident from Table 4 that QPSO-M and PSO-LD can jump out of any of the settled fixed initial positions and find the global minimum while the other algorithms are all trapped in these fixed initial algorithms. Fig. 1illustrates the coefficients learning curves and convergence behaviors of the algorithms for this case of design. The results shown are averaged over 100 random runs with randomly chosen initial positions.When the plant is a third-order system and filter is a second-order IIR filter, the error surface of the cost function becomes multi-modal because of reduced order filter. We have chosen the plant having numerator coefficients {−0.3, −0.4, −0.5} and denominator coefficients {1, −1.24, 0.5, −0.1}. The problem now reduces to that of finding coefficients {a0, a1, b1, b2} of the filter with transfer function H(z)=(a0+a1z−1)/(1+b1z−1+b2z−2).A uniformly distributed white sequence in the range (±0.5) is taken as the input x(k), and the SNR was kept fixed at 30dB.Results by various algorithms in case 2 are illustrated in Table 5. Results provide the mean best values and standard deviations of the filter coefficients. These results are obtained by averaging over 100 random runs with randomly chosen initial positions like that of case 1. Fig. 2(a) depicts the convergence behaviors for case 2 for different algorithms. It is evident from Table 5 that, mean best values produced by PSO, QPSO and QPSO-M are approximate, but by PSO-LD is exact. Convergence speed of QPSO and QPSO-M is faster than the other three algorithms as seen from Fig. 2(a).When the plant is a third-order system and filter is a third-order IIR filter, the error surface of the cost function becomes uni-modal because of same order filter. We have chosen the plant having numerator coefficients {−0.2, 0.4, −0.5} and denominator coefficients {1, −0.6, 0.25, −0.2}. The problem now reduces to that of finding coefficients {a0, a1, a2, b1, b2, b3} of the filter with transfer function H(z)=(a0+a1z−1+a2z−2)/(1+b1z−1+b2z−2+b3z−3).We have chosen same input as that of case 1. The best solution is to be located at {−0.2, 0.4, −0.5, −0.6, 0.25, −0.2}. The mean best values and standard deviations of filter coefficients in case 3 are provided in Table 6that gives results averaged over 100 random runs with randomly initial positions. Convergence behaviors for case 3 averaged over 100 random runs are produced in Fig. 2(b). As evident from Table 6, the filter coefficients found by QPSO are exactly located at the best solution and the standard deviation is smaller than that yielded by any other algorithms. QPSO-M and PSO-LD are the most and the second most robust algorithms as compared to two other algorithms. It is seen in Fig. 2(b) that the convergence speeds of QPSO, QPSO-M and PSO-LD are much faster than those of PSO. Computation time using MATLAB for different strategies for three examples outlined in Table 2. It is seen that PSO and QPSO are faster to solve than others, but approximating a problem, as QPSO-M and PSO-LD generally requires more constraints and additional variables, which increases computation time.From the above three examples, QPSO, QPSO-M and PSO-LD have shown their stronger search abilities both on the multi-modal problem and on the uni-modal one. QPSO, QPSO-M and PSO-LD outperform PSO in convergence speed, robustness and qualitatively of the final solutions [19].

@&#CONCLUSIONS@&#
