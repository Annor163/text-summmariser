@&#MAIN-TITLE@&#
Rigorously modeling self-stabilizing fault-tolerant circuits: An ultra-robust clocking scheme for systems-on-chip

@&#HIGHLIGHTS@&#
We introduce a novel modeling framework for fault-tolerant VLSI circuits.We cast a self-stabilizing clocking scheme from a companion article in this model.We discuss the implications of theory and model for the resulting implementation.We present the measures taken to avoid metastable upsets despite faults.We provide experimental data from a prototype FPGA implementation of the algorithm.

@&#KEYPHRASES@&#
Modeling framework,Clock synchronization,Hardware implementation,Experiments,Metastability,Dependability,Theoretical analysis,Hybrid state machines,Byzantine fault-tolerance,Self-stabilization,

@&#ABSTRACT@&#
We present the first implementation of a distributed clock generation scheme for Systems-on-Chip that recovers from an unbounded number of arbitrary transient faults despite a large number of arbitrary permanent faults. We devise self-stabilizing hardware building blocks and a hybrid synchronous/asynchronous state machine enabling metastability-free transitions of the algorithm's states. We provide a comprehensive modeling approach that permits to prove, given correctness of the constructed low-level building blocks, the high-level properties of the synchronization algorithm (which have been established in a more abstract model). We believe this approach to be of interest in its own right, since this is the first technique permitting to mathematically verify, at manageable complexity, high-level properties of a fault-prone system in terms of its very basic components. We evaluate a prototype implementation, which has been designed in VHDL, using the Petrify tool in conjunction with some extensions, and synthesized for an Altera Cyclone FPGA.In the past, computers have essentially been viewed as monolithic, synchronous, fault-free systems. If at all, fault-tolerance has been introduced (i) to deal with limited, specific failures (e.g. errors in communication or data read from storage, which are usually handled via error-correcting codes), and (ii) at the level of distributed systems comprised of multiple machines that are fault-prone or subject to attacks (e.g. data centers or peer-to-peer applications, which use some form of replication). Except for critical systems and extreme operational conditions (e.g. medical or aerospace applications [1]), there has been little motivation to build systems that are robust on all levels from scratch, a process that involves redesigning—or even reinventing—the very basics of how computations are organized and performed.Due to the tremendous advances of Very Large Scale Integration (VLSI) technology, this situation has changed. Enabled by ever decreasing feature sizes and supply voltages, modern circuits nowadays accommodate billions of transistors running at GHz speeds [2]. As a consequence, the assumption of chip-global (not to speak of system-global) synchrony [3] and no (or restricted) faults gradually became outdated [4]. Improved process technology and architectural-level fault-tolerance measures are common nowadays, and the lack of global synchrony has been tackled by accepting a certain level of asynchrony between different parts of the system.In the most extreme form of this approach, computations are completely unsynchronized at all levels [5], which requires to synchronize all dependent activities (like sending and receiving of data) explicitly via handshaking. In contrast, Globally Asynchronous Locally Synchronous (GALS) systems [6] make use of local clock sources to drive synchronous computations within each clock domain. Note that, in the wider sense, most multiprocessors fall into this category, as there is usually no single common clock that drives all processors. GALS systems again can be divided into two general classes: One that operates asynchronously at the inter-domain level, and the other consisting of multi-synchronous systems [7,8] that provide some, albeit reduced, degree of synchronization among clock domains. The former class suffers from the drawback that, for inter-domain communication, either strong synchronizers or stoppable clocks must be foreseen [9]. After all, every bit of the sender's data must have stabilized at the receiver before the clock edge used for reading the data occurs. This is avoided in multi-synchronous systems, where high-speed inter-domain communication via FIFO buffers can be implemented due to the available global synchronization [10]. Since the latter abstraction is also very useful for other purposes, multi-synchronous GALS is preferable from the viewpoint of a system-level designer.Naturally, establishing inter-domain synchronization comes at additional costs. While it is not too difficult to achieve and maintain in the absence of faults [11,12], the issue becomes highly challenging once faults of clocking system components enter the picture.We present an FPGA prototype implementation of a distributed clock generation scheme for SoC that self-stabilizes in the presence of up tof<n/3faulty nodes. It incorporates the pulse algorithm from [13] that tolerates arbitrary clock drifts and allows for deterministic recovery and (re)joining in constant time ifn−fnodes are synchronized; it stabilizes within timeO(n)with probability1−2−nfrom any arbitrary state. An additional algorithmic layer that interacts weakly with the former provides bounded high-frequency clocks atop of it. Nodes executing the compound algorithm broadcast a mere constant number of bits in constant time. The formal proofs of the properties of the pulse synchronization algorithm and the derived high-frequency clocks are given in [13].Deriving an implementation from the specification of the algorithm in [13] proved to be challenging, as the high-level theoretical model and formulation of the algorithm in [13] abstracts away many details. Firstly, it assumes a number of basic self-stabilizing modules above the level of gates and wires to be given. We devise and discuss self-stabilizing implementations of these building blocks meeting the specifications required by the high-level algorithm. Secondly, the algorithm's description is in terms of state machines performing transitions that are non-trivial in the sense that they do not consist of switching a single binary signal or memory bit. This requires careful consideration of metastability issues, since these state transitions are triggered by information from different clock domains. In order to resolve this issue, we introduce a generic Hybrid State Transition Machine (HSTM) that asynchronously starts a local synchronous execution of a state transition satisfying the model specification from [13]. Related to this matter, we thirdly discuss in detail how the algorithm and its implementation make a best effort to guard against metastable upsets. Here, we try to get the best out of the design decisions and rely on synchronizers only where absolutely necessary.These non-trivial implementation issues and the complex interactions between the basic building blocks raise the question under which circumstances the high-level properties of the algorithm shown in [13] indeed hold for the presented implementation. To answer this question, we devised a model that is able to capture the behavior of the constructed modules, including faults, resilience to faults, and self-stabilization, in a hierarchical fashion. By specifying the desired behavior of modules in terms of the feasible output generated in response to their inputs, we can also reason about the behavior of (implementations of) modules in a hierarchical manner. This property is crucial, as it permits to determine conditions under which our implementation indeed satisfies the requirements by the abstract model used in [13], and then soundly conclude that if these conditions are met, all statements made in [13] apply to our implementation. Since our approach is highly generic and permits to adjust the granularity of the description in order to focus on specific aspects of the system, we believe it to be of general and independent interest in the context of devising fault-tolerant systems.In order to verify the predictions from theory,33Or, to be scientifically accurate, we rather successfully failed at falsifying them. Our implementation primarily serves as a proof of concept, as clearly an FPGA implementation can merely hint at the properties of an ASIC.we carried out several experiments incorporating drifting clocks, varying delays, and both transient and permanent faults. This necessitated the development of a testbed that can be efficiently controlled and set up for executing a large number of test runs quickly. In our 8-node prototype implementation, the compound algorithm generates 8-bit clocks that in all runs stabilized within1.9⋅106dtime (where d is the maximal end-to-end communication delay). In our testbed, which runs at roughly 100 kHz, this amounts to less than 12 s. For a system running at GHz speed, this translates to about a millisecond. We also observed that the deterministic stabilization mechanism designed for more benign conditions operates as expected, recovering nodes by about two orders of magnitude faster.In the next section, we summarize the obstacles and design goals that need to be considered for clock synchronization in our setting; we also introduce the basic building blocks assumed in [13], which perform typical operations used by fault-tolerant synchronization algorithms. Section 3 introduces the formal model, alongside illustrating examples and proofs of some basic properties. Subsequently, in Section 4 we cast the modules informally discussed earlier in our formal framework, and interpret nodes, protocols, and the synchronization problem as modules as well. In Section 5, we move on to the description of the algorithm from [13] in terms of this framework. We provide high-level intution on the purpose of its various components and summarize the main statements proved in [13]. Section 6 follows up with presenting our implementations of the basic modules specified in Section 2.2, including the HSTM. In this context, we will also cover our efforts to minimize the probability for metastable upsets. In Section 7 we describe the testbed setup, the experiments, and their results. Finally, in Section 8 we evaluate to what extent our design goals are met and give an outlook on future work.Our goal is to design a scalable hardware clock generation scheme that is resilient to arbitrary transient and permanent faults and carefully minimizes the risk of metastability. We will now discuss our objectives in more detail and explain why tackling them in conjunction proves to be much harder than achieving them individually.In accordance with standard notions, in the following we will refer to clock domains as nodes, as they represent the smallest “independent” algorithmic building block we use. This is to be understood in the sense that we consider a node faulty if any one of its components is faulty, and non-faulty otherwise (irrespectively of whether other nodes behave correctly or not). Denoting by[i..j]the set{k∈N|i⩽k⩽j}, ultimately, each correct nodei∈[1..n]must at all times t output a (discrete) logical clockLi(t)∈Nthat fulfills certain properties despite the aforementioned obstacles; most obviously, we strive for minimizingmaxi,j∈[1..n],t⩾0{Li(t)−Lj(t)}.When synchronizing clocks, one needs to face that clocks are not perfect and that it cannot be exactly determined how much time it takes to communicate a clock reading. These fundamental uncertainties entail that synchronization can never be perfectly accurate and must be an ongoing process [14]. We formalize these notions as follows.Each nodei∈[1..n]can make use of local clocks that are inexact and therefore drift (i.e., do not progress at the same rate). Since we are only concerned with synchronizing clock domains with each other, we do not care about Newtonian time. Instead, we describe the system in terms of a reference time satisfying that any correctly operating clock progresses at a speed between 1 and some constant ϑ with respect to the reference timet∈R. A (local) clockC:R→Rthat is correct during a period of reference time[t−,t+]⊆Rguarantees that ∀t,t′∈[t−,t+],t<t′:t′−t⩽C(t′)−C(t)⩽ϑ(t′−t)(in particular, C is continuous and strictly increasing during[t−,t+]).44We use real-valued, unbounded clocks here to simplify the presentation. It will later become clear that the algorithm can indeed operate with discrete bounded clocks, as it does not need to access absolute clock values, but rather approximately measures bounded differences in time.In contrast to many “traditional” synchronization settings, we would like to tolerate quite large relative clock driftsϑ−1of up to about 20%, as accurate and stable oscillators are not available in a System-on-Chip (SoC) at low costs. Tolerating such large drifts permits to utilize very simple ring oscillators even under heavily varying conditions (temperature, supply voltage, etc.) [15].Node i communicates with node j via an abstract FIFO channel that (if correct) continuously makes i's state available to j, albeit delayed by an unknown value between 0 and the maximal delay d. We denote the input port of the channel from node i to node j bySiand its output port bySj,i. Node i also loops back its own state to itself on a channel. The time required for computations that are triggered by some communicated information is accounted for by d as well, i.e., d is an end-to-end delay.55This is the reason why we speak of an abstract channel. We will later introduce the (physical) channels that essentially represent the wires on the chip; the maximal delay d is then the sum of the maximal delay of the physical channels and the computing elements of the nodes.For the sake of a straightforward presentation, throughout this article we assume that all channels from node i to some node j are part of node i, i.e., faults of the channel are mapped to the sender node. We remark, however, that a more detailed treatment (as e.g. in [16]) can be beneficial and is supported by the modeling framework underlying this work.Increasing soft error rates of modern VLSI circuits [17], originating in ionizing radiation [18–21], cross-talk, and ground bouncing [22,23], make it vital to allow for recovery from transient faults. The most extreme transient fault scenario is that the entire system undergoes a period of an unbounded number of arbitrary faults.66The only restriction is that transient faults do not affect the non-volatile memory (and in particular not the algorithm itself), as this would induce a permanent fault.Algorithms that are capable of re-establishing regular operation after transient faults cease are called self-stabilizing[24]. This requirement is equivalent to stating that, if the system is fault-free, the algorithm converges to a valid state from an arbitrary initial configuration within a bounded time; we refer to this period as stabilization time. Due to this equivalency, self-stabilizing algorithms have the additional advantage of requiring no initialization, i.e., a self-stabilizing clocking system does not need to be booted with any initial synchrony.For self-stabilizing algorithms, stabilization time is obviously an important quality measure. As the fundamental time unit of the system is d, i.e., the time span it takes to effectively communicate and process any piece of information with certainty, guarantees on the stabilization time are clearly always some multiple of d; the respective prefactor typically is a function of the number of nodes n, the number of sustainable or actual permanent faults, and the clock drift ϑ. In our context, the stabilization time is not only of relevance to whether waiting for stabilization is bearable in terms of the down-time of the system; it is important to understand that a failure of the synchronization layer will quickly result in incoherencies of operations on higher layers, entailing the threat of data loss or corruption, potentially without any possibility of future recovery.Because of the need of maintaining accurate synchronization in the presence of drifting clocks, quite a few clock synchronization algorithms are self-stabilizing. In fact, conventional clock trees [3] are trivially self-stabilizing—after all, they simply disseminate the signal of a single oscillator throughout a chip. However, they cannot cope with any permanent fault of the clock source or the network distributing the clock. Similarly, one could easily make a system comprising several clock sources self-stabilizing, by picking one master clock and letting all other clocks synchronize to it. Again, this simplistic approach will fail if the master or its outgoing communication channels become faulty.Sustaining functionality in the presence of permanent faults necessitates redundancy. More precisely, it is known that tolerating f worst-case faults (traditionally called Byzantine faults in this context) is impossible ifn⩽3f(without cryptographic assumptions) [14,25].77Allowing cryptography would still necessitaten>2f[26,27]; we hence discard this option due to the additional complexity incurred.Hence, natural questions are whether assuming worst-case failures is too demanding and whether the fault model could be relaxed in order to circumvent the lower bound. Unfortunately, examining the lower bound reveals that it originates in the ability of a faulty node to communicate conflicting information to different receivers. This behavior can easily emerge from a faulty output stage in a circuit: If an analog voltage level in between the range for a valid “1” and that for a valid “0” is evaluated (for example due to a timing fault, a glitch on a signal line, or a defective driver output) by more than one receiver, some might read a “1” while others read a “0”. Note that this is a fundamental problem, as mapping the continuous range of possible voltages to discrete binary values entails that there is always a critical threshold close to which it is impossible to ensure that all receivers observe the same binary value. It is still an option to argue about the spatial distribution of (permanent) faults within the system, though, as we discuss in Section 8. However, in this article, we consider the worst case, which also motivates the choice of full connectivity88We are aware that this constitutes a serious scalability issue; again we refer to the discussion in Section 8.between the nodes due to a respective impossibility result [26,27].This lower bound entails that, due to their low connectivity, most existing distributed clock generation schemes [11,12,28,29] cannot cope with a reasonable number of worst-case faults. Nonetheless, dealing with up to f faults in a fully connected system ofn⩾3f+1nodes is—at least from a high-level perspective—still fairly easy, provided that we can rely on synchronization already being established. To illustrate this, consider the simple state machine of a node given in Fig. 1.In the figure, the node's states are depicted in circles and the feasible state transitions are indicated by arrows. A node switches, for example, from state ready to state propose if the condition next to the arrow is satisfied. In this example, this means that either3ϑ2dtime has passed on its local clock since it switched to state ready or its incoming channels (including its loop-back channel) showed at leastf+1other nodes in state propose since it switched to state ready. This behavior is realized by each nodei∈[1..n]having (binary) memory flagsproposei,jfor each nodej∈[1..n]: Node i's flagproposei,jis set to 1 at a time t iffSi,j(t)=proposeand the flag was in state 0 before. The flag is reset to 0 on node i's state transition to ready (in the figure indicated by the rectangular box on the respective arrow). Deciding whether the transition condition is satisfied at time t thus boils down to checking whether the timeout condition is satisfied or at leastf+1of the propose memory flags are in state 1.Now assume that each node runs a copy of this state machine, and at leastn−fnon-faulty nodes enter state increase during some time window[t,t+2d). As local clocks run at speeds between 1 and ϑ, all nodes will switch to state ready during[t+3d,t+2d+3ϑd). Hence, at the time when a node switches to ready, the delayed state information on the channels will not show non-faulty nodes in state propose any more. Therefore, no non-faulty node will switch to propose again due to memorizingf+1nodes (at least one of which must be non-faulty) in state propose before the first non-faulty node switches to propose. Thus, the latter must happen because3ϑ2dlocal time passed on a local clock, which takes at least until timet+3d+3ϑd>t+2d+3ϑd. By this time, all nodes will have switched to ready. This implies that at the time when the first node switches to increase again (which eventually happens because alln−fnon-faulty nodes switch to propose), all nodes will already have switched to ready. Given thatn⩾3f+1, we have thatn−2f⩾f+1, i.e., if at some non-faulty noden−fchannels show state propose, any node will observef+1channels in this state (though due to delayed communication maybe not at exactly the same instance in time). This implies that at most d time after the first node switched to increase again, all non-faulty nodes have switched to propose. Another d time later, alln−fnon-faulty nodes will have become aware of this and have switched to increase, i.e., within a time window of 2d. Repeating this reasoning inductively and assuming that the nodes increase their logical clocks (that initially are 0) by 1 whenever they switch to increase, well-synchronized logical clocks are obtained: The maximum difference in time between any two correct nodes performing their kth clock tick, the skew, is at most 2d for the above algorithm.A variation of this simple technique [30] is known for long and a closely related approach called DARTS has been implemented in hardware [31,32]. However, all these algorithms are not self-stabilizing. In fact, even if clocks would not drift, the delay d was arbitrarily small, and there was only a single faulty node (i.e., even if we allow forf>1, only one node is actually faulty), they still would not stabilize.To see this for the algorithm given in Fig. 1, first consider the following execution withn=3f+1, part of which is depicted in Fig. 2. The correct nodes are split evenly, into three subsetsAi,i∈{1,2,3}, of size f. SetA1initially is in state ready, with all memory flags corresponding to nodes inA3in state 1 and all other flags in state 0. The nodes inA2andA3are in state increase, with the timers of nodes inA2having progressed halfway towards expiring and the timers inA3just started (i.e., these nodes just left propose), and their propose signals are memorized by nodes inA1. Just when the nodes inA2are about to switch to ready, the faulty node sends propose signals to the nodes inA1, causing them to switch to propose. They will send propose signals, once receiving them memorize2f+1=n−fnodes in state propose, and thus proceed to state increase. However, the nodes inA2will still observe the propose signals of the nodes inA1after resetting their memory flags upon switching to ready. Thus, we end up in the same situation, except thatAi(indices modulo 3) takes the role ofAi−1. Repetition yields an execution that never stabilizes and has 3 sets of grossly desynchronized nodes that are not faulty. This execution can be generalized ton=kf+1for integersk⩾3: we split the correct nodes in k sets of size f and make them proceed equidistantly spread in time through the cycle. The difference is that now more than one group will linger in states ready or propose upon arrival of the next; the crucial point is that the single faulty node retains control over when groups proceed to state increase. The casesn=kf+2andn=kf+3require more involved constructions; it should be intuitive, though, that with 2 actually failing nodes the above construction can be modified to operate with one or two of the sets containingf+1nodes.Combining self-stabilization and resilience to permanent faults results in much more robust systems. Both properties synergize in that, as long as at all times there is some sufficiently large set (not necessarily the same!) of nodes that is non-faulty, an arbitrary number of transient faults is transparently masked, i.e., the system remains operational even though over time each individual component may repeatedly undergo transient failures and recover from them. This drastically increases the mean time until overall system failure: In a system that is not resilient to permanent faults, any fault will result in an immediate breakdown of guaranteed properties, whereas a system that is not self-stabilizing will fail (and might not recover without an external reboot) once the sum of faults exceeds one third of the nodes.99One could compromise by guaranteeing that nodes recover in bounded time, provided that the number of faults is never overwhelming. In fact, the algorithm presented in this article has the property that in this case nodes will recover faster and deterministically (in contrast to the slower, probabilistic stabilization from arbitrary system states). However, sacrificing stabilization from arbitrary states will not reduce the complexity of the algorithm significantly, and theory strongly indicates that the respective gain is limited to a constant factor in general.There is a considerable body of work on distributed synchronization algorithms that are self-stabilizing as well as resilient to permanent faults. However, until recently, there has been no solution worth considering for hardware implementation. Known algorithms exhibit a prohibitively large communication complexity (i.e., nodes sendΩ(n)bits over each channel in constant time) [33,34], incur an exponential stabilization time [35], require exponentially small clock drifts [36], or require much stronger assumptions on the system's behavior [37]. Recently, we proposed an approach that does not suffer from such drawbacks [13,38,39], whose implementation is the subject of this work.In our specific setting, minimizing the potential for metastability is particularly demanding. Metastability results from violating a stateful circuit's input timing constraints, e.g., by changing the data input of a flip-flop at the time of the clock transition. While this can be safely avoided during normal operation, a faulty node might exhibit arbitrary timing and hence cause such a violation. As this can never be prevented in the first place if worst-case faults are considered, it is mandatory to guard the channels against propagating metastability, e.g. by using synchronizers. In order to minimize the required length of synchronizer chains, decreasing latency and area consumption (the latter also on higher layers of the system), however, it is beneficial to avoid the potential for upsets by construction wherever possible.Apart from the (unavoidable) threat originating from faulty nodes, safely preventing timing violations is hindered by the lack of a common time base during the stabilization phase after an excessive number of transient faults. It has been shown that it is impossible to guarantee with certainty that no metastable upsets occur if the system is in an arbitrary initial state, even if all nodes adhere to the protocol [40]. Careful design is thus required in order to minimize the probability of upsets during stabilization, in particular since such upsets might obstruct the stabilization process.Once the system stabilized, i.e., the non-faulty nodes are synchronized, the algorithm can use this synchronization to structure communication in a way that entirely avoids metastable upsets caused by non-faulty nodes. Thus, in the absence of faults, we require that the system operates metastability-free. Note that even this seemingly simple task is not trivial, as one cannot employ the classical wait-for-all paradigm: Doing so would imply that just a single non-responsive node would cause the entire system to deadlock. Therefore, when depending on other nodes in the decision to take a state transition, it is necessary to wait for at mostn−fsignals. Safely reading signals thus cannot rely on handshaking, but must be based on suitable monotonicity and/or timing conditions (guaranteed by the use of memory flags and local clocks, for example). The bounded-delay “interlocking condition” used in DARTS [32] and the simple algorithm in Fig. 1 are showcases for such techniques.In order to be practical, the logical clocks need to run at a frequency in the GHz range. While one could obviously utilize frequency multiplication to achieve this goal, this is not straightforward to build in the self-stabilizing context. After all, clock multipliers involve complex devices like phase-locked loops and are hence not obviously self-stabilizing. Moreover, for a fixed guaranteed skew (of say 2d), naive frequency amplification also increases the logical clock imprecisionmaxi,j∈[1..n],t⩾0{Li(t)−Lj(t)}by the scaling factor, which may adversely affect certain services. For example, the size of the FIFO buffers used for inter-domain communication in [10] depends on the clock imprecision and must hence be adapted accordingly. On the other hand, by dividing frequencies, it is clearly possible to guarantee thatmaxi,j∈[1..n],t⩾0{Li(t)−Lj(t)}=1. Therefore, it is an important design goal to minimize clock imprecision while at the same time maximizing the frequency at which clocks run. Naturally, this becomes much more involved due to the design goals already presented.Being able to meet all the above design goals is meaningless if one cannot control the amount of resources devoted to the task of clock generation. Pivotal issues are the following:•Area consumption: The chip area used by the components of the synchronization algorithm decomposes into the area consumed by the nodes and their interconnections. The former can be captured by the gate complexity, i.e., the number of (constant fan-in) gates required to perform the algorithm's computations. The latter significantly depends on the chip layout, which is highly application-dependent and hence outside our scope of control. It is clear, however, that the number of channels and their bandwidth play a crucial role.Communication complexity: Apart from whether two nodes are connected or not, it is of interest how many wires are required. This is well-represented by the bit complexity of an algorithm, i.e., the number of bits it exchanges per time unit between communication partners. Note that while the number of wires can be reduced by means of time division, this will require additional memory and computational resources on the receiver's side and increase the communication delay. In any case, it is highly desirable to devise algorithms of (small) constant bit complexity. Moreover, broadcasting the same information to all nodes instead of different information to different receivers is to be preferred, as it allows us to use communication buses.Stabilization time: For reasons stated earlier, we would like to minimize the stabilization time. In particular, it is not good enough to know that an algorithm eventually stabilizes, as the required time might be well above what makes the algorithm self-stabilizing in any practical sense.Resilience: The number f of faults that can be concurrently sustained without losing synchronization or the capability to stabilize should grow with system size, as otherwise a larger system will suffer from more frequent outages. Note that while we must accept that stabilization is a random process (due to the unavoidable probability of metastable upsets), we demand that a system that is stable will always remain so as long as there are not too many faults (including upsets). As mentioned earlier, a lower bound shows that alwaysf<n/3, giving a precise meaning to “too many” here.Delays: As the maximal delay d accounts both for the delay incurred by communication as well as computation, it is vital to minimize both. Notwithstanding the fact that the communication delay and computing speed is mostly determined by parameters outside our control (technology, spatial distances, number of nodes, etc.), minimizing the gate complexity and, in particular, the depth of the circuits implementing the nodes' algorithms (that determine the computing delays) is important.Metastability: In larger and faster systems, the number of events per time unit that could cause metastable upsets is obviously larger. Therefore, it is vital to safely exclude metastability from occurring during regular operation by construction.1010Note that in this regard our approach is superior to standard GALS systems using synchronizers, where the risk of metastability is immanent (at every clock transition) also in normal operation.We admit metastability only during rare exceptional phases of system operation where it cannot be avoided in principle, like during stabilization or in case of faults. As the probabilities for metastable upsets are hard to quantify even in a final product, we do not use a “hard” measure here.1111It is worth mentioning, though, that the asymptotic increase in the number of events per time unit that could cause metastable upsets is clearly at least polynomial in n. As synchronizer chains decrease the probability of upsets exponentially, the required length of synchronizer chains will asymptotically grow asΩ(logn), increasing the system cost and (effectively) decreasing its operational frequency.Connectivity: In order to facilitate efficient placement and routing on a chip, it is vital to ensure that the communication network is sparse. Also, a sparse network will consume less area and is beneficial to fault containment.1212If a single event such as e.g. an ionizing particle hit can render multiple nodes faulty, even tolerating a large number of faulty nodes is of little use w.r.t. the overall resilience of the system.Tackling this issue is subject to our future work and hence beyond the scope of this article, however.Clock size: If the logical clocks have too few bits, i.e., overflow too frequently, they might be unsuitable for the application logic of the SoC. The algorithm we present in this article can in principle provide clocks of arbitrary bounded size. However, its stabilization time would grow linearly with the maximum clock value once we scale above 8-bit clocks. In a recent publication, we show how to construct larger clocks efficiently [41].We next introduce the basic modules that are assumed by the model used in [13]. We first give an intuitive description of the required modules. Subsequently, we introduce a novel formal framework for specifying self-stabilizing fault-tolerant modules and specify our basic modules in this framework. Any implementation satisfying this specification can be plugged into the high-level algorithm in order to yield a system guaranteeing the properties proved in [13].We now list the building blocks beyond standard logic gates that will explicitly or implicitly be used by the algorithm presented in Section 5. Each of these building blocks computes output signals that are constrained by (the history of) its input signals. If the logic function implies an output transition in reaction to an input change, this transition is not required to occur immediately; it must occur within a known time bound, however. Given the time bounds for the individual modules and the connecting wires, one can compute the maximum delay d. Moreover, informally speaking, it must be avoided that a single change in the input(s) causes multiple transitions of the output signal, as this could undermine the high-level algorithm's logic. Note also that statefulness, i.e., any sort of memory (including positive feedback loops), bears the potential for metastable upsets and requires careful attention in order to ensure self-stabilization.1313In other words, the module must recover from arbitrary corruptions of its memory.Purely combinational elements, on the other hand, differ in their ability to prevent metastable inputs from reaching the output under certain conditions.Each node will be a union of state machines that communicate via channels (both among each other and with remote nodes) and are composed of standard logic gates and all other modules we describe below. Fig. 5 depicts such a state machine.•Communication channels. We previously introduced the communication channels from node i to node j as abstract devices that convey the states with a delay of at most d that also accounts for computations. Viewed as a module, the (physical) communication channels do account for the time to communicate the state information only, whereas computations are performed by standard logic gates and the modules we will describe next. A communication channel of this type simply maps its input signal to its output signal. The reason why communication channels are nonetheless listed as modules here is that encoding a non-binary state signal in a glitch- and metastability-free manner is a non-trivial task, as in the absence of (reliable) synchrony both parallel and sequential communication present challenges. In our abstraction, this encoding is performed by the channel, which requires additional logic and thus potentially results in delays beyond the mere wire delays as well as the necessity to consider issues concerning metastability and self-stabilization.Memory flags. These are just simple binary storage elements that can be set to 1 by means of one input signal and can be reset to 0 by a second input signal; their state is externally accessible via an output signal. Simply put, a memory flag just “remembers” which input signal was 1 most recently. In our algorithms, memory flags will be used to memorize wether an input signal from a remote node was in state 1 at some time after the most recent reset upon a state transition of one of the node's state machines (in Fig. 1, e.g., a node resets its propose flags when switching from increase to ready).Threshold gates. Frequently, nodes will need to decide whether a certain threshold number (f+1orn−f) of signals (or sets of signals) satisfy some Boolean predicate (e.g., the conditions for switching to propose and increase in Fig. 1 involve such a threshold). A threshold gate takes the respective binary input signals and outputs 1 if the threshold is reached and 0 otherwise.Watchdog timers. Watchdog timers are the nodes' sense for the progress of time. Each timer(T,s,C), where T inR+is a duration and C a clock, is associated with state s and is either expired (output 1) or not expired (output 0). The timer is reset to 0 when the node's state switches to s and will expire after T time has passed according to clock C, unless it is reset again beforehand. Hence, if it is reset at (reference) time t, it will expire at some timet′∈[t+T/ϑ,t+T]. For instance, the transition to ready in Fig. 1 is triggered by a watchdog timer.Randomized watchdog timers. A randomized watchdog timer is identical to a regular watchdog timer except that the duration T of the timeout is drawn from a (bounded) random distributionDoverR+. That is, if randomized timer(D,s,C)is reset at time t, it will expire at timet′∈[t+T/ϑ,t+T], where T is drawn fromD. How this randomness is implemented subtly affects resilience and security properties of the system, see [13] for a formal definition of the way randomness is to be employed. For the purpose of this article, we confine ourselves to pointing out that, essentially, we require that faulty nodes do not have access to the value T drawn fromDbefore timet′.State transition modules. The algorithm does not demand zero-time state transitions. Nevertheless, it is non-trivial to ensure metastability-free state transitions and consistent memory states in our setting. On a transition from state s to states′, the node needs to switch its state signal (i.e., the input to its outgoing communication channels) exactly once from s tos′and reset any memory flag that is to be reset upon transitioning from s tos′. This is complicated because the condition under which the state transition occurs, a Boolean predicate over a blend of signals from incoming communication channels and local modules' outputs, may be satisfied for a brief period of time only, or a condition for switching to a different states″might become satisfied at almost the same instant. Even worse, it may e.g. be required that memory flags whose output is part of a predicate expressing the condition for switching from s tos′are to be reset upon the transition tos′. Resolving these issues is the purpose of a state transition module that controls the safe transition from one state to another.In this section, we introduce a novel formal framework for specifying self-stabilizing fault-tolerant modules. It is a non-trivial extension of [32,42] that allows us to rigorously express the properties related to self-stabilization as used in [13]. Using this framework, we then give a precise formal specification of our basic modules' behavior.We define (the trace of) a signal to be a timed event trace over a finite alphabetSof possible signal states: Formally, signalσ⊆S×R. The elements of σ are called events, and for each event(s,t)we call s the state of event(s,t)and t the time of event(s,t). Note that we allow for events(s,t)and(s,t′)∈σ, wheret<t′, without having an event(s′,t″)∈σwiths′≠sandt<t″<t′. In this case, we call event(s,t′)idempotent. In general, a signal σ is required to fulfill the following conditions:(i)From(s,t)∈σand(s′,t)∈σfollows thats=s′.For each time interval[t−,t+]⊆Rof finite length, the number of non-idempotent events in σ with times within[t−,t+]is finite.For any time t, there exists an event(s,t′)∈σwitht′⩽t.We say that signal σ switches to s at time t iff event(s,t)∈σis not idempotent. Due to property (ii), there is always a non-zero amount of time between two such events. These events describe when the corresponding physical signal undergoes an actual transition. Therefore, we define that signals σ andσ′are equivalent, iff they differ in idempotent events only, and identify all signals of an equivalence class. Each equivalence class[σ]contains a unique signalσmaxthat contains an event for each timet∈R. We identify this signal (and thus the entire class) with the function that maps each time t to the stateσ(t):=ssatisfying that(s,t)∈σmax. We callσ(⋅)the state function of signal σ, andσ(t)the state of signal σ at time t. Note that since the state function of a signal σ depends on[σ]only, we may add or remove idempotent events at will without changing the state function.Each module comprises a (possibly empty) set of input ports and a (possibly empty) set of output ports. These sets must be disjoint, i.e., we do not allow a module's output to be identical to one of its inputs; it may be identical to the input port of another module, however. An execution of ports P on intervalI⊆Rassigns a state to each port in P at any time in I. For convenience of notation, for any port p, we identify p and its state function whenever the respective execution is clear from the context. Moreover, we may omit the “onR” for all executions for whichI=Rwhen clear from the context. An execution of module M on I is an execution of M's input and output ports on I. The restriction of executionEon I toI′⊆Iis the restriction of all ofE's state functions to the intervalI′. LetEbe an execution of ports P on I. Then the restriction of executionEon I to portsP′⊆Pis the execution of portsP′on I with state functions equal toE's state functions onP′.Besides input and output ports, a module M further has a module specification. We allow two kinds of module specifications for a module M, distinguishing between basic and compound modules.In this case, the module specification is a functionΦMthat, for all I, maps every execution of the module's input ports on I to a set of executions of the module's output ports on I. The intended meaning ofΦMis to map each and every conceivable execution of M's input ports on I to the resulting possible reactions of M during the same time, which may be many different ones. For example, a module that may behave arbitrarily for some input executionEinis specified by settingΦM(Ein)to be the set of all conceivable output executions. For a basic module M we say that executionEof M on some I is feasible iffEout∈ΦM(Ein), whereEinandEoutare the restrictions of executionEto M's input and output ports, respectively.We require two properties forΦMto hold:(i)Non-emptiness:Φ(Ein)≠∅for all executionsEinof M's input ports.Properness: Any restriction (in time) of a feasible execution of M is feasible.Example 3.1A simple basic module is a (zero-time) inverter with (binary) input port i, (binary) output port o, and module specificationΦInvdefined by: For each intervalI⊆Rand each executionEinof input port i, an executionEoutof output port o on I is inΦInv(Ein)iff for allt∈Iit holds thato(t)=¬i(t).As an example of a timed basic module, consider a fixed-delay channel with input port i, output port o, and delayd>0. Its module specificationΦCis defined by: For each interval[t−,t+)⊆Rand each executionEinof input port i on[t−,t+), an executionEoutof output port o on I is inΦC(Ein)iff for allt∈[t−+d,t+)we have thato(t)=i(t−d).Clearly, a basic module needs to adhere toΦMonly on intervals I during which it is correct, and may behave arbitrarily when it is faulty. Subtle issues originate in the fact that a module may become correct after an earlier (transient) fault, in the sense that its internal components work as intended afterwards. At this point in time, it may or may not be the case that all traces of the transient fault have been vanished from the internal state of the module.The typical use of basic modules is the description of a (sub)problem. For instance, the module specification of a threshold module will be such that the output is required to indicate whether a certain number of binary input ports is in state 1. Basic modules are then employed with the understanding that they require an implementation matching their specification.This use of basic modules in our algorithms entails that correct modules have correct internal states. Although a basic module description abstracts away its internal state, this property can be characterized in a natural way by another constraint on the module specification: We say that the specification of some module M is extendable iff each feasible execution of M on some interval[t−,t+)⊂Ris the restriction of a feasible execution of M onR. In other words, for each execution on some interval[t−,t+), (i) a fault-free history exists that leads to the internal state of the module at timet−(and therefore, for the given input signals, to the same execution on[t−,t+)), and (ii) the module is capable of continuing to operate correctly in the future (i.e., there is no fault-free execution that eventually cannot be continued in a way that adheres toΦM). While (ii) should always be true for essentially the same reason that we demand non-emptiness for, (i) can be seen as the requirement of a correct internal state: For a correct module with an extensible specification, it is required that both all internal components of a physical implementation of the module operate according to their specification and all traces of an earlier transient fault (if any) from the internal state must have worn off, in the sense that it could have been reached by a non-faulty history. Most of the basic module specifications we are going to introduce will be extendable. However, there are also basic modules with non-extendable specifications; we will provide an example later on.The next lemma shows that any extendable module specification is uniquely characterized by its values on input executions of the respective module onR.Lemma 3.3Any function Φ that maps each executionEinof a set of input portsPinonRto a non-empty set of executionsEoutof a set of output portsPoutonRspecifies a unique module M such that (i)ΦMis extendable and (ii)ΦM(Ein)=Φ(Ein)for each execution of M's input portsPinonR. Conversely, for each module M whose specification is extendable,ΦMis uniquely characterized by its values on executions of portsPinonR.ProofWe show first that for a function Φ as assumed by the lemma there exists a module M such thatΦM(Ein)=Φ(Ein)for executionsEinof portsPinonRandΦMis extendable. To this end, we simply define that M is a module with input portsPinand output portsPoutwhose specificationΦMis given byEout∈ΦM(Ein)iffEinandEoutare restrictions of executionsEin′andEout′onR, respectively, such thatEout′∈Φ(Ein′). Clearly,ΦMsatisfies properness and extendability by construction. Regarding non-emptiness, we can extend any input executionEinon some interval I arbitrarily to an executionEin′onR. By assumption,Φ(Ein′)≠∅, so there must be some executionEout′∈Φ(Ein′). By definition, its restrictionEoutto I is inΦM(Ein), proving non-emptiness. HenceΦMis a valid specification of a module M with input portsPinand output portsPout.We now establish the second claim of the lemma, which will also show that the module M we constructed from Φ is unique and thus complete the proof. To this end, letEinandEoutbe any executions of the input and output ports of M, respectively. IfEout∈ΦM(Ein), by extendabilityEinandEoutare restrictions of executionsEin′andEout′onR, respectively, such thatEout′∈ΦM(Ein′). On the other hand, ifEout∉ΦM(Ein), properness necessitates that there are no such executionsEin′andEout′. This shows the second statement and the lemma follows.  □Enabled by this lemma, we will specify most of our basic modules by definingΦM(Ein)≠∅for input executionsEinonRonly. For instance, the fixed delay channel from Example 3.2 can now be specified equivalently as follows.Example 3.4A fixed-delay channel with input port i, output port o, and delayd>0is a basic module with extendable module specificationΦCdefined by: For each executionEinof input port i onR, an executionEoutof output port o onRis inΦC(Ein)iff for allt∈Rwe have thato(t)=i(t−d).Because we do not need to describe partial feasible executions on a finite intervalI=[t−,t+)here, the fact that the conditiono(t)=i(t−d)applies only to a subinterval of I in Example 3.2 becomes superfluous:(−∞+d;∞)=R, so no additional specification is required to describe the module's behavior near the lower bound of intervals[t−,t+)witht−≠∞. By contrast, in Example 3.2, we had to specify this explicitly, by stating that any behavior of the output in[t−,t−+d)is feasible (by leaving the behavior unspecified in this interval).Whereas the specification in Example 3.4 simplifies the one of Example 3.2 only marginally, this is very different for the modules exhibiting more complex relations between input and output introduced later on: Accurately describing all suffixes of a non-trivial partial execution can be error-prone and may result in unnecessarily involved module specifications.In this case, the module M is given by a finite setSMof submodules of M. These submodules are interconnected, i.e., an output port of one submodule can be the input port of another, and the ports of M may also be ports of submodules. We require thatSMis well-formed, i.e., the following conditions are satisfied:1.For each output port p of M, there is exactly one submoduleS∈SMsuch that S has output port p.For each input port p of a submodule of M, either p is an input port of M, or there is exactly one submoduleS∈SMsuch that S has output port p.For a compound module M, we now define the corresponding basic moduleβ(M).β(M)'s input and output ports are the input and output ports of M, whereas its module specificationΦβ(M)is defined as follows: LetEbe an execution of M onI⊆Rand denote byEinandEoutits restrictions to M's input and output ports, respectively. ThenEout∈Φβ(M)(Ein), iff there exists an executionEM,SMon I of all ports of modules in{M}∪SM, such that, for each moduleS∈SM, the restriction of executionEM,SMto the ports of S is feasible for S, andEM,SMrestricted to M's ports is equal toE. Note thatΦβ(M)satisfies properness: This follows directly from the definition and properness of all submodules if all submodules are basic, and for more complex modules by structural induction. We further require that the choice ofSMis such thatΦβ(M)fulfills non-emptiness. Hence,β(M)is indeed a basic module with specificationΦβ(M). We say that executionEof M is feasible for M iff it is feasible for the corresponding basic moduleβ(M). Intuitively, this means that we consider a compound module correct whenever all its submodules are correct.Example 3.5Consider a compound module InvChain with (binary) input port i and (binary) output port o. InvChain is specified by the set of modulesSInvChain={Inv1,Inv2,Chan1,Chan2}, whereInv1andInv2are (zero-time) inverters, andChan1andChan2are (binary) fixed-delay channels with delayd>0. We connect the modules' ports sequentially, as depicted in Fig. 3: The input port i of InvChain is also the input port ofInv1, whose output port a is the input port ofChan1; the output b ofChan1is fed intoInv2, whose output c in turn is input toChan2; finally, the output port o ofChan2is also the output port of M.LetEinbe the execution of port i whose state function isi(t)=0fort∈(−∞,0)andi(t)=1fort∈[0,∞). For the module specification of InvChain's corresponding basic moduleβ(InvChain)it then holds thatΦβ(InvChain)(Ein)={Eout}, whereEoutis the execution of port o witho(t)=0fort∈(−∞,2d)ando(t)=1fort∈[2d,∞).Note thatΦβ(InvChain)is extendable: Any feasible executionEon some interval[t−,t+)satisfieso(t)=c(t−d)=¬b(t−d)=¬a(t−2d)=i(t−2d)for all timest∈[t−+2d,t+). An infinite feasible execution of InvChain onRwhose restriction to[t−,t+)matchesEis easily found by (i) extending the input ports execution to[t−−2d,t−)by usingi(t−2d)=o(t)on[t−,t−+2d), (ii) extending the output ports execution to[t+,t++2d)by usingo(t+2d)=i(t)on[t+−2d,t+), and finally definingo(t)=i(t−2d)arbitrarily fort∈(−∞,t−)andt∈[t++2d,∞).From this observation, we can infer by Lemma 3.3 that the module specification of InvChain matches that of a fixed-delay channel with delay 2d, as in feasible executions of InvChain onRwe clearly haveo(t)=i(t−2d)at all timest∈R. This demonstrates that compound modules permit to reason about the behavior of complex systems in a hierarchical fashion, based on basic modules that can be understood much easier. The level of detail can be adjusted by the granularity at which one relies on basic modules; in particular, it is feasible to refine the analysis later on by further breaking down basic modules into compound modules.Example 3.6As an example of a non-extendable specification, consider the compound module Osc, a simple oscillator that is started at, say, time 0. The module has no input port and one (binary) output port c, the clock signal.1414In practice, one would of course need an input conveying the control signal starting the oscillator; we conveniently hide this detail within the module specification.It comprises a binary resettable fixed-delay channel (RChan) of delayd>0, with input port c and output port o, whose extendable specification demands that its output fulfillso(t)=0for all timest∈(−∞,0)ando(t)=c(t−d)for all timest∈[0,∞). Port o further is the input port of a (zero-time) inverter (Inv), whose output port is c, the clock signal.It is not hard to see that the only feasible execution of Osc onRmaintains signal 0 on port o and signal 1 on port c until time 0. Then the feedback loop starts to oscillate with frequency1/(2d), since the channel reproduces its input with fixed delay d and the inverter inverts the signal. The specification of Osc is not extendable. This can be seen from the execution on[0,∞)whereo(t)=kmod2for allt∈[kd/3,(k+1)d/3)andk∈N0, andc(t)=¬o(t)for allt∈[0,∞). This execution is feasible when restricted to each submodule sincec(t)=¬o(t)=o(t−d)for allt∈[d,∞), but the output signalc(t)oscillates at frequency3/(2d). Hence this execution is feasible, but not a restriction of the unique feasible execution of Osc onR.Note carefully that an oscillator according to Fig. 4involving a channel with delayd=0would be an example of a well-formed set of modules M that is not a compound module, since it violates non-emptyness ofβ(M).Examples 3.5 and 3.6 of InvChain and Osc reveal a crucial insight about the behavior of compound modules. While InvChain behaves like a fixed-delay channel in all feasible executions, it cannot be said for Osc that it always behaves like an oscillator of frequency1/(2d). Even though Osc will run at the fixed frequency of1/(2d)if it is feasible at all times, an inconsistent initialization or a transient fault can cause it to run forever at an arbitrarily high frequency. In general, when devising compound modules, typically our design goal will be to implement a certain basic module, that is, essentially ensure that the feasible executions of the compound module are also feasible according to the (usually more idealized) specification of the respective basic module. For InvChain, the intended basic module is a fixed-delay channel, where for Osc we had an (externally started) oscillator of frequency1/(2d)in mind. Since we strive for self-stabilization properties, intuitively InvChain would be a “good” implementation of a fixed-delay channel, whereas Osc is not satisfactory because it does not recover from transient faults.Before we formalize the concepts of implementation and self-stabilization, let us get a better understanding of the critical difference between InvChain and Osc. In both cases, the output depends on past inputs, so both compound modules have a sort of memory. In any real-world system, this cannot be avoided since physics entails positive delays for any building block we might use. However, InvChain eventually “forgets” about previous inputs, while Osc contains a feedback-loop that, in a feasible execution, determines the future output as a periodic function of the first d time units of the execution.This motivates the following definitions. ForF∈R0+, we say that a module is F-forgetful, iff its specification permits the following construction:1.Take an arbitrary feasible executionEof M on some interval[t−,t+]⊆R. Denote byEinandEoutits restrictions to the input and output ports of M, respectively.RestrictEouttoEout′on[t−+F,t+). Then for each executionEin′onRwhose restriction to[t−,t+]equalsEin, there is a feasible execution of M onRwhose respective restrictions to the input and output ports (and in time) areEin′andEout′.Let the circuit graph of a compound module M be the directed graph whose nodes are the submodulesSM, and for each port p that is an output port ofS∈SMand an input port ofS′∈SMit contains a (directed) edge from S toS′. We recursively define that M is feedback-free iff (i) all its basic submodules are forgetful, (ii) all its compound submodules are feedback-free, and (iii) its circuit graph is acyclic. Finally, we define the idealized basic moduleι(M)corresponding to compound module M (with the same ports as M) by executionEofι(M)being feasible iff it is the restriction of a feasible execution of M onR.According to these definitions, InvChain is clearly 2d-forgetful and feedback-free. Since we observed thatΦβ(InvChain)is extendable, it is in fact 0-forgetful and its corresponding basic and idealized basic modules are identical. In contrast, Osc satisfies neither of these properties. Note, however, that replacing a basic module in InvChain by a compound module might result in a module that does not have these properties either.We are now in the position to formulate a theorem that states that any feedback-free module will eventually behave like its idealized basic module in a feasible execution.Theorem 3.7Suppose M is a feedback-free compound module. LetPbe the set of paths in its circuit graph and submoduleS∈SMbeFS-forgetful. Then M is F-forgetful withF:=max(S1,…,Sk)∈P{∑i=1kFSi}.ProofConsider an arbitrary executionEon[t−,t+)on the ports of M and its submodules whose restriction to the ports of M (and thus also its restrictions to each submodule of M) is feasible, its restrictionsEinandEoutto the input and output ports of M, the restrictionEout′ofEoutto[t−+F,t+), and an arbitrary executionEin′onRsatisfying that its restriction to[t−,t+)equalsEI. Denote for eachS∈SMFS,M:=max(S1,S2,…,Sk=S)∈P{∑i=1kFSi}.Note thatmaxS∈SM{FS,M}=F.By induction on the submodules of M, we will construct a feasible executionEimaxon the ports of M and its submodules onRthat is feasible for M when restricted to the ports of M and whose respective restrictions equalEin′andEout′. In each step of the induction, we will add the output ports of some submodule of M to the execution. The induction halts once we run out of submodules of M afterimax⩽|SM|steps. The induction hypothesis states for the executionEion the union of the input ports of M and the ports of a subset of its submodules that•its respective restrictions to submodules are feasible,its restriction to the input ports of M equalsEin′,its restriction to output ports of M on[t−+F,t+)matches the restriction ofEout′to such ports, andfor each submoduleS∈SMwhose executionESis already fully specified byEi, we have thatESrestricted to[t−+FS,M,t+)⊇[t−+F,t+)equals the corresponding restriction ofE.To perform the induction step, suppose we have already constructedEi−1for somei∈[1..imax]. By definition ofimax, there is an uncovered submodule left, i.e.,Ei−1does not specify the execution on all ports of all submodules. Because M is feedback-free, its circuit graph is acyclic. Consider the subgraph of the circuit graph induced by the remaining uncovered submodules. As it is acyclic as well, there must be a module S without an incoming edge, implying thatEi−1specifies executions of all its input ports. By the induction hypothesis, we have that these executions restricted to[t−+FS,M−FS,t+)equal the respective restrictions ofE: Any such port is either an input port of M and therefore its executions inEi−1andEare identical on[t−,t+), or it is an output port of some moduleS′satisfying that there is some path(S1,…,Sk−1=S′,Sk=S)∈Pand therefore the executions of the port inEi−1andEare identical on[t−+FS′,M,t+)⊇[t−+FS,M−FS,t+). Since the output ports of S cannot be output ports of other modules and the input ports of M cannot be output ports of submodules,Ei−1does not specify executions for any of the output ports of S. We apply that S isFS-forgetful to the restriction ofEto the ports of S on the interval[t−+FS,M−FS,t+)and the input execution given by the restriction ofEi−1to the input ports of S, which is possible since we observed that the restrictions of these executions to the input ports of S on[t−+FS,M−FS,t+)are identical. We obtain a feasible execution of S onRwhose restriction to the input ports matches the restriction ofEi−1to these ports and whose restriction to the output ports and[t−+FS,M,t+)matches the respective restriction ofE. We defineEiby adding these executions of the output ports of S toEi−1. Overall, the first, second, and fourth claim of the induction hypothesis thus hold by construction. Taking into account that[t−+FS,M,t+)⊇[t−+F,t+)andEout′is a restriction ofE, the same holds for the third claim. Hence the induction step succeeds, finishing the proof.  □The bad news is that we have to introduce feedback-loops into the system at some point: After all, any clock source is some kind of oscillator. As demonstrated by Osc, we cannot expect a strong generic result like Theorem 3.7 for compound modules that are not feedback-free. Also, clearly such a structure cannot be forgetful. Hence, let us turn to a less restrictive concept of recovery from transient faults.Self-stabilization is the property of a system to recover from arbitrary transient faults in finite time, provided that all transient faults cease. Since arbitrary transient faults may result in arbitrary states, this requirement can be rephrased as the need to reach a valid state within finite time from any initial state. For basic modules, our framework encapsulates this concept by the notion of feasibility; executions completely hide the internal state of a basic module, and we assume (or hope) that the utilized implementation of the module will recover from faults that are considered transient, e.g. particle hits, crosstalk, magnetic fields, or power outages. For compound modules, one possible meaning of “valid state” in our context is given by the specification of the corresponding idealized basic module: If, viewed from the outside, the execution is indistinguishable from one that could occur if the module was entirely fault-free, we can safely assume that its state is valid; any fault that is masked is irrelevant to the higher layers of the system anyway, as it solely relies on the guarantees of the utilized model specification on the module's ports' executions.A more general concretization of the same underlying idea is the notion of self-stabilization we present next. It introduces two relaxations of the constraints on the behavior of a module. Firstly, we do not expect that the recovering module is perceived as functional by an outsider immediately after its execution becomes feasible. Rather, we allow for some stabilization time during which the module can “clean up” its internal state. Secondly, we do not require that the module fulfills its corresponding idealized basic module specification. Instead, we choose another, weaker specification that is sufficient for our purposes. One specification being stronger than another is captured by the following notion. We define that module M implements moduleM′iff both modules have the same sets of input and output ports andΦM(Ein)⊆ΦM′(Ein)for allEin. Thus,ΦMis stronger thanΦM′in the sense that it imposes at least as many constraints on the output executions asΦM′. ForT∈R0+, module M now is a T-stabilizing implementation of moduleM′, iff the restriction of each feasible execution of M on[t−,t+)⊆Rto[t−+T,t+)is a feasible execution ofM′. Clearly, forT>0, this allows forΦM(Ein)⊈ΦM′(Ein), i.e., a T-stabilizing implementation ofM′needs not be an implementation ofM′. For brevity, we may say that “M is T-stabilizing” to express that M is a T-stabilizing implementation of its corresponding idealized basic module. We simply say M is a self-stabilizing (implementation ofM′) iff it is a T-stabilizing (implementation ofM′) for someT∈R0+.From our previous definitions and results, we immediately can derive the following statements.Lemma 3.81.Every F-forgetful module is F-stabilizing.Every feedback-free compound module is self-stabilizing.If M is a self-stabilizing implementation ofM′andΦMis extendable, then M is an implementation ofM′.Every feedback-free compound module whose specification is extendable implements its corresponding idealized basic module.The first statement follows directly from the definition of F-forgetfulness. The second statement follows from Theorem 3.7 and the first statement. Regarding the third statement, by extendability every feasible execution of M on some intervalI⊆Ris the restriction of a feasible executionEof M onR. By the definition of self-stabilizing implementations, there is someT∈R0+such thatErestricted to(−∞+T,∞)=Ris feasible forM′. By properness, restrictingEto I yields a feasible execution ofM′. Since this restriction equals the original feasible execution of M, it follows that every feasible execution of M is a feasible execution ofM′. The last statement follows from the second and third.  □Recall that InvChain from Example 3.5 behaves like a fixed-delay channel with delay 2d. As InvChain is feedback-free and its specification is extendable, we could also infer this statement directly from Lemma 3.8. In contrast, Osc is not self-stabilizing at all, i.e., it is not self-stabilizing for anyT∈R0+, which follows from the same arguments that we used to show that its specification is not extendable. In general, determining whether a module M is a self-stabilizing implementation of some other moduleM′(or even bounding the stabilization time) requires a detailed stabilization analysis.One may as well generalize the above definition to also account for probabilistic stabilization by defining an appropriate probability space on the set of executions of M. For the sake of brevity and better readability, we only informally state the probability space in this work by introducing basic modules that act in a probabilistic manner, namely the Randomized Watchdog Timers. Probabilistic bounds on the stabilization time of compound implementations then are naturally derived from the respective distributions of their submodules. The interested reader is referred to [13], where we give a precise definition of the probability space over which our probabilistic stabilization guarantees are expressed.We next generalize the definition of feasibility for compound modules to potentially faulty submodules. For a compound module M andf⩾0, we define the corresponding f-faulty basic moduleβf(M)as follows:βf(M)'s input and output ports are the input and output ports of M. LetEbe any execution of M onI⊆Rand denote byEinandEoutits restrictions to the input and output ports of M, respectively. ThenEout∈Φβf(M)(Ein), iff there exists an executionEM,SMof all ports of modules in{M}∪SMand a subsetFofSMof size at most f, such that, for each module S inSM∖F, the restriction of executionEM,SMto the ports of S is feasible, andEM,SMrestricted to M's input and output ports is equal toE. As in the fault-free case, we require thatΦβf(M)fulfills non-emptiness. For compound module M andf⩾0, we say an executionEof M is f-faulty feasible iff it is feasible for the corresponding f-faulty basic moduleβf(M). Modules whose behavior is unrestricted in executionE(i.e., that belong to the setF) are called faulty (in executionE).Clearly, a compound module M cannot mask faults of submodules that have an output port that is also output port of M. Consequently, tolerance of ongoing faults necessitates to weaken the constraints on M's output. Hence, forf⩾0, compound module M, and some other moduleM′with the same set of input and output ports, we say that M is an f-tolerant implementation ofM′iff every f-faulty feasible execution of M is a feasible execution ofM′, i.e., the corresponding f-faulty basic module of M implementsM′. Analogously, M is an f-tolerant T-stabilizing implementation ofM′iff its corresponding f-faulty basic module is a T-stabilizing implementation ofM′.Note that this entails that the union of output ports of any set consisting of f submodules of M can be arbitrary. The module specification ofM′thus must not put concurrent restrictions on all its output ports to allow for fault-tolerance. Hence, one demands constraints on the outputs of non-faulty submodules of M (i.e., those whose executions are feasible) only.In the formal framework introduced above, a node is simply a compound module that will operate according to some specification whenever it is non-faulty. We now specify the submodules of a node introduced informally in Section 2.2 and reveal how they are connected. The reader might want to take a look at Fig. 5to get an idea of the general layout of a node at this point. Each nodei∈[1..n]has n input portsSi,j, wherej∈[1..n], andn+1output ports, namelyLiandSj,ifor allj∈[1..n]. We present all submodules as basic modules whose specifications are sufficiently strong to implement the model used in [13]. As a result, not all specifications can be satisfied by (physical) implementations of the modules for all input executions; we discuss these limitations and their implications in Section 6. All of the following specifications are extendable and therefore, by Lemma 3.3, are uniquely characterized by describing them on executions onRonly. It is trivial to verify non-emptyness for the given specifications, hence omit respective discussions.•Communication channels. For eachj∈[1..n], there is a communication channel of delaydChan∈R+from node i's internal portSito its output portSj,i. Formally, this communication channel is a basic module with input portSiand output portSj,i. The module specificationΦChanof the communication channel is as follows. LetEinbe an execution of input portSiandEoutan execution of output portSj,i. ThenEout∈ΦChan(Ein)iff there exists a continuous, strictly increasing (and thus invertible) delay functionτ:R→Rsuch that for allt∈Rboth (i)Sj,i(t)=Si(τ−1(t))and (ii)0⩽t−τ−1(t)<dChanhold.It is important to note that the assumptions on the communication channels are strictly weaker than those on fixed-delay channels, as the delay of a communication channel may vary arbitrarily (within bounds) during an execution.Memory flags. For each states∈Sand each nodej∈[1..n], there is a memory flagMemi,j,sat node i. It has two input ports, namelySi,jand a binary reset port, as well as a binary output portMemi,j,swhose name is for simplicity identical to the memory flag's name. Given an executionEinof the flag's input ports, an executionEoutof the output port is inΦMem(Ein)iff properties (Reset) and (Set) hold:–(Reset) For all timest∈R,Memi,j,s(t)=0iff the reset port has been in state 1 at some time betweensup{t′∈(−∞,t]|Si,j(t′)=s}and t.(Set) For all timest∈R,Memi,j,s(t)=1iff the reset port continuously has been in state 0 betweensup{t′∈(−∞,t]|Si,j(t′)=s}and t.Threshold gates. Node i may comprise an arbitrary number of threshold gates with arbitrary thresholds. The module specificationΦThrof a threshold gate with binary input portsa1,…,am, binary output port o, and thresholdk∈[1..m]is defined as follows. LetEinbe an execution of the input ports andEoutan execution of output port o. ThenEout∈ΦThr(EI)iff for allt∈R,o(t)=1if at least k input ports are in state 1 at time t, ando(t)=0otherwise.Watchdog timers. For each states∈S, there can be watchdog timers(T,s,C)at node i, whereT∈R+is the duration of the timer and C is a clock. The watchdog timer has input portSi,iand a binary output portTimeT,s,C. The timer's module specificationΦTimeis defined as follows. LetEinbe an execution of the timer's input portSi,iandEoutan execution of its output portTimeT,s,C. ThenEout∈ΦTime(Ein), iff the following holds:–(Clock) Clock C is correct at all times, i.e.,t′−t⩽C(t′)−C(t)⩽ϑ(t′−t)for allt,t′∈R,t<t′.(Reset) There exists a (binary) signalσT,s,C∈[TimeT,s,C](the equivalence class of the output port's signal) such that, for each timets∈RwhenSi,iswitches to state s, there is a timet∈[ts,ts+dTime]such that(T,s,C)is reset, i.e., event(0,t)∈σT,s,C. This is a one-to-one correspondence, i.e., for each suchtsthis time t is unique and(T,s,C)is not reset at any other times.(Expire) Denote byR⊂Rthe set of times when(T,s,C)is reset. For each timetR∈R, denote bytE(tR)the unique time satisfying thatC(tE(tR))−C(tR)=T. Then, for eacht∈R,TimeT,s,C(t)=0ifft∈⋃tR∈R[tR,tE(tR)).Randomized watchdog timers. A randomized watchdog timer(D,s,C)is a module with input portSi,iand output portTimeD,s,C, whereDis a bounded random distribution on(0,D]⊂R+, s is a state, and C a clock. The module specification of(D,s,C)is analogous to the module specification of a watchdog timer, except that property (Expire) is replaced by:–(Expire') Denote byR⊂Rthe set of times when(T,s,C)is reset. For each timetR∈R, denote bytE(tR)the unique time satisfying thatC(tE)−C(tR)=T(tR), whereT(tR)is drawn (independently) fromD. Then, for eacht∈R,TimeT,s,C(t)=0ifft∈⋃tR∈R[tR,tE(tR)).State transition modules. Node i's state transition module has input portsSi,jfor each nodej∈[1..n]as well as one binary input port for each of the memory flags, (randomized) watchdog timers and threshold gates it uses. Furthermore it has an output portSias well as one binary Reset output port for each of the memory flags it uses.A node's state transition module executes a state machine specified by (i) a finite setSof states, (ii) a function tr, called the transition function, fromT⊆S2to the set of Boolean predicates on the alphabet consisting of expressions of the form “p=s” (used for expressing guards), where p is from the state transition module's input ports and s is from the set of possible states of signal p, and (iii) a function re, called the reset function, fromTto the power set of the node's memory flags.Intuitively, the transition function specifies the conditions (guards) under which a node switches states, and the reset function determines which memory flags to reset upon the state change. Formally, let P be a predicate on the input ports of node i's state transition module. We define P holds at time t by structural induction: If P is equal top=s, where p is an input port of node i's state transition module and s is one of the states signal p can obtain, then P holds at time t iffp(t)=s. Otherwise, if P is of the form¬P1,P1∧P2, orP1∨P2, we define P holds at time t in the straightforward manner.For a given transition delaydTrans>0, the module specificationΦSTMof node i's state transition module is defined as follows. LetEinbe an execution of the state transition module's input ports andEoutan execution of its output ports. ThenEout∈ΦSTM(Ein)iff there is someε>0and a signal locked such that the following requirements are met. (The intuition is thatlocked(t)=0means that the node is ready to perform the next state transition once a guard becomes true, whereas in case oflocked(t)=1the node is currently executing a previously “locked” transition.)–(Safety) The node (i.e.,Si) does not switch states at any time t withlocked(t)=0. In every maximal interval[tl,tu)⊆Rsatisfying thatlocked≡1on[tl,tu), it switches states exactly once.(Delay) For each interval[tl,tu)as above,tu−tl⩽dTrans−ε.(Guard) For each interval[tl,tu)as above,(Si(tl),Si(tu))∈Tandtr(Si(t),Si(tu))is satisfied at some timet∈[tl−ε,tl].(Responsiveness) Iflocked(t)=0and there is a states∈Ssuch that(Si(t),s)∈Tandtr(Si(t),s)holds at time t, thenlocked(t+ε)=1.(Flags) For an arbitrary interval[tl,tu)as above, suppose that the node switches from stateSi(tl)to stateSi(tu)at timets∈[tl,tu). Then for each memory flag specified byre(Si(tl),Si(tu)), the corresponding reset output port of the state transition module is in state 1 at some time in(tl,ts](and therefore the flag is reset). Outside these time intervals, reset ports are in state 0.To account for the latency of the memory flags, threshold gates and (randomized) watchdog timers, their ports are not directly connected to the state transition module's ports, but via binary communication channels with respective delays. The resulting structure of the compound module node i is depicted in Fig. 5. Note that additional communication channels at the threshold gates' and memory flags' input ports allow to model the fact that memory flags are not necessarily reset at the same time, and signals may arrive shifted in time at the threshold gates.As mentioned earlier, for simplicity we consider the outgoing channels to remote nodes as part of the node. Hence, the output ports of node i comprise the output portsSj,i,j∈[1..n], of the channels disseminating its stateSi. In addition, in order to solve the actual problem of clock generation, we include the locally computed discrete clock valueLias an output port.We next formalize the concept of a protocol, like the one presented in Section 5, followed by what it means for a protocol to solve self-stabilizing clock synchronization in spite of f faults. Formally, a protocol (for an n-node system) is a compound module consisting of n modules referred to as nodes. The nodes are to be specified as modules themselves, and in our case will follow the layout we just described. It thus remains to state in Section 5 which (randomized) watchdog timers, memory flags and threshold gates our protocol uses as well as the state transition modules' transition and reset functions.A clock synchronization module withn∈Nnodes, clock imprecision Σ, amortized frequency boundsA−,A+∈R+, slacksτ−,τ+∈R0+, maximum frequencyF+, and at mostf∈Nfaults is a module without input ports and with output portsLi,i∈[1..n]. Its module specification is extendable. An execution of the module onRis feasible, iff there exists a subsetCof[1..n]of size at leastn−fsatisfying that•∀t∈R,i,j∈C:|Li(t)−Lj(t)|⩽Σ,∀t,t′∈R,t<t′,i∈C:A−(t′−t)−τ−⩽Li(t′)−Li(t)⩽A+(t′−t)+τ+, and∀t,t′∈R,t<t′,i∈C:Li(t′)−Li(t)⩽⌈F+(t′−t)⌉.We say a protocol Π (for an n-node system) with no input ports and output portsLi,i∈[1..n], solves self-stabilizing clock synchronization with clock imprecision Σ, amortized frequency boundsA−,A+, slacksτ−,τ+∈R0+, maximum frequencyF+, at most f faults, and stabilization time T (with probability p) iff it is an f-tolerant, (with probability at least p) T-stabilizing implementation of the clock synchronization module with the respective parameters.A (real-world) implementation will output bounded clocks of sizeK∈Nonly. In this case the output ports do not yieldLi(t), but onlyLi(t)modK. Nevertheless, we introduced the signalsLias abstract functions in this setting, as they allow to state the frequency bounds concisely. Note that there is no physical counterpart of these values in the real-world system; to be strictly accurate, it would be necessary to qualify the above definitions further by “with bounded clocks of size K” in order to distinguish this version of the problem from the abstract one with unbounded clocks.Our formal model incorporates a precise semantics of what it means for a module to implement some other module, namely, inclusion of all feasible (sub-)executions. Unfortunately, however, this strong requirement must often be relaxed when it comes to real implementations. This is primarily a consequence of the fact that there is no physical implementation of a circuit that can avoid metastability. Since preventing certain inputs to a module requires output guarantees from others, this is a challenging problem to systems that are self-stabilizing or tolerate persistent faults; combining these properties complicates this issue further.More specifically, in order to faithfully implement their specifications, basic modules must be able to (i) deal with all possible inputs and (ii) recover reliably from transient faults. Unfortunately, (i) is often impossible to achieve with real circuits. For example, simultaneous input changes may drive any implementation of a Muller C-gate into a metastable state, which implies that its output ports do not even carry signals according to our definition, and are hence not feasible. Of course, metastability can also be caused by physical faults affecting the module; such faults can obviously not be analyzed within our model either. This possibility obviously invalidates any guarantees that compound implementations containing this instance may provide, unless they can mask the error due to fault-tolerance. Moreover, real circuits cannot guarantee (ii) under all circumstances either, as it is impossible to always prohibit the propagation of metastable inputs to the outputs and the system may contain feedback-loops.In principle, it would be possible to extend the presented model to cover also generation and propagation of metastability explicitly, by replacing the finite alphabetSand discrete events with a continuous range of signal values (the voltages) [43]. Since this would dramatically increase the complexity of any analysis, we choose a different approach that also allows us to handle other implementation intricacies in a pragmatic way.In fact, even in the absence of metastability, it is not necessarily simple and even possible for real implementations to guarantee (ii) under all circumstances. Apart from the fact that transient faults may lead to permanent errors by damaging physical components,1616We remark that, technically speaking, excessively high voltages on the input wires could also be interpreted as an “input violation”, as this violates the definition of our signals. However, it makes sense to interpret such (hopefully exceptional) events as a fault of the module.our model does not prohibit that temporarily infeasible inputs result in permanent infeasibility, i.e., even when inputs become benign again at a later state of the execution of the module in question, there is no suffix of the execution that is feasible. The oscillator implementation given in Example 3.6 demonstrates this issue, and further modules exhibiting persistently faulty behavior after temporary violations of input constraints are easily conceived.As we aim for self-stabilization, it is clear that we cannot allow implementations that suffer from such drawbacks: Neither transient faults nor their consequences, i.e., temporarily arbitrary executions, may result in permanent faults. Clearly, both recovery from transient failures and resilience of a basic module to erroneous inputs, and hence the whole definition of what actually constitutes a transient fault in our model, is implicitly defined by the physical realization of an implementation.These observations have important consequences. On the one hand, careful design of the basic modules is of paramount importance. For instance, in a final product, a watchdog timer must not have its duration stored in a memory register that can be corrupted by a temporary charge injection (e.g. due to a particle hit), a ring oscillator should not be able to run unchecked at e.g. twice its frequency indefinitely (e.g. triggered by a voltage pulse), and one has to make sure that stateful components like memory flags or state transition modules eventually “forget” about potentially erroneous inputs in the past, and eventually behave according to their specification again. As discussed above, however, this cannot usually be perfect: There will always be (rare) scenarios, where an implemented circuit will not work like an ideal one, i.e., violate its specification. We incorporate this in our model, in a pragmatic well-known from critical system design, by means of the notion of imperfect implementation coverage. For a given module implementation, the coverage implicitly or explicitly determines the fraction of all possibly executions in which the implementation works as specified. Since exceptional scenarios like metastability are usually extremely rare, we do not bother with defining the notion of coverage formally here: The coverage should be very close to 100% anyway. In Section 6, we will argue that each of our basic modules will work as specified, except for very rare situations that may trigger metastability due to a violation of input timing constraints.Thanks to this approach, algorithms and proofs can rely on sufficiently simple specifications of basic modules, which usually also admit robust and efficient implementations in practice. Any unhandled scenarios are relegated to imperfect implementation coverage. This feature is essential for devising proofs of reasonable complexity that show self-stabilization of all compound modules, implying that the system indeed will recover once transient faults of (basic) modules cease. Due to the hierarchical composition of modules, compound modules fully derive their behavior from their submodules and can therefore be analyzed based on the properties of their submodules, while we may switch at will between viewing a module as given (i.e., basic), analyzing it in more detail as a compound implementation, or (for low-level modules) analyzing it in an even more detailed model. This way, our approach also inherently supports tight interaction between algorithmic design and design of the basic building blocks used in the algorithms.In this section, we recast the self-stabilizing clock synchronization algorithm introduced in [13] in the modeling framework of the previous section and summarize its most important properties. Since the main focus of our paper is on the implementation of our algorithm in this model, there is no need to provide a detailed description of the stabilization mechanism, let alone formal proofs of the stated claims; the analysis of the correctness and performance of the algorithm in [13] is based on a simpler abstract system model, assuming a globally valid end-to-end delay bound d covering any (local and remote) communication and processing action, which is fully compatible with our modeling framework. More specifically, all that is needed in order to reuse the results of the analysis in [13] is to compute the maximum end-to-end delay occurring in the implementation of our algorithm in the modeling framework introduced in Section 2.2.Recall from Section 2.2 that our top-level clock synchronization module is implemented as a compound module consisting of n nodes and their connecting top-level channels (with maximum delaydChan). Every node, in turn, is a compound module made up of a state transition module, watchdog timers, memory flags, and threshold modules interconnected by channels (modeling various delays) as shown in Fig. 5. Finally, a state transition module represents several communicating concurrent asynchronous state machines (with maximum transition timedTrans). It ensures that state transitions of every constituent state machine occur in an orderly fashion, i.e., that every transition happens exactly once and, if need be, memory flags are consistently reset. The state of each state machine is encoded in a few bits and conveyed via the top-level channels to all other modules in the system that need to receive it on some input port. Given this simple internal structure, computing the resulting end-to-end delay bound d (or, for the quick cycle,dmin+anddmax+, see below) from the constituent delay bounds is straightforward, see Section 6 for details.Obviously, the entire logic of our algorithm is encoded in the state machines of a node. In [13], we use a graphical representation that also reveals the layered structure imposed by their communication. We already employed this description in Fig. 1. With the definitions from the previous section at hand, we can now give our graphical representation a precise formal meaning that will allow us to translate the results from [13] to our modeling framework.Our graphical representation defines the set of possible statesSof a state machine (in Fig. 1ready, propose, and increase) and, by means of the arrows between the states, the set of possible state transitionsT⊆S2(here ready to propose, propose to increase, and increase to ready). If, for a state transition from s tos′,re(s,s′)≠∅, i.e., there are memory flags that need to be reset,re(s,s′)is given in a rectangular box on the arrow. Since for each node i and state s we will always reset all memory flagsMemi,j,sforj∈{1,…,n}together, we simply writes1,…,skin such a box to represent the fact that all flagsMemi,j,s,j∈{1,…,n},s∈{s1,…,sk}, are to be reset. Note that some of these states may be from a different state machine, i.e., the statess1,…,skneed not all be fromS.Completing the description, for each(s,s′)∈T,tr(s,s′)is given by the label next to the respective arrow. Again, we make use of a condensed notation. Assume that the state machine in question is part of node i. We will employ threshold conditions like “⩾f+1s1”, whereby we refer to at leastf+1of i's memory flagsMemi,j,s1being in state 1, or “⩾n−fs1ors2”, which is true if∑j∈Nmax{Memi,j,s1,Memi,j,s2}⩾n−f, i.e., for at leastn−fnodes j flagMemi,j,s1or flagMemi,j,s2is in state 1. An example for such a rule is the transition from propose to increase in Fig. 1. Such conditions will be translated to a binary signal by feeding the memory flags' signals (or, in the latter case, the output of n OR-gates with inputsMemi,j,s1andMemi,j,s2) into a threshold gate (of thresholdf+1orn−f, respectively). Further abbreviations we use for timeouts. Recall that for a timeout(T,s,C), we omit the clock C from the notation, i.e., write(T,s)instead of(T,s,C). Timeout(T,s)switches to 1 after T local time units (i.e., betweenT/ϑandT+dTransreference time) has passed since the last switch to state s was triggered. In case it is part of a transition rule, we write(T,s)for the conditionTimeT,s,C=1, and if the transition goes from the state s to which the timeout corresponds to some states′, we simply write T. For instance, the condition “3ϑdlocal time has passed” in Fig. 1 is concisely stated as “3ϑd”.Finally, as for memory flag resets, transition rules may also refer to a state s of another state machine. In the special case that a predicate solely depends on the current state of another of the node's state machines, we write “in s” or “not in s” to indicate the predicatesp=sand¬(p=s), respectively, where p is the input port connected to the channel communicating the other state machine's state to the state transition module. Finally, the above rules can be composed by logical AND or OR, which we display by connecting expressions with and or or, respectively. In Fig. 1, such a composition occurs intr(ready,propose).Each node is a collection of several state machines that are organized in a layered structure. On each layer, the state machines of the (at leastn−f) non-faulty nodes cooperate in order to establish certain synchronization properties of their output signals. The higher is a state machine in the hierarchy, the stronger are these guarantees; the lower it is, the weaker are the synchronization properties its input signals need to satisfy for stabilization. The lowest-layer state machine utilizes randomization to recover from any configuration (provided its basic modules are correct (again), i.e., guarantee feasible executions). Each other layer utilizes auxiliary information from the layer below to stabilize. Finally, the top level state machine outputs the logical clocksLi.More specifically, we have the following state machines.•At the top level, we have the quick cycle state machine (Fig. 6) that outputsLi. The quick cycle is very similar to the algorithm given in Fig. 1, except that it is coupled to the state machine beneath it in order to ensure eventual stabilization. Once the system is stabilized, it consistently and deterministically increasesLiat a high frequency while guaranteeing small clock imprecision.The main state machine (Fig. 8) is the centerpiece of the stabilization mechanism. Once stabilized, it generates slow, roughly synchronized “pulses” within certain frequency bounds. These pulses can be seen as a “heartbeat” of the system; at each pulse, the quick cycle's clocks are reset to 0 and the quick cycle's state machines are forced into stateaccept+(corresponding to the increase state in Fig. 1). This enforces exactly the initial synchrony that we explained to be necessary for the correct operation of the algorithm from Fig. 1.By itself, however, the main state machine is not capable of recovering from every possible initial configuration of the non-faulty nodes. In certain cases, it requires some coarse synchrony to be established first in order to stabilize, which is probabilistically provided by the underlying layer. We remark that, once stabilized, the main state machine operates fully independently of this layer (and thus deterministically).The auxiliary information potentially required for stabilization by the main state machine is provided by a simple intermediate layer we refer to as extension of the main state machine (Fig. 9). Essentially, it is supposed to be consistently reset by the underlying layer and then communicate information vital for stabilization to the main state machine. This information depends both on the time of reset and the current states of the n main state machines, which it therefore monitors.Finally, the resynchronization routine (Fig. 10) utilizes randomized timeouts to consistently generate events at all non-faulty nodes that could be understood as “randomized pulses”. Such a pulse is correct for our purposes if all non-faulty nodes generate a respective event in coarse synchrony and no non-faulty node generates another such event within a time window of a certain length. The crux of the matter is that a single such pulse suffices to achieve stabilization deterministically. Relying on (pseudo-)randomness on this layer greatly simplifies the task of overcoming the interference by faulty nodes at low costs in both time and communication. We note that the main state machine masks this randomness once stabilization is achieved, facilitating deterministic behavior of the higher levels and, ultimately, the nodes' clocksLi.We will now present the individual state machines. We refrain from a discussion of choosing appropriate durations for the timers, confining ourselves to stating a feasible family of choices later on.The quick cycle state machine is depicted in Fig. 6. It introduces an additional notation: As the statesready+andaccept+are not distinguished in any of the transition conditions in the other state machines, the same statenone+can be communicated here. This allows for a very efficient single-bit representation of the communicated states. In Fig. 6, this is expressed by dividing the circles representing states, putting the state names in the upper part and the communicated states in the lower part. Apart from saving a wire, this permits to use trivial encoding and decoding of the signal, a simplification of the logic that minimizes delays and therefore maximizes the clock frequency that can be achieved.Essentially, the quick cycle works as the algorithm given in Fig. 1, where the logical clock is increased whenever the machine switches to stateaccept+. However, the quick cycle differs from the algorithm in Fig. 1 in that there is an interface to the main state machine given in Fig. 8. These state machines communicate by means of two signals only, one for each direction of the communication: (i) The quick cycle state machine of node i generates thenextisignal by which it exerts some limited influence on the time between two successive pulses generated by the main state machine, and (ii) it observes the(T2+,accept)timer. This timer is coupled to the state accept of Fig. 8, in which the pulse synchronization algorithm generates a new pulse. The signal's purpose is to enforce a consistent reset of the quick cycle state machine (once the main state machine has stabilized). The feedback mechanism (i) makes sure that, during regular operation, the reset of the quick cycle does not have any effect on the clocks. This is guaranteed by triggering pulses (by means of the non-faulty nodes briefly changing thenextisignal to 1 and back to 0 again) exactly at the wrap-around of the logical clockLi, i.e., at the time whenLiis “increased” from the maximal clock valueK−1=2b−1(of a b-bit clock) to0=KmodK.Similar to Fig. 1, the transition conditions of the quick cycle ensure that the logical clocks never have a clock imprecision of more than one. To increase the frequency further, each node could increase the number of clock “ticks” generated in each iteration of the quick cycle by means of a high-frequency local clock (essentially, a watchdog timer together with a counter), at the expense of larger clock imprecision (see [13]).Before we show the complete main state machine, consider its basic cycle depicted in Fig. 7. Once the main state machines have stabilized, all non-faulty nodes will undergo the states of the basic cycle in rough synchrony. The states sleep,sleep→waking, and waking serve diagnostic purposes related to the stabilization process. The durationT2of the timer(T2,accept)triggering the transition from waking to ready is so large that the node will always be in state waking long before the timer expires. Thus, we can see that the basic cycle has an underlying structure that is very similar to the quick cycle. Due to the more complicated logic and conditions on the duration of timers required for the stabilization mechanism, it is however executed at a frequency that is by orders of magnitude smaller than that of the quick cycle.The difference in the rules for switching to propose and accept, respectively, are also mostly related to the stabilization process. An exception is the condition “T3andnexti=1” that can trigger a transition from ready to propose. ChoosingT3smaller thanT4and taking the signalnextiinto account, we permit the quick cycle to adjust the time between pulses (i.e., switches to accept) triggered by the main state machine: Once both state machines are roughly synchronized among all non-faulty nodes, the main state machines will always be in state ready before the logical clocksLimaintained by the quick cycle reach the wrap-around (i.e., become 0 modulo K) and trigger thenextisignals. Moreover, this happens at all nodes at close times and before any timer(T4,ready)expires at one of the non-faulty nodes. Hence, by a reasoning similar as for Fig. 1, all non-faulty nodes will switch to propose and subsequently accept in a well-synchronized fashion, caused by the wrap-around of the logical clocks.An important observation that is proved in [13] is that, once the main state machines stabilized, the nodes execute the basic cycle deterministically and any state transition is certainly completed before one of the conditions for leaving the basic cycle can be satisfied. Apart from small additional slacks in the timer durations, this is a consequence of the fact that none of the transition conditions of the basic cycle refer to the probabilistic lower layers of the protocol; all evaluated timers and memory flags solely involve states of the basic cycle only, and thenextisignal is provided by the quick cycle. As we will discuss in Section 6, this property prevents non-faulty nodes from introducing metastability once stabilization is achieved.We now turn our attention to the full main state machine that is shown in Fig. 8. Compared to the basic cycle, we have two additional states, resync and join, that can be occupied by non-faulty nodes during the stabilization process only, and an additional reset of memory flags on the transition fromsleep→wakingto waking.The various conditions for leaving the basic cycle and switching to recover are consistency checks. A node will only leave the basic cycle if it is certain that the system is not operating as desired. As the high-level operation of the algorithm is not the subject of this article, we limit our exposition to briefly discussing the two possible ways to re-enter the basic cycle, corresponding to two different stabilization mechanisms.The first stabilization mechanism is very simple, and it is much faster than the second one. Assuming that at leastn−fnon-faulty nodes are executing the basic cycle (i.e., the main state machines have already stabilized if we consider the remaining nodes faulty), a recovering node just needs to “jump on the train” and start executing the basic cycle as well. This is realized by the condition for switching from recover to accept. It is not hard to see that due to this condition, the node will switch to accept in sufficient synchrony with the majority ofn−fsynchronized, non-faulty nodes within at most two consecutive pulses and subsequently follow the basic cycle as well.Note that this condition makes direct use of the state signals instead of using memory flags. This potentially induces metastability at the joining node, but we will explain in Section 6 why the risk is low.1717Recall that during stabilization we cannot exclude metastability with certainty even in the absence of any further faults.On the plus side, this simplifies the algorithm, as the node does not need to implement frequent resets of the respective memory flags to ensure consistent observation of others' states; the sending nodes will just do this implicitly by leaving state accept.Clearly, the first stabilization mechanism will fail in certain settings. Most obviously, it cannot “restart” the system if all nodes are in state recover. Hence it may not surprise that the second stabilization mechanism, which deals with such cases, is much more involved. Careful attention has to be paid to avoiding the potential for system-wide dead- or live-locks. In view of our design goals, state-of-the-art deterministic solutions for this problem are not sufficiently efficient. Hence, the main state machine relies on a probabilistic lower layer that provides certain guarantees with a very large probability.The extension of the main state machine, given in Fig. 9, can be seen as a simple control structure for the phases of stabilization. The intricacy lies in designing the interface such that this control does not interfere with the basic cycle if the system is stable. Consequently, the influence of the extension of the main state machine is limited to (i) resetting the join andsleep→wakingflags upon “initializing” the stabilization process (by switching from dormant to passive) and (ii) providing the signals of the timers(T6,active)and(T7,passive)the main state machine utilizes in the transition rule from recover to join.Roughly speaking, the main state machines will stabilize deterministically under the condition that their extensions switch at all non-faulty nodes from dormant to passive in rough synchrony and then do not switch back to dormant too quickly, i.e., before the second stabilization mechanism of the main state machine completes its work. Putting it simply, we require a single, coarsely synchronized pulse, whose generation is the purpose of the lowest layer we present now.The resynchronization state machine is specified in Fig. 10. Strictly speaking, it actually consists of two separate state machines, one of which is however extremely simple. Every now and then, each node will briefly switch to the init state, seeking to induce the generation of a “pulse” (where the pulse here is locally triggered by switching to resync) that causes a consistent switch of all non-faulty nodes from dormant to passive. Leaving resync will force the extension state machine back into state dormant. This is the only interaction with the above layer, which is sufficient if a pulse is successfully generated once.The generation of a pulse is achieved by all non-faulty nodes following the advice of a single node switching to init, thus establishing the common time base required for a synchronized pulse. Two obstacles are to be overcome: possibly some of the non-faulty nodes already believe that the system is in the middle of an attempt to stabilize (i.e., they are already in state resync and thus not ready to follow the advice given by another node) and possibly inconsistent information by nodes that remain faulty (causing only some of the non-faulty nodes to switch to resync).In contrast to the higher levels, however, we are satisfied if only occasionally a successful pulse is generated. Hence, the above issues can be overcome by randomization. The source of randomness here is the randomized timer(R3,wait). The distributionR3and the logic of the second, more complicated state machine including the state resync are designed such that there is a large probability that within timeO(n)all non-faulty nodes will consistently switch to state resync. ThisO(n)is essentially the factor by which the second stabilization mechanism of the main state machine is slower than the first one.Clearly, in order for the protocol to operate as desired, the timer durations need to satisfy certain constraints. We state a feasible family of durations here; the minimal constraints that are required by the proofs are given in [13].Recall thatϑ>1and that d bounds the maximal end-to-end delay incurred between the time when a state transition condition is met and the time when the respective signal transition is observed at all receivers. As the logic of the quick cycle is much simpler than that of the other state machines, it typically permits much tighter upper and lower bounds on this end-to-end delay. As in [13], these bounds are denoted bydmin+anddmax+⩽d. In Section 6, we will discuss how d,dmin+, anddmax+can be computed out of the constituent delays incurred in our basic modules.Definingλ:=(25ϑ−9)/(25ϑ)∈(4/5,1)andα:=(T2+T4)/(ϑ(T2+T3+4d)),for anyϑ>1,α⩾1, the following family of timeout durations meets the requirements stated in [13] (see the reference for a proof):T1+:=6ϑ2d+6ϑ2dmax+−ϑdmin+T2+:=3ϑd+3ϑdmax+T3+:=6ϑ3d+6ϑ3dmax+−ϑ2dmin+T1:=4ϑdT2:=46ϑ3d/(1−λ)T3:=(ϑ2−1)46ϑ3d/(1−λ)+31ϑ3dT4:=46ϑ3(αϑ3−1)d/(1−λ)+35αϑ4dT5:=46ϑ4(αϑ3−1)d/(1−λ)+39αϑ5dT6:=46ϑ4d/(1−λ)T7:=92αϑ8d/(1−λ)+78αϑ5dand further,R1:=46ϑ6(3αϑ3−1)d/(1−λ)+109αϑ6dR2:=(92ϑ7(3αϑ3−1)/(1−λ)2+(218αϑ7+108ϑ3)/(1−λ))(n−f)dR3:=uniformly distributed random variable on[3ϑd+(92ϑ8(3αϑ3−1)/(1−λ)2+(218αϑ8+108ϑ4)/(1−λ))(n−f)d,3ϑd+(8(1−λ)+ϑ)(92ϑ7(3αϑ3−1)/(1−λ)2+(218αϑ7+108ϑ3)/(1−λ))(n−f)d].Finally, the maximal logical clock valueK−1is not arbitary, as we require(1)K∈[(46ϑ4/(1−λ)+52ϑ2)/(12+10dmax+/d),α(46ϑ4/(1−λ)+32ϑ2)/(12+12dmax+/d)].Note that, by manipulating α, we can make K arbitrarily large, but this comes at the expense of a proportional increase in the timer durations of the main state machine and its underlying layers, increasing the overall stabilization time.We conclude the section with a summary of the most important statements proved in [13], expressed in terms of the model employed in this article. To this end, we need to specify the protocol as a compound implementation about that we will formulate our theorems.Definition 5.1The FATAL+ ProtocolThe FATAL+protocol is a compound module consisting of nodesi∈{1,…,n}. It has no input ports and an output portLifor each node i. The n input ports of node i are connected to the output ports of the channelsSi,j,j∈{1,…,n}. Each node is comprised of one copy of each of the state machines presented in this section, and the implementation of each node is derived from the implementations (given in Section 6) of the basic modules defined in Section 2.2 that are connected as specified in this section. The output portLiof node i is the output port of its quick cycle state machine.The first theorem states a probabilistic stabilization result. Since we did not formally define probabilistically stabilizing implementations, its formulation is somewhat cumbersome. Intuitively (and slightly inaccurately), the statement is to be read as “no matter what the initial state and the execution, the protocol stabilizes almost certainly withinTslowtime”.Theorem 5.2Fix anyf′⩽f:=⌊(n−1)/3⌋and feasible α, setTslow:=(24(1−λ)+3ϑ)R2+R1/ϑ+T1++T3++(9ϑ+8)d+5dmax+−dmax−∈Θ(αn),and pickK∈Θ(αn)in accordance with inequality(1).Consider an execution on[t−,t+]of the FATAL+protocol where (at least)n−f′nodes are feasible. Assume that an adversary that knows everything about the system except that it does not learn about the durations of randomized watchdog timers before they expire controls all other aspects of the execution (clock drifts and delays of feasible submodules within the admissible bounds as well as the output ports' signals of faulty modules). Then the execution restricted to[t−+Tslow,t+]is with probability at least1−2−(n−f)a feasible execution of a clock synchronization module with clock imprecisionΣ=1, amortized frequency boundsA−=1/(T1++T3++3dmax+)andA+=1/(ϑ(T1++T3+)), slacksτ−=τ+=2, maximum frequencyF+=1/(ϑ(T1++T3+−2dmax++dmin+)), at mostf′faults, and clocks of sizeK∈Θ(αn).In this sense, for eachf′⩽f, the FATAL+ protocol is anf′-tolerant implementation of a clock synchronization module with the respective parameters that stabilizes with probability at least1−2−(n−f)within timeTslow∈O(αn).The above theorem corresponds to the slow, but robust, second stabilization mechanism. The next theorem, which corresponds to the faster first stabilization mechanism, essentially states that in an execution wheren−fnodes already stabilized, any further non-faulty nodes recover quickly and deterministically, withinO(α)time.Theorem 5.3We use the notation of the previous theorem. Moreover,Tfast:=T2+T4+(1+5/(2ϑ))R1+5d∈Θ(α).Suppose an execution of the FATAL+protocol is feasible on[t−,t+]with respect to the clock synchronization module specified inTheorem 5.2. Consider the set of nodesW⊆Nwhose restricted executions on[t−,t+]are feasible. Then the execution restricted to[t−+Tfast,t+]is feasible with respect to a clock synchronization module with the same parameters, except that it toleratesn−|W|faults only.We should like to mention that in [13] a number of further results on stabilization are given. In particular, if the faulty nodes exhibit only little coordination among themselves or do not tune their operations to the non-faulty nodes' states, also the “slow” stabilization mechanism will succeed quickly, granted that the resynchronization state machines are not in a “too bad” configuration, i.e., most timers of typeR2are expired and timeouts of typeR3are in (roughly) random states. We will informally discuss some of these scenarios in Section 7.Finally, we emphasize again that the power of the above theorems severely depends on the quality of basic implementations (cf. Section 4.2). While compound modules' properties can be formally analyzed, e.g. giving rise to the theorems above, these results are meaningless if too many basic implementations are infeasible too frequently. Hence it is vital to come up with robust implementations of the basic modules, which is the subject of the next section.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
In this work, we introduced a novel modeling framework for self-stabilizing, fault-tolerant asynchronous digital circuits and demonstrated its applicability to our recently introduced FATAL+ clock generation scheme for multi-synchronous GALS architectures. Our framework enables to reason about high-level properties of the system based on the behavior of basic building blocks, at arbitrary granularity and in a seamless manner. At the same time, the hierarchical structure of the model permits to do this in a fashion amenable to formal analysis. We believe this to be the first approach concurrently providing all these features, and therefore consider it as a promising foundation for future research in the area of fault-tolerant digital circuits.As the conclusion of our paper, we now assess to which extent the properties of our implementation of the FATAL+ algorithm, which have been expressed and verified within our modeling framework and tested experimentally, meet our design goals. Furthermore, we will discuss a number of potential improvements and future research avenues. Our exposition will follow the optimization criteria listed in Section 2.1.7.•Area consumption: For a suitable implementation, the total number of gates isO(nlogn)per node. This can be seen by observing that the complexity of the threshold gates is dominating the asymptotic number of gates, since theO(n)remaining components of a node have a constant number of gates each; using sorting networks to implement threshold gates, the stated complexity bound follows [48]. Trivially, this number of gates is a factor ofO(logn)from optimal. We conjecture that in fact this complexity is asymptotically optimal, unless one is willing to sacrifice other desirable properties of the algorithm (e.g. optimal resilience). Assuming that the gate complexity of the nodes adequately represents the area consumption of our algorithm, we conclude that our solution is satisfactory in that regard.Communication complexity: Our implementation uses 7 (1-bit) wires per channel, and sequential encoding of the states of the main state machine would reduce this number to 5. All communication are broadcasts. Considering the complexity of the task, there seems to be very limited room for improvement.Stabilization time: Our algorithm has a stabilization time ofO(n)in the worst case. Recent findings [49] show that a polylogarithmic stabilization time can be achieved at a low communication complexity; however, this comes at the expense of suboptimal resilience, a weaker adversarial model, and, most importantly, constants in the complexity bounds that make the resulting algorithm inferior to our solution for any practical range of parameters. Moreover, as formalized in [13] and demonstrated in Section 7, for a wide range of scenarios our algorithm achieves constant stabilization time. Considering the severe fault model, we conclude that despite not being optimal, our algorithm performs satisfactory with respect to this quality measure.Resilience: It is known that3f+1nodes are necessary to tolerate f faults [25,14] unless cryptographic tools are available. Since the complexity incurred by cryptographic tools is prohibitive in our setting, our algorithm features optimal resilience.Delays: As mentioned, the delay of wires is outside our control. Takingdmin+anddmax+into account in the quick cycle machine, we make best use of the available bounds in terms of the final frequency/synchrony trade-off. The delays incurred by the computations performed at nodes are proportional to the depths of the involved circuits. Again, the implementation of the threshold gates is the dominant cost factor here. The sorting network by Ajtai et al. [48] exhibits depthO(logn). Assuming constant fan-in of gates, this is clearly asymptotically optimal if the decision when to increase the logical clockLvnext indeed depends on alln−1input signals of v from remote nodes. We conclude that, so far as within our control, the design goal of minimizing the incurred delays is met by our algorithm.Metastability: We discussed several effective measures to prevent metastability in Section 6. Our experiments support our theoretical finding that, after stabilization, metastability may not occur in absence of further faults. However, since metastability is an elusive problem for which it is difficult to transfer insights and observations to other modes of operation of a given system—let alone to different implementation technology—a mathematical treatment of metastability is highly desirable. Our model opens up various possible approaches to this issue. For one, it is feasible to switch to a more accurate description of signals in terms of signals' voltages as continuous functions of time. Another option choosing an intermediate level of complexity would be to add an additional signal state (e.g. ⊥) for “invalid” signals, representing e.g. creeping or oscillating signals. Assigning appropriate probabilities of metastability propagation and decay to modules, this would enable a unified probabilistic analysis of metastability generation, propagation, and decay within a modeling framework using discrete state representations. Such an approach could yield entirely unconditional guarantees on system recovery; in contrast, our current description requires an a priori guarantee that metastability is sufficiently contained during the stabilization process.Connectivity: The algorithm presented in this work requires to connect all pairs of nodes and is therefore not scalable. Unfortunately, it is known thatΩ(n2)links are required for toleratingf∈Ω(n)faults in the worst case [26,27]. We argued for the assumption of worst-case behavior of faulty nodes; however, it appears reasonable that typical systems will not exhibit a worst-case distribution of faults within the system. Indeed, many interesting scenarios justify to assume a much more benign distribution of faults. In the extreme case where faults are distributed uniformly and independently at random with a constant probability, say, 10%, of a node being faulty, node degrees ofΔ∈O(clogn)would suffice to guarantee (at a given point in time) that the probability that more thanΔ/9neighbors of any node are faulty, is at most1−1/nc. Note that this implies that the mean time until this property is violated polynomially grows with system size. Using the FATAL+ protocol in small subsystems (of less than Δ nodes), system-wide synchronization will be much easier to achieve than if one would start from scratch. In this setting,Δ∈O(logn)would replace n in all complexity bounds of the FATAL+ algorithm, resulting in particular in gate complexityO(lognloglogn)per node, computational delayO(loglogn), and stabilization timeO(clogn)with probability1−1/nc. Thus, this approach promises “local” fault-tolerance ofΩ(Δ)faults in each neighborhood in combination with excellent scalability in all complexity measures, and realizing this is a major goal of our future work.Clock size: The constraint (1) entails that either clock size is bounded or large clocks result in larger stabilization time. This restriction can be overcome if we use the clocks of bounded size generated by FATAL+ as input to another layer that runs a synchronous consensus algorithm in order to agree on exponentially larger clocks [41].Overall, we consider the present work an important step towards a practical, ultra-robust clocking scheme for SoC. We plan to address the open problems discussed above in the future, and hope that this will ultimately lead to dependable real-world systems clocked by variants of the scheme proposed in this article.