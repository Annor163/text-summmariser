@&#MAIN-TITLE@&#
Full body movements recognition – unsupervised learning approach with heuristic R-GDL method

@&#HIGHLIGHTS@&#
We have used Gesture Description Language (GDL) for user actions recognition.A method for generation of knowledge base for GDL classifier has been proposed.The method analyzes unsegmented data recordings of gestures.The evaluation has been made on 770 samples of gym exercises.We obtained recognition rate at the level of 100% to 91%.

@&#KEYPHRASES@&#
Gesture recognition,Full body movements recognition,Unsupervised learning,Syntactic classification,Gesture Description Language,Reverse-Gesture Description Language,

@&#ABSTRACT@&#
In computer systems that are used for actions recognition the human movements are often represented by three-dimensional coordinates of body joints that are tracked by motion capture hardware. The motivation of our research was to propose a novel method for automatic generation of knowledge base for syntactic Gesture Description Language (GDL) classifier by analyzing unsegmented data recordings of gestures. We have proposed novel unsupervised learning approach to deal with this task. Because this process seems to be reverse engineering to GDL approach, the learning algorithm we introduce in this paper, is called Revers-GDL (R-GDL). The R-GDL machine-learning approach for full-body movements recognition is a novel method of time-varying multidimensional signals classification. The description of R-GDL and its validation is our original and never before published achievement. The evaluation of R-GDL was performed with k-fold cross validation on large dataset that contains 770 complete movements samples of 9 gym exercises performed by 14 persons and compared with results from multivariate normally continuous density hidden Markov model classifier. Depending on exercise type GDL obtained recognition rate at the level of 100% to 91%.

@&#INTRODUCTION@&#
Human movements or gesture recognition systems have been presented in literature and implemented for many years. In this scope, many scientific papers and commercial systems aim at the sign language analysis. This problem is especially important because efficient gesture recognition system of this type aids to interact with the deaf and deaf-mutes because very few people know how to communicate with person with disabled senses. From the other hand full body movements recognition (in this article we will use term full body gestures analysis as the synonym) was not so much examined. That was due to the fact that for many years systems (mostly hardware solutions) that enabled acquisition of gestural data (like motion capture technology) were relatively expensive and could not be easily set up in undedicated environment. In the last few years we can observe a boom on gesture recognition systems that is mostly caused by introduction to market new multimedia devices that uses infra-red projector and camera to generate depth maps (the most popular from this family is Kinect). The close look on methodologies of gestures recognition reveals that even though the problem of movements classification is well known and investigated for many years it is very difficult or nearly impossible to indicate the one leading methodology that deals with this problem and is reliable and in the same time applicable in commercial practice. The aim of this paper is to propose and present evaluation of a novel heuristic method that satisfies both of those needs.Although hand gesture recognition and full body movements classification seems to be different classes of problems both of them have much in common. Both of them deals will real-time analysis of human behavior and due to this fact often the similar approaches are used. Due to this fact the state-of-the-art knowledge about those approaches are crucial for every researcher.The first very important stage that has to be done before gesture analysis is extraction of features from input recording. In [1] the main objective behind using 15 Gabor filter responses is to reduce the complexity with better accuracy. In [2] hands postures are detected and hand gestures are recognized based on features of hand which are extracted using the Haar-Like features. The variety of object skeletons extraction methods from 2D [3] and 3D point cloud has been proposed [4–6]. Many researches choose formalisms that are invariant to localization of a data capturing sensor. For example work [7] introduces a new representation for Motion Capture data (MoCap) that is invariant under rigid transformation and robust for classification and annotation of MoCap data. This representation relies on distance matrices that fully characterize the class of identical postures up to the body position or orientation. In [8] each pose is described using an angular representation of the skeleton joints.The intrinsic dimensionality of human motion data is rather small compared to the typical dimensionality of the raw motion capture data. Therefore, motion modeling often begins with a dimensionality reduction step to enable generalization to novel motion from small amounts of data. It is very often done with PCA [9] (principal component analysis) or other approaches [7].After features selection and data transformation from original space into space determined by selected features there are large variety of computational intelligence methods that are used in process of gestures recognition. Among most popular are hidden Markov models (HMMs) [10,11], support vector machines (SVM) [1,8,12], decision forests [8,13], Gaussian process dynamical models [14], k-means clustering [2] or dynamic Bayesian networks [15].Some approaches use syntactic pattern recognition methods. Those classifiers requires special syntax for representing human gestures and sometimes rule sets that correspond to the basic spatial and temporal components of an action [16–19]. Creating domain-specific gesture recognizers of this type is not a simple task. It requires significant amounts of experimentation and training with large gesture corpora to determine a suitable set of features and rules set [2,20].The behavior analysis and modeling with computer-vision methods is novel and very interesting scientific problem. Some authors propose methods that can be applied to this task. For example in paper [9] authors used radial basis functions (RBF) and linear kernels associated with a nonlinear auto-regression model to learn a computer system interactions between a couple of actors and then replace one actor by the motion capture data of a human user during real time interaction. In [21] authors present a new approach for the learning of structured dynamical models for the synthesis of interactive body movements that is based on hierarchical Gaussian process latent variable models. The latent spaces of this model encode postural manifolds and the dependency between the postures of the interacting characters. In addition, this model includes dimensions representing emotional style variations (for neutral, happy, angry, and sad) and individually-specific motion style. Paper [22] combines an approach to the generation of culture-specific behaviors with full body avatar control based on the Kinect sensor. A study revealed that users are able to easily control an avatar through their body movements and immediately adapt its behavior to the cultural background of the agents they interact with.Besides issues we already mentioned there are also those that are crucial to make a gesture recognition method applicable in practice (for example in commercial products). At first many problems in vision involve the prediction of a class label for each frame in an unsegmented sequence [23]. In practice, the user is expecting from the system capability of recognition of his natural gesticulation. What is more lack of necessity of pre-segmentation (that is often manual or governed by a user) of training data highly reduces time that has to be spent to train classifier. Another important aspect is if interactive, action-based interface has low latency [24]. High latency causes the system's feedback to lag behind user actions and thus significantly degrades the interactivity of the user experience. In practice, the implementation of classifier should operate in real time (at least with the same speed as new data arrives from data acquisition hardware).As can be noticed from previous section there are several very important aspects that should be considered while choosing proper gesture classification approach. An optimal choice is a method that provides high recognition rate and in the same time operates on features that are invariant to rigid transform of observed object, has a stable training algorithm and operates in real-time on unsegmented (real-live) data. No less important are easily readable parameters and training results of the algorithm what make it possible to interpret what are decision borders between classes and to apply results to deep analysis of nature of some physical activity. In our previous researches we have introduced and evaluated Gesture Description Language (GDL) that is syntactic classifier that uses rule-based expert system approach for gesture description and recognition. The GDL showed to be efficient and effective classifier in multiple full body gestures recognition tasks like common – live gestures [16], natural user interface [25] and initially in karate techniques [18,19]. GDL incorporates all those aspects that are so important for gesture recognition system hover our solution lacked so far automatic training of its knowledge base (rules had to be tailored during implementation phase manually).The motivation of our research was to propose a novel unsupervised learning method for automatic generation of GDL knowledge base (that is written in context-free GDL script (GDLs) formal grammar) by analyzing unsegmented data recordings of gestures to be recognized. Because this process seems to be reverse engineering to GDL approach the learning algorithm we introduce in this paper will be called Revers-GDL (R-GDL). What is crucial the GDLs that is generated after this analysis preserves all most important features of manually – defined GDLs:–The GDLs generated by R-GDL can be very easily interpreted by a user and corrected/expanded/attached to any other GDLs file.The training of classifier is based on reliable unsupervised clustering method on unsegmented data recordings of gestures.The effectiveness of classifier is function of generated rules precision.The classifier runs in real time, which enable recognition with the same speed as data arrives from video capturing hardware.The GDLs description might be invariant to user position towards the data acquisition sensor.GDLs enables to define features set derived from raw skeleton tracking data.The GDL/R-GDL machine-learning approach for full-body movements recognition we propose is a novel method of time-varying multidimensional signals classification. In computer systems that are used for actions recognition the human movements are often represented by three-dimensional coordinates of body joints that are tracked by MoCap hardware. In our experiment we use Microsoft Kinect however we can utilize any type of device that is capable of body joints tracking. In case of cheap, multimedia MoCap devices the low sampling frequency (30 Hz or less) causes that the single action is registered on only several or several dozens of samples which makes frequency-based analysis very difficult or even impossible. The low-frequency MoCap data processing is still open and actual subject of researches and our methodology presented in this paper is one of the possible solution of this problem. Besides already mentioned features our methodology is additionally innovative because GDL methodology together with R-GDL training creates an unified solution in which features calculation, key-frames detection, signal identification and classifiers training are among single framework which common part are descriptions made with context-free GDLs grammar.The description of R-GDL approach is our original and never before published achievement. Together with evaluation and discussion of validation results of this methodology on large dataset that contains 770 complete movements samples of 9 gym exercises performed by 14 persons it is a content of the following sections of this paper. We have also compared classification results obtained with R-GDL/GDL approach with state of the art multivariate normally continuous density hidden Markov model classifier.The methodology that is utilized for solving signal identification problem is dependent on many factors. Those are inter alia the dimensionality of problem, features that are used to describe the phenomena or spectra characteristic of discrete samples. In this paragraph we will describe the characteristic of our dataset and we will introduce the methodology we propose to deal with full body gesture classification problem.Our classifier is trained and validated on so called skeleton recordings (SKL-files) that are initially processed data from depth camera of data acquisition sensor. In our case we store detection results of third-party image processing library (Microsoft Kinect SDK) that automatically detects 20 points on human body (so called body joints) and track them with the approximated 30 Hz frequency – see Fig. 1on right. The tracking frequency might be lower than 30 Hz if Kinect SDK image processing is delayed by other background threads of operating systems. Those fluctuations of tracking speed did not appear in our dataset. Each SKL recording frame stores three-dimensional position of each one of 20 body joints. The detection and tracking procedure is marker-less. Our dataset satisfies the following assumption:1.It contains recordings of each gesture to be recognized.The same gesture is performed by multiple people both man and woman.Each person preforms a single body movement periodically (five or more times).Recording of a person performing periodically single gesture is stored in separate SKL file. Because of that each SKL file can be assigned to particular person and particular body movement.There is no need to partition a single recording SKL to smaller samples. For instance if SKL file contains 10 repetitions of body weight squads we do not have to segment beginning and end frame of each exercise. This assumption is very important because it highly reduces the time of test and validation set preparation especially where there are several hundred or thousands of body movements samples. However, for reasons that we will revel later we segment not more than 5% exemplars of each type of movement for additional evaluation.All gestures are correctly performed that means each gesture in dataset can be assigned to single or multiple classes.While tailoring gesture recognition classifier we must be aware the number of aberrations that might appear in SKL recordings. Those aberrations might be partitioned into two groups: individual features of filmed participants and imperfection of data capturing hardware and processing software. Among first group most important factors are suppleness and physical condition of a user. After several repetitions of exhaustive physical exercise the speed and precision of body movements differs in comparison to beginning of the training session. Among second group most difficult to overcome are inaccuracies in body joint segmentation especially noticeable when a body part to be segmented is covered by another body part. That is quite frequent while performing natural physical activates. This is mainly the problem of segmentation algorithm rather than accuracy of the depth map, which is sufficient for this task [26,27]. Of course those two types of phenomena are present in all real-world (not artificially created) datasets and should be considered both in training and validation dataset.The user data gathered by Kinect and tracked by Microsoft Kinect SDK library already contains quite heavy tracking noises. According to research [28] in a more controlled body posture (e.g. standing and exercising arms), the accuracy of the joint estimation is comparable to motion capture. However, in general postures, the variability of the current implementation of the pose estimation is about 10 cm which is large value comparing to the size not only particular limb by the whole user's body. The measurements could be used to assess general trends in the movement, but for quantitative estimation an improved skeletonization with an anthropometric model is needed. Due to this fact the further classification is already made under noise and we did not find necessary to introduce additional signal disturbances to our dataset for the methodology evaluation purposes.The GDL classifier was described in details in our paper [16]. In this section we will only summarize the basic assumptions under this approach.The very heart of our approach is an automated reasoning module. It performs forward chaining reasoning (like a classic expert system) with its inference engine every time new portion of data arrives from the feature extraction library. All rules of the knowledge base are organized in GDL scripts (GDLs) having the form of text files that are parsed with a context-free grammar. The input set of body joints and all conclusions obtained from knowledge base are put on the top of the memory stack. In addition, each level of the stack has its own timestamp which allows checking how much time has passed from one data acquisition to another. Because the conclusion of one rule might form the premise of another one, the satisfaction of a conclusion does not necessarily mean recognizing a particular gesture. The interpretation of a conclusion appearance depends on the GDL script code.Let us assume that we have time varying3⋅d-dimensional MoCap signalP[ti..tj]sampled in discrete time momentsti,ti+1,…,tj(i<j).(1)P[ti..tj]=[pti‾,…,ptj‾]where:(2)pta‾∈R3⋅dd is number of body joints, 3 is number of dimensions in Cartesian frame per joint,a∈[i,j].As we already mentioned in Section 1.2 we have to recalculate each signal samplepta‾to make it invariant to relative position to camera. We call that step features calculation. Features are numeric values derived from MoCap data with functionF.(3)P[ti..tj]→Fftj‾wherefta‾∈Rm,a∈[i,j].FunctionFis defined by a set of GDLs Features expressions (see Appendix A).After applying (3) to (1) in each moment of time we get:(4)F[ti..tj]=[fti‾,…,ftj‾]The next step is calculation of rules' conclusions. Each of r-rules that are defined in GDLs might be either satisfied or not:(5)rta‾∈{true,false}rwhererta‾is a feature vector that contains all conclusions of rules. It is possible that conclusion of one rule is present as a premise of another one (see GDL MoCap data processing algorithm – b).We define:(6)R[ti..tj]=[rti‾,…,rtj‾]And:(7){F[ti..tj],R[ti..tj−1]}→ℜR[ti..tj]where ℜ is a function defined by a set of GDLs Rules expressions (see Appendix A).The top of GDL memory stack in timetais defined as:(8)sta‾={pta‾,fta‾,rta′‾}whererta′‾is a vector that contains only those rule's conclusions that are satisfied (true) at a moment of timeta.Eventually GDL stack S is defined as:(9)S[ti..tj]=[sti‾,…,stj‾]Fig. 2presents a flowchart of GDL. Symbol<<trigger>>means that data flow activates a given processing block, but data for the operation in a block is taken from GDL stack.The GDL performs according to the following algorithm (see also Fig. 1 and Fig. 2):GDL MoCap data processing algorithm1)Parse the input GDL script that consists of set of rules and feature, generate a parsed tree.Begin data capturing.Repeat the above instructions until the application is stopped:a.If a new skeleton arrives from the data capture algorithm, store it on the top of the memory stack with the current timestamp. Calculate values of all features defined in GDLs and store their values on the same level of memory stack as skeleton. Each level of the memory stack contains three types of data: a set of body joints, a set of features values and a set of rules names that was satisfied for the current/previously captured skeletons and features and the current/previously satisfied rules. Skeleton, features values and names of rules satisfied in previous iterations lie on memory stack layers corresponding to a particular previous iteration. The size of the memory stack in our implementation is 256 layers (about 8.5 seconds of data capture at the frequency of 30 Hz). We did not find a “deeper” stack useful, but because data stored in the stack is not memory consuming, the stack might be implemented as “virtually unlimited in depth”.Repeat until no new rules are satisfied•Check if body joints and features values satisfy any one rule in the memory stack whose name is not present in the top level of the memory stack. The GDLs may also consider joints from previous captures; for example Spine.x[0] is the actual x-coordinate of the torso joint while Spine.x[2] is the x-coordinate of the same joint but captured two iterations before – two levels below from the top position of the memory stack. Rule truth might also depend on the current and previously satisfied rules.If any rule is satisfied, add its name to the top of the memory stack at the same layer at which the last captured skeleton points were stored (top level of stack). As each level of the stack has its timestamp, it is possible to check if some rule was satisfied no earlier than a given period of time ago. Thanks to this mechanism, it is possible to check if the particular sequence of body joint positions (represented by rule names) appeared in the given time period. The sequence of body positions defines the gesture that we want to recognize.The biggest challenge in using GDL approach is preparation of GDLs description. While the description of gestures requires 3 to 4 features one can very easily handle it manually. However when the gestures are some sophisticated full-body activities with many constraints on body joints relative positions the problem becomes more difficult. Knowing that we tailored a new method for automatic generation of GDLs files that analyzed set of SKL recordings with unsupervised learning methods (R-GDL). To make R-GDL consistent to GDL approach we have to make some extension to GDLs 1.0 language from [16] and we introduce GDLs 1.1 specification, that is described in the following section.The GDLs 1.0 specification was complete and well-designed. However during usage several suggestions appear about possible extension of its syntactic potential. The most common request was possibility of defining numeric variables which will have similar form of GDLs rules. The second common postulate was to include possibility of checking persistence of rule conclusion on memory stack (how long given rule is present in different levels memory stack). In Appendix A we have presented formal definition of GDL 1.1, but only those elements that are extension of GDL 1.0. What is important those new syntactic elements suits very well with the needs of extension of GDLs to keep it compact with new paradigm of R-GDL, which will be presented in next section.In GDL approach we have the assumption, that a movement can be represented as a sequence of key frames. If those key frames play some crucial role for gesture identification they should also be “remarkably visible” in a future space. That means if a user is capable to find out what other person is doing by detection of some important phases of movement computer should also be capable to do so if we indicate proper features to be analyze. If those key-frames appear for all persons performing some movement they should be detected as centers of clusters in feature space where data to be clustered is set of SKL frames. The next issue is to choose the clustering algorithm that is sufficient for this type of analysis. We will discuss this in the following section.We are interested in detection of central points of clusters (Gaussian distribution model is reported to be applied in human movements analysis [23]) which will create the root of key frames. Because we want to classify human movements we can more or less estimate how many important key-frames are present in each movement activity. We have a large spectrum of clustering methods however many of them can be excluded from consideration because they requires a prior hard to determine (or varying between datasets) knowledge about SKL recordings. The hierarchical based clustering might not be a good choice because we are looking for centers of clusters and each cluster might have different distribution of elements which make proper definition of linking/splitting criterion very difficult. Also density-based clustering methods like DBSCAN requires prior knowledge about threshold values of considered densities. We have assumed that the data has a Normal distribution (Gaussian mixture model) however all parameters (mean vectors, covariance matrix and prior probabilities for each class) are unknown. Provided that we can estimate the probable number of key-poses (clusters) we can use k-means clustering approach to solve this problem. k-Means clustering is a very popular approximate method that is often use to deal with maximum-likelihood estimates of means [29]. This method often allows fast convergence and does not require additional assumptions on data model.The GDL classifier trained with R-GDL method works as follows. Let us assume that we have time varying m-dimensional signalF[ti..tj](4).A signal sample belongs to clusterCb(μb‾,σb‾,εb‾)when:(10)fta‾⊂Cb⇔|fta‾−σb‾|<‾μb‾+εb‾whereμb‾is a mean vector (center of cluster),σb‾is a standard deviation of cluster's elements andεb‾together withσb‾determines size of a cluster.<‾means that all coordinates of vector in the left side have lower value than corresponding to them coordinates in the right side (both vectors have the same dimension).GDL classifies signalF[ti..tj]to classCcwhen:(11)GDL(F[ti..tj])=Cc⇔{ftl1‾⊂Cc1∧⋯∧ftln‾⊂Ccn,tl1<⋯<tlntj−tln<tcn∧⋯∧tl1−tl2<tc1where[Cc1,…,Ccn]are n-element set of clusters (key frames) of an action,[tc1,…,tcn]are time constraints assigned to each key frame and[tl1,…,tln]are moments of time when particular samples belongs to a given cluster. We can simply prevent the situation that GDL classifier detects particular action multiple times in neighboring frames by adding constraint that next action of a same type cannot appear in time shorter thantcn.In R-GDL the initial configuration for k-means clustering is:–SKL file with movements exemplar on which classifier is trained.Input GDLs file with definitions of features. The clustering algorithm will be running in m-dimensional space where m is a number of features defined in this file. Those features are computed for all body joints in each SKL file frame. Clustering does not take into account time order of frames.Number of clusters (k) – this number has to be estimated by the user basing on his expectation about the optimal number of key frames.“Epsilon values” that widen the margin of cluster borders estimation.Initial features definition.Features generated by R-GDL that is:∘epsilon values for all features,centers of clusters computed as average value of all elements assigned to them (all features have the same weight),spatial dimensionality of clusters computed as standard deviation value of all elements assigned to them (all features have the same weight).Key frames definition: each key frame corresponds to the single cluster. Single key frame has the following pattern of rule definition (compare with (10)):RULE abs(feature 1 value – average value of feature 1 in given cluster) <= standard deviation of feature 1 in a given cluster + epsilon value for feature 1AND (...)AND abs(feature m value – average value of feature L in given cluster) <= standard deviation of feature K in a given cluster + epsilon value for feature LTHEN Key frame for a given clusterKey frames are order into sequences in the following form (compare with (11)):RULE key frame C& sequenceexists(”[key frame B, time of persistence of key frame B](...)[key frame A,Time of persistence of key frame A]”)THEN ConclusionThe previous rule will be satisfied as long as “key frame C” rule is satisfied and the sequence satisfies the time constraints. However if we would like to force GDL classifier to return the conclusions of gesture appearance only once per gesture that might be done with the following construction:RULE Conclusion & sequenceexists(”[!Conclusion,1]”)THEN Detected gesture nameThis is especially useful when we want to use generated script not only to check if the particular gesture exists in given SKL recordings but also to count how many time it appeared. An example complete GDLs description of a gesture generated by R-GDL approach is presented in Appendix C.The computational complexity of R-GDL is the same as k-means algorithm:O(tmni)[29] where t is number of training patterns, m is number of features, n number of clusters, i is number of iterations. The “precision” of final solutions (the difference between estimated parameters of normal distributions and “real-world” parameters) depends on motion-capture tracking quality of training samples. The more tracking inaccuracies samples contains the worse results (further from “real-world” value) we get. Also the recognition rate of GDL that operates on GDLs generated with R-GDL depends directly on k-means approximation results. That is because GDLs rules utilize derived parameters of normal distributions.The computational complexity of GDL that operates on rules generated with R-GDL is:O(sn+nmr)where s is a size of a GDL stack, n is number of classes (clusters), m is number of features and r is number of rules per class. The first part sn expresses complexity of calculation of sequenceexists functions that appears in n rules (there are the same number of rules with sequenceexists as classes). The second term nmr expresses a complexity of calculation of all rules.The next step after detection key frames of movements is ordering them in into sequences. The problem is challenging because unsupervised learning supply only information that particular key frame is situated statistically in the center of a cluster. The k-means algorithm with random initialization does not also guarantee that results of the data partitioning will return results accordant to our intuition. Of course it is possible to visualize several SKL frames that belongs to detected clusters or to determinate the order of key frames by checking it with dedicated application [30], however we will still not know what are the maximal time spans that passes between one key frame to another. The information of those time spans is crucial for creation of gesture sequences that are basis of GDLs descriptions.Let us assume for simplicity that the SKL recording was portioned into three clusters. We represent those partitioning by three key frames: key frame A, key frame B and key frame C. We can now expect that the movement can be described by those key frames and each one of them appears at least once in the description in some order for exampleC→A→B. However it is possible that the sequence might also contains more than one key frame on single type for exampleC→A→B→CorC→A→B→A→C. Knowing that we can consider the sequences of key frames that appears in the recording as series of n-grams which probability (estimated by a frequency of appearance in training set) should indicate the “real” order in movements to be classified. According to this definition probability of n-gram of gesture NG equals quantity of NG n-gram in considered dataset divided by quantity of all n-grams in this set.(12)P(NG)=#NG#alln-gramsOf course in practice the output sequences we obtain from GDL reasoning module looks for example like this:(13)S=[…,C,C,C,X,X,A,A,A,X,X,B,B,…]where X means that particular SKL frame is not assigned to any cluster. That is because quite often a number of neighboring SKL frames are assigned to the same cluster because body joints positions between them do not differ much. Together with sequence S we also have sequence T that holds the information how much time has passed since last SKL frame:(14)T=[t0,…,tk,tk+1,…,tn]wheret0=0,tk= time that passed between SKL frametk−1andtk,tn– time that passed between frametn−1andtn.For us the most important information is an event of transition of SKL frame between one cluster to another and time that passed from one transition to another. The transformation of (13) to the form that we can process can be done with the following algorithm:n-gram generation algorithmTime:=0NS:=EMPTYNT:=EMPTYJ:=0ActualCluster:=XFORallelementsSiinSIF(ActualCluster==X)IF(Si!=X)NSj:=SiNTj:=TimeActualCluster=SiELSETime:=Time+TiENDIFELSEIF(ActualCluster!=Si)IF(Si!=X)j:=j+1NSj:=SiNTj:=0ActualCluster=SiENDIFENDIFNTj:=NTj+TiENDIFENDFORNow NT contains sequence of clusters that can be easily used to compute (12).The comparison of probability of particular diagram type (for example 3-gram, 4-gram or 5-gram) might be not sufficient to correctly identify the proper order of key frames. That is because often the less numerous n-gram is more probable than the one with more elements. For example in the case of key frames sequence:ABCABABCABP(AB)=49whileP(ABCAB)=26.Of course without partitioning training dataset into one-gesture samples it is impossible to determine if AB orABCAB(or any other n-gram that is present here) is a correct answer. We assumed in Sections 2.1–5, that we do not want to do this type of partitioning to whole dataset. To check the correct order of frames we can however utilize some small part of training dataset (for example 5% of samples) and take longest n-grams that appear in most of the samples. The n-gram analysis on whole training dataset might not be much in use for finding appropriate order of frames and quantity of “n”, however it supplies us with information about time spans that passes between each key frame in sequence (NT). We can make “safe” estimation that timetA→Bwhich passes between key frames A and B is:(15)tA→B=maxi∈N⁡(tiA→B)wheretiA→Bis time that passes between i-th transitions from class A to B in sequence NS for a considered n-gram.ThetA→Bis also quantized to satisfy the condition:(16)mod(tA→B,tq)=0wheretqis quantization time. The quantization simplifies the manual analysis of obtained results.In this section we describe the setup and results of validation procedure of our R-GDL methodology. We also compare classification results obtained with R-GDL/GDL approach with state of the art multivariate normally continuous density HMM classifier.R-GDL was validated on SKL files dataset that contains a group of adult people with age 25+ both men and women that performed gym worm-up and fitness exercises. The dataset consists of SKL recordings for 14 participants, 4 women (W1–W4) and 10 men (M1–M10) – W means a woman, M – man, numbers defines id of a participant. The exercises that were performed were: body weight lunge left (bwll), body weight lunge right (bwlr), body weight squat (bws), dumbbell bicep curl (dbc), jumping jacks (jj), side lunges left (sll), side lunges right (slr), standing dumbbell upright row (sdur), tricep dumbbell kickback (tdk). In Table 1we have presented quantities of gestures of a given type that was performed on a given SKL recording by each person. As can be seen not every person have performed each gesture, also the numbers of gestures are not equal. That is because that recordings were made in a certain period of time and not all users were asked to perform all gestures (for example in four recordings bws was skipped). Those lacks were then completed by recordings from other four persons in order to complete the dataset. Each person was asked to perform those exercises how many times he is capable to, but not more than 10 (in order not to get too tired for next exercises). There were some people who made those exercises more than 10 times (for example M1). For the other hand many participants, were getting tired more quickly and it was decided to reduce number of repetitions to 5 of each type. The participant M4 after perfuming slr was not capable to perform sll correctly. The test set was portioned into 10 groups of recordings (the indexing number of test set is presented in second column of Table 1). We have used 14 SKL recordings (datasets) coming both from men and woman (W and M respectively). This partitioning is shown in first column. Those dataset was used to crate 10 test datasets for k-fold cross validation. The division on test datasets is shown in second column. Each test dataset besides one (the fourth) contains each type of examined gestures. The other columns contain quantities of gestures (exercises) of a given type that was performed in a given SKL recording. The criterion of division was that each set consists of SKL recordings from a single person. The movement samples of this person are not present in other sets. In case of users who were not performing particular gesture (bws in M3, M4, M5 and M8) those sets were complement with W3, W4, M6 and M9.In Fig. 3we have presented digitalized pictures that demonstrate important phases of each activity (they are not a GDLs key frames). We did not show body weight lunge left and side lunge left, because they are analogous to right ones.In order to generate GDLs we used R-GDL methodology presented in Section 2.4. The k-fold cross validation was used to evaluate the results, wherek=10. Each gesture was trained separately and the results from all clustering of the single dataset were integrated into one GDLs that contained definitions of all nine gestures. The final validation was performed on these integrated GDLs. We arbitrary set epsilon parameter to 10 in considered cases. Because all features we used were based on angles (angles in GDL are calculated in degrees from range[0,180]) this value is about 10% of overall possible range of angle values. We must remember that the data capturing hardware we used for data acquisition is a multimedia device that might be highly inaccurate. The quantization time (16) is set totq=0.5sec.The number of clusters in all cases was set to three. In our case the choice of three clusters is arbitrary however it was forced by the low quality of tracking data we are dealing with (we already discussed this in Section 2.1). It is a quite an obvious observation that the more clusters are used to govern the movement there will be smaller number of movements incorrectly classified to a given class. We have to remember however when there are too much classes the whole method is more sensitive to noises and might lose its generalization ability. As we already mentioned the variability of pose estimation is about 10 cm for each joint. Due to this fact we decided to choose the smallest number of clusters that are sufficient to describe dynamic of movement. If we have chosen 2 clusters most probably we would detect only the initial and middle part of movement where are the largest number of samples in SKL recordings. It can be illustrated in side lunge exercises (see Fig. 3) where user spends most of time in stretching position with one knee joint bent and the other is stretched. The choice of three clusters allows us to detect the middle position of the movement with low risk of finding outliners instead. The good recognition rate obtained in further experiment assures us that our initial assumptions were correct. In order to use larger number of cluster we would have to gather dataset with better tracking quality (for example from professional motion capture hardware) which would be very interesting goal for future research.To initialize the k-means method we used random partition method (all elements are randomly assigns to a cluster). Because of that we have repeated our training procedure three times and choose the results from which recognition count on training (not a validation) dataset was the highest. In rest of this evaluation we present only results of this chosen dataset.The features we used for training are presented in Appendix B. We intentionally have chosen the relatively small subset of all possible (and reasonable) features that in our opinion might be suitable to obtain high accuracy of classification and to avoid “curse of dimensionality” during clustering. All of those features are invariant to rotation (they are angles defined between vectors determined by body joints). The only exception is “Bowing” features, that checks angle between “spine” and arbitrary chosen vertical axis and “SideDirection” computes angle between pelvis and horizontal axis in tdk activity that are not invariant to rotation.The example clustering results (for one of the test sets) with assignment to GDLs key frames symbolized by different colors (yellow, cyan and dark red) is presented in Fig. 4– key frame definition is presented in Section 2.4.1. Blue elements are not assigned to any key frame. The dimensionality is reduced to three dimensions with PCA. The visualization represent from left to right: first row – bwll, bwlr, bws exercise; second row – dbc, jj, sdur; third row – sll, slr, tdk. Those visualizations clearly show that some movements where based on limited number of “most important” muscles – joints (or group of muscles), that were bending and expanding in all possible range (like in case of bws that was knees, in case of dbc, tdk and sdur elbows) and the projected samples are mostly extended along single dimension. Other cases were more complicated. Especially difficult was sll and slr which additionally introduced noises caused by imperfection of methodology of data capturing and body tracking. The extreme tracking inaccuracies are clearly visible in bwlr and blwr as outlier elements.In Fig. 5. we present the example frames that were assigned to each of the cluster from Fig. 4. As can be seen agreeable to our expectations those results corresponds to our “intuitive guesses” from Fig. 3. The skeleton visualizations are based on SKL recordings of test set. We did not show body weight lunge left and side lunge left, because they are analogous to right ones.After key frames detection we determined order of key frames by analyzing 3 exemplars of movements samples that we additionally segmented for each class of gestures. Of course all of those samples were taken from training (not validation) dataset. The time periods between key frames were computed using n-gram generation algorithm on whole training dataset (see Section 2.4.2).In Table 2we have presented validation results of our 10-fold cross evaluation. The evaluation was performed on unsegmented sequence of samples, where a person was performing the movement periodically. The class names in first column is an actual condition, columns 2–9 presents obtained results by GDL classifier (for example98%±6%of bwll exemplars were correctly classified while2%±6%where misclassified as bwlr). The rows in table do not sum up to 100%. Sums might either exceed or not this value. That is because GDL classifier is capable to detect multiple classes in the same recording sample (it is not a binary classifier); due to this fact we cannot measure its performance with precision-recall factors. This is very important feature of GDL, however in the case of our dataset we take an assumption that none of the movement should be treated as part of another one. Because of it two sll movements might be interpreted by single slr and vice versa (it will be explained in Section 3.3). We can see this kind of error (slr found in the sequence of sll) in13%±14%cases. This type of error (or “unwanted behavior”) can be easily minimized by introducing GDLs heuristic that will be described in the following section.We have compared our results with multivariate normally continuous density HMM classifier. HMM classifier was composed of 3-state forward-only HMM and was trained to recognize the same set of classes as previously presented GDL method (each HMM in classifier was trained to recognize single body gesture). We have used the same features set as in GDL classifier. We have utilized Baum–Welch learning method. The evaluation was done with 10-fold cross evaluation however the data in validation set has been previously sampled that each data sample consisted only one gesture (so data has been previously segmented in contrary to unsegmented validation of R-GDL approach). The choice of forward-only 3-states HMM architecture was based on similar assumptions as we made we in Section 3.2 about clusters count and because forward-only architecture already proved its usefulness in gestures classification [11]. In Table 3we have presented validation results of our 10-fold cross evaluation of HMM classifier. The rows in table sum up to 100% because our HMM classifier assigns one and only one label to each data sample.Results in Table 2 showed that in13%±14%cases the sll class was confused with slr. This problem can be solved by three possible approaches:1)If we want to exclude the possibility that one class of movement is a subset of another one we can simply clear the GDL stack any time the gesture of a given type is classified (this can be simply done with software API we have implemented).The second possibility is to introduce heuristics that are based on our knowledge on nature of movement.We can redo the features selection for the classifier for example by adding features that represents movements of body parts that will most probably help with distinguish gestures.The first approach would solve slr/sll confusion problem however that approach might disturb the recognition process of a gestures that might be also attached to our GDLs and which is a part of other gestures. Of course it is possible to run many instances of GDL classifier and separate the rules to those that are exclusive or not. We decided however to show that second and third approach, that uses only GDLs 1.1 instructions and does not require knowledge about implementation of classifier.The confusion of sll with slr is caused by the fact, that R-GDL rules do not investigate the direction of movement, but only appearance of particular key frames. In case of sll in initial phase the person is putting the weight on the left leg by moving body to the left side (in case of slr movement is in opposite direction). The final extreme body position are governed by angle values computed by R-GDL, however, during the phase of movement where body is returning from the extreme right or left positions the considered angles in knees and hips might be similar to those that are in key frames of slr in case of sll and slr in case of sll. Due to this fact during periodic repetition of sll and slr the additional incorrect recognition of the other movement might appear. In order to eliminate these phenomena, we can supply the GDLs with additional information about direction of movement by including the following rules:RULE Spine.x[0] - Spine.x[1] > 10THEN MovementsRightPartRULE Spine.x[0] - Spine.x[1] < -10THEN MovementsLeftPartRULE rulepersists(MovementsLeftPart,0.25,0.5)THEN MovementsLeftRULE rulepersists(MovementsRightPart,0.25,0.5)THEN MovementsRightRulepersits function governs the direction of movement and check if it is performed to the left or the right side (for detailed information about this function check Appendix A). Now the conclusion of appropriate rule have to be added to key frame definition (MovementsLeft for sll and MovementsRight for slr). Our GDLs implementation can be found in Appendix C. After introducing the above heuristic we have found that error classification of sll to slr was reduced by 11% to the level of2%±5%.The third case is typical tuning procedure of features set. By adding more features we add more constraints on data that might help in distinction between gestures classes. However we must remember that adding more constraints on data might cause lowering the ability of classifier's generalization. We have re-trained our GDL classifier for slr and sll classes using extended features set for sll/slr from Appendix B. After this we have found that error classification of sll to slr was reduced by 7% to the level of6%±10%. Neither second nor third approach however did cause increase in recognition rate of sll and slr.

@&#CONCLUSIONS@&#
