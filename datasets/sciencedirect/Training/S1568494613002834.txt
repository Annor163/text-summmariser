@&#MAIN-TITLE@&#
Combining classifiers using nearest decision prototypes

@&#HIGHLIGHTS@&#
We proposed a combining classifiers system which conquers the inefficiencies of Decision Template method.Our method extracts some decision prototypes to better represent the decision space.Our method outperforms Decision Template method specifically in the face of small sample size problem.

@&#KEYPHRASES@&#
Decision prototype,Decision templates,Classifier fusion,K-Nearest neighbor,

@&#ABSTRACT@&#
We present a new classifier fusion method to combine soft-level classifiers with a new approach, which can be considered as a generalized decision templates method. Previous combining methods based on decision templates employ a single prototype for each class, but this global point of view mostly fails to properly represent the decision space. This drawback extremely affects the classification rate in such cases: insufficient number of training samples, island-shaped decision space distribution, and classes with highly overlapped decision spaces. To better represent the decision space, we utilize a prototype selection method to obtain a set of local decision prototypes for each class. Afterward, to determine the class of a test pattern, its decision profile is computed and then compared to all decision prototypes. In other words, for each class, the larger the numbers of decision prototypes near to the decision profile of a given pattern, the higher the chance for that class. The efficiency of our proposed method is evaluated over some well-known classification datasets suggesting superiority of our method in comparison with other proposed techniques.

@&#INTRODUCTION@&#
Nowadays, decision making, especially classification, has a wide range of applications in different scientific researches and industrial projects. Classification is considered as a procedure to find a class label c∈C={c1, c2, …, cm}, for a pattern x=(x1, x2, …, xk) in a k-dimensional problem space Rk. Over the last decades, many classifiers with different viewpoints have been introduced to conquer various classification problems. Statistical classifiers such as Nearest Neighbors[13], Parzen Windows[13], naïve Bayes classifier [13], and Support Vector Machines[10], and Neural Networks such as Multilayer Perceptron (MLP) [16] and Radial Basis Function (RBF)[7] are the most frequent classifiers in the pattern recognition literature. Moreover, numerous optimizations are made to improve the accuracy and efficiency of classification, among which Combining Classifiers or Multiple Classifier Systems (MCS) are known to have interesting advantages [25,22].The basic idea behind the classifier combination is inspired from the human tendency to seek several opinions for making important decisions. Let d=(d1, d2, …, dk) be a set of classifiers, combining classifiers means to find the label of x based on the classifier decisions d1(x), d2(x), …, dL(x). In general, ensemble systems have more generality, resistance and accuracy than a single classifier [33,23].Numerous combining methodologies with different perspectives have been established showing proper efficiency in a variety of problems. Nevertheless, a blind combination may not necessarily lead to proficiency improvement. In other words, various conditions for basic classifiers such as diversity and accuracy should be satisfied until their combination improves the final performance. An extra requirement that affects the final performance is combining methodology, meaning that an appropriate combining methodology having the most compatibility with the problem space should be chosen [29,40].However, among all combining methods proposed in the last decades, Decision Templates (DT) based methods have special attributes that make them suitable for various applications [26,24]. This method models the behavior of basic classifiers according to their final real outputs. The operation of the method is based on constructing a characteristic template for each class and comparing them with outputs of basic classifiers on the input data. In contrast to most other classifier fusion methods that make their decisions based on corresponding outputs for a particular class, DT uses all of the classifier outputs to compute the final decision about that class.To date, some studies have proposed to improve the strength and accuracy of DT method. Hady and Schwenker [1] introduced a Decision Templates based on RBF network which utilizes a clustering method on training Decision Profiles (DP; the output of basic classifiers on an input data) of each class to find the RBF centers. The RBF network is considered as a combiner that fuses basic classifiers using nearest centers. The advantage of this method is caused by using more than one template for each class. Haghighi et al. [15] extend DT by exploiting the outputs of intermediate layers of neural network basic classifiers. For each layer, they construct representative templates for all classes. In testing phase, for every hidden layer the similarity between the DP of a given test sample and DTs of each class is computed. To make the final decision, a voting method is applied on the results obtained from all hidden layers.A more intelligent decision technique can significantly affect on DT's performance, Javadi and Sharifizadeh [21] suggested a finer representation of decision space which led to performance improvement by adding the DTs attained from misclassified samples to the DT set. In Weighted Multiple Decision Templates proposed by Ghaemi et al. [14], first the input space is partitioned into several subspaces and then the DT of each subspace is computed. For a given test pattern, higher weights are assigned to DTs closer to the DP of the testing pattern and low weights to those placed farther from the DP. Most of the methods mentioned above try to employ extra DTs to compensate lack of accuracy and sufficiency of classic DT, which relies only on a single prototype for each class. In these methods, higher classification rate is achieved through selecting more representative decision templates per each class.One of the main characteristics of DT is its global point of view toward decision space which precludes it from distribution information about DPs. This may lead to low efficiency for problems with small sample size, wide spread decision space, etc. This disadvantage can be resolved through employing a set of local decision templates (we say Decision Prototypes (DPRs)) that can properly represent the distribution of decision profiles.In this paper, we propose a new combining classifier method that fuses the basic classifiers by finding nearest training DPRs to their outputs on an input data. The set of DPRs is a subset of all training DPs which can retain the initial decision boundaries of nearest neighbor classifier over training DPs. In this manner, we may have more than one template to represent the decision space for each class which can then improve the classification performance.The remainder of this paper is as follows. In Section 2, we present the combining classifiers basic concepts. In Section 3, we describe the standard DT method and examine its weak points. Proposed method is presented in Section 4. Section 5 contains experimental results of proposed method and comparisons with other combining methods. Finally, Section 6 draws conclusion and summarizes the paper.With existence of classification difficulties such as insufficient and noisy training data, high dimensional feature space and highly overlapped classes, it is not feasible to find a single expert that achieves the best result on the entire problem space [34]. The main idea of combining classifiers is to weight basic classifiers and combine their decisions in order to obtain a classification system which outperforms each and every one of them [34]. In fact, MCS methods with a divide and conquer standpoint, exploit a set of simple and local experts in order to increase classification accuracy. The key to performance improvement by MCS lies in training base classifiers with an adequate tradeoff between two conflicting conditions: accuracy and diversity [29,40]. However, based on the Condorcet's Jury Theorem [5], this is proven that the MCSs using more diverse base classifiers considerably reduce the total error; meaning that, they can operate better than the best single classifiers [4,38]. Numerous strategies have been used to increase diversity, such as employing different types of classifiers, same classifiers with different topologies, perturbing feature sets, perturbing training sets or injecting randomness [42]. Bagging [6] and Boosting [37] are two most practical techniques to perturb the training set.Nevertheless, the combining strategy also plays an important role in increasing the performance of the MCS. Combining strategies can be categorized into two major types, hard-level and soft-level, considering outputs of each base classifier are provided as ordered discrete class labels or as continuous values for each class, respectively [42]. Different soft-level combiners deal with the continuous outputs of base classifiers from different perspectives. probabilistic and linear combiners interpret the classifier outputs as posteriori probabilities of each class while fuzzy [8,11] and evidence based [28,39] techniques consider these values as fuzzy membership and belief values, respectively. In trainable combiners, however, the outputs of basic classifiers are used as new features to train the combiner.In general, the MCS can be categorized into three approaches, in terms of combining methodologies. In the first approach, there are two types of combining methodologies: classifier selection and classifier fusion [43]. The main assumption of the classifier selection is that each classifier is expert on a local area of the problem space. By this assumption assigning a class label to an unknown sample is performed by one or more classifiers, which are expert on the surrounding region [2,19,35]. In contrasts, classifier fusion assumes that all classifiers are trained over the whole problem space, so base classifiers are considered to be competitive rather than complementary [30,44].The second approach is based on the involvement of the input pattern in the combining procedure, in which the combining structure can be divided into two major categories: static and dynamic. In static structure, the basic classifiers are combined by means of a mechanism which does not involve the input pattern, but in contrast, in dynamic structure, the actuating of combining mechanism is influenced by the input pattern [17].Finally, MCS methods, based on the nature of combiner, are categorized into trainable and non-trainable combiners. In non-trainable category, decision fusion is done by a simple function of the output values received from the base classifiers for each class, while the trainable combiners use a learner machine which is trained over the decision space of the basic classifiers [20].DT is a robust classifier fusion which combines the basic classifiers by comparing their outputs with characteristic templates of different classes. In this approach, for a given pattern x, the corresponding Decision Profile DP(x) is compared to the DT of each class and then the closest match designates the label of x[26]. In this section, we introduce the concepts of the DP and the DT, which are essential to follow the rest of the article.In standard DT method, DP of a pattern x is a matrix representing the outputs of the basic classifiers for that specific pattern. Let S be a system composed of n basic classifiers which are experts on the entire problem space with m classes, then DP(x) is an n by m matrix, where the element DPi,j(x) contains the decision of the ith classifier about the membership of sample x in the class j. Fig. 1illustrates the DP of a system consisting of n MLP experts on an m-class dataset.The basic idea behind the DT fusion method is to find a proper representative template for each class, so that the DP of an unknown pattern can be compared with them to reach the final decision. To produce a representative template for decision space of a particular class, the DPs of all training samples belonging to this class, Xc, are computed, and then the average of these DPs yields the corresponding DT. Eq. (1) shows the formulation of DT(1)DT(c)=∑x∈XcDP1,1(x)∑x∈XcDP1,2(x)⋯∑x∈XcDP1,m(x)∑x∈XcDP2,1(x)∑x∈XcDP2,2(x)⋯∑x∈XcDP2,m(x)⋮⋮⋱⋮∑x∈XcDPn,1(x)∑x∈XcDPn,2(x)⋯∑x∈XcDPn,m(x),∀c∈C,where C={c1, c2, …, cm} is the set of m classes and n is the total number of basic classifiers.In order to obtain the DT of all the classes, this process must be accomplished over all their training samples. To determine the label of a given unknown pattern, its DP is computed and then compared to each of the DTs based on some similarity measure Φ. The higher the similarity between the DP of a given pattern and the DT of the ith class, the higher the chance for that class [26,24]. The measure of similarity in this study is simply the Euclidian distance. In fact, the Euclidian distance of two matrices, p and q, is calculated based on the norm of their subtraction,(2)p−q=p1,1−q1,1⋯p1,n−q1,n⋮⋱⋮pm,1−qm,1⋯pm,n−qm,n,where the norm of a matrix is derived from the square root of sum of squares formulated as below:(3)Φ(p,q)=‖p−q‖=∑i=1m∑j=1n(pi,j−qi,j)2.It should be noted that the classical DT is considered as a soft-level classifier fusion method, since it combines the real outputs of each and every one of basic classifiers. In addition, the DT combination method is a trainable combiner for the reason that it is trained over the basic classifiers decision space. On the other hand, DT does not take the input pattern to perform its classification process, implying that it falls under the static and trainable category.Despite the advantages of the DT method, there are some exceptional cases that deeply affect its performance. The size of the training sample is one of such cases as the average is taken over the DPs of these samples. As a result, calculating the average of insufficient DPs is not reasonable and the obtained DT is not a confident estimation for the actual prototype. On the other hand, insufficient training samples might cause tendency of DT toward noisy samples and outliers, incurring misclassified testing samples.In Fig. 2, two sets of random points considered as DPs of a 2-classes problem, are generated by 2D normal distributions with means (0, 0) and (0, 3). The variances of both DP sets are set to (2, 2). In this figure training DPs are presented by red pluses and blue circles. The Black pluses and circles are misclassified test samples belonging to the red and blue classes, respectively. This misclassification is because of the inaccurate chosen templates placed in (0.8, 0.1) and (1.1, 2.6) for red and blue classes, respectively (i.e. setting the mean of each class as a template for that class is misleading because of the small number of samples).Besides, Fig. 3is meant to depict the island-shaped issue discussed above. The two red clusters are training DPs belonging to the class 1 generated by a 2D normal distribution with the mean values of (2, 4) and (6, 8) and variance of (1, 1) for both clusters. The blue cluster, on the other hand, represents the training DPs of class 2, which is formed by a 2D normal distribution with the mean and the variance of (6, 6) and (2, 2), respectively. As it is observable, the mean of the red samples is not a proper representative of the red class, which causes major misclassification in testing phase.

@&#CONCLUSIONS@&#
