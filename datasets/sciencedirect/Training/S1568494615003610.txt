@&#MAIN-TITLE@&#
GA-PARSIMONY: A GA-SVR approach with feature selection and parameter optimization to obtain parsimonious solutions for predicting temperature settings in a continuous annealing furnace

@&#HIGHLIGHTS@&#
GA-PARSIMONY combines feature selection and model parameter optimization.Selection of best parsimonious models according to cost and complexity separately.Lower number of features selected in 65% of 20 UCI and Statlib databases tested.GA-PARSIMONY proved useful in SVR control models for a hot dip galvanizing line.

@&#KEYPHRASES@&#
Genetic algorithms,Feature selection,Parsimonious model,Support vector machines,

@&#ABSTRACT@&#
This article proposes a new genetic algorithm (GA) methodology to obtain parsimonious support vector regression (SVR) models capable of predicting highly precise setpoints in a continuous annealing furnace (GA-PARSIMONY). The proposal combines feature selection, model tuning, and parsimonious model selection in order to achieve robust SVR models. To this end, a novel GA selection procedure is introduced based on separate cost and complexity evaluations. The best individuals are initially sorted by an error fitness function, and afterwards, models with similar costs are rearranged according to model complexity measurement so as to foster models of lesser complexity. Therefore, the user-supplied penalty parameter, utilized to balance cost and complexity in other fitness functions, is rendered unnecessary. GA-PARSIMONY performed similarly to classical GA on twenty benchmark datasets from public repositories, but used a lower number of features in a striking 65% of models. Moreover, the performance of our proposal also proved useful in a real industrial process for predicting three temperature setpoints for a continuous annealing furnace. The results demonstrated that GA-PARSIMONY was able to generate more robust SVR models with less input features, as compared to classical GA.

@&#INTRODUCTION@&#
A continuous hot dip galvanizing line (CHDGL) is a well-known steel industrial process consisting of several stages (Fig. 1). The initial product is a steel coil which is the result of prior rolling processes. First, the coil is unwound and runs through a series of vertical loops within a continuous annealing furnace (CAF). This thermal treatment is fundamental to improving the properties of the steel. Then, the steel strip continues through a molten-zinc coating bath, followed by an airstream wipe that controls the anti-corrosion coating thickness. Finally, the strip passes through a series of auxiliary processes which wind the product back into a coil or cut it into flat products.One of the most important stages in a CHDGL is the annealing treatment of the steel strip prior to zinc immersion (Fig. 2). When a steel strip receives a non-uniform heat treatment, it may cause inadequate steel properties, inconsistency in the quality of the coating layer, and other additional problems, such as surface oxidation or carbon contamination. Therefore, efficient control of this treatment is essential to ensuring adequate quality of coating and steel properties, as well as to reducing energy costs.Nowadays, many operating systems for CHDGL are based on data-driven models that predict the optimal CAF settings for each type of coil. These models usually establish furnace temperatures and strip velocity settings according to the dimensional characteristics of each coil, steel chemical composition, pre-established annealing curves of each type of steel, and the plant's operational requirements. Thus, several studies report reliable models for predicting galvanizing setpoints [1–5]. For instance, artificial neural networks (ANNs) are applied by [3] for coating control with significant improvements. Also, multilayer perceptron (MLP) models are developed by [6] for estimating the velocity setpoint of coils inside the CAF on a CHDGL.However, data-driven models of current CAF control systems achieve low accuracy when dealing with new steel coils that have not been previously mapped. Consequently, on-going variations in coil dimensions and chemical steel composition hinder their direct implementation in real time control systems or require data-driven models to be continuously updated. Indeed, the ever-changing product specifications and the inherent complexity of the production process make this long-established processing problem difficult to tackle and overcome.One way to solve this problem is to increase the generalization capability of prediction models. Developing parsimonious models presents a promising approach [7–10]. According to the parsimony criterion, the model with the least complexity that performs with similar accuracy is always preferred. However, determining the best model topology and its setting parameters still poses a challenging feat. An strategy to solving this issue is the use of genetic algorithms (GA) or other bio-inspired methods. Thus, through the principles of biological evolution, GA is capable of providing an efficient optimization process that can help determine the optimal model [11].This article presents a new GA method, GA-PARSIMONY, for creating parsimonious support vector regression (SVR) models. The main objective is to obtain SVRs capable of achieving high accuracy and stability, while also maintaining less complexity as compared to classic GA in terms of variables. When applied to the industry, a lower number of variables avoids uncertainty due to perturbations, such as noise and tolerance of sensors, both of which are indeed common in production lines. Regarding model management, these models are easier to interpret and update.The soft computing (SC) proposal focuses primarily on the use of genetic algorithms (GA) to optimize a SVR-scheme based on model parameter optimization (MPO), feature selection (FS), and parsimonious model selection (PMS). The choice of the best parsimonious models is carried out by a novel GA selection process based on two steps. In the first step, individuals are sorted by a fitness function based on a loss function. As a result, models with similar cost are rearranged by their complexity in order to promote individuals with less complexity. Unlike other proposals in which a penalty parameter is utilized to combine cost and complexity, this GA-proposal uses a GA selection process of the best models according to their cost and complexity - but separately. Therefore, a user-supplied penalty parameter is unnecessary and the automatic method for generating parsimonious models is simplified. In the last year, this methodology has been applied successfully to other fields [12–14].Section 2 begins by introducing the principle of parsimonious modeling and its related soft computing (SC). Section 3 describes the proposed GA-PARSIMONY methodology and includes a brief description of SVR methods. In Section 4, the results from two series of experiments are outlined. And, finally, Section 5 presents the conclusions and suggestions for future development of the algorithm presented herein.The principle of parsimony, also called Occam's Razor, postulates that between equally valid hypotheses the simplest explanation is preferred over those more complex ones. This principle, as applied to modeling processes, may be utilized when several valid models of varying complexities exist. And thus, according to parsimony criteria, the least complex model would be selected.Model complexity is usually defined by the number of input attributes, or by the models internal structure such as, for example, the sum of squared weights in neural networks, the number of leafs or levels in model trees, and so on and so forth [15]. This measurement directly relates to model flexibility, which refers to an algorithm's capability of fitting the data during the training process. More flexible models can adjust better to the training dataset, but they could also be over-fitted, which is detrimental to generalization capabilities. Therefore, to obtain a strong overall model, a trade-off between complexity and accuracy of prediction must be reached.More specifically, in actual industrial applications, an adequate overall model with a reduced number of input features has numerous advantages. For example, models with only the most important variables facilitate understanding the problem. These models also perform more robustly when faced with perturbations or noise, both of which are very common in industrial processes. And furthermore, less complex models significantly simplify future updating and exploiting stages, while they also reduce the human and economic effort involved in capturing and preprocessing the information.Selecting the least complex model is a common topic in machine learning in order to achieve greater generalization capacity for new predictions. Thus, determining the most relevant model inputs and the optimal setting parameters remains a challenging task that depends on multiple factors. In particular, when trying to reduce the computational cost of these tasks, soft computing (SC) seems to be an effective alternative to other classical approaches [16–21]. Several authors have reported SC strategies that combine feature selection (FS) and model parameters optimization (MPO) to determine final solutions with solid generalization capabilities [22]. Huang and Chang [23] propose GAs combined with k-fold cross-validation error (k-fold CV error) for FS, as well as for tuning support vector machines (SVM) in order to improve microarray classification. Ding [24] uses particle swarm optimization (PSO) to simultaneously select the best spectral band and optimize SVM parameters in hyperspectral classification of remote sensing images. Furthermore, Vieira et al. [25] employs a binary PSO with a wrapper approach with SVM to predict survived or deceased patients with septic shock. Huang and Dun [26] design a distributed PSO with SVM for FS and MPO. Ahila et al. [27] present a PSO method for tuning Extreme Learning Machines (ELM) and for FS in the classification of power system disturbances. Dhiman et al. [28] propose a hybrid approach with wavelet packet decomposition and a GA-SVM scheme for FS and MPO to obtain classification models capable of detecting epileptic seizures from background electroencephalogram signals. Castillo et al. [29,30] optimize membership functions of complex fuzzy controllers with ant colony optimization (ACO). Winkler et al. [31] report different evolutionary strategies determining inputs that optimize linear models, k-nearest neighbors (k-NN), ANNs and SVM. They aim to select the best models capable of identifying tumor markers. Chen et al. [32] also use an evolutionary approach to simultaneously optimize complexity and weights of learning vector quantification networks for bankruptcy prediction. Sanz et al. [8] reported a novel GA-based optimization to create better overall parsimonious ANNs for predicting setpoints in an annealing furnace of a steel galvanizing industrial plant.Although k-fold CV error usually guarantees reliable overall models without incorporating complexity evaluation, it cannot ensure that the best parsimonious model will be determined. Hence, the principal idea behind the abovementioned studies is to select the best models with the least model complexity. To this end, the majority of them utilize a penalty parameter (λ) to combine both cost and complexity into the same fitness function J. Therefore, a common practice is to use the k-fold CV error as cost, and the number of model input features (NFS) as the complexity measurement.This strategy is commonplace in many machine learning algorithms for training predictive models by minimizing a regularized “Loss+Penalty” function [33]:(1)minimizeβ0,β1,…,βp{L(X,y,β)+λP(β)}where L(X, y, β) is some loss or cost function quantifying how the model, parametrized by β, fits the training data (X, y) where X is the input data and y the response variable; and P(β) is a penalty function which usually is defined by the model complexity. For example,P(β)=∑j=1pβj2for ridge regression or SVM defines the “flatness” of the model which influences the variance of the prediction. Finally, λ is a non-negative tuning parameter which balances the cost and penalty function in order to control the bias-variance trade-off. Then, this regularization strategy is used by multiple methods such as ridge regression, the lasso, SVM with the penalty coefficient, ANNs with weight decay, etc. In these methods, the tuning process is usually performed by gridding search (or other tuning techniques) of λ and the other algorithm parameters, by minimizing the k-fold CV error in order to ensure a final model with adequate generalization characteristics.To optimize a wrapper scheme with GA, where FS and MPO are involved, a similar penalty parameter Λ is used to relate the complexity and generalization error in the fitness function J. However, in order to correctly compare several individuals through their J, Λ must be established prior to the optimization procedure. Consequently, defining Λ entails uncertainty because one cannot be certain as to which is the optimal value to obtain the best overall and parsimonious solution.Therefore, the main objective has been to develop an automatic GA optimization (GA-PARSIMONY) to indicate the best performing SVR models according to two separate criteria: k-fold CV error and complexity. The advantage is that this methodology automatically conducts the calibration of SVR models without incorporating any user supplied penalty weight Λ. This also simplifies the training process and allows the user to determine overall satisfactory parsimonious prediction models. To measure the capacities of the proposed approach, a test bed of cases is included to compare the GA-PARSIMONY with classical GA. In addition, the algorithm is tested on a practical application, a CHDGL for coating steel coils with protection against corrosion.A novel approach, referred to by the authors as GA-PARSIMONY, is proposed herein for both FS and MPO. This methodology is performed by using a wrapper FS and MPO scheme which can be fully tuned for each i individual of each g generation. In particular, SVR is the method chosen for the wrapper included in the algorithm presented herein.Fig. 3depicts the flowchart corresponding to the GA-based optimization of the SVR model. First, the process generates a set of chromosomes of the initial populationΛ0:{λ01,λ02,…,λ0p}using a Latin Hypercube algorithm (LHS()):(2)Λ0:{λ0i,i=1,…,P}←LHS()whereλ0iis the i chromosome of the first generation (g=0) and P is the number of individuals.In this manner, a diverse yet uniformly distributed set of individuals is assured to accelerate the convergence of the optimization process. Each individual represents a metamodel configuration whose chromosome consists of a set of genes containing the selected input features and the setting parameters to be optimized (Fig. 4):(3)λgi=[q,ϵ,C,γ]TThe binary-coded array q represents the particular input features selected where the total length of q is equal to the number of input attributes of each database. The rest of the chromosome comprises the real-coded SVR setting parameters as follows: the insensitive loss parameter ϵ, the penalty coefficient C, and the γ of the RBF kernel.The validation process of each metamodelλgiof the population Λgis conducted using a m repeated k-fold cross validation (CV). This method consists of dividing the initial dataset into k subsets and selecting k-1 to train the metamodel. A partial sample error is then determined from the subset that is not utilized in the training process (validation data). This procedure is repeated k times, each time with a different subset. The error is derived by the arithmetic mean of the k partial sample errors. Lastly, the k-fold CV is repeated m times in order to obtain a more reliable and robust metric. The final value is calculated as the average of a total number of k×m errors. Therefore, the metamodel evaluation is performed according to the fitness function:(4)J(Λg)=∑i=1k×merrork×mwhere error represents an error metric such as root mean squared error (RMSE), k is the number of folds, and m is the number of repetitions.Once the population Λgis evaluated and sorted by J(Λg),(5)ΛgJ←sort(J(Λg)GA-PARSIMONY incorporates, as a novelty, an additional ReRank function which rearranges models according to their complexity,(6)Λgs←ReRank(ΛgJ)The ReRank function is detailed in Algorithm 1. Its primary objective is to select individuals with high accuracy while maintaining the robustness of a parsimonious model. This function switches the position of two models if the first one is more complex than the latter and no statistically significant difference is found between them in terms of J. To this end, only those individuals with similar J are compared according to their complexity. Thus, the nonparametric Wilcoxon ranked test is used for comparing the fitness function J and determining if the corresponding p-values are lower than a pre-established level of significance α. This step is sequentially repeated for all individuals in the population.Algorithm 1Pseudocode of ReRank functionprogram ReRank ()input G(J, Model-Complexity): Individuals sorted by Jconst NUMINDIV = cte.; alpha = cte.;varPosFirst, PosSecond: 1..NUMINDIV;beginPosFirst:= 1; PosSecond:= PosFirst;repeatrepeatPosSecond:= PosSecond+1p-value:= test(G[PosFirst](J),G[PosSecond](J))if p-value >alpha AND G[PosSecond](Size) <G[PosFirst](Size)Swap(G[PosFirst],G[PosSecond])end ifuntil (p-value < = alpha OR PosSecond == NUMINDIV)if PosFirst==(PosSecond-1)PosFirst:= PosSecond;elsePosFirst:= PosFirst+1; PosSecond:= PosFirstend ifuntil (PosFirst == NUMINDIV)end.A practical example of the function ReRank is illustrated in Fig. 5. First, seven individuals are ordered by their fitness functionsJ(λgi). G1, G2 and G3 correspond to three possible sets of individuals with similar J. In this example, there is a statistically significant difference between the fitness functions J of the models A with D (pAD≤α), and D with E (pDE≤α), where pADand pDEare the p-values of A versus D, and D versus E, respectively. The complexity of the seven models is depicted by the size of their boxes, where small boxes correspond to less complex models.In the first step (n=1), complexities (size of the boxes) of the models A and B in positions PF=1 (fist position) and PS=2 (second position) are compared. Since the complexity of model A is lower than that of model B, their positions remain unchanged. Then, PSis incremented to the next position (PS=PS+1). Following the same procedure, in the second step, the models in positions PF=1 and PS=3 are switched because the complexity of model C is lower than A. Next, PSis increased (PS=PS+1=4) but because PSreaches G2, the algorithm turns back positions in the following way: PF=PF+1=2 and PS=PF+1=3. In step 3, function ReRank swaps model B and A which are now in positions 2 and 3. In this step, the algorithm also reaches the final position G1. Then, it places PFin the first position of the next group (G2). In step 4, there is only a member of G2 so that the new positions are located in the next group (G3). Finally, this process is sequentially repeated for all the individuals of G3. Therefore, once the function reaches the final step (n=7), all models have been completely re-sorted according to their complexity, but also considering the differences observed between their fitness functions.And finally, selection, crossover, and mutation operators are applied to create the next generation (g=g+1) in order to evolve towards better solutions.By applying the selection principle, and according to an elitism percentage, the elitist individualsΛg−1eare selected as parents for the next generation Λg,(7)Λg[1:Pe]←Λg−1e←select(Λg−1s[1:Pe])where Peis the number of elitist individuals andΛg−1s[1:Pe]is the previous generation (g−1) re-sorted with ReRank algorithm.For this purpose, several types of selection operators are available in the GA literature; in this case, random uniform, roulette wheel and tournament were the three operators selected.Then, a crossover operator is applied to the parents in order to generate new individuals,(8)Λg[Pe+1:P]←crossover(Λg[1:Pe])until completing the population. The crossover is performed by using a heuristic blending[34] expressed in the following equation:(9)pnew=βpmn−pdn+pmnwhere pmnand pdnare the n-th variable in the parent chromosomes, pnewis the new single offspring variable and β usually corresponds to a random number from the interval−0.1,1.1.Finally, a random mutation operator is applied to the population except for the two best individuals,(10)Λg[3:P]←mutation(Λg[3:P])The GA process comes to an end when the maximum number of generations G or a minimum threshold Jlimis reached. The performance and generalization capability of the best configuration is finally evaluated using the test data.Support vector machines (SVM) is a well-known machine learning algorithm developed by Vapnik et al. [35]. In 1997, Drucker et al. [36] proposed a modified version of Vapnik's SVM for regression tasks called the support vector regression (SVR) model. According to the number of citation, SVR is likely to be one of the most popular machine learning methods at present. Indeed, it boasts a strong theoretical background. One benefit of SVR is that it can use kernel function substitution to perform a non-linear transformation on the input vectors while maintaining the elegance of the optimization method used for linear SVR [37]. Since the accuracy of models is always affected by the nature of the optimization algorithm utilized, SVR's ability to escape from local minima in the solution space and determine better models is of notable significance.Given an initial datasetxi,yii=1Nwhereyi∈ℝandxi∈ℝD, and N is the number of observations, the predicted outputyˆis given by:(11)yˆ=wtφx+bwithw∈ℝD,b∈ℝandφxthe mapping function fromxto a higher dimensional feature space. The adjustment of the regression model is analogous to the problem of determining thewand b values that minimize an error function subjected to the following constraints: (ξi+,ξi−). However, instead of the traditional mean square error (MSE), a more sophisticated penalty function defined by [38] is considered in the minimization. In this sense, a loss function L, named ϵ-intensitive cost function, is defined as zero if the predicted values are inside an ϵ-intensitive tube according to Eq. (12):(12)L=0if|yi−yˆi|<ϵyi−yˆi−ϵotherwiseThus, since SVR optimization is built on inequality constraints [39], a quadratic programming problem is formulated as follows:(13)minimizew,b,ξ,ξˆ12wtw+C∑i=1Nξi+ξi+,C>0(14)suchthatwtφxi−b≤yi+ϵ+ξi+,i=1,…,N(15)wtφxi+b≥yi−ϵ−ξi−,i=1,…,N(16)ξi+,ξi−≥0i=1,…,Nwherewtwrepresents the hyperplane normal vector, C is the regularization parameter for controlling the trade-off between loss and complexity and ϵ is the distance that generates the mentioned tube in which L is zero. In case of infinite dimensional input spaces, Eqs. (13)–(16) cannot be solved and the question is how to determine the optimum solution via Lagrange multipliers. The resulting modelyˆ=fxgiven by Eq. (17) will be equivalent in the feature space to the original Eq. (11).(17)fx=∑i=1NαiKxi,x+bwith the kernel substitutionKxi,x=φxitφxand the parameters αiand b determined by a linear system of N equations. In this study, in order to define the kernel behavior, a radial basis kernel function (RBF) given by Eq. (18) was selected among other common kernel functions:(18)Kxi,x=e−∥xi−x∥2σ2=eγ∥xi−x∥2where x andσ∈ℝ+are the center and width of the RBF. The setting parameters of SVR which must be optimized are the ϵ-insensitive loss function ϵ, the penalty coefficient C and, in the case of RBF kernels, the valueγ=−12σ2.

@&#CONCLUSIONS@&#
The principal objective of this study was to develop and test a new GA wrapper automatic procedure that efficiently generated prediction models with reduced complexity and adequate generalization capacity. This approach, named GA-PARSIMONY, is primarily based on combining FS and MPO with a novel GA selection process (ReRank algorithm) in order to achieve better overall parsimonious models. These robust models ensure a reliable performance under changing industrial plant conditions. Thus, these models with reduced complexity can achieve satisfactory accuracy in predictions, and maintain the quality of products while working with constant changes in the production cycle. Furthermore, another advantage of these models is that they are easier to interpret and more robust when dealing with noise or perturbations. And what's more, less complex models reduce human and economic efforts in future stages of updating and exploitation.Unlike other GA methodologies that use a penalty parameter for combining loss and complexity measures into a unique fitness function, the main contribution of this proposal is that GA-PARSIMONY selects the best models by considering cost and complexity separately. For this purpose, the ReRank algorithm rearranges individuals by their complexity when there is not a significant difference between their costs. Thus, less complex models with similar accuracy are promoted. Furthermore, because the penalty parameter is unnecessary, there is no consequent uncertainty associated with assigning a correct value beforehand. As a result, with GA-PARSIMONY, an automatic method for obtaining parsimonious models is finally made possible.In order to assess GA-PARSIMONY performance, it was tested with 20 UCI and StatLib databases versus a classical GA methodology which uses only CV error for FS and MPO. In this experiment, GA-PARSIMONY obtained models with similar performance, but with a lower number of features in 65% of the databases. However, the analysis of time consumption did not reveal a notable differences between the two methodologies. This was presumably due to the fact that the main effort of this study was focused on reducing model complexity with medium/low-dimensional databases. One can infer then that the use of GA-PARSIMONY with high-dimensional datasets will probably reduce time consumption of the individual training and validation process due to the likely progressive reduction of features in the GA optimization. Therefore, future research will focus on assessing the computing time of GA-PARSIMONY with high-dimensional databases.Finally, the new proposal was applied to predict the three CAF temperature setpoints for a CHDGL. Due to the extremely time-consuming requirements of training SVR models with large databases, the proposed methodology was tested with different GA selection configurations and three restrictions: low number of population individuals, validation process with a low number of runs, and a reduced number of final generations to achieve the optimum model. In this case of study, random uniform and tournament selection operators worked better with a lower number of generations and population size. Despite the fact SVR are well-known to be less sensitive to the number of features as compared to other methodologies, the results herein demonstrated that the new methodology helped to obtain SVR-models with a lesser number of features, while maintaining similar overall precision.In light of the excellent results obtained, the main contribution of GA-PARSIMONY, the ReRank algorithm, can be easily included into the evaluation process of other SC methods so as to automatically obtain accurate parsimonious models in many application fields. And thus, such models would be useful to better understand real complex problems and for forecasting under constantly changing conditions.