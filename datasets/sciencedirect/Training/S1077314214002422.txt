@&#MAIN-TITLE@&#
Fast convergence of regularised Region-based Mixture of Gaussians for dynamic background modelling

@&#HIGHLIGHTS@&#
Derivation of O(1/k2) rate of convergence for online gradient method with momentum.Novel foreground detection algorithm for dynamic background modelling.Fast convergence of mixtures demonstrated on various sets of simulated data.Superior performance compared to other state-of-the-art algorithms on real videos.

@&#KEYPHRASES@&#
Momentum,Convergence rate,Generative models,Region-based Mixture of Gaussians,Dynamic background modelling,

@&#ABSTRACT@&#
The momentum term has long been used in machine learning algorithms, especially back-propagation, to improve their speed of convergence. In this paper, we derive an expression to prove theO(1/k2)convergence rate of the online gradient method, with momentum type updates, when the individual gradients are constrained by a growth condition. We then apply these type of updates to video background modelling by using it in the update equations of the Region-based Mixture of Gaussians algorithm. Extensive evaluations are performed on both simulated data, as well as challenging real world scenarios with dynamic backgrounds, to show that these regularised updates help the mixtures converge faster than the conventional approach and consequently improve the algorithm’s performance.

@&#INTRODUCTION@&#
Generative models have been extensively applied to many areas of computer vision. One of the most intensive areas of application, that has seen a lot of activity in this regard, is that of background modelling for foreground detection in surveillance video [1]. In particular, the Mixture of Gaussians (MoG) model proposed by Stauffer and Grimson [2] has, it could be argued, become the standard de facto approach to generative background modelling. From a Bayesian perspective, the online MoG algorithm uses a Robbins–Monro type stochastic approximation technique [3] to update the mixture parameters over time. These updates can be controlled by a learning rate parameter that can be tuned to vary the rate of learning of the mixture model. From a machine learning perspective, these updates can also be viewed as analogous to widely-used gradient descent methods.Whilst the MoG algorithm has been shown to work well for static background scenes, it has struggled to cope with scenes whose background, or part thereof, is dynamic. Sources of dynamism include moving backgrounds, such as the surface of water and vegetation blowing in the wind, and also camera movements due to vibration, or the fact it is on-board a moving vehicle, such as a bus or train. From a modelling perspective there are two main differences between static and dynamic backgrounds. Firstly, in a static background scenario, it is assumed that the source of a sample distribution has a one-to-one mapping with a given pixel. However, for the dynamic scenario, the mapping is one-to-many, i.e., the samples for a specific distribution may be captured over a region of the imaging sensor. Varadarajan et al. [4] previously addressed this issue by developing a region-based MoG (RMoG) algorithm which showed significant performance improvements over the standard MoG algorithm for dynamic backgrounds. Furthermore, we formally derived the update equations using a theoretical framework for which the standard MoG is a special case. Secondly, the number of samples acquired for a distribution may be much smaller in the dynamic case.A drawback of gradient descent methods in relation to the second issue, is that they converge slowly [5]. One way to speed up convergence is by introducing an additional momentum term to the update equation, which adds a fraction of the difference between the two previous values of the parameters. The additional term can be viewed as inertia as it takes a small step further in the same direction of the previous update, hence the name heavy-ball. This reduces oscillations when the direction of the gradient keeps changing. It can also aid the learning rate by pushing the update further towards the optimum value if there is no change in the direction of the gradient, which helps to increase the speed of convergence to the optimum value. Momentum can also help the system escape local minima better than conventional gradient methods.Wang and Miller [6] introduced the use of the momentum term for regularising the classification Expectation–Maximisation (EM) based MoG algorithm and presented some initial results for background modelling in video scenes that showed promise. In this work, we incorporate a momentum term into our region-based framework, in order to address the second issue, outlined above, in relation to dynamic backgrounds. Finally, important properties for learning algorithms are whether they are guaranteed to converge and, if so, their rate of convergence. Previously, a rate of convergence ofO(1/k2)has been proven for the batch version of heavy-ball [7]. Also, Varadarajan et al. provided a proof of convergence for the online version of this algorithm in [8]. While this shows that the method converges, the rate of convergence was not given. Therefore, the second major contribution of this work is to prove a rate of convergence ofO(1/k2)for the online algorithm.The remainder of this paper is structured as follows. We highlight some of the related work in literature in Section 2. The spatio-temporal modelling approach with RMoG algorithm is briefly introduced in Section 3. TheO(1/k2)rate of convergence for the online gradient method with a momentum term is derived in Section 4. This extrapolation step is then applied to the RMoG framework to derive a new background subtraction algorithm which is explained in detail in Section 5. Section 6 contains a comprehensive evaluation of the algorithm using simulated data and well-known real video sequences with dynamic backgrounds, whilst comparing it with other state-of-the-art MoG variants. The conclusion is presented in Section 7.

@&#CONCLUSIONS@&#
