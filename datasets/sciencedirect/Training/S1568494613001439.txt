@&#MAIN-TITLE@&#
Evaluation of reservoir sedimentation using data driven techniques

@&#HIGHLIGHTS@&#
There are large constraints in conventional direct methods of sediment deposition estimation.In the present study, different data driven techniques were tried to find suitable model for sediment deposition estimation.It was found that soft computing techniques can play important role in such modelling.The evolutionary genetic programming technique found to the best modelling technique.

@&#KEYPHRASES@&#
Reservoir sedimentation,Soft computing techniques,Artificial neural networks,Model trees,Genetic programming,

@&#ABSTRACT@&#
The sedimentation is a pervasive complex hydrological process subjected to each and every reservoir in world at different extent. Hydrographic surveys are considered as most accurate method to determine the total volume occupied by sediment and its distribution pattern in a reservoir. But, these surveys are very cumbersome, time consuming and expensive. This complex sedimentation process can also be simulated through the well calibrated numerical models. However, these models generally are data extensive and require large computational time. Generally, the availability of such data is very scarce. Due to large constraints of these methods and models, in the present study, data driven approaches such as artificial neural networks (ANN), model trees (MT) and genetic programming (GP) have been investigated for the estimation of volume of sediment deposition incorporating the parameters influenced it along with conventional multiple linear regression data driven model. The aforementioned data driven models for the estimation of reservoir sediment deposition were initially developed and applied on Gobindsagar Reservoir. In order to generalise the developed methodology, the developed data driven models were also validated for unseen data of Pong Reservoir. The study depicted that the highly nonlinear models ANN and GP captured the trend of sediment deposition better than piecewise linear MT model, even for smaller length datasets.

@&#INTRODUCTION@&#
Reservoirs, the key infrastructure for survival of mankind, meet various purposes like assured water for irrigation, industries, municipal water supply, hydropower generation, etc. Despite the various tangible and intangible benefits realised, reservoirs also have some drawbacks like interruption of natural flow regimes and natural eco-systems; land inundation, water quality degradation, downstream river bed degradation and the prominent among them is ‘reservoir sedimentation’. Due to increase in population and consequent developmental activities in the reservoir catchment area, the problem of reservoir sedimentation is increasing day by day. Thus the biggest challenge in reservoir operation is to study the rate, volume and pattern of the sediment deposition [1,2].Although hydrographic surveys are very expensive, cumbersome and time consuming, these are the most accurate method to estimate volume of sediment deposited in a reservoir [3]. Other drawback of these techniques is they do not provide the information about the origin of the sediments and factors affecting sedimentation. Therefore, considering reservoir sedimentation as a complex hydrological process and based on the observed data so far, data driven approaches such as ANN, MT and GP have been investigated for the estimation of volume of sediment deposition incorporating the parameters influenced it along with conventional multiple linear regression data driven model. The developed data driven models were also validated for unseen data of Pong Reservoir.In recent past, data driven soft computing techniques such as ANN, MT and GP have been applied to map and predict various kinds of hydrologic process. These techniques have shown exceptional performance as regression tools, especially when used for pattern recognition and complex non-linear function estimation. In the present study, various data driven soft computing models have been developed with lesser length data to estimate the volume of sediment retained in Gobindsagar Reservoir. The developed models were verified for their general applicability using the data of Pong Reservoir, located in the same sedimentation zone 1 of India.Shangle [4] classified the river basins of India into seven sedimentation zones based on multiple factors affecting rate of sedimentation as:1.Himalaya Region (Indus, Ganga, and Brahmaputra basins);Indo-Gangestic Plains;East Flowing Rivers up to Godavari (excluding Ganga);Deccan Peninsular East Flowing Rivers including Godavari and South Indian Rivers;West Flowing Rivers up to Narmada;Narmada and Tapi Basins; andWest Flowing Rivers beyond Tapi and South Indian Rivers.In the present study, Gobindsagar and Pong reservoirs which belongs to Region 1 have been taken up for development and generalisation of methodology, respectively. Both these reservoirs are located at upper hilly region of sedimentation zone 1 and have similar hydro-climatic variations. The location of these river-reservoir systems is shown in Fig. 1.The Bhakra dam is one of the oldest dams in India, commissioned in 1958, on Satluj River in Bilaspur district, Himachal Pradesh, in the foothills of Himalayan region, which ultimately led to creation of Gobindsagar Reservoir. Gobindsagar Reservoir has a designed dead storage capacity of 2431.81×106m3, with a live storage of 7436.03×106m3 leading to a total storage capacity of 9867.84×106m3. The reservoir has an enormous water spread area extending over 168.35km2 at full reservoir level. The river Satluj originating from Mansarover Lake along with its tributaries has a catchment area of 56,876km2.However, Beas Dam is a storage earth core cum gravel shell 132.6m high dam at village Pong in Kangra district of Himachal Pradesh about 13km from Talwara township in Punjab state, commissioned in 1974. It has an enormous waterspread of 260km2 at full reservoir level at 426.72m to impound 8578.99×106m3 of water with spread length of 42km and maximum width of 16km at normal storage. The live storage of the reservoir is 7291.22×106m3 and the dead storage is 1287.773×106m3. The catchment of these reservoirs is also very prone to landslides and slips which may be one of the major sources of sediment. According to Sharma et al. [5], the natural factors such as the steep topographic gradient, the poor structural characteristics of soils, and the widespread existence of limestone deposits attribute to high levels of sediment transport to the study region.Both these reservoirs are being controlled by Bhakra Beas Management Board (BBMB), which carrys out capacity surveys using range survey method regularly, as described by Morris and Fan [3]. A long-term annual rainfall and inflow data pertaining to Gobindsagar and Pong reservoirs for the period of 32 and 10 years, respectively, was available with the agency.Reservoir sedimentation is an inevitable process; there is an immediate need to develop more accurate methods and models. Although hydrographic surveys are very expensive, cumbersome and time consuming, these are the most accurate method to estimate volume of sediment deposited in a reservoir. Other drawback of these techniques is they do not provide the information about the origin of the sediments and factors affecting sedimentation. Therefore, considering reservoir sedimentation as a complex hydrological process and based on the observed data so far, data driven approaches such as ANN, MT and GP have been investigated for the estimation of volume of sediment deposition incorporating the parameters influenced it along with conventional multiple linear regression data driven model. The developed data driven models were also validated for unseen data of Pong Reservoir. The data-driven model approaches namely ANN, MT and GP; those have proved their applicability to model the complex behaviour of hydrological processes with very limited or without knowledge of physics involved in such processes; are being used only to model suspended or channel sediment yield. Therefore, these approaches can be used as strong regression tools to estimate sedimentation in case of reservoirs. This proposed methodology is an attempt to understand this process and develop more realistic models.An ANN is a massively parallel-distributed information-processing system that has certain performance characteristics resembling biological neural networks of the human brain, which can recognise patterns and learn from their interactions with the environment. ANN have been developed as a generalisation of mathematical models of human cognition or neural biology. Mathematically, an ANN is often viewed as a universal approximator [6]. The ability to identify a relationship from given patterns make it possible for ANN to solve large-scale complex problems such as pattern recognition, nonlinear modelling, classification, association, and control [7]. In comparison to the conventional methods, ANN does not require the knowledge of the complex underlying process under consideration to be explicitly described in mathematical terms. Moreover, it tolerates imprecise or incomplete data, approximate results, and are less vulnerable to outliers [8].A neural network architecture can be characterised in many ways based on the pattern of connection between nodes, its method of determining the connection weights, and the activation function. In most of the networks, the input (first) layer receives the input variables for the problem at hand. The input layer, consists of all quantities that can influence the output, is thus transparent and is a means of providing information to the network. The last or output layer consists of values predicted by the network and thus represents model output. The number of hidden layers and the number of nodes in each hidden layer are usually determined by trial-and-error procedure. The nodes within neighbouring layers of the network are fully connected by links. A synaptic weight is assigned to each link to represent the relative connection strength of two nodes at both ends in predicting the input–output relationship. The performance of the trained networks can be tested through the common statistical indicators. ANN can also be categorised based on the direction of information flow and processing such as feed forward, recurrent and radial basis function neural network.As name suggests, in a feed-forward network, the information passes from the input to the output side. The nodes in one layer are connected to those in the next, but not to those in the same layer. Thus, the output of a node in a layer is only a dependent on the inputs it receives from previous layers and the corresponding weights. The concept of artificial neurons was proposed by McCulloch and Pitts [9]; however, this technique has experienced renaissance altitude since the introduced backpropagation (BP) training algorithm by Rumelhart et al. [10]. The BP trained feed forward multilayered perceptron (MLP) with sigmoidal activation function neural networks are probably the most common of all of the ANN and are widely used.In inherently dynamic, recurrent neural networks (RNN), information flows through the nodes in both directions, from the input to the output side and vice versa. This dynamic nature is generally achieved by refeeding previous network outputs as inputs to the network, thus allowing for feedback. The three essential ways of introducing feedback data or memory to static neural network are tapped delay line, context/partial and fully RNN models [11]. The context models, fall somewhere between simple tapped delay line and the complex fully recurrent network, and they provide competitive solutions in many situations. The Elman [12] is most commonly used context type RNN model [13–20]. This is a discrete-time recurrent two-layer network with feedback loops from its hidden layer neurons back to its inputs that allow for adaptability and non-linearity [13,21]. This additional information of filtered input acts as a guidance to evaluate the current noisy input and its signal component. This internal recurrence gives the network dynamic properties, which make it possible for the network to incorporate temporal context information or past experience. The temporal representation capabilities of the RNN are better than those of purely feed forward networks, even those with tapped delay lines [22].The other variant of ANN is radial basis function (RBF) neural network, which is three layered (input, hidden with a non-linear RBF activation function and a linear output layer) feed forward network. In such networks, the Gaussian basis function has most widely been used among various transformation/activation functions multi-quadric, thin-spline, exponential and Gaussian [23–27]. The activation function has two important parameters centre and spread associated with it. As the Euclidian distance between the input vector and its centre increases, the output given by the activation function decays to zero. This rate of decrease in the output is controlled by the spread of the RBF [28]. In other words, when the input falls within small localised region of the input space, the basis functions in the hidden layer produce a significant non-zero response to input stimulus. The type of input transformation of the RBF neural network is the local non-linear projection using a RBF activation function. After non-linearly squashing the multi-dimensional inputs without considering the output space in unsupervised way, the RBF plays a role of regressor. Since, the output layer implements a linear transformation the only adjustable parameters are the weights of this regressor [29–31]. These parameters can therefore be determined easily by multiple regressions in supervised manner, which gives an important advantage for high convergence.The detailed review of literature on ANN is beyond the scope of this paper, the further details can be find in large number of books available dealing with fundamentals of ANN [8,32]. There is a rapidly increasing body of literature discussing the application of ANN in water resources, hydrology and their associated issues. Since the early nineties, ANN have been successfully used in hydrology-related areas such as rainfall-runoff modelling, stream flow forecasting, ground-water modelling, water quality, water management policy, precipitation forecasting, hydrologic time series and reservoir operations [11,32–36]. Very recently application of ANN in sedimentation studies is gaining importance, but not much significant research studies have been carried out especially for reservoir sedimentation. The successful application of ANN on sediment studies can be found in Abrahart and White [37], Cigizoglu [38], Cigizoglu [39], Baiocco et al. [40], Bazzoffi and Van Rompaey [41], Licznar and Nearing [42], Cigizoglu [43], Agarwal et al. [44], Bazzoffi et al. [45], Bhattacharya and Solomatine [46], Sarangi and Bhattacharya [47], Sarangi et al. [48], Cigizoglu and Alp [49], Lee et al. [50], Raghuwanshi et al. [51], and Garg and Jothiprakash [52,53].MT, a piecewise linear model, takes an intermediate position between the linear models and truly nonlinear models. In fact MT is a modular model, as it consists of modules that are responsible for modelling particular subspace of the input space [46,54–57]. The MT approach split the multi-dimensional parameter space into sub-spaces and builds in each of them a linear regression model. The splitting in MT follows the idea of a decision tree, but instead of the class labels it has linear regression functions at the leaves. The most commonly used algorithm in MT to induce tress is M5 algorithm, which was developed by Quinlan [58]. The algorithm combines the features of classification and regression: tree-structured regression is built on the assumption that the functional dependency is not constant in the whole domain, but can be approximated as such on smaller sub-domains (For example the input space between X1 and X2 is split into regions and separate regression models can be built for each of the regions) as shown in Fig. 2.The data splitting (regionalising) criterion for the M5 MT algorithm is based on treating the standard deviation of the class values that reach a node as a measure of the error at that node, and calculating the expected reduction in this error as a result of testing each attribute at that particular node [60]. The formula to compute the standard deviation reduction (SDR) is:(1)SDR=sd(X)−∑i|Xi||X|sd(Xi)where X=set of examples that reaches the node; Xi=subset of examples that have the ith outcome of the potential set; sd=standard deviation.The splitting process will terminate if the output values of all the instances that reach the node vary slightly, or only a few instances remain. After the initial tree has grown, there are several steps that have to be taken, such as calculation of error estimates, generation of linear models, simplification of linear models, pruning and smoothing. The splitting of the input space and subsequent formation of local regression models at the leaves, often produces over-elaborate structures. These structures must be pruned back, by replacing the sub-tree with the leaf, and as a result the number of regression models drop. Smoothing is accomplished by producing linear models for each internal node, as well as for the leaves at the time the tree is built. Following researchers have applied MT successfully in the field of hydrology: Bhattacharya and Solomatine [46,57], Solomatine and Dulal [56], Bhattacharya and Solomatine [61], Solomatine and Siek [62], Solomatine and Xue [63], Bhattacharya et al. [64], and Pal and Deswal [65].The GP technique is a relatively new and member of the evolutionary algorithm family, which is based on Darwin's natural selection theory of evolution [66]. It is an inductive form of machine learning based on genetic algorithm, which evolves computer program to perform an underlying process defined by a set of training samples [67]. The algorithm choose among the combination of input variables; appropriate numbers and functions, which include arithmetic operators (+, −, *, /), mathematical functions (sin, cos, exp, log), logical/comparison functions (OR, AND), etc.; based on some understanding of the underlying process and generates an initial population of programs randomly. This population of potential solutions is then subjected to an evolutionary process and based on stopping criteria, the fitness measures of the evolved programs are evaluated. From the initial population, the individual programs that best fit the data are then selected, discarding no-so-fit programs. The crossover (exchanging the parts of best programs with each other) and mutation (randomly changing programs to create new programs) operations are performed over selected best fit programs to exchange part of the information between them to produce better programs. This evolution process is repeated over successive generations to find best solution describing the process.According to initial concept of GP [66], the programs are represented as tree structures and expressed in the LISP functional programming language [68–72]. Later, due to its tree structural solution, researchers regarded it as tree based genetic programming (TGP). Recently, Linear Genetic Programming (LGP) concept, a subset of GP has emerged, which evolves programs in an imperative programming language (C/C++) and represent the graph-based functional structure [73,74]. The imperative program structure of LGP identifies the non-effective instructions efficiently and executes rapidly [72,75–77]. Moreover, the maximum size of the program is usually restricted to avoid over-growing programs without any condition in LGP [76].GP has been successfully applied to complex nonlinear problems and its solution best describes the input–output relationship. However, in contrary to ANN, the application of GP especially LGP in hydrological studies is at nascent stage, irrespective of the fact that both the techniques can be seen as alternative techniques for the same task. The successful application of GP techniques in hydrology field can be found in Garg and Jothiprakash [52], Whigham and Crapper [67], Guven [72], Savic et al. [78], Liong et al. [79], Khu et al. [80], Harris et al. [81], Jayawardena et al. [82], Parasuraman et al. [83], Singh et al. [84], and Sivapragasam et al. [85].

@&#CONCLUSIONS@&#
Widely used conventional methods for reservoir sedimentation assessment are reservoir capacity surveys (Range and Contour surveys). These methods are considered to be most accurate, but simultaneously, are cumbersome, time consuming and very expensive. Although the alternate RS technique approach to these hydrographic surveys may be cost effective, easy to use and requires less time in analysis as compared to the conventional methods, they suffer with constraints in terms of ground truth.Due these large constraints in direct measurement techniques, alternative data driven techniques (MLR, ANN, MT and LGP) were adopted for estimation of sediment deposition in a reservoir. The best RBF ANN and PUS MT model resulted in somewhat poor performance with r values for testing period as 0.874 and 0.792 respectively. The study on assessing the volume of sediment deposited also showed that LGP model has outperformed all the other applied data driven models (training, r=0.962; testing, r=0.914) and resulted in better performance even with shorter length of data. In this case, annual rainfall and inflow were determined as the most influential parameters in development of data driven models for sediment deposition studies. The generalisation study of the developed models with the data of another hydro-climatically similar catchment showed that LGP model found to be a better model than any other techniques. It was concluded that this LGP model can be considered as globally generalised model and can perform extrapolation, if required. The study discerned that the highly nonlinear models ANN and GP captured the trend of sediment deposition better than piecewise linear MT model. It was also observed that ANN and MT are data intensive and require large data for better generalisation. Even the range of all the three dataset namely training, testing and generalisation should be same.It was found that these models are more user friendly, cost effective, less time consuming and are recommended for their wide application to other reservoirs. These models are very transparent and convenient to use for its end user. The results were comparable in terms of accuracy with those developed during the last several decades. In the view of the authors the use of data driven techniques in hydrological process estimation has a promising future and further research in this area is highly recommended.