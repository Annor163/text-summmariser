@&#MAIN-TITLE@&#
Quality assessment of data discrimination using self-organizing maps

@&#HIGHLIGHTS@&#
We introduce two SOM-based methods for data clusterization quality evaluation.Each of these methods can be taken as a basis for the feature selection algorithm.This algorithm is suitable when the classes are separated nonlinearly.The algorithm does not require splitting the data into the training and test sets.The algorithm is not time-consuming.

@&#KEYPHRASES@&#
Artificial intelligence,Classification,Data mining,Feature selection,Self-organizing maps,Artificial neural networks,

@&#ABSTRACT@&#
MotivationOne of the important aspects of the data classification problem lies in making the most appropriate selection of features. The set of variables should be small and, at the same time, should provide reliable discrimination of the classes. The method for the discriminating power evaluation that enables a comparison between different sets of variables will be useful in the search for the set of variables.ResultsA new approach to feature selection is presented. Two methods of evaluation of the data discriminating power of a feature set are suggested. Both of the methods implement self-organizing maps (SOMs) and the newly introduced exponents of the degree of data clusterization on the SOM. The first method is based on the comparison of intraclass and interclass distances on the map. Another method concerns the evaluation of the relative number of best matching unitâ€™s (BMUs) nearest neighbors of the same class. Both methods make it possible to evaluate the discriminating power of a feature set in cases when this set provides nonlinear discrimination of the classes.AvailabilityCurrent algorithms in program code can be downloaded for free at http://mekler.narod.ru/Science/Articles_support.html, as well as the supporting data files.

@&#INTRODUCTION@&#
Today, artificial intelligence methods are being implemented for diagnostic tasks increasingly extensively. While undertaking this implementation, one can be faced with the problem of making the most appropriate feature set selection. Variables that describe the objects to be classified must be placed in a set that will provide the most reliable classification. At the same time, the data dimensionality should be as small as possible [1]. If the variables in each class show a normal statistical distribution and the classes can be linearly discriminated, some solutions can be found easily. We can mention as an example the problem of the selection of genes for the molecular diagnostics of tumors and the reduction of their number in the set [1,2]. Different approaches to the feature selection problem are observed in [3]. Usually, feature selection methods are divided into filter and wrapper approaches [3,4]. In [3], an embedded method is also mentioned, and arguments for using two stages for feature selection are provided. The first stage should include filter techniques, and the second stage should include the wrapper approach. Filter techniques work quickly, but they have a disadvantage in that the feature dependencies are not accounted for. Wrapper methods are computationally intensive and have a risk of overfitting. In the most difficult cases, distributions are not normal or even multimodal, and discrimination between the classes is nonlinear. In these cases, artificial neural networks (ANNs) could be implemented to make a discrimination of the classes. Filter methods are not applicable here [3]. To obtain the appropriate set of features in the training vectors, we must introduce some criteria for comparing different sets and, according to these criteria, select the set that will allow the most reliable discrimination into classes [5]. Such a scoring function is necessary for the wrapper methods. The most straightforward method is to evaluate the classification quality for different combinations of features using, for this purpose, the test samples set. However, there can be cases in which the number of samples is too small to be split into training and test sets for this purpose and obtain significant statistics. Additionally, this method is time-consuming.

@&#CONCLUSIONS@&#
In this article, non-time-consuming methods of selecting variables for further training of an artificial neural network are introduced. These methods are based on the self-organizing maps that are implemented for the data clusterization quality evaluation and are applicable in a situation in which there is nonlinear discrimination between classes. Additionally, these methods do not require splitting of the data into the training and test sets; this absence of a splitting requirement makes the methods feasible in situations in which there are small data volumes.The proposed methods can be implemented in combination with metaheuristics methods for optimal feature set selection, as described in [34] or [35]. This approach will reduce the time cost of the calculations significantly and increase the number of available variables.This work was supported by the Russian Foundation for Basic Research [12-04-90434-Ukr_a] and the Grant of the Saint-Petersburg Government (2012).