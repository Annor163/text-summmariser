@&#MAIN-TITLE@&#
Semantic spaces for improving language modeling

@&#HIGHLIGHTS@&#
The unsupervised techniques of word clustering are investigated in this article.Semantic spaces (HAL, COALS, BEAGLE, etc.) are used for clustering of words.Class-based language models are built from clusters and are interpolated with the base-line model.Final models improve performance on Czech, Slovak and English corpora.

@&#KEYPHRASES@&#
Class-based language models,Semantic spaces,HAL,COALS,BEAGLE,Random Indexing,Purandare and Pedersen,Clustering,Inflectional languages,Machine translation,

@&#ABSTRACT@&#
Language models are crucial for many tasks in NLP (Natural Language Processing) and n-grams are the best way to build them. Huge effort is being invested in improving n-gram language models. By introducing external information (morphology, syntax, partitioning into documents, etc.) into the models a significant improvement can be achieved. The models can however be improved with no external information and smoothing is an excellent example of such an improvement.In this article we show another way of improving the models that also requires no external information. We examine patterns that can be found in large corpora by building semantic spaces (HAL, COALS, BEAGLE and others described in this article). These semantic spaces have never been tested in language modeling before. Our method uses semantic spaces and clustering to build classes for a class-based language model. The class-based model is then coupled with a standard n-gram model to create a very effective language model.Our experiments show that our models reduce the perplexity and improve the accuracy of n-gram language models with no external information added. Training of our models is fully unsupervised. Our models are very effective for inflectional languages, which are particularly hard to model. We show results for five different semantic spaces with different settings and different number of classes. The perplexity tests are accompanied with machine translation tests that prove the ability of proposed models to improve performance of a real-world application.

@&#INTRODUCTION@&#
Language modeling is a crucial task in many areas of NLP. Speech recognition, optical character recognition and many other areas heavily depend on the performance of the language model that is being used. Each improvement in language modeling may also improve the particular job where the language model is used.Research into language modeling started more than 20 years ago and has evolved into a very mature discipline. Now it is very difficult to outperform the state of the art. Our research is focused on inflectional languages as we believe that these languages offer some room for improvement. We however also provide experiments for English (which is not a very inflectional language). Even in the case of English, we were able to obtain positive results.Czech and Slovak belong to the Slavic language group. These languages are highly inflectional and have a relatively free word order. Czech has seven cases and three genders. Slovak has six cases and also three genders. The word order is very variable from the syntactic point of view: words in a sentence can usually be ordered in several ways, each carrying a slightly different meaning. These properties of the languages complicate the language modeling task. The great number of word forms and more possible word sequences lead to a greater number of n-grams. Data sparsity is a common problem of language models. In Czech, Slovak and other Slavic languages, this problem is more evident.Class-based modeling is the most popular technique used for reducing the huge vocabulary-related sparseness of statistical language models (Brown et al., 1992). Individual words are clustered into a much smaller number of classes. As a result, less data are required to train a robust class-based language model. Both manual and automatic word-clustering techniques are being used. Standalone class-based models usually perform poorly, which is the reason why they are usually combined with other models. Many researchers have demonstrated that the combination of a standalone class-based language model and a standard word n-gram model reduces the model perplexity (Maltese et al., 2001; Whittaker, 2000; Whittaker and Woodland, 2003).An effective solution for language modeling is to use information about the morphology of the language. In Oparin (2008) experiments with morphological random forests in the Czech and Russian language are shown with the conclusion that they can be used effectively for inflectional languages. Authors of Vaiciunas et al. (2004) describe the language modeling of Lithuanian by means of class-based language models derived by word clustering and morphological word decomposition and their linear interpolation with the baseline word n-gram model. The authors present a perplexity reduction of 8–13% depending on the size of the corpora. A similarly effective solution is to use class-based language models where classes are derived from lemmas and morphological categories (Brychcín and Konopík, 2011). The article shows a perplexity reduction of 10–30% in corpora in the Czech and Slovak languages. A comparative study of several methods using morphological information for modeling conversational Arabic can be found in Kirchhoff et al. (2006). The usage of morphological information seems to be very effective for inflectional languages; however, it requires a huge number of manually annotated texts.In Brown et al. (1992) the MMI (Maximum Mutual Information) clustering algorithm was introduced. This algorithm is based upon the principle of merging a pair of words into one class according to the minimal mutual information loss principle. The algorithm gives very satisfactory results and it is completely unsupervised. Its complexity is however very problematic. This method of word clustering is possible only in very small corpora and is not suitable for large vocabulary applications. The authors in Yokoyama et al. (2003) used the MMI algorithm to build class-based language models. Their linear interpolation with the word n-gram model was applied to speech recognition of Japanese. The authors showed a 2% absolute improvement in word accuracy but only in very small corpora.Several authors have tried to approximate the MMI algorithm to reduce computational requirements and to make it more suitable for large vocabulary language models (Bai et al., 1998; Yamamoto and Sagisaka, 1999). Automatically derived clusters have been used for class-based language models of Japanese and Chinese (Gao et al., 2002). The authors concentrated on the best way of using the clusters; however, they did not focus on how to get them.Another way of improving language models is to use semantic information. This idea is based on the assumption that words with lexically different forms usually share similar meanings in cases where they frequently occur in similar contexts. The semantic information can be calculated using the Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dumais, 1997; Landauer et al., 1998) method or its probabilistic variant, the PLSA (Hofmann, 1999). A similar method to the PLSA is the Latent Dirichlet Allocation (LDA) (Blei et al., 2003) which is essentially the Bayesian version of the PLSA model. Bellegarda and his team were the first to introduce LSA into language modeling (Bellegarda et al., 1996). Their approach consisted in using LSA to derive word clusters for class-based language models.The approach then evolved to focus on documents instead of focusing on words. It is assumed that documents may vary in domain, topic and styles, which means that they also differ in the probability distribution of n-grams. This assumption is used for adapting language models to the long context (domain, topic, style of particular documents). LSA (or similar methods) are used to find out which documents are similar and which are not. This long context information is added to standard n-gram models to improve their performance. A very effective group of models (sometimes called topic-based language models) work with this idea for the benefit of language modeling. In Bellegarda (2000) a significant reduction in perplexity (down to 33%) and relative reduction in WER11The Word Error Rate (WER) measure is often used in speech recognition.(down to 16%) in the WSJ (Wall Street Journal) corpus was shown. Many other authors have obtained good results with PLSA (Gildea and Hofmann, 1999; Wang et al., 2003) and LDA (Tam and Schultz, 2005, 2006) approaches.In Liu and Liu (2007, 2008), the named entity recognition technique was applied to topic modeling. The topic modes were based upon LDA and clustering. The authors tested the hypothesis that named entities carry valuable information which can be useful for latent topic analysis. The authors presented a 14% perplexity reduction as their best result.Some comparisons between PLSA and LDA as well as some clustering methods can be found in Hahn et al. (2008). The authors present their results in English and Arabic broadcast news.An investigation into Topic Tracing Language Models (TTLM) and their application in speech recognition is presented in Watanabe et al. (2011). The TTLM is based on LDA and PLSA and integrates the ability to dynamically track changes in topics. The tracking is based upon focused text information and previously estimated topics.To put our approach into the context of the above-mentioned methods, we can state that our method also focuses on inflectional languages similarly to methods that use morphology. Our approach, however, is unsupervised. Our method also uses semantic information. We do not rely however on LSA, PLSA or LDA but on different methods (HAL, COALS, BEAGLE and others) that were not tested in the language modeling task before. These methods do not require the text to be partitioned into documents as in the case of LSA, PLSA or LDA. Our approach is in many ways similar to the approach of MMI clustering but the methods we use can deal with much larger data.The rest of the article is organized as follows. In the following section, we give an overview of the statistical modeling of semantic information. In Section 3 we explore the application of semantic information in language modeling. Section 4 shows various results of our experiment in detail. In the last sections we discuss the results and we conclude the article.The semantic models investigated in this work are based upon the idea that the word meaning is related to the context in which the word is usually used. The assumption is that two (lexically) different words share a similar meaning if they occur in similar contexts. Some studies (Rubenstein and Goodenough, 1965; Charles, 2000) have confirmed this assumption by empirical tests carried out on human test groups. The implication of the studies is that it is possible to compute the semantic similarity of words by a statistical comparison of their contexts. In this article we use the assumption and its implication to improve language models. Before we introduce the method of incorporating semantic similarity into language modeling we briefly explain several methods for calculating word similarity.In semantic spaces, each word is represented as a highly dimensional vector. The vectors are derived from the statistical properties of words and their contexts in a plain text corpus. The vectors are constructed in such a way that words similar in meaning should have a similar vector. The methods to calculate the vectors differ. In the following sub-sections we briefly explain the methods that were tested in this article.Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996; Burgess and Lund, 1997) creates a semantic space from word co-occurrences. Each word in the training data is examined and those words nearer than a fixed distance are recorded as co-occurring. Such a group of words is called a “window”. The words in the window are weighted according to the distance from the examined word. It is assumed that the closer the word is, the greater the impact it has on the focused word semantics. The co-occurring words are therefore inversely weighted according to their distance from the examined word.These windows are used to construct the |W|×|W| co-occurrence matrixM(|W| is the number of words being analyzed) in the following way. When a wordwjis found in the window of the examined wordwithen a value is added to the mi,jelement in the matrixM. The value depends on the distance of the wordwjfrom the wordwi. The exact formula to calculate the value is defined in Lund and Burgess (1996). The row and column vectors of the matrixMcontain co-occurrence information on words that appeared before and after respectively. HAL therefore also records simple word-ordering information. Naturally, many words do not appear in the vicinity of each other and the matrixMtends to be very sparse.For each word (meaning), certainly not all columns (co-occurred words) provide an equal amount of information. The entropy can be used to retain only a given number of significant columns. In this way, the dimensionality of the matrixMcan be reduced.Correlated Occurrence Analogue to Lexical Semantic (COALS) (Rohde et al., 2004) is a semantic space model based upon HAL and LSA ideas. The process of building the matrix starts almost identically to the HAL methods but COALS adds some tweaks.The algorithm constructs the matrixMin a similar way as HAL does. It however does not distinguish whether a co-occurred word comes before or after the focused word. The window that is used has the same length in both directions. The matrix is also normalized using correlation. Any negative values are set to zero and all other values are replaced by the square root.The final part of the algorithm is inspired by the LSA method. Singular Value Decomposition (SVD) is applied to the matrixMin order to reduce the dimensionality of the vector space (typically, dimension about 800 is used). The SVD reduction has the effect of bringing out the latent semantic relationships between words (as in LSA) so it can discover transitive relations between words. This final stage is not mandatory for the COALS method and is sometimes skipped.Random Indexing (RI) (Sahlgren, 2005) is based on the process of the accumulation of context vectors of words that are co-occurring. This incremental technique is used to construct the semantic space in a completely different way from the above-described models. Instead of constructing a word by word matrix and then deriving the context vectors, the process is reversed. First, vectors are generated and then the matrix is calculated.The Random Indexing method can be described in two step. In the first step, a randomly generated high-dimensional vector is assigned to each word. The dimensionality of vectors typically reaches thousands of dimensions. The vectors consist of a small number of randomly distributed nonzero vector values (−1, +1). In this way it is ensured that two vectors do not overlap very often. The generated vector is known as the index vector. During the second step, the algorithm scans the text and updates the context vectors by summing up all the index vectors of co-occurring words.This method does not require the dimension reduction phase as in the case of HAL and COALS. Here the dimension is set at the beginning and is much lower that in the case of HAL and COALS.In Sahlgren et al. (2008) the Random Indexing method is extended to keep the word-order information. The modification is inspired by the BEAGLE method (Section 2.4), but instead of using convolution operation this method is based upon the permutation of vector coordinates. While both methods are approximative, the permutation is more simple to calculate.Bound Encoding of the AggreGate Language Environment (BEAGLE) (Jones and Mewhort, 2007) is a computational model that builds a semantic space in a similar way to Random Indexing (Section 2.3). During the first phase, a high-dimensional index vector is also randomly generated; however, the values are given according to the Gaussian distribution. The mean value is set to 0 and the variance is set to 1/D, where D is the dimension (by default, D=1024).The meaning of words in the final semantic space is compounded from the co-occurrence information and word order information. The co-occurrence information is calculated as in Random Indexing by summing the vectors of the co-occurring words to the vector of the focused word. The word order information is calculated by convolution of the n-gram vectors that contain the focused word. The final semantic vector is then constructed as a combination of the co-occurrence vector and the word order vector and is sensitive to both the neighboring words and word order.The Purandare and Pedersen (P&P) (Purandare and Pedersen, 2004) model is another form of word sense induction, in which word meaning is inducted from different usages in training data.The process of building the semantic space is divided into two stages. First, the training data are processed and features most likely correlated with focused words are identified. The model uses two kinds of features: co-occurring words (similarly to HAL (Section 2.1) or the COALS model (Section 2.2)) and co-occurring bigrams. The features are selected from a close distance to the focused word (e.g. five-word distance). Features which are statistically significant are kept others are removed. This leads to the removal of words which possibly frequently co-occur with the focused word but which do not have a significant impact on the semantics of the focused word.In the second stage, the algorithm tries to construct the meaning of words from longer contexts (e.g. 20 words on both sides of the focused word). Only words that are in the features of the focused word are included in the longer context vectors, while others are removed. It is expected that these context vectors represent different usages of the focused word in the corpus. These vectors (usages) are then clustered into a predefined number of clusters. Each of the final clusters receives its own semantic vector and represents one of the meaning of the word. In the final semantic space, each word is described by n meanings. Its final semantic vector is created as a combination of clustered vectors.The fact that a word is characterized by a vector opens up the opportunity to easily compare two words. The more similar two words are in meaning, the more similar their vectors should be. The ability to compare two words enables us to use a clustering method. Similar words are clustered into bigger groups of words (clusters).The distance (similarity) between two words can be calculated by a vector similarity function. Leta→andb→denote the two vectors to be compared andS(a→,b→)denote their similarity measure. Such a metric needs to be symmetric:S(a→,b→)=S(b→,a→).There are many methods to compare two vectors in a multi-dimensional vector space. Probably the simplest vector similarity metrics are the familiar Euclidean (r=2) and city-block (r=1) metrics(1)Smink(a→,b→)=∑|ai−bi|rr,that come from the Minkowski family of distance metrics.Another often-used metric characterizes the similarity between two vectors as the cosine of the angle between them. The cosine similarity is defined as follows:(2)Scos(a→,b→)=cos(θ)=a→·b→∥a→∥·∥b→∥=∑aibi∑ai2∑bi2.In statistics, the Pearson product-moment correlation coefficient (sometimes referred to as the PPMCC) is a measure of the correlation (linear dependence) between two variables, X and Y, giving a value between +1 and −1 inclusive. Pearson's correlation coefficient between two variables is defined as the covariance of the variables divided by the product of their standard deviations(3)Scorr(a→,b→)=E[(a→−μa)(b→−μb)]σaσb=∑(ai−μa)(bi−μb)∑(ai−μa)2∑(bi−μb)2,where μais the mean value of the vectora→and σais the standard deviation of the vectora→.This metric is often used in semantic spaces for dense matrices, while the cosine metric is used for sparse matrices.The clustering of words with similar meaning is the most important and simultaneously the most time consuming part in our class-based language models.The goal of clustering is simple; to find an optimal grouping in a set of unlabeled data. There are, however, two problems. Firstly, the optimality criterion must be defined. This criterion depends on the task that is being solved. The second problem is the complexity of the problem. The number of possible partitioning rises exponentially22To be exact, the number of possible partitioning of a n-element set is given by the Bell number which is defined recursively:Bn+1=∑k=0nnkBk.with the number of elements in the set. It is therefore impossible to examine every possible partitioning of even a decently large set. The task is then to find a computationally feasible algorithm that would be as close to the optimal partitioning as possible.In our case, the optimality criterion is supplied from a semantic space and an appropriate similarity metric. The clustering in our case is complicated even more by the fact that we are dealing with really large quantities of data (in the order of hundreds of thousands).The type of algorithm plays a key role. Hierarchical methods go from the bottom (they start with many classes and join them together) and that is problematic in our case. We need relatively few classes and so a lot of joining operations must be executed. On the contrary, the partitioning methods go from the top (they start with one class and split it repeatedly). These methods are more suitable for us since much fewer operations of splitting than joining is required.Even with the partitioning clustering method we struggled with the computation complexity of the algorithms. It was soon apparent that a very efficient algorithm would be necessary. Our task was to experiment with approximative partitioning clustering methods. A useful guide was found in the article by Zhao and Karypis (2002) where a comparison of clustering methods was presented. We were able to conclude that the Repeated Bisection algorithm gave satisfactory results with acceptable computational requirements. We use the implementation of the algorithm from the CLUTO software package (Karypis, 2003).Class-based language models are the state-of-the-art approaches for language modeling. The main task of the approach is to replace the statistical dependencies between words with dependencies between a much lower number of word classes, thus reducing the data sparsity problem.Let W denote the set of possible words (word vocabulary) and C denote a class vocabulary. Then we can define a mapping function m:W→C, which maps every wordwi∈Wto some ci∈C. In our case, the classes are the word clusters derived from particular semantic spaces.The probability estimation of wordwiconditioned by its historywi−n+1i−1(where n is the length of the n-gram) is given by the following formula(4)P(wi|wi−n+1i−1)=P(wi|ci)·P(ci|ci−n+1i−1).We are using the Modified Kneser-Ney interpolation (introduced in (Chen and Goodman, 1998)) which at present is the state-of-the-art approach for smoothing methods. The formula for smoothing of word probabilities is(5)P(wi|wi−n+1i−1)=cnt(wi−n+1i)−D(cnt(wi−n+1i))∑wicnt(wi−n+1i)+γ(wi−n+1i−1)P(wi|wi−n+2i−1),where P() is the probability given by the Modified Kneser-Ney interpolation model and cnt() is the count of n-gram. The goal of discounting function D(cnt) is to save some probability mass for lower-order models. The normalization functionγ(wi−n+1i)∈(0,1)makes the probability distribution sum up to 1. The definition and derivation of this function can be found in the original paper.The main advantage of Modified Kneser-Ney smoothing is the clever way it calculates the unigram probability distribution(6)P(wi)=N1+(•wi)N1+(••),where symbol • means an arbitrary word (class) andNr(wi−n+1i)is the number of n-grams with frequency r (i.e. the number of such n-grams, wherecnt(wi−n+1i)=r). In different words, the unigram probability ofwiis given by the number of different bigrams ending inwidivided by the total number of different bigrams.We use a simple but very effective linear interpolation for combining different language models(7)PLI(wi|wi−n+1i−1)=∑k=1Kλk·Pk(wi|wi−n+1i−1),where λkis the weight of the kth language model Pk(). We use the Expectation Maximization (EM) algorithm described in Dempster et al. (1977) to calculate optimal weights λkby way of maximization of the probability of the held-out data.The linear interpolation can be extended to a method called bucketed linear interpolation, where weights become the function of the frequency of word history (Bahl et al., 1983). The main idea is that the weights λkshould be different for words with histories of varying frequencies. The formula then transforms to(8)PBLI(wi|wi−n+1i−1)=∑k=1Kλk(wi−n+1i−1)·Pk(wi|wi−n+1i−1).The weights λk() certainly cannot be different for each possible frequency of history. Too much of weights would be created and too little data would be available to train them. Instead, the whole frequency spectrum is divided into buckets, where each bucket holds some range of frequencies. Histories in buckets have the same weights. The number of buckets can be tuned in but it generally depends on the amount of training data available. The more training data are available, the more buckets can be used.

@&#CONCLUSIONS@&#
