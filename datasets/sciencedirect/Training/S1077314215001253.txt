@&#MAIN-TITLE@&#
Image segmentation via multi-scale stochastic regional texture appearance models

@&#HIGHLIGHTS@&#
We propose multi-scale stochastic regional texture appearance models for image segmentation.An image representation is constructed using an iterative bilateral scale space decomposition.Local texture features are extracted via random projections obtaining stochastic texture features.A texton dictionary is built and is used to represent the global texture appearance model.Based on experiments and comparisons, the method can be effective for textured images.

@&#KEYPHRASES@&#
Image segmentation,Texture segmentation,Stochastic texture models,Regional texture appearance models,

@&#ABSTRACT@&#
An ongoing challenge in the area of image segmentation is in dealing with scenes exhibiting complex textural characteristics. While many approaches have been proposed to tackle this particular challenge, a related topic of interest that has not been fully explored for dealing with this challenge is stochastic texture models, particularly for characterizing textural characteristics within regions of varying sizes and shapes. Therefore, this paper presents a novel method for image segmentation based on the concept of multi-scale stochastic regional texture appearance models. In the proposed method, a multi-scale representation of the image is constructed using an iterative bilateral scale space decomposition. Local texture features are then extracted via image patches and random projections to generate stochastic texture features. A texton dictionary is built from the stochastic features, and used to represent the global texture appearance model. Based on this global texture appearance model, a regional texture appearance model can then be obtained based on the texton occurrence probability given a region within an image. Finally, a stochastic region merging algorithm that allows the computation of complex features is presented to perform image segmentation based on proposed regional texture appearance model. Experimental results using the BDSD300 segmentation dataset showed that the proposed method achieves a Probabilistic Rand Index (PRI) of 0.83 and an F-measure of 0.77@(0.92, 0.68), and provides improved handling of color and luminance variation, as well as strong segmentation performance for images with highly textured regions when compared to a number of previous methods. These results suggest that the proposed stochastic regional texture appearance model is better suited for handling the texture variations of natural scenes, leading to more accurate segmentations, particularly in situations characterized by complex textural characteristics.

@&#INTRODUCTION@&#
Image segmentation is one of the most challenging problems in computer vision, and deals with the partitioning of an image into a set of disjoint segments (or regions), in a way that pixels with similar characteristics are grouped together in homogeneous segments that could be associated with different objects.Different approaches have been proposed to measure pixel similarity using image characteristics, such as intensity, color, texture, or other features. The similarity criterion to be used generally depends on the context, as well as on the desired details in the image representation.Texture is one of the most discriminative characteristics for evaluating similarity in image segmentation [1,2]. The main idea of texture is that image features can be better expressed by adjacent groups of pixels instead of isolated ones. In a more formal definition, a texture is a visual pattern (deterministic or stochastic) repeated over some area, in which the basic elements are called texture primitives or textons[3]. Next, we review some representative methods related to texture feature extraction, representation and region aggregation.Texture features may be seen as local variations across the image [4–6]. Such variations may be captured by Gabor filters [7], wavelet decomposition [8,9], and combinations of directional filters [10–14]. These approaches may produce accurate texture boundary information for a variety of applications, but accurately representing large regions in an image or handling intra-class texture variability remains challenging.Texture features can be learned in a way to reduce the number of features while improving class separability. For example, SIFT features and optimal spaces may be learned [15], or convolutional neural networks may be used for learning optimal features [16]. Despite the good performance of the aforementioned techniques in some contexts, supervised training is required to learn adequate parameters, which limits their applicability.A comprehensive set of local image statistics (i.e. within small adjacent pixel neighborhoods) can be used for representing textures [11,12,17]. However, pixel-wise statistics may not capture the long range correlations existing in some texture classes (e.g. natural scene textures).Recent research have addressed the apparent paradox of pixel-wise and region-wise texture representations [18–20], and image patches (i.e. small neighborhoods around pixels) have been proposed as feature vectors. This simple approach potentially can be effective to improve texture class separability [21]. Actually, patch features usually are obtained in a supervised manner, and can be made compact and rotationally invariant [19]. The potential of unsupervised schemes for patch features extraction remains largely unexplored, and is one of the contributions of this paper.Unsupervised image segmentation often relies on clustering texture features by maximizing/minimizing some energy or cost function [22,23]. In this case, most pixel pairs should be considered, but that may be expensive. Alternatively, sparse representation strategies like bag-of-features (BoF) may be used, which involves obtaining a textural feature occurrence histogram [24] by clustering feature vectors (e.g. using K-means [18] or K-SVD [25]) or by identifying textural feature prototypes (e.g. using the nearest neighbor rule or L1-type optimization [15]). The BoF approach can benefit by using many types of features [18,19,24–26], and later we present a new stochastic BoF scheme for unsupervised image segmentation.In order to compare image segments and obtain an image segmentation, the affinity among segments must be computed. Some methods model affinities and use graph cuts to group segments, obtaining the final image segmentation [22,23]. Alternatively, clustering algorithm may group adjacent segments using spatial constraints [20,27]. Recent research indicate that statistical criteria and simple features such as intensity or color with pixel-wise region initialization can improve segment merging and image segmentation [28–30]. As proposed in [28], adjacent region merging can be based on statistical criteria to discriminate regions sizes and intensities/colors. This concept was later extended in [29] to use a stochastic criteria based on the similarity likelihood of the image regions.The methods mentioned above perform on single scale image representations. However, Deng and Manjunath [10] suggested that multi-scale region growing, were iterative region growing algorithms are used to segment the image in decreasing level of detail (analogous to multi-scale methods) may be advantageous in image segmentation. In fact, using similarity likelihood of multi-scale regional descriptors can improve on those methods results, as discussed later in this work.We propose a novel image segmentation method that combines the benefits of discriminative texture features and of region merging-based segmentation approaches. This work introduces a new stochastic regional texture appearance model and a stochastic region-merging segmentation framework. The addition of stochastic elements in these process will better account for intra-class uncertainties as well as for color and illumination variations across the image. Specifically, this paper contributes in the following aspects:1.A bilateral scale space image decomposition technique is presented for the purpose of feature modeling, which will split the image details in levels of granularity (from coarse to fine).An innovative way of combining the bag-of-features and region merging strategies is presented, allowing a smooth transition from the pixel-wise features to region-wise representation as the regions grow in the region merging stage.By combining Stochastic Patch Features (SPF) and stochastic region merging, a new unsupervised image scheme is proposed that potentially can provide better results than available methods representative of the state-of-the-art.The proposed image segmentation technique has three main stages (as shown in Fig. 1): (i) feature extraction, (ii) texture description, and (iii) region merging. In the first stage, we aim at obtaining the low-level texture information at each point in the image. To do so, we employ a multi-scale bilateral scale space image decomposition strategy (to enhance the separability between coarse and fine textures). From this decomposition, patch features are extracted to construct a set of Stochastic Patch Features (SPF) via random projections. In the texture description stage, the SPF are used to produce a regional texture appearance representation. We construct a feature dictionary (from the SPF) to compose this appearance representation, and choose a bag-of-words technique to describe the segments in the image. The final stage of the proposed method is responsible for detecting the boundaries between texture regions in the input image. In here we employ an iterative region merging segmentation strategy, which uses the previously computed stochastic regional texture appearance representation.The remaining of this paper is organized as follows. Section 2 presents the proposed multi-scale feature extraction process to obtain the SPF representation. Section 3 describes the proposed region-wise texture appearance representation for characterizing texture properties for regions of arbitrary size and shape. Section 4 details the stochastic region merging algorithm based used to aggregate the pixels into texture regions. In Section 5, we present and discuss experimental results, as well as a comparison of our results with several state-of-the-art segmentation methods. Finally, in the Section 6, conclusions and future work are discussed.Let I be the input image which we desire to segmented, and consider that it is a 3-channel color image represented in the CIE L*a*b* color space, so there is minimal correlation between the luminance and color channels. The texture features proposed for representing the textural information at each point in the input image are obtained by applying three key steps. The first task is to represent I in a multi-scale manner, where fine to coarse details are decoupled. This is done via an iterative bilateral scale space decomposition. The second task is to extract the patch multi-scale features from the image. The final task of feature extraction is to perform dimensionality reduction in a way that is robust to texture rotation and translation, which is accomplished here via random projections. The pipeline of actions performed to accomplish these tasks can be seen in Fig. 2.In the proposed work, we wish to not only identify the texture characteristics of the image in matters of luminance and color channels (e.g., a set of texture features for each of the CIE L*a*b* channels), but also at different levels of details (e.g., texture features from coarse scale to fine scale). Therefore, to allow texture analysis and representation at many levels and to improve the discriminatory power of the extracted texture features, the image is decomposed into multiple scales based on the concept of bilateral iterative scale space [31]. The iterative bilateral scale space (BISS) was designed to decompose image details based not only on spatial locality, but also on photometric differences, resulting in a nonlinear multi-scale decomposition where image details at different scales are not only well separated, but also are well localized and well preserved.Let c denote a channel in {L, a, b} color space. For a given image at a given channel fc(j), where j denotes a pixel location, the multi-scale image decomposition f′c, iconstructed using iterative bilateral scale-space can be defined by a family of derived images F′c, i,(1)f′c,i(j)=∑q∈Nwp(j,Nq)ws(j,Nq)f′c,i−1(q)∑q∈Nwp(j,Nq)ws(j,Nq),wheref′c,0=fc,i ≤ N denotes scale,Nqdefines the pixel location within a local neighborhoodN,and wpand wsdenote Gaussian photometric and spatial weights on j, respectively,(2)wp(j,N)=exp[−12(∥f′c,i−1(x̲)−f′c,i−1(N)∥σp)2],(3)ws(j,N)=exp[−12(∥j−N∥σs)2],where, σpe σsare the standard deviation for the Gaussian model adopted in the photometric and spatial weights, respectively.Thus, while wsindicates a traditional Gaussian blurring kernel (considering a pixel’s spatial proximity to the central pixel), wprepresents an intensity disparity between the neighborhood pixels and the central pixel (i.e., photometric differences). The combination of these two weight functions in Eq. (1) results in an image decomposition where the image representation becomes less detailed with each increasing level (because of the spatial weights), but will also retain edge fidelity and locality (because of the photometric weights).Given the aforementioned multi-scale image decomposition, we wish to extract texture features and construct descriptors at each scale independently, so that each scale will provide a unique texture description.While features such as pixel intensity and color provide useful information about the edges (large gradients) within a picture, such information may not suffice for the purpose of automatic image segmentation, particularly when dealing with highly textured regions characterized by large local intensity variances. Achieving a better segmentation in these cases requires the use of more sophisticated texture features. Although there does not exist an universally accepted definition of texture, the main idea is that an identifiable local pattern repeats in some area, and the elements defining this pattern are called texture primitives (or textons) [1,3], that may be either periodic or stochastic.Some texture representation approaches represent texture patterns using local differences like gradients or Gaussian derivatives [17,18,32]. These texture features detect changes in brightness or color, so by computing these differences in several directions and scales, such as in Gabor filters or wavelet transforms, robust texture features are obtained. From the convolutions used to generate these local differences, Varma and Zisserman [21] observed that these texture features essentially are lower-dimensional projections of an image patch (i.e., a small neighborhood around a pixel). Therefore, instead of computing pixel to pixel differences as texture characteristics, our proposal is to directly use image patches as feature vectors. Such an approach have been proved to be as effective, or even more accurate than the gradients [19], with the advantage of a simple feature extraction process[21].In this work, the patch features are extracted from the bilateral scale space image decomposition F′. For a given pixel j, we define the patch feature vectorsNc,i(j)as lexicographically ordered vector representation of a square neighborhood of widthNscentered at j, in the scale f′c, iof the bilateral scale space decomposition. Observe that, for each pixel, a feature vector is extracted from each combination of the neighboring pixels colors and decomposition level (see Fig. 2).Although simply taking an image patch around each pixel as a feature vector provides a good texture representation, it still can be improved [19,33] for efficiency and robustness purposes. Here, we wish to improve the proposed features in two aspects: (i) invariance in the representation of the rotated textures, and (ii) dimensionality reduction.The patch features are, by definition, invariant to translation over the image, but since such features depend essentially on the spatial relationship (distance and angle) between neighboring pixels, they tend to become very sensitive to image rotations.Previous works on the literature have proposed to introduce this property by adding rotated patches to the texture model formulation, or by estimating the dominant gradient orientation in each texture patch [21], but in practice, these methods still present some problems [34]. The dominant orientation estimate tend to be unreliable for some texture regions, and the patch rotations make the feature clustering more expensive. In this work, rotation invariance is achieved by sorting the values of the patch feature vectors(4)Nc,i′(j)=sort(Nc,i(j)),where, sort denotes the sorting operation, andNc,i′(j)is the sorted patch vector for pixel j. As the sorting result is independent of the original positioning of elements inside the patchNc,i(j),the sorted featuresNc,i′(j)will be invariant to changes in texture rotation.Another important aspect that needs to be considered is the size of the feature vector. Considering a 3-channel color image I, represented in N levels of the BISS, and sorted features extracted from image patches with width w, each pixel will be represented by3×N×(Ns)2features (3N vectors of size(Ns)2). In a numerical example, usingN=3levels and 3 × 3 patches, each pixel will have 81 features; usingN=4levels and 5 × 5 patches will produce 300 features per pixel. This happens because the number of features grow linearly as N increases, but it grows exponentially in relation to w.Using too many features in unsupervised classification tasks (such as image segmentation) is known to harm the final result due to the lack of information of the feature space, and to the noisy information that is prone to happen in some features. To avoid these complications, dimensionality reduction may be applied to the proposed features. Ideally, a dimensionality reduction should be able to capture the most relevant information in the analyzed data, while decreasing the influence of irrelevant and incoherent channels. In this work, we employ the concept of random projections [19], which is able to efficiently capture the relevant texture information contained in the multiple texture patches.A random projection (RP) is a simple technique for reducing the dimension of sparse data (i.e., containing few non-zero values), which efficiently captures the most salient information in the source data. The RP works as a linear projection defined by a random matrixΦ∈Rm×n,with m ≪ n. Ideally, this projection must ensure that discriminative information in the original signal is preserved in the new representation, i.e., the distance between two vectors must be approximately the same before and after the projection [33]. It was shown by [35,36] that information preservation is ensured if the projection matrixΦ={ϕMiMj}(with1≤Mi≤mand1≤Mj≤n) is defined in a random, stochastic manner asϕMiMj={0withprobability12,1withprobability12.Random projection (RP) tends to compress sparse vectors while improving the discrimination of vector clusters in the obtained lower dimensionality space [37–39]. As shown in previous work in texture representation [19], an image patch can be seen as a sparse vector. Therefore, it can be represented by RP with reduced dimensionality so image patches clusters are more easily discriminated in feature space (see Section 5). Using the formulation proposed above, we define a stochastic texture representation (STR) vc, i(j) for the proposed features as(5)vc,i(j)=ΦNc,i′(j)=Φsort(Nc,i(j)),where j denotes the pixel location, c the color channel, and i the scale. A practical example of how the RP is applied to the patches is shown in Fig. 3. In this example, a RP matrix (Fig. 3(a)) is stochastically generated as shown in Fig. 3(d), and applied to a sorted patch vector (Fig. 3(b)), producing a projected vector (Fig. 3(e)), which will be the STR feature vector (Fig. 3(c)). It shall be observed that the RM matrices are initialized randomly (using random seeds). Therefore, if the same image patch occurs in distinct image locations, different STR vectors are generated to represent this patch (because of the non-deterministic nature of the stochastic process). However, these distinct STR vectors shall be similar and assigned to the same cluster.In this fashion we are able to produce a more efficient set of features than the original image patches. Not only are the STR features invariant to the texture rotation, they also have a much lower dimension than before, more robust to the presence of noise, and tend to improve data separability.Also, the RP matrixes are obtained with a pseudo-random number generator (initialized with pseudo-random seeds), so if the same patch vector occurs in different parts of the image, different STR vectors may be obtained for this patch. But, because of the inherent nature of a stochastic process, the final result will not change significatively by using different seeds.It may be observed that the multi-scale texture representation used in this work provides local statistics of multiple orders, and the texture is described in multiple levels of details. However, since different statistics are captured when scale changes, our STR features remain relatively sensitive to the size of the texture primitives. This implies that even if multiple order statistics are captured, the texture representation is not scale invariant, and will require a customized setup for segmenting textures at different scales.As discussed earlier, while simple features such as intensity and color variation may provide sufficient information about the edges of a picture in a general way, they are limited in their usefulness when characterizing the boundaries of highly textured objects. More complex texture features, such as multiple gradients and image patches (including the proposed STR features) can account for such situations, since these local features will capture patterns that are indefinable in the small area of the neighborhood around a pixel. While increasing the size of the image patches would allow the recognition of bigger texture primitives, it would harm the separation between different textures with smaller primitives. Therefore, rather than just taking local features, more comprehensive texture descriptors need to be used to achieve a more precise segmentation by representing not only local features, but also global appearance characteristics [19]. Moreover, since the goal of this paper is to perform reliable segmentation of texture regions within an image, we also need to define a representation for the texture information within a region, which may have many pixels and shall have and unknown arbitrary shape. Motivated by this, one of the key aims of this paper is to introduce a robust multi-scale regional texture appearance model for characterizing textural properties at multiple levels, within a region of arbitrary size or shape.By the previous definition of the texture primitives, we can consider a texture to be a pattern, stochastic or periodic, that is repeated over some area. Under such consideration the textural information in a region can be described by the statistics of the features occurring in that region. In this work, we introduce a bag-of-features strategy for constructing regional texture appearance models, and therefore describe the textures with the assistance of a dictionary of textons (texture primitives) [19,21].The bag-of-features strategy aims to represent a feature space with the aid of a finite and well defined set of feature prototypes, which are usually obtained by clustering the available samples. Different sets of data points in this feature space are then described by the histogram counts of all the prototypes. This general strategy has been studied for texture recognition in recent works [19], showing very promising results towards efficient and accurate description of a variety of textures, but remains insufficiently explored in image segmentation applications. In this work, we propose a novel strategy for combining such texture primitive dictionaries as a regional textural appearance model with region merging segmentation techniques.Most of existing texture recognition methods work with bag-of-features cluster features separately for each known texture class, producing a set of prototypes (i.e., cluster centroids) that will be strongly related with each class. These prototypes are called textons, and the collection of all textons from all classes will be the so called dictionary of textons[18,40].In this work, at a particular channel c and scale i, all of the extracted STR feature vectors are clustered using the k-means algorithm [41] to build a texton dictionary, where each texton is a found centroid, as can be seen in Fig. 4, which illustrates the proposed texton dictionary construction method. Previous works on texture recognition via texton dictionaries have shown that dictionaries containing textons equally from all classes will allow a fair representation of all textures in the training set [21]. But, since this work aims to tackle the automatic unsupervised image segmentation problem, there is no information available about the number of texture classes in the image; therefore, the textons will be determined from samples of the whole image regardless of its texture class in building a global texture appearance model. Through the use of such an approach, we are supposing that different texture classes will form different clusters in the feature space, which is the main idea behind such clustering techniques. Also, different textures may have a different number of textons in the dictionary. Here, for the purpose of image segmentation, we intend to show through our experiments that, using a pre-defined number of texton prototypes (centroids), we can build a texton dictionary that can represent all the textural characteristics reliably and efficiently as a global texture appearance model (see Section 5).Once the texton dictionary has been determined, a regional texture appearance descriptor T for an arbitrary texture region R can now be defined based on the texton occurrence probabilities at the different channels and scalesT(R)={Hc,i(R)|1≤c≤3,1≤i≤N},where, Hc, i(R) is the normalized histogram of texton occurrence (probability) in the region R for channel c and scale i. Since, we are dealing with 3-channel color images, represented in N levels of details of the bilateral scale space, a texture is now described by a set of 3N histograms, one for each combination of channel and scale. As such, the proposed regional texture appearance models accounts for color textural characteristics at multiple scales, and is suitable for regions of any shape or size.Given that the proposed regional texture appearance model is composed of a set of normalized texton occurrence histograms, the dissimilarity between two regions may be computed quantitatively by comparing the set of texton occurence histograms between the regions. Estimating whether the two distributions differ, or are consistent, is a problem that arises frequently, and there is a variety of methods for solving it [42]. In this work, we use the Bhattacharyya distance for measuring the dissimilarity between two normalized texton occurrence histograms. This metric is robust, unbiased, and have become commonly used for the comparison of two probability density functions (PDF). As a normalized histogram can be seen as a discrete estimation of a PDF, the Bhattacharyya distance can be used to compute the dissimilarity between two normalized histogramsp^andq^as(6)DB(p^,q^)=−ln(∑ip^iq^i).Since, the texture information within a region is described by a set of histograms, each histogram must be normalized (prior to comparing the Bhattacharyya distance) by dividing each bin by the total number of the samples in that histogram, so every bin indicates the probability of a dictionary texton occurring in a given texture patch at that channel and scale.Finally, the texture dissimilarity dTbetween two regions Raand Rbcan be defined as:(7)dT(Ra,Rb)=Wg[∑i=1NDB(HL,i(Ra),HL,i(Rb))∑i=1NDB(Ha,i(Ra),Ha,i(Rb))∑i=1NDB(Hb,i(Ra),Hb,i(Rb))]∑Wg.whereWg=[wL,wa,wb]is a weight given to each channel, which will determine the contribution of each channel in the segmentation result (for more details see Section 5).Once the STR features have been extracted from all pixels in the image, and the multi-scale regional texture appearance descriptors have been properly defined for all regions within the image, the segmentation process is ready to begin. In this section, we focus on defining the procedures involved in the proposed region merging technique, and how they will integrate with the regional texture appearance model for identifying the boundaries between adjacent textures of the input image.Let J be the discrete lattice where the image is defined, and j ∈ J be a site in this lattice, referring to a pixel location. Also, letFjP={fj|j∈J}be the texture observed at the image site j, andR={rs|s∈S}a texture region label field. The image segmentation can be formulated as a maximum a Posteriori (MAP) estimation problem [29,43](8)r^=arg maxr{P(r|f)},where P(r|f) is the posterior. From the Bayes theorem, solving Eq. (8) is equivalent to solving(9)r^=arg maxr{P(f|r)P(r)},where P(f|r) is the likelihood, and P(r) is the label prior. Although this is a simple formulation for this problem, it may be difficult to solve directly due to terms P(r) and P(f|r), that are generally unknown.The estimation ofr^is approached as a region merging (RM) problem, as explained next. Conditional random fields (CRF) or Markov Random Fields (MRF) [44] are commonly used for representing relations between pixels. Also, graph cuts can be used to maximize the probabilities in Eq. 8[45], however this method requires assembling a pixel-wise similarity matrix, which may be quite expensive. We solve the MAP problem in Eq. 8 iteratively, bottom-up, using a stochastic RM strategy, providing an approximate solution at a reasonable cost. The motivation for using this bottom-up strategy is to solve the labeling problem iteratively, by combining MRF-like estimates and region merging. In this way, the locality of the MAP inference is reduced to small pixel neighborhoods, favoring pixel-wise similarities [43]. The a priori probability P(r) is estimated in way that favors pixel-wise similarities [46], and the likelihood P(f|r) is approximated by a merging likelihood function that favors increasing patch-texture class similarity.More precisely, an approach similar to MRF is used to represent the pixel-to-pixel relations as an adjacency graph, accounting for P(r), and the merge criterion will account for P(f|r), so successively merging the nodes in this graph will maximize the class likelihood. To maintain the process consistent and make it more robust to local variations (e.g. local variations of luminance, color, and noise or artifacts in highly textured regions), the pairs of texture regions are sorted by ascendant dissimilarity. Finally, the segmentation is refined by an iterative merging process.As mentioned above, one effective way of handling local variations in the image is to model the prior probability P(r) [46] as in MRF. In the proposed approach the prior probability P(r) represents interactions between neighboring pixels, i.e. the probability of neighboring pixels belonging to the same class.In the beginning of the texture segmentation of a given input image I, with SN× SMpixels, we assume no prior knowledge and make no assumptions about the texture regions in the image, so there is no information on how many distinct texture regions are in the image. So, before we successively aggregate pixels forming larger texture regions, we assign a unique texture region labelR={1,⋯,SN×SM}to each pixel.In the initial stage of the segmentation process, we build a region adjacency graphG=(R,E)representing its current status. In this graph, each vertex represents a region R and the edges E connecting the vertices correspond to dissimilarities between neighboring regions. In the initial stage of the segmentation, G is set up to represent each pixel p as an unique region (vertex), and the edges are the 4-neighborhood of the pixels. The aforementioned scheme for adjacency graph initialization is represented in Fig. 5. At any subsequent state of the segmentation process, each vertex (texture region) may have an arbitrary number of pixels, and be connected to a different number of other vertexes (adjacent texture regions, which may be different of 4).Once the initial state of the adjacency graph have been set up, the algorithm proposed here begins by sequentially analyzing E and merge edges. Two adjacent regions Rxand Ryare merged with a probability of α(Rx, Ry), which accounts for P(f|r) in Eq. (9), and is obtained from a likelihood function. In this work, we use a texture region likelihood function that extends upon the stochastic region merging criterion proposed by Wong et al. [29](10)α(Rx,Ry)=exp[−dT(Rx,Ry)Λ(Rx,Ry)],where dT(Rx, Ry) is the texture dissimilarity between Rxand Ry, defined in Eq 7, and Λ is a statistical merging penalty, based on the size of the regions, defined as(11)Λ(Rx,Ry)=DI22Q[ln(Ψ(I)2)Ψ(Ra)+ln(Ψ(I)2)Ψ(Rb)],where Ψ(R) is the number of elements (pixels) in the region R, consequently Ψ(I) in the number of pixels in the image; DIrepresents the range of possible values in I (usually 256), and finally Q is a regularization term, which controls the merging likelihood (see Section 5).To determine if two regions should be merged, the likelihood function is compared with a random number u, withP(u)=unit(0,1). The regions are merged if the likelihood function value satisfies the following predicate:(12)P(Rx,Ry)={1ifu≤α(Rx,Ry),0otherwise.Compared to the stochastic region merging proposed by Wong et.al. [29], the proposed merging criterion is able to handle texture features more robustly. The merging criterion proposed in [29] was limited to simpler features such as color and luminance, in which dissimilarity can be easily measured by the difference of expected value of the regions. With the formulation proposed in Eq. (10), the merging criterion is adequate for more sophisticated texture representations, such as the multi-scale regional texture appearance model proposed here.During the merging process, whenever a pair of adjacent texture regions is analyzed, they are removed from the queue. Every time a pair is merged, the adjacency graph G is updated and the priority queue is modified to reflect these changes. Once, all the pairs of adjacent regions have been evaluated, the priority queue becomes empty, and the resulting adjacency graph will yield the initial segmentation result.From the proposed image segmentation algorithm formulation, we can detail some proprieties and advantages that arise from employing the proposed stochastic multi-scale texture model jointly with stochastic texture region merging strategy. Texture representations of small regions, with few pixels, tend to be statistically poor, and texture regions of the same type may be regarded as dissimilar. However, using Eq. (10) the merging probability increases as the texture region size decreases, so the small regions have higher probability of being merged. This not only helps handling noise and artifacts, but also makes the process more robust to over-segmentation (as smaller regions segmented at the early stages of the segmentation process tend to be merged). Taking advantage of this characteristic and to help merging texture regions consistently, the adjacency graph edges are ordered in a priority queue. Each pair of adjacent regions is associated with a unique adjacency graph edge, and this priority queue is used in all merging tests, where the edges are placed in the decreasing order of their weights (i.e. similarities, as discussed next).Since smaller regions are more likely to be merged, the order of the priority queue is crucial in the region merging process. In the initialization, each pixel j ∈ I is initially assigned to a unique region R in the adjacency graph, and region pairs are sorted based on their pixel-wise differences. In regions with just one pixel, the texture region description would be inaccurate for measuring the texture dissimilarity with Eq. (7), so we initialize the edge weights with the local image gradients. To incorporate some neighborhood information in the initial stage, we compute the gradients of the image multi-scale representation I′ as pixel-to-pixel differences. Since, this image representation describes each pixel by multiple order statistics, it produces a richer representation than the raw pixel values. Still, when computing the merging probability as the texture regions grow in size, the comparison between textons histograms of small regions cannot be avoided. To avoid the inaccuracy of an histogram representation of very small sets, whenever a region has less than 10 pixels, the texture model for that region is obtained from the same region dilated with an 8-neighborhood structureOn the other hand, the more similar two texture regions are the larger the value of α, promoting the homogeneity within the segmented texture regions as the segmentation process develops, even when regions are small. Also, the merging probability heavily depends on the regularization term. Larger values of Q tend to produce smaller α values, reducing the merging probability and increasing the number of texture segments found in the image. Yet, reducing the value of Q will increase the merging probability, consequently reducing the chance of obtaining over-segmentation.After the priority queue has been emptied, the adjacency graph represents the texture segmentation of the image. To ensure a more reliable segmentation (less sensitive to variations in the stochastic factors), the segmentation result is refined by repeating the stochastic texture region merging process.As illustrated by Fig. 6, the regions obtained in the first iteration are now used to build the new adjacency graph, and the new pairs of adjacent regions are inserted in the priority queue, which is sorted in the ascending order of the region dissimilarity. Differently from before, when the regions only had one pixel, the regions on the initialization map are larger, allowing the priority queue to be sorted according to the texture dissimilarity dTof the regions pairs. The merging process is then repeated with a smaller regularization term. The value of Q is exponentially decreased in each iteration according to(13)Qk=(Q−Qmin)*exp(1−k)+Qmin,where k ≥ 1 is the current iteration,Q=Q1is the value of the regularization term in the first iteration,Qmin=200is the minimum value for this term, and Qkis the value effectively used in the kth iteration of the region merging process.At each iteration, the number of regions is expected to be smaller than in the previous iteration, making the segmentation more accurate. These iterations are repeated until the segmentation of the image has converged, i.e., until a iteration produces no alterations in the adjacency graph (in our experiments, we also limit the process to 10 iterations as empirical results showed that it provides strong results). With these iterations we can avoid under-segmentation by setting a higher value for Q, with a lower risk of over-segmentation.Fig. 7 shows the partial results on each iteration of the proposed segmentation technique when applied to an example image.

@&#CONCLUSIONS@&#
In this paper, we tackle the problem of texture image segmentation. Even though there is a large variety of techniques to solve this problem available in the literature, the use of stochastic texture models remain insufficiently explored, particularly when dealing with regional representations for image segmentation. In this work we introduce a novel stochastic, multi-scale approach for regional texture appearance modeling for the purpose of image segmentation.Experimental results show that the proposed method tends to provide more accurate texture image segmentations than other methods previously proposed in literature (obtaining a PRI=0.83−+0.01andF=0.77@(0.92, 0.68) in our experiments). Visually, the proposed technique tends to be more robust to color and luminance variations inside the textures, as well is able to find the boundaries between similar textures, while avoiding over-segmentation in highly textured regions. Considering both visual and objective analysis and our comparative results, it becomes clear that stochastic texture models are better suited for handling the possible texture variations of natural scenes, and thus, lead to a more accurate segmentation.Future work include investigating the effectiveness of the proposed technique for recognizing specific textures on natural scenes, as in skin detection, material recognition or skin lesion segmentation. Other possibilities include studying novel manners of contracting an optimal dictionary for image segmentation in an unsupervised manner.