@&#MAIN-TITLE@&#
Chain code compression using string transformation techniques

@&#HIGHLIGHTS@&#
String transformation techniques for lossless chain code compression are examined.Burrows–Wheeler transform and Move-to-front transform are applied.We examine the effects of using simplified run-length encoding.The proposed method outperforms state-of-the-art chain code compression methods.

@&#KEYPHRASES@&#
Chain codes,Lossless compression,Burrows–Wheeler transform,Move-to-front transform,

@&#ABSTRACT@&#
This paper considers the suitability of string transformation techniques for lossless chain codes' compression. The more popular chain codes are compressed including the Freeman chain code in four and eight directions, the vertex chain code, the three orthogonal chain code, and the normalised directional chain code. A testing environment consisting of the constant 0-symbol Run-Length Encoding (RLE0L), Move-To-Front Transformation (MTFT), and Burrows–Wheeler Transform (BWT) is proposed in order to develop a more suitable configuration of these techniques for each type of the considered chain code. Finally, a simple yet efficient entropy coding is proposed consisting of MTFT, followed by the chain code symbols' binarisation and the run-length encoding. PAQ8L compressor is also an option that can be considered in the final compression stage. Comparisons were done between the state-of-the-art including the Universal Chain Code Compression algorithm, Move-To-Front based algorithm, and an algorithm, based on the Markov model. Interesting conclusions were obtained from the experiments: the sequential uses of MTFT,RLE0L, and BWT are reasonable only in the cases of shorter chain codes' alphabets as with the vertex chain code and the three orthogonal chain code. For the remaining chain codes, BWT alone provided the best results. The experiments confirm that the proposed approach is comparable against other lossless chain code compression methods, while in total achieving higher compression rates.

@&#INTRODUCTION@&#
Efficient representation of information has remained a challenge since the earliest days of computing. During those times, Freeman [1] invented a method (known as a chain code) for representing the borders of rasterised geometric shapes. Since then the chain codes have become the popular representation methods within various scientific and engineering disciplines [2–11].The chain code consists of a small number of instructions, which determine the boundary of a rasterised shape. The original Freeman chain code (in the continuation denoted as F8) contains 8 symbols describing a pixel's neighbourhood with 8-connectivity. This connectivity is coded with 8 codes from the alphabetΣF8={0,1,2,3,4,5,6,7}. Also a pixel's 4-connectivity neighbourhood can be used leading to the Freeman chain code in four directions (F4) having the alphabetΣF4={0,1,2,3}. Instead of pixels, the sequence of boundary edges can be described as proposed by Nunes et al. [12]. The alphabet of their Differential Chain Code (DCC) contains only three elements,ΣDCC={R,L,S}, whereR,L, and S stand for right, left, and straight, respectively. Another chain code with only three symbols in the alphabet,ΣVCC={1,2,3}, is the Vertex Chain Code (VCC) proposed by Bribiesca [13]. The elements of VCC represent the number of a shape's boundary pixels meeting within the considered raster vertex. In 2005 Sánchez-Cruz and Rodríguez-Dagnino [14] introduced another three-symbol chain code known as the Three OrThogonal (3OT) chain code with alphabetΣ3OT={0,1,2}. Its codes are determined as follows:•if the current coding direction is the same as the coding direction of its predecessor, the code is 0;if the current coding direction is equal to its first predecessor coding direction, which is different than the direction of its predecessor, the code is 1;otherwise the code is 2.Graphical representations of the chain codes can be found at many places in the literature (e.g. [15–17]). Although chain codes provide compact representations of shapes' boundaries, some redundancies still remain. This is the reason for developing the domain-specific chain code compression methods. In 1997 Nunes et al. proposed a near-lossless method with Huffman codes on their differential chain code DCC [12]. Liu and Žalik [15] derived the Directional Difference Chain Code (DDCC) by encoding the angular differences of F8 by Huffman codes. A few years later, Liu et al. [16] proposed three simple compression methods for VCC: E_VCC (Extended VCC), V_VCC (Variable VCC), and C_VCC (Compressible VCC). In order to achieve a more compact code, a modified (M_3OT) chain code was proposed by Sánchez-Cruz et al. [18]. They introduced additional symbols 3, 4, and 5 to encode frequent combinations of 3OT symbols 0 and 1. Instead of static Huffman codes, arithmetic coding was later applied for 3OT and DDCC [19]. However, the arithmetic coding as a part of chain code compression was firstly introduced in 2007 [20]. A context tree for describing the Freeman chain codes' context model followed by arithmetic coding had been proposed in [21] for compressing contour lines. In this case the geometric shapes are not necessarily closed. One of the more efficient methods was proposed by Alcaraz-Corona and Rodríguez-Dagnino [22]. Actually, the method was designed for bi-level image compression. At first, image objects are described by the chain codes. After that the symbol dependences are determined in order to calculate the conditional probability of a Markov model. The selection between various Markov orders is done by Bayesian information criterion – BIC. Finally, the Markov order with the minimal BIC is selected and used by the arithmetic coder. However, the number of probability combinations in the Markov model grows exponentially, which is computationally demanding, and even more importantly, it requires memory space to store the information about the used symbol dependences' probabilities. This is why the authors use a small Markov order (i.e. up to 5). A quasi-lossless chain code compression method was proposed in [23], where the less frequent angular differences (i.e. 135° and 180°) in the DDCC were replaced by the more frequent ones. Žalik and Lukač [17] developed a new lossless chain code compression approach for the more popular chain codes (F8, F4, VCC, 3OT, and NAD – a normalisation of DDC). In order to reduce the information entropy, the Move-To-Front transform was applied followed by the adaptive RLE. Very recently a Universal Chain Code Compression (UCCC) algorithm has been proposed in [24]. The chain codes have been binarised and then compressed, regardless of the chain code type, by a combination of three modes: RLE, LZ77 and COPY. The equivalence between the chain codes was formally proven in [25]. The chain codes can also be used in 3D [26,27].This paper introduces a new lossless chain code compression approach, which combines different string transformation techniques. A testing environment was set-up consisting of Move-To-Front Transform (MTFT), constant 0-symbol Run-Length Encoding (RLE0L) (where runs of L 0-symbols are replaced by a new symbol L), and Burrows–Wheeler Transform (BWT). In the next section, these techniques are briefly explained. In Section 3 the structure of the testing environment is given together with the entropy coder. Section 4 contains the conducted experiments using 24 benchmark shapes, where the comparisons were performed using various lossless chain code algorithms. The last section concludes the paper.This section briefly introduces the string transformation techniques used in this paper: Move-To-Front Transform (MTFT), Burrows–Wheeler Transform (BWT), and two variations of the Run-Length Encoding (RLE).MTFT [28,29] is a technique originally developed for memory paging and more efficient access to the elements of a list [30,31]. The elements, which have been accessed recently are located nearer to the top of the list. In data compression, MTFT is frequently used during the pre-processing stage, as it may reduce the information entropy [17]. LetΣS={σ0,σ1,…,σk−1}be the alphabet consisting of k symbols, andS={σi},σi∈ΣS,i∈[0,n−1]the sequence ofn=|S|input symbols. The list T contains an arrangement of allσi∈ΣS.MTFT incrementally takes symbolσifrom S, finds its position (index) within T, stores the index in the output array O, movesσito the first position in T and shift all others elements from T up to index for one position (see Pseudo-code 1for details).Let us consider an example.ΣS={a,b,c},S=[c,a,a,a,a,a,a,b,a,b,a,b,a]andT=[c,b,a]. Running MTFT results inO=[0,2,0,0,0,0,0,2,1,1,1,1,1]. As can be seen, the sequence of the same symbol results in runs of 0's, whilst the sequence of alternating symbols produces runs of 1's.In the next example, a less favourable case is shown:S=[c,a,b,a,a,b,a,c,a,b,c,b,a]givesO=[0,2,2,1,0,1,1,2,1,2,2,1,2], where runs of the same symbols do not exist. Such streams can be rearranged in, hopefully, a more suitable form for compression by the Burrows–Wheeler Transform, which is described next.Burrows–Wheeler Transform (BWT) is one of the more fascinating algorithms in computer science, being discovered in 1978 and published in 1994 [32]. A detailed explanation with various applications can be found in [33,34]. BWT transforms the input string S to the output string O such that symbols with a similar context are grouped closely together. It works over three steps:•generate n rows by left-shifting S n times;lexicographically sort the rows;read the characters from the last column, which represent BWT.The position of the initial string S should be stored in order to reconstruct the input string S fromBWT(S). A short example follows, whereS=[cabcabcd]:BWT(S) = [ccaadbbc] and the BWT-index for the reconstruction is 4. As can be seen, after BWT the same character tends to form runs and therefore, BWT(S) is more compressible. The procedure for the inverse BWT can be found in the literature (e.g. [33]). The main problem of BWT is its time complexity. A naive implementation works inO(n2log⁡(n))time, which represented a serious obstacle for its usage in the past. Fortunately, BWT can be obtained in the linear time from a suffix array of the considered string. The suffix array, proposed by Manber and Myers [35], is the lexicographically sorted array of the suffixes of a string. Unfortunately, the naive implementation for constructing the suffix array also works inO(n2log⁡(n)). Therefore, the more frequent approaches for obtaining the suffix array have been by the lexicographic traversal of a suffix tree, which can be done in linear time, whilst the suffix tree itself can be constructed inO(nlog⁡(n))time [33]. The algorithms for direct construction of the suffix array in timeO(n)have been known for a while [36,37], too. A scalable parallel suffix array construction as proposed by Kulla and Sanders [38] has been used in our case.Run-length encoding (RLE) is a well-known and very simple compression technique, which replaces the runs of the same symbol in S by the symbol itself and the number of its repetitions [34]. However, at chain code compression RLE should be applied carefully. Namely, the chain codes have very small alphabets and coding the number of repetition dramatically increases the number of symbols. To avoid this, two versions of RLE are used in this paper – the first can be applied before BWT and the second is part of the entropy coder:•ConstantRLE0Lreplaces the fixed runs of 0-symbols having the length L. LetS=[0,0,0,0,0,0,2,2,0,0,0,0,2,2]andL=4, thenRLE0L(MTFT(S))=[L,0,0,2,2,L,2,2]. A more elaborate approach for reducing the number of symbols in S was proposed in [39], where the new, extended alphabet was constructed by combining two or more chain code symbols.RLE0is used in the entropy coding. It encodes the runs of 0-symbols with length N, where N is coded with the variable-length coding (VLC) scheme. The first two bits specify the number of bits used for determining the range of N. Then the number N is coded with 2 to 5 bits according to Table 1.Parameter R specifies, how long the runs of 0-symbols should be that it is worth encoding it byRLE0(N). R is determined as follows:(1)R=|RLE0_Code|+2+2=|RLE0_Code|+4,N≥Rwhere theRLE0_Codeis the length of the code indicatingRLE0(N)mode (see explanation later in Section 3, Tables 2–6). If, for example, the|RLE0_Code|=3, the interval[7,65]of 0-symbols can be encoded.A testing environment (see Fig. 1) was constructed for evaluating the effects of various sequences of string transformation techniques applied on different chain codes in order to study their impacts on their compressions. As seen, BWT is always applied whilst the remaining transformations can be turned-on or off by the switch W. Only BWT is applied whenW=1. By settingW=2, MTFT is applied before BWT. WhenW=3,RLE0Lis applied after MTFT.The effects of possible transformations are demonstrated by an example of the fish shape (Fig. 2) using F4 chain code, when switchW=3.The input F4 chain code data stream is:S=00303033000300030303030333333232323223222232323323211111221221112212232332231333111111011111121111113330300330300111010001011111According to Fig. 1, the input stream is transformed by MTFT, where the initial state of the listT=[0,1,2,3]. The following stream of symbols is then obtained:MTFT(S)=00311110100110011111111100000311111101100011111011130000101101001011021101012100100000310000031000003003110101110200111001110000TheRLE0Lfollows. For this small exampleL=4yields:RLE0L(MTFT(S))=003111101001100111111111L03111111011000111110111300001011010010110211010121001L031L031L0300311010111020011100111LThe BWT is applied on the obtained stream. The index, which should be stored for the reconstruction, is 12.BWT(RLE0L(MTFT(S)))=301011201113L11001110110100101011L00LLL10011211111000101100101321110011103111030111110011110113130010100000011111Finally, MTFT is applied (with the initial state of theT=[0,1,2,3,L]) to obtain the following stream:MTFT(BWT(RLE0L(MTFT(S))))=31211032200342030100110111011111022010022010410000200111010111432003010013200221200001010001102112021110000010000,which is encoded as described in the next subsection. It should be noted that BWT groups the same symbols much better in the cases of longer streams than the very short stream used in this example [33].There are several possibilities for lossless entropy encoding of which Huffman and arithmetic coding are amongst the more popular [34]. However, due to the simplicity we apply a binarisation model combined with theRLE0in this paper.LetS={σi},σi∈Σ,0≤i<n,n=|S|, be the input stream, and B the output stream of the bits, where B is empty at the beginning. The binarisation depends on the number of symbols in the alphabet Σ as explained below.VCC and 3OT are chain codes with three symbols. For simplicity reasons both chain codes are encoded in the same way by rearranging VCC codes as follows: symbol 2 becomes symbol 0 and symbol 3 becomes symbol 2. After thatΣVCC=Σ3OT={0,1,2}. Two cases should be considered according to the position of switch W (see Fig. 1).1.W=3: In this case the alphabet is extended with an additional symbol denoting the sequence of L 0-symbols. Therefore,ΣVCCL=Σ3OTL={0,1,2,3}. The first symbolσ0is coded as follows:•Ifσ0=0then it is checked as to whether it starts the 0-symbols run. If it does,B=1⊕RLE0(N), where bit 1 indicates that the following bits representRLE0encoded number N (see subsection 2.3) and ‘⊕’ denotes the concatenation operation.Ifσ0does not start 0-symbols run,B=0⊕σ0(2), whereσ0(2)is a two-bit binary code ofσ0.For all remaining symbolsσi,0<i<n, an auxiliary variable p is needed. p stores the value of the previously compressed symbol, i.e.p=σi−1, initially,p=σ0. The domain ofp∈{ΣVCCL=Σ3OTL∪R}is extended with the symbol R, which denotes that the run of 0-symbols was previously encoded. The encoding of symbolsσiinto bits is performed according to Table 2. It is worth mentioning that ifp=0, runs of 0's cannot appear as this would have already been detected.W∈{1,2}:ΣVCC=Σ3OT={0,1,2}. Similarly as before,σ0is coded independently. Firstly, it is checked, ifσ0=0and whether it starts the 0-symbol runs in other case, the first symbol is coded with a two-bits binary code. The remaining symbols are coded according to Table 3.F4 and NAD are codes with four symbols. The alphabet of both codes are the sameΣF4=ΣNAD={0,1,2,3}. As in the previous case, if the switchW=3, an additional symbol denoting runs of L 0-symbols should be coded givingΣF4L=ΣNADL={0,1,2,3,4}. The first symbolσ0is coded independently as described in the previous subsection. The remaining symbols are encoded as shown in Table 4. WhenW∈{1,2}, the alphabet isΣF4=ΣNADand the coding summarises Table 5.The first symbolσ0∈ΣF8is coded with three bits. However, if the switchW=3,ΣF8Lhas even 9 symbols.σ0=7andσ0=8are in this case coded with 4 bits (7 as 1110 and 8 as 1111). The remainingσi,0<i<nare coded as given in Table 6. The codes are the same forΣF8andΣF8Lwith the exception of encoding symbols 8 and 9 as described above.

@&#CONCLUSIONS@&#
