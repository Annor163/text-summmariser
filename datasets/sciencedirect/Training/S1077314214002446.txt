@&#MAIN-TITLE@&#
On rendering synthetic images for training an object detector

@&#HIGHLIGHTS@&#
We propose a novel approach to generating synthetic training data.We introduce detector-specific similarity measures between real and synthetic images.We show that the choice of the similarity measure relies on the features used by the detector.We have tested our approach on different object detection tasks.

@&#KEYPHRASES@&#
Synthetic data,Synthetic image rendering,Object detection,

@&#ABSTRACT@&#
We propose a novel approach to synthesizing images that are effective for training object detectors. Starting from a small set of real images, our algorithm estimates the rendering parameters required to synthesize similar images given a coarse 3D model of the target object. These parameters can then be reused to generate an unlimited number of training images of the object of interest in arbitrary 3D poses, which can then be used to increase classification performances.A key insight of our approach is that the synthetically generated images should be similar to real images, not in terms of image quality, but rather in terms of features used during the detector training. We show in the context of drone, plane, and car detection that using such synthetically generated images yields significantly better performances than simply perturbing real images or even synthesizing images in such way that they look very realistic, as is often done when only limited amounts of training data are available.

@&#INTRODUCTION@&#
It is now widely accepted that when enough training data is available, statistical approaches can address image classification problems [1] very effectively. In the commercial world, this is a key ingredient of high performing face detection software deployed by companies such as Apple and Google. However, there are real-world scenarios in which the required training data is hard to obtain in sufficiently large quantities. For example, our work is motivated by the emerging need for Unmanned Aerial Vehicles (UAVs), or drones, to see and avoid each other as they become increasingly numerous and autonomous in the sky. In this application, training videos are rare and do not cover the full range of possible shapes, poses, and lighting conditions under which they can be seen.Our goal therefore is to supplement a small number of available real training samples with an arbitrary large dataset of synthetic ones to improve the detection accuracy of a final classifier. Using synthetic data has been spectacularly successful for 3D body pose estimation with a depth camera [2]. However, depth data do not vary with lighting, motion blur, and other artifacts that affect images from a regular camera, and are therefore comparatively simpler to synthesize.A training set can also be augmented by applying small deformations and adding noise to the images it contains [3,4]. This was done for character [5,6], face [7], and image patch recognition [8]. Such augmentations are typically necessary for the now popular Convolutional Neural Networks [14], which require large amounts of training data.However, this approach assumes that the original training set is already diverse enough, as the range of synthetic images that can be produced is limited. Moreover simple perturbations are often not enough and special car should be taken. More sophisticated approaches have also been proposed for human detection and pose estimation purposes in [9,10], but [9] does not model image-acquisition artifacts while [10] involves considerable amounts of manual interaction, which is less desirable. It was recently shown [11] that it is possible to use a 3D car model to first extract appearance information from real images of cars, and use this information to synthesize novel views. Using these images for training purposes improves performance but this approach does not account for other artifacts such as motion blur and is only applicable to objects with relatively simple geometry.Furthermore, to the best of our knowledge none of these approaches offers a principled way to choose the image synthesis parameters to match the behavior of real-world cameras in the presence of noise. The relevant parameters are typically tuned by hand, which quickly becomes unmanageable when the rendering pipeline is complex. To overcome this limitation, we therefore introduce a fully automated and generic method to estimate these parameters from a small set of available real images to maximize the performance of a detector trained using the resulting synthetic images.To this end, we start from a small set of real seed images containing a target object and corresponding background images without it, such as the ones depicted by Fig. 1(a). Given a very coarse 3D model of the object of interest, such as that of the drone of Fig. 1(a), we estimate the 3D pose of the object, overlaid onto the background image, and then post-process the resulting composite image so that it is as similar as possible to the real one. This is achieved by automated selection of the post-processing parameters to maximize a similarity between the two images. Once these parameters are found, we can then change the position and the orientation of the object in the images to generate arbitrary large synthetic datasets with realistic imaging artifacts.A key ingredient of our approach is the similarity function used to measure the difference between real and composite images. An obvious candidate would be the pixel-wise Euclidean distance. However, our goal is not to generate eye-pleasant images, but rather training data that is effective for our intended purpose. We will therefore show that the best similarity depends on the target detection method. We demonstrate this for three widely used methods that are representative of the state-of-the-art: The Deformable Part Model (DPM) method [12], an AdaBoost-based detector [13], and a detector based on Convolutional Neural Networks (CNN) [14]. Together, these methods cover the state-of-the-art in both object detection and image features.In short, our contribution is a novel and fully automated approach to generating synthetic training image databases that increases detection performance and outperforms the state-of-the-art techniques discussed above, irrespective of the specific detector used. We will demonstrate this in the context of drone, plane, and car detection.In the remainder of this paper, we first discuss the effects we want to model in our synthetic images. We then describe and compare the different similarity functions to quantify the similarity between synthetic and real images. Furthermore, we demonstrate the power of our approach on aircrafts of very different shapes and flying in various environments and lighting conditions. Finally, we compare our approach to recent work [11] on the Pascal VOC dataset.

@&#CONCLUSIONS@&#
