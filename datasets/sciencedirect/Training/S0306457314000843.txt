@&#MAIN-TITLE@&#
HPS: High precision stemmer

@&#HIGHLIGHTS@&#
New unsupervised stemming algorithm is introduced in this article.The algorithm exploits lexical as well as semantic information of words.Performance of stemming is measured on several languages (Czech, Slovak, Polish, Hungarian, Spanish and English).We outperform competing stemmers in inflection removal test, information retrieval task and language modeling task.

@&#KEYPHRASES@&#
Stemming,Morphology,Maximum entropy,Maximum mutual information,Language modeling,Information retrieval,

@&#ABSTRACT@&#
Research into unsupervised ways of stemming has resulted, in the past few years, in the development of methods that are reliable and perform well. Our approach further shifts the boundaries of the state of the art by providing more accurate stemming results. The idea of the approach consists in building a stemmer in two stages. In the first stage, a stemming algorithm based upon clustering, which exploits the lexical and semantic information of words, is used to prepare large-scale training data for the second-stage algorithm. The second-stage algorithm uses a maximum entropy classifier. The stemming-specific features help the classifier decide when and how to stem a particular word.In our research, we have pursued the goal of creating a multi-purpose stemming tool. Its design opens up possibilities of solving non-traditional tasks such as approximating lemmas or improving language modeling. However, we still aim at very good results in the traditional task of information retrieval. The conducted tests reveal exceptional performance in all the above mentioned tasks. Our stemming method is compared with three state-of-the-art statistical algorithms and one rule-based algorithm. We used corpora in the Czech, Slovak, Polish, Hungarian, Spanish and English languages. In the tests, our algorithm excels in stemming previously unseen words (the words that are not present in the training set). Moreover, it was discovered that our approach demands very little text data for training when compared with competing unsupervised algorithms.

@&#INTRODUCTION@&#
Word stemming tasks are among the basic preprocessing techniques in NLP (Natural Language Processing). IR (Information Retrieval) tasks, MT (Machine Translation) systems, LM (Language Modeling), and many other applications in NLP benefit from reducing the number of word forms by applying a stemming method. Current word stemming methods Goldsmith (2001), Majumder et al. (2007), Paik, Mitra, Parui, and Järvelin (2011a) are usually more task oriented and do not necessarily respect linguistic notations. They try to find different stems for words with different semantics and the same stems for words with the same semantics but a different function in the sentence. The distinction between the same and different semantics is given by the particular task to which the stemmer is being applied. For example, the words friend and friendly may be considered semantically equal for information retrieval but not for machine translation. The stemming results are often arbitrary parts of the input words (e.g., dur from durable) rather than linguistically correct morphological units – e.g., morphemes (such as friend from friendship). Creating correct morphological units would involve extra effort and might introduce errors, but having them is not always necessary. For the above mentioned tasks, it is sufficient to have a stem represented by a sequence of characters extracted from an input word that distinguishes meanings.A large class of languages (in the linguistic typology, these languages are called synthetic languages) tends to modify the basic word form by adding prefixes and suffixes according to the function of the word in a sentence. These word forms usually share the same basic meaning. Many stemming methods strip off these affixes. However, such a task is rather complicated in many cases, as illustrated by the following examples. The pairs of English words (A) shin and shining, (B) spar and sparing and (C) speak and speaking are lexically similar and differ in having/not having the suffix ing. The first and second pairs (A, B) consist of semantically different words, whereas words from the third pair (C) differ only in their verb tense. Stripping off the suffixes in A and B would be a stemming mistake, whereas in C it is a correct action. The same example can be made of the word pairs blue and blues or word and words. Again, the suffix s can mean two completely different words or just a different grammatical number. These examples illustrate that stemming algorithms cannot just strip off a known affix. It is instead necessary to decide in which case the affix can or cannot be stripped off.In this article we describe a novel approach to stemming. The distinguishing property of the approach is the ability to provide very accurate stems (high precision) at the cost of a small decrease in the recall rate. This property constitutes the basis for the name of our stemmer: the High Precision Stemmer (HPS), where the word precision comes from preferring precision over recall. Our method works in a fully unsupervised manner (it does not require labeled data or any knowledge about the language itself) and is multilingual. In order to prove the multilingual property, we experiment with four different language families: Slavic, Uralic, Romance and Germanic. The Slavic languages are represented by Czech, Slovak, and Polish; the Uralic languages by Hungarian, the Romance languages by Spanish, and the Germanic languages are represented by English.The rest of this article is organized as follows. In Section 2, we clarify some terms that are used throughout the article. Section 3 introduces state-of-the-art methods for stemming. Then we describe our algorithm in Section 4. We explain our motivation and the principles of the algorithm. In Section 5, we show the results of detailed performance tests of our method by comparing it with the morphologically annotated data and testing it on the IR and in language modeling tasks. The last sections, Section 6 and Section 7, are devoted to discussing, introducing open issues, describing avenues for further work, and drawing conclusions.Lexeme, lemma, stem: Throughout the article, we employ the terms lexeme, lemma and stem. To clarify these terms, we provide short definitions. Lexeme is a virtual dictionary entry of a given word. All inflectional variants of each word share the same lexeme. Lemma is one selected inflectional variant that is used to designate the lexeme. Lemmas have standardized morphological properties: it usually means that lemmas are words in the singular (nouns), masculine (nouns, adjectives), nominative (nouns), or infinitive (verbs). For example, the words speak, speaks, speaking share the same lexeme, which is designated by the lemma to speak.The term stem has different meanings in linguistic sources. In some of them, a stem is defined as a part of a word with meaning that can create new words through different linguistic processes. According to Huddleston (1988), stems can be combined together by a process called compounding (e.g., black-bird or day-dream) or affixes can be attached by a process called affixation (e.g., dur-able). The stems black or bird are called free stems because they are words by them self. The stem dur is called a bound stem since it needs an affix to form a word. In other sources (Kroeger, 2005), a stem is the common part of the word that stays the same for all the inflectional variants (e.g., daydream-s, daydream-ing).In this article, we use a third definition that was outlined in the introduction. We are interested in a common part of a word that carries the same meaning for all its lexical variants.1Lexical variants of words or lexically related words are the words that lexically resemble or lexically overlap one another: e.g., dur-able, dur-ation.1Our definition of meaning is task dependent (e.g., for information retrieval it is the part of the word that defines what a user looks for).Stemming errors: We distinguish two basic types of stemming errors: understemming and overstemming. Understemming means that the word is not shortened enough and the resulting stem does not cover all variants of the word. Overstemming has the opposite meaning: the word is shortened too much and the resulting stem covers more lexemes.Light and aggressive stemmers: Stemmers can be divided into light and aggressive stemmers. The light stemmers prefer precision over recall and are likely to understem the words. In disputable examples, the word is rather left intact instead of creating too short a stem (for example, reducing the words durable and duration to dura). The aggressive stemmers work the other way round. In disputable examples, their stemming is performed even at the risk of creating too short a stem (overstemming).Inflectional morphology vs. derivation (linguistic process): As we noted in the Introduction, stemming tools usually work with affixes. We distinguish two main types of affixes, given their effect on words: inflectional and derivational affixes. An example for English is the following: -s, -ed, -ing forming the words work-s, work-ed, work-ing are inflectional affixes and -able, -less, -ful, -ly, -ness forming blame-able, blame-less, blame-ful blame-less-ly, blame-less-ness are derivational affixes. The example clearly illustrates the different roles of each affix. Inflectional affixes form morphological variants of a given word with the lemma staying the same. Derivational affixes create new words with more or less related meaning. We can also clearly see that removing derivational affixes can be sometimes risky. The words blame-less and blame-ful share the meaning, however, they are antonyms. The question of whether stemmers should or should not remove derivational affixes is difficult and we will address it in our experiments (see Section 5.4) and in the discussion (Section 6).Stemming vs. lemmatization: Stemming and lemmatization are two related fields. In NLP, both the methods are often used for similar purposes: to reduce the number of word forms in a text. The fundamental difference is the different kind of results. The product of lemmatization is a lemma which is a valid linguistic unit. In contrast, the stem, as defined in the Introduction, is mostly task-oriented in NLP. Moreover, some stemmers also remove derivational affixes, whereas lemmatizers are restricted to inflections only. However, both stems and lemmas are intended for reducing the size of the dictionary. Stemming and lemmatization thus can replace one another in some cases. Stemming cannot be used if the output is requested to be a valid word form of a language, just as lemmatization can be too weak for some tasks (e.g., vague and vaguely have different lemmas – in this case, the lemmas are the same as the words: vague, vaguely – which may be a problem for IR). Another difference is that there are currently no means for training a lemmatizer in an unsupervised way: a labeled training corpus or set of manually created rules is needed. Moreover, stemmers are usually more semantically oriented: aggressive stemmers tend to join together semantically related lexemes. For example, runner and running may have one stem, run, but these would have two lemmas and two lexemes. A different example is familiar and unfamiliar. These would have one lemma (one lexeme) but usually two stems since the words have contradictory meaning.The current state-of-the-art stemming algorithms usually belong to one of two basic categories: the rule-based stemmers and the statistical ones. Rule-based stemmers attempt to transform the word form to its base form by using a set of language-specific rules created manually by linguists. The statistical stemmers usually use unsupervised training to estimate the parameters of a stemming model. The basic qualitative difference is that the rule-based stemmers tend to be better at applying rather complex linguistic rules. They are not limited to stripping off affixes, but they can also change the entire word when necessary. Creating such rules is, however, very time demanding2There are some languages (artificial or very regular) where a short list of simple rules is sufficient. This is, however, not the case for all tested languages.2and preferably requires a linguistic expert or at least a speaker of that particular language. On the other hand, statistical stemmers benefit from a large database of automatically learned rules or parameters. Due to their principle of processing large quantities of texts, they can capture less frequent and less obvious cases. Introducing a new language or a new dialect of a language is straightforward provided that the new language meets the assumptions3In every statistical stemmer some assumptions about the language are made. For example, the assumption that new word forms are derived from a basic form by adding affixes. Some languages may not conform to such an assumption, and then a different stemming approach must be used.3that were made for the given statistical stemmer. However, they fail when the particular linguistic process is outside the scope of the statistical model (e.g., statistical stemmers would fail for words such as sing and sang, and foot and feet, although they are quite frequent in English).The first published stemming algorithm ever is Lovin’s stemmer Lovins (1968), which was designed for stemming English. It needs only two steps for stemming a word according to predefined endings and transformation rules. This makes the algorithm very simple and very fast.Another popular algorithm called Porter’s stemmer Porter (1980) evolved into a whole stemming framework called Snowball. Snowball is a string-handling programming language developed by M. F. Porter. Stemming algorithms can be easily defined in this language. In addition, ANSI C or Java programs can be automatically generated. The framework is briefly described at http://snowball.tartarus.org, together with stemmers for several languages.In Dolamic and Savoy (2009), two rule-based stemmers (light and aggressive) for the Czech language are introduced.4Available at http://members.unine.ch/jacques.savoy/clef/index.html.4The aggressive stemmer exhibits slightly better results in IR than the light one. The authors present a MAP (mean average precision) improvement of about 46% by using the aggressive stemmer, and 42% by using the light stemmer in IR systems, compared with no stemming.In Savoy (2008), the investigation of information retrieval in Hungarian is presented. The Hungarian language is characterized by a complex morphology, thus two rule-based stemmers (light and aggressive) are used to improve IR. When compared to an IR scheme without stemming, the light stemmer was able to improve MAP by about 53% on average, and the aggressive stemmer, by about 67% on average.Many studies of the unsupervised learning of the morphology of a language have been published. An outstanding and exhaustive survey can be found in Hammarström and Borin (2011), which provides a description and comparison of the different approaches that deal with morphology at different levels of detail. In terms of that article, our approach belongs to the same-stem decisions level, which is defined as follows: Given two words, decide if they are affixations of the same lexeme.The authors in Xu and Croft (1998) present a method that uses the word form co-occurrences in a corpus to upgrade or create a stemmer. Their work is based on the assumption that word variants (inflected forms of the same word) should occur close to each other (perhaps within a 100 word text window). To model this fact, a variant of expected mutual information is used. The initial distribution of equivalence classes given by some aggressive stemmer (such as Porter’s) is refined using the co-occurrence statistics. According to experiments, the authors show that this additional information enhances the quality of a stemming algorithm.An interesting method for unsupervised stemming was described in Goldsmith (2001). This method is based on the principle of MDL (Minimum Description Length). The algorithm tries to find the optimal breakpoint for each word. Each instance of a given word in a corpus uses the same breakpoint, which splits this word into stem and suffix. The model for the optimal distribution of breakpoints minimizes the number of bits to encode the whole collection of words (this is mathematically equal to minimizing the entropy of this collection). The MDL criterion causes breakpoints to segment the words into relatively common stems as well as common suffixes. This method is implemented as a framework called Linguistica5Available at http://linguistica.uchicago.edu.5Goldsmith (2006).Automatic suffix discovery is investigated in Oard, Levow, and Cabezas (2001). At first, the frequencies of each n-gram character suffix (for n=1, 2, 3, 4) are counted from each word in the collection. The frequency of each n-gram suffix is subtracted from the frequency of the adequate suffix n-gram of the lower ordern-1(for example, the frequency of ing is subtracted from the frequency of ng). The altered frequencies are consequently sorted and a threshold for the optimal number of suffixes for each length is chosen. It is computed by plotting the frequency rank ratios and finding the local extreme. The suffixes with a frequency higher than the threshold are then stored so as to be stripped off during the stemming process. The suffixes are processed starting from the longest ones.In Bacchin, Ferro, and Melucci (2005) a new probabilistic model for word stemming is presented. The mutual relation between stems and suffixes is investigated. Two sets of substrings (prefixes and suffixes) are generated from the word lexicon by splitting the words at all possible positions. From these sets, the probabilities of prefixes and suffixes are estimated using the MLE (Maximum Likelihood Estimation) method. Three models for combinations of prefix and suffix probability estimations are defined. The stemmer selects the most probable split between stem and suffix given a chosen model. The authors experiment with several languages and measure retrieval performance in an IR system. The proposed algorithm produces results just as good as those produced by Porter’s stemmer for these languages.In Majumder et al. (2007), YASS6YASS (Yet Another Suffix Stripper) available at http://www.isical.ac.in/clia/resources.html.6stemmer was introduced. It is a simple approach based on word clustering. All the information needed is again taken entirely from the word lexicon. The set of string distance measures between word pairs is defined. These measures should approximate the morphological similarity between words. The lexicon is then clustered to discover morphologically related words (the equivalence classes). The authors present comparable results with rule-based stemmers (Porter’s or Lovin’s stemmers for English) in terms of retrieval effectiveness. Also for the French and Bengali languages, this approach improves results when compared with no stemming.Another unsupervised approach to stemming was introduced in Paik, Pal, and Parui (2011b). The method uses simple co-occurrence statistics reflecting how often word variants (sharing a common prefix of a given length) occur in the same document. A graph-based algorithm for merging morphologically similar words is then presented. The authors evaluate their stemmer on several languages, including European languages (Czech, Bulgarian, Hungarian, English) and Asian languages (Marathi, Bengali) in the context of IR. Stemmer outperforms YASS, XU stemmer Xu and Croft (1998), and rule-based stemmers.The novel graph-based stemmer GRAS (GRAph-based Stemmer) was introduced in Paik et al. (2011a). Similarly to the approach of YASS, GRAS is focused only on lexical information about words. The stemmer also works only with the collection of distinct words (given by the text collection). The morphological relation is represented by a graph, where the words are treated as nodes and potentially related word pairs are connected by edges. Then the pivot nodes are identified. The idea is that pivots having many neighbors are likely to be potential roots. The authors perform retrieval experiments on seven languages. According to the presented results, GRAS outperforms YASS, Linguistica, and stemmer by Oard et al. (2001) as well as the rule-based stemmer in all seven languages in the information retrieval task. For some languages, GRAS provides a more than 50% performance improvement in the IR task when compared with no stemming.Stemmers are usually evaluated indirectly via a target application, e.g., measuring the improvement in IR with and without stemming. However, there have been some attempts how to measure a stemmer’s quality directly (without a target application).One approach to direct measurement is described in Paice (1994). In the article, stemming is compared with manually created groups of morphologically and semantically related word forms. They measure overstemming and understemming errors, using indices denoted by OI and UI. The indices are defined as the ratios between the number of incorrectly merged words and the total merges, and incorrectly not-merged words and the total merges. The test is designed not to take into account the frequencies of words. An error in a word with frequency 1 has the same impact on the indices as that involving a word with, e.g., frequency 100. They also consider only distinct words and stems, discarding context information.Some of these attributes of the test may be perceived as problematic. Firstly, errors in highly frequent words have surely a higher impact on the performance of a target application than some infrequently occurring words. Secondly, decisions about the stems of words may be context dependent (i.e., the group of morphologically and semantically related word forms may differ for different contexts). Finally, the results of Paice’s test are hard to interpret as it is difficult to compare the stemmers with one another (the OI and UI indices are not in the same order).Due the above described reasons, we designed a novel approach to direct stemming evaluation, introduced in Section 5.4.Our approach consists of two main stages. The first one is based upon the idea that stemming should preserve the semantic information and remove the morphosyntactic information contained in words. The semantics should be an important clue to successful stemming. To model the semantic information, we use the findings from Charles (2000), Rubenstein and Goodenough (1965), which claim that word meaning can be determined from its context. It is expected that the more similar two words are in meaning, the more similar contexts they usually share. This assumption was confirmed in these articles by empirical tests carried out on human test groups. The implication of the studies is that it is possible to compute the semantic similarity of words by a statistical comparison of their contexts.In our work we use this finding by clustering together words occurring in similar contexts and sharing enough long common prefix (they are semantically and lexically similar). The output of this method can be directly used as the stemming result by reducing all the words in a given cluster to their longest common prefix. However, this method alone does not yield the best results. Instead, we use it to automatically generate the training data for the second stage.The second stage of our stemmer is motivated by the assumption that stemming is subject to some rules. Given a word, these rules can decide whether and how to stem the word, depending on some conditions. These conditions can model certain properties of the given word, for example, an occurrence of particular characters in the word, the presence of a certain suffix, etc. This motivation led us to treat the second stage as a classification problem, which naturally encodes the above-mentioned rules into features. We used the maximum entropy classifier that outputs stemming decisions for given words. As a training data, we employed the stemming examples generated from all clusters at the first stage. We also relied on two expectations. First, although the clusters from the first stage (the training data) may contain incorrect stems, we assume that from the statistical point of view, these errors are not significant. Second, we expect the learned rules to be general and thus our approach should work on previously unseen data. The results in Section 5 verify both expectations. Our approach is very successful for both known (seen) and unknown (unseen) words.The architecture of our system is depicted in Fig. 1. The first stage (clustering) is used only for generating the training data for the second stage. It is thus no longer required when the trained stemmer is used for a particular task. In the architecture of our system it is possible to replace the first-phase clustering algorithm with a different way of preparing the training data. A small set of manually prepared training data, another stemming algorithm, or a lemmatizer are viable ways of preparing the training data for the maximum entropy classifier. However, we believe that clustering based on semantic assumptions is the best approach, especially when no manually prepared training data are available.Our approach to clustering is motivated by the MMI (Maximum Mutual Information) clustering algorithm described in Brown, deSouza, Mercer, Pietra, and Lai (1992). The algorithm was originally developed to improve the language modeling task. In that task the clusters were created using the minimal mutual information loss scenario. After we manually observed the resulting clusters, it became apparent that they are semantically related. We believe that the semantic information comes from the principle of the algorithm to minimize the mutual information loss. As will be shown later, there is a direct connection between the similarity of neighboring words and the mutual information loss. The more similar are the neighboring words, the less mutual information is lost. A clustering method based on the similarity of neighboring words satisfies the conditions presented in Rubenstein and Goodenough (1965), Charles (2000). In these studies, the words occurring in similar contexts (having similar neighbors) are observed to be semantically similar.In our approach we take advantage of the MMI algorithm’s ability to find semantically related classes. At the same time we successfully reduce the computational costs by processing only words with a minimal (higher than a preset threshold) lexical similarity score. It is defined in the following subsection.We define the lexical similarity between two words as the length of their longest common prefix normalized by the maximum of their lengths:(1)Swa,wb=LCP(wa,wb)maxwa,wb,whereLCP(wa,wb)is the longest common prefix of wordswaandwb.It is expected that a word stem is related to the initial part of the word. Therefore, if two words are supposed to share a stem, it is expected that they share a significantly long initial part. After normalization,Swa,wbas a similarity metric for wordswaandwbis supposed to measure a certainty thatLCP(wa,wb)is the stem of the words (from a lexical point of view).In later stages of the clustering algorithm, the words are already members of some clusters. To compare two different clusters, we use the complete linkage algorithm:(2)Sca,cb=minwi∈ca,wj∈cbSwi,wj,where the resulting similarity is calculated as the minimum similarity between any member of first cluster and any member of the second.Let W denote the set of possible words (word vocabulary) and C denote the set of word clusters (class vocabulary). Note that in the following we make no distinction between class and cluster. Let m be a mapping functionm:W→C, which maps wordsw∈Wto a classc∈C(c=m(w)). The goal of our modified MMI clustering is to find the optimal mapping m for the stemming problem.The original MMI clustering Brown et al. (1992) is based on maximizing the average mutual information of adjacent classesI(CL;CR)(3)m∗=argmaxmICL;CR,where mutual information is defined as(4)ICL;CR=∑cLcRPcLcRlogPcLcRPcLPcR,cL∈CL,cR∈CR.The symbolcLcRdenotes any two consecutive classes (class bigram) in the training data. The probabilitiesP(cLcR),P(cL)andP(cR)are determined using MLE (Maximum Likelihood Estimation). The superscripts L and R always denote the left side and right side word classes in a bigram, respectively.However, there is no way to find such a partitioningm∗that maximizes the average mutual information over so many possibilities (WW). In the original paper, the problem is approximated by a greedy algorithm which is further tuned in order to decrease the complexity to the order ofW3. Such a complexity is however still very problematic. The iterative greedy algorithm merges two clusters into one cluster while maintaining a minimal mutual information loss. This means that in each step, it must find two clusters (clusters consist of already merged words or a single word) whose connection has a minimal impact on the mutual information of the whole training data. This can be formally described as follows: in each iteration i, letmi:W→Cidenote the mapping function we are trying to optimize, where the set of word clustersCiin the ith step is derived from merging the two particular clusterscaandcbfrom the preceding step (the other clusters remaining unchanged) into a new clustercab:(5)Ci=((Ci-1⧹{ca})⧹{cb})∪{cab},a≠b,ca,cb∈Ci-1,cab=ca∪cb.Note that the operator⧹denotes the set difference. The mappingmithat minimizes the mutual information loss compared to the preceding step can be expressed by the following formula:(6)mi=argminca,cbICi-1L;Ci-1R-ICiL;CiR,ca≠cb,ca,cb∈Ci-1,where the clustersCiare given by formula (5). This step is repeated until the desired final number of clusters is achieved.In our modification of the original MMI algorithm, not all possible pairs are allowed to be merged. The merge candidates are instead limited to those which fulfill a minimal lexical similarity score.Factoring the mutual information loss and the lexical similarity into formula (6) gives(7)mi=argmaxca,cbSca,cbICi-1L;Ci-1R-ICiL;CiR,ca≠cb,ca,cb∈Ci-1,whereSca,cbis the lexical similarity between clusterscaandcb(see Section 4.1.1).Using formula (7), the distribution of clusters heads toward maximizing the lexical similarity and minimizing the mutual information loss.The last issue of the algorithm is the selection of the termination criterion. The optimal number of clusters depends on the morphology of the analyzed language and it is not known in advance. Our solution is to repeat the process of merging clusters while there are still two clusters with lexical similarityS(ca,cb)⩾δ. The thresholdδis chosen empirically (see Section 5). The complete clustering process is shown by the following simplified algorithm transcript 1.Algorithm 1Find a word mapping m into morphologically related clusters1:δ⇐minimal lexical similarity between clusters2:C0⇐W3:m0⇐W→C04:i⇐05: While∃ca,cb:ca≠cb,Sca,cb⩾δdo▷ Repeat while there are still lexically similar clusters.6:i⇐i+17:mi⇐argmaxca,cbSca,cbICi-1L;Ci-1R-ICiL;CiR,ca≠cb,ca,cb∈Ci-1▷ Find the mapping.8:cab⇐ca∪cb9:Ci⇐((Ci-1⧹{ca})⧹{cb})∪{cab}▷ Merge the clusterscaandcbinto one cluster.10: end while11: returnmi▷ The resulting mapping maps lexically and semantically similar words into clusters.By introducing the lexical similarity constraint, we managed to reduce the complexity of the algorithm fromO(W3), toO(|W|2g)whereg≪|W|is the average size of a group of lexically similar words. In greedy clustering, it is no longer required to compare all clusters (words) to each other, but only to compare those pairs that satisfy the minimal lexical similarity constraint. It is apparent that g is very likely to be much smaller thanW, by several magnitudes.This section describes the second stage of our approach: the maximum entropy classifier. The stages are linked together by the clusters created in the first stage. In the second stage, they are taken as the training data for the classifier. In this way, we can use a supervised classifier while still the whole system remains unsupervised.The principle of the classifier consists in estimating the conditional probabilityp(y|x)of the random variable y, which is the observation on the output of a process given by the knowledge x about y. y is a member of a finite set of all possible outputs Y and x is a member of a finite set of all possible pieces of knowledge X.The training data are used to set constraints for the conditional distribution. Each constraint expresses a characteristic (knowledge) about the training data that is requested to be present in the final probability distribution. The facts (the knowledge) about the training data are captured by n real-valued feature functionsfi(x,y)∈0,1.The final model distribution is restricted in such a way that it has the same expected values for all features as seen in the training data. This can be formalized as(8)Efix,y=Ẽfix,y,1⩽i⩽n,whereẼfix,yis the expected value of a featurefix,yestimated from the training data andEfix,yis the expected value of this feature given by the final model.It was shown in Berger, Pietra, and Pietra (1996) that the requested conditional probabilitiesp(y|x)given the model from formula (8) have exponential form and can be estimated as follows:(9)p(y|x)=1Z(x)∏i=1neλifix,y,whereZ(x)=∑y∈Y∏i=1neλifix,yis a normalization function. The parametersλiof the maximum entropy model can be estimated by some algorithm for finding the global maximum of a function, such as IIS7IIS (Improved Iterative Scaling) is a hill-climbing algorithm for finding optimal parameters in log-likelihood space. The algorithm is described for example in Berger et al. (1996).7or by some more sophisticated method, for example by OWL-GN.8OWL-GN (Orthant-Wise Limited-memory Quasi-Newton) described in Andrew and Gao (2007) is an algorithm for the efficient optimization of larger numbers of parameters in log-linear models. It is based upon the L-BFGS (Limited-memory variation of the Broyden–Fletcher–Goldfarb–Shanno) algorithm. However, the authors show that it is much faster than other algorithms.8In order to apply the maximum entropy classifier to the word stemming task (suffix stripping), we need to solve a few issues. Firstly, we define y as the length of a suffix of a given word (it is the suffix that is being stripped off) andY={0,1,…,M}, where M is the maximum of all possible lengths of all suffixes. The x is the word itself. Secondly, we need to define a set of features which add constraints to the final model. We use four types of features, which are described in detail in the following sections. Finally, we use the clusters given in the previous Section 4.1 as the training data for the maximum entropy classifier.Before we describe the various features for the maximum entropy approach, we need to define a few variables. Firstly, let(10)w=l1l2⋯lL=l1L,L=|w|,denote a character string of the word w, where L is the length of the word. Thenlabdenotes the substring of the word from position a to position b.Let the stem be the longest common prefix of a word group c (note that the groups are provided by stage 1 of our algorithm and may contain errors):(11)stem(w)=l1min|LCPw,wi|,w,wi∈c,where w is a given word for which we need to find a stem andwiare other words belonging to the same cluster c.Then the suffix of a given word is the remaining part following the stem:(12)suff(w)=lmin|LCP(w,wi)|+1L,L=|w|,w,wi∈c.Now, we define an arbitrary ending of a word. It is simply a K character long ending of a word w:(13)endw,K=lL-K+1L,L=|w|.This feature represents the global distribution of suffixes according to the word length. The probability that an L character long word contains a suffix of length m is estimated using MLE:(14)PstatsL,m=#w∈W:w=L,|suffw|=m#w∈W:w=L,0⩽m⩽M.This is simply the number of times that the L character long word contains an m character long suffix, normalized by the total number of words with length L. The function#denotes the number of elements in a set. M is the maximum length of suffix to be stripped off.The feature function is(15)fstatsw,m=Pstatsw,m,0⩽m⩽M,where m is the position in the word w where the word should be split between stem and suffix.The motivation for this feature is the assumption that the length of the suffixes depends on the length of the stems. It addsM+1features to the maximum entropy classifier (one for every possible length of a suffix).The most important feature (our experiments show that removing this feature from the feature set causes the highest performance drop) for the classifier is the probability of being a suffix. It is defined as the probability that the word ending is the correct suffix. This probability is based on assessing the training data created in stage 1. For example, if the word ending ing is observed, it need not be the correct suffix (king, ring, sparing), but it can be (drinking, swimming, sleeping). This probability represents the amount of certainty that the observed word ending is a suffix causing the inflective form of the word (it is not a part of the stem) and therefore it needs to be stripped off. We can estimate the probability as follows:(16)Psuffsuff(w)=#wi∈W:suffwi=suff(w)#wi∈W:endwi,suff(w)=suff(w),which is essentially the number of times wheresuff(w)follows the stem of wordwi(for each word in each cluster), divided by the number of all times where the wordwiends withsuff(w).The corresponding feature for the classifier has the following form:(17)fsuffw,m=Psuffendw,m,0⩽m⩽M.The function addsM+1features to the final classifier.As shown earlier, the word ending that resembles a correct suffix (e.g., ing) does not always means that stripping it off is a correct action: e.g., drinking vs. king. In order to disambiguate such cases we introduce a feature that captures the context of the characters that precede the suffix.Let(18)ngramw,N,K=lL-N-K+1L-Kdenote an N-character substring (N-gram) of the word w, which ends K characters before the end of the word. This means that this substring starts at the positionL-N-K+1and ends at the positionL-K, whereL=|W|. Then we define the probability(19)Pngramngramw,N,K=#wi∈W:endstemwi,N=ngramw,N,K∑m=0M#wi∈W:ngramwi,N,m=ngramw,N,K,as the probability of stripping off a suffix after the observation of the N-gramngram(w,N,K)in the word w. This probability is calculated using MLE as the number of times where the N-gram is observed at the end of the stem, divided by the total number of times where the N-gram is observed in a word.The feature function is then defined as(20)fngramw,m=Pngramngramw,N,m,0⩽m⩽M.We have experimentally discovered that the best results are achieved by using N-grams of lengths 1, 2 and 3 (N is set to 1, 2, and 3). This means that this feature produces3(M+1)features for the maximum entropy classifier.We also assume that decisions about stemming depend on the length of the words. Therefore, we introduce the last type of feature function:(21)flengthw,m=1ifw=L0otherwise,0⩽m⩽M,where L ranges from 1 toLmax. Thus,Lmax(M+1)features are added to the maximum entropy classifier.The total number of all features for all possible splitting positions is thenM+1+M+1+3(M+1)+Lmax(M+1)=(5+Lmax)(M+1), where M is the maximum length of suffix to be stripped off.

@&#CONCLUSIONS@&#
