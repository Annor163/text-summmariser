@&#MAIN-TITLE@&#
Mixture model selection via hierarchical BIC

@&#HIGHLIGHTS@&#
A new HBIC criterion is proposed for model selection in mixture models.BIC ignores the clustered data structure while HBIC matches the structure well.HBIC is a large sample approximation of variational Bayesian lower bound.BIC is a less accurate approximation.The results show that HBIC outperforms BIC and BIC suffers from underestimation.

@&#KEYPHRASES@&#
Model selection,Mixture model,EM,Maximum likelihood estimation,BIC,Hierarchical BIC,Clustering,

@&#ABSTRACT@&#
The Bayesian information criterion (BIC) is one of the most popular criteria for model selection in finite mixture models. However, it implausibly penalizes the complexity of each component using the whole sample size and completely ignores the clustered structure inherent in the data, resulting in over-penalization. To overcome this problem, a novel criterion called hierarchical BIC (HBIC) is proposed which penalizes the component complexity only using its local sample size and matches the clustered data structure well. Theoretically, HBIC is an approximation of the variational Bayesian (VB) lower bound when sample size is large and the widely used BIC is a less accurate approximation. An empirical study is conducted to verify this theoretical result and a series of experiments is performed on simulated and real data sets to compare HBIC and BIC. The results show that HBIC outperforms BIC substantially and BIC suffers from underestimation.

@&#INTRODUCTION@&#
Finite mixture models (McLachlan and Peel, 2000) provide flexible and powerful tools for analyzing or modeling heterogeneous multivariate data, in particular, the clustered/grouped data collected from multiple sources, and has been widely used in many fields such as pattern recognition, signal processing, computer vision, engineering, bioinformatics, etc. The Gaussian mixture model (GMM) has achieved much attention in the literature, mainly because of its simplicity and wide applicability in real data analyses.However, when the covariance structure of GMM is full (FGMM), the number of model parameters to be estimated (i.e. the number of degrees of freedom) increases quadratically with the data dimension and linearly with the number of mixture components, usually leading FGMM to suffer from overfitting for high dimensional data. In such cases, it is necessary to use more parsimonious covariance structures to reduce the number of degrees of freedom in FGMM. Banfield and Raftery (1993), Bensmail et al. (1997) and Celeux and Govaert (1995) propose using various constraints on the eigen-decomposed component covariances, yielding a family of GMMs that enjoys a wide range of degrees of freedom. For clarity, this family is called eigen-decomposed GMMs (EDGMM) in this paper.The fitting for EDGMM can be carried out by maximum likelihood (ML) or maximum a posteriori (MAP) methods via the popular expectation maximization (EM) algorithm (Dempster et al., 1977). The well known R software mclust (Fraley and Raftery, 2002; Fraley et al., 2012) implements a subset of ten of these models. The Matlab software mixmod (Biernacki et al., 2006) and R software mixture (Browne and McNicholas, 2014) implement all of the fourteen models. Recent developments of EDGMM include the incorporation oftdistributions to deal with outliers (t-EDGMM;  Andrews and McNicholas, 2012), the incorporation of skew distributions to further tackle asymmetry (skew-EDGMM;  Vrbik and McNicholas, 2014) and the extension oft-EDGMM in the presence of missing data (Lin, 2014).A central issue in learning EDGMM is to determine a suitable number of mixture components and covariance structure. The Bayesian information criterion (BIC) proposed in Schwarz (1978) is commonly used for this purpose and has been taken as the default model selection criterion in mclust, mainly due to its theoretical consistency result (Keribin, 2000) and substantial practical evidence for satisfactory performance in a number of applications (see, e.g.  Fraley and Raftery, 1998, 2002). Despite its popularity, as can be seen in Section  2.4.1, BIC implausibly penalizes the complexity of each component using the whole sample size and completely ignores the clustered structure inherent in the data, leading to over-penalization.In this paper, we develop a novel criterion for model selection in EDGMM called hierarchical BIC (HBIC), which penalizes the component complexity only using its local sample size and well matches the clustered data structure. More importantly, we show theoretically that HBIC is an approximation of the variational Bayesian (VB) lower bound when sample size is large and the widely used BIC is a less accurate approximation. Our proposed HBIC is an extension of the criterion under the mixture of factor analyzers in Zhao et al. (2013), which is only applicable to the case of no constraints across components and cannot be used for learning EDGMM.The remainder of the paper is organized as follows. Section  2 reviews the EDGMM family and two commonly used model selection criteria. In Section  3 we propose a new criterion for EDGMM called hierarchical BIC, which is derived from the variational Bayesian (VB) lower bound in the large sample limit as shown in Section  3. In Section  4, we perform experiments to test our theoretical result and compare HBIC with related competitors. Section  5 ends the paper with a conclusion and some discussions on future works.In this section, we briefly review the family of eigen-decomposed Gaussian mixture models (EDGMM), its associated algorithms, and two frequently used model selection criteria.Letxbe ad-dimensional random vector drawn from ag-component Gaussian mixture model (GMM). The probability density function (pdf) ofxis given byp(x|θ)=∑k=1gπkp(xi|θk),whereπkis the prior probability of componentksatisfyingπk≥0,k=1,…,g, and∑kπk=1,θk=(μk,Σk)is the component parameter specified by meanμkand covarianceΣk,θ≡{πk,θk;k=1,…,g}collects the set of parameters in the GMM, and thek-component pdfp(xi|θk)is given byp(xi|θk)=(2π)−d/2|Σk|−1/2exp{−12(xi−μk)′Σk−1(xi−μk)}.Given a set of i.i.d. sample dataX={xi}i=1N, the log-likelihood of ag-component GMM isL(X|θ)=∑i=1Nlog[∑k=1gπkp(xi|θk)].The ML estimateθˆis obtained by maximizing the log-likelihoodθˆ=arg maxθ{L(X|θ)}.If the priorp(θ)for model parameterθis available, then the MAP estimateθˆcan be obtained byθˆ=arg maxθ{L(X|θ)+logp(θ)}.If the covarianceΣkis in general form, the number of free parameters ofΣkto be estimated increases quadratically with the data dimensiondand linearly with the number of componentsg. In the case that the sample sizeNis relatively small compared with the data dimensiond, GMM usually suffers from overfitting and it is necessary to employ more parsimonious covariance structures or constrainΣk’s to prevent overfitting. For example,Σk=Σ,k=1,…,g, restrict all the components to share a commonΣandΣk=λkI,k=1,…,g, restrict all of the component covariances to be spherical, whereIdenotes an identity matrix of suitable dimension. More generally, Celeux and Govaert (1995) propose the EDGMM family of 14 parsimonious GMMs which enjoys a wide range of degrees of freedom. EDGMM is obtained by employing various constraints on the eigen-decomposed component covariancesΣk’s as follows.(1)Σk=λkDkAkDk,whereλk=|Σk|1/dis the scalar determining the volume of the componentk,Dkis the matrix of eigenvectors determining its orientation andAkis proportional to the diagonal matrix of eigenvalues determining its shape (|Ak|=1). By either restricting some ofλk,AkandDkto be equal (‘E’) across components or allowing them to vary (‘V’), a family of fourteen different models can be obtained.For notation convenience, the free and common component parameters inθkare respectively denoted byθksandθ0, i.e.,θk=(θks,θ0). Correspondingly,DkandD0are the number of free parameters ofθksandθ0, respectively. Table 1lists all the members in the EDGMM family and their associated number of free (Dk) and common (D0) parameters to be estimated.The expectation maximization (EM) algorithm (Dempster et al., 1977) is a well known iterative procedure for finding the ML estimate. In thet-th iteration, the valueθ(t)of the model parameter is updated into its new versionθ(t+1)through an E-step to obtain the expected complete data log-likelihood, i.e., the so-calledQfunction, followed by a M-step to maximizeQwith respect to (w.r.t.)θ. Celeux and Govaert (1995) give the implementations of the EM for all members in the EDGMM family. Below we give a sketch on this.Consider the complete data(X,Z)={xi,zi}i=1N, wherezi=(zi1,…,zik,…,zig),zikis an indicator variable taking the value 1 ifxicomes from thek-th component, and 0 otherwise. The complete data log-likelihood isLc(X,Z|θ)=∑i=1N∑k=1gziklog(πkp(xi|θk)).E-step: Computes the expectedLcgivenXandθ(t):Q(θ|θ(t))=∑k=1gQk(θk|θ(t)),whereQk(θk|θ(t))=∑i=1NE(zik)log(πkp(xi|θk)),depending only on(πk,θk)of componentk. The posterior probability of data pointxibelonging to componentkis computed byrik(θ(t))≜E(zik)=P(zik=1|xi)=πk(t)p(xi|θk(t))∑k=1gπk(t)p(xi|θk(t)).It is convenient to define the local sample covariance matrix(2)Sk(t+1)≜1Nπk(t+1)∑i=1Nrik(θ(t))(xi−μk(t+1))(xi−μk(t+1))′.M-step: For ML estimate, maximizingQw.r.t.θunder the restriction∑i=1gπk=1, we have the following updating equations:πk(t+1)=1N∑i=1Nrik(θ(t)),(3)μk(t+1)=1Nπk(t+1)∑i=1Nrik(θ(t))xi,and the equation forΣkdepends on the parametrization. For the VVV model, we have(4)[λk(t+1),Dk(t+1),Ak(t+1)]=eig(Sk(t+1)),where[λ,D,A]=eig(B)performs the eigen-decomposition as in (1). LetS(t+1)=∑kπk(t+1)Sk(t+1). For the EEE model, we have(5)[λ(t+1),D(t+1),A(t+1)]=eig(S(t+1)).The updating formulae for the other models are available in Celeux and Govaert (1995).For MAP estimate,θ(t+1)is obtained by(6)θ(t+1)=arg maxθ{Q(θ|θ(t))+logp(θ)}.In this subsection, we briefly review two commonly used model selection criteria for the EDGMM family: Bayesian information criterion (BIC;  Schwarz, 1978) and integrated complete-data likelihood criterion (ICL;  Biernacki et al., 2000). Formally, both criteria can be summarized into the general form(7)L∗(g,c,θˆ(g,c))=L(X|θˆ(g,c))−P(θˆ(g,c)),wherecranges over 14 candidate covariance structures detailed in Table 1 andP(θˆ(g,c))is a penalty term that penalizes the higher values ofgand more complex covariance structurec. Given a range of values ofgfromgmintogmax, which is assumed to include the optimal one, the optimal model(gˆ,cˆ)is chosen as(gˆ,cˆ)=arg max(g,c){L∗(g,c,θˆ(g,c))}.The Bayesian information criterion (BIC;  Schwarz, 1978) is one of the most popular criterion for model selection in the EDGMM family due to its theoretical consistency in choosing the number of components (Keribin, 2000) and satisfactory performance in a number of applications (Fraley and Raftery, 1998, 2002). Forg-component EDGMM, the penalty term of BIC in (7) is given by(8)Pbic(θˆ)=12∑k=1gDklogN+D0+g−12logN,whereDkandD0are the number of free and common parameters inθk, respectively.It can be observed from (8) thatD0andDkare treated equivalently in the sense that both are penalized by the same sample sizeN, which means that BIC uses the whole sample sizeNto determine theDkof componentk. However, from (2) and (3), the effective sample size for estimating theθksof componentkin EDGMM is notNbutNπˆkonly. For illustration, Fig. 1shows a toy data set of two-well separated clusters, where cluster 1 and 2 have 50 and 450 observations, respectively. For this data, BIC uses the total sample sizeN=500to determineD1, rather than the effective sample size 50, resulting in over-penalization for component 1. In short, BIC ignores the clustered data structure completely and suffers from over-penalization.The integrated complete-data likelihood criterion (ICL;  Biernacki et al., 2000) is an extension of BIC that tends to favor well-separated mixtures. The penalty of ICL comprises the BIC penalty and an additional entropy term that measures the overlap among components(9)Picl(θˆ)=Pbic(θˆ)−∑i=1N∑k=1gMAP{rik}logrik,whereMAP{rik}is the MAP classification withrik, taking 1 ifarg maxl{ril}=kand 0 otherwise.We propose in Section  3.1 a new criterion called hierarchical BIC (HBIC) for model selection in the EDGMM family. In Section  3.2, we show that HBIC is a large sample approximation of the variational Bayesian (VB) lower bound. In Section  3.1.1, we discuss its relationship with BIC.The proposed criterion also has the general form (7) but with a new penalty term(10)Phbic(θˆ)=12∑k=1gDklog(Nπˆk)+D0+g−12logN,whereπˆkis the ML/MAP estimate,Dk(resp.D0) is the number of free parameters ofθks(resp.θ0).Different from BIC in (8), it can be seen from (10) that HBIC determinesDkonly using the corresponding effective sample sizeNπˆkand thus HBIC well matches the clustered data structure as detailed in Section  2.4.1. The key to HBIC is that the parameters are penalized only based on their relevant effective sample sizes. In fact, this idea is not new. For example, Steele (2002) has examined a special case of HBIC under the VVV structure for model selection of one-dimensional GMM and Gollini and Murphy (2014) have further investigated this criterion under mixtures of latent trait analyzers. A theoretical justification of this criterion is given in Zhao et al. (2013). The main difference from this paper is that Zhao et al. (2013) consider the case of no constraints across components while, for EDGMM, we must consider the case that there are constraints across components. Similar criteria for hierarchical or random effects models have also been suggested in Pauler (1998) and Raftery et al. (2007).On the other hand, it can be seen from (10) that HBIC seems not sensitive to the spurious components that have small values ofNπˆk. In Section  4.2.1, we find that this drawback can be easily eliminated by means of a component annihilation step detailed in Section  4.1.2.Comparing (8) and (10), we see that (i) wheng=1(i.e., one-component mixture), HBIC degenerates into BIC; and (ii) wheng>1, sinceNπˆk≤N, BIC penalizes the model more heavily than HBIC. Despite the difference, there exist close relationships between HBIC and BIC. In HBIC, BIC is performed in a ‘hierarchical’ manner: the first hierarchy is a global BIC and the second hierarchy is a local BIC.•global BIC forπk’s and common component parameterθ0: Since allNdata points are devoted to estimatingθ0and theπk’s under the constraint∑kπk=1, the corresponding BIC penalized term toθ0isD0/2logNandπk’s is(g−1)/2logN, which yields the second term in (10).local BIC for free component parameterθks: Since the estimation ofθksis based on the local effective sample sizeNπˆk, the corresponding BIC penalized term toθksisDk/2log(Nπˆk). Summing over all thegcomponents leads to the first term in (10).In the Bayesian treatment of EDGMM, the true posteriorp(Z,θ|X)is usually intractable. In such a situation, it is often preferred to find a deterministic approximation solution. To this end, the variational Bayesian (VB) adopts a tractableqto approximatepand optimizes a lower boundFof the log marginal likelihoodlogp(X). By Jensen’s inequality,(12)logp(X)=log∫p(X,Z,θ)dZdθ≥∫q(Z,θ)logp(X,Z,θ)q(Z,θ)dZdθ=F(q).The tractableqcan often be obtained by assuming thatqfactorizes overZandθ:(13)q(Z,θ)=q(Z)q(θ).Following Bishop (2006), we use the priors(14)p(θ)=p(π)p(θ0)∏k=1gp(θks),and ,p(π)=Dir(π|α0)=C(π0)∏k=1gπkα0−1,whereDir(⋅)denotes the Dirichlet distribution andC(π0)=Γ(gα0)/Γ(α0)gis the normalization coefficient ofp(π). With the priors in (14), we obtain the additional factorization(15)q(θ)=q(π)q(θ0)∏kq(θks).LetE[⋅]q(⋅)stand for an expectation w.r.t. the distributionq(⋅). Substituting (13) and (15) into (12) and maximizingFoverq(Z),q(π),q(θ0)andq(θk),k=1,…,g, yields the following VBEM updating steps:•VBE-step:(16)q(Z)=∏i=1N∏k=1gq(zik),where(17)logq(zik)=zik(E[logπk]q(π)+E[logp(xi|θk)]q(θks)q(θ0))+const.VBM-step:(18)q(π)=Dir(π|α),(19)logq(θ0)=∑k=1g∑i=1NrikE[logp(xi|θk)]q(θks)+logp(θ0)+const,and(20)logq(θks)=∑i=1NrikE[logp(xi|θk)]q(θ0)+logp(θks)+const,Substituting (16), (18)–(20) into (12), we can writeFas(21)F=E[logp(X,Z|θ)q(Z)]q(Z)q(θ)︸FD−KL[q(π)∥p(π)]︸F1−KL[q(θ0)∥p(θ0)]︸F2−∑k=1gKL[q(θks)∥p(θks)]︸F3.In the largeNlimit, we haveFD=L(X|θˆ)+O(1),F1=m−12logN−logp(πˆ)+O(1),F2=D02logN−logp(θˆ0)+O(1),andF3=∑k=1gDk2log(Nπˆk)−logp(θˆks)+O(1).The proofs for these limits are in general similar to those for the mixtures of factor analyzers in Zhao et al. (2013). Since the proof for the limit ofF1is directly available from Zhao et al. (2013), we simply give the proofs forF2,F3andFDin Appendices A and B. By further dropping the order-1 termp(πˆ)+p(θ0ˆ)+∑klogp(θˆks), we obtainTheorem 1As the sample sizeNtends to infinity,F=L(X|θˆ)−Phbic(θˆ)+O(1),wherePhbic(θˆ)is given by   (10).The HBIC criterion in Section  3.1 follows from Theorem 1. Theorem 1 shows that HBIC is a large sample limit of the VB lower boundF, which establishes the connection between HBIC and VB, similar to that between BIC and Bayesian. With this connection, we can use HBIC reliably when sample size is large.

@&#CONCLUSIONS@&#
