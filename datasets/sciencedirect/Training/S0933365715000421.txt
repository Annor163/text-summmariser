@&#MAIN-TITLE@&#
Learning from healthy and stable eyes: A new approach for detection of glaucomatous progression

@&#HIGHLIGHTS@&#
We present a two-step framework for glaucoma progression detection.The Markov prior increases significantly the change detection rate.The one-class kernel classifier performs better than the fuzzy-based classifier.The use of the whole 3D SD-OCT images improves the glaucoma change detection.

@&#KEYPHRASES@&#
Spatial dependency modeling,Copula theory,Kernel classifier,Glaucoma progression detection,

@&#ABSTRACT@&#
Glaucoma is a chronic neurodegenerative disease characterized by loss of retinal ganglion cells, resulting in distinctive changes in the optic nerve head (ONH) and retinal nerve fiber layer. Important advances in technology for non-invasive imaging of the eye have been made providing quantitative tools to measure structural changes in ONH topography, a crucial step in diagnosing and monitoring glaucoma. Three dimensional (3D) spectral domain optical coherence tomography (SD-OCT), an optical imaging technique, is now the standard of care for diagnosing and monitoring progression of numerous eye diseases.MethodThis paper aims to detect changes in multi-temporal 3D SD-OCT ONH images using a hierarchical fully Bayesian framework and then to differentiate between changes reflecting random variations or true changes due to glaucoma progression. To this end, we propose the use of kernel-based support vector data description (SVDD) classifier. SVDD is a well-known one-class classifier that allows us to map the data into a high-dimensional feature space where a hypersphere encloses most patterns belonging to the target class.ResultsThe proposed glaucoma progression detection scheme using the whole 3D SD-OCT images detected glaucoma progression in a significant number of cases showing progression by conventional methods (78%), with high specificity in normal and non-progressing eyes (93% and 94% respectively).ConclusionThe use of the dependency measurement in the SVDD framework increased the robustness of the proposed change-detection scheme with comparison to the classical support vector machine and SVDD methods. The validation using clinical data of the proposed approach has shown that the use of only healthy and non-progressing eyes to train the algorithm led to a high diagnostic accuracy for detecting glaucoma progression compared to other methods.

@&#INTRODUCTION@&#
Glaucoma is an optic neuropathy characterized by distinctive changes in the optic nerve head (ONH) and visual field. Glaucoma is often asymptomatic in its early stages and causes blindness if it remains without treatment. It results in progressive loss of retinal ganglion cells and their axons causing typical changes in the appearance of the retinal nerve fiber layer (RNFL) and the optic disk.The detection of glaucoma change over time is of high interest in the diagnosis and management of glaucoma particularly for patients as the detection of progression could indicate uncontrolled disease and possible need for therapy advancement. Hence, it is important to develop clinically relevant methods for progression detection in order to avoid permanent damage to the optic nerve head.In 1851, Helmholtz revolutionized the field of ophthalmology with the invention of the ophthalmoscope, which allowed physicians to examine the disc clinically and identify damages in the optic nerve head associated with glaucoma. This requires clinician identification of the outer and inner borders of the neuroretinal rim and visual estimation of the amount of rim tissue. However, the inner and the outer borders of the ONH neural tissue are not always visible by clinical examination techniques [1]. Furthermore, the clinical examination of the ONH remains subjective, qualitative and with limited reproducibility [2].In order to assist the expert in qualitative and quantitative analysis, automatic image processing methods have been proposed to facilitate the interpretation of the images obtained by objectively measuring the ONH structure and detecting changes between a reference image (baseline exam) and other images (follow-up exams).In this context, important advances in technology for non-invasive imaging of the eye have been made providing quantitative tools to measure structural changes in ONH topography, an essential element in detecting glaucoma and monitoring its progression. In particular, the heidelberg retina tomograph (HRT), a confocal scanning laser technology, has been commonly used for glaucoma diagnosis since its commercialization 20 years ago [3]. A limited number of strategies have been proposed that quantitatively and qualitatively detect glaucomatous progression using HRT images. In [4], the topographic change analysis was proposed for assessing glaucomatous changes. This technique has been shown to classify progressing and stable eyes with reasonably good sensitivity and specificity. Another method called the proper orthogonal decomposition [5] indirectly utilizes the spatial relationship among voxels by controlling the family-wise Type I error rate. The Markov random field (MRF) model was used in [6] to model the inter/intra observations dependency allowing a better glaucoma progression detection rate. However, the HRT imaging technique is limited by its lower resolution (i.e. 300μm axial resolution for the HRT3). Moreover, because HRT is limited to ONH surface topography, it cannot differentiate between retinal layers. It provides an indirect measure of RNFL thickness that is calculated as the difference between the retinal surface and a standard reference plane 50μm below the surface of the retina temporal to the ONH.In contrast, the 3D spectral domain optical coherence tomography (SD-OCT)can differentiate between retinal layers and provide quantitative estimates for change detection. SD-OCT is now the most commonly used instrument for imaging both the ONH and the RNFL thickness. Numerous studies have evaluated glaucoma detection using SD-OCT images. However, most of the studies use the RNFL measurements provided by the commercially available spectral-domain optical coherence tomographers for change detection [7]. Although those methods are successfully applied to SD-OCT images, its use is constrained by specific pre-requisite: it requires an accurate estimation of the RNFL layer thickness. In [8], authors showed that the instrument built-in segmentation software is relatively robust to the image quality and the noise may lower the accuracy of the RNFL layer thickness estimation.In this paper we propose a hierarchical framework for glaucoma progression detection using 3D Spectralis (Heidelberg engineering) SD-OCT images. This paper is an extended version of the conference paper [9]. Specifically, we explain in more details the change detection algorithm and we add more experiments in the results section. Moreover, we propose the use of a new kernel-based classifier to improve the results of the fuzzy classifier. In contrast to previous works that use the RNFL thickness measurement, we consider the whole 3D volume for progression detection. Our framework is divided into two steps: (1) change detection step which consists of detecting changes between a baseline image and a follow-up image and (2) a classification step which consists of classifying the detected changes into random changes or true changes due to glaucoma progression. For the first step, we propose a fully Bayesian framework for change detection since these methods are relatively simple and offer efficient tools to include a priori knowledge through the a posteriori probability density function (PDF). In particular, we propose the use of the MRF model to exploit the statistical correlation of intensities among the neighborhood voxels [10]. In order to develop a noise robust algorithm, we propose consideration of the change detection problem as a missing data problem where we jointly estimate the noise hyperparameters and the change detection map. The widely used procedure to estimate the different problem parameters is the Expectation-Maximization (EM) algorithm [11]. However, since we used the MRF model with the change detection map as the prior for the change detection map, the optimization step is intractable. Hence, we propose the use of a Monte Carlo Markov chain (MCMC) technique [12].Once the change detection map is estimated, we propose the use of kernel-based classifier for glaucoma progression detection. Kernel-based classifiers have several advantages compared with other approaches; they reduce the dimensionality of the data, increase the reliability and the robustness of the method in the presence of noise and allow flexible mappings between objects (inputs) represented by features vectors and class labels (outputs) [13]. As no prior knowledge on glaucoma progression is available, we are interested in the one class classifier where only the control data (healthy eyes and stable glaucoma eyes) are used to train the classifier. To this end, we have elected to use the support vector data description (SVDD) method [14] here. The SVDD one class-classifier method maps the data into a high-dimensional feature space. In this new space, a hypersphere that encloses most of the dataset belonging to the class of interest (the target class). Although basic kernel functions can be successfully applied for change detection [15–17], they do not exploit the additional constraints that are often available, such as the dependencies and the distribution of different features. We show in this paper that the classification should be more efficient if such information is integrated. To account for these characteristics in our change-detection scheme, we propose the use of a new kernel function that combines some properties of the old kernel functions with new information about the feature distribution and dependencies [17].The paper is divided into three methodology sections. In Section 2, the proposed change detection scheme is presented. In Section 3, we describe the classification step. Then, in Section 4 results obtained by applying the proposed scheme to clinical data is presented. The diagnostic accuracy of this novel proposed approach is compared to two existing progression detection RNFL based approaches: the artificial neural network classifier (ANN) and the support vectors machine (SVM) classifier.Let us consider the detection of changes in a pair of amplitude images. We denote by I0={I0(i)|i=1, ..., M} and I1={I1(i)|i=1, ..., M}, where M is the number of the image voxels, two images acquired over the same eye at times t0 and t1, respectively (t1 > t0), and coregistered. In this work, we assume that the noise is additive, white and normally distributed.The Spectralis SD-OCT instrument features three different options to enhance reproducibility and reduce the noise. A real time eye-tracking device (eye tracker) compensates for involuntary eye movements during the scanning process, a retest function assures that follow-up measurements are taken from the same area of the retina as the baseline examination and a Heidelberg noise reduction option that average automatically several images (10 images are the number recommended by the manufacturer) at the same location to increase image signal to noise ratio (SNR) and improve the quality of subsequent images. As recommended by the manufacturer, only images with a signal quality of equal or greater than 20dB were used in this study. The mean and the standard deviation of different image qualities are 26dB and 3, respectively (range 20–40dB). In this study, five images have a SNR equals to 40dB (1% of the whole number of images). We assume in these images that most of the noise was removed by the instrument. These images are then used to study the distribution of the retinal tissue intensities. Four distribution candidates: (1) gamma, (2) log-normal, (3) Weibull and (4) normal distributions were used to fit the retinal tissue intensities. Fig. 1, presents the fitting results of the different distributions. Using the Bayesian information criterion (BIC) [18], the gamma distribution was identified as the best candidate. It is important to note that the BIC criterion does not allow to test if one model is statistically better than the other one. In order to overcome this problem, the F-test can be used [18]. However, this test can only compare two nested models (i.e; one model is a special case of the other one). To this end, we used the generalized gamma distribution to test if adding a new parameter will significantly increase the goodness of fit. Note that the Weibull distribution and the log-normal distribution are also special cases of the generalized gamma distribution. Our simulations showed that there is no statistical difference between gamma and generalized gamma distributions. Therefore, the gamma distribution with the hyperparameters (α>0, β>0), denoted byG(α,β), is used in this paper to fit the retinal tissue intensities. The direct model for both images I0 and I1 is then given by:(1)I0=X0+N0;I1=X1+N1where X0={X0(i)|i=1, ..., M},X0(i)∼G(α0,β0)and X1={X1(i)|i=1, ..., M},X1(i)∼G(α1,β1)are the noise free 3D SD-OCT ONH images and N0={N0(i)|i=1, ..., M},N0(i)∼N(0,σ0)and N1={N1(i)|i=1, ..., M},N1(i)∼N(0,σ1)are additive, white and normally distributed noises. The change-detection problem is formulated as a hypothesis testing problem by denoting the “no-change”, the “increase change” (i.e; ONH hypertrophy) and the “decrease change” (i.e; ONH atrophy) hypotheses as H0, H1 and H2 respectively.Two widely used approaches for the change detection can be distinguished: the image-difference approach and the image-ratio approach. On the one hand, the image-difference approach leads to the following direct model: Δ=I0−I1=(X0−X1)−(N0−N1). The advantage of the image difference approach is that the resulting noise is normally distributed, as the difference of two normal distribution is a normal distribution as well, which allows an easy modeling of the likelihood. However, a given site of the free noise image difference at the position i∈{1, ..., M} Δ(i)=(X0(i)−X1(i)) follows a gamma difference distribution:(2)p(Δ(i);α0,β0,α1,β1)=∫0∞G(X0(i),α0,β0)G(X0(i)−Δ(i),α1,β1)dX0(i)In case where α0=α1=α, p(Δ(i);α, β0, β1) is given by:(3)p(Δ(i);α,β0,β1)=(1−c2)a+12(|Δ(i)|)aπΓ(a+12)2aba+1exp−cδ(i)bB−δ(i)b;awhere B(.;a) is the modified Bessel function of the second kind and of order a,Γ(y)=∫0∞ty−1exp−tdt,a=α−12,b=2β1β2β1+β2andc=β2−β1β1+β2.Proof. Let x0 and x1 be two independent random variables following a gamma distributionG(α0,β0)andG(α1,β1):(4)p(x0,α0,β0)=x0α0−1Γ(α0)β0α0exp−x0β0,p(x1,α1,β1)=x1α1−1Γ(α1)β1α1exp−x1β0x0,x1>0The PDF of the difference δ=(x0−x1) is given by:p(δ;α0,β0,α1,β1)=∫0∞G(x0,α0,β0)G(x0−δ,α1,β1)dx0=1Γ(α0)β0α0Γ(α1)β1α1∫0∞x0α0−1exp−x0β0(x0−δ)α1−1exp−(x0−δ)β1dx0In case where α0=α1=α, p(δ;α, β0, β1) is given by:(5)p(δ;α,β0,β1)=expδβ1Γ(α)2(β0β1)α∫0∞x0α−1(x0−δ)α−1exp−φx0dx0whereφ=1β0+1β1. Or from [19], we have(6)∫0∞x0α−1(x0−δ)α−1exp−φx0dx0=1π|δ|φα−12exp−δφ2Γ(α)B−δφ2;α−12whereB(.;α−12)is the modified Bessel function of the second kind and of orderα−12. Let us define a, b and c as:a=α−12,b=2β1β2β1+β2andc=β2−β1β1+β2. The gamma difference distribution is then given by:(7)p(δ;α,β0,β1)=(1−c2)a+12(|δ|)aπΓ(a+12)2aba+1exp(−cδb)B(−δb;a)Hence, the estimation of gamma difference distribution hyperparameters is complicated and therefore the image difference approach is not well suited to our problem. On the other hand, the image-ratio approach leads to the following direct model:R=I0I1=X0+N0X1+N1, R={R(i)|i=1, ..., M}. This model is unfortunately intractable. To overcome this problem, we propose a hierarchical change detection framework which consists of estimating the noise free SD-OCT imagesXˆ0andXˆ1and then use the image ratio approach for change detection. The direct model is then given byR=Xˆ0Xˆ1. The gamma ratio distribution is expressed as:(8)p(R(i);α0,β0,α1,β1)=Γ(α0+α1)Γ(α0)Γ(α1)R(i)α0−1β0β1α0R(i)+β0β1−(α0+α1)Proof. Let x0 and x1 be two independent random variables following a gamma distributionG(α0,β0)andG(α1,β1). The PDF of the ratio r=(x0/x1) is given by:p(r;α0,β0,α1,β1)=∫0∞G(x1r,α0,β0)G(x1,α1,β1)x1dx1=rα0−1Γ(α1)β1α11Γ(α0)β0α0rβ0+1β1α0+α1Γ(α1+α0)×∫0∞1rβ0+1β1α0+α1Γ(α1+α0)x1α0+α1−1exp−x1rβ0+1β1dx1=Γ(α0+α1)Γ(α0)Γ(α1)rα0−1β0β1α0r+β0β1−(α0+α1)since the PDF integrates to 1.In this work, we assume that α0=α1=α andβ=β0β1, p(R(i);α, β) is given by;(9)p(R(i);α,β)=Γ(2α)Γ(α)2R(i)α−1βα(R(i)+β)−2αIndeed, the scale parameters α0 and α1 measure how spread out the gamma distribution is. The larger the scale parameters, the more the spread out of the gamma distribution. Because glaucoma progression is often slow, the observed changes between two visits are relatively small. α0 was taken equal to α1 for sake of model simplicity. It is easy to extend the change detection scheme to the case of α0≠α1, particularly when we have long follow-up duration, by considering the non simplified gamma ratio distribution as follows:(10)p(R(i);α0,α1,β)=Γ(α0+α1)Γ(α0)Γ(α1)R(i)α0−1βα0R(i)+β−(α0+α1)In contrast to the gamma difference distribution, the estimation of the simplified gamma ratio distribution hyperparameters is easy to perform. Finally, the change detection is handled through the introduction of change class assignments Q. To introduce a spatial a priori knowledge on Q={Q(i)|i=1, ..., M}, we used the MRF model so that the change status Q(i) of a voxel R(i) depends on the change status of its neighborhood.The Bayesian model aims at estimating the model parameters (X0, X1, Q) and hyperparameters Θ. This requires the definition of the likelihood and prior distributions. We now present each term of the hierarchical Bayesian model.Likelihood: The definition of the likelihood depends on the noise model. As we assumed that the noise in both images I0 and I1 is white and normally distributed with standard deviations σ0 and σ1 respectively, the likelihood is then given by:(11)p(I0|X0;σ0)=N0,σ0;p(I1|X1;σ1)=N0,σ1Model prior: The prior on X0 and X1 is given by the gamma densityG:(12)p(X0;α0,β0)=∏i=1MG(x0(i),α0,β0);p(X1;α1,β1)=∏i=1MG(x1(i),α1,β1)To describe the marginal distribution of R conditioned to Hl,l∈{0,1,2}, we used the ratio gamma distribution:(13)p(R|Q=Hl;θl,κl)=∏i=1:MΓ(2θl)Γ(θl)2r(i)θl−1κlθl(r(i)+κl)−2θlConcerning the change variable, we assume that p(Q;Θ) is a spatial MRF prior. In this study we adopt the Potts model with interaction parameter ζ:(14)p(Q;ζ)=1Zexp−ζU(Q),U(Q)=∑i∼jδ(qi,qj)where Z is the normalization constant and δ the delta Kroneker function. Note that we have opted for the 6-connexity 3D neighboring system. The whole set of the model hyperparameters is then given by Θ={σj=0,1, αj=0,1, βj=0,1, θl=0:2, κl=0:2, ζ}. Because no knowledge on the noise variance is available, we retained the following prior distributions for σ0 and σ1:p(σ02)=1σ0,p(σ12)=1σ1. The non-negativity of the hyperparameters {αj=0,1, βj=0,1, κl=0:2, θl=0:2, ζ} is guaranteed through the use of the exponential densities:p(α0,β0)=1ηexp−α0η1ϑexp−β0ϑ,p(α1,β1)=1ηexp−α1η1ϑexp−β1ϑ,p(θl,κl)=1ηexp−θlη1ςexp−κlςandp(ζ)=1κexp−ζκ. Note that the values of {η, ϑ, ς, κ} are fixed empirically. The choice of these values do not influence the results but has an impact on the speed of the algorithm convergence [20]. By combining the likelihood and the prior knowledge using the Bayes’ rule and giving our hierarchical approach we adopted, the obtained posterior distribution p(Q, Θ|R) is intractable for our model. Hence, we propose the use of a MCMC procedure to estimate the model parameters and hyperparameters. The principle of MCMC method is to generate samples drawn from the posterior densities and then to be able to achieve parameter estimation. We use a Gibbs sampler based on a stationary ergodic Markov chain allowing to draw samples whose distribution asymptotically follows the a posteriori densities.The Gibbs sampler decomposes the problem of generating a sample from a high dimension PDF by simulating each variable separately according to its conditional PDF. Since no classical expressions for the posterior PDFs are available, there is no direct simulation algorithm for them. To overcome this problem, we perform Hastings-Metropolis steps within the Gibbs sampler [12] to ensure that all convergence properties are preserved [21]. Moreover, since no knowledge on the posterior definition is available, we have opted for the random walk version of the Hastings-Metropolis algorithm. The proposal distribution p is then defined as p(.|Y)=Y+p′(.) where p′ is a distribution which does not depend on Y and is usually centered on 0. Hence, at each iteration, a random move is proposed from the actual position. The choice of a good proposal distribution is crucial to obtain an efficient algorithm, and the literature suggests that, for low dimension variables, a good proposal should lead to an acceptance rate of 0.5 [12]. The determination of a good proposal distribution can be solved by using a standard Gaussian distribution with an adaptive scale technique [22]. The main Gibbs sampler steps are described in Algorithm 1.Algorithm 1Sampling algorithm1- Initialization ofX0[0],X1[0],Q[0]and Θ[0]2- For each iteration h repeat:i) SamplingX0[h]from p(X0|I0, Θ[h−1])ii) SamplingX1[h]from p(X1|I1, Θ[h−1]iii) CalculatingR[h]=X0[h]X1[h]iii) Creating a configuration of Q basing on R[h]iv) Calculating p[h](Q=Hl|R[h], Θ[h−1])v) Sampling Θ[h] fromp(Θ|X0[h],X1[h],I0,I1)vi) Until Convergence criterion is satisfiedWe used a burn-in period of hmin=500 iterations followed by another 1000 iterations for convergence (hmax=1500). The change detection map Q is estimated using the maximum a posteriori MAP estimator:Qˆ=argmaxHlp¯Hl, wherep¯H1=1hmax−hmin∑h=hmin+1hmaxp[h](Q=H1|R[h],Θ[h−1]),p¯H2=1hmax−hmin∑h=hmin+1hmaxp[h](Q=H2|R[h],Θ[h−1])andp¯H3=1−p¯H3−p¯H1.This step aims at classifying an image into non-progressing or the progressing classes based on the estimated change detection map. As no prior knowledge on glaucoma progression is available, we are interested in the one class classifier where only the control data (healthy eyes and stable glaucoma eyes) are used to train the classifier. To this end, we have elected to use the SVDD method [14] here. The SVDD enables us to distinguish between targets and outliers by defining a closed boundary around the target data. This is equivalent to map a data set defined over the input space into a higher-dimensional Hilbert space (the feature space) then to draw a minimum-volume hypersphere in the kernel feature space that includes all or most of the target objects.Let{xi}i=1...K, withxi∈ℝN, be the vector containing the N features of a given object (in our case the estimated change detection map). Kernel methods compute the similarity between samples xiusing pairwise inner products taken between mapped samples. The most common kernels functions are the linear kernelK(xi,xj)=xi.xjand the radial basis function (RBF)K(xi,xj)=exp(−||xi−xj||2/2σ2). These kernel functions do not exploit additional constraints such as the feature dependency (a measurement that models the distance to the independence for two random variables) and thus make the change detection less robust. Consider two signals that are both realizations of random processes. If they are affected by the same linear change, both random fields will tend to increase or decrease with the same amplitude. In this work, we opted for Gaussian copula to model non-linear dependency for the multivariate case [17] which has been shown to be effective for treating dependency [23]. In order to include the distance between features within the new kernel function, we use the RBF function, which offers some degrees of freedom because of the standard deviation hyperparameter σ with information about the distance to the independence of features using the copula theory [17]. Thus, we adopt the Gaussian copula, which is given here: ∀ y=(y1, ⋯, yL)∈IRL,(15)cG(y)=|Γ|−12exp−y˜T(Γ−1−I)y˜2wherey˜=(Φ−1(y1),⋯,Φ−1(yL))T, in which Φ(.) the standard Gaussian cumulative distribution function, Γ is the inter-data correlation matrix, and I is the L×L identity matrix. The proposed kernel function is(16)K(xi,xj)=(E[CG(xi,xj)])exp(−||xi−xj||2/2σ2)where CG(xi, xj)=[cG(xi(1), xj(1)), ..., cG(xi(K), xj(K))], in which K is the length of the vector xi. As the dependency of a couple (xi, xj) increases, the E[CG(xi, xj)] value approaches 1. The inter-data correlation matrix Γ is estimated using the maximum-likelihood estimator [24]. Note that the new kernel function satisfies Mercer's condition [17].Once we established the kernel function, we turn now to the classification step which consists of estimating the optimal hyper-sphere that includes all or most of the target objects. The sphere is characterized by its center a and its radius R>0. Minimizing the volume of the sphere reduces to minimizing R2 with the constraints [14]:K(xi−a,xi−a)≤R2∀i. To allow for the possibility of outliers in the target set, the distance from xito the center a should strictly not be smaller than R2 and larger distances should be penalized. Therefore, we introduce slack variables ζi≥0, and the minimization problem becomesminR,a,ζiR2+C∑iζiwith the constraint that almost all objects that belong to the target class should be within the sphere:K(xi−a,xi−a)≤R2+ζi∀i,ζi≥0. The parameter C controls the trade-off between the volume and the errors. The leave-one-out cross-validation estimator was used to estimate our model hyperparameters (R, a, ζi) [25]. This algorithm, often cited as being highly attractive for the purposes of model selection, provides an almost unbiased estimate.The selection of the features depends on the eye disease. The typical markers of the glaucoma progression are the RNFL thinning [26] and the rim area decrease [27]. Therefore, we focused on these areas to classify an eye into progressing and non-progressing (stable) classes. Fig. 2shows the RNFL and the rim area in a B-scan. The estimation of this area for each B-scan is addressed using the method presented in [28]. As in [5,9], we considered two features as input for the classifier: (1) feature1: the number of changed RNFL and rim area locations and (2) feature2: the ratio image intensity R of changed RNFL and rim area locations. In this work, only the loss of retinal height in neighboring areas is considered change due to glaucomatous progression because an increase in retinal height is considered improvement (possibly due to treatment or tissue remodeling). The benefits of the SVDD classifier lie in its ability to only use the target class (non-progressing class) for training in contrast to the SVM, for example, where the information about the outlier class (progressing) are needed in the training step. Fig. 3presents an example of a hypersphere enclosing most of the targets estimated using the SVDD approach and Fig. 4presents an example of a hyperplane separating most of the targets form the ourliers estimated using the SVM approach.

@&#CONCLUSIONS@&#
