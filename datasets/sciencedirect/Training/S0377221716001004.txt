@&#MAIN-TITLE@&#
Exposure at default models with and without the credit conversion factor

@&#HIGHLIGHTS@&#
Direct EAD models ignore CCF formulation and select EAD as response variablePerformance is compared to CCF and utilization change modelsDirect EAD model is more accurate in calibration than benchmark modelsDirect EAD and CCF based models can be combined to drive further performance upliftDirect EAD models without CCF formulation are an alternative for EAD modelling

@&#KEYPHRASES@&#
Exposure at default,Credit cards,Generalized additive models,Regression,Risk analysis,

@&#ABSTRACT@&#
The Basel II and III Accords allow banks to calculate regulatory capital using their own internally developed models under the advanced internal ratings-based approach (AIRB). The Exposure at Default (EAD) is a core parameter modelled for revolving credit facilities with variable exposure. The credit conversion factor (CCF), the proportion of the current undrawn amount that will be drawn down at time of default, is used to calculate the EAD and poses modelling challenges with its bimodal distribution bounded between zero and one. There has been debate on the suitability of the CCF for EAD modelling. We explore alternative EAD models which ignore the CCF formulation and target the EAD distribution directly. We propose a mixture model with the zero-adjusted gamma distribution and compare its performance to three variants of CCF models and a utilization change model which are used in industry and academia. Additionally, we assess credit usage – the percentage of the committed amount that has been currently drawn – as a segmentation criterion to combine direct EAD and CCF models. The models are applied to a dataset from a credit card portfolio of a UK bank. The performance of these models is compared using cross-validation on a series of measures. We find the zero-adjusted gamma model to be more accurate in calibration than the benchmark models and that segmented approaches offer further performance improvements. These results indicate direct EAD models without the CCF formulation can be an alternative to CCF based models or that both can be combined.

@&#INTRODUCTION@&#
The Basel II and III Accords define the standards for calculating regulatory capital requirements for banks across the world (Basel Committee on Banking Supervision, 2005, 2011). Under the Advanced Internal Ratings-Based approach (AIRB), banks are allowed to assess credit risk using their own internally developed models which target three key parameters for each credit facility: (i) Probability of Default, PD, (ii) Loss Given Default, LGD and (iii) Exposure at Default, EAD. These parameter estimates can be used to produce an estimate for the expected loss (EL) or to estimate the unexpected loss for which banks must hold capital. Beyond the purpose of calculating regulatory capital, these three parameters have wide ranging uses for banks, serving as inputs into economic capital models, stress testing, impairment forecasting, pricing and informing portfolio management across retail, corporate and wholesale portfolios.In retail credit risk, PD modelling has been the main focus of credit research for several decades and in recent years, LGD models with challenging bimodal distributions have also been the focus of research (Loterman, Brown, Martens, Mues, & Baesens, 2012). Although EAD distributions are comparatively as difficult to model, they have received much less attention in the literature.For credit card and overdraft portfolios, EAD estimation has proven a hard problem to tackle in practice. For fixed exposures such as residential mortgages and personal loans, the estimate for EAD can simply be taken from the current on-balance amount and little if any modelling is required. For credit cards though, the revolving nature of the credit line poses challenges with regards to predicting the exposure at default time. As credit card customers may borrow more money in the months prior to default, simply taking the current balance for non-defaulted customers would not produce a conservative enough estimate for the amount drawn by the time of default. The EAD could partially be driven by current or recent customer behaviour (i.e. credit usage, drawn, undrawn amounts, changes to undrawn amounts over time). As an example of two distinct behaviour groups, some customers, classified as transactors, tend to pay off their entire balance at the end of each month while others, termed revolvers, tend to pay off only part of the monthly balance and hence incur interest charges (So, Thomas, Seow, & Mues, 2014).To estimate the EAD for credit cards or other forms of revolving credit, the Basel II/III Accord has suggested the use of historic data to evaluate the Credit Conversion Factor (CCF), i.e. the proportion of the current undrawn amount that will likely be drawn down at time of default (Valvonis, 2008). The Accord did not explicitly require EAD models to use CCF calculations; however, CCFs are regularly referred to in the Accord. Once a CCF estimate is produced for a (segment of) variable exposure(s), the EAD is then given by: EAD=Current Drawn Amount+(CCF×Current Undrawn Amount). With this (indirect) approach, the accuracy of EAD prediction is obviously linked to the quality of the CCF model and such modelling has posed substantial challenges because the distribution of CCF does not conform to standard statistical distributions. CCF distributions tend to be highly bimodal with a probability mass at zero (no change in balance), another at one (borrowing has gone up to the credit limit), and a relatively flat distribution in between, not unlike some LGD distributions (Loterman et al., 2012). Furthermore, in many CCF datasets, one might see a substantial number of negative CCFs and CCFs greater than one (an example of the latter may be where the credit limit has increased between the point of observation and the time of default, allowing the customer to go over the original limit); since the final model estimates themselves would have to be constrained between zero and one, such individual observations are sometimes truncated to zero or one, respectively (Jacobs, 2010).Traditional regression modelling with ordinary least squares (OLS) may be less suitable for the CCF because predicted values may be less than zero or greater than one, leading to invalid CCF predictions. Additionally, the non-normality of the error term undermines many of the OLS tests. Standard logistic regression commonly used for PD models would also be inappropriate because the CCF response variable is proportional and not binary. Appropriate discretization of the CCF response would be necessary, which could result in some information loss, or alternatively, fractional response regression should be considered.Taplin, Minh To, and Hee, 2007 have argued that the CCF formulation is problematic because the bounded CCF distribution forces EAD to be equal to the credit limit when CCF equals 1. In practice, it is common to find accounts with EAD greater than the credit limit from charges accrued due to additional purchases over the limit and interest charges, or credit limit changes. The authors instead suggested models that predict EAD directly and ignore the CCF formulation. However, Yang and Tkachenko (2012) have contended that CCF models are more suitable given that the EAD response variable may be too statistically difficult to model given the granular scale of currency amounts and that the CCF formula is less prone to such scaling issues with its range being limited to the unit interval.The aims of this paper are to empirically assess alternative statistical methods for modelling the EAD by targeting the EAD distribution directly rather than focusing on the CCF; to evaluate this, we use a credit card portfolio from a large UK bank. We hypothesize that competitive EAD models can be developed by ignoring the CCF formulation and instead selecting EAD as the response variable in a statistical model. Two different direct EAD models are considered – an OLS model and a zero-adjusted gamma model (Rigby, & Stasinopoulos, 2005, 2007).The zero-adjusted gamma (ZAGA) model was explored to deal with the positively skewed nature of EAD and considering its prior use in predicting the LGD amount of residential mortgage loans (Tong, Mues, & Thomas, 2013). In this model, the EAD amount is modelled as a continuous response variable using a semi-parametric discrete-continuous mixture model approach with the zero-adjusted gamma distribution. Firstly, as the non-zero or positive EAD amount displays right-skewness, it is modelled with the gamma distribution. The mean and dispersion of the positive EAD amount are modelled explicitly as a function of explanatory variables. Secondly, the probability of the (non-)occurrence of a zero EAD amount is modelled with a logistic-additive model. All mixture components, i.e. the logistic-additive component for the probability of zero EAD and the log-additive components for the mean and dispersion of the EAD amount conditional on there being a non-zero EAD, can be estimated using account-level behavioural characteristics.The performance of these direct EAD models are benchmarked against three CCF models (with CCF as the response variable) using OLS, Tobit and fractional response regression and the utilization change model. These approaches are established methods used in industry and/or academia for EAD and LGD modelling (Brown, 2011; Bellotti & Crook, 2012; Bijak & Thomas, 2015).When borrowers are already close to maxing out the credit line and the undrawn amount is low, the CCF can become highly volatile and model performance may be compromised (Qi, 2009). Therefore, a combined approach is suggested that segments on credit usage (i.e. utilization rate, or the percentage of the committed amount that has been currently drawn) and then uses two separate models, with either the CCF or EAD as the response variable, depending on the utilization segment that the credit card falls into. We hypothesize that the combined use of CCF modelling for accounts with low utilization and direct EAD models for accounts with high utilization may improve the overall model performance.Our dataset included time to default as a variable. In practical model development, this variable would be considered unknown a priori for each customer and would not typically be used as a candidate covariate in predictive model fitting. Nonetheless, it has been used in previous empirical studies to study explanatory drivers of CCF (Moral, 2011; Brown, 2014; Jacobs, 2010). Therefore, discarding it would make our results less comparable to those reported by others. Furthermore, it would be interesting to explore this time effect on the various components of the ZAGA model, particularly the dispersion component as one would intuitively expect the error variance to increase the more time elapses between the point of observation and default.To allow a model with time to default as one of its explanatory variables to be applied to a prediction task, we propose an additional survival analysis model component. Survival analysis has previously been employed to model time to default in retail loan portfolios, providing insight into factors that predict when consumers are more likely to default (Stepanova & Thomas, 2002; Malik & Thomas, 2010; Tong, Mues, & Thomas, 2012). Similarly, we develop a PD model using the Cox proportional hazards model (Cox, 1972; Hosmer, Lemeshow, & May, 2008) with time to default as the event of interest but with the length of the cohort period as time horizon. We then show how the resulting monthly PD estimates can be combined with an EAD model that has time to default included as a covariate. This method for modelling EAD using a consistent probabilistic definition and a direct EAD estimation approach was proposed by Witzany (2011). Their research termed this method the ‘weighted PD approach’ and suggested the use of default intensities to estimate EAD by considering the time to default. Our paper extends their work by using a real banking dataset and explicit use of the Cox proportional hazards model. Leow and Crook (2015) have also combined survival and panel modelling methods comprising credit limit and drawn balance models to predict EAD for credit cards. We suggest this method could further incorporate the time to default as a predictive covariate in an EAD model to improve model performance.The novel aspects of our study thus are that we (1) evaluate whether competitive EAD models can be developed by targeting the EAD distribution directly without using a CCF component, (2) assess credit usage as a segmentation criterion allowing us to combine two types of EAD models to further improve performance, (3) compare the performance of these new approaches to CCF and utilization change models commonly used in industry and/or academia and (4) propose an additional survival analysis component to allow the use of time to default as a predictive covariate in EAD modelling. All models will be assessed out-of-sample using cross validation on a series of discrimination and calibration measures.The remainder of the paper is organized as follows. In Section 2, an overview of the dataset along with the application and behavioural characteristics used for the EAD models will be presented. The statistical and validation methods used in our experiments are discussed in Section 3. Next, the results of the models are discussed in Section 4. Section 5 will conclude the paper and suggest some further avenues for research.The dataset consisted of 10,271 observations of accounts from a major UK bank. The dataset derived from a credit cards portfolio observed over a three year period from January 2001 to December 2004. In the absence of additional data about other potential default triggers, for the purpose of this study, a default occurred when a charge off or closure was incurred on the credit card account. A charge off in this case was defined as the declaration by the creditor that an amount of debt is unlikely to be collected, declared at the point of 180 days or 6 months without payment. To compute the observed CCF value, the original data set was divided into two twelve-month cohorts. The first cohort ran from November 2002 to October 2003 and the second cohort from November 2003 to October 2004. In the cohort approach for CCF, discrete calendar periods are used to group defaulted facilities into 12-month periods, according to the date of default. Data was then collected on candidate EAD risk factors and drawn/undrawn amounts at the beginning of the calendar period and drawn amount at the default date.Fig. 1shows the empirical CCF distribution after truncation; the mean CCF value here was 0.515 (sd=0.464). The value is similar to that of S&P and Moody's defaulted borrowers' revolving lines of credit from 1985 to 2007, as reported by Jacobs (2010); there, the truncated mean was 0.422 (sd=0.409). Note that the bimodal nature of Fig. 1 shows similarities to reported LGD distributions (Loterman et al., 2012; Bellotti & Crook, 2012). Fig. 2displays the distribution we observed for the EAD, clearly showing the positively skewed nature of this variable. Please note that some of the scales on the figures in this study have been removed for data confidentiality reasons.As shown in Table 1, a total of 11 candidate variables were considered for the models. The first six candidate variables in Table 1 were suggested by Moral (2011). They were generated from the monthly data in each of the cohorts, where tdis the default date and tris the reference date (i.e. the start of the cohort). The latter five variables were previously suggested in Brown (2011), with the aim of improving the predictive performance of the models.The credit conversion factor for account i, CCFi, was calculated as the ratio of the observed EAD minus the drawn amount at the start of the cohort over the credit limit at the start of the cohort minus the drawn amount at the start of the cohort, i.e.:(1)CCFi=E(td)i−E(tr)iL(tr)i−E(tr)iThe following sections outline the different statistical modelling approaches used to regress the EAD, CCF or utilization change against the candidate drivers listed in Table 1. The direct EAD models (i.e. those with EAD as the response variable) are described in Section 3.1. The three types of CCF models used are outlined in Section 3.2. The utilization change model is described in Section 3.3. The segmented models are introduced in Section 3.4 and the survival model add-on is outlined in Section 3.5. Finally, the process of model validation and testing is described in Section 3.6.The credit cards portfolio is stratified into two groups, the first group having zero EAD (in the absence of further data, we have to assume these may potentially include a number of special or technical default cases, charge-offs related to other accounts, truncated/rounded observations, transfers of the outstanding amount to other repayment arrangements, or they could be the result of late payments subsequent to the default trigger entering the EAD calculation) and a second group having non-zero EADs. The latter appears to have a continuous positively skewed distribution (see Fig. 3) and accounts for the large majority of cases.Let yidenote the EAD observed for the ith account, i=1,...,n (for simplicity, the index i will be omitted from here on); x will be used to denote the vector of covariates observed for the account. A mixed discrete-continuous probability function for y can then be specified as:(2)f(y)={πify=0(1−π)g(y)ify>0where g(y) is the density of a continuous distribution and π is the probability of zero EAD.Fig. 3 shows different candidate distributions for g(y) fitted to the non-zero EADs. Three positively skewed distributions were explored: the gamma, inverse Gaussian and log normal distributions; the normal distribution is shown as a reference comparison. The candidate distributions were fitted onto a training set of a random representative sample. Fig. 3 indicates that the gamma distribution produced the most suitable fit for the histogram of positive EADs. There was further support for the fitted gamma distribution as it produced the lowest Akaike Information Criteria (AIC) when compared to the inverse Gaussian and log normal distributions. The zero-adjusted gamma distribution was hence selected to model f(y). The resulting model will be referred to in this paper as ZAGA-EAD.The probability function of the ZAGA (μ, σ, π) model, a mixed discrete-continuous distribution, is defined by Rigby and Stasinopoulos (2010):(3)f(y|μ,σ,π)={πify=0(1−π)Gamma(μ,σ)ify>0for0≤y<∞,where0<π<1,meanμ>0,dispersionσ>0,Gamma(y,μ,σ)=1(σ2μ)1/σ2y1σ2−1e−y/(σ2μ)Γ(1/σ2)with:(4)E(y)=(1−π)μandVar(y)=(1−π)μ2(π+σ2)The ZAGA-EAD model is implemented using the Generalized Additive Models for Location, Scale and Shape (GAMLSS) framework developed by Rigby and Stasinopoulos (2005). Their approach allows a range of skewed and kurtotic distributions to explicitly model distributional parameters that may include the location/mean, scale/dispersion, skewness and kurtosis as functions of explanatory variables. GAMLSS also allows fitting of distributions that do not belong to the exponential family as provided in the Generalized Linear Model (GLM) (Nelder & Wedderburn, 1972) and Generalized Additive Model (GAM) frameworks (Hastie, Tibshirani, & Friedman, 2009).The GAMLSS approach is a semi-parametric method that allows the relationship between the explanatory variables and response variable to be modelled either parametrically (e.g. where linearity is met), or non-parametrically, using spline smoothers, the latter of which is a key feature of the GAM approach.There are three components to the ZAGA-EAD model. The mean, µ, and dispersion, σ, of a non-zero EAD and the probability of zero EAD, π, are modelled as a function of the explanatory variables using appropriate respective link functions:(5)log(μ)=η1=x′1β1+∑j=1J1hj1(xj1)log(σ)=η2=x′2β2+∑j=1J2hj2(xj2)logit(π)=η3=x′3β3+∑j=1J3hj3(xj3)where x′kβkdenote parametric terms, hjk(xjk) are non-parametric terms such as smoothing splines and with k=1, 2, 3 for the distribution parameters (hence, each model component can have its own selection of covariates). The dispersion of non-zero EAD is the squared coefficient of variation, δ2/µ2, from the exponential family for the gamma density function (McCullagh & Nelder, 1989) where δ2 denotes the variance of the non-zero EAD distribution. The hjk(xjk) functions are modelled with penalized B-splines (Eilers & Marx, 1996). Such non-parametric smoothing terms have the ability to find non-linear relationships between the response and predictor variables (Hastie et al., 2009). Penalized B-splines were chosen because they are able to select the degree of smoothing automatically using penalized maximum likelihood estimation. This selection was done by minimizing the Akaike Information Criterion, i.e. AIC=−2L+kN, with L the log (penalized) likelihood, k the penalty parameter (set to 2), and N the number of parameters in the fitted model (Akaike, 1974). Automatic selection of smoothing may suggest non-linear or linear relationships to the response variable as discovered in the data.Each account, i, in this model is associated with a probability of zero EAD, πi, and a non-zero EAD amount, yi. These pairs are then used to form the following likelihood function:(6)L=∏i=1nf(yi)=∏yi=0πi∏yi>0(1−πi)Gamma(μi,σi)An algorithm developed by Rigby and Stasinopoulos (2005) was used, which is based on penalized (maximum) likelihood estimation. The estimates of the probability of zero EAD, mean and dispersion of g(y) are used to compute an estimate for f(y) which combines the probability of EAD and the EAD amount given that there is a non-zero EAD.The model was developed and implemented using the gamlss package by Rigby and Stasinopoulos (2007) in R 3.0.1 software (R Development Core Team, Vienna, Austria).The second direct EAD model was based on a standard OLS regression of the EAD response (untransformed) against the explanatory variables. We denote this model as OLS-EAD. A parsimonious model was selected through stepwise selection and backward elimination based on a 5 per cent α-level.Three models comprising OLS, Tobit and fractional response regression were developed to predict the CCF (rather than the EAD directly). An account-level estimate for EAD is then derived from the predicted CCF as follows:(7)EAD=CurrentDrawnAmount+(CCF×CurrentUndrawnAmount)Firstly, a standard OLS regression model, denoted OLS-CCF, was fitted for the CCF target. Secondly, a Tobit regression model (Tobin, 1958; Greene, 1997), denoted Tobit-CCF, was developed, which treats observations with CCF below zero and above one as censored with the response only observed in the interval [0, 1]. The Tobit model assumes a latent variable y*, for which the residuals conditional on covariates x are normally distributed. The two-sided Tobit model is given by:(8)y*=x′β+ɛwhere y*|x′∼N(μ, σ2)and(9)y=0,ify*≤0,=y*,if0<y*<1,=1,ify*≥1Maximum likelihood estimates are obtained for the β coefficients; for further details we refer to Greene (1997).Thirdly, a fractional response regression (denoted FRR-CCF) was run. This model has been used for modelling bimodal LGD distributions of credit cards and corporate loan portfolios (Bellotti & Crook, 2012; Qi & Zhao, 2011). FRR is a quasi-likelihood method proposed by Papke and Wooldridge (1996) to model a fractional continuous response variable bounded between zero and one, with valid asymptotic inference and is given by:(10)E(CCF|x′)=F(x′β)where x is a vector of explanatory variables, β is a vector of coefficients and F() represents the logistic functional form which ensures that predicted values are constrained between zero and one.(11)F(x′β)=11+exp(−x′β)To estimate the β coefficients, the log-likelihood function is maximized, i.e. the sum over all accounts of:(12)l(β)=CCF×log[F(x′β)]+(1−CCF)×log[1−F(x′β)]Similarly to the direct EAD models, variable selection for all CCF models was performed through stepwise selection and backward elimination. The OLS-EAD and all three CCF models were developed with SAS 9.3 software (SAS Institute Inc., Cary, NC, USA).An alternative benchmark model, which has been popular in industry, was developed based on the facility utilization change (Yang & Tkachenko, 2012). The utilization change models the outstanding dollar amount change as a fraction of the current commitment amount and is defined for account i as(13)util=E(td)i−E(tr)iL(tr)iA Tobit model, denoted Tobit-UTIL, was fitted as in Eq. (8) which treats observations with util below zero and above one as censored hence the response is only observed in the interval [0, 1].Segmented models were developed using the credit usage variable to partition accounts into low and high utilization accounts. A CCF model was then fitted to the low usage subset of the data, an EAD model to the latter. Sensitivity analysis was used to identify an optimal credit usage cut-off for the partitioning. Model calibration performance was evaluated by varying the credit usage segmentation cut-point from 10 per cent to 95 per cent. The cut-off that produced the highest calibration performance (i.e. lowest MAE, RMSE; cf. Section 3.6) was selected. When a cut-off was identified, low usage accounts were modelled with an FRR-CCF model since this is the model that achieved the highest calibration performance among the CCF models considered earlier. High usage accounts were tackled with OLS-EAD and ZAGA-EAD models. We denote the two resulting segmentation models by OLS-USE (the one comprising FRR-CCF and OLS-EAD) and ZAGA-USE (i.e. FRR-CCF combined with ZAGA-EAD), respectively.To allow the time to default variable to be used as an explanatory variable in practical model development, we propose that a Survival PD model be developed and applied in conjunction with the EAD model, we term this combination the Survival EAD model. The time to default variable is unknown a priori and cannot be used for predictive modelling with conventional EAD model frameworks. To avoid having to discard the variable, a Survival PD model component was developed with the Cox proportional hazards (PH) approach. Several aforementioned models, Tobit-CCF, FRR-CCF, Tobit-UTIL, ZAGA-EAD and ZAGA-USE, with time to default as an explanatory variable, were considered for the EAD component.The semi-parametric approach in hazard form for the Cox PH model is given by:(14)h(t|x′)=h0(t)exp(x′β)where h(t|x′) is the hazard or default intensity at time t conditional on a vector of explanatory variables x′, and in which h0(t) is the baseline hazard, i.e., the propensity of a default occurring around t (given that it has not occurred yet) when all explanatory variables are zero. The baseline hazard is left unspecified for the Cox PH model.Combining estimates from the Cox PH and EAD models, we calculate the expected EAD for account i, as follows:(15)EAD=∑t=112([S(t−1)−S(t)]1−S(12)×EAD(t))where S(t) is the survival function at time t,[S(t−1)−S(t)]thus gives the probability of default occurring in the tth month according to the Cox PH model, and EAD(t) is the EAD model estimate (according to Tobit-CCF, FRR-CCF, Tobit-UTIL, ZAGA-EAD or ZAGA-USE) conditional on the time to default being t. Hence (15) allows us to produce estimates of EAD without any prior knowledge of the time to default variable.Note that, to produce valid EAD estimates, the horizon length for the Cox model must be the length of each cohort period (12 months) and the origin of time is taken to be the start of the cohort period in which default occurs; this means no event time censoring is observed in the data and each of the produced default probabilities are indeed conditional on the account defaulting over the cohort period (i.e. S(12)=0). One could argue that, in the absence of censoring, other (non-survival) regression methods could also be considered, but its flexible baseline hazard still makes the Cox PH model an attractive candidate for modelling time to default. The Cox PH model was developed with SAS 9.4 software (SAS Institute Inc., Cary, NC, USA). Table 8 displays the results of the models fitted using (15).To assess the out-of-sample performance of the models thoroughly, 10-fold cross validation was conducted on the entire sample of accounts on a series of discrimination and calibration measures. All measures were derived from account-level EAD predictions (either direct ones or produced indirectly through a predicted CCF) to have a common base of comparison. To evaluate discriminatory power (i.e. the models' ability to discriminate between different levels of EAD risk), the Pearson r and Spearman's ρ correlation were computed. The Pearson r measures linear association and the Spearman's ρ correlation measures the correlation between the rank orderings of observed and expected EADs. Calibration performance (here seen as the model's ability to come up with accurate account-level estimates of EAD) was assessed with the mean absolute error (MAE) and the root mean square error (RMSE). A normalized version of these measures was also produced, where MAE and RMSE were calculated for EAD/Commitment Amount, which facilitated a percentage interpretation. These measures were termed MAEnorm and RMSEnorm respectively.

@&#CONCLUSIONS@&#
