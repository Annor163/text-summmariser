@&#MAIN-TITLE@&#
Probabilistic forecasting with discrete choice models: Evaluating predictions with pseudo-coefficients of determination

@&#HIGHLIGHTS@&#
Forecasts of discrete choice can be misjudged due to difficulties of pseudo-R2.We resolve these difficulties in the context of competitive event forecasting.McFaddenś pseudo-R2 is dependent on the mean number of alternatives.Pseudo-R2s can be given standard errors with bootstrap and asymptotic methods.We derive a relative pseudo-R2, a new measure of the economic value of forecasts.

@&#KEYPHRASES@&#
Forecasting,Decision analysis,Finance,Discrete choice models,Horseracing,

@&#ABSTRACT@&#
Probabilistic forecasts from discrete choice models, which are widely used in marketing science and competitive event forecasting, are often best evaluated out-of-sample using pseudo-coefficients of determination, or pseudo-R2s. However, there is a danger of misjudging the accuracy of forecast probabilities of event outcomes, based on observed frequencies, because of issues related to pseudo-R2s. First, we show that McFadden’s pseudo-R2 varies predictably with the number of alternatives in the choice set. Then we evaluate the relative merits of two methods (bootstrap and asymptotic) for estimating the variance of pseudo-R2s so that their values can be appropriately compared across non-nested models. Finally, in the context of competitive event forecasting, where the accuracy of forecasts has direct economic consequence, we derive new R2 measures that can be used to assess the economic value of forecasts. Throughout, we illustrate using data drawn from UK and Ireland horse race betting markets.

@&#INTRODUCTION@&#
Decision makers often face choices between different alternatives Ai,i=1,…,n,with payoffs xijdepending on the future state of the world Wj,j=1,…,m. Normative models of decision making indicate that the decision maker should select the option with the highest expected utility,(1)EU(Ai)=∑j=1mp(Wj)U(xij),where p(Wj) is the probability of future state Wjoccurring and U(xij) is the utility of outcome xijif alternative Aiwere selected and state Wjoccurred. Clearly, to determine which alternative is associated with the maximum expected utility it is important to develop accurate estimates of the utility values U(xij). In addition, rather than simply predicting which state of the world is most likely to occur, it is important to accurately forecast the probabilities of the different states of the world, p(Wj). It is the means of improving such probabilistic forecasts derived from discrete choice models that forms the focus of this paper.The importance that organizations attach to accurate probabilistic forecasts is highlighted by the significant growth in the adoption of prediction markets. These are essentially internal betting markets that attempt to tap into the dispersed ‘wisdom of the crowd’ to produce probability forecasts for a range of possible future events. They have been used by many companies such as Hewlett Packard, Eli Lilly, General Electric, and Google to predict a variety of uncertain outcomes, from the likelihood of the success of new products to the probability of meeting project deadlines (Cowgill, Wolfers, & Zitzewitz, 2009; Plott & Chen, 2002). Forecasts derived from prediction markets can be combined with other information (e.g., data concerning the characteristics of products that have been successful in the past or previous completion times) in an attempt to derive accurate probabilistic forecasts. Discrete choice models, such as conditional logit (CL) and multinomial probit, are ideal for performing this function and are widely applied in probabilistic forecasting. Their primary use has been in predicting individuals’ choices from a range of alternatives, so they have been employed in consumer choice and marketing (Lin & Sibdari, 2009) and econometrics (Maddala, 1983), but have also been adopted in such diverse fields as epidemiology (Breslow & Day, 1994), operations research (Cheng & Stough, 2006), and the forecasting of competitive events (Smith & Vaughan Williams, 2010).The development of novel and sophisticated discrete choice methods, particularly to improve the accuracy of forecast probabilities, has received a great deal of attention (e.g., Liu, 2011; Abe, 1999). In general terms, probabilistic forecasts are regarded as being accurate when the relative frequencies of observed events match the forecast probabilities (Maddala, 1983). However, there has been relatively little consideration in the literature to the manner in which probabilities derived from discrete choice models are evaluated out-of-sample, yet this is essential for maximizing the accuracy of probabilistic forecasts. A key property of any means of evaluating the accuracy of forecast probabilities is its comparability across empirical models (Kvålseth, 1985). Otherwise, the researcher cannot be certain whether differences in the evaluation arise because of changes in the model’s predictive power or because of confounding factors, such as properties of the data. Comparability is also reliant on being able to assign degrees of uncertainty to forecast point estimates, in order to ensure that conclusions drawn from evaluating measures of accuracy are statistically significant and hence robust to randomness.In linear models, the coefficient of determination R2 is widely used as a measure of a model’s ability to explain variation in the data, and thus the accuracy of the model’s forecasts. The properties of R2 and when and how it should be employed are now well-understood (e.g., Kvålseth, 1985; Draper & Smith, 1998), although with some caveats in the case of out-of-sample forecasting (Armstrong, 2001). However, in this paper, we are concerned with the out-of-sample forecasting accuracy of choice models, which are nonlinear. Measures such as information criteria would be useful for assessing predictive accuracy in this case, but only if one were interested in the accuracy of specific event prediction, such as binary win/loss predictions. This is not our focus. Rather, we are concerned with the calibration of probability forecasts. In such settings, pseudo-R2s, which are equivalent measures to R2 for nonlinear models, are generally recommended (e.g., Greene, 2012; Maddala, 1983). In fact, in any environment where one seeks to use probability predictions to make pecuniary gain (e.g., in options, futures, spread trading and betting markets), it can be shown that there is a direct link between increases in pseudo-R2 values and out-of-sample returns (e.g., Benter, 1994; Lessmann, Sung, Johnson, & Ma, 2012).Despite the literature’s focus on the use of pseudo-R2s to assess the out-of-sample forecasting accuracy of choice models, there is little consensus regarding the properties and appropriate application of pseudo-R2s for this task. To begin with, there are significant differences between R2s and pseudo-R2s. So, while pseudo-R2s are commonly reported (Cheng & Stough, 2006; Schnytzer, Lamers, & Makropoulou, 2010), their usage is seldom justified (Veall & Zimmerman, 1996), and there are still many unresolved issues associated with them. First, unlike R2, there is no single definition of pseudo-R2 that is universally employed. Rather, a variety of measures has been proposed, which may have different interpretations (Menard, 2000). Second, they are not necessarily comparable across different datasets. Finally, the distributions of R2s are complex and depend on unknown parameters (Ohtani, 2000). For pseudo-R2s, this issue is exacerbated because not only are the distributional properties of pseudo-R2s different to those of R2, they also depend on the particular definition of pseudo-R2 employed and the choice of model. Consequently, while pseudo-R2s are often reported, they are seldom accompanied by standard errors (Press & Zellner, 1978), meaning significance tests are often ignored. The above considerations have serious consequences for the development of accurate discrete choice models because they impact effective evaluation of the forecasting accuracy of these models. Since these issues are rarely examined, there is a significant danger of selecting a sub-optimal model or misinterpreting the relative forecasting ability of different discrete choice models.One of the many applications for discrete choice models is in the forecasting of competitive event (CE) outcomes (e.g., Lessmann, Sung, & Johnson, 2009; Lessmann et al., 2012). A CE is a contest between at least two rival participants, where (generally) one winner is declared and the outcome is uncertain, such as political elections or sporting events. Probabilistic forecasting in this context involves estimating the probability of the various competitors winning. Often, these events are associated with markets for betting or trading on their outcome, e.g., betting markets in the case of sporting events (Sung, Johnson, & Peirson, 2012), or prediction markets for political contests or for outcomes associated with business policies (Wolfers & Zitzewitz, 2006). Since the outcomes of CEs are of particular interest for economic (in the case of sporting events or business policies) or policy reasons (elections), the forecasting of CEs is a prominent subject in the literature (e.g., Schnytzer et al., 2010).The standard modeling approach is to view competitors as alternatives in a choice set with the winner being the participant whose attributes lead it to being ‘preferred’, hence, the suitability of discrete choice models. While the typical motivation for pseudo-R2 in out-of-sample forecast evaluation is as a measure of improvement from the null model (where each alternative is considered equally likely) to the fitted model (e.g., Benter, 1994; Franck, Verbeek, & Nüesch, 2010), a more useful measure would be improvement of the fitted model from a model based on the forecasts of prices from the associated market. In the case of prediction markets, this would be the degree to which the fitted model (incorporating the prediction market prices together with information from other data sources, such as the probability of meeting project deadlines in the past) improves on the probability forecasts derived directly from the prediction market prices. Such a measure, which we call relative pseudo-R2, would be of value because there would be a direct link between relative pseudo-R2s and the economic value of the forecast probabilities derived from the fitted model. Most importantly, this metric would be comparable across different market settings.In this paper, we address the above unresolved issues related to pseudo-R2s and illustrate these points empirically with data drawn from CEs. Throughout the paper we refer to the CL model, which is the most widely-used in this context, although our findings readily extend to other discrete choice models. We show that at least one commonly reported pseudo-R2 measure is not robust to changes in the number of alternatives in each event. Consequently, in order for discrete choice models to be comparable using these measures across non-nested models or across models evaluated on different datasets, we suggest a rescaling of the pseudo-R2. We also demonstrate, with an example from CEs, a potential misinterpretation that may arise from the bias if this rescaling is not employed.We also describe means of conducting significance tests for comparing pseudo-R2 values from different forecasting models. In particular, we describe two methods for obtaining estimates of the variance of pseudo-R2 measures, the bootstrap and asymptotic methods, and find that they both produce estimated variances that are reasonably close. Consequently, either method could be used to conduct significance tests for comparing pseudo-R2 values.In addition, we define relative pseudo-R2s that measure the improvement of a fitted model from a model based on the forecast prices in the associated betting or prediction market (in the context of out-of-sample forecasting of CEs). This provides a comparable metric across different market settings, and shows that there is a relationship between relative pseudo-R2s and the economic value of forecast probabilities, a finding that has important implications for assessing the efficiency of financial markets.Throughout the paper we illustrate the value of the approaches and measures we introduce using data drawn from horserace forecasting. These data contain the essential features of all complex choice modelling problems, including different numbers of alternatives (horses) in different choice sets (races). In addition, they offer the advantage, for the purpose of illustrating the applicability of the techniques we suggest, that a certain point in time (the end of the race) all uncertainty is resolved and we are able to assess the accuracy of forecasts. In addition, the associated betting markets provide us with a means of assessing the economic value of forecast probabilities.The conditional logit (CL) model (McFadden, 1974) is employed to estimate or forecast the probability of each alternative being chosen (based on attributes of the alternatives) in situations where a decision maker or ‘nature’ selects a specific alternative from a number of competing alternatives. The utility of each alternative i in event j is given by(2)Wij=β⊤xij+ϵij,whereβ=(β1,β2,…,βm)⊤are the coefficients of the attributes xij, and ϵijis an independent error term. There are likely to be many independent factors that contribute to the error term and, consequently, the central limit theorem can be used to justify the assumption that the ϵijare normally distributed, resulting in a probit specification. However, to make the forecast probabilities tractable, it is generally assumed that the error term has a double exponential distributionf(x)=exp[−x−exp(−x)](Maddala, 1983); in practice, the difference between logit and probit models is small (Judge, Garrett, & Griffiths, 1985). The probabilities are then given by(3)pij=Pr(Wij>Wkj,k=1,2,…,nj,k≠i)=exp(β⊤xij)∑k=1njexp(β⊤xkj),where njis the number of alternatives. The coefficients are estimated by maximum likelihood:(4)lnL=∑j=1N∑i=1njyijlnpij,whereyij=1if alternative i is chosen,yij=0otherwise, and N is the total number of choice problems. We denote the maximized log-likelihood function bylnL(β^).The coefficient of determination R2 is a popular goodness-of-fit measure in linear models. Varying between 0 and 1, it has multiple interpretations: (i) the proportion of variation explained by the model, (ii) the square of the correlation between predicted and observed values, and (iii) the improvement from a null model (with no independent variables) to the fitted model. For nonlinear models, a range of alternative pseudo-R2 measures have been proposed. The motivation for these is primarily interpretation (iii) above, i.e., improvement from null to fitted model. CL is an example of a model estimated by maximum likelihood, and in fact, for any model estimated by maximum likelihood, pseudo-R2s that satisfy this criterion can be defined.The most widely used measure (e.g., Franck et al., 2010) is the McFadden (1974) pseudo-R2, which is given by(5)RM2=1−lnL(β^)lnL(0),where ln L(0) is the ln L of the null model, where each alternative is assigned the same probability of being chosen:(6)lnL(0)=∑j=1Nln(1/nj).An alternative is the Maddala (1983) pseudo-R2, given by(7)RD2=1−exp{−(2/N)[lnL(β^)−lnL(0)]}.The McFadden pseudo-R2 has a maximum value of 1, while the maximum value of Maddala’s isRD2=1−exp{(2/N)lnL(0)}. Nagelkerke (1991) proposed that Maddala’s definition be rescaled so that it takes a maximum of 1, but the original definition actually has the desirable property of alternatives independence (see the next section), so we would not recommend this rescaling. In this paper we consider only these two definitions of pseudo-R2, which are the two most popular, but our results are readily extended to other versions that have been proposed (e.g., Nagelkerke, 1991).A major concern in probabilistic forecasting is that of evaluating the accuracy of forecast probabilities. We believe that pseudo-R2s are the most fundamentally important tool for assessing the performance of discrete choice models designed to forecast the probabilities of future events. Maximizing pseudo-R2 is equivalent to maximum likelihood, where the criterion essentially chooses the set of parameters which maximizes the probability of observing the particular set of alternatives that are observed ex post. It is especially important in choice models to maximize the model probability for the alternative that is eventually chosen. For example, from the perspective of an organization, it is important when estimating the probability of success of various products, to maximize the model probability of success for the product that turns out to be successful (from a range of possible products), as this will increase the chance that this product has the highest expected utility and is then the one that is selected. In addition, the forecast probability of success of this product is likely to affect the level of investment from the organization.Equally, in probabilistic forecasting of CEs, it is important to maximize the probability for the eventual winning competitor, since a decision of whether or not and how much to bet on that competitor will depend on the predicted ‘edge’ to a bet on that competitor pj/qj, where qjis some baseline probability that the model is being compared against (the null probability later in this section, and the market probability in Section 4). Indeed, this is an approach advocated by Benter (1994), who is reported to be the most successful bettor of all time. He recommends the application of CL models (incorporating a range of explanatory variables) to develop forecasts of the winning probability for each competitor and emphasizes the role of pseudo-R2 as a measure of the ‘explanatory power’ of a model. He indicates that pseudo-R2 is the best means he has found for comparing the efficacy of alternative models. In addition, it has been shown that probabilities estimated by maximum likelihood yield maximum in-sample return (on investments based on these forecast probabilities) to a log utility investor (Johnstone, 2011). Moreover, increases in out-of-sample returns generally result directly from an increase in pseudo-R2 (Lessmann et al., 2012). Note that the emphasis here is on the accuracy of the probabilities derived from discrete choice models, rather than a binary classification of win/lose (chosen / not chosen). It is a common misconception in forecasting of CEs that the goal above all else is to pick winners, whereas it is easy to demonstrate that maximizing edge is the key to success. Therefore, traditional measures of forecasting accuracy such as percentage of correct classifications are significantly less relevant in this context.When comparing two or more competing models, a problem may arise when the models are compared over different data. Specifically, the datasets might differ in their underlying characteristics. This could arise when subsets of data are sampled, or when data is split into training and holdout samples. For example, Hyndman and Koehler (2006) give the example from time series of scale-dependent measures being compared across datasets with different scales. Menard (2000) shows that the Maddala pseudo-R2 has a dependence on the base rate in logistic regression models. A specific example in discrete choice modelling is that, depending on how the data are sampled, the average number of alternatives available to each subject may vary. For example, in CEs, we might seek to analyze variations in the predictability of horseraces depending on the number of horses in each race. Here, we sample alternative datasets depending on the number of runners in each race. Consequently, the average number of competitors will be different in each dataset. This presents us with a problem, as we cannot adequately assess the predictability of these events by comparing R2s over these samples using the McFadden pseudo-R2. We now demonstrate this.Recall that the motivation behind pseudo-R2s is that they measure the degree of improvement from the null to the fitted model. Null model probabilities are 1/nj, since without predictors we cannot make any distinction between alternatives. Suppose that, for each set of alternatives j, our model assigns a probability ofpj=fj/njto the eventual chosen alternative, i.e., model probabilities have an ‘edge’ fj≤ njover the null probabilities. In this way, pseudo-R2s can be evaluated for their dependence on (or independence from) the number of alternatives. These dependencies are given in the following proposition (for a proof, see Appendix B).Proposition 1If the model probabilities assigned to the eventual chosen alternative arepj=fj/nj,then the McFadden and Maddala pseudo-R2s are given by(8)RM2=lnf˜lnn˜,RD2=1−1/f˜2,respectively, wheren˜=(∏j=1Nnj)1/Nandf˜=(∏j=1Nfj)1/Nare the geometric means of the number of alternatives and of fj, respectively.Here,f˜can be viewed as the part of R2 that measures the accuracy of probabilities derived from the model. In each case, asf˜increases, so do the R2s. However, the McFadden version has a predictable dependence on the number of alternatives: as njincreases,RM2decreases in proportion tolnn˜. Hence, in order to define an unbiased, rescaled McFadden pseudo-R2, we multiplyRM2bylnn˜,i.e.,(9)R˜M2=(lnn˜)[1−lnL(β^)lnL(0)].Note that this definition now has a maximum oflnn˜,rather than 1. The Maddala pseudo-R2 is already independent of nj.We now show empirically that the Maddala and rescaled McFadden R2s are unbiased by fitting CL models on subsets of horserace betting data categorized by the number of runners in each race. The data employed are final bookmaker odds (market prices) from 6064 horserace betting markets in the UK and Ireland in 2009 and 2010 (for a description of UK betting markets, see Appendix A). These models have just one independent variable, which is the log of winning probability as implied by market prices; the coefficient of this variable is given by β. The results are presented in Table 1 and Fig. 1.Clearly, the Maddala and rescaled McFadden R2s vary in a consistent manner as the number of alternatives is changed, while the standard McFadden version is inconsistent. In particular, there appears to be an increasing trend in the forecast accuracy of odds-implied probabilities as njis increased, but this trend is not captured by the standard McFadden version (note thatf˜maps almost perfectly onto the rescaled McFadden pseudo-R2). We confirm this in two ways: first, we fit linear regressions ofRD2−RM2andRD2−R˜M2on the number of competitors; gradients are given by 0.0174 (t=10.67,p=0.00) and -0.0035 (t=0.20,p=0.42), respectively, i.e., the difference between the Maddala and McFadden R2s increases with the number of alternatives while the difference between the Maddala and rescaled McFadden R2s does not. Second, we compare the trends in pseudo-R2s with two alternative measures of forecasting accuracy, used by Franck et al. (2010), the ROC area and the Brier score. We observed a monotonic relationship between pseudo-R2 and these information criteria, with the ROC area increasing with number of alternatives and the Brier score decreasing (a small Brier score indicates high forecast accuracy). The result here is that, had we interpreted the results only with reference to the McFadden R2, we might have concluded that forecast accuracy does not change with number of alternatives, which is clearly incorrect. In summary, forecasters must take great care when comparing pseudo-R2s between models fitted on different data. In particular, the specific definition of pseudo-R2 employed must be appropriate. A similar analysis could be carried out to test the consistency of other definitions.A common problem in forecasting is how to compare models that either (a) consist of different predictor variables (are non-nested), or (b) are evaluated on different datasets, or a combination of both. It is straightforward to compare forecasts from nested models (where one of the models includes all the independent variables from the other model) evaluated on the same data (with a likelihood ratio test, for example). However, it is significantly more difficult to compare the accuracy of non-nested models or out-of-sample forecasts evaluated on different data. Such comparisons are important because it is often the case in probability forecasting that one is trying to select the ‘best’ model from a number of candidate models (e.g., models incorporating different independent variables). One possibility is to directly compare measures of forecast accuracy, such as pseudo-R2s or the ROC area or Brier score mentioned above. Without standard error estimates, a model may be selected over another simply because its pseudo-R2 is apparently higher. However, the model with the higher pseudo-R2 could have a higher standard error (perhaps because it was estimated on a smaller dataset) and statistical tests may indicate that it is not in fact superior. This is particulary important in discrete choice modelling, where the inappropriate selection of one model over another (perhaps containing different predictor variables) may lead the researcher to mis-specify the behavioral factors influencing the consumers’ choices. Clearly, a statistical test for differences in the pseudo-R2 values requires their distributions, which are complex and depend on unknown parameters. However, it is possible to carry out significance tests by estimating the distribution of pseudo-R2s, and we demonstrate two methods here. The first is to adopt an M-bootstrap approach (Efron, 1979), as recommended by Ohtani (2000) for ordinary R2s. The bootstrap is commonly used when the theoretical distribution of a statistic is complicated, which is the case for the CL model. Suppose we have fitted CL models to two datasets, D1 and D2, consisting of N1 and N2 events, respectively. The M-bootstrap method proceeds as follows:1.Randomly sample N1 events, with replacement, from D1, to form a new dataset BD1. Similarly, randomly sample N2 events, with replacement, from D2, to form a new dataset BD2.Fit CL models on BD1 and BD2 and record the resulting values of pseudo-R2.Perform M iterations of steps 1 and 2.The sample means,μ(R12)andμ(R22),and sample variances,s2(R12)ands2(R22),of the sets of M pseudo-R2s are used to derive a standard normal test statistic,(10)z[μ(R2)]=[μ(R12)−μ(R22)]/s2(R12)+s2(R22),which can be used to test the alternative hypothesis that the probabilities derived from one model are more accurate than those derived from the other, against the null hypothesis of no difference.An alternative, much faster, method for estimating the distribution of a pseudo-R2 is to estimate its asymptotic distribution, i.e., the expected distribution as the number of events tends to infinity. Hu, Shao, and Palta (2006) derive analytically the asymptotic distribution of the Maddala pseudo-R2 in the multinomial logit model (a discrete choice model that is similar to the CL model). Here, we adapt their analysis to derive the asymptotic distribution of the McFadden pseudo-R2, our own rescaled McFadden pseudo-R2 specified above, and the Maddala pseudo-R2 for the CL model (for an outline proof, see Appendix B).Proposition 2Assume that the independent variables xij,j=1,2,…,N,i=1,2,…,njare independent and identically distributed random m-vectors with finite second moment (i.e.,E[xij2]finite). Let(11)H1=E[lnnj],H2=−E[∑i=1njyijlnpij],and let(12)Σ=(Var(nj)ηηϵ),g1=1lnλ(H2λlnλ1),g2=(1λ1),g3=2e2H2λ2(1λ1),where(13)λ=E[nj],η=E[nj∑i=1njyijlnpij]+λH2,ϵ=E[∑i=1njyij(lnpij)2]−H22.Then, as N → ∞,(14)N[RM2−(1−H2/H1)]→dN(0,σ12),N[R˜M2−(H1−H2)]→dN(0,σ22),N[RD2−(1−e2(H2−H1)]→dN(0,σ32),whereσi2=gi⊤Σgifori=1,2,3.The above proposition gives the asymptotic distribution of the pseudo-R2s. Hence, to obtain the estimates of the variance of point estimates of these pseudo-R2s, we can replace the unknown quantities with consistent estimators. So, denote byn¯,n˜,and s2(n) the arithmetic mean, geometric mean, and sample variance of the number of alternatives, respectively, i.e.,(15)n¯=(1/N)∑j=1Nnj,n˜=(∏j=1Nnj)1/N,s2(n)=1N−1∑j=1N(nj−n¯)2.Then, let(16)Σ^=(s2(n)η^η^ϵ^),g1^=1lnn¯(H^2n¯lnn¯1),g2^=(1n¯1),g3^=2e2H^2n¯2(1n¯1),where(17)η^=(1/N)∑j=1Nnj∑i=1njyijlnpij+n¯H^2,ϵ^=(1/N)∑j=1Nnj∑i=1njyij(lnpij)2−H^22.Here(18)H^2=−(1/N)∑j=1N∑i=1njyijlnpij.Then estimates of the variance of the McFadden, rescaled McFadden, and Maddala pseudo-R2s are given by(19)s2(RM2)=1N(g^1⊤Σ^g^1),s2(R˜M2)=1N(g^2⊤Σ^g^2),s2(RD2)=1N(g^3⊤Σ^g^3),respectively.We now verify the two methods by estimating variances of the three pseudo-R2s described above. It has been shown that the bootstrap method overestimates standard errors in large samples for standard logistic regression, relative to the asymptotic distribution (Teebagy & Chatterjee, 1989). Here, we compare values estimated from the bootstrap and asymptotic distribution methods on the same real data, which are described in Section 2. This time odds from two different markets (exchange and bookmaker) are used. We again fit CL models with just the log of odds-implied probability as the single explanatory variable; the coefficient of this variable is β. From the results presented in Table 2, it is clear that both methods produce reasonably similar estimates, with the differences not being significant according to F tests for difference of variances. Since the asymptotic method is very fast to calculate, we would therefore recommend that pseudo-R2s are always reported with a measure of dispersion, so that comparing two non-nested models can always be carried out with a significance test.We now turn to a specific application of discrete choice models: probabilistic forecasting of CEs for identifying and measuring the degree of inefficiency in betting markets. CEs, such as horseraces, usually have an associated market for trading on the outcome. From the prices available in the market (the odds), it is possible to obtain ‘public’ forecasts of the probabilities of each outcome. If these probabilistic forecasts are inaccurate, then the market is inefficient. Skilled forecasters are able to exploit this inefficiency for profit (e.g., Benter, 1994). We argue that, in this context, the primary interpretation of pseudo-R2s, as improvement from the null to fitted model, is misleading. This is because we can easily specify a ‘public’ model that has a single variable, which is the log of odds-implied probabilities (the model we used in Sections 2 and 3), and this model supersedes the null model that has no variables at all. Rather, pseudo-R2 in this context should measure the improvement of the relevant model over the public, where the model contains both the log of odds-implied probabilities together with variables that it is believed are not fully discounted by the public. In this section we define such a measure, which we call relative pseudo-R2, and link it directly to the model’s ‘edge’, i.e., the economic significance of the inefficiency identified by the model.Recall that our dataset consists of N events, where each event j is between nj≥ 2 competitors; for each event, there is one winner, given byyij=1,withyij=0otherwise. ‘Decimal odds’ are denoted by Dij> 1, withdij=1/Dijdenoting market prices. The decimal odds represent the potential return to a bettor from a bet on competitor i in race j, with a winning bet of $1 returning Dijif the bet wins. If the bettor assigns winning probabilities pij, the expected profit from a $1 bet ispijDij−1. Denote the winning probability that the bettor assigns to the eventual winner by pj, its market price dj, its decimal odds Dj, and the winning probability as implied by the oddsqj=dij/(1+Bj),where the ‘over-round’Bj=∑i=1njdij−1represents the market’s transaction costs. Then expected profit from a bet on this competitor, or ‘edge’, is given by(20)Wj=pjqj(1+Bj)−1.Now, we define the relative McFadden and Maddala pseudo-R2s by(21)R¯M2=1−lnL(p)lnL(q),R¯D2=1−exp{−(2/N)[lnL(p)−lnL(q)]},where the log-likelihood of the bettor’s and the public models are given by(22)lnL(p)=∑j=1N∑i=1njyijlnpij,lnL(d)=∑j=1N∑i=1njyijlnqij,respectively. Defined in this way, the relative pseudo-R2s measure the degree of improvement over the public odds for the winning competitor, which we now show is directly related to the bettor’s realized edge (profit per $1 bet).Substituting (20) into (21), we can write the relative McFadden pseudo-R2 as(23)R¯M2=lnGM(1+Wj)+lnGM(1+Bj)lnGM(Dj)+lnGM(1+Bj),whereGM(xj)=(∏j=1Nxj)1/Ndenotes geometric mean. As in Section 2, this has a dependence on the data: in this case, the average over-round and average odds of the winner. So, we define the relative rescaled McFadden pseudo-R2 by(24)R˜¯M2=(lnGM(Dj)+lnGM(1+Bj))R¯M2=lnGM(pj/qj).Then this measure has a log-proportional relationship with average edge. Rearranging,(25)GM(pj/qj)=eR˜¯M2.Similar relationships for the Maddala pseudo-R2 are(26)R¯D2=1−1GM(1+Wj)2GM(1+Bj)2,GM(pj/qj)=11−R¯D2.These relative pseudo-R2s are a novel tool for evaluating and comparing probabilistic forecasts of competitive events, since they allow us to quantify the relative gain of the out-of-sample probability forecasts of a model over those of the relevant benchmark, which is the public model and not the irrelevant null model.To illustrate the usefulness of relative pseudo-R2s, we conduct analysis using real betting market data. We train a number of CL models on a dataset consisting of 18040 horse races from the years 2007-2009, and the results are presented in Table 3. Each model includes a transformation of the public odds (LN_FIXED_ODDS_PROB), which itself adds some predictive power over and above the raw odds-implied probability, along with a successively increasing set of independent variables based on fundamental information pertaining to the horse’s winning chances, such as its lengths beaten in past races, speed in past races, ability of the jockey/trainer and weight carried. With model coefficients βkfixed from the in-sample data, we then evaluate the probability forecasts derived from each model on a holdout set of 6309 races from 2010, and calculate relative pseudo-R2s over this data (Table 4). To illustrate how relative pseudo-R2s derived from a model are related to its economic importance, we simulate a Kelly betting strategy (Kelly, 1956) on the outcomes of each race in the holdout set using the actual market prices available. A proportion of capital equal tomax((pijDij−1)/(Dij−1),0)is bet on each horse, which is the amount that maximizes the log of expected returns from the bet (it is assumed that the bookmaker will take bets of any size).The first column in Table 4 is based on the qijdirectly, without any model. Hence, the relative pseudo-R2s are 0 and no betting opportunities are found. Each of the other models are effectively being compared with this one. The relative pseudo-R2s here are small, so we present them as percentages, but they translate into increasingly large expected and actual profits, which are determined from the pijand from the identity of the winning horse, respectively (odds-implied number of wins are determined assuming win probabilities are qij). Note that while the expected profits increase consistently as the number of predictor variables is increased, actual profits are subject to the natural variance in the data, and therefore do not necessarily increase monotonically. However, because relative pseudo-R2s increase consistently with expected profits and have a natural interpretation of 0 being no expected profits, it is clear that the relative pseudo-R2s can be used as an indicator of the economic value of the model.In this section we have demonstrated that direct relationships can be derived between forecast probabilities estimated by CL models containing multiple predictor variables, and relative pseudo-R2 measures. These relationships are crucial because they represent the economic value of forecasting models and can be used to assess the economic significance of any market efficiency that the models unearth. They also contribute to an understanding of the context in which pseudo-R2s should be reported. In a broader range of problems, the context-specific public model might be replaced by other types of ‘base’ model that are not simply the 1/n null model. For example, an organization, wishing to maximize their subjective expected utility from their decision making, might already be in possession of hard data on some possible courses of action, such as their current practices. In this case, it might be more appropriate to compare their forecasts with their existing data, in which case it is the relative expected gain that is important; in a choice model context this could be measured by relative pseudo-R2.

@&#CONCLUSIONS@&#
