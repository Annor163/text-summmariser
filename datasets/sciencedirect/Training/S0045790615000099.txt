@&#MAIN-TITLE@&#
The pseudo-distance technique for parallel lossless compression of color-mapped images

@&#HIGHLIGHTS@&#
A pseudo-distance technique (PDT2) for lossless compression of images is given.Parallelization of the technique is presented.The performance of the PDT2 in Hyper-Threading is evaluated.

@&#KEYPHRASES@&#
Parallel image compression,Hyper-Threading,Color-mapped images,PDT,BWT,PNG,

@&#ABSTRACT@&#
Data compression is a challenging process with important practical applications. Specialized techniques for lossy and lossless data compression have been the subject of numerous investigations during last several decades. Previously, we studied the use of the pseudo-distance technique (PDT) in lossless compression of color-mapped images and its parallel implementation. In this paper we present a new technique (PDT2) to improve compression gain of PDT. We also present a parallelized implementation of the new technique, which results in substantial gains in compression time while providing the desired compression efficiency. We demonstrate that on non-dithered images PDT2 outperforms PDT by 22.4% and PNG by 29.3%. On dithered images, PDT2 achieves compression gains of 7.1% over PDT and 23.8% over PNG. We also show that the parallel implementation of PDT2, while compromising compression less than 0.3%, achieves near linear speedup and utilization of Intel Hyper-Threading technology on supported systems improves speedup on average 18%.

@&#INTRODUCTION@&#
For long term storage and efficient transmission of data, use of data compression is necessary. The purpose of a data compression process is to reduce the file size while minimizing the deterioration of information on the data. Reducing redundant data in a file is one of the common approaches used by most compression algorithms. Data compression consists of two main parts: encoding and decoding. Encoding is the process of generating data to represent original data in a more compact form. Decoding process is the reverse of the encoding process. In decoding, using compressed data, the original data is reconstructed.There are two main types in data compression: lossy and lossless. In lossy compression, some information of the data may be ignored in encoding. When the encoded data is decoded, the original file and the decompressed file may differ. When portions of the data are not completely essential and minor loss in data is acceptable, lossy compression algorithms may be preferred in order to maximize compression gain. However, in some cases, e.g. medical images, when the quality of the image is top priority and any loss on data is not tolerable, the use of lossless compression algorithm is the preferred practice. In lossless compression, while the quality of a file is preserved, the reduction in the file size may not be as good as in lossy compression.Color image quantization [1,2] is a commonly used technique, especially for low-cost display devices. Such devices have certain limitations most notably the inability to display the full RGB color space. An image file that uses RGB color space may include 16.8million (224) possible color combinations. To represent the whole spectrum, an RGB color image requires three bytes for each pixel; one byte for each color (red, blue, and green). The color quantization technique is applied to RGB images to map 24bits to 16, 8, or 4bits per pixel. In this technique, after creating a suitable palette to reduce number of colors, each pixel in the RGB image is replaced with the index of the closest color in the palette. When a palette size is 256 (28) or less, each index can be represented with 8bits (a byte). Hence, the size of image can be reduced from three bytes to one byte per pixel by applying the quantization process.During the quantization process, certain quantization errors, e.g. color distortion, may be introduced. To minimize quantization errors, utilization of dithering algorithms is an accepted practice. Many of the commonly used dithering algorithms employ some sort of a diffusion process where colors that are not available in the palette are approximated by a diffusion of colored pixels from within the available palette. Further information on dithering algorithms utilizing diffusion are available in, for example, [3,4].Quantized images are stored and transmitted after they are compressed lossless with one of the standard image compressors, e.g. GIF, PNG, JPEG-LS, or JPEG2000. To improve compression gains, before using one of these compressors, re-indexing of the color table indices have been investigated by many researchers [5–8]. In the survey paper [5], Pinho and Neves compared various re-indexing schemes and concluded that “the pairwise merging heuristic proposed by Memon et al. is the most effective, but also the most computationally demanding.” They also “found that the second most effective method is a modified version of Zeng’s reordering technique, which was 3–5% worse than pairwise merging, but much faster.” In a more recent publication [8], Battiato et al. proposed a new re-indexing algorithm and showed that “by using the PNG codec, the values of bpps of the proposed approach are considerably lower than the other methods; in some cases, the differences are substantial”.Instead of re-indexing color-table indices, Kuroki et al. in [9] proposed a different technique called the pseudo-distance technique (PDT) for lossless compression of non-dithered and dithered color-mapped images. PDT uses neighboring pixels to create a pseudo-distance table and transforms index values into pseudo-distance table values. In our earlier studies [10,11], we showed that better compression gains over other techniques are possible when PDT is applied to non-dithered color-mapped images along with an arithmetic coder. In [12], we examined the PDT for dithered color-mapped images and demonstrated that PDT is effective also on dithered images. Later, we investigated the use of a dynamic pseudo-distance table and in [13] showed that further improvement on compression gain is possible when the entries of the pseudo-distance table are updated based on the neighboring pixels.The increased availability and lower prices of multi-core central processing units (CPU) for personal computers and mobile devices created an interest in parallelization of commonly used data compression algorithms. Howard and Vitter [14] proposed an efficient parallelization of Huffman and arithmetic coder for lossless image compression. Gilchrist [15] developed a parallel implementation of bzip2. In [16], Gilchrist and Cuhadar presented various parallel algorithms of block sorting technique, e.g., Burrows–Wheeler Transform (BWT) for lossless image compression. In [17], we showed that PDT is highly parallelizable and suitable for systems with multi-core CPUs. When we parallelized the PDT algorithm in [11], we obtained near linear speedup and showed that by using the parallel implementation of PDT, compression time can be reduced significantly for lossless compression of the non-dithered color-mapped images.In this paper, first, we present a new pseudo-distance technique (PDT2) that improves the efficiency of our previous sequential PDT algorithm. The main improvements are: PDT uses three neighbors of a pixel to encode the pixel value but PDT2 uses up to five neighbors of the pixel; while the PDT algorithm updates one row of the pseudo-distance table for each pixel, PDT2 may update up to five rows based on the availability of neighbors of the pixel. In addition, PDT2 swaps the entries of a pseudo-distance values depending on neighboring pixels to better predict error values from the pseudo-distance table.Second, we parallelize the PDT2 using the same technique that we utilized in [17] to parallelize PDT. Furthermore, we evaluate the performance of the parallel implementation of PDT2 along with the Hyper-Threading technology on the dithered and non-dithered color-mapped images. With PDT2, we improve the compression gain 22.4% over our previous study [11] on the non-dithered images, 7.13% on the dithered images [13]. The speedup with the parallelization of PDT2 agrees with the theoretical prediction form Amdahl’s Law [18], with only 0.3% compression loss, and in case of Hyper-Threading with Casey’s Law [19].This paper is planned as follows: In Section 2, the description of the PDT2 is outlined. In Section 3, the description of the block-sorting transformation and the arithmetic coder used is presented. In Section 4, the experimental results are discussed, and finally, in Section 5, the conclusions of our paper are summarized.In this section, we briefly describe the Euclidean distance, encoding and decoding algorithms of PDT2, and a parallel implementation of PDT2 algorithm.This subsection defines the Euclidean distance and how the distance matrix uses the color-map table while creating the pseudo-distance matrix. A color-mapped image consists of two parts; a header and an image data. In the header part, a table of (Ri,Gi,Bi) values of the colors used in the image. The color table of image Music.bmp, from our set of test images, using 8 colors is displayed in Table 1.The Euclidean distance is defined between two color indices a and b (0⩽a, b⩽255) from the color-mapped table of an image as follows:(1)D[a,b]=(dR)2+(dG)2+(dB)23whereD[a,b]is the distance between index a and index b, dRthe difference between the R values of a and b, dG the difference between the G values of a and b, dB is the difference between the B values of a and b.By calculating the Euclidean distance between every pair of indices, a distance matrix D can be constructed. The distance matrix D of Music.bmp is illustrated in Table 2.The values in a given row of the Euclidean distance matrix D may not be necessarily unique. This problem makes the encoding ambiguous. The problem can be solved by forming a new pseudo-distance matrix P, where each row elementP[a,b]gets a unique integer value output, where 0⩽output⩽255, as follows. In each row, the smallest distance value is set to 0, the next distance value is set to 1, and so on, until the largest value is set to 255. In case of equal distance values, they are set to consecutive integer values bases on the location of the distance values in the row. The pseudo-distance matrix P of Music.bmp is given in Table 3.In this section we describe our new pseudo-distance encoding and decoding algorithms which we will call PDT2 and reverse PDT2. In encoding, we first form a distance matrix D by calculating Euclidean distance between every pair of indices from a color-map table and using the distance matrix D, we create a pseudo-distance matrix P. For simplicity, x, and its neighborsa,b,c,d,edenote the pixel values and their locations shown in Table 4. PDT2 works as described in Algorithm 1 below.Algorithm 1Pseudo Distance Transform (PDT2).1: repeat2:ac←P[a,c],bc←P[b,c],cc←P[c,c],dc←P[d,c],ec←P[e,c]3:fori←1tonumberofcolorsdo4:if(ac>P[a,i])then5:P[a,i]←P[a,i]+16:end if7:if(a!=b)and(bc>P[b,i])then8:P[b,i]←P[b,i]+19:end if10:if(a!=c)and(b!=c)and(cc>P[c,i])then11:P[c,i]←P[c,i]+112:end if13:if(a!=d)and(b!=d)and(c!=d)and(dc>P[d,i])then14:P[d,i]←P[d,i]+115:end if16:if(a!=e)and(b!=e)and(c!=e)and(d!=e)and(ec>P[e,i])then17:P[e,i]←P[e,i]+118:end if19:end for20:P[a,c]←0,P[b,c]←0,P[c,c]←0,P[d,c]←0,P[e,c]←021:output←P[a,x]22:swap(P[a,x],P[a,c])23:if(a!=b)then24:swap(P[b,x],P[b,c])25:end if26:if(a!=c)and(b!=c)27:swap(P[c,x],P[c,c])28:end if29:if(a!=d)and(b!=d)and(c!=d)then30:swap(P[d,x],P[d,c])31:end if32:if(a!=e)and(b!=e)and(c!=e)and(d!=e)then33:swap(P[e,x],P[e,c])34:end if35:xx←P[x,x]36:fori←1tonumberofcolorsdo37:if(xx>P[x,i])then38:P[x,i]←P[x,i]+139:end if40:end for41:P[x,x]←042: until all pixels processedThe final pseudo-distance matrix after applying the PDT2 transformation of Music.bmp is shown in Table 5. It is evident in this table that the uniqueness of entries of each row is preserved under the PDT2 transformation, which is central to successful decoding. We state and prove the following theorem:Theorem 2.1The PDT2 transformation preserves the uniqueness of the entries of each row of the pseudo-distance matrix P.We consider the five neighborsa,b,c,d,eof pixel x, as shown in Table 4. We look up the values of these five neighbors and select the distinct values only so that no row gets updated more than once. We update a row as follows. We consider, for example, the row determined by the value of a and let the value at the entry(a,c)be y. We then increase by 1 the entries of this row that are less then y. The values less then y are0,1,2,…,y-1. After the increment they become1,2,3,…,y. Next, we set the value y at(a,c)to 0. Thus the updated entries of the row that are less than y remain unique. Since we did not modify the entries that are greater than y, and the entries were unique before the update, all entries of the row given by the value of a remain unique. Next, we observe that each swap operation swaps two elements of the same row and thus uniqueness of rows remains intact. Finally, the row determined by the value of x is subjected to the same increment process as prescribed previously which preserves uniqueness. This shows that entries of each row of the pseudo-distance matrix P remain unique under PDT2 transformation. □In predicting x, the rows are updated. Since the same colors may occur again as neighbors, usage of a dynamic pseudo-distance matrix results in more 0s, and hence in smaller compressed files. In Fig. 1, the effects of described PDT2 algorithm can be seen on Frymire.bmp.In decoding, steps similar to the ones used in the encoding algorithm are employed to obtain original image file. The encoded image file includes the color-map table of the original image and error values. By using the color-map table, the Euclidean distance and the pseudo-distance matrices D and P can be easily reconstructed. The recovery of the original image file starts with copying the first pixel from the encoded image. Then, the steps in Algorithm 2 below need to be followed.Algorithm 2Reverse Pseudo Distance Transform (PDT2)1: repeat2:ac←P[a,c],bc←P[b,c],cc←P[c,c],dc←P[d,c],ec←P[e,c]3:fori←1tonumberofcolorsdo4:if(ac>P[a,i])then5:P[a,i]←P[a,i]+16:end if7:if(a!=b)and(bc>P[b,i])then8:P[b,i]←P[b,i]+19:end if10:if(a!=c)and(b!=c)and(cc>P[c,i])then11:P[c,i]←P[c,i]+112:end if13:if(a!=d)and(b!=d)and(c!=d)and(dc>P[d,i])then14:P[d,i]←P[d,i]+115:end if16:if(a!=e)and(b!=e)and(c!=e)and(d!=e)and(ec>P[e,i])then17:P[e,i]←P[e,i]+118:end if19:end for20:P[a,c]←0,P[b,c]←0,P[c,c]←0,P[d,c]←0,P[e,c]←021:fori←1tonumberofcolorsdo22:if(input==P[a,i])then23:x←i24:end if25:end for26:swap(P[a,x],P[a,c])27:if(a!=b)then28:swap(P[b,x],P[b,c])29:end if30:if(a!=c)and(b!=c)then31:swap(P[c,x],P[c,c])32:end if33:if(a!=d)and(b!=d)and(c!=d)then34:swap(P[d,x],P[d,c])35:end if36:if(a!=e)and(b!=e)and(c!=e)and(d!=e)then37:swap(P[e,x],P[e,c])38:end if39:xx←P[x,x]40:fori←1tonumberofcolorsdo41:if(xx>P[x,i])then42:P[x,i]←P[x,i]+143:end if44:end for45:P[x,x]←046: until all pixels processedAfter constructing a pseudo-distance matrix P, the copies of matrix P are assigned to available threads. The use of multiple copies of matrix P instead of a single matrix is required to avoid thread synchronization issue since the PDT2 algorithm updates the matrix P after each pixel processed. The row apportionments to available threads are equal number of rows, a possible exception of last thread, as shown in Table 6. After the pseudo-distance transform, each thread simultaneously starts the data compression process and produces an output file as shown in Fig. 2. Similar to the parallel encoding phase, decoding transform can be completed using only single thread or multiple threads.In this section, we briefly explain the utilized data compression tools that are used after applying the PDT2. These tools include Burrows–Wheeler Transformation (BWT), the run-length encoder (RLE), and the binary arithmetic coder (BAC).Given a data string, for example,ω=[γ,α,γ,α,β], a matrix M is constructed by simply cyclically shifting each row after the first row of ω:M=γαγαβαγαβγγαβγααβγαγβγαγα.After constructing M, by sorting the rows of M in lexical order, we transform M toM′asM′=αβγαγαγαβγβγαγαγαβγαγαγαβ.Burrows and Wheeler in [20] showed that by using the last column ofM′and the length of ω, the original data string ω can be recovered. This transformation is called Burrows–Wheeler Transformation (BWT) and its utility resides in the fact that “the transformation tends to group characters together so that the probability of finding a character close to another instance of the same character is increased substantially. Text of this kind can easily be compressed with fast locally-adaptive algorithms,” such as a run-length encoder.Run-length encoder (RLE) takes the output of the pseudo-distance transformation as input data. It detects long run sequences and regroups the repetitive sequences with the information of the current symbol and the number of repetitions. Use of RLE provides two main advantages. By reducing the number of zeros on input data, RLE increases the efficiency of entropy coder. A context-model arithmetic coder calculates non-zero symbol probabilities more precisely and uses less data to represent the input. This is one of the reasons why we usually get better compression gain when we utilize a RLE before applying an arithmetic coder. RLE also helps to decrease the length of the data sequences since number of zeros is represented in more compact form. This feature of RLE increases the efficiency and decreases computation time of the entropy coder. In [21], a run-length encoding algorithm is used on 0 and 1, while in [22] only 0 run-length transformed encoder is utilized. In this study, we used a RLE similar to one in [22].Binary arithmetic coders work with a limited source alphabet. This limitation overcomes some drawbacks of a structured arithmetic coder because it helps to solve complexity problems on the data. In binary arithmetic coder, only 0s and 1s are used which makes the cumulative distribution task in encoding and decoding simpler. Most data can be represented in binary system and a binary arithmetic coder can be used to encode them.In this study, we used a context-adaptive binary arithmetic coder similar to one in [22]. It has wide range of probability modes for different contexts. Basically, in the first stage, the encoder takes the input and converts the non-binary data to binary data. After that, the encoder determines which probability model is suitable by using information acquired from neighboring elements. Later, data is encoded. Context-modeling helps the probability estimation and is beneficial in compression.

@&#CONCLUSIONS@&#
