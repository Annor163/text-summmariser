@&#MAIN-TITLE@&#
Variational inference for medical image segmentation

@&#HIGHLIGHTS@&#
We present a generalisation of the brain segmentation algorithm implemented in the SPM software, which exploits variational Bayesian inferenceWe test the accuracy and robustness of our method in segmenting brain tissues using synthetic and real MRI dataWe introduce an empirical Bayes framework to learn tissue specific intensity priors from large data sets

@&#KEYPHRASES@&#
Image segmentation,Bayesian inference,Variational Bayes,Neuroimaging,MRI,

@&#ABSTRACT@&#
Variational inference techniques are powerful methods for learning probabilistic models and provide significant advantages over maximum likelihood (ML) or maximum a posteriori (MAP) approaches. Nevertheless they have not yet been fully exploited for image processing applications. In this paper we present a variational Bayes (VB) approach for image segmentation. We aim to show that VB provides a framework for generalising existing segmentation algorithms that rely on an expectation–maximisation formulation, while increasing their robustness and computational stability. We also show how optimal model complexity can be automatically determined in a variational setting, as opposed to ML frameworks which are intrinsically prone to overfitting. Finally, we demonstrate how suitable intensity priors, that can be used in combination with the presented algorithm, can be learned from large imaging data sets by adopting an empirical Bayes approach.

@&#INTRODUCTION@&#
When analysing neuroimaging data, it is often necessary or helpful to partition brain tissues into different types. This represents indeed the primary stage for performing brain volumetry, which is extremely valuable both in research and for clinical practice (Ashburner and Friston, 2000; Giorgio and De Stefano, 2013). In fact, quantifying brain structure volume not only has a major role for unraveling the mechanisms underlying neurodegenerative and psychiatric disorders, but can also significantly help in disease diagnosis and treatment planning or monitoring (Mazzara et al., 2004).For healthy subjects the tissues of interest are typically gray matter, white matter and cerebrospinal fluid, while for patients, additional classes may be defined, such as tumour, edema or necrosis (Moon et al., 2002; Prastawa et al., 2004). In this framework, magnetic resonance imaging (MRI) is usually the most convenient imaging modality to work with, as it provides simultaneously high spatial resolution, excellent soft tissue contrast and good signal to noise ratio.Many widely used image segmentation algorithms rely on probabilistic modelling techniques to fit the intensity distributions of images. These methods commonly operate by means of unsupervised clustering algorithms and assume that the data are drawn from mixture distributions, with different mixture components being associated to different tissue types. In particular, Gaussian mixture models (GMM) have been extensively adopted as they provide a flexible and computationally efficient framework that can be easily applied to solve the problem of automatically partitioning images into homogeneous regions (Dugas-Phocion et al., 2004; Greenspan et al., 2006; Guillemaud and Brady, 1997; Moon et al., 2002; Noe and Gee, 2001; Van Leemput et al., 1999; Wells III et al., 1996; Woolrich et al., 2009; Zhang et al., 2001).Intensity based segmentation tools of this sort have been developed profusely over the past twenty years. Most of them either rely directly on an explicit Bayesian formulation, or exhibit an implicit probabilistic interpretation. Nevertheless almost all of them are based on maximum likelihood (ML) or maximum a posteriori (MAP) estimation of the model parameters (Ashburner and Friston, 2005; Greenspan et al., 2006; Kovacevic et al., 2002; Liang et al., 1994; Lorenzo-Valdés et al., 2004; Rajapakse and Kruggel, 1998; Van Leemput et al., 2003; Wyatt and Noble, 2003; Xiaohua et al., 2004; Zhang et al., 2001), without exploiting the potential of full Bayesian inference.Indeed, the choice of ML or MAP techniques ensures mathematical tractability and sufficient segmentation accuracy for many applications. Nonetheless there is still a crucial theoretical point that makes these methods somehow suboptimal, regardless of their mathematical and computational convenience: the fact that they just provide point estimates of the model parameters instead of full posterior probability distributions. In other words, information is missing on the posterior uncertainty in estimating unobserved variables, and this often results in the occurrence of overfitting as well as in the difficulty to perform model comparison (Attias, 1999).On the other hand, full Bayesian inference has been poorly explored in the field of medical image segmentation, in spite of a promising potential, which was shown for example by Woolrich and Behrens (2006) and Tian et al. (2011). The reason for this is most probably related to the computational challenges that arise when trying to evaluate the model evidence or the posterior probability distributions over the model parameters. In fact, very often and also for relatively simple models, integrating out all the unobserved variables turns out to be intractable in analytical form. On the other hand, numerical integration is generally impractical because either the dimensionality, or the complexity, of the problem would make the computational resources necessary to integrate over all possible parameter configurations too large for real world applications.One approach for dealing with the mathematical difficulties that arise in Bayesian inference is to make use of stochastic techniques to sample from the probability distributions that are of interest (Andrieu et al., 2003). In particular, Markov Chain Monte Carlo methods can provide rather accurate solutions at the expenses of a long processing time. As to be expected, the time required to reach convergence increases with the size of the data set. The result of this being the fact that, for large-scale problems, sampling techniques can become computationally prohibitive. The work of Iglesias et al. (2012) is among the very few attempts to exploit MCMC sampling methods to integrate out model parameters in the context of Bayesian medical image segmentation. Their atlas based segmentation method takes into account the uncertainty in the estimates of the deformations that bring the individual images in alignment with the reference anatomical space. However, they report a running time of the sampling of approximately three hours, which might still be non-viable for some applications, even if this additional computational time ensures higher segmentation accuracy. Related work, on solving image segmentations problems making use of stochastic techniques, has been done also by Fan et al. (2007), Kato (2008) and da Silva (2009).A second family of approaches is based on introducing analytical approximations. For instance, one possibility is to approximate an unknown posterior probability distribution by a Gaussian, centred at the mode of the posterior, or at one of the modes, if the distribution is multimodal. Such a method, known as the Laplace approximation, overcomes many of the limitations of sampling techniques, since the number of required computations is much lower in this case. Nevertheless, depending on how different the actual posterior distribution is from a Gaussian, the method might provide a poor approximation. In particular the underlying Gaussian assumption might become inadequate for samples that are far from the mode of the density.Variational Bayes (VB) represents an alternative way of obtaining approximate solutions to inference problems. It often relies on analytical approximations, as the Laplace method, and likewise it is much less computationally expensive than MCMC. However, the VB framework is more general and flexible than the Laplacian approach. In fact, even if for computational reasons it is often necessary to constrain the posterior distributions to have a specific form or factorisation, they are not necessarily forced to be Gaussian. In other words, variational Bayesian inference permits finding a trade off between allowing sufficient complexity and accuracy of the estimated posteriors and ensuring computational tractability. Stochastic variational algorithms have also been proposed (Hoffman et al., 2013).Even if the estimated posteriors will almost never be exact, variational methods have proved to be more convenient than standard ML or MAP techniques, since, at a substantially similar computational cost, they significantly alleviate the problems related to overfitting, which are intrinsic to the other methods. In other words, variational techniques open up the possibility of learning the optimal model structure (the one with highest generalisation capability) without performing ad-hoc cross validation analyses (Attias, 1999; Bishop et al., 2006; Corduneanu and Bishop, 2001). Another interesting aspect of working within a VB framework is that it leads to a more general formulation of the EM algorithm, which has the same convergence properties and higher computational stability. In fact, one significant limitation of the ML formulation (for mixture models) is the presence of singular points of the likelihood function, which have to be avoided during the optimisation process to assure numerical stability.So far, very few authors have explored the applicability of the variational Bayes framework to perform medical image segmentation. In particular, Woolrich and Behrens (2006) exploited variational inference to fit spatial mixture models to medical imaging data while automatically tuning the parameter controlling spatial regularisation. Tian et al. (2011) proposed a variational algorithm for segmenting brain MRI data, which combines variational Bayes techniques with a genetic algorithm to initialise the priors on tissue intensities.In this paper, we present an extension of the tissue classification algorithm presented by Ashburner and Friston (2005) and publicly distributed as part of the SPM12 software. Specifically, we replace the maximum likelihood approach adopted in Ashburner and Friston (2005) to estimate the Gaussian mixture parameters that model the distribution of image intensities, with a fully Bayesian inference scheme relying on variational approximations.This greatly increases the robustness of the method if suitable intensity priors are introduced, thus reducing significantly the chance of the algorithm failing due to the mismatch or misregistration of the tissue probability maps with the individual scans. Additionally we demonstrate that, in principle, having tissue specific intensity priors yields fairly accurate segmentations also in a completely atlas- (and registration-) free setting.Secondly, we illustrate how the fundamental problem of determining the optimal model complexity, i.e. the number of Gaussian components that are necessary to model the distributions of the different tissues, can be effectively addressed in a variational setting. Such a framework, in fact, implicitly implements an automatic relevance determination scheme, where redundant mixture components are automatically pruned out of the model.Finally we present a parametric empirical Bayes approach to learn informative intensity priors from sufficiently large data sets and demonstrate how the priors estimated in this fashion can increase the robustness of the presented segmentation algorithm. We also address the common problem of different MRI images having different intensity values by incorporating a free global rescaling parameter that is optimised, within the same Bayesian framework, so as to increase the consistency of intensities across scans.In this section we summarise the underpinnings of variational Bayesian inference. We highlight the advantages of VB over point estimation techniques and illustrate some of the challenges that arise in the variational framework.Variational Bayesian inference can be formulated as a maximisation (or minimisation) problem.Let us consider the marginal log likelihood log p(X) given by(1)logp(X)=log∫p(X,Υ)dΥ,where X indicates the observed data and ϒ the set of unobserved variables (model parameters and latent variables).If we introduce a distribution q(ϒ) over the unobserved variables, the log evidence in (1) can be re-expressed as(2)logp(X)=∫q(Υ)logp(X)dΥ=∫q(Υ)log{p(X,Υ)q(Υ)}dΥ+∫q(Υ)log{q(Υ)p(Υ|X)}dΥ,which is a decomposition of log p(X) that holds for any q(ϒ).The second integral in the last line of (2) is the Kullback–Leibler divergence DKL(q‖p) between q(ϒ), which is a variational approximating posterior, and p(ϒ|X), which is the true posterior distribution (Bishop et al., 2006).Since DKL(q‖p) ≥ 0, the first integral in the last line of (2) defines a lower boundL(q)on the logarithm of the model evidence(3)logp(X)≥L(q)=∫q(Υ)log{p(X,Υ)q(Υ)}dΥ.The previous statement can also be derived from (1) by applying Jensen’s inequality.In summary Eq. (2) can be rewritten as (Tzikas et al., 2008)(4)logp(X)=L(q)+DKL(q∥p).As anticipated, DKL(q‖p) is always non negative and, in particular, it is equal to zero if and only ifq(Υ)=p(Υ|X). In such a case the variational posterior is an exact solution and the lower bound is exactly equal to the evidence.In all the other cases, DKL(q‖p) > 0 andL(q)<logp(X),which means that q(ϒ) is an approximate posterior.In summary, the inference problem can be solved by maximizing the functionalL(q)with respect to the distribution q(ϒ), which is equivalent to minimizing the Kullback–Leibler divergence between the variational and the true posterior distribution.The lower bound on the model evidence (negative variational free energy) can be further decomposed as(5)L(q)=∫q(Υ)logp(X|Υ)dΥ+∫q(Υ)log{p(Υ)q(Υ)}dΥ.This shows that the lower bound comprises a likelihood term which is equal to the expected value of the log likelihood log p(X|Υ) under the variational posterior q(ϒ)(6)L1=∫q(Υ)logp(X|Υ)dΥ=EΥ[logp(X|Υ)],and a regularizing term which is the negative Kullback–Leibler divergence between the approximating posterior q(ϒ) and the prior distribution over the unobserved variables p(ϒ) (Attias, 1999)(7)L2=∫q(Υ)log{p(Υ)q(Υ)}dΥ=−DKL(q∥p0).This last term penalises overly complex or implausible models (Occam factor) (Attias, 1999).While in principle arbitrary variational posterior distributions q(ϒ) can be used, a commonly adopted strategy to solve the inference problem consists in restricting the space of q(ϒ) so as to ensure mathematical tractability, which also means that DKL(q‖p) > 0, or, in other words, that q(ϒ) ≠ p(ϒ|X). In particular, it is often convenient to assume that q(ϒ) factorises into a product of terms, each one involving just a subset of ϒ (mean field theory):q(Υ)=∏s=1Sqs(Υs).In such a case, the lower bound depends on the generic factorqs^(Υs^)as follows (Bishop et al., 2006)(8)L(qs^)=−DKL(qs^∥p^(X,Υs^))+const,with(9)p^(X,Υs^)∝exp(Es≠s^[logp(X,Υ)]).Eq. (8) shows that the optimal form of the factorqs^(Υs^)corresponds to the one that minimises the Kullback–Leibler divergence betweenqs^(Υs^)andp^(X,Υs^)which is defined in (9). Thereforeqs^(Υs^)≡p^(X,Υs^).Note that this solution is not analytical, since the different factors have optimal forms that depend on one another. As a result, the natural approach for solving this variational optimisation problem consist in iteratively updating each factor given the most recent forms of the other ones. This leads to a scheme that turns out to be very similar to the structure of the EM algorithm (Bishop et al., 2006; Tzikas et al., 2008).For some complex models, a fully Bayesian treatment of all unobserved variables might still be extremely impractical, if not impossible, even when variational techniques are used. One other advantage from adopting a VB approach is that its generality allows it to be combined with standard MAP and ML techniques in a unified and principled framework. If one of the subsets{Υs}s=1,…,Sof the unobserved variables cannot be treated in a fully Bayesian manner, it is still possible to obtain MAP point estimates for the corresponding parameters. Such values are obtained in a way that is a generalisation of the M-step in the EM algorithm. In particular, the function that needs to be optimised is the expectation of the logarithm of the joint probability of X and ϒ,E[logp(X,Υ)]. The main difference from the EM algorithm for ML (or MAP) estimation is that in the VBEM case, expectations are computed not only over the latent variables of the model (as in the EM), but also over all the model parameters that are described in terms of a full posterior distribution.This section describes the mathematical model adopted for this work. We introduce all variables, illustrate their conditional dependencies and, finally, we derive a variational objective function (lower bound).Let X denote the observed data, that is to say the intensities corresponding to D images of the same subject acquired with different modalities. The signal at voxel j can then be represented by a D-dimensional vectorxj∈RD,withj∈{1,…,N}.We can model the distribution of xjas a multivariate Gaussian mixture consisting of K clusters parameterised by mean vectors{μk}k=1,…,Kand covariance matrices{Σk}k=1,…,K. The mixing proportions of the different components are given byΘπ={πjk}with πjk∈ [0, 1] and∑kπjk=1. Essentially πjkindicates the prior probability of signal at spatial location j being drawn from cluster k.Moreover we can assume that the K Gaussians are partitioned into T subsets, corresponding to different tissue types. Let{Ct}t=1,…,Tdenote these subsets, with⋃tTCt={1,…,K}. This means that each tissuet∈{1,…,T}is itself represented by a GMM consisting of Ktcomponents with∑tKt=K.The prior probability of voxel j belonging to tissue t is considered to be given a priori (through a probabilistic atlas) and is indicated by τjt. Furthermore, these tissue priors are allowed to be rescaled by a set of weights{wt}t=1,…,Tto accommodate individual differences in tissue composition. Finally it is necessary to introduce a set of parameters{gk}k=1,⋯,Kdenoting the normalised weights of the different Gaussians associated with one tissue type, so that(10)∀t∈{1,…,T}:∑k∈Ctgk=1.As a result the mixing proportions Θπof the presented GMM can be expressed as(11)πjk=gkτjtwt∑t′Tτjt′wt′,where {τjk} are known parameters, while{wt}t=1,…,Tand{gk}k=1,⋯,Khave to be estimated from the observed data X.To correct for intensity non-uniformity artifacts, a multiplicative D-dimensional bias field, denoted by{bj(Θβ)}j=1,…,N,is introduced in the model, where Θβis a vector of parameters. Each of the D components of the bias is modelled as the exponential of a linear combination of discrete cosine transform basis functions (Ashburner and Friston, 2005).Finally, to account for the variability of anatomical shapes among subjects, the probabilistic atlas given by{τt}t=1,…,Tis allowed to be deformed according to a displacement field parameterised by the set of vectorsΘα={αj}j=1,…,N. The warped tissue priors can therefore be expressed as{τt(φ(Θα))}t=1,…,T,where φ(Θα) is a coordinate mapping from the individual image space into the atlas space. The parameterisation adopted here consists in adding to the identity transform a small displacement field{αj}j=1,…,N,so that(12)y˜j=yj+αj,where the vector yjencodes the coordinates of the centre of voxel j.If we introduce a set of binary latent variables Z denoting the class memberships of the observed data X, the probability of Z given the mixing proportions Θπand the deformation parameters Θαis given by(13)p(Z|Θπ,Θα)=∏j=1N∏k=1K(τjt(φ(Θα))wt∑t′Tτjt′(φ(Θα))wt′)zjk,where we have assumed that all data are independent.The conditional distribution (class conditional density) of the observed intensities given the latent variables, the Gaussian means Θμand covariances ΘΣand the bias field parameters Θβcan be expressed as(14)p(X|Z,Θμ,ΘΣ,Θβ)=∏j=1N∏k=1K(|Bj|N(Bjxj|μk,Σk))zjk,withBj=diag(bj),modelling the bias field.The joint probability of all the random variables conditioned on the mixing proportions is given, for the presented model, by(15)p(X,Z,Θμ,ΘΣ,Θβ,Θα|Θπ)=p(X|Z,Θμ,ΘΣ,Θβ)p(Z|Θπ,Θα)p(Θμ,ΘΣ)p(Θα)p(Θβ).The voxel specific mixing proportions Θπare treated here as deterministic parameters depending on the available anatomical atlas, on the tissue weights{wt}t=1,…,Tand on the within-tissue mixing proportions{gk}k=1,…,K,therefore they are determined via ML estimation.It should be noted that we have kept the formal distinction between latent variables Z and model parametersΘfor clarity, even if the treatment of these is essentially the same with variational inference techniques.The priors on the means and covariances of the different classes are modelled as Gaussian–Wishart distributions(16)p(Θμ,ΘΣ)=∏k=1Kp(μk|Σk−1)p(Σk−1),with(17)p(μk|Σk−1)=N(μk|m0k,b0k−1Σk),(18)p(Σk−1)=W(Σk−1|W0k,ν0k),whereW(W,ν)indicates the probability density function of a Wishart distribution with ν degrees of freedom and scale matrixW(see Appendix A for a more detailed description of Gaussian–Wishart priors).Such a choice is algebraically convenient, as it leads to posterior distributions having the same functional form of the priors (conjugate priors).The parameters governing the priors will be indicated as(19)Φ0={β0k,m0k,ν0k,W0k}k=1,…,K.The terms p(Θα) and p(Θβ) represent prior probability distributions over the deformation and bias field parameters. Their function is to regularise the solution obtained through model fitting by penalizing improbable parameters values. In doing so, they assure greater physical plausibility of the resulting non-uniformity and deformation fields, while also improving numerical stability within the optimisation process. Here the same regularisation scheme described in Ashburner and Friston (2005) is adopted. The question of how to determine optimal forms for the regularisation terms is beyond the scope of this work and therefore is not addressed here. Interestingly, such a problem could also be solved in a variational inference framework, as shown in Simpson et al. (2012).A lower bound on the marginal likelihood p(X, Θβ, Θα|Θπ) is given by(20)L=∑Z∫∫q(Z,Θμ,ΘΣ)×log{p(X,Z,Θμ,ΘΣ,Θβ,Θα|Θπ)q(Z,Θμ,ΘΣ)}dΘμdΘΣ.To make the problem tractable we assume that the variational distribution q(Z, Θμ, ΘΣ) factorises asq(Z,Θμ,ΘΣ)=q(Z)q(Θμ,ΘΣ),so that(21)L=∑Z∫∫q(Z)q(Θμ,ΘΣ)logp(X|Z,Θμ,ΘΣ,Θβ)dΘμdΘΣ+∑Z∫∫q(Z)q(Θμ,ΘΣ)×log{p(Z|Θπ,Θα)p(Θμ,ΘΣ)q(Z)q(Θμ,ΘΣ)}dΘμdΘΣ+p(Θβ)+p(Θα).The described probabilistic model can be represented by a directed acyclic graph, as shown in Fig. 1. It should be noted that such a model is generative. In fact Eq. (15) allows to generate synthetic observations X via sampling from the joint distribution of all random variables conditioned on the mixing proportions (Bishop et al., 2007).Once the model has been learned, it is directly possible to infer the tissue labels by examining the posterior distribution p(Z|X). In fact, the decision on which class each voxel belongs to should be taken according to Bayes decision rule (BDR), which states that the class to be chosen is the one that minimises the probability (”risk”) of error. In practice,(22)xj∈classk⇔p(zjk=1|xj)>p(zjk′=1|xj)∀k′≠k.The statistical model descried in Section 3 can be fit to data adopting a variational version of the standard EM algorithm for MLE. The objective of this optimisation procedure is to learn optimal solutions for the variational posterior distribution q(Z)q(Θμ, ΘΣ), to estimate MAP values for the parameters {Θα, Θβ} and ML values for{gk}k=1,…,Kand{wt}t=1,…,T.In the variational generalisation of the EM algorithm (VBEM) we can still distinguish two steps: VE-step and VM-step. In the variational E-step the functionalLin (20) is maximised with respect to the posterior factor q(Z) over the latent variables (Bishop et al., 2006). Making use of (9) we find that(23)q(Z)∝exp(logp(Z|Θπ,Θα)+Eμ,Σ[logp(X|Z,Θμ,ΘΣ,Θβ)]).If we define(24)logρjk=logp(Z|Θπ,Θα)+Eμ,Σ[logp(X|Z,Θμ,ΘΣ,Θβ)],it follows that(25)q(Z)∝∏j=1N∏k=1K(ρjk)zjk.By normalizing the variational distribution q(Z) we obtain(26)q(Z)=∏j=1N∏k=1K(ρjk∑c=1Kρjc)zjk=∏j=1N∏k=1K(γjk)zjk.The quantity log ρjkcan be computed from (24) to give(27)logρjk=logπjk(φ(Θα))−D2log(2π)+12EΣk[log|(Σk)−1|]−12Eμk,Σk[(Bjxj−μk)TΣk−1(Bjxj−μk)].The expectations that appear in (27) have to be computed with respect to the current estimates of the variational posterior distributions over{μk}k=1,…,Kand{Σk}k=1,…,K(Appendix A).The terms {γjk} which are computed during the VE-step are equal to the expectations of the latent variables with respect to their posterior variational distribution (responsibilities) (Bishop et al., 2006). They can be used to compute the following sufficient statistics of the observed data, which will serve to update the posterior distributions of{μk}k=1,…,Kand{Σk}k=1,…,K,during the VM-step(28)s0k=∑j=1Nγjk,s1k=∑j=1NγjkBjxj,S2k=∑j=1Nγjk(Bjxj)(Bjxj)T.It should be noted that the computational complexity of this VE-step is identical to that of the E-step in the standard EM algorithm.In the following VM-step we can derive approximate solutions for the posterior distributions over the cluster means and covariance matrices (Bishop et al., 2006). Making again use of (9) we obtain(29)q(Θμ,ΘΣ)∝exp{∑j=1N∑k=1KγjklogN(Bjxj|μk,Σk)+∑k=1Klogp(Θμ,ΘΣ)}.It can be proved (Appendix B) that the posterior distributions on the means and covariances of the different Gaussians take the same form as the corresponding priors (Bishop et al., 2006), that is(30)q(Θμ,ΘΣ)=∏k=1Kq(μk|Σk−1)q(Σik−1),with(31)q(μk|Σk−1)=N(μk|mk,bk−1Σk),(32)q(Σk−1)=W(Σk−1|Wk,νk).The parameters that govern these posterior distributions(33)Φ={βk,mk,νk,Wk}k=1,…,K,can be computed as a function of the prior hyperparameters and the sufficient statistics obtained in the previous VE-step, as follows (Appendix B)(34)bk=b0k+s0k,mk=b0km0k+s1kb0k+s0k,Wk−1=W0k−1+S2k+b0ks0km0km0kTb0k+s0k−s1ks1kTb0k+s0k−b0ks1km0kTb0k+s0k−b0km0ks1kTb0k+s0k,νk=ν0k+s0k.The point estimates of the mixing proportions{gk}k=1,…,Kwithin each tissue type can instead be updated by(35)gk=s0k∑c∈Cts0c,while for the tissue weights{wt}t=1,…,Twe obtain the following(36)wt=∑k∈Cts0k∑j=1Nτjt(φ(Θα))∑t′=1Tτjt′(φ(Θα))wt′.A brief discussion on how to solve the bias and deformation optimisation problems is provided in Appendix C and Appendix D.The hyperparameters Φ0 reflect prior beliefs on how signal intensities should be distributed within each tissue type. With a Gaussian–Wishart parameterisation, the following hyperparameter setting ensures minimally informative and non improper priors(37)β0k≃0∧ν0k≃D−1⇒p(μk,Σk)≃const.With such a choice, the posterior distributions of the Gaussian parameters would be essentially determined by fitting the data, in a similar way to the maximum likelihood framework, and the regularisation term of the lower bound would reduce to the entropy of the posterior distributions.On the contrary, choosing more informative priors can potentially increase the robustness of the algorithm by enforcing meaningfulness and plausibility of the estimated posteriors and, at the same time, ensure faster convergence. However, defining pertinent priors is a non trivial problem, as ideally such priors should express information derived from previously acquired data, rather than simple subjective beliefs. Therefore an appropriate hyperparameter configuration should be learned from large population data. Essentially, informative empirical priors should allow transferring the posterior information inferred from a training data set onto new unseen testing data (Lawrence and Platt, 2004; Raina et al., 2006; Seeger, 2002).Interestingly, the model described so far can be further extended to represent a data set comprising scans of different subjects and, therefore, it provides a natural framework for estimating empirical priors. In fact, a lower bound on the marginal likelihood can be expressed, for a population of M subjects, as follows(38)L=∑i=1M∑Z∫∫qi(Zi,Θμ,ΘΣ)×log{pi(Xi,Zi,Θμ,ΘΣ,Θβ,Θα|Θπ)qi(Zi,Θμ,ΘΣ)}dΘμdΘΣSupposing that the posteriors{qi(Θμ,ΘΣ)}i=1,…,Mhave been estimated, Eq. (38) can be maximised with respect to p(Θμ, ΘΣ). Since we are assuming that the functional form of this distribution is parametric and known (Gaussian–Wishart), standard non-linear optimisation techniques can be exploited to find maximum likelihood estimates of the hyperparameters Φ0.Indeed, the lower bound in (38) can be expressed as a function of Φ0(39)L(Φ0)=∑i=1m∫∫qi(Θμ,ΘΣ)logp(Θμ,ΘΣ)dΘμdΘΣ+const=12∑i=1M∑k=1K{E[log|Σik−1|](ν0k−D)−νkTr(W0k−1Wik+β0k(mik−m0k)(mik−m0k)TWk)}+M2∑k=1KDlogβ0k2π−D∑i=1M∑k=1Kβ0kβik+2M∑k=1KlogBW(W0k,ν0k)+const,where BWindicates the normalizing constant of the Wishart distribution.We also derived the first and second derivatives ofL(Φ0),which are useful to solve this optimisation problem using gradient based techniques. Such derivatives are reported in Appendix F.In summary, a convenient strategy for learning Gaussian mixture priors consists in, first, initializing the hyperparameters so as to obtain weak priors, secondly, estimating the posterior distributions for a population of M subjects, finally, optimizingLwith respect to Φ0. The estimates of the hyperparameters (Φ0) can then be further refined by using these empirical priors to reestimate the posteriors and so on, thus leading to an iterative learning scheme.

@&#CONCLUSIONS@&#
