@&#MAIN-TITLE@&#
Non-parametric efficiency estimation using Richardson–Lucy blind deconvolution

@&#HIGHLIGHTS@&#
We propose a non-parametric three-stage strategy for efficiency estimation.The proposed algorithm (RLb) identifies firm-specific inefficiencies.RLb needs no distribution assumption, tolerates data noise and heteroscedasticity.RLb efficiently distinguishes nuance among Finnish energy suppliers.

@&#KEYPHRASES@&#
Stochastic frontier estimation,Richardson–Lucy blind deconvolution,Efficiency estimation,Nonparametric,

@&#ABSTRACT@&#
We propose a non-parametric, three-stage strategy for efficiency estimation in which the Richardson–Lucy blind deconvolution algorithm is used to identify firm-specific inefficiencies from the residuals corrected for the expected inefficiency μ. The performance of the proposed algorithm is evaluated against the method of moments under 16 scenarios assumingμ=0. The results show that the Richardson–Lucy blind deconvolution method does not generate null or zero values due to wrong skewness or low kurtosis of inefficiency distribution, that it is insensitive to the distributional assumptions, and that it is robust to data noise levels and heteroscedasticity. We apply the Richardson–Lucy blind deconvolution method to Finnish electricity distribution network data sets, and we provide estimates for efficiencies that are otherwise inestimable when using the method of moments and correct ranks of firms with similar efficiency scores.

@&#INTRODUCTION@&#
Productive efficiency analysis, which is a quantitative approach for evaluating the performance of a firm, can, e.g., offer insights into its performance and help managers make correct decisions. Generally, a productive efficiency analysis can be viewed as a two-step process: first, the production or cost frontiers are estimated by using parametric or non-parametric methods, and then the inefficiencies from the residuals estimated in the first step are predicted. In neoclassical theory based approaches such as Data Envelopment Analysis (DEA), the residuals are considered to be the inefficiencies (Charnes, Cooper, & Rhodes 1978; Farrell, 1957). In frontier production models such as Stochastic Frontier Analysis (SFA) (Aigner, Lovell, & Schmidt 1977; Meeusen and Van den Broeck (1977)) and the stochastic semi-parametric model Stochastic Non-smooth Envelopment of Data (StoNED) (Kuosmanen & Kortelainen, 2012), the residuals are assumed to be a composite of both the inefficiencies and the random noise. In the single output multiple input setting, StoNED contains the traditional DEA and SFA as its special cases. In stochastic frontier models, two-stage strategies are conventionally used for efficiency estimation, wherein the conditional mean E(y|x) or the frontier is estimated in the first stage and the disturbance term (difference between the estimated and observed y) is decomposed into the inefficiency and the random noise in the second stage. In the first stage, the frontier can be estimated by using parametric or nonparametric regression techniques. Parametric models postulate a specific functional form for f and the parameters are estimated using techniques such as Modified Ordinary Least Squares (MOLS) (Aigner et al. 1977) and the maximum likelihood (ML) approach, with the latter being more frequently used. Non-parametric models do not assume a particular functional form but they do need to satisfy certain regularity axioms, with the frontier being determined using, e.g., Convex Nonparametric Least Squares (CNLS) (Kuosmanen & Kortelainen 2012). Keshvari and Kuosmanen (2013) relaxed the concavity assumption of CNLS (Keshvari & Kuosmanen, 2013). Given that semiparametric approaches such as StoNED bridge the gap between DEA and SFA, there is a growing interest in this method. StoNED is a well-established method that is superior to other existing methods given its stochastic and semi-parametric nature (Kuosmanen & Kortelainen, 2012). By adopting the StoNED framework and relaxing its parametric assumptions, a fully nonparametric approach for efficiency identification that integrates the standard DEA and SFA models can be developed. In StoNED, techniques such as method of moments (MM) are conventionally used for identifying efficiencies from the residuals coming from the first step. Wang et al. (2014) developed a quantile-version of CNLS and StoNED (Wang, Wang, Dang, & Ge, 2004). However, these methods heavily depend on the accuracy of the distributional assumption of the error components and thus suffer from many problems such as wrong skewness (Kuosmanen & Fosgerau, 2009). In addition to the two-stage strategies, other approaches have been applied in stochastic frontier models to account for the impact of environmental factors. The frontier inefficiency residuals were modeled as a function of various causal factors and a random component to study the systematic effect of the conditions that contribute to inefficiencies (Reifschneider & Stevenson, 1991). A generalized production frontier approach was reported by (Kumbhakar, Ghosh, & McGuckin, 1991) to estimate the determinants of inefficiencies. Huang and Liu proposed a hybrid of a stochastic frontier regression: the model combines a stochastic frontier regression and a truncated regression to estimate the production frontier with non-neutral shifting of the average production function (Huang & Liu, 1994). Conditional efficiency measures, such as conditional FDH, conditional DEA, conditional order-m and conditional order-α, have rapidly developed into a useful tool to explore the impact of exogenous factors on the performance of DMUs in a nonparametric framework (Daraio & Simar, 2005, 2007a, 2007b). A more recent paper examined the impact of environmental factors on the production process in a new two-stage type approach by using conditional measures to avoid the drawbacks of the traditional two-stage analysis, which provides a measure of inefficiency whitened from the main effect of the environmental factors (Badin, Daraio, & Simar, 2012).Deconvolution has previously been shown to be a useful statistical technique for unknown density recovery (Meister 2006) which, in most cases, requires specifying the measurement error distribution (Stefanski & Carroll, 1990). For example, Kneip et al. (2012) applied deconvolution to estimate the boundary of a production set foe which the measurement error has an unknown variance; however, a lognormal distribution of the noise term is crucial to ensure the identifiability in context (Kneip, Simar, & Van Keilegom, 2012). Additionally, Schwarz et al. (2010) defined an estimator of the frontier function where partial information on the error distribution was assumed, i.e., zero-mean Gaussian random variable with an unknown variance (Schwarz, Van Bellegem, & Florens, 2010). Meister (2006) relaxed this assumption and consistently estimated both the target density and the unknown variance of the normal error, assuming that the target density was from the ordinary smooth family of distributions (Meister, 2006). Although fewer assumptions were needed for the error term in Meister’s estimator, the target distribution was restricted to distributions such as Laplace, exponential, and gamma. Other attempts at relaxing constraints were made under a scenario wherein the contaminated errors ε (ɛ=u+v,u and v of each stand for the inefficiency and random noise, respectively, were not directly observable but represented an additive term of a regression model such asy=α+βx+ɛ(α and β are the coefficients; x and y are the inputs and output). Horowitz and Markatou (1996) developed an estimator to handle cases that do not require specifying the component distributions of ε (Horowitz & Markatou, 1996). However, this method relies on the information along the time-axis of the panel data to identify densities in the composite error term, which cannot be applied to cross-sectional data, whose the error density is rarely entirely known. More importantly, Horrace and Parmeter (2011) proposed a cross-section complement of Horowitz and Markatou’s method, which proved to be semi-uniformly consistent in identifying target density if u is ordinary smooth (Horrace & Parmeter, 2011). As a regression generalization of (Meister, 2006), the constraints posed in Meister’s estimator are inherited in this method. For example, it is semi-parametric because it relies on a distributional law for v and because the density of u belongs to the ordinary smooth family. Further, as the methods of (Horowitz & Markatou, 1996) and (Horrace & Parmeter, 2011) work for data of the regression form, replacing ε with the regression residuals may introduce frontier estimation errors and can thus lead to a biased estimation of the inefficiencies.Unlike the aforementioned efforts for applying deconvolution in frontier estimation, we are interested in inefficiency estimation using deconvolution in a non-parametric stochastic setting. To overcome the difficulty of estimating the expected inefficiency using kernel deconvolution, we return to the field where deconvolution is originated and explore the existing techniques. Deconvolution was originally applied in signal and image processing, where the point spread function (PSF) is used to describe the response of an imaging system to a point source (Haykin, 1993). Projected onto efficiency estimation problems, it is equivalent to the function of converting the inefficiencies to the observed residuals. Blind deconvolution is a technique for recovering the blurred object without any prior knowledge of the PSF (which is often costly or impossible to obtain). There are five categories of blind deconvolution methods: a priori blur identification methods (Cannon, 1976), zero sheet separation methods (Ghiglia, Romero, & Mastin, 1993), autoregressive moving average (ARMA) parameter estimation methods (Biemond, Tekalp, & Lagendijk, 1990), nonparametric methods based on high-order statistics (HOS) (Jacovitti & Neri, 1990; Wu, 1990), and nonparametric iterative methods (Ayers & Dainty, 1988; Kundur & Hatzinakos, 1998; McCallum, 1990). These methods differ in their assumptions about the PSF and the true object. After considering the advantages and limitations of each method, we are left with the nonparametric iterative methods. We restrict our options in this fashion because (1) the a priori methods are parametric; (2) zero sheet separation methods are highly sensitive to noise and prone to inaccuracy for large objects; (3) the ARMA parameter estimation methods may converge poorly and be computationally expensive if the number of parameters is very large; (4) nonparametric methods based on HOS require accurate modeling of the true object by a known non-Gaussian probability distribution and may be trapped in local minima in the estimation process; and (5) the results from algorithms in the first three categories are usually not unique unless additional assumptions are made about the PSF. Nonparametric iterative methods iteratively estimate PSF and the true object without any prior parametric assumptions. However, several constraints are required that, in the context of efficiency analysis, are as follows: (1) the inefficiencies are non-negative, (2) the range of inefficiency is known (e.g., within 0 and 1), and (3) the background noise is random. Typical algorithms that belong to this class are non-negativity and support constraints recursive inverse filtering (NAS–RIF) (Kundur & Hatzinakos, 1998), simulated annealing (SA) (McCallum, 1990), and iterative blind deconvolution (IBD) (Ayers & Dainty, 1988), which differ in their objectives of minimizing the cost functions and how these functions are constructed. Because NAS–RIF has certain requirements for the PSF, such as bounded-input bounded-output (BIBO), and because the choice of iteration parameters (e.g., perturbation scale) in SA is difficult, which affects its performance and convergence rate, we return our focus interest to IBD. IBD minimizes the cost function with respect to both the PSF and the true object simultaneously, and it is the most widely applied algorithm in blind deconvolution. The typical algorithms adopted for IBD in its iterative operations include a Wiener-type filter or the Richardson–Lucy (RL) algorithm. Because the Wiener-type filter assumes stationary noise, we are left with the RL algorithm. Further appealing is the probabilistic nature of the RL algorithm. We thus chose to apply the blind RL deconvolution (Fish, Brinicombe, & Pike, 1995) algorithm (abbreviated as RLb here) for inefficiency estimation.The performance of the RLb was tested against that of MM (which is conventionally used in StoNED) under sixteen simulated scenarios, including those from (Aigner et al., 1977). In the RL deconvolution algorithm, the true object (e.g., inefficiency) was assumed to follow a Poisson distribution. By approximating a Poisson distribution using a Gaussian distribution which was assumed for the inefficiency term, we added a sufficiently large term to the inputs and subtracted it from the deconvoluted results. The results show that the RLb method outweighs MM in at least 4 aspects. It is (1) non-parametric and exempted from any distributional assumption, which leads to (2) the circumvention of many common issues such as the wrong skewness problem; (3) it is insensitive to data noise; and (4) it is robust to data heteroscedasticity. Additionally, we applied the RLb method to an empirical problem, which used the residuals taken from the cost frontier estimation of 89 Finnish electricity distributors. We are among the pioneers in deploying deconvolution in efficiency estimation, and we are the first to identify inefficiencies using a fully non-parametric method.The rest of the text is organized as below. The non-parametric three-stage efficiency estimation procedure wherein RLb is used for firm-specific inefficiency decomposition is described in detail in the ‘Methods’. In the ‘Monte Carlo Simulation’, the data generating processes, performance measures and results are described and summarized. The data and results of the real case application are presented and discussed in the ‘Empirical study’ section. Finally, we summarize the key findings, contributions, limitations and possible future directions in ‘Conclusion’.

@&#CONCLUSIONS@&#
In this study, we deploy a fully non-parametric algorithm, the RL blind deconvolution method, to decompose firm-specific inefficiencies from their composite errors corrected by the expected inefficiencyμ=0in productive efficiency analysis. By comparing the performance of RLb and MM under 13 scenarios assumingμ=0,we show that the RLb method outweighs conventional methods such as MM in four tested aspects. First, it never outputs null or zero values due to incorrect skewness or low kurtosis of the inefficiency density. Second, it is insensitive to the distributional assumption of the inefficiency term u, and it does not require any additional assumptions such as iid (independently and identically distributed) samples. Third, it is robust to data noise levels. Fourth, it gives consistent estimates, regardless of data heteroscedasticity. In addition, we applied RLb to the Finland electricity distribution network data set, wherein the efficiencies inestimable using MM are provided and firms with similar efficiency scores are correctly ranked. We are one of the pioneers in applying deconvolution in inefficiency estimation, and we are the first to report a fully non-parametric method for composite error decomposition, compared with other groups, which use kernel deconvolution techniques.It is worth noting that the RLb algorithm was initially developed to solve image degradation problems in a three-dimensional space. Thus, its utility in panel data warrants further exploration. Additionally, we could extend RLb to solve cases wherein the frontier is non-convex and non-parametrically determined. Despite the advantages of RLb, we should be aware of its sensitivity to frontier estimation error, i.e., the inefficiency estimates are shifted by the difference between the estimated and true frontier. Additionally, the RLb method is not unbiased because of its isoplanatic assumption on u and v. Exploring how to overcome these problems and further improve the estimation accuracy are interesting topics next steps. In addition, applying this more robust tool to solve some empirical problems may offer high practical values and is suggested here for future research.