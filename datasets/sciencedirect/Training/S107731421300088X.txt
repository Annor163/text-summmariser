@&#MAIN-TITLE@&#
Particle filters and occlusion handling for rigid 2D–3D pose tracking

@&#HIGHLIGHTS@&#
A robust algorithm for 2D visual tracking and 3D pose estimation is proposed.We focus on partial occlusions that distort the region properties of an object.Optimal pose of an object is estimated via particle filters in a decoupled manner.The degree of trust between system’s predictions and measurements is controlled.The resulting methodology improves tracking performance in clutter and occlusion.

@&#KEYPHRASES@&#
2D–3D pose estimation,Object tracking,Occlusion handling,Particle filters,

@&#ABSTRACT@&#
In this paper, we address the problem of 2D–3D pose estimation. Specifically, we propose an approach to jointly track a rigid object in a 2D image sequence and to estimate its pose (position and orientation) in 3D space. We revisit a joint 2D segmentation/3D pose estimation technique, and then extend the framework by incorporating a particle filter to robustly track the object in a challenging environment, and by developing an occlusion detection and handling scheme to continuously track the object in the presence of occlusions. In particular, we focus on partial occlusions that prevent the tracker from extracting an exact region properties of the object, which plays a pivotal role for region-based tracking methods in maintaining the track. To this end, a dynamical choice of how to invoke the objective functional is performed online based on the degree of dependencies between predictions and measurements of the system in accordance with the degree of occlusion and the variation of the object’s pose. This scheme provides the robustness to deal with occlusions of an obstacle with different statistical properties from that of the object of interest. Experimental results demonstrate the practical applicability and robustness of the proposed method in several challenging scenarios.

@&#INTRODUCTION@&#
Visual tracking has been a significant topic of research in the field of computer vision; see [1–4] and references therein. The ultimate goal of visual tracking is to continuously identify the 3D location of an object of interest from an image sequence. This amounts to what is known as the 2D–3D pose tracking problem [5,6]. However, due to the difficulty of developing a tractable solution for estimating the 3D position from a 2D scene, many researchers have tacitly restricted the tracking problem to be concerned with only the relative 2D location of the object in which segmentation is often employed in conjunction with Kalman or particle filters [7,8]. Recent techniques attempt to revisit the 2D–3D pose tracking problem for challenging scenarios by leveraging on 2D image segmentation to estimate the 3D location [9–12]. While impressive results have been obtained that rival both pose tracking and segmentation based algorithms, these schemes did not fully exploit the underlying system dynamics that is inherent in any visual tracking task. Thus, in order to effectively treat the temporal nature of the observed 2D scene, we propose to extend a similar framework proposed by [13] in which we now incorporate a particle filter to perform 3D pose estimation of a rigid object. However, before doing so, let us revisit several contributions related to the proposed method.Several algorithms have been introduced to solve the 2D–3D pose tracking task. In general, they are based on local or region attributes for feature matching. For example, such features include points [14], lines [15], polyhedral shape [16], complete contours [17,18], or surfaces [12]. Specifically, in [16], the authors perform a 2D global affine transformation as an initialization to 3D pose estimation, and then the 3D object pose is computed by an energy minimization process with respect to an approximate polyhedral model of the object. The authors in [18] present a 3D pose estimation algorithm by using visible edges. That is, they use binary space partition trees for finding and determining the visible line to track the edges of the model. However, since these methods rely on local features, the resulting solutions may yield unsatisfactory results in the presence of noise or cluttered environments. To overcome this, an early attempt to couple segmentation and pose estimation is given in [10]. In their work, the authors propose a region-based active contour that employs a unique shape prior, which is represented by a generalized cone based on a single reference view of an object. More recently, authors in [13] as well as Schmaltz et al. [12] propose a region-based model scheme for 2D–3D pose tracking by projecting a 3D object onto a 2D image plane such that the optimal 3D pose transformation coincides with the correct 2D segmentation. In a similar fashion, Kohli et al. [9] proposed a joint segmentation and pose estimation scheme using the graph cut methodology. Although these methods perform exceptionally well for many cases, they do not exploit the underlying dynamics inherent in a typical visual tracking task. We should note, in the context of the proposed work, the incorporation of system dynamics can be viewed as an extension to these baseline algorithms.In addition to the aforementioned approaches for 2D–3D pose tracking, many works may be found in the literature which purely focus on restricting the visual tracking problem to the 2D domain. Because a complete overview of existing methods is beyond the scope of this paper, we just consider those methods that employ various filtering schemes such as the Kalman filter [19], unscented Kalman filter [7,20], and particle filter [8,21] as well as explicit algorithms for occlusion handling [22,23]. Specifically, the authors in [24,25] employ a finite dimensional parameterization of curves, namely B-splines, in conjunction with the unscented Kalman filter for rigid object tracking. Generalizing the Kalman filter approach, the work in [26] presents an object tracking algorithm based on particle filtering with quasi-random sampling. Since these approaches only track the finite dimensional group parameters, they cannot handle local deformations of the object. As a result, several tracking schemes have been developed to account for deformation of the object via the level set technique [27,28]. In relation to our work, some early attempts for 2D visual tracking using level set methods can be found in [29,30]. In particular, authors in [30] propose a definition of motion for a deformable object. This is done by decoupling an object’s motion into a finite group motion known as deformotion with that of deformation, which is any departure from rigidity. Building on this, authors in [31] introduce a deformable tracking algorithm that utilizes the particle filtering framework in conjunction with geometric active contours. Other approaches closely related to these frameworks are given in [19,32,33]. Here the authors use a Kalman filter for predicting possible movements of the object, while the active contours are employed only for tracking deformations of the corresponding object.In addition to employing filtering schemes to increase the robustness of tracking, many algorithms invoke a systematic approach to handle occlusions. We should note that although the main contribution of our work focuses on employing particle filtering to estimate the 3D pose during 2D visual tracking, a neat feature of the resulting methodology is its ability to handle occlusions effectively. Thus, we briefly revisit several attempts to specifically handle occlusions in the context of visual tracking [22,34,35]. Such occlusions can occur when another object lies between the target and a camera, or the target occludes parts of itself. In general, in the case of contour tracking, most methods incorporate shape information of an object of interest into a tracking framework online [35] or offline [23] to make up for poor distinguishable statistics between the object and background or missing parts of the object. To this end, a shape prior can be obtained or learnt from linear principal component analysis (PCA) if the assumption of small variations in shape holds [36]. Otherwise, for highly deformable objects, locally linear embedding (LLE) [37] or non-linear PCA [22] may be employed. In the case of template tracking, in which a complete region occupied by the object is not tracked, recently, the L1 tracker [38] is modified to accurately represent occlusions using the trivial templates in [39]. In [39], the energy of the trivial coefficients is adaptively controlled to capture the effect of occlusions.Contribution: The algorithms proposed in this paper are closely related to the works in [13] and [40]. In the work of [13], the authors derive a variational approach to jointly carry out tasks of 2D region based segmentation and 3D pose estimation. This method shows robust performance for segmenting a 2D image and estimating the 3D pose of an object over image sequences even in cluttered environments. However, since this method ignores the temporal nature of the observed images, it cannot handle erratic movements or challenging occlusions. That is, the variational technique relies only on image information to drive the corresponding 3D pose estimate, which may cause unsatisfactory results in the presence of occlusions that are statistically different from that of the object of interest; see the experiments in Section 4.2. In the work of [40], the set of 3D transformation matrices is randomly constructed to find the optimal pose of an object of interest by Monte Carlo sampling on the special Euclidean group, and then a region based statistical energy is applied to evaluate the optimality of 2D projection of each transformed 3D model. This approach shows promising results in estimating 2D–3D pose of the object of interest under a cluttered environment. However, it easily fails to maintain track in the case of its dynamic movement under occlusions; see the experiments in Section 4.2. In addition this framework suffers from high computational complexity due to the nature of a sampling based method.In this paper, not only to utilize both frameworks above but also to overcome their disadvantages, we extend them by incorporating a particle filter to exploit the underlying dynamics of the system, and by developing an occlusion handling scheme to continuously track the object of interest in a more general and challenging environment. Compared with the approaches described in [13,40], we employ a more natural particle filtering scheme to generate and propagate the translation and rotation parameters in a decoupled manner in order to find the optimal pose of an object of interest. In addition, we focus more on occlusion detection and handling to maintain track in the presence of occlusions. In particular, we deal with partial occlusions that distort the region properties of the tracked object, which is a key feature for region-based tracking methods. To this end, we improve the l-iteration scheme introduced in [31,41] by controlling dependencies between predictions and measurements of the system according to the degree of occlusion and variation of the object’s 3D pose from previous accumulated results; see Section 3.4. This improvement provides the robust method to deal with occlusions of an obstacle with different statistical properties in a tracking framework that relies heavily on region properties of an image. To the best of our knowledge, this is the first attempt to exploit the degree of dependencies between predictions and measurements of the system for these particular case of occlusions taken place in region-based frameworks involving iterative and derivative formulation.Moreover, in the present work, the variational approach is embedded into the proposed framework in designing a measurement model to reduce computational complexity and to facilitate the effective search of a local optimum in a particle filtering framework. This method allows the samples to move further into modes of a filtering distribution so that only a few samples are necessary; see Section 3.3. Variational methods, such as Mean-shift [42], are typically gradient based optimization methods minimizing a cost functional in order to find the local mode of a probability distribution. To effectively reduce the sample size of the particle filtering framework, variational approaches are embedded into particle filters in a number of works. The authors in [43] present a mean shift embedded particle filter, in which a smaller number of samples is used to estimate the posterior distribution than conventional particle filters by shifting samples to their neighboring modes of the observation so that samples are moved to have large weights. In [44], the underlying probability density function (pdf) is represented with a semi-parametric method using a mean-shift based mode seeking algorithm to solve a tracking problem for high dimensional color imagery. The authors of [45] fuse a deterministic search based on gradient descent and random search guided by a stochastic motion model, then, in object tracking, they effectively switch two search methods according to a momentum using the inter-frame motion difference.The remainder of this paper is organized as follows. In the next section, we briefly explain an overview of the two fundamental concepts used in the proposed method, particle filtering and the gradient descent flow presented in [13], respectively. Section 3 describes the overall particle filtering approach for 2D object tracking and 3D pose estimation. Specifically, we derive a prediction model, a measurement model, as well as an occlusion handling scheme for the task of object tracking. In Section 4, we provide experiments on both synthetic and real life imagery in hopes of highlighting the viability (and limitations) of the proposed algorithm in the context of visual tracking. Lastly, we conclude and discuss possible future research directions in Section 5.Sequential Bayesian filtering estimation with Monte Carlo simulation, called particle filtering, was first introduced by Gordon et al. [8]. In recent years, it has proven to be a powerful scheme for non-linear and non-Gaussian estimation problems due to its simplicity and versatility.The overall objective is to estimate a (hidden) variable of interest governing a particular underlying system with respect to what is being observed or measured. With this goal, we let stbe a state vector and ztbe a set of observations. We then model the state transition equation and the observation equation by(1)st+1=ft(st,ut)zt=ht(st,vt)where utand vtare independent and identically distributed (iid) random variables representing noise whose probability density functions (pdfs) are known. Furthermore, we assume that the initial state distribution p(s0) is known.Drawing N≫1 samplesstii=1,…,Nfrom p(st∣z1:t), one can properly construct an empirical estimate of the true hidden state st. However, generating samples from the posterior distribution p(st∣z1:t) is usually not possible (e.g., f and h are non-linear). Thus, if one can only generate samples from a similar density q(st∣z1:t)≈p(st∣z1:t), the problem becomes one of importance sampling. That is, one can form a Monte Carlo estimate of stby generating N samples according to q(st∣z1:t) with associated importance weightswtii=1,…,Nat each time t. More importantly, as the algorithm progresses and if N is chosen sufficiently large, the proposal distribution can be shown to evolve toward the correct posterior distribution, i.e., q(st∣z1:t)=p(st∣z1:t).Thus, the generic algorithm begins by first sampling N times from initial state distribution, p(s0). Following this, the algorithm can be decomposed in two steps: the prediction step and the update step. Using importance sampling [21], the prediction step is the act of drawing N samples from the alternative proposal distribution q(st∣z1:t). As new information arrives online at time t from the observation zt, one needs to evaluate the fitness of the predicted samples or particles. In other words, as ztbecomes available, the measurement or update step in particle filtering is incorporated through the importance weights by the following equation(2)w̃ti=w̃t-1ipzt|stipsti|st-1iqsti|st-1i,z1:twherepzt|stiis the likelihood of the arrived observation at time t. From the above approach, the filtering distribution is represented by a set of samplesstiand its associated weightswtias(3)p(st|z1:t)≈∑i=1Nwtiδst-stiwhere δ denotes the Dirac function, andwtiis the normalized weight of ith particle:∑i=1Nwti=1. Moreover, one can now obtain an empirical estimate of the state stvia maximum likelihood or through a different statistical measure. We should note that in all particle filtering methods, a resampling operation is generally performed to eliminate particles with low weights (sample degeneracy). On the other hand, if one resamples inefficiently, there may be a loss of diversity for a set of particles (sample impoverishment). We refer the reader to Section 3 for more information on resampling.Like [13,40], we assume we have prior knowledge of the 3D shape of interest for which we would like to estimate the corresponding 3D pose from the 2D scene. Let X=[X, Y, Z]Tbe the coordinates of a point inR3with respect to the referential of the camera. Here, it is assumed that the calibrated camera is already given and is modeled as a pinhole camera:π:R3↦Ω;X↦xwhereΩ⊂R2is the domain of an image I(x) and x=[x, y]T=[X/Z, Y/Z]Tdenotes coordinates in Ω.S⊂R3is a smooth surface representing the shape of interest and N=[N1, N2, N3]Tdenotes the outward unit normal to S at each point X∈S. Let R=π(S)⊂Ω be the region on which the surface S is projected and Rc=Ω⧹R be the complementary region of R. Similarly, the curve ĉ=π(C)⊂Ω is the projection of the curve C⊂S and ĉ also denotes a boundary of R, ĉ=∂R. Note, the curve ĉ in 2D and the curve C in 3D are referred to as the silhouette and the occluding curve, respectively.For our particular segmentation problem, we seek to find a boundary that optimally partitions the object of interest or foreground from the corresponding background in a given 2D image. Inspired by region-based active contour models [46–48], the authors in [13] define an objective energy functional based on the global statistics of an image so that the curve ĉ (and 3D pose) is evolved to maximize the image statistical measure of discrepancy between its interior and exterior regions. This is given as follows:(4)E=∫Rro(I(x),cˆ)dΩ+∫Rcrb(I(x),cˆ)dΩwherero:χ,Ω↦Randrb:χ,Ω↦Rare functions measuring the visual consistency of the image pixels with a statistical model over the regions R and Rc, respectively. Here, χ is the space that corresponds to photometric variable of interest. In this work, roand rbare given by:(5)ro=-log(Σo)-(I(x)-μo)2Σo,rb=-log(Σb)-(I(x)-μb)2ΣbHere Σoand Σbare variances inside and outside the curvecˆ, and are given by(6)Σo=∫R(I(x)-μo)2dΩ∫RdΩ,Σb=∫Rc(I(x)-μb)2dΩ∫RcdΩwhere(7)μo=∫RI(x)dΩ∫RdΩ,μo=∫RcI(x)dΩ∫RcdΩFor gray-scale images, μo/b and Σo/b are scalars and for color images,μo/b∈R3andΣo/b∈R3×3are vectors and matrices. Note that roand rbcan be chosen as various forms describing the region properties of the pixels located inside and outside the curve (e.g., mean intensities [46], distinct Gaussian densities [47], and generalized histograms [48]). As seen above, roand rbare chosen to be the region based functional of [47].LetX0∈R3be the coordinates of points on S0 where S0 is the identical reference surface in 3D. By the rigid transformation g∈SE(3), one can locate S in the camera referential by S=g(S0). Written in a point wise fashion yields X=g(X0)=RX0+T with R∈SO(3) denoting the rotational group andT∈R3representing translations. Here, 3D pose motions are represented by a set of six parameters. The parameters of the rigid motion g will be denoted by λ=[λ1, …, λ6]T=[tx, ty, tz, ωx, ωy, ωz]T. Rotations are represented in exponential coordinates, which is a more compact form than using quaternion (4 entries) or basic rotation matrices in three dimensions (12 entries); see [5]. Now, since we assume that the 3D shape of the rigid object is known, our objective is to minimize energy E in Eq. (4) by exploring only the regions R and Rcthat result from projecting the surface S onto the image plane. For a calibrated camera, these regions are functions of the transformation g only. Solving for the transformation that minimizes E can be undertaken via gradient descent over the parameters λ. This is described next.The partial differentials of E with respect to the pose parameters λi’s can be computed using the chain-rule:(8)∂E∂λi=∫cˆ(ro(I(x))-rb(I(x)))∂cˆ∂λi,nˆdsˆ+∫R∂ro∂cˆ,∂cˆ∂λidΩ+∫Rc∂rb∂cˆ,∂cˆ∂λidΩwheresˆis the arc-length parameterization of the silhouettecˆandnˆis the (outward) normal to the curve at x.Using the arc-length s of C andJ=01-10, one has(9)∂cˆ∂λi,nˆdsˆ=∂cˆ∂λi,J∂cˆ∂sˆdsˆ=∂π(C)∂λi,J∂π(C)∂sds=1Z3∂X∂λi,0Z-Y-Z0XY-X0∂X∂sds=1Z3∂X∂λi,∂X∂s×Xds=‖X‖Z3κXκtK∂X∂λi,Ndswhere K denotes the Gaussian curvature, and κXand κtdenote the normal curvatures in the directions X and t, respectively, where t is the vector tangent to the curve C at the point X, i.e.t=∂X∂s. Note that the last two terms in Eq. (8) collapse due to the choice of the roand rbin Eq. (5). Now we have the following gradient descent flow (see [13] for detailed computation):(10)∂E∂λi=∫C(ro(I(π(X)))-rb(I(π(X))))·‖X‖Z3κXκtK∂X∂λi,Ndswhere the term∂X∂λi,Ncan be computed for the evolution of the pose parameter λiwhich is a translation parameter (i=1, 2, 3) or a rotation parameter (i=4, 5, 6):•For a translation parameter,(11)∂X∂λi,N=∂RX0+T∂λi,N=∂T∂λi,N=Niwhere the Kronecker symbol δi,jwas used (δi,j=1 if i=j, and δi,j=0 otherwise) and T=[tx, ty, tz]T=[λ1, λ2, λ3]T.For a rotation parameter,(12)∂X∂λi,N=∂RX0∂λi,N=exp0-λ6λ5λ60-λ4-λ5λ400-δ6,iδ5,jδ6,i0-δ4,i-δ5,iδ4,i0X0,N

@&#CONCLUSIONS@&#
