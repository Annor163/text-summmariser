@&#MAIN-TITLE@&#
ECG compression retaining the best natural basis k-coefficients via sparse decomposition

@&#HIGHLIGHTS@&#
Compression algorithm for the ECG signals based on sparse representation.Dictionary of waveforms directly extracted from the cardiac signal.High compression rate within a controlled approximation measure.Evaluation on the MIT-BIH Arrhythmia database.

@&#KEYPHRASES@&#
ECG compression,Sparsity recovery,Orthogonal projections,Fixed-point iteration scheme,

@&#ABSTRACT@&#
A novel and efficient signal compression algorithm aimed at finding the sparsest representation of electrocardiogram (ECG) signals is presented and analyzed. The idea behind the method relies on basis elements drawn from the initial transitory of a signal itself, and the sparsity promotion process applied to its subsequent blocks grabbed by a sliding window. The saved coefficients rescaled in a convenient range, quantized and compressed by a lossless entropy-based algorithm.Experiments on signals extracted from the MIT-BIH Arrhythmia database show that the method achieves in most of the cases very high performance.

@&#INTRODUCTION@&#
In the last few years, the need of ECG signal recordings has been enormously augmented due to the increasing interest in health care [23]. Portable ECG recording systems (e.g. Holter) record ECG signals continuously for long time periods ranging between several hours and few days. One of the most important problem of the Holter systems is the huge amount of space required to store long time records [18]; for example a one channel ECG signal sampled at a frequency of 512Hz with 11bits of quantization resolution, recorded for 24h requires 58MB of storage size.To face such a high growth rate of storage request, many ECG compression algorithms have been developed to encode digital ECG signals [20,26,12,9,5,14]. They can be classified into two major categories: lossless and lossy algorithms. The lossless algorithms such as the classical Run-Length Encoding [22], Huffman [15] and Lempel–Ziv–Welch [28] do not produce reconstruction error, but the compression ratio is generally small. Conversely, the lossy algorithms, as for instance Foveation-SPIHT [8] and DCeq-MMP [10], pay a quantization error to obtain a higher compression level.The lossy methods can be further categorized according to the scheme of compression. The direct schema [16,17,19] analyzes the signal directly, aiming at eliminating the redundancies. On the contrary the transform schema applies at first some data transformations to extract the most informative signal characteristics and then the coefficients so found are encoded via lossless compression algorithms. In this category wavelet based methods [4] has proven their validity. Chen et al. [6] represented ECG signals by wavelet functions (like the Haar, Coiflet4, Deaubuchies18, Battle3 and Symmlet8) and used them as atoms of the dictionary. Lu et al. [21] proposed a method based on the EZW coder, called the Set Partitioning in Hierarchical Trees (SPIHT) algorithm which represents a state-of-art method. Later Huang et al. [14] extended SPIHT combining a dynamic learning vector quantizer and a context modeling arithmetic coding techniques. The method proposed by Agulhari et al. [5] is noteworthy. At first it determines the wavelet function that minimizes the distortion of the reconstructed ECG signal, and then it encodes the position of the most informative coefficients via a modified version of the RLE.Recent contributes have given a new impulse to this research field highlighting that both the use of natural basis ECG signal dictionaries [11,12], and the compressive sensing (CS) [23,26] help in augmenting the effectiveness of the compression methods. Mamaghanian et al. [23] were the first applying CS to ECG. After that several variants have been proposed: in [20] a group sparse reconstruction algorithm is adopted aiming at reducing the measurements and lowering the complexity computation costs. In [25] a multi-scale dictionary learning based on wavelet is presented. In [12] CS is applied on personalized dictionaries, consisting of pieces of a patient ECG signal. Dictionaries contain 1000 atoms of size equal to 256, with no particular care about the position of the R wave.In this article we propose a dictionary building technique resembling to that of the method explained in [12], because of the use of natural basis, i.e. waveforms directly extracted from the cardiac signal at hand. Contrary to [12] we try to make the atoms the most possible aligned. Specifically, our preprocessing method basically consists in signal block-wise segmentation in correspondence to R-peaks. This yields to an alignment of the R-waves at the extremes of the segments. To eliminate the influence of heart rate variability, the waveforms so obtained are normalized to a constant number of samples padding with zeros in the middle of the RR-periods. In this way the QRS complex regions, which carry the most of energy, are aligned and the energy is concentrated. This natural basis dictionary represents a good starting point for the subsequent sparsity promotion processing. The sparse solver we use here has been proposed in [2] and represents the heart of the compression process. The method provides in most of the cases a very high compression rate within a controlled approximation measure.The performance of the proposed algorithm is evaluated on the MIT-BIH Arrhythmia database in terms of compression ratio and reconstructed signal quality.The paper is organized as follows. In Section2, we recall the essence of the sparse decomposition technique used as core of the overall compression algorithm. In Section3 we explain how to use the sparse representation based on a dictionary built on a natural basis after a signal normalization. In Section4 we report the simulations on some ECG signal of the MIT-BIH dataset and finally, in the last section, we draw some conclusions and future directions to work on.Sparsity is a desired property that may exhibit solutions to the underdetermined linear system of equations Φα=x, where matrixΦ∈ℝn×m(with m>n) andx∈ℝnis a vector in the column space of the matrix. In order to avoid discussions about the existence of a solution for the linear system, we shall hereafter assume that matrix Φ be full-rank, implying that its columns span the entire spaceℝn. To capture the notion of sparse representation, many different sparsity optimization problems have been defined, but all are some sort of variant of the well-known NP-hard combinatorial optimization problem(1)minα∈ℝm∥α∥0subject toΦα=x,where ∥·∥0 denotes the ℓ0 quasi-norm which counts the nonzero entries in a vector. Such a functional is directly linked to the support of the vector α=(α1, …, αm) defined as supp(α)={j:αj≠0}.However, in real-world applications the exact constraint Φα=x is hardly satisfied and hence it is often relaxed to an approximation measured in terms of norm penalty, i.e. ∥Φα−x∥≤ɛ, giving rise to the more realistic regularization problem(2)minα∈ℝm∥α∥0subject to∥Φα−x∥≤ɛ,for a fixed error tolerance ɛ>0 and error measured by the standard Euclidean norm ∥·∥.The addition of ɛ in the problem formulation is sometimes motivated by the need of introducing a restriction on the feasible solution space due to specific constraints, providing in this case proper subsets ofℝncontaining k-sparse vectors. The most natural scenario to promote sparsity is given by the presence of noise in the signal x, forcing its representation to assume the form x=Φα+η, where α is k-sparse and η is a nuisance vector of finite energy.The ECG compression task can be naturally recasted in the noisy model above, and hence can be carried out by solving the problem (2). To this aim, we briefly describe in the next subsection a sparse promotion strategy based on a fast iterative fixed-point scheme.In recent works [2,3], we introduced a technique that relies on a parametric family of nonconvex mappings that yields, under suitable conditions, locally optimal solution to the problem (1). Furthermore, adopting the noise setting, the algorithm named k-LiMapS in [2] allows us to fix a priori the sparsity level k to find approximate sparse solution of a slightly different problem, namely(3)minα∈ℝm∥Φα−x∥subject to∥α∥0≤kThese considerations suggest an heuristic method for solving problem (2): run k-LiMapS by progressively increasing the sparsity level k until the error ∥Φα−x∥ goes under a fixed threshold ɛ.In order to remind the principle which k-LiMapS is based on [1], we briefly recall the main elements which constitute the algorithm: a first nonlinear parametric mapping combined with a linear orthogonal projector, followed by another nonlinear projection which mapsℝmto the k-sparse vector set.More specifically, the former mapping is defined by the transformation overℝm:(4)α↦PNΦ[α⊙(1−e−λ|α|)]+νwhere ν=Φ†x, λ>0 is a shrinking parameter, ⊙ and |·| denote element-wise product and absolute value respectively, whilePNΦ=I−Φ†Φis the orthogonal projector on the null spaceNΦof matrix Φ, that can be built with the Moore–Penrose pseudo-inverse Φ†.The latter mapping is invoked after each application of (4) and consists of a nonlinear orthogonal projection involving absolute values of coefficient vector α. Formally, it is defined fromℝmto the k-sparse vector setΣk={β∈ℝm:∥β∥0≤k}by the transformation(5)PΣk(α)=arg minβ∈Σk∥α−β∥.Notice that, due to the nonconvexity of Σk, the solution of problem (5) is in general not unique, as can be checked easily by a suitable instance of α.The algorithm consists of an iterative system starting from an initial guessα0∈ℝmand its k-sparse projectionαˆ0=PΣk(α0)and then repeatedly applies mappings (4) and (5) in order, thus yielding the sequence{αˆt}t≥0in Σk. In fact, we have the iterative steps:(6)αt+1=PNΦ[αˆt⊙(1−e−λt|αˆt|)]+ν(7)αˆt+1=PΣk(αt+1),where{λt}t≥0is an increasing sequence of parameters conveniently chosen. The convergence of the sequence {αt} as well as the sequence{αˆt}mainly concerns with the fixed points of mapping (4), which are minimizers of suitable smooth non-convex quasi-norms approximating the ℓ0 quasi-norm. Since this issue goes beyond the scope of the paper, refer to [1] for more details.Many works in sparse representation domain demonstrate that the difficult problem is to find the coefficient support rather than to find the values of the coefficients themselves. k-LiMapS deals with this critical problem by seeking a trade-off between two needs: the convergence of sequences {αt} and the detection of the subspace in Σkcontaining the limit point of{αˆt}solution of (2). In the next section we show how to adapt this general purpose algorithm to the task of giving sparse representation of ECG signals by controlling the typical error bound stated for this problem.Concerning the actual computation of the algorithm, we obviously cannot provide k-LiMapS with the ideal scheduling for the above-mentioned parameter sequence. However, using strictly increasing sequence for parameter λ, we experimentally observe its effectiveness: in most of the cases the sparsest solution of (1) is achieved. An empirical measure of its performance is given by the SNR (signal-to-noise ratio) measure which is higher with respect to those achieved by other well-known algorithms in literature (see experiments in [3]).The algorithm for ECG signal compression is summarized in Fig. 1. It consists in a block-wise sparse coding preceded by standard preprocessing and dictionary creation, and followed by a final quantization and lossless compression of both sparse coefficients and normalization parameters, as detailed in the following subsections.Given an ECG signal, an overcomplete dictionary is built using the cardiac patterns of the signal extracted from an initial transient of few second duration. The signal has to be segmented into the most regular and close to cyclostationary patterns, in order to facilitate the sparse representation stage. This suggests to include in a segment both the P and T waves, and the QRS complex. Any choice that respects this criterion would be effective (e.g. the R-peak in the center or in the extreme of the segment). Our choice has been to segment the signal between successive RR intervals, concentrating the distribution or RR-peaks at the extremes of each segment. Thus each segment s contains two halves of the QRS complex with the T-wave on the left and the P-wave on the right.Data preprocessing plays a very important role in many deep compression techniques, thus the signal s is then normalized by first applying a mean-centering step and then extending its length to a fixed length by adding a suitable number of zeros in the middle (so-called zero padding), as depicted in Fig. 2. Normally, a reasonable size n of the final zero-mean padded segment is the value of the sample frequency, Fs, but, in order to avoid to break longer RR segments in cases such as arrhythmia, n can be augmented multiplying Fswith a factor, γ, greater than 1. γ has to be fixed guaranteeing the respect of the constraint n<m, that is the segment dimension has to be smaller than the number of segments stacked in the dictionary. In particular, taking both n and m depending on Fs, we setn=γ·Fs<m=λ·Fs⇒γ<λ.Experimentally, we fixed γ=1.5 and λ=2 as a good trade off between the necessity of having a sufficiently long segment and a not too big dictionary. Of course there could happen that a RR segment exceeds the given dimension. In this case an artificial signal interruption is inserted, causing a bad compression of the involved segments. This should be very rare and less expensive than storing a bigger dictionary.Thus, the slight overhead given by the normalization phase is widely compensated by the high level of sparsity that could be potentially reached. This stage prepares the data so that the sparse representation algorithm may achieve a reduction of the number of significant coefficients, bringing to a decisive high compression ratio.After normalization, the dictionary Φ=[ϕ1, …, ϕm] is obtained by stacking a set of m heart-beat segments ϕi(i=1, …, m) of size n, with n<m. The operator Φ represents a linear mapping into a suitable feature spaceS⊆ℝn, and each ϕidenotes its basis vector (or atom).The main purpose of the sparsity promotion phase is to adaptively choose the sparsity level k in order to guarantee an upper bound on the discrepancy between the ith ECG segment siand its sparse representationsˆi. To this end, we introduce the main criterion involved in such approximation in order to meet the bounded error requirement, namely the percent root-mean square difference (PRD):PRD(si,sˆi)=100∥si−sˆi∥∥si∥.In this setting, the early stage compression can be recasted into the problem of finding the sparsest representation of the ith segmentsi∈Sinto the dictionary Φ. A sparse representation for siis a linear combination of as less as possible basis elements, i.e. the smallest number of atoms such thatsi=∑j(αi)jϕj, or equivalently, in matricial formΦαi=si.According to the problem (2) given in previous section and fixed a bound PRDmax on the PRD involving each segment si, the algorithm based on k-LiMapS (called k-LiMapS_ECG) aims at approximately solving the above linear system with the regularization problemminαi∈ℝm∥αi∥0subject toPRD(si,Φαi)≤PRDmax,representing an error-rescaled version of (2) by a factor 10−2∥si∥.The k-LiMapS-based solver pseudo-code is listed in Algorithm 1. After executing Algorithm 1, a ki-sparse vector αiis carried out yielding a good approximationsi≈sˆi=Φαiwhose percent mean-square error satisfiesPRD(si,sˆi)≤PRDmax.Algorithm 1k-LiMapS_ECG – Sparse representationAs depicted in Fig. 1, for long term signals of exactly N heart-beats, the data to be compressed are of three kinds: sparse coefficients, support of the coefficient vector and zero-padding values. The stages of this compression process are briefly summarized in the following:1.The set of coefficients contained in the sparse represented vectors α1, …, αNis compressed. Among these coefficients, the total number of significant (namely non-null) values amounts toK=∑i=1Nki. To compress them, we fix a q bit quantization, to attain 2quniform quantization levels and simply rescale and quantize each coefficient c into the integer number(8)cq=roundc−αminαmax−αmin2q,where αmin and αmax are the minimum and maximum coefficient values respectively.Together with the non-null coefficient values, we have to compress its support or the relative positions within each coefficient vector αi. This operation can be thought as a characteristic bit-vectorχαi=(bi1,…,bim)setting to 1 the elements in the positions corresponding to non-null coefficients of αi. The concatenation χ of the vectors{χα1,…,χαN}forms a sparse bit-vector with K ones over mN bits, with K≪mN. An effective way to compress χαis to use the Huffman coding [15] for bit-vectors. To this end, the χαis partitioned into blocks of fixed size l and statistics are collected on the frequency of occurrence of the 2lbit patterns. For sparse vectors, the most probable l-bit configurations are those consisting of all zeros or at most a single bit equal to 1, resulting in an average codeword length of the Huffman code smaller than l.The last list of N numbers ρ1, …, ρNto be compressed consists of the number of zeros added in each heart-beat (i.e. segment) by the zero-padding normalization task. An effective way to encode such a list is with the delta encoding, which stores the differences between pairs of successive values in the list. Such encoding is particularly useful in our case where most of the differences are small. Finally, the sequence of differences can be thus compressed much more efficiently by most of the compression algorithms.As for the computational complexity of the proposed algorithm, we have two distinct phases to analyze: offline and online computation. In the former, the Moore–Penrose pseudo-inverse Φ† is the most expensive computation and substantially consists of an SVD requiring TSVD(n, m)=2n2m+4nm2+(9/2)m3 floating-point operations [27] using the Golub–Kahan–Reinsch method. In the online phase, the complexity of the transformation (4) is dominated by the application of the projectionPNΦwhich requires O(mn) steps using a trivial matrix–vector multiplication, while the solution of problem (5) is easily obtained by retaining the greatest elements of α in absolute value and setting to zero the remaining entries, thus requiring O(m) steps. Hence, the computational complexity of each call to the heuristic k-LiMapS is of O(nm) times the number of iterations in k-LiMapS[3]. Therefore, considering in both the while-loops of Algorithm 1 the loop variable kiis monotonic so that the number of iterations is less than n, we have that the total computational cost of Algorithm 1 at each step of the compression process is lower than O(n2m) floating-point operations.In order to evaluate the compression performance of the k-LiMapS_ECG algorithm, we performed computer simulations on widely used ECG signal recordings. In particular, tests were conducted using the records 100, 101, 102, 109, 111, 112, 113, 115, 117, 119 and 121 of the MIT-BIH Arrhythmia database [13]. These signals are 30min registrations sampled at 11bits with a sampling frequency of fs=360Hz.The compression performance for a cardiac signal x of fixed duration is computed, as usual, in terms of Compression Ratio (CR):CR=#bits ofx#bits ofxˆ.In our setting, the signal to be compressed is a sequence of N cardiac segments x=[s1, …, sN] picked sequentially by a sliding window without overlapping. As already mentioned in the previous section, to accomplish this task we have to compress three kinds of data: non-null coefficients in each vector αi, characteristic bit-vectorsχαiand padding numbers ρi. Putting these three factors together, the CR becomesCR=N×n×11Bα+Bχ+Bρ,where n is the number of samples in each segment, which is exactly the sampling frequency n=fsbecause of the normalization, and Bα, Bχ, Bρare the number of bits used for coding the coefficients, the characteristic vectors, and the padding numbers respectively.Regarding the error, since the PRD measure heavily depends on the mean values¯of the ECG signal s, it is sometime more suitable to use the normalized PRD measure given byPRDN=100∥s−sˆ∥∥s−s¯∥.As far as the experimental equipment is concerned, the tests have been carried out on an AMD Athlon II X4 630 2.8GHz 64-bit Processor with 4GB of memory, implementing our algorithm in MATLAB v.2011b.11Demo code can be downloaded at http://dalab.di.unimi.it/klimaps.html.We point out that all runs have been done in real time thanks to the fact that the convergence is achieved executing the k-LiMapS core only a bounded number of times (no more than 2 or 3 times) for each fixed sparsity level k. This surprisingly little number of iterations speeds up the entire process, allowing at the same time to control locally the error magnitude imposed by PRD at the end of k-LiMapS_ECG stage. The final reconstructed signal from compressed coefficients entails a slightly worse PRD due to the padding cancellation.In Table 1we report compression ratios achieved on the records selected from the MIT-BIH Arrhythmia database. Operatively, each recording has been split in two parts: the first one of about 10min, shown in column Dict T, is used for the dictionary creation phase, and the second one of about 20min, shown in column Comp T, has been used for testing the sparse representation and signal compression. Finally, each test signal has been quantized using two different quantization rates: 6 and 7bits. The only signal that has a drastically decreased quality, passing from a quantization of 7bits to 6bits, is the record 119. This difficulty is surely due to its irregularity in both the RR intervals and the QRS complex shapes.In Fig. 3we depicted four representative examples of compressed signal extracted from the records 100, 102, 112 and 119, corresponding to a range from the best to the worst level of CR and PRD. For such records, we illustrate in the subfigures from top to bottom a 5s window of original signal without baseline, reconstructed signal, and reconstruction error. Note that the sample errors are equally distributed over all regions of the cardiac signal, hence giving more worth to the proposed strategy.Direct comparisons signal-to-signal with other methods can hardly be done because of the strong dependency of the performances on the adopted data normalization, often not explicitly given; besides, a direct comparison of the average CR and PRD values is not completely satisfactory since the pool of signals chosen for testing is not always detailed. Nonetheless, we report an indirect comparison, highlighting that our method, tested on a representative selection of the database, achieves on average CR=39.97:1 and PRD=0.79 (see Table 1, last row), better than the state of the art on the ECG compression methods based on sparse representation [26,12,23]: among them Fira et al. [12] reported the best average performances over 24 unspecified records from the MIT-BIH database, achieving a PRD=1.08 with an average CR=15:1.Another interesting comparison level concerns the performance analysis of the k-LiMapS algorithm only with well-known sparsity solvers such as the greedy strategy, called OMP (orthogonal matching pursuit) [24] and the linear programming called basis pursuit (BP) [7]. Such comparison shows that the k-LiMapS algorithm obtained the highest performances. Regarding the computational times, we have experimented that our algorithm is faster than both OMP and BP, especially on the instances for which high accuracy on the response is required. This in turns requires a better approximation on sparse representation implying a large number of atoms or basis elements (low sparsity case). The explanation of this behavior is easily provided by the intrinsic mechanisms on which the two algorithms are based-on: an orthogonalization process for OMP and the simplex method for BP. Both these techniques lay on very expensive computational procedures that may turn out to be inappropriate for running on small devices. On the contrary, in our iterative method, the number of loops can be reduced without decreasing significantly the performances, while lowering the computational costs.

@&#CONCLUSIONS@&#
