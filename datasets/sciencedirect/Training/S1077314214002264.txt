@&#MAIN-TITLE@&#
Large margin learning of hierarchical semantic similarity for image classification

@&#HIGHLIGHTS@&#
Novel large margin formulation for semantic similarity learning.Efficient optimization algorithm to solve the proposed semi-definite program (SDP).Thorough experimental study to compare the performances of several algorithms for hierarchical image classification.State-of-the-art classification performance under the hierarchical-loss criterion.

@&#KEYPHRASES@&#
Image classification,Similarity learning,Semantic representation,Large-margin framework,

@&#ABSTRACT@&#
In the present paper, a novel image classification method that uses the hierarchical structure of categories to produce more semantic prediction is presented. This implies that our algorithm may not yield a correct prediction, but the result is likely to be semantically close to the right category. Therefore, the proposed method is able to provide a more informative classification result. The main idea of our method is twofold. First, it uses semantic representation, instead of low-level image features, enabling the construction of high-level constraints that exploit the relationship among semantic concepts in the category hierarchy. Second, from such constraints, an optimization problem is formulated to learn a semantic similarity function in a large-margin framework. This similarity function is then used to classify test images. Experimental results demonstrate that our method provides effective classification results for various real-image datasets.

@&#INTRODUCTION@&#
Recognizing categories of objects and scenes is one of the most critical problems in computer vision. Although continuous progress has been made in this field, there still remains a large gap between machine performance and human intelligence. Unlike machines, humans can categorize at least tens of thousands of objects and scenes without any difficulty [1]. Furthermore, they can build a hierarchy of categories by simply observing images, and exploit it to produce semantically more meaningful judgement. For example, someone may mistakenly classify a dog as a cat but hardly misclassify a dog as a car. This example shows that one can produce a more informative classification result by considering the similarity between two semantic concepts. In the current work, we focus on this issue and attempt to make the image classification algorithm more semantic and human-like.To achieve this goal, image classification algorithm should be developed under a new performance evaluation criterion. This criterion can be formulated by utilizing the hierarchical loss, which reflects the hierarchy of various semantic concepts, and not the flat 0/1 loss. Similar to [2], the hierarchical loss can be defined based on WordNet [3], a lexical semantic network for modeling human psycholinguistic knowledge. Under the hierarchical loss-based criterion, misclassifying an image as a different but semantically close category incurs a smaller loss than misclassifying it as a semantically distant category. Therefore, image classification can be significantly more informative by learning the algorithm based on the hierarchical loss. As shown in Fig. 1, the use of the hierarchical loss can provide substantial benefit to the results of classification.In the present paper, a new image classification method is presented, which utilizes the hierarchical loss function and produces semantically more meaningful results. The key idea of this approach is a novel combination of semantic representation and similarity function learning. Several recent works are available that explicitly estimate high-level semantic attributes for various applications, such as description of generic or unfamiliar images [4,5], zero-shot transfer learning [6], and intermediate features that can aid in visual recognition [5–8]. We adopt semantic representation for the latter purpose to produce low-dimensional semantic feature vectors. The low dimensionality of our feature vector helps in the subsequent similarity learning being performed very efficiently compared with the conventional similarity or distance function learning [9–11] that usually directly handles high-dimensional low-level feature vectors. Moreover, because our feature vector is semantic, enforcing constraints among semantic concepts for the minimization of hierarchical loss is easy. More specifically, learning the similarity function, which is the core learning problem of our approach, is formalized within a large-margin framework and is guaranteed to minimize empirical hierarchical loss.The contributions of the present work include the following:•A novel large-margin formulation for semantic similarity learning is proposed. This learning problem can be viewed as an instance of a semidefinite program (SDP) [12], and an efficient optimization algorithm is developed.A thorough experimental study is conducted for comparing the performances of several algorithms for hierarchical image classification [13–15]. For this purpose, the Caltech [16,17] and ImageNet [18] datasets are used.The proposed method is shown to achieve a state-of-the-art classification result under the hierarchical-loss criterion. Furthermore, a noticeable gain in the conventional measures, such as accuracy and precision, can also be obtained.The rest of the current paper is organized as follows. In Section 2, some related works are discussed. Section 3 presents the framework of the proposed method, followed by the presentation of the experimental results in Section 4. Finally, Section 5 concludes this paper.

@&#CONCLUSIONS@&#
