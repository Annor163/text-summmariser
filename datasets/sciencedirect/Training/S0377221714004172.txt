@&#MAIN-TITLE@&#
Applying oracles of on-demand accuracy in two-stage stochastic programming – A computational study

@&#HIGHLIGHTS@&#
We devise variants of the L-shaped method using the concept of on-demand accuracy (ODA).In many of the iterations only an approximate cut is added to the master problem.These cuts do not require the solution of second-stage subproblems.ODA reduces average solution time by 55% on 105 problems.ODA combined with regularization reduces average solution time by 79%.

@&#KEYPHRASES@&#
Stochastic programming,Two-stage problems,Decomposition,Bundle methods,

@&#ABSTRACT@&#
Traditionally, two variants of the L-shaped method based on Benders’ decomposition principle are used to solve two-stage stochastic programming problems: the aggregate and the disaggregate version. In this study we report our experiments with a special convex programming method applied to the aggregate master problem. The convex programming method is of the type that uses an oracle with on-demand accuracy. We use a special form which, when applied to two-stage stochastic programming problems, is shown to integrate the advantages of the traditional variants while avoiding their disadvantages. On a set of 105 test problems, we compare and analyze parallel implementations of regularized and unregularized versions of the algorithms. The results indicate that solution times are significantly shortened by applying the concept of on-demand accuracy.

@&#INTRODUCTION@&#
Decomposition is an effective and time-honoured means of handling two-stage stochastic programming problems. It can be interpreted as a cutting-plane scheme applied to the first-stage variables. Traditionally, there are two approaches: one can use a disaggregate or an aggregate model. A major drawback of the aggregate model is that an aggregate master problem cannot contain all the information obtained by the solution of the second-stage problems. The disaggregate master problem, on the other hand, may grow excessively. It is not easy to find a balance between the effort spent in solving the master problem on the one hand, and the second-stage problems on the other hand. The computational results of Wolf and Koberstein (2013) give insights into this question.In this study we report our experiments with a special inexact convex programming method applied to the aggregate master problem of the two-stage stochastic programming decomposition scheme. The convex programming method is of the type that uses an oracle with on-demand accuracy, a concept proposed by Oliveira and Sagastizábal (2014). We are going to use a special form which, when applied to two-stage stochastic programming problems, integrates the advantages of the aggregate and the disaggregate models. This latter feature is discussed in Fábián (2012). We also examine the on-demand accuracy idea in an un-regularized context, which results a pure cutting-plane method in contrast to the level bundle methods treated in Oliveira and Sagastizábal (2014).The paper is organized as follows. In Section 1.1 we outline the on-demand accuracy approach to convex programming, and present an algorithmic sketch of the partly inexact level method. In Section 2 we overview two-stage stochastic programming models and methods. Specifically, in Section 2.1 we sketch a decomposition method for two-stage problems based on the partly inexact level method. Section 3 discusses implementation issues. Our computational results are reported in Section 4, and conclusions are drawn in Section 5.Let us consider the problemminφ(x)suchthatx∈X,whereφ:IRn→IRis a convex function, andX⊂IRnis a convex closed bounded polyhedron. We assume thatφis Lipschitz continuous overXwith the constantΛ.Oliveira and Sagastizábal (2014) developed special regularization methods for unconstrained convex optimization, namely, bundle level methods that use oracles with on-demand accuracy. The methods work with approximate function data, which is especially useful in solving stochastic problems. Approximate function values and subgradients are provided by an oracle with on-demand accuracy. The accuracy of the oracle is regulated by two parameters: the first is a descent target, and the second is a tolerance. If the estimated function value reaches the descent target, then the prescribed tolerance is observed. Otherwise the oracle just detects that the target cannot be met, and returns rough estimations of the function data, disregarding the prescribed tolerance. The method includes the ideas of Lemaréchal, Nemirovskii, and Nesterov (1995), Kiwiel (1995) and Fábián (2000); and integrates the level-type and the proximal approach.In this paper we are going to use a special method that falls into the ‘partly inexact’ category according to Oliveira and Sagastizábal, and applies only the level regularization of Lemaréchal et al. (1995). The method is discussed in detail in Fábián (2012).In the following description,ϕ¯denotes the best function value known, andϕ̲is a lower estimate of the optimum. The gapΔ=ϕ¯-ϕ̲measures the quality of the current approximation. The descent target isϕ¯-δ, where the tolarenceδis regulated by the current gap. If the descent target is reached, then the oracle returns an exact subgradient. Otherwise the oracle just detects that the target cannot be met, and returns rough estimations of the function data. Iterations where the descent target is reached will be called substantial.Algorithm 1A partly inexact level method.1.0 Parameter setting.Set the stopping tolerance∊>0.Set the level parameterλ(0<λ<1).)Set the tolerance regulating parameterγsuch that0<γ<(1-λ)2.1.1 Bundle initialization.Leti=1(iteration counter).Find a starting pointx1∈X.Letl1(x)be a linear support function toφ(x)atx1Letδ1=0(meaning thatl1is an exact support function).1.2 Near-optimality check.Computeϕ¯i=min1⩽j⩽iφ(xj).Letϕ̲i=minx∈Xφi(x), whereφi(x)=max1⩽j⩽ilj(x)is the current model function.LetΔi=ϕ¯i-ϕ̲i. IfΔi<∊then near-optimal solution found, stop.1.3 Finding a new iterate.Letxi+1be the projection ofxiontoXi=x∈X|φi(x)⩽ϕ̲i+λΔi.1.4 Bundle update.Letδi+1=γΔi.Letli+1(x)be a linear function such thatli+1(x)⩽φ(x)(x∈X),∥∇li+1∥⩽Λ,andeitherli+1(xi+1)⩾ϕ¯i-δi+1(descent target could not be reached),orli+1(xi+1)=φ(xi+1)(descent target has been reached).Incrementi, and repeat from step 1.2.In step 1.3, above, the projection ofxiontoXimeans finding the point inXinearest toxi. It means solving a convex quadratic programming problem.Convergence of Algorithm 1 follows from Theorem 3.9 in Oliveira and Sagastizábal (2014). It yields the following theoretical estimate: to obtainΔ<∊, it suffices to performcV/∊2iterations, where the constantscandVdepend on parameter settings, and problem characteristics, respectively.Remark 2Concerning the practical efficiency of the (exact) level method of Lemaréchal et al. (1995), in Nemirovski (2005) (Chapter 5.3.2) observes the following experimental fact. When solving a problem of dimensionnwith accuracy∊, the level method makes no more thannln(V/∊)iterations, whereVis a problem-dependent constant.This observation was confirmed by the experiments reported in Fábián and Szőke (2007) and Zverovich, Fábián, Ellison, and Mitra (2012), where the level method was applied in decomposition schemes for the solution of two-stage stochastic programming problems.Following Lemaréchal et al. (1995), we define critical iterations for Algorithm 1. Let us consider a maximal sequence of iterations such thatΔ1⩾Δ2⩾⋯⩾Δs⩾(1-λ)Δ1holds. Maximality of this sequence means that(1-λ)Δ1>Δs+1. Thenxs→xs+1will be labeled critical. The above construction is repeated starting from the indexs. Thus the iterations are grouped into sequences, and the sequences are separated with critical iterations.There is an analogy between the critical iterations of level-type methods, and the serious steps of traditional bundle methods. In this paper we use the former terminology which we feel more precise in the present setting.First we present the notation with a brief overview of the models. The first-stage decision is represented by the vectorx∈X, the feasible domain being defined by a set of linear inequalities. We assume that the feasible domain is a non-empty convex bounded polyhedron, and that there areSpossible outcomes (scenarios) of the random event, thesth outcome occurring with probabilityps.Suppose the first-stage decision has been made with the resultx, and thesth scenario has realized. The second-stage decisionyis computed by solving the second-stage problem or recourse problem that we denote byRs(x). This is a linear programming problem whose dual isDs(x):(1)Rs(x)minqsTysuchthatTsx+Wsy=hs,y⩾0,Ds(x)maxzT(hs-Tsx)suchthatWsTz⩽qs,zisareal-valuedvector.In the above formulae,qs,hsare given vectors andTs,Wsare given matrices, with compatible sizes. We assume thatRs(x)is feasible for anyx∈Xands=1,…,S. Moreover we assume thatDs(x)is feasible for anys=1,…,S. Letqs(x)denote the common optimum. This is a polyhedral convex function called the recourse function.The customary formulation of the first-stage problem is(2)mincTx+∑s=1Spsqs(x)suchthatx∈X.The expectation part of the objective,q(x)=∑s=1Spsqs(x), is called the expected recourse function.Since the two-stage stochastic programming problem (2)– (1) features discrete finite distributions and linear functions, it can be formulated as a single linear programming problem that we call the equivalent linear programming problem.Given a finite subsetU∼sof the feasible domain ofDs(x), the function(3)q̃s(x)≔maxus∈U∼susT(hs-Tsx)(x∈X)is a lower approximation ofqs(x)overX. Having appropriate subsetsU∼sfors=1,…,S, the disaggregate model of the first-stage problem (2) is constructed as(4)mincTx+∑s=1Spsνssuchthatx∈X,νs∈IR(s=1,…,S),usT(hs-Tsx)⩽νsholdsforanyus∈U∼s(s=1,…,S).The expectation in the objective,q̃(x)=∑s=1Spsq̃s(x), is called the disaggregate model function.An aggregate model of the first-stage problem is(5)mincTx+νsuchthatx∈X,ν∈IR,∑s=1SpsusT(hs-Tsx)⩽νholdsforany(u1,…,uS)∈U∼,whereU∼⊂U∼1×⋯×U∼Sis a certain subset of the Cartesian product. Namely, each element ofU∼belongs to a (potential) facet in the graph of the functionq̃(x). There may be facets not represented inU∼. The expectation(6)f̃(x)=max(u1,…,uS)∈U∼∑s=1SpsusT(hs-Tsx)is called the aggregate model function.Of course we haveq(x)⩾q̃(x)⩾f̃(x), since the disaggregate model function is based on the setsU∼s(s=1,…,S), and the aggregate model function is based on the setU∼⊂U∼1×⋯×U∼S.Methods working with a disaggregate master problem are often called multicut methods, while those working with an aggregate master problem are called single-cut ones. By using disaggregate cuts, more detailed information is stored in the master problem. This is done at the expense of larger master problems. Based on the numerical results of Birge and Louveaux (1988) and Gassmann (1990), Birge and Louveaux (1997) conclude that the multicut approach is in general more effective when the number of the scenarios is not significantly larger than the number of the constraints in the first-stage problem. Results of the computational study (Zverovich et al., 2012) confirm that the scale-up properties of solvers based on aggregate models are better than those of solvers based on disaggregate models, though the break-even thresholds are generally high. The results of the computational study (Wolf & Koberstein, 2013) provide further insights into the effects of cut aggregation.In order to apply the on-demand accuracy approach to two-stage stochastic programming problems, Oliveira and Sagastizábal (2014) propose inserting a new solver component between the aggregate master problem and the second-stage problems. The role of the new component is to provide approximate values and gradients of the expected recourse function, based on the information represented inU∼s(s=1,…,S).In this paper we work with Algorithm 1 adapted to the two-stage problem. The objective function iscTx+q(x), its disaggregate model iscTx+q̃(x), and its aggregate model iscTx+f̃(x).LetD‾denote the best objective value known at the present stage of the solution process, and letxi+1denote the iterate just obtained. Let us assume thati>1, and the iteration(i-1)→iwas non-critical. As shown in Fábián (2012), the following simple rule can be used to decide whether the second-stage problems need to solved in the current iteration:(7)cTxi+1+q̃(xi+1)⩾κcTxi+1+f̃(xi+1)+(1-κ)D‾,whereκis a parameter such that0<κ<1-λholds with the level parameter in Algorithm 1. – Simply put, (7) means that the disaggregate model function value is significantly higher than the aggregate one, as evaluated at the new iterate.If (7) holds, then the aggregate model function is updated by adding a linear support function ofq̃(x)atxi+1. This can be constructed without solving second-stage problems. The above procedure resultsAlgorithm 3A decomposition method for two-stage problems based on Algorithm 1.3.0 Parameter setting.Set the stopping tolerance∊>0.Set the level parameterλ(0<λ<1).Set the descent target parameterκsuch that0<κ<1-λ.3.1 Bundle initialization.LetU∼s=∅(s=1,…,S), and letU∼=∅Leti=1(iteration counter).Find a starting pointx1∈X.Solve the dual recourse problemsDs(x1)(s=1,…,S);letus(s=1,…,S)be the respective optimal basic solutions.AddustoU∼s(s=1,…,S), updatingq̃; and add(u1,…,uS)toU∼, updatingf̃.3.2 Near-optimality check.LetD‾=min1⩽j⩽i{cTxj+q(xj)}. LetD̲=minx∈X{cTx+f̃(x)}.LetΔ=D‾-D̲. IfΔ<∊then near-optimal solution found, stop.3.3 Finding a new iterate.Letxi+1be the projection ofxiontox∈X|cTx+f̃(x)⩽D̲+λΔ.3.4 Bundle update.If the iteration(i-1)→iis non-critical and (7) holds, thenConstruct a support function ofq̃(x)atxi+1,in the form∑s=1SpsuˆsT(hs-Tsx), withuˆs∈U∼s.Adduˆ1,…,u^Sto the setU∼, updatingf̃.OtherwiseSolve the dual recourse problemsDs(xi+1)(s=1,…,S);letus(s=1,…,S)be the respective optimal basic solutions.AddustoU∼s(s=1,…,S)updatingq̃; and add(u1,…,uS)toU∼, updatingf̃.Incrementi, and repeat from step 3.2.

@&#CONCLUSIONS@&#
