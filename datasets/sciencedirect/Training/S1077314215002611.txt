@&#MAIN-TITLE@&#
Robust traffic lights detection on mobile devices for pedestrians with visual impairment

@&#HIGHLIGHTS@&#
TL–recognizer detects traffic lights from a mobile device camera.Robust method for unsupervised image acquisition and segmentation.Robust solution: traffic lights are clearly visible in different light conditions.Solution is reliable: precision 1 and recall 0.8 in different light conditions.Solution is efficient: computation time ∼100 ms on a Nexus 5.

@&#KEYPHRASES@&#
Assistive technologies,Computer vision,Visual impairment,Traffic lights,Mobile devices,

@&#ABSTRACT@&#
Independent mobility involves a number of challenges for people with visual impairment or blindness. In particular, in many countries the majority of traffic lights are still not equipped with acoustic signals. Recognizing traffic lights through the analysis of images acquired by a mobile device camera is a viable solution already experimented in scientific literature. However, there is a major issue: the recognition techniques should be robust under different illumination conditions.This contribution addresses the above problem with an effective solution: besides image processing and recognition, it proposes a robust setup for image capture that makes it possible to acquire clearly visible traffic light images regardless of daylight variability due to time and weather. The proposed recognition technique that adopts this approach is reliable (full precision and high recall), robust (works in different illumination conditions) and efficient (it can run several times a second on commercial smartphones). The experimental evaluation conducted with visual impaired subjects shows that the technique is also practical in supporting road crossing.

@&#INTRODUCTION@&#
Most mobile devices are accessible to people with visual impairment or blindness (VIB)11In case the reader is unfamiliar with accessibility tools for people with VIB, a short introduction video is available at http://goo.gl/mEI6Uz.. This makes it possible to use these devices as platforms for the development of assistive technologies. Indeed, applications specifically designed for people with VIB are already available in online stores. For example, iMove supports independent mobility in urban environment by “reading aloud” the current address and nearby points of interest22At the time of writing, iMove is available for free download from AppStore: https://itunes.apple.com/en/app/imove/id593874954?mt=8.. Other solutions proposed in the scientific literature adopt computer vision techniques to extract contextual information from the images acquired through the device camera. In particular, this paper focuses on the problem of recognizing traffic lights with the aim of supporting a user with VIB in safely crossing a road.A number of solutions have been proposed in the scientific literature to recognize traffic lights. Existing solutions have a common problem: they use images acquired through the device camera with automatic exposure. With this approach, in conditions of low ambient light (e.g., at night) traffic lights result overexposed (see Fig. 1) while in conditions of high ambient light (e.g., direct sunlight) traffic lights are underexposed (see Fig. 2).This paper presents TL-recognizer, a traffic light recognition system that solves the above problem with a robust image acquisition method, designed to enhance the subsequent recognition process. Experimental results show that TL-recognizer is reliable (full precision and high recall) and robust (works in different illumination conditions). TL-recognizer has also been optimized for efficiency, as it can run several times a second on commercial smartphones. The evaluation conducted on subjects with VIB confirms that TL-recognizer is a practical solution.This paper isorganized as follows: Section 2 discusses the related work and defines the objectives of this contribution. The basic acquisition and recognition technique is presented in Section 3, while improvements are described in Section 4. Section 5 reports the results of the extensive experimental evaluation and finally Section 6 concludes the paper.Independent mobility is a challenge for people with sight impairments, in particular for what concerns crossing a road at a traffic light. A solution to this problem consists in the use of acoustic traffic lights. There are many different models of acoustic traffic lights. For example, in Italy, there are acoustic traffic lights that produce sound on demand by pushing a button placed on the pole. The sound signals to the person with VIB when the light is green. In Germany, there are models that always produce a sound when the light is green (no button has to be pushed) and they adapt the intensity of the sound according to the background noise.Nonetheless, as reported by many associations for blind and visually impaired persons, in most industrial countries (e.g., Italy, Austria, France, Germany, etc.), acoustic traffic lights are not ubiquitous; they are present in some urban areas but may be absent in small towns. Furthermore, acoustic traffic lights are not always working properly because damages often take a long time to be reported and fixed. The situation can be even worse in developing countries.

@&#CONCLUSIONS@&#
This paper presents TL-recognizer, a system to recognize pedestrian traffic lights aimed at supporting people with visual impairments. The proposed technique, in addition to the pure computer vision algorithms, implements a robust method to acquire images with proper exposure. The aim is to guarantee robust recognition in different illumination conditions. Experimental results show that TL-recognizer actually achieves this objective and is also efficient, as it can run several times a second on existing smartphones. Positive results were also obtained with a preliminary evaluation conducted on subjects with VIB: they were able to detect traffic lights in different illumination conditions.In future work it would be interesting to integrate TL-recognizer with a video tracking system, possibly based on the use of accelerometer and gyroscope. Also, user interaction should be carefully studied, with the aim of providing all the required information without distracting the user from its surrounding environment. The design of effective user interfaces will become even more challenging if TL-recognizer is integrated with other solutions that collect and convey to the user contextual information, for example, the current address or the presence of pedestrian crossings.Regarding exposure robustness, improvements could be derived from the adoption of HDR techniques to extend the acquisition dynamic range. In this case tests should be performed to verify the trade-off between reliability gains and computational costs.In order to ease the adoption of the proposed technique in different countries, a (semi) automated technique can be implemented to tune the parameters. This could be possibly based on a learning technique that gradually tunes the parameters in order to adapt to different contexts.An effort will also be devoted to the development of a commercial product based on TL-recognizer. Indeed, it could be possible to integrate this software with iMove, a commercial application that supports orientation of people with VIB developed by EveryWare Technologies. This will require tuning the system in order to detect pedestrian traffic lights in countries other than Italy. Also, in the near future it will be possible to implement TL-recognizer as an application for wearable devices (e.g., glasses). This will solve one of the main design issues: the fact that the user needs to hold the device in one hand.The notation used in the proof refers to Figs. A.22and A.23.ProofThe ground is approximated to an infinite plane. Thus, line l, which points from the device camera to the horizon, is parallel to the ground plane and angleFDP^is π/2.We define h through its angle θ and a point P where h passes. The general form is:(A.1)sin(θ)x+cos(θ)y+(sin(θ)Px+cos(θ)Py)=0We now show how to compute θ and P.Consider Fig. A.22. Let P be the point where the image plane intersects line l. Thus, point P lies on the horizon line h and P is inside the image. Also, since point D is the device, segmentDC¯is perpendicular toCP¯. Hence PCD is a right triangle. Since CD is the focal distance f and angle PDC is the device pitch angle ρ, the distance (in pixel) between the image center C and point P isd=f·tan(ρ).In the image plane, the device roll θ is the inclination of the device’s x axis with respect to the ground plane. Since the horizon line h is parallel to the ground plane, θ is also the inclination of the horizon in the image. Consider Fig. A.23. Let Q be the projection of C on the line parallel to the x axis (in the device reference system) that passes through P. SinceCPQ^+θ=π/2,it follows thatPCQ^=θ. Since the distance d is known, then the distance between point P and point C along the x axis isdx=PQ¯=d·sin(θ). Analogously, the distance between point P and point C along the y axis isdy=CQ¯=d·cos(θ). Thus, the coordinates of point P areP=<Cx−sin(θ)d,Cy−cos(θ)d>.Finally, substituting d and P in Eq. A.1 we obtain:(A.2)sin(θ)x−cos(θ)y−sin(θ)(Cx+tan(ρ)sin(θ)f)+cos(θ)(Cy+tan(ρ)cos(θ)f)=0□To ease the reading of the proof, please refer to Fig. 13. Note that, in the figure, points B and T are above point C. Since d1 is defined as the directed vertical distances between C and B, in case B is below C, the value of d1 is negative. The same holds for d2. Under this consideration, it is easily seen that the following proof holds when both B and T are below C and also when B is below C and T is above C.ProofSincedh=DA,by considering triangle DAG, it holds that(A.3)dh=DA=GD·cos(GDA^)Thesis easily follows by showing that(A.4)GD=lh·sin(π/2−arctan(d2f)−ρ)sin(arctan(d2f)−arctan(d1f))=lh·cos(arctan(d2f)+ρ)sin(arctan(d2f)−arctan(d1f))and(A.5)GDA^=arctan(d1f)+ρFor what concerns GD, by considering triangle GED we have:(A.6)GD=EG·sin(GED^)sin(EDG^)EG is the lens height lhgiven in input.EDG^is equal toTDB^that, in turn, is equal toTDC^−BDC^. Since TDC and BDC are right triangles, it holds thatTDC^=arctan(CTCD)andBDC^=arctan(BCDC)whereCT=d2,BC=d1andCD=f. Hence:(A.7)EDG^=TDB^=arctan(d2f)−arctan(d1f)For what concernsGED^,by considering right triangle EDA we have that:(A.8)GED^=AED^=π/2−EDA^=π/2−(EDI^+IDA^)whereEDI^=TDC^andIDA^is the device pitch ρ. So, it follows:(A.9)GED^=AED^=π/2−arctan(d2f)−ρFor what concernsGDA^,it is equal toGDI^+IDA^whereGDI^=BDC^andIDA^is the device pitch ρ. Hence:(A.10)GDA^=arctan(d1f)+ρFinally, we show the value ofdv=AG. Consider the right triangle ADG whereAD=dhandGDA^is known (see above). Consequently,(A.11)dv=AG=AD·tan(GDA^)=dh·tan(arctan(d1f)+ρ)□Notation used in the following proof refers to Fig. 14.ProofConsider right triangle ADB:ADB^=arctan(AB/AD)whereAB=wc/2andAD=f.Now consider right triangle CDE:CE=CD·tan(CDE^)whereCD=dandCDE^=ADB^. Hence:(A.12)EF=2·CE=d·wcfFinally, consider right triangle FEG:(A.13)w=GF=EF/cos(α)=d·wcf·cos(α)□