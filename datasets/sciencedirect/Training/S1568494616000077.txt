@&#MAIN-TITLE@&#
A sensitivity analysis method for driving the Artificial Bee Colony algorithm's search process

@&#HIGHLIGHTS@&#
The Artificial Bee Colony algorithm is improved with a sensitivity analysis method.Morris’ OAT method detects high influential dimensions.Morris’ OAT method drives the neighborhood search of the ABC algorithm.ABC-Morris outperforms the ABC algorithm on classical optimization functions.

@&#KEYPHRASES@&#
Metaheuristic,Optimization,Artificial Bee Colony,Sensitivity analysis,Morris’ method,

@&#ABSTRACT@&#
In this paper, we improve D. Karaboga's Artificial Bee Colony (ABC) optimization algorithm, by using the sensitivity analysis method described by Morris. Many improvements of the ABC algorithm have been made, with effective results. In this paper, we propose a new approach of random selection in neighborhood search. As the algorithm is running, we apply a sensitivity analysis method, Morris’ OAT (One-At-Time) method, to orientate the random choice selection of a dimension to shift. Morris’ method detects which dimensions have a high influence on the objective function result and promotes the search following these dimensions. The result of this analysis drives the ABC algorithm towards significant dimensions of the search space to improve the discovery of the global optimum. We also demonstrate that this method is fruitful for more recent improvements of ABC algorithm, such as GABC, MeABC and qABC.

@&#INTRODUCTION@&#
For the past twenty years, social interaction and swarm intelligence based algorithms have caused a strong research interest in solving optimization problems. Instead of being based on best individuals selection, they focused on low intelligence but very communicative individuals, who collaborate and share information. These behaviors have been described through ant colony, particle swarm, bacterial foraging or Artificial Bee Colony optimization. In 2005, D. Karaboga introduced the Artificial Bee Colony algorithm [1], based on the food foraging behavior of honey bees. In this algorithm, a food source represents a solution. The quality of this food source, its nectar amount, is represented by its fitness. The algorithm describes how three categories of honey bees (employed bees, onlooker bees and scout bees) explore the search space around the food sources, and their strategies of abandoning the exhausted ones to discover new ones. The algorithm is detailed more precisely later. ABC is a very simple algorithm, easy to implement and with few control parameters. The ABC algorithm has proven its efficiency with respect to other population-based or swarm intelligence algorithms such as genetic algorithms, differential evolution or particle swarm optimization [2]. These advantages make ABC the most studied and improved algorithm of recent years, among all bee-behavior inspired algorithms.Previous works about the ABC algorithm focused on solving the balance between exploration and exploitation or addressing issues like premature convergence. These issues are similar in all metaheuristics, the strategies differ in each case. In the ABC algorithm, these strategies are few and simple: the neighborhood search, the quantification of how promising a solution is (exploitation) and the search of new solutions (exploration). These issues have been improved by several techniques.Exploitation has been enhanced by introducing modified fitness function and food source selection probability function (during onlooker bee phase) [3,4]. The neighborhood search has been improved through the addition of a best-so-far solution [5,6] or global best solution [7–10] which modify the movement of the exploration. Kiran et al., in [11], propose several search techniques of generating a new solution in a variable search strategy adapted to the problem.Regarding the exploration of the search space, despite the fact that ABC algorithm has a good exploration capability [8,12], some researches focused on the population initialization to maximize the distribution in the search space [10,13]. In [14], Monica et al. implement an efficient choice of a new food source during scout bee phase with space-filling design, using Sobol sequence. Bin and Hua Qian aimed at getting out of a local optimum, enhancing the diversity of the population by adapting the number of food sources [15]. In [16], a use of a modification rate is introduced, to offset more than one dimension, in [17], the authors propose to provide better convergence in updating a directional information.In [18], the authors introduce a scaling factor based on the golden ratio to manage exploration and exploitation behaviors. Akay and Karaboga [16], Aderhold et al. [7] studied the influence of control parameters for the algorithm behavior.ABC is also a suitable algorithm for hybridization as in [19,20] with memetic search, foraging the best solution with a simulated annealing algorithm in [21], or with the introduction of genetic algorithm or differential evolution algorithm operations as in [10,15,22,23]. Li et al., as well as Zhu and Kwong, introduce an inertia weight and an acceleration rate, inspired from particle swarm optimization algorithm behavior during the search process [24,8].In the ABC algorithm, as detailed in Section 3, there are several random-based steps (commonly present in many metaheuristics), such as initializing the population, discovering a new food source, choosing a food source to explore, choosing a dimension to offset. As we have seen, many works deal with the first three. In many real world problems, when the objective function involves many dimensions, only a few have a real influence on its behavior. Our idea is that all dimensions of a solution may not have an equivalent impact on the objective function. The ABC algorithm would consume many useless objective function evaluations in exploring non valuable dimensions. A new approach to improve the ABC algorithm is to focus on the dimension selection step. A new solution is generated using a random function to choose, from the original solution, a dimension to offset. So it would be suitable, for both exploration and exploitation processes, if the algorithm could explore in priority the dimensions which have a high impact on the objective function. Our goal is to orientate the choice of these dimensions according to their impact (their effect) on the objective function evaluation.Sensitivity analysis aims at determining which parameters of a model have a significant influence on the model. Saltelli, in [25], gives this definition: “Sensitivity Analysis is the study of how the uncertainty in the output of a model (numerical or otherwise) can be apportioned to different sources of uncertainty in the model inputs”. The idea is to study how a variation of an input parameter has an effect on the model output. It is commonly used in simulation, modeling, in various applications such as atmospheric chemistry, population dynamics, macroeconomic modeling, traffic simulation.In the context of continuous optimization, the referenced model is the objective function f, an input is a decision vector X=(x1, …, xn), an input parameter xiis a decision variable (single dimension) of this decision vector and the output is the image of the input through the function f(X). When the objective function is known, we can assume that there is little uncertainty concerning the output and most of the dimension weights are estimated. Sensitivity analysis’ role is here to guide a random-based search, make it focus mainly on the most valuable dimensions. Combining a sensitivity analysis algorithm with a metaheuristic helps to prioritize the unidimensional direction exploration.In [25,26], the authors list several approaches of sensitivity analysis (SA), local SA (OAT), screening (method of elementary effects) or global (variance-based) SA. The search of a new solution in the ABC algorithm is very similar to the space exploration of a sensitivity analysis called Morris’ method [27]. They both analyze, dimension by dimension, the influence of an offset on an output.We propose to include a screening method in the ABC algorithm to improve its performance (efficiency in the exploration process, focusing on dimensions that are worth exploring). This paper describes how sensitivity analysis can fit with the ABC algorithm. In Section 2, we describe Morris’ method, and then, after recalling the ABC algorithm in Section 3, we explain why this method is suitable for this algorithm. In Section 4, we present our improved algorithm, its advantages and limits. We present, in Section 5, our statistical test protocol and the continuous optimization functions we used. We discuss the results and conclude in Section 6.Screening methods aim at identifying the effects of input parameters in a model. It permits the efficient exploration of a multidimensional space.Morris developed a method, the elementary effect method [27], which has been improved [28,29] for screening the influence of parameters from a complex model. It can detect influential parameters with linear or non linear effects and interactions between the parameters. Starting from several evaluations of a function f, sensitivity analysis allows to identify which input parameter xi, in an input X=(x1, …, xD), has a significant influence on f.For an input X, the elementary effect of the ith input parameter is calculated by (1), where Δ is an offset:(1)EEi(X)=f(x1,…,xi+Δ,…,xD)−f(X)ΔThe method design is to build trajectories for a sample of inputs. A trajectory is composed of successive offsets, one input parameter at time (OAT). It means that, at each step, the offset shifts the input in one dimension only (as described in Fig. 1). This figure is an example, in a three-dimensional process. It iterates three times: at step i, an offset Δiis applied on an input parameter xiof an input X, producing another input X′. Then it loops to step i+1, replacing X by X′.The elementary effect described in (1) gives a local evaluation of the sensitivity (only for the input parameter xiof the input X).The global impact for all input parameters is given by generating a trajectory for all inputs. It means randomly choosing r inputs in the search space (Xj, j∈1, …, r). For each input parameter of all r inputs, generate a new Δ value, evaluate each effect EEi, i∈{1, …, D} with (1). Then the sensitivity of each input parameter can be evaluated by the absolute means (introduced in [29]) of the elementary effects EEi, as in (2):(2)μi*=1r∑j=1r|EEi(Xj)|A low value forμi*means that the ith input parameter has a low effect on the model, a high value means that the input parameter has a high effect on the model. However, this information is not sufficient: if we consider only the absolute means, we might miss some important information about non linear behavior of some input parameters. The study has to include the standard deviation of each input parameter (3).(3)σi=1r∑j=1r(EEi(Xj)−μi)2The standard deviation, σ gives more information about possible effects of input parameters. There are four cases depending on the couple (μ*, σ) values (Fig. 2illustrates all these interactions):•low value for μ* and σ implies input parameters with low effects on the model,low value for σ and high for μ* implies input parameters with high linear effects on the model,low value for μ* and high for σ implies input parameters with high non linear effects on the model,high value for both μ* and σ implies input parameters with high non linear effects on the model or strong interactions between them.The ABC algorithm has been introduced and developed by Karaboga and Basturk since 2005 [1,30] for numerical optimization problems. This algorithm is a population based algorithm, inspired by the honey bees foraging behavior, in which a candidate solution for the optimization problem is represented by the position of a food source. Each food source has a nectar amount, it characterizes the quality (or fitness) of the solution.There are three kinds of bees flying over the search space to search for food sources: employed bees, onlooker bees and scout bees. Employed bees fly in the neighborhood of each food source to find a better food source. They share the quality of the location with the onlooker bees. Onlooker bees forage close to the best food sources. When a food source seems to be sufficiently visited, scout bees explore the search space to discover new food sources. Algorithm 1 summarizes this process.There are as many employed and onlooker bees as food sources, while there is usually one scout bee. A food source is a D-dimensional vector, D is the problem dimension.First, the algorithm randomly initializes a population of SN food sources using (4), Xmin and Xmax are vectors filled with min and max values of each dimension, j∈{1, …, SN}, i∈{1, …, D}.(4)xij=xminij+rand[0,1](xmaxij−xminij)Algorithm 1ABC algorithm.1:Initialize food sources2:while not end criteria do3:send employed bees to all food sources4:set food sources’ fitness5:send onlooker bees to good food sources6:send scout bee to explore new food source7:save best food source8:end whileEach employed bee produces a new food source, in the neighborhood of an existing food source according to (5). For the jth employed bee (j∈{1, …, SN}), i∈{1, 2, …, D}, a randomly chosen dimension, ϕijis a uniformly distributed real value in [−1;1] and k∈{1, …, SN}, a randomly chosen food source (k≠j). If the new produced solution Njis better than Xj, it replaces the old one; if not, a counter variable of visits is increased.(5)nij=xij+ϕij(xij−xik)An onlooker bee will visit a food source with a good nectar amount. This information is shared by the employed bees using a fitness value (6). The probability of a food source j being selected, is computed using the fitness value described in (7). The production and replacement of a new solution follow the same processes as with employed bees phase (5).(6)fitj=1f(Xj)+1,f(Xj)≥0,1+|f(Xj)|,f(Xj)<0.(7)pj=fitj∑n=1SNfitnEach time a food source is not improved during employed and onlooker bees phases, a number of visits is increased for this food source. The scout bee gets the food source with the higher value of the counter variable and checks if that value is greater than a maximum number of visits value. If so, it replaces it by generating a new food source according to (4).An improved ABC algorithm, called ABC-Morris is presented. A Morris’ method is included in the original algorithm in order to analyze which dimensions are worth exploring, during its processing. It self-adjusts to get better convergence with the global optimum. We use Morris’ method to evaluate the influence of a dimension offset in the objective function computation. Then, we associate an influence rate to each dimension. This rate allows to prioritize the random selection of different dimensions.The classic ABC algorithm modifies one dimension, randomly chosen, of the current solution during both employed and onlooker bees phases. The offset of the solution value for one dimension, defined by (5), is similar to Morris’ method of elementary effects (1). This behavior is the same as in Morris’ algorithm to evaluate the influence of input parameters. Our idea is to use this offset in the elementary effects computation. As long as the ABC-Morris algorithm is running, we can use the evaluations of the objective function to iteratively readjust the influence of the chosen dimension's offset in the elementary effects matrix.In the original ABC algorithm, for a given solution Xj, a new neighbor Njis produced using (5). This neighbor is evaluated through the objective function, giving f(Nj). So, if we considerΔ=ϕij(xij−xik), we can easily match it with (1) and get new μ*(2) and σ(3) values.While the ABC-Morris algorithm is running, the influence of each dimension gets more accurate. We replace the random choice of a dimension i, in (5), by a oriented smart choice. In order to select an influential dimension, we must take into account the four cases described in Fig. 2. We can notice that high impact input parameters are far from the origin. So we chose the euclidean distance-to-origin function as a numerical function to evaluate the influence of an input parameter on the model (8).Then, we determine influence rates (or weights) of each dimension for the objective function. We calculate a percentage of influence for each dimension, using the distance function applied to each input parameter, according to (9):(8)∀i∈{1,…,D},d(μi*,σi)=μi*2+σi2(9)∀i∈{1,…,D},infli=d(μi*,σi)∑j=1Dd(μj*,σj)The choice of the dimension to offset will not be a uniformly distributed random value in {1, …, D}, but is sampled according to distribution defined in (9). We used the inverse transform sampling method to replace this original random choice of a dimension. This method uses the vector of cumulative influence rates (10) as described in Algorithm 2.(10)∀i∈{1,…,D},inflcumul[i]=∑j=1iinfljAlgorithm 2Dimension selection algorithm.1:i←12:// random number for dim. selection3:rndm←random(0,1)4:// iterate the influence vector to select dim.5:while inflcumul[i]<rndm do6:i←i+17:end while8:return i // return selected dim.Before the algorithm is looping, we create a matrix of elementary effects with SN rows (population size) and D columns (dimension). First, all elementary effect values are initialized to 1. All μ* will be equal to 1 and σ will be equal to 0. According to (8) and (9), the distances are equal to 1 and each factor has the same influence: 1/D. So they have the same probability of being chosen. The ABC algorithm is modified as shown in Algorithms 3 and 4.Algorithm 3ABC-Morris algorithm.1:Initialize food sources2:Initialize elementary effects matrix3:Initializeμ*andσvectors4:while not end criteria do5:send employed bees for Morris-foraging (see in Algorithm 4)6:updateμ*andσvectors7:set food sources’ fitness8:send onlooker bees to good food sources for Morris-foraging9:updateμ*andσvectors10:send scout bee to explore new food source11:save best food source12:end whileAlgorithm 4Morris-foraging algorithm.1:j←foodSourceSelectionAlgorithm()2:i←dimensionSelectionAlgorithm() (see Algorithm 2)3:Xk←randomFoodSource()4:offset←ϕij(xij−xik)5:Nij←Xij+offset6:solutionSelectionProcess(Xj, Nj)7:updateElementaryEffectsMatrix(i, j, offset)Morris’ method maximizes the exploration of the search space by testing small but also large values for Δ. In our algorithm, Δ values are chosen according toΔ=ϕij(xij−xik). The convergence of the ABC algorithm leads to small values for(xij−xik)and consequently, Δ≡0 and f(X+Δ)≡f(X). According to (1) and (2), we face two cases.On the one hand, a high influence value means that an optimum value might be in [X, X+Δ], so it is worth exploring this dimension. On the other hand, for the ith dimension, all the effects are close to zero, this dimension's influence (μi*) will be a low value and it will be less explored by the algorithm. It means that we have reached an optimum value for the ith dimension. This is an unusual behavior of Morris’ method which reinforces the interest of our algorithm. When it has converged on the most influential dimensions, their influence value will be lower than those of the less explored dimensions. The algorithm will then explore these former less influential dimensions and get closer to the global optimum.The advantage is that the algorithm will explore, in priority, promising dimensions but will give a chance to explore dimensions with lower impact.To evaluate the influence of Morris’ method on ABC algorithm and on more recent versions of this algorithm, we retained the test protocol of CEC 2013 competition [31]. The study focuses on two aspects.The first one is to evaluate if integrating Morris’ method in the ABC algorithm (or in a more recent version of this algorithm) gives similar results than usual algorithms when all dimensions are active. Morris version of four ABC algorithms are compared with NBIPOP-ACMA-ES [32], the best algorithm of the CEC 2013 competition, PSO (as used in [12]) and I-ABC [24] on the CEC 2013 benchmark (Table 1).Then, the second objective is to test the efficiency of adapting Morris's method with an algorithm when several dimensions have no impact on the objective function. There is a comparison between the standard version of each algorithm and their Morris version, with respectively 75%, 50%, 25% and 10% of active dimensions (Tables 5–8).Furthermore, as we will explain after, to highlight the interest of Morris’ method on non-rotated function, a third set of tests (Tables 9–11) is studied.We used the 28 functions of the CEC 2013 benchmark [31] to test our algorithm. Functions f1 to f5 are unimodal, f6 to f20 are multimodal and functions f21 to f28 are composition functions. All the functions are rotated, excepted functions f5, f11, f14, f17 and f22.The method is tested on four state of the art ABC algorithms: ABC, GABC [8], MeABC [33] and qABC [12]. Morris’ method is added to each of them to test its efficiency. We chose these algorithms because of their different search strategies. GABC includes a PSO-like coefficient to include the global best solution in the offset of the current solution. MeABC includes a memetic search in the algorithm to improve the exploitation around the best solution. qABC algorithm makes a difference between employed bees and onlooker bees behaviors. The offset equation of employed bees phase remains the same as in the original algorithm, while the onlooker bees’ one includes a neighborhood distance to select a local best solution for the offset.Recent papers [34–36] highlight that the mean and standard deviation can be improper for statistical comparison since the probability distribution of the results is far from being normal but it is asymmetric with a long tail. So, even if an algorithm performs well, a few number of statistical accidents (the end of the tail) considerably increase the mean and the standard deviation. The rank statistics are not affected by extreme values. For this reason, we used the median and the Wilcoxon statistical test (instead of the usual Student) in our comparison. The Wilcoxon test is used in determining if the sampling leads to the rejection of the following null hypothesis according to a significant level,(11)H0:thetwoalgorithmsareequivalentH1:thereisasignificantdifferencebetweenthealgorithmsThe test result is the p-value (Tables 2–4 and A.12) and it is commonly assumed that if:(12)p<0.01:thereisaverystrongpresumptionagainstH0p∈[0.01,0.05[:thereisastrongpresumptionagainstH0p∈[0.05,0.1[:thereisaweakpresumptionagainstH0p≥0.1:thereisnopresumptionagainstH0The aim is to determine if there is a significant difference between the standard version and the Morris version of each algorithm as the number of active dimensions decreases.

@&#CONCLUSIONS@&#
