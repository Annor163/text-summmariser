@&#MAIN-TITLE@&#
Preprocessing for elderly speech recognition of smart devices

@&#HIGHLIGHTS@&#
Preprocessed elderly voice signals were tested with an android smart phone.Speech recognition accuracy increased to 1.5% by increasing the speech rate.Speech recognition accuracy increased to 4.2% by eliminating intersyllabic pauses.Speech recognition accuracy increased to 6% by boosting formant frequency bands.After all the preprocessing, 12% increase in the recognition accuracy was achieved.

@&#KEYPHRASES@&#
Elderly voice interface,Speech recognition,Aging society,

@&#ABSTRACT@&#
Due to the increasing aging population in modern society and to the proliferation of smart devices, there is a need to enhance speech recognition among smart devices in order to make information easily accessible to the elderly as it is to the younger population. In general, speech recognition systems are optimized to an average adult's voice and tend to exhibit a lower accuracy rate when recognizing an elderly person's voice, due to the effects of speech articulation and speaking style. Additional costs are bound to be incurred when adding modifications to current speech recognitions systems for better speech recognition among elderly users. Thus, using a preprocessing application on a smart device can not only deliver better speech recognition but also substantially reduce any added costs. Audio samples of 50 words uttered by 80 elderly and young adults were collected and comparatively analyzed. The speech patterns of the elderly have a slower speech rate with longer inter-syllabic silence length and slightly lower speech intelligibility. The speech recognition rate for elderly adults could be improved by means of increasing the speech rate, adding a 1.5% increase in accuracy, eliminating silence periods, adding another 4.2% increase in accuracy, and boosting the energy of the formant frequency bands for a 6% boost in accuracy. After all the preprocessing, a 12% increase in the accuracy of elderly speech recognition was achieved. Through this study, we show that speech recognition of elderly voices can be improved through modifying specific aspects of differences in speech articulation and speaking style. In the future, we will conduct studies on methods that can precisely measure and adjust speech rate and find additional factors that impact intelligibility.

@&#INTRODUCTION@&#
Demographics among developed nations around the world are shifting towards an aging population; for instance, the proportion of the elderly population in South Korea has reached unprecedented levels (Korea National Statistical Office, 2006). As the market penetration of information technology has spread widely, smart devices are not only used by young people for prolonged periods but also by the elderly with high usage indoors and outdoors during the entire day. Advanced interface methods supported by smart devices are largely conventional interfaces that depend on touch, voice, and motion commands. But touch-sensitive technology and finger-recognition motions used on smart devices are difficult to use among the elderly due to deterioration of eyesight and sense of touch. We therefore deemed that a speech-recognition interface in a smart device would increase its ease-of-use for the elderly. This interface would especially be utilized by senior citizens in emergency situations including a traumatic event that renders them physically limited.Voice interfaces embedded in existing smart devices use speech recognition methods that are optimized to the speech and speaking patterns of average young and middle-aged adults and therefore, show a decline in performance when there is a deviation in the speech rate. Furthermore, tongue thickness, range of movement and duration of movement decreases as humans reach old age (Bennett et al., 2007) and due to this, feedback becomes slower when articulating speech sounds, in which it can partially affect articulation functions (Sonies, 1987). In other words, among the elderly, speech rate becomes slower, silence lengths are longer and speech becomes less precise (Kahane, 1981).This study aims to investigate the factors that impair speaking ability among the elderly by comparing speech patterns between the elderly and young adults while also identifying which areas of speech that can be normalized or adapted for improving speech recognition. After the analysis of elderly speech, a preprocessing method was conducted to modify voice signals from elderly adults, which were then tested to determine if accuracy was improved in speech recognition without any modification of an automatic speech recognition system installed in an existing smart phone (Fig. 1). Test results showed that voice signals from the elderly increased the error rates among existing speech recognition systems, but accuracy showed marked improvement when the speech rate, silence lengths, and formant of the voice signals were preprocessed. The study results show promise in offering better accessibility to smart devices among the elderly and the speech-impaired who are left behind in the information age due to the lack of modifications in automatic speech recognition systems installed in existing smart phones.In Section 2, speech patterns of the elderly and related studies on speech recognition are summarized, and in Section 3, the methods used to analyze and measure characteristics of speech patterns among the elderly are described. Section 4 goes into the comparative analysis of speech patterns between young adults and the elderly in order to identify differences in speech patterns. The speech signals are subsequently preprocessed for these differences and tested with existing automatic speech recognition systems installed on a smart device, and the results are analyzed. The conclusion of this paper is given in Section 5.In this section, we provide a brief review of studies on elderly speech based on prior technological research in the field of automatic speech recognition as well as research in fields such as applied linguistics and biomechanics. When humans reach old age, their eyesight, cognitive processes, sensory abilities, and physical functions that allow them to communicate verbally decline in comparison to younger people. The occipital lobe, which controls muscle movement and the lungs, shows a marked decline while the mucous membrane in the vocal cords becomes thinner and keratinized. In addition, the thyroid cartilage in men over 65 becomes nearly completely ossified, while, in women, ossification is limited to the lower part of the cartilage (Lee, 2011). Due to these aging factors, people over the age of 65 exhibited a significantly slower articulation speed and speech rates compared to that of the younger generation, and also have a change in resonance properties of the larynx which creates a fainter pitch (Kim, 2003). In addition, there is an increase in fluency breaks, resulting in longer and more frequent silence lengths (Manning and Monte, 1981). Ryan empirically proved that 70–80 year old adults exhibited a significant increase in mean vocal intensity for reading and speaking conditions and a significant decrease in the overall reading rate and mean sentence reading rate compared to younger adults (Ryan, 1972). In addition, 60–70 year old adults exhibited a significant decrease in mean sentence reading rate compared to 40–50 year old adults.Biological changes have an impact on voice signals formed from vocalization. Fundamental frequency (F0) varies with gender and age. The average fundamental frequency of elderly males is 121.8Hz, while the average of young and middle-aged males is 118.7Hz. Elderly females have an average fundamental frequency of 174.8Hz compared to that of young and middle-aged females which is about 224.82Hz. The formant frequency that appears when making vowel sounds not only expresses the characteristics of the vowel but also includes most of the vocal energy, which plays an important role in measuring the quality of phonetic information. The average first formant (F1) for elderly males and middle-aged males is 688.2Hz and 703.17Hz, respectively, with a standard deviation of 50.02 for the former and 30.07 for the latter. For elderly females and middle-aged women, the average F1 is 765.01Hz and 827.79Hz respectively with a standard deviation of 40.05 for the former and 33.08 for the latter (Jin et al., 1997; Wilpon and Jacobsen, 1996; Kim and Ko, 2008). The study results show that those over 60 will undergo changes in key frequencies in their speech compared to a middle-aged adult. Moreover, the special energy distribution across frequencies is a key factor in speech recognition and is thus a topic of interest in this study.Speech intelligibility is the degree that a spoken utterance can be understood by the listener (Han, 2011). Intelligibility is determined according to various factors such as articulation, resonance, cadence, breathing, vocalization, environment noise, circumstance, the listener, and other factors. Ptacek and Sander had a test on the ability of 10 listeners to differentiate the voices of younger adults (under age 35) from older (over age 65) subjects on the basis of a prolonged vowel, a reading sample played backward, and a reading sample played forward. Listeners were able to differentiate the voices of younger adults from aged speakers with impressive accuracy under each of the three successive listening conditions of decreasing difficulty (Ptacek and Sander, 1966). Ultimately, this test proved the claim that people can tell the difference between the voice of an elderly person and that of a young adult through their sense of hearing. Based on Harnsberger's experiment, it became possible to increase the accuracy in identifying acoustic patterns by age with the key factors being F0 and speech rate. Therefore, a rough estimate of a person's age can be made based on his or her voice by using these two features (Harnsberger et al., 2008). However, acoustically, the variables related to intelligibility are the duration of the vowel sound, location of the F1 and second formants (F2), fricative sound, and noise duration of an affricate (Pyo and Shim, 2005).To reduce the degradation in speech recognition performance caused by variation in vocal tract shape among speakers, vocal tract length normalization (VTLN) is in use in almost all state-of-the-art Gaussian Mixture Model/Hidden Markov Model based automatic speech recognition systems. For speaker normalization, Lee and Rose estimated a linear frequency warping factor and warped frequency by modifying the filterbank in mel-frequency cepstrum feature analysis (Lee and Rose, 1998). This method stretched or compressed the frequency scale of the filters for frequency warping. Their results showed that frequency warping was consistently able to reduce word error rate by 20% even for very short utterances. Potamianos and Narayanan proposed a speaker normalization algorithm that combines frequency warping and model transformation is shown to reduce acoustic variability and significantly improve automatic speech recognition (ASR) performance for children speakers (Potamianos and Narayanan, 2003). They computed the average scaling factors between children and adult speech for all phonemes and then combined frequency warping and spectral shaping. The experimental results showed about 28.7% improvement for connected digit and sub-word recognition tasks. Elenius and Blomberg presented dynamic VTLN method in connected-digit recognition of children's speech using models trained on male adult speech (Elenius and Blomberg, 2010). This is to incorporate time varying speaker characteristic properties into the acoustic model. The word error rate was reduced by 10% relative to the conventional utterance-specific warping factor.By utilizing findings in the aforementioned prior research, it is possible to predict the relative differences between the speech of an elderly person and that of a young adult in terms of speech rate, silence length, fluency break, F0, F1, and F2. Variability in some of these factors is known to impact the accuracy of speech recognition systems. It is possible to counteract errors in speech recognition that arise when people speak rapidly by using cepstrum normalization (Richardson et al., 1999; Kwon, 2011; Kwon et al., 2013). Automatic speech recognition algorithms, which inherently have trouble with voice samples of a fast speaker, have seen a modest performance increase of 1.9% in terms of accuracy after modeling the changes of a person's rapid speaking rate (Zheng et al., 2000; Siegler and Stem, 1995). However, no study taking into account all of the aforementioned factors has been carried out yet nor has any correlation been made between speech recognition and changing speech patterns caused by aging. In this study, factors attributed to aging in speech patterning are considered as areas that can enhance the accuracy rate of speech recognition through preprocessing.The speech rate of the elderly is slower than the rate of young adults due to the aging of the mucous membrane, larynx, and pulmonary functions. Intelligibility of elderly speech tends to suffer as well. Thus, when analyzing the characteristics of vocalization among elderly adults, features such as speech rate, silence length, and formants of voice, which are measurable, are used for the analysis.Generally, the average phoneme rate and syllable rate can be utilized as a method for measuring the speaking rate. The formula below in Eq. (1) is used to find the speech rate for this study (Chu and Povey, 2010). Given an utterance i, composed of a sequence of Ni words: [wi,1, wi,2, wi,Ni]. j of Eq. (1) is the label number of words. After analyzing the difference in the average speech rate between the elderly and young adult, the resulting data can be used to modify the voice signal by artificially adjusting the speech rate.(1)F(i)=∑i=1Nit(wi,j)∑i=1Nin(wi,j)F(i)=speechratet(wi,j)=thedurationofawordn(wi,j)=thenumberofphonesinawordThe Synchronized Overlap-Add (SOLA) algorithm is one basic method used to adjust the speech rate according to measured ratio on a time-scale. As shown in Eq. (2), the overlapped window of the fixed length is used to extract the signal (x) that is derived from regular intervals (SA) (Hejna and Musicus, 1991; Kwon, 2012). The resulting window can be compressed, extended, or converted according to any given ratio and then be combined again. The segment (SA) cut from an input signal, which is then overlapped with segment (SS) can be expressed as the ratio α = SS/ SA, whereupon the signal will be converted accordingly. If α > 1, the conversion will be an extension, but if α < 1, the conversion will be a compression. The converted signals will overlap during the overlapped-add stage, and fine-tuning is needed to make the overlapped areas as identical as possible.(2)xm[n]=x[mSA+n]forn=0,…,W−10otherwiseEnd point detection (EPD) is an algorithm that can separate the silence components in human speech. EPD generally detects energy to determine the existence of spoken sounds in each analysis frame (typically 25ms). In order to enhance the accuracy in separating voice components, zero crossing rate (ZCR) is additionally used. After separating voiced sounds in speech, the silence regions and voiceless sounds are separated by using ZCR.Formant frequency can be detected from speech spectra by peak picking method (Wood, 2003). The peak picking algorithm uses the N-point FFT to apply a parabola interpolation and peak picking to a discrete number sampling in order to scan for the formant. The shape of the parabola is as follows:(3)y(λ)=aλ2+bλ+cIf y(0) defines the discrete peak value where y(−1) represents the left sample and y(1) represents the right sample, then the coefficient of the parabola that crosses all three points is as follows:(4)c=y(0)b=[y(1)−y(−1)]/2a=[y(1)+y(−1)]2−y(0)The differential calculated from dy(λ)/dλ=0 is the peak point, which makes λp=−b/2a the maximum value. If the discrete peak is located at np, the formant point and the interpolated formant's bandwidth value can be expressed as follows.(5)F=(np+λp)fs2NB=−b2−4a[c−0.5y(λp)]1/2fsaN

@&#CONCLUSIONS@&#
