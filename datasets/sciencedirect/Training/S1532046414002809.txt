@&#MAIN-TITLE@&#
Clustering clinical models from local electronic health records based on semantic similarity

@&#HIGHLIGHTS@&#
We propose a method for clustering clinical models based on SNOMED CT.Semantic similarity and aggregation techniques facilitate hierarchical clustering.We evaluate the method using templates from local electronic health record systems.Dendrograms provide an overview of semantic similarity of templates.The clustering method can be used to compare and summarize multiple clinical models.

@&#KEYPHRASES@&#
Computerized medical records,Semantics,SNOMED CT,Medical record linkage/standards,Medical record linkage/methods,Algorithms,

@&#ABSTRACT@&#
BackgroundClinical models in electronic health records are typically expressed as templates which support the multiple clinical workflows in which the system is used. The templates are often designed using local rather than standard information models and terminology, which hinders semantic interoperability. Semantic challenges can be solved by harmonizing and standardizing clinical models. However, methods supporting harmonization based on existing clinical models are lacking. One approach is to explore semantic similarity estimation as a basis of an analytical framework. Therefore, the aim of this study is to develop and apply methods for intrinsic similarity-estimation based analysis that can compare and give an overview of multiple clinical models.MethodFor a similarity estimate to be intrinsic it should be based on an established ontology, for which SNOMED CT was chosen. In this study, Lin similarity estimates and Sokal and Sneath similarity estimates were used together with two aggregation techniques (average and best-match-average respectively) resulting in a total of four methods. The similarity estimations are used to hierarchically cluster templates. The test material consists of templates from Danish and Swedish EHR systems. The test material was used to evaluate how the four different methods perform.Result and discussionThe best-match-average aggregation technique performed better in terms of clustering similar templates than the average aggregation technique. No difference could be seen in terms of the choice of similarity estimate in this study, but the finding may be different for other datasets. The dendrograms resulting from the hierarchical clustering gave an overview of the templates and a basis of further analysis.ConclusionHierarchical clustering of templates based on SNOMED CT and semantic similarity estimation with best-match-average aggregation technique can be used for comparison and summarization of multiple templates. Consequently, it can provide a valuable tool for harmonization and standardization of clinical models.

@&#INTRODUCTION@&#
Semantic interoperability is a highly desired characteristic of (EHRs). To this end, standardization of information models and terminologies is needed. However, going from local customizability to global standardization is a challenge, especially in terms of modeling and managing clinical models (CMs) because this is the place where local clinical requirements are expressed in computerized form. CM is a relatively new construct resulting from the fact that modern EHR architectures separate reference information models from clinical models, these are called two-level modeling approaches [1,2]. CMs define documentation structures used in clinical situations such as physical examination, nutrition screening or vital signs measurement, and for each clinical situation CMs can be bound to relevant terminology [3]. CMs are often referred to as either templates or archetypes or both. In this study, the word template is used in its common meaning as a structure intended for data entry for a specific clinical situation, i.e. defining the fields on the interface level not at the database level. Consequently, “template” does not refer to any standard such as openEHR or HL7, who have their own definitions of templates. A variety of CMs are needed to handle clinical documentation needs which make modeling and managing CMs complex. Getting an overview of the complexity requires insight, which can be gained by analyzing semantic similarities of existing templates.For example, a vital sign template at one hospital could contain pulse, blood pressure, temperature, oxygen saturation and respiration frequency, each being a text field where quantities as well as comments could be written. Another hospital could have a template where quantities, comments and protocol-related fields are kept separately. An example of a pulse excerpt is shown in Fig. 1. Manual comparison of the templates gives an idea about the semantic content of a vital signs template, and we can characterize the differences between the templates in natural language. Based on this analysis, we would be able to give guidance to hospitals that want to create new vital signs templates or suggest changes to existing templates that would support harmonization. However, imagine the case where there are ten different vital sign templates possibly expressed in different languages and we want to analyze semantic content, similarities and differences and make suggestions for a national or an international standard. The complexity of the material and the labor of a manual analysis make the task overwhelming, given the large number of needed pair-wise comparisons and the challenge of synthesizing these. Consequently, analyzing existing CMs requires an automated or at least semi-automated method. If such a method could be developed, it would be valuable at a local, national as well as an international level.At the local level, requirement engineering is difficult and time consuming due to the complexity of the health care domain [4]. Reusing CMs, like templates for physical examinations or nutrition screening, could speed up the requirement engineering process. However, overcoming the lack of acceptance of templates developed elsewhere, known as the “Not invented here” syndrome, is a challenge. Reuse might also be a challenge because EHR-system failure has been associated with inability to support the micro detail of clinical work [5]. The result is that there is an unknown diversity of CMs used in clinical practice. In this context, analysis of differences and similarities between hospitals and departments could provide insight on whether harmonization is beneficial and/or possible. Moreover, given a better overview, design of new templates could take its point of departure in existing ones. E.g. if a group of templates all intended for physical examinations are known, a canonical model can be developed on this basis. The next time a physical examination template is designed the canonical model can be used as point of departure, hence ideally creating harmonization and avoiding duplication of effort. A canonical model can also be used as a point of reference for similarity of different templates.Nationally, health provider organizations and medical societies strive to manage health care by balancing resource management and treatment quality. One approach is development and implementation of clinical guidelines and national integrated care pathways to ensure a high and uniform quality of care. The feasibility of guidelines and pathways depend on uniform documentation procedures and quality indicators, hence, harmonized templates are beneficial. Medical societies also have an interest in harmonized documentation, because, in many cases, clinical research depends on uniform information. Harmonization could be supported by overviews of existing templates on a national level. However, no such overview exists, and getting it requires a way to compare templates that are currently expressed using local proprietary information models.Internationally, different approaches to clinical modeling exist. They are aimed at developing, refining, implementing, and evaluating information models to ensure clinical involvement as well as semantically-interoperable systems [1,2,6–10]. Recently, an analysis criticized that many existing clinical modeling approaches violate good modeling practice since they fail to model the requirements of the health care domain using a consistent healthcare-specific ontology [11]. It can be questioned, whether the analysis takes into account that requirement engineering processes are not the main scope of all the different clinical modeling approaches. However, the general conclusion that standardized models maybe are too distant from health care practice and actual clinical information systems might be supported by the fact that the adoption of standards, apart from DICOM, is slow[12] and there is a limited progress towards full semantic interoperability [13]. Developing bottom-up approaches for international clinical modeling might help adoption of these models. As for the national level, this requires overview and comparison of existing clinical documentation templates. However, language barriers increase the complexity of the challenge. Beside bottom-up approaches, semantic similarity analysis might also be relevant in getting an overview of existing clinical models in internationally available repositories such as the openEHR clinical knowledge manager [14], the clinical element model browser from Intermountain Healthcare [15], the Australian clinical knowledge manager [16] and HL7 FHIR resources [17]. Stakeholders in the international modeling community are also concerned with information model harmonization and have joined forces in CIMI (Clinical Information Modeling Initiative) [18]. In such harmonization efforts, overview of existing CMs could also be useful.Summing up, semantic similarity analysis of CMs could be valuable for a number of local, national and international applications. Therefore, the aim of our study was to develop a method for CM comparison. The method should be able to compare and give an overview of multiple CMs whether these are local templates or standardized information models. Comparison is challenged by lexical differences. Therefore, it is necessary to base the comparison on stable concept definitions. In this study, SNOMED CT is chosen based on its coverage and flexibility compared to other terminologies [19–22]. In addition, SNOMED CT has been tested in different clinical fields [23–25]. This means that a common semantic reference can be obtained. To be able to automate the method, semantic similarity estimation is used as a means to analyze similarities and differences. This is expanded on in the background section.A semantic-similarity estimate can be understood as a numerical value reflecting the closeness in meaning between two terms or two sets of terms [26]. Both term similarity and set-of-term similarity are examined in the following.Generally, semantic-similarity estimates are classified according to the underlying theoretical principles and the knowledge sources used. [27] Knowledge sources can be domain corpora, ontologies/taxonomies and thesauri. Theoretical principles denote whether the estimate is based on edges or on information content (IC). Edge-based estimates are based on the number of edges between two terms and variations hereof. An edge is the links between two terms e.g. if cow and pig are both mammals then the number of edges between cow and pig would be two (1:pig-mammal, 2:mammal-cow). IC-based measures are based on the IC of the two terms in question and variations thereof. The IC of a term is the logarithm of the probability of finding the term in a given corpus.More than in other domains, semantic similarity estimation is often based on ontology in biomedical informatics. Explanations are that general-purpose resources like WordNet have limited coverage of biomedical terms [28], and that biomedical informatics has many available concept systems (e.g. Read codes, LOINC and SNOMED CT) [27]. Even though some of the available concept systems are not ontologies in the strict sense, they are used as such in some similarity estimation research e.g. Read codes in [29].An estimate based solely on an ontology is called intrinsic. Intrinsic methods were the focus of a combined study and review done by Sánchez et al. in 2011 [27]. Their study focused on systematically reviewing and re-formulating edge-based and IC-based semantic similarity estimates in an intrinsic information-theoretical context. The estimates reviewed were both edge-based [30,31] and IC based [32,33]. They also developed a method so that they could approximate set-theory estimates in terms of IC. The similarity estimates were evaluated using SNOMED CT and a reference set of 30 medical term pairs. In a previous study, the reference term pairs had been rated by physicians and coders in terms of their similarity [28]. An average based on these ratings serves as “gold standard” in Sánchezet al’s study, because the ratings can be interpreted as a quantification of experts’ perception of similarity. Sánchez et al’s study shows that classic edge-based and IC-based semantic similarity estimates improve their correlation with the expert ratings when re-formulating them from corpora-based to intrinsic. In addition, some of the similarity estimates taken from set-theory outperform classic similarity estimates in terms of correlation with the expert ratings. The basis of most of Sánchez et al’s estimates is the IC shown in Eq. (1).(1)IC(c)=-logp(c)≅-log|leaves(c)||subsumers(c)|+1max_leaves+1In this equation leaves(c) is the set of concepts found at the end of the taxonomical tree under concept c. This can also be expressed as the descendants of c that do not have any children themselves [34]. Subsumers(c) is the complete set of taxonomical ancestors of c including itself. Max_leaves is the number of leaves of the least specific concept (the root concept). In a SNOMED CT context this means the number of leaves of 138875005 | SNOMED CT Concept|.In Sánchez et al’s study, the best agreement between expert similarity scores and similarity estimates is obtained when applying information content (IC) based similarity measure re-formulated from the set-theory estimate first published by Sokal and Sneath [27]. This is shown in Eq. (2).(2)sim(c1,c2)=IC(LCS(c1,c2))2×(IC(c1)+IC(c2))-3×IC(LCS(c1,c2))In this equation c1 and c2 are the two concepts of interest and LCS is the least common subsumer which means the most specific taxonomical ancestor common to c1 and c2. IC is estimated using Eq. (1).When comparing the estimate in Eq. (2) with classic IC-estimates like Lin’s [32], which is shown in Eq. (3), it can be noted that it consists of the same components namely the IC of the two concepts and IC of LCS.(3)sim(c1,c2)=2×IC(LCS(c1,c2))IC(c1)+IC(c2)The presented similarity estimates always result in a number in the range [0;1].One possibility when comparing two sets of concepts is to compare each concept in the first set with each concept in the second set. For two sets with a magnitude of 10–50 concepts, this result in a similarity matrix containing 100–2500 similarity estimates. If detailed analysis of differences and similarities are required, similarity matrices might be applicable; however, for overview purposes, simpler estimates are required. Therefore, semantic similarity estimation between sets of concepts is examined in the next section.Pesquita et al. have reviewed techniques in gene product comparison based on Gene Ontology (GO) annotation, which is a specialization of the problem of semantic comparison of sets of concepts. Their classification of methods to find gene product similarity helps getting an overview of possible approaches [26]. In the following, the classification is presented in general terms instead of GO-specific.•Group-wise (set, graph or vector approaches). Sets of concepts are compared directly without calculating individual similarities between concepts. In set approaches, overlap between sets is used as an estimate of similarity. In graph approaches the concepts of each set are represented as sub graphs of the original ontology and graph matching or similar techniques are used for comparison. In vector approaches a set of concepts is represented as a vector with each dimension representing a concept in the original ontology. E.g. each coordinate of vectors can be binary, denoting absence or presence of a term.Pair-wise (all pairs or best pair approaches). Given a pair-wise comparison of concepts i.e. the similarity matrix, the pair-wise approaches propose ways to aggregate the similarity estimates in the similarity matrix. The all-pairs methods use MIN, MAX or AVG functions. The best-pairs methods takes the AVG of the maximum values in each set’s directions, see Eq. (4) as proposed among others by [35]. In other words, given a similarity matrix the maximum value of each row and each column is found. All maximum values are added and normalized using the number of concepts in the sets.The method section will present how similarity estimation was used in the CM comparison.

@&#CONCLUSIONS@&#
