@&#MAIN-TITLE@&#
A novel incremental conceptual hierarchical text clustering method using CFu-tree

@&#HIGHLIGHTS@&#
This paper presents a novel down-top incremental conceptual hierarchical text clustering approach using CFu-tree (ICHTC-CF) representation.For summarizing a cluster, we use the term-based feature extraction in text clustering.A new measure criterion, Comparison Variation (CV), is presented for judging whether the clusters can be merged or split.The incremental clustering method is not sensitive to the input data order.Experimental results show that the performance of our method outperforms k-means, which indicate our new technique is efficient and feasible.

@&#KEYPHRASES@&#
Text clustering,CFu-tree,Comparison Variation (CV),Incremental hierarchical clustering,

@&#ABSTRACT@&#
As a data mining method, clustering, which is one of the most important tools in information retrieval, organizes data based on unsupervised learning which means that it does not require any training data. But, some text clustering algorithms cannot update existing clusters incrementally and, instead, have to recompute a new clustering from scratch. In view of above, this paper presents a novel down-top incremental conceptual hierarchical text clustering approach using CFu-tree (ICHTC-CF) representation, which starts with each item as a separate cluster. Term-based feature extraction is used for summarizing a cluster in the process. The Comparison Variation measure criterion is also adopted for judging whether the closest pair of clusters can be merged or a previous cluster can be split. And, our incremental clustering method is not sensitive to the input data order. Experimental results show that the performance of our method outperforms k-means, CLIQUE, single linkage clustering and complete linkage clustering, which indicate our new technique is efficient and feasible.

@&#INTRODUCTION@&#
Text clustering is of great practical importance and is widely employed for automatically structuring large document collections today. With the rapid growth of information and the explosion of electronic text from the complex World Wide Web, more and more knowledge you needed is included. But, the massive amount of text also takes so much trouble to people for finding useful information. For example, the standard Web search engines have low precision, since typically some relevant Web pages are returned mixed with a large number of irrelevant pages. So, one appropriate way of organizing this overwhelming amount of documents is necessary. Clustering algorithms take a set of unlabeled items and group them into some larger sets. Unlike text classification, which is the process of assigning predefined category labels to new documents based on the classifier learnt from a large, often prohibitive, number of labeled training examples [1–4], text clustering does not require any training data. But, it is often an ill-defined problem. That is, clustering does not have very clear objectives. For another example, k-means [5,6], which mainly focuses on distance-based cluster analysis, can be applied to find clusters in many statistical analysis systems only when the mean of a set of objects is defined. And, the necessity for users is how to specify k, the number of clusters, which can be seen as a disadvantage. The problem of specifying k is one of the most challenging issues involved with it, since there is really no good solution that can predict the optimal number of clusters to use in every possible situation. The best assignment method of k largely depends on the experiments on the task and data set being considered. So, in other words, most of the time, users have no explicit clues to choose the best k.Motivated by this, we present a novel down-top incremental conceptual hierarchical text clustering approach. The method is implemented based on a CFu-tree that represents a cluster hierarchy, which makes it effective for incremental and dynamic clustering of incoming objects. The down-top hierarchical clustering approach starts with each input as a separate cluster. That is, an input item is defined a single cluster. The clustering process will proceed by joining two or several existing clusters to form a new one, until certain termination conditions are satisfied. We use the term-based feature extraction to summarize a cluster and use data matrix to store the cluster feature vectors in text clustering. In order to solve the problem of “how to choose k”, during clustering, a measure criterion, called Comparison Variation (CV), is also used for judging whether the clusters can be merged or split. And, our text clustering method is not sensitive to the input data order.Text clustering has become an increasingly important and a widely applicable technique for enhancing search engine results, topical Web crawling, unsupervised document organization, information retrieval [7–9], distributed digital libraries [10] and peer-to-peer (P2P) information management systems [11]. Hammouda and Kamel [12] presented a hierarchical topology for distributing k-means, which started at the lowest level of the hierarchy, and the local solutions were aggregated until the root peer was reached. This method is scalable and efficient only when the mean of a cluster is defined. And the selection of parameter k is an obvious disadvantage. Programmers should specify different k to different datasets before beginning the experiments. Trappey et al. [13] presented a fuzzy ontology-based document clustering approach, in which, a fuzzy logic control approach was used to match suitable document clusters based on their derived ontological semantic webs. The ontology-based clustering approach could bring in semantics in clustering and make the clustering results more natural and understandable. However, the performance of clustering is greatly influenced by the scalability and quality of ontology. Also, as time goes on, the update of Web pages and datasets would require for updating the ontology. The outdated ontology could degrade the quality of clustering. CLIQUE, as a grid-based clustering method, is also a typical clustering algorithm based on the grid structure [14]. Similar texts would gather to the same grid. While, the performance of CLIQUE is influenced by the number of grid and the density which decides the maximal region for each cluster. Hu et al. [15] presented a method to enhance traditional content similarity measure for text clustering based on a concept thesaurus extracted from Wikipedia. Mahdavi and Abolhassani [16] proposed the clustering methods which were classified into two major categories: similarity-based approaches and model-based approaches. Tan and Mitra [17] presented a method that judged whether a Web page had changed before downloading it based on clustering. The method fetched a sample of Web pages from a cluster to check if the pages have changed since their last download. A probabilistic hierarchical clustering framework following normal linkage algorithms and using probabilistic models to define cluster similarity was developed by Heller and Ghahramani [18]. Two hierarchical clustering methods, single linkage clustering and complete linkage clustering, were put forward by Sibson and Defays [19,20]. They merge or split clusters according to the distance between the clusters. And how to decide the margin value and whether the margin value could obtain a high-quality cluster are very important consideration. Also, the kind of method that judges whether two clusters could be merged or split according to the distance between them would be sensitive to outliers or noisy data, which is also a major reason to low-quality clusters.In addition, some essential criteria are introduced in [21]. Also, some researchers utilized and integrated WordNet knowledge into text clustering [22,23]. Clustering analysis has been studied for many decades and has many disciplines according to its broad applications such as business intelligence [24], image pattern recognition [25,26], Web search [27–29], biology [30,31], and security [32,33]. For example, financial investment, clustering analysis could help investors choose which stock should invest and make the best decision. And different application goals may influence the choice of different clustering methods.The rest of the paper is organized as follows. Section 2 presents the process of our text clustering and details feature selection, similarity measure, CV and the clustering process using CFu-tree. Several comprehensive experiments are performed to evaluate the effectiveness of our method in Section 3 in which experimental settings, performance metrics and results are provided. Section 4 draws the conclusions.Clustering is the process of partitioning a set of data sets into some subsets, and each subset is a cluster. In a cluster, the elements are similar in one or more aspects, and are dissimilar to elements in other clusters [34]. Unlike the classification algorithms, clustering algorithms are based on unsupervised learning, which means that they do not require any training data.In this paper, a novel conceptual clustering method is presented for finding and characterizing each cluster. Our method is an agglomerative hierarchical clustering method, which uses a down-top strategy. That is, it starts by letting each object be a cluster and iteratively merges clusters into larger clusters, until certain termination conditions are satisfied. For the merging step, it finds the two clusters that are closest to each other (according to the similarity measure, Eq. (5)), and combines the two to form one cluster.We take a set of unlabeled instances and group them together. Before proceeding, the criteria used in the clustering process must be decided. That is, the items are grouped using the clustering algorithm based on the criteria. After determining the criteria, how to assign items to clusters must be determined. In the following, we will detail the essential of clustering.In the process of processing texts, neither clustering algorithm nor classification algorithm could operate on the original form of text documents. A document should be represented as a vector before it is classified or clustered. Cluster features are essentially a summary of the statistics for the given clusters, by which we can easily derive many useful statistics of a cluster. Extracting features from clusters into some feature space is defined by the feature extracting function. The feature space may be extremely large, especially for very large vocabularies. If we use all the words in a document as the features, the dimension of clustering texts is equal to the number of all the different words in the dataset. However, low frequency words and stopwords usually do not contribute to the semantics of the document, that is, they have no values to the clustering process. So, we must prune the feature space, which can improve efficiency observably. Stopwords and low frequency features in the dictionary are filtered out. The low frequency features in the dictionary are still cluster features, and can participate features selection in the next clustering, as long as the frequency meets the requirements.In a document, different word has its own frequency. We judge the categorization of a document according different frequency of different words. The process of removing irrelevant words is called feature selection. And the feature selection methods may vary. Assigning weights to the features could directly reflect the importance of each feature. Cluster features will be normalized before applying similarity calculations. In data pre-processing, we apply stopword removal and tfc (Eq. (1)) [35] feature selection, and remove the commoner morphological and inflexional endings from words using Porter Stemming Algorithm11http://www.tartarus.org/~martin/PorterStemmer/.whose main use is as part of a term normalization process that is usually done when setting up information retrieval systems.(1)xij=fij×log(N/nj)∑k=1M[fik×log(N/nk)]2where fijis the frequency of word j in document i, N is the number of documents in the collection, M is the number of all the features. njand nkare the numbers of documents where word j and k occur, respectively.For assessing how similar or dissimilar clusters are in comparison to one another, we use data matrix (as shown in Figs. 1 and 2) to store the cluster feature vectors. Each row corresponds to a cluster feature vector. Suppose that we have N clusters described by M features, where xijis the value for cluster xiof the jth feature. That is, we hereafter refer to cluster xias feature vectors.Therefore, clusters can be represented using a data matrix in Fig. 1. And an example is also illustrated in Fig. 2 after feature extracting.Given two clusters (Ci, Cj) and their features (Fi, Fj), the distance is typically computed using the equation as follows:(2)Dist(Fi,Fj)=∑k=1m|xik−xjk|2,(i≠j)where(xi1,xi2,…,xim)and (xj1, xj2, …, xjm) are two m-dimensional feature vectors that represent Fiand Fj. For the distance calculation of the two clusters, the maximum distance is that no common features between them. Opposite situation, the minimum is that the two cluster feature vectors identical, i.e., xik=xjk(i≠j). Therefore, according to Eqs. (1) and (2), the maximum and minimum of the distance are calculated as follows:(3)maxDist(Fi,Fj)=∑k=1m|xik−xjk|2,(i≠j)=∑k=1mxik2+∑k=1mxjk2=∑k=1mfik×log(N/nk)∑l=1m[fil×log(N/nl)]22+∑k=1mfjk×log(N/nk)∑l=1m[fjl×log(N/nl)]22=1+1=2(4)minDist(Fi,Fj)=∑k=1m|xik−xjk|2,(i≠j)=0Therefore, the range of values allowed for the distance is 0 to2. To normalize the similarity calculation, the similarity between clusters i and j is defined as:(5)Sim(Fi,Fj)=1−∑k=1m|xik−xjk|22,(i≠j)That is, the range of values allowed for the similarity is from 0 to 1.A core need is to measure the similarity between two clusters, and the closest pair of clusters will be merged into one larger cluster iteratively. So, merging clusters is based on Eq. (6) as follows:(6)Cluster(Ci,Cj)⇔max{Sim(Fi,Fj)|i≠j}&&Sim(Fi,Fj)≥θwhere Ciand Cjare clusters can be merged, Sim(Fi, Fj) is the similarity between Ciand Cj, Fiand Fjare the features of Ciand Cj, respectively. θ is the threshold that the condition is satisfied.The problem of choosing θ is a challenging issue involved with clustering. No highly suitable method exists that can predict the similarity threshold θ of the clusters to use in every possible situation. Instead, the best choice of the threshold θ largely depends on the task and data set being considered. Therefore, the threshold θ is most often chosen by running experiments that evaluate the quality of the clusters. In this paper, we present a method, called Comparison Variation (CV), to measure whether the closest pair of clusters can be merged which is described as follows:Given the closest pair of clusters Ciand Cj, {dk} is the collection of documents which belong to cluster Ci. CV of them (Cjcorresponding to {dk}) is defined as Eq. (7):(7)CV(Ci,Cj)=1,(∏k=1n(1−Sim(dk,Cj))λ≤max{Sim(dk,Cj)|dk∈Ci})0,(otherwise)where Sim(dk, Cj) is the clusters similarity calculated by Eq. (5). We compare∏k=1n(1−Sim(dk,Cj))λand all Sim(dk, Cj) to select the maximum. If a Sim(dk, Cj) is the largest, i.e., CV(Ci, Cj)=1, then Cjis similar with dk, and Ciand Cjcan be merged. However, if∏k=1n(1−Sim(dk,Cj))λis the largest, i.e., CV (Ci, Cj)=0, then Cjis not similar with any dk, and Ciand Cjcannot be merged. λ (λ>0) is a weight for reflecting the relative importance of similarity value when merging clusters. Obviously, the higher the value of λ is, the lower the similarity value of merging clusters is required.ProofIf λ→0, then∏k=1n(1−Sim(dk,Cj))λ→1. In this case,∀(Ci,Cj):∏k=1n(1−Sim(dk,Cj))λ>max{Sim(dk,Cj)|dk∈Ci}. So, ∀(Ci, Cj):CV(Ci, Cj)=0, that is, all documents are the separate clusters. Conversely, if λ→+∞,∏k=1n(1−Sim(dk,Cj))λ→0. In this case,∀(Ci,Cj):∏k=1n(1−Sim(dk,Cj))λ≤max{Sim(dk,Cj)|dk∈Ci}. So, ∀(Ci, Cj):CV(Ci, Cj)=1, that is, all documents are grouped into an entire cluster.□For different specific similarity value request, an appropriate λ is selected. According to Eq. (6), merging clusters is redefined as follows:(8)Cluster(Ci,Cj)⇔max{Sim(Fi,Fj)|i≠j}&&(CV(Ci,Cj)=1)A tree structure called CFu-tree, which is used to represent the process of hierarchical clustering, is presented in this paper. The hierarchical clustering method works by grouping data objects into a “tree” of clusters. Figs. 3 and 4show a set of dendrograms and illustrate how objects are merged (agglomerated) together step-by-step. The cluster Ci(and the feature Fi), which is the node in CFu-tree, has formalized representation of a tuple nodei=(Ci, Fi, {dj}). {dk} is the collection of documents which belong to cluster Ci.A CFu-tree has a parameter λ (as shown in Eq. (7)), which specifies the relative importance of similarity value when clustering. λ controls the size of the resulting tree.A CFu-tree represents a cluster, even though a single node. A leaf node represents a document which is a sub-cluster. A non-leaf node, which represents cluster Ci, in the CFu-tree has descendants which merge into Ci. The non-leaf nodes store sums of the cluster features and documents of their children, and thus summarize clustering information about their children.Initially, our method places each document into a cluster of its own. The clusters are then merged step-by-step according to the criterion. The major steps of clustering represented by a CFu-tree include the following:Step 1. A document is viewed as a single cluster Ci. After preprocessing and feature extracting, a CFu-tree is built including only one node, that is Ci.Step 2. Add the CFu-tree to the CFu-tree list, and cluster for each pair of clusters.Step 3. Scan the CFu-tree list to calculate the similarity.Step 4. Merge or split clusters according to similarity criteria.Step 5. Repeat Step 3 until no cluster meets the conditions.The CFu-tree is built dynamically when a single cluster (also is a node) is inserted. Thus, the method is incremental. The node is inserted into the closest leaf node or non-leaf node. If the similarity between any leaf nodes after insertion is less than the requirement, then the leaf node and possibly other nodes are split. After the merging of the new nodes, clustering features and the set of documents of the root node will be also merged.Figs. 3 and 4 show an example of clustering process. After each iteration Im of the clustering process, the number of clusters is reducing gradually, until the clustering results do not change.Figs. 5 and 6show another example of splitting process. After each iteration Im of the splitting process, the number of clusters is increasing gradually, until the clustering results do not change.Algorithm 1 is the implementation of our clustering process, which can be depicted as follows according to the example above.Given a document, an input, is treated as a separate cluster Ci, which maintains a feature vector Xi=(xi1, xi2, …, xij, ..., xim), i=1, 2, …, n, where xifrepresents the weight of corresponding document, m is the number of dimension (the number of features space) and n is the number of the clusters. After preprocessing, vectors X1, …, Xn, are represented as the cluster feature vectors. Once a new cluster Cijoins, for each pair of clusters (Ci, Cj), a similarity value, Sim(Xi, Xj), which determines whether the pair of clusters are merged, will be computed. After the computing, the pairwise clusters meeting the conditions are merged (as shown in Figs. 3(b) and 4(a)). The iterative cluster computing process repeats until the clusters no longer change.Algorithm 1Algorithm of Text Clustering. Sim (Fj, Fk) and CV (Cj, Ck) are computed by Eqs. (5) and (7), respectively.Algorithm of Text ClusteringInput: A set of documents d1,…,di,…dn; Parameter λ.Output: An array List of CFu-trees R1.Procedure Text Clustering2.Initialize R3.for each diin the set4.Ci←preprocess (di)5.add Cito R as a separate cluster6.repeat7.for each pair of clusters Cjand Ckin R8.ifCjand Ckare the closest pair of clusters in Rthen9.ifCV (Cj, Ck)=1 then10.merge (Cj, Ck)11.compute cluster feature vectors changed12.end if13.if (CV (Cj, Ck)=0) and (Cj, Ck∈Cr) then14.split (Cj, Ck)15.compute cluster feature vectors changed16.end if17.end if18.end for19.until all clusters are not changed20.end for21.returnR22.end procedure

@&#CONCLUSIONS@&#
In this paper, we presented a novel down-top incremental conceptual hierarchical text clustering approach using CFu-tree representation. Compared with the k-means method, we also proposed a measure criterion, called Comparison Variation (CV), for clustering. Our method and the experimental results draw the following conclusions.The incremental hierarchical clustering approach using CFu-tree representation makes the clustering method effective for incremental and dynamic clustering of incoming documents. In other words, it overcomes the difficulty in agglomerative clustering methods: the inability to undo what was done in the previous step.For verifying the effectiveness of the CV, we run the clustering system to find an optimal tradeoff between Precision and Recall. And, the measure CV effectively ensures and enhances the performance of clustering. Also, CV solves the problem of how to choose the number of clusters during the clustering process. Furthermore, for comparing the performance of the text clustering based on different techniques clearly, we also implement k-means, CLIQUE, single linkage clustering and complete linkage clustering methods used for comparison. Our method shows significant performance improvement over them on the four datasets.In addition, the results, which derived from the test of the number of features versus clustering performance, indicate the reduction of the features leads to greatly lower clustering accuracies. Therefore, the most of the features are not superfluous. In summary, according to the experimental results, the text clustering method presented in this paper is efficient and feasible, and can be designed to collect all the possible information of the hidden categories.