@&#MAIN-TITLE@&#
A reduced feature set for driver head pose estimation

@&#HIGHLIGHTS@&#
We present a new automatic approach for head yaw angle estimation of the driver.We rely on a set of geometric features computed from just three representative facial keypoints.The method has a confidence mechanism to decide the reliability of a sample label.The results are comparable to the state-of-the-art techniques.The method can be easily integrated in massive consume devices.

@&#KEYPHRASES@&#
Head pose estimation,Driving performance evaluation,Subspace based methods,Linear regression,

@&#ABSTRACT@&#
Evaluation of driving performance is of utmost importance in order to reduce road accident rate. Since driving ability includes visual-spatial and operational attention, among others, head pose estimation of the driver is a crucial indicator of driving performance. This paper proposes a new automatic method for coarse and fine head's yaw angle estimation of the driver. We rely on a set of geometric features computed from just three representative facial keypoints, namely the center of the eyes and the nose tip. With these geometric features, our method combines two manifold embedding methods and a linear regression one. In addition, the method has a confidence mechanism to decide if the classification of a sample is not reliable. The approach has been tested using the CMU-PIE dataset and our own driver dataset. Despite the very few facial keypoints required, the results are comparable to the state-of-the-art techniques. The low computational cost of the method and its robustness makes feasible to integrate it in massive consume devices as a real time application.

@&#INTRODUCTION@&#
Driver fatigue/drowsiness and distraction are known to be behind a large amount of traffic accidents. Accordingly, different systems have been developed to detect such situations [1–4]. Distractions are specially challenging because many times are difficult to predict in advance since they may be due to sudden events in the environment or in the cabin. Indeed, a more general challenge including distractions is driving performance. Evaluation of driving performance is of utmost importance in order to reduce road accident rate. Behavior analysis while driving generally points out the abilities of the driver, which include cognitive (attention, executive functions, and memory) and (visual-spatial) perception skills, as well as, their fatigue levels or attention capability [5]. These abilities can be analyzed from several points of view, either measuring non-visual features like heart rate variability [6], or analyzing visual features such as eye blinking behavior [7], gaze direction estimation [8] or analysis of motion of the hands [9] or the feet [10]. In particular, head pose is a crucial indicator of driver's field of view and his/her attention focus [11] and, like most of the indicators named above, it deserves further consideration.Head pose estimation is a challenging problem in itself due to the variability introduced by factors such as driver's identity and expression, cabin and outdoor illumination, etc. [12]. In fact, during the last decade there has been an increasing interest in developing methods to estimate head pose [13] for different applications such as security and surveillance systems [14], meeting rooms [15], intelligent wheelchair systems [16], and driving monitoring [1,17]. In the particular case of driving performance, when drivers are paying attention to the road ahead, their facial direction is within approximately ±15° from the straight normal [18]. Thus, the yaw angle of the driver could contribute to determine if he/she perceives road elements such as traffic lights, roundabouts, crosswalks and the attention he/she devotes. Accordingly, in this paper we focus on the computation of such angle from still images. Such a method can be very useful as part of a multi-cue system [19,20] for early detection of abnormal driving performance in common situations. For example, if the driver does not pay attention to the correct direction in a roundabout, or if he/she attends in a crosswalk but does not see a pedestrian crossing on, an advanced driver assistance system (ADAS) combining driving monitoring and exterior hazard assessment, can decide to elevate the warning level or even braking the car.Murphy-Chutorian and Trivedi [13] divide head pose estimation methods in 8 categories. Three of them are regression methods, geometric methods and manifold embedding ones.Regression methods apply a regression model on a training set in one or more directions (angles). The regression model usually is a non-linear one, such as, support vector regressors [21], sparse Bayesian regression [22] or neural networks [23]. One drawback of these methods is that they take into account the whole face image, so that its high dimensionality decreases the efficiency. In some cases, the dimensionality can be reduced, like when the face is well localized. Still, this high dimensionality makes not clear if an specific regression tool is capable of learning the proper curve modeling the directions arch.Geometric methods are an alternative to directly explode most influencing properties on human head pose estimation, which are usually based on human perception. These features can be divided on two types, those based on face appearance, such as, orientation information of head images [24,25], skin color histogram [26] or facial contours [27], and those relying on a set of (usually 5–6) local facial keypoints [28]. In the first case, computational cost is still high, since they need to analyze the whole face image. In the second case, facial features detection needs to be highly precise and accurate. To overcome the limitations of a single category method, manifold embedding methods are usually combined with them, gaining in accuracy [12,29,30].In the same fashion, this paper combines the three methods explained above to estimate the continuous angle of head pose of a driver. Roughly, given an image of the driver's head, we rely on a small set of geometric features computed from just three representative facial keypoints, namely the center of the eyes and the nose tip. Our method is based on a combination of subspace projections, as Fisher's linear discriminant (FLD) and principal component analysis (PCA), as well as multiple linear regression adjusted for each pose interval. Figs. 1 and 2sketch the main steps of the method, split in training the system and testing new samples. For the training (Fig. 1), from a set of samples, we extract the facial keypoints to compute a geometric feature vector. A projection of the samples on a FLD allows the system to suppress some samples not useful to train. Then, the new set of samples is projected on another subspace based on PCA and a multiple linear regression is computed to estimate the regression parameters. When a new sample is given (Fig. 2), it is projected and then classified on the FLD subspace and on the one based on PCA. A combination of both classifications gives us the final coarse yaw angle estimation while the regression parameters serve to compute the continuous yaw angle. Besides, the method integrates a mechanism to self-evaluate the likelihood of the generated hypothesis and discard non likely poses by comparing the discrete angle obtained from FLD and the continuous angle from the regression.The analysis of the results assessing the reliability of the method shows that, although the very few facial keypoints required, the approach has high accuracy and precision, which makes it comparable to the methods present in the literature. Besides, the computational cost is as low as it can run in real time, making easy to integrate it in massive consume devices such as tablets or mobiles and be part of a multi-cue system for driving performance evaluation. As well, the robustness of the method against noise in the facial keypoint detection is proven.The remains of the paper are organized as follows. Section 2 describes the mathematical tools involved in the driver's yaw angle estimation. Section 3 describes the detection of the facial keypoints and the geometric features derived from them while Section 4 presents the workflow of the method. The experimental setting and the measures used to assess the reliability of the method are detailed in Section 5. Results and their analysis, are shown in Section 6, while the method is compared with the ones in the literature in Section 7. Last section, 8, is devoted to conclude the paper with some final remarks.In this section, we explain the mathematical tools used along the method.Statistical methods based on subspaces have been broadly used in computer vision and related fields for recognition and classification tasks due to their appealing capabilities and good practical behavior. Among the most popular methods we find FLD and PCA.FLD computes a linear transformation that minimizes the scatter of the samples within each class, while maximizing the scatter between classes. The main goal of FLD is to find a projection matrix, Wfld, of the linear subspace W that maximizes the Fisher's criterion:(1)Wfld=argmaxW|WTSBW||WTSWW|whereSB=∑j=1c(x¯j−x¯)(x¯j−x¯)Tis the between-classes scatter matrix andSW=∑j=1c∑i=1mj(xij−x¯j)(xij−x¯j)is the within-class scatter matrix. c is the number of classes, mjthe number of samples of the jth class and the total number of samples isM=∑j=1cmj.xijis the ith sample of the jth class,x¯jis the mean vector of the samples of the jth class, andx¯is the mean vector of all the samples.If SWis not singular, the Fisher's criterion is maximum when the vectors in Wfldare the eigenvectors associated at the non-zero eigenvalues of(SW−1SB). The new vectors (projected samples) are linear combinations of the original ones:(2)xfldi=WfldTxii=1,…,MPCA is used for dimensionality reduction, compression and feature extraction, which preserves the maximum variability in the original sample space (vectors). A high correlation of input vectors involves redundant information. The PCA method reduces this redundancy by uncorrelating these vectors. PCA seeks to maximize the total scatter of the projected vectors by the following criteria:(3)Wpca=argmax∥w∥=1|WTSTW|whereST=∑i=1M(xi−x¯)(xi−x¯)Tis the total scatter matrix.Wpca∈ℝd×ris a matrix with r orthogonal columns, r<d.The above criteria is maximum if the column vectors that form Wpcaare the r first eigenvectors associated to the r first eigenvalues of ST, ordered by descending value.The new vectors (transformed samples) are linear combinations of the original ones and are constructed in the order of importance given by the total scatter of the vectors, as:(4)xpcai=WpcaTxii=1,…,MRegression models describe the relationship between a dependent variable, Y, and one or more independent (explanatory) variables. In the particular case of multiple linear regression, the general model can be written as follows:(5)Y=Xβ+ɛwhich is equivalent to:y1⋮yn=1x11⋯x1d⋮⋮⋱⋮1xn1⋯xndβ0⋮βd+ɛ1⋮ɛnandyi=β0+β1xi1+β2xi2+…+βdxid+ɛiis the ith response, i=1, …, n, βkis the kth regression coefficient, k=1, …, d, xikis the ith observation on the independent variable xk, and ɛiis the ith noise term, which models the random error.The regression parameters can be estimated as:(6)β=(XTX)−1XTYThe facial keypoints we use are the left eye center (LE), right eye center (RE), and nose tip (N) as shown in Fig. 3(a). These keypoints are enough to calculate 10 geometric features consisting of Euclidean distances, ratios, differences and angles.Fig. 3(b)–(d) shows the features that can be directly extracted from RE, LE and N detection in the three different views, right, frontal and left. Let x=[d(RE, N), θE, θRE, θLE, r(dE, θE), r(θE, θRE), r(θE, θLE), r(θRE, θLE), r(dRE, dRL, θE), s(θLE, θLR)]T∈ℝ10be a 10-dimensional vector with the geometric features described in Table 1.The method presented in this paper for estimating head yaw angle is split in two main steps, one for training the system and another one for computing the angle of the head in a new image.LetX=(x1,…,xM)∈ℝd×Mbe a matrix of M d-dimensional vectors (samples) with the geometric features proposed above, partitioned according to c possible classes associated to c discrete yaw angles of the head. That is:X=(x11,…,xm11,…,x1c,…,xmcc)∈ℝd×MBriefly, our method proceeds as follows. The samples are projected into a subspace easier to classify by means of FLD and, there, uncertain samples are removed by k-NN. Afterwards, dimensionality reduction is performed by PCA. The resulting subspace is further aligned (rotated) and a new dimension is generated in the transformed subspace. Finally, the piecewise multiple linear regressors that output yaw angles are computed given the transformed (FLD-PCA-Rotated) geometric features. The details are described below and Fig. 4shows the main space transformations and projections.First, the geometric feature vectors of the training set are transformed into a FLD subspace following Eq. (1) for finding the projection matrix, Wfld:Xfld=WfldTX∈ℝd×(c−1)The transformed vectors keep their class label, but this allows us to remove uncertain transformed ones. In particular, for each transformed vector we apply a k-NN classifier, and if the classification result does not match its class label we consider it as uncertain and, thus, it is no used for the rest of the process. Fig. 4(a) shows the first two dimensions of our sample set in the FLD subspace, partitioned in 5 classes. Uncertain samples (outliers) are marked in green.The resulting matrix,Xf=(x1,…,xMf)∈ℝd×Mf, Mf≤M, is then transformed into its PCA subspace, following Eq. (4):Xpca=WpcaTXf∈ℝr×Mfwhere r is the new dimension of the reduced subspace so that r⩽d. In order to keep the 90% of the cumulative energy, we experimentally found that only the two first components are needed. Therefore, from now on we can assume r=2.In order to distribute the samples as symmetrical as possible with respect to the second principal component, and orthogonal to the first one, the samples in Xpcaare rotated an angle α from the first principal component. Thus, α is calculated from the arctan (in degrees) of the slope of the line joining the center of the clusters that are farthest away. That is, if C1=(pc1,1, pc1,2) and C2=(pc2,1, pc2,2) are the centroids of two clusters containing the samples that are farthest away, α is:(7)α=arctanpc2,2−pc2,1pc1,2−pc1,1Those clusters are computed using a Fuzzy C-means clustering [31] into the PCA subspace. In this way the rotated samples are:(8)Xpcaα=RαXpca=cos(α)−sin(α)sin(α)cos(α)Xpcawhere Rαis the rotation matrix. Fig. 4(b) shows the samples transformed in the PCA subspace before rotating them. As well, the centroids of the two clusters that are farthest away are marked in green and the slope of the line joining them shows the angle α to rotate the samples.The new feature representation lie on a nonlinear manifold in a 2-dimensional subspace that are still a bit superposed. Thus, in order to discriminate the samples and obtain a better feature representation, a new dimension is generated and added to theXpcaαplane.This discriminative dimension is computed by means of a nonlinear function with the same sign of the first component (that is, positive whenXpcaα(1)>0and negative whenXpcaα(1)<0). In particular, the function chosen is the four-quadrant inverse tangent atan2(y, x), which computes the angle between the positive x-axis of a plane and the point given by the coordinates (x, y). This function is positive for the upper half-plane, y>0, and negative for the lower half-plane, y<0. Unlike the single arctan function, it returns the appropriate quadrant of the computed angle. Then, the new dimension for each sample is computed as:(9)fi=atan2(Xpcaα(1,i),Xpcaα(2,i))andF=(f1,…,fMf)∈ℝMf. So the new representation of samples is:X′=XpcaαF∈ℝ3×MfFig. 4(c) shows the samples set in the new representation space. A hyperplane is adjusted for each class following the linear regression model explained in Section 2.2 and, accordingly to Eq. (6), regression parameters are estimated for each class, obtaining the following matrix:(10)B=(β1,…,βc)∈ℝ3×cIn this case, we force that the independent term for each class be the own angle of the class.The result of all this training process is the following: a Wfldprojection matrix on FLD subspace, a Wpcaprojection matrix on PCA subspace, a Rαrotation matrix for the two principal components, a new samples set X′ and the regression coefficients B. The steps of the whole process are shown in Algorithm 1, where the input of the algorithm is the set of training samplesX∈ℝ10×Mwith their corresponding discrete angleΦ∈ℝM.Algorithm 1Model trainingInput:X∈ℝd×M,Φ∈ℝM.Output:Wfld∈ℝd×(c−1),Wpca∈ℝd×2,Rα∈ℝ2×2,B=(β1,…,βc)∈ℝ3×c.1:Transform the space containing the original sample set X into a FLD subspace, obtaining the projection mappingWfld∈ℝd×(c−1)and the transformed sample setXfld=WfldTX.2:Remove outliers into the FLD subspace by means of k-NN to obtainXf∈ℝd×Mf, Mf≤M.3:Transform the space containing the reduced sample set Xfinto a PCA subspace, obtaining the projection mappingWpca∈ℝd×2and the transformed sample setXpca=WpcaTXf.4:Find the centroids C1=(pc1,1, pc1,2) and C2=(pc2,1, pc2,2) (using a Fuzzy C-means clustering) of the samples that are farthest away into Xpca.5:Calculate the rotation angle, α, from the arctan in degrees of the slope of the line joining the centroids, α=arctan(pc2,2−pc2,1/pc1,2−pc1,1)6:Rotate the transformed samples Xpca,Xpcaα=RαXpca=cos(α)−sin(α)sin(α)cos(α)Xpca7:Generate the transformed training set,X′=XpcaαF∈ℝ3×MfwhereF=(f1,…,fMf)∈ℝMfandfi=2atan(Xpcaα(1,i),Xpcaα(2,i)).8:For each class, compute the regression coefficients by Eq. (6) to obtainB=(β1,…,βc)∈ℝ3×c.9:Return Wfld, Wpca, Rα, BIn the test process, the head pose of new samples is classified, and the yaw angle is estimated as well. Following the work flow shown in Fig. 2, firstly the geometric feature vector,y∈ℝd, is computed. The label of the test sample is predicted as a coarse head pose by means of the combination of two probabilities of belonging to a class, the one obtained after projecting the vector on the FLD subspace and the one obtained after projecting it on the rotated and extended PCA subspace.That is, on the one hand, the feature vector y is projected on the FLD subspace,yfld=WfldTyUsing a k-NN classifier, the probabilities that yfldbelongs to each class c are computed, and their maximum is chosen:m1=maxc(P(yfld∈c))The class label corresponding to m1 is denoted as l1.On the other hand, the feature vector y is projected on the regression space in two steps. First, it is projected on the rotated PCAypcaα=RαWpcaTySecond, as in the training process, a third coordinate is added toypcaα:fy=atan2(ypcaα(1,1),ypcaα(2,1))and the 3D point in the regression space isy′=ypcaαfyThe maximum of the probabilities that y′ belongs to each class c is computed by means of k-NN:m2=maxc(P(y′∈c))and a new label, l2, is obtained.Finally, both labels, l1 and l2, are compared and the discrete angle, φdis computed. If l1 and l2 do not coincide, the most likely class among both cases decides the classification. If both classifiers assign the same probability to the sample but for two different classes, the sample is labeled as uncertain. The result can be summarized as:(11)φd=φ(l1),l1=l2φ(l1),l1≠2,m1>m2φ(l2),l1≠2,m1<m2uncertain,otherwise.where φ(li), i=1, 2 is the angle assigned to the jth chosen class from li.The estimation of the continuous head's angle is computed by means of the regression coefficients of the jth class to which the sample belongs to:(12)φcont=(βj)Ty′Algorithm 2 shows the steps to follow in the test phase, where the input is a test sample,y∈ℝd, and the outputs, the corresponding discrete and continuous angles, φdand φcont, respectively.Algorithm 2Yaw computation of test samplesInput:y∈ℝd.Given: Wfld, Wpca, Rα, BOutput: φd, φcont1:Project y into the FLD subspace,yfld=WfldTy.2:Calculate the maximum of the probabilities of belonging to each class in the FLD subspace:m1=maxc(P(yfld∈c))and associate it to the corresponding class label l1.3:Project y into the rotated PCA subspace,ypcaα=RWpcaTy.4:Generate the new 3D point asy′=ypcaαfy,fy=atan2(ypcaα(1,1),ypcaα(2,1)).5:Calculate the maximum of the probabilities of belonging to each class in the regressed space:m2=maxc(P(y′∈c))and associate it to the corresponding class label l2.6:Compute the discrete angle φdby (11).7:Calculate the continuous angle φcontby (12).8:Return φdand φcontOur method has been tested on two sets of data, one based on images from a controlled scenario and the other based on images acquired while driving.11The driver's database is available at: http://adas.cvc.uab.es/site/index.php/datasets/.For the first dataset, we have considered the CMU-Pose, Illumination and Expression database [32], which contains 13 images of 68 persons that present head pose changes in the horizontal axis of [−135°:22.5°:135°] for a total of 884 images of 640×480 pixels each. In our particular case, since the final application only requires poses within the angular area useful to drive, we only take into account 5 poses with the angles [−45° −22.5° 0° 22.5° 45°], for a total of 340 images. Fig. 5illustrates the database by showing the images used for one of the subjects.As second dataset, we have used our own one acquired while driving in real scenarios. It is composed of 606 samples of 640×480 pixels each, acquired in different days from 4 drivers (2 women and 2 men) with several facial features like glasses and beard, classified in 3 possible classes [18,33]. The first class is the “looking-right” class and contains the head angles between −45° and −30°. The second one is the “frontal” class and contains the head angles between −15° and 15°. The last one is the “looking-left” class and contains the head angles between 30° and 45°. Fig. 6shows a driver of our dataset.The experiments carried on to evaluate the validity of our feature set and our overall head's yaw angle estimation explore two aspects of the method: its reliability and its robustness against facial keypoint detection.The reliability of the method is assessed by means of the absolute error for the continuous head pose estimation and the accuracy for the coarse one. Since we aim at evaluating our yaw estimation method independently of the procedure for detecting the required keypoints, we run the experiments using manual ground truth and using an automatic detection based on deep convolutional network cascade [34]. This method cascades three convolutional networks to make coarse-to-fine prediction. In the first level they use three individual networks to detect the whole face, eyes and nose, and nose and mouth. In the second and third levels, they refine the prediction by taking local patches centered at the previous predicted positions. It is worth to mention that this method is an external detector off-the-shelf and can effectively predict and locate our keypoints with high accuracy, even when low-level features from local regions are ambiguous or corrupted.The absolute error (AE) between the continuous angle estimated by our approach, φcont, and the discrete angle from the ground truth, φ, is computed as:AE=∥φcont−φ∥The accuracy (Acc) of the coarse head pose estimation is computed on both datasets as:Acc=TPNwhere N is the total number of test samples and TP the number of samples correctly classified.Besides, in order to visualize the reliability of the method and the classes of missclassified samples the confusion matrix between ground truth and the estimated angle is computed for both datasets.The robustness of the method is assessed by means of the reliability of the method with added Gaussian noise to the facial keypoint detection. That is, we have moved each facial keypoint, independently of the others, along eight principal directions [0°:45°:315°] as Fig. 7shows, up to 20 pixels and we have computed the accuracy of the method. It is worth to note that the added noise is independent from the head image size, but it implicitly includes noise in the 10-dimension feature vector, since the computed geometric features are normalized.

@&#CONCLUSIONS@&#
In this paper, we have introduced a new methodology for driver coarse and fine head's yaw angle estimation by using a feature set generated from a reduced set of facial keypoints. The approach is based on a combination of subspace methods, as PCA and FLD, and multiple linear regression. As well, it integrates a mechanism to self-evaluate the likelihood of the generated hypothesis and discard uncertain poses by comparing pose label from FLD and PCA.The reliability of the method has been assessed in two different datasets, a controlled scenario (CMU-PIE) and in real driving (our own database). The global average accuracy for CMU-PIE dataset shows that the method performs like other state-of-the-art strategies. The best performance of our method reaches an accuracy of 100% for manual detection of facial keypoints and a 98.85% for automatic one. In the case of the driving dataset, a continuous angle is computed with an absolute average error below 5.13°. As well, for the coarse estimation, the misclassified samples keep a high correlation (first near neighbor) with the real angle.Thus, we can conclude that 3 facial keypoints, corresponding to the center of both eyes and the nose tip, are enough to extract 10 geometric features based on angles and Euclidean distances and obtain accurate and precise results for both coarse and fine head pose estimation. This is probably due to the fact that nose tip and eyes are fixed parts of the face so that their degrees of freedom are closely linked to the same degrees of freedom as the face. In this case, adding more facial keypoints, like the edges of the mouth could add more angles and distances that might be redundant or even hinder a reliable angle, since the degrees of freedom of the mouth are independent from driver's face. Indeed, mouth state information could serve to detect driver fatigue [4], but only the edges might not be enough to report a robust state of the mouth and detect events like yawning.Besides, our method has proven its robustness, maintaining a high accuracy in noisy detections up to 4 pixels for all directions. Several factors such illumination changes can hinder this robustness, misleading proper outputs in some frames. In order to avoid that errors and other induced from the acquisition devices, it would be interesting to explore the improvements of adding some kind of temporal continuity. However, this temporal continuity could be incompatible with the detection of abrupt changes corresponding to drivers behavior abnormalities or distractions. Consequently, to detect abnormal driving performance, abrupt changes of drivers head pose should be carefully modeled and distinguished from those actions implying a short and fast movement of the head. For that aim, the gradient of the head motion would be interesting to incorporate in the model, although it requires a further analysis.Finally, the low computational cost of the overall method allows it to be integrated in a tablet or mobile for a real time application, which will be done in mid term.