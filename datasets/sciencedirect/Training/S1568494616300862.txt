@&#MAIN-TITLE@&#
Analysis of OpenMP and MPI implementations of meta-heuristics for vehicle routing problems

@&#HIGHLIGHTS@&#
We parallelize a sequential meta-heuristic based on simulated annealing.The island and master-worker models, and OpenMP and MPI implementations are used.Vehicle routing problems with time windows are used as benchmarks.The performance of multithreading and multi-core processing is analyzed.The different implementation alternatives are statistically compared by using ANOVA.

@&#KEYPHRASES@&#
Parallel meta-heuristics,MPI,OpenMP,Master-worker model,Island model,Vehicle routing problems with time windows (VRPTW),

@&#ABSTRACT@&#
The parallelization of heuristic methods allows the researchers both to explore the solution space more extensively and to accelerate the search process. Nowadays, there is an increasing interest on developing parallel algorithms using standard software components that take advantage of modern microprocessors including several processing cores with local and shared cache memories. The aim of this paper is to show it is possible to parallelize algorithms included in computational software using standard software libraries in low-cost multi-core systems, instead of using expensive high-performance systems or supercomputers. In particular, it is analyzed the benefits provided by master-worker and island parallel models, implemented with MPI and OpenMP software libraries, to parallelize population-based meta-heuristics. The capacitated vehicle routing problem with hard time windows (VRPTW) has been used to evaluate the performance of these parallel strategies. The empirical results for a set of Solomon's benchmarks show that the parallel approaches executed on a multi-core processor produce better solutions than the sequential algorithm with respect to both the quality of the solutions obtained and the runtime required to get them. Both MPI and OpenMP parallel implementations are able to obtain better or at least equal solutions (in terms of distance traveled) than the best known ones for the considered benchmark instances.

@&#INTRODUCTION@&#
Heuristics and meta-heuristics have proven very effective for complex combinatorial optimization problems appearing in several economic, industrial, and scientific domains. Nevertheless, meta-heuristic approaches need a high computation power to solve difficult problems. Fortunately, parallel and distributed architectures have offered an improvement to sequential algorithms [3], while these algorithms are nowadays helping computer designers on the advance of computer architecture. A large number of papers have been proposed to improve the efficiency of sequential algorithms in several domains by using parallel computing. These parallel implementations aim to reduce the runtime required to obtain similar quality solutions to those obtained by the sequential algorithms and/or higher quality solutions than the sequential versions without increasing the runtime required by the latter.The paper analyzes the performance of MPI and OpenMP using alternative implementations of evolutionary parallel models, such as master-worker and island models, as programming alternatives to implement different parallel implementations of population-based meta-heuristics in shared-memory multi-core systems. Comparing MPI and OpenMP for a given application is interesting as for some programs, it could be more efficient to use MPI to send messages in a shared-memory multi-core systems, i.e. to copy data, than to use OpenMP threads that share accesses to all the memory, including the critical sections. Some authors have evaluated and compared the performance of both programming models in clusters of processors using different applications, including the NAS parallel benchmarks (NPB) [28,37], engineering applications such as OVERFLOW and AKIE [28], and matrix multiplication and Sobel kernels [37]. In the paper here presented, both parallel implementations are compared on a single multi-core chip sharing memory for solving the capacitated vehicle routing problem with time-windows (VRPTW), an important economic and organizational problem with practical applications in logistics and transportation management that also requires a high computational cost and can take advantage of the increase in the number of cores that will be included in the foreseeable future in the microprocessor chips used in the single node platforms.The vehicle routing problem (VRP) [19], and its multiple variants, including the VRPTW, consists in providing goods from a supply point to several geographically dispersed demand points by satisfying a usually large number of constraints. This problem is often coupled with inventory and production decisions [11] and constitutes an important economic issue since the delivery cost accounts for a significant portion of the total logistic costs [1]. Furthermore, current concerns over global warming, resource depletion, and social impact of traffic congestion and pollution are driving companies, governments, and researchers to improve the efficiency of logistics and distribution operations [34].The remaining of the paper is further organized as follows: Section 2 introduces the main parallel implementations often used in computational optimization, including the master-worker and the island models, and the main software libraries used in parallel processing, including MPI and OpenMP. Section 3 formally describes the VRPTW and offers a review of some previous parallel approaches for solving vehicle routing problems. Section 4 presents the general framework of the comparative study, including the heuristic algorithm parallelized, which is based on Simulated Annealing. Results of the empirical analysis carried out in a multi-core workstation are given in Section 5, while conclusions are drawn in Section 6.Parallel processing involves several processes (or threads), executed by different processors/cores, simultaneously working on the solution of a given problem instance. The use of parallel processing enables the improvement of the quality of the results of the sequential algorithms, to reduce time requires solving the problem instance, and/or solving larger problem instances without increasing the running time. Four main issues to be considered when applying parallel processing are:•The parallel architecture: Before the present multi-core era, parallel processing was only possible on multiple-processor systems. These high-performance computing (HPC) systems included architectures in which each processor has its own private memory, such as the computer clusters, and shared memory architectures which allow several CPUs to have access to a single memory space. In the past decade, the Moore's law reached physical barriers that hinder the performance of electronic components [33]; it is not feasible to design processors doubling the clock speed of the previous models periodically, while independently their instructions per cycle increase. Nowadays, the multi-core processors, based on including more processing cores and a shared cache memory in a single microprocessor, are gradually becoming more prominent in the commercial market [10]. In fact, nowadays, personal computers, laptops and workstations may include not only dual-core, quad-core processors, but also hexa-core, octa-core, deca-core, dodeca-core, or even hexadeca-core processors, which contain 2, 4, 6, 8, 10, 12, and 16 cores, respectively. This involves an important advance, since parallel processing can be implemented, not only in HPC architectures, but also in low-cost personal computers and workstations using standard software components [38].The parallelization model: Fig. 1shows a typical classification of parallel evolutionary models [15], which is also valid for the parallelization of other population-based meta-heuristics. There are two main parallelization models: the distribution of the cost function computation for the solutions in the population and the concurrent execution of the meta-heuristics over multiple sub-populations. In the first case, the individuals of the population are distributed among several worker processes that compute the cost functions corresponding to the individuals in the corresponding sub-population. Once the cost function is evaluated, the data obtained by the workers is sent to a central process that is responsible of completing the iteration by applying the corresponding operators to the individuals of the whole population. Then, the population is distributed again among the worker processes that compute again the new values of the objective functions. The main advantage of this parallelization model is that it does not modify the convergence characteristics of the corresponding sequential algorithm. The second parallelization model works by concurrently executing iterations of the corresponding algorithm, including the cost function evaluation step, on different sub-populations. In this case, the convergence behavior could be different in sequential and parallel versions of the algorithms because the different sub-populations usually evolve independently and only exchange some individuals periodically.The implementation strategy: The implementation strategy to be applied depends on the nature of the given problem, the hardware components of the parallel system, and the software libraries used in the parallelization. In the following, the main strategies usually applied to exploit the intrinsic parallelism of population-based meta-heuristics are described.∘The master-worker paradigm, where the master process delegates work to the worker processes, is the simplest approach. Fig. 2(a) shows how the master divides the work amongst the workers, who complete the required work and return the result to the master which organizes the received information. The master processor is responsible for synchronizing communications, collecting, distributing data, etc. Therefore, the search space exploration is conceptually identical to that of the sequential executions. The main disadvantage of this approach is that it requires a high degree of synchronization and can involve important overheads in case of using message passing-based software. In practice, this involves that in those implementations in which the worker processors are only responsible for evaluating the objective functions, it is only possible to obtain great advantages in terms of the speedup if these objective functions are complex and time consuming. Otherwise, communication time could become larger than the computation time, thus reducing the speedup obtained.The island-based paradigm, also termed distributed or coarse-grained paradigm, consists in dividing the entire population of the sequential algorithm into several sub-populations distributed among different processors, as Fig. 2(b) shows. These sub-populations or islands, whose number is often equal to the number of processors/cores, evolve, mainly in isolation, by executing all the steps of the algorithm, although it is also possible to migrate solutions between islands. The performance of island-based parallelizations is often influenced by two main design parameters: the migration topology and the frequency of these migrations. Some authors have parallelized meta-heuristic approaches, including differential evolution and simulated annealing with adaptive neighborhood, in order to evaluate the impact of several migration strategies often used when using the Island model [42]. Two main advantages of the Island model are that synchronization only occurs through migrations (that could reduce the overheads with respect to the master-worker alternative), and that, by using different initial solutions or parameter settings, each island could explore different regions of the search space.A third implementation is the diffusion (or fine-grained) paradigm (see Fig. 1), which deals with one conceptual population like the master-worker paradigm, but this population contains only a few individuals. There are also hybrid models [17,4], not included in Fig. 1, that combine different implementation strategies.With respect to the implementation of the models, the first parallelization model (i.e. the distribution of the fitness calculation for the population) is implemented by using the master-worker paradigm, while the second one (i.e. the concurrent execution of iterations of the corresponding algorithm) can be also implemented by using the master-worker model. Nevertheless, as in this implementation the master processor could become a bottleneck that reduces efficiency, this parallel model is usually implemented by using the island-based paradigm or the diffusion approaches.The software libraries: The use of parallel processing requires sharing resources and distributing the workload dynamically to utilize the global resources efficiently. Some specific software libraries, including OpenMP [16] and MPI [43], are often used with this purpose. Both, OpenMP and MPI, have been used to solve vehicle routing problems [21,18].∘MPI (Message Passing Interface) allows efficient parallel programs to be written by providing routines to initiate and configure the message environment as well as managing some of the tasks of the parallelization, such as decomposing and distributing the starting points of search, moments of communication, synchronization of communications, etc. The problem decomposition may concern the algorithm, so that different tasks working on the same data are allocated into different processors and are executed in parallel with periodic exchange of information. On the other side, it could also concern the problem-instance data, where the feasible domain of the problem to be solved is partitioned and a particular solution methodology is used to address the problem on each of the components of the decomposition. Communications, synchronous or asynchronous, are performed to improve the performance of the algorithm, but the frequency of these communications and the volume of exchanged information must be controlled to prevent large overheads. Moreover, the transfer between the sequential and parallel code using MPI involves significant changes, and from the programmer's point of view, MPI ignores the fact that cores inside a single node work on shared memory, which is why, traditionally, when parallelizing code on distributed memory architectures, such as clusters of computers, MPI is used in combination with the island paradigm that involves autonomous operation of each island with sporadic communications.OpenMP provides an API that supports shared memory multiprocessing programming on most shared memory architectures (either physically shared or distributed) using several programming languages. The standard OpenMP allows the multi-threaded execution of a program thanks to the fact that compiler directives exploit loop level parallelism. In particular, OpenMP provides a fork-and-join execution model in which a program begins execution as a single process or thread, here called central thread. This central thread is sequentially executed until a parallelization directive for a parallel region (e.g. a for loop) is found, when that central thread creates a team of threads and controls their operation. All the threads then execute their statements and at the end of the parallel constructs, all of them are synchronized. While in the case of using message passing libraries, such as MPI, it is necessary to carefully control the communications between processes, the main advantage of OpenMP is that an existing code can be easily parallelized by placing OpenMP directives around time consuming loops which do not contain data dependencies. However, optimizing work-flow and memory access is not a trivial issue when using OpenMP, and data dependencies, race conditions, etc. must be controlled by the programmer. In particular, overheads can become an issue when the size of the parallel loop is small. In general, OpenMP is preferred to parallelize the master-worker paradigm on shared memory architectures, since this parallel paradigm requires a continuous synchronization.Hybrid MPI/OpenMP: It is also possible to use a hybrid programming model which uses OpenMP for parallelization inside the node and MPI for message passing between nodes. In the context of multi-core architectures, the question arises whether it might be advantageous or not to use more than one MPI process with multiple threads running on a node so that there is at least some explicit intra-node communication. Some authors have analyzed the performance of hybrid MPI/OpenMP implementations in different problems [4,14,50].Given a parallel architecture, the next decision is to determine which parallel model, implementation strategy, and software library should be used to solve a specific application. This paper is focused on the parallelization of algorithms for shared memory work-stations with a single multi-core processor including several levels of (either local or shared) cache memory. Therefore, at first sight, OpenMP would be selected to develop the parallel code because it seems to be more efficient to share memory locations than to send data to other threads. Nevertheless, as an MPI process has exclusive access to its process memory, sending data inside the shared-memory multi-core microprocessor implies making a data copy, that for a given application could have less cost than sharing a critical section. Therefore, since both the OpenMP and the MPI paradigms have different advantages and disadvantages [28], and as the VRPTW is a problem whose cost functions are relatively easy to compute but the search space is very large, it is not possible to determine a priory on which implementation strategy and software library would obtain the best results. This is, precisely, the main issue of this research.The vehicle routing problem (VRP) and its variants are NP-hard multi-constrained combinatorial optimization problems [36] linked with branches of mathematics, economics, computer science, and operations research. The basic VRP [19] has been extended to take into account real constraints and details, and also to include different characteristics of its nature, such as dynamicity, time dependency, and/or stochastic aspects. The object of this research is to solve the VRPTW [24], which involves the routing of a set of vehicles with identical capacity stationed at a central depot (logistic center) which operate within a certain time windows and are used to visit and fully supply the demands of set of customers. Routes are designed to start and end at the depot and the total demand met by any route cannot exceed the vehicle capacity. The customers, whose demands can only be supplied once by exactly one vehicle within a certain time window, are placed in diverse geographical locations and have pre-established requirements of goods and service time. The time windows imply that the customer cannot be serviced after the time window has expired, i.e. the vehicle can arrive before the time window opens but not after it has closed. The total distance traveled by all the vehicles defines the traveling times.Some exact methods have been proposed for routing problems, including Lagrange relaxation-based methods, column generation, and dynamic programming [24]. However, since VRPs are NP-hard problems, these approaches would need an extremely large runtimes to obtain optimal solutions in large problem instances, being then more suitable to apply approximated methods. This is why investigators have proposed several heuristic approaches to solve VRPs. Heuristic approaches, which are often classified into evolutionary computation and local search methods, are capable of finding high quality solutions for real-life problems in a limited time [22]. A large number of heuristic approaches have been proposed in the past to solve this problem, including multi-start local search [12], genetic algorithms [1,48], tabu search [11], particle swarm optimization and variable neighborhood search [31], and the results obtained show that these methods obtain good results in an acceptable runtime [13].Many papers dealing with VRPTW are based on a two-stage approach where the number of routes is minimized in the first stage and the total distance traveled is then minimized in the second stage, i.e. minimizing the number of vehicles is considered as the primary objective, while for the same number of vehicles, the total distance traveled (or total duration) is often used as the secondary objective [12,40]. However, several practical aspects suggest a direct consideration of the total traveling distance as the objective to be optimized instead of using the number of tours as primary objective: (i) vehicles often belong to the company and therefore, their cost is almost independent of their use; (ii) the drivers often work either for the owner of the company of the goods or for the owner of the company of the vehicles, which is why they are remunerated in some way even in the case of not driving any vehicle; (iii) since the traveling time depends on the traveling distance, this objective is more accurate when transporting perishable goods; (iv) the traveling distance has a higher economic impact because fossil fuel consumption is the most expensive variable cost of the transportation process. Besides, some authors have demonstrated that both objectives (number of vehicles and traveling distance) may be positively correlated, or they may be conflicting, i.e. fewer vehicles employed in service do not necessarily reduce the traveling distance [46].The mathematical definition of the objectives and constraints of the VRPTW, including the frequently used notations such as route, depot, customer, and vehicles can be modeled as a graph theoretical problem [24]. Let G=(V, E) be a non-directed complete graph, where the vertices V={1, …, N} correspond to the depot and the customers, and the edges e∈E{(i, j): i, j∈V} to the links between them.Decision variableXijk=1if vehiclektravels from nodeito nodej0otherwiseParametersajis the earliest time for customer j to allow the service,bjis the latest time for customer j to allow the service,Cijis the cost for traveling from node i to node j (here, Cijis considered as the distance or time required for traveling from node i to node j),djis the demand at customer j,K is the maximum number of vehicles that can be used,L is a large scalar (L≥∑i=1N∑j=1NCij),N is the number of customers plus the depot (the depot is noted with number 1, and the customers are noted as 2, …, N),Qkis the loading capacity of vehicle k (the same capacity, Q, will be considered for all vehicles).The VRPTW can be stated as follows:(1)minimize:TD=∑k=1K∑i=1N∑j=1NXijkCijsubject to:(2)Xiik=0(∀i∈{1,…,N},∀k∈{1,…,K})(3)Xijk∈{0,1}(∀i,j∈{1,…,N},∀k∈{1,…,K})(4)∑k=1K∑i=1NXijk=1(∀j∈{2,…,N})(5)∑i=1N∑j=2NXijkdj≤Qk(∀k∈{1,…,K})(6)∑k=1K∑j=2NX1jk≤K(7)∑j=2NX1jk−∑j=2NXj1k=0(∀k∈{1,…,K})(8)ski+Cij−L(1−Xijk)≤skj(∀i,j∈{1,…,N},∀k∈{1,…,K})(9)aj≤skj≤bj(∀i,j∈{1,…,N},∀k∈{1,…,K})Eq. (1) is the objective function of the problem. Eq. (2) denotes that a vehicle must travel from one node to a different one. Eq. (3) indicates thatXijkis equal to 1 if vehicle k goes from node i to node j, and it is equal to 0 otherwise, i.e. a route between two customers can or cannot be covered by a vehicle. Eq. (4) states that a customer is visited once by exactly one vehicle. By specifying the constraint of Eq. (5), it is taken into account that for a given vehicle k, the load that has to be transported to complete the routes assigned to such vehicle cannot exceed its capacity Qk(it is considered that all vehicles have the same capacity, Qk=Q). Eq. (6) specifies that there are up to K routes going out of the delivery depot. Eq. (7) guarantees that the vehicles depart from and return to the depot. As specified in [24], variable skjis the time vehicle k starts to service customer j (as commented above, it is equivalent to the total distance traveled by vehicle k to arrive at customer j). Thus, the inequality represented in Eq. (8) specifies that, if vehicle k is traveling from customer i to customer j, the vehicle cannot arrive at customer j before ski+Cij. Eq. (9) ensures that time windows are observed.Deterministic and approximated methods often perform poorly in intermediate and large problem instances of VRPs, including the VRPTW, due to the fact that the number of feasible solutions increase exponentially with the number of customers and vehicles. In this context, parallelization provides benefits in practical settings where quality solutions must be produced within a short time interval. Since there is a clear trade-off between computational times and the quality of the solution, parallel processing can increase service quality by allowing a larger number of requests to be routed within a reasonable computation time. Several parallel algorithms have been implemented for solving VRPs [29]. One of the pioneers on using parallel processing in the field was Taillard [45], who presented a parallel method that decomposes the VRP into polar regions including a large number of vehicles into sub-problems that were solved independently. Other authors have proposed a parallel tabu search heuristic for the VRPTW that used a master-worker scheme, where each worker performs a tabu search and the master coordinates the work and feeds the workers with new starting solutions [5]. Parallel tabu search was also applied in [7], obtaining as conclusion that the asynchronous master-worker algorithm outperformed the synchronized algorithm in terms of the speedup, while also obtaining results that are comparable to those found by the synchronous and the sequential tabu search. Other approaches also apply parallel tabu search for solving the VRP without time-windows (i.e. CVRP) by using the master-worker model, where each worker executes a tabu search procedure with different parameters and the master is responsible for collecting the best local solution from each worker process and transmitting the best among them to workers, to start up a new iteration of the method [41]. In [8] the VRPTW is solved using the master-worker message-passing paradigm where the master component coordinates genetic operations and handles parent selection while the worker elements concurrently execute reproduction and mutation operators. Parallel simulated annealing algorithm implemented with the master-worker model has been applied to optimize a mixed integer programming mathematical formulation of the vehicle routing problem with simultaneous pickup-delivery and time windows [49]. With the aim of reducing the communication bottleneck some researchers have presented a parallel algorithm for the VRP based on the master-worker model in which information is exchanged by reading and writing the files containing the routes discovered by the workers [26]. The master-worker parallelization for solving the two-phase VRPTW was also considered in [25], where the number of vehicles is minimized in the first search phase with the minimization of the total travel distance in the second search phase, and concluded that it gains in terms of the computing time required to generate solutions of a given quality.In addition to the use of the master-worker parallelizations, other cooperative approaches have also been applied for solving VRPs. Thus, in [6] it is presented a cooperative population learning algorithm for VRPTW in which different agents search in parallel and cooperate by exchanging information about solutions or other search space characteristics. Other authors present a parallel cooperative multi-search method for the VRPTW in which search threads cooperate by asynchronously exchanging information on the best solutions identified [35]. In [9] it is minimized the total route cost of the multi-depot vehicle routing problem by applying a cooperative coevolutionary algorithm in which customers are distributed among the depots based on their distance from the depots and closest neighbors.There is an increasing interest on the design of faster methods for solving VRPs incorporating speedup techniques such as parallel implementations of heuristics [13]. To our knowledge, the parallelization of meta-heuristic methods for solving the VRPTW using different parallel strategies and software libraries on multi-core processors not been reported in the literature. This section describes the characteristics of the sequential algorithm, that is based on a population-based simulated annealing strategy, called multiple-temperature simulated annealing (MT-SA), and the way in which this sequential algorithm has been parallelized using the model based on the concurrent execution of iterations of the corresponding algorithm (see Fig. 1), with two alternative implementations, the master-worker and island-based paradigms, and software libraries, MPI and OpenMP.Some basic aspects of the implementation, such as the representation of the solutions, the construction of initial solutions, the search operators, the selection strategy, and the termination conditions are now introduced.The integer representation is considered to represent the solutions. Since our approach is a population-based extension of simulated annealing, this population P consists of p individuals (solutions), P={I1, I2, …, Ip}, where each individual represents the routes traveled by K vehicles to deliver all the customers. Thus, each individual, Ii, is represented by a set of chromosomes, Cik, which consists of a variable number of genes,Cik={1,Gik1,Gik2,…,Gikl,1}representing the route of the kth vehicle in the ith individual, where2≤Gikj≤N. For example, chromosome C7,2={1, 11, 72, 37, 25, 1} indicates that the second vehicle of the seventh individual departs from the depot and visits customers 11, 72, 37, and 25, before returning to the depot, which is represented by identifier 1.The initial routes are built by using three fast strategies, all of them taking into accounting that it is mandatory to satisfy the time windows and capacity constraints. The first one consists in randomly assigning customers to vehicles until all the former are visited by the latter. The second strategy is similar to the first one, but the customers are inserted according to their identifier. Finally, the third approach assigns customers to vehicles in ascending order of time windows, i.e. those non-assigned customers with the earliest time in their time windows are visited first.Once the initial solutions have been built using one of the initialization strategies described above (e.g. Fig. 3(a)), these individuals are optimized by applying variation (mutation) operators. The literature provides a large number of mutation operators for VRP [1,24,46]. In order to diversify the search, we have implemented ten of these operators. A brief description of these operators is now provided:•Customer random reallocation: a vehicle and a random customer associated with it are chosen, then this customer is reallocated to another position of the same vehicle if the new solution is feasible.Customer best reallocation: a vehicle and a random customer associated with it are chosen, then the reallocation of this customer between all the pairs of subsequent customers of its vehicle is analyzed and it is finally inserted in the feasible position with the highest positive gain.Customer random migration: a vehicle and a random customer associated with it are chosen, then the reallocation of this customer to another non-empty vehicle is tried, and is accepted if it is feasible.Customer best migration: a vehicle and a random customer associated with it are chosen, then the reallocation of this customer between all the pairs of subsequent customers of other non-empty vehicle is analyzed and it is finally inserted in the feasible position with the best fitness.Customers random exchange: two vehicles and a random customer associated with each of them are chosen, then exchanging the customers if it is feasible.Customers best exchange: two vehicles are chosen, then exchanging the pair of customers with the highest positive gain.Customers exchange with coincident time-window: a vehicle and a random customer associated with it are chosen. The exchange of this customer with that customer from other vehicle who has coincident time windows, i.e. more or less the same start time and end time, is analyzed, accepting the exchange if it is feasible.Route partition: a vehicle and a random customer associated with it are chosen. The route covered by that vehicle is then divided in two, so that those customers originally visited earlier to that random customer are visited by the same vehicle, while that random customer and those visited later are assigned to a non-used vehicle.New route: a vehicle and a random customer associated with it are chosen, then assigning that random customer as the first and only customer of one of the non-used vehicles of the fleet.Route elimination: a vehicle is randomly chosen and, one after the other, all of the customers are reallocated in feasible positions of other routes.When applying variation operators, it is important to determine whether to accept the offspring solutions or not. The sequential algorithm here presented is based on accepting or rejecting the offspring individuals according to the Metropolis criterion [39] often used by simulated annealing (SA) [32]. SA is a local search meta-heuristic that simulates the process undergone by misplaced atoms in a metal when heated and then slowly cooled. SA optimizes a solution by exposing it to a high initial temperature, Ti, cooling it by means of a cooling rate, Tcooling, until the temperature falls below a given threshold, Tstop. Therefore, better neighboring solutions are always accepted, whereas worse solutions are accepted with a certain probability, which is dependent on the current temperature, t. Variable t is included within the Metropolis criterion [39] and acts, simultaneously, as a control variable for the number of iterations of the algorithm and as a probability factor for a definite solution to be accepted. The decrease of t reduces the probability of accepting movements that worsen the cost function. Some authors have concluded that the use of different annealing parameters in the same run increases the diversification and mitigates the random effect of using a fixed annealing scheme [2]. Therefore, our approach also considers the establishment of an interval of initial temperatures[Timin,Timax], so that the initial temperature of individual I1 isTimin, while individual Ipstarts inTimax, and the others are equally distributed along this interval.Several termination conditions can be established, including a maximum number of iterations, a maximum number of evaluations of the fitness function, a maximum runtime, or that the execution finishes when the temperature falls below a given threshold. Fig. 3(c) shows a possible final solution obtained after the execution of MT-SA.The goal of the parallel implementations presented here is to obtain solutions of a higher quality than the sequential algorithms. OpenMP programs start with a single thread (master thread), which creates parallel worker threads (fork) in parallel regions, such that statements in parallel block are executed in parallel by every thread until at end of parallel region all threads synchronize and join master thread (join), i.e. OpenMP uses a strategy similar to the master-worker paradigm. On the other hand, MPI programs allows to apply distributed strategies in which parallel tasks exchange data through passing messages to one another, i.e. MPI allows to simulate the operation of island parallel model, in which each island evolves a different population of individuals and best individuals are shared between islands periodically. With the aim of implementing parallel algorithms that present the same characteristics of the sequential code (a parallel simulation of the sequential code), both the master-worker and the island implementations have been implemented using synchronous communications, i.e. asynchronous message passing (MPI) and the nowait clause (OpenMP) have not been considered. Both paradigms have been adapted to our problem in the following way:(i)Master-worker paradigm with OpenMP (see Algorithm 1): the master thread initializes the population of solutions (each one containing a valid set of routes to visit all the customers satisfying the constraints), and, in each iteration, the master thread distributes the p individuals of population P into the number of thread executed (NP), including itself, so that each thread is in charge of optimizing p/NP individuals according to the variation operators (Mut_Ops) and the Metropolis function. Once the worker threads have computed their assigned solutions, they return them to the master thread, which re-orders the pool of solutions, and distributes again the work among all the available threads. The master thread is also responsible of controlling the termination condition and returning that solution having the best fitness.Algorithm 1Master-worker paradigm with OpenMP#include <omp.h>Input: p, NUM_THREADS, Probmut, Iter_SA, Tmin, Tmax, Tstop;If (omp_get_thread_num()=0) thenFor each Ii∈PIi←InitializeIndividuals(i);Ii.Ti←Tmin+i*((Tmax−Tmin)/p);Ii.Tcooling←Cooling(i, Tmin, Tmax, Tstop, Iter_SA);For each j∈Mut_OpsIi.Probmut(j)←rand(5,15);Adjust_Probabilities_To_Sum_100pc(Ii);While (TerminationCondition=FALSE)omp_set_dynamic();omp_set_num_threads(NUM_THREADS);#pragma omp parallel#pragma omp forFor each Ii∈PI′i←Mutation(Probmut, Ii.Probmut(j)); ∀j∈Mut_Ops;Ii←Metropolis(Ii,I′i,Ii.t);Ii.t←Ii.t*Ii.Tcooling;If (omp_get_thread_num()=0) thenReorder_Pool_Individuals();If (omp_get_thread_num()=0) thenReturn (Best_Global_Solution);Island paradigm with MPI (see Algorithm 2): each process (island) initializes and optimizes p/NP individuals autonomously. Periodically, the best solution of each island is sent to a central process which, temporally, is responsible of determining the global best solution and distributing it between the remaining islands. These islands are responsible for copying the received solution in a given percentage of solutions of the population, after which they continue the search process. When the termination condition is fulfilled, all the islands send the solutions to the central process, which returns the global best solution.Algorithm 2Island paradigm with MPI (syncronous version)#include <mpi.h>Input: p, NUM_PROCESSES, CR, Probmut, Iter_SA, Tmin, Tmax, Tstop;MPI_Init(&argc, &argv);MPI (MPI_COMM_WORLD, &process_ID);MPI_Comm_size(MPI_COMM_WORLD, & NUM_PROCESSES);For each Ii∈Pprocess_IDIi←InitializeIndividuals(i);Ii.Ti←Tmin+i*((Tmax−Tmin)/p);Ii.Tcooling←Cooling(i, Tmin, Tmax, Tstop, Iter_SA);For each j∈Mut_OpsIi.Probmut(j)←rand(5,15);Adjust_Probabilities_To_Sum_100pc(Ii);While (TerminationCondition=FALSE)For each Ii∈Pprocess_ IDI′i←Mutation(Probmut, Ii.Probmut(j)); ∀j∈Mut_ Ops;Ii←Metropolis(Ii,I′i,Ii.t);Ii.t←Ii.t*Ii.Tcooling;If (current_iteration mod CR=0) thenIf (process_ID=0) thenFor each process (island) PR(id); 0<id<num_processes;MPI_Recv(&Best_individual,...,PR(id),...);Best_global_individual←BestPoolSolutionsReceived();For each process (island) PR(id); 0<id<num_processes;MPI_Send(&Best_global_individual,...,PR(id),...);Else If (process_ID>0) thenMPI_Send(&My_Best_individual,...,PR(0),...);MPI_Recv(&Best_global_individual,...,PR(0),...);Copy(Best_global_individual,%_Individuals_Replaced);If (process_ID=0) thenReturn (Best_global_individual);The parallel architecture used in our empirical study is a workstation with a single Intel Core 2 Quad Processor Q6600 (4 cores, 2.40GHz, 1066MHz front-side bus, 8MB Cache, 4GB RAM). The sequential algorithm, coded in C++, has been parallelized using MPI (MPICH2 version 1.2.1p1), and OpenMP (version 3.1).The performance evaluation of the implemented algorithms is analyzed using the Solomon's benchmark problems [44], which are composed by a total of 56 instances divided into six groups of between 8 and 12 problem instances, and often used for benchmarking algorithms for VRPTW. Each benchmark contains a total of 100 customers and one central depot, and defines the characteristics of the fleet (maximum number of vehicles and their capacities) and customers (geometric coordinates, demand, time-windows and service time). The depot's service time and demand are zero, the vehicle fleet is homogeneous and the capacity is defined according to the data set. The distance between two customers is calculated from their Cartesian coordinates using Euclidean distances, which are also used as time units. Each customer has a given time window which represents the time interval to arrive at that customer. The capacity of the vehicles and the customer's demands are defined in each type of instance. These six categories are called C1, C2, R1, R2, RC1, and RC2, where the alphabetic characters indicate the spatial customer distribution and the number indicates the time windows constraints. In reference to the spatial customer location, problem category R has all customers located randomly, problem category C has all customers clustered in groups, and problem category RC has a mixture of random and clustered customers. Therefore, the customers are located closer to each other in problem category C than in R, implying the latter category a longer total traveling distance, thus making it more difficult to be solved, since a small change in the sequence of visited customers of a given route may result in larger differences in terms of the total routing distance. The problems in category 1 (C1, R1, and RC1) have smaller time windows than those belonging to category 2 (C2, R2, and RC2), implying that some candidate solutions are more likely to become unfeasible after a small change in the sequence of visited customers in problems of category 1 than in those of category 2. Furthermore, the time window is smaller at the depot for the instances of group 1 than for those of group 2, which means that a smaller number of customers can be served by one vehicle in benchmarks belonging to group 1. In each group of problems, the geographical distribution of customers, the demand and the service time do not change, while they differ in the percentage of customers with time windows and in the time windows intervals. Tan et al. [46] described in detail the Solomon's benchmark problems. The performance of the parallel implementations is tested on two of the problem instances of each of the six groups that form Solomon's benchmark: C103, C108, C203, C208, R103, R108, R203, R208, RC103, RC108, RC203, and RC208, which are representative of all C/R/RC-types. In reference to the parameter settings, the sequential algorithm uses a population of 200 individuals (|P|=p=200). These 200 individuals are distributed among the islands (assigned to cores). The individuals of the population are initialized using the three heuristics described above. The probability of applying a mutation operator is 25%. If mutation is applied, each of the ten mutation variants is applied with a probability that oscillates between 5% and 15% (all them sum 100%). The use of different probabilities for those ten operators in each individual allows balancing convergence and diversity. For example, those individuals with higher probabilities of applying mutation variants Customer best reallocation, Customer best migration or Customer best exchange have a higher convergence pressure than those with higher probabilities of applying Route partition or New route, which will explore different areas of the search space.As it was previously commented, each individual has its own particular annealing scheduling, so that an initial interval of temperatures Ti=[10, 1000] and a slow cooling rate (Tcooling=0.995) is considered, while the minimum temperature is Tstop=0.001. If the termination condition is not fulfilled and the current temperature falls below Tstop, the temperature is reinitialized (t=Ti) and the search process continues. When processes or threads communicate to share their best found solutions, the best one is copied in the 25% of the solutions of the population (master-worker paradigm) or each island (island paradigm). According to our preliminary results, this percentage (25%) is an accurate trade-off between the search process independence derived from the island model and the elitist pressure derived from the meta-heuristic, while a higher percentage becomes disruptive. To justify the effectiveness of the parallel simulated annealing approach using the master-worker model with OpenMP and the island paradigm with MPI, both parallel implementations are executed using two, four, and eight processes (or threads). When comparing different algorithms or implementations, it is possible to determine that one technique is better than another one if it obtains a better performance at a given amount of computational cost. For computational optimization practitioners, this cost is typically measured considering a maximum number of fitness evaluations for all the methods, but this criterion assumes that the cost of other operations is either the same or almost the same in both algorithms. However, the VRPTW requires not only the evaluation of the fitness of the solutions and the constraints satisfaction, but also the performance of other expensive operations such as updating the temperature, accepting or rejecting solutions according to the Metropolis criterion, etc. Moreover, it is unknown whether the island paradigm using MPI would expend more clock cycles to perform the message passing than the master-worker parallelization using OpenMP to manage the fork-and-join execution model or not. Thus, the runtime seems to be the best way to measure the computational cost and it has been recently used as termination criterion in the context of routing problems [20].

@&#CONCLUSIONS@&#
The parallelization of heuristic methods allows the researchers both to explore the solution space more extensively and to accelerate the search process. The generalization of multi-core processors allows us to take advantage of parallel processing even on desktop and laptop computers to solve hard problems arising in real life optimization problems. The design of efficient methods for solving vehicle routing problems has become an area of research that has attracted much attention due to its influence in transportation, logistics, supply chain management, and many other practical approaches. Often, the number of customers combined with the complexity of real-life data does not produce an exact solution of the problem using deterministic approaches, being then necessary to apply approximated methods, such as heuristic approaches, which are often applied to obtain good quality solutions in runtimes which are acceptable for transportation and logistics managers. Nevertheless, whenever the number of customers is very large, it is necessary to incorporate techniques to improve the efficiency of these heuristics and the use of parallel algorithms is well suited for this purpose. This way, different strategies for parallelization, the master-worker and island paradigms programmed with OpenMP and MPI patterns, respectively, are analyzed for a population-based meta-heuristic based on Simulated Annealing, the Multi-Temperature Simulated Annealing, which is proposed by the authors for the vehicle routing problem with time windows (VRPTW). With this aim, several independent executions of both parallel implementations have been performed and evaluated in a computer with a shared memory multi-core microprocessor to obtain results with statistical significance on a subset of the well-known Solomon's benchmarks. These results show that parallelized algorithms improve the performance of the sequential algorithm with respect to the quality of the solutions and runtimes for the considered benchmarks. The Analysis of Variance (ANOVA) and Tukey's tests show that the parallel implementations and the number of processes have an important influence on the quality of the solutions, but this importance decreases when the runtime grows, i.e. when the algorithms converge to solutions of good quality. Since this investigation aims to compare both parallel models and software libraries in a real application in low-cost multi-core systems, two main conclusions are obtained: the master-worker paradigm programmed with OpenMP is able to improve its performance even when using a number of threads higher than the number of available cores in the processor; the island model programmed with MPI outperforms the master-worker paradigm programmed with OpenMP when the number of processes is not higher than the number of cores and when the number of solutions in each sub-population (island) is sufficiently large, while its performance is clearly reduced when these conditions are not satisfied.