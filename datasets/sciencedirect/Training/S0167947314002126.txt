@&#MAIN-TITLE@&#
Consistent and asymptotically normal PLS estimators for linear structural equations

@&#HIGHLIGHTS@&#
Consistent PLS estimates path coefficients and indicator loadings consistently.Consistent PLS can estimate parameters of nonrecursive structural equation models.A family of goodness-of-fit measures makes PLS suitable for confirmatory research.Consistent PLS performs comparably to covariance-based structural equation modeling.

@&#KEYPHRASES@&#
Partial least squares,Structural equation modeling,Consistency,Recursiveness,Goodness-of-fit,

@&#ABSTRACT@&#
A vital extension to partial least squares (PLS) path modeling is introduced: consistency. While maintaining all the strengths of PLS, the consistent version provides two key improvements. Path coefficients, parameters of simultaneous equations, construct correlations, and indicator loadings are estimated consistently. The global goodness-of-fit of the structural model can also now be assessed, which makes PLS suitable for confirmatory research. A Monte Carlo simulation illustrates the new approach and compares it with covariance-based structural equation modeling.

@&#INTRODUCTION@&#
Structural equation modeling (SEM) has become the tool of the trade in survey-based research. Researchers embrace its abilities, such as modeling latent variables and correcting for measurement error, while simultaneously estimating parameters of entire theories. Two families of structural equation modeling techniques prevail (Chin, 1998; Reinartz et al., 2009): covariance-based SEM and variance-based SEM. The latter appears to be increasingly popular, as seen in recent methodological advances (cf.  Bry et al., 2012; Hwang et al., 2010; Lu et al., 2011; Tenenhaus and Tenenhaus, 2011) and frequent application (Ringle et al., 2012; Hair et al., 2012). Researchers appreciate the advantages of variance-based SEM, such as the lack of convergence problems and factor indeterminacy (Henseler, 2010), relatively mild distributional assumptions (Reinartz et al., 2009), and the possibility of estimating models having more variables or parameters than observations. Variance-based SEM includes different techniques, such as regression based on sum scores or principal components (Tenenhaus, 2008), partial least squares path modeling (PLS, see  Wold, 1982; Tenenhaus et al., 2005), and generalized structured component analysis (Hwang et al., 2010; Henseler, 2012). All variance-based SEM techniques have the characteristic of approximating latent variables using linear composites of observed variables.Among variance-based SEM techniques, PLS path modeling is regarded as the “most fully developed and general system” (McDonald, 1996, p. 240). Various extensions and advances of PLS path modeling have been developed and discussed, such as multigroup analysis (Chin and Dibbern, 2010; Sarstedt et al., 2011), testing moderating effects (Chin et al., 2003; Henseler and Chin, 2010), assessing common method bias (Liang et al., 2007; Chin et al., forthcoming), testing measurement invariance (Hsieh et al., 2008; Money et al., 2012), modeling nonlinear relationships (Henseler et al., 2012), and analyzing hierarchical component models (Ringle et al., 2012; Wetzels et al., 2009). The large number of extensions and advances are in line with the widespread dissemination of PLS path modeling across business disciplines such as management information systems (cf.  Ringle et al., 2012), marketing (cf.  Hair et al., 2012; Henseler et al., 2009), strategy (cf.  Hulland, 1999), or operations management (cf.  Peng and Lai, 2012).Yet, the use of PLS as an estimator for structural equation models is not without disadvantages. First, PLS estimates–particularly path coefficients and loadings–are only consistent at large (Wold, 1982). Consequently, “(p)arameter estimates for paths between observed variables and latent variable proxies are biased upward in PLS (away from zero), while parameter estimates for paths between proxies are attenuated” (Gefen et al., 2011, p. vi). Second, PLS does not provide overall goodness-of-fit measures. This means it is impossible to test or compare theories, as is done with covariance-based SEM, is not possible (Fornell and Bookstein, 1982; Henseler and Sarstedt, 2013). Of these two deficiencies of PLS, the lack of consistency is probably more serious, because of its adverse consequences for substantial research findings. If PLS underestimates the true parameter, Type II errors are likely. If PLS overestimates the true parameter, the Type I error is inflated. Finally, the lack of consistency entails that there is no guarantee that meta studies based on PLS estimates come closer to the true value than single studies.Against this background, we introduce an important advancement to PLS that overcomes these deficiencies: consistent PLS. While maintaining all the strengths of PLS, consistent PLS provides several key improvements. It consistently estimates the path coefficients, construct correlations, and indicator loadings; it allows non-recursive models to be estimated; and it provides a global assessment of goodness-of-fit. The outcomes of a Monte Carlo simulation help compare the performance of PLS, consistent PLS, and covariance-based SEM. Finally, implications for future research are provided.Scandinavian econometrician and statistician Herman Wold developed partial least squares in the 1960s, 1970s, and 1980s to analyze high-dimensional data that reflects unobserved entities that are interconnected in path diagrams (Wold, 1966, 1975, 1982). The inspiration were principal components and canonical variables. Linear compounds are constructed to serve as proxies or stand-ins for the latent variables, leading to straightforward estimates of structural parameters, such as path coefficients and loadings. Compound weights are generated, using a variety of alternating least squares algorithms. These are cycles of regressions that update natural subsets of weights in turn, stopping when consecutive estimates no longer change significantly. Convergence, which is the rule, is typically very fast. PLS has become a vibrant field of both applied and theoretical research; see, for example, Tenenhaus et al. (2005) and the Handbook of Partial Least Squares by Vinzi et al. (2010) for overviews.In the spirit of principal components and canonical variables, PLS has been useful as a method to extract information from high-dimensional data. However, as a means of estimating the parameters of latent variable models, PLS has shortcoming: the relationships between linear compounds can never duplicate the relationships between the latent variables. The simple and fundamental reason is that no linear combination of the indicators of a block can ever replicate the corresponding latent variable, except when some measurement errors are zero (for an analysis of the general case see  Krijnen et al., 1998). See Section  2.3 and Eq. (14) for the fundamental relationship between the correlations among the proxies, the true correlations among the latent variables, and those between each proxy and corresponding latent variable. In fact, in linear factor models, PLS tends to overestimate the absolute value of loadings and underestimate the multiple and bivariate (absolute) correlation coefficients. Dijkstra (1981, 1983, 2010) shows how to correct for this tendency. The consistent version of PLS is denoted by PLSc. Subsequent sections outline the PLSc approach and show that it gives consistent and asymptotically normal estimators (CAN estimators) for the focal parameters.A starting point for PLS analysis is the “basic design”, in essence a second-order factor model. A number of i.i.d. (independent and identically distributed) vectors of indicators are assumed to exist from a population with finite moments of at least order two (the precise order depends on other distributional assumptions or requirements). All indicators have zero mean and unit variance. The vector of indicatorsyis composed of at least two subvectors, each measuring a unique latent variable, and each subvector contains at least two components. For theith subvectoryiwe have(1)yi=λi⋅ηi+ϵiwhere the loading vectorλiand the vector of idiosyncratic errorsϵihave the same dimensions asyi, and the unobservable latent variableηiis real-valued. For convenience, the sufficient but by no means necessary assumption is made that all components of all error vectors are mutually independent, and independent of all latent variables. The latter has zero mean and unit variance. The correlations betweenηiandηjare denoted byρij. They are collected in a matrixΦ,Φ≔(ρij). At this stage, the nature of the relationships between the latent variables–whether linear or nonlinear–is not relevant.A particular set of easy implications is that the covariance matrixΣiiofyican be written as(2)Σii≔Eyiyi⊤=λiλi⊤+ΘiwhereΘi, the covariance matrix of the measurement errors of theith latent variable, is diagonal with non-negative diagonal elements, and we have for the covariance betweenyiandyj(3)Σij≔Eyiyj⊤=ρijλiλj⊤.The sample counterparts ofΣiiandΣijare denoted bySiiandSij, respectively. The sample data is assumed to be standardized before being analyzed. Therefore, the observed data has zero mean and unit (sample) variance. Note that the assumptions made so far entail that the sample counterparts are consistent and asymptotically normal estimators of the theoretical variance and covariance matrices.PLS features a number of iterative fixed-point algorithms, of which the mode A algorithm is selected. In general, the mode A algorithm is numerically the most stable algorithm (for discussions of other PLS modes, see  Lohmöller, 1989). As a rule, the algorithm converges and is usually very fast (for example, for the models analyzed in this paper, no more than five iterations are needed to obtain five significant decimals). The outcome is an estimated weight vectorŵwith typical subvectorŵiof the same dimensions asyi. With these weights, sample proxies are defined for the latent variables:η̂i≔ŵi⊤yiforηi, with the customary normalization of a unit sampling variance, soŵi⊤Siiŵi=1. In Wold’s PLS approach, theη̂i’s replace the unobserved latent variables, and the loadings and structural parameters are estimated directly, in contrast to covariance-based SEM, which, for example, follows the opposite order. In mode A, for eachi:(4)ŵi∝∑j∈C(i)sgnij⋅Sijŵj.Here,sgnijis the sign of the sample correlation betweenη̂iandη̂jandC(i)is a set of indices of latent variables. Traditionally,C(i)contains the indices of latent variables adjacent toηi, in other words, those that appear on the other side of the structural or path equations in whichηiappears. This setup is not always a good idea, particularly when the correlations between the indicators ofηiand those of its adjacent variables are weak. In general, the use of allj≠iis suggested in Eq. (4). Clearly,ŵiis obtained by a regression of the indicatorsyion the sign-weighted sum of the selected proxies:∑j∈C(i)sgnij⋅η̂j. Other versions exist (for example, with correlation weights); this version, which is the original, is one of the simplest (see  Wold, 1982). There is little motivation in the PLS literature for the coefficients ofSijŵj. The particular choice can be shown to be irrelevant for the probability limits of the estimators. The algorithm takes an arbitrary starting vector and then basically follows the sequence of regressions for eachi, each time inserting updates when available (or after each full round; the precise implementation is not important).Dijkstra (1981, 2010) showed that the PLS modes converge with a probability that tends to one when the sample size tends to infinity for essentially arbitrary starting vectors. The weight vectors that satisfy the fixed-point equations are locally continuously differentiable functions of the sample covariance matrix ofy. Therefore, they and other estimators that depend smoothly on the weight vectors andSare jointly asymptotically normal.Let us denote the probability limit ofŵi, plimŵi, byw¯i. We can get it from Eq. (4) by substitutingΣforS. So(5)w¯i∝∑j∈C(i)sgnij⋅Σijw¯i=∑j∈C(i)sgnij⋅ρij⋅λi⋅λj⊺w¯j=λi∑j∈C(i)sgnij⋅ρij⋅λj⊺w¯jwhere the last sum is just a real number, and we conclude thatw¯iis proportional toλi. Because of the normalization (unit variance) the proportionality constant has to be such thatw¯i⊺Σiiw¯i=1. This entails that(6)w¯i=λiλi⊺Σiiλi.In PLS, the loadings are estimated by a regression of the indicatorsyion their direct sample proxyη̂i. However, because doing so in general removes the proportionality, this tradition is not followed (for mode A).As in Dijkstra (1981, 2010), the following estimator forλiis proposed:(7)λ̂i≔ĉi⋅ŵi,where the scalarĉiis such that the off-diagonal elements ofSiiare reproduced as best as possible in a least squares sense. Therefore, the Euclidean distance is minimized between:[Sii−diag(Sii)]and[(ci⋅ŵi)(ci⋅ŵi)⊤−diag((ci⋅ŵi)(ci⋅ŵi)⊤)]as a function ofciand the following is obtained:(8)ĉi≔ŵi⊤(Sii−diag(Sii))ŵiŵi⊤(ŵiŵi⊤−diag(ŵiŵi⊤))ŵi.More explicitly,(9)ĉi=[∑a≠bŵa,iŵb,iSii,ab∑a≠bŵa,i2ŵb,i2]12,whereŵa,iis element ‘a’ of the weight vectorŵi,ŵb,iis defined similarly, andSii,abis the sample covariance between the indicators ‘a’ and ‘b’ of theith block. When pairs of errors of the same block are suspected to be correlated, one can delete the corresponding terms in both numerator and denominator.11For extensions and alternative approaches, see Dijkstra (2013).In sufficiently large samples,ĉiwill be well-defined, real, and positive. (In all samples in this paper and those in another study,ĉiattained proper values.) Its calculation does not require additional numerical optimization. Verifying that the correction does its job is straightforward by replacingSiibyΣiiandŵibyw¯i: the matrix in the denominator equals the matrix in the numerator, apart from a factor(λi⊤Σiiλi)−1; therefore:(10)c¯i≔plimĉi=λi⊤Σiiλi.Now, in particular(11)plimλ̂i=plim(ĉi⋅ŵi)=c¯i⋅w¯i=λi.Defining a population proxyη¯ibyη¯i≔w¯i⊤yiis useful. Clearly, the squared correlation between a population proxy and its corresponding latent variable is(12)R2(ηi,η¯i)=(w¯i⊤λi)2,which equals(13)(λi⊤λi)2λi⊤Σiiλi=(λi⊤λi)2(λi⊤λi)2+λi⊤Θiλi.With a “large” number of “high quality” indicators, this correlation can be close to one (“consistency at large” in PLS parlance). A trivially deduced but important algebraic relationship is(14)R2(η¯i,η¯j)=(w¯i⊤Σijw¯j)2=ρij2⋅R2(ηi,η¯i)⋅R2(ηj,η¯j).Because proxies can never replicate the latent variables exactly, barring special situations where measurement errors are identically zero, entails that their correlation matrix can never equal the one that describes the relationships between the latent variables. The probability limits of the bivariate squared correlations are too small, as are those of the multiple correlations (see  Dijkstra, 2010). In addition, regression coefficients and coefficients of simultaneous equations are misrepresented. See Section  3.3 for a numerical example.Now note that(15)R2(ηi,η¯i)=(w¯i⊤λi)2=(w¯i⊤⋅(w¯i⋅c¯i))2=(w¯i⊤w¯i)2⋅c¯i2enabling an estimation of the (squared) quality of the proxies consistently by(16)R2(ηi,η¯i)̂≔(ŵi⊤ŵi)2⋅ĉi2.Also, with(17)R2(η¯i,η¯j)̂≔(ŵi⊤Sijŵj)2,see Eq. (14); the correlations between the latent variables can be consistently estimated using(18)ρ̂ij2≔R2(η¯i,η¯j)̂R2(ηi,η¯i)̂⋅R2(ηj,η¯j)̂.Therefore,Eηiηjis estimated using the sample covariance between the proxiesη̂iandη̂j, each divided by its estimated quality. Finally, letΦ̂≔(ρ̂ij).Note that standard PLS software for mode A produces all the necessary ingredients for a consistent estimation; all that is required is a simple rescaling of the weight vectors.22An alternative approach, even simpler than PLS, would be to use fixed weights without iterations. See Dijkstra (2013) and Dijkstra and Schermelleh-Engel (2013), Section 2.4. The estimators of structural parameters and loadings, etc. will depend on the weights chosen. This is unlike PLS, which neutralizes the initial choice by iterating until a fixed point appears. Since PLS is so fast, the numerical gains may be modest, but the statistical merits of the fixed weights certainly need to be investigated. They are currently examined in the context of non-linear factor models.It is important to note that when the latent variables are mutually related through linear equations, whether recursively or with feedback patterns, CAN estimates of their coefficients are also obtainable. This can happen if they are identifiable from the second-order moment matrix of the latent variables through smooth (locally continuously differentiable) mappings. In principle, partially identifiable structures can also be handled using Bekker and Dijkstra (1990) and Bekker et al. (1994).The method of choice in this study is the old econometric workhorse 2SLS (two-stage least squares). 2SLS estimates each equation separately and is a limited-information technique. It is probably the simplest estimation method around, and does not–despite appearances–require additional iterations. Boardman et al. (1981) employed an iterative version of 2SLS, Wold’s fix-point method, using the original PLS input, not corrected for inconsistency.To be complete, the 2SLS method is specified here. To this end, consider the linear structural equations(19)ηp+1:p+q=B⋅ηp+1:p+q+Γ⋅η1:p+ζ,whereηis partitioned into a vector ofpcomponents,η1:p, the exogenous latent variables, and a vector ofqcomponents,ηp+1:p+q, the endogenous latent variables. The residual vectorζhas zero mean and is uncorrelated with (or independent of)η1:p. Theq×qmatrixBcaptures the feedback or reciprocal relationships between the endogenous variables, and is such that the inverse ofI−Bexists. The latter assumption enables one to write, withΠ≔(I−B)−1Γ(20)ηp+1:p+q=Π⋅η1:p+(I−B)−1ζ,which is a set ofqregression equations. Identifiability is assumed, which means thatBandΓsatisfy zero constraints that allow the unambiguous recovery of the values of their free parameters from the knowledge ofΠ. As is well-known, this is equivalent to the specification of the ranks of certain sub-matrices ofΠand to the invertibility of a certain matrix, as specified below. The observation that led to 2SLS is that(21)ηp+1:p+q=B⋅(Π⋅η1:p)+Γ⋅η1:p+(I−B)−1ζ(withΓ=(I−B)⋅Π). Therefore, the free elements in a row ofBandΓare regression coefficients. They can be obtained through a regression of the corresponding endogenous variable on the predicted values of the endogenous variables on the right-hand side of the equation (the relevant elements ofΠ⋅η1:p), and the exogenous variables of the equation, which are the relevant elements ofη1:p.The solutions for theith row are spelled out. LetIiselect the free parameters in theith row ofB(therefore,Iiis a vector containing the positions in theith row ofBcorresponding to free parameters), and letJibe defined similarly for theith row ofΓ. Therefore, the column vector of free parameters in theith row ofB, denoted byβi, equalsB(i,Ii)⊤, and for the free parameters in theith row ofΓ, we defineγi≔Γ(i,Ji)⊤. Then(22)[βiγi]=[cov(Π(Ii,1:p)η1:p)E(Π(Ii,1:p)η1:p⋅ηJi⊤)cov(ηJi)]−1[E(Π(Ii,1:p)η1:p⋅ηp+i)E(ηJi⋅ηp+i)].Because of identifiability, the matrix inverted is invertible. Using straightforward algebra leads to the following:(23)[βiγi]=[Φ(p+Ii,1:p)Φ(1:p,1:p)−1Φ(1:p,p+Ii)Φ(p+Ii,Ji)Φ(Ji,p+Ii)Φ(Ji,Ji)]−1[Φ(p+Ii,1:p)Φ(1:p,1:p)−1Φ(1:p,p+i)Φ(Ji,p+i)].Φis the covariance (correlation) matrix ofη=[η1:p;ηp+1:p+q], which is taken to be invertible (no redundant latent variables).Eq. (23) clarifies how to obtain CAN estimators for the parameters of the structural equations: simply replaceΦby its CAN estimator derived in the previous section. The ensuing vector, with componentsβ̂iandγ̂i, is a smooth transformation (in the neighborhood of the true values) ofΦ̂. Evidently, straightforward estimators (direct sample counterparts) forΠand for the covariance matrices of the residuals (for the structural and reduced form) share the asymptotic properties. In fact, all parameter estimators derived so far, plus the implied estimator forΣ, are consistent and asymptotically jointly normal. They will not be asymptotically most efficient when the sample comes from a distribution like the Gaussian or an elliptical distribution: neither the weights, loadings, and correlations, nor the structural form coefficients are determined by taking all information optimally into account. There is also an advantage, although this study does not elaborate on it: full-information methods are potentially vulnerable to misspecification anywhere in the system; the approach outlined here can be expected to be more robust.Because of the speed of PLS, simulating the distribution of the estimators on the basis of the empirical distribution of the sample is quite feasible. Correction of bias, if any, and estimation of standard errors and confidence intervals are, in principle, well within reach. Some simulation results are reported below. Alternatively, one may use the delta method with the Jacobian matrix calculated numerically. One can then obtain estimates of the standard errors based on Gaussian or distribution-free asymptotic theory (through higher-order moments or the bootstrap).As the impliedΣ̂based on direct substitution of the corrected PLS estimators is consistent and asymptotically normal, one may consider the use of overall tests as in the covariance-based SEM literature by defining a proper distance such as the trace of the square of the residual matrixS−Σ̂. When scaled by the number of observations, the distance is distributed asymptotically as a non-negative linear combination of independentχ2(1)-variables. The coefficients are eigenvalues of a certain matrix that depends on the true parameters. One could replace them with appropriate estimates and employ a suitable approximation for the probability value. Alternatively, and more conveniently, the probability value can be estimated using the bootstrap. This requires a pre-multiplication of the observation vectors byΣ̂12S−12, meaning that the covariance matrix of their empirical distribution satisfies the assumed (H0) structure (see, for example,  Yuan and Hayashi, 2003, for a general discussion and elaboration in the context of covariance analysis). We report some simulation results below.To assess the quality of the estimators provided by consistent PLS, a computational experiment was conducted. In particular, the performance of consistent PLS is compared with covariance-based SEM, using a Monte Carlo simulation.The first illustrative test asks for a non-trivial model that is not too large. The experiment should be challenging, meaning that the sample size should be modest and the number of indicators small. It is investigated how accurate the limited information method PLSc performs when pitted against the most efficient alternative—full information maximum likelihood (FIML). The effect of non-normality will also be investigated.The model chosen is Summers’ (1965) classical model, which is used in many econometrics studies to directly observe endogenous and exogenous variables. Here, a latent vector is composed of six components,η1:6, linked to each other through the following equations:(24)η5:6=Bη5:6+Γη1:4+ζ,whereζis a two-dimensional residual vector independent ofη1:4, and the coefficient matrices take the following forms:(25)B≔[0β12β210]and(26)Γ≔[γ11γ120000γ23γ24].Spelled out:(27)η5=β12η6+γ11η1+γ12η2+ζ1(28)η6=β21η5+γ23η3+γ24η4+ζ2.The endogenous variables influence one another reciprocally (for a causal interpretation of these equations, see Pearl, 2009, who builds on Haavelmo, 1944). The structural form equations are not regressions, but the reduced form equations are, with(29)Π=11−β12β21[γ11γ12β12γ23β12γ24β21γ11β21γ12γ23γ24].Of course,1−β12β21≠0is required. All structural form coefficients can be recovered unambiguously fromΠif and only if each of the submatricesΠ(1:2,1:2)andΠ(1:2,3:4)has rank one. Therefore,γ11andγ12cannot both be zero, nor canγ23andγ24. The following coefficients were chosen:(30)B=[00.250.500]and(31)Γ=[−0.300.5000000.500.25].All covariances (correlations) between the exogenous latent variablesη1:4are equal to 0.5 and the correlation betweenη5andη6is0.5. All numbers are chosen arbitrarily. The regression matrix equals(32)Π=[−0.34290.57140.14290.0714−0.17140.28570.57140.2857].The correlation matrix between the latent variables can be verified as(33)Φ=[10.50.50.50.05000.40000.510.50.50.50710.62860.50.510.50.29290.77140.50.50.510.25710.62860.05000.50710.29290.257110.50.40000.62860.77140.62860.51].Note that the correlation betweenη5andη1is particularly weak (0.05), which may create a challenge for PLSc.One can verify that the R-squared for the first reduced form equation is a mere 0.3329, whereas the corresponding value for the second equation is a healthy 0.7314. The implied value for the covariance matrix ofζis(34)Σζζ=[0.5189−0.0295−0.02950.1054].As for theλ’s, the main experiment takes just three indicators per latent variable, and all components of all loading vectors equal 0.70 (making their squared correlation with their corresponding latent variable less than one half).The model contains 31 free parameters—two fromB, four fromΓ, six from the correlation matrix ofη1:4, one correlation betweenζ1andζ2, and 18 from the loadings. Using the normalizations, all other parameters can be derived from the ones referred to. Therefore, a sample sizenof 300 with about 10 observations per parameter seems modest. For some experiments,n=600andn=1200. The leading distribution is the multivariate normal distribution. With the parameters specified as above, we can determine the covariance matrixΣof size 18×18 and generatenrandom drawings by randn(n,18)⋅Σ12. The observation vectors are standardized and fed to PLSc, 2SLS is employed, and outcomes noted. This process is repeated 10,000 times.It is customary to study the effects of non-normality by the Fleishman–Vale–Maurelli procedure (see  Fleishman, 1978; Vale and Maurelli, 1983). In this approach, the standard normal latent variables are replaced by well-chosen linear combinations of powers of standard normal variables whose correlations are such that the new latent variables have the same correlations as the original latent variables. “Well-chosen” means that specified requirements concerning the non-normal skewness and (excess) kurtosis are satisfied. If the transformations as suggested maintain the independence between latent variables and idiosyncratic errors, the asymptotic robustness of normal-theory statistics may apply and lead one to believe incorrectly that normality is not an issue (see  Hu et al., 1992). We follow the latter authors in simply rescaling the vector of indicators by multiplying each component by the same independent random factor. Hu et al. (1992) chose3⋅(χ2(5))−12, whose squared value has expectation one. This approach deliberately destroys the independence between the latent and idiosyncratic variables, but leavesΣand the linear relationships (as well as the symmetry) undisturbed. The kurtosis of the indicators increases by six. The same effects can be obtained by multiplying by a standard normal variableZ, which appears to yield representative samples for smaller sizes; therefore, this approach is used. In addition, multiplication is employed by a positive scale factorabs(Z)⋅π2whose squared value also has expectation one. This multiplication increases the kurtosis by a lesser amount:3π2−3=1.7124.Although a comparison with the traditional PLS method is not the main goal of this paper, specifying the probability limits for the estimators of PLS mode A seems appropriate for the main parameters. Direct calculation using the true covariance matrixΣand 2SLS for the structural parameters yields the following probability limits:•for the loadings (regression of indicators on proxy): 0.8124 instead of 0.7for the correlations between the exogenous latent variables: 0.3712 instead of 0.5for the correlation between the endogenous latent variables: 0.5250 instead of 0.7071 (=0.5)forB:[00.29270.59380]instead of[00.250.500]forΓ:[−0.16110.299700000.36240.2188]instead of[−0.300.5000000.500.25]forΠ:[−0.19490.36280.12840.0775−0.11580.21540.43860.2648]instead of[−0.34290.57140.14290.0714−0.17140.28570.57140.2857].The implied squared correlations for the two reduced form equations are 0.1726 and 0.4421, to be compared with 0.3329 and 0.7314, respectively.The unrestricted regression matrix yields[−0.17050.36920.11620.0740−0.03130.23860.40720.2386].Clearly, when not corrected for inconsistency, PLS tends to give the wrong idea of the relative sizes of the parameters of the underlying covariance structure. Although the signs are correct, this is generally not guaranteed. Counterexamples for regression models are easy to provide (cf.  Dijkstra, 1981, pp. 75–76; a program generating counterexamples is available from the first author on request).

@&#CONCLUSIONS@&#
