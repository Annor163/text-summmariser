@&#MAIN-TITLE@&#
Partial mutual information based input variable selection for supervised learning approaches to voice activity detection

@&#HIGHLIGHTS@&#
Voice activity detector based on likelihood ratio and many distinct features.Substantial reduction in input vector dimension without undermining the detection performance.A supervised learning classifier significantly outperformed the statistical model-based detector.Among SVM, neural network and Boost, the latter showed more consistent performance.

@&#KEYPHRASES@&#
Voice activity detection,Partial mutual information,Supervised learning,Receiver operating characteristics curves,

@&#ABSTRACT@&#
The paper presents a novel approach for voice activity detection. The main idea behind the presented approach is to use, next to the likelihood ratio of a statistical model-based voice activity detector, a set of informative distinct features in order to, via a supervised learning approach, enhance the detection performance. The statistical model-based voice activity detector, which is chosen based on the comparison to other similar detectors in an earlier work, models the spectral envelope of the signal and we derive the likelihood ratio thereof. Furthermore, the likelihood ratio together with 70 other various features was meticulously analyzed with an input variable selection algorithm based on partial mutual information. The resulting analysis produced a 13 element reduced input vector which when compared to the full input vector did not undermine the detector performance. The evaluation is performed on a speech corpus consisting of recordings made by six different speakers, which were corrupted with three different types of noises and noise levels. In the end, we tested three different supervised learning algorithms for the task, namely, support vector machine, Boost, and artificial neural networks. The experimental analysis was performed by 10-fold cross-validation due to which threshold averaged receiver operating characteristics curves were constructed. Also, the area under the curve score and Matthew's correlation coefficient were calculated for both the three supervised learning classifiers and the statistical model-based voice activity detector. The results showed that the classifier with the reduced input vector significantly outperformed the standalone detector based on the likelihood ratio, and that among the three classifiers, Boost showed the most consistent performance.

@&#INTRODUCTION@&#
Voice activity detection is a technique in speech processing by which presence of speech is detected in a given signal frame. This problem can be seen as a dual hypothesis problem, where a signal frame is classified as either containing speech or containing noise. In a voice activity detector (VAD), the absence of speech usually presumes presence of noise only. This system is not only of great importance for many applications, like mobile telephony, internet telephony, hearing aid devices, but also for robotics if speech oriented systems are utilized like speaker localization, speech and speaker recognition. For most of the stated research problems, it is indispensable to save on bandwidth resources by coding noise with significantly less bits, while for others it is mandatory to completely ignore frames with noise.A VAD must provide a robust and reliable decision procedure in varying acoustical conditions. This task gets quite formidable with the varying level and type of background noise. Approaches to voice activity detection mostly differ in the type of the extracted features and in the decision models used to reach a speech/non-speech decision based on those features. A lot of attention was given to statistical model-based VADs, in which certain probabilistic properties are assumed on the coefficients of the discrete Fourier transform (DFT). For an example, in [1] they are assumed to have Gaussian distribution and this approach was further developed in [2–8]. Furthermore, special attention was given to derivation of various noise robust features and decision rules in [9–11]. Concerning supervised learning approaches, they have been utilized in various sound processing scenarios, e.g. music classification [12], general audio signal classification (music, news, sports, etc.) [13], speech intelligibility quantification [14], etc. Supervised learning based voice activity detection approaches have so far been mostly focused on applying support vector machine (SVM) by treating as features: a priori signal-to-noise ratio (SNR), a posteriori SNR and/or statistical model-based likelihood ratio [15,16], mel frequency cepstral coefficients (MFCCs) [17], sub-band and long-term SNR [18,19], or features used in the standard G.729B [20,21]. Furthermore, a recent work [22] presented a novel unsupervised learning approach called support-vector-regression-based maximum margin clustering which was also tested in a voice activity detection scenario and showed comparable performance to supervised approach based on support vector machine method.Our work presented in this paper surveys the supervised learning approaches to VAD and builds on upon the aforementioned related works with the following main contributions. Firstly, to the best of our knowledge, we are the first to introduce a method for input variable analysis based on partial mutual information algorithm in the context of voice activity detection. This method systematically classifies features on those that should be included and those that could be omitted from the input set, which we find extremely important when extending input spaces of supervised learning algorithms. Secondly, we extend the input space with distinct features under the hypothesis (which is tested) that this will improve the performance of VADs. While most of the features in the related works are variants on the SNR estimation (a priori, a posteriori, predicted, sub-band and long-term), with two exceptions – one which used only MFCC [17] and other which is based on features from G.729B [20], in the present paper we extended this feature space by using information from the SNR estimation in the form of a statistical-based likelihood ratio (LR) by modeling the distribution of the spectral envelope, along with several distinct features like magnitudes of some of the DFT coefficients, spectral flux, spectral centroid and bandwidth, power-normalized cepstral coefficients, MFCCs, etc. Furthermore, for the classification task we present a systematic quantiative evaluation of the following three supervised learning algorithms: Boost, artificial neural networks (ANNs) and SVM, while all the related work papers on VAD utilize only SVM. The algorithms were tested and compared under varying noise conditions, namely three types of noises and three different SNRs, and showed similar performance with a slight advantage in the direction of the Boost classifier.Although a detector can be considered as a binary classifier, for clarity throughout the paper we use the term detector to denote the statistical model-based detector based on the likelihood ratio, while the term classifier or supervised learning based VAD denotes the SVM, Boost and ANN classifiers. The rest of the paper is organized as follows. Section 2 presents the statistical model-based VADs. In Section 3, the implemented algorithms for noise spectrum estimation and a priori signal-to-noise ratio are presented. Section 4 presents the utilized speech corpus and evaluation metrics, while Section 5 presents the input variable selection algorithm, the input variable set and the resulting analysis. Section 6 presents the experimental evaluation of the algorithms, and Section 7 concludes the paper.These VADs rely on statistical modeling of the DFT coefficients. All the statistical model-based VADs assume a two hypotheses scenario. Since speech is degraded with uncorrelated additive noise, the two hypotheses are as follows:(1)H0:speechabsent⇒X=NH1:speechpresent⇒X=N+S,where the DFT coefficients of a K-point DFT of the noisy speech, noise, and clean speech are denoted as X=[X0, X1, …, XK−1]T, N=[N0, N1, …, NK−1]T and S=[S0, S1, …, SK−1]T, respectively.The form of the probability density function (pdf) of X conditioned on the hypotheses, i.e. p(X|H0) and p(X|H1), depends on the distribution used to model each DFT coefficient. After the pdfs p(X|H0) and p(X|H1) are determined, usually a likelihood ratio on all the DFT coefficient indices k is calculated:(2)Λk=p(Xk|H1)p(Xk|H0),where Λkbecomes a vector of length K. This information is then used to calculate geometric mean which is then compared to a certain threshold in order to reach a final decision in favor of either the hypothesis H0 or H1:(3)logΛ=1K∑k=1KlogΛk≷H0H1η.The VAD based on Gaussian distribution was first proposed in [1], where the DFT coefficients are asymptotically independent and zero-mean complex Gaussian random variables. When both speech and noise are present, we have for each coefficient a sum of independent Gaussian variables (speech plus noise), thus resulting with a pdf of variance λx,k=λs,k+λn,k. Hence, the conditional pdfs of Xkon hypotheses H0 and H1 are as follows:(4)p(Xk|H0)=1πλn,kexp−Xk2λn,k,(5)p(Xk|H1)=1π(λn,k+λs,k)exp−Xk2λn,k+λs,k.Under the Gaussian distribution (GD) model, the LR is simply calculated as the ratio of (5) and (4):(6)ΛkGD=p(Xk|H1)p(Xk|H0)=11+ξkexpγkξk1+ξk,where ξk=λs,k/λn,kis the a priori SNR, and γk=|Xk|2/λn,kis the a posteriori SNR. A more detailed derivation can be found in [23], while the algorithms for the estimation of these values are presented in Section 3.In the approach proposed in [24], derived from [25], the DFT coefficients are still modeled as having a Gaussian distribution, but instead of using their joint distribution, the distribution of the signal envelope is used. The envelope of a signal,|Xk|=XR,k2+XI,k2, is actually the euclidean norm of the real and imaginary coefficients. Therefore, instead of looking at the distribution of the coefficients, the distribution of the signal envelope is analyzed.Under hypothesis H0 the signal is only noise, which means that the DFT coefficients are both independent, zero-mean Gaussian variables with variance λn,k/2=E[|Nk|2]. Under that assumption, the pdf of the euclidean distance of such DFT coefficients is a Rayleigh distribution:(7)p(Xk|H0)=2|Xk|λn,kexp−|Xk|2λn,k.Under hypothesis H1, the envelope is the euclidean norm of two independent, non-zero-mean Gaussian variables. Such pdf is a Rician:(8)p(Xk|H1)=2|Xk|λn,kexp−1λn,k|Xk|2+|Ak|2I02|Ak||Xk|λn,k=2|Xk|λn,kexp−|Xk|2λn,k−ξkI02ξk|Xk|2λn,k,where Akis the amplitude of the clean speech spectrum, ξk=|Ak|2/λn,kis the a priori SNR and I0(·) is the modified Bessel function of the first kind and order zero. In [24] this VAD was implemented by calculating the a posteriori probability p(H1|Xk) of voice activity from (7) and (8) via Bayes’ formula. Since in this paper the a priori SNR estimation, presented in Section 3, for all frequency bins is implemented, we propose the LR instead of the a posteriori probability p(H1|Xk). Finally, we derive the LR for Rayleigh and Rice distribution (RRD) model:(9)ΛkRRD=exp{−ξk}I0(2ξkγk).In [23] we have extensively analyzed and compared the performance of three statistical model-based VADs: the GD model [1], the generalized Gaussian distribution model [5], and the RRD model [24]. The models were compared in detection performance and computational demand. On average, under three different types and levels of noises, the RRD VAD showed the best results in detection accuracy, and ranked second in computational demand. This is the reason why we chose to work further with the RRD VAD and why we try to enhance its performance with a supervised learning approach by adding, next to the LR, several other distinct features.We can see from Section 2 that the RRD VAD requires estimation of the noise spectrum λn,kand the a priori SNR ξk. First we shall address the estimation of λn,kand then the estimation of ξk.In most VADs the noise spectrum estimation is done in a way to assume that in the first several frames only noise is present and for that time λn,kis estimated by time averaging the spectrum of the recorded signal. Then, the VAD itself is used to discriminate between frames where speech is present and where only noise is present. When only noise is detected, λn,kis again estimated in a time-averaging fashion.In this paper an algorithm proposed by [26,27] called minima-controlled recursive averaging (MCRA) is used since it performs well in varying noise situations and it allows estimation from all frames, and not just the ones where no speech is detected.As stated earlier, a common technique for noise spectrum estimation is to apply temporal recursive smoothing during the frames when only noise is present. Now, we have the following hypotheses:(10)H0:λn,k(l+1)=anλn,k(l)+(1−an)|Xk(l)|2,H1:λn,k(l+1)=λn,k(l),where 0<an<1 is a smoothing parameter.Let ps,k(l)=p(H1|Xk(l)) denote the conditional speech presence probability at time frame l. Hence, we can write (10) as follows:(11)λn,k(l+1)=λn,k(l)ps,k(l)+[anλn,k(l)+(1−an)|Xk(l)|2](1−ps,k(l))=a˜n,k(l)λn,k(l)+(1−a˜n,k(l))|Xk(l)|2,where(12)a˜n,k(l)=an+(1−an)ps,k(l)is a time-varying smoothing parameter. We can see that the noise spectrum is estimated by averaging past power spectral values, using a smoothing parameter that is adjusted by the speech presence probability ps,k(l). In order to determine ps,k(l), speech absence is calculated by looking at the ratio of the local energy of the noisy signal and its minimum within a certain time frame. For details on the estimation of ps,k(l) please confer [26].The decision directed (DD) estimation approach for the estimation of ξk, the a priori SNR, was proposed in [28]. Firstly, the Wiener gain is introduced as the following ratio:(13)ζk=ξkξk+1.Now, we can define the estimator for ξk:(14)ξk(l)=αaζk2(l−1)γk(l−1)+(1−αa)max{γk(l)−1,0},where 0<αa<1 is a smoothing parameter.The noise spectrum λn,kand the a priori SNR ξkare continuously updated via the MCRA and DD methods, respectively, and are afterwards used in the RRD VAD. An overview on advancements in speech enhancement can be found in [29].In order to analyze the supervised learning based VAD algorithms and performance thereof, we used the NOIZEUS speech corpus by [30]. Although the corpus was originally created for testing speech enhancement algorithms, we used it for the following reasons: (i) the recordings are of high quality and were made in a sound-proof booth, (ii) it offers eight different types of noises from AURORA database by [31] which corrupt the original recordings at four different SNR levels, (iii) the recordings were made by six different speakers – three male and three female, (iv) it uses the IEEE sentence database which contains phonetically balanced sentences with relatively low word-context predictability, and (v) the corpus is available to researchers free of charge. The percentage of the speech segments amounted to 61.28%, which is as twice as high as compared to [1,4], but less than 5% higher than in the cases of [5,8]. The recordings were sampled at the rate of 25kHz and were later downsampled to 8kHz. The total length of all the recordings was 80.04s, which offered, with 50% overlap and frame length of L=256, in total 5000 frames for detection. However, in order to test the performance and train the classifier for different types of noises and noise levels, we have added to the clean speech also versions corrupted with babble (SNR 15dB, 10dB, 5dB), car (SNR 15dB, 10dB, 5dB) and white Gaussian noise (SNR 20dB, 15dB, 10dB). In total, this gave us 50,000 frames for evaluation.Usually, in order to test and train the algorithms, the speech segments are hand-labeled. However, in the present work we used signal energy calculated via Parseval's theorem as the indicator of speech presence, which enabled automatic frame labeling. We find this approach justifiable in the case of the NOIZEUS corpus, since the clean recordings were made in a sound-proof booth resulting with the speech-absent frames having energy a thousand times lower than the weakest speech frame.The evaluation metrics we used are based on the standard elements of the confusion matrix: true positive (TP) – voice classified as voice, true negative (TN) – silence classified as silence, false positive (FP) – silence classified as voice, false negative (FN) – voice classified as silence. We also used speech detection rate (SDR) – percentage of speech frames classified as speech, and false alarm rate (FAR) – percentage of noise frames classified as speech. The former and latter are calculated as follows:(15)SDR=TPTP+FN,FAR=FPFP+TN.These two rates are actually used in order to draw a receiver operating characteristics (ROC) curve. An ROC curve is a two-dimensional depiction of classifier performance. Usually, they are produced by graphing pairs of SDR and FAR values as a function of changes in the threshold value. To compare different classifiers it is practical to reduce the information in the ROC curve to a single scalar value. A common method is to evaluate the area under an ROC curve (AUC). For an example, since both the SDR and FAR take values in the range of0,…,1, for a perfect classifier the AUC value would be 1, since it is able to make a perfect SDR without any false alarms. A completely random classifier would have AUC value of 0.5, since the ROC curve would be a diagonal line in the SDR–FAR space. This would be equivalent to predicting based on fair coin tosses. More on the ROC curves and metrics for evaluation of classifiers can be found in [32,33].Another balanced measure of classification performance with respect to all elements is the Matthews correlation coefficient (MCC) which we chose as additional metric for performance comparison. It is calculated as follows [32]:(16)MCC=TP×TN−FP×FN(TP+FN)(TP+FP)(TN+FP)(TN+FN).The MCC is always between −1 and +1, where −1 indicates total disagreement and +1 indicates total agreement. The MCC is 0 for completely random predictions. If two variables are independent, then their MCC is 0. The converse in general is not true.Before we start with classification, we need to choose input variables, i.e. features, upon which the classifiers will make decision and which, in effect, will be combined to form a strong classifier. We already mentioned that LR is one of the features, but we hypothesize that by adding other features we could improve the classification results.The partial mutual information (PMI) based input variable selection (IVS) algorithm used in [34,35] overcomes two main issues that limit the applicability of many IVS techniques. Those are the underlying assumption of linearity and redundancy within the available data. The way that PMI IVS works is that it first selects the most informative input variable, then it searches for the next most informative variable but by taking into account information already received from the previously selected variable. This process continues until an introduction of an additional input variable increases the mean squared error of the prediction, i.e. the square of the expected value minus the label, or PMI drops below a certain threshold. Hereafter, we present the mathematical background of the PMI IVS.Assuming y is a classification outcome, i.e. signal frame label, x is a currently considered input variable (feature), and z is a set of previously selected variables, partial mutual information in x about y given z is formulated as follows:(17)PMI=∬pu,v(u,v)lnpu,v(u,v)pu(u)pv(v)dudvwhereu=y−E[y|z],v=x−E[x|z], and E[.] is the expectation operator.In order to obtain probability density functions for PMI from the data, we used kernel density estimators (KDEs). E.g., in order to calculate E[x|z] we used the following KDE:(18)pˆ(x,z)=1n1(2πh)d|Σ|∑i=1nexp−∥[xz]T−[xizi]T∥Σ2h2,where ∥[xz]T−[xizi]T∥Σ=([xz]−[xizi])Σ−1([xz]T−[xizi]T) is the Mahalanobis distance, and h is the kernel bandwidth, for which we used the Gaussian reference bandwidth throughout this paper:(19)h=4d+21d+4n−1d+4where d is the dimension of the multivariate variable set, and n is the sample size.Note that for E[x|z] we needpˆ(x|z). If we take(20)Σ=ΣxxΣxzΣzxΣzz,we get(21)pˆ(x|z)=1n1(2πh)d|Σ¯|∑i=1nexp−∥xT−xi¯T∥Σ¯2h2whereΣ¯=Σxx−ΣxzΣzz−1Σzxandxi¯=xi+ΣxzΣzz−1(z−zi).Finally,(22)E[x|z]=∑i=1nwi[xi+ΣxzΣzz−1(z−zi)]where each sample is weighted by its weighting factor introduced in [34]:(23)wi=exp−∥zT−ziT∥Σzz2h2∑j=1nexp−∥zT−zjT∥Σzz2h2.The pseudocode of IVS based on PMI utilized in the present paper is given in Algorithm 1.Algorithm 1Input variable selection based on partial mutual information.In the ensuing paragraphs we present the features that form the potential input variable set. Each of them was analyzed as a standalone detector and as a candidate for the reduced input vector by the PMI IVS.Magnitude of the DFT coefficients. A K-point transform was used to analyze the spectrum of the recorded frames. The magnitude of the first 32 coefficients of the transform were used as a feature for the classifier.Zero-crossing rate. The zero-crossing rate (ZCR) of a signal is the rate of sign changes along the signal. It is defined as follows:(24)fZCR=∑i=2LZi,whereZi=1,ifsign{x(i)}−sign{x(i−1)}≠00,otherwise.Human voice consists of voiced and unvoiced sounds. Voiced sounds have higher ZCR value than the unvoiced sounds do. Therefore, it is a reasonable assumption that ZCR of either voiced or unvoiced parts of speech will be different than the ZCR of noise in the silent periods.Spectral flux. Spectral flux (SF) measures how quickly the spectrum of the signal is changing. It is calculated by comparing the power spectrum of the current frame with the power spectrum of the previous frame.(25)fSF=∑k=1K(|Xk(l)|2−|Xk(l−1)|2)Speech changes quickly between voiced and unvoiced parts, thus resulting with high SF values.Spectral rolloff. Spectral rolloff (SR) is defined as the a-quantile of the total energy in |Xk|2. It is a frequency under which a fraction of the total energy is found. If K is the length of the signal DFT, then SR can be defined as:(26)fSR=maxyy:a>∑k=1y|Xk|2∑k=1K|Xk|2Spectral rolloff was calculated at six quantiles equally spaced in [0, 1].Mel-frequency cepstral coefficients. Mel-frequency analysis is a technique inspired by human sound perception. The human ear acts as a filter and concentrates only on specific spectral components. The filters are non-uniformly spaced on a frequency scale, and their density is higher in the low frequency regions. The MFCCs are calculated in several steps: (i) the magnitude spectrum |Xk| is filtered with a bank of non-uniformly spaced overlapping triangular filters, (ii) the logarithm is taken, and (iii) the MFCC are obtained by computing the discrete cosine transform of the result. In [36] where authors consider a voice conversion system, MFCC feature is identified as a feature that does not consider any particular speech model, i.e. feature that is useful for general voice activity detection, without considering any speaker in particular.Power-normalized cepstral coefficients. In [37,38] a feature extraction algorithm called power-normalized cepstral coefficients (PNCC) was proposed, which instead of log nonlinearity like MFCC uses power-law nonlinearity and a gammatone filterbank. In [37] it was shown to outperform MFCC, among others, in speech recognition accuracy. After adapting the algorithm proposed in [37] to our scenario, we have used the first thirteen PNCCs which were the result of a 20 element gammatone prefiltering.Spectral centroid. Spectral centroid (SC) is a statistic that measures where most of the power of a speech segment is spectrally located. It is defined as follows:(27)fSC=∑k=1Kk|Xk|2∑k=1K|Xk|2.Spectral bandwidth. Spectral bandwidth (SBW) describes spreading of the spectral components with respect to the spectral centroid:(28)fSBW=∑k=1K(k−fSC)2|Xk|2∑k=1K|Xk|2.Feature aggregation. In total the following features were aggregated: 1 LR, 32 DFT magnitude coefficients, 1 ZCR, 1 SF, 6 SR quantiles, 15 mel-frequency cepstral coefficients, 13 power normalized cepstral coefficients, 1 SC and 1 SBW. Thus, we had a feature vector of 71 for input variable analysis. Similar approach was used in [39,12] for music classification.Each of the afore presented features can be considered as a detector in itself, whose performance might indicate the suitability of being an element in the input vector. As an intuitive preliminary analysis, we utilized the ROC curves, i.e. the related AUC score, of each feature evaluated on the whole data set at once. Table 1shows the AUC for all the features presented in the current section. We can see that the LR has the highest score, followed by the first PNCC, SF, the first MFCC coefficient, while the third and ninth PNCC have the lowest score. Furthermore, ROC curves for five features with the highest AUC score are depicted in Fig. 1, while the values of three features with the highest AUC score along with the label for 200 frames are depicted in Fig. 2.Due to high memory requirements the analysis based on partial mutual information was carried out on the set consisting of the clean signal, and its versions corrupted with babble (SNR 10dB), car (SNR 10dB), and white Gaussian noise (SNR 15dB) separately. The analysis on each set was stopped once the addition of another feature caused increase in the mean squared error. Based on the results we kept those features that were chosen in at least two sets: the LR, DFT indexes 7, 8, 9, 11, the 1st and 2nd SR, the 1st MFCC, SC, SBW, and 1st, 2nd and 3rd PNCC. It is interesting to note that the PMI algorithm chose the 3rd PNCC as a good feature, although it has by far the lowest AUC score than many other features. However, the PMI chooses features which bring additional information when all the information from other features is taken into account, meaning that in certain scenarios the 3rd PNCC contributed to correct classification. In total this amounts to 13 features forming a reduced vector of input variables, which is an 82% decrease in the size of the feature vector.Although from Fig. 1 we can see that the LR as a standalone detector outperforms other features, we conjecture and shall test (i) that a trained classifier based on LR and other features should outperform a statistical model-based detector based on LR, and (ii) that a detector with carefully chosen reduced input vector should not significantly underperform the detector based on a full feature vector. We shall test these hypotheses on 50,000 learning examples and by meticulous analysis with ROC curves and the AUC metric.In the present paper we utilized and compared three supervised learning algorithms; SVM, Boost, and ANN, which were to classify if a signal frame contains speech or not based on the full and the reduced feature set generated by algorithm in Section 5. The three have different approaches to learning and all have their advantages, and we shall briefly introduce each in the following paragraphs. But it is important to notice at this point that the goal of the present paper is not to provide a detailed tutorial in either of the classifiers, but to analyze and compare the performance of the three for the specific purpose of voice activity detection based on various features and not in general. For training and testing the three learning algorithms we used the OpenCV library [40].Essentially, SVM [41,42] is a learning algorithm that constructs a hyperplane or a set of hyperplanes which define boundaries for the data to be discriminated. The data, most often, is not linearly separable and this problem is addressed by SVM in a way that non-linearly maps the input vector with a kernel function to a high-dimensional feature space. They can also be used in regression tasks, but in the present paper we use them in the context of a binary classifier. An introduction to the theory behind SVM and some practical insights can be found in [43]. In the present paper we used C-support vector classification and radial basis function RBF as the kernel function.The main idea behind boosting algorithms is to use many simple detectors which should have performance a bit better than 50% at least (i.e. better than random guessing) – these are called weak classifiers – and combine them to obtain highly accurate classifier – usually called a strong classifier. In its original form, Boost handles binary classification problems only, although there are extensions to handle multi-class and even multi-label classification problems [44]. In the present paper, a variant of the Boost algorithm proposed in [45] called Real Boost is used [46].The ANNs are a product of the desire to imitate the workings of the biological brain. They involve a network of simple processing elements (artificial neurons) which can exhibit complex global behavior. One of the most important properties of ANNs is the ability to approximate any continuous function up to a given precision. They have been extensively used in both classification and regression tasks and more on the ANNs can be found in [47]. In the present paper we utilize a static multilayer perceptron network (MLP) with a sigmoid activation function, a single hidden layer with 5 neurons, while the network parameters are learned using the resilient propagation (RPROP) algorithm [48].In this section we analyze the performance of the classifiers. The data was constructed by concatenating the clean signal with its corrupted versions thus, with frame length of L=256 samples, yielding 50,000 examples for evaluation. For the full input vector we had 71 features, while the reduced input vector consisted of 13 features. Prior to the learning process, all the features were scaled in a way to have a zero mean value and standard deviation of one.The evaluation was performed by K-fold cross-validation. Essentially, the original dataset was partitioned randomly into K subsets of equal size. Of the K subsets, one was retained for testing the classifier while the other K−1 subsets were used for training. The cross-validation process was repeated K times thus yielding K results which were used for drawing the average ROC curves. As discussed in [33], by drawing just an ROC curve of different classifiers and seeing which one dominates to assess the performance might be misleading, since we do not have a measure of variance. Therefore, it is suggested to generate results from several test subsets, by a cross-validation or bootstrap method, and average these results in order to obtain a measure of variance. The ROC curves can be either averaged vertically by fixing FAR and averaging over SDR, or by the threshold, where for each threshold value an SDR–FAR pair is found and their values are averaged thus yielding both vertical and horizontal variance. In the present paper we used 10-fold cross-validation and threshold averaging for evaluation of the VAD algorithms.Firstly, we compared intra-classifier performance, i.e. performance of each classifier working with either the full or the reduced input vector. Henceforth, all the figures depicting ROC curves have for each point a confidence interval of three standard deviations included, along with the AUC score and three standard deviations thereof. These deviations indicate just how consistent the classifier performance was with respect to different cross-validation sets. Fig. 3shows the averaged ROC curves and their AUC score for the SVM, from which we can see that the classifier with the reduced feature set did not significantly underperform compared to the classifier trained on the full feature set. In Fig. 4we can see a bit different result for the Boost classifier. In this case the classifier showed practically equal performance both in the mean and standard deviation when being trained on the full and the reduced input set. Finally, Fig. 5shows the averaged ROC curves and their AUC score for the ANN. It performed slightly better in the mean and standard deviation of the AUC score with the full input vector, but overall exhibited larger deviations than any of the other two classifiers. This means that it did not perform as consistently over all the subsets.To conclude the intra-classifier analysis, we can assert that the results supported our second hypothesis from the Section 5: neither of the classifiers significantly underperformed when being trained on the reduced input vector formed by a careful IVS. Henceforth, we shall only include in the analysis the classifiers trained on the reduced input vector.For the inter-classifier performance we also included the statistical model-based detector presented in Section 2.2 which too was evaluated by K-fold cross-validation. Since it does not require training it was simply tested on the same K subsets and these results were averaged. Fig. 6shows ROC curves for the three supervised learning classifiers and the RRD detector based on LR, from which we can see that the supervised learning approach with several additional features can significantly increase the performance of a detector. Moreover, judging from the AUC scores shown in Fig. 6 we can assert that the Boost classifier slightly outperforms the other classifiers, since it has the largest AUC mean value and the smallest AUC standard deviation. Furthermore, by inspecting Figs. 3–5 we can also see that Boost overall exhibited smaller deviations in the ROC curves, which further tips the balance in Boost's favor.During the K-fold cross-validation we also monitored the performance of the trained classifiers for each subset by calculating the SDR, FAR, and MCC presented in Section 4. Since all the classifiers were trained to output a value between −1, for non-speech, and 1, for speech frames, we set the threshold to zero, thus all the frames with score larger or equal to zero were classified as containing speech, while the other were classified as non-speech frames. This essentially would correspond to only a single point in the ROC curve graph, but it is very practical since it provides a tangible sense of performance for a single threshold value. The average of these statistical scores for the aforementioned 10 subsets is shown in Table 2, where we also provide error rate (ERR=(100−SDR)+FAR) since it is often used in other works.To conclude the inter-classifier performance, from the above presented results we can see that the classifiers significantly outperformed the statistical model-based detector, and that due to having the highest AUC score with the smallest standard deviation, and exhibiting no significant deviations anywhere in the ROC curve, the Boost algorithm had the advantage over the other algorithms for this specific application of speech activity detection based on various features. Therefore, we can assert that the results supported our first hypothesis from Section 5 that a trained classifier based on LR and other features should outperform a statistical model-based detector based on LR.These experiments were designed so as to find a LR model that will show the best results [23], which we would then extend with features meticulously analyzed with PMI IVS and encompass it all in a supervised learning framework which showed the best and most consistent performance. Furthermore, the corpus that we used is freely available to all researchers [30] which will enable direct comparison of detection algorithms in the future. Comparison of our results to works which utilized a supervised learning approach [15–17,20,18,19] is not straightforward due to utilization of a different speech corpus, graphical result representation (no score presented) or non-direct metric (word accuracy rate in speech recognition). However, some do provide ERR score for different noise levels and types which we will use for crude comparison with our results. For an example, in [15] the best ERR was 5.38% and 13.47% for vehicle and office noise, respectively, while [16] reports 9.4% and 20.9% for vehicle and babble noise, respectively. In [20] authors report ERR from 7.83% to 41.39% for different test sequences. The authors in [17] report a score named equal error rate for which equality 1−SDR=FAR holds. For three different datasets they report equal error rate of 8.0%, 13.1%, and 19.0% for an SVM trained on MFCC. Comparing these results with Table 2 we can see that our results do not deviate and are in the rank of their performance. However, since different datasets were used in these papers, a direct comparison is not possible.

@&#CONCLUSIONS@&#
