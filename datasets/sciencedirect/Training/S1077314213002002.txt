@&#MAIN-TITLE@&#
Persistent tracking of static scene features using geometry

@&#HIGHLIGHTS@&#
We introduce a view geometric constraint to the tracking problem.We provide a closed form solution from appearance and geometry.This paper provides persistent tracking of features even when they are occluded.Bayesian-based selection of the best features.Our experiments demonstrate robust tracking performance.

@&#KEYPHRASES@&#
Feature tracking,Tracking,Projective geometry,Projective invariants,Occlusion,Persistent tracking,

@&#ABSTRACT@&#
Despite many alternatives to feature tracking problem, iterative least squares solution solving the optical flow constraint has been the most popular approach used by many in the field. This paper attempts to leverage the former efforts to enhance feature tracking methods by introducing a view geometric constraint to the tracking problem. In contrast to alternative geometry based methods, the proposed approach provides a closed form solution to optical flow estimation from image appearance and view geometry constraints. We particularly use invariants in the projective coordinates generated from tracked features that results in a new optical flow equation. This treatment provides persistent tracking of features even when they are occluded. At the end of each tracking loop the quality of the tracked features is judged using both appearance similarity and geometric consistency. Our experiments demonstrate robust tracking performance even when the features are occluded or they undergo appearance changes due to projective deformation of the template.

@&#INTRODUCTION@&#
Tracking a set of features in video finds application in many low and high level computer vision tasks including but not limited to structure from motion [1], action recognition [2], object recognition [3], video segmentation [4] and mobile vision [5]. This wide applicability makes the problem attractive to many researchers in the field which attempt to solve the hard problem of generating feature trajectories that are least affected by the changes in the feature’s appearance as the camera moves. The camera motion usually manifests itself in the appearances of tracked features as projective changes in the features vicinity.The most intuitive and a commonly applied approach is to detect features at every frame and establish correspondences across frames by template matching [6]. Alternatively, correspondences can also be established by modeling the motion either deterministically [7,8] or stochastically [9] both of which may enforce one or more “qualitative motion heuristics” [7]. These heuristics include proximity, maximum velocity, small velocity change, rigidity and common motion. The deterministic methods attempt to solve feature tracking as a combinatorial optimization problem, which is expected to provide one to one correspondences. The solution is commonly obtained using greedy [10] or optimal assignment methods [11] which do not handle uncertainty and noise in observations. In contrast, probabilistic methods solve the correspondence problem by recursive Bayesian filtering which predicts possible correspondences and corrects internal models using the new observations [12]. These methods suffer from occlusions which results in loss of tracked features.Another line of feature tracking approach, which relates more to our approach, is to iteratively estimate the motion of a feature. Motion estimation exploits the optical flow equation derived from the brightness constancy constraint and assume the feature and its neighboring pixels move with the same speed, e.g. common motion heuristic. This assumption can be exercised to introduce multiple instances of optical flow equations with the same unknown flow vector and can be solved using the least squares adjustment [13]. In fact, this treatment is the most commonly applied motion estimation approach referred to as Lucas Kanade optical flow method. Later, Lucas Kanade approach is extended to feature tracking by Tomasi and Kanade [14] and is referred to the KLT tracker. This extension included an additional step to check the quality of tracked appearance by means of computing the sum of square differences for affine compensated templates. In the later years, KLT has become and arguably still is the most commonly applied feature tracking method due to its ease of implementation, and low computational complexity. In the case the feature is occluded, these tracking algorithms estimate incorrect trajectories.Despite its wide use, methods that track each feature independently from the others are susceptible to fall into local solutions due to background clutter, cross-cutting trajectories and scene parallax. These shortcomings have been addressed in stochastic correspondence based methods by employing joint probability data association filtering [15], particle filtering [9] and multiple hypothesis tracking [16]. These stochastic methods use a set of possible tracks for each feature, which compete against each other to solve the coalescence problem. The dimensionality of the possible solutions, however, become intractable and heuristic constraints for pruning excess tracks are required. The stochastic models which include motion models can handle short occlusions under constant velocity or acceleration models. These methods, however, will not recover object trajectories in case the occlusion is long or the motion model does not confirm the feature motion.Another direct approach has also been studied which includes local geometric heuristics to estimate the motion of a feature with respect to the other features. In fact, the common motion heuristic used in the KLT tracker is an instance of local geometric constraint, where the neighboring pixels are assumed to lie on a plane perpendicular to the projection ray. An extended application of the motion heuristic is introduced in [17] where the authors assume features in a small neighborhood follow the same affine motion. This approach works well when proximal features are coplanar in 3D. Non-coplanar features lying across multiple planes due to depth discontinuity introduce an incorrect bias to motion estimation. Alternatively, Brox et al. [18] proposed a continuous, rotationally invariant energy function for estimating optical flow based on both the brightness constancy and the gradient constancy. These assumptions were combined with a 2D discontinuity-preserving spatio-temporal regularizer. This method, however, does not use a 3D geometric constraint either. A more restrictive geometry is exploited in [19] using cross-ratio invariance, where a set of features are assumed to be collinear in 3D.Instead of the local geometry used by the aforementioned methods, global scene geometry provides a stronger constraint on the types of motions the features can undergo. A complicated solution to tracking based on the global scene geometry is to estimate both the 3D scene and the camera matrices. In [20], Torresani and Bregler used the non-rigid structure from motion to track features. Their approach assumed all the tracked features lie on a single articulated object. The tracking is then achieved by first tracking a set of appearance-wise reliable features until the end of the sequence to learn the 3D shape basis, which is later imposed as a constraint on the trajectories of the “unreliable” features to estimate correct trajectories. The approach, however, requires processing of the video offline first, which is then used to constrain the second pass to find remaining trajectories. Alternatively, Irani [21] incorporated geometric constraints in the optical flow equation assuming rigid body and orthographic cameras. Its performance, however, degrades rapidly for non-planar scenes when the camera rotates. Ji and Fermuller [22] improved [21] by enforcing constancy of 3D surface normals computed by estimating structure of the scene. While this additional constraint made the approach computationally feasible for longer sequences, it required offline processing of the video. Moreover, the plane segmentation required at the beginning of the algorithm limits its application to scenes which is composed of a set of planes. In [23], Zhang et al. proposed a non-consecutive feature tracking approach based on shape from motion constraints. Their approach consisted of a consecutive point tracking step followed by a non-consecutive track matching step. While provided decent results, their approach required offline processing. A more explicit geometric constraint using the 3D scene is considered by Wang et al. [24]. Their approach used images from a stereo camera-rig to simultaneously recover 3D and track features using a 3D elastic motion model. Stereo camera-rig, however, may not always be available in a generic feature tracking framework.This paper exploits the global scene geometry and defines a quality measure that ties well to the appearance based quality measure used in KLT. We particularly try to address the tracking problem illustrated in Fig. 1and introduce a method that tracks a feature by maximizing both the appearance and geometric quality. In this regard, the proposed approach leverages the motion estimation based tracking approaches by introducing a view geometry constraint to the optical flow equation. The proposed geometric constraint uses a set of samples from the posterior distribution of invariant projective coordinates. These samples are used to formulate the geometric optical flow equation which accompanies the brightness based optical flow equation. This new treatment improves our earlier approach [25], which estimates optical flow vectors in a regularization based solution similar to that of the Horn and Schunck approach [26]. The tracking is achieved by solving the new tracking function with shared geometry constraint using the least squares adjustment. In addition, we provide a detailed discussion on how the geometric invariant which exploits the cyclopean constraint is satisfied in a tracking framework. The proposed tracking approach is capable of estimating feature trajectories during occlusions. This is primarily due to using a collection of features from around the scene in estimating the optical flow vectors. In our experiments, despite the appearance variations due to projectivity or short duration occlusions, the tracked features quantitatively provide better accuracy compared to alternative methods.The remainder of the paper is organized as follows. In Section 2, the tracking model is discussed. We will give details of the geometry shared across consecutive frames in Section 3. The integration of appearance and geometry in the proposed tracking model is introduced in Section 4. In Section 5, the selection of salient features is given. Experimental evaluation of the proposed approach is given in Section 6. This section is organized into two subsections: one on general sequences and the other one on the application of the system to the NASA astronaut navigation system. Section 7 concludes the paper.The tracking problem is typically formulated by exploiting the brightness constancy:I(x,y,t)=I(x+dx,y+dy,t+dt). The Taylor series expansion of this equation results in the optical flow constraint equation:(1)uIx+vIy+It=0,where the subscripts denote spatial and temporal image derivatives andu=(u,v)=(dx/dt,dy/dt)is the optical flow vector. Optical flow constraint defines a line in the(u,v)coordinates as shown in Fig. 2(a). Assuming the derivatives are accurate, the unknown flow u lies on this line and is decomposed into parallel flow and normal flow components as depicted in the same figure. The normal flow, which defines the motion in the edge normal direction, is computable and lies at the intersection of the line with a perpendicular line passing through the origin. The parallel flow, on the other hand, cannot be computed due to the aperture problem.In order to overcome the aperture problem, a common strategy is to assume that the motion of a pixelxis equivalent to the motion of the neighboring pixelsxi. This assumption geometrically relates to coplanarity ofxi, such that the appearance atxiprovides an ortho-texture. In this setting, for each pixel, the optical flow constraint provides an equation which can be considered as a linel(xi)(see Fig. 2(b) for illustration). The optical flow, hence the true flow, estimated by line fitting using least squares estimation. This treatment, however, is known to have problems in case of depth discontinuities, where the coplanarity and ortho-texture assumptions are violated. Alternative to the coplanarity assumption, we introduce a general geometric constraint which introduces another line in the(u,v)space:(2)ua+vb+c=0,which is based on the invariant projective coordinates and is shown in Fig. 2(c). The optical flow is determined as the intersection of two lines that are computed from the appearance constraint and the geometry constraint.Observed motion of a feature depends on the camera motion and the scene geometry. Under the assumption that the scene geometry is constant, it is possible to derive measures that are geometrically invariant under different viewpoints. We exploit these geometric invariants to derive the parameters of the geometric optical flow given in (2). From among various projective invariants discussed in [27], we adopt the cross-ratios of planes due to its less restrictive geometric construction and adoptability for the general feature tracking problem. This projective invariant can provide point to point correspondences compared to epipolar geometry which only provides point to epipolar line correspondences.For the sake of completeness, we start our discussion by reiterating some of the important results from [27, pp. 47–63] while modifying them as necessary for feature tracking. For a general 3D featureX, assume that there exists five salient 3D features,{X1,X2,X3,X4,X5}with the condition that no four are coplanar. Let any two features,XiandXj, from among the salient features define a pencil of planes generated by using feature triplets{Xi,Xj,Xk}or{Xi,Xj,X}as shown in Fig. 3. We should note that the planes assumed in the invariant coordinates are virtual planes and they do not need to exist in the 3D scene.The 3D features project to pixels in two images at different viewpoints:x↦x′and{x1,x2,x3,x4,x5}↦{x1′,x2′,x3′,x4′,x5′}.Under the assumption that two images generate a cyclopean stimuli, it can be shown that a projective invariant for a general featurexcan be computed as:(3)Ci,j(x,x′)=Vijkl′Vijmx′Vijkm′Vijlx′,from the cross-ratios of determinantsVijkl′=|Ri′Rj′Rk′Rl′|, wherei<jrefers to indices of the salient features generating the pencil,k<l<mare the indices of the remaining salient features andRi′=(xi,yi,xi′,yi′)⊤[28]. The projective invariant,Ci,j(x,x′), for this setup is known as the invariant projective coordinate ofx. The cyclopean stimuli is related to generating a synthetic image from a virtual viewpoint residing between the two stereo views (see Fig. 4for illustration) [29]. In context of tracking, we consider that the frame at time t is the left image and the frame at timet+1is the right image. Under the condition that the camera motion is small, the consecutive images satisfy the cyclopean stimuli as will be discussed in the following.In [28], Barrett et al. suggests that the image formation is restricted to “cyclopean imaging” under the condition thatD=D′, whereDandD′represent four-vectors comprised of the imaging system parameters (camera position and orientation). Based on this observation, the collinearity equation can be algebraically modified to containD=D1D2D3D4=sinϕ-sinωcosϕcosωcosϕ-(D1Xc+D2Yc+D3Zc).In these equationsωandϕdenote the camera rotation angles around x and y axis, and(Xc,Yc,Zc)represents the camera perspective center. Without the loss of generality, we assume the camera external parameters at frame 1 define the object coordinate system. In this case,ω,ϕ,Xc,YcandZcbecome zeros, such thatD1=0,D2=0,D3=1,D4=0. Considering that the camera motion between two consecutive frames is small, we can use small angle approximations:sinω′≈ω′≈0,cosω′≈1,sinϕ′≈ϕ′≈0andcosϕ′≈1. Under these conditions, the cyclopean condition can be shown satisfied:(4)D1′-D1=sinϕ′≈0,(5)D2′-D2=-sinω′cosϕ′≈0,(6)D3′-D3=cosω′cosϕ′-1≈0,(7)D4′-D4=-(D1′Xc′+D2′Yc′+D3′Zc′)≈0.Let there be three consecutive frames satisfying the cyclopean condition in which the positions of salient features in all three frames are known and the position of the general feature is only known in the first two frames:x↦x′↦<unknownx″at framet+2>,{x1,x2,x3,x4,x5}↦{x1′,x2′,x3′,x4′,x5′}↦{x1″,x2″,x3″,x4″,x5″}.An illustration of this setup is demonstrated in Fig. 5where the position ofx″denoted by□is unknown att+2. We conjecture that the constancy of the projective invariant coordinate for the general featurexat consecutive time instants,Ci,j(x,x′)=Ci,j(x,x″), provides necessary conditions to estimate the position ofx″=(x″,y″), hence the optical flow, from framet+1tot+2.Given the computedCi,j(x,x′)and rearranging (3), we have:(8)Ci,j(x,x′)=Vijkl″(Sijm+Pijmx″+Qijmy″)Vijkm″(Sijl+Pijlx″+Qijly″),whereVijkl″=|Ri″Rj″Rk″Rl″|,Sijl=-x|EiEjEl|+y|FiFjFl|,Pijl=-|GiGjGl|,Qijl=|HiHjHl|,withRi″=(xi,yi,xi″,yi″)⊤,Ei=[yi,xi″,yi″]⊤,Fi=[xi,xi″,yi″]⊤,Gi=[xi,yi,yi″]⊤,Hi=[xi,yi,xi″]⊤and|.|denotes determinant.Once the invariant projective coordinates from cross-ratio of planes is computed we can define the geometric constraint in (2) by rearranging (8):(9)Ci,j(x,x′)Vijkm″(Sijl+Pijlx″+Qijly″)-Vijkl″(Sijm+Pijmx″+Qijmy″)=0.Substitutingx″=(x″,y″)intox′+u=(x′+u,y′+v), (9) can be written as:(10)Ci,j(x,x′)Vijkm″(Sijl+Pijl(x′+u)+Qijl(y′+v))-Vijkl″(Sijm+Pijm(x′+u)+Qijm(y′+v))=0.This equation can be rearranged to form the geometry based optical flow equations introduced in (2),au+bv+c=0, where:(11)a=Ci,j(x,x′)Vijkm″Pijl-Vijkl″Pijm,(12)b=Ci,j(x,x′)Vijkm″Qijl-Vijkl″Qijm,(13)c=Ci,j(x,x′)Vijkm″(Sijl+Pijlx′+Qijly′)-Vijkl″(Sijm+Pijmx′+Qijmy′).Here,a,band c are functions of the pencils denoted in Fig. 3, and they are calculated from the image coordinates of a general 3D featureXand five other salient 3D features{X1,X2,X3,X4,X5}projected at three different viewpoints. We have observed that instead of using consecutive three frames, selecting every fifth frame for 15fps video results in a better geometry due to the increase in camera baseline while satisfying the cyclopean geometry. A possible drawback to extend selection beyond the fifth frame, however, may invalidate the cyclopean constraint,Ci,j(x,x′)=Ci,j(x,x″).Given the two optical flow equations in (1) and (2), the optical flow vector with two unknowns can be estimated by substitution, which will result in a closed form solution given by:(14)u=cIy-bItbIx-aIy,v=aIt-cIxbIx-aIy,wherea,band c parameters are given in (11)–(13).An important observation regarding the closed form solutions in (14) is their dependency on the image gradients which are usually noisy. Following the common treatment, we conjecture this undesired effect can be reduced by taking the average of the spatiotemporal gradient in the neighborhood of the tracked feature. Considering that we can use multiple salient feature configurations the averaging of the spatiotemporal gradients results in the following cost function:(15)E(xi)=(1-λ)Eappearance(xi)+λEgeometry(xi),whereλis weighting parameter between the two terms, and:(16)Eappearance(xi)=1Nw∑xj∈Wi(uiIxj+viIyj+Itj)2,(17)Egeometry(xi)=(uia+vib+c)2,whereWidenotes the neighborhood ofxi. The window defines the number of pixels,Nw, used to estimate the smoothed spatiotemporal gradient. Minimizing the cost function in (15) by taking its derivative and rearranging the terms result in the new closed form optical flow solution given by:(18)ui=e1f2-f1e2d1e2-e1d2,vi=f1d2-d1f2d1e2-e1d2whered1=1-λNw∑xj∈Wi(IxjIxj)+λ(a2),e1=1-λNw∑xj∈Wi(IxjIyj)+λ(ab),f1=1-λNw∑xj∈Wi(IxjItj)+λ(ac),d2=1-λNw∑xj∈Wi(IyjIxj)+λ(ab),e2=1-λNw∑xj∈Wi(IyjIyj)+λ(b2),f2=1-λNw∑xj∈Wi(IyjItj)+λ(bc).The details of the proposed tracking algorithm is given in Algorithm 1. We should note that since the proposed approach requires three frames, the geometry term in (15) is initially ignored by settingλ=0for the first two frames in an image stream. In this case, the optical flow estimation reduces to appearance only solution discussed in [14].Algorithm 1Proposed feature tracking algorithm.Discussion on occlusions: Tracking features during occlusions or recovering from occlusions are two important problems that are still unresolved in the field. In the following discussion, we assume that about half of the tracked features are not occluded and can serve as salient features during sampling to track the occluded features. Considering that the tracked template changes dramatically during an occlusion, the feature can be labeled as lost or occluded by computing the similarity between the tracked template and the model template using sum of square differences. In the case when the template similarity dramatically reduces, theλparameter in (15), which is typically set to 0.5 for normal tracking, is changed to 1, such that the tracking is performed purely based on geometry until the feature is visible again. At the time the occlusion is detected, a model template is also recorded to test if occlusion is resolved in future frames. This test is realized by comparing the current template similarity score with the similarity score recorded right before the occlusion has occurred. Similar to the case before the occlusion happens, the selected set of five salient features for a tracked point is different at each frame to guarantee the best geometry configuration for the feature.In Section 3, we introduced five “salient” features to estimate the optical flow for a general feature. The salient features, however, are not known a priori, and due to their geometric configuration they may be different for each tracked feature. Let there be N features at frame t, such that there areM=C(N,5)combinations of subsets inS={Si(5):i∈N;1⩽i⩽M}. For a general feature, we conjecture that there exists a number of subsetsSi(5)that provides the necessary geometry shown in Fig. 3 to estimate its position at frame t. The selection of the subset from an unknown distribution and its geometric saliency suggests us to define a measure to judge the quality of the position estimate for the general feature. We define this measure as the probability of the selectedSi(5)being a salient set given the appearance of the general feature,Wi, and the historical trajectoriesTiof salient feature candidates:PSi(5)|Ti,Wi. Considering that the trajectories of salient features and the appearance of the general feature are independent, applying the Bayes’ rule results in:(19)PSi(5)|Ti,Wi=P(Si(5))PTi|Si(5)PWi|Si(5)P(Ti)P(Wi),whereP(Si(5)),P(Ti)andP(Wi)are uniformly distributed, hence have no effect in the maximum a posteriori (MAP) estimate. The second term,PTi|Si(5), in (19) enforces geometric saliency of the estimated position. This term can be computed from the equivalency of the projective invariant coordinates,Ci,j(x,x′)=Ci,j(x,x″), across frames 1, 2 and 3. We conjecture that the difference between the projective invariants are distributed with zero mean Gaussian distribution:(20)PTi|Si(5)=12πσc2exp-dCi,j(x,x′),Ci,j(x,x″)22σc2,whereσccontrols disagreement between the two projective invariant coordinates. Considering that the invariants may not be exactly the same between the consecutive frames, for instance due to the tracking bias introduced by the appearance term, a smallσc, such asσc=1, offsets small variations.The third term in (19),PWi|Si(5), enforces similarity of the appearance of the tracked feature computed using geometric constraints. We compute this term by measuring the similarity of the model template to the tracked templateW″atx″:(21)PWi|Si(5)=12πσa2exp-dWi,W″22σa2,whereσacontrols appearance similarity to allow slight appearance changes in time. We usedσa=10for all our experiments. Introducing (20) and (21) into (19), we reformulate the estimation problem for the Gaussian kernel in (19):(22)PSi(5)|Ti,Wi=Cexp-12di⊤Σ-1di,where C is constant and:di=dCi,j(x,x′),Ci,j(x,x″)2dWi,W″2andΣ=σc200σa2.This equation quantifies the shared geometry and appearance similarity between the tracked and estimated position of the feature; hence, it provides us with the ability to judge its quality better than appearance-only based measures. In order to reduce the computational complexity of the selection process in (22), we use importance sampling to model the underlying distribution. In particular, for all the experiments, we chose 50 samples from S to represent underlying distribution. This distribution can be visualized in 2D in terms ofx″from the selectedSi(5)and is shown in Fig. 6, wherex″=x′+u, andurefers to Eq. (18) by settingλ=1.

@&#CONCLUSIONS@&#
We introduce a new formulation to estimate the true optical flow vector for features using a geometric constraint. The geometric constraint exploits the shared scene geometry based on the projective invariant coordinates and provides a new geometric optical flow equation. Together with the appearance constancy based optical flow constraint, the geometric optical flow equation provides a closed form solution to the feature tracking problem. In addition, we introduce a Bayesian approach for assessing the quality of the tracked features based on both the appearance and shared scene geometry. Our experiments have shown that the introduced quality measure and the geometry based constraint provides persisted feature tracking and is resilient to occlusions and projective changes in the features appearance caused by camera motion. Experimental evaluations against commonly used KLT, JFT and robust matching methods show superior performance which is required for higher level computer vision tasks such as 3D scene recovery. We note that the proposed tracking approach is for tracking static components of the scene. In the future, we will study extending the proposed approach to tracking dynamic objects.Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.cviu.2013.10.009.Supplementary Video 1Supplementary Video 2Supplementary Video 3Supplementary Video 4Supplementary Video 5Supplementary Video 6Supplementary Video 7Supplementary Video 8Supplementary Video 9Supplementary Video 10Supplementary Video 11Supplementary Video 12Supplementary Video 13Supplementary Video 14Supplementary Video 15Supplementary Video 16Supplementary Video 17Supplementary Video 18Supplementary Video 19Supplementary Video 20Supplementary Video 21Supplementary Video 22Supplementary Video 23Supplementary Video 24Supplementary Video 25Supplementary Video 26Supplementary Video 27Supplementary Data 1