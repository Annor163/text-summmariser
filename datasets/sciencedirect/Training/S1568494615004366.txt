@&#MAIN-TITLE@&#
Prediction approach of software fault-proneness based on hybrid artificial neural network and quantum particle swarm optimization

@&#HIGHLIGHTS@&#
We present a hybrid method using ANN and QPSO for software fault-prone prediction.ANN is used for the classification of software modules.QPSO is controlled more easily than PSO.

@&#KEYPHRASES@&#
ANN,Software metrics,Fault-prone prediction,QPSO,

@&#ABSTRACT@&#
The identification of a module's fault-proneness is very important for minimizing cost and improving the effectiveness of the software development process. How to obtain the correlation between software metrics and module's fault-proneness has been the focus of much research. This paper presents the application of hybrid artificial neural network (ANN) and Quantum Particle Swarm Optimization (QPSO) in software fault-proneness prediction. ANN is used for classifying software modules into fault-proneness or non fault-proneness categories, and QPSO is applied for reducing dimensionality. The experiment results show that the proposed prediction approach can establish the correlation between software metrics and modules’ fault-proneness, and is very simple because its implementation requires neither extra cost nor expert's knowledge. Proposed prediction approach can provide the potential software modules with fault-proneness to software developers, so developers only need to focus on these software modules, which may minimize effort and cost of software maintenance.

@&#INTRODUCTION@&#
Currently, software plays an important role in various fields, therefore software testing [1,2] is considered to be a fundamental task. However, with the development of the software industry, software size becomes bigger and bigger, and it has been a very expensive task and consumes a lot of effort and time in the software development. Because human are strongly dependent on the software system in daily lives, software faults can have serious and even fatal affects, especially for high risk systems. To solve this problem, to predict the potential faultiness of software modules during the development process can be of great help for planning activities. And we also noticed that studies show that the majority of faults are often found in only a few software modules [3], so people just need to focus on a few software modules with fault-proneness. On the other hand, guidelines to design modules non fault-proneness are also desired. These problems can be solved by extracting knowledge from historical data and constructing a model for fault-proneness prediction to be applied in future projects.Although we develop software according to some laws, there are still many factors affecting software quality. Software fault is a potential source to cause software error, the running failure and collapse. Currently, one of the tasks of software engineering is to develop high quality software systems and reduce the number of software faults. In the development process, software faults are found sooner the better, because the cost of repairing software faults usually spend more several times after the software release. Unfortunately, software faults cannot be directly measured, however it can be estimated based on software metrics. A number of studies provide empirical evidence that correlations exist between some software metrics and fault-proneness [4,5]. Identification of software modules with fault-proneness is commonly achieved by binary classifiers that predict whether or not a module is fault-proneness using various software metrics.Early software fault-proneness prediction approaches are based on statistics, however the prediction performance of these approaches was unsatisfactory. Because of this, most recent works have introduced the machine learning techniques including data mining [6], naive Bayes algorithm [7], support vector machine [8], ANN [9–11], and fuzzy logic [12], etc. Although software faults have been studied using these techniques, there are still many aspects of faults remaining unclear. We notice that the relationships between software metrics and fault-proneness are often complex and nonlinear [13], the adequacy of traditional linear models is compromised, which results in the development of non-linear models, and is expected to provide superior performance than the linear models.ANN is an interconnected group of artificial neurons that uses a computational model for information processing based on a connectionist approach to computation. In most cases, an ANN is an adaptive system that changes its structure based on external or internal information that flows through the network. An ANN possesses [11] the following characteristics:(1)It has a rapid training process, simple structure and good extend ability.It learns from experience, so, has no need for any a priory modeling assumptions.It is capable of inferring complex non-linear input–output transformations.Recently, ANN has been introduced as an effective model in machine learning and data mining communities for solving classification and regression problems [14,15]. The advantage of ANN is its ability to be used as an arbitrary function approximation mechanism that ‘learns’ from observed data. ANN can be adopt at modeling nonlinear functional relationships that is difficult to model with other techniques, and thus, is attractive for software fault-proneness prediction modeling. Many prediction models based on ANN have also been already proposed [9–11]. These studies show that ANN models have better predictive performance than some statistical models [5]. Moreover, the software fault-proneness prediction models are constructed only by software metrics, does not require expert's knowledge, which is one of the advantages of ANN. This also shows that using ANN as a classifier is appropriate. Therefore, in this paper, we use ANN as classifier to classify software modules into fault-proneness or non fault-proneness categories.In general, the performance of ANN depends on an appropriate selection of the most relevant input variables because some irrelevant and/or redundant input variables generally exist in the input space that not only make prediction harder, but also degrade generalization performance of ANN. The software metrics cannot all be used for training fault-proneness prediction model, and therefore the reducing dimensionality of input space is an essential task. In addition, each software metrics has different influence for software fault-proneness prediction, and some metrics have little effect for evaluating software fault-proneness prediction. Therefore, it is also necessary to remove these low impact metrics for reducing discriminatory of input space and improving the efficiency and performance of the prediction model.The most common approach to reduce dimensionality is Principal Component Analysis (PCA) [16] in software engineering. Some of the authors have used PCA to reduce dimensionality [13]. However, a disadvantage of PCA is that, in contrast to the original input variables, the derived dimensions may not have an intuitive interpretation. Another common approach for dimension reduction is partial least squares (PLS) [17]. Currently, PLS has been applied in many fields for dimensionality reduction [18–20]. The implementation process of classical PLS is based on the nonlinear iterative [21]. Often, some assumptions or transformations of the original sample data are required for conventional estimation methods to be used. However, in the realistic situations, the original sample data may be affected by many factors such as operational environment, testing strategy and resource allocation and so on. Therefore, to satisfy these assumptions is very difficult in real problems.At present, for the identification of relevant metrics and the removal of irrelevant metrics, three different methods can be employed, namely the filter [22,23], wrapper [24,25], and hybrid models [26,27]. The filter model relies on general characteristics of the data to evaluate and select metrics subsets without involving prediction algorithm. The wrapper model first implements an optimizing algorithm that adds or removed metrics to produce various metric subsets, and then employs a prediction algorithm to evaluate each metric subset. The hybrid model attempts to take advantage of the filter and wrapper models by exploiting their complementary strengths [28]. A disadvantage of these existing approaches is to deal with metric sequentially one by one, which is very difficult to overall evaluate the metrics.Swarm intelligence optimization is a series of relatively new optimization algorithm including bee colony optimization [29–31], ant colony optimization [32–34] and particle swarm optimization (PSO) [35–39] and so on. It simulates the swarm behavior of the social individuals, using the information interchange and cooperation between individuals to achieve the optimization. Compared to the other swarm intelligence algorithms, PSO is a simple algorithm, fewer control parameters and better convergence performance and so on. It is mainly used to solve some nonlinear complex optimization problems [40]. In addition, in the implementation process of PSO, it does not need any assumptions for software data, and only uses the characteristics of the data itself. PSO has attracted many researchers and has emerged as the most popular tool for intelligent optimal problems. However, PSO also has some disadvantages such as slower convergence rate in the late phase of PSO, lower convergence performance, usually to has three parameters and easy to fall into local minimum and so on [41]. We notice that, in addition to the advantages of PSO [42], quantum particle swarm optimization (QPSO) [43] has better global search capability, non-velocity vector and QPSO only one parameter, which indicates QPSO easier to control. Therefore, in this paper, QPSO is applied for reducing dimensionality and obtains selected metrics from the optimal solution of QPSO.This paper is organized as follows. Section 2 provides model methodology on the problem area including the block diagram of proposed approach and software metrics used in the paper. Preprocessing process is introduced in Section 3, which includes data standardization and reducing dimensionality based on QPSO. Software fault-proneness prediction approach is introduced in Section 4. Section 5 provides prediction performance measures for evaluating the performance of proposed prediction approach. The experiment results are in Section 6. Finally, conclusions are given in Section 7.We used the hybrid ANN and QPSO to develop the software fault-proneness prediction approach.It is known that, ANN has been widely applied in pattern recognition for the reason that ANN-based classifiers can incorporate both statistical and structural information and achieve better performance than the simple minimum distance classifiers [44,45], was also widely used in software engineering [46,47]. Currently, the most common ANN model is a multi-layer feed-forward neural network based on error back propagation (BP) algorithm. The neural network model includes an input layer, one or more hidden layer and an output layer. The adjacent layers achieve full connectivity between neurons, while there is no connection between neurons in a layer. In this paper, used ANN is a three-layer network. Moreover, QPSO is a global convergence guaranteed search method which introduces the quantum theory into the traditional PSO. QPSO performs better than traditional PSO on many problems [35,36]. Thus, QPSO is applied for reducing dimensionality. By the hybrid ANN and QPSO, an effective software fault-proneness prediction approach is proposed in this paper. The block diagram proposed software fault-prone prediction approach is shown in Fig. 1.Most of the current researches use software metrics to identify the fault-proneness modules and their researches showed that software metrics are very useful to predict the fault-proneness classes [48,49]. In this paper, the used metrics are McCabe [50], McCabe and Butler [51], Halstead [52] metrics, etc. Selected metrics are lines of code, cyclomatic complexity, essential complexity, design complexity, etc. Selected Halstead metrics are length, volume, program length, difficulty, intelligence, effort, effort estimate, programming time, lines if code, lines of comment, lines of blank, lines of comment and code, unique operators, unique operands, total operators, and total operands et al. Table 1lists description of these metrics.In the experiments of this paper, each software module is represented by 21 metrics for predicting software fault-proneness.For ANN, given a dataset (x(i), y(i)) (i=1, 2, …, q), where x(i)∈Rdis a vector of software metric values that quantify the metric of the ith class, and q is the total number of data samples. The output value expected of the ith class from the output neuron is y(i), and its value is “1” corresponding the fault-proneness and “0” corresponding the non fault-proneness. When training an ANN, it is common to normalize each input to the same range. This tends to improve the behavior of the training process, ensures that every initial input is equally important.We notice that the upper bound of software metrics is usually unlimited in their value range. It is necessary to obtain upper and lower bounds of the software metrics value range in order to normalize. For specific datasets and software metrics, we can get the value of each metric according to datasets. In these datasets, the value of each metric has given, so it is easy to obtain the maximum and minimum values. In this paper, for each software metric, let min(x(j)) and max(x(j)) be the minimum and maximum values of the jth software metric in the dataset, respectively. Then the scaled value X(j) isX(j)=x(j)−min(x(j))max(x(j))−min(x(j))So, each recorded value is mapped to the closed interval [0,1]. The normalized dataset is denoted by (X(i), y(i)) (i=1, 2, …, q), and the normalized metrics vector is denoted by X(i)∈Rd.PSO is a population-based optimization technique developed by Eberhart and Kennedy in 1995 [53], each particle adjusts its position in the search space from time to time according to the flying experience of its own and of its neighbors. It is initialized with a population of random potential solutions and the algorithm searches for optima satisfying some performance. The potential solutions, called particles, are flown through a multidimensional search space.Each particle i has a position represented by a position vector Xi. A swarm of particles moves through a d-dimensional problem space, with the velocity of each particle represented by a vector Vi. The particle velocity and position equations form are given by(1)Vi(t+1)=w⋅Vi(t)+c1r1(Pi,best(t)−Xi(t))+c2r2(Pglobal(t)−Xi(t))(2)Xi(t+1)=Xi(t)+Vi(t+1)where t is current iteration number, w is inertia weight, c1 and c2 are positive constants, r1 and r2 are uniformly distributed random numbers in the range [0,1]. Pi,bestand Pglobalare the best previously visited position of the particle i and the best value of all particle position values, respectively. Where Xi(t)=(Xi1(t), Xi2(t), ..., Xid(t)), and Vi(t)=(Vi1(t), Vi2(t), ..., Vid(t)). The initial velocities in particles are probabilities limited to a range of [0,1].The first part of Eq. (1) represents the inertia of the previous velocity, the second part is the cognition part and it tells us about the personal experience of the particle, the third part represents the cooperation among particles and is therefore named as the social component. And w, c1 and c2 are predefined.The cost value of particle i, at iteration t, is as follows:(3)C(Xi(t))=1q∑k=1q∑j=1d(Xij(k)(t)−Xˆij(k)(t))2where for particle i,Xij(k)is the jth output component of the kth sample, andXˆij(k)is the jth actual component of the kth observation sample. For Eq. (3), C(·) is as small as possible.For minimization problem, the smaller the objective function value, the better the cost value is. The best position Pi,bestof particle i is updated by the following formula:(4)Pi,best(t+1)=Xi(t),ifC(Xi(t))<C(Pi,best(t))Pi,best(t),ifC(Xi(t))≥C(Pi,best(t))At each update step of PSO, the velocity of each particle is calculated according to (1) and the position is updated according to (2). When a particle finds a better position than the previously best position, it will be stored in the memory. The algorithm goes on until a satisfactory solution is found or the max number Gmax of iterations is met [53].For improving the performance of traditional PSO, one of the recent developments in PSO is the application of quantum theory to observe the behavior of particles. This is because, in quantum space, the aggregation of particles is described by generated bound state accordance to some attract potential field in the particles sports center. The particles with quantum bound states can occur at any point of the solution space with a certain probability. The particles with aggregation state can search in the entire feasible solution space, but will not diverge to infinity.In the quantum model of PSO, the state of a particle is described by wave function Ψ(x, t) instead of position and velocity. The dynamic behavior of the particle is widely divergent from the particle in traditional PSO. In this context, the probability of the particle's appearing in position X from probability density function Ψ(x, t)2, the form of which depends on the potential field of the particle [54].The particles move according to the following iterative equations [55]:(5)Xi(t+1)=pi(t)+α⋅mi,best−Xi(t)⋅ln1ui(t),ifs≥0.5(6)Xi(t+1)=pi(t)−α⋅mi,best−Xi(t)⋅ln1ui(t),ifs<0.5where ui, s are uniformly distributed random numbers in the interval [0,1], and uniformly parameter α is called contraction-expansion coefficient. And(7)pi(t)=φi(t)⋅Pi,best(t)+(1−φi(t))⋅Pglobal(t)(8)mi,best=1Q∑i=1QPi,best(t)where φiis uniformly distributed random number in the interval [0,1], Q is number of all particles, and mi,bestis defined as the mean of the best positions of all particles in the population.The termination criterion of QPSO is that, if the absolute difference between the C(t+1) and C(t) of 10 times continuously is less than δ, then stop the algorithm; otherwise until the maximum number Gmax of iterations is met. Where δ is a training threshold.The flow chart of QPSO algorithm is shown as follows.QPSO algorithmInitialize the positions and the Pi,bestpositions of all the particlesDoCalculate mi,bestusing Equation (8) for particle i, where i=1, 2, …, QSelect a suitable value αFor particles i=1 to QCalculate the cost value of particle i according to Equation (3)Update Pi,bestusing Equation (4)Update Pglobalas followsIf C(Pi,best(t))<C(Pglobal(t−1)), then Pglobal(t)=Pi,best(t); else Pglobal(t)=Pi,best(t−1)For dimension 1 to dφ=rand (0,1)u=rand (0,1)If s=rand (0,1)≥0.5update particle positions using Equation (5)elseupdate particle positions using Equation (6)Until terminal condition is metAssume that D=(S,M) is a dataset with q samples and d metrics, where M={m1, …, md} and S={S1, …, Sq} is metrics and sample sets, respectively. C={c1, …, ct} refers to type set.To reduce dimensionality, we encode the solution X of QPSO into a binary string according to the following operation:For any given random number, rand, in the range [0,1],(9)if(rand<S(X))thenX=1,elseX=0where S(·) is a sigmoid function, namely S(x)=1/(1+e−x).In proposed reducing dimensionality approach, the position of particle i is represented by a binary (0 or 1) string, e.g., Xi={Xi1, Xi2, …, Xid}. And 1 represents a selected metric, while 0 represents a non-selected metric. In QPSO, once the final solution Pglobalis obtained, the metrics of the best particle's Pglobalcan be tracked with regard to its position. We let the selected metric set be M1⊂M, and the number of selected metrics be l<d.For ANN, all input data and desired outputs have been normalized so that they always fall in the interval [0,1]. In the software fault-proneness prediction approach, ANN is regarded as a mapping from the data space to “1” or “0”. The number of input neurons of ANN is determined by the number of selected metrics. In this paper, the selected metrics set M1 is fed to the input layer of the ANN. The number of output neurons is 1.Up to now, we can acquire software fault-proneness prediction algorithm.Algorithm QASFPQPSO+ANN based software fault-prone prediction algorithmThe input metrics x was normalized, and obtain the normalized metrics X. They always fall within the interval [0,1].We divided preprocessed dataset into two subsets randomly, i.e., training and testing subsets according to appropriate ratio.Develop ANN model on the training subset, and obtain trained ANN.Obtain a reduced dimensionality subset M1 from M based on QPSO, and we can obtain the reduced input data setX′’ and the simplified ANN.Develop simplified ANN model on training subset, and obtain new ANN.Predict the module (i.e., fault-proneness or non fault-proneness) using new ANN in the testing subset.For analyzing performance of the software fault-proneness prediction approach, compute the prediction performance measures values and obtain the performance evaluation.The performance of prediction approach for two-class problem is commonly evaluated using the data in the confusion matrix [56], which is shown in Table 2.In Table 2, fijexpresses that the number of actual class=i is predicted into class=j, where i, j∈{0,1}. We will give the commonly used prediction performance measures: Sensitivity and Specificity to evaluate prediction approaches quantitatively. These measures are derived from the confusion matrix.Sensitivity is known as fault detection rate. It is defined as the ratio of the number of classes correctly predicted as fault-proneness to the total number of classes that are actually fault-proneness. It is calculated as follows:Sensitivity=f11f11+f10Specificity is also known as correctness. It is defined as the ratio of the number of classes correctly predicted as non fault-proneness to the total number of classes predicted as non fault-proneness. It is calculated as follows:Specificity=f00f00+f01Both Sensitivity and Specificity are important performance measures. The higher the Sensitivity is, the less effort is wasted in testing and inspection; the higher the Specificity is, the fewer fault-proneness modules go undetected.Many authors have preferred the analysis of the receiver operating characteristic (ROC) curve [57] for trade-off between sensitivity and specificity. The ROC curve gives a good visualization of the performance of a classifier [58]. The most northwest the point, the best the associated classifier. However, frequently, for comparison purposes of classification algorithms, a single measure for the classifier performance is needed. In this paper, we used the area under the ROC curve (AUC) to measure performance of proposed software fault-proneness prediction approach. The higher AUC, the more performance of the prediction approach. The AUC has an attractive property [58], i.e., it is insensitive to changes in class distribution, and it also is the most general and robust measure for classifier.

@&#CONCLUSIONS@&#
This paper investigated the application of hybrid ANN and QPSO to develop the prediction approach of software fault-proneness. The proposed prediction approach can identify fault-proneness of software modules only by software metrics. The main advantages of proposed prediction approach are as follows:(1)In this paper, QPSO is used for reducing dimensionality of the metrics space, and ANN is used for prediction fault-proneness of software modules. QPSO+ANN has been proven to be very effective for establishing relationship between software metrics and fault-proneness.The proposed reduction dimensionality approach is very simple. This is because PSO has three control parameters, QPSO only one, therefore QPSO more easily controls. Experiment results confirm that it is possible that the reduction dimensionality technique based on QPSO can simplify ANN architecture and also ensure that QPSO+ANN has better performance than existing most other prediction approaches.Those selected few metrics from metric space have more significant effect than other non selected metrics for the fault-proneness of software modules, which tells us that, in the software development process, developers should pay more attention to these selected metrics, but not all metrics.Proposed prediction approach can effectively predict software fault-proneness, so developers only need to focus on those software modules with fault-proneness, which may minimize the cost of software maintenance.