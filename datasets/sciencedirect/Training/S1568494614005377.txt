@&#MAIN-TITLE@&#
Enhanced leader PSO (ELPSO): A new PSO variant for solving global optimisation problems

@&#HIGHLIGHTS@&#
A novel optimisation algorithm, named enhanced leader PSO (ELPSO), is introduced.ELPSO mitigates premature convergence problem of conventional PSO.ELPSO is mainly based on a five-staged successive mutation strategy.At each iteration, the successive mutation strategy is applied to swarm leader.The results confirm the outperformance of ELPSO over other compared algorithms.

@&#KEYPHRASES@&#
Particle swarm optimisation,Global optimisation,Heuristics,

@&#ABSTRACT@&#
Particle swarm optimisation (PSO) is a well-established optimisation algorithm inspired from flocking behaviour of birds. The big problem in PSO is that it suffers from premature convergence, that is, in complex optimisation problems, it may easily get trapped in local optima. In this paper, a new PSO variant, named as enhanced leader PSO (ELPSO), is proposed for mitigating premature convergence problem. ELPSO is mainly based on a five-staged successive mutation strategy which is applied to swarm leader at each iteration. The experimental results confirm that in all terms of accuracy, scalability and convergence rate, ELPSO performs well.

@&#INTRODUCTION@&#
A lot of real-world problems in different areas of sciences and technologies can be translated into optimisation problems. In literature, there exist various strategies for solving optimisation problems. Among them, particle swarm optimisation (PSO) is a heuristic optimisation algorithm that is being applied to different optimisation problems [1–3]. Some features of PSO can be listed as below.•It has few parameters to be tuned by user [4].Its underlying concepts are quite simple and its coding is easier than other standard heuristic algorithms such as bacterial foraging optimisation algorithm (BFOA) and artificial bee colony (ABC).It does not require preconditions such as continuity or differentiability of objective functions.As mentioned above, the main problem in PSO is that its exploration capability is weak. This problem makes its application in difficult optimisation problems, especially multimodal problems, problematic, so, devising new PSO variants with strong explorative capability is of high value and importance. Therefore, in this paper, the objective is to develop a new PSO variant with strong explorative and exploitative capabilities.The rest of the paper is organised as follows; in Section 2, an overview of PSO is provided and some modified PSO variants are reviewed. In Section 3, the proposed PSO variant is introduced. Section 4 contains the experimental results and analysis. Finally, some conclusions are drawn in Section 5.PSO is launched with initialisation of a swarm of particles in the n-dimensional search space (n is the dimension of problem in hand). In PSO, each particle keeps two values in its memory; its own best experience whose position and objective value are called Piand Pbest respectively and the best experience of the whole swarm, whose position and objective value are called Pgand gbest respectively. The position and velocity of particle i is represented by the following vectors:Xi=(Xi1,Xi2,…,Xid,…,Xin)Vi=(Vi1,Vi2,…,Vid,…,Vin)The velocities and positions of particles are updated in each time step according to the following equations [5]:(1)Vid(t+1)=Vid(t)+C1r1d(Pid−Xid)+C2r2d(Pgd−Xid)(2)Xid(t+1)=Xid(t)+Vid(t+1)where C1 and C2 are called cognitive and social acceleration coefficients respectively, r1dand r2dare two random numbers in the interval [0,1].However, primary PSO characterised by (1) and (2) did not work desirably [6]. Therefore, the inertia weight PSO was introduced wherein the velocity update equation is as follows [6]:(3)Vid(t+1)=ωVid(t)+C1r1d(Pi−Xid)+C2r2d(Pgd−Xid)where ω is called inertia weight and is commonly decreased during the run.After introducing primary versions of PSO, the researchers developed many different modified PSO variants to mitigate shortcomings of primary versions. The main focus in these modified variants is to enhance PSO's exploration capability and alleviate premature convergence problem [7–10]. In [11], incorporating a constriction factor was proposed to bound particles movements and enhance PSO's performance. In [12], PSO as an algorithm with strong exploitation capability and weak exploration capability is hybridised with artificial bee colony (ABC) as an algorithm with weak exploitation capability and strong exploration capability. In this hybridisation, the global best solutions obtained by the PSO and ABC are recombined and the solution obtained from this recombination is given to the populations of the PSO and ABC as the global best and neighbour food source for onlooker bees, respectively. Application of this hybrid algorithm to mathematical benchmark functions and energy demand estimation problem showed its superiority over ABC and PSO. Actually, the results indicated that the hybrid algorithm exposes strong capability both in exploration and exploitation.In [3], Newton's laws of motion are incorporated into PSO. It exposes desirable convergence speed, solution accuracy and global optimality. In [13], in a variant called “compact PSO” the search logic of PSO is employed but instead of having an actual swarm of solutions, a probabilistic representation of the population is used. In [14] a new PSO variant is proposed wherein two strategies including a diversity enhancing mechanism and two neighbourhood search strategies are incorporated into PSO. The former strategy is helpful to increase the swarm diversity by adjusting the dissimilarities among particles. The latter strategy is beneficial for accelerating the convergence rate because of the attraction of the previous best particles and the global best particle. By combining these two strategies, an appropriate trade-off between exploration and exploitation is attained. In [15], cellular PSO is proposed which hybridises cellular automata (CA) and PSO. A review of some recent new PSO variants has been tabulated in Table 1.In conventional PSO, there exist two crucial issues; first issue is that since all particles are attracted towards swarm leader, they may converge prematurely without enough exploration of search space, that is, conventional PSO is prone to premature convergence. The latter issue is that if the particles are trapped in local optima, i.e., Pi≅Xi≅Pg, then according to Eq. (3), we have:(4)Vid(t+1)≅ωVid(t)Since ω<1, Vid→0, that is, the particles’ velocities tend to be zero. Therefore, the particles cannot jump out from local optimum meaning that in conventional PSO, there is no mechanism for jumping out from local optima.In PSO, all particles are attracted towards swarm leader (Pg). So, having high quality leader can make the search process more efficient. Like a society or organisation wherein existence of a good leader can result in more success. Enhanced leader PSO (ELPSO) is a new PSO variant whose main characteristic is the enhancement of swarm leader at each iteration of search process [8]. In ELPSO, at each iteration, a five-staged successive mutation strategy is applied to swarm leader. After applying each mutation, if the mutated Pghas better fitness value than the current Pg, it takes the position of current Pg. By applying this successive mutation strategy to swarm leader, swarm leader is enhanced at each iteration, so a more efficient search process is achieved [8].At the first stage of the successive mutation strategy, Gaussian mutation is applied to swarm leader as below.(5)Pg1(d)=Pg(d)+(Xmax(d)−Xmin(d))⋅Gaussian(o,h)ford=1,2,…,nwhere Xmax(d) and Xmin(d) represent upper and lower bounds of decision vectors in dth dimension and h is standard deviation of Gaussian distribution. If the fitness of Pg1 is better than fitness of Pg, then Pg1 takes the position of Pg.The standard deviation of the Gaussian distribution is decreased linearly during the run as Eq. (6). This is to ensure that the exploration capability is stronger at initial iterations and it fades out during the run to have more exploitation capability.(6)h(t+1)=h(t)−1tmaxAt the second stage of the successive mutation strategy, Cauchy mutation is applied to swarm leader as below.(7)Pg2(d)Pg(d)+(Xmax(d)−Xmin(d))⋅Cauchy(o,s)ford=1,2,…,nwhere s is scale parameter of Cauchy distribution which is decreased linearly during the run as Eq. (8). This is to ensure that the exploration capability is more at initial iterations and it fades out during the run to have more exploitation capability.(8)s(t+1)=s(t)−1tmaxIf the fitness of Pg2 is better than fitness of Pg, then Pg2 takes the position of Pg.At the third stage of the successive mutation strategy, opposition-based mutation is separately applied to all different dimensions of Pgas below.(9)Pg3(d)=Xmin(d)+Xmax(d)−Pg(d)ford=1,2,…,nIf the fitness of Pg3 is better than fitness of Pg, then Pg3 takes the position of Pg.At fourth stage of the successive mutation strategy, opposition-based mutation is applied to the whole Pgas below.(10)Pg4=Xmin+Xmax−PgIf the fitness of Pg4 is better than fitness of Pg, then Pg4 takes the position of Pg.At fifth stage of the successive mutation strategy, DE-based mutation operator is applied to Pgas below.(11)Pg5=Pg+F(Xr−Xs)where r and s are two random unequal particles in swarm and F is a control parameter called scale factor.If the Pg5 is fitter than Pg, then Pg5 takes the position of Pg.ELPSO addresses both mentioned crucial issues of conventional PSO. Since at each iteration, different regions of search space are explored to find the best possible swarm leader, the premature convergence problem is expected to be mitigated. Moreover, the successive mutation strategy of ELPSO acts as a jumping out mechanism when the particles are trapped in local optima. The positive features of ELPSO are listed below [8].1.In all five stages of successive mutations applied to Pg, Pgis mutated if the new Pgpossesses better objective than the current Pg. In other words, the mutations are conditional. By conditional mutations used in ELPSO, Pgand the whole swarm are more attracted towards regions with good objective values and more quality solutions can be achieved.By using five successive mutations, different regions of search space are searched efficiently, therefore premature convergence probability decreases. Actually, at each iteration, in stage 1 (Gaussian mutation) one, in stage 2 (Cauchy mutation) one, in stage 3 (opposition-based mutation on dimensions) “n”, in stage 4 (opposition-based mutation on Pgas a whole) one and in stage 5 (DE-based mutation) one new position for finding better leader is investigated. Therefore, at each iteration, “n+4” new positions for finding better swarm leader are tested. The position with the best objective value among these “n+4” cases and the position achieved by PSO flight equations is determined as the swarm leader for that iteration. This feature diminishes premature convergence probability. For better imagination, assume an optimisation problem whose dimension is 20. While in conventional PSO, at each iteration one position is swarm best, in ELPSO, at each iteration 25 positions are tested and the best one is determined as swarm leader, so we have an enhanced swarm leader. However, in ELPSO, number of function evaluations for the same number of iterations and particles is different from number of function evaluations in conventional PSO. That is, unlike conventional PSO whose number of function evaluations is computed by Eq. (12), in ELPSO, it is computed by Eq. (13).(12)NFECPSO=Np⋅(1+tmax)(13)NFEELPSO=Np⋅(1+tmax)+(n+4)tmaxwhere NFECPSO and NFEELPSO represent number of function evaluations in conventional PSO and ELPSO respectively, Npis number of particles and tmax is maximum number of iterations.In ELPSO, explorative capability decreases during the run, because in stages 1 and 2 of successive mutation (Gaussian and Cauchy mutations), the standard deviation and scaling parameter are decreased during the run. This property is very important for optimisation algorithms.Unlike most existing mutation operators which are applied to positions and velocities of particles, in ELPSO mutations are applied to swarm leader. Therefore, the leader is improved which attracts all particles towards better regions of search space.Mutations can be classified into two groups; mutations with short jump and mutations with long jump. The mutations with short jump can reduce premature convergence probability in PSO but are not able to jump out particles from local optimum when the swarm stagnates. On the other hand, mutations with long jump are efficient jump-out mechanisms and can successfully jump out particles from local optimum after occurrence of stagnation. In ELPSO, stages 1, 2 and specially 3 are considered long jump mutations whose presence may make the PSO very capable in jumping out local optimum after stagnation.The flowchart of ELPSO is illustrated in Fig 1[8]. Its pseudocode has also been presented in Appendix A.1.

@&#CONCLUSIONS@&#
In this paper, a novel optimisation algorithm, named as enhanced leader PSO (ELPSO), has been introduced to mitigate premature convergence problem of conventional PSO. ELPSO is mainly based on a five-staged successive mutation strategy which is applied to swarm leader at each iteration. For validating ELPSO, its performance is compared with linearly decreasing inertia weight PSO (CPSO), opposition-based differential evolution (ODE), competitive differential evolution (CDE), harmony search (HS), genetic algorithm (GA), firefly swarm optimisation (FSO), gravitational search algorithm (GSA), brainstorm optimisation algorithm (BSOA), artificial bee colony (ABC), fixed inertia weight PSO (FIWPSO), chaotic inertia weight PSO (CIWPSO), random inertia weight PSO (RIWPSO), time varying acceleration coefficient PSO (TVACPSO) and constricted PSO (COPSO). The comparisons approve outperformance of ELPSO in all terms of accuracy, scalability and convergence rate.Pseudocode of ELPSOSet ELPSO parameters (C1, C2, Wi, Wf, Np, tmax, Vmax, F, h, g)Set problem parameters (n, Xmin, Xmax, f) % f stands for objective function which is intended to be minimised.Initialise swarm randomly, set the objective value of each particle as its personal best and set the best objective value among all particles as swarm best.t=0% t is iteration numberWhile t⇚tmax doFor i=1:Npdo% Flight equationsUpdate the velocities of particles by two following flight equationsVi(t+1)=ωVi(t)+C1r1(Pi−Xi)+C2r2(Pg−Xi)Xi(t+1)=Xi(t)+Vi(t+1)Update personal bests and swarm best% End of flight equations% Start of five successive mutations on Pg% Stage 1: Gaussian mutationFor d=1:n doPg1(d)=Pg(d)+(Xmax(d)−Xmin(d))·Gaussian(o, h)End forBound position of Pg1If f (Pg1)<gbest doPg=Pg1gbest=f(Pg1)End ifh=h−(1/tmax)% End of stage 1(Gaussian mutation)% Stage 2: Cauchy mutationFor d=1:n doPg2(d)=Pg(d)+(Xmax(d)−Xmin(d))·Cauchy(o, s)End forBound position of Pg2If f (Pg2)<gbest doPg=Pg2gbest=f(Pg2)End ifs=s−(1/tmax)% End of stage 2 (Cauchy mutation)% Stage 3: opposition-based mutation operator separately for each dimension of PgFor d=1:nPg3=PgPg3(d)=Xmin(d)+Xmax(d)−Pg3(d)If f (Pg3)<f(Pg3)Pg=Pg3End ifEnd forEndBound Pg3(d) to feasible regionIf f (Pg3)<gbest doPg=Pg3gbest=f(Pg3)End if% End of stage 3 (opposition-based mutation separately for each dimension of Pg)% Stage 4: opposition-based mutation on Pgas a wholePg4=Xmin+Xmin−PgBound position of Pg4If f (Pg4)<gbest doPg=Pg4gbest=f(Pg4)End if% End of stage 4 (opposition-based mutation on Pgas a whole)% Stage 5 DE-based mutationPg5=Pg+F(Xr−Xs) % r and s are two random unequal particles in swarmBound Pg5 to feasible regionIf f (Pg4)<gbest doPg=Pg4gbest=f(Pg4)End if% End of stage 5 (DE-based mutation)% End of comprehensive premature convergence alleviatorω=ω−(1/tmax)End forEnd whilePrint Pg and gbestTable A.1.Tables A.2–A.6.