@&#MAIN-TITLE@&#
Support vector regression for loss given default modelling

@&#HIGHLIGHTS@&#
New support vector regression (SVR) techniques are applied to recovery rates of corporate bonds.The proposed techniques outperform other methods significantly.We modify the SVR algorithm to account for heterogeneity of bond seniorities.Transformation of recovery rates does not improve the prediction accuracy.Improved SVR models capture the features of each segment better than segmented ones.

@&#KEYPHRASES@&#
Support vector regression,Loss given default,Recovery rate,Credit risk modelling,

@&#ABSTRACT@&#
Loss given default modelling has become crucially important for banks due to the requirement that they comply with the Basel Accords and to their internal computations of economic capital. In this paper, support vector regression (SVR) techniques are applied to predict loss given default of corporate bonds, where improvements are proposed to increase prediction accuracy by modifying the SVR algorithm to account for heterogeneity of bond seniorities. We compare the predictions from SVR techniques with thirteen other algorithms. Our paper has three important results. First, at an aggregated level, the proposed improved versions of support vector regression techniques outperform other methods significantly. Second, at a segmented level, by bond seniority, least square support vector regression demonstrates significantly better predictive abilities compared with the other statistical models. Third, standard transformations of loss given default do not improve prediction accuracy. Overall our empirical results show that support vector regression techniques are a promising technique for banks to use to predict loss given default.

@&#INTRODUCTION@&#
The introduction of the Basel II and Basel III Accords (BIS, 2005a, 2005b, 2011) requires that banks in the G20 countries hold specified amounts of capital to reduce the chance of their insolvency. The amount of capital required under the Internal Rating Based (IRB) advanced approach is based on the calculation of the proportions of defaulted loans that the bank will never recover, termed Loss Given Default (LGD). Similarly the proportion has been recovered can be defined as recovery rate (RR) equals to one minus LGD. Yet compared with the extensive research on modelling the probability of default, there is relatively little research on LGD, and that which has been published shows very poor predictive accuracy. In this paper we present improved support vector regression (SVR) models that give substantial increases in predictive accuracy compared with previously published methods.Two types of predictive models have been applied in the empirical literature: parametric and non-parametric. Among the parametric models the most popular are linear regression models that have shown robustness and effectiveness in LGD prediction and explanation. Acharya, Bharath, and Srinivasan (2007) conclude from including the industry distress dummies into a linear regression model that industry distress conditions have negative effects on the RR of defaulted firms’ debts. Their results suggest RR falls during distress periods due to both the downward trend in asset values and liquidity constraints. Qi and Yang (2009) in a study of LGD of residential mortgages demonstrate that LGD can be explained by linear regression that includes debt characteristics, with loan-to-value playing the single most important role. These results are confirmed by Khieu, Mullineaux, and Yi (2012) who estimate RR of bank loans with loan characteristics, borrower characteristics and macroeconomic conditions. They suggest loan characteristics are more significant determinants of RR than the other factors. Leow, Mues, and Thomas (2013) investigate the role of macroeconomic variables in two retail loans data sets. They find that the inclusion of macroeconomic variables can improve the prediction of residential mortgage LGD but bring little improvements for personal loan LGD.Empirical LGD distributions are often bi-modal and usually bounded between [0,1], suggesting that a linear regression model might fit poorly. Therefore, in order to improve the fit and predictive accuracy of the model, various transformations of LGD have been tried prior to the modelling stage. Gupton and Stein (2002) propose to transform the distribution of LGD into a normal distribution by a beta distribution function and then to model the transformed target with nine factors. They conduct extensive validation studies showing that such beta transformed linear regression gives better predictions than historical average methods. Another attractive alternative to linear regression is a generalized linear model such as a fractional response model. Dermine and Neto De Carvalho (2006) employ a complementary log–log model to predict the cumulative RR of corporate loans from a Portuguese bank and report the R2 as 0.13 for the 12-month prediction. Jacobs and Karagozoglu (2011) propose a beta-link generalized linear model to estimate LGD at firm and instrument levels jointly and report a significant improvement in terms of both in-sample and out-of-sample performances. Leow and Mues (2011) investigate a two-stage model to predict the LGD of UK residential mortgage loans with a combination of a probability of repossession model and a haircut model (a model that predicts a proportion of lost value for a repossessed property). This study suggests that such a two-stage modelling approach works better than a single-stage model. Calabrese (2010) applies an inflated beta regression model to predict RR of loans from The Bank of Italy where the dependent variable is assumed as a mixture of a continuous beta distribution on (0,1) and a discrete Bernoulli distribution to model the probability mass at the boundaries 0 and 1. This study shows that the out-of-sample prediction of the inflated beta regression model outperforms fractional response regression models in terms of both MSE and MAE. Bellotti and Crook (2012) benchmark a number of different transformations and algorithms to predict the LGD for a credit cards data set. Surprisingly, they find that linear regression (OLS) with no variable transformations gives greater predictive accuracy.Although parametric models are simple to implement and easy to explain, past research reports rather poor predictions of LGD, and generalized linear regression models do not achieve significant improvements compared with linear regression. Zhang and Thomas (2010) compare both linear regression and survival regression for modelling RR of personal loans from a UK bank, and report the out-of-sample R2 as low as 0.0904 for linear regression, and the parametric survival models exhibit even poorer predictions. It is also surprisingly interesting to see that given the versatility of the distribution allowed in the Cox approach, the predictive accuracies can still not be improved compared with linear regression model. Similar evidence provided by Bellotti and Crook (2012) show the model fit of simple linear regression to be rather weak with R2 of 0.1428, and still the predictions of this model outperform the other ones including logit and probit models.In contrast, non-parametric methods provide much more flexibility in modelling LGD, although literature on this topic is not as extensive as for parametric models. One of the major advantages of non-parametric methods is that they do not assume a specific distribution for LGD. Unlike parametric models which imply a specific form of the LGD distribution, non-parametric methods do not make any prior assumptions when fitting a regression model. This often leads to a better performance compared with parametric techniques, as reported by previous research. For example, Bastos (2010) compares parametric fractional response regression and a non-parametric regression tree model to forecast bank loans RR and finds that the latter is superior. More strong evidence comes from Qi and Zhao (2011) who compare six modelling methods including four parametric statistical models and two data mining techniques (decision trees and neural networks) for a mixed portfolio of bonds and loans. They find non-parametric methods perform significantly better than other parametric methods in terms of both model fit and prediction accuracy. Tong, Mues, and Thomas (2013) develop a zero adjusted gamma model to predict LGD of a UK bank where the non-parametric smoothing splines are incorporated into the predictor of a mixture gamma distribution. The findings show that such a semi-parametric formulation gives favorable out-of-sample predictions compared with the traditional linear regression.This study focuses on another promising non-parametric data mining technique: support vector machines (SVM) and application to LGD modelling. SVM was first studied by Vapnik (1995, 1998) and are widely applied in engineering, bioinformatics and decision sciences. Previous research has revealed that SVM can not only handle non-linear problems well, but also avoid the over-fitting problem that is common in neural networks based on the principle of structural risk minimization. SVM models have been widely applied in credit risk modelling as a tool to solve classification problems such as in credit scoring, i.e. to classify credit applicants into ‘Good’ or ‘Bad’ risks. On the other hand, support vector regression (SVR) adapted to regression problems has been developed and effectively applied to non-linear regression and to time series prediction problems. However, until now only one published paper, by Loterman, Brown, Martens, Mues, and Baesens (2011), has investigated the application of SVR to LGD modelling. They conduct a comprehensive benchmarking study on six retail loan data sets with 24 techniques, some of which are two-stage models including both linear and non-linear techniques, and they find that non-linear techniques including neural networks and SVR models consistently outperform other traditional linear methods. But they do not make any further improvements on SVR models.Our paper makes three distinct contributions based on the analysis of the RR of corporate bonds. First, the predictive performance of RR is modelled by using different intercepts or dummy variables to explain the unobservable heterogeneity of different bond seniorities. Second, SVR models are applied to losses from corporate bonds for the first time. In addition, the dataset comprises a longer time series of observations than previous studies and uses a more comprehensive set of predictor variables, including the debt characteristic, the accounting ratios from obligors’ financial statements. Macroeconomic factors are also included to allow for any possible systematic differences in LGD over time. Third, the paper investigates whether transforming LGD values using a logistic or beta transformation prior to analysis can improve SVR model fitting and prediction accuracy. The results show that all SVR models substantially outperform other statistical models in terms of both model fit and out-of-sample predictive accuracy, and we find that the robustness of SVR models is comparable to that of statistical models. However, a logistic or beta transformation prior to modelling does not provide any improvement in prediction.The rest of the paper is organized as follows. Section 2 presents the models, the data used in this research is described in Section 3. Section 4 discusses the results and conclusions are drawn in Section 5.In this section both parametric regression and SVR models are presented and the proposed SVR models are elaborated in more detail. Note that in line with literature and our data the target variable is RR instead of LGD.Previous empirical research shows that linear regression models appear to be of comparable predictive accuracy as other more complicated statistical models (Bellotti & Crook, 2012; Qi & Zhao, 2011) even though they have the potential risk to make predictions out of the range between 0 and 1. Consider a datasetD={(xi,yi)}i=1Nwith the covariates xi∊Rmwhich is m-dimensional and the related dependent variable is yi∊R, andβdenotes a vector of population parameters. The linear regression model is given as(1)yi=βTxi+εiεi∼N(0,σ2),Maximum likelihood methods can be applied to estimate the parameters.Fractional response regression is defined by Papke and Wooldridge (1996) and has been widely applied in RR modelling (Dermine & Neto De Carvalho, 2006; Bastos, 2010; Bellotti & Crook, 2012; Khieu et al., 2012). In this model, the dependent variable is bounded between 0 and 1 by imposing a link function. The model is defined as(2)E(yi|xi)=G(βTxi),where G(•) denotes some link function such as a logistic transformation function or a complementary log–log function such as:(3)G(βTxi)=exp(βTxi)/(1+exp(βTxi))G(βTxi)=exp(-exp(-βTxi),and the quasi maximum likelihood function can be written as follows(4)logL=yilog(G(βTxi))+(1-yi)log(1-G(βTxi)).In the following we present three support vector regression models. The first one is least squares support vector regression (LS-SVR) proposed by Suykens and Vandewalle (1999) and Suykens et al. (2002). Two improved models are proposed based on LS-SVR.Consider the dataset given in Section 2.1. The LS-SVR is defined based on the quadratic loss function such as(5)minJ(w,b;ui)=12‖w‖2+C2∑i=1Nui2s.t.yi=wTφ(xi)+b+ui,i=1,…,N,where w denotes the parameter vector of the associated covariates and b is the intercept. Notice that the error termsui2are scaled by a regularized parameter C, and φ(xi) denotes the kernel function that maps the data from original data space to a higher dimensional space. This model is solved by its dual form problem which can be derived from a Lagrangian function such asL(w,b,ui;αi)=J(w,ui)-∑i=1Nαi(wTφ(xi)+b+ui-yi),where αiis the Lagrangian multiplier. Based on the KKT condition, the solution of the dual form is equivalent to solving the following linear equation systems(6)0eTeK‾bα=0y,wheree=(1,…,1︸1×N)T, y=(y1,… ,yN)T, α=(α1,… ,αN)T,K‾=K+1CI, where K is the kernel matrix with K(xi,xj)=φ(xi)⋅φ(xj) and I is the identity matrix. The closed form solution is obtained as(7)α∗=K‾-1(y-b∗e)b∗=eTK‾-1yeTK‾-1e,Finally the estimated regression model can be written as(8)g(x)=∑iαi∗K(xi,x)+b∗.Now we consider extending LS-SVR by introducing heterogeneity for different groups. In this model we assume that observations in the same group have an unobserved homogeneity that can be represented by intercepts. Now consider a clustered cross sectional data set such as D={(xkj,ykj)},j=1,… ,pk,k=1,… ,M where xkjdenotes the covariates of the jth sample in the kth group, and pkis the number of individuals in this group. The total number of cases in the whole dataset is p1+p2+⋯+pM=N, where M indicates the total number of groups in this dataset. The least squares SVR model with different intercepts can be constructed as follows(9)minJ(w,bk;ukj)=12‖w‖2+12∑k=1Mbk2+C2∑k=1M∑j=1pkukj2s.t.ykj=wTφ(xkj)+bk+ukjk=1,…,Mj=1,…,pk.Notice that bkis a group specific intercept. With such specifications this model is able to predict the out-of-sample individuals. The Lagrangian function of model (9) can be written asL(w,bk,ukj;αkj)=12‖w‖2+12∑k=1Mbk2+C2∑k=1M∑j=1pkukj2-∑k=1M∑j=1pkαkj(wTφ(xkj)+bk+ukj-ykj).The KKT conditions are(10)∂L∂w=w-∑k∑jαkjφ(xkj)=0⇒w=∑k∑jαkjφ(xkj)∂L∂bk=bk-∑jαkj=0⇒bk=∑jαkj∂L∂ukj=Cukj-αkj=0⇒ukj=αkjC.Then the dual form problem is given as(11)min12αTKα+12αTWα+12CαTα-yTα.Here W is a block diagonal matrix defined asW=W1W2⋱WM, and each Wkis a pk×pkmatrix with all elements equal to 1. To solve for the optimal solution it is only necessary to solve the following linear system by taking the partial derivative of model (11) with respect to α(12)K+W+1CIα=y,where I denotes a N×N identity matrix, and K is defined as above. Denoting the solution of the above equation asα∗, the optimal solutionw∗,bk∗for Eq. (9) is obtained as(13)w∗=∑k∑jαkj∗φ(xkj)bk∗=∑jαkj∗.This section presents a semi-parametric model where dummy variables are applied to denote the unobservable heterogeneity of the seniorities of bonds. In this semi-parametric model, we assume dummy variables influence the dependent variable linearly while other variables are still equipped with kernel functions such that(14)minJ(w,bk;ukj)=12‖w‖2+12βTβ+12b2+C2∑k=1M∑j=1pkukj2s.t.ykj=wTφ(xkj)+βTzkj+b+ukjk=1,…,Mj=1,…,pk,where zkjis a vector consisting of the dummy variables and β is the vector of the corresponding parameters. Here β is treated as a vector of fixed effects with respect to the group specific variables while bkare replaced by a common intercept b as in model (5). The Lagrangian function and KKT conditions are as aboveL(w,b,ukj;αkj)=12‖w‖2+12βTβ+12b2+C2∑k=1M∑j=1pkukj2-∑k=1M∑j=1pkαkj(wTφ(xkj)+βTzkj+b+ukj-ykj),and(15)∂L∂w=w-∑k∑jαkjφ(xkj)=0⇒w=∑k∑jαkjφ(xkj)∂L∂β=β-∑k∑jαkjzkj=0⇒β=∑k∑jαkjzkj∂L∂b=b-∑k∑jαkj=0⇒b=∑k∑jαkj∂L∂ukj=Cukj-αkj=0⇒ukj=αkjC.The dual form is(16)min12αTKα+12αTZα+12αTVα+12CαTα-yTα,whereZij=zkiTzkj, and V is a N×N matrix with all elements equal to 1. All the other notations are the same as model (5). Model (16) can be solved with the same procedure as above and the linear equation systems can be obtained as follows(17)K+Z+V+1CIα=y.The solution for w and β can be derived as(18)w∗=∑k∑jαkj∗φ(xkj)β∗=∑k∑jαkj∗zkjb∗=∑k∑jαkj∗.A two-stage model is proposed by Bellotti and Crook (2012) to predict the LGD of credit cards from a UK retail bank. They first split the LGD into three classes including LGD equal to 0 or 1 and 0<LGD<1 by a decision tree, and then estimate the LGD belongs to the interval (0,1) by an ordinary linear regression.Two different transformations are employed in this study; one is a logistic transformation defined as follows(19)ylogit=lny1-y,and the other is a beta transformation that is well recognized in LGD modelling since it was proposed in Gupton and Stein’s seminal paper (Gupton & Stein, 2002). The beta distribution is defined within the interval (0,1) as follows(20)f(y;p,q)=Γ(p+q)Γ(p)Γ(q)yp-1(1-y)q-1,p>0,q>0,where p and q are two parameters that control the shape of distribution. Following from the idea of Moody’s LossCalc model, the transformed dependent variable becomes(21)ybeta=N-1(Beta(y;p,q)),where N−1(⋅) denotes the inverse cumulative normal distribution. We examine the applications of these two different transformation methods to all the SVR models in the following empirical study.The source of data is Moody’s Ultimate Recovery Database (MURD) which contains more than 6000 default debt instruments including bonds, loans and revolvers issued by more than 1700 American companies. Here the focus is on corporate bonds that are categorized into five types: senior secured, senior unsecured, senior subordinated, subordinated and junior subordinated. The sample has 1413 observations that range from 1985 to 2012. We follow Qi and Zhao (2011) to adopt the preferred method recommended by Moody’s analysts as the ultimate RR of each instrument (Moody’s Analytics, 2012).1There are three methods provided in MURD to calculate RR for each instrument. (1) Discount_Settlement_Total: The nominal settlement recovery amount discounted back from each settlement instrument’s trading date to the last date cash paid of the individual defaulted instruments, using the defaulted instrument’s effective interest rate. (2) Discount_Liquidity_Total: The nominal liquidity recovery total discounted back from each settlement instrument’s trading date to the last date cash paid of the individual defaulted instruments, using the defaulted instrument’s effective interest rate. (3) Discount_Trading_Price: The trading price nominal recovery value discounted from the trading date to the instrument’s last date cash paid using the effective interest rate of the pre-defaulted instrument.1Table 1describes the frequency and the percentage of each seniority as well as corresponding mean values of RR. It is clear that the mean RR tends to be higher for more senior bonds. Subordinated bonds have low frequencies (especially junior subordinated bonds) and this may affect the quality of estimation. Therefore, junior subordinated, subordinated and senior subordinated bonds are merged together, and are referred to as “Subordinated bonds”. Fig. 1shows the distribution of RR for all observations. Clearly the distribution is highly skewed between 0 and 0.2, indicating that a large proportion of observations have a RR lying in this interval.According to Resti and Sironi (2007) the drivers of RR can be categorized into five classes: characteristics of the exposure, characteristics of the borrower, bank’s internal factors and macroeconomic factors. For the exposure characteristics we only select the variables with enough non-missing values. We select the accounting and macroeconomic characteristics that are commonly used in the literature are available in our dataset (Acharya et al., 2007; Khieu et al., 2012; Qi & Zhao, 2011). The accounting information of the borrowers is incorporated by using the ticker (unique identification) of each obligor to match the bond information from MURD with financial statements of the corresponding companies in Compustat.2Compustat Database is integrated into Wharton Research Data Services.2Macroeconomic variables are included to capture economic cyclical effects while bank internal factors are unavailable in this study. The summarized statistics of all variables are listed in Table 2.Collateral_rank refers to the relative importance of the collateral. A higher collateral rank of an instrument is expected to be associated with a higher RR. Percent_Above indicates the percentage of debt of an obligor that is more senior than the current instrument. It is expected to have a negative effect on RR, because a high value of Percent_Above means the current instrument has to wait for the complete recovery of the debt of senior instruments before it can be recovered. The Orginal_Amount denotes the face amount of the instrument when it was issued. There is no agreement on the effects of this variable on RR based on previous research. Dermine and Neto De Carvalho (2006) find that it negatively affects the RR, but Acharya et al. (2007) suggest that larger loan volume means the obligor has greater bargaining power in the bankruptcy proceedings that will result in a higher RR.For the accounting variables, we consider both profitability and solvency characteristics of the obligors (Acharya et al., 2007). It is expected that with higher profitability and solvency, the obligor should have higher RR of its corresponding instruments. EBITDA is used to represent the profitability, and solvency is described by the remaining accounting variables. Notice that Leverage and Debt_Ratio are expected to have negative effects on RR because higher values indicate weak solvency of the company. All the accounting variables are lagged one year before the instrument default date.Macroeconomic variables are considered to reflect the economic cycle over the period covered in this dataset. Because all the obligors are US companies, we only select macroeconomic variables closely related with the US economy. GDP and Unemployment_Rate are used as coincident indicators to explain the overall economic condition. S&P_Return is the Standard and Poor’s 500 index annual return denoting the market performance for each year. The three months Treasury bill rate T_Bill is taken as a risk-free interest rate. All the macroeconomic variables are lagged one year before default date. In summary, the regression model can be presented as followsRecovery Ratei,t=Intercept+Recovery Characteristicsi+Accounting Variablesi,t-1+Macroeconomic Variablest-1where subscript i denotes the ith instrument and t is the related default year.The empirical experiment in this paper is presented in two subsections: firstly for all seniorities pooled together and secondly, models for individual seniority are presented separately.The aggregated sample is split into training and testing sets, with a stratified sampling method in order to keep the same proportions of different bond seniorities in both the training and test sets. For each stratum seventy percent of observations are randomly drawn as a training set and the remaining thirty percent of observations are left as a testing set. The procedure is repeated 100 times as cross-validation (with new random samples drawn each time) to ensure the robustness of the results. The regularized and the kernel parameters of SVR models are selected based on the principle of design of experiment proposed by Staelin (2003) and the out-of-sample prediction results on the testing sets are reported. To compare the performance of these models, pair-wise t-tests are used to examine the differences in the mean values of different models and t values with corresponding significance levels are presented.The models presented in Section 2 have been fitted to the aggregated training samples. For parametric regression techniques these include linear regression, fractional response regression where the logistic link function is adopted, linear regression with a beta transformation and a two-stage regression model. For the two-stage model, the observations with RR=0 and RR>0 are first classified by logistic regression, and then the cases with RR>0 are further separated such that RR=1 and 0<RR<1, and finally an OLS regression is applied to values of RR in the interval (0,1). SVR techniques include least squares support vector regression (LS_SVR, Eq. (5)), least squares support vector regression with different intercepts (LS_SVR_DI, Eq. (9)) and semi-parametric least squares support vector regression (Semi_LS_SVR, Eq. (14)). Two different transformation methods are applied to all these three SVR models. The abbreviation names are used in the following description for convenience. For example, Beta_LS_SVR denotes the least squares support vector regression with a beta transformation on RR, and Log_Semi_LS_SVR means semi-parametric least squares support vector regression with a logistic transformation.RR varies with respect to different bond seniorities, and to control for this and to check if segmentation affects the predictive performance of models, the aggregated dataset has been split into three subsets, as described in Section 3. Since the models are fitted to each subset separately, LS_SVR_DI and Semi_LS_SVR including related models with transformed RR are not evaluated here. All the parametric models and LS_SVR models with both original and transformed RR are applied to instrument segments. The same procedure of cross-validation is followed as described in the previous section. Similarly, the pair wise t-tests are also employed.In this experiment variables including Total Assets, Original Amount of the instrument and GDP are subjected to a log transformation to scale the variable into an appropriate range. The outliers of each numeric variable defined as either larger than 99th or smaller than 1st percentile are replaced by the corresponding median value. Two different performance measurements are selected including root mean squared errors (RMSE), mean absolute errors (MAE) and R square (R2) defined as follows(22)RMSE=1n∑i(ri-rˆi)2MAE=1n∑i|ri-rˆi|R2=1-∑i(ri-rˆi)2∑i(ri-r¯)2,r¯=1N∑iri.All support vector models adopt the RBF kernels defined asK(xi,xj)=exp-‖xi-xj‖22σ2.where σ is the scale parameter of the kernel and is tuned in cross-validation.In this paper all models are implemented in SAS 9.2 (SAS Inc., 2009), where linear regression and fractional response models are fitted in SAS PROC REG and NLMIXED respectively, and all SVR models are programmed in SAS PROC IML.

@&#CONCLUSIONS@&#
As far as we know there is no paper that compares the predictive performances of SVR methods to predict the RR of defaulted corporate instruments. The aims of this research were first to investigate whether SVR methods give more accurate predictions of RR for such instruments than other methods in the literature and, second, to devise novel SVR methods that are able to explain the unobservable heterogeneity of bond seniorities which would allow a financial institution to predict RR for these instruments more accurately than other currently available techniques. We have proposed two SVR models; one that specifies different intercepts for the seniorities of the instruments and a second includes dummy variables as a semi-parametric SVR.By comparing the predictive accuracy of these two models with available techniques using a large sample of defaulted instruments that are observed between 1985 and 2012 we draw the following conclusions. First, when treating all of the instruments in aggregate, both SVR techniques allow more accurate predictions of RR to be made than linear regression, fractional response regression or a two-stage method that is commonly used in practice. Second, if we consider instruments segmented into seniority classes and model the RR within each class separately, SVR gives more accurate predictions than the other techniques for more senior categories of bonds and LS_SVR and fractional response models give predictions with similar accuracy at lower levels of seniority. Third, by incorporating unobservable heterogeneity the improved SVR methods parameterized on an aggregate sample, surprisingly, gives more accurate predictions than one parameterized on sub-samples. Fourth, transformations of the RR do not improve the predictive accuracy of SVR models and may well make things worse. Fifth, although published work has used different datasets, over different time periods and for different credit segments compared with our work, the proposed SVR methods we present appear to give more accurate predictions than those quoted by other papers.A limitation of using SVR techniques to predict RR is that they have the characteristics of a ‘black box’ in that the role of each variable is difficult to discern. Nevertheless in the context of predicting RR this is less important than predictive accuracy.As we explained earlier LGD is an important component of the regulatory capital formula in the Basel Accords. By adopting a more accurate method to predict LGD than the method currently used, a bank can more accurately compute the regulatory capital that is required and so gain a more accurate estimate of the amount of Tier 1 capital that it needs to hold so as to fulfil the requirements of its national regulator.