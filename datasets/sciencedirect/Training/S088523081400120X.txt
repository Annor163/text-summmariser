@&#MAIN-TITLE@&#
Native and non-native class discrimination using speech rhythm- and auditory-based cues

@&#HIGHLIGHTS@&#
New models of speech rhythm and auditory knowledge are proposed.Use both duration and intensity metrics to classify native and non-native accents.Perform accent classification by logistic regression (LR) and with baseline systems.LR-based approach provides the best classification of native/non-native Arabic speech.Combination of auditoryc indicative features and rhythm metrics provides the best classification.

@&#KEYPHRASES@&#
Speech rhythm,Auditory cues,Duration,Intensity,SVMs,GMMs,Hybrid GMM/SVM,Logistic regression,

@&#ABSTRACT@&#
In recent years, the use of rhythm-based features in speech processing systems has received growing interest. This approach uses a wide array of rhythm metrics that have been developed to capture speech timing differences between and within languages. However, the reliability of rhythm metrics is being increasingly called into question. In this paper, we propose two modifications to this approach. First, we describe a model that is based on auditory cues that simulate the external, middle and inner parts of the ear. We evaluate this model by performing experiments to discriminate between native and non-native Arabic speech. Data are from the West Point Arabic Speech Corpus; testing is done on standard classifiers based on Gaussian Mixture Models (GMMs), Support Vector Machines (SVMs) and a hybrid GMM/SVM. Results show that the auditory-based model consistently outperforms a traditional rhythm-metric approach that includes both duration- and intensity-based metrics. Second, we propose a framework that combines the rhythm metrics and the auditory-based cues in the context of a Logistic Regression (LR) method that can optimize feature combination. Further results show that the proposed LR-based method improves performance over the standard classifiers in the discrimination between the native and non-native Arabic speech.

@&#INTRODUCTION@&#
Recent modeling in speech processing systems has used rhythm-based features to differentiate between speech varieties. One example is an automatic language identification study by Rouas et al. (2005), who modeled vocalic and consonantal durations with a Gaussian Mixture to distinguish among stress-timed, mora-timed and syllable-timed language groups. The modeling was also able to identify the seven languages in the study with reasonable success. This rhythm-based approach uses a wide array of speech rhythm measures that have been developed to capture timing properties of different languages. Among the numerous other applications of these measures is the study of the rhythm component of second language phonology. White and Mattys (2007) found that certain metrics discriminate between native speakers of English and Spanish learners of English with more success than other metrics, specifically, %V and VarcoV vs. nPVI-V (these metrics are defined below).The present study focuses on the classification of native (L1) and non-native (L2) Arabic speech. In our previous work on Arabic (Alotaibi and Selouani, 2009), we found that there are clear auditory differences between native and non-native speech; among the most noticeable features are speaking rate and the length of vowels and consonants. Besides this, it is worth noting that regional and foreign linguistic variation of Arabic speech rhythm have been the focus of only a small number of studies. Ghazali et al. (2002) documented rhythmic variation across geographic regions; Eastern Arabic dialects (Syria and Jordan) are distinct from Western dialects (Morocco and Algeria) with Tunisian and Egyptian dialects occupying an intermediate position. Meftah et al. (2013) found that even within the same dialect there can be rhythmic differences between male and female speakers. Improvements to the robustness of automatic speech recognition systems of Arabic require improved modeling of different accents, including those of L2 speakers.In the next two subsections of this introduction, we will present the main concepts and definitions that we use in this study as well as a summary of our main contributions.In terms of rhythm, the traditional classification of languages is based on the principle of isochrony, attributed to Pike (1945) and Abercrombie (1967). Isochrony is defined as the property of speech to organize itself into portions of equal or equivalent durations. According to this perspective, languages such as English and Swedish are considered to be stress-timed, and their fundamental unit for equal-timed intervals is the foot. Syllable-timed languages, such as French and Italian, have the syllable as the fundamental isochronous unit. Based on this classification, Beckman (1992) investigated the basis of the rhythm class hypothesis; however, like other researchers, she found a lack of experimental support for isochrony in speech. Laver (1994) noted that, despite the popularity of the rhythm class hypothesis among linguists, researchers have not found support from duration measurements for isochronous timing on any absolute basis.The weak empirical evidence for isochrony led Ramus et al. (1999) and Grabe and Low (2002) to propose an approach to the description of the rhythmic structure of languages that uses acoustic-phonetic measurements. They suggested a number of speech rhythm metrics, based on the durations of vocalic and intervocalic (consonantal) segments, that can capture the timing properties of different languages. Many studies have used these rhythm metrics to investigate cross-language differences and to classify languages into syllable-timed and stress-timed categories (Arvaniti, 2009; Wiget et al., 2010). Studies of the rhythm component in second language phonology have also made use of rhythm metrics (Gut, 2012; White and Mattys, 2007). Similarly, research on language dialects (Ferragne and Pellegrino, 2008; White et al., 2009) and on speech impairments (Liss et al., 2009) have taken this approach.However, despite their popularity, researchers have noted a number of shortcomings of rhythm metrics. Arvaniti (2012) finds that rhythm metrics fail to capture consistent cross-linguistic differences because of large within-language variability that is due to effects such as elicitation methods, test materials and inter-speaker variation. Gut (2012) observes that most rhythm metrics do not yield similar results across different studies, even for the same language; this leads her to question the reliability of these measures.Other considerations cast doubt on the validity of rhythm metrics. Some researchers suggest that the notion of rhythm is not limited to the duration of segments such as vowels and consonants (Dauer, 1983; Nolan and Asu, 2009). They argue that rhythm is located at a level above the segment; that is, rhythm is seen as the result of phonological, lexical and syntactic facts (Cummins and Port, 1998). Given these considerations, Bertinetto and Bertini (2010) conclude that the rhythm-metric model is not entirely predictive. Cumming (2008) suggests the inclusion of other prosodic features such as fundamental frequency in the definition of rhythm. Ferragne and Pellegrino (2008) and He (2012) study the inclusion of intensity. Thus, a more effective quantitative model of rhythm may lie in combining rhythm metrics with other prosodic features and with levels of analysis that are above the level of the segment.For many researchers rhythm is primarily a perceptual phenomenon (Couper-Kuhlen, 1993). However, our current knowledge about the perception of speech rhythm is not well developed. Crystal (1985) defines rhythm as the perception of regular prominent units in speech. Speech rhythm is also related to an individual's perception of speech and of differences between languages, language varieties and individual speakers. Indeed, listeners are sensitive to rhythm in a variety of ways. Lorch and Meara (1989) find that native listeners can assign rhythmic signatures to languages by classifying them as pleasant, flowing, and choppy. In addition, Stockmal et al. (2005) suggest that speech rhythm is a key factor in the perception of foreign accents. Many studies demonstrate that the ability to process rhythm is important for understanding speech (Hawkins, 2003). For instance, during infancy, the identification of words and syllables is mainly achieved by rhythm through prosodic features (Nazzi et al., 1998). Mairano and Romano (2012) report that the structure of rhythm is described not only by acoustic phenomena, but also by a subjective component. They base their argument on experiments that show that listeners tend to hear and perceive groupings of a succession of identical stimuli even when they do not exist.Our approach finds its justification in many recent studies and in experimental evidence that suggest that the perception of rhythm is a combination of different acoustic properties that are not limited to duration but could include changes in other parameters such as those related to auditory prominence (Loukina et al., 2011). In fact, it is worth noting that little is known about the auditory perception of speech rhythm. An interesting study in this less-studied field (Furl et al., 2011) has shown that higher-order statistics modulate the neural responses to sound sequences, leading to the hypothesis that these modulations are associated with higher-order statistical learning that may contribute to the perception of natural sounds with hierarchical structures.The considerations about the limitations of rhythm metrics and about the perceptual basis of rhythm provided the motivation for the present study. Specifically, they have led us to explore the possibility of adding auditory information to a rhythm-based approach to speech processing. While we are not in a position to study or to model the perceptual basis of the prominence that is often referred to as rhythm, we are nevertheless able to propose a model that captures acoustic phonetic information found in the speech signal that passes through the ear. The review of research on rhythm metrics has also led us to consider adding another prosodic feature, intensity, to the set of features that can serve to provide a better definition of rhythm.In this paper, we make three main proposals that contribute to the study of the classification of speech varieties. These three contributions are as follows:(i)a computational model that aims at extracting auditory-based cues from the speech wave signal. The interest of this model is that it recognizes that perception mechanisms have a place in the modeling of the rhythmic structure of speech utterances. As well, the model takes into account the domain of inter-pausal intervals; that is, it goes beyond the level of the segment.a feature selection and classification scheme that uses both intensity- and duration-based rhythm metrics and auditory-based cues. We argue that this scheme will allow for an optimal combination of the rhythm and auditory features.an experimental approach that evaluates how efficiently different classification systems can discriminate between groups such as native and non-native speakers of Arabic.This paper is organized as follows. Section 2 describes intensity- and duration-based rhythm metrics. Section 3 introduces the ear-based model used to extract auditory perceptual information. Section 4 describes the Gaussian mixture model (GMM), support vector machine (SVM) and the GMM/SVM classifiers that can be used to perform a classification of language varieties. Section 5 presents the logistic regression (LR) based classification and feature selection scheme that allows for an optimal combination of the auditory and rhythm cues. In Section 6, we describe our setup of the L1/L2 Arabic discrimination experiments, and we report and discuss the results obtained by the systems presented in Sections 4 and 5. Finally, we summarize our major findings in Section 7.Over the past decades, durational measurements have yielded extensive knowledge about the characteristics of the rhythmic structure of languages. Ramus and colleagues suggested measures based on the percentage of vocalic interval durations (%V) and on the standard deviation of consonantal interval durations (Ramus et al., 1999). Grabe and Low (2002) proposed the raw (r) and normalized (n) pairwise variability indices (nPVI and rPVI, respectively) that are calculated from the differences in vocalic and consonantal durations between successive syllables.Recently, numerous approaches based on segmental duration measures have been proposed to capture many typologies of speech and language related to rhythm (for example, Arvaniti, 2009; Nolan and Asu, 2009). Wiget et al. (2010) offer an overview of widely used rhythm metrics and make recommendations about their effectiveness and reliability. In our study, we considered the durational metrics used by White and Mattys (2007) in their study on the quantification of rhythmic differences between first and second language. We also considered syllable-based measures that were first defined by Liss et al. (2009). The list of duration metrics studied here is given in Table 1.Our decision to include intensity-based metrics in our approach to class discrimination is motivated in part by the many criticisms of the durational paradigm which has dominated speech rhythm studies but which has not provided a clear picture of speech rhythmicity. We note in particular the notion that while duration is an essential cue to rhythm it is not the only aspect that might contribute to rhythm. Some psychophysical studies of speech rhythm have revealed that duration and intensity interact and co-vary, thus better reflecting rhythmicity if they are jointly considered (Hay and Diehl, 1999; Kuroda et al., 2012).Only a few studies of rhythm have included intensity measures. He (2012) introduced syllable-based intensity metrics to differentiate between native speakers of English and Mandarin L2 speakers of English. Fuchs (2014) measured differences in vowel loudness between speakers of Indian English, a postcolonial variety, and native speakers of British English. Ferragne and Pellegrino (2008) found that using PVI measures of consonant and vowel intensities enhances the separation of 13 regional dialects of British English. Although each study presents different ways of measuring intensity, it is clear that this feature adds significant information towards distinguishing between varieties. In the present study, we have tried to generalize the use of intensity metrics by calculating them in a manner that is similar to duration-based metrics. Descriptions of the intensity metrics that we use are given in Table 2.Throughout the history of speech processing, intensive efforts have been made to capture the most relevant information from speech phenomena that can be described as versatile, uncertain and imprecise. While humans can easily deal with unpredictable acoustical environments and adverse conditions to understand speech, conventional acoustic feature extraction methods remain largely ineffective and fail to provide satisfactory robustness. The capabilities of humans to recognize speech accents, speakers’ emotions and other paralinguistic aspects have always fascinated researchers and led them to investigate the human auditory system more deeply. Indeed, the auditory system plays a vital role in the treatment of acoustic information that is sent to the brain. The ability of the human auditory system to effectively process and interpret speech, even in the worst conditions, makes auditory modeling a very attractive and successful approach among a wide range of speech-enabled applications such as speech recognition (Stern, 2011).Generally speaking, feature extraction based on auditory modeling aims at analyzing the response of the basilar membrane and the auditory nerve to sounds. An advanced processing which simulates the auditory cortex can also be performed. Flanagan proposed a computational model for estimating basilar membrane displacement. This model has been found useful to relate the subjective auditory behavior and the acoustomechanical operation of the ear (Flanagan, 1960). The well-known classical models such as the Mean-Synchrony Auditory representation, the Cochlea Model, and the Ensemble Interval Histogram processing, proposed respectively by Seneff (1988), Lyon (1982) and Ghitza (1994), have been the basis of numerous subsequent approaches. These models simulate cochlear filtering by using a bandpass filter bank. The differences among these models are as follows: Seneff's model emulates the synchrony detection that occurs in the cochlear nucleus; Lyon's model is characterized by its orientation towards higher abstraction levels than the hearing process as such; Ghitza's model imitates firing patterns of auditory nerve fibers.The prodigious progress in calculation speed and complex algorithms opened promising perspectives towards feature extraction methods based on the understanding of both auditory physiology and psychoacoustics. As noted by Stern and Morgan, in recent years there has been a renewed interest in the development of front-end signal processing to extract robust features inspired by work related to auditory modeling (Stern and Morgan, 2012). The auditory front-ends become important for feature extraction. For instance, in the field of speech recognition, since the very popular Mel-Frequency Cepstral Coefficients (MFCC) (Davis and Mermelstein, 1980) and Perceptual Linear Predictive (PLP) coefficients (Hermansky, 1990) to the more recent Gammatone Features (GFCC) (Schluter et al., 2007) and Power-Normalized Cepstral Coefficients (PNCC) (Kim and Stern, 2012), auditory physiology and psychoacoustics are the main sources of inspiration of these robust feature extraction techniques.The auditory model used throughout our experiments simulated the external, middle and inner parts of the ear. The next subsections will present all parts of the auditory model which was first proposed by Caelen (1985) and adapted to be used as a front-end module in speech recognition systems by Selouani (2011). A bandpass filter was used to simulate the various adaptive ossicle motions in the external and middle ear. The basilar membrane (BM) that represents the inner part of the ear is stimulated by a nonlinear filter bank. The BM model also considers the electro-mechanical transduction of hair cells and afferent fibers from which the encoding signal is generated at the synaptic endings. Owing to variations in physiology and anatomy, different regions along the different organs involved in hearing and perception are sensitive to sounds with different spectral properties. Therefore, each region along the BM has a specific resonance frequency for a given input sound. Fig. 1shows the bloc diagram representation of the auditory model used in this study.The external and middle ear are modeled using a bandpass filter, which can be expressed as follows:(1)s′(k)=s(k)−s(k−1)+α1s′(k−1)−α2s′(k−2)where s(k) is the speech signal, s′(k) is the filtered signal, k=1, …, K is the time index and K is the number of frame samples. The coefficients α1 and α2 depend on the sampling frequency Fs, the central frequency of the filter and its Q-factor. The values of 1500Hz as a central frequency and 1.5 as Q-factor are used (Selouani et al., 2007). The output s′(k) is used to calculate the absolute energy of the signal which passes through the mid-external ear. For each frame T, the mid-external ear energy, denoted by WME, is calculated as follows:(2)WME(T)=20log∑k=1Ks′(k)WMEis the first parameter calculated by the ear model. In the following subsections, seven other auditory-based cues are calculated thanks to the basilar membrane model.The BM is simulated by 24 digital overlapping filters that represent the cochlear filter bank. The vibration of a given location of the BM is represented by the frequency response of a given filter of the bank for an auditory stimulus in the outer ear (Caelen, 1985). The output of each filter is given by:(3)yi(k)=β1,iyi(k−1)−β2,iyi(k−2)+Gi[s′(k)−s′(k−2)]and its transfer function can be written as:(4)Hi(z)=Gi[1−z−2]1−β1,iz−1+β2,iz−2where yi(k) represents the vibration amplitude at position xiof the BM and constitutes the BM response to a mid-external sound signal s′(k). The Gi, β1,iand β2,iparameters represent the gain and coefficients, respectively, of the filter i. Ncis the number of overlapping cochlear filters or channels and is set to 24 in our case. The total length of the BM is 35mm. Therefore, each filter covers approximately Δx=1.46mm of the BM. For simplicity, only the coupling effect of the electro-mechanical transduction in hair cells and fibers was considered. Hence, the behavior of hair cells and fibers is expressed by the Ci, Eiand Aicoupling parameters. In Appendix A, we provide the details of the sample-by-sample algorithm, which providesyi′(k), the resulting stimulus after the passage through all components of the ear model. The energy of each channel is calculated by usingyi′(k).The absolute energy in the output of each channel was calculated as follows:(5)Wi′(T)=20log∑k=1Kyi′(k)wherei=1,2,…,NcIn Eq. 5, T refers to the frame index; i refers to the channels and Ncis the total number of channels that is 24; k denotes samples and therefore K is the frame length. A smoothing function is applied in order to reduce the energy variations:(6)Wi(T)=c0Wi(T−1)+c1Wi′(T)where Wi(T) is the smoothed energy, and c0 and c1 are coefficients for averaging Wi(T−1) andWi′(T), such that their sum is unity. In our case, the value of 0.5 was retained for both c0 and c1.Auditory distinctive features were calculated after performing linear combinations of energies of the channel outputs. In our study, seven cues were selected as discriminative features. This selection was based on studies of perceptual distinctive features conducted by Chomsky and Selouani (1968) and Jakobson et al. (1963) and on the acoustic-phonetic and physiological investigation of El-Ani (1970). Jakobson et al. (1963) showed that 12 perceptual cues acoustically characterize all languages, but all 12 are not mandatory to represent a specific language. In a previous study, we found that seven discriminative cues, derived from the ear model described above, were best suited for the Arabic language (Selouani and Caelen, 1999). These seven cues are as follows: grave/acute (G/A), open/closed (O/C), diffuse/compact (D/C), flat/sharp (F/S), mellow/strident (M/S), continuant/discontinuant (C/D), and tense/lax (T/L). The calculation of the seven cues uses the resonant frequencies of the basilar membrane provided by the model and given in Table 3. For instance, based on acoustic-phonetic knowledge, we know that the principal characteristic of strident phonemes is the presence of noise due to a turbulence at their articulation point. Hence, a phoneme is considered as strident if the frequency band ranging from 3800Hz to 5300Hz contains more energy than the band ranging from 1900Hz to 2900Hz, which leads to the quantification formula of (M/S) given in Table 4. The effectiveness of this approach has been proved experimentally by using the seven cues as input features in speech recognition systems (Tolba et al., 2002). The calculation, over each frame, of all the seven cues is given in Table 4.Figs. 2and 3provide examples of the auditory-based feature cues extracted from native and non-native speech for the following phrase: /marℏabanmaʔismuka?/ (translated as: Welcome, what is your name?), taken from the LDC West Point Corpus (LDC, 2002).The proposed auditory model performs a low-level auditory processing and then a high-level processing by providing relevant indicative features. These high-level indicative features are expected to convey measures of speech rhythm that account for differences between speakers’ accents. In our study, we investigated the higher order statistics of the indicative features obtained from the proposed auditory modeling. By applying Linear Discriminant Analysis (LDA) (McLachlan, 2004) to the second-order moment of the seven auditory cues, we found disjoint homogeneous regions with respect to native and non native speakers, as shown in Fig. 4. On the basis of this finding, classification experiments are done in order to assess the ability of auditory indicative features to discriminate between native and non-native Arabic speech.In this section, we present a feature selection and classification scheme that uses rhythm metrics and auditory-based cues. The goal is find a scheme that allows for an optimal combination of these features.GMMs and SVMs are considered to be among the most powerful techniques for pattern classification. GMM is a generative technique used to estimate the probability density function (PDF) from a set of data. SVM is a discriminative binary classifier that models the boundary between two classes. To benefit from the advantages of the two approaches, several hybrid frameworks combining SVMs and GMMs have been developed (Campbell and Karam, 2009). The GMM, SVM and hybrid GMM/SVM methods were applied to the discrimination between native and non-native speakers of Arabic. We used the LDC West Point Arabic Speech Corpus (LDC, 2002), described below. The new rhythmic and auditory sets of features are used in the front-end level of the classifiers independently.In the baseline GMM-based approach used to discriminate between native and non-native speech, the PDF of the feature vector is represented by the following general Gaussian model:(7)P(x∣c)=P(c)∑m=1MλmcN(x;μmc,Σmc),whereN(x;μmk,Σmc)is the mixture of M d-dimensional Gaussian distributions for the class c; λmcis the mixture weights;μmc∈Rd, andΣmc∈Rd×drepresent the mean vector and covariance matrix, respectively, for the mth Gaussian distribution representing the cth class; P(c) is the a priori probability of the class c. The P(x∣c) distribution can be written as:(8)N(x;μmk,Σmc)=(2π)−d/2|Σmc|−1/2e−12(x−μmc)TΣmc−1(x−μmc).The SVM is well suited for the two-class (L1/L2) discrimination. For SVM baseline classification, we trained a target model for the native accent vs. the non-native class. The conventional expression of an SVM is given by f(x) (Campbell and Karam, 2009):(9)f(x)=∑n=1NαnK(x,xi)+d,with(10)∑n=1Nαn=0,where K(., .) is a kernel function, xiis the support vector, and d is obtained by optimization and αn≠0. A straightforward method of performing accent classification with SVMs is to use nonlinear kernels that compare sequences of feature vectors. The purpose of nonlinear kernel functions is to perform a projection of the input data into a higher-dimensional space (mapping) in which linear classification can be done. Several kernel functions have been developed to be mainly used by SVM applied to speaker verification. The radial basis function (RBF) kernel has been successfully used in the cases where numerous feature frames were extracted to represent the classification object. The RBF used in our application was formulated as:(11)K(xi,xj)=e−xi−xj22σ2.The σ parameter defines the width of the RBF and is tunable during experiments.A close relationship exists between GMMs and SVM using RBF kernel function (Scholkopf and Smola, 2002). Indeed, the distance to the decision hyper-plane of a given class is proportional to the probability of this class:(12)P(x∣c)∝∑xj∈SccαnK(x,xj)+d,where Scis the solution set of the supervectors for classc;k∈−1,+1for the two-class problem. The decision rule of the conventional SVM can be rewritten by including the RBF kernel as follows:(13)x=argmaxc∑xj∈Sccαne−x−xj22σ2+d.The GMM decision rule can also be expressed as(14)x=argmaxc∑m=1MP(c)λmcN(x;μmc,Σmc).This decision rule can be rewritten as(15)x=argmaxc∑m=1MP(c)λmc2πσD/2e−x−μmc22σ2.The relationship between the SVM decision rule given by Eq. 13 and the GMM decision rule given by Eq. 15 is evident by setting:(16)αj=P(c)λmjc2πσD/2,and(17)μcj=xj.The GMM/SVMM classifier is trained separately on the two sources of the rhythm and auditory based acoustic features.The combination of different acoustic features is frequently used to improve the performance of various classification tasks related to speech. For instance, to reduce the word error rate of speech recognition systems, Zolnay et al. (2005) have shown that the log-linear combination of different feature streams leads to a good performance. The use of multiple types of features was expected to help capture complementary knowledge brought by each individual type of feature.In the framework of the present study, the auditory-based cues were calculated on the basis of the typical temporal resolution of a 10ms frame rate and 25ms window size that respects the stationary criterion. However, each of the rhythm metrics was computed with the longest sequence rate composed of several vocalic and consonantal intervals that span a whole sentence. To perform a reliable classification of native and non-native speech, we proposed to combine the sets of features obtained independently for the two acoustic streams that evolved on the basis of frame rate and sentence rate. To optimize the feature combination, we proposed to use the logistic regression (LR) method, which is considered to be a useful alternative to GMMs and SVMs in the context of speaker identification (Katz et al., 2006; Landwehr et al., 2005). The principle of LR is to model the posterior probability of class membership using a linear function (Katz et al., 2006):(18)f(x)=βTx,where β is the weight vector and x is the sample vector. Eq. 18 was rearranged to estimate the P(x, β) probability. By using the logit transfer function, we obtained the following(19)logit(P(x,β))=logP(x,β)1−P(x,β)=βTx,and thus, led to(20)P(x,β)=11+e−f(x).The estimate of the posterior probability was thus made under the premise that the training data were derived from a Bernoulli distribution. This assumption was well justified in our case because we processed a binary (native/non-native) classification problem. Hence, the posterior probability can be written as(21)P(x,β∣c)=11+e−f(x)c=+11−11+e−f(x)c=−1.According to the Bernoulli distribution, Eq. 21 was thus formulated as:(22)P(x,β∣c)=P(x,β)c(1−P(x,β))1−c.The negative log-likelihood of Eq. 22, denoted by ll[β], was formulated as follows:(23)ll[β]=∑i=1N−ciβTxi+log(1+eβTxi).In most implementations, a penalty term was added to ll[β] to avoid overlearning. Thus, Eq. 23 became:(24)ll[β]p=ll[β]+θ2β2,where θ is called the regularization parameter. The optimization aims at finding the best set of weights βiby minimizing ll[β]p, and was done by setting the derivative of ll[β]pto zero. The re-weighted least square iterative algorithm, referred to as the IRLS algorithm (Wolke and Schwetlick, 1988), was used to find the optimal weights,βopt, that are expressed as follows:(25)βopt=(XTWX+θI)−1XTWz,where z is expressed by:(26)z=Xβold+W−1(c−p),where I is the identity matrix; p is the vector composed of the fitted probability elements where the jth element is P(βold, xj);W is the NxN matrix with the probabilities P(βold), xj)(1−P(βold, xj)) on the diagonal.The LR technique is used to perform classification from the combination of the rhythm- and auditory-based acoustic features.It is important to note that LR has the advantage of performing feature selection in addition to carrying out classification (Katz et al., 2006). Thus, in addition to classification as carried out by using Eq. 24, relevant and best discriminative features were also identified according to Eqs. 25 and 26, which provided the optimal weights,βopt, with respect to each input vector component.

@&#CONCLUSIONS@&#
