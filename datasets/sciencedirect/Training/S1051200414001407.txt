@&#MAIN-TITLE@&#
Fuzzy-based discriminative feature representation for children's speech recognition

@&#HIGHLIGHTS@&#
A novel fuzzy-based discriminative feature selection was proposed.The proposed method enhanced discriminative ability of the cepstral features.HMM and MLP classifiers were adopted based on parameter selection.New arrangement of static, delta and delta–delta cepstral coefficient was obtained.The features outperformed the conventional MFCCs in children's speech recognition.

@&#KEYPHRASES@&#
Children's speech recognition,Mel-frequency cepstral coefficient,Multi-layer perceptron,Hidden Markov model,Fuzzy-based feature selection,

@&#ABSTRACT@&#
Automatic recognition of the speech of children is a challenging topic in computer-based speech recognition systems. Conventional feature extraction method namely Mel-frequency cepstral coefficient (MFCC) is not efficient for children's speech recognition. This paper proposes a novel fuzzy-based discriminative feature representation to address the recognition of Malay vowels uttered by children. Considering the age-dependent variational acoustical speech parameters, performance of the automatic speech recognition (ASR) systems degrades in recognition of children's speech. To solve this problem, this study addresses representation of relevant and discriminative features for children's speech recognition. The addressed methods include extraction of MFCC with narrower filter bank followed by a fuzzy-based feature selection method. The proposed feature selection provides relevant, discriminative, and complementary features. For this purpose, conflicting objective functions for measuring the goodness of the features have to be fulfilled. To this end, fuzzy formulation of the problem and fuzzy aggregation of the objectives are used to address uncertainties involved with the problem.The proposed method can diminish the dimensionality without compromising the speech recognition rate. To assess the capability of the proposed method, the study analyzed six Malay vowels from the recording of 360 children, ages 7 to 12. Upon extracting the features, two well-known classification methods, namely, MLP and HMM, were employed for the speech recognition task. Optimal parameter adjustment was performed for each classifier to adapt them for the experiments. The experiments were conducted based on a speaker-independent manner. The proposed method performed better than the conventional MFCC and a number of conventional feature selection methods in the children speech recognition task. The fuzzy-based feature selection allowed the flexible selection of the MFCCs with the best discriminative ability to enhance the difference between the vowel classes.

@&#INTRODUCTION@&#
Industries and regular people are adopting the application of automatic speech recognition (ASR), such as voice-controlled systems for mobility-impaired people, voice dialing, simplified systems based on speech communication, content-based spoken audio search, speech therapy [1], and others. Despite the versatility of the English language, efforts on developing ASR systems are not limited to English. In several languages, such as Chinese [2,3], Japanese [4,5], Thai [6,7], Portuguese [8], and Arabic [9], research in this area continues to develop efficient ASR systems. While most speech recognition systems focus more on adults than children, the speech recognition of children has numerous applications, such as in educational games for children [10–12] aids for pronunciation [13,14], and in reading based on interactive platforms [10,15]. The speech sounds of children have higher spectral variations and more dynamic characteristics than that of adults due to the extension of the vocal tract size in growing children [16,17]. The performance of ASR systems adapted by adult speech degrades under the context of children speech. To deal with this effect in speech recognition, some approaches used a larger number of samples [18]. Variation of acoustic parameters is larger in children compared to that of adults [19,20], and thus, a frequency warping algorithm is used to reduce the variation of acoustic parameters [21]. In this study, a discriminative feature extraction method is proposed to deal with recognition of children's speech.Conventionally, the extraction of helpful information about speech signals utilizes various signal-processing techniques to investigate relevant signal characteristics such as energy and spectrum. Mel-frequency cepstral coefficients (MFCCs) and perceptual linear prediction (PLP) parameters are the commonly used feature extraction methods. However, the MFCC does not provide an optimal feature space for the purpose of discrimination. Optimization of speech features such as MFCC is studied in some approaches. Occasionally, the extraction and the optimization of the features are simultaneously accomplished [22–25,29]. The reason for developing such structures is that the degradation of ASR systems can be the result of a mismatch between the acoustic conditions of training and application environments including additive noise, channel distortion, different speaker characteristics, and so on. To develop speaker independent ASR algorithms, speaker normalization is accomplished to produce a pitch-independent representation of speech [26–28]. In this way, several speaker normalization approaches utilize the formant-ratio theory. According to this theory, the quality of the spoken vowel depends on the log frequency intervals between formants (defined as a ratio). Consequently, invariant representation of vowels can be realized by shifting activations along a log frequency axis [29–35]. Computing MFCC involves a somewhat spectral smoothing carried out by triangular filter banks on the spectrum of the speeches. Consequently, filters with higher bandwidth result in higher spectral-smoothing and fewer filters as well as fewer coefficients (MFCCs). This effect is responsible for the degradation of the ASR approaches in the context of children's speech. To cope with this effect we employed narrower filters in obtaining MFCCs thus loss of spectral information can be prevented. However, this method results in many redundant and non-informative features.In various intricate application tasks, such as phoneme recognition where the systems are developed based on real data, processing a large number of features is frequent. However, many of the features are not relevant to the problem of interest. In addition, numerous features demand a significant amount of computations, which slow down the overall process. Under these circumstances, automatically dismissing irrelevant features is necessary to achieve a model that is accurate and reliable in solving the problem at hand [36,37]. Additionally, using the feature selection technique reduces the computational cost, which enhances the response time of the process. Feature selection (FS) is one of the most productive research areas and has attracted a considerable amount of attention over the past three decades. Proposed FS methods in the literature vary in terms of evaluation criteria for the selected subsets. Frequent criteria recommended in the literature include relevance [38], gain entropy [39], and contingency table analysis [40]. These methods offer no intrinsic order of features. For the FS task, two well-known methods for feature evaluation are used. The first one uses distance metrics to measure the overlap between different classes [41]. Under this strategy, probability density functions of the sample distribution can also be considered. Consequently, the subset for which the average overlap is minimal is considered as a solution. Meanwhile, intra- as well as inter-class distances can be measured by considering the fuzziness and entropy of the features [42]. Through the second method, classification errors based on the feature subset candidates are evaluated. Consequently, the subset with minimal misclassification is selected as a solution. However, given that2nfeature subsets can be generated by n features, we have to conduct a number of computations to obtain the optimal subset. Consequently, numerous related techniques have been proposed in the literature. Several methods in [43] have been compared and a Genetic algorithm (GA) method has been implemented for the variable selection. As recommended in this study, GA is a good choice for large-scale optimization in which the number of variables exceeds 50. In some feature selection approaches, particle swarm optimization [44,45] and ant colony optimization [46–48] have been employed. In different areas of researches multi-objective optimization based on GA have been performed [49] and dealing with conflicting objectives has been studied. Toward the problem of feature subset selection, multiple objectives are defined in several cases. Consequently, compromising between these objectives is necessary to solve the problem. Meanwhile, using flexibilities to define the optimization problem is helpful. For this purpose, fuzzy set theory is used to codify the flexibilities for the objective functions. This technique leads to achieving extra trade-off to solve this problem. Fuzzy optimization techniques have been frequently used in the optimization of conflicting objectives [48,50–52]. In most cases, having a priori knowledge about the priority of the aggregating objectives ensures incorporation of these techniques in an accumulative structure. Thus, combining the multiple objectives in such approaches is unnecessary. An effective idea for weighing the importance of conflicting objectives is using fuzzy decision making. Through fuzzy logic, uncertainties associated with different objective functions can be effectively formulated to combine using fuzzy aggregation functions [53]. In this study, specific fuzzy codification is proposed based on the data structure in the feature space, which considers the statistical dependence of the selecting features. Therefore, complementary discriminative features can be properly selected to enhance the performance of the proposed children's speech recognition method.This paper is organized as follows. Section 2 provides a brief explanation of the MFCC as the speech feature extraction method. Additionally, a proposed fuzzy-based feature selection method is presented in Section 2. MLP-based speech recognizer as well as HMM-based speech recognizer are explained in Section 3. Experiments are presented in Section 4 and results are discussed in Section 5, while the conclusion is discussed in Section 6.Providing a rational representation of the speech information, known as speech feature extraction, is the first step to accomplish any recognition procedure. One of the most popular acoustic feature extraction methods widely used in various ASR systems is the MFCC. Although its name seems incomprehensible, it conveys its nonlinear characteristic based on Mel distances. Mel distances or Mel scales are obtained from the resolution of human hearing. The Mel scales have a larger spread from higher frequencies to lower frequencies. In other words, its frequency scale has linear frequency spacing below 1 kHz and logarithmic spacing for more frequencies. Therefore, people can discriminate bass sounds better than treble, and MFCCs can be obtained by a Fourier transformation of the log–log warped frequency spectrum.The approximate formula to obtain the Mels for a given frequency f in Hz is given as follows:(1)Fmel=2595.log10(1+(f100))To realize the MFCC in the given DFT of the input signal in Eq. (2), the Mel-frequency filterbank is defined with p triangular filtersmj(j=1,2,…,p), as shown in Fig. 1.(2)Xa[k]=∑n=0N−1[n]e−j2πk/N,0≤k≤NThe filter bank is applied to the FFT by multiplying each FFT magnitude coefficient to its corresponding filter gain from the Mel filter bank followed by a summation of the result. This operation is performed by using Eq. (3):(3)mj=∑k=0N−1|Xa[k]|2Hj[k],0≤j≤pwhereHj[k]denotes the transfer function of filter j. The Mel-frequency cepstrum as described by Eq. (4) represents the discrete cosine transform of the p filter outputs.(4)MFCCi=2N∑j=1pmjcos(πiN(j−0.5))In standard MFCC, frequently proposed in literature, the number of Mel filters is 13. As a result, 13 cepstral coefficients are produced. Afterward, 13 deltas and 13 delta–delta coefficient are computed from the cepstral coefficients to provide 39 MFCC features. In the present study, to allow higher flexibility to select relevant features, 40 filters are used. Subsequently, relevant features are selected based on the proposed feature selection method for children's speech recognition.We suppose that one is assigned a labeled data set X, which consists of n number of labeled patternsxk∈Rp, where p denotes the number of features. If we denotexkmas the value of the mth feature inFmof patternXk, then we can represent each patternxkof set X by a vectorXk=[xk1,xk2,...,xkp].Where all features are assumed to be represented by set F, we obtain(5)F={f1,f2,...,fp}On this occasion, classification problem is defined as mapping of theXkto the class space C characterized by a set of possible class labels expressed as follows:(6)C=[c1,c2,...,cw]Dealing with the problem involves developing a hypothesis, known as a classifier, to predict the labels of new instance data. Meanwhile, developing such a hypothesis requires a machine learning process in which the feature value of the instances is employed. An essential concern in feature selection is describing the relevance of the features to the problem of interest. The feature selection method attributes the relevance of a given subset to the processing task. Despite the belief that a higher number of features provides more discriminating power to the classification, in practice, as a limited amount of training data is accessible, more features slow down the process and classifiers are prone to overfitting [54–56]. Therefore, using irrelevant features degrades the learning performance. One of the key aspects of the feature selection research area is evaluating the goodness of the features. Consequently, various types of measuring criteria have been recommended in the literature [57] and are categorized as follows:Filter method: This technique evaluates the quality of the features through feature subset measurement [58]. Feature selection is accomplished as an independent process to the selected predictor.Wrapper method: This method evaluates feature subsets using the error rate of the classifiers [59]. Wrapper method does not feature what is happening inside the classifiers but only scores features based on the classification results.Embedded method: This method includes the learning process and selects variables based on the employed modeling method [60]. This method can also achieve more functionality for a couple of objective functions including the addition of a number of variables and optimization of the fitness quality.Hybrid method: An emerging method in feature selection [61,62], this method is a mixture of the wrapper and filter methods where the optimization procedure used in the wrapper method is guided by the scoring of feature subsets obtained from the filter method.In fact, for classifiers with large computational costs, using the wrapper method is a time-consuming strategy. This condition is worse when the classifier is applied to large databases. In contrast, a filter method that uses a discriminant measurement for feature subsets can face a feature selection problem without addressing the classification issue. However, because the filter method itself cannot predict the classification error in a real classification problem, this study uses the hybrid strategy with the help of the wrapper method. First, a filter method is used to sort the features based on their power of prediction as well as linear independence of the features. In the proposed hierarchical feature-sorting algorithm, a fuzzy criterion is used to measure the predictive power of the features. The criterion is analogous to Fisher's discriminant criterion but is associated with a fuzzy codification that considers the independent measurement of the features. In the next step, the classifiers are adapted to the sorted features to produce the optimal feature subsets that still have higher discrimination ability for the classification. In this step, to prevent overfitting while the classifier is applied to the data, cross-validation method is used. Note that the samples used for the feature selection process are randomly selected from the training set and an equal number of samples from each class should be used.We suppose thatxi,nmis the mth feature of the nth sample from class i. Inter-class distance between two different classes of i and j based on the feature m is expressed as follows:(7)di,j,m=12cn∑l=12cn(Ul−U¯)2(8)U¯=12cn∑l=12cnUl(9)U=[xi,1m,xi,2m,...,xi,cnm,xj,1m,xj,2m,...,xj,cnm]wherecnis the number of samples from class i or j.To evaluate the discriminability of the classes based on a feature in addition to the intra-class distance, we have to consider the inter-class distance. Consequently, the discriminative criterion definable for a specific class, i, based on the feature m, can be expressed as follows:(10)Di,m=∑j(di,j,mλm+dj,j,m)whereλmis a tuning coefficient to adjust the relative effect of the inter- and intra-class distance on the criterion. Here,λmis equal to 0.25 of the average of the intra-class distances (λm=0.25N∑j=1Ndj,j,m).λmguarantees limited effect of the intra-class distance on D. As Eq. (10) indicates, a higher value of D represents a higher discriminability of class i against the other classes based on the feature m. In reality, some features cannot provide similar discrimination scoring for all of the classes. Consequently, scoring a feature should address uncertainties about the goodness of the feature. To generalize the criterion for all of the classes and, at the same time, codify the uncertainties in this study, we use fuzzy aggregation operation. Thus, fuzzy memberships are employed to fuzzify the criteria and aggregate them using fuzzy aggregation operations.To introduce fuzzy goals into the feature selection problem, mapping functions, namely, fuzzy memberships are used. LetCrkwithk=1,...,Mbe a fuzzy criterion introducing an objective function, defined by a membership functionμCrk, which maps the range of criteria into the interval between zero and one. The objectives are determined based on the corresponding optimization criteria.Different fuzzy membership functions, namely, triangular, Gaussian, and trapezoidal, are commonly used in the literature to map the crisp data into the fuzzy space. An example of such membership functions has been shown in Fig. 2. In a trapezoidal membership function,x1andx2, and in a Gaussian membership, σ determine the dropping rate of the membership to zero based on the difference of the fuzzifying value from the central value of the membership. In a triangular membership function,x1determines this effect. Fig. 2(d) presents a comparison of the membership functions used for this study. For the experiments, the valuesx2in trapezoidal function,x1in triangular function and σ in Gaussian function are set in a manner that corresponds to a high amount of criteria for the feature with the highest discriminative criteria for all of the classes. The membership value for these criteria is 0.6. This relation can be expressed as follow:(11)μi,m(a)=0.6,a=argmaxi(Di,m)The amounts ofx1,x2, and σ are sensitive to the number of samples used in the test set. The reason to use different shapes of membership functions is to study the effect of their shape on the result. Note that three experiments were conducted and in each, only one of the mentioned membership functions was employed.The fuzzy criteria must be aggregated to be used in the feature selection problem. To achieve the optimal solution of a problem, the constraints and objectives must be fulfilled at the same time to a maximum degree [51]. For this purpose, an optimal feature subset is definable by aggregating the fuzzy memberships,μCrk, to produceμsas follows:(12)μs=μCr1∘μCr2∘...∘μCrkThrough the aggregation operations, numerous fuzzy sets can be combined whereby each operator has its own properties that can be useful depending on the in-hand problem [63]. This operation is conducted to generate a single representation of the fuzzy and crisp dataset. Czogała and Zimmermann [64] have used a number of operations mostly based on triangular norms including t- and s-norms to conduct decision making based on fuzzy aggregation operations. A widespread survey has been presented by Dubois and Prade [65], which focused on the aggregation operations used in fuzzy sets for decision making and information processing. Aggregation of different criteria using ordered weighted average (OWA) operator has been introduced by Yager [66]. This criteria showed cumulative properties of fuzzy “or” as well as fuzzy “and” operators, behaving somehow between the two operators. The authors in [67] employed a neural network structure to conduct an information fusion for computer vision system. Their method was based on the fuzzy-set theory. Several properties of the aggregation scores including self-identity of fuzzy sets were investigated by Yager and Rybalov [68]. The authors of [69] proposed a method to aggregate the decisions using generalized mixture operators. The performance of their method was examined by comparing OWA and weighted average (WA) methods. Generalizing the study conducted by Pasi, in [70] the author used fuzzy sets to represent individual preferences through values placed between 0 and 1. Subsequently, the authors computed the collective preferences using OWA. In a group decision-making context, Pasi [71] investigated the problem of considering the individual opinion in decision making. The decline of the individual value into a representative value, known as majority opinion, is regularly accomplished via the aggregation procedure. They described this concept based on the fuzzy set theory using a linguistic quantifier (such as most), which is formally defined as a fuzzy subset. A generalization of certain well-known aggregation operators were presented by Vaníček et al. [72], who discussed certain conditions for which an operator can be considered as an aggregation operator.In this study, “fuzzy-and” operator proposed by Werners [73] is used to aggregate the fuzzy memberships. The fuzzy set decision is defined by the membership function(13)μD(x)=γmini=1Nμi(x)+(1−γ)1N∑i=1Nμi(x)whereμi(x)denotes the membership function of the fuzzy sets added using the fuzzy-and operator, γ is the compensation degree, and i is the ith fuzzy membership. Forγ=1, the fuzzy-and operator behaves as min-operator, and behavior ofγ=0is similar to the arithmetic average of the fuzzy memberships. Fig. 3presents the “fuzzy and” operator based on different value of compensation degree γ and aggregating memberships.Plugging Eq. (10) into (13), we obtain the fuzzy discriminative criteria that present the discrimination power of the features.(14)μm(d)=γmini=1Nμi,m(d)+(1−γ)1N∑i=1Nμi,m(d)whereμmis the criterion that we will use to sort the features from the highest to the lowest level of importance,μi,mdenotes the fuzzified version ofDi,m, and N is the number of classes. For the experiments, the aggregation parameter, γ was set to 0.35.In fuzzy feature selection approaches, dependence of the features is not embedded in the equations. In the sorting framework for feature selection, we consider the statistical independence of the features in addition to their discriminatory power in determining the feature arrangement. Independent features with discriminative power contain valuable discriminative information for classification. In this study, a measurement of linear dependence between the features is considered to resort them. For this purpose, Pearson's distance is used [74]. Having n samples of two variablesxiandyi, the Pearson distance for these variables is expressed as(15)Dp(x,y)=1−rx,y(16)rxy=n∑xiyj−∑xi∑yj(n−1)∑xi2−(∑xi)2(n−1)∑yi2−(∑yi)2whererx,yandDpare the correlation and the Pearson distance of the variables, respectively.Eq. (15) indicates that for a given couple of features, lower linear dependence results in higher Pearson distance. Note that the value of the correlation as well as the Pearson distance is between zero and one. Considering the uncertainty of the linear independence of the features for selection, we should fuzzify the distance to be included in feature sorting through fuzzy aggregation. A triangular membership function is used to fuzzify the distance. The value of the parameterx1in this membership function is set in a manner that the highest distance between the feature with highest discriminative power and other features corresponds to 0.45 in the membership scale. Note that lower Pearson distances correspond to lower membership values.To map the fuzzy aggregation to the crisp data, numerous methods have been proposed in the literature. These methods vary in terms of required computational cost and precision. The most frequently used defuzzification methods are center of gravity and mean of maxima [75]. This study uses the mean of maxima method. To explain this method, we use the notations in [76] and [75].Let A be a mapping from X (an ordinary nonvoid set) into the interval[0,1]. The valueA(x)of A inx∈Xindicates the degree of membership of x in A. Eq. (17) provides defuzzification of A.(17)MeOM(A)=∑x∈core(A)x|core(A)|where the core of A is the set of elements with the largest degree of membership in A.(18)core(A)={x|x∈Xand¬(∃y∈X)(A(y)>A(x))}In fact, this method estimates the mean of all elements of the core of a fuzzy set.As a primitive step for the proposed feature selection method, the feature arrangement from higher to lower importance is determined based on the previously discussed criteria. The feature sorting algorithm is enumerated as follows:1.Make a list of features sorted based on their fuzzy discriminative power from the highest to the lowest and select the feature with the highest discriminative power as the first feature in the sorting arrangement.Consider the first P consecutive features in the feature list, provided in step 1, and compute the distance (Dp) of the P features from the feature selected in the previous step.Compute the membership of each distance.Compute the fuzzy aggregation of the memberships including the discriminative and linear independence criteria (Dp) using the “fuzzy-and” operator as follows:(19)μa=γmini=12μm+(1−γ)12∑i=12μDpDefuzzify the fuzzy scores to obtain the crisp scores using Eq. (17) and select the feature with the highest score. Remove the selected feature from the list provided in step 1.Repeat steps 2 to 5 for all of the features in the feature list provided in step 1.The measure of linear independence formulated in the sorting algorithm aims to enhance the complementarity between the consecutive features. Consequently, features provided in this method are complementary and discriminative. Two parameters that adjust these conflicting effects at the same time are provided. First is the number of consecutive features, P, used in step 2, and second is the fuzzy aggregation parameter, γ, used in step 4. Value of the parameters γ and P in this study were 0.25 and 6, respectively. The value (0.25) results in higher effect of discriminative criteria on the sorting results.The final step of the proposed feature selection method includes evaluating the number of features in the feature subset for which optimal classification accuracy is achieved. For this purpose, the classifiers are applied to the randomly selected data from the training database. After sorting the processing data using the hierarchical sorting method explained in the previous section, we adapt two well-known classifiers: MLP and HMM. Through this step, beside the optimal number of features, the optimal parameters of the classifiers are obtained for the experiments. Adjusting the classifier parameters and the number of features is conducted based on the leave-one-out method.Proposed ASR approaches in literature, in the aspect of statistical modeling, are categorized in two main groups, which are generative and discriminative.Phone recognition involves finding the best possible sequence of phones (Ph) that should be proper for a given input speech signal, X. Consequently, optimal phone sequence,Ph⁎, should be found such that:(20)Ph⁎=argmaxPhP(Ph|X)Based on the Bayes rule, Eq. (20) can be rewritten asPh⁎=argmaxP(X|Ph)P(Ph). According to this expression, the phone sequence can be determined based on the corresponding phone class membership and the learned model that indicates the conditional probability distribution of the observed acoustic features X.During the learning process in generative approaches, the learned model generates the input observations to fit the model. HMMs, hidden trajectory models, Gaussian mixture models (GMMs), stochastic segment models, Bayesian networks, and Markov random fields are examples of such models.In contrast, the discriminative approaches aim at maximizing the discriminability of the different classes while modeling the posterior class distribution. Examples of this approach are methods based on maximum entropy models, logistic regression, neural networks (multi-layer perceptron (MLP), time-delay neural networks (TDNN) or Boltzmann machines), support vector machines (SVMs), and conditional random fields (CRFs).To benchmark the proposed feature extraction method, this study tests the phone recognition methods from both categories. HMMs with Gaussian mixtures are used in the generative modeling approach, while the MLP is used as the discriminative modeling approach.A common tool dealing with classification problems, the use of ANNs is proposed frequently in literature. The name “artificial neural network” bears a resemblance to the structure of its computing units and the nervous system. However, mostly because of historical success of Hidden Markov Model (HMM) based ASR systems, commercial sectors prefer to employ these kinds of systems without considering any proof of their superiority to other contemporary methods. Through a neural network, phonemic, lexical, syntactic, pragmatic, and semantic knowledge of speech can be effectively integrated for speech recognition procedures such as segmentation and labeling. Hence, knowledge and constraints are distributed across the neurons in the training procedure.Quite complex nonlinear classifiers and mapping functions can be effectively generated through ANN training [77,78]. As a result, assumptions about the underlying probability distributions are no longer necessary. In addition, given the simplicity and uniformity of the underlying computing components, the development of such systems is interesting for hardware implementation.Recent neural network structures have been shown to be capable of time warping the phonemes observed in continuous speech [79]. Dynamic feed-forward networks have demonstrated their efficiency for this purpose as much as the best HMM based systems could. However, the availability of good training data and good strategy is critical for developing an efficient ANN-based ASR system. The capability of the ANNs motivated us to employ the MLP to solve the classification problem. ANN-based speech recognition system requires an optimal number of hidden neurons and input features. If the recognition task involves continuous speech, specific knowledge needs to be introduced to the ASR system for adjustments in the acoustic parameters. Several researchers used articulatory features for better phoneme discrimination [80]. As discussed before, based on discriminative feature selection [81,82], the present study proposes a method to obtain the optimal selection of MFCCs. In addition, the optimal MLP parameters are computed to improve the accuracy of the speech recognition. For training the MLP back-propagation algorithm proposed by Rumelhart et al. in 1986 [83] is frequently employed in literature. In this algorithm some parameters involve the learning process including learning rate, momentum, back-propagation error, number of epochs and stopping criterion.Normally, the stopping criterion is applied based on a predefined maximum epoch number or a predefined minimum back-propagation error rate. In the proposed neural network, in addition to the mentioned stopping criteria, a heuristic cost function is defined based on the error gradient:(21)|Eg(n)−Eg(n−100)||Eg(n−100)−Eg(n−200)|<thEgorEpoches>thEporE(n)<thEwhereEg,thEg,thEp,thE,E(n)denote the error gradient (Eg(n)=E(n)−E(n−1)), its threshold, the maximum epoch number, minimum error value for stopping the training process and back-propagation error in nth epoch, respectively.A three-layer MLP is used for the speech recognition of the Malay vowels. Log-sigmoid is used as the activation function. The output layer has six output neurons corresponding to six Malay vowels. Fig. 4presents the block diagram of the proposed speech recognition approach.One of the most widely used statistical modeling methods for time series and speech sequences is HMM, which is commonly employed in ASR systems. An infinite number of possible sequences is properly modeled by HMM via defining some probability distributions.According to the famous description as stated by Rabiner [84], an HMM consists of a number of states that correspond to situations in columns of a multiple alignment. Each state produces a symbol based on symbol-release probabilities. An interconnection between states exists based on state transition probabilities. Consequently, a sequence of states is obtainable by starting from an input state and transitioning to other states until arriving at an ending state. Entering each state, a symbol is produced based on the release probability distribution that results in a sequence of symbols.In dealing with speech recognition problems, we need to train the HMMs. With the large amount of speech data, finding the optimal structure of HMM including appropriate parameters properly describing the current speech data is significant. In ASR systems, the Baum–Welch training algorithm is frequently employed because it is an extensively studied technique for speech recognition. The algorithm randomly initializes a set of HMM parameters and alternatively updates the parameters. This process continues until it reaches a local maximum of sample likelihood (for more details see [85] and [86]). After finding the HMM parameters, the Viterbi algorithm is commonly used for recognition of a particular speech sequence. Based on the observed event, the Viterbi finds the most probable state sequence [87]. Both algorithms are suboptimal because their processes result in finding the local optima. The HMM model used in this study consists of six states of left to right HMM with each state containing eight Gaussian mixtures. The covariance matrix in all the states was diagonal. Each speech sample was associated with an appropriate label, which included one of the six Malay vowels to be used for the training process.

@&#CONCLUSIONS@&#
