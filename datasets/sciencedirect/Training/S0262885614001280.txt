@&#MAIN-TITLE@&#
Partial least squares-based human upper body orientation estimation with combined detection and tracking

@&#HIGHLIGHTS@&#
We propose a method for estimating the human upper body orientation.Our algorithm uses partial least squares-based gradient and texture feature models.Integration with an UKF-based movement prediction increases the performance.Comparison with the state-of-the-art shows the benefit of our algorithm.Experiment results using image, video, and camera are provided.

@&#KEYPHRASES@&#
Human upper body orientation,Partial least squares,Multi-level HOG-LBP,Random Forest classifier,UKF tracker,

@&#ABSTRACT@&#
This paper deals with the problem of estimating the human upper body orientation. We propose a framework which integrates estimation of the human upper body orientation and the human movements. Our human orientation estimator utilizes a novel approach which hierarchically employs partial least squares-based models of the gradient and texture features, coupled with the random forest classifier. The movement predictions are done by projecting detected persons into 3D coordinates and running an Unscented Kalman Filter-based tracker. The body orientation results are then fused with the movement predictions to build a more robust estimation of the human upper body orientation. We carry out comprehensive experiments and provide comparison results to show the advantages of our system over the other existing methods.

@&#INTRODUCTION@&#
Human body orientation estimation is one of challenging yet useful tasks for the mobile robot and surveillance applications. The human body orientation can tell us how people interact with each other in the surveillance scenes. For example, we may predict that a group of persons facing each other for a long time are having conversation, or other social semantic predictions such as waiting for the bus together.From the mobile robots point of view, the human body orientation can assist the robot to get a better prediction for avoiding a person when doing navigation tasks. It also helps the robot to make a social interaction with the human in outdoor navigation, such as approaching a person and asking the way. Here the robot certainly needs the estimation of human orientation for facing the person.Several works try to accomplish the human body orientation problem. Andriluka et al. [3] use banks of viewpoint specific part based detectors and linear Support Vector Machine (SVM) for estimating the whole body orientation. Another recent work is done by Weinrich et al. [4] which performs the human upper body orientation estimation using Histogram of Oriented Gradient (HOG) features and SVM Tree. A work by Baltieri et al. [21] employ a Mixture of Approximated Wrapped Gaussian (MoAWG) weighted by the detector outputs for increasing the correct estimation rates. The above researchers do not consider how the person movements will affect the overall estimation results.One notable work by Chen et al. [5] uses multi-level HOG features and sparse representation for classifying the human pose. They also employ a soft-coupling technique between the whole body orientation and its velocity using the particle filter framework.To solve the problems we have mentioned above, here we propose a system for detecting and estimating the human upper body orientation, as well as its motion. We prefer to use the upper body part rather than the whole body for gaining the robustness under occlusion; the full body is often partially occluded by small objects such as chair, table, bicycle, and so on.Our main contribution resides in the use of the partial least squares (PLS)-based model of gradient and texture features for estimating the human upper body orientation. Here, our PLS model is a modification of the one used in [16] which has been successfully applied for human detection. We also provide an Unscented Kalman Filter (UKF) framework integrating the human movement prediction and the orientation estimation for improving the estimation results.We organize the paper as follows. First we describe the detection and estimation of human upper body orientation using partial least squares models in Section 2. Section 3 explains the integration of the orientation estimation results and the tracking. We then provide the comparison of several methods and the result of extensive experiments in Section 4. Lastly, we conclude our work and give some possible future works.Our system is built in a hierarchical manner. First we detect and create bounding boxes around the human upper body using a fast detector. These detection results are then fed to the orientation estimator part. We divide the orientation into eight quantization (see Fig. 1). Results from the detector and the orientation estimator are used as the observation inputs for the tracker. Fig. 2explains our framework on the human upper body orientation estimation as described above.Our system is composed of the image-based orientation estimation and the tracking system (see Fig. 2). The influence between the systems is one-directional. That is, the orientation estimation system runs independently, and its results are used by the tracking system for increasing the robustness of its pose and motion estimation. This also implies that the orientation estimation system can be applied to still images.Histogram of Oriented Gradients (HOG) [1] is one of state-of-the-art descriptor for the whole body person detection. However, the original HOG algorithm is slow and unsuitable for the real time applications. Here we exploit the extended work of HOG by [12], which employs Adaboost for selecting features and cascade rejection for speeding up the detection time. Instead of using the whole human body, we prefer to exploit the upper body part for gaining the robustness under occlusion, as we will show in Section 4.2. The detection results (bounding boxes of the human upper body) are then used as the input for the orientation estimator part.The other works (e.g. [4] and [5]) solely depend on the gradient features, which capture the body shape. For the human upper body orientation case, we make an assumption that using only the body shape is not enough, for example, there is no big difference between the shape of the body facing front and back. Therefore, we propose a combination of the gradient-based and texture-based features which grab the shape and the texture cues. Here we expect the textured part of the body such as the face will be captured. We then apply the modified partial least squares (PLS) models for enhancing the important cues of the human orientation.For capturing the human upper body shape, we use a multi-level HOG descriptor [1]. The gradient magnitude for each upper body image sample is first computed using 1-D mask [−1 0 1] on each x and y direction. Every sample is divided into 3×4, 6×8, and 12×16 blocks, where each block consists of four cells. The gradient orientation is then quantized into nine bins. Overall we have 252 blocks and 9072 dimensional feature vectors of HOG descriptor.We use Local Binary Pattern (LBP), adopting the work of [2], to make a texture descriptor. We calculate image textures using LBP8,1 operator for each pixel(1)ILBPc=∑p=072pςIp−Ic,ςa=1fora≥00otherwise,where Icis the center pixel from which we calculate the LBP value ILBPcand p is eight-surrounding pixels of Ic. We then divide the LBP images into multiple blocks similar to the HOG features above. For each block, we make a histogram containing 59 labels based on uniform patterns.11According to the original paper [2], the uniform patterns contain at most two bit transitions from 0 to 1 and vice versa. For an 8-bit data, there are 58 uniform patterns, and the other patterns which have more than two bit transitions are grouped into one label, so we have the total 59 labels.This procedure gives us 14,868 dimensional feature vectors in total.Partial least squares (PLS) is a statistical method used for obtaining relations between sets of observed variables through the estimation of a low dimensional latent space which maximizes the separation between samples with different characteristics []. The PLS builds new predictor variables called latent variables, as combinations of a matrixXof the descriptor variables (features) and a vectoryof the response variables (class labels).Let us consider a problem with γ samples,X⊂ℝδbe an δ-dimensional space representing the feature vectors and y⊂ℝ denote a 1-dimensional space of the class labels. The PLS then decomposes the (γ×δ) matrix of zero mean variablesXand the vector of zero mean variables y into(2)X=TPT+E,y=UqT+r,whereTandUrepresent (γ×s) matrices of s extracted latent vectors, the (δ×s) matrixPand the (1×s) vectorqare the loadings, and the (γ×δ) matrixEand the (γ×1) vectorrare the residuals.Here we implement the nonlinear iterative partial least squares (NIPALS) algorithm [17] to find a set of projection vectors (weight vectors)W={w1,w2,…,ws}such that(3)covtiui2=max|wi|=1covXwi,y2,wheretiis the i-th column of matrixT,uiis the i-th column of matrixU, and cov(ti,ui) is the sample covariance between latent vectorstiandui. The process of constructing projection vectorsWis shown in Algorithm 1.Algorithm 1PLS/NIPALS algorithmHere we introduce our PLS-based feature models, Block Importance Feature Model of PLS (BIFM-PLS). We also provide another simple PLS model called Combined Feature Model of PLS (CFM-PLS) for comparison purpose.The Combined Feature Model of PLS (CFM-PLS) is created by concatenating all of HOG–LBP features into one vector, and simply run the PLS algorithm on it with specified number of the latent vectors. As the result, the CFM-PLS produces a reduced set of features by projecting the feature vectorsf⊂Xonto the weight vectors,(4)x⌣=Wf.This result is then used as the input for the classifier. Fig. 3explains the procedure of building CFM-PLS.The CFM-PLS method, which resembles an ordinary PLS technique, is only used for comparison purposes, and from now we focus on the BIFM-PLS method.The Block Importance Feature Model of PLS (BIFM-PLS) is built by an idea that not all of blocks or features have a high contribution to the classification. Here we want to examine the contribution of each block and discard the one which has low importance. Unlike the CFM-PLS which highly reduces the feature space, the BIFM-PLS is intended to retain some details about the features to be fed up to the classifier.We adopt and extend the method of [16] for picking out the representative blocks. For creating BIFM-PLS, we employ the feature selection called Variable Importance on Projection (VIP) (see [16] and [18]). The VIP gives a score for each feature representing its predictive power in the PLS model. The VIP of the i-th featurefis given by(5)VIPif=κ∑j−1pbj2wij2∑j=1pbj2,where κ is the number of features,wijdenotes the i-th element of vectorwj, and bjrepresents the regression weight for the j-th latent variable, bj=ujTtj.We then extend the definition of VIP by introducing block importance score (BIS). The BIS ranks the predictive power of each block in the PLS model. The BIS on a multi-level blocks exhibits a “hierarchical” modeling, which first tries to find the important blocks from the multi-level/multi-size blocks and retrieve more detail information from the blocks which have better importance score. Algorithm 2 and Fig. 4show the procedure of creating the BIFM-PLS model.The BIFM-PLS algorithm consists of two important stages: building the block importance and projecting the features on the important block. Let n be the number of blocks for each HOG and LBP features, m be the total number of concatenated HOG-LBP features in one block,fidenotes the feature sets at the i-th block, andfi,jrepresents the j-th feature at the i-th block with i={1,…,n} and j={1,…,m}.In the first stage, we extract a PLS model, take first p1 latent variables for each block, and concatenate them to build a modelffirst, as follows(6)fifirst=PLSfip1,where PLS(fi,p1) is a function for extracting p1 elements fromfi. Herefifirsthas elementfi,j1firstwhere j1={1,…,p1}.We assume that a small number of p1 are enough to see the contribution of each block to the orientation estimation, since the PLS considers the response variables (i.e. class labels). To see the importance of each block, we compute the VIP scores using Eq. (5) and get the block importance score (BIS) as(7)BISi=1p1∑j1=1p1VIPfi,j1first,i=1,…,n.The top part of Fig. 4 shows the processes explained by Eqs. (6) and (7) above.In the second stage, we build the BIFM feature vectors by calculating and concatenating the first p2 latent variables on the important blocks, similar to Eq. (6)(8)The blocks which have BIS under the threshold, which mean less important blocks, are then discarded. These procedures are illustrated by the bottom part of Fig. 4.Here we use p2>p1, to get more detail information on each important block. We will examine the proper value of p1 and p2 in the experimental section (Section 4.4).Algorithm 2BIFM-PLS algorithmOne of the notable classifier which works well on the multi-class data is Random Forest, introduced by Breiman [8]. It is an ensemble learning method which combines the prediction of many decision trees using a majority vote mechanism. The Random Forest is devoted for its accuracy on the large dataset and multi-class learning. These advantages make us choose the Random Forest for training our eight-orientation classification problem with a large set of features.Our Random Forest is constructed by multiple trees T={T1,T2,…,TN}, where N is number of trees. Letdi∈Di=1…Kdenote K training sets andci∈Ci=1…Kbe its corresponding labels or classes (in our case, we have eight classes,C=C1C2…C8), whereD⊂ℝMis the feature space. In the training phase, the Random Forest learns the classification functionT:D→C. Details of the Random Forest algorithm can be discovered at the original paper [8].We use a linear split function [19](9)Φf=qTf+z,where q is a vector which has the same dimensions as the feature vectorfandzis a random number. The recursive training is run until the stopping criteria are reached, i.e. the maximum depth is met or no further information gain can be drawn.In the testing phase, for a test case d, the Random Forest provides the posterior probability of each class as(10)pc|d=1N∑i=1Npic|d,where(11)pic|d=Λi,c∑j=c1c8Λi,j,pi(c|d) is the probability estimation of classc∈Cgiven by the ithtree, and Λi,cis the number of leaves in the ithtree which votes for class c. The overall multi-class decision function of the forest is then defined as(12)Cd=argmaxc∈Cpc|d.In the normal situation, the possibility that the person body orientation will be the same with its movements increases along with its speed. Based on this observation, our orientation estimation system is built using an assumption that the human body orientation is aligned with the direction of the human movements. In this case, we utilize both the result of the orientation detections and that of human movement estimation. On the opposite, we only rely on the orientation information from detections when the human movements are slow or even in a static condition.To handle those matters, we predict the movement of persons in the world coordinate. We then use the UKF framework to combine the orientation estimation from the detection results and the movement predictions. The idea of coupling the detection result and the person movement is also used by [5], but at least three (3) things distinct our works from the [5]; the baseline (upper body vs whole body), features (HOG–LBP–PLS vs Sparse-HOG), and the framework (UKF vs particle filter).We derive movement of the persons from the change of their positions. We follow the work of [9] and [10] for projecting the position of each detected person from the 2D image coordinate to the 3D world coordinate. Let us consider a pinhole camera model, with the following parameters: focal length fc, camera height yc, horizontal center point μc, and horizon position ν0. According to [10], the projection to the world coordinate is given by(13)Wd=ycμd−μcνd−ν0fcycνd−ν0hdycνd−ν0,where (μd,νd) is the bottom center point of the extended bounding box,22The extended bounding box is calculated by scaling the height of the bounding box to that of the human body, so that the bottom center is on the ground plane.andhdis the height of each detected person d in the 2D image. Vector Wd=[xdworld,ydworld,hdworld]Tdenotes the position (xdworld,ydworld) in the real world relative to the camera position and the height hdworldof the detected person d in the image.The horizon position v0 is obtained by collecting line segments in the image using Hough line detector and running RANSAC to evaluate all segments and get the intersection. This horizon estimation is done off-line33To keep the speed of algorithm, we compute and use the horizon estimate as a constant. We assume the camera tilt is small (or in other words, the ground plane where the robot runs is flat).and we use it as a pre-calibrated value for the on-line tracking. For each frame, we send the position (xdworld,ydworld) to the tracker for getting the movement estimation in the real world.Our state model is decomposed from the person position (xk,yk), its derivative (x˙k,y˙k), and the orientation components (φkM,φkD,θk) where φkMand φkDrespectively denote the person orientation due to the movement and the detection estimation and θkdenotes the final orientation of the person after considering the movement and the detection result, as stated by following tuple,FX=xkykx˙ky˙kφkMφkDθk. A constant velocity model is used for representing the person position and its derivative, as follows(14)xk=xk−1+x˙k−1δt+ϵx,yk=yk−1+y˙k−1δt+ϵy,x˙k=x˙k−1+ϵx˙,y˙k=y˙k−1+ϵy˙,whereϵxϵyϵx˙ϵy˙are the gaussian noises for each component, and δtis the time sampling.The orientation components ofFXare then described as(15)φkM=arctany˙k−1x˙k−1+ϵφMυk−1,φkD=φk−1D+ϵφD,θk=φk−1D+ω1−e−υk−11+e−υk−1φk−1M−φk−1D+ϵθ,whereϵφDϵθare the gaussian noises and ω is a constant.The noise functionϵφMυk−1in the first line of Eq. (15) is defined as(16)ϵφMυk−1=N0,gυk−1,gυk−1=συexp−υk−1,υk−1=x˙k−12+y˙k−12,where υk−1 is the magnitude of the person movement, and συis a constant. Using Eq. (16), the noise functionϵφMυk−1can be read as a zero-mean gaussian of which the variance varies w.r.t the person movement υ.Eq. (15) suggests the influence of the person movement to the orientation estimation. This equation, together with Eq. (16), tells us that when the velocity of a person is small enough (υk−1≈0), the uncertainty of the movement orientation becomes large (seeϵφMυk−1), and the orientation estimation will mainly depend on φkD(which is updated using the orientation detection result in the observation model (see Eq. (17))). On the contrary, when the velocity is large then the movement orientation becomes more reliable and the orientation estimation depends on both detection and the person movement.Here the constant ω has a duty for controlling the influence of the detection and the person movement to the overall orientation. The value of ω is later investigated in the experimental section (Section 4.5).We then use the observation model for the person position in the world coordinate (derived from Eq. (13)) and for the person orientation from the result of Section 2, as follows(17)HX=μk=xkfcycykyc+μc+ϵμνk=fcycyk+ν0+ϵνhk=fcycykhdyc+ϵhCk=fCd,φkD+ϵcwhere (xk,yk) denote the person position, (fc,yc,μc,ν0) are the camera parameters (focal length fc, camera height yc, horizontal center point μc, and horizon position ν0), and {ϵμ,ϵν,ϵh,ϵc} are the gaussian noises for each observation component. μk,νk, and hkhave the same definition with μd,νd, and hdin Section 3.1. The function f(C(d),φkD) maps the result of the multi-class decision function C(d) (Eq. (12)) into their equivalent angle of the orientation φkD.For choosing the tracker, we consider our hardware limitation and the system nonlinearities. Several well-known filters can be adopted for handling the nonlinearities, such as Extended Kalman Filter (EKF), Unscented Kalman Filter (UKF), and particle filter. Here we choose UKF because we want to avoid the resource costs of the particle filter (used by [5]) and the calculus of the Jacobian matrices used in the EKF.The UKF [20] employs the unscented transform, a deterministic sampling technique, to take a minimal set of sample points called sigma points around the mean. Consider the current state with the meanx^and its covariance Px. We build a set of 2L+1 sigma points matrixX, as follows(18)X0=x^Xi,k−1=x^+L+λPxi,i=1…LXi,k−1=x^−L+λPxi−L,i=L+1,…,2Lλ=α2L+κ−L,where L is the dimension of the augmented state, α and κ are constants for controlling the spread of the sigma points.We then define W(m) and W(c) as the weight for the mean and covariance respectively, given by(19)W0m=λL+λW0c=λL+λ+1−α2+βWim=Wic=12L+λ,i=1,…,2Lwhere β integrates prior knowledge of the distribution ofx^. We use the default value, α=10−3,β=2, and κ=0.We compute mean and covariance of the prior estimation (Xk|k−1and Pk−) using the state modelFXin Eqs. (14) and (15), and the sigma points above as follows(20)Xi,k|k−1=FXi,k−1,i=1,…,2Lx^k−=∑i=02LWimXi,k|k−1Pk−=∑i=02LWicXi,k|k−1−x^k−Xi,k|k−1−x^k−T+Qwhere Q is the covariance of the process noises.In the measurement update phase, we project the sigma points through the observation functionH(X)(21)Zi,k|k−1=HXi,k|k−1,i=1,…,2Lz^k−=∑i=02LWimZi,k|k−1,Pzkzk=∑i=02LWicZi,k|k−1−z^k−Zi,k|k−1−z^k−T+R,wherez^k−denotes the predicted measurement,Pzkzkis its covariance, and R is the covariance of the observation noises. The state measurement cross-covariancePxkzkand the Kalman gain K are computed as(22)Pxkzk=∑i=02LWicXi,k|k−1−x^k−Ζi,k|k−1−z^k−T,K=PxkzkPzkzk−1.The mean and covariance of the posterior estimation are then calculated as(23)x^k=x^k−+Kzk−z^k−,Pk=Pk−−KPzkzkKT.We also give to the tracker, the color histogram information Htrretrieved from the lower third of the bounding box, from which we expect to get clothing features.44In the upper body bounding box, the lower third part usually contains the human shoulder and body. We can expect that the cloth color can be retrieved from this region and does not change much during the tracking process.In the implementation using a moving robot (see Section 4.7), the camera movement is currently compensated by using the odometry of the robot. The use of Kanade–Lucas–Tomasi (KLT) features tracker [10] or the visual odometry [11] for compensating the camera movement are further investigated in the future.We treat each detection of the human upper body as an observation, to be associated with the trackers. For every observation, we use the position and histogram information for calculating the relative distance to each tracker Δob,tr, given by(24)Δob,trP=e−12μ¯ob−μ¯trT∑ob+∑tr−1μ¯ob−μ¯tr2ϕ|∑ob+∑tr|(25)Δob,trH=∑IHobI−H¯obHtrI−H¯tr∑IHobI−H¯ob2∑IHtrI−H¯tr2(26)Δob,tr=ηΔob,trP+ςΔob,trHwhere Δob,trPis the gaussian correlation between mean and covariance of the observed positionμ¯ob,∑oband the tracker positionμ¯tr,∑tr. Δob,trHis the color histogram correlation distance of the observation Hoband the tracker Htr, and η and ς are constants.Under the nearest neighbor assumption, each observation is assigned to the tracker when the distance Δob,tris below the threshold. A new tracker is born from the observation which has no association, and any track which is not associated with any observation and has a large uncertainty is then deleted.For reducing the calculation time, we do a hierarchical Region of Interest (ROI)-based tracking (including the orientation estimation) along the frame sequences. First, we search the whole space of the image to obtain initial observations and trackers. The next sequences utilize the position information from the trackers as the ROI. We do the detection around the ROI and in the area where persons may come up to the scene (see Fig. 5). This technique55Basically, this heuristic method is enough for our cases. For more general scenarios, we open this problem to the interested reader.considerably reduces the time for detection. Lastly, every 15 frames, we do the whole space search to anticipate the missing detection.

@&#CONCLUSIONS@&#
We have described a framework of orientation estimation and tracking. Our human upper body orientation classification system, utilizing a partial least squares model-based shape-texture features combined with the random forest, is proved to work better than any existing methods. Its integration with the tracking system boosts up the performance of the orientation estimation even further. Another notable result is that our system works real-time, giving a possibility to be used in the real robot application such as the person tracking.Future work for our system, besides the implementation on a real robot for specific purposes, is to combine it with other sensors such as laser range finders. By adopting multi-sensory fusion, we expect to build a more robust system for person localization. There are also possibilities for choosing and adding better heuristics in the integration of orientation estimation and tracking, such as a better way for selecting the ROI, so that the system will be applicable for more general scenarios.Supplementary data to this article can be found online at: doi: 10.1016/j.imavis.2014.08.002.The following is the Supplementary data to this article.Supplementary Video.Supplementary data to this article can be found online at http://dx.doi.org/10.1016/j.imavis.2014.08.002.