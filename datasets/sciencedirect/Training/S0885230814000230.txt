@&#MAIN-TITLE@&#
Fast vocabulary acquisition in an NMF-based self-learning vocal user interface

@&#HIGHLIGHTS@&#
We design a vocal user interface (VUI) which learns keywords from user interactions.We use weakly supervised NMF as a feasible approach for realistic VUI training.Using discriminatory features such as phone posteriorgrams yields fast word learning.Soft VQ with speaker-dependent trained Gaussians yields high accuracy.Stacking both features in NMF leads to fast and accurate keyword learning.

@&#KEYPHRASES@&#
MIDA,Phone posteriorgram,NMF,Fast learning,Vocabulary acquisition,

@&#ABSTRACT@&#
In command-and-control applications, a vocal user interface (VUI) is useful for handsfree control of various devices, especially for people with a physical disability. The spoken utterances are usually restricted to a predefined list of phrases or to a restricted grammar, and the acoustic models work well for normal speech. While some state-of-the-art methods allow for user adaptation of the predefined acoustic models and lexicons, we pursue a fully adaptive VUI by learning both vocabulary and acoustics directly from interaction examples. A learning curve usually has a steep rise in the beginning and an asymptotic ceiling at the end. To limit tutoring time and to guarantee good performance in the long run, the word learning rate of the VUI should be fast and the learning curve should level off at a high accuracy. In order to deal with these performance indicators, we propose a multi-level VUI architecture and we investigate the effectiveness of alternative processing schemes. In the low-level layer, we explore the use of MIDA features (Mutual Information Discrimination Analysis) against conventional MFCC features. In the mid-level layer, we enhance the acoustic representation by means of phone posteriorgrams and clustering procedures. In the high-level layer, we use the NMF (Non-negative Matrix Factorization) procedure which has been demonstrated to be an effective approach for word learning. We evaluate and discuss the performance and the feasibility of our approach in a realistic experimental setting of the VUI-user learning context.

@&#INTRODUCTION@&#
Command-and-control (C&C) speech recognition allows users to interact with systems like domestic devices, assistive technology, computers, smart-phones or other mobile devices. The user speaks a command or a phrase to control different functions in the environment like the central heating or the light units in the house, to retrieve information on their smartphone or to navigate through a menu on a computer. C&C applications are especially useful for people with a physical disability affording them handsfree control of their wheel chair, the positioning of their bed or other independent living aids.In most speech driven C&C applications, the spoken commands are restricted to a predefined list of phrases described by a restricted grammar and vocabulary. The size of the vocabulary ranges from a few to a few hundred words and the grammars are mainly rule-based. Although the targeted VUI application allows a developer to consider many interaction scenarios beforehand, the use of a VUI is not always successful when the interaction oversteps the clear boundaries of the lexicon, the grammars or the dialogue models. Even in less restrictive frameworks, such as in the now popular Siri speech recognition application for the iPhone, performance degrades rapidly if the acoustic models do not match the speech material used to train the system, for example on accented or dysarthric speech. The goal of this paper is to investigate a VUI model which is able to associate any utterance to a C&C action allowing command and control usability by deviant speech as well.Over the past decade, various approaches have been proposed for adaptation to unexpected circumstances in real-life situations. For instance, Paek and Chickering (2007) proposed a statistical model for mobile devices that tracks the past of the user's behaviour in order to predict commands. In Heinroth et al. (2012), grammars were able to adapt dynamically to real-life communication making interactions more natural. In Potamianos and Narayanan (1998) and Kuhn et al. (2000), speaker-independent acoustic models were adapted to speaker-dependent models allowing for better recognition of the user-specific vocalizations. There are plenty more studies that paved the way to more natural interaction with machines and devices by means of human-centred design and user adaptation. For instance, in a study of Parker et al. (2006) a robust speech recogniser was developed to adapt to dysarthric speech as well. In the “Speech Training And Recognition for Dysarthric Users of Assistive Technology” (STARDUST) project (Parker et al., 2006), the problem was tackled by adaptation in two directions: a training package assisting dysarthric speakers to improve the recognition likelihood of their utterances (users adapting to speech recognition systems) and speech recognition systems having greater tolerance to variability of dysarthric vocalizations (speech recognition models adapting to users) were developed.However, all these approaches have in common that these systems are still based on acoustic and language models that are trained beforehand and adapted through interaction to the spoken utterances of the user. While these methods focus on adaptation, we focus on grounding: learning both vocabulary and acoustics directly from the user during the usage of the VUI. The grounding process (Clark and Schaefer, 1989) refers to the process by which common ground or meaning is built between the user and the system. Situated in the “Adaptation and Learning for Assistive Domestic Vocal Interfaces” (ALADIN) project (van de Loo et al., 2012; Gemmeke et al., 2013), we aim to design a VUI that learns to understand speech by mining the speech input from the end user and the changes that are provoked on a device.The VUI should learn to understand classes referring to devices, actions or properties by using cross-situational evidence and learning the statistical regularities between two modalities, namely, the spoken utterances of the user and the feedback coming from the device(s). Supervision coming from the device is weak in the sense that the information provided to the VUI consists of signals referring to states and actions in a machine without any chronological information, orthographic nor phonetic transcriptions. Earlier studies have demonstrated that multi-modal Non-negative Matrix Factorisation (NMF) is a useful tool to learn weakly co-occurring regularities over two modalities in order to find the intra- and inter-modality patterns. For instance, in Caicedo et al. (2012), NMF is used to generate multimodal image representations that integrate visual and text features for image collections guided by ratings, comments and tags on the web. Akata et al. (2011) used a similar approach and called it multiview clustering to cluster images and predict image labels. Similar to NMF-based keyword discovery in Driesen et al. (2012a), we use NMF to learn co-occurrences between acoustic feature vectors emerging from the spoken utterances and semantic label vectors describing the action properties.In order for a self-learning approach to be useful as a VUI, the learning process should be fast. At the same time, after sufficient training tokens have been presented, the accuracy should be high. The contribution of this work is twofold. First, we investigate to what extent the learning speed and accuracy can be improved by using more advanced feature representations in NMF. We use phone classifiers to create phone confidence measures to replace the conventional acoustic input in NMF learning (Driesen, 2012; Sun, 2012). In addition to phone classifiers, we also evaluate a speaker-dependent version of soft Vector Quantization (soft VQ), which is a data-driven and probabilistic procedure to cluster the acoustic data of the speaker. We tested the usefulness of this data-driven approach for small training sets as user-specific data is expected to be scarce in the beginning of the VUI usage.A second contribution of this work is that we investigate to what extent the NMF machine learning procedure can be used for a VUI under realistic constraints. While previously, NMF evaluations were typically speaker-independent, we will work speaker-dependent since the system is self-learning and builds its representations from scratch. Also, while there is some prior work on investigating the learning speed of NMF (ten Bosch et al., 2009; Driesen and Van hamme, 2012a; Ons et al., 2012), this made unrealistic assumptions on the amount of speech material available during training for building the lower-level acoustic representations. Here, we will use speaker-independent material from different annotated datasets to train the phone classifiers or VQ clusters beforehand and use only speaker-dependent training data in proportion to the expected accumulative production of speech in a real VUI-user learning context, therefore, evaluating the feasibility of the VUI by limiting access to available data corresponding to a realistic operating mode.The remainder of the paper is organised as follows. In Sections 2 and 3, we introduce the learning framework, including the feature representations, acoustic models and NMF procedure used throughout the paper. In Section 4, we conduct a series of experiments to evaluate the effectiveness of the NMF approach on the ACORNS database (Boves et al., 2007) containing normal speech. NMF learning has been evaluated on ACORNS data (e.g. Driesen, 2012) and therefore we use ACORNS as well to introduce a proper baseline. We discuss related work and present our thoughts on future work in Section 5.Self-learning refers to the VUI's ability to learn from interactions with the end user. The training token consists of speech paired with the demonstration of the intended action. For instance, the user utters the command: “Close the door, please” and the VUI forwards that command to the automatic door closing system. However, if the VUI is lacking confidence, the user is asked to demonstrate the intended action, for instance, by pushing the correct button on an environmental control system (EVS). The VUI inferres the executed action from information sent by the control device. This assumes that a number of properties and actions enabling the control of a device are predetermined and a placeholder is provided for each one of them to represent the spoken words and to relay them to this control information during the learning process. The user's command and the demonstrated action is counting as one training example. If the VUI parses the wrong command, then the user has the opportunity to overrule the action. The overruling action will then serve as grounding information. In this study, we evaluate how many demonstrated actions, i.e. a command and a correct demonstration, are necessary to obtain a particular performance.In Table 1, the learning problem is demonstrated by means of a toy example. Supervision in Table 1(a) is displayed by the pictograms representing semantic tags like a device, an activity, or a property produced by a button-push. Assuming that the acoustic representations of the spoken utterances are represented by the text characters and the semantic tags by the pictograms in respectively the first and the second column of Table 1(a), then the learning process consists of finding the recurrent acoustic patterns (at least two adjacent letters) and their co-occurring semantic tags that make up the discriminative parts of the user's commands. These recurring patterns are displayed in Table 1(b).Learning a word by means of the statistical co-occurrence of multimodal evidence across situations is called cross-situational learning (Quine, 1964). In earlier studies (ten Bosch et al., 2009; Van hamme, 2008; Driesen et al., 2012a), weakly supervised Non-negative Matrix Factorization (NMF) has been presented as a useful machine learning procedure to discover and learn the acoustic representation of words accompanied by weak supervision. NMF works by factorizing a collection of utterance-based representations into the product of a matrix containing the latent factors describing the recurrent acoustic patterns (such as words) in utterances, and a matrix describing for each utterance which latent factors are active. In weakly supervised NMF, the utterance-based representations are accompanied by grounding information, i.e. label vectors referring to the semantic tags, and the aim is to find the recurrent patterns that co-occur with the semantic tags in the first matrix of the factorization and the activations of these semantic tags in the second matrix component (see Table 1).In the first experiment, we compare two processing flows that differ only in the mid-layer representation soft VQ (Section 3.3.1) or phone posteriorgrams (Section 3.3.2).Code books have been used before in NMF learning in numerous studies (Driesen et al., 2009, 2012a,b; Driesen and Van hamme, 2011b; Ons et al., 2012; Sun and Van hamme, 2011, 2011b, 2012) and we set the baseline by adopting multiple parameter settings from these studies. As in Ons et al. (2012), we used code books with different scales of granularity, L=20, 100 and 400. Similar to Driesen et al. (2012a), we used HAC's with frame-lags, τ=2, 5 and 9. Also the k-means procedure explained in Section 3.3.1 was shared with former studies (Driesen et al., 2009, 2012a,b; Driesen and Van hamme, 2011b; Ons et al., 2012; Sun and Van hamme, 2011, 2011b, 2012). However, the Gaussians and the soft VQ clusters were usually estimated by using the training data from the keyword-learning training set. Within the context of our study, it is regarded as an unrealistic simulation of the VUI-user usage (see Section 4.1). Nevertheless, this processing scheme is used to set a baseline in respect with earlier studies, and in the mean time, it serves as an upper bound for the best performance expected from soft VQ since any mismatch between code-book train- and test-set would result in less effective clusters degrading overall performance. The processing flow referring to this baseline setting is named MFCC_SVQ(ACN).Code book training is completely unsupervised and data-driven, but the training of a phone HMM requires transcribed speech data. Spoken utterances of the user lack phonetic transcriptions, therefore, in a realistic VUI-user training scenario, phone models should be developed beforehand on transcribed speech data. We used WSJCAM0 for simulating the case that the phone models are trained on the native language of the end user, and CGN for the case that the trained phone models are originating from a different language. We refer to these processing flows with the names MFCC_PHN(WSJ) and MFCC_PHN(CGN), respectively. These two processing are considered realistic. Phone models trained on ACORNS are not considered realistic because the ACORNS speech data represents the spoken utterances of the user in our simulated VUI-user context. However, similar to MFCC_SVQ(ACN), it is still an interesting case for investigating the potential gain if phone models could be trained unsupervised. The processing scheme is called MFCC_PHN(ACN).In the low-layer representation of the architecture (see Fig. 1), two alternative spectro-temporal processing steps were explained. In the previous experiment, we used MFCC features in all processing flows, but here, we evaluate potential gains obtained from MIDA features in the same processing flows investigated before. We split the experiment in two parts: firstly, we evaluate the gains for MIDA using soft VQ, and secondly, we investigate MIDA features for the three phone recognisers adopted from the previous experiment.In the first part, the three training sets for training the different MIDA transformations are ACORNS, WSJCAM0 and CGN. Code book training, the next step in the architecture, is then performed on the MIDA-transformed ACORNS features. According to the naming procedure (see Section 4.2.3), the processing flows are called: MIDA(ACN)_SVQ(ACN), MIDA(WSJ)_SVQ(ACN) and MIDA(CGN)_SVQ(ACN). By keeping the code book training set constant, the effects of the three MIDA-transformation are comparable.In the second part of the experiment, we implement the MIDA variant for each phone recogniser treated in the previous experiment resulting in the following three processing flows MIDA(ACN)_PHN(ACN), MIDA(WSJ)_PHN(WSJ) and MIDA(CGN)_PHN(CGN).Contrary to the previous experiments (see Sections 4.3 and 4.4), user-centred NMF for keyword learning is pursued here with separate NMF keyword representations for every individual speaker instead of one keyword model counting for all four speakers together. Such a setup corresponds better to a realistic training context of the VUI where only a single end user is expected to train and use the system. We investigate the effect of speaker-specific input to the VUI on keyword learning. The investigated processing flows here adopt the MIDA variant of the processing flows investigated in the first experiment (Section 4.3). Note that the learning curves share the same names as some learning curves in the preceding experiment because the low- and the mid-layers are identical, but the obtained accuracies might differ as keyword learning is user-dependent. The setup is identical to the previous experiments (see Section 4.2.1), except for the training sets containing utterances of each separate speaker only.The advantage of training code books beforehand is that large speech corpora can be used, such as those employed in the field of speech recognition. However, the acoustic-model training set is then recorded in different conditions (e.g. different microphones, different room acoustics and maybe cleaner speech) and with different speakers than the speech data originating from the user. We use WSJCAM0 as acoustic-model training set to simulate the case where the acoustic-model training set is different from the keyword-training set ACORNS. We refer to this processing flow with the name MIDA(WSJ)_SVQ(WSJ).The speech data of the user has no phonetic transcriptions in a real VUI-usage environment, but, since code book training is data-driven, the user data can be used to train the code books. However, the data will be limited to the set of utterances that the user has spoken up until a particular moment in time, and thus, the training data is rather scarce especially during the initial VUI usage. We refer to the processing flow as MIDA(WSJ)_SVQ(ACN,SSD). We follow the code book training procedure explained in Section 3.3.1 but we add one constraint by prohibiting further splitting of clusters when the number of frames joining one cluster becomes less than 78 frames, a measure that allows for a more reliable estimation of the covariance matrix of the Gaussians. However, by fulfilling this constraint, the number of clusters is variable and gradually increases with the number of utterances in the training sets. For instance, for the training set sizes with N=50, 100, 200, 800, and >1750, we obtained on average code book sizes of L=51, 93, 148, 191 and 330 for all the folds.The aim of this experiment is to investigate whether code book training for scarcely available but speaker-dependent data yields higher accuracies compared to the case where data is speaker-independent, but abundantly available in the field of speech recognition. These two realistic cases are accompanied by one unrealistic cases where code books are trained on all available speaker-dependent data in ACORNS. It serves as a reference for the case that large amounts of speech data from the user would be available before the VUI usage. We call the learning curve MIDA(WSJ)_SVQ(ACN,SD). Note that such a scenario can be realistic when speech from the end user is recorded beforehand for example by reading a standard text before the usage of the VUI. However, instructing a user to read a standard text would require additional effort and the effect on the VUI depends strongly on the precise implementation of the instructions.In this study, the goal is to combine the realistic processing flows that yielded the best results for the average user in all the former experiments. Within the set of realistic learning curves, MIDA(WSJ)_SVQ(ACN,SSD) provided the highest accuracy at the end of the learning curve and MIDA(WSJ)_PHN(WSJ) provided the highest accuracy in the beginning of the learning curve. By combining both processing flows, we investigate whether the best of both worlds can be obtained for the whole range of the learning curve.The two flows are combined in NMF by stacking the data matrices Vaof both processing flows in one large data matrix giving both streams equal weights, i.e. both streams are normalised so the sum of all entries in each stream are equal. Naturally, weights can be tuned to favour one of the two performance indicators.The two streams MIDA(WSJ)_PHN(WSJ) and MIDA(WSJ)_SVQ(ACN,SSD) are adopted from the former two experiments in Sections 4.5 and 4.6. The combined stream is called MIDA(WSJ)_comb.

@&#CONCLUSIONS@&#
