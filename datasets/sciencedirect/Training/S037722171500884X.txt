@&#MAIN-TITLE@&#
An experimental investigation into the role of simulation models in generating insights

@&#HIGHLIGHTS@&#
We perform an experiment exploring the role of simulation in generating insights.Students work on a task using a model's animation or statistical outputs, or no model.Using statistical results generates insights more frequently than not using a model.Using statistical outcomes generates insights more rapidly than using animation.False insights emerge less frequently using statistical results than using animation.

@&#KEYPHRASES@&#
Discrete-event simulation,Insight,Animation,Experimentation,Behavioural operational research,

@&#ABSTRACT@&#
It is often claimed that discrete-event simulation (DES) models are useful for generating insights. There is, however, almost no empirical evidence to support this claim. To address this issue we perform an experimental study which investigates the role of DES, specifically the simulation animation and statistical results, in generating insight (an ‘Aha!’ moment). Undergraduate students were placed in three separate groups and given a task to solve using a model with only animation, a model with only statistical results, or using no model at all. The task was based around the UK's NHS111 telephone service for non-emergency health care. Performance was measured based on whether participants solved the task with insight, the time taken to achieve insight and the participants’ problem-solving patterns. The results show that there is some association between insight generation and the use of a simulation model, particularly the use of the statistical results generated from the model. While there is no evidence that insights were generated more frequently from statistical results than the use of animation, the participants using the statistical results generated insights more rapidly.

@&#INTRODUCTION@&#
Discrete-event simulation (DES) is a popular modelling technique that is claimed to support problem solving and decision making. Indeed, it is often said that clients gain ‘insights’ as a result of simulation interventions, especially from the simulation animation (Bayer, Bolt, Brailsford, & Kapsali, 2014; Belton & Elder, 1994; de Vreede & Verbaeck, 1996; Hurrion, 1986; O'Kane, 2004; Pidd, 2010; Proudlove, Black, & Fletcher, 2007; van der Zee & Slomp, 2009). However, the term ‘insight’ is used quite loosely to mean an improved understanding. Cognitive psychologists explain that insights may refer not just to the acquisition of better understanding, but also to the experience of sudden shifts in understanding or ‘Aha!’ moments. More specifically, insight is defined as ‘the cognitive process by which a problem solver suddenly moves from a state of not knowing how to solve a problem to a state of knowing how to solve that problem’ (Mayer, 2010, p. 276).Given the claims about simulation in insight generation and the huge growth in simulation literature over the last two decades (Powers, Sanchez, & Lucas, 2012), it is surprising that there is almost no empirical evidence to support the claims about insight. Evidence of learning outcomes is scarcely published in simulation papers (Fone et al., 2003). Even where the learning outcomes are reported, there is generally no explanation of the causal mechanism for learning, let alone Aha! moments. Therefore, any claim that the catalyst for insight is a simulation model, and more specifically the animated display of the simulation model, has relied largely on supposition and anecdotal evidence from case studies. Meanwhile, relatively little task-based behavioural research has been conducted aiming to support the above claims; and where it has, the results are mixed (Bell & O'Keefe, 1995).To address this dearth of evidence, this paper describes an experimental study that aims to test whether and how insights are generated from DES models. Our contribution is to provide a more in-depth understanding of insight in the context of simulation and empirical evidence on the role of simulation in generating insight.The paper is organised as follows. In Section 2, we describe in more detail the concept of insight and discuss how it relates to the simulation context. Then, we review the limited evidence that exists surrounding the use of simulation models and insight. In Section 3, we present the experimental study, explaining the research hypotheses, the experimental design, the participants, the procedure, the dependent measures and the materials used. Section 4 details the results of the study, followed by a discussion on the value of simulation models in insight generation, the limitations of the study and suggestions for future work (Section 5).This section provides the conceptual foundation for our experimental study. We first introduce the concept of insight from relevant fields and we discuss its relevance to the simulation context. We then discuss the evidence that currently exists in the academic literature regarding the role of simulation in generating insight.The word ‘insight’ is used in two ways. It is used as a state of understanding – that is, to have insight into something (Smith, 1995). Insight is also described as an experience, an Aha! experience, involving a moment of epiphany (Schooler, Fallshore, & Fiore, 1995). This view is originally encountered in the story of Archimedes of Syracuse when he discovered the principle of displacement–‘eureka’. For this research we adopt this latter concept, proposing it as an approach to measure the value of simulation as a means for creating knowledge.To explore Aha! insight in more depth, relevant literature is considered and in particular the theoretical domain of Gestalt theory (Maier, 1940, Mayer, 2010), creative cognitive psychology (Sternberg, 2009) and a collection of studies on insight that have attempted to conceptualise the phenomenon (Kounios & Beeman, 2009, Metcalfe & Wiebe, 1987). Despite the fact that these streams of literature do not share the same theoretical foundations, it seems that they all agree upon the phenomenological perspective of the concept: a satisfactory solution to a problem suddenly emerges after overcoming an impasse. An impasse is the state in which a problem solver realises that initial ideas do not solve the problem, but at the same time feels that all the possibilities have been exhausted (Schooler et al., 1995). Generating insight is a productive activity which is about doing something new or novel. New ideas that do not lead to the solution itself but are relevant to finding the solution are described as false insights (Isaak & Just, 1995). They usually occur when the cause of a problem is misunderstood. In particular, in false insight, a person approaches the problem in a new or novel way, but without having a correct view of the problem. When false insights emerge, the suggested idea is not a satisfactory solution to the problem.Insight differs from other problem solving approaches, such as intuition, which are often used synonymously in everyday speech. Dane and Pratt (2007) explain that while the concept of insight involves some degree of non-conscious thought, it arises through logical connections between a problem and the solution. Intuition, in contrast, relies on non-conscious associative connections. Insight also differs from guessing in that the latter does not require making any sort of connection (i.e. conscious or unconscious). As a result, in problem solving with insight, a person is able to justify the suggested solution, whereas in problem solving with intuition, or guessing, the person is not.Social scientists have offered many explanations about the mental mechanisms of insight generation which seem somewhat interrelated. In short, in achieving illumination (i.e. generating insight), a problem solver may overcome implicitly imposed constraints (Weisberg, 1995), change mental representations, become aware of a new association between parts of a system, change the meaning of some problem element, or assimilate possible solutions from the environment (Davidson, 1995). In other words, it is believed that prior knowledge and experience constrain people's worldview, and, as a result, this knowledge may prevent them from seeing the world as it really is. Nevertheless, by using past experience as a building block, avoiding being confined by habits or irrelevant associations, the problem solver may eventually identify the appropriate way to solve a problem; and hence insights emerge. For a more detailed discussion on insight see Gogi, Tako, and Robinson (2014).Applying the above in the context of DES, it can be claimed that insight occurs when people using a simulation model suddenly know how to improve the performance of a system after several failed alternative attempts (i.e. what-ifs scenarios). The strategy used to achieve major improvements in the system involves doing something new or novel. So, we can identify that insight occurs during the experimentation phase of a simulation project if the users go through a problem solving pattern where an impasse (i.e. a phase in which simulation users only run what-if scenarios which are similar to existing unsuccessful strategies) is followed by a sudden realisation/generation of new or novel ideas that achieve major improvements in the performance of as system. After the experience of an Aha! moment, the users must be able to justify the rationale underlying the suggested solution that has arisen. We note that since a simulation model is a simplification of the real world (Robinson, 2008) insights only relate to the model, and they must be interpreted in the light of wider knowledge about the real-life system.Table 1provides a non-exhaustive list of instances of insight found in the simulation literature. For each example, a possible explanation about the mental mechanisms of experiencing insights is suggested with respect to the cognitive psychology theories introduced in Section 2.1.In this research, we use a single problem (i.e. the NHS111 case study as described in Section 3.6.1) to explore the value of simulation in generating insights. As such, it is not possible to study assimilating possible solutions from the environment (i.e. there is no second game and consequently transferring knowledge acquired from one simulation game to another is not possible). In our case study, we also make explicit the parts of the system and all the associations between them. Therefore, it is not possible to study changing mental representation or becoming aware of some problem elements. Instead, we consider both implicitly imposed constraints and misconceptions of some problem element in order to study the value of simulation in generating insights.There is very limited empirical evidence on the role of DES in learning and generating insight. This is despite Richels’ (1981) suggestion, over 30 years ago, that models should be used as a means of education and exploration so that insights can be made available to clients. Indeed, van der Zee and Slomp (2009, p. 17) are surprised that simulation continues to be seen “as just a methodology to analyse design decisions”. Although Robinson (2005) warns that alternative uses of DES are crucial for the future of the technique, it is hard to find cases in the literature in which simulation deviates from its analytical problem-solving role. It seems that DES continues to be seen as just a ‘hard’ OR technique.Practitioners and scholars often argue that DES interventions help stakeholders to gain insights about their problems and subsequently to generate effective ideas on how to address them (Bayer et al., 2014; Belton & Elder, 1994; de Vreede & Verbaeck, 1996; Hurrion, 1986; O'Kane, 2004; Pidd, 2010; Proudlove et al., 2007; van der Zee & Slomp, 2009). Simulation case studies, however, rarely report on insights and even less on the stimuli that help people in real simulation settings arrive at insights. For instance, Taylor, Eldabi, Riley, Paul, and Pidd (2009) perform a literature review of recent published simulation research and observe that there is a relative lack of research with real world involvement, and there is an even greater lack of evidence of the value of simulation in decision making. Other domain-specific reviews of the simulation literature have reached the same conclusion (Fone et al., 2003; Günal & Pidd, 2010; Jun, Jacobson, & Swisher, 1999).Among the few examples of case studies that provide evidence of insight generation during a DES study are den Hengst, de Vreede, and Maghnouji (2007), Elder (1992), Lehaney, Clarke, and Paul (1999), Robinson (2001) and Fletcher, Halsall, Huxham, and Worthington (2007). All these present real simulation projects in which the interest was not to use simulation as an analytical tool, but as a means to facilitate debate. In particular, the authors discuss how some model results, that were not anticipated, led to reassessment of initial plans and hence the generation of new and effective ideas on how to address a problem (i.e. insights).While these studies provide some support that insights were generated during the interventions, it is difficult to identify the causal mechanisms of experiencing these Aha! moments. Because insight involves some degree of non-conscious thought, people appear unable to explain what elements of the DES intervention help them generate new ideas (Schooler et al., 1995). Proponents of visual interactive simulation (VIS) (Bell & O'Keefe, 1987; Crookes, 1982; de Vreede & Verbraeck, 1996; Macal, 2001; Paul, Giaglis, & Hlupic, 1999; Wenzel & Jessen, 2001), which is the process of interacting with the graphical environment of a model during its execution for the purpose of model experimentation and analysis, argue that by watching the animation of a simulation model clients learn about their systems and so make better decisions. Again, the evidence to support this assertion is largely anecdotal and the empirical evidence is weak.Laboratory based experimental studies could provide an empirical basis for exploring the claims about DES and insight. To our knowledge, however, the only elements of a DES intervention that have been tested in an experimental setting are the involvement of the client in the modelling process (Monks, Robinson, & Kotiadis, 2014), the visual representation of a model (Carpenter, Bauer, Schuppe, & Vidulich, 1993; Swider, Bauer, & Schuppe, 1994), and experimentation with a model (Bell & O'Keefe, 1995; Chau & Bell, 1994; Luehrmann & Byrkett, 1989; O'Keefe & Pitt, 1991; Parker, 1991). These experimental studies provide useful findings about the role of simulation models in problem understanding and decision making. What they do not provide is a study on the generation of insight. Further, DES has developed significantly since the original studies were performed. For instance, in the work of Parker (1991) the simulation model did not include any animated displays (moving icons).The experimental studies of Carpenter et al. (1993) and Swider, Bauer and Schuppe (1994) focus on the role of animation in facilitating communication with clients. The authors use students to determine which combinations of visual presentations (i.e. the movement, colour and detail of icons) and speed are best for communicating invalid model behaviour. The results suggest that animation with movement is better than animation with no movement or dynamically changing bar charts in communicating the operations of the simulation model. In addition, when the presentation speed is slow, response times are shorter with greater accuracy in problem identification.In the experimental work of Bell and O'Keefe (1995), which is a refined version of O'Keefe and Pitt (1991), participants could choose among three different types of displays to solve a task: an animation (moving icons), a dynamically changing graphic and a numerical display (animation switched off). Although participants showed a strong preference for using the animation, its use did not result in better solutions and overall performance was poor. Nonetheless, participants who preferred to use the animated display tended to solve the problem with less effort (i.e. fewer alternatives) and quicker than participants who had switched off the animation.Chau and Bell (1994) perform an experiment that aims to determine whether the use of VIS supports decision making. The authors compare task performance of students who made use of a VIS model to students who used a no-VIS model (animation is switched off and only numerical output are produced). The results reveal that the use of VIS is more effective than the traditional simulation model in terms of problem understanding, solution rates, solution time and number of scenario runs. However, in a similar experiment, Luehrmann and Byrkett (1989) find the opposite; participants who made use of a VIS model were slower and less accurate in their decisions than participants who used a no-VIS model. The difference may be because Luehrmann and Byrkett only performed a small pilot study.The most recent experimental study by Monks et al. (2014) focuses on learning from DES studies and hints at a mechanism for generating insight. Through a laboratory experiment they specifically study the impact of model reuse versus model building on the client's ability to solve a task. They find that the clients appear to learn more from the use of the model than from their involvement in model building. However, new and effective ideas on how to address the problem were generated more frequently by participants who were involved in the model building. Although not identified by Monks, Robinson and Kotiadis, this suggests that involvement in model building is a mechanism for generating insight.One particular stream of empirical research that is relevant to insight generation has been carried out by researchers in the system dynamics (SD) field. Researchers in this area are concerned with understanding how people learn from SD. Richardson, Andersen, Maxwell, and Stewart (1994) operationalise learning as a mix of mental processes by which people change their mental models. The concept of mental model has been central to SD since the beginning of the field (Forrester, 1968). Put simply, a mental model depicts a person's (or a group's) ideas on the structure of a system and how that is responsible for the system's behaviour. Behavioural decision theory reveals that people suffer from bounded rationality because of a number of judgemental biases and heuristics that people employ in complex situations (Kahneman, 2011). Accordingly, people tend to misperceive dynamic rules and simplify the way a complex system is structured. That means that people's mental models are often unable to predict the behaviour of a dynamic system (Sterman, 1994). Nonetheless, if people manage to refine their initial deficient mental models, learning occurs (Rouwette, Vennix, & Felling, 2009). As their mental models radically change, they may create new strategies to improve the performance of a system. Such productive activity denotes the use of the double-loop learning system as described by Argyris and Schön (1996) and is closely related to the concept of insight.The counterintuitive behaviour of a system can mislead people about the actual causes of a problem. As a result they may not realise that their current strategies worsen the performance of the system (Forrester, 1968). The analysis of a dynamic system with the use of a SD model provides a practical way to make explicit and test people's mental model (Hsaio & Richardson, 1999). However, findings from experimental studies provide little evidence that the experimentation with a SD model support users in overcoming flaws in their mental model (Cronin, Gonzalez, & Sterman, 2009; Langley & Morecroft, 2004; Monxes 2004; Rouwette, Größler, & Vennix, 2004; Sterman, 1989; Sterman, 1994; Sterman & Booth Sweeney, 2007). The studies show that participants often appear unable to explain or intuitively predict the behaviour even of the simplest dynamic system. Rouwette, Korzilius, Vennix, and Jacobs (2011) provide some empirical evidence that SD interventions support people in changing their mental models and hence generating insights. However, it is not made explicit what elements of the intervention help produce those insights. Through an experimental study, Howie, Sy, Ford, and Vicente (2000) suggest that misperception of feedback is in part due to inferior interface design rather than just cognitive biases. The empirical evidence of this study shows that an improved human-computer interface can reduce the difficulties people have in dealing with complex systems, but it cannot eliminate them (i.e. optimal performance was not achieved by any participant).Overall, the conclusion that can be drawn from this discussion is that although it is assumed that simulation models support problem solving and decision making, there is lack of empirical evidence on the efficacy of simulation models in generating insight. Similarly, the elements of a simulation intervention that stimulate insight generation are even less well understood. A specific claim is that watching the animated display of a simulation model is more helpful in making better decisions than relying on the statistical outcomes generated from simulation runs; but again, there is very limited evidence to support this.This section describes the details of the experimental study, explaining the hypotheses that are tested and the experimental design, and giving details of the participants, the experimental procedure, the dependent measures and the materials used for the experiment, including the case study based on the NHS111 telephone service for non-emergency health care.The objective of our research is to test whether simulation models, and specifically DES models, support users in generating insights. Our aim is to establish whether insights are generated primarily by watching the animated display of a simulation model or by using the statistical outcomes generated from simulation runs.In order to address the above research objectives two hypotheses are examined.Hypothesis 1Insights are generated more frequently when a simulation model is used compared to when it is not used.This hypothesis follows from the claim in the simulation literature that simulation can help generate insights (Bayer et al., 2014; Belton & Elder, 1994; de Vreede & Verbaeck, 1996; Hurrion, 1986; O'Kane, 2004; Pidd, 2010; Proudlove et al., 2007; van der Zee & Slomp, 2009) and from case studies in which simulation has helped people in generating new and effective ideas (den Hengst et al., 2007; Elder, 1992; Fletcher et al., 2007; Lehaney et al., 1999; Robinson, 2001). In order to test this hypothesis we compare the frequency of insights generated by the participants who used a simulation model against the participants who did not. The latter participants formed the control group of this research. They were allowed to create scenarios on paper and then analyse them using their problem solving skills (calculators were permitted).Hypothesis 2The contribution of animation to the process of insight generation differs from the contribution of statistical outcomes.Based on the work of Chau and Bell (1994) and Bell and O'Keefe (1995), our expectation is that the animated display is more helpful in generating insights than the statistical outcomes generated from simulation runs. However, due to a lack of recent evidence to support the above claim, and the opposite findings in Luehrmann and Byrkett (1989), Hypothesis 2 has been set with no specific direction for the prediction. In order to test the contribution of animation versus statistical outcomes, we compare the frequency of insights, the task duration and the time to insight generated by the participants in the animation versus the statistics condition.We conducted a controlled experiment to meet the objectives of this study. We randomly assigned undergraduate students to two experimental conditions, namely animation and statistics, and asked them to solve a task either by using the animation or only the statistical outcomes of a simulation model. We compared their task performance to a more experienced group of undergraduate students who did not use the simulation model to solve the same task. The latter group is called control group, in that the subjects did not receive treatment (i.e. no simulation model was provided). As such, this study uses a quasi-experimental non-equivalent group design. Although the design is not completely randomised, quasi-experiments can still provide plausible causal knowledge about the impact of experimental factors (Campbell & Stanley, 1963; Thyer, 2012).Results from a small-scale pilot study suggested that the proportion of people who solved the task without the use of a simulation model was 15 percent (2 out of 13). The subjects were all PhD students with varied backgrounds who voluntarily participated in the pilot study. Four were familiar with simulation, including the two solvers, and six had prior experience with the context of the case study (NHS111 or a similar service); among them was one solver. The results of the pilot study also indicated that the proportion of solvers increased to 54 percent (7 out of 13) when a simulation model is used. Therefore, we estimated that a scientifically important difference in the proportion of solvers between the experimental conditions group and the control group is 0.40. This effect size may be categorised as medium (Cohen, 1988). In accordance with Fleiss, Levin, and Paik (2003, p. 76) we estimated that the required sample size is at least 20 per group at a significance level α = 0.05 and power 80 percent.The study was conducted at two universities in the UK. The two experimental conditions (i.e. simulation with animation or statistics) included in total 47 undergraduate students who took business, but no simulation modules, at Loughborough University. Students were randomly assigned to each experimental condition. The control group (i.e. no simulation model) of 20 participants consisted of undergraduate students who took simulation modules at Warwick University. All participants heard about the possibility to participate in this study through emails and announcement in lectures. To encourage participation in our study, monetary incentives were used (£10) (Abeler & Nosenzo, 2013). An additional small monetary reward (£5), linked to the achievement of the task's goal, was given to all participants that provided valid solutions (Bonner & Sprinkle, 2002). For the experimental conditions, participants were invited to attend one parallel session held in February 2014 and for the control group, one session was run in April 2014.Demographic characteristics of the participants are presented in Table 2. There were no statistically significant differences between the experimental conditions group and the control group in terms of gender and prior experience with the context of the case study (NHS111 or a similar service). The selective criterion differed between the experimental conditions group and control group (i.e. taken simulation modules), and the groups also differed in degree year and in the number of modules taken with a quantitative content. In particular, Warwick students (control group) were more advanced in their studies and had taken more quantitative modules than Loughborough students (experimental conditions group). As such, if simulation does not affect task performance, we expected the control group to perform better than the experimental conditions group.

@&#CONCLUSIONS@&#
This research explores the role of DES models in generating insights. A controlled quasi-experiment is employed, using a between-groups design. One independent variable is manipulated: the features of the simulation model. Participants work on a task either using only a simulation animation or only the statistical results of the model. Meanwhile, a control group works on the task without the use of a model.To our knowledge, ours is the only experimental study attempted so far that has tried to investigate the use of simulation models in insight generation. To an extent, the results of the experiment support the claim that insights are generated more frequently when a simulation model is used. While the differences in insight frequencies between the statistics condition and the control group are statistically significant, those between the animation and control group are not. Although there is no evidence that insights are generated more frequently when participants used the statistical outcomes rather than the animation of the simulation model, we find that the use of statistical outcomes reduces the emergence of false insights, and that has a direct impact on task duration.Despite some limitations in this experimental study, the results of the research provide empirical evidence about the value of DES in insight generation. We also provide workers in behavioural operational research with an approach for studying insight using laboratory based experiments. Future work should continue to explore insight generation from models, and from the wider activity of developing and using models.