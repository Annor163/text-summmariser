@&#MAIN-TITLE@&#
Histogram of 3D Facets: A depth descriptor for human action and hand gesture recognition

@&#HIGHLIGHTS@&#
A new 3D local shape descriptor for hand gesture and human action recognition from depth sensors.Extensive results demonstrate the outstanding discriminative power of the proposed descriptor.The proposed descriptor is compact in dimension, which is thus economic to compute.A Dynamic Programming based Temporal Segmentation framework to represent dynamic depth videos.

@&#KEYPHRASES@&#
Computer vision,RGBD image processing,Gesture and action recognition,Time sequence representation,Histogram of 3D Facets,

@&#ABSTRACT@&#
The recent successful commercialization of depth sensors has made it possible to effectively capture depth images in real time, and thus creates a new modality for many computer vision tasks including hand gesture recognition and activity analysis. Most existing depth descriptors simply encode depth information as intensities while ignoring the richer 3D shape information. In this paper, we propose a novel and effective descriptor, the Histogram of 3D Facets (H3DF), to explicitly encode the 3D shape information from depth maps. A 3D Facet associated with a 3D cloud point characterizes the 3D local support surface. By robust coding and circular pooling 3D Facets from a depth map, the proposed H3DF descriptor can effectively represent both 3D shapes and structures of various depth maps. To address the recognition problems of dynamic actions and gestures, we further extend the proposed H3DF by combining it with an N-gram model and dynamic programming. The proposed descriptor is extensively evaluated on two public 3D static hand gesture datasets, one dynamic hand gesture dataset, and one popular 3D action recognition dataset. The recognition results outperform or are comparable with state-of-the-art performances.

@&#INTRODUCTION@&#
3D shape representation is a significant component of object categorization and action recognition. Compared to 2D image-based appearance representation, 3D depth-map-based representation is more robust to viewpoint and pose changes and holds great promise for modeling physical-related attributes such as positions, poses, shapes, and scene contexts. Over the last few years, the successful commercialization of depth sensors and corresponding development toolkits have made 3D shape information more accessible for computer vision applications [4,8,9,11]. Compared to traditional RGB cameras, RGBD cameras provide more information about object sizes, shapes, poses, and positions and capture strong boundary clues and spatial layouts, especially in environments with cluttered backgrounds and large illumination changes. The depth sensors have motivated recent research efforts to explore object and human gesture recognition by using 3D information [5,6,10]. However, these methods for 3D depth-map-based hand gesture recognition have only applied the existing 2D feature descriptors to the depth images, such as Gabor filter bank [5] or contour matching [6].In order to directly and effectively capture and encode 3D shape information from depth maps, we propose a novel characteristic descriptor named Histogram of 3D Facets (H3DF). In 3D depth maps, a 3D cloud point together with its surrounding points is defined as a “3D Facet”, which includes the informative local surface pattern surrounding the cloud point. Each facet is modeled by a small plane. Then a spatial centric pooling strategy is applied to organize the collection of facet planes based on their normal orientations to describe the current region of interest (ROI), which forms the final H3DF descriptor. In applications of hand gesture recognition and human activity recognition, a region of interest may be an image patch describing a hand gesture or a body part. To integrate the static depth map descriptor with temporal information in depth video sequences, we propose two approaches: (1) we approximate the depth video sequence as an ordered collection of a number of representative frames. The optimal collection of representative frames is selected by minimizing a sequential loss function defined by using only selected frames to represent the whole video using Dynamic Programming (DP). (2) We capture and represent the local temporal structure patterns via N-gram modeling. The N-gram model can be viewed as a collection of “visual word transitions,” which is insensitive to different temporal structures caused by different execution rates.Compared to existing depth-map descriptors, our proposed H3DF depth-map descriptor has three advantages: (1) it explicitly captures the 3D shape patterns conveyed by depth maps. (2) It applies a compact representation to describe a depth map compared to other 2D feature descriptors, e.g. Histogram of Orientated Gradients (HOG) [1]. 3) Compared to existing surface normal-based descriptors such as HONV [20] and HON4D [18], H3DF utilizes a circular grid for spatial pooling to encode more information such as shape and local depth patterns, which implicitly manifests the importance of the center part and makes the descriptor more robust to external contour deformations. Compared with the earlier conference version of this paper [21] which only demonstrated the effectiveness H3DF for static hand gesture recognition, we further extend the proposed H3DF descriptor to handle temporal sequences for recognizing dynamic hand gestures and human activities. By utilizing dynamic programming-based temporal segmentation and N-gram-based representation [22], we generate more robust representations for depth video sequences by combining H3DF with temporal structure information. We evaluate the proposed descriptor on two public datasets of hand gesture recognition: the NTU hand digits dataset [6] and the ASL finger spelling dataset [5]; one dynamic hand gesture data set: the MSR 3D gesture dataset [16], and one popular action recognition dataset: the MSRAction3D [4]. The recognition results on all the tasks demonstrate that our approach outperforms or is comparable to state-of-the-art methods.The rest of this paper is organized as follows. Section 2 reviews the related work on depth map-based human action and hand gesture recognition. Section 3 describes the procedures of computing the H3DF descriptor and how to apply H3DF to static image-based hand gesture recognition. Section 4 presents the modeling and implementation details of dynamic programming-based temporal segmentation and N-gram-based temporal pattern exploration with the proposed H3DF. Section 5 provides the implementation of H3DF for sparse representation-based hand gesture and human action recognition. Experimental results and discussions are presented in Section 6. Finally, we conclude the remarks of this paper in Section 7.

@&#CONCLUSIONS@&#
In this paper, we have proposed a novel discriminative 3D descriptor (H3DF) which can effectively capture and model the rich surface shape information of the depth maps. Applying orientation normalization, robust coding and concentric spatial pooling, our H3DF descriptor is robust to translation, view angle and scaling changes. Local H3DF is also able to evolve into denseH3DF for modeling more local patterns. To tackle the task of dynamic hand gesture and human action recognition from depth video sequences, two temporal extension approaches are developed: dynamic programming-based temporal partition and N-gram-based method. The two approaches are applied to build augmented descriptors with robust representative description. We have extensively evaluated the effectiveness of the proposed H3DF descriptor on four public datasets including static hand gesture recognition from single depth image, dynamic hand gesture and human action recognition from depth sequences. The experimental results demonstrate that our proposed approach outperforms or achieves comparable accuracy to the state-of-the-art for action and hand gesture recognition.