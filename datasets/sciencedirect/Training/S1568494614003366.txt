@&#MAIN-TITLE@&#
On the entropy for Atanassov's intuitionistic fuzzy sets: An interpretation from the perspective of amount of knowledge

@&#HIGHLIGHTS@&#
Develop new entropy of A-IFSs to overcome the drawbacks the existing ones bring about.It can be used to distinguish some cases that cannot be done so by the existing ones.A real-life example is provided for a discussion on the application of the models.

@&#KEYPHRASES@&#
Atanassov's intuitionistic fuzzy sets (A-IFSs),Entropy,Amount of knowledge,Decision making,

@&#ABSTRACT@&#
Given the fact that totally unknown information cannot be expressed by Zadeh's fuzzy sets but can well be represented by Atanassov's intuitionistic fuzzy sets (A-IFSs), we provide in this paper a new idea, from the perspective of amount of knowledge, that the entropy for such information should be unique so that it may differ significantly from the entropy of a particular fuzzy element of which the membership and non-membership values are both equal to 0.5. Motivated by this idea, we present a more strict axiomatic definition of entropy on A-IFSs, and then develop a new entropy measure of A-IFSs that satisfies the refined axioms, with the aim of overcoming some drawbacks and ambiguities the existing entropy brings about and building a new entropy measurement model in the A-IFSs context to show some unique features of A-IFSs from the viewpoint of a specific purpose, notably those related to decision making. This allows us to capture the intrinsic amount of knowledge and the additional features that may be relevant when making decisions, and ultimately may be able to help us distinguish such particular cases in which the membership values are equal to the corresponding non-membership values in the A-IFSs context. Finally, a real-life example is provided for an in-depth discussion on the application of the developed models in decision making under uncertainty.

@&#INTRODUCTION@&#
Atanassov [1,2] initiated the concept of intuitionistic fuzzy sets (A-IFSs), which are characterized by a membership function and a non-membership function, thus generalizing Zadeh's [31] fuzzy sets (FSs) that only assign to each element a membership function. Motivated by interval-valued fuzzy sets (IVFSs) conceived by Zadeh [33], Atanassov and Gargov [3] further extended A-IFSs to interval-valued intuitionistic fuzzy sets (A-IVIFSs), which are characterized by membership functions and non-membership functions whose values are intervals rather than real numbers. After Atanassov's pioneering work, the theory of A-IFSs/A-IVIFSs has been extensively investigated from different aspects. So far, the A-IFSs/A-IVIFSs theory, for its excellent flexibility and agility in coping with vagueness or uncertainty, has been widely used in many areas, notably those related to decision making under uncertainty [8–10,14,25,27–29,38].Entropy, as a measure of fuzziness or uncertain information in the FSs theory, was first mentioned by Zadeh [32]. De Luca and Termini [6] presented the axioms with which the fuzzy entropy should comply, and defined the entropy of FSs based on Shannon's function. Kaufmann [13] pointed out that the entropy of a FS could be gotten through the distance between the FS and its nearest crisp set, while Yager [30] did it by the distance between the FS and its complement. Higashi and Klir [11] extended Yager's [30] concept to a very general class of fuzzy complements. Since then, as an active research topic, the investigation of entropy for FSs or A-IFSs has been receiving much attention from researchers [4,7,12,15–21,23,24,34–37]. Among these formulations there is a set of axioms proposed by Hung and Yang [12] for the entropy of A-IFSs from a probabilistic point of view, which differs from the other attempts in the basic structure of the axioms. In addition, Bustince and Burillo [4] were interested in modeling the hesitancy aspect of uncertainty, while Zhang et al. [37] handled the fuzziness aspect of uncertainty based on the Hausdorff metric. As for the connection between entropy and similarity in the A-IFSs context, Szmidt and Kacprzyk [18] believed that in general there was not a simple way of drawing any conclusion about one measure knowing the other. However, Farhadinia [7] and Wei et al. [24] took issue with them on this topic and showed that the entropy and the similarity could be transformed by each other in a general way; while Li et al. [15] characterized the sufficient conditions that these transformations should satisfy. In this paper, we are interested only in the non-probabilistic-type entropy and shall not go into further details of these formulations.It is worth noticing that most of the existing entropy of A-IFSs [15,17–20,34,35,37] cannot be used to distinguish such particular cases in which the membership values are equal to the corresponding non-membership values. This is mainly because in these cases the entropy of A-IFSs is all calculated to be the same value equal to 1, no mater what degrees of hesitancy are. A similar situation also occurs in the A-IVIFSs context [16,24,36]. The main reason for this, we think, is the need for the traditional consistency of entropy between the ordinary FSs and the A-IFSs. Szmidt et al. [22] indicated that this is a proper feature of entropy measure but a question naturally arises that if such a measure conveys all important knowledge from the viewpoint of decision making. Intuitively, in the particular cases above, an A-IFS with greater degree of hesitancy seems to be more uncertain given its larger amount of unknown information. After all, as far as the amount of knowledge conveyed by the A-IFS, the less we know about it, the fuzzier it is. We also note that by A-IFSs we may easily express our pure ignorance about something, but by ordinary FSs we cannot. In other words, the A-IFSs theory, despite a derivation from FSs, should have its own unique features. In this sense, some basic measurement model such as the entropy of A-IFSs may need further improving. Since there is essential difference between the two particular situations, one with the membership and non-membership values both equal to 0.5, and the other when we know absolutely nothing, it is necessary to develop a new measurement model of A-IFSs, from the perspective of amount of knowledge, to reveal this distinction and overcome some drawbacks and ambiguities the existing entropy brings about. These are the motivations and targets of this work.The rest of this paper is organized as follows: ‘A brief introduction to A-IFSs and A-IVIFSs’ section briefly recalls some basic notions of A-IFSs and A-IVIFSs, and ‘Recent studies on entropy for A-IFSs and A-IVIFSs’ section reviews recent studies on entropy for A-IFSs and A-IVIFSs, respectively. We then present, in ‘New entropy for A-IFSs from the perspective of amount of knowledge’ section, a more strict axiomatic definition of entropy on A-IFSs, based on which a new entropy measure of A-IFSs is developed from the perspective of amount of knowledge. After that, we further extend in section‘Extension of the developed entropy to A-IVIFSs’ the developed axioms and entropy measure from A-IFSs to A-IVIFSs. ‘Application in decision making under uncertainty’ section considers a practical application to illustrate and examine the developed entropy, followed by ‘Conclusions’ section.Atanassov [1,2] generalized Zadeh's [31] concept of FSs and defined the notion of A-IFSs as follows.Definition 1[1]. An A-IFS A in a finite set X is an object having the following form:Another parameter of A-IFSs is πA(x)=1−μA(x)−νA(x), known as A-IF index (or hesitation margin) of x∈A, and it expresses a lack of knowledge of whether x belongs to A or not. It is clearly seen that for ∀x∈X, 0≤πA(x)≤1. Obviously, when μA(x)=1−νA(x) for all elements of the universe of discourse, the concept of ordinary FSs is recovered.For any two A-IFSs A1 and A2 in X, the following relations and operations can be defined:1)A1⊆A2 iffμA1(x)≤μA2(x)andνA1(x)≥νA2(x)for ∀x∈X;A1=A2 iff A1⊆A2 and A1⊇A2;The complement setA1c=x,νA1(x),μA1(x)|x∈X;The tripletμA1(x),νA1(x),πA1(x)is called an Atanassov's intuitionistic fuzzy value (A-IFV).As an important measure of A-IFSs, the normalized Hamming distance between any two A-IFSs A1 and A2 inX=xii=1,2,…,n, the so-called Szmidt and Kacprzyk form of A-IF Hamming distance [2], is defined as(1)dAIFSA1,A2=12n∑i=1nμA1xi−μA2xi+νA1xi−νA2xi+πA1xi−πA2xi.Atanassov and Gargov [3] further extended A-IFSs to A-IVIFSs as follows.Definition 2[3]. An A-IVIFSA˜in a finite set X is an object having the following form:Similarly, for any two A-IVIFSsA˜1andA˜2in X, we define the following relations and operations:1)A˜1⊆A˜2iffμA˜1L(x)≤μA˜2L(x),μA˜1U(x)≤μA˜2U(x),νA˜1L(x)≥νA˜2L(x)andνA˜1U(x)≥νA˜2U(x)for ∀x∈X;A˜1=A˜2iffA˜1⊆A˜2andA˜1⊇A˜2;The complement setA˜1c=x,ν˜A˜1(x),μ˜A˜1(x)|x∈X;The tripletμ˜A˜1(x),ν˜A˜1(x),π˜A˜1(x)is called an Atanassov's interval-valued intuitionistic fuzzy value (A-IVIFV).The normalized Hamming distance between any two A-IVIFSsA˜1andA˜2inX=xii=1,2,…,nis defined as(2)dAIVIFS(A˜1,A˜2)=14n∑i=1nμA˜1L(xi)−μA˜2L(xi)+μA˜1U(xi)−μA˜2U(xi)+νA˜1L(xi)−νA˜2L(xi)+νA˜1U(xi)−νA˜2U(xi)+πA˜1L(xi)−πA˜2L(xi)+πA˜1U(xi)−πA˜2U(xi),LetX=xii=1,2,…,nbe a universe of discourse. We denote the family of all A-IFSs in X by AIFS(X), and the family of all A-IVIFSs in X by AIVIFS(X). Szmidt and Kacprzyk [19] extended De Luca and Termini's [6] axiomatic definition of entropy into A-IFSs as follows.Definition 3[19]. A real function E:AIFS(X)→[0, 1] is called an entropy on AIFS(X), if E has the following properties:(EPAIFS1′) E(A)=0 iff A is a crisp set;(EPAIFS2′) E(A)=1 iff μA(xi)=νA(xi) for ∀xi∈X;(EPAIFS3′) E(A)≤E(B) if A is less fuzzy than B, i.e.,μA(x)≤μB(x) and νA(x)≥νB(x) for μB(xi)≤νB(xi), ∀xi∈X,μA(x)≥μB(x) and νA(x)≤νB(x) for μB(xi)≥νB(xi), ∀xi∈X;(EPAIFS4′) E(A)=E(Ac).For ∀A∈AIFS(X), we review here some different entropy as follows, which all satisfy the axioms EPAIFS1′–4′ in Definition 3 except Bustince and Burillo [4]. It should be noted that the notations used in [4,19,34] have been changed in order to be consistent with the others in this paper.Bustince and Burillo [4] gave a different axiomatic definition of entropy that allowed us to measure the degree of intuitionism of A-IFSs but could not measure the fuzziness of A-IFSs, because there are many different possibilities of combination between a membership function and a non-membership function for each fixed hesitation margin. In other words, their definition basically considers the entropy as a measure of how far the A-IFS is from being a FS. Their normalized entropy is shown as(3)Ebb(A)=1n∑i=1nπA(xi).Starting from the geometrical interpretation of A-IFSs and the meaning of entropy, Szmidt and Kacprzyk [19] defined a non-probabilistic-type entropy for A-IFSs as(4)Esk,1(A)=dAIFS(A,Anear)dAIFS(A,Afar)=1n∑i=1nminμA(xi),νA(xi)+πA(xi)maxμA(xi),νA(xi)+πA(xi).In fact, such a measure actually examines how far the A-IFS is from being a crisp set, and thus is defined to model the fuzziness aspect of uncertainty.Later, Szmidt and Kacprzyk [17,20] proposed another entropy based on the distance between an A-IFS and its complement, which is simpler in the sense of calculation than the previous one, shown as(5)Esk,2(A)=1−12dAIFS(A,Ac)=1−12n∑i=1nμA(xi)−νA(xi).Zeng and Li [34] expressed the axioms of Szmidt and Kacprzyk [19] using the notation of IVFSs and discussed the relationship between entropy and similarity. They defined the entropy for A-IFSs as(6)Ezl(A)=1−1n∑i=1nμA(xi)−νA(xi).Li et al. [15] also investigated the relationship between entropy and similarity in the A-IFSs context. They developed the entropy from a similarity of A-IFSs, defined as(7)Eldl(A)=1−12n∑i=1nμA(xi)−νA(xi)3+μA(xi)−νA(xi).It should be noted that the above measures (5)–(7) make full use of μA(xi) and νA(xi) but not πA(xi). In other words, these measures could not capture all facets of uncertainty associated with an A-IFS. In fact, Szmidt and Kacprzyk [21] stressed the necessity of taking into account all three functions (namely membership, non-membership, and hesitancy) in description of A-IFSs while considering entropy. They further pointed out that omitting hesitancy in the measure of entropy, distance, or similarity may lead to counter-intuitive results. Consider four single elements xi(i=1, 2, 3, 4) of an A-IFSs, for example, each of which is described via an A-IFV, namely, x1=(0.7, 0.3, 0), x2=(0.6, 0.2, 0.2), x3=(0.5, 0.1, 0.4), x4=(0.4, 0, 0.6).The entropy by the above measures respectively gives the following results:Esk,2(xi)=0.8,Ezl(xi)=0.6,Eldl(xi)=0.768,i=1,2,3,4.It is clear that none of these measures can be used to definitely distinguish xi(i=1, 2, 3, 4) in the sense of entropy although these elements are obviously different.Zhang and Jiang [35] developed a new non-probabilistic-type entropy of vague sets by means of the intersection and union of membership functions and non-membership functions, defined as(8)Ezj(A)=1n∑i=1nminμA(xi),νA(xi)maxμA(xi),νA(xi).Owing to the ignorance of πA(xi), this measure cannot be called a good one, either. Consider the following elements, for example, i.e.,y1=(0, 0.6, 0.4), y2=(0, 0.5, 0.5), y3=(0, 0.4, 0.6), y4=(0, 0.3, 0.7).From the measure (8), we easily have Ezj(yi)=0 (i=1, 2, 3, 4), which are obviously incorrect results as if these elements were all crisp ones.In addition, Szmidt and Kacprzyk [18] pointed out that the conditions of EPAIFS3 in Definition 3 were not sufficient to assess the entropy for any possible A-IF element. Instead, they gave a weaker alternative one which was consistent with the geometrical representation of A-IFSs and expressed as(9)mindAIFS(M,Ai),dAIFS(N,Ai)≤mindAIFS(M,Bi),dAIFS(N,Bi),i=1,2,…,n,where dAIFS(·, ·) means a distance between the point M=(1, 0, 0) or N=(0, 1, 0) representing crisp element, and each ith elementAi=xi,μA(xi),νA(xi)(or Bi)(i=1, 2, …, n) from the A-IFS A(or B). In fact, we can further simplify the inequality (9) as follows. Note thatdAIFS(M,Ai)=1−μA(xi),dAIFS(N,Ai)=1−νA(xi),dAIFS(M,Bi)=1−μB(xi),dAIFS(N,Bi)=1−νB(xi).The inequality (9) can then be rewritten asmin1−μA(xi),1−νA(xi)≤min1−μB(xi),1−νB(xi)⇔1−maxμA(xi),νA(xi)≤1−maxμB(xi),νB(xi).With this result, we get the simplified version of the inequality (9) as(10)maxμA(xi),νA(xi)≥maxμB(xi),νB(xi),i=1,2,…,n.That is to say, for ∀A, B∈AIFS(X), we may have E(A)≤E(B) when the condition of the inequality (10) is satisfied. In other words, an A-IFS is less certain with greater value of membership or non-membership, no matter how it tends to the ideal positive point M or to the ideal negative point N. It seems to be reasonable. Unfortunately, this weaker condition is not exactly correct. Take two single-element A-IFSsA=x,0.39,0.44andB=x,0.26,0.07for example. It is obvious that the two A-IFSs satisfy the weaker condition (10), namely νA(x)=0.44>0.26=μB(x). By using the measures (4)–(8), we may respectively get the different entropy of them, E(A) and E(B), but they are all characterized by E(A)>E(B).By comparison, few studies in the literature deal with the entropy of A-IVIFSs. Liu et al. [16] gave the axiomatic definition of entropy on A-IVIFSs below.Definition 4[16]. A real function E:AIVIFS(X)→[0, 1] is named an entropy on AIVIFS(X), if E has the following properties:(EPAIVIFS1′)E(A˜)=0iffA˜is a crisp set;(EPAIVIFS2′)E(A˜)=1iffμ˜A˜(xi)=ν˜A˜(xi)for ∀xi∈X;(EPAIVIFS3′)E(A˜)≤E(B˜)ifA˜is less fuzzy thanB˜, i.e.,A˜⊆B˜forμB˜L(xi)≤νB˜L(xi)andμB˜U(xi)≤νB˜U(xi), ∀xi∈X,A˜⊇B˜forμB˜L(xi)≥νB˜L(xi)andμB˜U(xi)≥νB˜U(xi), ∀xi∈X;(EPAIVIFS4′)E(A˜)=E(A˜c).For∀A˜∈AIVIFS(X), we review here some different entropy as follows, which all satisfy the axioms EPAIVIFS1′–4′ in Definition 4. Also, the notations used in [16,36] have been changed accordingly.Liu et al. [16] extended the entropy (4) as(11)Elzx(A˜)=∑i=1n2−maxμA˜L(xi),νA˜L(xi)−maxμA˜U(xi),νA˜U(xi)∑i=1n2−minμA˜L(xi),νA˜L(xi)−minμA˜U(xi),νA˜U(xi).However, this measure cannot recover to the measure (4) when the A-IVIFSA˜reduces to an A-IFS.Zhang et al. [36] extended the entropy (8) as(12)Ezjj(A˜)=1n∑i=1nminμA˜L(xi),νA˜L(xi)+minμA˜U(xi),νA˜U(xi)maxμA˜L(xi),νA˜L(xi)+maxμA˜U(xi),νA˜U(xi).Wei et al. [24] also independently generalized the entropy (4) as(13)Ewwz(A˜)=1n∑i=1nminμA˜L(xi),νA˜L(xi)+minμA˜U(xi),νA˜U(xi)+πA˜L(xi)+πA˜U(xi)maxμA˜L(xi),νA˜L(xi)+maxμA˜U(xi),νA˜U(xi)+πA˜L(xi)+πA˜U(xi).It is obvious that if the A-IVIFSA˜reduces to an A-IFS, then the measures (12) and (13) can naturally recover to (8) and (4), respectively.It can be shown that given any value ofπ˜A˜(xi), the entropy ofA˜by the measures (11)–(13) will consistently reach the maximum equal to 1 forμ˜A˜(xi)=ν˜A˜(xi). Obviously, we can’t definitely distinguish some particular A-IVIFSs like this in the sense of entropy by these measures that may suffer from great limitations in some particular cases when solving real problems.In addition, Vlachos [23] argued that a good entropy measure of FSs should possess a desirable property that the average amount of total entropy on the separate elements Ai(i=1, 2, …, n) from a FS A∈FS(X) should be equal to the entropy on A as a whole, that is,(14)E(A)=1n∑i=1nE(Ai),whereAi=xi,μA(xi)represents the ith element from the FS A, and FS(X) denotes the family of all FSs in X. Obviously, the functional form of entropy shown as the formulas (11) violates the desirable property (14).It is clear from above that by the traditional entropy we cannot definitely distinguish some particular A-IFSs or A-IVIFSs of which the membership values are equal to the corresponding non-membership values. Szmidt et al. [22] tried to tackle this issue with the amount of knowledge conveyed by A-IFSs. In particular, they were interested in making the difference between the following situations:-We have no information at all, andWe have a large number of arguments in favor but an equally large number of arguments in disapproval.By combining the entropy (4) with the hesitation margin, they finally achieved the goal. However, their technique seems to suffer from some limitations mainly concerning the logic of comparison with value for AIFS(X) between the entropy and the resulting amount of knowledge. Unlike them, we manage to handle this issue only by the corrected entropy. Just as mentioned before, there is essential difference between the two situations from the perspective of amount of knowledge. Thus, a new entropy measure of A-IFSs may need to be developed to reveal this distinction or to show some unique features of A-IFSs. In order to do that, we may loosen the constraint on the traditional consistency of entropy between the ordinary FSs and the A-IFSs. Specifically, when measuring the entropy of A-IFSs we may not need to be strongly tied to the consistency that requires the both situations above are of the same entropy equal to 1. With this understanding in mind, for a A-IFS A∈AIFS(X) we consider the following changes to the existing axioms of entropy that (1) the entropy of A should reach the maximum equal to 1 if and only if πA(xi)=1 for ∀xi∈X (i.e., we know absolutely nothing), and (2) in the case of μA(xi)=νA(xi)≠0 (i.e., we have a number of arguments in favor but an equally number of arguments in favor of the opposite statement), the greater value of the associated πA(xi), the fuzzier the corresponding element Ai. These results seem to be more informative and rational from a practical point of view, especially for decision making in real world applications.Based on these analyses, we develop here a new entropy measure of A-IFSs as follows. We start with the following refined axioms that the new entropy of A-IFSs should satisfy.Definition 5A real function E:AIFS(X)→[0, 1] is called an entropy on AIFS(X), if E has the following properties:(EPAIFS1) E(A)=0 iff A is a crisp set;(EPAIFS2) E(A)=1 iff μA(xi)=νA(xi)=0 for ∀xi∈X;(EPAIFS3) E(A)≤E(B) if A is less fuzzy than B, i.e.,A⊆B for μB(xi)≤νB(xi), ∀xi∈X,A⊇B for μB(xi)≥νB(xi), ∀xi∈X;(EPAIFS4) E(A)=E(Ac).We then present, based on the distance between an A-IFS and its complement in combination with the hesitancy associated with it, the new entropy of A-IFSs for a separate element Ai(i=1, 2, …, n) from an A-IFS A∈AIFS(X), i.e.,(15)EAIFS(Ai)=1−dAIFS(Ai,Aic)1+πA(xi)2=1−μA(xi)−νA(xi)1+πA(xi)2,i=1,2,…,n,wheredAIFS(Ai,Aic)is the distance (1) between the ith elementAi=xi,μA(xi),νA(xi)from A, and its complement. It can be shown that EAIFS(Ai)∈[0, 1] for all i=1, 2, …, n. Obviously, our analyses are concerned with the relationship between two facets of uncertainty of an A-IFS, i.e., the fuzziness and lack of knowledge, thus capturing the intrinsic features in the A-IFS context.The formula (15) describes the entropy for a single element belonging to an A-IFS. For ∀A∈AIFS(X), we have(16)EAIFS(A)=1n∑i=1nEAIFS(Ai)=1n∑i=1n1−μA(xi)−νA(xi)1+πA(xi)2.Theorem 1Let A∈AIFS(X). A real function EAIFS(A)∈[0, 1] defined by the formula (16) is an entropy for A-IFSs.Definitely, as a meaningful entropy of A-IFSs, it should satisfy the axioms EPAIFS1–4 in Definition 5.(EPAIFS1): Let A be a crisp set. We then have μA(xi)=1 or νA(xi)=1 for ∀xi∈X. From the formula (16), we easily have EAIFS(A)=0. On the other hand, suppose EAIFS(A)=0. Given that1+πA(xi)/2∈[0.5,1], it can only be deduced from the formula (16) that1−μA(xi)−νA(xi)=0for ∀xi∈X, which means μA(xi)=1 or νA(xi)=1 for ∀xi∈X and therefore implies A is a crisp set.(EPAIFS2): Let μA(xi)=νA(xi)=0 for ∀xi∈X. From the formula (16) we easily have EAIFS(A)=1. We now suppose EAIFS(A)=1. From the formula (16) we get1−μA(xi)−νA(xi)1+πA(xi)2=1⇔1−μA(xi)−νA(xi)1+πA(xi)=2⇔πA(xi)−μA(xi)−νA(xi)1+πA(xi)=1⇔μA(xi)−νA(xi)1+πA(xi)=−1−πA(xi).In view of the ranges of three functions (namely membership, non-membership, and hesitancy), to maintain the above equality we have to let μA(xi)=νA(xi)=0 and πA(xi)=1 for ∀xi∈X.(EPAIFS3): For ∀A, B∈AIFS(X), let A⊆B for μB(xi)≤νB(xi), ∀xi∈X. Thus we have the inequality μA(xi)≤μB(xi)≤νB(xi)≤νA(xi) for ∀xi∈X. From the formula (16), we have(17)EAIFS(B)−EAIFS(A)=1n∑i=1n1−μB(xi)−νB(xi)1+πB(xi)2−1−μA(xi)−νA(xi)1+πA(xi)2=1n∑i=1n1+μB(xi)−νB(xi)2−μB(xi)−νB(xi)2−1+μA(xi)−νA(xi)2−μA(xi)−νA(xi)2=1n∑i=1n1+μB(xi)−νB(xi)1−μB(xi)2−νB(xi)2−1+μA(xi)−νA(xi)1−μA(xi)2−νA(xi)2=1n∑i=1n1+μB(xi)2−3νB(xi)2−μB2(xi)2+νB2(xi)2−1+μA(xi)2−3νA(xi)2−μA2(xi)2+νA2(xi)2=1n∑i=1n12μB(xi)−μA(xi)−32νB(xi)−νA(xi)−12μB2(xi)−μA2(xi)+12νB2(xi)−νA2(xi)=1n∑i=1n12μB(xi)−μA(xi)1−μB(xi)−μA(xi)+12νB(xi)−νA(xi)νB(xi)+νA(xi)−3.Since μA(xi)≤νA(xi)∧μB(xi)≤νB(xi)⇒μA(xi)≤0.5∧μB(xi)≤0.5 for ∀xi∈X, we then get μA(xi)+μB(xi)≤1, ∀xi∈X. Thus the formula (17) is non-negative, i.e. EAIFS(A)≤EAIFS(B) holds.Similarly, as for A⊇B for μB(xi)≥νB(xi)(∀xi∈X), we also have EAIFS(A)≤EAIFS(B).(EPAIFS4): Trivial from the definition of Ac. □Let us return to the aforementioned elements expressed as A-IFVs, i.e., x1=(0.7, 0.3, 0), x2=(0.6, 0.2, 0.2), x3=(0.5, 0.1, 0.4), x4=(0.4, 0, 0.6); y1=(0, 0.6, 0.4), y2=(0, 0.5, 0.5), y3=(0, 0.4, 0.6), y4=(0, 0.3, 0.7).From the formula (16) we easily getEAIFS(x1)=0.30,EAIFS(x2)=0.36,EAIFS(x3)=0.42,EAIFS(x4)=0.48EAIFS(y1)=0.28,EAIFS(y2)=0.375,EAIFS(y3)=0.48EAIFS(y4)=0.595.Consider now the following particular elements, namely, z1=(0.5, 0.5, 0), z2=(0.3, 0.3, 0.4), z3=(0.2, 0.2, 0.6), z4=(0, 0, 1).From the formula (16) we easily getEAIFS(z1)=0.5,EAIFS(z2)=0.7,EAIFS(z3)=0.8,EAIFS(z4)=1.It is obvious that these results clearly show the differences between these A-IFSs in terms of the intrinsic amount of knowledge associated with them, as we would expect. Thus, the developed measure is, as Szmidt and Kacprzyk [21] commented on their work [19], not only mathematically correct but simultaneously rendering the sense of entropy not as a pure mathematical construction but as a measure to be useful in real world applications.Based on the idea hidden in the formula (15), we then extend this new entropy into the general A-IVIF situation. Similarly, we start with the following refined axioms that the new entropy of A-IVIFSs should satisfy.Definition 6A real function E:AIVIFS(X)→[0, 1] is called an entropy on AIVIFS(X), if E has the following properties:(EPAIVIFS1)E(A˜)=0iffA˜is a crisp set;(EPAIVIFS2)E(A˜)=1iffμ˜A˜(xi)=ν˜A˜(xi)=[0,0]for ∀xi∈X;(EPAIVIFS3)E(A˜)≤E(B˜)ifA˜is less fuzzy thanB˜, i.e.A˜⊆B˜forμB˜L(xi)≤νB˜L(xi)andμB˜U(xi)≤νB˜U(xi), ∀xi∈X,A˜⊇B˜forμB˜L(xi)≥νB˜L(xi)andμB˜U(xi)≥νB˜U(xi), ∀xi∈X;(EPAIVIFS4)E(A˜)=E(A˜c).We then present the new entropy of A-IVIFSs for a separate elementA˜i(i=1,2,…,n)from an A-IVIFSA˜∈AIVIFS(X), that is(18)EAIVIFS(A˜i)=1−dAIVIFS(A˜i,A˜ic)1+0.5πA˜L(xi)+πA˜U(xi)2=1−12μA˜L(xi)−νA˜L(xi)+μA˜U(xi)−νA˜U(xi)1+0.5πA˜L(xi)+πA˜U(xi)2,i=1,2,…,n,wheredAIVIFS(A˜i,A˜ic)is the distance (2) between the ith elementA˜i=xi,μ˜A˜(xi),ν˜A˜(xi)fromA˜, and its complement. It can be shown thatEAIVIFS(A˜i)∈[0,1]for all i=1, 2, …, n.The formula (18) describes the entropy for a single element belonging to an A-IVIFS. For∀A˜∈AIVIFS(X), we have(19)EAIVIFS(A˜)=1n∑i=1nEAIVIFS(A˜i)=1n∑i=1n1−12μA˜L(xi)−νA˜L(xi)+μA˜U(xi)−νA˜U(xi)1+0.5πA˜L(xi)+πA˜U(xi)2.Theorem 2LetA˜∈AIVIFS(X). A real functionEAIVIFS(A˜)∈[0,1]defined by the formula (19) is an entropy for A-IVIFSs.Definitely, as a meaningful entropy measure of A-IVIFSs, it should satisfy the axioms EPAIVIFS1–4 in Definition 6.(EPAIVIFS1): LetA˜be a crisp set. Then we haveμA˜L(xi)=μA˜U(xi)=1orνA˜L(xi)=νA˜U(xi)=1for ∀xi∈X. From the formula (19), we easily haveEAIVIFS(A˜)=0. On the other hand, supposeEAIVIFS(A˜)=0. Given that1+0.5πA˜L(xi)+πA˜U(xi)/2∈[0.5,1], it can only be deduced from the formula (19) that1−0.5μA˜L(xi)−νA˜L(xi)+μA˜U(xi)−νA˜U(xi)=0for ∀xi∈X, which meansμA˜L(xi)=μA˜U(xi)=1orνA˜L(xi)=νA˜U(xi)=1for ∀xi∈X and therefore impliesA˜is a crisp set.(EPAIVIFS2): Letμ˜A˜(xi)=ν˜A˜(xi)=[0,0]for ∀xi∈X. From the formula (19) we easily haveEAIVIFS(A˜)=1. We now supposeEAIVIFS(A˜)=1. From the formula (19), we get1−12μA˜L(xi)−νA˜L(xi)+μA˜U(xi)−νA˜U(xi)1+0.5πA˜L(xi)+πA˜U(xi)2=1⇔1−12μA˜L(xi)−νA˜L(xi)+μA˜U(xi)−νA˜U(xi)1+12πA˜L(xi)+πA˜U(xi)=2⇔12μA˜L(xi)−νA˜L(xi)+μA˜U(xi)−νA˜U(xi)1+12πA˜L(xi)+πA˜U(xi)=12πA˜L(xi)+πA˜U(xi)−1.In view of the ranges of three functions (namely membership, non-membership, and hesitancy), to maintain the above equality we have to letμ˜A˜(xi)=ν˜A˜(xi)=[0,0]andπ˜A˜(xi)=[1,1]for ∀xi∈X.(EPAIVIFS3): For∀A˜,B˜∈AIVIFS(X), letA˜⊆B˜forμB˜L(xi)≤νB˜L(xi)andμB˜U(xi)≤νB˜U(xi), ∀xi∈X. Thus we have the inequalitiesμA˜L(xi)≤μB˜L(xi)≤νB˜L(xi)≤νA˜L(xi)andμA˜U(xi)≤μB˜U(xi)≤νB˜U(xi)≤νA˜U(xi)for ∀xi∈X. From the formula (19), we haveEAIVIFS(A˜)=1n∑i=1n1−12μA˜L(xi)−νA˜L(xi)+μA˜U(xi)−νA˜U(xi)1+0.5πA˜L(xi)+πA˜U(xi)2=1n∑i=1n1−12νA˜L(xi)−μA˜L(xi)+νA˜U(xi)−μA˜U(xi)1+0.51−μA˜U(xi)−νA˜U(xi)+1−μA˜L(xi)−νA˜L(xi)2=1n∑i=1n1+14μA˜L(xi)+μA˜U(xi)−34νA˜L(xi)+νA˜U(xi)+18νA˜L(xi)+νA˜U(xi)2−μA˜L(xi)+μA˜U(xi)2.Similarly,EAIVIFS(B˜)=1n∑i=1n1+14μB˜L(xi)+μB˜U(xi)−34νB˜L(xi)+νB˜U(xi)+18νB˜L(xi)+νB˜U(xi)2−μB˜L(xi)+μB˜U(xi)2.We can then calculate(20)EAIVIFS(B˜)−EAIVIFS(A˜)=1n∑i=1n14μB˜L(xi)+μB˜U(xi)−34νB˜L(xi)+νB˜U(xi)−14μA˜L(xi)+μA˜U(xi)+34νA˜L(xi)+νA˜U(xi)+18νB˜L(xi)+νB˜U(xi)2−μB˜L(xi)+μB˜U(xi)2−18νA˜L(xi)+νA˜U(xi)2−μA˜L(xi)+μA˜U(xi)2=1n∑i=1n14μB˜L(xi)+μB˜U(xi)−μA˜L(xi)−μA˜U(xi)−34νB˜L(xi)+νB˜U(xi)−νA˜L(xi)−νA˜U(xi)+18νB˜L(xi)+νB˜U(xi)2−νA˜L(xi)+νA˜U(xi)2−18μB˜L(xi)+μB˜U(xi)2−μA˜L(xi)+μA˜U(xi)2=1n∑i=1n14μB˜L(xi)+μB˜U(xi)−μA˜L(xi)−μA˜U(xi)−34νB˜L(xi)+νB˜U(xi)−νA˜L(xi)−νA˜U(xi)+18νB˜L(xi)+νB˜U(xi)+νA˜L(xi)+νA˜U(xi)νB˜L(xi)+νB˜U(xi)−νA˜L(xi)−νA˜U(xi)−18μB˜L(xi)+μB˜U(xi)+μA˜L(xi)+μA˜U(xi)μB˜L(xi)+μB˜U(xi)−μA˜L(xi)−μA˜U(xi)=14n∑i=1nμB˜L(xi)+μB˜U(xi)−μA˜L(xi)−μA˜U(xi)1−12μB˜L(xi)+μB˜U(xi)+μA˜L(xi)+μA˜U(xi)+νA˜L(xi)+νA˜U(xi)−νB˜L(xi)−νB˜U(xi)3−12νA˜L(xi)+νA˜U(xi)+νB˜L(xi)+νB˜U(xi).SinceμA˜U(xi)≤νA˜U(xi)∧μB˜U(xi)≤νB˜U(xi)⇒μA˜U(xi)≤0.5∧μB˜U(xi)≤0.5for ∀xi∈X, we certainly haveμA˜U(xi)+μB˜U(xi)≤1andμB˜L(xi)+μB˜U(xi)+μA˜L(xi)+μA˜U(xi)≤2for ∀xi∈X. Thus the formula (20) is non-negative, i.e.EAIVIFS(A˜)≤EAIVIFS(B˜)holds.Similarly, as forA˜⊇B˜forμB˜L(xi)≥νB˜L(xi)andμB˜U(xi)≥νB˜U(xi)(∀xi∈X), we also haveEAIVIFS(A˜)≤EAIVIFS(B˜).(EPAIVIFS4): Trivial from the definition ofA˜c. □Let us extend the aforementioned A-IF elements zito the A-IVIF onesz˜i(i=1, 2, 3, 4), i.e.,z˜1=([0.5,0.5],[0.5,0.5],[0,0]),z˜2=([0.3,0.4],[0.3,0.4],[0.2,0.4]),z˜3=([0.1,0.2],[0.1,0.2],[0.6,0.8]),z˜4=([0,0],[0,0],[1,1]).From the formula (19), we getEAIVIFS(z˜1)=0.5,EAIVIFS(z˜2)=0.65,EAIVIFS(z˜3)=0.85,EAIVIFS(z˜4)=1.These results clearly show, again, the distinctions betweenz˜i(i=1, 2, 3, 4) in terms of the intrinsic amount of knowledge associated with them, as we would expect.It is obvious that when an A-IVIFSA˜reduces to an A-IFS, then the entropy (18) and (19) reduces to (15) and (16), respectively. Also, our developed entropy forms shown as the formulas (16) and (19) follow the desirable property (14), thus we may say for sure that the two types of entropy are well defined.The entropy we discuss here can be used in multi-attribute decision making (MADM) for the attribute weight in terms of the information content of attribute, so called the entropy weight on attribute [5]. Generally speaking, the more the information content of an attribute, the less the entropy of the attribute and the greater the entropy weight on this attribute. In this section, a real-life example (adapted from Xu and Yager [29]) is provided for an in-depth discussion on the application of the developed models in decision making under uncertainty.Example 1Located in Central China and the middle reaches of the Changjiang (Yangtze) River, Hubei Province is distributed in a transitional belt where physical conditions and landscapes are on the transition from north to south and from east to west. Thus, Hubei Province is well known as “a land of rice and fish” since the region enjoys some of the favorable physical conditions, with a diversity of natural resources and the suitability for growing various crops. At the same time, however, there are also some restrictive factors for developing agriculture such as a tight man–land relation between a constant degradation of natural resources and a growing population pressure on land resource reserve. Despite cherishing a burning desire to promote their standard of living, people living in the area are frustrated because they have no ability to enhance their power to accelerate economic development because of a dramatic decline in quantity and quality of natural resources and a deteriorating environment. Based on the distinctness and differences in environment and natural resources, Hubei Province can be roughly divided into seven agro-ecological regions:a1 – Wuhan–Ezhou–Huanggang; a2 – Northeast of Hubei; a3 – Southeast of Hubei; a4 – Jianghan region; a5 – North of Hubei; a6 – Northwest of Hubei; a7 – Southwest of Hubei.In order to prioritize these agro-ecological regions a1 (i=1, 2, …, 7) according to their comprehensive functions, a committee comprised of three DMs dk(k=1, 2, 3) has been formed with a weighting vector λ=(0.5, 0.2, 0.3)T. The attributes which are considered here in the assessment of ai(i=1, 2, …, 7) are: c1 – ecological benefit; c2 – economic benefit; c3 – social benefit.Assume that the importance of the attributes cj(j=1, 2, 3) is completely unknown. The individual opinions of the DM dkon the agro-ecological regions aiwith respect to the attribute cjare expressed as an individual decision matrixR(k)=rij(k)7×3whererij(k)=μij(k),νij(k),πij(k)(i=1, 2, …, 7; j, k=1, 2, 3) are A-IFVs, i.e.,R(1)=(0.8,0.1,0.1)(0.9,0.1,0.0)(0.7,0.2,0.1)(0.7,0.3,0.0)(0.6,0.2,0.2)(0.6,0.1,0.3)(0.5,0.4,0.1)(0.7,0.3,0.0)(0.6,0.1,0.3)(0.9,0.1,0.0)(0.7,0.1,0.2)(0.8,0.2,0.0)(0.6,0.1,0.3)(0.8,0.2,0.0)(0.5,0.1,0.4)(0.3,0.6,0.1)(0.5,0.4,0.1)(0.4,0.5,0.1)(0.5,0.2,0.3)(0.4,0.6,0.0)(0.5,0.5,0.0),R(2)=(0.9,0.1,0.0)(0.8,0.2,0.0)(0.8,0.1,0.1)(0.8,0.2,0.0)(0.5,0.1,0.4)(0.7,0.2,0.1)(0.5,0.5,0.0)(0.7,0.2,0.1)(0.8,0.2,0.0)(0.9,0.1,0.0)(0.9,0.1,0.0)(0.7,0.3,0.0)(0.5,0.2,0.3)(0.6,0.3,0.1)(0.6,0.2,0.2)(0.4,0.6,0.0)(0.3,0.4,0.3)(0.5,0.5,0.0)(0.3,0.5,0.2)(0.5,0.3,0.2)(0.6,0.4,0.0),R(3)=(0.7,0.1,0.2)(0.9,0.1,0.0)(0.9,0.1,0.0)(0.9,0.1,0.0)(0.6,0.2,0.2)(0.6,0.2,0.2)(0.4,0.5,0.1)(0.8,0.1,0.1)(0.7,0.1,0.2)(0.8,0.1,0.1)(0.7,0.2,0.1)(0.9,0.1,0.0)(0.6,0.3,0.1)(0.8,0.2,0.0)(0.7,0.2,0.1)(0.2,0.7,0.1)(0.5,0.1,0.4)(0.3,0.1,0.6)(0.4,0.6,0.0)(0.7,0.3,0.0)(0.5,0.5,0.0).In the following our developed entropy (16) is used to derive the attribute weighting vectorw, with which the alternative agro-ecological regions are finally ranked.Step 1. Aggregate all of the individual opinions,R(k)=(rij(k))7×3(k=1, 2, 3), into a group one,R=rij7×3by using the intuitionistic fuzzy weighted averaging (IFWA) operator [26], whererij=IFWAλrij(1),rij(2),rij(3)=1−∏k=131−μij(k)λk,∏k=13νij(k)λk,∏k=131−μij(k)λk−∏k=13νij(k)λk.Thus the group opinion is shown asR=(0.803,0.100,0.097)(0.885,0.115,0.000)(0.801,0.141,0.058)(0.801,0.199,0.000)(0.582,0.174,0.244)(0.622,0.141,0.236)(0.472,0.447,0.081)(0.734,0.199,0.067)(0.681,0.115,0.205)(0.877,0.100,0.023)(0.759,0.123,0.118)(0.824,0.176,0.000)(0.582,0.160,0.259)(0.770,0.217,0.013)(0.590,0.141,0.269)(0.294,0.628,0.078)(0.465,0.264,0.271)(0.394,0.309,0.297)(0.435,0.334,0.231)(0.530,0.424,0.046)(0.522,0.478,0.000).Step 2. Calculate the fuzzy entropy for each attribute cj(j=1, 2, 3) by using the formula (16). After dual normalization of the entropy, the set of entropy weights on the attributes cj(j=1, 2, 3) can be derived and denoted by an attribute weighting vectorw=(0.332,0.342,0.326)T.Step 3. With the obtained w andR=(rij)7×3, make a group assessment rion each alternative ai(i=1, 2, …, 7) by using the IFWA operator again, whereri=IFWAwri1,ri2,ri3=1−∏j=13(1−μij)wj,∏j=13(νij)wj,∏j=13(1−μij)wj−∏j=13(νij)wj,i=1,2,…,7.Thus the group assessments on the agro-ecological regions are shown asr1=(0.836,0.117,0.047),r2=(0.684,0.170,0.146),r3=(0.646,0.218,0.137),r4=(0.826,0.129,0.045),r5=(0.661,0.170,0.168),r6=(0.389,0.370,0.241),r7=(0.498,0.407,0.095).Step 4. In order to rank the alternatives, evaluate the values of ri(i=1, 2, …, 7) by the following technique [8]ZAIFV(ri)=1−12πriμri+12πri,i=1,2,…,7,where the larger the value of ZAIFV(ri)∈[0, 1], the better the A-IFV ri. Thus we haveZAIFVr1=0.839,ZAIFVr2=0.702,ZAIFVr3=0.665,ZAIFVr4=0.829,ZAIFVr5=0.683,ZAIFVr6=0.448,ZAIFVr7=0.519.Step 5. Finally, rank all agro-ecological regions with respect to the set of attributes in terms of the values ofZAIFVri(i=1, 2, …, 7), i.e., a1≻a4≻a2≻a5≻a3≻a7≻a6.With the weighting vectors pre-assigned by subjectivity, Xu and Yager [29] ranked all of the alternatives ai(i=1, 2, …, 7) by the combination of the dynamic intuitionistic fuzzy weighted averaging (DIFWA) operator and the closeness coefficients. It is worth noticing that our ranking list is exactly the same as theirs.To further examine the effectiveness of our developed entropy, several different measures are used below to derive the entropy weights on the attributes cj(j=1, 2, 3) for a comparative analysis. The main results by these measures are shown in Table 1.It is clear from Table 1 that most results are so close to each other and the ranking lists are exactly the same as it did by our developed entropy that is proved to work well and can be put at ease to practical use.Example 2Let us now consider a more complex situation in which the individual decision matrices in Example 1 are expressed via A-IVIFVs, i.e.,R˜(k)=r˜ij(k)7×3wherer˜ij(k)=μ˜ij(k),ν˜ij(k),π˜ij(k)=[μijL(k),μijU(k)],[νijL(k),νijU(k)],[πijL(k),πijU(k)](i=1, 2, …, 7; j, k=1, 2, 3), shown asIn this case our developed entropy (19) is used to derive the attribute weighting vector w, with which the alternative agro-ecological regions are finally ranked.Step 1. Aggregate all of the individual opinions,R˜(k)=(r˜ij(k))7×3(k=1, 2, 3), into a group one,R˜=(r˜ij)7×3by using the interval-valued intuitionistic fuzzy weighted averaging (IIFWA) operator [28], wherer˜ij=IIFWAλr˜ij(1),r˜ij(2),r˜ij(3)=1−∏k=131−μijL(k)λk,1−∏k=131−μijU(k)λk,∏k=13νijL(k)λk,∏k=13νijU(k)λk,∏k=131−μijU(k)λk−∏k=13νijU(k)λk,∏k=131−μijL(k)λk−∏k=13νijL(k)λk.Thus the group opinion is shown asR˜=([0.733,0.840],[0.000,0.160],[0.000,0.267])([0.582,0.734],[0.141,0.245],[0.021,0.277])([0.381,0.500],[0.174,0.346],[0.154,0.445])([0.682,0.783],[0.000,0.162],[0.054,0.318])([0.500,0.673],[0.141,0.300],[0.027,0.359])([0.221,0.321],[0.478,0.600],[0.079,0.301])([0.414,0.532],[0.300,0.418],[0.049,0.286])([0.723,0.859],[0.000,0.141],[0.000,0.277])([0.693,0.859],[0.000,0.141],[0.000,0.307])([0.522,0.700],[0.141,0.266],[0.034,0.337])([0.515,0.616],[0.162,0.318],[0.066,0.323])([0.515,0.616],[0.174,0.300],[0.084,0.311])([0.381,0.600],[0.153,0.283],[0.117,0.465])([0.693,0.838],[0.000,0.115],[0.048,0.307])([0.528,0.700],[0.123,0.245],[0.055,0.349])([0.591,0.714],[0.100,0.217],[0.069,0.309])([0.469,0.590],[0.200,0.346],[0.064,0.331])([0.281,0.481],[0.384,0.485],[0.034,0.335])([0.372,0.582],[0.281,0.418],[0.000,0.347])([0.307,0.522],[0.266,0.410],[0.068,0.427])([0.400,0.613],[0.246,0.370],[0.017,0.354]).Step 2. Calculate the fuzzy entropy for each attribute cj(j=1, 2, 3) by using the formula (19). After similar treatment, the entropy weights on the attributes can then be derived, the set of which is collectively denoted by a weighting vectorw′=(0.340,0.341,0.319)T.Step 3. With the obtainedw′andR˜=r˜ij7×3, make a group assessmentr˜ion each alternative ai(i=1, 2, …, 7) by using the IIFWA operator again, wherer˜i=IIFWAw′r˜i1,r˜i2,r˜i3=1−∏j=131−μijLw′j,1−∏j=131−μijUw′j,∏j=13νijLw′j,∏j=13νijUw′j,∏j=131−μijUw′j−∏j=13νijUw′j,∏j=131−μijLw′j−∏j=13νijLw′j,i=1,2,…,7.Thus the group assessments on the agro-ecological regions are shown asr˜1=([0.718,0.853],[0.000,0.147],[0.000,0.282]),r˜2=([0.541,0.689],[0.148,0.274],[0.038,0.311]),r˜3=([0.431,0.575],[0.167,0.309],[0.116,0.402]),r˜4=([0.644,0.782],[0.000,0.165],[0.053,0.356]),r˜5=([0.524,0.664],[0.140,0.281],[0.054,0.336]),r˜6=([0.292,0.469],[0.375,0.497],[0.033,0.333]),r˜7=([0.375,0.556],[0.270,0.400],[0.044,0.355]).Step 4. In order to rank the alternatives, evaluate the values ofr˜i(i=1, 2, …, 7) by the following technique [8]ZAIVIFV(r˜i)=121−14(πr˜iL+πr˜iU)μr˜iL+μr˜iU+12(πr˜iL+πr˜iU),i=1,2,…,7.Thus we haveZAIVIFVr˜1=0.795,ZAIVIFVr˜2=0.641,ZAIVIFVr˜3=0.550,ZAIVIFVr˜4=0.732,ZAIVIFVr˜5=0.624,ZAIVIFVr˜6=0.429,ZAIVIFVr˜7=0.509.Step 5. Finally, rank all agro-ecological regions with respect to the set of attributes in terms of the values ofZAIVIFVr˜i(i=1, 2, …, 7), i.e., a1≻a4≻a2≻a5≻a3≻a7≻a6.With the weighting vectors pre-assigned by subjectivity, Xu and Yager [29] ranked all of the alternatives ai(i=1, 2, …, 7) by the combination of the uncertain dynamic intuitionistic fuzzy weighted averaging (UDIFWA) operator and the closeness coefficients. Again, our ranking list is exactly the same as theirs.Similarly, several different measures are used below to derive the entropy weights on the attributes cj(j=1, 2, 3) for a comparative analysis in such a situation. The main results by these measures are shown in Table 2.Again, from Table 2 we clearly see that most results are close to each other and the ranking lists are exactly the same as it did by our developed entropy in the A-IVIFSs context. Surely this shows the effectiveness of our study on the entropy of A-IFSs/A-IVIFSs from the perspective of amount of knowledge.

@&#CONCLUSIONS@&#
In this paper, we present a more strict axiomatic definition of entropy on A-IFSs, and then develop a new entropy measure of A-IFSs that satisfies the refined axioms, with the aim of showing some unique features of A-IFSs from the perspective of amount of knowledge and to overcome some drawbacks and ambiguities the existing entropy brings about. What is significant in this paper is that our developed entropy can be used to reveal the distinction between the two particular situations, one with the membership and non-membership values both equal to 0.5, and the other when we know absolutely nothing. This may ultimately help us distinguish such particular cases in which the membership values are equal to the corresponding non-membership values in the A-IFSs context. For this purpose, we have to loosen the constraint on the traditional consistency of entropy between the ordinary FSs and the A-IFSs. In addition, an effort is made to extend the developed axioms and entropy measure from A-IFSs to A-IVIFSs. Finally, a practical application of the developed models in decision making under uncertainty shows the effectiveness of our study on the entropy of A-IFSs/A-IVIFSs from the perspective of amount of knowledge.