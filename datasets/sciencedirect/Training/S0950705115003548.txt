@&#MAIN-TITLE@&#
Gravitational fixed radius nearest neighbor for imbalanced problem

@&#HIGHLIGHTS@&#
We use the gravitational scenario into the fixed radius nearest neighbor rule.The proposed GFRNN deals with imbalanced classification problem.GFRNN does not need any manual parameter setting or coordination.Comparison experiments on 40 datasets validate its effectiveness and efficiency.

@&#KEYPHRASES@&#
Fixed radius search,Nearest neighbor rule,Imbalanced data,Pattern classification,

@&#ABSTRACT@&#
This paper proposes a novel learning model that introduces the calculation of the pairwise gravitation of the selected patterns into the classical fixed radius nearest neighbor method, in order to overcome the drawback of the original nearest neighbor rule when dealing with imbalanced data. The traditional k nearest neighbor rule is considered to lose power on imbalanced datasets because the final decision might be dominated by the patterns from negative classes in spite of the distance measurements. Differently from the existing modified nearest neighbor learning model, the proposed method named GFRNN has a simple structure and thus becomes easy to work. Moreover, all parameters of GFRNN do not need initializing or coordinating during the whole learning procedure. In practice, GFRNN first selects patterns as candidates out of the training set under the fixed radius nearest neighbor rule, and then introduces the metric based on the modified law of gravitation in the physical world to measure the distance between the query pattern and each candidate. Finally, GFRNN makes the decision based on the sum of all the corresponding gravitational forces from the candidates on the query pattern. The experimental comparison validates both the effectiveness and the efficiency of GFRNN on forty imbalanced datasets, comparing to nine typical methods. As a conclusion, the contribution of this paper is constructing a new simple nearest neighbor architecture to deal with imbalanced classification effectively without any manually parameter coordination, and further expanding the family of the nearest neighbor based rules.

@&#INTRODUCTION@&#
Class distribution is defined as the proportion of patterns belonging to different classes in a dataset and plays a pivotal role in pattern recognition [11,12,38]. As a special case of class distribution, the imbalanced dataset is the case that the number of patterns from one class is far less than those belonging to the other classes [3,5,12,15]. Further in real-world classification tasks, the class with less patterns generally attracts more interests than the others and then is defined as the positive class [2,11,24]. Correspondingly, the other classes with more patterns are defined as the negative classes. To be convenient, this paper abbreviates the positive class (i.e., the minority class) to POS and the negative classes (i.e., the majority classes) to NEG. Generally in binary-class imbalanced problems, one indicator named the Imbalance Ratio (IR) [26] is defined in Eq. (1) to measure the imbalance degree of one dataset:(1)IR=NNEGNPOS,where NNEGand NPOSmean the number of patterns from NEG and POS, respectively.The previous research studies [2,3,11,15] have revealed that the traditional classification methods deteriorate more or less when dealing with the imbalanced datasets. The classical k Nearest Neighbor search algorithm (kNN) has no exception [19,20,23,33]. kNN used to be evaluated as one of the top 10 algorithms in data mining [37] because of its simple but powerful principle, which recognizes a query pattern only based on the most frequent class distribution of its k nearest neighbors in testing steps [8]. However, kNN might be misled in imbalanced problems because that 1) the decision of kNN might be dominated by the NEG patterns around the query pattern [23] and 2) the selection of k is data-dependent and difficult to tune [33]. For instance, a kNN withk=7is prone to classify a query pattern into NEG in a binary-case, in spite of the fact that the two nearest neighbors belong to POS and the other five farside patterns belong to NEG.The existing solutions for imbalanced problems can be categorized into three parts: firstly, the data-oriented methods use sampling techniques to achieve the equilibrium of the class distribution, such as the typical over-sampling method named SMOTE [7] that increases the size of POS with the synthetic patterns. Secondly, the cost-sensitive methods consider the penalties associated with misclassifying patterns [34]. Finally, the ensemble methods improve the performance of each used classifier [11]. Nevertheless, research studies on kNN with imbalanced datasets are far from enough [20]. In general, the corresponding interests of kNN for imbalanced problem can be divided into two branches: the pattern-oriented methods aiming to amplify the effect of each POS pattern [20,39] and the distribution-oriented methods trying to acquire more informative prior knowledge of global distribution [10,19,23]. For the first branch, one typical pattern-oriented method is the k Exemplar-based Nearest Neighbor (ENN) [20] that first selects the pivot POS patterns and then expands the boundary of them into Gaussian balls. In detail, ENN uses a newly-defined distance between the query pattern and the surface of the ball of one pivot POS pattern instead of the original distance between them, leading to a nearer connection between the query one and the pivot one. Afterwards, the Positive-biased Nearest Neighbor (PNN) [39] is proposed to boost ENN by dynamically comparing the distance between the kth nearest local neighbor and the query to the distance between the rth nearest POS pattern and the query. According to the rule of PNN, one of the two parameters k and r is finally selected to balance the local distribution of the binary-class patterns. That is, the search area of the query pattern is dependent on the value of the two parameters. Differently from ENN, PNN has no training steps [39]. As a result, PNN can deal with test patterns faster. Another idea is to learn the relationship between the query and its neighbors to correct the substantial bias to major class iteratively [13]. In detail, the coordination of each iteration considers the infection from patterns of both intra-classes and inter-classes as the weight and adopts the Geometric Means metric (GM) as the measurement [13].As for the second branch of kNN-related methods for imbalanced problems, the typical distribution-oriented strategies are listed as follows. First, the Class Confidence Weighted kNN algorithm (CCW-kNN) [23] obtains the different weights by calculating the mixture models or Bayesian networks and then imposes the weights on various neighbors as the confidence. Besides, the Class Based Weighted k Nearest Neighbor [10] generates weights for each pattern by calculating the rate of misclassification of each class by the original kNN. In addition, the Class Conditional Nearest Neighbor Distribution (CCNND) [19] compares the query pattern to patterns of each class in turn, and thus finds the most eligible class, in which the query pattern defeats the most intra-class patterns in terms of the pointwise comparison of distances to their k neighbors. Moreover, the Informative k Nearest Neighbor (IkNN) containing the localized version (LI-kNN) and the globalized version (GI-kNN) introduces a new metric that measures the informativeness between patterns first, and then finds the top I nearest patterns as the final candidates from the basic k nearest neighbors [33]. Finally, there are methods combining both of the pattern and the distribution-oriented idea, e.g., the Fuzzy-Rough k Nearest Neighbor Algorithm [14] constructs relation between the query patterns and its neighbors based on the fuzzy membership function, while some related approaches [21,22] are proposed based on the fuzzy knowledge.It can be concluded that most of the mentioned nearest neighbor algorithms, to a certain extent, intend to learn and utilize global information to make a progress, but there is still possibility for overcoming the existing drawbacks and improving the previous work: firstly, the learning models usually seem too complex and not easy to be approached without adopting special data structures; secondly, some of the classifiers have to tune many parameters and the process for finding optimal parameters costs extra time; thirdly, the influence of the global information might be weakened during the training process. In this paper, we intend to find a simpler but more robust way to make a progress. Differently from the existing methods, we first consider to separate rather than combine the processes to acquire both global and local knowledge. In detail, we first try to eliminate ineligible patterns globally and then deal with the surviving ones that are called as candidates in this paper. To fulfill the global search task, we prefer to adopt the Fixed Radius Nearest Neighbor search strategy (FRNN) [4,25] instead of the traditional kNN. Further inspired and modified by the typical gravitation-based methods including the Data Gravitation based Classification (GDC) [27,28] strategy and the Gravitational Search Algorithm (GSA) [29,30], we aim to design and introduce a new local decision criterion based on the gravitational rule into the FRNN. Furthermore, all steps are not expected to require any parameter input manually. To our best knowledge, it is the first time to propose the gravitation-inspired FRNN without any manually-coordinated parameters. To be convenient, the novel method is called GFRNN in short.The major contribution of this paper lies in the following aspects:•Motivation: This paper tries to propose a new easy-to-approach nearest neighbor learning model by introducing the calculation of universal gravitation into the traditional FRNN, in order to overcome the drawback of the original nearest neighbor rule on imbalanced classification problems.Novelty: The proposed method is expected to eliminate unnecessary patterns through the FRNN strategy and makes the final decision according to the sum of the gravitational forces between the query and the surviving patterns. Moreover, none of parameters are manually set or coordinated in the whole learning procedure.Experiments: The experiments are designed to compare GFRNN to some typical algorithms. Specially, nine classifiers including CCNND [19], ENN [20], LI-kNN [33], PNN [39], kNN [8], FRkNN [25], C4.5 [6], Logistic Regression (LR) [16], and the Support Vector Machine (SVM) [35] are considered. Finally, the results demonstrate the effectiveness and the efficiency of the proposed method.The rest of this paper is organized as follows. Section 2 presents the architecture of the proposed method and demonstrates relevant analyses on it. Section 3 reports on all the experimental results. Finally, conclusions are given in Section 4.In this paper, we focus on the binary-class recognition for imbalanced datasets even though GFRNN can be generalized into the multi-class problems. At first, we suppose that there is an binary-class imbalanced dataset. The training set of the dataset is XAllthat includes the set of POS:XPOS={(x1,φ1),(x2,φ1),…,(xnPOS,φ1)},and the set of NEG:XNEG={(xnPOS+1,φ2),(xnPOS+2,φ2),…,(xnPOS+nNEG,φ2)},whereφi∈{1,−1}is the label,i=1for POS andi=2for NEG. nPOSand nNEGare the number of training patterns belonging to POS and NEG, respectively. Therefore, the number of the total training patterns can be written as:(2)nALL=nPOS+nNEG.Moreover, we define the query patternyand formulate the function d( · ) in Eq. (3) to measure the distance between two patterns. To be simple, we adopt Euclidean distance here though any appropriate measurements could be used.(3)d(xp,xq)=∥xp−xq∥2.In addition, the training pattern that survives from the FRNN is named candidate in sequent parts. Finally, it can be summarized that the formulation of GFRNN includes three main steps: first selecting candidates aroundythrough FRNN, then calculating the distance betweenyand each candidate based on the simplified law of gravitation, and making the decision according to the sum of the gravitational forces of ally-candidate pairs at last.According to the literature [25], there are three most popular nearest neighbor search methods: kNN, FRNN, and the combination of kNN and FRNN (i.e., FRkNN). At first, kNN can be defined as [25]:(4)kNN(y,XAll,k)=A,here A is the set that satisfies the following conditions, in which |A| means the number of patterns in the set A:|A|=k,A⊆XAll,∀xp∈A,xq∈XAll−A,d(y,xp)≤d(y,xq).Secondly, FRNN is defined as follows [25]:(5)FRNN(y,XAll,R)={xp∈XAll,d(y,xp)<R},where R is generally a constant obtained either from the user or some preliminary calculation. Moreover according to the definition mentioned at the beginning of this section, the patterns selected through FRNN are called candidates in this paper. Finally, the combination of both kNN and FRNN is given as the Fixed Radius k Nearest Neighbor (FRkNN) that can be formulated as follows [25]:(6)FRkNN(y,XAll,k,R)=A,with respect to:|A|≤k,A⊆XAll,∀xp∈A,xq∈XAll−A,d(y,xp)≤d(y,xq)andd(y,xp)<R.Since the candidates are obtained through FRNN, the key is to find the suitable radius R. Here we are inspired by the literature [32] to regard the mean squared mutual Euclidean distance between training pattern pairs as the value of R. The correlated calculation is as follows:(7)R=12nALL(nALL−1)∑xp,xq∈XAlld(xp,xq).In the literature [32], R is called the cutoff distance and treated as the threshold to present the local density of the query patterny. We utilize this method because it is one of the simplest and non-manual-parameter-required methods to obtain a suitable R without any inclination to either class but considering the global distribution. Once R is determined, we can use FRNN to select the candidates out to form the new localized set XCandi. It should be noted that any FRNN data structure could be adopted here [25]. To be convenient, we simply use the linear search structure for FRNN.After obtaining the set of candidates, we regard each pattern of XAllas an entity with its corresponding mass. Based on the gravitation-based methods [27–29,36], the present patternywould be apt to move towards the direction of the composition of the gravitational forces generated from all they-candidate pairs. Differently from DGC or GSA, we do not consider the direction of the gravitation here, i.e., we simply do the calculation as if all candidates are located in a straight line and regard the weight for each feature of the pattern as one. Finally, the composition of the gravitational forces acting on the query patternyis obtained through the function F( · ) as follows:(8)F(y)=∑i=1,2.xi∈XCandiφiD(y,xi)where the function D( · ) is developed from the formula of gravitation and designed for calculating the gravitation between the query patternyand each candidatexi∈ XCandi:(9)D(y,xi)=Gmymxid(y,xi)2.To simplify Eq. (9), both the mass of the query myand the gravitational constant G are set to one. The next step is to acquire the value ofmxibased on the density. In fact, it is not rare to find the application of the localized density conception in solving classification problems [17]. Accordingly, we consider to utilize IR of XAllto achieve the balance among candidates on condition that the following equation holds:(10)nPOSmPOS=nNEGmNEG,where mPOSis the mass of candidates belonging to POS and mNEGis the mass for candidates from NEG. From Eq. (10), we can find thatmPOS=nNEGnPOSmNEG=IR·mNEGbased on the premise that the distribution of the training patterns is consistent with or at least similar to the distribution of the whole dataset. Now we can formulate the piecewise-defined function to get themxias follows:(11)mxi={nNEGnPOS(i.e.,IR),xi∈XPOS∩XCandi1,xi∈XNEG∩XCandi.Afterwards, the decision criterion is that the query patternyis classified into NEG if F(y) < 0, whileyis classified into POS if F(y) > 0. Finally, the completed procedure of GFRNN summarized in pseudo-code is given in Table 1.In this part, we discuss some specialties of GFRNN. We first explain why FRNN rather than kNN is chosen as the preprocessing strategy for GFRNN , and then demonstrate the effectiveness of GFRNN, i.e., why this model works. In detail, we choose FRNN instead of kNN for two main reasons. As mentioned in Section 1, the first reason is that the selection of parameter k in kNN is too crucial for the manual setting to achieve the optimal value. Accordingly, some unpredictable trouble might be caused in imbalanced learning situation when k is initialized in wrong interval. An example is given in Fig. 1. In this example, the k of kNN is set to 7 and the radius R of FRNN is obtained in the same way as that in GFRNN, i.e., R is the mean squared mutual Euclidean distance between pattern pairs. From Fig. 1, three extra patterns of NEG are included in the seven neighbors of the query pattern according to kNN rule, which might lead the classifier to make a wrong decision. Further, it could be naturally imagined that kNN might lose its power if the distribution of patterns is both sparse and imbalanced. However, it cannot be asserted that FRNN absolutely outperforms kNN in all cases, e.g., kNN might make the correct decision if k is set to 1, 3, or 5 in Example 1 (Fig. 1). But we are immediately trapped in the first problem mentioned above, i.e., the selection of the optimal k is in fact a difficult task because the right choice is usually dataset-dependent in most real-world situation [33]. On the other hand, FRNN might be more robust and adaptive since we find a way to make the radius R be correlated with the dataset.Furthermore, we focus on analyzing the main formula of GFRNN in order to make it easier to be understood. We first define that there aren˜POScandidates of XPOSandn˜NEGcandidates of XNEGafter FRNN. According to Eq. (11), we can expand Eq. (8) as follows:(12)F(y)=∑xi∈XPOS∩XCandiGmymxid(y,xi)2−∑xj∈XNEG∩XCandiGmymxjd(y,xj)2=∑xi∈XPOS∩XCandiIRd(y,xi)2−∑xj∈XNEG∩XCandi1d(y,xj)2.To simplify the problem, we suppose thatd(y,xi)2=d(y,xj)2=d˜,i.e., all the candidates have the same distance from the query pattern. Then Eq. (12) is rewritten as:(13)F(y)=(IRn˜POS−n˜NEG)d˜=(nNEGn˜POSnPOS−n˜NEG)d˜.Besides, we know that the query is classified to POS when F(y) > 0. So we further consider that on what condition the result of Eq. (13) is greater than 0. Since the sum of distances is always non-negative, the following inequation holds:(14)nNEGn˜POSnPOS>n˜NEG.In most situation,n˜POS>0. Hence we can obtain the following inequation:(15)IR=nNEGnPOS>n˜NEGn˜POS=ir.Now it can be clearly seen that the query would be determined to belong to POS if IR > ir. Furthermore, it is obvious that IR is indeed greater than ir except two cases. The first case is thatn˜POS≥n˜NEGwhen the query appears near the clustering center of POS. The second case is thatn˜POS=0when the query locates among patterns of NEG and is far away from patterns of POS. In fact, the second case never happens in our experiments, but we can imagine thatn˜POSis small enough to approach to zero. Further, it should be noted that here the distanced˜is fixed while in fact both of these two special cases are easy to be dealt with because in first case the query obviously belongs to POS and in second case it can be easily classified in NEG in terms of the sum of distances. As a conclusion, Eq. (15)implies that if the local density is just the same to the training density, i.e.,n˜POS=nPOSwhilen˜NEG=nNEG,and all the candidates have the same distance from the query pattern, makingF(y)=0means that the query shares the same possibility to be classified into POS or NEG. Further, the key is that at the preprocessing step, the number of patterns from NEG is far more sharply reduced than those form POS, so as to achievenNEGnPOS>n˜NEGn˜POSwhen the query locates between POS and NEG. It implies that the query that first locates between POS and NEG is usually apt to “move” to the boundary of POS. Example 2 in Fig. 2illustrates that after FRNN, the number of patterns from NEG is nearly reduced to half while the number of patterns from POS barely changes. Therefore, GFRNN could be in fact treated as some kind of under-sampling idea combined with modified voting strategy. The speciality of GFRNN is that the whole procedure does not need any parameter-setting manually.In this part, we discuss the differences between GFRNN and the other previous gravitation-based methods. At first, it is obvious to find two main differences between GFRNN and GSA. The first one is about the motivation. Even though it is used as a preprocessing to select more useful patterns for the subsequent classifier in the later literature [31], GSA is not specially designed for the classification tasks. Instead, it is one of the heuristic optimization methods inspired by swarm behaviors in nature [29]. On the other hand, GFRNN is designed mainly to solve the recognition problems. Secondly, GSA dynamically searches for the optimal solution and considers more parameters than GFRNN that is analytic and does not need to change any parameters during the calculation. As for GSA, the acceleration is an important coefficient while the gravitational constant G might change after each iteration. Both of the two factors are unnecessary and omitted in GFRNN. Further in GSA, three definitions of mass are defined as: the inertial mass, the active gravitational mass, and the passive gravitational mass, while in GFRNN the definition of mass is simpler [29].It seems that GFRNN is more similar to DGC than to GSA because both GFRNN and DGC are designed for classification and based on the similarity between patterns. In addition, the final decision of both two methods depends on the distance and the mass instead of G or other coefficients. However, GFRNN is substantially different from DGC. Before the analysis, we briefly introduce DGC at first. As for DGC, all the operations are done between the basic entities named the “particle”. One particle is a subset (or cluster) of the original dataset. The mass of the particle is defined as the number of patterns included in it and the feature (or location) of the particle is the centroid of those included patterns. The special case is that one pattern could be treated as one particle. DGC first constructs a gravitational field by calculating all gravitational forces between each particle-pair and then classifies the query pattern to the class, whose particles impose the strongest force to the query and dominate the final direction of the composition of forces. It should be noted that each class could contain more than one particle and the division strategy for particles is crucial. In literature [28], one novel DGC for imbalanced problem named IDGC has been proposed. To modify the original DGC, IDGC adds one new parameter named the Amplified Gravitation Coefficient (AGC) γi, i ∈ {POS, NEG}, which in binary-class imbalanced case is defined as follows. To make the subsequent analysis convenient, we here treat each pattern as one particle in IDGC:(16)γi=nNEG+nPOSni,i∈{POS,NEG}First, we consider the training patterns of POS. The function for the composition of the gravitational forces of POS pattern in IDGC, which is denoted asAPTARANORMALF˜POS(·),is given as follows:(17)APTARANORMALF˜POS(y)=∑k=1nPOSγPOSmkAPTARANORMALd˜(y,xk)2.In Eq. (17), the denominator could be further expanded as follows, because in DGC each feature shares different weight and d is the number of features in one pattern:(18)APTARANORMALd˜(y,xk)2=∑l=1dwl(yl−xk,l)2.Moreover, mkin Eq. (17) is set to 1 because each particle now has only one pattern. So if we further suppose that all features share the same weight, i.e.,w=1d×1,Eq. (17)could be simplified to:(19)APTARANORMALF˜POS(y)=∑k=1nPOSγPOSd(y,xk)2.Eq. (19) means the composition of the gravitational forces from all POS patterns for the query pattern. Now we inspect the composition forces of POS in GFRNN, which can be calculated as:(20)FPOS(y)=∑k=1n˜POSmPOSd(y,xk)2=∑k=1n˜POSnNEGnPOSd(y,xk)2=IR∑k=1n˜POS1d(y,xk)2.Obviously, Eq. (19) does not equal to Eq. (20)mainly because the sum of the mass from them is different. For the further investigation, we bringγ=nNEG+nPOSnPOSinto Eq. (19) and obtain:(21)APTARANORMALF˜POS(y)=(nNEG+nPOSnPOS)∑k=1nPOS1d(y,xk)2=(IR+1)∑k=1nPOS1d(y,xk)2.The difference between GFRNN and IDGC still remains. Same conclusion could be achieved from the NEG situation. Therefore, GFRNN and DGC are totally two different methods.

@&#CONCLUSIONS@&#
In this paper, we propose a novel learning model named GFRNN to operate the classification problems on imbalanced datasets. The proposal of the new model is based on the classical fixed radius nearest neighbor search strategy and inspired by the gravitation-based methods. Specially, GFRNN does not need any manually-set parameters in the whole process. In the process, GFRNN first selects the appropriate patterns out as the candidate through the FRNN rule, and then calculates the gravitational force between the query pattern and each candidate. As a result, the composition of those gravitational forces is obtained to be the criterion of the final decision. Experiments are designed to compare GFRNN to nine representative algorithms such as CCNND, ENN, LI-kNN, PNN, kNN, FRkNN, SVM, C4.5, and LR on forty selected binary-class imbalanced datasets and validate the effectiveness of the proposed method. Furthermore, the time comparison on large size datasets between GFRNN and six selected typical algorithms demonstrates the efficiency of it. Finally, the main contribution of this paper can be concluded as designing an effective and structurally-simple nearest neighbor learning model to deal with the imbalanced recognition tasks more effectively and efficiently.