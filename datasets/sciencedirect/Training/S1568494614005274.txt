@&#MAIN-TITLE@&#
Improving the prediction of petroleum reservoir characterization with a stacked generalization ensemble model of support vector machines

@&#HIGHLIGHTS@&#
Despite successful applications of ensembles, the petroleum industry has not benefited enough.SVM is promising but its performance depends mostly on the regularization parameter.We propose an SVM ensemble with diverse opinions on the regularization parameter.The proposed model outperformed Random Forest but competitive with SVM Bagging.There is great potential for ensemble models in petroleum reservoir characterization.

@&#KEYPHRASES@&#
Stacked generalization ensemble,Support vector machines,Regularization parameter,Porosity,Permeability,

@&#ABSTRACT@&#
The ensemble learning paradigm has proved to be relevant to solving most challenging industrial problems. Despite its successful application especially in the Bioinformatics, the petroleum industry has not benefited enough from the promises of this machine learning technology. The petroleum industry, with its persistent quest for high-performance predictive models, is in great need of this new learning methodology. A marginal improvement in the prediction indices of petroleum reservoir properties could have huge positive impact on the success of exploration, drilling and the overall reservoir management portfolio. Support vector machines (SVM) is one of the promising machine learning tools that have performed excellently well in most prediction problems. However, its performance is a function of the prudent choice of its tuning parameters most especially the regularization parameter, C. Reports have shown that this parameter has significant impact on the performance of SVM. Understandably, no specific value has been recommended for it. This paper proposes a stacked generalization ensemble model of SVM that incorporates different expert opinions on the optimal values of this parameter in the prediction of porosity and permeability of petroleum reservoirs using datasets from diverse geological formations. The performance of the proposed SVM ensemble was compared to that of conventional SVM technique, another SVM implemented with the bagging method, and Random Forest technique. The results showed that the proposed ensemble model, in most cases, outperformed the others with the highest correlation coefficient, and the lowest mean and absolute errors. The study indicated that there is a great potential for ensemble learning in petroleum reservoir characterization to improve the accuracy of reservoir properties predictions for more successful explorations and increased production of petroleum resources. The results also confirmed that ensemble models perform better than the conventional SVM implementation.

@&#INTRODUCTION@&#
The ensemble learning paradigm is the most recent Computational Intelligence (CI) tool for combining a “mixture of experts”. It has proved to be relevant in solving most challenging industrial problems. Its superior performance over the conventional method of learning individual techniques has been confirmed when applied on classification and regression problems. Its superior performance over the conventional method of learning individual techniques has been confirmed when applied on classification and regression problems. The ensemble learning paradigm is an advancement in the supervised machine learning technology. While the latter searches for the best hypothesis among all possible hypotheses that describe the solution to a problem, the former combines the best hypothesis of different instances of the base learner and its associated hypotheses. The ensemble learning paradigm has gained much ground with classification problems in many fields. However, it is still a new technology whose great benefit is still waiting to be tapped in the petroleum industry. The ensemble learning methodology is a close emulation of the human socio-cultural behavior of seeking several people's opinions before making any important decision [1]. With the reports of the successful application of ensemble modeling over their individual base learners in other areas [2–6], the petroleum industry is in dire need of this new modeling approach in the petroleum reservoir characterization business.A lot of data is being generated and acquired in the petroleum industry due to the proliferation of various sensor-based logging tools such as Wireline, Open-Hole, Logging-While-Drilling, Measurement-While-Drilling, and seismic measurements of increasing dimensions. Due to the high dimensionality that may be involved in the data acquired through these systems, the ensemble methodology is most ideal for extracting useful knowledge out of them without compromising expert opinions and model performance. For those outside the facilities that may not have access to these voluminous data, the ensemble methodology is still the ideal technique to manage the little data that may be available to them. The ensemble learning methodology is ideal for handling both cases of too much data and too little data [7]. Ensemble models have the capability to combine different architectures of their base models, diverse data sampling methodologies, different recommended best input features, and various optimized parameters obtained from different experts in the optimization of estimates and predictions of petroleum reservoirs properties.CI techniques have been well applied in the petroleum industry over the years, especially in reservoir characterization, but with a pace that does not match up with the rate of advancement and the dynamics of the technology. Interestingly, researchers in the petroleum industry have moved progressively from the use of empirical correlations and linear regression models to the use of CI and machine learning techniques [8]. However, the application of CI in the petroleum industry have mainly been limited to Artificial Neural Networks (ANN) and Fuzzy Logic [9–12] with very few work in the area of hybrid CI modeling [13–18] but almost nothing yet in the application of ensemble models.Reservoir characterization, an essential process in the petroleum industry for estimating various properties of petroleum reservoirs, needs the ensemble learning methodology to improve the accuracy of predictions that are important for the qualitative and quantitative evaluation of petroleum reserves to further increase the consequent success of exploration, drilling and production activities. A marginal increase in the prediction accuracy of these petroleum properties is capable of improving the efficiency of exploration, drilling and production of petroleum resources with less time and effort.In this study, we propose an ensemble model of support vector machines (SVM) based on the diversity exhibited by the regularization parameter. The regularization parameter is an important parameter to tune and optimize the SVM model during the training process. The major motivation for this study is the continued quest for better predictions of reservoir properties and the various reports, in other fields of application, of superior performance of ensemble techniques over their individual base learners. This paper is aimed at achieving the following objectives:•To review the application of ensemble techniques, especially in petroleum reservoir characterization.To establish a premise for the typical need of ensemble techniques in petroleum engineering.To investigate the applicability of the stacked generalization ensemble model of Support vector machines based on different expert opinions on the regularization parameter.To investigate the possible outperformance of the proposed SVM ensemble model over the conventional bagging method and the Random Forest technique using representative reservoir porosity and permeability datasets.To demonstrate the superiority of the bagging-based SVM ensemble model over the Random Forest technique.To confirm whether the performance of the ensemble technique is better than or as good as the best from among its individual base models.This is the first study to address the applicability of the ensemble learning paradigm in the prediction of petroleum reservoir properties especially porosity and permeability. By demonstrating the superior performance of the proposed regularization parameter-driven SVM ensemble learning model over that of the conventional bagging method, this study is expected to boost the interest of petroleum engineering researchers in this new learning paradigm as it promises to open windows of possibilities in its application in petroleum reservoir characterization.The rest of this paper is organized as follows: Section 2 presents a rigorous review of literature on the ensemble methodology, its application in reservoir characterization, overview of the SVM technique, and the effect of the regularization parameter on its performance. Section 3 discusses the architecture of the three ensemble models implemented in this study. Section 4 describes the methodology employed in this study from data description through the evaluation criteria to the details of the implementation of the ensemble models. Results are presented and discussed in Section 5 while conclusions on the results are drawn in Section 6.The ensemble learning methodology combines multiple “expert opinions” to solve a particular problem [19]. Each opinion is represented in each instance of the base learners that make up the ensemble model. In regression tasks, each instance attempts to search for the best hypothesis that solves the problem. In ensemble learning, the best hypotheses identified by the base learners are combined using any of the various combination rules to evolve the best overall solution offered by the ensemble model. This methodology of combining the opinions of different “experts” to obtain an overall “ensemble” decision is deeply rooted in human culture such as in the classical age of ancient Chinese and Greece and formalized during the Enlightenment with the Condorcet Jury Theorem that proved that the judgment of a committee is superior to those of individuals, provided the individuals have reasonable competence [1]. This also explains why most human activities are usually implemented using the committee system.The ensemble methodology was originally applied on classification and clustering problems which include bio-informatics [20], object detections [21,22], gene expressions [1], protein synthesis [3] and later extended and applied to time series prediction problems [23,24]. With the ensemble methodology, the selection of the overall best hypothesis helps to improve the performance of a model and reduces the likelihood of an unfortunate selection of a poor model. This resembles the way humans solve problems. Having a committee of experts reduces the risk of taking a wrong decision on a problem. The ensemble methodology makes the selection of such candidate models (representing the best hypotheses) more confident, less risky and unbiased. A generalized flowchart for ensemble techniques is shown in Fig. 1. The ensemble methodology starts with the conventional identification of training and testing data subsets. A finite number of base learners is then established. Each base learner is then constructed with the desired diversity such as using different input features, random sampling of the input data or different values of a tuning parameter. The individual results produced by the base learners are then combined using a relevant combination algorithm to evolve a single ensemble solution.As a methodology for classification and clustering problems, it was successfully implemented in the Adaptive Boosting (AdaBoost) technique. It was later extended for regression problems in the form of Bootstrap Aggregate method, abbreviated as bagging, and implemented in the Random Forest technique [25]. Bagging involves training each of the ensemble base learners on a subset that is randomly drawn from the training data with replacement while giving each data sample equal weight [26].The major motivation for the ensemble learning paradigm is the statistically sound argument that the paradigm is part of human daily lives: We ask the opinions of several experts before making a decision: we seek the opinions of several doctors before accepting a medical procedure; we read user reviews before choosing a web hosting service provider; we evaluate reports of references before taking new employees; manuscripts are reviewed by experts before accepting or rejecting them; etc. In each case, the primary objective is to minimize the error associated with the final decision that will be made by combining the individual decisions [7].Petroleum reservoir characterization is the process of estimating and predicting various reservoir properties for use in full-scale reservoir models for the determination of the quality and quantity of a petroleum reservoir. Some of the reservoir properties that are of interest to Petroleum Engineers include porosity, permeability, water saturation, pressure, volume, temperature, oil and gas ratio, bubble point pressure, dew point pressure, well-bore stability, diagenesis and lithofacies. Out of these, porosity and permeability are the most important as they jointly serve as key indicators of reservoir quality. The accuracy of almost all other properties depends on the accuracy of these two properties.Porosity is the percentage of pores in core samples that are usually extracted from a petroleum reservoir during a coring process. The process involves using specialized devices to take cylindrical samples of rocks at intervals of about one foot for laboratory measurements. The higher the percentage of pores in a rock sample, the more will be its ability to hold hydrocarbons, water and gas. Permeability is a measure of how the individual pores in the core samples are interconnected. No direct relationship has been universally established between these two properties especially in carbonate geological formations. Hence, if a rock sample is very porous, it may not necessarily be of high permeability [27].Before the CI technology was embraced in the petroleum industry, these properties used to be calculated from various empirical correlations and then followed by the use of linear regression techniques. Presently, the concept of Hybrid Computational Intelligence (HCI) is gaining more popularity in the petroleum engineering domain. As the quest for increased performance of predictive models in reservoir characterization continues to soar, the ensemble methodology offers a great potential for developing better performing and more robust predictive models. Despite the reasonable number of successful applications of CI and HCI techniques in reservoir characterization [8,28–30], the great opportunities of robust model development offered by the ensemble learning technique has not been adequately utilized.Most of the applications of ensemble methodology are found in classification and clustering tasks. Kim et al. [31] showed that even though SVM has been reported to provide good generalization performance, often the classification results are far from the theoretical expectations. Based on this premise, they proposed an ensemble model of SVM and tested it on IRIS classification, hand-written recognition and fraud detection datasets. The reported results showed that the proposed ensemble SVM outperformed the single SVM in terms of classification accuracy. Another ensemble model of SVM was proposed by Chen et al. [21] to detect the occurrence of road accidents. They reported that the ensemble model outperformed the single SVM model. Sun and Li [6] reported a significantly superior performance of an SVM ensemble over an individual SVM model in the prediction of financial distress.In the bio-informatics, Peng [32] and later Chen and Zhao [33] presented an ensemble of SVM and ANN classifiers respectively for the classification of microarray gene data. They both reported that their respective ensemble techniques performed better than using the single SVM and ANN techniques. Nanni and Lumini [3] proposed an ensemble of SVM classifiers for the prediction of bacterial virulent proteins using features that were extracted directly from the amino acid sequence of a given protein rather than those from the evolutionary information of a given protein as it is usually done in the literature. Despite their deviation from the well known feature source, they showed that the ensemble model performed better than a single SVM model used on the conventional feature extraction method. Similarly, positive conclusions were given by Valentini et al. [34] and Caragea et al. [20] about their SVM ensemble models for cancer recognition and Glycosylation site prediction respectively.Other interesting areas of application of ensembles include hydrogeology [5], time series forecasting [23], customer churn prediction [35], control systems [36], Soil Science [37], detection of concept drift [38], and short-term load forecasting [39]. Other base learners used apart from ANN and SVM include Neuro-Fuzzy Inference System [40], Bayesian Inference [41], Fuzzy Inference Systems [42], Decision Trees [43], and Extreme Learning Machines [44].Despite the ample reports of the successful application of the ensemble learning paradigm in literature, the benefits offered by the ensemble learning paradigm has not been harnessed in the prediction of porosity, permeability, and other petroleum reservoir properties. This paper is expected to serve as a motivating factor for its appreciation, acceptance and continued application of this new methodology in the petroleum industry.SVM is a set of related supervised machine learning methods used for classification and regression. It belongs to the family of Generalized Linear Classifiers. It can also be considered as a special case of Tikhonov Regularization as it maps input vectors to a higher dimensional space where a maximal separating hyperplane is constructed [45]. A conceptual framework of how SVM works is shown in Fig. 2. Input datasets, especially those belonging to the non-separable case, are mapped to a higher dimensional hyperplane where classification and regression becomes easier. The hyperplane is then optimized to evolve a solution to a problem. The generalization capability of SVM is ensured by special properties of the optimal hyperplane that maximizes the distance to training examples in the high dimensional feature space.SVM was initially introduced for the purpose of classification when Vapnik et al. [46] developed a new ɛ-sensitive loss function technique that is based on statistical learning theory, and which adheres to the principle of structural risk minimization, seeking to minimize an upper bound of the generalization error. The new technique that emerged out of this modification is known as Support Vector Regression (SVR). It depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction (within a threshold ɛ). It has been shown to exhibit excellent performance in prediction tasks. Some of the kernel functions used in SVM are:Linear:(1)k(x,x′)=x*x′RBF:(2)k(x,x′)=e−param*x−x′Polynomial:(3)k(x,x′)=(x*x′+1)paramwhere param is the kernel parameter in the SVM feature space [47].More details about the theoretical basis of SVM can be found in Burges [45,47] while cases of successful applications can be found in Abe [13,48,49].Among the SVM parameters that must be set appropriately for optimal performance is the regularization parameter, C. It is also known in literature as the penalty factor [50] and the penalty parameter [51]. The regularization parameter is one of the important tuning parameters of SVM whose value could have a great effect on the SVM model performance. It needs to be chosen diligently to avoid the problems of overfitting and underfitting. It helps to maintain a balance between training error and model complexity. It is so sensitive to the performance of the model that its value needs to be chosen carefully. Kecman [51] considered it as one of the two most important learning parameters that can be utilized in constructing SVMs for regression tasks. A lot has been written about its sensitivity and importance but no fixed value has been known to be universally recommended for it. It turns out, just like the other machine learning-based techniques, to be controlled by the nature of the available data.Alpaydin [50], while proving that the regularization parameter is critical to the performance of the SVM model, recommended that a “proper” value is chosen for it. Due to the difficulty in doing this, he only cautioned that choosing a number that is too large may result in a high penalty for non-separable points which will lead to overfitting. Conversely, choosing a number that is too small may result in the model not having enough strength to generalize on new cases which will lead to underfitting. Joachims [52] posited that “a small value for C will increase the number of training errors, while a large C will lead to a behavior similar to that of a hard-margin SVM” but did not give any recommendation. Shawe-Taylor and Cristianini [53] suggested that the value of this parameter should be varied “through a wide range of values”, again without giving any specific recommended value. In order to avoid any wrong assumption or giving a false recommendation, Kecman [51] and Cherkassky and Mullier [54] simply advised users to “carefully select the best value”. But “how do users carefully select the best value for this parameter?” remains an unanswered question.The aforementioned arguments are the various and diverse expert opinions on the regularization parameter, C, ranging from being a critical factor to being an important parameter whose value needs to be chosen carefully [50,51]. Though, these expert propositions are enough to establish the existence of the required diversity on the regularization parameter, we still went ahead to present an experimental proof of the diversity (to be discussed in Section 4.3). Given the importance of this parameter to the performance of SVM, the most reasonable way to optimize it is to incorporate all the expert views in an ensemble model and let the model itself combine the results of the propositions to proffer the best solution to the problem. This will serve the purpose of being focused on solution rather than the techniques.The major justifications for proposing this novel ensemble algorithm include the diversity in the opinions of experts on the best value to assign to this parameter and the effect of this parameter on the performance of SVM (as further discussed in Section 4.3). Diversity is a major requirement for ensemble learning [55,56].For the proposed ensemble model, we implemented a stacked generalization [7] of the SVM technique. The reason for choosing this architecture is simply that it has not been applied to any petroleum engineering problem. This is a case of an existing method with a new application. A conceptual framework of the proposed algorithm is shown in Fig. 3. Given the perceived diversity in the effect of the regularization parameter on the performance of SVM, we set up 10 instances of SVM as base learners viz. Model 1, Model 2, …, Model 10. Each learner uses a different value of C viz. C1, C2, …, C10 and the commonly used values for the other tuning parameters. The ensemble model was first trained using bootstrapped samples of the training data combined with a value of Cn. This created the Tier 1 models, whose outputs were then used to train a Tier 2 model (meta-model). In essence, the Tier-2 model combined the outputs of the Tier-1 models. Hence, there was no need for a separate combination algorithm as applies in the conventional bagging method.The underlying idea of this architecture was to ensure that the Tier-1 models were properly learned from the examples in the training data. This is justified by a scenario in which a particular model incorrectly learned a certain region of the input data feature space, and hence consistently failed to correctly predict the instances coming from that data region. Then the Tier-2 model will be able to learn this behavior. Along with the learned behaviors of the other models, the Tier-2 model can correct such improper training process. The proposed ensemble technique worked according to the following procedure:Algorithm 11. Set up the SVM model with the other fixed parameters (epsilon, lambda, kernel and step size).2. Do for n=1 to 10//there are 10 instances of C to create the Tier-1 models3.Set Cnto an assigned value4.Randomly divide data into training and testing5.Use the training data to train the SVM model, Sn, using Cn6.Use the test data to predict the target (porosity and permeability)7.Keep the output of the above as Hypothesis, Hn8. Continue9. All hypotheses, Hnwhere n=1, 2, …, 10 become input to Tier-2 model10. Tier-2 model is trained with the combined hypotheses.11. Tier-2 model outputs the final decision.The conventional ensemble method for regression tasks is the Bagging [25] and implemented in the Random Forest technique [57], an ensemble of Classification And Regression Trees (CART). Its counterpart for classification is called Boosting and was implemented in Adaboost technique. In the bagging method, the contribution of each base learner in the ensemble model is given an equal weight. To improve model variance, bagging trains each model in the ensemble using a subset that was randomly drawn from the training set with replacement. The results from the base learners are then averaged over all the base learners to obtain the overall result of the ensemble model.The main concept of using the bagging method to increase the prediction accuracy of ensembles is similar to reducing a high-variance noise using a moving average filter that averages each sample of the data over all available samples. The noise component will be averaged out while the information content of the entire data is unaffected by the averaging operation [58]. When the prediction errors made on the data samples are averaged out, the error of the overall output is reduced. Prediction errors are composed of two controllable components: the accuracy of the model (bias); and the precision of the model when trained on different training sets (variance). Therefore, since averaging has a smoothing (variance-reducing) effect, the goal of the bagging-based ensemble systems is to create several classifiers with relatively fixed (or similar) bias and then use the averaging combination rule on the individual outputs to reduce the variance [58]. This is the statistical justification for the bagging method.The SVM ensemble with the bagging methodology was implemented using the following procedure:Algorithm 21. Start SVM with all parameters set as optimal as possible.2. Set N to the number of desired iterations.3. Set T to the desired percentage of data for bootstrapped training data.5. Do for n=1 to N6.Randomly extract T% of the data for training7.Use the training data to train the SVM model, Sn8.Use the remaining (100−T)% test data to predict the target variables9.Keep the result of the above as Hypothesis, Hn11. Continue12. Compute the average of all Hypotheses,Hfinal(x)=argnmaxμj(x)using the Mean() rule:μj(x)=1n∑∑1nHn(x)Random Forest is an ensemble learning-based technique that consists of a bagging of un-pruned Decision Tree learners [25] with a randomized selection of input data samples and predictors. The algorithm [59] is based on the bagging technique developed by Breiman [25] and the randomized feature selection by Ho [60,61]. Random Forest begins with building a Tree and then grows more Trees using a bootstrap subsample of the data until the minimum node is reached in order to avoid overfitting that comes with larger number of Trees. More details about Decision Trees can be found in Sherrod [62] and application cases can be found in Park et al. [63] and Leibovici et al. [64]. Random Forest has been shown to be effective and accurate [65] but with reports of possible overfitting [66–68] hence liable to perform poorly in prediction tasks.The algorithm of Random Forest is presented [59] as follows:Algorithm 31. Starting with a tree:a. Set N=number of training cases.b. Set M=number of features.c. Select a subset m of input variables such that m≪M.d. Do for n=1 to Ni. Train this tree with a bootstrap sample of the training data.ii. Use the rest of the cases to estimate the prediction error of the tree.iii. Replace the bootstrap sample.Continuee. Calculate the best split based on these m variables in the training set.2. The above procedure is iterated over all trees in the ensemble.3. Calculate the mean of the performance of all trees. This represents the performance of the Random Forest technique.For the design, testing and validation of our proposed ensemble model, three porosity datasets and three permeability datasets were used. The porosity datasets were obtained from a petroleum reservoir in the Northern Marion Platform of North America (Site 1) while the permeability datasets were obtained from a reservoir in the Middle East (Site 2). The datasets from Site 1 have six predictor variables for porosity, while the datasets from Site 2 have eight predictor variables for permeability. These are shown in Tables 1 and 2.In order to effectively evaluate the performance of the base learners ahead of the ensemble models, we used the three commonly used measure of model performance viz. correlation coefficient (R-Square), root mean-squared error (RMSE) and mean absolute error (MAE) to evaluate the individual base learners.The R-Square is a statistical measure of how strong a relationship is between n pairs of two variables, x and y. It is expressed as:(4)R-Square=n∑xy−∑x∑yn∑x2−∑x2n∑y2−∑y2The RMSE is a measure of the spread of the actual x values around the average of the predicted y values. It is expressed as:(5)RMSE=∑i=1n(xi−yi)2nThe MAE is the average of the absolute errors of the predicted y values relative to the actual x values. It is given by:(6)MAE=1n∑i=1n|xi−yi|The R-Square, RMSE and MAE were used to evaluate the performance of the proposed ensemble model and the conventional SVM technique. For the conventional bagging ensemble model, we used the Mean(R-Square), Mean(RMSE) and Mean(MAE) to obtain the overall performance (presented in Algorithm 2). Random Forest has its Mean() rule for its models combination already embedded in the algorithm (presented in Algorithm 3). Our comparative analysis of the three ensemble models was based on these sets of evaluation criteria.The major requirement for the implementation of ensemble learning paradigm is the existence of diversity in the system [7,56,58]. Due to the importance of diversity [69,70], we used a number of measures to ensure that our proposed ensemble is valid. Most of the diversity measures that were proposed for ensemble learning in literature were mainly for classification [71–73]. Since our work is on regression, we considered those proposed by Dutta [74]: correlation coefficient, covariance, chi-square, disagreement measure, and mutual information entropy. However, we observed that the first three measures are related to each other. Also disagreement measure is exclusively for classification ensembles similar to that proposed by Kuncheva and Whitaker [75]. Hence, in order to avoid redundancy, we selected diversity correlation coefficient (DCC) and mutual information entropy (MIE).The DCC is the degree of closeness between any two of the base learner outputs Ymand Ynsuch that:(7)ρ=∑i=1,…,N(yim−μYm)(yin−μYn)∑i=1,…,N(yim−μYm)2∑i=1,…,N(yin−μYn)2where Ymand Ynrepresent the continuous valued outputs of the models Rmand Rn. Ymand Ynare N-dimensional vectors withym=y1m,y2m,…,yNmandyn=y1n,y2n,…,yNn.The diversity of two predictors is inversely proportional to the correlation between them. Hence, two predictors with low DCC between them (high diversity) are preferred over those with high values. In this study, we defined high correlation to be greater than 0.7.MIE diversity measure is defined in terms of Eq. (7) as:(8)I(Ym,Yn)=−12log(1−ρ2)where ρ, Ymand Ynremain as previously defined in Eq. (7).We also showed diversity by using graphical visualizations. We plotted the effect of different values of regularization parameter in terms of R-Square, RMSE and MAE to determine their degree of diversity. Those graphs that show a high degree of roughness or non-smoothness would be an indication of high diversity. We were able to show pre-implementation and in situ diversity in Section 4.4, and post-implementation diversity in Section 5.The first step was to extract the various “expert opinions” on the value of C. Since none of the previous studies used our datasets, we could not use the same C values as we pointed out in Section 2.4. Hence, we derived a “digital” version of these opinions from our datasets. Taking into consideration all the divergent “expert views” in Section 2.4, we performed a sample run of a simple conventional SVM model using all the parameters that worked well in our previous studies with the same datasets and their 70:30 ratio stratification strategy [17,30,76] while varying the value of C between the extreme of 0 and 10,000. Following the common practice in petroleum engineering CI modeling, each dataset was divided into 70% training and 30% testing subsets using a randomized stratification approach [27,77,78]. With this, 70% of each dataset was used for training while the remaining 30% was used for testing. The training subset represents the cored section of the oil and gas reservoir with complete log-core data while the testing subset represents the uncored section with only the log data available while the core values are to be predicted. The division of the datasets is shown in Table 3. The optimal settings for the other tuning parameters used to determine the possible optimal values for C are:•Error allowance, Lambda=1e−7Penalty for overfitting, epsilon=0.1Type of Kernel=Polynomial (Eq. (3))Kernel step size=0.2The results of this expert opinion extraction process showing the performance of the SVM model with respect to the C values are shown in Figs. 4–9for all the datasets respectively. The figures showed that there is a lot of diversity in choosing different values for the regularization parameter as the SVM model behaved differently with each C value with respect to each dataset. Some points clearly showed the occurrence of overfitting. With this sample run, we were able to establish and confirm the existence of pre-implementation diversity. With our observation of this pre-implementation diversity of the effect of the regularization parameter in our sample run, the assertion of Cherkassky and Ma [79] that this parameter has negligible effect on the generalization performance needs to be further re-considered.From the pre-implementation diversity test results, the values of C corresponding to the points of optimal performance of the SVM sample run were extracted using the criterion of least overfitting – points with the least separation between the training and testing lines while maintaining optimality in generalization performance. Table 4shows the extracted values of C meeting the stated criterion. The extracted values were then collapsed to the 10 shown in Table 5to ensure simplicity of implementation [80–82] of the proposed ensemble model. We finally applied the six datasets, in turn, on our proposed SVM ensemble with the identified values of C serving as our “expert opinions”. We used a customized version of the original MATLAB-based Least-Square SVM code available in [83]. The choice of this version of SVM is due to reports of its excellent performance in literature [84,85].After implementing the ensemble models as detailed in the algorithms, we carried out the in situ diversity test using the measures defined in Section 4.3. The base learners were paired consecutively and the diversity measures were applied on their predicted outputs. The results of this test are shown in Table 6. Other pairs were also considered but all gave results that are very close to those displayed. From the results, both the DCC and MIE of the paired base learners are low. This is another confirmation of the existence of diversity in our proposed ensemble model. We used the average of all the C values (4200) for the Tier-2 meta-model. The outputs of the Tier-1 models were combined using the Tier-2 model. With this, the training outputs of the Tier-1 models became the training input data for the Tier-2 model with the original target values remaining unchanged. The testing outputs of the Tier-1 models also became the testing input data for the Tier-2 model with the original testing target remaining kept for evaluating the generalization capability of the ensemble model. We then used the R-Square, RMSE and MAE criteria to obtain the overall performance of the ensemble model for the purpose of comparison.For the conventional bagging technique, we also created 10 instances of SVM and gave a bootstrap sample of each dataset to each instance for training. The remaining samples were used for testing the trained instances. The same LS-SVM code [83] but customized with other user-defined functions was used to implement the bagging algorithm. The performance of this model was also measured using the R-Square, RMSE and MAE of the individual results to obtain the average for the overall performance of the ensemble model.For the Random Forest, we customized the algorithm found in MATLAB CENTRAL [86] using other functions and toolboxes from the NETLAB repository [87]. Following the usual pattern, the performance of the algorithm with each dataset was measured using the R-Square, RMSE and MAE of the individual results. Since this is the fundamental ensemble algorithm, the Mean() combination rule has been internally applied on the output of the algorithm which automatically represents the overall performance of the ensemble Tree model. So, we did not make any modification to the combination segment of the code.After the implementation of the algorithms, we performed the post-implementation diversity test. This was done by visualizing the performance of each base learner of our proposed ensemble model with respect to the evaluation criteria. The graphical visualizations are shown in Figs. 10–15for all the datasets. From the R-Square (CC), RMSE and MAE performance plots, the performance of the Tier-1 models was uniquely but consistently different from each other. The general irregularity non-smoothness of the plot lines is a confirmation of the heterogeneity in the performance of the Tier-1 models, hence confirming the existence of high diversity. The result of these three levels of diversity test (pre-, in situ, and post-implementation) is an indication that SVM with the diverse regularization parameter is an ideal candidate for ensemble modeling.When the performance of our proposed ensemble model was compared with those of the conventional SVM model, SVM with the bagging method and the Random Forest technique, the results obtained are shown in Fig. 16through 20. In terms of the R-Square criterion, Fig. 16 shows that our proposed stack generalization ensemble model outperformed the others with the highest correlation in most of the six cases. In particular, our proposed model proved to be superior to the conventional SVM and Random Forest techniques on all the datasets. However, the proposed model outperformed the SVM bagging method in four cases (Data 1 Well 1, Data 1 Well 2, Data 2 Well 1 and Data 2 Well 3) while exhibiting stiff competitions in two cases (Data 1 Well 3 and Data 2 Well 2). Between SVM Bagging and Random Forest, the former proved to be better in five cases out of the six. The reason for the lower R-Square of SVM Bagging than that of the Random Forest on the Site 1 Well 2 could not be explained at the moment. We treated it as a lone and rare case that needs to be further investigated in our future work with more and different datasets.With the RMSE criterion, the same trend of the superior performance of our proposed model over the others is shown in Fig. 17for porosity datasets and Fig. 18for permeability. Our proposed ensemble model gave the least RMSE for all the datasets. It was interesting, however, to observe that the better performance of Random Forest in terms of R-Square over SVM bagging with Site 1 Well 2 in Fig. 16 has been nullified by the lower RMSE of SVM Bagging than Random Forest (Fig. 17). That does not however preclude the necessity of conducting more studies with more and different sets of data to further investigate the comparative performance of the two techniques.In terms of MAE, our proposed ensemble model showed superiority over the others for all datasets (Figs. 19 and 20). The SVM bagging has lower errors than the Random Forest technique in five out of the six cases while there was stiff competition between SVM bagging and the proposed ensemble model on all datasets except Data 1 Well 1. In the overall evaluation, we posit that our proposed model performed the best with few cases of stiff competition with the SVM bagging method. Hence, both SVM bagging and the stacked generalization ensemble model proved to be better models for successful and improved petroleum reservoir modeling and prediction than the conventional SVM model and the Random Forest technique. This further confirmed the report of the robustness and scalability of SVM in handling data of different sizes and dimensionality [4,17,45,47,48,51,84] and the limitation of Decision Tree, which forms the basis of the Random Forest technique, in handling data of small size and high dimensionality [66–68].

@&#CONCLUSIONS@&#
