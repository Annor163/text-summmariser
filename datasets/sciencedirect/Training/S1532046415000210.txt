@&#MAIN-TITLE@&#
Efficient and sparse feature selection for biomedical text classification via the elastic net: Application to ICU risk stratification from nursing notes

@&#HIGHLIGHTS@&#
To date, ICU mortality prediction has relied on structured clinical features.However, clinical free text features seem to perform just as well, or better.Existing clinical NLP methods largely do not rely on feature selection techniques.Applying regularization can aid discovery of important features in these problems.Indeed, only a small fraction of all features are needed to predict mortality well.

@&#KEYPHRASES@&#
Text mining,Feature selection,Elastic net,ICU,Risk stratification,Machine learning,

@&#ABSTRACT@&#
Background and significanceSparsity is often a desirable property of statistical models, and various feature selection methods exist so as to yield sparser and interpretable models. However, their application to biomedical text classification, particularly to mortality risk stratification among intensive care unit (ICU) patients, has not been thoroughly studied.ObjectiveTo develop and characterize sparse classifiers based on the free text of nursing notes in order to predict ICU mortality risk and to discover text features most strongly associated with mortality.MethodsWe selected nursing notes from the first 24h of ICU admission for 25,826 adult ICU patients from the MIMIC-II database. We then developed a pair of stochastic gradient descent-based classifiers with elastic-net regularization. We also studied the performance-sparsity tradeoffs of both classifiers as their regularization parameters were varied.ResultsThe best-performing classifier achieved a 10-fold cross-validated AUC of 0.897 under the log loss function and full L2 regularization, while full L1 regularization used just 0.00025% of candidate input features and resulted in an AUC of 0.889. Using the log loss (range of AUCs 0.889–0.897) yielded better performance compared to the hinge loss (0.850–0.876), but the latter yielded even sparser models.DiscussionMost features selected by both classifiers appear clinically relevant and correspond to predictors already present in existing ICU mortality models. The sparser classifiers were also able to discover a number of informative – albeit nonclinical – features.ConclusionThe elastic-net-regularized classifiers perform reasonably well and are capable of reducing the number of features required by over a thousandfold, with only a modest impact on performance.Feature selection methods have recently been growing in importance within the fields of genomics, bioinformatics, and computational biology, where they have found wide utility in problems ranging from microarray DNA analysis to genome-wide association studies, among others [1–3]. During the course of microarray DNA analysis, for example, one common objective is to classify tumor samples from patients with cancer, based on the gene expression profiles of those samples. However, the number of genes under consideration is almost always much larger than the number of tumor samples, with only a small subset of these genes being putatively associated with the tumor classes. This problem hence exemplifies the so-called “p≫n” setting [4,5], where one is faced with many more candidate features p than examples n. Thus, an ideal classifier for this problem setting should not only be accurate and exhibit other good performance characteristics – it should be able to select only those genes that make up the pathways present in the cancer of interest, easing interpretation of the resulting predictions, while also neglecting genes that are not discriminative of the outcome.With this in mind, many parallels can be drawn between the example of microarray DNA analysis given above and the usual setting of biomedical text classification, where the goal may be to predict outcomes (i.e., mortality) from text or to perform information extraction [6], such as ongoing smoking status [7], or receiving a procedure such as mechanical ventilation in the ICU [8]. In these setting, nearly all input features derived from the underlying text are noisy in the sense that they carry little information about the outcome or clinical entity of interest, and therefore are not discriminative. The problem is also further complicated by the fact that the dimensionality of the input feature space often proves large – and can be increased even further by extracting bigram or higher-order n-gram features, or via other, more sophisticated methods of feature extraction.Very generally, feature selection algorithms for linear models, including logistic regression and support vector machines (SVM) can be classified as follows: a method may carry out explicit feature selection by setting some feature weights or parameter estimates zero based on a set of criteria (which vary with the algorithm used). On the other hand, an algorithm may instead perform shrinkage, where the feature weights are smoothly shrunk toward zero while never being made exactly zero (ridge penalty), or implicit variable selection via a shrinkage process that allows for making at least some weights exactly zero (lasso), or while also performing ridge-like shrinkage (elastic net). In the cases of the ridge, lasso, and elastic net penalties, the shrinkage and selection effects are enforced by a constraint on the feature weights, or a regularization penalty that constrains how large the weights can be while they are being estimated. The lasso penalizes the sum of the absolute values of the weights (L1 norm), while the ridge penalizes the square root of the sum of the squared weights (L2 norm), and the elastic net combines these penalties into a linear combination of the L1 and L2 norms.Two examples of explicit feature selection are stepwise regression, and exhaustive best-subset methods, which have been widely applied within the biostatistical and epidemiological literature [9], but have enjoyed somewhat less application elsewhere, particularly to high-dimensional learning and other, related contexts [10]. One reason for this is that exhaustive best-subset methods suffer from the limitation that with p candidate input features, the algorithm must train 2p−1 classifiers (less the null classifier). While forward and backward stagewise methods have worst-case O(p2) complexity, they are not guaranteed to select the best possible permutation of input features [11,12]. In either case, when p is in the hundreds of thousands to millions of features, as is usually the case when dealing with text-based problems, these approaches quickly become computationally infeasible. In addition, stepwise regression performs poorly when features are correlated; in practice, it does not exhibit a grouping effect, a desirable property of a feature selection method, where correlated features tend to be included together in a final model [13].Examples of shrinkage-based methods for linear models include ridge regression [14] and the lasso [15]. Prior to the development of the lasso, ridge regression enjoyed preeminence among shrinkage methods, in part because the ridge penalty, represented as the L2 norm of the vector parameter estimates, resembles the usual ordinary least squares objective and thus is easier to optimize [16]. In contrast, optimizing those non-smooth objective functions that include the lasso penalty requires specialized algorithms, e.g. least-angle regression (LARS) [17]. Moreover, ridge regression has generally been found to outperform lasso and exhibit a grouping effect when p<n; it is only when p grows larger than n that the performance of ridge regression begins to degrade and it no longer handles correlated features well [4]. However, the lasso penalty enforces automatic feature selection by forcing at least some features to be zero, as opposed to ridge regression, where only shrinkage is performed. Nevertheless, the use of the lasso proves problematic when at least some features are highly correlated. In this case, the lasso will select from among these features at random. Moreover, given n training examples, the lasso is capable of selecting only at most n features [13].The elastic net, in contrast, represents a compromise between the ridge and lasso penalties [13]. Indeed, the elastic net penalty is simply written as a linear combination of these two penalties: the lasso penalty term acts to encourage sparsity in the parameter estimates of the resulting model, while the ridge term acts to “average out” the parameter estimates of correlated features, which imposes a grouping effect [4,18]. Hence, the elastic net performs both shrinkage (although milder than that obtained via ridge regression) and automatic feature selection. Depending on the preferences of the user and the properties of the underlying problem, the elastic net penalty can be smoothly adjusted so as to give more weight to either the lasso or ridge penalties. Compared to the lasso, the elastic net is able to yield a model including more features p than training examples n, but with possibly far fewer than would be selected via the ridge penalty alone (depending on the parameter settings chosen), which, taken with the fact that the elastic net exhibits a grouping effect [13], represents a clear advantage over either method.Despite holding promise for feature selection and model development within the usual problem setting of biomedical text classification, the elastic net has yet to be applied toward these problems in clinical NLP. In particular, the elastic net allows those relevant word or n-gram features associated with the outcome to be discovered far more easily, and hence has the potential to supersede existing “black-box” approaches [8,19,20], e.g., unregularized SVMs, which are widely used in clinical NLP Furthermore, it is also possible that regularization techniques applied to classifiers could be used to validate and improve on what we term the “expert input” approach [7,21], where potentially relevant word or n-gram features are manually selected and extracted before a classifier is trained. In particular, Walsh and Hripcsak’s recent work [21] constitutes an example of the use of “expert input”: while they developed a series of classifiers with the aid of the lasso, using a combination of free text and a series of clinical features, these text features were manually chosen in advance based on their potential association with readmission, before combining them with other features with which to train the classifier.Here, we describe the application of elastic net regularization to a pair of classifiers developed to predict mortality risk among adult ICU patients based on the free text of their first 24h of nursing notes, and we report a sample of the relevant features discovered by these classifiers. (A full list of the features of one such classifier, along with their coefficients, is included in the Supplementary Information.) Nursing notes constitute a good candidate source of information for mortality risk prediction, as they contain a detailed and regularly-updated record of the interventions performed, medications administered, vital signs, and physical examination findings, all of which carry highly specific information about the patient’s dynamic physiological state and eventual outcome. We then characterize what we term the sparsity-performance tradeoff of both classifiers as the elastic net regularization parameter is varied. We also report examples of informative features found by the classifier, and compare them to what is currently known of predictors of ICU mortality. Finally, we also compare the performance of our models to an existing method of ICU risk stratification based on the Simplified Acute Physiology Score (SAPS) and validated on the same dataset.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
