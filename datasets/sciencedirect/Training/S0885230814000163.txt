@&#MAIN-TITLE@&#
Web-based possibilistic language models for automatic speech recognition

@&#HIGHLIGHTS@&#
We propose new statistical language models for ASR based on theory of possibility.The Web is used to evaluate the possibility of a word-sequence.Various schemes are compared for integrating possibilistic scores into ASR systems.Possiblistic models outperform significantly probabilistic ones on speciality domains.

@&#KEYPHRASES@&#
Speech processing,Language modeling,Theory of possibilities,

@&#ABSTRACT@&#
This paper describes a new kind of language models based on the possibility theory. The purpose of these new models is to better use the data available on the Web for language modeling. These models aim to integrate information relative to impossible word sequences. We address the two main problems of using this kind of model: how to estimate the measures for word sequences and how to integrate this kind of model into the ASR system.We propose a word-sequence possibilistic measure and a practical estimation method based on word-sequence statistics, which is particularly suited for estimating from Web data. We develop several strategies and formulations for using these models in a classical automatic speech recognition engine, which relies on a probabilistic modeling of the speech recognition process. This work is evaluated on two typical usage scenarios: broadcast news transcription with very large training sets and transcription of medical videos, in a specialized domain, with only very limited training data.The results show that the possibilistic models provide significantly lower word error rate on the specialized domain task, where classical n-gram models fail due to the lack of training materials. For the broadcast news, the probabilistic models remain better than the possibilistic ones. However, a log-linear combination of the two kinds of models outperforms all the models used individually, which indicates that possibilistic models bring information that is not modeled by probabilistic ones.

@&#INTRODUCTION@&#
State-of-the-art large vocabulary continuous speech recognition (LVCSR) systems rely on n-gram language models that are estimated on text collections composed of billion of words. These models have demonstrated their efficiency in a wide scope of applications but their accuracy depends on the availability of huge and relevant training corpora that may not be available, for instance for low resource languages or for specific domains.One of the most popular approaches for dealing with this lack of training data consists in collecting text material on the Internet and in estimating classical n-gram statistics on these automatically collected datasets (Kemp and Waibel, 1998; Bulyko et al., 2003). This approach benefits from two interesting characteristics of the Internet: large coverage and continuous updating.Coverage relies on the fact that the Web may be viewed as a close-to-infinite corpus, where most of the linguistic realizations may be found. The Internet provides a linguistic coverage significantly larger than the text corpora usually involved in LM training (Keller and Lapata, 2003).Updating is provided by users who continuously add documents containing new words and new idiomatic forms. This last point was largely exploited for various aspects of statistical language modeling, typically for new words discovery (Asadi et al., 1990; Bertoldi and Federico, 2001; Allauzen and Gauvain, 2005), n-gram model adaptation, unseen n-gram scoring (Keller and Lapata, 2003), etc.Nevertheless, exploiting the large coverage and the updating for statistical language modeling is limited by technical issues which are related to the size and the instability of the Internet contents. The standard approach would be to regularly collect all data available on the Internet and to estimate n-gram models on the resulting corpus. Such a technique is clearly unfeasible; some authors proposed solutions that are supposed to enable the estimation of huge LM: Guthrie and Hepple (2010) tackled footprint memory reduction of sparse n-gram models; fast smoothing techniques were proposed in Brants et al. (2007); technological solutions based on distributed data storing and processing are presented in Ghemawat et al. (2003), Chang et al. (2006). Finally, even if software and hardware technologies are continuously progressing, the training of up-to-date LMs on the whole Web contents is still a challenging problem.Another issue is related to word sequence distributions on the Web. They are poorly reliable due to the diversity of the document sources, the variability of production and usage contexts, etc. Distributions are not only unreliable, but they would not match a targeted application context which determines the potential topics, speech styles, language levels, etc.Considering these practical and theoretical limits of using the whole Web, most of the previous studies consisted in extracting relevant and tractable Web subsets, which are used as classical corpora for estimating n-gram statistics. Corpora are obtained by automatic querying search engines (Monroe et al., 2002; Wan and Hain, 2006; Lecorve et al., 2008). The query composing technique determines the corpus accuracy in terms of coverage, language styles, etc. Unfortunately, querying is based on prior knowledge or on an automatic extraction of domain-related descriptors that are potentially incomplete or inaccurate (Sethy et al., 2005). Moreover, independently of the query composing techniques, the collected data collections depend on the search strategies that are implemented in commercial engines, which may be totally or partially confidential.Even if these methods were successfully applied in various application contexts, some authors tried to get further benefit from the Web specificities by using dynamic approaches of n-gram estimate. In Berger and Miller (1998), a just-in-time adaptation process, based on an on-line analysis of the document topic and fast LM updating, is proposed. In Zhu and Rosenfeld (2001), the authors proposed a back-off technique which estimates a word sequence probability by counting the number of Web documents that contain it. This number is the number of hits returned by using a search engine which was queried with the targeted word sequence. This paper focused on LM adaptation to a specialized domain, but it introduced the idea of using a search engine for ad-hoc estimate of linguistic scores. We developed this idea in Oger et al. (2009a), where we proposed an efficient way of using Web search engine hit ratios as probabilities in an ASR system. Ad hoc n-gram estimate provides updated statistics but does not address the Web-statistic reliability issue. In order to tackle this problem, we proposed language models that take into account the existence or the non-existence of word-sequences rather than their frequencies Oger et al. (2009b). These models are based on the possibility theory which provides a theoretical framework to deal with uncertainty. We proposed a way to quantify the possibility of word sequences by querying the Web and to integrate this possibilistic measure into a probabilistic ASR system.Probability-based language models perform well in most situations, especially on high- and medium-frequency events. The low-frequency event probability estimation rely generally on a back-off or smoothing strategy, which led to less reliable probabilities. The proposed possibility language models only operate on these low-frequency events, by measuring their plausibility, which is not actually measured by the smoothing and back-off techniques used to estimate the probabilities on these events. Therefore, the proposed possibility-based language models do not replace the probability-based language models, but rather complement them in situations where they are not reliable, that is, mainly, on low-frequency events. The goal of the possibility-based language models is thus to estimate the plausibility of these low-frequency events in order to filter them when the main language model wrongly assigns them a higher probability that it should be.This paper presents an in-depth study of possibilistic language models. We will state motivations and theoretical foundations of these models as well as present a method for empirically estimating possibilities and new ways to integrate them in an ASR system. Possibilistic models are compared and combined with classical n-gram probabilities estimated on both Web and classical text corpus.Experiments are conducted on two tasks: broadcast news transcription, for which large training materials are available, and transcription of medical videos that are dedicated to training surgeons. The latter application context corresponds to a very specialized domain with only low resources available.The rest of the paper is organized as follows. The next section proposes a step-by-step description of possibilistic Web models, starting from classical corpus probabilistic models. Section 3 presents various strategies for the integration of possibilistic language models into a statistical ASR system. Section 4 describes the experimental setup and the comparative experiments that were conducted. Finally, Section 5 concludes and propose some perspectives.In this section, we present new approaches for improving language modeling by using a new data source, the Web, and a new theoretical framework, the possibility theory. We first describe the classical corpus-based probabilistic language models, as used in most of the state-of-the-art speech recognition systems. Then, we introduce a new approach for estimating these probabilities from the Web. Finally, we propose to use concepts from the possibility theory for building a new measure that can be estimated on the Web as well as on classical closed corpora: the possibility measure.In the ASR domain, language models are mainly designed with the purpose of estimating the prior probability P(W) of a word sequence W:W=(w1,w2,…,wn),wi∈VThis probability may be decomposed as the product of conditional probabilities:(1)P(W)=∏i=1nP(wi|w1,w2,…,wi−1)This formula assumes that a wordwicould be predicted only from the preceding word sequence. Globally, n-gram models consist in a collection of a conditional probabilities that will be used, in the ASR engine, for the prediction of a word, given a partially transcribed hypothesis.As expressed in Eq. (1), a word probability depends on the whole linguistic history. In practice, such long-term dependencies cannot be estimated due to complexity and to the limits of the corpus: the amount of training data required for estimating such long sequences would be huge, and it is usually impossible to perform a direct estimate of n-gram statistics of high order (n>6). Therefore, most state-of-the-art ASR systems use only 4 or 5 gram models.Some alternative approaches for linguistic scoring were proposed to enable the estimation of long-sequence probability, mainly with neural networks that offer efficient (but implicit) inference and smoothing mechanisms (Bengio et al., 2006; Mnih and Hinton, 2007). Nevertheless, the ideal situation would be to estimate directly accurate probabilities on an exhaustive corpus, where all possible sentences would be found. This would suggest that the problem of ASR could be viewed as the search of the correct transcription in a closed collection of text documents Borges (1944).Considering that we are never going to have such an infinite corpus, n-gram language models were introduced in speech recognition systems by Jelinek (1976), especially to deal with the problem of long word sequence modeling. The global approach consists in limiting the size of the history, in order to be able to perform a good estimation of conditional probabilities. Then, speech is viewed as a Markovian source of words, of order n−1:P(wi|w1,w2,…,wi−1)≈P(wi|wi−n+1,…,wi−1)By using this approximation, the probability of the word sequence W becomes:(2)P(W)≈∏i=0NP(wi|wi−n+1,…,wi−1)where the parameter n is the order of the model and N the size of the sequence W. A high value for n is supposed to improve the model accuracy but requires a larger corpus to estimate the model. As is the case for most of the parametric models involved in speech processing, the choice of the value of n results from a trade-off between accuracy and estimate requirements in terms of amount of training data and CPU consumption.The cut-off presented in Eq. 2 impacts the language modeling process by limiting the complexity of the estimation process (this was the main purpose). Nevertheless, it also causes a loss of precision, as reported in many papers whose empiric results confirmed this theoretic observation: increasing n generally yields some improvement in performance under the condition that there is sufficient amount of training data.As previously described, n-gram probabilities are usually estimated from word sequence counting on corpora. Using this approach to estimate the probability of a given word sequence on the Web requires to know the frequency of the word sequence in at least a part of the documents that can be found on the Web. In order to do this, we can rely on statistics obtained with a Web search engine: most of them provide the number of documents that satisfy a given query; this query can be an n-gram word sequence. Using the number of documents that contain a specified word sequence, we can deduce the number of n-grams. This approach is presented in Zhu and Rosenfeld (2001) where the authors propose to estimate the n-gram frequency from the document frequency with the formula:(3)fWeb(wi−n+1,wi−n,…,wi)≈α×dfWeb(wi−n+1,wi−n,…,wi)βwhere fWeb(W) is the frequency of the word sequence W on the Web and dfWeb(W) the document frequency of W. α and β are constants for a given n-gram order n.Zhu and Rosenfeld have estimated the values of α and β for n-gram orders from 1 to 3 Zhu and Rosenfeld (2001). Their results are reported in Table 1. The first point we can note is that the value of β is always close to 1. This indicates that there is a proportional relationship between the document frequency and the word frequency. Moreover, as expected, when the n-gram order increases, the value of the proportionality factor α approaches 1. For example, for n=2, the value of α is about 1.2.Given this information, we can consider that the β coefficient is about equal to 1, and we should estimate the value of the proportionality factor α for each n-gram order. However, given that the purpose of the measure is the estimation of probabilities and that in our case the estimation of such probabilities is a frequency ratio, the proportionality factors cancel each other, leaving a document frequency ratio.We thus propose to estimate the Web n-gram probabilities by relying directly on the number of documents that contain a given n-gram. For a given wordwi, we first note byψinits history of size n−1 in an n-gram:ψin=wi−n+1,…,wi−1. Thus, in order to obtain the probability of a Web n-gram, we use Eq. (4):(4)PWeb(wi|ψin)=H(ψin,wi)H(ψin)where H(S) is the number of documents that contain word sequence S retrieved by the search engine, and n is the order of the n-gram model. However, Eq. (4) is not easy to use, because it assigns a zero probability to the word sequences that are not on the Web. To tackle this issue, we usually redistribute one part of the probability mass assigned to the events seen during training to unseen events. Given that the necessary statistics for using a state-of-the-art back-off technique, such as the modified Kneser-Ney method (Goodman, 2006), are not available in this specific the Web in this manner, we will interpolate our distribution with lower order distributions, which have proven to work well. Probabilities are therefore computed by using Eq. (5):(5)PWeb*(wi|ψin)=λ1·PWeb(wi|ψin)+λ2·PWeb(wi|ψin−1)+⋯+λn·PWeb(wi)where λiare positive real numbers such that∑i=1nλi=1. However, a difficulty related to the estimation of the unigram probability still exists in this formulation.In the Web context, the frequency of a word is computed as the number of Web documents that contain this word, and the size of the corpus corresponds to the total number of documents indexed by the search engine. For an estimation of the latter value, we use the number of documents that contain the most frequent word in the natural language of interest (for English, the word the), thus hoping to cover most of the documents in this language, that are indexed by the search engine. The probability computed this way will never be zero if one makes sure that all the words in the vocabulary are present in at least one Web document.We therefore propose a Web n-gram probability estimation method that does not result in zero probabilities, even for unseen word sequences. With Web n-gram probabilities thus defined, there are several manners of using them for computing word sequence probabilities.The possibility theory is a mathematical framework devoted to handling uncertainty resulting from incomplete knowledge (Dubois, 2006). Originally designed in order to formalize the notion of linguistic uncertainty Dubois (2006), possibility theory has been recently given a formal status akin to that of the Probability theory. This advance relies on measure-theoretic concepts, thus transforming it into a quantitative framework for reasoning with incomplete knowledge (de Cooman, 1997). Therefore, possibility measure reflects the uncertainty rather than the imprecision, two concepts that are merged in probability measures.Possibility theory is based on a pair of dual functions, possibility and necessity. Possibility function, denoted π(e), represents the knowledge that distinguishes what is plausible from what is less plausible, and what is atypical from what is “normal”. This function is a mapping from a set E of events to the unit interval [0;1]. When π(e)=0, then event e is known as impossible; (ii) if π(e)=1, then event e is considered as totally possible (plausible).In a manner akin to probability theory, a possibility measure can be computed from the possibility distribution on a bounded set of events (de Cooman, 1997). Considering a set E of events, a possibility measure Π on the set of events E can be defined as:Π(E)=maxe∈Eπ(e)Therefore, the possibility of set E is the possibility of the most plausible event belonging to E. Globally, Π(E) evaluates to the extent that the set E of events is consistent with the knowledge π. For any two subsets A and B of E, the joint possibility measure of A and B is constrained by:(6)Π(A∩B)≤min(Π(A),Π(B))The application of possibility theory to language modeling relies on the fact that empirically estimated probabilities are dramatically imprecise on very low-frequency events: the unobserved (or rare) word sequences are evaluated by smoothing functions that provide very coarse approximations of low probabilities. Rather than trying to infer linguistic scores from a so partial knowledge, we aim to perform a reliable estimate of plausibility of infrequent linguistic events. This approach encounters two major problems due to this specific interest of possibility on low-probability domain and to the estimation of possibility-based linguistic scoring. Indeed, literature lacks of a practical formula for automatically estimating a possibilistic measure on a sequence of words, given a training corpus. We will fill this gap in the next sections.Numerous research reports how one can take advantage of the statistics of word sequences on the Web. In the previous section, we proposed another similar approach. Nonetheless, the absence of an n-gram from the Web could represent relevant information, which could be integrated into an LM. To the best of our knowledge, this information has never been studied in the literature; possibility theory provides a theoretical framework for modeling this information Zadeh (1978), Dubois (2006).In this section, we propose practical formulas for estimating a possibility measure for word sequence by using statistics from the Web.The possibility measure has to represent the possibility that a word sequence exists. For this, we rely on the existence of this sequence and of its sub-sequences on the Web. By existence on the Web, we mean here the fact that there exists at least one Web document that contains the word sequence under discussion. The idea is that the longer sub-sequences of a word sequence exist on the Web, the more the word sequence is possible. However, one needs to limit the search of sub-sequences in order to obtain a reliable measure. Indeed, the smaller the corpus considered for computing the possibilistic measure (here, the Web), the less the non-existence of long sequences is significant.First of all, for each desired LM order n, we recursively construct a distinct set of possibility distributions πnto π1, according to the equation:(7)πn(W)=|Wn∩Webn|+γ·|Wn∖Webn|·πn−1(W)|Wn|where W is a sequence of n or more words, Wnis the set of word sequences of size n in W, Webnis the set of word sequences of size n on the Web, ∖ is the set subtraction operator and 0≤γ≤1 is the back-off coefficient. The terminal condition for the recursion is π0(W)=0.For a given word sequence W, this distribution expresses the number of its sub-sequences of length n that exist on the Web, with respect to the total number of its sub-sequences of length n. The possibility mass that is lost because of the absence of sub-sequences of length n on the Web, is redistributed to the possibility measure of lower order. In our experiments, Web-based statistics will be estimated by requesting the Web search engine. For instance, |Wn∖Webn| is obtained by counting all the sub-sequences of size n of W that cannot be found by the search engine.The set of possibility distributions previously defined allows us to construct a corresponding set of possibility measures Πn, according to Eq. 8:(8)Πn(Θ)=maxW∈Θ(πn(W))where Θ is a set of sequences of n or more words; if Θ has only one element W, then Πn({W})=πn(W).In the previous section, we proposed a Web-based possibility measure that relies on the existence or the non-existence of a word sequence on the Web. Here, we propose to generalize the formula of the Web-based possibility distribution and thus enable the computation of a possibility measure on any arbitrary corpus.The principle of corpus-based estimator is similar to Web-based one: possibilities can be deducted from the presence/absence of the word sequences in the observation source, for instance the train corpus. Therefore, we use the same back-off strategy which interpolates high order possibilities from its sub-sequence possibilistic scores. For each desired language model order n, possibility distributions are computed recursivelyπnctoπ1c, according to Eq. (9):(9)πnc(W)=|Wn∩Cn|+γ·|Wn∖Cn|·πn−1(W)|Wn|where W is a sequence of n or more words, Wnis the set of word sequences of size n in W, Cnis the set of word sequences of size n in the corpus, and 0≤γ≤1 is the back-off coefficient. The terminal condition for the recursion isπ0c(W)=0. The higher the number of sequences W found in the corpus, the higher the possibility of W.The possibility distributionπncdefined above, in Eq. (9), allows us to derive the possibility measureΠnc, according to Eq. 10:(10)Πnc(A)=maxW∈A(πnc(W))where A is a set of sequences of n or more words; if A has only one element W, thenπnc({W})=πnc(W).We have thus proposed a formula that allows us to estimate a possibilistic distribution on the Web as well as on a classical text corpus. Unlike in classical n-gram-based language models, the possibility measures are estimated directly on complete word utterances, without computing individual possibility measures for sub-sequences.The possibility paradigm of the proposed measures is highly different from the traditional probabilistic paradigm used in modern ASR systems. In this section we propose different ways of combining this kind of measures in a state-of-the-art probabilistic ASR system.The most obvious way of using the proposed measures in an ASR system is to use them as full linguistic scores.The possibility measure can be seen as a stand-alone linguistic measure and can be used as such in ASR tasks, for example in combination with the acoustic score. Starting from Eq. (7) where the Web-based possibility of a word sequence is defined, the size and the nature of this word sequence in the context of an ASR hypothesis evaluation have to be defined. Such a word sequence of m wordswi, for i∈{1…m}, is denoted by Smand can be expressed as a set of n-sized word sequencesSnmsuch that:(11)Snm={(ψin,wi),fori∈{n,…,m}}Given that Smis the hypothesis to evaluate, in a manner akin to classical n-gram models, we can measure the possibility of its sub-sequencesSnmand combine them according to the inequality given in Eq. (6). When we do not have complete information on an event, the possibility theory compels us to choose the maximal estimate for the value of the possibility measure for this event. Thus, we can use the equality case in Eq. (6) to assign a possibility to Sm(given by Eq. (11)):(12)Πn(Snm)=min(Πn({ψnn,wn}),Πn(Snm∖{ψnn,wn}))=min(Πn({ψnn,wn}),…Πn({ψmn,wm}))=min(πn(ψnn,wn),…,πn(ψmn,wm))The shortcoming of this first approach is that it reduces the possibility of a hypothesis to its least possible element. As a consequence, if several hypotheses hold a common low-possibility sub-sequence, no matter the other sub-sequences, all these hypotheses would have the same possibility. With an aim of scoring these hypotheses, it would be better to assign different possibility values to them. Therefore, we propose to measure the possibility of the whole word sequence Sm, while verifying Eq. (6), by directly applying Equation refposs_phrase to Sm:(13)Πn(Sm)=πn(Sm)This last equation leads to get a smoothed possibility of the whole word sequence Sm, rather than the lowest possibility of its sub-sequences. The results of these approaches are presented in Section 4.4.A way of using the Web for building LMs is to consider that probabilities estimated from the Web are reliable, and thus not to interpolate these probabilities with the LM learned from the corpus; this is shown in Eq. (14):(14)Pˆ(wi|ψin)=PWeb*(wi|ψin)This approach is justified when the corpus used for learning the LM is too small or too poorly adapted to the task. In Section 4.3 we present several experiments with these two approaches, in an ASR task.In Section 2, we proposed possibilistic and probabilistic measures that can be computed on the Web, as well as on classical text corpora. To take advantage of all these measures, we need a way to combine them that respects both the possibilistic and probabilistic theories. In this section we propose several strategies in this direction.There are several definitions of the relation between possibilities and probabilities. Here, we use the definition provided in Dubois and Prade (1988), where it is stated that, for a probability measure P, the possibility measure Π that corresponds to P satisfies Eq. (15):(15)∀A⊆S,P(A)≤Π(A)where S is the set of events.This theoretical inequality is passed by in practice, due to smoothing techniques: on rare events, classical n-gram models interpolate probabilities from the scores of their sub-sequences. So, impossible word sequences will have a positive probability computed from the probability of its sub-sequences. In the most general case, it is likely that the probability assigned to rare events by the general LM be sometimes greater than the possibility assigned to this event by the possibilistic LM. We can further use this property for improving a probabilistic LM on the low probability domain.Hence, we can redistribute this exceeding probability mass between the well-learned events in the general LM. Eq. (16) formalizes this idea:(16)Pˆ(wi|ψin)=Πn(ψin,wi)f,ifΠn(ψin,wi)f<PLM(wi|ψin)β·PLM(wi|ψin),otherwisewhere Πnis a possibility measure, f is a scaling factor that controls the fraction of the probabilities affected by the cut; with f=0, no probabilities are modified. β is a normalization factor, defined in Eq. (17):(17)β=1−∑u∈UψinPˆ(u|ψin)1−∑u∈UψinPLM(u|ψin)whereUψinis the set of wordswiwith the historyψinof size n−1, for which the probability is higher than the possibility.Starting from this idea, the Web-based possibilities can be seen as upper-bounds of corpus-based probabilities, and the corpus-based possibilities can be seen as upper-bounds of Web-based probabilities.Starting from the idea that the back-off probabilities of the probabilistic n-gram language models are poorly estimated, we can use another measure for improving these probabilities.An approach, which has been used recently in Zhu and Rosenfeld (2001), consists in using the Web probability to improve the baseline LM back-off. This boils down to giving to the n-gram probabilities in the corpus a higher confidence level than to the Web-based probabilities. LetUψinbe the set of wordswiwith the historyψinof size n−1, for which the baseline LM has to back-off. In formal terms, this can be written as in Eq. (18):(18)Pˆ(wi|ψin)=ρ·PLM(wi|ψin)+(1−ρ)·PWeb*(wi|ψin),ifwi∈Uψinβ·PLM(wi|ψin),otherwisewhere ρ is a positive, empirically chosen, weighting factor, and β is a normalization factor, defined in Eq. (17).Starting from the same idea, we can combine the possibility and probability measures only when the probability measure is not reliable. The possibility measures previously introduced inform us on the confidence that we can have in the existence of a word sequence. If we have a higher confidence in the training corpus than in the Web, then all the n-grams seen in this corpus are totally possible (πn(ψin,wi)=1). On the contrary, the n-grams composed by using back-off strategies are subject to controversy. We thus propose to weight the probability that the language model assigns to the unseen n-grams in the training corpus, with the possibility estimated from the Web. This idea is formalized in Eq. (19):(19)Pˆ(wi|ψin)=Πn({ψin,wi})·bo(ψin)·P(wi|ψin−1),ifwi∈Uψinβ·PLM(wi|ψin),otherwisewherebo(ψin)is the baseline language model back-off factor. We thus redistribute, through the β factor defined in Eq. (17), the probability mass wrongly assigned to impossible events according to the Web, to the events that were seen in the training corpus.The probabilistic and possibilistic LMs can also be considered as complementary linguistic scores. In a typical ASR system, each hypothesis is assigned a score, computed as a log-linear combination between an acoustic score (S(X|W)), and a weighted linguistic probability (P(W)a, with 0≤a≤1). To improve this score, we can add other linguistic information to it, by adding terms to the log-linear combination.For instance, linguistic possibility measures can be integrated into the ASR framework as in Eq. (20), where W is a hypothesis, and X is the sequence of acoustic observations for this hypothesis:(20)S(W|X)=S(X|W)×P(W)a×Π(W)bwhere S(X|W) is the acoustic score, P(W) is the linguistic probability, Π(W) is the linguistic possibility, a and b are positive, empirically chosen, weighting factors.According to this approach, we can combine in all possible ways the four measures that we proposed for estimating the global score of the hypotheses: Web- and corpus-based possibility measures, and Web- and corpus-based probability measures.

@&#CONCLUSIONS@&#
