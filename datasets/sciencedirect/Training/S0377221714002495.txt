@&#MAIN-TITLE@&#
Optimistic MILP modeling of non-linear optimization problems

@&#HIGHLIGHTS@&#
We present a new piecewise linear approximation of non-linear optimization problems.It is a variant of classical triangulations that leaves more degrees of freedom.We show theoretical properties of the approximating functions.We provide computational evidence of the impact within MILP models.

@&#KEYPHRASES@&#
Nonlinear programming,OR in Energy,Piecewise linear approximation,

@&#ABSTRACT@&#
We present a new piecewise linear approximation of non-linear optimization problems. It can be seen as a variant of classical triangulations that leaves more degrees of freedom to define any point as a convex combination of the samples. For example, in the traditional Union Jack approach a (two-dimensional) variable domain is split by a rectangular grid, and one has to select the diagonals that induce the triangles used for the approximation. For a hyper-rectangular domainU∈RL, partitioned into hyper-rectangular subdomains through a grid defined bynlpoints on the l-axis (l=1,…,L), the number of potential simplexes isL!∏l=1L(nl-1), and an MILP model incorporating it without complicated encoding strategies must have the same number of additional binary variables. In the proposed approach the choice of the simplexes is optimistically guided by one between two approximating objective functions (one convex, one concave), and the number of additional binary variables needed by a straightforward implementation drops to only∑l=1L(nl-1). The method generalizes to the splitting ofUinto L-dimensional bounded polytopes inRLin which samples can be taken not only at the vertices of the polytopes but also inside them thus allowing, for example, off-grid oversampling of interesting regions. When addressing polytopes that are regularly spaced hyper-rectangles, the methods allows modeling of the domain partition with a logarithmic number of constraints and binary variables. The simultaneous use of both convex and concave piecewise linear approximations reminds of global optimization techniques, which are, on the one side, stronger because they lead to convex relaxations and not only approximations of the problem at hand, but, on the other hand, significantly more arduous from a computational standpoint. We show theoretical properties of the approximating functions, and provide computational evidence of the impact of their use within MILP models approximating non-linear problems.

@&#INTRODUCTION@&#
The linearization of non-linear functions plays an important role in Mixed Integer Linear Programming (MILP) approaches to the solution of non-linear optimization problems, especially in the real-world context (see, e.g., (Borghetti, D’Ambrosio, Lodi, & Martello, 2008; Irion, Lu, Al-Khayyal, & Tsao, 2012; Silva & Camponogara, 2014) just to mention a few recent ones). Classical approaches are based on piecewise linear approximation.For functionsf(x1)of a single variable, this is obtained by splitting the variable domain into intervals, and by sampling the function at their extremes: the function is then approximated by the segments produced by the convex combination of the sampled function values. Fig. 1(a) shows a function of a single variable, along with the partition of its domain into 4 regions (segmentsr1,…,r4). Fig. 1(b) gives the resulting convex combinations.For functionsf(x1,x2)of two variables, various methods provide piecewise linear approximations. The most common approaches are based on triangulations, pictorially illustrated in Fig. 2. The variable domain is split by a grid, and each cell of the grid is subdivided into two triangles: the convex combinations of the function values sampled at the vertices of a triangle uniquely provide the linear approximation. Fig. 2 reports the domain partition and the sampling points both for the original function (Fig. 2(a)) and for the corresponding piecewise-linear approximation (Fig. 2(b)). Various approaches are discussed, e.g., in Todd (1977), Babayev (1997), D’Ambrosio, Lodi, and Martello (2010).In general, we will consider a closed domainU⊆RL, and functionsf(x1,…,xL):U↦Rof L variables. We want to obtain a piecewise linear approximation of f by splitting its domainUinto a finite collectionRof L-dimensional convex bounded polytopes inRL, that we will call regions. Recalling that the interior of an l-dimensional polytope is the set of its points not belonging to any of its facets, we require that regions satisfy two constraints. Namely,C1ifr′,r″∈R, then either they have disjoint interiors orr′=r″;ifr′,r″∈R,s′is an ((L-1)-dimensional) facet ofr′ands″is an ((L-1)-dimensional) facet ofr″, then eithers′ands″have disjoint interiors ors′=s″,Let us denote the number of vertices (0-dimensional faces) of region r byν[r], and the vertices themselves byvj[r](j=1,…,ν[r]). The value of function f at the vertices is obtained by sampling the function atvj[r]forr∈Randj=1,…,ν[r]. No global knowledge is assumed for defining the approximation, i.e., the only available information is carried by samples of f. For a non-degeneracy argument, we will also impose that any regionr∈RsatisfiesC3ν[r]>Lwith at least L of theν[r]vertices linearly independent.As r is a convex bounded polytope, a pointx=(x1,…,xL)∈rcan be expressed as a convex combination of the vertices of r. The most natural approach to define a region r is to useL+1independent vertices (i.e., an L-dimensional simplex) so that the convex combination is unique as well as the resulting piecewise linear approximation of f. This is the most classical approximation method (see, Todd (1977) and Lee and Wilson (2001)).As already observed, in the case of functions of two variables (i.e.,L=2), the regions are triangles. Note that each rectangle in the sampling grid, say of verticesv1,v2,v3andv4(see Fig. 3(a)), can be subdivided into two triangles by selecting either diagonal[v1,v4]or diagonal[v2,v3]. As a consequence, a point x in the shaded area of Fig. 3(a) can be expressed as a convex combination of two distinct sets of vertices, namely{v1,v2,v4}and{v1,v2,v3}. In Fig. 3(b) we show the plot of the two possible interpolations for all the points in the shaded domain of Fig. 3(a), with dashed lines connecting each interpolation with the samples used for its computation.On the other hand, a region r does not need to be an L-dimensional simplex, and a pointx∈rcan be expressed in different ways as a convex combination of its vertices. Again for the caseL=2, this would mean to use all four vertices of the rectangle in Fig. 3(a) to define the convex combination. As it will be shown in the next sections, such a degree of freedom can be exploited to discriminate combinations among each other, for example in the case where special properties of f are known.Beyond samples on the vertices of each region, we may want to consider samples inside the regions. Hence, for eachr∈Rwe define a set ofω[r]⩾0pointswj[r]withj=1,…,ω[r]such thatwj[r]∈rand for whichf(wj[r])is known. Once the domain partitionRis decided, regionsr∈Rfor whichω[r]>0can be thought of as an “oversampled” region in which information is collected without a predetermined structure.Based on the above definitions we give the two approximating functions,fˇandfˆ, by the solutions of the two linear problems ((1) s.t. (3)–(6) and (2) s.t. (3)–(6)):(1)fˇ(x)=min∑j=1ν[r]αjf(vj[r])+∑j=1ω[r]βjf(wj[r])(2)fˆ(x)=max∑j=1ν[r]αjf(vj[r])+∑j=1ω[r]βjf(wj[r])(3)αj⩾0j=1,…,ν[r](4)βj⩾0j=1,…,ω[r](5)∑j=1ν[r]αj+∑j=1ω[r]βj=1(6)∑j=1ν[r]αjvj[r]+∑j=1ω[r]βjwj[r]=x.Among all possible convex combinations,fˇ(resp.fˆ) selects the one yielding the minimum (resp. maximum) value of the combination of the corresponding function values. Note that system above is identical for any possible choice of region r. However, in the special case in whichω[r]=0and r itself is a simplex for whichν[r]=L+1there is nothing to optimize as the convex combination is unique.In the more general case, assume now that the function f is the objective function of a non-linear minimization (resp. maximization) problem, and that such a problem is solved by approximating f withfˇ(resp.fˇ). In this case the minimization (resp. maximization) definingfˇ(1) (resp.fˆ(2)) is implied by the minimization (resp. maximization) requested by the problem.Hence, the solver of the resulting piecewise linear problem implicitly uses an approximation that yields, for each feasible point and among all the possible linear interpolations of the samples that are available due to non-simplex regions and oversampling, the one that agrees most with the sense of the optimization problem: the largest when seeking a maximum or the smallest when seeking a minimum.Actually, this is the optimistic attitude that gives the name to the proposed approximation scheme when embedded in an optimization problem.For the case depicted in Fig. 3 andω[r]=0,fˇ(x)would use the lower triangle in Fig. 3(b), andfˆ(x)the upper triangle.Note that the use of a piecewise linear approximation that may be locally convex or concave may remind of global optimization techniques (see, e.g., Belotti et al. (2013)). Actually, in that context, a convex piecewise linear under-estimation and a concave piecewise linear over-estimation of a generic non-convex function are used to create a convex polyhedral relaxation, and those under- and over-estimations are iteratively improved by branching. This leads to compute a global optimal solution for the problem at hand, with the significant difficulty that finding appropriate under- and over-estimations might be hard, especially forL>2, and refining them is computationally arduous (see again Belotti et al. (2013) and Tawarmalani, Richard and Xiong (2013)). Yet, in our approach, convex and concave approximations are not simultaneously employed but implicitly chosen by the solver and not refined over time.The result is that the computed solution is approximated, i.e., it might not even be feasible when the function f appears in a constraint of the original MINLP problem, with the relevant advantage that the out-of-the-shelf solution of a single MILP suffices.In the remainder of the paper we will show thatfˇandfˆhave favorable theoretical properties (Section 2) and that they are especially good if used within an MILP approach (Section 3), resulting in more tractable MILP models with respect to traditional models. Computational results using the proposed approximations are reported in Section 4, where we evaluate its performance in the MILP context. Some conclusions are finally drawn in Section 5.In this section we derive the properties of the piecewise-linear approximation introduced in the previous section under the assumption that the target functionf(x1,…,xL):U↦Rhas second-order derivatives arranged in an Hessian matrixHjk=∂2f∂xj∂xk(j,k=1,…,L) that is bounded, i.e., such that(7)maxj,k{maxU|Hjk|}⩽Mfor some finiteM>0.Given a regionr⊆U, we define.•its diameter as the smallest positiveδ[r]such that the Euclidean norm satisfies‖x′-x″‖⩽δ[r]for any pair of pointsx′,x″∈r;the setF▵(r)of the functionsf̃:r↦Rthat are piecewise linear interpolations of f, with “pieces” being non-overlapping simplexes whose vertices belong to{v1[r],…,vν[r][r]}∪{w1[r],…,wω[r][r]}. In other words,F▵(r)contains all possible piecewise linear interpolations based onL+1sampling points at a time;the difference between any two functions f and g restricted to r as their maximum absolute deviation, i.e.Δr(f,g)=maxx∈r|f(x)-g(x)|;the maximum error due to an approximation inF▵(r)as Strang and Fix (1973, chap. 3)(8)Dmax(r)=maxf̃∈F▵(r)Δr(f,f̃),which is known to beO(Mδ[r]2).Note that, unless assumptions can be done both on the nonlocal behavior of the function to approximate and on the shape of the regions (e.g., on the smallest angle between any two incident edges of any possible simplex with vertices in{v1[r],…,vν[r][r]}∪{w1[r],…,wω[r]}), neither the second-order trend inδ[r]nor the value of the constant implicit in theOnotation can be transformed into a stricter guarantees on the approximation error.The following result ensures that the approximations introduced in the previous section can be profitably used to reduce the complexity of piecewise-linear models of non-linear optimization problems without impairing approximation quality.Though some of the points in Theorem 1 can be inferred from ongoing studies on the properties of convex and concave envelopes of non-linear functions (see, e.g., Theorem 2.4 in Tawarmalani, Richard, & Xiong (2013)), we chose to present them in a self-contained form to integrate other issues that are equally important for our purposes like continuity of the approximation, asymptotic error bounds, optimality of approximation for convex and concave non-linear functions.Theorem 1The approximationsfˇandfˆdefined by(1)–(6)are such thatA.fˇrestricted to anyr∈Ris convex whilefˆrestricted to anyr∈Ris concave;fˇandfˆare continuous;fˇandfˆrestricted to anyr∈Rbelong toF▵(r);ifx∈r, then for any y such thatfˇ(x)⩽y⩽fˆ(x)there is a set of real numbersαj′forj=1,…,ν[r]andβj′forj=1,…,ω[r]such thatαj′,βj′⩾0∀j,∑j=1ν[r]αj′+∑j=1ω[r]βj′=1,∑j=1ν[r]αj′vj[r]+∑j=1ω[r]βj′wj[r]=x, and∑j=1ν[r]αj′f(vj[r])+∑j=1ω[r]βj′f(wj[r])=y;for anyr∈Rwe haveΔr(f,fˇ)⩽Dmax(r)Δr(f,fˆ)⩽Dmax(r);if f is linear thenfˇ=fˆ=f;if f is convex (resp. concave) in a certainr∈R, then in that regionfˇ(resp.fˆ) is the best possible linear interpolation of the samples in the sense ofΔr(f,·).if f is convex (resp. concave) in a certainr∈Rand we computefˇ(resp.fˆ) relaying on the sampling points{v1[r],…,vν[r][r],w1[r],…,wω[r][r]}andfˇ′(resp.fˆ′) relaying on the sampling points{v1[r],…,vν[r][r],w1[r],…,wω[r][r],wω[r]+1[r]}, then in that regionfˇ′is not worse thanfˇ(resp.fˆ′is not worse thanfˆ) in the sense ofΔr(f,·)and at least one pointx∈rexists such thatfˇ′(x)(resp.fˆ′(x)) gives a non-zero coefficient to the samplef(wω[r]+1[r]).For any region r, definef=(f(v1[r]),…,f(vν[r][r]),f(w1[r]),…,f(wω[r][r]))andγ=(α1,…,αν[r],β1,…,βω[r]).Further letσ(x)be the set of the coefficients of the affine combinations of the sampling points that yield x, i.e.,σ(x)=γ:∑j=1ν[r]αj+∑j=1ω[r]βj=1and∑j=1ν[r]αjvj[r]+∑j=1ω[r]βjwj[r]=x, andηthe set of nonnegative coefficients inγ, i.e.,η={γ:αj⩾0forj=1,…,ν[r]andβj⩾0forj=1,…,ω[r]}. With this, the feasibility polytope of the problem definingfˇ(x)andfˆ(x)isσ(x)∩η.A.Focus onfˇ, and consider any two pointsx1,x2∈r∈Rand a generic pointxε=εx1+(1-ε)x2forε∈[0,1]. With reference to (1), we know thatfˇ(x1)=γ(x1)fTfor some vectorsγ(x1)∈σ(x1)∩η, and thatfˇ(x2)=γ(x2)fTfor someγ(x2)∈σ(x2)∩η. Henceεfˇ(x1)+(1-ε)fˇ(x2)=(εγ(x1)+(1-ε)γ(x2))fT=γ(xε)fTwithγ(xε)∈σ(xε)∩η. It follows thatγ(xε)contains the coefficients of one of the convex combinations yieldingxεin terms of the vertices of the sampling points and, from (1),fˇ(xε)⩽γ(xε)fT=εfˇ(x1)+(1-ε)fˇ(x2), which proves the thesis.As far asfˆis concerned, given the target function f, define the functionφ=-f. Sincefˆ=-φˇandφˇis convex,fˆis concave.It follows from A thatfˇandfˆare continuous in the interior of anyr∈R. To include the boundaries of the regions in the continuity set offˇandfˆ, let us first reason from within a givenr∈Rand consider a pointx′on the boundary and a pointx″in the interior of r.Measure the distance between any two subsets S and T ofRν[r]by means of the Hausdorff distanceh(S,T)=maxsups∈Sinft∈T‖s-t‖,supt∈Tinfs∈S‖s-t‖,where‖·‖is the usual Euclidean norm. Since the two affine subspacesσ(x′)andσ(x″)correspond to each other by means of a translation proportional tox′-x″, there exists a constant C such thath(σ(x′),σ(x″))⩽C‖x′-x″‖. It follows thatlimx″→x′h(σ(x′)∩η,σ(x″)∩η)⩽limx″→x′C‖x′-x″‖=0.Hence, asx″→x′, the feasibility set of the problem definingfˇ(x′)(resp.fˆ(x′)) tends to coincide with the feasibility set of the problem definingfˇ(x″)(resp.fˆ(x″)). Since the objective function offˇ(x)(resp.fˆ(x)) is linear, hence continuous, also its values atx′andx″must tend to each other.We have thus proved thatfˇandfˆare continuous on the closure of each region. As each region has an ((L-1)-dimensional)-facet in common with every other neighboring region, this implies continuity across regions, and ultimately over the whole domainU.Sinceσ(x)is a(ν[r]+ω[r]-(L+1))-dimensional affine subspace ofRν[r]+ω[r], the vertices of the feasibility polytope of the linear programming problems definingfˇandfˆare such that at leastν[r]+ω[r]-(L+1)positivity constraints on theαj’s andβj’s are active, i.e., their solutions feature at mostL+1non-zero variables among the coefficients inγ. Hence, at each point x, the values offˇ(x)andfˆ(x)are the linear interpolation ofL+1values of the given function f at the vertices of an L-dimensional simplex containing x.Given theν[r]+ω[r]sampling points in r, one may constructN[r]=ν[r]+ω[r]L+1such simplexes by choosing anyL+1vertices from{v1[r],…,vν[r][r]}∪{w1[r],…,wω[r][r]}. Denote such simplexes assjforj=1,…,N[r], and associate to each of them the functionfj:sj↦Rthat yields the linear interpolation of the values of f at the vertices ofsj. Further letS(x)={j:1⩽j⩽N[r],x∈sj}be the set of those simplexes which contain x. Then(9)fˇ(x)=minj∈S(x){fj(x)}(10)fˆ(x)=maxj∈S(x){fj(x)},which are clearly piecewise-defined expressions.Let us now concentrate onfˇand characterize the corresponding “pieces”. Ifx′is a point in the interior of a piece, thenfˇ(x)is linear in a ballb(x′)with centerx′. Without loss of generality, we may assume that in such a neighborhood we havefˇ(x)=f1(x), and prove thatfˇ(x)=f1(x)holds for anyx∈s1. To do so, consider any pointx∈s1⧹b(x′), the segmentxx‾′connecting it withx′, and a further pointx″∈xx‾′∩b(x′)⧹{x′}, which is surely non-empty. Letε∈[0,1]be the real number such thatx″=εx′+(1-ε)x. Sincefˇis convex, we havefˇ(x″)⩽εfˇ(x′)+(1-ε)fˇ(x), from whichfˇ(x)⩾11-εfˇ(x″)-ε1-εfˇ(x′). Sincefˇ(x′)=f1(x′)andfˇ(x″)=f1(x″), we havefˇ(x)⩾11-εf1(x″)-ε1-εf1(x′)=f1(x),where we have exploited the fact thatf1is linear. The fact thatfˇ(x)⩾f1(x), paired with (9) and withx∈s1, yieldsfˇ(x)=f1(x). Hence,fˇis linear in the whole interior of simplexs1, and, from the continuity offˇ, we get that it is linear over the whole simplex and thus that it belongs toF▵(r).The same arguments prove thatfˆbelongs toF▵(r).Sincefˇ(x)⩽y⩽fˆ(x)we may setε=(fˆ(x)-y)/(fˆ(x)-fˇ(x))to havey=εfˇ(x)+(1-ε)fˆ(x). Sincefˇ(x)=γˇfTfor someγˇ∈σ(x)∩η, andfˆ(x)=γˆfTfor someγˆ∈σ(x)∩η, we get thatγ′=εγˇ+(1-ε)γˆ∈σ(x)∩ηand is such thatγ′fT=y.Immediate from C.If f is linear then its Hessian is vanishing and we may setM=0in (7), so from (8) we getDmax(r)=0. From E we finally obtainfˆ=fˇ=f.Let us first concentrate on the convex f case. By Jensen (1906) inequality, for anyγ∈σ(x)∩η, we havef(x)⩽γfT. Since the inequality holds for any convex combination we havef⩽fˇover the whole r. Pairing this with the definition offˇ, iff̃is any linear interpolation of the values of f at the vertices of r, we havef⩽fˇ⩽f̃. It follows that, at any pointx∈r,f̃(x)-f(x)⩾fˇ(x)-f(x)⩾0hence the thesis.If f is concave we may follow the same path and arrive atf⩾fˆ⩾f̃yielding the same result.Let us first concentrate on the convex f case. Since the problems definingfˇ′has one more degree of freedom (the coefficient of the additional sampling pointwω[r]+1[r]) with respect to the one definingfˇ, we clearly havefˇ′⩽fˇ. Yet, from A we also havef⩽fˇ′and thusf⩽fˇ′⩽fˇimplying thatfˇ′is not worse thanfˇin the sense ofΔr(f,·).Moreover, ifγis the vector of coefficients of any affine combinations of the sampling points{v1[r],…,vν[r][r],w1[r],…,wω[r][r]}used to computefˇ, the convexity of f and Jensen (1906) inequality ensure thatf(wω[r]+1[r])⩽γfT.Hence, at least for the computation offˇ′(wω[r]+1[r])the additional sample cannot be neglected without violating optimality requirement in (1)–(6).If f is concave we may follow the same path and arrive atf⩾fˆ′⩾fˆand at the fact that at leastfˆ′(wω[r]+1[r])involves the additional sample with a non-null coefficient.□Fig. 4depicts, for the caseL=2and a single region given by a regular heptagon, the three-dimensional plots offˇ(x)andfˆ(x)and the induced triangulations of the heptagonal region. Fig. 4(a) (resp. Fig. 4(b)) considersfˇ(resp.fˆ) forν[r]=7andω[r]=0, while Fig. 4(a′) (resp. Fig. 4(b′)) addsω[r]=3sampling points at the vertices of an equilateral triangle. The new sampling points affectfˇin Fig. 4(a′) clearly improving the approximation quality with respect to any target convex function f with those samples. The same points do not affectfˆin Fig. 4(b′) since the “direction” of its intrinsic optimism prevents such an approximation to take into account novel “bad” news about the behavior of the target function. Convexity and concavity, as well as their piecewise-linearity can be verified by simple visual inspection in all cases.Theorem 1 holds for regions that are not necessarily simplexes, and establishes the “equivalence” betweenfˇandfˆon one hand and classical piecewise linear interpolation of f within triangulated domains on the other hand. Indeed, bothfˇandfˆuse the samples in each region to implicitly triangulate it in a number of simplexes within which they are nothing but a conventional piecewise linear interpolation. For this reason, in absence of a priori information on the target function, further to its values at the sampling points, the maximum approximation error is of the same magnitude as that of any other piecewise linear interpolation.Bothfˇandfˆmay be exploited in the definition of a piecewise linear approximation of a non-linear optimization problem by means of a Mixed Integer Linear Programming problem.Formally speaking, we address the optimization problem(11)minf(x)(12)Ax⩽b(13)g(x)⩽0(14)x∈U,wheref(x):RL↦Ris a smooth non-linear objective function,Ax⩽bis a set of linear constraints,g(x)⩽0is a set of smooth non-linear constraints, andU⊆RL.Remind from Section 1 thatUis partitioned into polytopes r collected inR. Our piecewise linear approximation of problem (11)–(14) is based on the values of f and g at the sampling points. We will denote byV=∪r∈R{v1[r],…,vν[r][r]}the set of all vertices of all regions inR, which we will assume to contain a total ofν[R]elements and byW=∪r∈R{w1[r],…,wω[r][r]}the set of all the non-vertices sampling points in all regions inR, which we will assume to contain a total ofω[R]elements.The linear interpolation of the function values at the sampling points can be modeled by associating a real variableαvto any vertexv∈V(the collection of all such variables will be simply indicated byα), a real variableβwto any internal sampling pointsw∈W(the collection of all such variables will be simply indicated byβ) and a binary variablehrto any regionr∈R, and by imposing constraints that force non-zeroαvandβwvariables to be those corresponding to the same region. The piecewise linear approximation is then(15)min∑v∈Vαvf(v)+∑w∈Wβwf(w)(16)αv⩾0v∈V(17)βw⩾0w∈W(18)∑v∈Vαv+∑w∈Wβw=1(19)Ax⩽b(20)∑v∈Vαvg(v)+∑w∈Wβwg(w)⩽0(21)∑v∈Vαvv+∑w∈Wβww=x(22)αv+∑w∈W:v,w∈r∈Rβw⩽∑r∈R:v∈rhrv∈V(23)∑r∈Rhr=1(24)hr∈{0,1}r∈R,where (19) is the original set of linear constraints, (15)–(18), (20), (and) (21) express the convex combination of the vertices ((15) and (20) being the approximations of the objective function and of the non-linear constraints, respectively) and (22)–(24) impose that the convex combination uses only the vertices of the unique region r withhr=1, along with the possible additional sampling points within it. Recall that model (15)–(24) is an approximation and not a valid relaxation of the original problem (see the considerations on global optimization techniques at the end of Section 1).Traditional approaches to linear approximation assume that the regionsr∈Rare simplexes and no oversampling, which, for any given x, implies that the convex combination is unique. Model (15)–(24) is however valid for any choice of the regions and any oversampling provided Conditions C1-C3 of Section 1 are satisfied. In particular, if there are regions with more than L linearly independent vertices (see Condition C3) or additional samples are considered, then for some x values the convex combination is not unique, hence the resulting solution minimizes the objective function. We refer to such kind of situations as optimistic modeling.This model adopts a standard formulation. Effective, more complex methods, requiring a smaller number of variables and constraints, were recently proposed by Vielma and Nemhauser (2011) and are discussed and extended to oversampled cases in the second part of Section 4.From a practical viewpoint, it turns out that the smaller the number of regions inRthe lighter the resulting formulation. On the other hand, the approximation error is bounded by the diameterδ=maxrδ[r]of the widest region inR. In this section we evaluate the trade-off between approximation quality and tractability of the MILP problem. Although a specific tuning can only be done when the shape of domainUand the position of the sampling points are known, the potential of optimistic modeling may be appreciated with few considerations.Consider a normalized setting in which domainUhas an overall unit volume, and establish the approximation quality by prefixing the value ofδ. Within this bound, and depending on their shape, the regions inRwill have a maximum volume ofμso that not less thanR=1/μof them are needed in the partition ofU.Any polytope with a diameter bounded byδfits within a hypersphere of the same diameter, hence its volume is bounded from above by(δ/2)LπL/2/Γ(L/2+1), whereΓis the Gamma function. Hence,Ucannot be partitioned in less thanRσ=δ-L2Lπ-L/2Γ(L/2+1)regions. We will compare two simplex partitioning strategies with the proposed hyper-rectangular approach.One of the most common triangulations employs regions that have the same volume of the so called standard simplex, i.e., a simplex with L equal-length sides converging at right angles to a single vertex. A standard simplex with diameterδhas a volume equal toδ/2L/L!, so the total number or regions inUcannot be less thanRss=δ-L2L/2L!.If generic simplex shapes are used with diameterδ, the volume of each region is bounded from above by the volume of the regular simplex of sideδ, i.e., by(δ/2)LL+1/L!. Hence, any triangulation ofUcannot have less thanRrs=δ-L2L/2L!/L+1regions.By using regular hyper-rectangular shapes, a diameterδimplies a region volume of(δ/L)Land thus a number of regions equal toRr=δ-LLL/2.Fig. 5reports on a logarithmic scale the ratio between the minimum number of regions needed by the above tessellations and the lower boundRσ, when L varies from 2 to 7, namelyRssRσ(dots),RrsRσ(squares), andRrRσ(diamonds). It is clear that standard simplex triangulation is not the best tessellation strategy, and that triangulations in general yield a number of regions whose trend is unfavorable with respect to the one of simple rectangular tessellations. More complex tessellations are difficult to characterize forL>3because they strongly depend on the dimensionality of the domain.The most common variable domainsUare hyper-rectangles defined by bounds on the variable values. Hence, although any collection of regions satisfying Conditions C1-C3 of Section 1 can be adopted to approximate a given function, it is natural to partition the domain into hyper-rectangular regions, also because this yields the simplest structure of all. For this reason, this section is devoted to the specialization of the MILP model of Section 3 to the case in whichUis a hyper-rectangle, and samples of the objective and constraint functions are defined on a regular grid.We assume thatnlpointsx1l<x2l<…<xnl-1l<xnll(x1landxnllbeing the lower and upper bounds, respectively) are given for eachl=1,…,L, and that the pointv(j1,…,jL)=(xj11,…,xjLL)is a sampling point for any set of indicesjl=1,…,nland for anyl=1,…,L.The domain is partitioned into∏l=1L(nl-1)hyper-rectangular regions (each of them identified by the L-ple of indices identifying its “bottom-left” corner)(25)r(j1,…,jL)=xj11,xj1+11×⋯×xjLL,xjL+1Lforjl=1,…,nl-1andl=1,…,L. Sub-domainr(j1,…,jL)has the2Lverticesv(k1,…,kL)for all possible assignments ofkl∈{jl,jl+1}(l=1,…,L).When a full triangulation of an hyper-rectangularUis adopted, it is common to proceed by steps. First,Uis partitioned into hyper-rectangular sub-domainsr(j1,…,jL)as in (25). Then, each sub-domain is further partitioned intoL!equivalent simplexes that share a single edge, which is also one of the possible diagonals of the containing hyper-rectangle. Common diagonals change from one sub-domain to an adjacent one, following a regular pattern that avoids the possibility of identifying a preferred direction. Due to the well-recognizable pattern of its two-dimensional realization, such a globally isotropic triangulation is often referred to as Union Jack (Todd, 1977).Simplexes are all equivalent to the standard one and their arrangement allows the implementation of extremely effective high-dimensional interpolators when the time needed to compute the value of the approximation in a given point is the key factor (Kuhn, 1960; Rovatti, Borgatti, & Guerrieri, 1998).When straightforwardly embedded in an MILP model, this approach requires a binary variable per region (Lee & Wilson, 2001), hence a total ofL!∏l=1L(nl-1)binary variables. We next show that in the approach we propose the number of binary variables is drastically reduced by defining the regions as hyper-rectangles, hence avoiding the partition through simplexes.For the case of hyper-rectangular domains, model (15)–(24) can be stated as follows. For each vertexv=v(j1,…,jL), we representαvasα(j1,…,jL)and define the set J of all possible index L-tuplesJ={1,…,n1}×{1,…,n2}×…×{1,…,nL}along with the setJ(l,j∗)of all possible index L-tuples in which the l-th index is fixed toj∗, i.e.,J(l,j∗)={1,…,n1}×…×{1,…,nl-1}×{j∗}×{1,…,nl+1}×…×{1,…,nL}. Then, the piecewise linear approximation (15)–(24) can be expressed as(26)min∑j1=1n1…∑jL=1nLα(j1,…,jL)f(v(j1,…,jL))+∑w∈Wβwf(w)(27)α(j1,…,jL)⩾0(j1,…,jL)∈J(28)βw⩾0w∈W(29)∑j1=1n1…∑jL=1nLα(j1,…,jL)+∑w∈Wβw=1(30)Ax⩽b(31)∑j1=1n1…∑jL=1nLα(j1,…,jL)g(v(j1,…,jL))+∑w∈Wβwg(w)⩽0(32)∑j1=1n1…∑jL=1nLα(j1,…,jL)v(j1,…,jL)+∑w∈Wβww=x(33)α(j1,…,jL)+∑w∈W:w∈r(j1,…,jL)βw⩽hj∗-1l+hj∗l(j1,…,jL)∈J(l,j∗)l=1,…,Lj∗=1,…,nl(34)∑j∗=1nl-1hj∗l=1l=1,…,L(35)hj∗l∈{0,1}j∗=1,…,nl-1l=1,…,Lwhereh0l=0for eachl=1,…,L. Equations (26)–(32) and (35) are the straightforward counterparts of equations (15)–(21) and (24), respectively. Consider now (34), and letjl∗denote the unique indexj∗for whichhj∗ltakes the value one (l=1,…,L): the set of such L indices identifies the selected regionr(j1∗,…,jL∗)(see (25)). It follows that (33) automatically sets to zero allαcoefficients corresponding to vertices which do not belong to the selected region and theβcoefficients corresponding to additional sampling points outside that region.Since this is separately imposed for each of the L dimensions, the model requires in total only∑l=1L(nl-1)binary variables for modeling∏l=1L(nl-1)regions. This compares very favorably with the numberL!∏l=1L(nl-1)of binary variables required by the classical models for triangulation.Recently, Vielma and Nemhauser (2011) proposed new, more complex methods for modeling triangulations, based on properly encoded binary decision trees, which require a number of binary variables and constraints that is logarithmic in the number of regions.When applied to non-oversampled models (i.e., to models withW=∅), the method relies onB=log2(R)binary variableshj∈{0,1}forj=1,…,Band substitutes constraints (33)–(35) with2Binequalities of the kind(36)∑v∈Vjαv⩽∑u∈Hjhu+∑u∈{1,…,B}⧹Hj(1-hu)forj=1,…,2Band properly chosen setsVj⊂VandHj⊂{1,…,B}.In our setting, the same inequalities can be used to accommodate oversampling by generalizing them to(37)∑v∈Vjαv+∑∃v∈Vjw∈W:∃s∈τ(R)v,w∈sβw⩽∑u∈Hjhu+∑u∈{1,…,B}⧹Hj(1-hu)whereτ(R)is the triangulation ofR, i.e. the collection of all the simplexes triangulating each region ofR.From (33) and (37) we get that, both in the standard and logarithmic implementations, once the model is laid down encoding the samples on the grid, further samples can be injected in it by adding a single column per additional sample.Moreover, for hyper-rectangular domains sampled on regular grids, the B binary variables and2Bconstraints (36) (and thus the corresponding constraint (37)) can be partitioned in two disjoint sets, one dedicated to model the selection of a single hyper-rectangular region and the other in charge of selecting a single simplex within that hyper-rectangle. Hence, the optimistic model can be implemented exploiting the results in Vielma and Nemhauser (2011), generalized as in (37), by simply discarding the second set of binary variables and constraints (i.e., rows). In Section 4 we provide a computational comparison of the different implementations of both methods.In this section we perform a detailed computational evaluation of the proposed approximation within MILP models. Specifically, in the next subsections we consider three simple Non-Linear Programming (NLP) problems in two, three, and four variables, respectively, to be piecewise linear approximated by our optimistic modeling. Although such problems are easy and small, they allow an evaluation of the impact of the optimistic modeling within an MILP approach. The resulting MILP models were solved by CPLEX version 12.4 on a PC equipped with an hyperthread-enabled Intel Core-i7 and 16gigabyte of RAM.In Section 4.4 we consider a real-world Mixed Integer Non-Linear Programming (MINLP) problem arising in an industrial context.Consider the NLP with two variables, a non-linear objective function and a non-linear constraint(38)maxζ=e-8x-132-3y-232(39)1-10x-122-10y-122⩽0(40)(x,y)∈[0,1]2,whose optimal solution isx=0.309054,y=0.752071with objective function valueζ=0.973753.In order to piecewise linear approximate both the objective function (38) and the non-linear constraint (39) and obtain an MILP, the square domain[0,1]2is partitioned, for different values of m, into(m-1)2square sub-domainsr(j1,j2)=j1-1m-1,j1m-1×j2-1m-1,j2m-1,withj1,j2=1,2,…,m-1form∈{3,5,9,17,33,65,129,257,513}.Models usingr(j1,j2)as regions are indicated as “HR”, while models further partitioning eachr(j1,j2)into triangles following the Union Jack pattern are indicated as “UJ”. The fact that an optimistic approximation is used that implicitly considers a piecewise-linear concave interpolation of the objective function in any region is indicated with an Op superscript.The standard implementation ((15)–(24) for full UJ models and (26)–(35) for HR models) is indicated with “-std”, while the logarithmic implementation (Vielma & Nemhauser, 2011) is indicated with “-log”. When oversampling is considered, a further “-ovr” is added to indicate that 10 points, randomly drawn with a uniform probability in the rectangle containing the true optimum solution, have been added as non-structured samples.As an example, HROp-log-ovr indicates a model with hyper-rectangular regions, with a logarithmic implementation and oversampling that implicitly uses optimistic approximation of the original problem.The results are reported in Table 1. As far as the sizes of the four models are concerned, we report, for each value of m, the number of constraints (constr.), the overall number of variables (var.), the number of non-zero entries (non-zero), and the number of binary variables (0–1 var.).The performance is reported as the time needed by CPLEX to complete optimization expressed in CPLEX ticks (i.e., roughly, an estimation of the computing time that is based on the deterministic number of operations performed by the MILP solver, and that is therefore independent of the current overhead due to the machine load), the percent gain in solution time due to the adoption of an HR model instead of an UJ model, the adjustment in the constraint radius (110in the non-approximated problem) that is needed to make the CPLEX solution feasible (the constraint is non-linear and thus approximated), the value of f computed at the CPLEX solution and the corresponding percent loss with respect to the true maximum (possibly after the relaxation needed to make the CPLEX solution feasible).As a preliminary remark, note that the approximation of the constraint is very good form>3and improves consistently as m increases.Results in Table 1 are very satisfactory. The percentage speed-up obtained by the optimistic models is consistent and often very impressive. This is true both for the standard and the logarithmic implementations. It is remarkable that the reduction in the MILP model size obtained by applying the optimistic approach corresponds to a significant advantage for the MILP solver even when applied on top of the already very compact logarithmic formulation. This shows that the optimistic models are also somehow easier to “assimilate” by the solver.The quality of the solutions if no oversampling is applied is exactly the same between “UJ” and “HR”. However, if oversampling is used, a sometimes impressive improvement of both “UJ” and “HR” with respect to their standard versions can be observed, with the only exception ofm=9. Although it only happens in two cases and with a quite small difference (namely,m=129andm=257), the version of “HR” with oversampling seems to obtain slightly better solutions (in terms of quality) than the corresponding version of “UJ”. Computing times of the versions with and without oversampling are very comparable. Just to give a rough information of CPU times in seconds, the highest values, corresponding to the casem=513, are around 450 CPU seconds.Fig. 6gives an intuitive view of the effect of the piecewise linear approximation both on the objective function and on the constraint. Specifically, Fig. 6(a) reports, form=5, the plot of the optimistically approximated objective function within the approximated constraint, to be compared with the original objective function and constraint in Fig. 6(b). In both pictures the light-gray dot indicates the approximated optimal solution, to be compared with the true solution represented by the black dot.Consider the NLP with three variables, a non-linear objective function and two linear constraints(41)maxζ=[1+sin(πz2)]e-8x-15cos(2πz)-122-8y-15sin(2πz)-122(42)x+y+z-65⩽0(43)-x+y⩽0(44)(x,y,z)∈[0,1]3,whose optimal solution isx=y=0.291929,z=0.616142, with objective function valueζ=1.79436.The cubic domain[0,1]3is partitioned into(m-1)3cubic sub-domainsr(j1,j2,j3)=j1-1m-1,j1m-1×j2-1m-1,j2m-1×j3-1m-1,j3m-1,withj1,j2,j3=1,2,…,m-1and form∈{3,5,9,17,33,65,129}.As m increases, standard non-logarithmic implementations become unmanageable. Hence, for both HR and UJ models, only the logarithmic implementation (Vielma & Nemhauser, 2011) is considered in which oversampling is possibly introduced by considering 10 points, randomly drawn with a uniform probability on the cube containing the true optimum solution.The results are reported in Table 2using the same notation as in Table 1.The analysis of the results in Table 2 is quite similar to that of Table 1. The speed-up is consistent and significant, and we are considering here only the logarithmic approach. There are more differences here in the solution quality of the “UJ” models with respect to the “HR” ones, even without oversampling. Overall, as far as the objective function value is concerned, the best model is HROp-log-ovr the only exception beingm=9. Finally, though the improved performance of oversampled models comes at some computational price, differences remain reasonable.Consider the NLP with four variables, a non-linear objective function and a linear constraint(45)maxζ=1+w+x+y+z×sin2πw+15sin2πx+25sin2πy+35sin2πz+45(46)w+x+y+z-53⩽0(47)(w,x,y,z)∈[0,1]3,whose optimal solution isw=0.0623355,x=0.362335,y=0.162335,z=0.462335with objective function valueζ=2.02484.The cubic domain[0,1]4is partitioned into(m-1)4hyperrectangular sub-domainsr(j1,j2,j3,j4)=j1-1m-1,j1m-1×j2-1m-1,j2m-1×j3-1m-1,j3m-1×j4-1m-1,j4m-1withj1,j2,j2,j3=1,2,…,m-1, andm∈{3,5,9,17,33}.In this case too, as m increases, standard implementations become unmanageable. Hence, the tested models are the same as in the 3-variable case.The results, reported in Table 3, are consistent with those in Tables 1 and 2, with the only exception of small values of m (3 and 5) for which the approximated solutions are very far away from the optimal one. This is due to the high dimensionality of the problem whose geometry is badly captured by too coarse partitions. For higher values of m, the speed-up is significant, the quality improves by doing oversampling, and oversampling is not in general slower.The final step of this experimental evaluation deals with a real-world MINLP. We consider the problem of finding the optimal scheduling of a multi-unit hydro power station, known as Short-Term Hydro Scheduling problem. In a short-term time horizon it is required to maximize the power selling revenue by assuming that (a) the generation company acts as a price-taker, and (b) the electricity prices and the inflows are forecast. We consider the MINLP model presented in Borghetti et al. (2008) in which all constraints are linear, while the objective function has a non-linear term, as the power production is a non-convex, non-concave functionφ(q,v)of the water flow q and the water volume v in the reservoir.In Borghetti et al. (2008) a specific approach for the piecewise approximation of functions of two variablesf(x1,x2)has been proposed. The method, called Improved Rectangle (IR) modeling, was later analyzed in detail in D’Ambrosio et al. (2010) and consists of a “basic approximation” and of its quality improvement. The basic approximation is obtained by sampling one of the two variables, sayx2, for a fixed number of values, sayℓ, and using a piecewise linear approximation to linearize theℓresulting univariate functions. The approximation quality improvement is obtained by adding a “correction factor” that takes into account the difference between the real value of variablex2and the two closest among theℓsampling points. Although the IRmodeling is not as general as the UJ and HR ones (for example, it does not – directly – extend to functions of more than two variables and does not admit a logarithmic implementation), we include comparison with its performance mainly because it has been developed and tailored for the problem at hand.The other approximations we consider are those proposed in this paper with no oversampling.In fact, though we performed some experiments with oversampling and obtained non-negligible improvements in the quality of some of the recomputed solutions, the results are not conclusive and we decided to not report them in full detail. The reason for that is twofold. On the one side, the objective function to be approximated, involved in a maximization problem, is non-concave and we have no indication of where the true optimal solution is. Thus, the only way of performing the experiments is to oversample in each region and this leads to huge MILPs even with only one extra sample point per region. Runs exceeding memory or time limits are thus too frequent to allow a significant comparison.On the other hand, and more structurally, the non-concavity of the function sometimes causes the extra information associated with oversampling to be neglected because of the optimistic attitude of the approximation. Indeed, an extra point in the interior of a region that tightly approximates the value of the function could be ignored in the convex combination if the function is not locally coherent with the objective function of the MILP.Both these observations suggest that oversampling could be better exploited in a dynamic algorithmic approach that refines the region of interest based on intermediate MILP problems (possibly approximatively solved with light heuristic techniques). Such an approach is compatible with triangulation as well, although increasing triangulation resolution entails a complete reformulation of the model where adding oversampling in an optimistic model can be done by simple insertion of new variables in existing equations. A compromise would be to use triangulation first and then hyper-rectangular oversampling, so as to locate regions in which the objective function is locally concave or convex thus allowing a better approximation by means of cheap optimistic oversampling. Developing such an approach is beyond the scope of the current paper and could be the topic of future research.For the computational evaluation we consider a specific Short-Term Hydro Scheduling instance with 168 time periods introduced in D’Ambrosio et al. (2010).Table 4reports, for each value of the sampling parameterm∈{10,20,30,40,50}, the size of the three MILP models (HR, UJ and IR) with the same notation as in the previous tables. The table refers to the non-logarithmic implementation (which is the only one possible for the IR model).Table 4 shows, for the HROp-std model, a huge reduction in all indicators with respect to the UJ-std one: both the number of constraints and the number of binary variables are one order of magnitude smaller, while the number of variables and non-zero entries is halved. With respect to the IR model, there is the same drop in terms of number of constraints, the number of binaries and non-zero is equivalent, while the overall number of variables is one order of magnitude bigger.The results of the CPLEX computation are presented in Table 5, where we report, for each value of m and for different time limits (5, 10, 60 CPU minutes on an Intel Core2 CPU 6600, 2.40gigahertz, 1.94gigabyte of RAM running under Linux) (i) the solution value obtained at the time limit (or earlier, if CPLEX could prove optimality), recomputed using the original non-linear function; (ii) the initial and final percentage gaps; (iii) the CPU time in seconds (or “T.L.” when time limit occurred); and (iv) the number of branch-and-bound nodes. Note that, differently from the previous tables, we consider, in Table 5 and in the following ones, CPU times in seconds instead of CPLEX ticks. The reason is that ticks are especially interesting when the computing times are small so as to be able to appreciate the “granular” differences, while computing times in Table 5 are never negligible (often coinciding with the time limit), thus providing a very neat measure of the performance of the different models. The results show that size matters. Indeed the MILP resulting from the UJ model cannot be solved to optimality but for the casem=10with a time limit of 600 CPU seconds, while all other runs reach the time limit without having proven optimality, and, in most cases, without having found any feasible solution at all. Both the HROp-std and the IR models behave quite well, the former being always faster (sometimes much faster) than the latter. This is shown not only by the shorter computing times but also from the number of time limits (1 vs. 6 out of 11 runs). In addition, when both executions hit the time limit (m=30with 300 CPU seconds), the number of enumerated branch-and-bound nodes for the HROp-std model is roughly four times bigger than that for the IR model. The initial percentage gap for the HROp-std model is slightly better than that for the UJ-std model and much smaller than that for the IR one. All these indicators clearly show that, as expected, the MILPs associated with the HROp-std modeling are by far the most competitive ones.Coming to the quality of the approximation, in the unique case in which all MILPs are solved to optimality (m=10with 600 CPU seconds), the (recomputed) solution value is identical. In general, the (recomputed) solution value for the HROp-std model is better than that for the IR model (when both MILPs are optimally solved). Note that in this case it is not trivial to trace the improvement of the approximation quality when m grows, as we do not know the true optimal value.Concerning the logarithmic implementation, Table 6reports, with the usual format, the size of the HROp-log and UJ-log models. As expected, the difference in size between the two models collapses: the number of variables (both binary and continuous) is similar (but smaller for the HROp-log modeling), the HROp-log model uses few less constraints, while the number of non-zero entries, although comparable, shows a clear advantage for the HROp-log model. This results in a very similar performance of the CPLEX solver on the two versions of the MILP as shown by Table 7.Finally, in Tables 8 and 9, we compare the MILPs obtained with the HROp-std modeling with those obtained with the UJ-log (Vielma & Nemhauser, 2011). Table 8 shows that the size of the MILPs resulting from the former modeling are of course bigger than those resulting from the latter, but not much bigger. In fact, the number of constraints and binary variables is considerably larger, but, on the other hand, the number of non-zero entries is much smaller. The results in Table 9 show an acceptable difference also in terms of solution quality and CPU time for the solver. The entries in the “% err” column give the error of the solution value with respect to the value of the original non-linear function at the solution point. The solution values for HROp-std are slightly better or identical, while the solution times are larger but comparable. This is a significant advantage for the HR modeling because going logarithmic is definitely not for free in terms of complexity of the implementation.

@&#CONCLUSIONS@&#
We have discussed a novel piecewise linear approximation of non-linear optimization problems that, with respect to classical triangulations, guides the choice of the simplexes through one of two approximating objective functions. We have investigated theoretical properties of such functions and established upper bounds on the error they can produce. Moreover, we have shown that the number of artificial binary variables required within an MILP model is dramatically reduced with respect to the UJ triangulations. The proposed approach allows the use of recent methods for representing such variables logarithmically. Computational experiments highlighted the impact of the method when embedded in MILP models.