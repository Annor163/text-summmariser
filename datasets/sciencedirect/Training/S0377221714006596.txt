@&#MAIN-TITLE@&#
Analytic hierarchy process for multi-sensor data fusion based on belief function theory

@&#HIGHLIGHTS@&#
Development of a new approach to manage conflict and uncertainty of sensor readings represented with belief functions.A modified AHP method is proposed.Assessment of sensor reading weights based on multiple criteria using the AHP method.Combination of sensor readings according to their weights.

@&#KEYPHRASES@&#
Belief function theory,Multiple criteria analysis,Sensor reading weights,Uncertainty measures,Conflict measures,

@&#ABSTRACT@&#
Multi-sensor data fusion is an evolving technology whereby data from multiple sensor inputs are processed and combined. The data derived from multiple sensors can, however, be uncertain, imperfect, and conflicting. The present study is undertaken to help contribute to the continuous search for viable approaches to overcome the problems associated with data conflict and imperfection. Sensor readings, represented by belief functions, have to be fused according to their corresponding weights. Previous studies have often estimated the weights of sensor readings based on a single criterion. Mono-criteria approaches for the assessment of sensor reading weights are, however, often unreliable and inadequate for the reflection of reality. Accordingly, this work opts for the use of a multi-criteria decision aid. A modified Analytical Hierarchy Process (AHP) that incorporates several criteria is proposed to determine the weights of a sensor reading set. The approach relies on the automation of pairwise comparisons to eliminate subjectivity and reduce inconsistency. It assesses the weight of each sensor reading, and fuses the weighed readings obtained using a modified average combination rule. The efficiency of this approach is evaluated in a target recognition context. Several tests, sensitivity analysis, and comparisons with other approaches available in the literature are described.

@&#INTRODUCTION@&#
Due to its wide range of applications, multi-sensor data fusion technology has received significant attention in recent research and industry. This technology combines data from multiple sensors to achieve a complete and accurate description of an environment or process of interest. Multi-sensor fusion systems have been widely applied in various areas of robotics, including environment mapping and target recognition, detection and localization. Khaleghi, Khamis, Karray, and Razavi (2013) provided a comprehensive review for multi-sensor data fusion state of the art, exploring its conceptualizations, benefits, challenging aspects, and existing methodologies. The application of multi-sensor data fusion has attracted the attention of several researchers, including Dong and He (2007), Mercier, Cron, Denœux, and Masson (2009) to cite only a few.A single sensor may not be enough to derive a desired level of target estimation or hypothesis identification, and data fusion from multiple sensors is, therefore, often required. It allows to extract a greater volume of information and to attain a more precise level of recognition. Nevertheless, the data derived from multiple sources (signals or humans) is usually imperfect (imprecise, uncertain, and even conflicting). The imperfection and unreliability of sensor data are often attributed to technical and noise (environmental noise, presence of unknown targets, meteorological conditions, etc.) factors. Guo, Shi, and Deng (2006) classified the causes of sensor unreliability at three levels, namely the levels of the sensor, the data, and the symbol.Since multiple sensors are uncertain and conflicting, information fusion becomes a fundamental issue. In fact, most fusion systems are optimistic in that they assume that all sensors are reliable and pay more attention to uncertainty modeling and fusion methods. The performance of the fusion system is, however, highly dependent on sensor performance and adaptability to the working environment and ability to estimate the reliability of each sensor readings (pieces of evidence). Sensor reading reliability needs to be incorporated into the fusion process so as to avoid decreasing system performance (Elouedi, Mellouli, & Smets, 2004; Liu, Dezert, Pan, & Mercier, 2011; Liang, Feng, & Liu, 2010). In fact, the reliability of pieces of evidence is an index for quantifying sensor performance and weighing readings. Accordingly, weights express reliability and credibility and demonstrate the relative importance of the collected pieces of evidence.Belief function theory (Shafer, 1976) represents one of the most important tools for modeling and fusing multi-sensor pieces of evidence. It is a powerful mathematical mechanism to deal with imperfection and conflict and a flexible framework for representing and reasoning with various forms of imperfect information and knowledge. Within this theory, information fusion relies on the use of a combination rule allowing the pieces of evidence to be combined. In this context, Dempster’s combination rule (Shafer, 1976) plays a central role, since it verifies a number of interesting mathematical properties, such as commutativity and associativity. Counterintuitive behaviors for high conflicts between pieces of evidence might, however, emerge (Zadeh, 1979).Several methods have been developed to cope with the problems of conflict management (Dubois & Prade, 1988; Smets, 1990; Yager, 1987). Some of those proposals suggested other combination rules to deal with the inconvenient assignment of a total mass to a minority opinion. More recent studies (Deng, Shi, Zhu, & Liu, 2004; Florea & Bossé, 2009; Jiang, Zhang, & Yang, 2008; Martin, Jousselme, & Osswald, 2008) proposed the use of a discounting operation before combining to handle conflicting evidence combination and consider sensor reliability. Murphy (2000) presented a proposal based on the arithmetic average of belief functions to deal with the inconvenience associated with the loss of majority opinion. Later, she proposed a modified average approach where equal weights were assigned to each piece of evidence and the average operation was incorporated into the Dempster’s rule.Nevertheless, since the information sources in multi-sensor data fusion are always unequally important, the concept of weights of pieces of evidence has been introduced. In fact, the literature presents several proposals (Chen, Shi, Deng, & Zhu, 2005; Chen & Que, 2005; Deng et al., 2004) that assess the weights of pieces of evidence using a single criterion (distance of evidence or similarity measures). The use of a single criterion to assess the weight of sensor information is, however, not reliable since the mono-criteria approach is not enough sufficient to reflect reality. Conflict is not the unique criterion for use in weight assessment; sensor reading imperfection should also be taken into account for it reflects the reasonability of a given piece of evidence.Both conflict and reading imperfection can be quantified using different measures (multiple criteria). Considering that multi-criteria decision aid (MCDA) is an appropriate decision support approach that provides valuable tools for solving complex problems where multiple conflicting decision factors have to be considered, the present study opts for the application of an MCDA-based approach using the Analytical Hierarchy Process (AHP) (Saaty, 1980). Accordingly, multiple assessment criteria are introduced based on the AHP method to estimate the weights of a sensor readings set.The AHP is one of the most popular methods used in the MCDA approach. It allows users to assess the relative weight of each element of the hierarchy (criteria and decision alternatives) using pairwise comparisons. The major advantage of the AHP is its ability to decompose, in a detailed, structured, and systematic way, the decision problem into more easily comprehended sub-problems that can be analyzed independently. Despite its popularity, AHP has been criticized by several researchers for presenting a number of drawbacks. This method is reported to entail a lot of subjectivity particularly in the pairwise comparison of the different criteria involved. It may be difficult for the decision maker to judgmentally compare two criteria and assign them an objective weight according to their relative importance. AHP is also described to present a high number of judgments that the decision maker has to give on the actions set. The method is also limited by the problem of inconsistency in judgment. Another drawback of AHP relates to the lack of representation of ignorance. In fact, several attempts have been made to overcome the inadequacies of AHP. The combined DS/AHP (Dempster–Shafer/Analytic Hierarchy Process) (Beynon, Curry, & Morgan, 2000; Beynon, 2002) is a ranking approach that incorporates belief function theory with the philosophy of the AHP method.The present study is interested in the use of multi-sensor data fusion to solve conflicting and imperfect data problems and in the automation of pairwise comparisons to eliminate subjectivity and reduce inconsistency. It aims to determine the relative importance (weight) associated to each sensor piece of evidence based on a modified AHP method and to fuse the weighed readings, represented by belief functions, using a modified average combination rule. The conflict between the sensors (conflict, distance and dissimilarity measures) and the imperfection of the information provided by each sensor (contradiction, imprecision and ambiguity measures) are both taken into account during weight calculation.Section 2 provides an inventory of the basic concepts in the belief function theory. The third section presents an overview on the combination rules and approaches used for conflict management in this theory. The AHP method is introduced in Section 4. The fifth section is devoted to the presentation of assessment criteria. Section 6 presents a modified average combination rule based on AHP method and describes the process of weighing and fusing sensor readings. Section 7 provides a description for a modified version of the fusion strategy. Section 8 includes the experimental results and discussion on a target recognition application. In this application, sensor readings, represented by belief functions, are weighed and fused to reduce conflict and imperfection. Several tests, sensitivity analyses, and comparisons with other approaches available in the literature are also described. The final section contains a brief conclusion and avenues for further research.The belief function theory was initially introduced by Dempster (1967), later formalized by Shafer (1976), and axiomatically justified by Smets and Kennes (1994) in a transferable belief model. It is a general framework for modeling uncertainty and imprecision where the available information is imperfect. Furthermore, the belief function theory is considered as an interesting alternative for information fusion and decision making using combination and decision rules, respectively.A belief function model is defined by a finite and exhaustive set Θ called frame of discernment of the problem under consideration. The set containing all subsets of Θ is named the power set and denoted by 2Θ.A Basic Probability Assignment function (BPA) is a mapping m: 2Θ→[0, 1]. It assigns to every subset A⊆Θ a number m(A), called the mass of A which represents the degree of belief attributed exactly to A, and to no one of its subsets. This function must satisfy the following conditions: m(∅)=0, andΣ{m(A)/A⊆Θ}=1.When m(A)>0, A is named focal element of m. The set of focal elements of m is notedIand the pair(I,m)is called body of evidence (BOE).A BPA can equivalently be represented by its associated belief and plausibility functions. A belief function is a mapping Bel: 2Θ→[0, 1], defined as:(1)Bel(A)=∑B⊆Am(B)∀A⊆ΘBel(A) measures the total belief completely committed to A⊆Θ.A plausibility function is a mapping Pl: 2Θ→[0, 1], defined as:(2)Pl(A)=∑B∩A≠∅m(B)∀A⊆ΘPl(A) can be viewed as the maximum amount of belief that could be potentially given to A. It is also possible to state that plausibility can be derived from belief:(3)Pl(A)=1-Bel(A‾)whereA‾is the complement of A.In the belief function theory, total uncertainty (total ignorance) is expressed by m(Θ)=1 and m(A)=0 for all A≠Θ. The associated belief function is defined by: Bel(Θ)=1 and Bel(A)=0 for all A≠Θ, and is called vacuous belief function. Total certainty is expressed by m({θi})=1 for one particular element of Θ and m(A)=0 for all A≠{θi}.According to Smets’ two-level view (credal level and pignistic level) in transferable belief model (TBM) (Smets & Kennes, 1994), a BPA m must be transformed into pignistic probability function BetP. This transformation consists of equally distributing each mass m(A) among the statements that compose A⊆Θ. Formally, BetP is defined as:(4)BetP(A)=∑B⊆Θm(B)|A∩B||B|∀A⊆Θwhere ∣B∣ is the cardinality of a subset B. BetP(A) can be viewed as betting commitment to A and represent the total mass value that A can carry.Dempster’s rule of combination is a useful operation that plays a crucial role in the belief function theory. Let us denote by miand mjtwo BPAs obtained from two distinct sources i and j in the same frame of discernment Θ. According to Dempster’s rule of combination (Shafer, 1976), we have:(5)m(A)=11-K∑B∩C=Ami(B)×mj(C)∀A⊆Θ,whereK=m⊕(∅)=∑B∩C=∅mi(B)×mj(C)The function m is called the orthogonal sum of miand mjand is denoted by m=mi⊕mj. The coefficient K=m⊕(∅) reflects the conflict between miand mj. The denominator 1−K is a normalization factor. It guaranties that no belief is associated to empty set and that the total belief is equal to one. The function m is not defined whenever K=1. In this case, the two belief functions are totally contradicting each other. When K=0, however, no conflict occurs between the two belief functions. The coefficient K can be viewed as the global conflict of combination (Shafer, 1976). When there are high conflicts between bodies of evidence, the Dempster’s combination rule cannot be used since counterintuitive behaviors will generate (Zadeh, 1979).Several rules and methods have been later proposed to further develop conflict management within the belief function theory. Yager’s rule regards that the conflict mass is transferred to the frame of discernment (Yager, 1987). Moreover, the Dubois and Prade’s rule argues that the combined belief mass resulting from the pair of focal elements B and C is transferred to subset B∩C if B∩C≠∅ and subset B∪C if B∩C=∅ (Dubois & Prade, 1988). Another important development is the Smets’ rule where the conflict is stored in the belief mass given to the empty set and the belief mass is not normalized (Smets, 1990). Although the Yager’s and the Dubois and Prade’s rules allow for the combination of highly conflicting information sources, they are not associative and do not solve the problem of unreliable sources. The Smets’ rule is, on the other hand, a useful tool for combination, but only when the frame of discernment is not exhaustive. In addition, it does not take BOEs unreliability into account. Accordingly, the search for viable methods to solve conflict continued and still continues. A number of researchers (Deng et al., 2004; Destercke, Dubois, & Chojnacki, 2006; Jiang et al., 2008; Jousselme, Grenier, & Bosse, 2001; Martin et al., 2008) used the discounting operation before combining. They developed approaches assessing the reliability degrees of each body of evidence.Other rules have been proposed to deal with the inconvenience of the loss of majority opinion. Murphy (2000) has, for instance, proposed, for the first time, the average rule where the belief mass of a subset A⊆Θ provided by independent sources are averaged to determine the global belief mass on A. Let us suppose n information sources, with each one providing a BOE (Ii, mi). The average rule is defined as:(6)m(A)=1n∑i=1nmi(A)∀A⊆ΘandA≠∅This rule does not, however, offer the convergence toward certainty. In fact, it is not always adequate to yield reasonable results, particularly when the evidence has a high degree of conflict. For this reason, Murphy has proposed another combination rule based on the idea of incorporating the average operation into the Dempster’s rule of combination. If we have n BPAs to be combined, they are at first aggregated using the average rule (Eq. 6). The induced BPA are then repeatedly combined by the Dempster’s rule n−1 times. The result is the combined BPA using the modified Murphy’s average rule.Murphy’s averaging rule is recommended in cases where the objective is to preserve the opinion of the majority when one source contradicts with several other sources that are consistent (Liu, 2006). In this rule, however, all information sources are equally important, and, therefore, have the same weight (1/n) in the sum of evidence (Eq. 6), which is not always reasonable in real cases.Because of their high stability, some sources (sensors) may be more reliable, and since the reliability of a sensor reflects its importance, they are more important than others. It is also worth noting that the way the weight of sensor BOE is assigned is very important particularly when the BOEs provided by the multi-sensors have a high degree of conflict. The literature (Chen et al., 2005; Chen & Que, 2005; Deng et al., 2004) presents several approaches that estimate BOEs weights using distance (similarity and dissimilarity) measures. The use of a single conflict measure is not, however, sufficient to assess the weights; other measures dealing with imperfect data should also be taken into account. The importance of sensors BOE is, therefore, shown to depend on several criteria. Accordingly, this study opts for the application of the modified AHP method to estimate reading weights and to subsequently implement the modified Murphy’s average rule.Several studies used only a single criterion to estimate the reliability attributed to each BOE. In fact, while certain approaches were based on the quality of information (calibration or specificity (Destercke et al., 2006; Dubois & Prade, 1985), non-specificity (Beynon, 2005), and uncertainty (Harmanec & Klir, 1994; Klir, 1993) measures), other approaches were based on the conflict between sensors. In fact, Martin et al. (2008) proposed an estimation of BOEs reliability based on the distance of evidence. Besides, Liu et al. (2011) introduced a new dissimilarity measure to assess discounting factors. In addition, Deng et al. (2004) and Chen et al. (2005) used the similarity measure to determine weights. Florea and Bossé (2009), on the other hand, characterized BOEs reliability on the base of dissimilarity measures.Though using different measures, most of previous studies used a single criterion for the estimation of BOEs weights. Considering that single-criterion approaches do not reflect real world situations, the present study adopts a multi-criteria approach wherein criteria can be classified into two broad classes. The first class of criteria relates to the imperfection of information provided by each sensor (contradiction, imprecision, and ambiguity measures). The second class pertains to the conflict between sensors BOE (conflict, dissimilarity, and disparity). Combined together, all these criteria are deemed to yield into a more efficient evaluation of reading weights.The first class of assessment criteria concerns the imperfection in the BOE provided by each sensor and includes contradiction and imprecision, and, hence, ambiguity. In the belief function theory, the imperfection comes from two distinct aspects (Harmanec & Klir, 1994), namely contradiction (strife) and imprecision (non-specificity). Another measure of imperfection, entitled amount of uncertainty (ambiguity), captures both strife and non-specificity. It is not, however, a composite of those two aspects of imperfection measures.Contradiction is connected with dissonance and confusion in the evidence. These latter are encountered in the BOE whenever non-zero degrees of belief are allocated to disjoint subsets of a universal set. This derives directly from the fact that the element of interest cannot belong to only one or several disjoint subsets of the universal set. Whenever the evidence suggests that an element can belong to two disjoint subsets, then a clear conflict is present in the evidence. Klir and Parviz (1992) introduced the strife measure St(m), to expresses the conflict among disjunctive set-valued statements, which is defined as follows:(7)St(m)=-∑A∈Fm(A)Log2∑B∈Fm(B)(|A∩B|/|A|)This measure presents a number of properties proved by Vejnarovà and Klir (1993). When m represents a probability measure, then St(m) reduces to Shannon entropy. St(m) reaches its minimum, St(m)=0, if and only ifIcontains a single focal subset (m(A)=1). In this case, the BOE (m,I) can be said to not contain a contradiction. St(m) reaches its maximum (St(m)=Log2∣Θ∣), only for the uniformly distributed probability measure (m({θi})=1/∣Θ∣ ∀θi∈Θ). In this case, the BOE is totally dissonant. The most conflicting BOE is obtained by distributing the masses of probability among the maximal number of disjoint subsets and by equally assigning them among these subsets so as to achieve maximal contradiction. Accordingly, the strife measure is selected in this work as a criterion to minimize, which allows the assessment of imperfect data.The degree of imprecision of a BOE (m,I) can be seen as a degree of non-specificity (Dubois & Prade, 1985; Yager, 1983). In fact, the larger the subset is, the less specific the characterization is. The measure of non-specificity was first proposed for possibility and necessity measures. It was later generalized by Dubois and Prade (1985) for belief functions. The measure of non-specificity is defined as:(8)I(m)=∑A∈Fm(A)×Log2[|A|]It follows that, I(m) reaches its minimum (I(m)=0) if and only if all focal elements of m are singletons (probability measures). In fact, a probability measure corresponds to maximally precise statement, and we have a specific BOE. The maximum of I(m) is reached when we have total ignorance (m(Θ)=1); then I(m)=Log2 [∣Θ∣]. In fact, the BOE expressing total ignorance models the most imprecise statement on Θ. Hence, the more important the degree of belief attributed to focal elements of great cardinality is, the more imprecise the BOE becomes. The present work, therefore, selects the non-specificity I(m) as a criterion to minimize.Harmanec and Klir (1994) defined an amount of uncertainty measure (AU) ingrained in any BOE as:(9)AU(m)=Max℘Bel-∑θi∈ΘpθiLog2pθiwhere the maximum is taken over all probability distributions (pθi/θi∈Θ) that satisfy the constraints:pθi∈[0,1]∀θi∈Θand∑θi∈Θpθi=1;Bel(A)⩽∑θi∈Apθi⩽Pl(A)∀A⊆Θ.This measure, also called the aggregate uncertainty, presents a number of properties proved by Harmanec and Klir (1994). When (I, m) generates a probability measure, AU(m) is once again Shannon’s entropy. AU(m) is minimal (AU(m)=0) if and only if a BOE contains a single focal singleton θi∈Θ such that m({θi})=1. In this case, a certain BOE is obtained. The maximum of AU(m)(AU(m)=log2∣Θ∣) is obtained in several cases including the following:1 – The uniformly distributed probability measure, 2 – the uniformly distributed BPA over all possible subsets of 2Θ(m(A)=1/(2∣Θ∣−1)∀ A∈2Θ, A≠∅), and 3 – the total ignorance.In the first two cases, this situation induced that a partially uncertain BOE is obtained, whereas the third case implies that a totally uncertain BOE occurs. Hence, the lower the amount of uncertainty measure value is, the less uncertainty the BOE contains. Therefore, this measure is a criterion to minimize.The second class of assessment criteria relates to conflicting data. The definition of the conflict between two BOEs can be interpreted qualitatively as the fact that one sensor strongly supports one target and the other strongly supports another target, and both targets are different (Liu, 2006; Liu et al., 2011). Conflict can be quantified taking both conflict (m⊕(∅)) and distance measures (dissimilarity and disparity) into account. A more efficient description of the conflict between two sensor readings would be obtained when these measures are considered together.Let us consider two BOEs (Ii, mi) and (Ij, mj) provided by two sensors i and j. In belief function theory the coefficient K (Eq. 5) has long been interpreted as the global conflict between these two BOEs (Dempster, 1967; Klir, 1993; Ristic & Smets, 2006; Shafer, 1976). The weight of conflict between two BOEs (Ii, mi) and (Ij, mj) should be monotonically increasing with K and is usually expressed in the literature (Klir, 1993; Ristic & Smets, 2006) by the following function:(10)Conf(mi,mj)=-Log2(1-K)Conf(mi, mj), is called Shafer’s weight of conflict. If (Ii, mi) and (Ij, mj) do not conflict at all, Conf(mi, mj)=0 and if they totally conflict each other, Conf(mi, mj)=∞.Using the Shafer’s weight of conflict measure, we can define the conflict measure between one sensor i and the other n−1 sensors. Let us consider a set ξ of n BOEs, ξ={1,2,…,n}. To determine the conflict of each BOE, we must quantify the conflict of a given BOE i∈ξ, with an artificial BOE a (Ia, ma) where mais the average BPA of all the BOEs in ξ except i, using the following formula:(11)ma=∑j=1,j≠inmj/n-1ma can be used as an estimation of the majority opinion (Murphy, 2000). So, using Eq. (10), we can calculate the conflict between (Ii, mi) and (Ia, ma), which is called conflict measure and is denoted by Conf(i)=Conf(mi, ma).The smaller the conflict value is, the less conflicting the BOE is, and the more effect on the final combinatorial results it has. Hence, conflict is a criterion to minimize.Jousselme et al. (2001) proposed a method for measuring the distance between BOEs called dJ. This method has been used by Beynon (2006) to develop a consensus building approach to group decision making using DS/AHP, as well as to elucidate inter-group alliances. This distance is an appropriate measure of the difference or the lack of similarity between any two BOEs. The more two BOEs are far from each other, the more they are dissimilar. Hence, the dissimilarity between two BOEs (Ii, mi) and (Ij, mj) supplied by the sensors i and j, respectively, can be defined by:(12)Diss(i,j)=dJ(mi,mj)=12m→i-m→jTD̲̲m→i-m→jwhereDis a (2Θ×2Θ)-dimensional matrix whose elements are:(13)dhl=1ifAh=Al=∅|Ah∩Al|/|Ah∪Al|∀Ah,Al∈2Θm→iandm→jare 2Θ-dimensional column vectors having the coordinates mi(Ah) and mj(Ah), respectively. In particular, if two different BOEs (Ii, mi) and (Ij, mj) have focal elements with no common objects, Diss(i, j) will be almost equal to one. However, if these two BOEs have the same focal elements(Ii=Ij), Diss(i, j) will be close to zero.In order to evaluate the dissimilarity between a given BOE i∈ξ and all other BOEs belonging to this set, we must calculate an average dissimilarity for each BOE. Let us consider a set ξ of n BOEs, ξ={1,2,…,n}. The average dissimilarity between one BOE i and the other n−1 BOEs can be expressed by the mean of Diss(i, j) for all j≠i as follows:(14)Diss(i)=∑j=1,j≠inDiss(i,j)n-1The smaller the dissimilarity is, the less conflicting the sensor BOE is, and the more important it becomes. Hence, the dissimilarity measure is a criterion to minimize.Tessem (1993) proposed a distance between betting commitment (TBM distance) of two BOEs denoted as DifBetP. This distance is based on the difference between two pignistic probabilities associated to two BOEs for judging whether the latter are disparate or not. Formally, let us consider BetPiand BetPjpignistic probabilities derived from two BOEs (Ii, mi) and (Ij, mj) supplied by the sensors i and j, respectively as follows:(15)Disp(i,j)=DifBetP(i,j)=MaxA⊆Θ(|BetPi(A)-BetPj(A)|)TBM distance has been extensively used owing to its implementation simplicity. In addition Liu (2006) demonstrated that DifBetP(i, j) values are consistent with the severeness of the degree of disparity in beliefs. Moreover, this measure has been used by several authors to represent conflict between BOEs (Tessem (1993), Bauer (1997), Liu (2006), Ristic & Smets (2006, 2007)).Let ξ be a set of n BOEs, ξ={1,2,…,n}. To compute an average disparity for each BOE, we must calculate the maximum of differences between the pignistic probabilities associated to a given BOE i∈ξ and a BOE j∈ξ for all j≠i. Thereafter, we can express the average disparity for one BOE i and the other n−1 BOEs using Disp(i, j)∀j≠i as follows:(16)Disp(i)=∑j=1,j≠inDisp(i,j)n-1The smaller the disparity is, the less the sensor BOE conflict is, and the more credible it becomes. The disparity measure is, therefore, a criterion to minimize.The AHP method, first introduced by Saaty (1980), is one of the most popular and commonly used decision support tools in the MCDA approach. This method has been successfully applied in solving real world problems a wide range of areas, including health care, government, industry, education, logistics, business, environment, manufacturing, military, marketing, agriculture, and sports (Ho, 2008; Omkarprasad & Sushil, 2006). It is a multi-criteria decision-making method that uses hierarchic or network structures to represent a decision-making problem and develops priorities for the alternatives based on the decision maker judgments throughout the system (Saaty, 1980). It starts from the binary comparison matrices to arrive to assess a weight vector that allows for comparing the actions of the decision problem. This method is divided into five steps:1.Decompose the decision problem into a hierarchical structure, define the decision goal, place alternatives for reaching this goal, and set the criteria and sub-criteria allowing the evaluation of these alternatives.Evaluate the elements of the hierarchy by comparing them in pairs to obtain pairwise comparison matrices A whose elements aijare the decision maker relative preference degree of criterion i over each criterion j. These elements verify the following properties: aij>0 and aij=1/aji. For the pairwise comparison, Saaty (1980) proposes a scale composed of nine levels. A pairwise comparison matrix is perfectly consistent when, for all i, j, and h, aij=aih×ahj.Determine the relative importance (priority or weight) of each hierarchical structure element (criterion or alternative) from pairwise comparison matrices.We begin by normalizing the matrix A, we obtain:A′=aij′i=1⋯nj=1⋯nwhereaij′=aij∑i′=1nai′jFrom the normalized matrix A′, we obtain the vector w=(wi)i=1⋯naswi=∑i=1naij′n, w is called weight vector (eigenvector). The higher this value is, the more important the corresponding element is.Check the consistency of the judgments and eventually revise the comparison matrices in case of large inconsistencies.A pairwise comparison matrix is consistent when, for all i and j, aijis equal to the ratio: aij=wi/wj. It is preferable that the property of assessment transitivity is verified. That means that for i, j, and k, aij×ajk=aik. It is evident that the consistency is equivalent to the transitivity.More precisely, it can be shown that Aw=λmax×w, where λmaxis the maximum eigenvalue of the pairwise comparison matrix. We have complete consistency when λmax=n (matrix’ order). In the case of inconsistency, Saaty (1980) developed a consistency test that allows a certain level of acceptable deviation. The inconsistency level of a given pairwise comparison matrix can be measured by a consistency ratio CR=IC/RI, where IC=(λmax−n)/(n−1) is an inconsistency coefficient and RI is a random index whose values depend on matrices order. If CR⩽0.1, the inconsistency is acceptable. Otherwise, the decision-maker is asked to revise the binary comparison matrix until an acceptable level of consistency is achieved.Synthesize judgments to yield overall weights for the hierarchy. Once the weights for all elements represented in the hierarchy are determined and the level of consistency is reached, the weights are synthesized to find out the overall score of each element using the weighed sum.Among the other multi-criteria methods, the AHP presents several advantages. It has the advantage of flexibility and ability to check inconsistency through the use of inconsistency rate. It also offers the opportunity to decompose a decision problem in a hierarchical structure of criteria, thus making the importance of each criterion clear (Macharis, Springael, De Brucker, & Verbeke, 2004). Moreover, it helps to capture both subjective and objective evaluation measures. While providing a useful mean for checking the consistency of the evaluation measures and alternatives, AHP reduces bias in decision making. Last but not least, AHP is uniquely positioned to help model situations of uncertainty since it is able to derive scales where measures do not normally exist (Millet & Wedley, 2002).In order to implement the AHP method, Saaty developed a multi-attribute decision support software tool entitled EXPERT CHOICE. This program allows for the building of the hierarchic structure and its associated matrices easily and for automatically solving the priority vectors at all levels.Multi-sensor data fusion systems have been widely employed in a wide range of domains, including surveillance and target recognition. They take advantage of using multiple sensors to get a broader view of the target under observation. This multiplicity of sensors can lead to highly conflicting pieces of evidence (BOEs), a conflict that can be efficiently resolved through combining the collected readings to acquire rapid and reasonable recognition results.As all sensors do not have the same importance, the interest should be focused on weight determination so as to verify which one is more credible. In fact, credible sensor readings should be more important than those of incredible ones. Weights provide the analyst with a valuable tool to quantitatively measure the importance and credibility of sensor readings. The weight that one can associate to the BOEs has, therefore, to be expressed. In this study, we propose to use a modified AHP method to define BOEs weights wi. We start by building a hierarchical structure, then construct the pairwise comparison matrix, and conclude by determining and synthesizing weights.The weight assessment approach proposed in this work consists of presenting classes and criteria into a hierarchical structure that will be employed by the multi-sensor data fusion system for the evaluation of the sensor reading weights. This hierarchy is neither exhaustive nor complete. It is also a possible and not unique representation of the weighing assessment process. The suggested hierarchical structure is kept to a simple form, but is explanatory in nature. We consider in our approach a three-level hierarchy with n possible sensor alternatives in the bottom level, as shown in Fig. 1.We define the top goal of the hierarchy and represent it at the top level of the hierarchy (level 0), i.e weight evaluation of the sensor BOEs. Thereafter, we represent the decision classes of criteria (level 1), namely imperfect data and conflicting data, which are supposed to affect the top goal.Each class is decomposed into more elementary constituents that represent the problem criteria (level 2): contradiction, imprecision, and amount of uncertainty are related to the first class, and conflict, dissimilarity, and disparity to the second class. They directly influence the elements of the level just above and are directly influenced by the level just below. The last level, at the bottom of the hierarchy, concerns the different alternatives (sensors BOEs) contributing to target recognition. Once again, we underline that the proposed decomposition of the structure is flexible and not absolute, and might, in some specific cases, require slight modifications.In the AHP method, the decision maker is invited to compare, firstly, each criterion k with each criterion h and, secondly, each BOE (sensor reading) i with each BOE j to generate the pairwise comparison matrices. These matrices are generated in a subjective way. In order to remove subjectivity and improve the inconsistency ratio, we propose a modified AHP that maintains the advantages of Saaty’s AHP and improves its weakness by the automation of the determination of the pairwise comparison matrix elements (akhfor all criteria k=1, …, m and h=1, …, m and aijfor all BOEs i=1, …, n and j=1, …, n).In our approach, we consider six criteria:–C1: Contradiction quantified by St(mi)C2: Imprecision quantified by I(mi)C3: Amount of uncertainty quantified by AU(mi)C4: Conflict quantified by Conf(i)C5: Dissimilarity quantified by Diss(i)C6: Disparity quantified by Disp(i).We calculate the evaluation gk(i) of each BOE i (i=1, …, n) according to each criterion Ck(k=1, …, 6) using Eqs. (7), (8), (9), (10), (14), and (16). Then, g1(i)=St(mi), g2(i)=I(mi), g3(i)=AU(mi), g4(i)=Conf(i), g5(i)=Diss(i) and g6(i)=Disp(i). The values of gk(i) are elements of the decision matrix G.Example 1Let us consider the following three BPAs on the frame Θ={θ1, θ2, θ3}:m1({θ1})=0.8,m1({θ2})=0.1,m1(Θ)=0.1m2({θ1})=0.2,m2({θ1,θ2})=0.1,m2({θ2,θ3})=0.7m3({θ2})=0.8,m3({θ3})=0.1,m3({θ2,θ3})=0.1Table 1summarizes the results obtained applying the equations of Section 5.In order to obtain close values for gk(i) defined on the same measurement scale, we proceed by normalization based on Max–Min-Scaling. This operation allows the generation of another matrix denoted G′ whose elementsgk′(i)∈[0,1] are called dimensional index and are defined by:(17)gk′(i)=gk(i)-mini(gk(i))maxi(gk(i))-mini(gk(i))The Max–Min-Scaling aims for linearly transforming data so that the minimum and the maximum of the obtained values are respectively 0 and 1.Table 2shows the results after the normalization using Eq. (17) for the Example 1.For a pair of criteria k and h, the elements of the modified pairwise comparison matrix are given by:(18)akh=∑i=1nexpgk′(i)∑i=1nexpgh′(i)∀k,h=1,…,mIn this case, the auto-generated pairwise comparison matrix is perfectly consistent since it is evident that akl×alh=akhholds for all pairs of criteria (consistency ratio is null).Proofakl×alh=∑i=1nexpgk′(i)∑i=1nexpgl′(i)×∑i=1nexpgl′(i)∑i=1nexpgh′(i)=∑i=1nexpgk′(i)∑i=1nexpgh′(i)=akh∀k,h,l=1,…,m□The elements of the pairwise comparison matrix of Example 1are presented in Table 3.According to each criterion k, we define the deviation matrix Dkwhose elementsdijkare defined as follows:(19)dijk=gk(i)-gk(j)ifthecriterionistomaximisegk(j)-gk(i)ifthecriterionistominimise∀i,j=1,…,nFrom this definition, it is clearly seen thatdijk=-djik.We present in Table 4deviation matrix of Example 1 according to the amount of uncertainty criterion.For eachdijkwe associate anaijk(element of the modified pairwise comparison matrix).). Ifdijk=0then the two alternatives i and j are equally important. Hence,aijk=1according to the Saaty’s scale. For the maximum value ofdijk,iis absolutely more important than j and thenaijk=9(the maximum value of Saaty’s scale). We deduce the modified pairwise comparison matrix which is automatically generated using the proposed linear transformation:(20)aijk=1-αkdijk+βifdijk<01ifdijk=0αkdijk+βifdijk>0We can prove for all criterion k that(21)β=1andαk=Smaxk-1maxijdijkProofWe associate to each zero in the deviation matrix the score 1 and to the largest value the maximum scoreSmaxk. According to Saaty’s scale, the maximum score associated tomaxkmaxijdijkis 9. Hence, for each criterion k, we associateSmaxkwhich is proportional tomaxijdijkand deduced as follow:(22)Smaxk=9maxkmaxijdijk×maxijdijkWhendijk>0, we use linear interpolation to deduce the correspondingaijk.dijk-0/aijk-1=maxijdijk-0Smaxk-1⇒aijk=Smaxk-1maxij(dijk)×dijk+1Since we haveaijk=αkdijk+βthen:β=1andαk=Smaxk-1/maxijdijk. □The linear transformation (Eq. 20) ensures thataijk=1/ajik.ProofWhendijk<0we haveaijk=1/-αkdijk+βSincedijk=-djikwe haveaijk=1/αkdjik+β=1/ajik. □This method for the auto-generation ofaijkguarantees the consistency (CR<0.1). If we would like to improve the consistency ratio, we can change the maximum score associated tomaxkmaxijdijk.Let us return to Example 1. We aim in this step to determineaijk’s values. For illustration, we consider onlyaij3. According to the deviation matrices (D1, …, D6) we infer that:maxijdij3=0.859;maxkmaxijdijk=maxijdij4=0.988.Then:Smax3=[9/0.988]×0.859=7.827(Eq. (22)) and α3=(7.827−1)/0.859=7.945 (Eq. (21))Based on αkand β, we calculate the pairwise comparison matrix elements for alternatives. Table 5presents the results ofaij3associated to the criterion 3.Once the comparisons are carried out at every level and are proved to be consistent, the pairwise comparison matrices are introduced in EXPERT CHOICE software and therefore the weights associated to each element of the hierarchical structure are calculated.Let us note by wkthe weight of each criterion k. wkis defined as:(23)wk=∑h=1makh′nwhereakh′=akh∑l=1malhThe results associated to criteria weights determination are given in Table 6.We note also byvikthe weight of each alternative i according to each criterion k.(24)vik=∑j=1naijknwhereaij′k=aijk∑i′=1nai′jkThen, the weight wiassociated to each BOE i is obtained with regards to the most relevant aspects of their performance (conflicting and imperfect data). It is calculated as:(25)wi=∑k=1mwkvikIn our example, BPA’s weights are presented in Table 7.The process of BOE’s weight determination is summerized in the following chart (Fig. 2):In order to combine the sensor readings, simple combination rules (Dempster’s and Murphy’s rules) have often been used, with the result that the factors affecting the readings, including imperfection and unreliability of sensor data, being often ignored. The Dempster’s rule badly manages the conflict between different experts at the normalization step. The Murphy’s rule, on the other hand, supposed that all BOE have the same weights. Nonetheless, in multi-sensor data fusion, sensor readings do not have the same importance because of the difference of adaptability to working environment. To ensure the performance of the fusion system, it is necessary to include the relative importance of sensor readings. The importance of sensor BOE depends on several criteria, namely conflict between the sensors (conflict, distance, and dissimilarity measures) and imperfection of information (contradiction, imprecision, and Amount of uncertainty measures).The suggested process of weighing and combining sensor readings can be summarized in four steps:Step 1:Within n sensors, calculate the evaluation gk(i) of all BOE i (i=1, …, n) according to each criterion Ck(k=1, …, 6).Determine the relative importance of each criterion and the weight values wiassociated to every BOE i using the modified AHP method (cf. Section 6.2). The AHP implementation steps will be simplified by using the Expert Choice software.Calculate the weighed averaging BPAs:(26)mwa(A)=∑i=1nwi×mi(A)∀A⊆ΘUse the classical Dempster’s rule to combine the weighed averaging BPAs n−1 times, as it has been stated in the Murphy’s approach.This process of the multi-sensor data fusion system is presented in the chart below (Fig. 3). The system is composed of three main parts: determination, weighing, and fusion of sensor BPAs. To obtain these BPAs, the detected target characteristics are processed by the existing basic probability assignment generators, such as the neural network classifiers (Hu & Gao, 2003), the wavelets analysis (Perrin, Duflos, Vanheeghe, & Bibaut, 2004), and the distance measure (Jiang, Deng, & Peng, 2011). In the weighing part, we use the obtained BPAs to determine the evaluation function of the sensor BOE i according to each criterion k (gk(i)). Thereafter, we employ the modified AHP method to assign a weight for each sensor BOE. Finally, we fuse the weighed averaging BPAs using the Dempster’s combination rule for n-1 times.To illustrate and analyze the performance of the proposed procedure for weighing and the modified fusion of multi-sensors readings, we suggest a simplified aerial target recognition problem that could be easily translated into a battlefield target recognition context. Assume five sensors that can present their readings with regards to the class of the detected target. Three classes are possible: Θ={target1 (θ1), target 2 (θ2), target 3 (θ3)}. In fact, each sensor must supply a reading i (i=1, …, 5) over the same frame of discernment Θ={θ1, θ2, θ3} (Table 8). We can note that four sensors (1, 2, 4, and 5) assign most of their belief to θ1, while sensor 3 gives its largest mass of belief to θ2.Given the conflict between sensors and the imperfection in their readings, the Dempster’s rule provides highly biased results. Our purpose is to manage the information conflict and imperfection by weighing prior to combining the sensor readings.We assess each BOE i according to each criterion k by computing gk(i), for all i=1, …, 5 and k=1, …, 6. The values of gk(i) obtained by simply applying the formulas (7), (8), (9), (10), (14), and (16) are given in Table 9.To determine the weight of each sensor, we use the hierarchical structure presented in the previous section. Pairwise comparisons are made between the elements of each level in the hierarchy using Eqs. (17) and (18); the associated matrices are automatically constructed and evaluated to provide the eigenvectors presented in Table 10. The calculation details are given in Appendix A.The calculation of each weight criterion generates a null consistency ratio, which indicates complete consistency.Our interest will then be focused on the five sensor readings and the auto-generation of six matrices of pairwise comparisons using Eq. (20). For these matrices, we infer eigenvectors. The consistency ratios of these eigenvectors vary between 0 and 0.05, which are acceptable values since they are smaller than 0.1. This implies that the modified pairwise comparisons are strongly consistent and do not require revisions.The priorities are propagated through the branches of the hierarchy to be synthesized in order to obtain reading weights wiusing all criteria, and then only the imperfection class criteria, and finally only the conflict class criteria. The obtained weights are presented in Table 11. The calculation details are given in Appendix B.Additionally, we compute the weights using the approaches of Deng et al. (2004) (similarity measures) and Liang et al. (2010) (uncertainty and credibility degree). The calculations are shown in Table 11.The results indicate that the imperfection class of assessment criteria has a higher priority (0.5270) and, based only on this class, sensor 1 has a high weight (0.4270). Therefore, S1 is the most important sensor (w1=0.2808). Table 11 also shows that the ranking of all sensors according to their dissimilarity is similar to that obtained via the Deng’s approach since he used similarity.The weights shown in Table 11 are used to calculate the weighed averaging BPAs. Finally, we proceed by combining these weighed BPAs using the Dempster’s rule four times. The combination results are presented in Table 12.According to the combination of all sensors using Dempster’s rule (without discounting), θ2 is strongly supported (0.9091) while θ1 has no chance to occur. Such result is considered biased since the majority of sensors assign most of their belief to θ1 and only one sensor distributes a large mass of its belief to θ2.However, in Murphy’s, Deng’s, and Liang’s approaches, as well as in our approach, the target θ1 is the most supported. We note that in our approach, the combination results support θ1 more than those obtained by other approaches. This is due to the fact that sensor S1 considerably supports θ1(m1({θ1})=0.90) and the weight attributed to S1 is the highest since several criteria are taken into account and since this sensor is best positioned on most of those criteria. If we rely on only imperfection measures, according to which S1 is the best one, target 1 will be strongly supported (m({θ1})=0.9065).The main reason behind the results attained in Table 12 is that the use of several criteria in the weight estimation of BOEs decreases the weights of poor BOEs, thus assigning them less effect on the final combination results.The results also revealed that the combination results attained via the Deng’s approach are almost similar to those yielded by our approach using only dissimilarity since the sensors have approximately the same weights. In fact, Deng gave a weak importance to sensor readings of a high quality and high conflict with others. The weight considering conflict and ignoring imperfection is, therefore, biased.The performance of a sensitivity analysis is considered important to assess the importance of each criterion. Expert Choice offers a graphical sensitivity analysis that allows tracking changes in sensor weights when the priority of any criterion is increased or decreased. We use sensitivity analysis to investigate how sensitive the decision (target 1 is highly supported) is to changes in the importance of the criteria. In Fig. 4, we show the trend of m({θ1}) if we consider only one criterion Ck∀k=1, …, 6. If we rank all criteria according to their support for θ1, we observe that if we take only strife measure (C1), θ1 will be highly supported with regard to its degree of support by other criteria used separately. The changing belief assignment and decreasing ranking criteria are illustrated in Fig. 4 below.To show the effect of the interaction of all criteria on the decision, we start the analysis using the strife measure criterion (C1), in that it is the most important criterion in terms of support to θ1, and then add Amount of uncertainty (C3), being the second least important one, and so on until having all criteria added. Then we proceed by eliminating, each time, the criterion that highly supports θ1 till obtaining a single criterion (imprecision: C2) that least supports θ1.This analysis is applied to show the influence of adding and removing criteria on sensors’ weights wi(Fig. 5).The obtained weight values are used to compute the weighed averaging BPAs. Then, we deduce the trend of the belief assignment of the hypothesis θ1(Fig. 6).One can clearly see from Figs. 5 and 6 that the elimination or addition of a criterion alters sensors’ weights as well as combination results, and, hence, the degrees of support for all targets. It is therefore important to consider all criteria for the weight estimation of sensors.The advantage of this approach lies in the concurrent use of multiple criteria (conflict and imperfection measures) for the weight assessment of sensors involved in the fusion process.To test the robustness of the results of our approach, we propose another sensitivity analysis, in which, sensor data are subject to little changes. In fact, we modified the BPA associated to first sensor, and more precisely we changed the mass assigned to θ1(m1({θ1})) by an amount Δm1({θ1}). So,m1′({θ1})=m1({θ1})+Δm1({θ1}). There are two cases: in the first one, Δm1({θ1}) is allocated to {θ1, θ3}, whereas in the second one, it is assigned to Θ so that we increase the ignorance. Let us denote by Δ1m({θ1}) and Δ2m({θ1}) the variation of the combined mass assigned to θ1(m({θ1})) respectively in first and second case.Table 13shows that Δ1m({θ1}) and Δ2m({θ1}) are proportional to Δm1({θ1}) and Δ2m({θ1}) is more important than Δ1m({θ1}) because the mass distribution structure are not altered (focal elements setIremain unchanged) in the first case contrarily to the second one where Θ is a new focal element. Additionally, small changes introduced into the sensor reading lead to small changes into the combination results. These results reveal that our approach is robust.

@&#CONCLUSIONS@&#
