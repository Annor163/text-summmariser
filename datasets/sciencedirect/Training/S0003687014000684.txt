@&#MAIN-TITLE@&#
How different types of users develop trust in technology: A qualitative analysis of the antecedents of active and passive user trust in a shared technology

@&#HIGHLIGHTS@&#
This paper investigated antecedents of trust in technology for active/passive users.A list of system features that influenced trust was derived from qualitative analysis.Different antecedents of trust related to trust/distrust differently.Active/passive users evaluated trust according to similar sets of system features.

@&#KEYPHRASES@&#
Passive user,Trust in technology,Shared technology,Team,

@&#ABSTRACT@&#
The aim of this study was to investigate the antecedents of trust in technology for active users and passive users working with a shared technology. According to the prominence-interpretation theory, to assess the trustworthiness of a technology, a person must first perceive and evaluate elements of the system that includes the technology. An experimental study was conducted with 54 participants who worked in two-person teams in a multi-task environment with a shared technology. Trust in technology was measured using a trust in technology questionnaire and antecedents of trust were elicited using an open-ended question. A list of antecedents of trust in technology was derived using qualitative analysis techniques. The following categories emerged from the antecedent: technology factors, user factors, and task factors. Similarities and differences between active users and passive user responses, in terms of trust in technology were discussed.

@&#INTRODUCTION@&#
Trust is a fundamental factor in all relationships (Montague, 2010). In social-technical systems, there are three types of trust that are critical for optimal system outcomes: interpersonal trust, trust between two or more people (Larzelere and Huston, 1980), institutional trust, a person's trust with an organization (Castelfranchi and Falcone, 2001), and technological trust, a person's trust with a technology or device (Muir, 1987). Specifically, trust in technology is “the attitude that an agent will help achieve an individual's goals in a situation characterized by uncertainty and vulnerability” (Lee and See, 2004). Research in trust in technology continues to grow in the field of Human Factors and Ergonomics as it plays a vital role in human–technology interaction (Madhavan and Wiegmann, 2007; Parasuraman and Wickens, 2008).Previous research has found that a user's level of trust in technology influences the user's strategy towards the use of the technology (Bagheri and Jamieson, 2004; Lee and Moray, 1994; Muir, 1987). Inappropriate trust in technology can potentially lead to misuse and disuse of the technology (Parasuraman and Riley, 1997). Over trust of technology often results in misuse of technology that leads to complications and errors in the work system. On the other hand, lack of trust in technology prevents the user from utilizing the system to its full extent, and can lead to a decrease in productivity. Recently, there have been efforts to integrate the concept of trust in technology with the technology acceptance model (Ghazizadeh et al., 2012; Pavlou, 2003; Wang and Benbasat, 2005) to predict a user's intent or behavior to adopt technologies. In addition, an individual's trust in technology may also influence his/her trust in other elements of the system, such as interpersonal trust and institutional trust (Muir, 1994). This issue is critical for industries such as healthcare (Montague and Lee, 2012) and e-commerce (Lee and Turban, 2001) where interpersonal trust and institutional trust are important.Human factors researchers have investigated the means for trust calibration (Lee and See, 2004) and how users develop an appropriate level of trust toward the technology. In order to develop effective means for trust calibration, one needs to first identify the antecedents of trust in technology. Researchers have identified a wide variety of factors that influence an individual user's level of trust in technology. Among those factors, reliability of the technology was widely cited in trust in automation studies (Bisantz and Seong, 2001; Lee and Moray, 1992; Lewandowsky et al., 2000; Madhavan et al., 2006). Factors related to interface design were also identified, such as etiquette (Cassell and Bickmore, 2000; Parasuraman and Miller, 2004), usability (Corritore et al., 2003; Koufaris and Hampton-Sosa, 2002), social presence (Hassanein and Head, 2004), and visual design (Fogg et al., 2003; Kim and Moon, 1998; Weinstock et al., 2012). Furthermore, factors related to individual difference, such as age (Sanchez et al., 2004) and propensity to trust technology (Merritt and Ilgen, 2008), were also investigated.The concept of trust in technology has been previously explored by researchers in work on trust between users and automation (Lee and See, 2004), information technology (Marsh and Dibben, 2005), and the world wide web (Egger, 2001; Wang and Emurian, 2005). However, the majority of current research on trust in technology focuses on situations that involve an individual user's interactions with a technology. When multiple people or groups use a technology, trust in technology could be a factor that influences how the technology is used and collaboration between group members. For example, whether or not the team trusts the technology appropriately can affect overall team performance (Bowers et al., 1996).Trust in technology at the group level is especially relevant for multi-user shared technologies, such as health technologies shared by physicians, nurses, and patients, or interactive interfaces shared by customer service representatives and customers, or robots used by a military team. Under such shared technology scenarios, the users of the technology sometimes act as active users, who have direct control over the technology, or passive users, who do not have direct control but interact with both the technology and the active users (Inbar and Tractinsky, 2009, 2012; Montague and Xu, 2012; Xu and Montague, 2012). A related concept discussed in the use of technology on the individual level is supervisory control (Sheridan, 2002). Furthermore, if the primary task of the user is monitoring task, then the user is considered to be a passive process operator (Persson et al., 2001). This role of the individual user is characterized by indirect control of task process through automated systems. At the group level the role of individual users can be further differentiated where a passive user of a shared technology is characterized by indirect control of the technology through the active user. For example, in a face-to-face customer service encounter, the customer service representative plays the active user role and the customer plays the passive user role (Inbar and Tractinsky, 2010). The case is similar for the patient and the clinician's roles with regards to the use of computers in a clinical encounter (Montague and Xu, 2012). In other circumstances, an individual could be an active user for some aspects of the shared technology while a passive user to other aspects. An example of this could be the roles of two pilots in a commercial aircraft cockpit. Also the active/passive user roles may switch among individuals during the interaction process; collaboration in robotic surgeries (Hanly et al., 2006) could be an example of such cases.Previous research has found that active users and passive users have different ways of calibrating trust. Specifically, active users' trust is influenced primarily by trust in the co-user and by direct interaction with the technology, while passive users' trust is influenced by the communication with the active users (Montague and Xu, 2012). However, little research had been conducted to investigate antecedents of trust in technology in the use of a shared technology. It is unclear that previous identified antecedents of trust in technology in single-user scenarios can be applied to multi-user scenarios. Thus, further research is needed to better understand trust in technology from differing perspectives in multi-user systems.As previously reviewed, there are a variety of factors that could potentially influence a user's trust in technology. In the human–computer interaction domain, the prominence-interpretation theory proposed by Fogg (2003) could provide a framework for understanding how different factors affect user's trust. As shown in Fig. 1, the process of trust calibration involves two elements, namely prominence and interpretation. Prominence refers to the likelihood of a specific system element being perceived by a user. Interpretation refers to how a user evaluates the system element in terms of trust. The overall trust of the user towards the technology is the combined effect of the factors that are perceived by the user and the user's corresponding evaluation of the system factors. The prominence and interpretation are related to subjective perceptions of the user about the technology, and these perceptions are influenced by objective factors, such as the task being performed, user expertise, and individual differences, etc. (Fogg, 2003). So the collection of system elements that could potentially affect a user's trust in technology becomes a pool of potential antecedents of trust in technology. However, these system elements have to be perceived and evaluated by the user in order to have impact on the user's trust. In addition, the users' roles in the group (whether active or passive user) may affect the users' prominence as well as interpretation of system elements since different users interact with the technology in different ways.The purpose of this experimental study was to understand the antecedents of users' trust in technology in a multi-user system involving active users and passive users. First, as a comparison of general levels of trust in technology of the active user and passive user, a quantitative scale for trust in technology was used in the experiment to answer the following question:(a)What is the effect of being an active user or a passive user of a shared technology under varied technological/task conditions on the ratings of trust in technology?To further understand the antecedents of trust in technology in such a setting, qualitative data was collected and analyzed. Prominence and interpretation related to a user's trust in technology was investigated through open ended questions about the factors (prominence) that led the user to trust/distrust (interpretation) the technology. The following research questions were addressed:(b1)What are the antecedents of trust in technology reported by the users?What are the similarities and differences in reported antecedents of trust in technology from the active users and the passive users?How do the technological/task conditions influence the type of antecedents of trust in technology reported by the user?Finally, the quantitative data and qualitative data were integrated to answer the following research question:(c)What is the relationship between the rating of trust in technology and the reported antecedents of trust in technology?

@&#CONCLUSIONS@&#
