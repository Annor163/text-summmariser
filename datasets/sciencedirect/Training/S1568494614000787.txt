@&#MAIN-TITLE@&#
Replication and comparison of computational experiments in applied evolutionary computing: Common pitfalls and guidelines to avoid them

@&#HIGHLIGHTS@&#
Pitfalls in the comparison of computational experiments within the field of applied evolutionary computing are quite common.Common pitfalls found in replication and comparison of computational experiments for economic load dispatch problem are presented.Guidelines on setting and conducting computational experiments for evolutionary algorithms are provided.

@&#KEYPHRASES@&#
Evolutionary algorithms,Experiments replication,Algorithms comparison,Economic load dispatch,

@&#ABSTRACT@&#
Replicating and comparing computational experiments in applied evolutionary computing may sound like a trivial task. Unfortunately, it is not so. Namely, many papers do not document experimental settings in sufficient detail, and hence replication of experiments is almost impossible. Additionally, some work fails to satisfy the thumb rules for Experimentation throughout all disciplines, such that all experiments should be conducted and compared under the same or stricter conditions. Also, because of the stochastic properties inherent in evolutionary algorithms (EAs), experimental results should always be rich enough with respect to Statistics. Moreover, the comparisons conducted should be based on suitable performance measures and show the statistical significance of one approach over others. Otherwise, the derived conclusions may fail to have scientific merits. The primary objective of this paper is to offer some preliminary guidelines and reminders for assisting researchers to conduct any replications and comparisons of computational experiments when solving practical problems, by the use of EAs in the future. The common pitfalls are explained, that solve economic load dispatch problems using EAs from concrete examples found in some papers.

@&#INTRODUCTION@&#
“If I have seen a little further, it is by standing on the shoulders of giants[24],” this famous saying by Sir Isaac Newton, implies how important it is that theories should be established by correct and fair experimentation, so that pioneers may guide and assist subsequent researchers towards attaining even greater heights. In almost all science and engineering disciplines, the replication and comparison of experiments in a correct and fair manner is of utmost importance, in addition to the novelties and contributions (e.g., fast, accurate, robust, simple, high-impact, generalisable, and/or innovative [5]) regarding a newly proposed algorithm/method/methodology/technology/theory itself. Without such experimental results, researchers may be led along the wrong paths and evolution may be further delayed. Hence, the ability to build upon past results is crucial for progress in any science. It is interesting to point out that there has recently arisen a strong movement in computer science towards computational reproducibility[19,39]. The problem of reproducibility regarding experiments and the verifications of others’ results in the field of EAs has also been identified by Eiben and Jelasity [17]. However, replications of computational experiments and good comparisons amongst different meta-heuristic methods [5] are difficult tasks. As noted by Barr et al. [5] “When a new heuristic is presented in the computational and mathematical sciences literature, its contributions should be evaluated scientifically and reported in an objective manner, and yet this is not always done.”In order to tackle these issues, Barr et al. [5] presented discussions and comprehensive general guidelines as to how to design, compare, and report on computational experiments amongst different heuristic methods. Barr et al. also reminded researchers that reproducibility, specificity regarding all heuristic factors, preciseness of timing, availability of parameter settings, utilisation of Statistics, reduction in the variability of results, and the production of comprehensive results, are essential components needed in any report in order to assist future studies. After publishing Barr et al.'s work for more than 15 years, we are interested to know whether such issues have been addressed.Despite more papers having been recently published on the designing and reporting on computational experiments (e.g., [6,17,36]) and on statistical methodology for comparing EAs (e.g., [3,7,15,20,21,37]), we still come across numerous works where the replications and comparisons of computational experiments11In the continuation we will skip the term ‘computational’, but whenever ‘experiment’ is mentioned we will have ‘computational experiment’ in our minds.are often improperly done, which can lead to improper conclusions whilst comparing different meta-heuristic methods. Moreover, if such experiments are then further used within other experiments, a rippling effect may occur and any comparison becomes worthless. It seems that the guidelines [5,6,17,36] have been mostly overlooked by practitioners in this field. One reason for overlooking these important works might be that practitioners often prefer concrete examples where the consequences of poorly replicating an experiment and unfair comparisons among algorithms could be clearly observed. Therefore, in this paper common pitfalls in experiment replication are discussed using concrete examples. In such a manner, common mistakes become more easily recognisable by practitioners and common pitfalls will be easier to avoid in the future. Note also that proper replication of experiments is a very fundamental and prerequisite for further comparisons of algorithms. If the experiment is not replicated with sufficient care, any performance measures and statistical approaches cannot remedy the problems introduced by inexact experiment replication. In other words, if collected data are gathered from experiments which exhibit large deviations the comparison is meaningless despite statistical test being applied. Hence, it is crucial that experiment replications are properly conducted. This paper is not about discussing which performance measures EA practitioners should use or which performance measure is the more appropriate. Although, in Section 2 a success rate (SR) [18] is used for particular examples. However, some performance measure (e.g., Mean Best Fitness (MBF) [18], average number of evaluations to a solution (AES) [18], expected running time (ERT) [23]) should be used during algorithm comparison in order to show the effectiveness of an approach.This paper reviews a number of papers that have utilised EAs [18] to solve the economic load dispatch (ELD) problem [43], a practical real-world problem within those power plant operations whose objective functions try to allocate power generation to match load demand at minimal possible cost using specific system constraints. Our studies show that the pitfalls when replicating experiments and their comparisons are commonly seen in some of these papers. The main objective of this paper is not to criticise and challenge the results presented in these papers. Hence, all information which may point to particular works has been removed (e.g., authors’ names, papers’ titles, publishing information, algorithms’ names). However, the removed data have been accessible to reviewers in the earlier versions of this paper enabling verification of data. More specifically, Authors1 work was utilised as a benchmark for comparing their newly introduced algorithm for several papers. However, since the main purpose of Authors1 work was to find only the best result (i.e., minimum cost) of the ELD problem, some important data for later experiment replications and comparisons were not shown in their paper. For example, (1) execution time was reported rather than the number of fitness evaluations. Such a measure, however, is subject to the hardware specifications of the platform conducting experiments; (2) minimum, mean, and maximum costs were reported, but the standard deviations for mean costs were missing, which makes experiment comparison less accurate with respect to Statistics; and (3) as for the performance measure a complex frequency of convergence was used rather than the simpler SR. The former measure is more difficult to interpret than the latter one and, therefore, was usually omitted in subsequent experiments. Although, there still can be some problems, as discussed later, with SR [11]. Additional drawbacks were also discovered in some of the subsequent works. For example, (1) only a partial test-suite was replicated and compared; (2) the number of independent runs were fewer than its benchmark paper; (3) large deviations in the number of fitness evaluations consumed; (4) whether the experiments utilised the best parameter settings was unreported; (5) not all performance measures were utilised; and (6) the statistical significance of the results was not shown. With such missing information and different experimental settings, experimental results and conclusions of the subsequent papers may become less convincing. The aforementioned problems are not only pertinent to the ELD problem, but also to many other studies solving practical problems (e.g., turning operations [38], milling operations [27], welded beam design [1], and pressure vessel design [1]). Hence, the ELD problem was chosen arbitrarily to designate the quite common problem of experiment replication and comparison in the field of applied evolutionary computing. On the other hand, we should point out that there are also numerous works (e.g., [2,25,26,32]) where performance measurements have been collected with variances, and statistical tests have been performed. Further recent competitions on real-parameter optimisations [23,28,40,41] provide an excellent experimental setup, which still needs to be accepted by EA practitioners. On the other hand, in all of these competitions, the algorithms were run on the same computer platform, and good performance measures were easier to define. But, the same computing environment is much harder to achieve by practitioners for practical industrial problems, and comparison is usually done based on reported results. It is also interesting to point out that many other applied sciences have problems performing empirical studies (e.g., Software Engineering [35], Medicine [44]). For example, Welch and Gabbe in [44] reported that more than half of the studies were undocumented regarding sufficient details, and replications were impossible. We chronologically detail the common pitfalls of experiment replications and comparisons found in ELD papers as a friendly reminder – through these studies, this paper extends comprehensive general guidelines from [5,6,17,36], and the guidelines from Črepinšek et al. [13] with special emphases on guidelines for replicating experiments in applied evolutionary computing. A checklist is also introduced to remind researchers to avoid such common pitfalls in the future.This paper is organised as follows: Section 2 summarises the common pitfalls of EAs replications and comparisons on the ELD problem. The chronological development and drawbacks of the experiments are then further detailed. Section 3 offers the guidelines learned from Section 2, along with a checklist to assist researchers on the replication of experiments in applied evolutionary computing, followed by the conclusions in Section 4.EAs have been used, from [4,18,22,34]'s inception, for solving hard optimisation problems. Solving real-world problems is actually the ultimate purpose of any EAs. In this section, common pitfalls regarding experiment replication and comparison are explored and explained on the economic load dispatch (ELD) problem (also known as economic dispatch (ED) problem), which is inherently a high-nonlinear and non-convex problem [33,43,45]. The problem, allocating power generation to match load demand at minimal possible cost using specific system constraints, has been intensively studied for the last 20+ years (Scopus search on “economic load dispatch” performed on July 1, 2012, returns 1526 hits, whilst search on “economic dispatch” returns 3143 hits). This problem has recently been extended into a multi-criteria problem, also including a request for minimising emission levels [8].Although the work by Authors1 was not amongst the first for solving the ELD problem using EAs techniques, it has served as ground research for many other researchers who have used the same experimental data. The ELD problem is a highly practical problem, which triggers interest amongst researchers into power systems. Hence, researchers have always been very pragmatic in their approaches. Their main goal was to solve this difficult problem regardless of execution time. Researchers have mainly reported the best and the mean solutions over certain numbers of independent runs, but standard deviations and statistical tests have often gone unreported. Researchers have even been less interested in how many fitness evaluations that a particular algorithm might consume in order to find a solution. Hence, these data have mostly been uncollected, which leads to the currently unsatisfactory results regarding algorithm comparisons. From such experiments only a conclusion about best solution found so far is possible but further generalisation about algorithm's performance is not feasible. Although finding the best solution so far is a great achievement in itself, real progress cannot be expected, and full benefits of EAs in practice cannot be achieved. Fig. 1summarises the timeline and the citation relationship amongst some of the ELD works presented in this paper. The development and experimental drawbacks of these works are chronologically detailed in the following paragraphs. Note that authors’ names and publishing information have been deliberately removed.In the work by Authors1, the authors used and compared Algorithm1–Algorithm4 for solving the ELD problem for 3/13/40 generating units. Because the case-study on 40 generating units is the most difficult and often used by other researchers, this paper only reports on the experiment replication and comparison associated with these 40 generating units. Similar problems with experiment replication and comparison have also been found for smaller problems but are unreported here. The following control parameters have been reported by Authors1 (note that this paper only reports on those control parameters that are important for calculating the number of fitness evaluations): number of independent runs (run=50) and population size (pop_size=60). However, the number of generations (G) was visible only from the convergence graph and was not explicitly mentioned within the text. This has caused a certain number of troubles for other researchers. By carefully looking into the convergence graph, it was noticed that Algorithm4 was running 400 generations, Algorithm2 600 generations, Algorithm3 800 generations, and Algorithm1 1000 generations. By superficial comprehension of this convergence graph it could be concluded that all the compared algorithms were run for 1000 generations. For example, Authors9 wrote: “The number of iteration is taken as 1000 to match with the previous analysis...” It should be emphasised that Authors1 used the same number of generations for Algorithm1–Algorithm4 in a case-study with 3 generating units, but a different number of generations for case-studies on 13 and 40 unit generators. Hence, if control parameters are unclearly specified then an experiment might be incorrectly repeated, despite the fact that researchers strongly believe that the same experimental conditions had been established. The results of the experiment, which was performed on a 350MHz Pentium-II with 128MB RAM PC, are presented in Table 1.At first sight, the experiment by Authors1 was performed well. The best solution was found by Algorithm4 and its execution time was also amongst the fastest. Hence, why care about the number of fitness evaluations? Whilst for practitioners execution time is indeed more important than the number of fitness evaluations, it is necessary to be careful about how these results will be used in other research work. Over the years, computers have become more and more powerful and able to perform many more operations per second than those computers used in some earlier experiments. Hence, data about execution time might be easily abused by other researchers (as is shown in this section). Moreover, there is another important reason for comparing algorithms based on the number of fitness evaluations. These algorithms will also be used to solve many other practical problems, where one fitness evaluation might take several minutes, if not hours. In such cases, the best algorithm for a particular problem that consumes many more fitness evaluations might not be the most efficient and useful one anymore regarding other practical problems. Hence, this is a danger in the sense that other researchers may carelessly select the “best” algorithm with more fitness evaluations and apply it directly to some other problems. In addition reported results on execution time might sometimes also include the time spent on some other tasks (e.g., garbage collection, operating system tasks), or depend on a programmer's experience and skills. Can we really be sure that reported execution time is spent solely on problem evaluation? When carefully examining mean time from Table 1, questions may be asked as to why Algorithm4 combining Algorithm1 and Algorithm2 is not slower than Algorithm1? We are strongly convinced that the differences in execution times from Table 1 mostly come from the different numbers of fitness evaluations (Algorithm1−1000×60×1=60,000, Algorithm2−600×60×1=36,000, Algorithm3−800×60×1=48,000, and Algorithm4−400×60×2=48,000). However, since the algorithm was imprecisely described (actual code or even pseudocode were not provided in Authors1’ paper, this reasoning is just our speculation. Based on the aforementioned observations, we suggest that the number of fitness evaluations is a fair and objective measure (but not the only one) for experiment comparisons, because it excludes potential intervention from hardware. But this does not mean that researchers should not report on actual execution times, which should be reported since many algorithms perform some costly calculations besides fitness evaluation. Note that a number of fitness evaluations can be used in two different approaches [23]: fixed-cost (measuring the quality of a solution reached by a pre-defined number of fitness evaluations) and a fixed-target scenario (measuring the number of fitness evaluations needed to find a (sub-)optimal solution).Moreover, Authors1 did not report on other important information that would be essential for future comparisons with newly developed algorithms. In particular, what were standard deviations for the performance measures of the achieved results? Further, they reported on the relative frequency of convergence, which can be regarded as an approximation for SR, such a measure is harder to interpret than SR and, as such has often been omitted in other studies. Note that there are also some intrinsic problems with SR as well. Clerc in [11] pointed out that we should use performance measures for algorithm comparison only when such measures have converged reasonably. Namely, if the number of independent runs is too small then SR may diverge and as such might not be a reliable measure for algorithm comparison. In that case, standard deviation for SR should be reported, too. An easiest way to achieve this is to have the number of independent runs high enough. But, this might not be always feasible for practical problems with time-consuming fitness evaluations. Hence, whenever SR is reported the number of independent runs should be high enough implying small variance in SR. Otherwise, SR variance should be explicitly mentioned or another more reliable measure should be used (e.g., ERT [23]). In order for other researchers to apply the same algorithm regarding this problem or on any other problem, it is extremely important to know whether good solution(s) can be found in every run and what kind of deviations in the result might be expected. The latter measure is also very important because the statistical significance of one algorithm's performance over the other can be computed. Hence, the presented results by Authors1 were not fully satisfactory (i.e., missing standard deviations, number of fitness evaluations were not the same for the compared algorithms, missing statistical tests) so that other researchers should not simply base their own research on such results. Of course, it is hard to blame Authors1 since their objective was to solve the ELD problem. It is other researchers’ responsibilities to reasonably use available data. However, to make your own research useful to others, more information should be available. Hence, every practitioner must be interested in fully reporting on any performed experiment. You should keep asking: “Did we provide enough information about the results, as well as about the experiment, so that other researchers can use our results in their works, as well as replicating the same experiment?”Let us show further chronological development when solving the ELD problem. Authors2 hybridised Algorithm5 with a local search technique for solving the ELD problem. A newly-developed Algorithm6 was compared to the results presented by Authors1. Although the same case-studies using 3/13/40 generating units were used, the experiment was not an exact replication of the experiment by Authors1 (e.g., number of independent runs was smaller). Authors2 reported the following control parameters: number of independent runs (run=30), number of generations (G=100), and population size (pop_size=100). Unfortunately, Authors2 did not report on the number of fitness evaluations, which reinforces our impression that the numbers regarding fitness evaluations for practitioners when solving the ELD problem, is of secondary importance. Because a local search technique was used for improving each member of the population, the exact number of fitness evaluations cannot be re-computed from the available data, and can only be estimated as ≫10,000. The experiment was performed on a 500MHz Pentium-II PC, and the results are presented in Table 2.From the results in Table 2Authors2 concluded: “It is clear from Table2, the mean cost value and simulation time obtained by the proposed method is comparatively less compared to all the other methods.” There was no discussion about any threats to the validity of such conclusions with respect to:•different computers used (350MHz vs. 500MHz);different number of independent runs (50 vs. 30); anddifferent number of fitness evaluations (Algorithm4−48,000 vs. Algorithm6−≫10,000 vs. Algorithm5−10,000).Whenever the environmental factors (e.g., computing environment, programmer's skills) cannot be held uniformly across the compared algorithms, the effect of these factors needs to be determined and discussed as threats to validity. In such cases it is often better that the compared algorithms are re-implemented (if implementation is not publicly available), and re-run within the same computing environment. This is especially true when previous experiments are badly documented and/or measurements concerning central tendencies, as well as variability, are not compiled. More often than not, re-implementation is nonviable due to lack of details regarding the original experiment (e.g., too abstract algorithmic descriptions, unknown values for control parameters). Such experiments simply cannot be replicated and researchers resort to publishing results without discussion on any threats to validity. As can be observed from Table 2Authors2 were again mainly concerned about the final solution. Information about the results and how the experiment was performed was again unsatisfactory. Missing information about standard deviations again prohibits statistical comparison with previous and future experiments. Moreover, the authors did not provide the relative frequency of convergence, which can be regarded as an approximation for SR used by Authors1.The next study that attracted our attention was the work by Authors3 where Algorithm7 has been used for solving the ELD problem. The results were compared against the Algorithm1–Algorithm4 from Authors1 on a case-study of 40 generating units. The case-studies on 3/13 generating units were not taken, thus making their comparison slightly useless. On the other hand, a few other experiment replications have been done. For better comparison with a particular algorithm, all available case-studies (or benchmark functions) should be used. Otherwise, other researchers may question the possible reasons for omitting some case-studies (benchmark functions). Note that producing the best result concerning the most difficult problem does not necessarily mean that a proposed algorithm will also perform better on a less difficult problem. A good example is described later in this section using data from Authors8. The reported data regarding how the experiment was performed by Authors3 were very scanty. Authors3 reported on the number of independent runs (run=100) and they experimented with different population sizes (pop_size=20, 30, 40), whilst the number of generations (G=150), can be guessed from the convergence graph. However, it has never been known from the presented results (Table 3) as to which population size was actually used, nor how many fitness evaluations were consumed. This is due to unknown actual population size as well as that Algorithm7 heavily uses local search to enhance solutions. Again, only the best result was of importance, and the authors did not even report on the execution times, and the mean and maximum costs (Table 3). However, the relative frequency of convergence was given. Missing information about standard deviations again prohibits statistical comparison with previous and future experiments. Such an experiment is only useful for showing that good results can be achieved using the proposed algorithm, but stronger conclusions may not be derived at. For example, there is no evidence that Algorithm7 will perform equally well on 13 generating units. It is more anecdotal evidence rather than sound scientific evidence supported by well-documented experiments. Moreover, others cannot replicate such poorly documented experiments.In the next study Authors4 developed a new hybrid Algorithm8, which was combined with a generator of chaos sequences for solving the ELD problem. Their work was compared with the results from Authors1, Authors2, and Authors3 using a 1.1 GHz AMD Athlon processor with 112MB of RAM. Authors4 reported the following control parameters: the number of independent runs (run=50), number of generations (G=600), and population size (pop_size=30). Number of fitness evaluations was again unreported. Since their approach, Algorithm8, uses local search, the number of fitness evaluations can only be estimated as ≫18,000. The results are presented in Table 4.For the first time so far standard deviation on mean cost was reported. On the other hand, Authors4 did not provide a table with the relative frequency of convergence, as appeared in Authors1's and Authors3's work. Moreover, execution times amongst different approaches were compared simply based on seconds, regardless of the different machines used during the experiments: 350MHz for Algorithm4, 500MHz for Algorithm5 and Algorithm6, and 1.1GHz for Algorithm8. Authors4 did not mention any threats to the validity of their conclusions. By briefly looking into Table 4 it might be wrongly concluded that Algorithm8 is about 80 times faster than Algorithm4. Comparing execution times without noticing that the reported times cannot be directly compared can also be found in some other studies.The next example comes from Authors5, where a hybrid Algorithm10 was applied to the ELD problem on 3/13/40 generating units from Authors1's work. Authors5 reported the following control parameters: number of independent runs (run=100), number of generations (G=8500), and population size (pop_size=5). Again, the number of fitness evaluations was unreported and can only be estimated at ≫42,500, since occasional migration phases in Algorithm9 and Algorithm10 have been performed, thus generating new individuals. Moreover, local search on the best individual was performed. The experiment was performed using Pentium 1.5GHz with 768MB of RAM, and the results are presented in Table 5.It seems that bad practice (i.e., without complying with standard deviations, the number of fitness evaluations is unreported, and not performing statistical tests) is already widespread regarding the ELD problem, and researchers have performed experiments that have slightly deviated from those done by Authors1. Yet, in this case, Authors5 included a table with relative frequency of convergence as appeared in Authors1's and Authors3's work. Authors5 reported slightly better minimum cost compared to work by Authors4 but omitted the information about standard deviations as was reported by Authors4. However, when comparing the mean cost of Algorithm8 (122,295.1278) with Algorithm10 (122,304.30), on average Algorithm8 actually performs better. But, on the other hand, Authors4 used only 50 runs, whilst Authors5 used 100 runs. It is difficult to come to proper and valid conclusions if experiments are inexactly replicated.The next example comes from Authors6, where Algorithm11 has been applied to the ELD problem. Authors6 reported the following control parameters: number of independent runs (run=50), number of generations (G=125), and population size (pop_size=500). Hence, the number of fitness evaluations is 62,500. The experiment was performed using a Pentium IV 2.8GHz with 512MB of RAM, and the results are presented in Table 6.Standard deviation and execution times were not provided by Authors6. However, the frequency of convergence was reported. From amongst 3/13/40 generating units only those case-studies with 40-units were replicated. The results by Authors6 have been compared to Algorithm4 and Algorithm7. The conclusions were drawn regardless of the number of fitness evaluations. Note that Algorithm11 consumed many more fitness evaluations (62,500 fitness evaluations) than Algorithm4 (48,000 fitness evaluations), whilst data about fitness evaluations for Algorithm7 is unknown. When a conclusion is drawn from an experiment where more fitness evaluations were consumed, we should be sure that its compared algorithms cannot also find the same better solution by additional fitness evaluations. Researchers too often assume that this is indeed the case. Since Authors1 did not report that they experimented with an increased number of generations, we cannot simply assume that Algorithm4 cannot find better solutions when the number of fitness evaluations is increased to 65,000. On the other hand, the study by Authors6 is interesting in that the authors explicitly mentioned that even with increasing population size, and hence with more fitness evaluations, better results were unachievable. Yet, they did not mention what happened if the number of generations increased. Overall, it can be reasonably assumed that this approach cannot find any better solution, even in a case where the number of fitness evaluations is increased. Such an observation is crucial for future algorithm comparison. A similar observation was also found by Authors8. There is another interesting observation by Authors8, where their newly-proposed Algorithm8 has been compared with different case-studies. Authors8 wrote: “From the simulation results of Algorithm8in solving ELD problems, we cannot conclude that Algorithm8is universally better than other methods.” However, it seems that such a conclusion did not come from any lack of showing the statistical significance of Algorithm8 over other algorithms (actually no statistical tests were performed), but the claim was due to the fact that Algorithm8 did not outperform other algorithms during a case-study with 20 generating units. Such a claim was unique because other authors always claimed the superiorities of their approaches, although not always statistically proven, and often some test case-studies were excluded in the experiment. This example also clearly shows that an algorithm that outperformed others on bigger problems might not be the best for smaller problems. Researchers too often assume that it is sufficient to run their algorithm on the more difficult problems assuming that it will also perform better on less difficult problems. Such an assumption is mere speculation.Although the modified differential evolution (MDE) algorithm by Yuan et al. [45] for solving the ELD problem was not applied for 40 generating units from Authors1, it is interesting from the unique perspective of execution time. This was the first paper on ELD where we have found that the authors acknowledged that the comparison of execution times using very different computers cannot be fair. Yuan et al. [45] proposed the following CPU time-scaling:Scaled CPU time (s)=CPU speed from previous experimentCPU speed from current experiment×CPU time from previous experimentRecently, some works solving the ELD problem have reported on standard deviations (e.g., [31]) enabling better comparisons amongst algorithms. The work by Authors10 nicely summarised most of the works on the ELD problem for 40 generating units (Table 7). Table 7 shows how researchers reported best solutions found over the years (the table is sorted by minimum cost). Even the smallest improvements on the best solution almost granted a publication, even though the experimental data were deficient (see example row for Algorithm23 in Table 7). Consequently, Table 7 should be read with sufficient care as most of the data are not directly comparable due to inexact replication of the experiments (e.g., different number of independent runs, different number of fitness evaluations). The work by Authors10 is also important since for the first time the number of fitness evaluations (FE) for other algorithms were recorded, although we did some corrections (indicated byain Table 7) whenever we found that number of fitness evaluations were reported incorrectly. This was mainly because those algorithms using local search (additional fitness evaluations are consumed, but the exact number was unreported and is unknown).To the best of our knowledge, we are unaware of any works on the ELD problem that have recorded SR for the various algorithms presented in Table 7. However, for solving practical problems where employed EA is actually continuously used within an industrial setting it is very relevant to know if the performance of EA is stable. Showing the statistical significance of one algorithm over another is also rare on the ELD problem. One exception is the work by Ling et al. [29], where the statistical significances were presented using the t-Test. However, in their case, all the used algorithms were re-implemented and re-run to collect missing information. But, we are unaware of any other work on the ELD problem that shows statistical significance simply by using the provided data from other experiments. As pointed out in the very first statement of Section 1, real advancement can be expected if we can stand on the shoulders of others. This is now hardly possible on the ELD problem because (1) statistical measurements such as standard deviations of means have mostly been unrecorded; (2) only particular problems from the benchmark suite have been used; and (3) inexact replications of experiments.Table 8shows the identified shortcomings regarding the experiment replications used in this study. It also explains why the experiment comparisons were unsatisfactory for the ELD problem. A whole test-suite should be used during experiment replication when comparing different algorithms. Outperforming on one test case does not guarantee outperformance on other test cases. The works presented in this paper, in the main, did not perform the whole test-suite (Row 2 in Table 8). The numbers of independent runs should be the same or higher than the original experiment (only one work failed to oblige us regarding this rule; Row 3 in Table 8). Comparing the algorithms based on execution times was unfair and CPU time was not scaled (Row 4 in Table 8). Not all performance measures (mean cost, relative frequency of convergence) were reported (Row 5 in Table 8) and standard deviations were mostly unrecorded (Row 6 in Table 8), which prohibits useful statistical analysis (Row 7 in Table 8) and the number of consumed fitness evaluations is mostly unknown (Row 8 in Table 8). Control parameters’ values were not always clearly described (Row 9 in Table 8). Moreover, there is no guarantee that the mentioned algorithms used the best parameter settings (Row 10 in Table 8). Either, there is no guarantee that the mentioned algorithm ran using an appropriate number of fitness evaluations such that a better solution could not have been achieved (Row 11 in Table 8). In such cases, can it be assured that the algorithm with the best minimum cost is really the best algorithm?

@&#CONCLUSIONS@&#
