@&#MAIN-TITLE@&#
Partitioned Bonferroni mean based on linguistic 2-tuple for dealing with multi-attribute group decision making

@&#HIGHLIGHTS@&#
The proposed method presents an attempt to model a new kind of interrelationship structure among the attributes.The concept of partitioned Bonferroni mean is introduced based on linguistic information.A novel method is proposed to determine the weight vector of the decision makers.

@&#KEYPHRASES@&#
Linguistic 2-tuple,Partitioned Bonferronimean,2-Tuple linguistic partitioned Bonferroni mean,Multi-attribute group decision making,

@&#ABSTRACT@&#
In this study, a multi-attribute group decision making (MAGDM) problem is investigated, in which decision makers provide their preferences over alternatives by using linguistic 2-tuple. In the process of decision making, we introduce the idea of a specific structure in the attribute set. We assume that attributes are partitioned into several classes and members of intra-partition are interrelated while no interrelationship exists among inter partition. We emphasize the importance of having an aggregation operator, to capture the expressed inter-relationship structure among the attributes, which we will refer to as partition Bonferroni mean (PBM). We also investigate the behavior of the proposed PBM operator. Further to aggregate the given linguistic information to get overall performance value of each alternative in MAGDM, we analyze PBM operator in linguistic 2-tuple environment and develop three new linguistic aggregation operators: 2-tuple linguistic PBM (2TLPBM), weighted 2-tuple linguistic PBM (W2TLPBM) and linguistic weighted 2-tuple linguistic PBM (LW-2TLPBM). Based on the idea that total linguistic deviation between individual decision maker's opinions and group opinion should be minimized, we develop an approach to determine weight of the decision makers. Finally, a practical example is presented to illustrate the proposed method and comparison analysis demonstrates applicability of the proposed method.

@&#INTRODUCTION@&#
Multi-attribute group decision making (MAGDM) is the process of selecting the best alternative from a set of predefined alternatives which are assessed by a group of decision makers based on multiple attributes. In day-to-day life, there are many practical instances, such as, selecting applicants for different kinds of scholarships, selecting projects for different kinds of funding policies, admitting students in graduate programs [1] and evaluating design, ‘comfort’ or ‘speed’ of various kinds of cars [2], in which decision makers’ preferences, cannot be expressed precisely in exact quantitative form, but may be in qualitative one. In such situations, they prefer to provide their assessment in linguistic terms. For example, when evaluating ‘comfort’ or ‘design’, terms like ‘excellent’, ‘good’ or ‘bad’ are used to express experts’ preferences and for evaluating cars’ ‘speed’, terms like ‘very fast’, ‘fast’ or ‘slow’ are generally used. To deal with linguistic information in the decision process, several linguistic computational models [3–5] have been proposed. Among them, 2-tuple linguistic model, proposed by Heerra and Martinez [5,6], has been successfully applied in MAGDM problems [7–10,12–17] due to its capability in linguistic information processing without any loss or distortion of information.Several aggregation operators have been developed to aggregate 2-tuple linguistic information. In Ref. [5], Herrera and Martinez was the first to develop 2-tuple linguistic aggregation operators. They extended classical arithmetic mean, weighted arithmetic mean and OWA operator in 2-tuple linguistic environment, and denoted them as 2-tuple linguistic averaging operator, 2-tuple linguistic weighted averaging operator and 2-tuple linguistic ordered weighted averaging operator, respectively. Jing and Fan [9] introduced 2-tuple linguistic weighted geometric operator and 2-tuple linguistic ordered weighted geometric operator. Wei [11] presented a MAGDM method based on extended 2-tuple linguistic weighted geometric operator and extended 2-tuple linguistic ordered weighted geometric operator, in which weight of input arguments was modeled by 2-tuple linguistic information. In Ref. [12], Wei introduced several new aggregation operators, such as, generalized 2-tuple linguistic weighted average operator, generalized 2-tuple linguistic ordered weighted average operator and induced generalized ordered weighted average operator. Wei and Zhao [13] developed some dependent 2-tuple linguistic aggregation operators in which the associated weight only depends on the aggregated 2-tuple linguistic information. Wan [14] developed several hybrid 2-tuple linguistic aggregation operators, such as, 2-tuple hybrid linguistic weighted average operator, extended 2-tuple hybrid linguistic aggregation operators. Park et al. [15] defined 2-tuple linguistic harmonic operator, 2-tuple linguistic weighted harmonic operator, 2-tuple linguistic ordered weighted harmonic operator and 2-tuple linguistic harmonic hybrid operator, and proposed a MAGDM technique. Merigo and Gil-Lafuente [16] introduced induced 2-tuple linguistic generalized ordered weighted averaging operator and generalized it by using quasi arithmetic means. The common feature of all the preceding aggregation operators is that they put accent on importance of each input without reflecting any kind of interrelationship among the aggregated arguments.However, in real-life MAGDM problems, there always exists some sort of interrelationship among the attributes and this inter-relationship has a reflection in corresponding arguments. In order to capture the interrelationship among the 2-tuple linguistic aggregated arguments, few aggregation operators have been proposed in the literature. Based on Choquet integral, Yang and Chen [17] developed some new aggregation operators: 2-tuple correlated aggregation operator, 2-tuple correlated geometric operator and generalized 2-tuple correlated averaging operator where correlation between aggregated arguments are measured subjectively by experts. Based on power average operator, Xu and Wang [18] developed three new linguistic aggregation operators: 2-tuple linguistic power average operator, 2-tuple linguistic weighted power average operator and 2-tuple linguistic ordered weighted power average operator which allow the aggregated arguments to support each other in the aggregation process. Both Choquet integral and power average operator focus on capturing interrelationship by taking different weight generation strategies and they do not directly take any conjunction among the aggregated arguments.In this respect, Bonferroni mean (BM), introduced by Bonferroni [19], has the capability to capture inter-relationship among the aggregated arguments by taking conjunction among each pairs of aggregated arguments. Although BM was developed in 1980, its capability of capturing inter-relationship among aggregated arguments was first explored and generalized by Yager [20]. In ref. [20], Yager interpreted BM as a sum of the product of each argument with the average of rest of the arguments and suggested the generalization of BM by replacing the simple average with other averaging operators, such as, Choquet integral [21] and ordered weighted average operator. Beliakov et al. [22] explored the modeling capability of BM and showed that it is capable to model any type of mandatory requirements in the aggregation process. Beliakov et al. [33] also proposed a generalization of BM that averages conjunctions between the respective means of a designated subset-size partition. BM also been successfully applied in multi-attribute decision making (MADM) by many authors and extended in various environments for processing different uncertain information [20,23–29]. Owing to this attractive feature, BM has also been extended in 2-tuple linguistic environment. Zhang and Wu [30] extended BM in 2-tuple linguistic environment and proposed two approaches for solving MAGDM problems with different natures of weights information for attributes and decision makers. Jiang and Wu [31] also extended BM and geometric BM in 2-tuple linguistic environment and developed an approach for solving MADM problems. Recently, Dutta et al. [32] proposed an interesting extension of BM for capturing heterogeneous relationship among aggregated arguments and extended it in 2-tuple linguistic environment on the basis of which they developed a novel approach for solving MAGDM problem.However, our point of view is something different from the previous works. BM operator was successfully applied in the decision making process with the assumption that each attribute is related to the rest of the attributes. In real-life decision making situation it may not be always happened. For example, consider a mobile phone selection problem where the best alternative among a number of mobile phone options is selected based on five attributes: basic requirements (A1), physical characteristic (A2), technical features (A3), brand choice (A4) and customer excitement (A5). The attributes are partitioned in two classes: P1={A1, A2, A3} and P2={A4, A5}. It is found that attributes A1, A2 and A3 are interrelated therefore, they belong to the same class P1, similar interpretation can be provided for the members of P2. However, there is no relationship among the members of P1 and P2. The expressed interrelationship structure among the attributes is intrinsically connected to the corresponding input arguments. Thus, we continue to interpret the situations in the same sense where input arguments are partitioned into several classes and each member of any class is interrelated to the rest of the member of that particular class. Up to now there is no suitable aggregation operator which can fuse the input arguments with this kind of relationship among inputs. This motivates us to develop a new aggregation operator which refers to as partitioned Bonferroni mean (PBM). We provide the mathematical interpretation of our proposed PBM operator and also provide an example to illustrate the working nature of PBM operator in comparison to other aggregation operators. After introducing the concept of PBM operator, we analyze the proposed operator in linguistic 2-tuple environment and afterwards, we develop an approach to deal with linguistic MAGDM problems.In this context of linguistic MAGDM problem, assigning weight to each expert is an important task. In the existing literature [6–17] of MAGDM with 2-tuple linguistic information, two kinds of weight information were used to obtain group performance from individual performance. In the first case, the weights of the decision makers are chosen heuristically and weighted averaging operator was utilized to form the group performance. However, it would be more worthwhile to compute the weights on the basis of available information as the heuristic approach does not usually contribute the proper weight. On the other hand, when weight vector is completely unknown, ordered weighted aggregation (OWA) operator was used to form group opinion. The OWA operator assigns high weights to the decision makers who have provided high ratings for the alternatives with respect to attribute in comparison to others. In other words, the OWA operator over emphasizes the influence of bigger elements and ignores the smaller ones to ultimate aggregation results. Thus, the decision makers with low ratings are neglected. These demerits of existing approaches motivate us to develop a new method for obtaining weight vector of decision makers. The proposed weight generation method is based on the idea of minimizing the linguistic deviation between individual expert's opinion and group opinion regarding the alternatives.In a nutshell, this work focuses on developing a MAGDM technique under linguistic environment where weights of the decision makers are unknown and attributes have a specific interrelationship structure. To do this, the rest of the paper is organized as follows. In Section 2, a brief review of 2-tuple linguistic model is presented, including the concept of linguistic distance measure. Section 3 introduces the concept of partitioned BM (PBM). In Section 4, we analyze the proposed operator in 2-tuple linguistic environment and develop several 2-tuple linguistic aggregation operators, such as, 2-tuple linguistic partitioned Bonferroni mean (2TLBBM), weighted 2-tuple linguistic partitioned Bonferroni mean (W2TLPBM) and linguistic weighted form of 2TLPBM and study their desirable properties. An approach for solving MAGDM problem is proposed in Section 5. In Section 6, a practical example is given to illustrate our proposed method. A comparison analysis with prominent existing MAGDM methods is drawn in Section 7, while Section 8 concludes the discussion.Let S={l0, l1, …, lg} be a linguistic term set with the odd cardinality g+1. Any term li∈S denotes a possible value for linguistic variable. The following properties should hold for the term set S[5]:•the set S should be ordered, i.e., li≥ljif i≥j,negation of any linguistic term li∈S: neg(li)=ljsuch that j=g−i,the maximum of any two linguistic terms li, lj∈S: max(li, lj)=liif li≥lj,the minimum of any two linguistic terms li, lj∈S: min(li, lj)=liif li≤lj.S={l0=verylow(VL),l1=low(L),l2=moderatelylow(ML),l3=normal(N),l4=moderatelyhigh(MH),l5=high(H),l6=veryhigh(VH)}Various models have been suggested in literature [3–5] for processing linguistic information. Here, we have adopted 2-tuple linguistic representation model, which was developed by Herrera and Martinez [5,6] based on the concept of symbolic translation. The definition of symbolic translation is given below.Definition 1[5]Let us assume that β∈[0, g] be the result of symbolic aggregation operation on the indices of the labels of linguistic term set S={l0, l1, …, lg}. If i=round(β) and α=β−i, be two values such that i∈[0, g] and α∈[−0.5, 0.5), then α is called the symbolic translation.On the basis of symbolic translation, Herrera and Martinez [5] represented the linguistic information by means of 2-tuple (li, αi) where li∈S represents the linguistic label and αi∈[−0.5, 0.5) denotes the symbolic translation.The conversion of symbolic aggregation result into equivalent linguistic 2-tuple can be done using following function:Definition 2[32]Let S={l0, l1, …, lg} be linguistic term set and β∈[0, g] be the numerical value which is obtained from symbolic aggregation operation on the labels of S, then 2-tuple that conveys the equivalent information to β is given by the following functionΔ:[0,g]→S×[−0.5,0.5),Δ(β)=(li,α)where i=round(β) is the usual round operation on label of index, i.e., i is the index of the considered label closest to β, and α is the value of symbolic translation given byα=β−i,α∈[−0.5,0.5)ifi≠0,hβ,α∈[0,0.5)ifi=0β−h,α∈[−0.5,0]ifi=hExample 1 Assume that S={l0, l1, l2, l3, l4, l5, l6} represents a linguistic term set and β=4.2 is obtained from symbolic aggregation operation. Then from Definition 2, we can convert β=4.2 into linguistic 2-tuple Δ(4.2)=(l4, 0.2), which is presented in Fig. 1.For the purpose of understanding, we can give the following interpretation of (l, α) in the context of modeling decision maker's linguistic assessment. When a decision maker expresses his/her judgment by linguistic 2-tuple (l, α), then l refers to the closest linguistic term in the predefined term set S and α represents expert's deviation from that linguistic term. For example, a company thinks regarding a location that the possibility of its further extension is ‘almost very high’. In this scenario, location's rating can be quantified by linguistic 2-tuple (l6, α), i.e., the rating of the location is not exactly l6, however little bit less than l6, which can be modeled by α.Definition 3[5]Let S={l0, l1, …, lg} be a linguistic term set. For any linguistic 2-tuple (li, αi), its equivalent numerical value is obtained by the following function:Δ−1:S×[−0.5,0.5)→[0,g]Δ−1(li,αi)=i+αi=βiwhere βi∈[0, g].Example 2 Assume that S={l0, l1, l2, l3, l4, l5, l6} represents a linguistic term set and (l3, −0.3) be a linguistic 2-tuple. Based on Definition 3, the equivalent numerical value of (l3, −0.3) is Δ−1(l3, −0.3)=3+(−0.3)=2.7From Definitions 2 and 3, it is noted that any linguistic term can be converted into a linguistic 2-tuple as follows: l∈S⇒(l, 0).The ordering of two linguistic 2-tuples (lm, αm) and (ln, αn) can be done according to lexicographic order as follows:(1)If m>n then (lm, αm)>(lm, αm).If m=n then(a)(lm, αm)=(ln, αn), for αm=αn.(lm, αm)>(ln, αn), for αm>αn.In literature [12,28,34], distance measures between linguistic 2-tuples information were proposed. However, the distance methods basically compute crisp distance measure. Now in complex decision making scenario, if the linguistic terms are not defined precisely, how the distance between them can be presented by an exact numerical value. The uncertainty inherently involved in linguistic information should have a reflection in the corresponding distance measure. Thus, it would be more reasonable that the distance between two linguistic terms should be expressed in linguistic manners. With this interpretation, we provide the definition of distance between two linguistic 2-tuples as follows:Definition 4Let S={l0, l1, …, lg} be a linguistic term set. Consider a linguistic term setS´={l0´,l1´,…,lg´}whose termli´represents linguistic distance between the terms lmand lnof S such that |m−n|=i. Let (li, αi) and (lk, αk) be two linguistic 2-tuples. Then linguistic distance measure is a mappingd:ST×ST→S´Tsuch that(1)d((li,αi),(lk,αk))=ΔS´(|ΔS−1(li,αi)−ΔS−1(lk,αk)|)The linguistic distance measure satisfies the following properties:(i)d((li,αi),(lk,αk))=l0´if and only if (li, αi)=(lk, αk).d((li, αi), (lk, αk))=d((lk, αk), (li, αi)) for all (li, αi), (lk, αk)∈ST.d((li, αi), (lk, αk))≤d((li, αi), (lj, αj))+d((lj, αj), (lk, αk)) for any (li, αi), (lj, αj), (lk, αk)∈ST.In 1950, Bonferroni [19] defined Bonferroni mean aggregation operator, and it was generalized by Yager [20] and others [22,24,32,33]. In its original form, it is defined as follows:Definition 5[19]For any p, q≥0 with p+q>0, the BM aggregation operator of dimension n is a mappingBM:(ℝ+)n→ℝ+such that(2)BMp,q(a1,a2,…,an)=1n(n−1)∑i,j=1i≠jnaipajq1p+qwhereℝ+is the set of non-negative real numbers.BM in its inherent structure assumes that each input argument is related to the rest of input arguments. But in many practical situations, input arguments can be partitioned into several distinct classes and the members of each class may be interrelated to other members of that particular class. These situations can be mathematically interpreted as follows:Let A=(a1, a2, …, an) be the collection of inputs related to the attributes C={A1, A2, …, An}. Basically, ais are non-negative real numbers. Suppose that on the basis of the interrelationship pattern attribute set C is partitioned into d distinct classes P1, P2, …, Pdsuch that Pi∩Pj=∅ and∪h=1dPh=C. We further assume that attributes of each Piare interrelated to each others and there is no interrelationship among attributes of any two partitions Piand Pjwhenever i, j∈{1, 2, …, d} and i≠j. The relationship among the attributes is depicted in Fig. 2. With this information in background, the partitioned Bonferroni mean (PBM) operator of the collection of inputs (a1, a2, …, an) can be defined as follows:Definition 6For any p, q≥0 with p+q>0, the PBM operator is a mapping PBM:[0, 1]n→[0, 1] such that(3)PBM(a1,a2,…,an)=1d∑h=1d1|Ph|∑i∈Phaip1|Ph|−1∑j∈Phj≠iajq1p+qwhere |Ph| denotes the cardinality of Ph.Thus depending on the construction of the PBM operator, the aggregation of alternatives’ performance under such partitioned structure of the attributes in the context of MADM is interpreted in the following way:To keep the matter simple, we consider the case when p=q=1 and assume that (a1, a2, …, an) denotes degree of satisfaction of the alternative x under the attributes {A1, A2, …, An}. From Eq. (3), we obtain the aggregated values of the alternative x as follows:PBM(a1,a2,…,an)=1d∑h=1d1|Ph|∑i∈Phai1|Ph|−1∑j∈Phj≠iaj12It is observed that(1/(|Ph|−1))∑j∈Phj≠iajindicates average satisfaction of the attributes belong to Phexcept ai. Then the expressionai(1/(|Ph|−1))∑j∈Phj≠iajmodels conjunction of satisfaction of the attribute aiwith the average satisfaction of the rest of attributes of Ph. The computation of((1/|Ph|)∑i∈Phai((1/(|Ph|−1))∑j∈Phj≠iaj))12provides the satisfaction of interrelated attributes which are in Phby taking average of each of the statements: ‘satisfaction of aifor i∈Phand average satisfaction of rest of the attributes of Ph’. Finally, PBM(a1, a2, …, an) gives the average satisfaction degree of all the attributes which are belonged different d distinct classes P1, P2, …, Pd.Now, we provide an example to illustrate the computation procedure of PBM.Example 4: Let C={A1, A2, A3, A4, A5} be a collection of attributes partitioned into two classes P1={A1, A3, A5} and P2={A2, A4}. Assume that for alternative x, satisfaction degree with respect to different attributes are as follows: a1=0.3, a2=0.5, a3=0.4, a4=0.7, a5=0.6. For the sake of simplicity, we take p=q=1. satisfaction degree of interrelated attributes belong to partition P1 is1|P1|∑i∈P1ai1|P1|−1∑j∈P1j≠iaj12=130.30.4+0.62+0.40.3+0.62+0.60.3+0.421/2=13(0.3×0.5+0.4×0.45+0.6×0.35)1/2=0.4243satisfaction degree of interrelated attributes belong to partition P2 is1|P2|∑i∈P2ai1|P2|−1∑j∈P2j≠iaj12=12(0.5×0.7+0.7×0.5)1/2=0.5916overall satisfaction degree of alternative x isPBM(a1,a2,…,an)=12∑h=121|Ph|∑i∈Phai1|Ph|−1∑j∈Phj≠iaj12=12(0.4243+0.5916)=0.5079.Moreover, by using BM operator, the aggregated value of the given set of inputs can be computed as follows:BM(0.3,0.5,0.4,0.7,0.6)=150.30.5+0.4+0.7+0.64+0.50.3+0.4+0.7+0.64+0.40.3+0.5+0.7+0.64+0.70.3+0.5+0.4+0.64+0.60.3+0.5+0.4+0.74=0.4950which is different from the result obtained by PBM as BM does not capture the exact relationship among the aggregated arguments.Now, we use Beliakov et al.'s [33] Bonferroni mean based on partition (BPM) to aggregate the above input set a=(a1, a2, a3, a4, a5). For that purpose, we need to fix the several parameters associated with BPM. The interpretation of all the parameters are provided in Ref. [33]. For the sake of simplicity, we take subset-size parameter k=2, simple arithmetic mean as averaging operators and product operator as conjunctive function. As k=2, there are 5C2=10 partitions of the input set and BPM captures inter-relationship among them. On the basis of the above parameters, BPM of the above set of inputs can be computed as follows:(4)BPM(a)=110∑i=11012∑j∈Eiaj13∑j∈Eicaj1/2From Table 1by utilizing Eq. (4), we obtain the aggregated value BPM(0.3, 0.1, 0.4, 0.7, 0.6)=0.4062 which is significantly different from our proposed PBM operator. In analogy to the foregoing discussion, the proposed PBM operator is semantically different from the Beliakov et al.'s [33] proposed BPM operator as the later considers all possible partitions of the input set for a designated subset-size of partition and take account of interrelationship among various partitions while the former considers a single partition of the input set into different distinct classes and captures the interrelationship among the members of the each class.The above example clearly indicates that PBM has certain advantage over BM and BPM. It is also observed that the semantic structure of PBM and BPM are completely different.Some important observations of Eq. (3) are put as remark below.Remark 1 When all the inputs are belonged to the same class and they are interrelated, i.e., d=1 then Eq. (3) becomes BM as follows:(5)PBM(a1,a2,…,an)=1|P1|∑i∈P1aip1|P1|−1∑j∈P1j≠iajq1p+q=1n∑i=1naip1n−1∑i,j=1i≠inajq1p+qRemark 2 In Eq. (3), we have considered that the members of the each class in the partition are interrelated. But in some cases it may happen that some of the attributes are not related to any other attributes, i.e., they do not belong to any class of interrelated attributes. In that case, we can divide the attributes into two sets: C1: set of attributes which are related to others and C2: set of attributes which are not related to any attribute such that C1∩C2=∅. It is assumed that the attributes of C1 are partitioned as earlier based on the interrelationship pattern. Then, to compute the aggregated value of inputs, Eq. (3) can be modified as follows:(6)PBM(a1,a2,…,an)=n−|C2|n1d∑h=1d1|Ph|∑i∈Phaip1|Ph|−1∑j∈Phj≠iajq1p+q+|C2|n1|C2|∑i∈C2aiLet us discuss the properties of PBM.Theorem 1(Idempotency) Let p, q≥0 and ai=a for all i=1, 2, …, n. Then(7)PBM(a,a,…,a)=aProof: From Eq. (3), we havePBM(a,a,…,a)=1d∑h=1d1|Ph|∑i∈Phap1|Ph|−1∑j≠ij∈Phaq1p+q=1d∑h=1d1|Ph|∑i∈Phapaq1p+q=1d∑h=1d(ap+q)1p+q=aTheorem 2(Monotonicity) Let p, q≥0 and ai≤bifor all i=1, 2, …, n. Then(8)PBM(a1,a2,…,an)≤PBM(b1,b2,…,bn)Proof: Since ai≤bi, for all i1|Ph|−1∑j∈Phj≠iaq≤1|Ph|−1∑j∈Phj≠ibq,h=1,2,…,d⇒1|Ph|∑i∈Phaip1|Ph|−1∑j∈Phj≠iajq1p+q≤1|Ph|∑i∈Phbip1|Ph|−1∑j∈Phj≠ibjq1p+qh=1,2,…,dHence,PBM(a1,a2,…,an)≤PBM(b1,b2,…,bn)Theorem 3(Boundedness) Letal=miniaiandau=maxiai, then, for any p, q≥0(9)al≤PBM(a1,a2,…,an)≤auProof: As ai≤aufor all i, from Eq. (8), we have(10)PBM(a1,a2,…,an)≤PBM(au,au,…,au)It follows from Eq. (7)(11)PBM(a1,a2,…,an)≤auSimilarly,(12)PBM(a1,a2,…,an)≥alBased on Eq. (3), the definition of 2-tuple linguistic partitioned Bonferroni mean (2TLPBM) for aggregating 2-tuple linguistic information can be given as follows:Definition 7Let (li, αi)(i=1, 2, …, n) be a collection of linguistic 2-tuples partitioned into d distinct classes P1, P2, …, Pd. For any p, q≥0 with p+q>0, 2-tuple linguistic partitioned Bonferroni mean aggregation operator of dimension n is a mapping 2TLPBM:(ST)n→STsuch that(13)2TLPBMp,q((l1,α1),(l2,α2),…,(ln,αn))=ΔS1d∑h=1d1|Ph|∑i∈Ph(Δ−1(li,αi))p1|Ph|−1∑j∈Phj≠i(Δ−1(lj,αj))q1p+q=ΔS1d∑h=1d1|Ph|∑i∈Phβip1|Ph|−1∑j∈Phj≠iβjq1p+qwhereβi=ΔS−1(li,αi).Based on the number of classes in the partition and different values of the parameters p and q, we can derive the following special cases of 2TLPBM:(1)when there is no partition among the set of input arguments, i.e., all the inputs belong to the same class, then Eq. (13) reduces to 2-tuple linguistic Bonferroni mean aggregation operator as follows:(14)2TLPBMp,q((l1,α1),(l2,α2),…,(ln,αn))=ΔS1|P1|∑i∈P1βip1|P1|−1∑j∈P1j≠iβjq1p+q=ΔS1n∑i=1nβip1n−1∑i,j=1i≠jβjq1p+qwhen all the inputs belong to the same class and q=0, then Eq. (13) reduces to 2-tuple linguistic power arithmetic mean as follows:(15)2TLPBMp,0((l1,α1),(l2,α2),…,(ln,αn))=ΔS1|P1|∑i∈P1βip1|P1|−1∑j∈P1j≠iβj01p=ΔS1n∑i=1nβip1pwhen all the inputs belong to the same class and, p=1 and q=0, then Eq. (13) reduces to 2-tuple linguistic arithmetic mean as follows:(16)2TLPBM1,0((l1,α1),(l2,α2),…,(ln,αn))=ΔS1|P1|∑i∈P1βi11|P1|−1∑j∈P1j≠iβj0=ΔS1n∑i=1nβiwhen all the inputs belong to the same class and, p→0 and q=0, then Eq. (13) reduces to 2-tuple linguistic geometric mean as follows:(17)limp→02TLPBMp,0((l1,α1),(l2,α2),…,(ln,αn))=limp→0ΔS1|P1|∑i∈P1βip1|P1|−1∑j∈P1j≠iβj01p=limp→0ΔS1n∑i=1nβip1p=ΔSlimp→01n∑i=1nβip1p=ΔS∏i=1nβi1nThe desirable properties of 2TLPBM have been described in the following theorem (the validity of the theorem follows directly from Theorem 1, Theorem 2 and Theorem 3):Theorem 4Let (li, αi)(i=1, 2, …, n) be a collection of linguistic 2-tuples partitioned into d distinct linguistic classes P1, P2, …, Pd. Then following properties hold:(1)(Idempotency) If (li, αi)=(l, α) for all i, then(18)2TLPBMp,q((l,α),(l,α),…,(l,α))=(l,α)(Monotonicity) If(li′,αi′)(i=1,2,…,n)be another collection of linguistic 2-tuples with same partition as in collection (li, αi)(i=1, 2, …, n) such that(li,αi)≤(li′,αi′)for all i, then(19)2TLPBMp,q((l1,α1),(l2,α2),…,(ln,αn))≤2TLPBMp,q((l1′,α1′),(l2′,α2′),…,(ln′,αn′))(Boundedness) Let(lmin,αmin)=mini(li,αi)and(lmax,αmax)=maxi(li,αi), then(20)(lmin,αmin)≤2TLPBMp,q((l1,α1),(l2,α2),…,(ln,αn))≤(lmax,αmax)It is to be noted that 2TLPBM, defined in Eq. (13), does not consider the importance of the input arguments. However, in many real-world problems, especially, in MADM, the weight vector of the attributes plays an important role in the aggregation process. In order to take account of weight vector of input arguments, we define two weighted form of 2TLPBM in this subsection.In this case, weights of the attributes are completely known to decision makers and they are expressed in exact numerical values.Definition 8Let (li, αi)(i=1, 2, …, n) be a collection of linguistic 2-tuples partitioned into d distinct classes P1, P2, …, Pd. For any p, q≥0 with p+q>0, weighted 2-tuple linguistic partitioned Bonferroni mean (W2TLPBM) aggregation operator of dimension n is a mapping W2TLPBM:(ST)n→STsuch that(21)W2TLPBMp,q((l1,α1),(l2,α2),…,(ln,αn))=ΔS1d∑h=1d1∑i∈Phwi∑i∈Phwi(Δ−1(li,αi))p1∑j∈Phj≠iwj∑j∈Phj≠iwj(Δ−1(lj,αj))q1p+q=ΔS1d∑h=1d1∑i∈Phwi∑i∈Phwiβip1∑j∈Phj≠iwj∑j∈Phj≠iwjβjq1p+qwherewi(i=1,2,…,n)indicates the relative importance of the input argument (li, αi)(i=1, 2…, n) and satisfies the conditions:wi≥0and∑i=1nwi=1, andβi=ΔS−1(li,αi).Remark 3 When each input argument (li, αi)(i=1, 2, …, n) has equal importance, i.e.,w=(1/n,1/n,…,1/n)T, then Eq. (21) becomes 2TLPBM(22)2TLPBMp,q((l1,α1),(l2,α2),…,(ln,αn))=ΔS1d∑h=1d1∑i∈Ph1n∑i∈Ph1nβip1∑j∈Phj≠i1n∑j∈Phj≠i1nβjq1p+q=ΔS1d∑h=1d1|Ph|n∑i∈Ph1nβip1|Ph|−1n∑j∈Phj≠i1nβjq1p+q=ΔS1d∑h=1d1|Ph|∑i∈Phβip1|Ph|−1∑j∈Phj≠iβjq1p+qRemark 4 When all the inputs belong to same class and, p=1 and q=0, W2TLPBM reduces to weighted linguistic 2-tuple arithmetic mean(23)W2TLPBM1,0((l1,α1),(l2,α2),…,(ln,αn))=ΔS1∑i∈Ph1n∑i∈Ph1nβip1∑j∈Phj≠i1n∑j∈Phj≠i1nβj011+0=ΔS(∑i=1nwiβi)In some situations, it is not always possible to determine the relative importance of the attributes in exact numerical value due to lack of information, time pressure or incomplete knowledge of the studied system. In such scenario decision makers prefer to provide the importance of attributes in linguistic terms. In order to calculate the overall performances of the alternatives where the importances of the attributes are expressed in linguistic terms, we define linguistic weighted form of 2TLPBM as follows:Definition 9Let (li, αi)(i=1, 2, …, n) be a collection of linguistic 2-tuples partitioned in d distinct classes P1, P2, …, Pd. For any p, q≥0 with p+q>0, linguistic weighted 2-tuple linguistic partitioned Bonferroni mean (LW-2TLPBM) aggregation operator of dimension n is a mapping LW−2TLPBM:(ST)n→STsuch that(24)LW−2TLPBMp,q((l1,α1),(l2,α2),…,(ln,αn))=ΔS1d∑h=1d1∑i∈Phβ´i∑i∈Phβ´iβip1∑j∈Phj≠iβ´j∑j∈Phj≠iβ´jβjq1p+qwhere (ri, αi)(i=1, 2, …, n) is the linguistic importance of the input arguments (li, αi) andΔS−1(ri,αi)=βi´.Remark 5 When all the inputs belong to same class and, p=1 and q=0, LW-2TLPBM reduces to linguistic weighted 2-tuple linguistic arithmetic mean(25)LW−2TLPBM1,0((l1,α1),(l2,α2),…,(ln,αn))=ΔS1∑i∈Phβ´i∑i∈Phβ´iβi11∑j∈Phı≠jβ´j∑j∈Phj≠iβ´jβj011+0=ΔS1∑i∈Phβ´i∑i∈Phβ´iβiMoreover, it can be easily proved that LW-2TLPBM satisfies the Idempotency, Monotonocity, and Boundedness properties of aggregation operator.In this section, we present an approach to solve MAGDM problem in which decision maker's weight is completely unknown and the weights of the attributes are given in linguistic terms.There is a group of t decision makers {J1, J2, …, Jt} and a set of m alternatives {X1, X2, …, Xm}. The decision makers’ aim is to choose the best alternative among m alternatives depending on n attributes {A1, A2, …, An}. Each decision maker comes from different background and, possesses different level of knowledge and abilities which make their importance different in the decision making process. The attributes are partitioned into d classes P1, P2, …, Pdand members of intra class have interrelationship. The kth decision maker Jk(k=1, 2, …, t) provides his/her rating of an alternative Xi(i=1, 2, …, m) with respect to the attribute Aj(j=1, 2, …, n) as a 2-tupledijk=(lijk,αijk)wherelijkbelongs to the predefined linguistic term set S andαijk∈(−0.5,0.5]. The decision maker Jk's ratings of the alternatives is summarized in 2-tuple linguistic decision matrixDk=(dijk)m×n.Assume that the decision makers provide the weight vector of the attributes in 2-tuple linguistic form. LetWk=((w1k,η1k),(w2k,η2k),…,(wnk,ηnk))be the 2-tuple linguistic weight vector of the attributes given by the decision maker Jk, wherewjkbelongs to a predefined linguistic term setS´and ηjk∈(−0.5, 0.5].With this available information our main concern is to select the most desirable alternative from the finite alternatives set {X1, X2, …, Xm}. To do this, an algorithm has been proposed here. Before presenting the proposed MAGDM algorithm, we first describe some key steps of the decision making process as follows:For each alternative Xi, the evaluation over all attributes, denoted as(li1k,αi1k)(k=1,2,…,t), is calculated from individual decision matrixDk=(dijk)m×nby utilizing the LW-2TLPBM operator as follows:(26)LW−2TLPBMp,q((li1k,αi1k),(li2k,αi2k),…,(link,αink))=ΔS1d∑h=1d1∑j1∈Phβ´j1k∑j1∈Phβ´j1k(βij1k)p1∑j2∈Phj2≠j1β´j2k∑j2∈Phj2≠j1β´j2k(βij2k)q1p+qwhereβij1k=ΔS−1(lij1k,αij1k)andβ´j1k=ΔS−1(wj1k,ηj1k).The evaluation of the overall performance of the alternatives can be presented in the matrix format as follows:J1J2…JtO=X1X3⋮Xm(r11,α11)(r12,α12)…(r1t,α1t)(r21,α21)(r22,α22)…(r2t,α2t)⋮⋮…⋮(rm1,αm1)(rm2,αm2)…(rmt,αmt)Let λ=(λ1, λ2, …, λt)Tbe the weight vector of the decision makers such that λk≥0 and∑k=1tλk=1. For each alternative Xithe group overall opinion (ri, αi) can be calculated by using Eq. (23) as follows:(27)(ri,αi)=ΔS∑k=1tλkΔ−1(rik,αik)=ΔS∑k=1tλkβikwhereβik=Δ−1(rik,αik).In order to determine the weight vector λ of decision makers, we propose a method inspired by the idea of Wu and Chen's [37] maximization deviation approach, which is described as follows:Let(ϵik,αik)be the linguistic deviation between kth decision maker's opinion and group overall opinion about the alternative Xi. Then, the values of(ϵik,αik)can be derived by using Eq. (1) as follows:(ϵik,αik)=ΔS´(|ΔS−1(rik,αik)−ΔS−1(ri,αi)|)=ΔS´(|βik−∑k=1tλkβik|)(i=1,2,…,m;k=1,2,…,t)Our aim is to choose the weight vector λ in such a way that it will minimize the total linguistic deviation∑i=1m∑k=1t(ϵik,αik)(i=1,2,…,m;k=1,2,…,t)between group overall opinion and individual decision makers’ opinions. Since each linguistic 2-tuple is equivalent to its numerical value, the minimization of total linguistic deviation is equivalent to its total numerical value. Let(28)πik=ΔS−1(ϵik,αik)=|βik−∑k=1tλkβik|(i=1,2,…,m;k=1,2,…,t)be the equivalent numerical value of the 2-tuple linguistic deviation(ϵik,αik). Now, the minimization of total linguistic deviation leads to following optimization problem:(29)min∑i=1m∑k=1t(πik)2=∑i=1m∑k=1t(βik−∑k=1tλkβik)2s.t.∑k=1tλi=1λk≥0,k=1,2,…,t.To solve this quadratic programming problem, Lagrange's function can be constructed as follows:(30)L(λ,η)=∑i=1m∑k=1t(βik−∑k=1tλkβik)2−2η(∑k=1tλi−1)where η is the Lagrange's multiplier.Differentiating the Lagrange's function L(λ, η) with respect to λu(u=1, 2, …, t) and η partially and set all the partial derivatives equal to zero, we obtain the following set of equations:(31)∂L(λ,η)∂λu=−2∑i=1m∑k=1t(βik−∑k=1tλkβik)βiu−η=0(u=1,2,…,t)(32)∂L(λ,η)∂η=∑k=1pλi−1=0Eq. (31) can be written as(33)∑i=1m∑k=1t(∑k=1tλkβikβiu−βikβiu)−η=0(u=1,2,…,t)Eq. (33) can be simplified into the following form(34)∑k=1tλk∑i=1mtβikβiu−∑i=1m∑k=1tβikβiu−η=0(u=1,2,…,t)Eq. (34) can be written as matrix form(35)Bλ−V−ηe=0where,V=∑i=1m∑k=1tβikβi1,∑i=1m∑k=1tβikβi2,….,∑i=1m∑k=1tβikβitT,e=(1,1,…,1)TB=∑i=1mtβi1βi1∑i=1mtβi2βi1…∑i=1mtβitβi1∑i=1mtβi1βi2∑i=1mtβi2βi2…∑i=1mtβitβi2⋮⋮…⋮∑i=1mtβi1βit∑i=1mtβi2βit…∑i=1mtβitβitEq. (32) can be put in the form(36)eTλ=1Solving Eqs. (35) and (36), for λ and η, we obtain(37)η=1−eTB−1VeTB−1e(38)λ=B−1e(1−eTB−1V)eTB−1e+B−1VThe weight vector λ, determined from the Eq. (38), is the optimal weight vector of the decision makers.The steps of the proposed MAGDM method can be given as follows:AlgorithmStep 1: Give the individual decision maker's decision matrix Dk(k=1, 2, …, t) and the weight vectors of the attributes Wk(k=1, 2, …, t).: By using each decision maker's decision matrix Dk(k=1, 2, …, t), calculate the individual evaluation of the alternatives over all attributes by using Eq. (26) and construct the matrix O.: Give the decision maker's weight information. Depending on the nature of weight information, group overall opinions of the alternatives, i.e., (ri, αi)(i=1, 2, …, m) can be computed as follows:Case-IIf the decision makers’ weights are completely known and are given in exact numerical values, then group overall opinions are computed by using weighted 2-tuple linguistic arithmetic mean (Eq. (23)).If the weight information is known and provided in linguistic term, then group overall opinions are computed by using linguistic weighted 2-tuple arithmetic mean (Eq. (25)) as follows(39)(ri,αi)=ΔS∑k=1tΔS´−1(λk,ξk)∑k=1tΔS´−1(λk,ξk)ΔS−1(rik,αik)=ΔS(∑k=1tλkβik)where ((λ1, ξ1), (λ2, ξ2), …, (λt, ξt))Tbe the linguistic weight vector of the decision makers.If the weight information is completely unknown, then determine the decision makers weight vector (λ1, λ2, …, λt)Tby solving the optimization problem (Eq. (29)) and group overall opinions about alternatives are obtained by utilizing Eq. (23): Rank the alternatives based on group performance values (ri, αi)(i=1, 2, …, m) by using the comparison method of linguistic 2-tuple described in Section 2 and choose the best alternative.We next present a real-life example to illustrate the proposed MAGDM technique.An investment company has a large amount of capital to invest. Company's board has identified five possible investment areas:(i)X1: commodity marketX2: stock marketX3: goldX4: real estateX5: long-term bondsIn order to select the best possible investment option, company's board has decided to appoint an expert panel which consists of four decision makers J1, J2, J3, J4. The expert asks to evaluate the investment options on the basis of following five inter-related criteria:(i)A1: the risk of losing capital sumA2: the amount of interest receivedA3: the vulnerability of capital sum to modification by inflationA4: market potentialA5: growth potentialBased on the interrelationship pattern, the following partition structure can be assumed for the attribute set: P1={A1, A2, A3} and P2={A4, A5}.Due to the uncertainty of the financial market, the decision makers may use linguistic terms to express their opinions. The decision makers employ the linguistic term set S, given as below to evaluate the alternatives against the attributes.S={l0=extremelybad,l1=verybad,l2=bad,l3=somewhatbad,l4=medium,l5=somewhatgood,l6=good,l7=verygood,l8=extremelygood}The ratings of the alternatives Xi(i=1, …, 5) given by the decision makers Jk(k=1, …, 4) under all attributes Aj(j=1, …, 5) are presented in Tables 2–5.The decision makers also use linguistic variable from linguistic term setS¯to assess the importance of different attributes.S¯={s´0=extremelylow,s´1=verylow,s´2=low,s´3=somewhatlow,s´4=medium,s´5=somewhathigh,s´6=high,s´7=veryhigh,s´8=extremelyhigh}The linguistic weight vector given by the decision makers are summarized in the Table 6Now, the proposed MAGDM technique is applied for the selection of the best investment options as follows:Step 1: Since all the inter-related attributes are divided into two class, so the number of class in the partition is d=2. For the sake of simplicity in the calculation, we set p=q=1 in the Eq. 26. First individual decision matrix Dkwith the help of Tables 3–5 are constructed. Then by utilizing the linguistic weight vector given in Table 6, and Eq. 26, each decision maker's evaluation over all attributes about each alternative is calculated and presented in the Table 7.Step 2: Let λ=(λ1, λ2, λ3, λ4)Tbe the weight vector of the decision makers Jk, (i=1, 2, 3, 4). Then λ can be determined by minimizing the total linguistic deviation between group opinion and individual opinions. To calculate total linguistic deviation, we have to computeπik(k=1,2,…,4,i=1,2,…5). For example, linguistic deviations of decision maker J1 from group overall opinion with respect to all alternatives can be derived by using Eq. 28 and Table 7 as follows:Π11=|2.2727−(2.2727λ1+3.0439λ2+3.9494λ3+2.3954λ4)|Π21=|3.6819−(3.6819λ1+3.9141λ2+2.9631λ3+3.5993λ4)|Π31=|2.5696−(2.5696λ1+2.9464λ2+2.6223λ3+2.9324λ4)|Π41=|3.0963−(3.0963λ1+3.1111λ2+4.4177λ3+2.8675λ4)|Π51=|3.3012−(3.3012λ1+4.8066λ2+2.7589λ3+2.5085λ4)|In a similar manner, deviations of all decision makers for all alternatives can be calculated and the value of λ is obtained by solving the following quadratic programming problem:min∑i=15∑k=14(πik)2=∑i=15∑k=14(βik−∑k=1pλkβik)2s.t.∑k=14λi=1λk≥0,k=1,2,3,4.Solving the above optimization problem, we obtain λ=(0.25, 0.25, 0.25, 0.25)T.Step 3: Using Eq. (23), the group performance values of the alternatives are calculated as follows: (r1, α1)=(l3, −0.0834), (r2, α2)=(l4, −0.4604), (r3, α3)=(l3, −0.2323), (r4, α4)=(l3, 0.3731), (r5, α5)=(l3, 0.3438).Step 4: According to the group performance values (ri, αi)(i=1, 2, 3, 4, 5), the ranking order of the alternatives is X2>X4>X5>X1>X3. Thus, stock market is the most suitable option to invest capital.In Eq. 23, if we take different values of p and q, the overall group opinion about the alternatives Xi(i=1, 2, …, 5) are changed which is depicted in details in Figs. 5–9.From the above mentioned Figs. 3–9, one may observe that group performance of the alternatives also depends on the value of the parameters p and q. In general, p and q can take any values between zero to infinity and these parameters are not robust. However, the larger values of p and q require more computational effort to compute alternatives’ performances [23]. In special case, when one of the parameter becomes zero no interrelationship among the aggregated arguments is captured. Therefore, from application point of view, we suggest the decision maker to take the values of p and q as one, which is not only intuitive and simple in computation but also allows us to capture the inter-relationship among the aggregated arguments in a comprehend way.To further illustrate the rationality and flexibility of the proposed decision making approaches, we utilize investment decision making problem to compare the proposed method with the five existing MAGDM methods, based on linguistic 2-tuple, presented by Wei [11], Wei and Zaho [13], Park et al. [15], Xu and Wang [18] and Zhang and Chou [30].The methods proposed in Ref. [11,13,15,30] consist of two main phases. In first phase assessment of each alternative, over all attributes corresponding to each decision maker is calculated. At next stage overall group preference values of the alternatives are obtained and finally the ranking is done. On the other hand, in Ref. [18] from individual decision matrices, group decision matrix is obtained and subsequently, overall group preference values of the alternatives over all attributes are computed. The proposed method also follows the former structure. The final ranking order of the alternatives obtained by the aforementioned methods is presented in Table 8. It has been observed from Table 8 that ranking orders of the alternatives obtained by five existing methods are significantly different from ranking order obtained by the proposed method. In view of such observations, it is irresistible to have a deeper look at the existing models [11,13,15,18,30] from the perspective of their methodologies and identify the reasons as to why the ranking results yield a significant difference. In this regard, we have made some observations which are sum below.•In Refs. [11,13,15,18,30] different linguistic aggregation operators were used. However, none of the aggregation operators can model exact relationship among the aggregated arguments. In the proposed method the specific interrelationship pattern among the attributes is captured by LW-2TLPBM. Compared with the former this is one of the main advantage of the proposed one.In Refs. [13,15,18,30], weight vector of the attributes are given in exact numerical form. In complex decision making scenario, decision makers feel more comfortable to provide the attributes’ weight in linguistic terms rather than exact numerical values. However, in Ref. [11] attributes’ weights are presented by using linguistic terms. Further, it may also be happened that each decision maker assigns different weights to different attributes and it should also be considered during the process decision making. However, these situations were not modeled by existing ones. In view of this, the proposed method can deal with both linguistic and numerical weight vector of the attributes and it can also take account of different weight vectors of attributes provided by the decision makers, which are notable difference of the proposed method from existing ones.Another critical issue is regarding the method for computing unknown weights of the decision makers. In Ref. [11,13], decision makers’ weights are unknown and OWA aggregation operators are used to obtain the group overall evaluations about the alternatives. However, the OWA operator assigns high weight to those decision makers who have given high ratings of the alternatives with respect to attributes in comparison to others and neglects the decision makers’ opinions with low ratings. To overcome this drawback, in the proposed method, a novel approach has been developed to derive decision makers’ weight based on the idea that linguistic distance between group and individuals overall opinions should be minimized.The ranking orders of the alternatives obtained by Wei [11] and Park et al. [15] are slightly different from the proposed method. The alternative A2 is identified as a best alternative by all these methods and A4 is the second best alternative. The ranking orders of rest of the three alternatives A3, A5, A1 obtained by Wei [11] and Park et al. [15] are A3>A5>A1 and A3>A1>A5 whereas ranking order by the proposed method is A5>A1>A3. The difference has occurred due to the partition of attributes and inter-relationship among the attributes are not captured by the methods proposed in Refs. [11,15]. The ranking result obtained by Xu and Wang [18] and the proposed method are completely same although these methods are significantly different in structure. In the former, first group was formed by using linguistic power aggregation operator (2TLPWA), which allows the inputs being aggregated to support and reinforce each other [35], whereas the later concentrates on capturing the interrelationship among attributes by averaging pair-wise satisfaction of attributes. The methods proposed in Ref. [13] identifies A5 as the best alternative which is different from other methods. The ranking result of [13] differs from others due to using D2TOWA operator to compute the group overall opinions about the alternatives. Because, D2TOWA assumes that decision makers’ opinions are dependent on the each other. On the other hand the method developed in Ref. [30] use W2TLBM operator by assuming that each attribute is related to the rest of the attributes and thus, the ranking order produces by Zhang and Wu [30] differs significantly from proposed one for not capturing exact relationship among the attributes.

@&#CONCLUSIONS@&#
