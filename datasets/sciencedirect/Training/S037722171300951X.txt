@&#MAIN-TITLE@&#
Genetic-algorithm-based simulation optimization considering a single stochastic constraint

@&#HIGHLIGHTS@&#
We derive two new GA-based algorithms for the constrained DOvS problem.The first one offers global convergence as the sampling effort goes to infinity.The second heuristic algorithm can take advantage of the desirable mechanics of GA.The second algorithm is better able to find near-optimal solutions in finite time.An empirical study is performed to compare the performance of these algorithms.

@&#KEYPHRASES@&#
Metaheuristics,Genetic algorithm,Simulation,Simulation-based optimization,Feasibility determination,

@&#ABSTRACT@&#
In this paper, we consider the discrete optimization via simulation problem with a single stochastic constraint. We present two genetic-algorithm-based algorithms that adopt different sampling rules and searching mechanisms, and thus deliver different statistical guarantees. The first algorithm offers global convergence as the simulation effort goes to infinity. However, the algorithm’s finite-time efficiency may be sacrificed to maintain this theoretically appealing property. We therefore propose the second heuristic algorithm that can take advantage of the desirable mechanics of genetic algorithm, and might be better able to find near-optimal solutions in a reasonable amount of time. Empirical studies are performed to compare the efficiency of the proposed algorithms with other existing ones.

@&#INTRODUCTION@&#
Optimization via simulation (OvS) is the process of optimizing the expected performance of a discrete event, stochastic system through computer simulation (e.g., Abo-Hamad & Arisha, 2013; Arreola-Risa, Giménez-García, & Martínez-Parra, 2011; Chen, 2011; Hong & Nelson, 2009; Tsai, 2013; Tsai & Chu, 2012; Yu, Tsai, & Huang, 2010). Hong and Nelson (2009) classified OvS problems into three categories based on the feasible region structure: continuous OvS, discrete OvS (DOvS), and ranking and selection (R&S). For the R&S problems, the number of alternatives in the feasible region is so small (often less than 500) that we may simulate all solutions and choose the best (or near the best) among them with a specified confidence level (see Kim & Nelson (2006) for a survey). For DOvS problems, we have a very large number of feasible solutions (discrete design variables), and the existing algorithms often emphasize a global convergence to the optimal solution asymptotically. In practice, decision makers usually need to consider multiple performance measures rather than a single one due to physical or managerial requirements. For instance, in a typical flow-line problem, the decision maker is interested in finding a buffer allocation setting to maximize the expected throughput over a fixed planning horizon, while also keeping the expected overall work-in-process no greater than a certain level. Recently, more research interest in the OvS literature has been directed to solving problems with stochastic constraints or multiple performance measures. Morrice and Butler (2006) developed a R&S procedure based on multi-attribute utility theory to allow tradeoffs between conflicting targets. Kabirian and Ólafsson (2009) proposed a heuristic iterative algorithm for finding the best solution in the presence of multiple stochastic constraints. Kleijnen, van Beers, and van Nieuwenhuyse (2010) combined methodologies from metamodeling and mathematical programming for solving constrained optimization of random simulation models. Bhatnagar, Hemachandra, and Mishra (2011) and Szechtman and Yücesan (2008) proposed stochastic approximation algorithms for constrained optimization via simulation. Luo and Lim (2011) proposed a new approach that converts a constrained optimization problem into an unconstrained one by using the Lagrangian function. Similarly, Park and Kim (2011) presented a method called penalty function with memory, which is added to the objective function and then replaces a DOvS problem with stochastic constraints into a series of new unconstrained problems. Vieira Junior, Kienitz, and Belderrain (2011) proposed a novel simulation allocation rule to be used in a locally convergent random search algorithm, called COMPASS (Hong & Nelson, 2006), to handle stochastic constrained problems. Hunter and Pasupathy (2013) and Pujowidianto, Hunter, Pasupathy, Lee, and Chen (2012) applied a large-deviations approach to provide an asymptotically optimal sample allocation that maximizes the rate at which the probability of false selection tends to zero. These methods have a requirement that all the solutions be simulated at least once, so they are more appropriate to the setting where the solution space is finite and contains a small number of elements. Andradóttir and Kim (2010) presented one type of R&S procedure (called the Feasibility Determination Procedure, or FDP) that checks the feasibility of each solution among a finite set with respect to a stochastic constraint. Instead of giving a guarantee of correctly choosing the best solution, Bayesian procedures maximize the posterior probability of correct selection within a given simulation budget. There are some works which developed efficient Bayesian procedures to address the constrained optimization problem (e.g., Guan, Song, Ho, & Zhao, 2006; Jia, 2009; Lee, Pujowidianto, Li, Chen, & Yap, 2012; Li, Lee, & Ho, 2002). It should be noted that the existing algorithms handling stochastic constraints are more appropriate to be used when the number of solution designs is not too large. This implies that they will become inefficient (i.e., require excessive sampling cost) when applied to a very large solution space.Most commercial OvS solvers use optimization metaheuristics, such as tabu search, neural nets, and genetic algorithms (GAs), that have generally been designed and proven to be effective on difficult and deterministic optimization problems. While these algorithms often find promising solutions quickly, they may also become pure random search methods if the stochastic variation of output (i.e., simulation noise) is high or the number of obtained samples for each solution is set too low. That is, their implementations do not always adequately account for the presence of statistical errors. In addition, these algorithms do not provide any statistical guarantee regarding the quality or goodness of the final selected solution. To handle the aforementioned issues, Boesel, Nelson, and Kim (2003) proposed an adaptive genetic-algorithm-based procedure to account for simulation noise in the stochastic optimization context. Their procedure also guarantees to return the best solution over the solutions visited by a heuristic search procedure. Subsequently, Xu, Nelson, and Hong (2010) used the niching GA together with COMPASS (Hong & Nelson, 2006) to establish local optimality with statistical confidence when simulating only a small portion of the feasible solutions. Nazzal, Mollaghasemi, Hedlund, and Bozorgi (2012) proposed a simulation optimization methodology that combines the GA and a R&S procedure under common random number (CRN). They developed a new R&S procedure to select a nonempty subset so that the best solution is contained in the subset with a pre-specified probability. Fitness values and selective probabilities are then computed based on the estimated performances which are obtained from the aforementioned R&S procedure. Notice that these genetic-algorithm-based procedures are adapted for solving the DOvS problem, but can only optimize the expected value of a single performance measure (see Ólafsson (2006) for a review of metaheuristics for OvS). To handle the multi-objective simulation optimization problem, Lee, Chew, Teng, and Chen (2008) developed a solution framework which integrates evolutionary algorithm with multi-objective computing budget allocation method (MOCBA). They employed MOCBA to efficiently allocate simulation replications to solutions in the current population. The proposed approach is applied on a multi-objective aircraft spare parts allocation problem to find a set of non-dominated solutions. Horng, Lin, Lee, and Chen (2013) used GA in combination with a surrogate model to find a set of good solutions in the global search stage. In the second stage they employed a probabilistic local search method to identify the approximate local optima. In the final stage OCBA is used to obtain the best solution among the promising ones identified previously.In this paper, we propose two efficient algorithms (based on GA) that are both theoretically robust and of practical value for solving the DOvS problem with a single stochastic constraint. In both proposed algorithms we use GA to guide the search process, because it is a population-based algorithm that simultaneously considers multiple candidate solutions and is shown to be more robust to stochastic noise (Xu et al., 2010). GA works with a population of potential solutions and moves this population toward the optimum iteratively. The terms iteration and generation are used interchangeably in this work to refer to the process of transforming one population of solutions to another. The proposed GA is adapted to handle two performance measures with stochastic noise (i.e., a stochastic objective and constraint) in a simulation environment. Two types of R&S procedure are incorporated into our DOvS algorithms to enhance its statistical efficiency and validity. We use FDP (see Andradóttir & Kim (2010)) repeatedly in the proposed GA to ensure that the candidate solutions in a population are feasible with respect to the stochastic constraint (with some confidence). See Appendix A.1 for the statistical guarantee provided by FDP. At the end of the algorithms we also invoke the clean-up procedure proposed in Boesel, Nelson, and Ishii (2003) to select the best with respect to the stochastic objective from a set of potential solutions. See Appendix A.2 for a detailed description of this clean-up procedure. The first proposed DOvS algorithm guarantees global convergence as the simulation effort goes to infinity (under the condition that the picked solution is feasible), and also guarantees to choose the best among all evaluated possibly feasible solutions with a specified confidence level. Of course it is somewhat reassuring to have convergence statements, in the sense that the algorithms will eventually reach the global optimal when given large enough simulation effort. However, the algorithm’s finite-time efficiency may be sacrificed to maintain this theoretically appealing property. Further, this algorithm has to visit every solution infinitely often to guarantee convergence, which is not very practically meaningful especially when the sampling budget is limited. We therefore propose the second DOvS algorithm, which is more heuristic-oriented and can take advantage of the desirable inherent properties of GA (e.g., the adaptive constraint-handling techniques and the mechanism of elite population, see Coello (2002)). The second algorithm is designed to identify the best solution among the final elite population, and may deliver competitive performance in a reasonable computation time.The paper is organized as follows. In Section 2 we define the DOvS problem with a single stochastic constraint, and introduce the relevant notations and assumptions. Sections 3 and 4 present two DOvS algorithms that adopt different sampling rules and searching mechanisms, and thus provide different statistical guarantees. We give a high-level review of the existing techniques we incorporate, and a detailed description of only the most critical enhancements. An empirical evaluation to compare different algorithms is provided in Section 5, while the paper ends with some concluding remarks in Section 6. The convergence proof and some details of our algorithms are contained in Appendix A.Our goal is to select the solution with the largest or smallest expected performance (in terms of the stochastic objective) among a large number of candidate solutions that satisfy a single stochastic constraint. LetGj(xi)denote the jth simulation observation taken from solutionxi(associated with the objective performance measure), and letHj(xi)be the jth simulation observation taken from solutionxi(associated with the stochastic constraint). The ith solutionxiis a vector of d integer decision variables in a feasible regionΩ, and is denoted byxi=(xi1,xi2,…,xid)={xiℓ,ℓ=1,2,…,d}. The expected performances with respect to the objective and the constraint are defined asg(xi)=E[Gj(xi)]andh(xi)=E[Hj(xi)]fori=1,2,…,k,j=1,2,…, respectively. A general formulation of the constrained DOvS problem of interest is described as follows:minxi∈Ωg(xi),where the feasible regionΩis defined by the following stochastic constraint:h(xi)⩾Q,where the constant Q is specified by the analyst. The expected valuesg(xi)andh(xi)are unknown and cannot be evaluated analytically, but can be measured or estimated via a simulation experiment at settingxi. We make the following assumption on(Gj(xi),Hj(xi))fori=1,2,…,k,j=1,2,…:Assumption 1Gj(xi)Hj(xi)∼iidBNg(xi)h(xi),Σi,This normality assumption can be justified when the vector(Gj(xi),Hj(xi))is the average of some output random variables from independent multiple replications, or is a batch of successive observations within a single run of a steady-state simulation (see Law (2007)). Further, it is reasonable to assume dependence betweenGj(xi)andHj(xi), because the observations with respect to different performance measures from the same simulated solution are usually correlated in practice (see Andradóttir & Kim (2010) for some examples). We also assume that all solutionsxifori=1,2,…,kare simulated independently (i.e., CRN is not used across different solutions). Our DOvS algorithms consist of two types of R&S procedures: FDP and the clean-up procedure. The efficiency of FDP will not be benefited by the use of CRN, since a feasibility check does not require comparisons among solutions. In addition, it is difficult for the two-stage clean-up procedure to account for the effect of CRN, especially when the number of candidate solutions is very large (see Nelson, Swann, Goldsman, & Song (2001) for a discussion). Without loss of generality, we assume that a smallerg(xi)corresponds to better performance (ifxiis a feasible solution). If larger expected performance is desired, we will then multiply each observationGj(xi)by −1 before executing the algorithm. To achieve global convergence for the algorithm described later in Section 3, we also need the following assumption on the expected performance function associated with the constraint.Assumption 2For everyxi=(xi1,xi2,…,xid)∈Zdandxj=(xj1,xj2,…,xjd)∈Zd, ifxiℓ⩽xjℓ,∀ℓ∈{1,2,…,d}, thenh(xi)⩽h(xj)holds.Note thatZdrepresents d-dimensional vectors with integer components. The above assumption implies that the constraint expected performanceh(xi)increases if we increase any component ofxi(some other components are probably fixed). This nondecreasing assumption is quite reasonable for many service systems, such as call center staffing and emergency vehicle dispatching (e.g., Atlason, Epelman, & Henderson, 2004; Cezik & L’Ecuyer, 2008). For instance, in a call center staffing problem,h(xi)could represent the aggregated service level over all periods, defined as the fraction of calls answered within a specified time limit. The staffing levelxiis defined as a vector, with the jth componentxijgiving the number of employees working the jth shift. In the literature related to this problem, it is often assumed that the service level is concave and component wise increasing as a function of the staffing-level vector.In the following sections, we describe two GA-based algorithms for solving the DOvS problem with a single stochastic constraint. The proposed simulation optimization framework for both algorithms has three stages, Initialization, Searching & Comparing, and Selecting. Assumptions 1 and 2 are required for the statistical validity of the first algorithm. This delivers global convergence as the simulation effort approaches infinity, and also guarantees to select the best among all potential solutions that have been declared to be feasible by FDP. On the other hand, the second algorithm only needs Assumption 1. It can only pick up the best among a small number of promising solutions (without achieving global convergence), but may demonstrate better empirical performance when given a fixed amount of sampling budget.In this section we present a statistically valid algorithm that can offer global convergence as the number of samples goes to infinity, and at the same time guarantees to select the best among all potential solutions that have been declared to be feasible by FDP with a specified confidence level. In the Initialization stage we intend to pick up a number of possibly feasible solutions that serve as starting solutions for the next stage. We use FDP to ensure that the initial solutions are feasible with a specified confidence level. In the Searching & Comparing stage we employ the Stochastically Constrained Genetic Algorithm (SCGA) in conjunction with FDP to explore the feasible region for promising solutions, where each operator of the conventional GA is adapted for efficient use in a stochastic environment. The operators used to make iterations are grouping and mating, where a subset of current solutions (i.e., parents) are selected based on their performance, and crossover, where the properties of each pair of solutions are integrated into one, as well as mutation, where each offspring is modified based on some rule. In the final stage we invoke the clean-up procedure proposed in Boesel et al. (2003) to select the best (with respect to the stochastic objective) from the set of feasible solutions identified by SCGA. We use their clean-up procedure instead of the typical Rinott’s (1978) procedure, because the number of observations already taken from each solution is usually unequal. Notice that a heuristic search procedure like GA may revisit candidate solutions, and thus take different numbers of observations from them. We usePtto denote the set of current population at the tth generation, and useFt(orUt) to denote the feasible set (or infeasible set) at the tth generation. A flowchart of GCDA is given in Fig. 1. A high-level description of the global convergent constrained DOvS algorithm (denoted as GCDA) is described as follows. In the remainder of this section we will fill in specific pieces of the algorithm.Global Convergent Constrained DOvS AlgorithmInitialization: Randomly pick M solutions from the solution space and taken0observations associated with(Gj(xi),Hj(xi))for each solution. We then implement FDP to identify feasible solutions with a specified confidence level. If the set of feasible solutions is empty, we return to the beginning of this stage. If the number of feasible solutions is less than M, we generate new solutions from the current feasible ones by increasing any of their coordinate components (inside the solution space) until M solutions are collected. Taken0observations associated with(Gj(xi),Hj(xi))for those newly generated solutions. Set the generation countert=0, and let the set of M solutions be the initial populationP0and also the feasible setF0. Move the infeasible solutions identified by FDP in this stage to the infeasible setU0.Searching & Comparing (SCGA):Take the following steps,Step 1. Grouping: Apply the Linear Ranking Scheme and Grouping Procedure (based on the aggregated sample means with respect to the objective performanceGj(xi)) to assign selection probabilities to each solution amongPt.Step 2. Mating: Implement Stochastic Universal Sampling to choose M parents randomly, with replacement, based on the previously assigned selection probabilities for each solution amongPt.Step 3. Crossover: Apply Max–min Arithmetical Crossover to each pair of parents to generate four offspring. For each group of offspring, use the Set Check Procedure to determine their feasibility, and pick up the best two solutions according to the aggregated sample means of the objective performance measure.Step 4. Mutation: For each offspring, determine if it needs to undergo mutation according to the specified mutation probability p. Choose one coordinate of that solution with a probability1/d. Use the Golden Section Method in combination with the Set Check Procedure to find the lower boundary of the feasible region (along the direction of the chosen coordinate). Then apply Uniform Mutation to change its value.Step 5. Stopping rule: Terminate SCGA and go to Selecting stage if the budget or improvement conditions are satisfied. Otherwise, lett=t+1and go to Step 1.Selecting (Clean Up): Apply Clean-up Procedure to all solutions in the feasible setFtto identify the best one.IfAssumptions 1 and 2hold, then SCGA attains global convergence as the number of generations goes to infinity. Furthermore, in the Selecting stage GCDA guarantees to select the best solution among the final feasible setFtwith a specified confidence level.The proof of Theorem 1 is provided in Appendix A.In the Initialization stage, we first randomly choose M solutions from the solution space, although some of these may not be feasible. Notice that we require the initial solutions to be feasible with some confidence level in order to deliver convergence to the global optimum as the number of simulated observations goes to infinity (see Appendix A.3). Therefore, we apply FDP to identify feasible solutions that satisfy the single specified stochastic constraint among these M solutions. If some solutions are eliminated, we then generate new ones by increasing any coordinate value of the surviving solutions (uniformly inside the solution space), until the number of initial solutions is added up to M. More specifically, forxi(current feasible one) andxj(the new one), we will havexiℓ<xjℓfor one coordinate component andxiℓ=xjℓfor the others. Based on Assumption 2, we can conjecture that these newly generated solutions are also feasible. On the other hand, if FDP returns an empty set, then we go back to the beginning of this stage (i.e., generate M new solutions from the solution space). The same process is then repeated with the new set of solutions.In the second stage we employ SCGA to search the entire solution space for promising solutions. The basic elements of SCGA include the fitness function, grouping procedure, parent selection, crossover operators, and mutation operators. “Fitness” is a term used in the GA literature to represent the value of the performance measure for a solution. Note that the fitness function we mainly use in the Grouping and Crossover steps corresponds to the aggregated sample means with respect to the objective performance. In general, the solutions with better fitness values are assigned a better chance of being selected for mating. We use FDP from time to time in the algorithm to check the feasibility of candidate solutions, and this is embedded into the set check procedure (described in Section 3.2.3). When using FDP in the Crossover and Mutation steps, the fitness function depends on the evaluation of the constraint performance measure. Each step of the algorithm is described in detail as follows.The basic principle of GA is that solutions with better performance should have a higher probability of surviving and being selected to generate new solutions through the crossover operator (i.e., they pass on their characteristics to future populations). Since our algorithm operates in a noisy environment, it is more difficult to declare which solutions are truly better because of the sampling variance. We thus use the adapted linear ranking scheme (see Baker (1987)) together with a statistical grouping procedure (see Boesel (1999)) to assign selection probabilities to each solution amongPt. In the linear ranking scheme, each solution’s survival probability depends only on the rank, not the magnitude, of the solution’s fitness value (estimated performance) within the current population. The implementation of a rank-based scheme means that the selection probabilities do not change too drastically as a result of incorrect ranking (due to the presence of stochastic variation). We also adopt the grouping approach described in Boesel (1999), where the sample variance information is considered and all members of each group are assigned their group’s average selection probability. This approach is proven to minimize the difference between the selection probabilities assigned in deterministic and stochastic environments. In this way, the selection scheme is adapted for efficient use in a stochastic setting.We use stochastic universal sampling (SUS) to determine which individuals are chosen for mating (reproduction). The SUS scheme constructs a roulette wheel where the area for each solution in the current population is proportional to its selection probability (which is computed in the Grouping step). A single spin of the roulette wheel will simultaneously select all the M individuals for reproduction. More specifically, after the spin the pointer will rest on the area of some individual which is then selected as the first parent. We then select other individuals as parents by advancing the pointer at a constant spacing of1/M(until we choose a total of M parents). By contrast, the ordinary roulette wheel scheme (RWS) only selects one individual for one spin. The selected individuals are placed back into the pool, and can be selected again. The process is repeated iteratively until M individuals are obtained. Therefore, the SUS scheme can achieve a certain spread because it ensures that individuals are picked from the full range of descending fitness values, whereas the RWS scheme might only pick from the top end of that ranking. In summary, the use of SUS scheme helps to prevent premature convergence by holding back outstanding individuals from taking over the population within a few generations (see Baker (1987)).In this step we need to implement FDP repeatedly to determine the feasibility of candidate solutions if necessary. To avoid incurring excessive sampling cost because of using FDP too often, we develop the set check procedure, which is valid because of Assumption 2 (described as follows). Since a GA tends to generate the same solution more than once, the set check procedure first checks the database to see if the solution has been visited previously, or if the feasibility of that solution can be determined directly based on Assumption 2. If not, we then implement FDP, and need to take observations from the solution. Suppose we have a new candidate solutionxk=(xk1,xk2,…,xkd)∈Zd, and a feasible setFtand an infeasible setUt, which consist of the solutions that have been evaluated and determined to be feasible or infeasible in previous iterations. We implement the set check procedure with regard toxkaccording to the following steps:•For anyj∈Ft, ifxkℓ⩾xjℓ,∀ℓ∈{1,2,…,d}, then we putxkintoFt.For anyj∈Ut, ifxkℓ⩽xjℓ,∀ℓ∈{1,2,…,d}, then we putxkintoUt.Otherwise we implement FDP to check the feasibility ofxk.In a typical GA we usually apply a crossover operator to inherit some characteristics of any two parents to generate the offspring. In SCGA we use the max–min arithmetical crossover, which makes it easier to check the feasibility of each offspring (as will be explained later). Given two parentsxi=(xi1,xi2,…,xid)andxj=(xj1,xj2,…,xjd), the arithmetical crossover creates two offspring,x1′andx2′, by generating a random numberβfromU(0,1)distribution, and lettingx1′=βxi+(1-β)xjandx2′=(1-β)xi+βxj. The other two offspring we generate (for each pair of parents) arex3′={x3ℓ′,ℓ=1,2,…,d}withx3ℓ′=max(xiℓ,xjℓ)andx4′={x4ℓ′,ℓ=1,2,…,d}withx4ℓ′=min(xiℓ,xjℓ). Based on the condition that the solutions inPtare declared to be feasible and the establishment of Assumption 2, we can directly conjecture thatx3′is feasible with some confidence (without employing FDP). On the other hand, we need to use the set check procedure to check the feasibility ofx4′, and the results may help us quickly determine the feasibility ofx1′andx2′. The detailed steps are summarized as follows:•Ifx4′is determined to be feasible, we can then declare thatx1′andx2′are also feasible with some confidence based on Assumption 2. We then put the solutionsx1′,x2′,x3′, andx4′into the feasible setFt, and taken0observations (with respect to the objective performance) from these solutions. We choose the two best solutions based on the aggregated sample average ofGj(xi), and proceed to the next step.Ifx4′is determined to be infeasible, we then putx4′into the infeasible setUtand implement the set check procedure tox1′andx2′:–Ifx1′andx2′are both determined to be feasible, we put them into the feasible setFt, and taken0observations (with respect to the objective performance) fromx1′,x2′andx3′. We choose the two best solutions from them based on the aggregated sample average ofGj(xi), and proceed to the next step.Ifx1′is determined to be feasible (or infeasible) andx2′is identified to be infeasible (or feasible), we put them intoFtorUtand choose the two feasible solutionsx3′andx1′(orx2′).Ifx1′andx2′are both declared to be infeasible, we put them intoUt, select any one of the parents (xiorxj) along withx3′and proceed to the next step.In summary, we apply the max–min arithmetical crossover to each pair of parents to produce four offspring (i.e., the linear combinations, the maximum and the minimum values of all coordinates of any two parents). We then pick two solutions (corresponding to each pair of parents) that are declared to be feasible and have better estimated objective performance. This mechanism is designed to ensure that the solutions in the poolPtare feasible (at least with a statistical guarantee), which is required to attain global convergence of SCGA. We use the max–min arithmetical crossover in order to explore as many different candidate solutions as possible, and therefore making the searching process of our algorithm more likely to escape from local optimums.The purpose of using mutation operators is to randomly modify the individual contents to promote the diversity of the current population. To achieve this, we implement uniform mutation, which is one of the commonly used mutation operators in real coded GAs (see Michalewicz (1996)). For each offspring, we determine if mutation needs to be performed based on the specified mutation probability p. We then choose one coordinate of that solution with a probability1/d, and perturb its value randomly along the feasible region. For instance, if the second coordinate of the offspringxiis chosen, we then have a new mutated solutionxi′, as follows:xi=(xi1,xi2,…,xid)→xi′=(xi1,xi2′,…,xid).More specifically, we choose a new value of that coordinate randomly along the feasible region. However, the DOvS problem we want to solve contains a stochastic constraint, which means it is difficult (or impossible) to directly determine the boundary of the feasible region with respect to the chosen coordinate of the offspring. We thus propose using the Golden Section method (see Bazaraa, Sherali, & Shetty (2006)) in combination with the set check procedure to efficiently find the lower boundary of the feasible region (along the direction of the chosen coordinate). The Golden Section method was originally developed as a sequential search technique for finding the minimum or maximum of a unimodal function by successively narrowing the range of parameter values. Notice that the assumption of unimodal function is quite analogous to Assumption 2 (see Section 2). The basic idea is to regard the boundary of the feasible region of our problem as the optimal point of a unimodal function (assumed by the routine Golden Section method). One advantage of using Golden Section method is that we do not need to determine the number of function evaluations in advance, prior to starting the search (see Bazaraa et al. (2006)). We give an illustrative example as follows. Suppose that the range of parameter values of the chosen coordinate is between 0 and 100. We first use the golden ratioτ=0.618to find two pointsx1=⌊100×0.382⌋=38andx2=⌊100×0.618⌋=61. We then use the set check procedure to check the feasibility ofx2: if it is feasible, we keep searching inside the interval to the left ofx2(since the interval to the right ofx2are feasible based on Assumption 2); if it is infeasible, we instead search inside the interval to the right ofx2. In this case we assume thatx2is feasible, and therefore evaluate a new pointx3=⌊61×0.382⌋=23. The pointx3is determined to be infeasible, and thus we go to the right-hand side ofx3and evaluatex4=23+⌊(61-23)×0.618⌋=46. This point is found to be feasible, and we then evaluatex5=23+⌊(46-23)×0.618⌋=37. Finally, we find thatx5is infeasible andx1is feasible, and therefore the feasible region we adopt in this case is [38,100].At the end of the Searching & Comparing stage we have a set of solutions (Ft) that have been declared to be feasible, with a specified statistical guarantee, either by employing FDP or by comparing with those visited solutions already determined to be feasible based on Assumption 2. In this final stage those solutions are passed to the clean-up procedure (see Boesel et al. (2003)), which provides a statistical confidence as to which of these is the best (in terms of the stochastic objective function). The clean-up procedure was originally developed to help identify the best solution among those discovered during a searching process. It is designed to quickly filter out obviously inferior solutions with a screening procedure (using only the samples already collected by the searching process), and then distinguish the best solution by carrying out additional simulation effort from the remaining solutions. Our DOvS algorithm thus guarantees that it returns the best solution with a specified confidence level from all those visited by the searching process. Of course, if the searching process visits all the feasible solutions, then the statistical validity applies to the entire solution space. Notice that the screening procedure takes into account the relative performance of the candidate solutions, and this is a favorable feature, because there is very likely to be a large number of potential solutions with widely differing performance at the end of the stochastic search.In this section we propose a heuristic algorithm that can take advantage of the essential mechanics of GA, such as the adaptive constraint-handling techniques (see Coello (2002)) and elitism. The implementation of an elite population means that superior solutions are always conserved through evolutions, which is often used in routine GAs to speed up convergence. However, a consequence of using an elite population is that it invalidates the global convergence statement of our GA-based DOvS algorithm. The algorithm’s finite-time performance may be improved, because the algorithm design is more flexible without adhering to the convergence claim. Another desirable feature of this algorithm is that we do not use FDP so often (compared to GCDA); in this case FDP is only applied to the elite population once in the final step. Notice that we do not need to ensure that each solution in the current population is feasible with some specified guarantee, which is however required for GCDA to attain global convergence. We thus expect to use less simulation effort in determining the feasibility, which means more solutions can be evaluated under a given amount of sampling budget. Therefore, this approach is more likely to uncover promising solutions in the searching process (in terms of the stochastic objective). In the Initialization stage we randomly pick up M solutions from the solution space and note that we do not use FDP here. In the Searching & Comparing stage we employ the Heuristic Constrained Genetic Algorithm (HCGA) to explore the feasible region for promising solutions. A critical difference between SCGA and HCGA is the fitness assignment. For HCGA, we treat the objective and constraint separately when computing the fitness value, which is one of the commonly used constraint-handling techniques. We also introduce a penalty factor to eliminate the selection bias (see Section 4.1). In the final stage we invoke the clean-up procedure proposed in Boesel et al. (2003) to select the best (with respect to the stochastic objective) among the elite population identified by HCGA. The Selecting stage is basically the same as that in SCGA, and the only difference is that the clean-up procedure is applied to the possibly feasible solutions among the elite population, instead of the feasible set. A flowchart of HCDA is given in Fig. 2. A high-level description of the heuristic constrained DOvS algorithm (denoted as HCDA) is given as follows.Heuristic Constrained DOvS AlgorithmInitialization: Randomly pick up M solutions from the solution space and taken0observations associated with(Gj(xi),Hj(xi))for each solution. Set the generation countert=0, the elite populationE0=∅, and let the set of M solutions be the initial populationP0.Searching & Comparing (HCGA): Take the following steps,Step 1. Compute the proportion of feasible solutions (denoted aspf) amongPtbased on sample means associated with the stochastic constraint.Step 2. Grouping: For each solutionxiamongPt, apply the Linear Ranking Scheme and Grouping Procedure to assign selection probabilities for the stochastic objective and constraint, respectively (denoted assianddi).Step 3. Fitness assignment: Usesi,di, andpf(treated as a penalty factor) to compute the fitness valuef(xi)for each solution amongPt.Step 4. Mating: Implement Stochastic Universal Sampling to choose M parents randomly, with replacement, based on the fitness value for each solution amongPt.Step 5. Crossover: Apply the traditional Arithmetical Crossover to each pair of parents to generate two offspring.Step 6. Mutation: For each offspring, apply Non-uniform Mutation to perturb its value, with mutation carried out according to the specified mutation probability p.Step 7. Evaluation: For each newly generated offspring, obtainn0observations associated with the stochastic objective and constraint.Step 8. Elite population: Based on the aggregated sample means, choose the best solution in terms of the objective performance, among the solutions satisfying the stochastic constraint. If all solutions violate the constraint, choose the one with the smallest amount of violation. In either case, obtain additionalNAobservations for the chosen solution, and move it to the elite populationEt.Step 9. Generation of the next population: Combine the current offspring and parents, and choose the best M solutions according to the rule described in the above step.Step 10. Stopping rule: Terminate HCGA and go to Step 11 if the budget or improvement conditions are satisfied. Otherwise, lett=t+1, and go to Step 1.Step 11. Feasibility check: Apply FDP to all solutions in the elite populationEt. If all solutions are declared to be infeasible, then return to the Initialization stage. Otherwise, go to the Selecting stage.Selecting (Clean Up): Apply the Clean-up procedure to the potential solutions (that have been declared to be feasible by FDP) amongEtto identify the best one.IfAssumption 1holds, then in the Selecting stage HCDA guarantees to select the best solution among the elite populationEtwith a specified confidence level.The proof here is straightforward.To obtain the selection probabilities with respect to the stochastic objective (denoted assi), we apply the linear ranking scheme and statistical grouping procedure to the objective performance values of each solution amongPt. Loosely speaking, the solutions with better objective performance are assigned a higher probability of being selected for mating. In addition, the selection probabilities with respect to the stochastic constraint (denoted asdi) are determined by evaluating each solution’s amount of violation (compared to the constraint threshold). Our approach is to treat constraint satisfaction and objective performance separately when computing the fitness value, which is one strategy used to handle constrained problems in the GA literature (see Ray, Kang, & Chye (2000)). Further, we introduce a so-called penalty factorpfto balance the dominance betweensianddifor feasible and infeasible solutions. This can be done by discounting the fitness value of an infeasible individual by the penalty factorpfwhich represents the proportion of feasible solutions in the current populationPt. The feasibility of each solution is determined at this point by comparing the sample average of the constraint performance with the threshold. We then specify the fitness value as follows:•Ifxiis feasible, we setf(xi)=(si+di)2.Ifxiis infeasible, we setf(xi)=(si×pf)+di2.When all the solutions inPtare determined to be feasible (based on the aggregated sample average), we then setpf=1and the value ofdiis the same across all solutions. This implies that the fitness valuef(xi)is only dependent on the objective performance (i.e.,si), and therefore the solutions having better objective performance will be assigned a higher probability of surviving. On the other hand, when the current populationPtcontains feasible and infeasible solutions and the parameterpfis not used, the fitness value of infeasible solutions might be significantly better than that of feasible ones (if it has a very competitive objective performance). To alleviate this selection bias, the penalty factorpfis used to decrease the value off(xi)for infeasible solutions. As a result, if the feasibility proportionpfis high, its impact against the fitness value of infeasible solutions is not obvious. In contrast, if the feasibility proportionpfis low, the selection probability of infeasible solutions will be significantly decreased, which implicitly makes feasible solutions easier to select. This approach appears to be similar to the idea of stochastic ranking proposed in Runarsson and Yao (2000).In the Crossover step we apply the typical arithmetical crossover to each pair of parents to produce two offspring. Michalewicz (1996) suggested that the arithmetical operator works the best for real coded GAs. Given two parentsxi=(xi1,xi2,…,xid)andxj=(xj1,xj2,…,xjd), the arithmetical crossover produces two offspringx1′andx2′by generating a random numberβfrom theU(0,1)distribution, and lettingx1′=βxi+(1-β)xjandx2′=(1-β)xi+βxj. This can be regarded as a linear combination of two vectors. In the Mutation step we apply non-uniform mutation to perturb the value of each offspring, given that it needs to perform mutation according to the specified mutation probability p. By contrast, for GCDA we need to use uniform mutation to preserve the guarantee of global convergence. The mutated element ofxi, sayxiℓ′, is given by:xiℓ′=xiℓ+Δ(t,uiℓ-xiℓ),with probability0.5,xiℓ-Δ(t,xiℓ-liℓ),with probability0.5,where t is the generation counter,uiℓandliℓare the upper and lower bounds ofxiℓ, respectively, andΔ(t,y)=y1-β1-tTb. Hereβis a uniformly distributed random number in the interval[0,1],Tis the maximum number of generations allowed, and b is a parameter determining the degree of non-uniformity. In the initial generations the non-uniform mutation operator tends to explore the solution space uniformly, and in the later generations it tends to search the space locally (Michalewicz, 1996).In the Evaluation step, we taken0observations for each newly generated offspring. Based on the accumulated sample means, we choose the best solution in terms of the objective performance (if this solution satisfies the stochastic constraint). If all solutions violate the constraint, we then choose the one with the smallest amount of violation. In either case, we move the chosen solution to the elite populationEtand also obtain its additionalNAobservations. Due to the effect of stochastic variation, the solution with the best sample average may not coincide with the true best solution encountered during the searching process. Therefore we propose using the elite population to preserve all of the potentially promising solutions and carrying out additional simulation replications to obtain more precise performance estimators.In this section, an extensive empirical evaluation is performed to compare the performance of the following algorithms for solving the DOvS problem with a single stochastic constraint:1.The constrained DOvS algorithms proposed in Sections 3 and 4 are performed, including the Global Convergent Constrained DOvS Algorithm (GCDA) and Heuristic Constrained DOvS Algorithm (HCDA).We also perform a modified version of GCDA, where FDP is replaced by a fixed-sample-mean comparison, and it is called Fixed Sample Constrained Algorithm (FSCA). Notice that the algorithm maintains the advantages of using a genetic algorithm, but cannot guarantee the feasibility and global convergence of the selected solution.Lee et al. (2012) proposed a new optimal computing budget allocation (OCBA) approach for constrained R&S problems. The goal is to maximize the posterior probability of correctly selecting the best feasible solution within a given simulation budget. Notice that the OCBA approaches do not provide a guaranteed probability of correct selection, but they work well for average cases.We consider FSCA in the empirical study, because it can be viewed as a naive approach where the feasibility of each solution is determined via a sample average, without using any statistically valid procedure. The sample size is fixed and chosen arbitrarily by the analyst before the experiment, and is set as thirty in the following experiment. Note that the performance difference between GCDA and FSCA represents the effect of using FDP (or not) in the constrained DOvS algorithm. For GCDA, HCDA and FSCA, we can terminate the Searching & Comparing stage at any point at which acceptable performance has been achieved, although GCDA is globally convergent as the simulation effort goes to infinity. That is, the Searching & Comparing stage can be terminated when the improvement in solution quality obtained upon a specified number of consecutive generations is insignificant. This represents the case when the chosen solution dominates the others within these iterations. Of course, we can also terminate the algorithm when the specified sampling budget (in terms of the number of simulation replications) is exhausted at the current generation. As for the OCBA algorithm, in each trial we first randomly select a fixed number of solutions without replacement (within the solution space), and treat them as the set of available solutions at the beginning of the experiment. We then try to select the best solution by taking into consideration of both optimality and feasibility among the randomly chosen solutions. Notice that the OCBA algorithm finds good enough solutions with high probability by randomly sampling and evaluating enough of them. By contrast, GCDA and HCDA are guaranteed to choose the best from a small set of promising solutions identified by the Searching & Comparing stage.To obtain good control over the factors that can affect the performance of the algorithms, we select two known response-surface functions to which we add normally distributed noise. The objective function of the first test problem is the multimodal function used in Xu et al. (2010), and is referred to asg1(xi1,xi2), where the ith solution is denoted byxi=(xi1,xi2). We also impose a constraint functionh1(xi1,xi2)to define the feasibility of each solution. The first test problem is then formulated as follows:ming1(xi1,xi2)=sin6(0.05πxi1)22xi1-10802+sin6(0.05πxi2)22xi2-10802s.t.h1(xi1,xi2)=ln(xi1+1)+ln(xi2+1)⩾Qwhere we set0⩽xiℓ⩽100,xiℓ∈Z+, forℓ=1,2. Therefore, we have a total of1012=10,201candidate solutions. When only considering the objective function, this problem contains twenty-five local optimums and a global one, and it represents a response surface where most solutions differ widely in performance. To examine the performance of the algorithms when encountering different proportions of feasible solutions (in the solution space), the value of Q is varied withQ=6.6,7.4, and 8.5 in the experiments. For the case ofQ=6.6, we have 7628 feasible solutions (accounting for 75% of the solution space), which is considered as a scenario with a high percentage of feasible solutions in the whole space. The global optimum is located at(30,30), with the objective value −1.83. For the case ofQ=7.4, we have 5599 feasible solutions (accounting for 55% of the solution space), which is considered as a scenario with a medium level percentage of feasible solutions in the whole space. The global optimum is located at(50,50), with the objective value −1.41. The worst case we consider in the experiment corresponds to the setting ofQ=8.5, where we have 1897 feasible solutions (accounting for only 18% of the solution space). The global optimum is located at(70,70), with the objective value −0.92. We add normally distributed noise with zero mean to the objective and constraint functions. The standard deviation of the noise is set atσ=0.3or 0.6 in the experiment, representing a low or high level of output variability, respectively.The second known test function of the response surface is the singular function extended from Hong (2004). We also add a stochastic constraint to make it a constrained DOvS problem. The ith solution of the problem is denoted byxi=(xi1,xi2,xi3,xi4). The constrained problem is formulated as follows:ming2(xi1,xi2,xi3,xi4)=(xi1+10xi2)2+5(xi3-xi4)2+(xi2-2xi3)4+10(xi1-xi4)4+1s.t.h2(xi1,xi2,xi3,xi4)=0.2xi1+0.3xi2+0.4xi3+0.5xi4⩾Qwhere we set-100⩽xiℓ⩽100,xiℓ∈Z, forℓ=1,2,3,4. Therefore, we have a total of2014≈1.6×109candidate solutions. When only considering the objective function, this problem has three local optimums and a global one, and these solutions are close to each other. This implies that we have a small number of tightly clustered local optimums in the solution space. Similar to the previous test problem, we use three different values of Q in the experiments. For the case ofQ=-25, we have1.16×109feasible solutions (accounting for72%of the solution space), which is considered as a scenario with a high percentage of feasible solutions in the whole space. The global optimum is located at(0,0,0,0), with the objective value 1. For the case ofQ=-5, we have8.9×108feasible solutions (accounting for55%of the solution space), which is considered as a scenario with a medium level percentage of feasible solutions in the whole space. The global optimum is located at(0,0,0,0), with the objective value 1. The last scenario we consider in the experiment corresponds to the setting ofQ=25, where we have around4.7×108feasible solutions (accounting for only29%of the solution space). The global optimum is located at(24,0,18,26), with the objective value 2233. We add normally distributed noise with zero mean to the functionsg2andh2. The standard deviation of the noise is set asσ=10or 40 in the experiment, representing a low or high level of output variability, respectively.Notice that in practice the simulated system is often so complex that the objective and constraint functions are not analytically tractable, and thus are unknown. However, the expected function structure is not required for the application (or validity) of our algorithms. For each problem configuration, 100 macro-replications (complete repetitions) of each algorithm are performed. In all experiments, the nominal probability of correct selection is set at 1−α=0.95. To compare the performance of the algorithms we record the Average Performance (i.e., expected value of the objective response function) of the final selected solution (AP), the Average Number of Visited solutions (ANV), the Average Number of Samples (ANS) required by each algorithm (without dividing by the number of visited solutions). We also record the estimated Probability of Correct Decision (PCD) which stands for the probability of correctly identifying feasible solutions (for 100 macro-replications). The feasibility of the final selected solution in each trial can be determined easily by comparing the expected constraint function with the threshold value. To simplify the experimental presentation, we round the values of AP to the nearest thousandth, the values of PCD to the nearest hundredth, and the values of ANV and ANS to the nearest whole number. The detailed parameter settings for GCDA are provided in Table 1. The parameter settings for HCDA are essentially the same as those of GCDA, except that we setα1=α2=1-1-α. Notice that after using FDP to pick up feasible solutions in the elite populationEt, we restart from scratch the simulation of each survivor in the Selecting stage. This process can be regarded as one variant of the Restarting Procedure, which is defined in Section 4.2 of Andradóttir and Kim (2010). Therefore we can use the same derivation as in Theorem 3 of Andradóttir and Kim (2010), and conclude that the probability of correct selection is greater than(1-α1)×(1-α2). The additional control parameters for HCDA are the non-uniformity parameterb=1.5, maximum number of generations allowedT=50(for the Mutation step), and the additional number of observations (NA=20) taken for solutions within the elite population. As for the OCBA algorithm, the ANV value shown in the experimental results represents the number of candidate solutions randomly chosen within the solution space. Each solution is initially sampled withn0=10replications and additionalΔreplications are allocated incrementally in each iteration until the total sampling budget is exhausted. The value ofΔis set around one-third of the ANV value. We present experimental results of OCBA whose ANS value is close to the specified sampling budget.

@&#CONCLUSIONS@&#
In this paper we present two GA-based efficient algorithms that are specifically designed for the DOvS problem with a single stochastic constraint. We use GA to guide the searching process in a noisy environment, because it seems quite robust to statistical errors introduced in the performance estimates (see Boesel (1999)). The first proposed algorithm, GCDA, guarantees global convergence as the simulation effort goes to infinity, and at the same time guarantees to select the best among all visited solutions that have been declared to be feasible (upon termination of the algorithm) with a specified confidence level (see Remark A.2). The second algorithm HCDA pursues finite-time empirical efficiency and focuses on employing adaptive constraint-handling techniques and the mechanism of elite population. If statistical validity is a concern of the analyst, we recommend using GCDA. If the analyst can tolerate the lack of statistical validity and the problem dimension d is not small (or the level of stochastic variation is low), then HCDA is recommended.There are several possible directions to extend the methodology developed in the paper. For instance, we could exploit variance reduction techniques to improve the algorithm’s efficiency (Tsai & Kuo, 2012). Another possible extension is to use other metaheuristics to guide the searching process (e.g., simulated annealing). The other possible extension is to consider the DOvS problem with multiple stochastic constraints (Batur & Kim, 2010). Notice that the conservatism of the feasibility check procedure becomes more serious as the number of solutions or constraints increases. This implies that our GA-based algorithms may lead to excessive sampling cost, especially for the case of GCDA where FDP is repeatedly employed in the searching process. We may also need to determine appropriate weights regarding the relative importance of each constraint when evaluating each solution’s amount of violation (compared to the constraint threshold). The other research topic worthy investigating is to incorporate OCBA into GA-based algorithms for constrained simulation optimization. For instance, we could stop an OCBA call in each iteration when an approximate probability of good selection or the expected opportunity cost reaches a pre-specified level (see Branke et al. (2007)). We also could employ the subset selection procedure of Chen, Fu, and Lee (2008) to choose an elite population for each generation of GA. Some extension of these existing procedures is required to handle stochastic constraints. It would be better to exploit accumulated simulation observations since GA may revisit candidate solutions.In this subsection we describe the Feasibility Determination Procedure (FDP) proposed by Andradóttir and Kim (2010) that will be used repeatedly in our algorithms. FDP checks the feasibility of each solution among a finite set with respect to a stochastic constraint. However, for stochastic problems it is not always possible to identify the solutions that satisfy the stochastic constraint exactly with a finite number of simulated observations. Andradóttir and Kim (2010) thus introduced a tolerance level, which allows choosing the solutions whose expected performance corresponding to the stochastic constraint is within a so-called acceptable region, which is looser than the defined true feasible region. The following description and notations are based on Andradóttir and Kim (2010). First, we need to specify the lower and upper bound values for the constant Q, namely(Q-,Q+), such thatQ-⩽Q⩽Q+. Then, three regions for solutionxiare defined:•h(xi)⩾Q+: This is called the desirable region. Any solution in this region is feasible and accepted.Q-<h(xi)<Q+: This is called the acceptable region. Any solution in this region is near feasible and can be accepted.h(xi)⩽Q-: This is called the unacceptable region. Any solution in this region is infeasible and will be rejected.For convenience, we also define the following notations:k≡the number of solutions to be examined with regard to their feasibility;n0≡initial sample size for each solution(n0⩾2);r≡the current observation counter(r⩾n0);Si2≡sample variance ofH1(xi),…,Hn0(xi)fori=1,2,…,k;R(r;v,w,z)≡max{0,wz2v-vr2}, for anyv,w,z∈R,v≠0;g(η)≡12×(1+2η)-(n0-1)/2.Feasibility Determination Procedure (FDP)Setup: Select initial sample sizen0⩾2and confidence level0<1-α1<1. For the constraint under consideration, choose q and∊such thatq=(Q-+Q+)/2and∊=(Q+-Q-)/2. Computeη1>0, a solution tog(η1)=1-(1-α1)1/k.Initialization: LetL={1,2,…,k}andF=∅be the set of solutions whose feasibility is not determined yet and the set of feasible solutions, respectively. Leth12=2η1(n0-1). Obtainn0observationsHj(xi),j=1,2,…,n0, from each solution i. ComputeSi2, set the observation counterr=n0and go to Feasibility Check.Feasibility Check: For each solutioni∈L, if∑j=1r(Hj(xi)-q)⩾+R(r;∊,h12,Si2), then move i from L to F; else if∑j=1r(Hj(xi)-q)⩽-R(r;∊,h12,Si2), then eliminate i from L.Stopping Rule: If|L|=0, then return F as a set of feasible solutions. Otherwise, take one additional observationHr+1(xi)from each solutioni∈L, setr=r+1, and go toFeasibility Check.Remark A.1Andradóttir and Kim (2010) proved that under Assumption 1, FDP will return a set of solutions that includes all desirable and some acceptable solutions with a specified confidence level1-α1.For convenience we define the following notations:k≡the number of remaining solutions from which we want to select the best;ni≡the number of replications already carried out on solution i (in the searching process);Si2≡sample variance ofG1(xi),…,Gni(xi)fori=1,2,…,k;X‾i(1)≡sample mean ofG1(xi),…,Gni(xi)fori=1,2,…,k.Clean-Up ProcedureStep 1. Set the overall desired confidence level1-α2such that1/k<1-α2<1, andαs=α2/2for the screening procedure,αI=α2/2for the selection procedure, and indifference-zone parameterδ>0.Step 2. GivenGj(xi),i=1,2,…,k;j=1,2,…,ni, computeX‾i(1)and sample varianceSi2. LetWij=ti2Si2ni+tj2Sj2nj1/2whereti=t(1-αs)1/(k-1),ni-1.Step 3. SetJ={i:1⩽i⩽kandX¯i(1)⩽X¯j(1)+Wij,∀i≠j}and return J as the subset of retained solutions.Step 4. Setnmin=mini{ni}andh=h(2,(1-αI)1/(k-1),nmin)which is the extended Rinott’s (1978) constant. Determine the total required sample size for each solutioni∈J:Ni=maxni,hSiδ2.Step 5. TakeNi-niadditional replications from each solution i. Select the best solution i with the smallest overall sample meanX‾i(2)=∑j=1NiGj(xi)/Ni.Remark A.2Recall thatg(xi)denotes the expected performance measure of objective function of solution i, fori=1,2,…,k. Suppose that, unknown to us,g(xk)⩽g(xk-1)⩽⋯⩽g(x1)and that a smaller mean is better. Boesel et al. (2003) proved that under the assumption that the simulation output dataGj(xi)are normally distributed (which is implied by Assumption 1), the clean-up procedure satisfies the following probability statement:Pr{selectxk|g(xk-1)-g(xk)⩾δ}≥1-α2.We use a similar argument as in Appendix A.1 of Xu et al. (2010). First, we introduce the notations that we will use in this proof. LetΩdenote the feasible region that is defined by the stochastic constraint, and letα1denote the allowable error of feasibility check in FDP. We also letSkdenote the set of solutions sampled in the kth generation. To attain the global convergence of SCGA as the number of generations goes to infinity, we need to satisfy the following requirements:1.In the Initialization stage, the initial M solutions are feasible (at least with the specified confidence level).In the Mating step of the kth generation, any solution sampled in the(k-1)th generation (i.e., populationPk) has a probability of at leastp1>0(based on the selection probabilities assigned in the Grouping step) of being selected for crossover.In the Crossover step, any parent solution has a probability of at leastp2>0of being generated as an offspring. Further, the generated offspring has a probability of at least1-α1>0of being determined as feasible (if it is truly feasible).In the Mutation step, any offspring solution generated in the above step has a probability of at leastp3>0(i.e., the specified mutation probability p) of being chosen for mutation.In the Mutation step, each coordinate of the chosen offspring might be selected with a probability of at leastp4>0(i.e.,1/d), because we implement uniform mutation here. Further, each feasible solution along the chosen coordinate may be selected with a probability of at leastp5>0, because we employ the Golden Section method in combination with the set check procedure here.Notice that the validity of the set check procedure depends on Assumption 2 and the condition that the solutions amongPkare feasible with some confidence. We also need Requirement 1 to specify that the solutions amongP0are feasible with some confidence. Of course, the analyst could remove Assumption 2 to make it a more general algorithm. The only consequence is that we need to implement FDP to nearly every sampled solution to check its feasibility, resulting in more computational time being needed. Further, we establish Assumption 1 to support the validity of FDP and the clean-up procedure. For any two solutionsxi,xj∈Ω, we can find a path denoted asxi,y1,y2,…,y|Ω|-1,xj, which means that any two consecutive solutions in the path differ at most by one coordinate. Ifxi∈Sk, then it could be selected for crossover in the(k+1)th generation with a probability of at leastp1. If it is selected as a parent solution, it is reproduced as an offspring with a probability of at leastp2. It is then chosen for mutation with a probability of at leastp3. We implement FDP to each sampled solution once, at most. Therefore, it is determined to be feasible (if it is truly feasible) with a probability of at least1-α1. Finally, if it is chosen for mutation, theny1will be visited in the(k+1)th generation with a probability of at leastp4p5. We thus have Pr{y1∈Sk+1|xi∈Sk}⩾∏i=15pi×(1-α1), and can obtainPr{xj∈Sk+|Ω||xi∈Sk}=Pr{y1∈Sk+1|xi∈Sk}Pr{y2∈Sk+2|y1∈Sk+1}×⋯×Pr{xj∈Sk+|Ω||y|Ω|-1∈Sk+|Ω|-1}≥∏i=15pi×(1-α1)|Ω|.Notice that the above result holds for any two solutions in the feasible region, which means that any solution could be sampled in any possible path of|Ω|generations (with a positive probability). This implies that all feasible solutions are visited infinitely often as the number of generations goes to infinity. The global convergence of SCGA is thus a direct consequence of the strong law of large numbers.