@&#MAIN-TITLE@&#
Continuous probabilistic model building genetic network programming using reinforcement learning

@&#HIGHLIGHTS@&#
This paper proposes a novel continuous estimation of distribution algorithm (EDA).A recent EDA named PMBGNP is extended from discrete domain to continuous domain.Reinforcement Learning (RL) is applied to construct the probabilistic model.Experiments on real mobile robot control show the superiority of the proposed algorithm.It bridges the gap between EDA and RL.

@&#KEYPHRASES@&#
Estimation of distribution algorithm,Probabilistic model building genetic network programming,Continuous optimization,Reinforcement learning,

@&#ABSTRACT@&#
Recently, a novel probabilistic model-building evolutionary algorithm (so called estimation of distribution algorithm, or EDA), named probabilistic model building genetic network programming (PMBGNP), has been proposed. PMBGNP uses graph structures for its individual representation, which shows higher expression ability than the classical EDAs. Hence, it extends EDAs to solve a range of problems, such as data mining and agent control. This paper is dedicated to propose a continuous version of PMBGNP for continuous optimization in agent control problems. Different from the other continuous EDAs, the proposed algorithm evolves the continuous variables by reinforcement learning (RL). We compare the performance with several state-of-the-art algorithms on a real mobile robot control problem. The results show that the proposed algorithm outperforms the others with statistically significant differences.

@&#INTRODUCTION@&#
Despite the selection operator based on the concept of “Survival-of-the-fittest”, classical Evolutionary Algorithms (EAs) generally evolve the population of candidate solutions by the random variation derived from biological evolution, such as crossover and mutation. However, numerous studies report that the results of EAs strongly rely on the configurations of the parameters associated with the stochastic genetic operators, such as crossover/mutation rate. For concrete problems, the parameter settings generally vary. Hence, the parameter tuning itself becomes an optimization problem. Meantime, the stochastic genetic operators sometimes may not identify and recombine the building blocks (BBs, defined by high-quality partial solutions) correctly and efficiently due to the implicit adaptation of the building block hypothesis (BBH) [1,2], which causes the problems of premature convergence and poor evolution ability. These reasons have motivated the proposal of a new class of EAs named estimation of distribution algorithm (EDA) [3], which has received much attention in recent years [4,5]. As the name implies, EDA focuses on estimating the probability distribution of the population using statistic/machine learning to construct a probabilistic model. Despite the selection operator which is also used to select the set of promising samples for the estimation of probability distribution, EDA replaces the crossover and mutation operators by sampling the model to generate new population. By explicitly identifying and recombining the BBs using probabilistic modeling, EDA has drawn its success to outperform the conventional EAs with fixed, problem-independent genetic operators in various optimization problems.Numerous EDAs has been proposed, where there are mainly three ways to classify the existing EDAs. (1) From the model complexity viewpoint, EDAs can be mainly classified into three groups [6]: univariate model, pairwise model and multivariate model, which identify the BBs of different orders. Univariate model assumes there is no interactions between the elements,11The elements refer to the variables/alleles in genetic algorithm (GA), or nodes in genetic programming (GP).hence constructing the probabilistic model by marginal probabilities to identify BBs of order one. Similarly, pairwise and multivariate models use more complex methods to model BBs of order two and more. One can easily observe that estimating the distribution is not an easy task and modeling more accurate model generally requires higher computational cost [4,7]. (2) From the perspective of individual structures, EDA can mainly be classified into two groups, which are probabilistic model building genetic algorithm (PMBGA) [8] and PMB genetic programming (PMBGP) [9]. PMBGA studies the probabilistic modeling using GA's bit-string individual structures. PMBGP explores EDA to tree structures which provide more complex ways to represent solutions for program evolution. (3) For different problem domains, EDA can be grouped into discrete EDAs and continuous EDAs, which solve the optimization problems of discrete domain [4,8] and continuous domain [10–13].A novel EDA, called probabilistic model building genetic network programming (PMBGNP), was recently proposed [14–16]. PMBGNP is inspired by the classical EDAs, however, a distinguished directed graph (network) structure [17–21] is used to represent its individual. Hence, it can be viewed as a graph EDA that extends conventional EDAs like bit-string structure based PMBGA and tree-structure based PMBGP. The fundamental points of PMBGNP are:1.PMBGNP allows higher expression ability by means of graph structures than conventional EDAs.Due to the unique features of its graph structures, PMBGNP explores the applicability of EDAs to wider range of problems, such as data mining [22,14,23] and the problems of controlling the agents’ behavior (agent control problems) [16,24–26].In the previous research, it has been demonstrated that PMBGNP can successfully outperform classical EAs with the above problems.However, PMBGNP is mainly designed for discrete optimization problems. In other words, it cannot deal with (or directly handle) continuous variables which are widely existed in many real-world control problems. To solve this problem, the simplest way is to employ discretization process to transfer the continuous variables into discrete ones, however, which will cause the loss of solution precision.This paper is dedicated to an extension of PMBGNP to continuous optimization in agent control problems. Different from most of the existing continuous EDAs developed by incremental learning [10], maximum likelihood estimation [11], histogram [27] or some other sorts of machine learning techniques [28–32], the proposed algorithm employs the techniques of reinforcement learning (RL) [33], such as actor critic (AC), as the mechanism to estimate the probability density functions (PDFs) of the continuous variables. Although most of the classical continuous EDAs formulate the PDFs of continuous variables by Gaussian distributionN(μ,σ2), the proposed algorithm applies AC to calculate the temporal-difference (TD) error to evaluate whether the selection (sampling) of continuous values is better or worse than expected. Based on the idea of trial-and-error, a scalar reinforcement signal which can decide whether the tendency to select the sampled continuous value should be strengthened or weakened is formulated by the gradient learning for the evolution of Gaussian distribution (μ and σ).Most importantly, as an extension of PMBGNP, the proposed algorithm mainly possesses the ability to solve the agent control problems, rather than the conventional continuous EDAs only for function optimization problems. Accordingly, the applicability of continuous EDAs is explored in certain degrees.In this paper, the proposed algorithm is applied to control the behavior of a real autonomous robot, Khepera robot [34,35], in which the robot's wheel speeds and sensor values are continuous variables. To evaluate the performance of this work, various classical algorithms are selected from the literature of standard EAs, EDA and RL for comparison.The rest of this paper is organized as follows. Section 2 briefly introduces the original framework of PMBGNP in the discrete domain. In Section 3, extending PMBGNP to continuous domain is explained in details. The experimental study is shown in Section 4. Finally we conclude this paper in Section 5.From the explicit viewpoint, PMBGNP distinguishes itself from the classical EDAs by using a unique directed graph (network) structure to represent its individual, depicted in Fig. 1. The directed graph structure is originally proposed in a newly graph-based EA named Genetic Network Programming (GNP) [17,18,36]. Three types of nodes are created to form the program (individual) of GNP:•Start node: it has no function and conditional branch.Judgment node: it has its own judgment function and multiple conditional branches.Processing node: it has its own processing function but no conditional branch.Each program is composed of one start node, multiple judgment and processing nodes. Start node only plays the role on deciding the first node to be executed. Judgment nodes imitate the “if-then” decision-making functions to deal with the specific inputs of the problems, such as the sensor values of the robot. Processing nodes enforce the action functions for task solving, such as determining the wheel speeds of the robot. By separating judgment and processing functions, the distinguished directed graph can deal with various combinations of judgments and processing to efficiently evolve the compact programs by only selecting the necessary judgments and processing. Such separation and selection by necessity can efficiently generate partially observable markov decision process (POMDP) [36] and ensure high generalization ability. The number of judgment and processing nodes is defined in advance and problem specific. As a result, the directed graph never causes the bloat problem of GP. Although a small number of nodes is prepared, such a structure can obtain good performance by well realizing the repetitive process by the frequent reuse of nodes.More empirically, the directed graph can be encoded into bit-strings as shown in Fig. 1, which is defined by a tupleG=(Nnode,B,LIBRARY),where Nnode and B are the sets of nodes and branches in an individual, respectively; LIBRARY is a set of judgment and processing functions given by the tasks. Each node i∈Nnode is defined by a tuple22Viand xidenote the state-value and continuous variables of node i, which will be described in the next section. They are not included in the discrete PMBGNP.i=(NTi,NFi,B(i),Ci).NTidefines the node type, where 0, 1 or 2 for start, judgment or processing node, respectively. NFi∈LIBRARY represents its function. B(i) represents its set of branches. Ciconsists of a set of Cikindicating the node connected from the kthbranch of node i.In standard GNP, NTi, NFiand B(i) are generally unchanged, while evolution is carried out to change Ci, which means that the task of evolution is to find the optimal solution g*∈G by evolving the node connections.In GNP, since the predefined and unchanged start node without function is only used to determine the first node to be executed, the start node and its branch are not considered in the formulation of G for simplicity.In a certain respect, GNP can be considered as an extension of GP to graph structure, which naturally allows more flexibility and higher expression ability for some complex problems. In addition to a class of related EAs, including parallel algorithm discovery and orchestration (PADO) [37], parallel distributed GP (PDGP) [38], cartesian GP (CGP) [39] and evolutionary programming (EP) [40], GNP does not require the terminal node and the nodes can be connected arbitrarily. Most importantly, by separating judgment and processing functions, GNP can efficiently generate POMDP by selecting only the necessary judgments for the current state of the problems.As indicated above, the role of evolution in GNP is to evolve the node connections to determine the optimal solutions. Different from standard GNP using stochastic crossover and mutation for evolution, PMBGNP enforces a probabilistic model from a set of selected individuals (i.e., top N individuals by truncation selection), and uses the model to generate new population.The probabilistic modeling of PMBGNP is inspired by the univariate EDAs [8,41]. The probabilistic model Pncis composed of a set of probabilities Pnc(b(i), j), which indicates the connection probability from branch b(i) of node i to node j, as shown by:(1)Pnc={Pnc(b(i),j)|∀i,j∈Nnode;∀b(i)∈B(i)}.To calculate the probabilities of node connections, various approaches proposed in PMBGNP [14,16,24] can be applied. In this paper, we use a RL [33] based method called reinforced PMBGNP (RPMBGNP) [16] to construct Pnc. In this method, the episodes of RL can be obtained during the execution of individuals:Definition 1(Episode).Given an individual g∈G, an episode is defined by the sequence of node transitions obtained during the execution of g.Each node connection is defined as a state-action pair33In RPMBGNP, a state is defined as a branch of a node in PMBGNP, and an action is defined as a node. Therefore, node connection (b(i), j) is equivalent to state-action pair (b(i), j).of RL. By factorizing the individuals to the sequences of state-action pairs, we can generate and maintain the state-action Q table efficiently. The Q values can measure the qualities of their corresponding node connections, which are used to calculate the probabilities of node connections.In each generation, the episodes from the top N individuals are selected, in which the Q values of the state-action pairs (substituted by the set of node connections in PMBGNP) are updated by RL. Since the aim of applying RL is to accumulate the actual knowledge of the individuals, an on-policy method called Sarsa Learning (Sarsa) [33] which uses the true experience of the agents is selected to update the Q values. Suppose the current state-action pair at time step t is node connection (b(i), j), and the state-action pair at time step t+1 is node connection (b(j), k). Then, the Q value is updated by:(2)Q(b(i),j)←Q(b(i),j)+αs[rj+γsQ(b(j),k)−Q(b(i),j)],where, αsand γs: learning rate and discount factor of Sarsa. rj: reward of choosing node j at branch b(i) of node i, and,1.If j is a judgment node, rj=0.If j is a processing node, rjis given after processing node j.The procedure of updating Q values in each generation is shown in Fig. 2. With this procedure, the good state-action pairs will be rewarded with higher Q values, and vice-versa, accordingly, which explicitly shows the quality of the substituted node connection. The probabilistic modeling is hence derived by incorporating such learnt knowledge:(3)Pnc(b(i),j)=exp((Q(b(i),j))/(T))Z(b(i)),and Z(b(i)) is the normalization function calculated by:(4)Z(b(i))=∑j′∈A(b(i))expQ(b(i),j′)T,where, A(b(i)) is the set of possible nodes connected from branch b(i) of node i.Boltzmann distribution is used to relax the sample pressure, due to the sensitive diversity loss of PMBGNP (The proof of its diversity loss can be found in [16]). The temperature parameter T is defined adaptively as follows:(5)T=τt+1,where, τ is a coefficient and t is the current number of generations.PMBGNP is dedicated to solve the discrete optimization problems, since its search space is formulated by the node connections Cifor all i∈Nnode while the function of each node is fixed and un-evolvable. In other words, for the problems including functions with continuous variables that take any real numbers within the given intervals, discretization should be carried out to transform the problems to discrete cases in PMBGNP. By discretization the continuous variables are substituted by a set of constant values or segmentations of sub-intervals, denoted as distinct functions in LIBRARY. However, this will cause the problem of losing the solution precision. Proposing a continuous version of PMBGNP would be a meaningful work by making it more general in order to adapt to wider range of problems.In order to make the directed graph structure G suitable for expressing continuous search space, each node i∈Nnode is defined by a tuple i=(NTi, NFi, xi, B(i), Ci). Comparing with discrete PMBGNP, an additional variable xiis added, which is a continuous variable in a range of [loweri, upperi], denoting the lower and upper bounds of variable xi, as an example shown in Fig. 3.Most attempts on extending EDA to continuous domains are to use Gaussian distribution [10,11,42]. The advantages of utilizing Gaussian distribution is its simplicity of implementation and without loss of generality for solutions which ensures the performance in continuous domains. As a result, this paper straightforwardly applies Gaussian distribution for the representation of continuous variables in each nodes, denoted byNi(μ,σ2)for ∀i∈Nnode. Thus, the evolution of each xiis transformed to the updating of the mean μ and standard deviation σ of its Gaussian distribution.Let Pcvbe the probabilistic model of continuous variables, which consists of a set of probabilitiesPcvi(x;μ,σ), denoting the probability density function (pdf) of continuous variable xiof node i. Thus, we have:(6)Pcv={Pcvi(xi;μ,σ)|∀i∈Nnode}.andPcvi(xi;μ,σ)is described by:(7)Pcvi(xi;μ,σ)=12πσ2exp−(xi−μ)22σ2,where xiis the continuous variable of node i.In order to evolve the mean μ and standard deviation σ, one may concern with the incremental learning [10] or maximum likelihood estimation [11]. In this paper, we propose a novel method to update the Gaussian distribution, which contains the following parts.First, we calculate the partial derivative of parameter μ and σ for each variable as follows:(8)∂Pcv(x;μ,σ)∂μ=22πσ2exp−(x−μ)22σ2︸>0×(x−μ),(9)∂Pcv(x;μ,σ)∂σ=12πσ2exp−(x−μ)22σ2︸>0×(x−μ)2σ2−1As indicated, the first terms in the right side of Eqs. (8) and (9) are always greater than 0. Therefore, inspired by the idea of gradient descent we can obtain the updating directions of μ and σ as follows given a sampled value x of the Gaussian distribution:(10)∇(μ;x)=x−μ,(11)∇(σ;x)=(x−μ)2σ2−1.These two equations are the simplified versions of Eqs. (8) and (9). Then, we have the following updating rules of μ and σ:(12)μ←μ+αμ∇(μ;x),(13)σ←σ+ασ∇(σ;x),where αμand ασare the learning rates (step size) of the corresponding parameters, respectively.Second, based on the calculated gradients, a novel algorithm is proposed by integrating a RL technique – Actor critic (AC) [33].In RL, an agent is interacted with its environment through observations and actions. At every step, the agent observes the current state of the environment, then chooses an action to change the state of the environment. At every step of choosing actions, a scalar reinforcement value is formulated according to the reward the agent obtains. This value is backwardly sent to the agent which allows the modification of its actions to maximize the reinforcement value.Based on this idea, the continuous version named PMBGNP-AC is proposed to update the Gaussian distribution. To incorporate AC, we define the state and action according to the structure of PMBGNP as follows:Definition 2(State).State s is defined as a node in the directed graph.Definition 3(Action).Action a is defined as the selection of the values in the continuous variable of each node.Fig. 3 shows an example of such definitions. Note that these two definitions are different from that of Sarsa used in Pncwhich shows the form of node connections. In AC, the set of states are denoted by Nnode and the action space is infinite and bounded according to the range of each continuous variable. With such appropriate definitions, AC can be easily incorporated, where the Gaussian distribution can be regarded as the actor since it is used to select the actions (sample continuous variables), and the critic is formulated as the state-value function V to criticize the actions made by the actor.At each action selection, the critic evaluates the new state to determine whether this selection is better or worse than expected. The evaluation is formulated by the temporal-difference (TD) error δ as follows:(14)δt=rt+γacV(st+1)−V(st),where, rt: reward obtained by the agent at time step t. V(st): value function of state s at time step t. γac: discounted factor of AC.After obtaining the TD error of each time step, it is sent back to update the state-value function V by:(15)V(st)←V(st)+αacδt,where, αac: learning rate of AC.This TD error can evaluate the action of each time step. If δtis positive, it suggests that the tendency to select atshould be strengthened, and vice-versa. Accordingly, we formulate a scalar reinforcement signal θtto indicate whether the tendency to select this action should be strengthened or weakened.(16)θt=−1,forδt<00,forδt=01,forδt>0Inserting the scalar reinforcement signal of Eq. (16) into Eqs. (12) and (13), we get the final updating rules of the Gaussian distribution of PMBGNP-AC as follows:(17)μ←μ+αμ∇(μ;x)θt,(18)σ←σ+ασ∇(σ;x)θt.The algorithm of PMBGNP-AC is a combination of the probabilistic model Pncof node connections and Pcvof continuous variables of each node. As a result, the final probability of generating individual g by PMBGNP-AC is:(19)P(g)=∏i∈NnodePcvi(xi;μi,σi)∏b(i)∈B(i)Pnc(b(i),j).The detailed pseudocode of the proposed algorithm PMBGNP-AC is described in Fig. 4. The initial Q values and V values are prepared in advance, which are set to zero in this paper. Pncis initialized to uniform distribution, while initial values of μ and σ in Pcvis determined problem-specifically.

@&#CONCLUSIONS@&#
This paper extended a recent EDA algorithm named PMBGNP from the discrete domain to continuous cases. This study followed the conventional research on the topic of continuous EDAs and reformulated a novel method to learn Gaussian distributionN(μ,σ2)by a Reinforcement Learning method, i.e., Actor-critic (AC). The resulting PMBGNP-AC method can be thought as an extension of PBILc, where AC can implicitly update the PDF of Gaussian distribution by considering multivariate interactions. The results show that in the wall-following problems of autonomous robots, PMBGNP-AC outperforms several state-of-the-art algorithms, including conventional genetic operators based EAs, discretization based EDA, PBILc based variant and classical RL. Moreover, the scalability of PMBGNP-AC is confirmed by different settings of the problem size.On the other hand, although EDA has been addressed to be the combination of EC and Machine Learning (ML) techniques, most of the existing EDAs use the techniques of Bayesian Network (BN) or some related probabilistic graphical models, and little attention has been deserved to RL techniques even though it is an important branch of ML. This work uses the techniques of RL to build its probabilistic models and shows attractive performance, which is expected to provide an attempt to bridge the gap between EDA and RL. Comparing with the BN based approaches, RL provides a direction to construct the accurate probabilistic models with less computational effort in EDA community.In the future, the proposed method of incorporating AC will be extended from univariate Gaussian distribution to multivariate Gaussian distribution in order to model more complex variable interactions.