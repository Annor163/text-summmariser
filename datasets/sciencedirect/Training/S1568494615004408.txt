@&#MAIN-TITLE@&#
ECG heart beat classification method based on modified ABC algorithm

@&#HIGHLIGHTS@&#
An ECG heart beat classification method is proposed based on modified ABC algorithm.Total 38 feature set is calculated, then most distinctive feature subset is used.Classification accuracy is achieved as 99.30% on examined ECG data from MITBIH db.Sensitivity values are higher than 89% for all sub types of examined arrhythmias.The result of proposed method is compared with seventeen other classifiers’ results.When balanced data set is used, MABC provided the best result among all classifiers

@&#KEYPHRASES@&#
Artificial Bee Colony,Modified Artificial Bee Colony,ECG beat classification,Arrhythmia detection,Data clustering,Swarm intelligence,

@&#ABSTRACT@&#
Electrocardiogram is the most commonly used tool for the diagnosis of cardiologic diseases. In order to help cardiologists to diagnose the arrhythmias automatically, new methods for automated, computer aided ECG analysis are being developed. In this paper, a Modified Artificial Bee Colony (MABC) algorithm for ECG heart beat classification is introduced. It is applied to ECG data set which is obtained from MITBIH database and the result of MABC is compared with seventeen other classifier's accuracy.In classification problem, some features have higher distinctiveness than others. In this study, in order to find higher distinctive features, a detailed analysis has been done on time domain features. By using the right features in MABC algorithm, high classification success rate (99.30%) is obtained. Other methods generally have high classification accuracy on examined data set, but they have relatively low or even poor sensitivities for some beat types. Different data sets, unbalanced sample numbers in different classes have effect on classification result. When a balanced data set is used, MABC provided the best result as 97.96% among all classifiers.Not only part of the records from examined MITBIH database, but also all data from selected records are used to be able to use developed algorithm on a real time system in the future by using additional software modules and making adaptation on a specific hardware.

@&#INTRODUCTION@&#
One of the most commonly used tools to diagnose the heart diseases is electrocardiogram (ECG signal) analysis. ECG signal is a record of heart's electrical activity, basically consists of P-wave, QRS complex and T wave (Fig. 1) [1]. The differences on electrical activity of the heart are used in order to diagnose heart diseases. However, manual analysis of long records by visual inspection of a cardiologist is too much time consuming and not practical. To make this analysis easier and automated, computer based analysis system development is still an active research study topic [2–10].Irregular heart beats with features different than normal beats are called arrhythmias. Arrhythmias represent different heart diseases. Characteristics of waves in ECG signals, such as morphology, area, length, amplitude and time intervals between major parts, are different on normal beats and arrhythmic beats. Those characteristics are called features. Same arrhythmias sometimes can have different wave characteristics on different patients. Even normal beats may differ on the same patient in different conditions, such as effort or resting condition. Because of that reason, there may be an overlap between heart beat types (classes), so arrhythmia classification becomes harder. In order to help cardiologists to diagnose the arrhythmias automatically, new methods for automated, computer aided ECG analysis and arrhythmia detection methods are being developed [2–10]. In this field, mostly used two terms are clustering and classification.Clustering is an unsupervised learning method which is an algorithm that autonomously partitions the data into clusters, in a way so that the data in each cluster is grouped in feature space. Classification is a similar term to clustering, but has a different meaning: Classifier is a supervised system that performs a mapping from a feature space to a set of labels. Basically it assigns pre-defined class labels to the samples.In some articles, clustering algorithms are classified into two categories: hierarchical clustering and partitional clustering [11–13], while in another article [14], various categories can be seen as hierarchical clustering, partition-based clustering, density-based clustering and artificial intelligence based clustering. In fact, there are so many published clustering algorithms [14,15], such as statistics [16], artificial neural networks [17,18], evolutionary algorithms [19], swarm intelligence algorithms [20,21], etc., thus they cannot be easily categorized.One of the most popular clustering methods is K-means, because of its efficiency and simplicity [15,22]. However it has some disadvantages such as depending on initial cluster centers and converging to local minima [12,14,23]. In order to solve these problems, new and hybrid K-means based clustering algorithms have been introduced [8,24–26].Beyond the classical clustering techniques, new approaches are presented in literature such as population based (nature inspired) modern heuristic methods. They can be divided into two main categories, evolutionary algorithms (GA, DE, ES, etc.) and swarm intelligence (SI) algorithms (ACO, PSO, ABC, etc.) [27–30]. To solve complex real world problems, scientists have been interested in natural processes and creatures, as models, for years. There has been a remarkable growth in the field of nature-inspired search and optimization algorithms.Swarm behavior is a collective behavior exhibited by animals. Bonabeau defines the swarm intelligence as “… any attempt to design algorithms or distributed problem-solving devices inspired by the collective behavior of social insect colonies and other animal societies”. It is a discipline that deals with natural and artificial systems composed of many individuals that coordinate using decentralized control and self-organization. There are two fundamental concepts which have high importance for the collective performance of a swarm, self-organization and division of labor. Inside a swarm, there are different tasks which are performed simultaneously by specialized individuals. This phenomenon is called division of labor.Those SI algorithms have been successfully applied to classification problems in the literature such as Particle Swarm Optimization (PSO) [31], Ant Colony Optimization (ACO) [3,20,21,32], Artificial Bee Colony (ABC) algorithm [12,14] as well as various hybrid algorithms [8,13,33].Artificial Bee Colony (ABC) algorithm was first proposed by Karaboga based on the foraging behavior of honey bees [34]. Then next year in IEEE Swarm Intelligence Symposium, ABC optimization algorithm was described for numerical function optimization [35]. This algorithm provides basic structure of today's widely used method. After that date, Karaboga and many other researchers used this algorithm in various application areas successfully [36–46]. There are also recent survey studies [47,48]. Major reasons of high success rate are mentioned as honey-bee colony's foraging behavior, learning, memorizing and information sharing characteristics. Especially in optimization problems, it can converge to the global minimum, independent from initial search locations and not stop at local minima. It is a simple and flexible algorithm, uses few control parameters.Using ABC algorithm on clustering is one of the most recent research topics [12–14,33] while ECG heart beat classification is also an active research area [2–10,49]. However application of the ABC algorithm on ECG signal analysis and beat classification does not exist in literature. In this study, a new method based on ABC algorithm for ECG beat classification is developed. During experimental study, an improvement on original ABC algorithm is performed; this new version is called Modified ABC (MABC). Result of MABC is compared with base line method ABC, as well as classical and recent methods such as evolutionary (GA) and population based algorithms (PSO and ACO).Clustering problem is briefly explained in Section 2. Then inspiration from natural bee colony, analogy about real and artificial bees, ABC algorithm and its application to clustering problem are presented in Section 3. In Section 4, application of MABC algorithm to ECG arrhythmia classification is explained. Then classification results are presented and compared with other methods in Section 5. In Section 6, conclusions are presented.Clustering is a process of partition a data set into subsets, based on some predefined rules. This data set is actually representing a set of objects, in multidimensional space. During clustering, objects are grouped with similar other objects, form subsets, so at the end a classification can be done. Those subsets are called clusters. One class can be consists of one or more clusters.A data set “O”, which consists of “n” objects (O={o1, o2, …, on}) being every object oiis characterized by a “p” dimensional, real valued sample vector, can be represented by a data matrix, with n rows and p columns.Let D={C1,C2, … Ck} be the “k” clusters. Target of clustering algorithm is to determine a partition D which satisfies following three conditions (Cg≠Φ, ∀g; Cg∩Ch=Φ, (∀g, h g≠h);⋃g=1kCg=O). This partition D, should make the objects in the same cluster as similar as possible, and objects from different clusters are dissimilar.In p dimensional space, “p−1″ dimensional decision surfaces/functions should be defined to partition C1, C2, … Ckclusters. In three dimensional space, two dimensional surfaces; in two dimensional space, one dimensional lines separate the clusters.Ideally those clusters should be separated by above mentioned decision surfaces/functions. However in real problems, some of the objects might be closer to other cluster centers, in other words clusters may be overlapped. In order to avoid this problem, a feature set with highest divergent values should be selected. For those cases, which clusters cannot be separated, overlap is tried to be minimized by choosing high divergent features [50]. If that overlap cannot be minimized by this approach, transferring the data to another dimension set can be used [8,51].To be able to measure the success rate of partition, a similarity measurement method should be defined. For this purpose several distance measures are used; the following are the most common: Euclidean distance, Squared Euclidean distance, Chebychev, Manhattan distance and Mahalonobis distance. In this study, Euclidean distance is used as similarity metric.To measure the performance of a partition D, below method can be used. The distance between any object (oi) and any cluster center (Cg) is used as Euclidean distance as in Eq. (1).(1)Euclideandistance=ED(Oi_,Cg_)∑m=1poim−Cgm2i=1, 2, … n; g=1, 2, … k“p” dimensional sample vectors are assigned to the nearest clusters, using Euclidean distance measures. The purpose of a clustering problem is to find a partition D (D={C1,C2, … Ck}, cluster center coordinates set), such that the sum of the distances of all objects to their assigned clusters (i.e. cluster centers) should be minimum. Sum of the distances of all objects to their assigned clusters is called as performance of this partition, Eq. (2).(2)Perf(O,D)=∑i=1nminED(o_i,C_g)g=1,2,…,kOne problem with distances is that they can be greatly influenced by variables that have the different scale values. One way around this problem is to standardize the variables, in this study normalization of data is applied (Section 4.2).In a bee colony, forager bees search and collect the food, also share some information about the food source with other bees. Minimal model of a forager behavior consists of three essential components: food sources, employed foragers, unemployed foragers. It defines two leading modes of the behavior: recruitment to a food source and abandonment of a food source. The value of a food source depends on many factors such as its proximity to the nest, richness or concentration of energy, and the ease of extracting this energy. For the simplicity, the “profitability” of a food source can be represented with a single quantity such as quality or fitness.Employed foragers are associated with a particular food source which they are currently exploiting. They carry the information about this particular source, its distance and direction from the nest, the profitability of the source, and then share this information with a certain probability.Unemployed foragers are looking for a food source to exploit. There are two types of unemployed foragers: Scouts, search the environment surrounding the nest for new food sources (the mean number of scouts averaged over conditions is about 10%). Onlookers, wait in the nest and find a food source through the information shared by employed foragers.Self organization characteristics of a bee colony can be explained as follows [52]:-Positive feedback: As the nectar amount of a food source increases, the number of onlookers visiting it increases, too.Negative feedback: As the nectar of a food source decreases, the number of bees visiting it decreases and at the end its exploitation process is stopped.Fluctuations: The scouts carry out a random search process for discovering new food sources.Multiple interactions: Bees share their information about food sources with their nest mates on the dance area.By using these characteristics of honey bee colony, a new optimization algorithm is described, called Artificial Bee Colony (ABC). Analogy about real bees and artificial bees is presented in Table 1[52].Artificial Bee Colony (ABC) algorithm is one of the recently introduced algorithm in literature, based on a model inspired by the honeybee swarms, proposed by Karaboga in 2005 [34]. It is a problem solving method, developed based on the foraging behaviors of the honey bee colony.Foragers evaluate the nectar amount in the food source, memorize it and share this information in the hive with other colony members with a special communication method called waggle dance [53]. In this sense, more bees waiting in the hive go to the richer food sources, so colony can find and use best food sources in a shortest time.A possible solution in the problem is represented by the position of a food source in the nature. The nectar amount in that food source represents the fitness value of that solution.There are 3 types of foraging bees in ABC algorithm.1.Employed bees, Ne itemsOnlooker bees, No itemsScout beesEmployed bees: Every employed bee works on a food source. Position of this food source symbolizes a possible solution. Employed bee calculates the fitness of this solution and saves the position information in its memory. Number of employed bees in the hive “Ne”, represents the number of solutions in ABC algorithm in one iteration step.Onlooker bees: Those bees wait in the hive and receive information about the position of food sources from employed bees. Each onlooker bee selects a food source to exploit, depending on the nectar amount. This selection is done by modeling the nectar amount as a probability (Pq, probability of “q”th employed bee being selected by onlookers). The more nectar amount exists, the higher probability that employed bee is selected. After this selection, each onlooker bee searches new position near the selected food source (employed bee location).Scout bees: A bee that forages new food sources without any information is called scout bee. They randomly search whole environment. A scout bee becomes an employed bee when it starts to work on a food source.Total bee number N in the colony, is equal to the sum of employed (Ne) and onlooker (No) bees: N=Ne+No. Here, N is a user defined parameter, some authors select Ne as the half of it. In the beginning of algorithm, all foraging bees start as scout bees. When they found a food source and start to work on it, they become employed bees.Combination of forage activities of all bees, search steps of employed, onlooker and scout bees, are repeated in a loop. A predetermined iteration number of this loop is called “MCN” (maximum cycle number) and also used as a control parameter. If the nectar value of a food source (fitness value of that employed bee) cannot be improved after a certain predetermined iteration, that food source is abandoned, then that employed bee becomes a scout bee. This iteration number is called “Limit” and used as another control parameter in ABC algorithm.Detailed steps of ABC algorithm are as follows:-Initialization-Input data set O={o1, o2, …, on}-Set N, Ne, No, Limit, MCN values-Send Ne employed bees to random positions Eq. (3), calculate the fitness values Eq. (4),cycle=1 (beginning of iterations in loop)-Do While the termination conditions are not met (Repeat MCN times for all colony)-For each employed bee (Employed Bee Search Phase):-Find a new position near to the current position Eq. (5)-Calculate the fitness (Fq) of the new position Eq. (4)-Apply the greedy selection to the current position of the bee and the new positions, save the better position.-Abandon the current position, if there is no improvement during last “limit” iteration steps of main loop, (in that case, this employed bee becomes a scout bee).End For-Calculate the probability value Pq for each position Eq. (6)-For each onlooker bee (Onlooker Bee Search Phase):-Choose the destination (which employed bee's location to go) for the onlooker, depending on the probability values Pq-Find a new position near to that employed bee's position Xq Eq. (5)-Calculate the fitness of new position (Fq) Eq. (4)-Apply the greedy selection to the current and new positions, save the better position.End For-If any employed bee became a scout bee in that iteration step,-For all scout bees: (Scout Bee Search Phase):-Send the scout bee to a random position to find a new solution Eq. (3) and calculate its fitness value Eq. (4). This scout bee becomes an employed bee again.End For-End if-Memorize the best solution (position and fitness value) found up to now.cycle=cycle+1End While (Until cycle=MCN)When scout bees searching food sources (new positions) randomly in p-dimensional solution space, they use Eq. (3) to calculate the coordinates of that new position Xq(t). Xq(t) is a matrix of k rows and p columns, contains the coordinates of the cluster centers in iteration step “t” (i.e. in that cycle). Its details are given in Section 3.2, Fig. 2. Xq(1) is chosen randomly at the beginning of the iteration as given in Eq. (3).(3)xgjq(t)=ojmin+ojmax−ojmin×rand[0,1]ojmin: in p-dimensional search space, minimum value of jth dimension feature values of objects in set O.ojmax: In p-dimensional search space, maximum value of jth dimension feature values of objects in set O.g=1, … k; j=1, … p; q=1, … NeIn order to avoid bees to go out of the search space, every value ofXq(=xgjq)is limited with [ojmin, ojmax].In clustering problem, to calculate the fitness value of Xqposition, Eq. (4) is used. In that equation, first, for each object's minimum distance to all cluster centers is determined. Then sum of all minimum distances is calculated to find the performance of that bee's position Xq, (q=1, … Ne). Here qth employed bee represents the last calculated cluster centers.(4)F(Xq)=1Perf(O,Xq)=1∑i=1nminED(o_i,x_gq)g=1,2,…,kWhen employed bees search better solutions near their current position, they use Eq. (5) to calculate new coordinates. As it can be seen in this equation, current positions (at iteration step “t”) of this bee and randomly chosen another employed bee, are used to generate new position in iteration step “t+1″. New position is determined by changing only one chosen dimension onx_gq, as given in Eq. (5).(5)xgjq(t+1)=xgjq(t)+ϕgj×(xgjq(t)−xgjr(t))xgjq(t+1): coordinates of “q”th employed bee, at iteration step “t+1″.xgjq(t): coordinates of “q”th employed bee, at iteration step “t”.xgjr(t): coordinates of “r”th employed bee, at iteration step “t”. It is chosen among other employed bees randomly. q=1, … Ne; r=1, … Ne and r≠q; g=1, … k (cluster number); j=1, … p (dimension number of solution space); ϕgj: rand [–1, 1] (gets different random value, for each bee and each dimension. At each iteration step, only one randomly chosen dimension of the coordinates is changed).Fitness value of “qth” bee is shown as F(Xq) or Fq. When new position is generated, fitness values of current and new positions are compared by greedy selection. Location of better solution (location which has higher fitness value) and its fitness value are kept in memory.Searching new position of an employed bee Eq. (5) is performed once in every iteration step. If there is no improvement on fitness value of this bee, after predetermined times (“limit”) search, this location is abandoned and this employed bee becomes a scout bee. New solution (location) search process starts from a random position by using Eq. (3).After the employed bee search phase, onlookers receive the information (position and fitness values) provided by employed bees. Each onlooker bee selects an employed bee (qth employed bee) according to the probability (Pq) related to its fitness value (Fq) by using Eqs. (4) and (6). Each onlooker bee chooses the position which has higher fitness value, with higher probability. There might be more onlooker bees choosing the same position if this position has higher fitness value.(6)Pq=F(Xq)∑q=1NeF(Xq)Onlooker bee searches a new solution Eq. (5), near to the chosen position, and calculates the fitness of this new position Eq. (4), then compares the fitness values of those two positions (new and old) and keeps the better one in memory. This memory belongs to that employed bee; onlooker bees do not have the memory.If there are abandoned positions, scout bees search new solutions randomly by using Eq. (3).All that process is repeated MCN (maximum cycle number) times. When the termination condition is met, loop ends and best achieved solution, center coordinates of clusters, is generated.ABC algorithm uses iterative approach and does not know the labels of training data set during training phase in clustering application. In this method, user defines the number of partitions. So this method can be considered as a user defined, unsupervised partition based clustering method.ABC algorithm uses cluster number as an input parameter. In order to ensure the proper number of clusters, some validity indices must be considered. There are various validity indices in literature, such as Dunn, Davies–Bouldin, Jaccard etc. However those validity indices are usually suitable for measuring crisp clustering, on data sets which has non-overlapping partitions [54].Let “O” be a data set, which is consist of “n” objects (O={o1, o2, …, on}) and D={C1, C2, … Ck} be the “k” clusters. Any “qth” solution Xq(t) in solution space consists of p dimensional, real valued k vectors (g=1, … k; j=1, … p). These vectors represent cluster center coordinates. For example, in p=4 dimensional solution space with k=3 clusters, cluster center coordinates and their explanations for the qth bee are shown in Fig. 2.Each bee in ABC algorithm consists of the predicted cluster centers, so each bee is represented as a matrix of k rows and p columns. The problem here is to find out the best locations of cluster centers.When searching new position (better solution) in clustering problem, only one randomly chosen dimension value is changed.ABC algorithm's performance is tested on phantom space test data set, by using software developed in this study. Algorithm can make classification with 100% success rate when variance within cluster is small and distances between clusters are long. 100% success is valid as long as there is no interference between the clusters, even if variance within cluster gets bigger or distance between clusters is shorter. Classification success rate decreases when there is interference between clusters.In this study, a modification on original form of the algorithm has been described, for the application of ABC algorithm on ECG signal analysis based ECG arrhythmia classification.This modification is not to abandon a location immediately, if there is no more improvement on fitness of that solution, after “limit” times iteration. In the original ABC algorithm, that location is abandoned, after limit time search with no improvement. However, in that approach, there is a risk to lose one of the good solutions, even if it is the best one. Especially in case maximum cycle number “MCN” is much higher than “limit”, during earlier iteration steps the best possible fitness value could be reached and there would be no more improvement. After that step, when “limit” times more iteration have been performed, original ABC algorithm abandons that location and starts to search from a random new location. However, it may be a high fitness value location or even the best one. Obviously, this has negative effect on classification accuracy. In this study, when experiments were carried out on phantom space data, this situation was observed.In MABC (Modified ABC) algorithm, developed in this study, a new control parameter “Scout Conversion Threshold Ratio (SCTR)” is introduced. If a location of an employed bee is one of the top fitness value solutions (in other words, if fitness of this location is higher than SCTR times “best fitness value achieved so far”), this location is not abandoned and saved in the memory. A value between 0.5 and 0.99 is suggested to be used for SCTR, in this study we used 0.7 for it. After this modification, better classification results are observed.In this study, search process starts from the location of a sample vector of each cluster, not from a random location. With this approach, best fitness location can be reached by less number of iterations. That means training period would be shorter.During search process we defined borders to the search space. If the calculated new position's coordinate is out of [ojmin, ojmax] interval, new value is assigned as this feature's border. In other words, if calculated value is lower than “ojmin” (or higher than “ojmax”), then it is set to “ojmin” (or it is set to “ojmax”). Otherwise, during tests on phantom space data, it is observed that there would be unacceptable, meaningless locations, far away from borders of the search space. In this study we used Eq. (7), instead of Eq. (5).(7)xgjq(t+1)=xgjq(t)+ϕgj×(xgjq(t)−xgjr(t))xgjq(t+1)=ojminifxgjq(t+1)<ojminxgjq(t+1)=ojmaxifxgjq(t+1)>ojmaxModified ABC algorithm steps are shown below. Other procedures are same as ABC algorithm.-Initialization-Send Ne employed bees to locations of sample vectors, (not to random positions).cycle=1-Do While the termination conditions are not met (Repeat MCN times)-Employed Bee Search Phase:-Find a new position near to the current position Eq. (7)-Abandon the current position, if there is no improvement during last “limit” iteration steps of main loop, and if fitness value is not close to the best location's fitness value so far (if this bee's fitness<SCTR times “The best fitness achieved so far”). In that case, this employed bee becomes a scout bee. Otherwise go to the next step. SCTR is a parameter, by which a user can set the allowed distance from best achieved position so far, for an employed bee to abandon its current position.-Onlooker Bee Search Phase-Find a new position near to that employed bee's position Eq. (7)-Scout Bee Search Phase-Send the scout bees to random positions-Memorize the best solutioncycle=cycle+1End While (Until cycle=MCN)In order to see the effect of the modification and the differences from original method on the results, we tested both methods, original ABC and modified ABC (MABC), by using one of the common well known data set, “Iris”. Iris data set consist of 150 samples of Iris flowers. There are 50 samples of three different types of Iris flower, Virginica, Setosa and Versicolor. Each sample of those three classes have four features; sepal length, sepal width, petal length and petal width.On this data set, we run both algorithms ABC and MABC, by using different values for control parameters, total 500 times. Used values for the control parameters are as follows: MCN=10, 20, 30, 40 and 50; limit=2, 4, 6, 8 and 10. The average accuracies for both methods are presented in Fig. 3. Average classification success in ABC algorithm based classifier changes according to MCN and limit values. Detailed comparison can be found in [55].Both ABC and MABC algorithms perform equal times iteration (MCN), so there is no running time difference between them. However MABC starts search process from sample locations, so it reaches to optimal solution earlier.In addition, as it can be seen in Fig. 4, best fitness value in MABC is regularly increasing, however in ABC algorithm, there are some drops on best fitness value. It is because, if the best fitness value cannot be improved after limit times iteration, that location is abandoned and search process starts from a random location. On the other hand, MABC has SCTR control which keeps the best fitness value location, even if there is no improvement after certain times iteration.Heart beats in electrocardiogram are classified in two groups as “Normal” and “Arrhythmic” beats. ECG signal acquired from a healthy person is defined as “Normal” beat. Any heart beat, different from a normal beat in terms of features, is called abnormal beat or arrhythmic beat. There are different types of arrhythmic beats and can be classified into different classes such as Premature Ventricular Contraction (PVC) and Atrial Premature Beat (APB). ECG signals are characterized by their features. Those features can be in time domain and/or frequency domain. Beat rate or other features of normal heart beat may change on different people, as well as on the same person in different conditions, such as excitement, effort, etc. Data obtained from different people, at different conditions, by different measurement systems, must be normalized to be used all together.In this study, ECG signals are taken from the MIT-BIH Arrhythmia database, which is developed by Massachusetts Institute of Technology and ECG recordings obtained by Beth Israel Hospital Arrhythmia Laboratory. The MIT-BIH database is available in Physionet web site [56]. This database contains 30minutes long 48 records obtained from 25 male and 22 female people in different ages, with different health conditions. ECG signals are digitized at 360 samples per second, with 11-bit resolution over a ±5mV range. Those records are provided systematically for research studies and can be obtained from Physionet web site [56].Data used in this study are obtained from records 100, 105, 116, 119 and 209 of MIT-BIH database. Those data contains various noises from electrical sources and record system, so they have to be filtered out before used. For the normalization, each beat's features are divided by arithmetic mean of the last eight normal beat's features.In the first stage, “ecgpuwave, physio toolkit/rdann, rdsamp” software packages are used to filter noises and to detect exact beginning, peak and end points of P, QRS and T waves in ECG signal. In addition to them, it also provided the ability to trace which sample vector belongs to which exact data in the record in MITBIH database. In this study, not some small selected parts but all data in all above mentioned records are examined. By this approach we have targeted to use our developed algorithm in a future real-time system.Some features have higher distinctiveness than others. In a second stage, in order to find highest distinctive features, a detailed study has been done on time domain features. For this purpose, a new software package is developed to generate total 38 different features of heart beats, such as “QRS height, QRS width, QRS area, R–R interval” etc. in the selected MITBIH records. Those features are listed in Appendix A. By using divergence analysis [50], they are reduced to six features, shown as below, which are the most distinctive features for normal and arrhythmic beats.a)Absolute QRS area,Minimum value between R-to-R peaks, (negative peak, mostly S wave amplitude)Time interval between R(t)-to-R(t−1),Time interval between R(t+1)-to-R(t),R-peak amplitude from isoelectric line, andQRS area.Those features’ values after the normalization are shown in Fig. 5. As there are too many (6896) examined normal beats, only part of them is drawn in these figures for better visual analysis of all cluster types. During the classification tests, it is observed that the classification success rate for the first four feature set is the best among the other classification rates for two, three, four, five combinations out of those six feature set. So, first four features are used in this study.In this study, besides Normal beats, Premature Ventricular Contraction (PVC) type and Atrial Premature Beat (APB) type arrhythmias are examined as seen in Fig. 5.In cluster analysis problem, to determine optimal cluster number is an important step. In order to calculate the proper number of clusters, there are various validity indices in literature. We tested below validity indices on our data set. We found different optimal cluster numbers for the same data set as shown in Table 2.However none of those cluster numbers give the best classification success result. It is mainly because our examined data set has overlapped clusters. Then we tested our clustering method with different cluster numbers. We observed that best classification result is obtained by using seven clusters on this data set.Ventricle originated premature beat is generally called PVC, but they have differences among each other. In this study, total four types of PVC beats are examined, with two different morphologies, while one of them has three sub types. Different morphologies and types of PVC beats are shown in Fig. 6. PVC1, PVC2, PVC3 and PVC4 names are assigned for these types of PVC beats in this study. In different PVC types, “minimum value between the R-to-R peaks” is different. Those differences provide significant information about the ventricular area depolarization. It is not easy to distinguish those differences by visual inspection; however they have importance to evaluate the problems on heart rhythm and gives additional information to cardiologist.Atrium originated premature beat is called APB; similarly there are differences on APB type beats. In this study, two types of APB beats are examined. They are shown in Fig. 7. First type APB1 occurs as single event among normal beats (Fig. 7a–c). Second type APB2 is also called SVTA (Supra Ventricular Tachycardia Attack); many APB beats are following each other (Fig. 7d). Normal beats (N-type) have significant differences in different records (different patients). Because of that reason, as much as possible N type beats are used.In real life ECG records, there is much higher number of normal beats versus arrhythmic beats. However, for the evaluation of a classification method, usually similar balanced sample numbers from different classes are used in the literature [3,8,9]. In order to analyze both cases, we formed two groups of data. First one has all beats from examined MITBIH records, except the highly noisy parts, in order to represent real life case. This set is called complete data set. Second one has limited number of randomly chosen normal beats, in the range of arrhythmic beat sample numbers. That set is called balanced data set. In the both groups, arrhythmic beats are same.Then, each of those data sets is divided in two subsets: training set and test set. There are different ratios of sample numbers in training set to test set in the literature. In this study, 1/2 ratio is chosen for training/test set. Beat types and sample numbers in these sets are shown in Tables 3 and 4.In this study, a classifier is developed based on MABC algorithm, it is called MABCC. By using the same training and test data sets, result of MABCC is compared with results of other classifiers which can be grouped as Linear and Higher Degree Polynomial Classifiers, Normal Density Based Classifiers, Nonlinear Classifiers and Nature Inspired Classifiers. New MABC method and all other classification methods are applied to training set, to calculate cluster centers. Then those locations are used to make classification of sample vectors in test set. Names of those classifiers, abbreviations and specific parameters are given in Table 5.

@&#CONCLUSIONS@&#
