@&#MAIN-TITLE@&#
Stable growing neural gas: A topology learning algorithm based on player tracking in video games

@&#HIGHLIGHTS@&#
Algorithm to learn the topology of the environment to navigate in video games.Learn from one or more human players how to use the environment by representing it with a graph.The SGNG learning is continuous to reflect the dynamicity of the environment.

@&#KEYPHRASES@&#
Growing neural gas,Imitation learning,Topology representation,Video games,Agent,

@&#ABSTRACT@&#
Characters in video games usually use a manually-defined topology of the environment to navigate. To evolve in an open, unknown and dynamic world, characters should not have pre-existing representations of their environment. In this paper, characters learn this representation by imitating human players. We here put forward a modified version of the growing neural gas model (GNG) called stable growing neural gas (SGNG). The algorithm is able to learn how to use the environment from one or more teachers (players) by representing it with a graph. Unlike GNG, SGNG learning is in-line, reflecting the dynamic nature of the environment. The evaluation of the quality of the learned representations are detailed.

@&#INTRODUCTION@&#
This work contributes to designing a behavioral model for controlling believable characters [1,2] in video games [3]. Characters are controlled by computer programs we call agents. We define a believable agent as a computer program able to control a virtual body in a virtual environment in such a way that other human users in the environment believe that the virtual body is controlled by a human user [4,5].Contrary to what is done in most video games [6,7], agents’ perception abilities must be similar to those of a player. Agents should be able to handle the flow of time, remembering information from the past and thinking ahead, enabling it to make plans. Finally, agents have to be able to evolve, changing their behavior to make them more efficient and believable [8,9]. This evolution must be fast enough for the players not to notice it, making them feel as if they are playing against an evolved being.In order to achieve behavior believability, the best method is imitation learning by which agents learn their behavior by observing one or more players [10,8]. According to our definition of believability, this is the best way for agents to resemble players. Indeed, the aim of imitation learning is to make agents act like human players.For agents to adapt to unknown environments, we enhanced the growing neural gas model (GNG) [11]. GNG is an algorithm usually used to perform unsupervised tasks like clustering, interpolation or vector quantization. It represents data with a graph and automatically adapts the graph topology to the data. In our case, the data concerns one or several players’ positions. For a given player, the data can be seen as a temporal series. Indeed, the position of the player at time t is dependent on its position at time t−1. The model can then learn a representation of the volume of the environment which also corresponds to how the players use that environment. The final representation used by the behavior model is composed of the graph nodes alone: each node represents a place where the agent can go but the edges do not provide any information about how to access one node from another [12].GNG appears to be an interesting algorithm for learning maps by imitation because of its ability to learn topology. We argue that this aspect will make agents use the environment in a more human-like fashion. It also removes the burden from the map designers of manually defining the navigation graph. Its settings do not change over time. This is particularly useful as we want the algorithm to be able to adapt to changes in the map (for example a wall that collapses).However, GNG was not designed to work with temporal series but with samples. Its node insertion policy does not suit our needs as it inserts nodes at constant iterations and in the area with the maximum error. The constant iteration insertion policy causes the network to grow infinitely over time and the insertion policy in the area containing the maximum error is not appropriate when working with temporal series. By proceeding in this way, we may over-sample an area of the map not frequently visited by players. The primary goal of this article is to provide a node insertion policy that overcomes these problems.Moreover, although GNG has already been used in a video game [12], its characteristics and parameters have not been studied for this kind of application. The following questions have not been answered: What influence do the parameters have on learning? How should we choose the parameters? How can we accelerate the learning process? Answering these questions is the second objective of this article.In the following sections we will present a modified version of GNG, called SGNG. Unlike GNG, SGNG learning is continuous, thus reflecting the dynamic nature of the environment. In addition, we will analyze the algorithm's characteristics, the quality of the learned topology and the influence of each parameter. Finally, we compare GNG with SGNG. To be fair, this comparison will be made with both temporal series and samples. We will see how our changes improve performance over temporal series and the impact of results on data samples. The paper is organized as follows: first we will present the SGNG (Section 2). Then, we will present the results (Section 3). Finally, we conclude (Section 4).First we will explain the reasons for choosing the GNG algorithm to learn the representation of the environment and describe its principles in Section 2.2. Then we will introduce some modifications for the algorithm to suit our needs in Section 2.3.Models which control virtual humanoids use different kinds of representations to determine paths to go from one point to an other. All the meshes used to render the environment are too complex for agents to handle. As a consequence, classic approaches use a graph to represent accessible places, with nodes and paths between each place by edges (see Fig. 1). Current solutions tend to use a simple mesh, with different degrees of complexity to represent the accessible zones (see Fig. 2). The problem with the latter solution is that it requires an algorithm to find the optimal path between two points, a path which may not be natural or believable. Moreover, a graph solution is more suitable: each node of the graph can be used by the model to attract or repel the character.In video games, classical bots are designed to follow pre-defined waypoints determined by the map designer. These bots need to have a waypoint file for each map, or a pathnode system embedded in the map. For example, Quake 3 Arena bots use an area awareness system file to move around the map, while Counter-Strike bots use a waypoint file. Unreal Tournament's series bots use a pathnode system embedded in the map to navigate. To support the many community-created maps, some games include an automatic mesh generation system (for example Valve bots). The first time users attempt to play a custom map with bots, the generation system will build a navigation file for that map. Starting at a player spawn point, walkable space is sampled by “flood-filling” outwards from that spot, searching for adjacent walkable points. Finally, dynamic bots are able to dynamically learn levels and maps as they play. RealBot, for Counter-Strike, is an example of this. However, this learning is not guided by human behavior. Navigation points obtained will therefore not produce believable behavior. The paths the bots use to go from one point in the environment to another do not resemble those human players would take. This problem comes not from the decision-making process itself, but from the representation of the environment it uses. Indeed, the bots use navigation points in the environment which may not accurately or naturally represent how players use the same environment.In order to achieve optimum believability, we want the nodes of the graph to be learned by imitating human players (tracking human navigation). This work was done in [13] where nodes and a potential field were learned from humans playing a video game. Agents could then use this representation to navigate the game environment, following the field defined at each node. In order to learn the positions of the nodes, Thurau used an algorithm called GNG.GNG[11] is a graph model capable of incremental learning. Each node has a position (x, y, z) in the environment and has a cumulated error which measures how well the node represents its surroundings. The fewer errors a node has, the better it represents its surroundings. Each edge links two nodes and has an age informing us when it was last activated. This algorithm needs to be omniscient because the position of the human teacher needs to be known at any given time.The principle of GNG is to modify its graph for each input of the teacher's position in order to alter the graph to match the teacher's position. The model can add or remove nodes and edges if they do not fit the behavior and change the position of the nodes to better represent the teacher's position. Fig. 3provides pseudo code of the algorithm and its corresponding steps in Fig. 4. In Fig. 4, we assume that the iteration number is a multiple of η.The version of GNG we use is modified as shown in Fig. 5. The two main problems with GNG is that it fails to handle temporal series appropriately, and it continues to grow over time.The constant iteration insertion policy causes the network to grow infinitely over time. This problem can easily be overcome by increasing the number of nodes in the network, but how can we define this maximum number of nodes? Obviously, a large map probably needs more nodes than a smaller one. We could also stop the learning process when a suitable number of nodes is reached but we also need the GNG to adapt to variations in the use of the map. If the teacher suddenly uses part of the map which he/she has not yet explored, the GNG should be able to learn this new part even if it has been learning for a long time.The insertion policy in the area containing the maximum error may over-sample an area of the map not frequently visited by players. Inserting a node in the current area of the temporal series seems to be more appropriate.We propose that instead of inserting a new node each η input, a node should be inserted when the error of a node is superior to a parameterErr¯. In this way, we add nodes only in the area just visited by the player (avoiding maximum error area search) and we add nodes only when the network fails to fit the data suitably. As the error of each node is reduced by a small amount Err for each input, the modified GNG algorithm does not need a stopping criterion. Indeed, if there are many nodes which accurately represent the environment, the error added for the input will be small and for a set of inputs the total added error will be distributed among several nodes. Decreasing the error will avoid new nodes being added to the GNG, thus resulting in a stable state. However, if the teacher goes to a place in the environment he/she has never been before, the added error will be high enough to counter the decay of the error, thus creating new nodes.This algorithm has five parameters which influence the density of nodes, the quality of the representation, adaptability and convergence time:•The attractionattract1→applied to n1 toward (x, y, z).The attractionattract2→applied to the neighbors of n1 toward (x, y, z).The error decay for nodes, Err.The maximum error for the nodes,Err¯.The maximum age for the edges,Age¯.SGNG has a complexity of O(n) where n is the number of nodes in the graph. This should leave plenty of computing power for other algorithms. The algorithm is entirely capable of handling data from several teachers. By giving input from several teachers we should greatly increase learning speed. The only drawback with this technique is that different teachers can use the environment very differently. As a consequence, the learned SGNG may reflect neither of the individual teachers’ behaviors.

@&#CONCLUSIONS@&#
