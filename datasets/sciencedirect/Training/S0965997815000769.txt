@&#MAIN-TITLE@&#
Extending parallelization of the self-organizing map by combining data and network partitioned methods

@&#HIGHLIGHTS@&#
Data partitioning and network partitioning parallelization methods are combined.Large performance gains are found on small networks.The network is parallelized to the level of a single node dimension.Parallelization is maximized to allow robustness for future hardware.

@&#KEYPHRASES@&#
GPU,Self-organizing map,Parallel computing,Neural network,Data visualization,High-dimensional data,

@&#ABSTRACT@&#
High-dimensional data is pervasive in many fields such as engineering, geospatial, and medical. It is a constant challenge to build tools that help people in these fields understand the underlying complexities of their data. Many techniques perform dimensionality reduction or other “compression” to show views of data in either two or three dimensions, leaving the data analyst to infer relationships with remaining independent and dependent variables. Contextual self-organizing maps offer a way to represent and interact with all dimensions of a data set simultaneously. However, computational times needed to generate these representations limit their feasibility to realistic industry settings. Batch self-organizing maps provide a data-independent method that allows the training process to be parallelized and therefore sped up, saving time and money involved in processing data prior to analysis. This research parallelizes the batch self-organizing map by combining network partitioning and data partitioning methods with CUDA on the graphical processing unit to achieve significant training time reductions. Reductions in training times of up to twenty-five times were found while using map sizes where other implementations have shown weakness. The reduced training times open up the contextual self-organizing map as viable option for engineering data visualization.

@&#INTRODUCTION@&#
Many problems facing industry today are investigated using data acquired through a number of mediums (e.g., sensor recordings or simulation results). Data is becoming cheaper to acquire while at the same time becoming more accurate. Leveraging available data is increasingly important for remaining on industry’s cutting edge. One problem continuing to emerge is the shear amount and corresponding dimensional complexity of data being investigated. No longer can basic plotting methods (e.g., orthogonal plots, scatterplots, etc.) alone be relied upon to understand data characteristics. Researchers have realized this for a number of years and have developed many techniques in an effort to visualize growing levels of data complexity [1–4]. A number of current techniques map n-dimensional data sets into a two or three-dimensional representation that is directly interpretable by human visual perception. If, for example, a seven variable data set is to be visualized, it is common to set four or five of the variables to constant values and then plot the remaining two or three dimensions using a traditional graphing technique. This can be carried out multiple times using permutations of the dimensions held constant in an attempt to understand the complex relationship therein. This becomes impossible to understand as the number of permutations increase, leading to the necessity of new or further developed techniques.Much research has been conducted on multidimensional data representations attempting to present pertinent data to a user [5]. Methods like parallel coordinates [6], graph morphing [7], and linking multiple visuals [1,8] lessen mental load on an investigator by limiting the visual complexity displayed in a single view. Each of the noted methods, however, remain difficult to interpret for high levels of dimensionality. Pixel-based [9] techniques approach the problem from a different direction by showing massive amounts of data while limiting focus only to overall data trends. In many cases a single choice from prior developed methods is not fully effective on its own. This has led to the development of tools that integrate multiple visualization techniques [3] into a suite of tools. Each of the methods noted, whether alone or combined, provide insights into the data under investigation but leave a large part of the complexity remaining for the investigator to make inferences from.Kohonen’s Self-Organizing Map (SOM) [10] allows for low-dimensional visualization of a high-dimensional space while the data’s topology is preserved. It has been used extensively to avoid many of the issues noted with modern data visualization methods [11–14]. The Contextual SOM (CSOM) supplements the standard SOM with the addition of a contextual label on the individual nodes of the resulting SOM. Prior work showed that the added contextual information allows an investigator to identify characteristics of the data set that are otherwise extremely difficult to find [15].However, the original SOM method is a serial method and trains the map one data point at a time. It can take hours to days for a single 100,000 data set to train on a modern desktop computer. This limitation has led to the development of a batch processing method of training SOMs called batch-SOM [16]. The batch method allows for faster convergence with large data sets by allowing the independent processing of each data point in parallel. Prior literature [17–19] has shown training time reductions using the batch-SOM in combination with the two parallelization techniques commonly used to breakup the SOM training process: network partitioning and data partitioning. These methods alone are useful and can reduce training times, but are limited to certain cases as will be described in the remaining sections. A new implementation of the batch-SOM is developed in this work by combining network partitioning and data partitioning and further parallelizing the network partitioning method traditionally used.In 1982, Tuevo Kohonen modeled the human brain’s learning processes in the cerebral cortex using an artificial neural network [10]. The SOM uses an unsupervised learning strategy to train a lattice of neurons. Determining the structure of the original neuron lattice is done in a heuristic fashion by the investigator based on data set size and dimensionality, for example. Each neuron i in the lattice has its own weight vector w as shown in Eq. (1) with the same dimensionality as each data point x as shown in Eq. (2). This structure allows the SOM to scale to any dimensionality k as shown in Eqs. (1) and (2).(1)wi=〈wi1,wi2,…,wik〉(2)x=〈x1,x2,…,xk〉SOM training involves two phases, ordering and convergence, each containing many iterations through the data. The number of iterations for the method to run is decided upon by the investigator. To begin an iteration, a data point is randomly selected from the data set and is compared against each node in the map using the Euclidean distance metric shown in Eq. (3). The neuron found with the smallest Euclidean distance is determined to be the winner or “activated” neuron.(3)Distance=(x1-wi1)2+(x2-wi2)2+…+(xk-wik)2)With the winning node decided, the weight vector w of each node in the surrounding neighborhood h of the map is “influenced” using Eq. (4) to have its values become more like the current data point x. The influence’s magnitude depends on the neuron’s position, ri, relative to the winning node’s position, rj, in the map as well as the current number of training iterations n that have elapsed. η is a time-varying learning rate and defines the amount of the influence on neighboring nodes. The neighborhood influence h is a Gaussian-based influence factor that effects nodes closer to the winner more than those farther away. Learning rate and neighborhood influence are shown in Eqs. (5)–(7) respectively.(4)wi(n+1)=wi(n)+η(n)hj,i(n)(x(n)-wi(n))(5)η(n)=η0∗exp-n/λ(6)σ(n)=σ0∗exp-n/λ(7)hj,i(n)=exp-‖ri-rj‖2σ(n)2Applying a label to each node of the map is referred to as the Contextual Self-organizing Map (CSOM). When using the CSOM, data points are passed into the SOM a final time to determine the closest node, again using Euclidean distance. In the contextual phase, the data point’s label is added to the activated neuron instead of updating the neighborhood. Fig. 1[20] shows an example from Haykin that trained a CSOM using animal attributes. The animal’s attributes make up the training data and the contextual label is the animal’s name. The trained map shows groups of animals with similar attributes. Zebras, horses, and cows were grouped together by the training. This group is noted by Haykin as peaceful, four-legged large mammals.Using the traditional SOM as described above, the map node weights are updated after every data point. With the batch-SOM, however, all data points are first evaluated (for their winner) before updating the map [16]. Two types of parallelization become possible using the batch formulation. First, because node updates only occur once per iteration (as opposed to per data point), all winning node calculations can be performed in parallel. Secondly, the map itself can be parallelized because each node is only required to perform a summation of the influence of all other nodes. The formulation can be thought of as similar to a weighted average being performed across the map. Eq. (8) shows the batch-SOM equation for updating the weight vectors of each node following a single map iteration where t0 and tfrepresent the start and finish of the present iteration, respectively.(8)wi(tf)=∑t′=t0t′=tfhji(t′)x(t′)∑t′=t0t′=tfhji(t′)The standard personal computer today often has between two and eight cores that make up the CPU. The drive behind an increasing number of cores has been due to the hardware venders beginning to reach the limits of how much performance can be achieved from each core. Multi-core processing has thus become the standard route to achieve higher performance levels [21]. More recently, the use of graphics processing units (GPUs) containing upwards of 2000 cores have become popular. Up until the early 2000s, the primary us of a GPU was to generate graphics to be displayed on a screen. Improvements to the hardware in recent years have opened up the control over the graphics pipeline for software developers to use the GPU for general purpose computation. GPU research has shown large improvements in computational capability over that of a modern CPU [22]. Today there are multiple software packages to choose from that allow programming on a GPU in a similar way to modern C/C++ programming [23–25].CUDA [25] is a programming language created by NVIDIA Corporation to take advantage of NVIDIA GPUs for general purpose computing. The CUDA libraries provide a large range of computational functionality that can be used for general scientific computing and lower the barrier to entry for GPU parallelization of algorithms.One primary difference with writing software to be executed on a CPU is that on a GPU the software developer must choose the memory storage location manually. This makes understanding the layout of memory (see Fig. 2) on a GPU very important. As an example, accessing the global memory that is available to all threads in a kernel can take from 200 to 800 clock cycles while instead using shared memory located on the processor die and limited to a single block of threads can take ∼1 clock cycle [26]. This difference could entirely change a method’s implementation. The GPU model provides a lot of computational power, but also leads to limitations ranging from those similar to the memory limitation noted above to hardware limitations like limited double-precision floating point operations [22].There are two commonly used techniques for parallelizing the SOM whether for CPU or GPU parallelization. The first technique breaks up the map itself, and is known as network partitioning. In this case the winning node calculation and node updates are all computed on separate threads. This method of parallelization is often used with the original Kohonen SOM [10] formulation because all data points can be trained sequentially with map updates meaning the original algorithm’s integrity is held. This method is most effective when implemented with large maps as using small maps causes a large amount of time to be spent simply communicating between threads and writing to memory rather than performing calculations [23,24].Ozdzynski et al. implemented a network-partitioned algorithm [28] on the CPU and tested three different update kernels for the maps using from one to eight processors cores on varying sized maps from small (2×4), medium (20×30), and large (30×40) and found that the time taken to train on smaller maps increased when increasing the number of parallel computing threads. The larger the maps used, the smaller the parallelization training time required [29]. Rauber et al. developed an improved technique implementing a similar network partitioned approach with an increased focus on limiting communication and increasing the use of cache memory [30]. Testing was done by parsing and classifying a collection of 420 articles from Time magazine by topic. Up to a twenty-two times increase in performance using the parallel method with caching was found.The second method of parallelizing SOM training is to divide up the data amongst the individual threads, known as data partitioning. This implementation requires the use of a modified version of a SOM called the batch-SOM [16] noted in Section 1.2. With the batch-SOM, map training does not depend on the order of data points being trained because the map update does not occur until the winning node is found for all data points. This allows the data set to be divided among the parallel threads for calculating the winning nodes in parallel.Lawrence et al. developed a method with a focus on data partitioned batch-SOMs [29]. They trained against sample retail and census data using only CPU parallel processing. Results showed comparable clustering to the traditional SOM methods but with linear speed increases as the number of processor increased. Similar parallel methods have been developed using the CUDA [18] and OpenCL [17] GPU libraries to achieve up to 44 times speed-ups compared to previous serial implementations with large maps [18].Parallel implementations of the SOM thus far have generally made use of either network partitioning or data partitioning, but not both. The network partitioned methods were limited in their benefit to the size of the maps and the data partitioned methods did not leverage the fully parallelizable nature of the SOM. This work attempts to leverage the benefits of both network and data partitioning in a similar manner to [31] while at the same time increasing the parallelization further by breaking the network not only into separate nodes like standard network partitioning implementations, but also separate dimensions as will be described in detail in the remaining sections.

@&#CONCLUSIONS@&#
In this article a method was developed for reducing the training time of self-organizing maps using CUDA on a modern graphical processing unit. Initial implementations using data partitioning and network partitioning separately were each effective at reducing training time, but were limited in their respective benefits.The method developed in this work first broke up the thread structure into separate maps thereby leveraging data partitioning, but at the same time then broke up each of said maps all the way down into each respective weight value to leverage network partitioning in a way that had not been done before. The number of threads available for execution using the developed method is equal to the number of maps, M, multiplied by the total number of neurons in the map, C, multiplied by the number of weight values found within a neuron, K. The structure developed will allow for its continued use as GPUs continue to increase the number of threads they can process in a single clock cycle.In the experiment we tested the method against for data set size and dimensionality. Results showed greater performance enhancements as data set sizes increased and in some cases saw a performance gain of nearly 20X. The method showed a diminishing benefit when increasing dimensionality of the data set. Further refinement of hardware resources used by the algorithm could likely achieve similar results for dimensionality as for data set size, but is left for future work.With the growing number of threads available per GPU clock cycle in today’s hardware, the method developed will continue to grow in its performance benefit. It is important to again point out that each test was run on a small map (10×10 neurons), something many current parallel methods do not perform well on and often perform worse on in comparison with serial methods. A full analysis of the method’s scalability will be left for future work.The growth of both size and complexity of data in industry continues to demand methods that make data usable to an investigator. The self-organizing map is often used for this purpose, but is computationally demanding and limits its feasibility for use in practice. Modern graphics processing units (GPUs) have developed as a promising means by which to reduce the impact of these computational demands. The method developed in this article combined a fully parallelized network partitioning method with prior literature in data partitioning. The developed method is fully scalable and lends itself to increasingly larger benefit as graphics cards continue to grow in their computational power in years to come.