@&#MAIN-TITLE@&#
Cross-pose face recognition based on multiple virtual views and alignment error

@&#HIGHLIGHTS@&#
Cross-pose face recognition with only a single frontal gallery face is a challenging task.Multiple virtual views are generated from a single frontal gallery face in advance.Alignment error of two faces is calculated after a two-phase (pose and individual) alignment.Correlation between patches is considered by overlapping and covariance.Cross-pose face recognition accuracy is significantly improved than compared methods.

@&#KEYPHRASES@&#
Face recognition,Virtual views,Alignment error,

@&#ABSTRACT@&#
Graphical abstractImage, graphical abstract

@&#INTRODUCTION@&#
Face recognition has been investigated for several decades. According to the 2009 NIST MBGC report [1], face recognition remains a challenging endeavor on account of variations in poses, illumination, occlusion, and aging. Among these, pose variance is the most difficult to address. Face recognition algorithms can be used to identify criminals from surveillance systems for public security. In addition, they can be applied to automatically annotate digital photos for individuals. Moreover, commercial face recognition software is publicly available, such as Google Picasa and Apple iPhoto [2].Discriminative facial features are important for both accuracy and speed. Local features, such as local Gabor binary patterns (LGBP) [3] and high-dimensional local binary patterns (LBP) [4], are effective for face recognition. Wright et al. proposed use of sparse representation for face recognition; it can handle illumination, expression variance, and occlusion [5]. The face recognition problem can be divided into two categories: face identification and face verification. Face identification serves to identify a probe face from a set of gallery faces with known identities. Face verification is used to determine whether two images belong to the same subject.Face verification is a useful branch of the face recognition problem. Recently, a joint Bayesian model trained from a large dataset of labeled faces was successfully used for face verification [6]. Based on this model, a transfer learning method was proposed for combining ample cross-domain source data [7]. In [8], the Facebook AI Research group proposed the DeepFace framework, which uses deep networks for face verification. The system reached a state-of-the-art accuracy of over 97% on the Labeled Faces in the Wild (LFW) database.Face identification is another challenging endeavor for face recognition. Under controlled settings, such as a frontal face with little illumination variance, the face identification performance has approached human capacity. While pose and illumination variances exist in most applications, face identification accuracy significantly decreases when test faces are non-frontal. Most facial image databases contain only frontal faces, such as driver licenses. The Department of Motor Vehicles collects frontal view images of each driver. Thus, it is necessary to process cross-pose matching to identify a randomly posed face from a frontal view database. Cross-pose identification remains a challenging problem. Moreover, in cases in which only a single frontal face is available, cross-pose matching becomes more difficult. The difficulty in identifying a face with different poses is that the ‘between-subject’ differences are less than the ‘between-pose’ differences. There are two solutions for handling this problem: geometry-based methods and pose-invariant-based methods.The geometry-based method uses an alignment method to build correspondences among different poses. Based on these correspondences, a probe face with different poses can be normalized to a frontal face. This method can be performed in both 2D and 3D cases. Regarding 2D methods, a Markov random field (MRF) is used to find correspondences between a frontal face and profile faces [9,10]. The 2D displacement is captured using MRF by minimizing the energy, which includes the residual of two corresponding nodes and smoothness between neighboring nodes. MRF is effective on some databases; however, it incurs lengthy computation times. Lucas–Kanade is another effective 2D normalization method [11]; it calculates the transformation parameters for each of the correspondences of two images.The 3D morphable method is an effective normalization method [12] that builds a 3D general model and fits it to a probe 2D face. The fitted shape and texture coefficients are used for face identification. Li et al. [13] synthesized a probe face by estimating 3D displacement fields from a 3D face database. It has been reported that 3D techniques can achieve impressive results on many databases. However, 3D face databases are required for these methods. Furthermore, recovery of a 3D virtual face from a 2D image is difficult because of insufficient information. Moreover, fitting a 3D model to a 2D image is sensitive to factors such as illumination and expression.The pose-invariant-based method employs pose-invariant features or pose-insensitive classifiers to eliminate the pose influence. Tied-factor analysis has been proposed for representing a non-frontal face by a pose-contingent linear transformation of identifiers [14]. The resultant pose-invariant identity subspace is used for identification. Another subspace, called the discriminant-coupled latent subspace, has been proposed [15]. It is used to find projections of the same subject from different poses that are maximally correlated in the latent subspace. One-shot similarity (OSS) and two-shot similarity (TSS) are pose-insensitive classifiers [16]; they require a third-party dataset with no probe and gallery faces in it. Each subset can be of the same subject with different poses. Similarities between two faces are calculated by models built from these faces and the subsets using linear discriminant analysis (LDA) or support vector machine (SVM). Similarly, cross-pose face recognition likewise requires a third-party dataset [17]. Faces from different poses are linearly represented by the third-party dataset based on a subspace method. The obtained linear coefficients are used for face identification. Recently, neural networks [18] and deep learning [19] have been applied for calculating the pose-invariant features. The networks are trained by converting a non-frontal face to a frontal face; the pose-invariant features are obtained in a specific layer.In this paper, we propose a novel algorithm based on multiple virtual views and alignment error (MVV–AE) for face recognition under large pose changes with only a single frontal face available for each subject in a gallery. The main contributions of this paper are as follows: (1) scale-invariant feature transform (SIFT)-matching score based on multiple virtual views is proposed to improve the performance of SIFT for the large pose change. A frontal face is transformed into multiple virtual views using learning warps across poses by the Lucas–Kanade algorithm [11]. SIFT is used to calculate the keypoints from these virtual views and to match them to a probe face; (2) a two-phase alignment method is proposed to calculate the alignment error between a probe face and a gallery face. Offline alignment is used to calculate pose differences, while online alignment is used to calculate individual differences. Overlapping and covariance are adopted to capture correlations between patches; (3) a hybrid similarity between probe and gallery faces is obtained by the number of matched keypoints from SIFT and the two-phase alignment error.The remainder of this paper is organized as follows. In Section 2, we describe the framework of the proposed face recognition algorithm based on MVV–AE. We describe the MVV generation method in Section 3. In Section 4, we introduce the proposed two-phase AE with the correlation method. The combination of these two methods is used for the similarity calculation in Section 5. In Section 6, we apply the above algorithm to the FERET [23] database and present the experimental results. Finally, we present our conclusions in Section 7.Local features, such as LBP and SIFT, are effective for face recognition with small pose changes. However, the performance significantly decreases with large pose variations. To enable the effectiveness of these local features for large pose changes, we propose a novel framework based on MVV–AE. The challenge of this objective is that gallery faces and probe faces are of different poses and only a single frontal face is stored in the gallery. Fig. 1shows the framework of the proposed MVV–AE method, whereby warps between poses are learned from numerous face pairs of different poses. Given a gallery face and a probe face, we can calculate their similarity based on two parts: the number of matched keypoints and the alignment error. Since pose estimation is out of the scope of this paper, we assume the pose of a probe face is annotated with the ground truth. The input data for a test consist of a probe face image and its ground-truth pose.The number of matched keypoints is obtained from the SIFT matching algorithm between the probe face and generated virtual views of a gallery face. For each gallery face, we generate multiple virtual views in advance using the learned warps between poses. SIFT keypoints are detected from these virtual views and matched with the probe face.The alignment error is calculated by a hybrid alignment method with consideration of the correlations between patches. The differences between two face images are primarily from the pose variance and identity differences. In this study, we consider both of these differences in our hybrid alignment, which contains a pose and individual alignment.Without 3D information, a probe face with a profile angle is difficult to transform into a frontal face because of occlusion, especially in the nose area. However, frontal faces contain intact information with no occlusion. This motivates us to transform a frontal face into multiple profile views instead of normalizing a profile face into a frontal view. In this scheme, transformation of a frontal face into virtual views is performed in advance, while normalization of a profile face into a frontal view should be simultaneously conducted with the face recognition.To transform a frontal face into multiple virtual views, we must learn warps among multiple poses. As a simple and efficient warp, the affine transformation is used in this study. The human face contains significant 3D depth information; a single affine warp for the entire face is insufficient for capturing transformations between poses. Thus, we divide a face into multiple subregions or patches; a warp is learned for each patch. The Lucas–Kanade algorithm [11] is effective for learning warps between poses, as shown in Fig. 2.To obtain generic warps, numerous face pairs are used to learn warps between poses. This can be performed by averaging two sets of faces; the averaged face pairs are used to learn warps. However, an averaging of faces results in neutralization, which is too generalized to represent a specific individual. The stack flow method [11] provides a better solution by calculating warps that minimize all face pairs from two poses as follows:(1)Er(stk)=∑j∑x(Ij,r(W(X,P))−Tj,r(X))2where I and T are images from two poses, and j and r are the jth pair of images and the patch index, respectively. W(X, P) is the warp function, which is the affine transformation in this study, and(2)W(X,P)=PX=(1+p1p3p5p21+p4p6001)(xy1).The Lucas–Kanade algorithm provides a solution for Eq. (1) by iterating the update of P with ΔP as(3)ΔP=H(stk)−1∑j∑X(∇Ij,r∂W∂P)T(Tr(X)−Ir(W(X,P)))where∇Ir=(∂Ir∂x,∂Ir∂y),∂W∂P,and Himgare the gradient of Ir, the Jacobian of the warp and a pseudo Hessian matrix, respectively.Let N and M be the number of patches in an image and the number of poses, respectively. LetΦ=(P1,P2,…,PN)be the warps between two poses. A series of warps,Φ1,Φ2,…,ΦM, are learned between the frontal view and profile poses. Based on these warps, a frontal face is transformed into multiple virtual poses. The SIFT keypoints are detected from these virtual views and matched with the probe face. Accordingly, the number of matched keypoints between the probe face and gallery faces is obtained.The probe face is compared with all generated virtual views because it is not easy to accurately estimate the pose; moreover, images from other poses provide additional facial information. However, considering computation time, an interval is required to maintain sparsity. SIFT is chosen for the features on account of its scale-invariant characteristic and good performance for pose variance within 25°, according to our experiments. In this regard, poses are divided into four quadrants, we define the poses in the same quadrant as the same orientation. As for the FERET database, it consists of poses from left to right captured at 60°, 40°, 25°, 15°, −15°, −25°, −40° and −60°. We divided them into two orientations: left and right. Poses at 60°, 40°, 25°, and 15°belong to the same orientation. We generate a virtual view for each pose, and based on our experiments, we found that the virtual views from different orientations of the probe face had a negative impact on the face recognition accuracy (see Section 6.1). Thus, we estimate the orientations (up-left, up-right, down-left, down-right) of the probe face. The virtual views of the same orientation of the probe face are used to calculate the matched keypoints. In this way, the compared virtual views are significantly reduced, and the estimation of orientations is much easier than that of poses.Learned warps are the correspondences between poses and are trained from numerous face pairs. When directly applying the learned warp to the probe face, it is not sufficiently accurate on account of individual differences. Once exact warps between faces are available, we can easily calculate the difference between these two faces. The difference of two faces is derived from the pose and individual difference. The learned warps described in Section 3 demonstrate the pose alignment process. To obtain more accurate warps, we propose a two-phase alignment process, which is comprised of the pose alignment and individual alignment. The pose alignment is fulfilled by the learned warps between poses, as outlined in Section 3; it is performed in an offline manner without incurring much computation time. Individual alignment, which is conducted in an online approach, converges quickly because of the pose alignment performed prior to it. Let Ppand Pibe the pose warps and individual warps matrices, respectively. The two-phase alignment error (AE) is used to minimize the following error:(4)Er=∑x(Ir(W(X,Pp,Pi))−Tr(x))2whereW(X,Pp,Pi)=PiPpX, and the multiplication of the two-warp matrices, Ppand Pi, corresponds to the pose alignment and individual alignment, respectively. Ppis learned by numerous face pairs of frontal faces and non-frontal faces with ground-truth pose p in advance. For each probe face, an online alignment is performed with each gallery face i using the Lucas–Kanade method by the iteration of update Pi. The online alignment converges quickly due to the pose alignment prior to it. In our experiments, Piconverged after approximately 10–15 iterations, which indicates no significant increase in computational time.The block effect is incurred during the division of patches. Moreover, the Lucas–Kanade method is performed for each patch separately. The method readily warps adjacent patches to much different areas. Thus, we propose the use of overlapping of patches. Four neighboring patches are overlapped, as shown in Fig. 3. The small patch at the center is overlapped by four neighboring patches; the warp parameter of the center patch is the average of these four patches.After Piis obtained, the alignment error can be calculated by Eq. (4) for each patch. Because overlapping is used in our scheme, there is a correlation among patches. Considering these correlations, we multiply the covariance of the alignment error of each patch to calculate the alignment error of the whole face as(5)Emah=(E−E¯)TCov−1(E−E¯)whereE=(E1,E2,…,EN), N is the number of patches, Cov is the covariance of E,E¯is the mean of vector E, Cov andE¯are statistics of alignment errors calculated across the entire gallery faces, Emahis the alignment error of a gallery face and a probe face. Eq. (5) shows that Emahis actually the square of the Mahalanobis distance of E. By using the Mahalanobis distance, the correlations between patches are taken into account in the alignment error.The number of matched keypoints calculated in Section 3 represents the similarity of two faces, while the alignment error in Section 4 corresponds to the dissimilarity between two faces. We combine these two factors to calculate the similarity index of two faces as follows:(6)Si=λMimaxi(Mi)−(1−λ)Eimaxi(Ei)where Miand Eiare the number of matched keypoints and the alignment error between the probe face and gallery face, i, respectively. Miand Eiare normalized to [0,1] by dividing them with the maximum value of Miand Eiamong all the subjects. λ is the weight for these two factors. In general, the proposed MVV–AE face recognition method that is based on matching and the alignment error can be summarized as follows:Algorithm: MVV–AE similarityPre-computation:1.Learn a series of warp parameters,Φ1,Φ2,…,ΦM, betweenMdifferent poses from a frontal face usingMstack images; each stack image is from the same pose.2.For each frontal face in the gallery, we generateMvirtual views based on the learned warp parameters.3.Compute the SIFT keypoints of theseMvirtual views and store them in a keypoint database.Recognition:1.For each probe face, detect the SIFT keypoints and compare these keypoints with keypoints of each subject with the same orientation in the keypoint database. The number of matched keypoints is obtained.2.For each probe face, calculate the alignment error with each subject using the correlated two-phase alignment error by Eq. (5).3.The similarity index between a probe face and gallery faces is calculated using both the number of matched keypoints and the alignment error using Eq. (6).4.The gallery face that has the maximum similarity index in relation to the probe face is considered a matched face.

@&#CONCLUSIONS@&#
