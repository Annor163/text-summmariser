@&#MAIN-TITLE@&#
Direct diffeomorphic reparameterization for correspondence optimization in statistical shape modeling

@&#HIGHLIGHTS@&#
We propose an approach for optimizing shape correspondence across a population.B-splines are used for shape representation and reparameterization.The quality measure of the statistical shape model is the description length.An adjoint method for deriving analytical sensitivity is developed.The approach improves shape correspondence in a group-wise manner.

@&#KEYPHRASES@&#
Statistical shape model,Shape correspondence,Direct reparameterization,Adjoint method,

@&#ABSTRACT@&#
In this paper, we propose an efficient optimization approach for obtaining shape correspondence across a group of objects for statistical shape modeling. With each shape represented in a B-spline based parametric form, the correspondence across the shape population is cast as an issue of seeking a reparameterization for each shape so that a quality measure of the resulting shape correspondence across the group is optimized. The quality measure is the description length of the covariance matrix of the shape population, with landmarks sampled on each shape. The movement of landmarks on each B-spline shape is controlled by the reparameterization of the B-spline shape. The reparameterization itself is also represented with B-splines and B-spline coefficients are used as optimization parameters. We have developed formulations for ensuring the bijectivity of the reparameterization. A gradient-based optimization approach is developed, including techniques such as constraint aggregation and adjoint sensitivity for efficient, direct diffeomorphic reparameterization of landmarks to improve the group-wise shape correspondence. Numerical experiments on both synthetic and real 2D and 3D data sets demonstrate the efficiency and effectiveness of the proposed approach.

@&#INTRODUCTION@&#
A statistical shape model (SSM) provides a compact characterization of the shape variability in a set of shapes. It was initially used as a tool for facilitating automatic image segmentation  [1,2]. It has since seen many other applications including facial recognition  [3], computer animation  [4], medical diagnosis  [5,6], patient-specific modeling  [7–10] and human body modeling  [11], to name but a few. Finding correspondence across all shape instances is a fundamental task in building SSM. Manual identification of landmarks is effective under some circumstances but in general is not a reliable strategy since it tends to be subjective, time-consuming, error prone, and difficult to be applied in large scale data sets  [12]. Consequently, methods for automatically identifying the shape correspondence have been a major research focus in the field.The automatic identification of the shape correspondence across a set of objects can be achieved by manipulating correspondence either in the object space or in the parameter space. Thus far, one of the most common approaches to achieving shape correspondence is through deforming in the object space a template shape to each shape instance in the training set, and the found pairwise correspondences are then propagated through the common template reference to form the group-wise correspondence. Group-wise registration has also been developed  [13]. In such a deformation based correspondence manipulation approach, the deformed landmarks may not actually lie on the object shape before the optimization convergence is achieved. Further, the deformation usually reduces to a problem where a “similarity” measure between the template and each shape instance is minimized and some regularization constraints are satisfied. Typically such a measure is related to geometric descriptors such as spatial distance and shape feature, and the optimization is essentially a rigid or non-rigid registration problem  [14–17]. However, these geometric descriptors and the regularization constraints are not necessarily a good basis for correspondence.An alternative is to manipulate the correspondence through reparameterizing the shapes in the parameter space. For example, in  [18], objects of spherical topology are mapped to a sphere and correspondence is manipulated through concatenations of symmetric theta transformations on the spherical map. Reparameterization of shapes in the parameter space thus allows convenient manipulation of correspondence of surface points by simply changing point parameters. Reparameterized points always lie on the object shape during the optimization process. As such, the reparameterization lends itself to a more principled approach for establishing correspondence: optimizing the quality of the resulting statistical models. During the past few years, SSM quality measures have evolved from the model covariance trace  [19], to the model covariance determinant  [20], and finally to the Description Length (DL)  [18,21,22] and its simplification  [23] or variants  [24]. This information theoretic objective function of description length has shown to be an effective measure  [21] for the population-based correspondence optimization.Although the population-based approach to shape correspondence does not require the pre-selection of a template and tends to provide a more faithful characterization of the variability pattern, this approach is still far from being widely used to build SSMs due to its low efficiency in identifying optimal correspondence across the shape population. In the minimum description length based group-wise correspondence optimization approach originated in  [21], the group-wise shape correspondence search consists of successive small-scale optimizations, each of which uses only a few optimization variables to relocate landmarks in a local region of each shape instance. In each optimization, only landmarks in local regions are moved. This necessitates a huge number of successive optimizations to manipulate all the landmarks, thus leading to inefficiency. Some researchers use analytical gradient formula whenever possible to speed up the gradient evaluation  [25,26]. However, in these implementations, the landmark positions in the training set shapes are non-differentiable with respect to optimization variables, the gradients are thus only partially analytical. In  [27], spline representation of 2D shapes is proposed so a full analytical gradient of reparameterization can be derived.In our proposed approach, we cast shape correspondence as an issue of seeking optimal reparameterizationD(u)of the parametric fielduof each shape so that a quality measurefof the resulting shape correspondence across a group of objects is optimized. The reparameterization is applied to the parametric domain of parameterized curves or surfaces. Our SSM is based on the point-distribution model  [28]. In our approach, each landmark pointS(u)in a given shape is changed toS(D(u))in order to improve correspondence via the reparameterizationD(u). Our approach thus requires the parameterization of each shape, that is, every pointxof the shape in the physical space is mapped to a pointuin the parametric domain. In our implementation, we choose the B-spline representationS(u)of each shape instance, which can be reconstructed from triangular mesh representation of 3D objects. The parametric domain then undergoes a reparameterization represented via another tensor-product B-splineD(u)with B-spline coefficientsbas the optimization parameters. We choose the description length as the objective function of the shape correspondence.Fig. 1illustrates the proposed idea. A group of hand contours are shown in Fig. 1(a). Each shape is represented with B-splines, and Fig. 1(b) shows such a B-spline representation for one shape with control points and knots. Initially landmarks are uniformly sampled over the parameter domain of the B-spline shapeS(u)as shown in Fig. 1(c). To change the landmark positions, reparameterizationD(u)is applied to the parameter domain of each B-spline shape. This reparameterization is also represented with B-splines as shown in Fig. 1(d) where each red point represents a B-spline coefficient for the reparameterization. The landmarks are redistributed as shown in Fig. 1(e) after the reparameterization. The landmark redistribution can be seen from the four highlighted landmarks, wherea,b,c,dmoved toA,B,C,Drespectively over the other side of the finger tips.The salient feature of this approach is as follows:•Diffeomorphic through B-splines. Existing technique  [18] for reparameterization concatenates a series of simple homeomorphic mappings. One optimization run with this reparameterization technique leads to the deformation of a local parametric region and it cannot provide any information on the search direction for subsequent local deformations in other regions. Therefore it requires the concatenation of a large number of simple mappings and causes severe inefficiency (see Section  5.1). Instead of concatenation of many local mappings, we propose the use of single B-spline function to directly represent the diffeomorphic reparameterizationD(u)for the parameterizationuof each shape instanceS(u). The injectivity for the reparameterization is guaranteed by enforcing the Jacobian positivity constraint.Full differentiability of the objective functionf(i.e. description length) with respect to reparameterization variablesb. The objective functionf(i.e. description length) is a function of landmark positions. The landmark positions in each shape are differentiable with respect to reparameterization parametersbdue to the parametric representationS(u)of each shape and diffeomorphic reparameterizationD(u). This ensures that the description length is fully differentiable with respect to the reparameterization variablesb.Constraint aggregation. The B-spline based diffeomorphic reparameterization leads to a large number of constraints on Jacobians for ensuring the mapping is bijective. In order to facilitate fast convergence in gradient-based optimization, a constraint aggregation technique is used where the large number of constraints are aggregated into one or a few constraints.Adjoint method for computing sensitivity. The adjoint approach is used to compute the sensitivity of the objective function with respect to reparameterization parametersb, which is more efficient than direct differentiation of the objective functionf. In computing the sensitivity of the description length w.r.t. optimization variablesb, eigenvalues of the covariance matrix and their derivatives are needed. Since each eigenanalysis is expensive, the adjoint method is thus especially efficient for computing the sensitivity in this kind of optimization problems that have larger number of optimization variables and fewer functions (after constraint aggregation). In this adjoint method, the derivatives of a function w.r.t. a large number of optimization variables only involves one eigenanalysis of the covariance matrix. On the other hand, in the direct differentiation method, the number of eigenanalysis is the same as the number of optimization variables.The remainder of this paper is organized as follows. Section  2 reviews the basic computing procedures in statistical shape modeling and the role of diffeomorphic reparameterization in SSM. Section  3 details our proposed approach on direct diffeomorphic reparameterization for shape correspondence. Section  4 presents the developed optimization techniques with experimental results in Section  5. This paper is concluded in Section  6.In this section, we review the basic computing procedures in statistical shape modeling and the role of diffeomorphic reparameterization in SSM.Statistical Shape Model was initially called Point Distribution Model (PDM)  [28], and it requires each shape instance in the training set be represented by a set of points, known as the landmarks   [19]. Regardless of the geometric form of the training set, landmarks are constrained to be on the boundary of a shape instance, and they form a point-based representation that approximates the original shape  [29]. The statistical modeling framework requires that the same number of landmarks sampled on all shapes across the training set.Suppose a training set{Ti}(i=1,2,…,nS)comprisesnSshape instances and each is represented bynPlandmarks. Thej-th landmark of thei-th instance isxi(j)=[x(j),y(j),z(j)]iT∈R3. Owing to the correspondence assumption of landmarks, all thenSlandmarks{xi(j)}(i=1,2,…,nS)with labeljshould correspond across all instances. The landmark representation of each instance is usually written into a concatenation ofnPlandmarks ordered by labels as a shape vector expression:xi≐[xi(1),yi(1),zi(1),xi(2),yi(2),zi(2),…,xi(nP),yi(nP),zi(nP)]T.All thenSshape vectors{xi}could be concatenated into a3nP×nSshape vector matrix:(1)XS≐[x1,x2,…,xnS].In order to align a group of shapes stored inXS, the Generalized Procrustes Analysis (GPA)  [30] operation is conducted, denoted by an alignment operator as below(2)XA=A(XS).More specifically, the GPA of group-wise alignment is done by iteratively performing the pair-wise Procrustes Analysis (PA) between each shape and the mean. The PA brings a shape vectorxto a fixed shape vectoryby similarity transformationmint,s,R‖y−sR(x−t)‖2≐∑j=1nP‖y(j)−sR(x(j)−t)‖2where shape irrelevant factors including translationt, scalingsand rotationRare removed. For more details in the iterative procedure, refer to Algorithm 2.1 in  [31].If the training shapes are continuous and thei-th shape is parameterized by the mappingSi(u), the continuous representation of the covariance matrix expressed in the(μ,ν)-th entry is(3)Eμν=1(nS−1)A∫[Sμ(u)−S̄(u)]⋅[Sν(u)−S̄(u)]dA(u),whereS(⋅)is the vector-valued function that defines the continuous representation of thei-th shape by mapping the parameter space to the physical space.S̄(u)is the mean shape defined as follows:S̄(u)≐1nS∑i=1nSSi(u).Ais the surface area of the mean shape. For numerical implementation, the continuous covariance matrix is obtained via discretization through a set of discrete landmarks as(4)Eμν=1(nS−1)nP∑i=1nS(xi−x̄)μ(xi−x̄)νwhere on each shape thei-th landmarkxicould be obtained as sampling at thei-th parameter pointui: i.e.xi=S(ui).This could be written simply in a matrix form  [21](5)E=1(nS−1)nPXcTXc,whereXcis defined by(6)Xc≐[x1A−x̄,x2A−x̄,…,xnSA−x̄],andxiAis thei-th shape vector after alignment operationA, i.e. the component ofXAin (2); the mean shape vector of SSM is(7)x̄=1nS∑i=1nSxiA.The Principal Component Analysis (PCA)  [32] is then conducted to extract the principal modes of shape variability via the eigenvalue decomposition of the covariance matrix(8)Evm=λmvm(m=1,2,…,nS−1),wherevmis them-th eigenvector andλmthe corresponding eigenvalue.The mean shapex̄, modes{vm}and variances{λm}​constitute the statistical shape model. This statistical model is a much more compact representation of the shape variability pattern of the implied shape class than the original training set. What is more, it makes possible to represent any valid instancexbelonging to the shape class by a linear approximation using only the firstm˜(m˜≤nS−1)modes(9)x≈x̄+∑m=1m˜βmvm,where them-th mode parameter is found by projection(10)βm=(x−x̄)Tvm.The quality of the linear approximation has a great influence on the utility of the statistical model in subsequent applications, and it is evidently decided by the quality of the SSM, which is directly tied to the quality of the groupwise correspondence.The PCA step defined by (5) and (8) is compactly written as(11)λ=C(XA).The objective function of Description Length was originally derived by Davies in  [21] and elaborated in  [29]. A simplified version presented in  [23] defined as below is used in our correspondence optimization:(12)f≐∑m=1nS−1Lm,where each mode’s contribution isLm={1+log(λm/λcut)λm≥λcut,λm/λcutotherwise .The thresholdλcutis determined by landmark resolution and shape scale(13)λcut=2lminrmax,wherelminis the smallest edge length in the landmark-based representation andrmaxis the radius of largest circumscribing sphere over training set shapes.In this section, we present how B-splines can be used for representing reparameterization of parametric curves and surfaces and be used for manipulating shape correspondence. We show how such single direct reparameterization function differs from concatenations of multitude of simple mappings into one reparameterization function. We then show constraints for ensuring the B-spline based reparameterization is diffeomorphic.Since our correspondence manipulation is based on reparameterization of parametric curves and surfaces, the training shapes must be in the form of parametric curves or surfaces. In this paper, we choose to use B-splines to represent the shapes. A B-spline curve of degreedis defined byS(u)=∑k=0nBk,dPk0≤u≤1;whereBk,dis the B-spline basis function  [33] of degreedassociated with thek-th control pointsPkrecursively defined on a non-decreasing knot vectorΞ={ξ̄0,ξ̄1,…,ξ̄n+d+1}as(14)Bk,d(u)=(u−ξ̄i)Bk,d−1(u)ξ̄k+d−ξ̄k+(ξ̄k+d+1−u)Bk+1,d−1(u)ξ̄k+d+1−ξ̄k+1,Bk,0(u)={1ξ̄k≤u≤ξ̄k+1,0otherwise .A B-spline surface of degreedandeis defined byS(u,v)=∑k=0nu∑l=0nvBk,dBl,ePk,l0≤u≤1,0≤v≤1;whereBk,dandBl,efollowing the basis definition in (14) is associated with the(k,l)-th control pointPk,l, and the knot vectors along the two parametric directions areΞ={ξ̄0,ξ̄1,…,ξ̄nu+d+1}andH={η̄0,η̄1,…,η̄nv+e+1}.We show below how B-splines can also be used for representing the reparameterization of parametric curves and surfaces.Fig. 2shows the reparameterization of a 2D curve where points sampled onuhave been moved toD(u). Such a reparameterization functionD(u)can be represented with the concatenation of simple mappings. For example, the reparameterization functionD(u)shown in Fig. 2 is represented in Fig. 3(a) with 4 Cauchy kernels (centered atc1,c2,c3,c4) which are sequentially superimposed and integrated intoD(u)as proposed by  [29]; see Appendix I for details regarding Cauchy kernel and its concatenation.In this paper, we propose to directly representD(u)as a single B-spline function as shown in Fig. 3(b) where empty circles represent B-spline coefficients{bi}. The reparameterization functionD(u)for a parametric curve can be represented by a B-spline withnbcoefficients(15)D(u)=∑i=0nb−1Bi,p(u)bi,0≤u≤1;whereBi,pis the B-spline basis function  [33] of degreepassociated with thei-th B-spline coefficientbi, and it is recursively defined on a non-decreasing knot vectorU={ū0,ū1,…,ūn+p+1}.We assume that the starting and ending points of all curves are already in correspondence. With this assumption, the boundary of the parameter domain (two ends atu=0,1) is fixed even with the reparameterization function. Therefore, we use a clamped knot vector (i.e. repeating the first(p+1)and last(p+1)knots) andb0=0,bn=1for representing the reparameterization so thatD(0)=0andD(1)=1.A reparameterization of a 3D surface is illustrated in Fig. 4, where cubes and spheres respectively represent two sampled points before and after the reparameterization. Since a 3D surfaceS(u)=[x(u),y(u),z(u)]is mapped to a 2D parametric domain, i.e.u=(u,v), the reparameterizationD(u)for 3D surfacesS(u)​has two components inuandvdirectionsD(u)=[Du(u,v),Dv(u,v)]. This reparameterization function could also be visualized by a vector field in Fig. 4(c), and its parametric grid is shown in Fig. 4(d).Such a reparameterization field can be represented by the concatenation of simple mappings. For example, two Clamped Plate Spline (CPS) warps  [29] are applied sequentially in the parameter domain shown in Fig. 5(a) and (b). These two CPS warps are with centersc1andc2and red dotted circles as the CPS range. See Appendix II for details on CPS warps.Instead of using concatenation of simple mappings, we propose the use of single B-spline functions to directly represent the reparameterization ofuandv​components of the parametric domain, as shown in Fig. 5(c) where8×8B-spline coefficientsbi(red circles) are used to represent the reparameterization fieldD(u). In general, the reparameterizationD(u)=[Du(u,v),Dv(u,v)]in the square planar parameter domain is defined as(16)Du(u,v)=∑i=0nb1−1∑j=0nb2−1Bi,p(u)Bj,q(v)bi,ju,Dv(u,v)=∑i=0nb1−1∑j=0nb2−1Bi,p(u)Bj,q(v)bi,jv,0≤u,v≤1;whereBi,pandBj,qare the B-spline basis functions (14) of degreepandqassociated with the(i,j)-th B-spline coefficient 2-tuplebi,j=(bi,ju,bi,jv); the coefficient number along theu- andv-direction arenb1andnb2respectively. They are respectively defined on two sets of non-decreasing knot vectorsU1andU2.The four boundaries of all the shapes are assumed to be in correspondence already. With this assumption, the four boundaries of the square parametric domain are fixed during reparameterization, i.e.(17)Du(0,v)=0,Du(1,v)=1,Dv(u,0)=0,Dv(u,1)=1.Therefore, the two knot vectors are chosen to be of clamped type and the B-spline coefficients at the four boundaries are either 0 or 1.SettingdD(u)/du>0in (15) gives the bijectivity constraint for diffeomorphic reparameterization of curves for correspondence manipulation. SinceD(u)is a degree-pB-spline function, its derivative is a degree-(p−1)B-spline function withbi+1−bias B-spline coefficients  [34], we thus have the following explicit constraint for ensuring diffeomorphic reparameterization of curves(18)bi−bi+1<0,i=0,1,…,nb−2.The bijectivity of reparameterization can be guaranteed by the positivity of Jacobian throughout the parameter domain, i.e.(19)J(D(u))=|∂Du(u,v)∂u∂Du(u,v)∂v∂Dv(u,v)∂u∂Dv(u,v)∂v|>0,∀(u,v)∈[0,1].Assuming there arenb1×nb2B-spline coefficient tuplesbi,jfor representing the reparameterization in (16). Due to the boundary constraint (17), there are only(nb1−2)×(nb2−2)interior coefficientsbi,jthat can be used to manipulate correspondence for each shape. Consequently, there are2×(nS−1)×(nb1−2)×(nb2−2)optimization variables fornSshapes. To ensure the reparameterizationD(u)is diffeomorphic, the bijectivity condition (19) that prevents the self-intersection of the parametric field can be cast as constraints on the reparameterization parameters, i.e. interior B-spline coefficient tuplesbi,j.Diffeomorphism via constraints in the B-spline form of Jacobian. We give below a sufficient condition for ensuring the Jacobian fieldJ(u)(19) is positive so that the reparameterizationD(u)is diffeomorphic. The scalar Jacobian fieldJ(u)defined in (19) for the B-spline based reparameterization (16) consists of derivatives of B-splines (piecewise polynomials) and thus remains piecewise polynomials. Therefore,J(u)itself can be cast in the B-spline form as described in  [35,36]. More specifically, the JacobianJ(u)can be expressed as(20)J(u,v)=det[∂D(u,v)∂u∂D(u,v)∂u],∂D(u,v)∂u=∑i=0nb1−2∑j=0nb2−1Bi,p−1(u)Bj,q(v)γi(bi+1,j−bi,j),∂D(u,v)∂v=∑k=0nb1−1∑l=0nb2−2Bk,p(u)Bl,q−1(v)ηl(bk,l+1−bk,l),where:γi=pūp+i+1−ūi+1;ηl=qvq+l+1−vl+1. Using the notationΔbi,ju=(bi+1,j−bi,j)andΔbk,lv=(bk,l+1−bk,l)and noticing that the product of two B-splines is a higher-degree B-spline  [37], the Jacobian could be written as the following B-spline form(21)J(u)=J(u,v)=∑i=0nb1−2∑j=0nb1−1∑k=0nb1−1∑l=0nb2−2Bi,p−1(u)Bj,q(v)Bk,p(u)Bl,q−1(v)γsηldet[Δbi,juΔbk,lv]=∑s=02nb1−3∑t=02nb2−3Bs,2p−1(u)Bt,2q−1(v)Js,tBSP({b}),whereJs,tBSPis the B-spline coefficient of the B-spline form of the JacobianJ(u)for the reparameterizationD(u)in (16); eachJs,tBSPis a function of{b}. The above B-spline form of Jacobian leads to the following sufficient condition for ensuring the reparameterizationD(u)is diffeomorphic. Due to the non-negativeness of B-spline basis functions,J(u)>0when every B-spline coefficient in (21) is positive, i.e.(22)Js,tBSP({b})>0,s=0,1,…,nJ1−1;t=0,1,…,nJ2−1,wherenJ1=2nb1−2andnJ2=2nb2−2are the number of B-spline coefficients of the B-spline form of Jacobian along theu- andv-direction; thus there are totalnJ=nJ1nJ2=4(nb1−1)(nb2−1)positivity constraints.The above condition is a sufficient, but not necessary condition. In order to make the condition less conservative, one can extract Bézier patches out of the B-spline representation and obtain more tighter bound of the Jacobian based on Bézier coefficients, as suggested in  [35,36]. Such Bézier extraction involves the following intermediate steps:1.Decompose B-spline of control coefficients{b}into Bézier patches (each patch with control coefficients{b˜}) by knot insertion algorithms elaborated in  [34].Find the Bézier representation of the Jacobian of the Bézier patches as directed in  [38].Repose the Jacobian representation of Bézier patches to form the Jacobian B-spline ofC0inter-patch continuity with control coefficients{Js,tBEZ}.Jacobian constraints on landmarks. A simple alternative to the above rigorous diffeomorphic conditions is to enforce Jacobian positivity at a finite set of parameter points that correspond to landmarks. The evaluated Jacobian values are all functions of the B-spline coefficients{b}computed by (20). The parameter points chosen are usually those associated with the parametric locations of the landmarks. Suppose there arenJ1=nP1andnJ2=nP2landmarks along theu- andv-direction respectively, and the(s,t)-th parameter point inus,t. The constraints can be formulated as(24)Js,tB=J(us,t,{b})=det[∂D(us,t)∂u∂D(us,t)∂u]>0s=1,…,nJ1;t=1,…,nJ2,wherenJ1andnJ2are the number of sampled Jacobians along theu- andv-direction, in this case equaling the landmark number along each direction; thus there are totallynJ=nJ1nJ2=nP1nP2positivity constraints. Although the Jacobian between two landmarks with positive Jacobians is not necessarily positive in theory, we find that in all examples reported in this paper Jacobians are all positive in between with the sufficient landmark resolution chosen. This alternative gives better efficiency than enforcing the constraints in the B-spline form of Jacobian, as shown in Section  5.It should be noted that the proposed B-spline based reparameterization is based on Free Form Deformation(FFD)  [39]. FFD based techniques have been used extensively, for example, in matching CT/MRI images (image registration) in a multitude of medical imaging applications  [40–43]. The differences in our approach are the following: it is the parametric domain, rather than physical surfaces, that is deformed; our formulation for enforcing diffeomorphic is different; our approach for enforcing the constraints via aggregation shown in next section is also different. It should be noted that splines have been used in landmark matching, e.g. in  [44,45] where the deformation is driven by energy minimizing cost function with various kernels and the diffeomorphism is guaranteed by the flow solutions to ODE. It should also be noted that diffeomorphisms illustrated in for example Fig. 2(b) are akin to fingerprints observed in  [46]. There are also alternative for diffeomorphic reparameterization for curves as reported in  [47–49].With the above B-spline representation of reparameterization functionsD(u)and the diffeomorphic conditions, we thus have the following optimization formulation for using B-spline based reparameterization for manipulating shape correspondence:(25a)minbf(b)=∑λi≥λcut[1+logλk(b)λcut]+∑λk<λcutλk(b)λcut(25b)s.t.[CT(b)C(b)]vk(b)=λi(b)vk(b)(25c)vkT(b)vk(b)=1,k=1,…,nS(25d)gl(b)<0,l=1,…,nG.In this formulation,bis the set of optimization variables and represents the collection of interior B-spline coefficient tuplesbforns−1shapes, where one shape from the training set is selected as a reference. The objective functionf(b)is the simplified description length, which is a function of eigenvalues computed from (25b) and (25c). The matrixCis related to the covariance matrixEbyE=CTCwithC=Xc(nS−1)nP.The constraint (25d) represents the diffeomorphic conditions, i.e.  (18) for curves and (23), (24) for surfaces, each of which is a function of optimization variablesb.The optimization formulation given in (25) leads to a large-scale optimization problem. For SSM of 3D surfaces, there are2×(nS−1)×(nb1−2)×(nb2−2)B-spline coefficients as optimization variables withnGconstraints based on (23), (24), which will be detained in Section  4.3.1.In order to efficiently obtain optimized shape correspondence, we have developed a gradient-based optimization approach. We have derived analytical gradient of the cost function (25a) with respect to optimization variablesbwith both direct differentiation and the adjoint sensitivity method. We have also developed a technique to approximate the constraints and to aggregate the large number of Jacobian constraints into one constraint in order to speed up the convergence.Analytical gradient provides an efficient and accurate mean to obtain gradient for optimization. It is especially important in large-scale optimization problems where the finite difference based approach for computing gradient would be inefficient. It turns out the analytical gradient can be derived for all the differentiable intermediate steps since all steps in our formulation, including reparameterization, sampling, alignment, PCA and DL computation are differentiable, the gradient product gives the analytical objective gradient due to the chain rule as follows:(26)dfdbr=∑i∂f∂λi∑j∂λi∂xjA∑k∂xjA∂xkS∑l∂xkS∂D(ul)∑r∂D(ul)∂brwhereD(ul)is thel-th reparameterized landmark point in the parameter domain,xkSis thek-th landmark in the physical domain andxjAis thej-th aligned landmark. Among them the∂f∂λi,∂λi∂xjAand∂xjA∂xkSare inherently differentiable, and∂D(ul)∂bris also differentiable as long as a differentiable reparameterization technique such as (15) and (16) is used. The analytical gradient∂xkS∂D(ul)requires the differentiability of the geometric representation of training set shapes. In this paper, we use the quadratic B-spline ofC1smoothness to represent the shapes in the training set. Therefore, full analytical gradients can be derived.The total sensitivity from (25a) is just(27)df(b)dbr=∑i∂f[b,λ(b)]∂λi∂λi(b)∂brwherer=1,…,nD; the optimization variable numbernD=(nb−2)(nS−1)for curves andnD=2(nb1−2)(nb2−2)(nS−1)for surfaces.Under the state equation formulation in (25), we can obtain the objective sensitivity in (27) with either the direct method or the adjoint method, giving rise to the direct sensitivity and adjoint sensitivity. The adjoint sensitivity is significantly faster than the direct sensitivity. We provide the derivations for both to highlight the characteristics of the adjoint method.The direct sensitivity computes the gradient of the cost function by directly differentiating the cost function (25a). That is,(28)∂f(b)∂br=∑m=1nS∂f∂λm∂λm∂br.The term∂λm/∂br(m=1,…,nS)is obtained by differentiating Eqs. (25b) and (25c) with respect tobr, which leads to the following resulting linear system(29)[0vmTvmλmInS−CTC][∂λm∂br∂vm∂br]=[0∂(CTC)∂brvm].The linear equation system can be solved to obtain∂λm/∂brand∂vm/∂br. Plugging∂λm/∂brinto (28) yields the desired gradient.In order to avoid the direct computation of∂λm/∂brand∂vm/∂br, we introduce a Lagrangian quantity by augmenting the objective function (25a) with2nSsets of constraints (25b) and (25c) as(30)L=f(λ)+∑m=1nSμmT(CTCvm−λmvm)+∑m=1nSνm(vmTvm−1)where adjoint variablesμmandνm(m=1,…,nS)are the Lagrange multipliers.Differentiation of (30) w.r.t.brgives∂f∂br=∂L∂br=∑m=1nS∂f∂λm∂λm∂br+∑m=1nSμmT(∂(CTC)∂br−∂λm∂brInS)+∑m=1nSμmT(CTC−λmInS)∂vm∂br+∑m=1nS2νmvmT∂vm∂br,which could be simplified to(31)∂f∂br=∑m=1nSμmT∂(CTC)∂brvm+∑m=1nS(∂f∂λm−μmTvm)∂λm∂br+∑m=1nS[μmT(CTC−λmInS)+2νmvmT]∂vm∂br.The key idea of the adjoint method is to circumvent the direct computation of the sensitivity of the state variablesλwith respect to optimization variablesb. A more general discussion on the adjoint method is available in  [50–52]. Specifically in this situation, in order to bypass the direct calculation of∂λm∂brand∂vm∂brin (31), their coefficients in (31) are set to zero. This is possible since the arbitrariness of adjoint variablesμmandνm. This therefore leads to the linear adjoint equations below for solving these adjoint variables(32){∂f∂λm−μmTvm=0,μmT(CTC−λmInS)+2νmvmT=0.The above equation can be rearranged into the following linear system(33)[vmT0CTC−λmInS2vm][μmνm]=[∂f∂λm0]from which the adjoint variablesμmandνmcan be solved. The sensitivity in (31) can then be obtained through the following simplified expression(34)∂f∂br=∑m=1nSμmT∂(CTC)∂brvm.It is worth noting that, with the direct method, the linear system (29) is solved for each optimization variablebr,r=1tonD, and every eigenvalueλm. On the other hand, with the adjoint method, the linear system (33) is solved just once for every eigenvalueλm. Numerical examples in Section  5.2 are presented to demonstrate the accuracy and efficiency of the adjoint sensitivity.Based on the description in Section  3.2.2, we consider the two options: (a) Jacobian B-spline (via Bézier implementation) (23); and (b) sampled Jacobian (24). In either situation, the large quantity of originalnJconstraints is rather undesirable for the optimization with already many optimization variables. For the purpose of reducing the number of constraints while not sacrificing the differentiability of constraint functions, the constraint aggregation [53,54] technique is employed here. The Kreisselmeier–Steinhauser (KS) function  [55] is used here and we choose to aggregate all thenJ​Jacobian constraints for each shape into one single constraint to be applied as the optimization constraints in (25d), yielding totallynG=nS−1constraints fornS−1shapes as(35)gl(b)=1Kln[∑z=1nJe−KJz({b}l)]<0,l∈1,..,nS;l≠iRwhereJzis original Jacobian positivity, being eitherJs,tAin (23) for the Jacobian B-spline constraint orJs,tBin (24) for the direct sampled Jacobian constraint; the linearly ordered index is obtained byz=s+(t−1)nJ1.iRis the reference shape index whose parameter domain is not reparameterized and landmarks are fixed.{b}lare just the reparameterization B-spline control coefficients for thel-th shape. ParameterKis a control parameter here chosen to beK=15.It could be shown that this new constraint is more conservative than the original constraints. We have found that the two options of Jacobian positivity constraints differ only slightly as long as the landmark resolution of the direct sampled Jacobian constraint option is sufficiently large.For a gradient based optimization approach, both the gradient of the cost function and the constraints with respect to optimization variables are needed. The gradients of the objective function have been discussed in great detail in the previous section. The analytical gradients of the inequality constraints based on (35) can also be derived. The gradient ofl-th inequality constraint w.r.t. ther-th optimization variablebris(36)∂gl∂br=−1∑z=1nJe−KJz({b}l)∑z=1nJe−KJz({b}l)∂Jz({b}l)∂brwhere the Jacobian gradient∂Jz({b}l)∂brwhenJs,tAandJs,tBcan be evaluated easily from (23) and (24).

@&#CONCLUSIONS@&#
