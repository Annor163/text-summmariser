@&#MAIN-TITLE@&#
Exactly satisfying initial conditions neural network models for numerical treatment of first Painlevé equation

@&#HIGHLIGHTS@&#
A new stochastic intelligence method is developed to solve first Painlevé equation.Design of three unsupervised ANN models that satisfying exactly initial conditions.Optimization capability of SQP is exploited for training of design parameter of ANNs.Accuracy and convergence are validated in term of various performance criterions.Impact on effectiveness of the models is investigated by varying neurons in ANNs.

@&#KEYPHRASES@&#
Painlevé transcendents,Neural network,Sequential quadratic programming,Nonlinear differential equations,Activation functions,

@&#ABSTRACT@&#
In this paper, novel computing approach using three different models of feed-forward artificial neural networks (ANNs) are presented for the solution of initial value problem (IVP) based on first Painlevé equation. These mathematical models of ANNs are developed in an unsupervised manner with capability to satisfy the initial conditions exactly using log-sigmoid, radial basis and tan-sigmoid transfer functions in hidden layers to approximate the solution of the problem. The training of design parameters in each model is performed with sequential quadratic programming technique. The accuracy, convergence and effectiveness of the proposed schemes are evaluated on the basis of the results of statistical analyses through sufficient large number of independent runs with different number of neurons in each model as well. The comparisons of these results of proposed schemes with standard numerical and analytical solutions validate the correctness of the design models.exact solution of first Painlevé equationreference results with fully explicit Runge–Kutta methodapproximate neural networks solutionapproximate solutionuˆ(x)using log-sigmoid activation functionapproximate solutionuˆ(x)using radial basis activation functionapproximate solutionuˆ(x)using tan-sigmoid activation functionapproximate neural networks solution with exactly satisfying initial conditionsapproximate solutionu˜(x)using log-sigmoid activation functionapproximate solutionu˜(x)using radial basis activation functionapproximate solutionu˜(x)using tan-sigmoid activation functionapproximate analytical solutionapproximate solution y(x) with Homotopy perturbation methodapproximate solution y(x) with variational iterational methodapproximate solution y(x) with modified Homotopy perturbation methodthe root mean square error associated with differential equationobjective or fitness functionglobal root mean square error associated with the differential equationglobal mean square errormean fitnessunknown weight vector

@&#INTRODUCTION@&#
The design of computational intelligence algorithms by exploiting the strength of feed-forward artificial neural networks (ANNs) have been widely used to solve variety of initial and boundary value problems (BVPs) associated with linear and nonlinear differential equations [1–4]. For example, numerical solvers based on ANN optimized with global and local search algorithms have been employed to solve the nonlinear oscillatory system for both stiff and non-stiff variants modeled with second order nonlinear Van der Pol equations [5], magnetohydrodynamic (MHD) problems in fluid mechanic [6], Nonlinear Jeffery-Hamel flow problem for both convergent and divergent channels [7], fluid flow problems [8], the nonlinear Schrodinger equations [9], two dimensional nonlinear Bratu's problems [10–12], Troesch's problem [13,14], Fuel ignition model in combustion theory [15,16], nonlinear singular systems based on Lane Emden flower equations [17], and nonlinear BVPs governed with pantograph functional differential equation [18]. A survey article [19] is a good source of reference that presents history, current trends and future development of research scenarios in this field. The applicability domain of such solvers has also been extended to solve effectively the linear and nonlinear ordinary fractional differential equations [20,21]. For example, the Riccati differential equation of arbitrary order and the fractional system of Bagley-Torvik equation [22,23] are other considerable contributions. Besides these applications of soft computing techniques based on neural networks models for nonlinear systems governed with differential equations, many other problems arising in different field addressed with neural networks such as binary image denoising [24], control of nonlinear dynamic systems [25], detection of glaucomatous damage [26], stock market prediction [27], implementation of an intelligent and video-based fall detection system [28], Retinal vessel segmentation [29], etc. All these motivation factors for authors to explore in applications of soft computing approaches based on neural networks for solving strong nonlinear system associated first Painlevé equation.In this paper, artificial intelligence framework have been constructed for finding the solution of first PE using ANN model based on log-sigmoid, radial basis and tan-sigmoid function in the hidden layers that by default assure the initial conditions exactly. The training of weights of the given models has been made with sequential quadratic programming (SQP) method that considered to be the best in the class for constraint optimization problems. The dominance of SQP is well recognized since its introduction by Nocedal and Wright [30] in term of efficiency, accuracy and percentage of successful solutions over a large number of optimization scenarios. For instance, few recent applications can be seen in [31,32] and good source of reference material for SQP methods can be found in [33,34]. The accuracy, reliability and effectiveness of the proposed models are validated on Monte Carlo simulations and their statistical analysis. The comparative studies of these solvers have been carried out with standard numerical results calculated with fully explicit Runge–Kutta methods using built-in routines in MATHEMATICA environments, as well as, state of art analytical solvers such as variational iteration, Homotopy perturbation and modified Homotopy perturbation methods [35,36].Organization of rest of paper is as follows, the problem statement and significance of the study has been given in Section 2, proposed methodology based on design of three unsupervised neural networks models along with formulation of fitness evaluation function as well as learning procedures are given in Section 3, results of detail simulation studies for each models by varying number of neuron is presented in Section 4 along with comparison with state of art analytical solvers, comparative study based on sufficient large number runs in term of many performance operators is given in Section 5. Conclusions and future research directions in this domain are given in last section.The aim of this study is to apply intelligence computing approaches based on ANN models to solve the first Painlevé equation (PE). The equation along with initial conditions is written as follows:(1)d2u(x)dx2=6u(x)2+x,u(0)=0,du(0)dx=1The Eq. (1) was introduced by French scientist Painlevé, about one century ago and is the first equation of the set of six second order irreducible differential equations. Presently, the Painlevé equations have been used extensively by the research community for the description of various physical processes, including both pure and applied mathematics [37,38], along with theoretical physics as well [39]. Recently, first PE has been exploited to model the viscous shocks in Hele-shaw flow as well as Stokes phenomena [40], and the existence of Tronquée and hyperasymptotics solutions [41,42]. Furthermore, researcher has addressed different applications in broad fields using first PE [43–46]. Recently, the evolutionary computing and swarm intelligence approaches hybrid with local search algorithms [47–49] are reported to solve the first PE reliably and accurately. In these reported stochastic solvers the neural networks models are developed are unable to satisfy the initial conditions exactly. The analysis of the results is made on data set that arises on the basis of only 10 numbers of neurons in neural networks. All the investigations are carried out with neural networks using log-sigmoid transfer function only. This study is basically the extension of previous works and authors are motivation to explore the possibility for designing different type of neural networks models that satisfying the initial condition exactly. Moreover, the effect of changing the number of neurons in the models on accuracy and convergence is examined for solving the first PE.The advantages of proposed ANNs modes can be summarized as:•ANNs can provide the results on continuous inputs within the trained interval while the traditional numerical methods can provide the results on predefined discrete grid of inputs. In order to get the results on any other inputs the ANN can readily get the result whereas other methods have to perform the whole sequential procedure.The processing of the applications in the real time environment, strength of ANN is the requirement of minimum hardware for storage of optimal weights and processing of necessary mathematics, rather than providing an expensive hardware platform to run any numerical procedure to get results.Analytic solvers including Adomian decomposition, HPM, and VIMs, etc. provide accurate solution only in the close vicinity of the initial guess, as the inputs get wider, it will accumulate the error. Whereas, ANNs methodologies are less prone to these effects.Ease in implementation, simplicity of the concept and wider applicability domain are other salient features of the proposed schemes.With the advent of signal processing platform, availability of sophisticated hardware and software packages, these solvers are worthy to get the reliable and accurate solution of complex mathematical problem even on larger input spans.The detail description of design methodology is presented here in term of mathematical models, construction of fitness function by defining unsupervised error functions, and procedure adopted for training of design parameter of the networks.Mathematical models for first PE are developed by using the following continuous mapping in neural network methodology for nth order derivative, dnu/dxn, of solution, u(t), using log-sigmoid, 1/(1+e−x), radial basis, e−x2, and tag-sigmoid, −1+2/(1+e−2x), transfer functions, and given, respectively, as [47–49]:(2)dndxn(uˆLS(x))=∑i=1mαidndxn11+e−(wix−βi)dndxn(uˆRB(x))=∑i=1mαidndxn(e−(wix−βi)2)dndxn(uˆTS(x))=∑i=1mαidndxn21+e−2(wix−βi)−1where α, w, and β are vectors of real-valued bounded adaptive parameters, m is number of neurons. Approximate neural network models for the solutionsuˆLS,uˆLRanduˆTSalong with their 1st and 2nd order derivatives, i.e., using n=0, 1 and 2 in (2), are written as:(3)uˆLS(x)=∑i=1mαi11+e−(wix−βi)ddx(uˆLS(x))=∑i=1mαiwie−(wix−βi)(1+e−(wix−βi))2d2dx2(uˆLS(x))=∑i=1mαiwi22e−2(wix−βi)(1+e−(wix−βi))3−e−(wix−βi)(1+e−(wix−βi))2(4)uˆRB(x)=∑i=1mαi(e−(wix−βi)2)ddx(uˆRB(x))=∑i=1mαiwi(−2(wix−βi)e−(wix−βi)2)d2dx2(uˆRB(x))=∑i=1mαiwi2(−2e−(wix−βi)2+4(wix−βi)2e−(wix−βi)2)(5)uˆTS(x)=∑i=1mαi21+e−2(wix+βi)−1ddx(uˆTS(x))=∑i=1mαiwie−(wix+βi)(1+e−(wix+βi))2d2dx2(uˆTS(x))=∑i=1m2αiwi28e−4(wix+βi)(1+e−2(wix+βi))3−4e−2(wix+βi)(1+e−2(wix+βi))2The arbitrary combinations of the networks represented in above sets are used to model the nonlinear first PE along with the initial conditions approximately. An extended version of neural network models is constructed that satisfy initial conditions exactly and its is given generally for the solutions y(x), and nth derivative as:(6)u˜(x)=S(x)+T(x)uˆ(x),dnu˜dxn=dnSdxn+dnTdxnuˆ(x)+T(x)dnuˆdxnwhere,uˆ(x)anddnuˆ/dxnare any one of the networks given in (2), respectively, S(x) and T(x) are known functions of inputs x. With appropriate choice of function S(x) and T(x) the initial condition of the first PE are satisfied exactly. For instance, by taking S(x)=x and T(x)=x2 in (4) extended neural networks for solution u(x), first du/dx and second du2/dx2 derivatives are given, respectively, as:(7)u˜(x)=x+x2uˆ(x)du˜dx=1+2xuˆ(x)+x2duˆdxd2u˜dx2=2uˆ(x)+4xduˆdx+x2d2uˆdx2where,uˆ(x),firstduˆ/dxand secondduˆ2/dx2derivatives are the respective networks given in any one set of Eqs. (3)–(5). These networks as given in Eq. (7) for any model can arbitrary combine to construct a fitness function for the first PE with ability to exactly satisfying the initial conditions. The fitness function is formulated based on mean square error and it is given as:(8)ε=1N∑m=1Nd2dx2u˜m−6u˜m2−xm2,x∈(0,X)where N=X/h, is the total number of grid points in interval [0,X], i.e., inputs xϵ[x0=0, x1=0.1, x2, …, xN=X],u˜m=u˜(xm),xm=mh,u˜(x)andd2u˜/dx2are the networks as given in Eq. (7). It is quite evident with the availability of design parameter, i.e., weights (αi, wi, βi) for i=[1,2, …, m] for each neural network models such that the value of fitness functions ɛ approach zero, the solution u(x) of the first PE is approximated withu˜(x)as given in (7). The generic form of neural networks architecture for first PE is shown in Fig. 1.In this section, the learning method based on sequential quadratic programming (SQP) algorithm is described briefly, which is used for the training of adaptive weights for each ANN models of the equation.In standard working of SQP algorithm, an approximation is made for the Hessian of the Lagrangian function by using the quasi-Newton updating scheme at each iteration. With the help of Hessian of the Lagrangian function a quadratic programming (QP) sub-problem is created whose solution is determined a search direction for a line search procedure. It is well known that SQP algorithm is a kind of local search technique, and usually finds a local minimum for an optimization problem. The SQP method usually implemented in three main steps, given as:i.Calculation for approximate Hessian matrix of the Lagrangian function using quasi-Newton method;Formulation of the QP sub-problem;Calculation of line search and merit functions.For the detail description for the working of SQP algorithm, interesting readers are referred to [30–34]. In this study, the strength of SQP method is exploited for calculation of desired weight of neural networks. The generic flow diagram of overall process of the proposed models for finding the design parameters is shown in Fig. 2.Necessary explanation about the procedural steps for optimization with SQP algorithm is presented here:Step 1:Program initialization: An initial weight vector is generated randomly with bounded real values of length equal to the number of design parameters in each neural network model and given as.W={α,w,β}={α=α1,α2,...,αm,w=w1,w2,...wm,β=β1,β2,...βm},where m represents number of neurons. The parameter settings used to initialize the program of SQP algorithm are critical and set wisely with care, through experience and fine tuned with experimentations in order to avoid premature convergence.Step 2:Tool initialization:Initialized the tool using MATLAB built-in function ‘optimset’ with settings as given in Table 1before the execution of the algorithm.Step 3:Fitness evaluation: MATLAB routine ‘fmincon’ for constraints optimizations problems is used for each solver by giving the following•Initial weights vector W and ‘optimset’ tool as initialize in steps 1 and 2, respectively.Fitness evaluation function, ɛ, as define in (8) for each model.Step 4:Termination criteria: Terminate the execution for the solver, if any of the following criteria satisfied:•The predefined value of fitness is achieved, for example, ɛ≤10−15.The iterative process is stopped, i.e., total number of iterations is executed.Any of the set value given for function tolerance (TolFun), maximum function evaluation (MaxFunEvals), X-tolerance (TolX), and constraints tolerance (TolCon) is achieved.Step 5:Storage: Store the values for final weight vector; its fitness and computational time for finding the optimal weights for this independent run of algorithm.Step 6:Statistical analysis: Continues the steps 1–5 for sufficient large numbers of independent runs for each solvers by taken different neurons to generate initial weight vector of step 1. This larger data set is necessary to draw useful inferences about each solver.In this section, the results are presented for the detail simulations performed for solving first PE by the proposed neural networks solvers. A brief overview of analytical solvers for solving the problem is also provided in order to compare the results.The analytic solution of the IVP (1) has been provided using variation iteration method (VIM), Homotopy perturbation method (HPM), and modified Homotopy perturbation method (MHPM) [35,36]. The reported results of VIM [35,36] for the given problem for first three iterations with initial estimateyVIM0(x)=x+16x3are written as:(9)yVIM1(x)=x+16x3+12x4+115x6+1336x8yVIM2(x)=x+16x3+12x4+115x6+17x7+,…,187764400x14+1100800x16+15757696x18yVIM3(x)=(x+16x3+12x4+115x6+17x7+,…,11871210325276160000x14+160939454466400x36+17768399149858816x38)Similarly, the reported solutions based on first five iteration obtained by HPM [35,36] with initial guess ofyHPM0(x)=x+16x3are given as:(10)yHPM1(x)=x+16x3+12x4+115x6+1336x8yHPM2(x)=(x+16x3+12x4+115x6+17x7+,…,140x9+1746200x11+126208x13)yHPM3(x)=x+16x3+12x4+115x6+17x7+,…,52198408400x14+3551144144000x16+95224550144x18yHPM4(x)=x+16x3+12x4+115x6+17x7+,…,16346914378364000x19+163451491203440000x21+14580637139501513648000000x23yHPM5(x)=x+16x3+12x4+115x6+17x7+,…,404709982723087434930560000x24+14580637139501513648000000x26+748491737058836344832x28While, the reported solutions obtained using MHPM [35,36] initialized from yMHPM0(x)=x based on first seven iterations are given as:(11)yMHPM1(x)=x+16x3+12x4yMHPM2(x)=x+16x3+12x4+115x6+17x7yMHPM3(x)=x+16x3+12x4+115x6+17x7+1336x8+140x9+128x10yMHPM4(x)=x+16x3+12x4+115x6+17x7+,…,7146200x11+233080x12+3364x13yMHPM5(x)=x+16x3+12x4+115x6+17x7+,…,52198408400x14+13164680x15+3720384x16yMHPM6(x)=x+16x3+12x4+115x6+17x7+,…,1986795295200x17+489952952x18+75193648x19yMHPM7(x)=x+16x3+12x4+115x6+17x7+,…,136714121727305600x20+33067266826560x21+2192711072x22The exact solution of nonlinear first PE is not known therefore, the solution is calculated with fully explicit Runge–Kutta numerical methods, uref(x), are used as a reference result. Moreover, the comparison is also made with the performance operator based on root mean square (RMS) value of error associated with differential equation. It is defined as follows:(12)εRMS=1N∑m=1Nd2dx2um−6(um)2−xm2,where xϵ[0,1] with N is number of grid points, um=u(xm), and xm=mh.The numerical experimentation of each neural network models is carried out based on 5, 10 15 and 20 number of neurons which means that the length of optimal weights vectors are 15, 30, 45 and 60, respectively. The unsupervised error function is constructed for first PE as provided in Eq. (8) for each models for inputs x∈(0, 1] with a step size of h=0.1 and is given as:(13)ε=110∑m=110d2dx2u˜m−6u˜m2−xm2The accuracy of the models is dependent upon the strength modeling and the level of tolerance as given in (13) is achieved. SQP algorithm is applied to find the optimal weights of each neural network models. The initial weights are generated randomly using the real values between −1 and 1and is provided to SQP procedure as a start point to get trained weights of each model by incorporating the parameter settings as listed in Table 1 unchanged. The system is repeated for 100 independents runs with similarly generated input data set and obtained 100 final weights of each model. These weights are used in first equation of set (3)–(5) to get the approximate solutions for the models with log-sigmoid,u˜LS,radial basisu˜RB,and tan-sigmoidu˜TSfunctions, respectively. The proposed solution for each model, as well as, the results of Runge–Kutta methods are determined for inputs x∈[0,1] with a step size of h=0.1. The values for absolute error (AE) from reference solution,|u˜ref(x)−u˜(x)|,is also calculated for each runs of model with LS, RB and TS activations functions. One set of weights with minimum value of mean absolute error (MAE) for each technique is provided in Fig. 3for 10 neurons and in Fig. 4for 5, 15, and 20 neurons. Proposed results are provided in Tables 2 and 3for inputs between 0 and 1 with a step of 0.1 with the weights given in Figs. 3 and 4. The reference numerical and reported solutions based on 3rd iteration of VIM, 5th order HPM and 7th iteration of MHPM are also tabulated in Table 2 for the same inputs.The results on the basis of AE for VIM, HPM, MHPM, and our proposed methodologies are shown graphically in Fig. 5. It is be seen that value of AE for VIM, HPM, MHPM are decreasing with the increase in number of iterations in the method, because with increase iteration number the governing solutions are based on larger truncated series. It is also seen that for these analytical solutions closed to the initial guess the values of AE lies around 10−09–10−10, while for inputs approaches 1 the values of absolute error is around 10−02–10−04. The values of AE for proposed schemes ANN-LS, ANN-RB and ANN-TS lies in the range 10−06–10−08, 10−05–10−06 and 10−06–10−07, respectively, in case of 5 neurons, 10−06–10−08, 10−05–10−06 and 10−05–10−07, in case of 10 neurons, respectively, 10−06–10−08, 10−05–10−06 and 10−06–10−07, respectively, in case of 15 neurons, and 10−06–10−08, 10−04–10−07 and 10−05–10−07, respectively, in case of 20 neurons. In order to evaluate the performance of proposed techniques, as well as, analytical solvers, results are compared on the basis of MAE from reference numerical results, uref(x) and values of RMS error ɛRMS. The values calculated for MAE and ɛRMSare presented in Table 4for proposed scheme based on different number of neuron and analytical solvers on different number of iterations. It is seen that the values of MAE and ɛRMSof proposed neural networks models are generally of the order of 10−05–10−06 and 10−04–10−05, respectively, while the respective values of MAE and ɛRMSare of the order 10−04 and 10−01 for 3rd iterate VIM, 10−05 and 10−02 for 5th order HPM and 10−06 and 10−02 for 7th order MHPM. It can be inferred from the results presented in Table 4 that solution obtained by proposed schemes are more accurate and superior than that of analytical solvers like VIM, HPM and MHPMs.In order to evaluate in-depth performance of proposed solution by each model, using the same set of weights, as well as, the results of Runge–Kutta methods are calculated for intermediate inputs x∈[0.05, 0.95] with a step size of h=0.1. The results are shown graphically in Fig. 6a–c for the solution in case of ANN-LS, ANN-RB and ANN-TS, respectively, while the respective results based on AE are shown in Fig. 6d–f. It is seen that generally the small values of AE are obtained for each model and results for untrained intermediate inputs are almost similar to trained inputs, which established the correctness of the proposed methods.In this section, the comparative studies for the proposed neural network models are presented for solving first PE in term of the values for the MAE, mean square error (MSE), RMS error, ɛRMS, mean fitness, MF, achieved, global mean absolute error, GMAE, global root mean square error, ɛGRMS, convergence rate and computational complexity.The selection of number of neurons in three neural networks model is bit tricky and important as well. The performance operator based on MAE, MSE and RMS error (RMSE) are taken to determine the most suitable number on neuron in the model. The average value of MAE, MSE and RMSE are plotted against number of neuron for ANN-LS, ANN-RB and ANN-TS in Fig. 7. It is seen from Fig. 7a that with increase in number of neuron the accuracy of the algorithm improve but this inference is not evident from Fig. 7b and c due to the error in averaging. The single bad runs effect greatly on accuracy, their truncated mean based on 50% of the runs, the results are plotted in Fig. 7d–f in case of ANN-LS, ANN-RB and ANN-TS, respectively. From the results in these figures, it is quite clear that with the increase on number of neuron in any three of neural networks model the accuracy in the results enhanced but at the cost of few more computation efforts.An authenticated and useful inference on the performance of proposed neural networks in term of accuracy and convergence can only be made on results of statistical analysis based on sufficient large number of independent runs rather than single successful runs of the algorithm. The values obtained for fitness functions are plotted in ascending order against rearrange number of all 100 independent runs all three proposed neural network models with 5, 10, 15 and 20 neurons in Fig. 6. The values calculated for MAE and RMS error, ɛRMS, based on inputs x between 0 and 1 with step size of 0.1 are also shown in Fig. 5 in ascending order against number of runs of algorithms. These values shown graphically on semi-log scale because difference in the values is very small. It can be inferred that the convergence in the results is observed for ANN-LS and ANN-TS methods in almost all the independents runs while in case of ANN-RB method convergent runs are rather less. To clearly identify the convergence of the proposed algorithms, percentage of independent runs are determined on the basis of acceptability criteria on different level of fitness achieved, values of MAE and ɛRMS, and the results are provided in Table 5for all three proposed neural network models based on 5, 10, 15 and 20 number of neurons.It can be inferred from the results presented in Fig. 8and Table 5 that with the increase in the number of neurons in any of the neural networks models the better accuracy is achieved, which is quite evident by increasing the number of neuron form 5–10, however, by increasing further the number of neurons, i.e., 10–15 or 20, a slight gain in precision is observed. In addition to that stability of all the three proposed models have been improved with increase in the number of neurons but the advantages of these effect are very little by adding more than 10 neurons. Generally, the results obtained by ANN-LS and ANN-TS are consistently more accurate and convergent than that of ANN-RB, however, when comparing the results of ANN-LS and ANN-TS, there is no noticeable difference in the results but to some extent superior results are obtained by ANN-TS.The results of statistical parameters in term of mean, standard deviation (STD), minimum (MIN) values of AE from reference numerical results are used to explore further the strength and weakness of the proposed designed models. The values of MIN, mean and STD are calculated on the basis of convergent independent runs (based on fitness≤10−04) to optimized the fitness functions as given in (13) by ANN-LS, ANN-RB and ANN-TS methods. Results are given in Fig. 9for inputs 0 and 1 with step of 0.1 for the three algorithms in case of 5, 10, 15, and 20 numbers of neurons. It may observed that on average the proposed results with 20 number of neurons provides the best values of statistical parameters as compared to the results with less number of neurons. However, in few cases misinterpretations in the presented results exist, for instance, in case of mean values shown in Fig. 9, the result of ANN-RB with 5 neurons is giving the lower values but this aspect is diminished due to the fact that 90% of runs with inferior fitness values are discarded while using 10, 15 and 20 number of neurons in ANN-RB relatively lower number of runs, i.e., 77%, 69% and 49%, respectively, are ignored.The analyses on accuracy and convergence of proposed schemes continues on the bases of three performance parameters named as mean fitness MF, global MAE, GMAE, and Global RMS error, ɛGRMS, and these are defined as average values of fitness, MAE and RMS error ɛRMSover all independent run of the algorithms, respectively, and mathematical written as:(14)εGRMS=1R∑r=1R1N∑k=1Nd2dx2u˜k−6(u˜k)2−xk2rGMAE=1R∑r=1R1N∑k=1N|uref−u˜k|rMF=1R∑r=1R(ε)rwhere inputs xϵ[0,1] with N number of grid points, R is the total number of independent runs of solvers. In numerical experimentation, the inputs are taken between 0 and 1 with step of 0.1, i.e., N=10 and 100 independent runs i.e., R=100. The values of MF, GMAEand ɛGRMSare calculated using (14) for 100 independent runs of each proposed models in case of 5, 10, 15 and 20 numbers of neurons and results are listed in Table 6. Similarly, these values are also determined based on criteria of fitness ≤10−04 and MAE≤10−02 and results are also given in Table 6. It is found that values of MF, GMAEand ɛGRMSare generally of the order 10−01–10−02, 10−02–10−03 and 10−01–10−03, respectively, based on results for all the runs of each solver and lie in the range 10−05–10−08, 10−03–10−05 and 10−03–10−04, respectively, for convergent runs. It is also observed from the values given in Table 6 that with increased of neurons in neural network modeling, the values of performance parameter of all the three proposed models improved, however, from increasing the neurons form 10–15 or 20 relatively a minute gain in the results.On the other hand, with the increase of number of neurons the computational complexity of the algorithms also increases significantly, therefore, there is always a tradeoff between computational complexity and accuracy while selecting the value of number of neuron in neural networks models. The computational complexity of these NN models is analyzed by calculating the time of execution of each algorithm for 100 numbers of independent runs by taking different number of neurons. The values mean execution time (MET) for all and convergent runs of solvers are provided in Table 6. On average the MET values are 28s, 54s, 69s and 87s in case of 5, 10, 15 and 20 neurons in the models, respectively. It is seen that values of MET for ANN-LS and ANN-TS is relatively more than that of ANN-RB but this aspect of ANN-RB model is overshadows due to its inferior performance in term of accuracy and convergence. The time analysis provided in this article is carried out using Dell Latitude D630 Laptop with Intel(R) Core 2 Duo CPU T9300@2.50GHz and 2.00GB of RAM running MATLAB version 7.9.0.529 (R2009b).

@&#CONCLUSIONS@&#
