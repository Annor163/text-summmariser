@&#MAIN-TITLE@&#
Feature selection using data envelopment analysis

@&#HIGHLIGHTS@&#
Feature selection is regarded as a multi-index evaluation process.A novel feature selection framework based on data envelopment analysis is proposed.The framework evaluates features from a perspective of “efficient frontier” without parameter setting.A simple feature selection method is proposed based on super-efficiency DEA.

@&#KEYPHRASES@&#
Feature selection,Data envelopment analysis,Super-efficiency,Relevance,Redundancy,

@&#ABSTRACT@&#
Feature selection has been attracting increasing attention in recent years for its advantages in improving the predictive efficiency and reducing the cost of feature acquisition. In this paper, we regard feature selection as an efficiency evaluation process with multiple evaluation indices, and propose a novel feature selection framework based on Data Envelopment Analysis (DEA). The most significant advantages of this framework are that it can make a trade-off among several feature properties or evaluation criteria and evaluate features from a perspective of “efficient frontier” without parameter setting. We then propose a simple feature selection method based on the framework to effectively search “efficient” features with high class-relevance and low conditional independence. Super-efficiency DEA is employed in our method to fully rank features according to their efficiency scores. Experimental results for twelve well-known datasets indicate that proposed method is effective and outperforms several representative feature selection methods in most cases. The results also show the feasibility of proposed DEA-based feature selection framework.

@&#INTRODUCTION@&#
In many data mining applications, identifying the most characterizing features (or attributes, variables, hereafter they will be used interchangeably) of the observed data is critical to optimize the classification result. Tremendous new computer and internet applications, e.g. the prevalent use of social media, generate large amounts of data at an exponential rate in the world. Massive irrelevant and redundant features existing in the feature space deteriorate the performance of machine learning algorithms, and thus present challenges to feature selection.Feature selection is desirable and essential for a number of reasons, such as reducing the complexity of training a classifier and the cost of collecting features, improving the quality of the data, and even resulting in an improvement in classification accuracy [1,2]. Roughly, there are three types of feature selection methods [3,4]: Embedded methods, filters and wrappers. As for embedded methods in C4.5 [5] or SVM-RFE [6], the process of selecting features is integrated into the learning algorithm [5]. Wrappers rely on performance estimated by a specific learning method to evaluate and select features. The drawbacks of embedded methods and wrappers are their expensive computational complexity in learning and poor generalization to other learning methods as they are tightly coupled with specified ones. In contrast, filters assess features based on some classifier-agnostic criteria (e.g. Fisher score [7],χ2-test [3,8], mutual information [9–11], symmetrical uncertainty [1], Hilbert–Schmidt operator [12], etc.) and select features by focusing only on the intrinsic properties of the data. Developing efficient and effective filter methods has attracted great attention during past years [1,9,13,14].Feature ranking and feature subset selection are two typical categories of feature selection regarding the output style. The former outputs ranked features weighted by their predictive power [15–17] while the latter evaluates feature subsets and searches for the best one [18,1,19–21]. Since finding an optimal subset is usually intractable and many problems related to feature selection have been shown to be NP-hard [22,23], a trade-off between result optimality and computational efficiency has been taken under consideration in literature. Heuristic methods with various feature evaluation criteria have thus been proposed [17,24,19,1]. These criteria mainly focus on the measurement of feature relevance, redundancy, conditional independence, inter-dependence, etc., and the combination of such criteria (e.g. relevance analysis with mutual information+redundancy analysis with conditional mutual information) brings about diversity of feature selection methods. Nevertheless, most of the combinations evaluate features with either prior arguments or constant coefficients, and the relative importance (weight) of each feature property such as relevance or redundancy usually cannot be identified. For example, MIFS [17] applies two information-theoretic metrics to respectively measure feature dependence (D) and redundancy (R), and usesmax(D-βR)to evaluate the quality of selected features. Parameterβplays a role of mediating the weight of measured redundancy and thus any changes to it may influence the quality of the finally-selected features. Owing to more than one feature property or criterion to be considered and utilized, feature selection can thus be categorized as a multi-index evaluation process.Data Envelopment Analysis (DEA) is an effective nonparametric method for efficiency evaluation and has been widely applied in many industries. It employs linear programming to evaluate and rank the Decision Making Units (DMUs) when the production process presents a structure of multiple inputs and outputs. Inspired by this, we regard feature selection as evaluation process with multiple inputs and outputs, and introduce in this paper a novel DEA-based feature selection framework. An effective feature selection method based on this framework is then proposed and evaluated. The remainder of the paper is organized as follows: Section 2 briefly reviews related works. Section 3 introduces some related information-theoretic metrics and Section 4 introduces a novel feature selection framework based on DEA. Then Section 5 proposes a feature selection method based on this framework. In Section 6, experimental results are given to evaluate the effectiveness of proposed method comparing with the representative feature selection methods on twelve well-known datasets, and some discussions are presented. Section 7 finally summarizes the concluding remarks and points out the future work.

@&#CONCLUSIONS@&#
In this paper, we regard feature selection as an evaluation system with multiple inputs and outputs and propose a DEA-based feature selection framework: Properties of features such as relevance and redundancy are taken as the inputs and the outputs of the evaluation system and a DEA method is then applied to evaluate the relative efficiency of the features and then to achieve feature rankings according to the efficiency values. To illustrate the effectiveness of this framework, we propose a feature selection method with super-efficiency DEA, which employs two important feature properties namely conditional independence and relevance to establish the evaluation system. For salient features should have strong discriminative power which would not be impaired given other features, we take the conditional independence between the currently-evaluated featureFp(the DMU) and the class given every other feature in the feature space as the input and take the relevance betweenFpand the class as the output of the evaluation system, and apply super-efficiency DEA technique to get feature rankings. We evaluate our method by comparing it with the most representative feature ranking and subset selection methods through several classification experiments. It can be verified by the results that proposed method is feasible and superior to other feature selection methods in most cases.However, there still exist some issues in our feature selection method needed to be improved. Since the LP solver integrated in MATLAB runs very slowly, it is really time-consuming for our method to deal with an even medium-sized dataset. Advanced optimizers such as Gurobi and CPLEX which solve large-scale LP problems more efficiently will be considered in our future work. It is also noted that a rule-of-thumb in DEA [52] suggests that the number of DMUs should be at least three times the total number of inputs plus outputs used in the models. In model (4), this rule can be shown asn⩾3×(m+s). However in our method, the number of inputs (|F|-1) is nearly the same to that of DMUs (|F|) and hence the results may not perform well in some cases. Meanwhile, alternative non-radial DEA measures that estimate technical inefficiency a la Pareto–Koopmans (e.g. the Enhanced Russell Graph [53], RAM [54], and BAM [55]) are possible to be applied in our feature selection algorithms to make trade-offs between inputs or between outputs. In addition, excessively focusing on the ‘gap’ between the inputs and the outputs and unreasonable solutions of super-efficiency methods are also needed to be improved in our future work.