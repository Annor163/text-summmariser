@&#MAIN-TITLE@&#
Using objective ground-truth labels created by multiple annotators for improved video classification: A comparative study

@&#HIGHLIGHTS@&#
We present a strategy to create objectively labeled ground-truth set of videos.Such a strategy mitigates the subjective biases of the annotation process.Objective labels show superior consistency than subjective labels.A classifier is trained to predict objective labels for 51K unlabeled videos.

@&#KEYPHRASES@&#
Video classification,Large dataset analysis,Data annotation,

@&#ABSTRACT@&#
We address the problem of predicting category labels for unlabeled videos in a large video dataset by using a ground-truth set of objectively labeled videos that we have created. Large video databases like YouTube require that a user uploading a new video assign to it a category label from a prescribed set of labels. Such category labeling is likely to be corrupted by the subjective biases of the uploader. Despite their noisy nature, these subjective labels are frequently used as gold standard in algorithms for multimedia classification and retrieval. Our goal in this paper is NOT to propose yet another algorithm that predicts labels for unseen videos based on the subjective ground-truth. On the other hand, our goal is to demonstrate that the video classification performance can be improved if instead of using subjective labels, we first create an objectively labeled ground-truth set of videos and then train a classifier based on such a ground-truth so as to predict objective labels for the set of unlabeled videos.With regard to how we generate the objectively-labeled ground-truth dataset, we base it on the notion that when a video is labeled by a panel of diverse individuals, the majority opinion rendered by the panel may be taken to be the objective opinion. In this manner, using judgments provided by multiple human annotators, we have collected objective labels for a ground-truth dataset consisting of randomly-selected 1000 videos from the TinyVideos database that contains roughly 52,000 videos from YouTube (courtesy of Karpenko and Aarabi [1]).Through a fourfold cross-validation experiment on the ground-truth set, we demonstrate that the objective labels have a superior consistency compared to the subjective labels when used for video classification. We show that this claim is valid for several different kinds of feature sets that one can use to compare videos and with two different types of classifiers that one can use for label prediction.Subsequently, we use the ground-truth dataset of 1000 videos to predict the objective category labels of the remaining 51,000 videos. We compare the objective labels thus determined with the subjective labels provided by the video uploaders and qualitatively argue for the more informative nature of the objective labels.

@&#INTRODUCTION@&#
Massive amounts of image and video data has been and continues to be uploaded to Internet based visual content databases like Picasa, Flickr, YouTube, Archive.org, Hulu and others. The content for these databases is generally created in realistic settings from sports and news coverage; documentaries on travel, science, and technology; social events; and so on. In recent years, this type of data has fast become the experimental data of choice in computer vision research because it encompasses large inter- and intra-class variability and presents very interesting challenges for problems like object detection and tracking, face recognition, human activity analysis, and so on.In this paper, we address the problem of consistently and objectively labeling the content in these large databases. Although applicable to all large video databases, our work focuses specifically on the videos that are uploaded to the YouTube database. YouTube requires that every video uploaded to its servers be assigned one of the 15 broad category labels listed in Table 1. Karpenko and Aarabi [1]have noted that there is a significant amount of labeling noise in the categories assigned to the YouTube videos. The uploader may assign a category label based on his/her subjective judgment about the theme of the video content. Also the label assigned to a video may be motivated by a particular section of the video, and the rest of the video may be totally unrelated to the label. It may also be influenced by the motivation that led to creation of the video in the first place, or by the uploader’s opinion about the target audience. The upshot is that different uploaders may assign different category labels to videos with similar content. We obviously need a more objective and consistent way of assigning category labels to the videos.Towards that end, we demonstrate that by first creating a ground-truth dataset of videos with objectively assigned category labels and then by using such a set to train a video classifier that can predict objective labels for new unlabeled videos, we end up with superior labeling for the unlabeled videos. By superior we mean labels that are more consistent compared to the labels that would be predicted purely on the basis of the uploader-supplied subjective labels. We demonstrate these results using the TinyVideos dataset that consists of 52,000 YouTube videos [1]. Just to give the reader a flavor of these videos, we show in Fig. 1the keyframes extracted from five randomly chosen videos that correspond to five different uploader-supplied subjective labels. Looking at these key-frames, one could say that the other labels—labels other than those used by the uploaders—may also be applicable to these videos. For example, the 1st, 2nd and 5th videos could alternatively be assigned the labels Sports, News and People,respectively. As to how a video’s keyframes are selected, we will discuss that in Section 4.1.1.Creating a ground-truth set of objectively labeled videos is obviously critical to the research reported here. For that purpose, we use a majority voting approach, which is described in detail in Section 3. Our objective labeling process involves collecting labels for every video from multiple annotators. For each video, the annotators were asked to browse through the full video and to then mark all the category labels that applied to it. Due to practical considerations of time and effort, this could only be done for a relatively small set of 1000 videos. We refer to this set of 1000 videos as our objectively labeled ground-truth dataset. With regard to this set, we are faced with the following two questions:1.Why should we believe in the objective labels for the ground-truth dataset?Can we propagate the objective labels from this set to the rest of the database that contains 51,000 video clips?We supply the answer to the first question by applying a fourfold cross-validation analysis to the ground-truth dataset. And we supply the answer to the second question by claiming that the video labeling algorithm used in the cross-validation on the ground-truth dataset can be directly used for predicting the objective labels for the videos in the larger database.In order to establish that our answers to the two questions raised above are not dependent on any particular feature set used to compare videos or on any particular classifier for assigning labels to the videos, our results in this paper are based on several different kinds of features sets and for two different types of classifiers for label prediction: the K-Nearest Neighbors (KNN) and Support Vector Machine (SVM).With regard to the specifics of video classification used for both the cross-validation on the ground-truth dataset and for the labeling of the remaining 51,000 videos, the objective labels of the labeled videos are first propagated to the keyframes extracted from the same videos. Each keyframe extracted from a video is represented by a feature vector in the manner described in Section 4.1.1. The fourfold cross-validation on the ground-truth dataset and the label prediction for the remaining 51,000 videos is carried out in the space spanned by these feature vectors. Since it is possible for the classifier to yield different labels for the different keyframes in a target video, we pool together all such labels for all the keyframes extracted from a video by averaging and thresholding and then decide on the overall label for the video.Although we demonstrate the robustness of using objective labels in the label prediction algorithm with a relatively small ground-truth set of 1000 randomly selected videos, it stands to reason that the larger the ground-truth dataset, the greater the accuracies that can be had by using the label prediction algorithm. As to why we have limited ourselves to a ground-truth dataset of only 1000 videos, note that the larger the ground-truth dataset, the greater the difficulty of collecting the human-annotated labels for the videos. That would certainly be the case for a typical university-based laboratory such as ours. However, large video service providers like YouTube would easily be able to scale up the methodology we present in this paper to create ground-truth datasets consisting of tens of thousands of videos by allowing the viewers (in addition to uploaders) to click on the most relevant category labels applicable to the online videos. This user feedback would be similar to the existing feedback mechanisms that allow the viewers to enter their comments and rate the videos.The overall research reported in this paper consists of the following four steps:1.Assuming that the opinion of a majority can be considered to be objective, we ask a panel of individuals with diverse backgrounds to label 1000 videos that are randomly selected from the collection of 52,000 videos belonging to TinyVideos dataset. We permit the individuals to give multiple category labels to a video if such is warranted by the content of the video. The labels for any particular video provided by the multiple annotators are combined through majority voting. We refer to this 1000 video set as the ground-truth dataset and the majority-voted labels as the objective labels.We then establish through fourfold cross-validation the superiority of the objective labels vis-a-vis the subjective labels in the ground-truth dataset. Since the cross-validation analysis involves training a video classifier based on feature vectors and label data, we demonstrate the superiority of the objective labels for several different kinds of features and for two different types of video classifiers.We predict the objective labels for the remaining 51,000 videos in the TinyVideos dataset using a classifier trained on the video keyframe feature vectors and objective labels of the ground-truth set.We compare the predicted objective labels of the videos in this larger set with their corresponding subjective labels assigned by the uploaders.In the rest of this paper, we start with a survey of the related work in Section 2. In Section 3, we describe the methodology for creating the objective labels for videos in the ground-truth dataset. Section 4 presents the fourfold cross-validation experiment to demonstrate the superior consistency of the objective labels in the ground-truth dataset compared to the subjective labels. In the same section, we also present the procedure to train the video classifiers for objective label prediction (Section 4.1). Subsequently, in Section 5, we use the label prediction algorithm to determine the objective labels for the unlabeled set of 51,000 videos in the TinyVideos database. We also give some qualitative arguments about their more informative nature compared to the subjective labels. Finally, in Section 6 we conclude and present our views on how this research can be extended further.

@&#CONCLUSIONS@&#
The human uploaders deciding for themselves as to what label to assign to a video creates a large degree of label noise in a large database such as the one made available by YouTube. There are legitimate practical reasons for wanting to see a greater consistency in these category labels.With the methodology we provide in this paper, a video service like YouTube could still allow for subjective labeling of the videos, as is currently the practice, but now the viewers could also be given a choice of accessing videos through the more consistent objective labels.Our work has demonstrated that for three different types of visual features and for two different types of video classifiers, the objective labels result in superior label consistency on the unlabeled set as compared to the subjective labels. We also showed that it is possible to predict objective labels for the videos in a large database of 51,000 videos by training a video classifier based on the objective labels created originally for a more manageable subset of the database.With regard to how the work reported here can be extended, obviously our work represents just one approach to the problem of labeling large video databases in a consistent manner. We can surely expect that future work in this area would include other algorithms for keyframe extraction, other visual features for the representation of the keyframes, and, possibly, also other classifiers. Future approaches may also experiment with different logic for the pooling of multiple labels for a video when more than one label is available for a video. Yet another expansion on our work would be the use of a larger ground-truth set – larger than the 1000 videos used in our work here.