@&#MAIN-TITLE@&#
An improved particle swarm optimizer based on tabu detecting and local learning strategy in a shrunk search space

@&#HIGHLIGHTS@&#
Search space of each dimension is divided into many equal subregions. According to statistics information in particles’ historical best position, some dominant subregions can be determined.A tabu detecting strategy helps the global historical best position to find a better subregion based on the statistics information.A shrinking strategy enables PSO make the optimization in a smaller search space.A local learning strategy of elites particle can improve the accuracy of the solutions.The strategies proposed in this paper have general applicability.

@&#KEYPHRASES@&#
Particle swarm optimization,Detecting strategy,Shrink search space,Local learning,Subregions,Premature convergence,

@&#ABSTRACT@&#
To improve the performance of the standard particle swarm optimization (PSO) which suffers from premature convergence and slow convergence speed, many PSO variants introduce lots of stochastic or aimless strategies to overcome the convergence problem. However, the mutual learning between elites particles is omitted, although which might be benefit to the convergence speed and, prevent the premature convergence. In this paper, we introduce DSLPSO, which integrates three novel strategies, specifically, tabudetecting,shrinking andlocal learning strategies, into PSO to overcome the aforementioned shortcomings. In DSLPSO, search space of each dimension is divided into many equal subregions. Then the tabu detecting strategy, which has good ergodicity for search space, helps the global historical best particle to detect a more suitable subregion, and thus help it jump out of a local optimum. The shrinking strategy enables DSLPSO to take optimization in a smaller search space and obtain a higher convergence speed. In the local learning strategy, a differential between two elites particles is used to increase solution accuracy. The experimental results show that DSLPSO has a superior performance in comparison with several other participant PSOs on most of the tested functions, as well as offering faster convergence speed, higher solution accuracy and stronger reliability.

@&#INTRODUCTION@&#
Particle swarm optimization (PSO), which was introduced by Kennedy and Eberhart in 1995 [1,2], is a population-based optimization technique inspired by the emergent motion of swarms such as birds flocking and fish schooling. As a swarm intelligence algorithm, a PSO system shares some common characteristics with other evolutionary algorithms (EA). However, unlike EA, in which some genetic operations including selection, crossover and mutation operators are adopted to manipulate new solutions (called individuals), PSO searches for new solutions by particles’ flying through the problem space according to their own experiences and neighborhoods’ best experience.During the last decades, PSO gained increasing popularity because it can obtain promising results for optimization problems by simple implementation [3]. Due to the merit, PSO and its variants have been applied in many research areas [4–7]. However, those experiments have shown that the standard PSO algorithm suffers from premature convergence when solving complex multimodal problems [8]. Furthermore, PSO is also a population-based iterative algorithm which causes PSO computationally be inefficient as measured by the number of function evaluations (FEs) required. These drawbacks have restricted wider application of PSO [9].A major concern in PSO today is how to overcome the shortcomings and to improve its applicability. A lot of theoretical analysis have been done on PSO [10–14], and extensive studies have revealed that PSO's capability of finding a global optimum mainly depends upon its two characteristics. The first one is the ability to preserve the diversity of swarm, the aim of which is to avoid premature convergence, and thereby to enhance PSO's exploration ability. The other one is the capability of local search, which called exploiting ability, by which PSO can improve the accuracy of solutions. Aiming at the issues, many studies have been carried on during the past decades to accelerate convergence speed of PSO and to prevent the swarm falling into a local optimum [15–17], the detail of which will be discussed in Section “Related works”.Although a lot of efforts have being spent on improving these abilities, it is seen to be difficult to achieve both goals simultaneously. For example, the comprehensive-learning PSO (CLPSO) [18] has a desirable performance on preserving the diversity, which causes it more suitable for solving multimodal problems. However, it is not the best choice to use CLPSO optimizing unimodal problems due to the slow convergence speed. On the contrary, UPSO [19] is more suitable for unimodal problems than multimodal problems. Although one algorithm cannot offer better performance than all the others on every kind of problem, these variants PSO have provided diverse effective strategies to solve different problems. Based on the state-of-art, three strategies, specifically, tabudetecting,shrinking andlocal learning are proposed to improve the performance of PSO. The novel PSO variant, called DSLPSO, is proposed in this paper.In DSLPSO, four new features are adopted to improve its performance. First, search space of each dimension is divided into many equal subregions. Based on it, the dominant searching regions can be obtained according to the statistics of some superior particles. Second, a novel detecting strategy is adopted to attract particles to jump out of local optima traps. Third, the search space is shrunk according to the statistical information of particles’ historical best position. Fourth, a simple local search process based on the differential between the global historical best particle and the second historical best particle is executed at the end of the search.The rest of this paper is organized as follows. Section “Related works” describes the framework of standard PSO and reviews some PSO variants. The detail of DSLPSO algorithm is presented in Section “DSLPSO”. Section “Experimental verification and comparisons” experimentally compares the DSLPSO with various existing PSO algorithms using a set of benchmark functions. Furthermore, in order to verify performance of DSLPSO in engineering applications, DSLPSO is also used to optimize a set of PID controller parameters. Finally, discussion and further investigations on the DSLPSO are introduced in Section “Conclusions”.

@&#CONCLUSIONS@&#
In this paper, we have proposed a novel PSO variant which called DSLPSO. In DSLPSO, the search space of each dimension is divided into multiple equal subregions. Furthermore, the historical information of pbest and gbest are imported to statistical analysis. According to the statistical results, some superior and inferior subregions are determined. A tabu-based detection strategy, based on the superior and inferior subregions, is introduced to help PSO to jump out of a local optimal solution. When the gbest's value in a dimension sustained stagnation and exceed a predefined threshold, search space of the dimension will be shrunk. After the implementation of shrinking strategy, PSO can make a higher accurate search in a smaller space. At the later evolution process, gbest executes a local learning operator, based on the differential between gbest and the second global historical best position, to improve solution accuracy.Comprehensive experiments have been conducted on sixteen benchmarks, including unimodal, multimodal and shifted functions. The experimental results demonstrate that the strategies in this paper enable DSLPSO significantly outperform participant PSOs on most of the tested functions, contributing to faster convergence speed, higher solution accuracy and stronger reliability. And the t-test results show that DSLPSO is more suitable for multimodal function optimization though it can also achieve favorable performance on unimodal functions. Furthermore, the three strategies enable the two different PSO variants (GPSO and CLPSO) get better performance, which means that the strategies proposed in this paper may have general applicability. The experiment results of tuning PID parameters also indicate that DSLPSO has a relatively better performance on engineering application fields. The experiment results of tuning PID parameters also indicate that DSLPSO has a relatively better performance on engineering application fields.Further work includes: (1) performing more precise statistical analysis and providing more useful information for gbest to take detecting operator; (2) introducing an expanding strategy may be helpful for PSO to find the global optimal solution since the shrinking strategy may cause the global optimal not in the searching subregion, and (3) applying an improved DSLPSO to optimize large-scale multimodal functions and many engineering applications and many engineering applications.