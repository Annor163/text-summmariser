@&#MAIN-TITLE@&#
A high-dimension two-sample test for the mean using cluster subspaces

@&#HIGHLIGHTS@&#
A two-sample test using hierarchical clustering was proposed.Hotelling’s statistics are computed in cluster-subspaces and summed as the statistic.Highly correlated variables take priority for being processed.A cutoff distance is used to restrain the effect of statistical fluctuations.High performance was demonstrated in simulations and real data analysis.

@&#KEYPHRASES@&#
High-dimension data,Two-sample problem,Hierarchical clustering,Hotelling’s test,

@&#ABSTRACT@&#
A common problem in modern genetic research is that of comparing the mean vectors of two populations–typically in settings in which the data dimension is larger than the sample size–where Hotelling’s test cannot be applied.Recently, a test using random subspaces was proposed, in which the data are randomly projected into several lower-dimensional subspaces, and Hotelling’s test is well defined. Superior performance with competing tests was demonstrated when the variables were correlated.Following the research of random subspaces, a modified test was proposed that might make more efficient use of covariance structure at high dimension. Hierarchical clustering is performed first such that highly correlated variables are clustered together. Next, Hotelling’s statistics are computed for every cluster-subspace and summed as the new test statistic. High performance was demonstrated via simulations and real data analysis.

@&#INTRODUCTION@&#
A common problem in genetics is that of testing whether a set of dependent gene expressions differs between two populations, typically in a setting where the data dimension is larger than the sample size.For correlated variables, a classic test is Hotelling’s test. For instance, two samplesX=(X1,…,Xn1)andY=(Y1,…,Yn2)of sizen1andn2are generated in an independent and identically distributed (i.i.d.) manner fromp-dimensional multivariate normal distributionsN(μ1,Σ)andN(μ2,Σ), respectively, where the mean vectorsμ1andμ2and positive-definite covariance matrixΣare all fixed and unknown; the hypothesis testing problem of interest isH0:μ1=μ2versusH1:μ1≠μ2,the Hotelling’s test statistic is defined byT2=n1n2n1+n2(X¯−Y¯)TΣ̂−1(X¯−Y¯),whereX¯=1n1∑j=1n1XjandY¯=1n2∑j=1n2Yjare the sample means,Σ̂is the pooled sample covariance matrix, given byΣ̂=1n∑j=1n1(Xj−X¯)(Xj−X¯)T+1n∑j=1n2(Yj−Y¯)(Yj−Y¯)T, andn=n1+n2−2(Anderson, 1984). Under the null hypothesisH0,n−p+1npT2has an F-distribution with degrees of freedompandn−p+1(Muirhead, 2005). Let the significance be chosen asαand the threshold be denoted asFα(p,n−p+1)=n−p+1npTα2. Under the null hypothesisH0, the probability ofT2≥Tα2is called the false positive (type I error) rate. Under the alternative hypothesisH1, the probability ofT2≥Tα2is called the true positive rate (power).By using a test suitable for correlated variables, it is possible not only to take the multivariate dependence structure into account but to gain more power from these dependences (Thulin, 2014). However, whenp>n, the matrixΣ̂is singular, and Hotelling’s test cannot be applied.Many studies have addressed the “largep, smalln” problem. Chung and Fraser (1958) proposed a nonparametric test that treats each variable independently. Dempster (1958, 1960) proposed a so-called “non-exact” significance test based on the quantity(X¯−Y¯)T(X¯−Y¯), which can be viewed as replacingΣ̂withIp×p. It was later refined by Bai and Saranadasa (1996) and Chen and Qin (2010). However, the statistics based on(X¯−Y¯)T(X¯−Y¯)lack desirable invariance properties under rescaling transformation. Srivastava and Du (2008) proposed a test based on(X¯−Y¯)TDΣ̂−1(X¯−Y¯), whereDΣ̂is the diagonal matrix associated withΣ̂, i.e.,(Σ̂)ii=(DΣ̂)ii. To make use of the multivariate dependence structure, Srivastava (2007) proposed using the Moore–Penrose pseudo-inverse ofΣ̂when computing Hotelling’s test statistic. Cai et al. (2014) applied a regularization technique to obtain a sparse estimator of the matrix and proposed a test statistic. Shen and Lin (2015) proposed a test that selects important variables against the null hypothesis.To make more use of the multivariate dependence structure, Lopes et al. (2011, 2012) proposed a test in which the data are randomly pseudo-projected into several lower-dimensional spaces, where Hotelling’s test is well defined. Hotelling’sT2statistic is computed for each pseudo-projection, and the result is then averaged over all pseudo-projections. Superior performance with competing tests was demonstrated when the variables were correlated. Thulin (2014) proposed a modified test using random subspaces to improve the invariance properties. Random permutation resampling was utilized to improve the null distribution of the statistic.This study followed the research of Thulin (2014) and proposed a test that can make more efficient use of the covariance structure at high dimension. Hierarchical clustering is performed first so that highly correlated variables are clustered together. Then, Hotelling’sT2statistics are computed for every cluster-subspace and summed as the new test statistic.The rest of the paper is organized as follows. In Section  2, we propose the new test based on cluster subspaces. In Section  3, we compare the test with other two-sample tests with Monte Carlo simulations. The new test and the other tests are applied to a breast cancer dataset in Section  4. In Section  5, we discuss how the clustering method can group the variables successfully. Conclusions are presented in Section  6.In Thulin’s random subspaces test, variables are randomly selected to construct subspaces in which Hotelling’s statistics are computed; therefore, the correlations between the variables are utilized and higher power is obtained. In the test in this study, hierarchical clustering is performed so that highly correlated variables are grouped together; therefore, the covariance structure might be more efficiently utilized. By clustering, the high dimension data are also projected to cluster subspaces of lower dimension in which Hotelling’s statistics can be computed and summed as the new test statistic.Hierarchical clustering was widely used in modern genetic research to find related genes or individuals. By clustering, variables with high similarity metrics (or low distances) are grouped together (Eisen et al., 1998). In this study, 1—Pearson correlation coefficient was used as the distance. Highly correlated and thus small distance variables would be clustered together. The correlation coefficients are also related to the covariance matrixΣ̂(if the distributions of all the variables are normalized so that the variances are equal to 1, the covariances are equal to the correlation coefficients).There may be statistical fluctuations for the correlation coefficients. For largep, some of the coefficients may be large by chance. To restrain the effect of statistical fluctuations, clusters were first calculated based on a cutoff distancedc. The Fisherz′-transformation of correlation coefficientrisz′=12ln1+r1−r. The standard error ofz′isσz′=1/n−1(Norman and Streiner, 2008). Denotetcas the1−2p(p−1)quantile of standard normal distribution; then,zc′=tc/n−1,rc=(e2zc′−1)/(e2zc′+1),dc=1−rcwere used in the calculation. Therefore, in the case where there was no correlation between thepvariables, only about one of thep(p−1)2correlation coefficients was greater thanrc, the distance was less thandc, and the corresponding two variables were grouped together due to statistical fluctuations.All the variables were clustered into several clusters. Some of the clusters had many variables. If a cluster (or sub-cluster) had more thankcvariables, it was further clustered into two sub-clusters, and so on, until each cluster (or sub-cluster) had no more thankcvariables. Lopes et al. (2012) and Thulin (2014) have shown that⌊n/2⌋is the optimal dimension of subspace and gives the highest power. In this study,kc=⌊2n/3⌋was used so that the cluster dimension might be distributed around⌊n/2⌋. For example, if the dimension of a cluster was less than or equal tokc, the cluster would no longer be subdivided, and its dimension would be less than or equal to⌊2n/3⌋; if the dimension of a cluster was greater thankc, the cluster would be further divided into two subclusters with an average dimension greater than⌊n/3⌋.The algorithm is as follows.Algorithm 1ClusteringStep 1.Set parametersdc,kc.Hierarchical clustering is performed (1—Pearson correlation coefficient distance and average linkage are used). Clusters are calculated based on cutoff distancedc; all the variables are clustered into several clusters.For each cluster (or sub-cluster), if it has no more thankcvariables, it will no longer be sub-divided; otherwise, it will be further clustered into two sub-clusters.Repeat Step 3 until each cluster (or sub-cluster) has no more thankcvariables.For cluster (or sub-cluster)ithat includespivariables (pi≤kc), the projections of the two samples on theith cluster subspace arexi=(xi,1,…,xi,n1)andyi=(yi,1,…,yi,n2), and the Hotelling’s statistic for theith cluster subspace can be defined by(1)Ti2=n1n2n1+n2(x¯i−y¯i)TΣ̂i−1(x¯i−y¯i),wherex¯i=1n1∑j=1n1xi,jandy¯i=1n2∑j=1n2yi,jare the sample means andΣ̂iis the pooled sample covariance matrix, given byΣ̂i=1n∑j=1n1(xi,j−x¯i)(xi,j−x¯i)T+1n∑j=1n2(yi,j−y¯i)(yi,j−y¯i)T.According to Algorithm 1, the whole space is composed of several (denoted asNc) cluster subspaces. Having computed the Hotelling’s statistics for all the cluster subspaces, one can make a sum of them and obtain an overall test statistic(2)Tcs=∑i=1NcTi2.The clustering algorithm is based on Pearson correlations that are invariant under rescaling transformations by diagonal nonsingular matrices; therefore, the algorithm is rescaling invariant. As the Hotelling’s statistics are also rescaling invariant, the new test is rescaling invariant, as is Thulin’s random subspaces test.The null distribution could be obtained by permutation. As the number of permutations is normally too large, we approached the distribution by random permutation. Random permutation was performedBtimes; each time, all the individuals of the two samples were mixed together and randomly permuted and then re-assigned to the two samples, assuming that there was no difference between them.Random permutation distributions were used as the null distributions ofTcsas well as other statistics for hypothesis testing andp-value computing in this study. Thep-value was obtained as the probability that the statistic of the permutation sample-pair was greater than or equal to the statistic of the original sample-pair. The algorithm of random permutation may refer to Thulin (2014).Permutation does not affect the correlations between variables and does not affect the result of Algorithm 1. Therefore, hierarchical clustering can be performed only once, for the original sample-pair.To evaluate the performance of the cluster subspaces test, we first compared it to other two-sample tests with Monte Carlo simulations.All the computations were conducted in Matlab. Hierarchical clustering was carried out by Matlab functions pdist, linkage, and cluster. Moore–Penrose pseudo-inverse was carried out by Matlab function pinv.The same Monte Carlo setup as Thulin (2014) was used to generate sample-pairs for studying the behavior of each test.For each sample-pair,n1individualsX=(X1,…,Xn1)(sample 1) andn2individualsY=(Y1,…,Yn2)(sample 2) were generated according top-dimension normal distributionN(μ1,Σ)andN(μ2,Σ), respectively. In this study,p=200or 1000;n1=n2=50;Σ=Σa,b=(σij), wherei=1,…,p,j=1,…,p,σij=1,fori=j,a,fori≠j,⌈i/25⌉=⌈j/25⌉,b,for⌈i/25⌉≠⌈j/25⌉,Σa,bdenotes a covariance matrix with unit variances andp/25equal-sized blocks (with size 25), and covarianceσijisaifiandjbelong to the same block andbotherwise;μ1=0,μ2=(μ2j), wherej=1,…,p,μ2j=d,for⌈j/25⌉≤m,mod(j−1,25)<20,0,otherwise ,for sample 2, we shifted the means of 20 out of 25 variables evenly in each ofmout of thep/25blocks;m∈{1,8}forp=200,m∈{8,40}forp=1000;d=D/20m, and therefore,Dequals Euclidean distance‖μ1−μ2‖.Multivariate normal distributions of three covariance structures,Σ0,0,Σ0.5,0.1andΣ0.9,0.2, were studied.Chung and Fraser (1958) proposed a non-parametric test in which the scaled difference in sample means is computed for each variable; then, a sum of the absolute values or a sum of the squares is computed for all the variables. The Chung–Fraser test was chosen for comparison, and the sum of the squares was adopted as the test statistic.Dempster (1958, 1960) proposed a test based on the quantity(X¯−Y¯)T(X¯−Y¯), which was later refined by Bai and Saranadasa (1996) and Chen and Qin (2010). The Chen–Qin test was chosen as a representation of them.Srivastava and Du (2008) proposed a test based on(X¯−Y¯)TDΣ̂−1(X¯−Y¯), which has invariance properties under rescaling transformation. The Srivastava–Du test was chosen for comparison.Srivastava (2007) proposed using the Moore–Penrose pseudo-inverse of the sample covariance matrix when computing Hotelling’s test statistic. It was chosen for comparison.Cai et al. (2014) applied a regularization technique to obtain a sparse estimator of the matrix and proposed a test statistic. Shen and Lin (2015) proposed a test that selects important variables against the null hypothesis. The Shen–Lin test was chosen as a representation of them. However, in a small pilot study, we found the Shen–Lin test to be computationally expensive and to have lower power than the competing tests for the Monte Carlo samples of this study. It was therefore not included in the larger study.Lopes et al. (2011, 2012) proposed a test in which the data are randomly pseudo-projected into several lower-dimensional spaces. Thulin (2014) proposed a modified test using random subspaces to improve the invariance properties. The Thulin test was chosen as a representation of them.For a given sample-pairXandY, each test statistic could be calculated.To calculateTcs, hierarchical clustering should be performed beforehand, and thus thepvariables are clustered into several clusters. Fig. 1shows a sample-pair generated underH0withp=200,μ1=μ2=0,Σ=Σ0.9,0.2. Its variables were clustered into 7 clusters according to Algorithm 1. Supplementary Fig. 1 shows a sample-pair generated underH1withp=200,μ1=0,‖μ1−μ2‖=2(m=8),Σ=Σ0.9,0.2(see Appendix A). Its variables were also clustered into 7 clusters.To evaluate the type I error rates for the tests, a Monte Carlo study was performed under the null hypothesisH0:μ1=μ2=0forp=200or 1000 andn1=n2=50. Sample-pairs were generated in casesΣ0,0,Σ0.5,0.1andΣ0.9,0.2(see Table 1).For a sample-pair generated underH0, each test statistic could be computed. The null distribution could be obtained by random permutation resampling (B=500times). Fig. 2(a) shows the null distribution ofTcsfor the sample-pair of Fig. 1 (generated underH0). The original statisticTcsis marked with a solid line. The statistic of random permutation resampling at level1−α, denoted asTcs,α, is marked with a dashed line. IfTcs≥Tcs,α, a false positive sample-pair is accounted. The proportion of the histogram on the right side of the original statisticTcsis thep-value of the statistic.Tcs≥Tcs,αis equivalent top-value≤α.α=0.05was used in this study.Having generated many sample-pairs, one can calculate the false positive (type I error,p-value≤α) rates for the tests (shown in Table 1). For each case ofp=200, 1000 sample-pairs were generated under the null hypothesis. For each case ofp=1000, 200 sample-pairs were generated.All tests have acceptable type I error rates.The power of a test is the true positive rate under the alternative hypothesisH1:μ1≠μ2.For a sample-pair generated underH1, each test statistic could be computed. The null distribution could be obtained by random permutation resampling. Fig. 2(b) shows the null distribution ofTcsfor the sample-pair of Supplementary Fig. 1 (generated underH1) (see Appendix A). IfTcs≥Tcs,α(p-value≤α), a true positive sample-pair is observed.Having generated many sample-pairs, one can count the sample-pairs of true positive and calculate the true positive rates (powers) for the tests. The powers of the tests as functions of the resulting Euclidean distance‖μ1−μ2‖are shown in Figs. 3 and 4. We generated 1000 sample-pairs for each case ofp=200and 200 sample-pairs for each case ofp=1000.The Chung–Fraser, Chen–Qin, and Srivastava–Du tests are not based on correlation structure. They worked well forΣ0,0. However, forΣ0.5,0.1andΣ0.9,0.2, the powers became poorer and poorer.The Srivastava test gained a little power from the correlation structure. Whenp=200, its powers were lower than or equal to the three non-correlation tests forΣ0,0andΣ0.5,0.1and were higher than the three non-correlation tests forΣ0.9,0.2. Whenp=1000, its powers were lower than the three non-correlation tests except in the case ofm=8forΣ0.5,0.1andΣ0.9,0.2.The powers of Thulin’s random subspaces test were universally higher than the test of Srivastava. The powers of Thulin’s test were a little bit lower than the three non-correlation tests forΣ0,0and became more and more superior to those of the three non-correlation tests as the correlation increased, except in the case ofp=1000,m=40,Σ0.5,0.1(Fig. 4(d)), where the powers were approximately equal to the three non-correlation tests.The cluster subspaces test worked well forΣ0,0as well as forΣ0.5,0.1andΣ0.9,0.2. Its powers were obviously higher than the three non-correlation tests forΣ0.5,0.1andΣ0.9,0.2. Whenp=200, the cluster subspaces test had performance similar to Thulin’s test. Whenp=1000, the cluster subspaces test offered higher power than Thulin’s test in the case of high correlation (Σ0.9,0.2) or a high number of shifted variables (m=40).The above tests were also used to analyze a public dataset of breast cancer.Gravier et al. (2010) studied patients with small node-negative breast carcinoma without axillary lymph node involvement (T1T2N0 tumors, dataset GSE19159). They examined 2905 gene expression levels from 168 patients using comparative genomic hybridization arrays. Their sample consisted ofn1=111patients with no event at 5 years after diagnosis (good-outcome) andn2=57patients with early metastasis (poor-outcome); therefore,kc=110.We applied the tests to three gene-sets from the data. The first was a set ofp=374genes located on chromosome 1; the second, a set ofp=233genes located on chromosome 2; the third, a set ofp=191genes located on chromosome 12.The hierarchical clustering of the genes is shown in Fig. 5, Supplementary Figs. 2 and 3 (see Appendix A). According to Algorithm 1, the 374 genes of chromosome 1 were clustered into 12 clusters based on the cutoff distance, in which the 2nd cluster contained 128 genes (greater thankc) and was further clustered into two sub-clusters 2a, 2b. Similarly, the 233 genes of chromosome 2 were clustered into 15 clusters based on the cutoff distance; the 191 genes of chromosome 12 were clustered into 12 clusters, and each of the clusters contained genes less thankc.We usedk=83,B1=100(dimension, number of random subspaces for Thulin’s test, Thulin, 2014) and a maximumB=1,000,000(number of random permutation resamplings) for computing thep-values.By comparing the real test statistics with those of random permutation resampling,p-values could be obtained (shown in Table 2). Highly significant differences in the gene expression levels of the two patient groups were found for all three gene-sets. The Srivastava test gained some benefit from gene dependence. The random subspaces test and cluster subspaces test were more significant.Test statisticTcswas also applied to each gene-set of the obtained clusters from chromosomes 1, 2, and 12. The results are shown in Tables 3–5and Fig. 6. For chromosome 1, the overall statistic was mainly contributed from its 6th cluster; for chromosome 2, mainly from its 3rd cluster; for chromosome 12, mainly from its 6th cluster.

@&#CONCLUSIONS@&#
This study proposed a test that can make more efficient use of covariance structure at high dimension.Hierarchical clustering is performed such that highly correlated variables are grouped together; the high dimension data are also projected to cluster subspaces of lower dimension in which Hotelling’s statistics can be computed.A cutoff distance was used in calculating clusters, and the effect of statistical fluctuations was restrained.High performance was demonstrated in simulations and real data analysis.Supplementary Figures, the Matlab code for performing the new test and an example of computing the test power with Monte Carlo samples are included in the supplementary material and can be found online.Supplementary material related to this article can be found online at http://dx.doi.org/10.1016/j.csda.2015.12.004.The following is the Supplementary material related to this article.MMC S1An example of computing the test power with Monte Carlo samples.MMC S2MATLAB figure of Fig. 1 presented in the main manuscript.MMC S3MATLAB figure of Fig. 2 presented in the main manuscript.MMC S4MATLAB figure of Fig. 3 presented in the main manuscript.MMC S5MATLAB figure of Fig. 4 presented in the main manuscript.MMC S6MATLAB figure of Fig. 5 presented in the main manuscript.MMC S7MATLAB figure of Fig. 6 presented in the main manuscript.MMC S8MATLAB figure of Supplementary Fig. 1 presented in the online supplementary material.MMC S9MATLAB figure of Supplementary Fig. 2 presented in the online supplementary material.MMC S10MATLAB figure of Supplementary Fig. 3 presented in the online supplementary material.MMC S11MATLAB figure of Supplementary Fig. 4 presented in the online supplementary material.MMC S12MATLAB figure of Supplementary Fig. 5 presented in the online supplementary material.MMC S13MATLAB code performing the test using cluster subspaces.MMC S14MATLAB code performing the test using subspaces of two clusters.MMC S15Supplementary Figs. 1–5.