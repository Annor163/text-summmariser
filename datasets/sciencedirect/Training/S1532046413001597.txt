@&#MAIN-TITLE@&#
Compass: A hybrid method for clinical and biobank data mining

@&#HIGHLIGHTS@&#
Traditional hypotheses testing is not ideal for knowledge discovery in large data.We developed an approach for data mining intended as a hypothesis generating tool.Our approach is able to handle incomplete information.The model can handle both categorical type questionnaire data and continuous variables.The model generates variable groups acting as “hotspots” for significant associations.

@&#KEYPHRASES@&#
Data mining,Clinical data,Rule extraction,Self-Organizing Map,Association mining,

@&#ABSTRACT@&#
We describe a new method for identification of confident associations within large clinical data sets. The method is a hybrid of two existing methods; Self-Organizing Maps and Association Mining. We utilize Self-Organizing Maps as the initial step to reduce the search space, and then apply Association Mining in order to find association rules. We demonstrate that this procedure has a number of advantages compared to traditional Association Mining; it allows for handling numerical variables without a priori binning and is able to generate variable groups which act as “hotspots” for statistically significant associations. We showcase the method on infertility-related data from Danish military conscripts. The clinical data we analyzed contained both categorical type questionnaire data and continuous variables generated from biological measurements, including missing values. From this data set, we successfully generated a number of interesting association rules, which relate an observation with a specific consequence and the p-value for that finding. Additionally, we demonstrate that the method can be used on non-clinical data containing chemical–disease associations in order to find associations between different phenotypes, such as prostate cancer and breast cancer.

@&#INTRODUCTION@&#
Due to the existence of large amounts of data, such as biobank data, collected by scientific groups over the years, there is a growing interest in mining such data for the purpose of new knowledge discovery [1,2]. Traditional hypotheses testing approaches are typically not ideal in more comprehensive data mining efforts aiming for new and unexpected patterns due to the immensely large search space, particularly in high-volume data sets [3].Methods for unsupervised data mining have commonly been employed in market basket analysis and fall under the category of Association Mining (AM). The main goal of AM in market basket analyses is to find interesting associations of the form “{chips}→{beer}” which would indicate that people who buy chips are likely to also buy beer. More complex rules involving more items may also be formed such as “{ham+cheese}→{milk+bread}, i.e. those who buy ham and cheese are likely simultaneously to also buy milk and bread. In these rules items appearing on the left side of the arrow are called antecedent, while items on the right side are called consequent. As market basket data sets tend to be large, and the number of possible combinations between items may be extremely high, the central effort in this type of mining has been to restrict the search space in a sensible way.The concept of association rules was popularized by Agrawal et al. [4], although the concept may have already been created as far back as 1966 [5]. AM has traditionally consisted of two steps: first to find frequent item sets, and second to generate rules by calculating the confidence, which may give an estimate of the interestingness of the rule. Frequent item sets are collections of items, such as “{bread, milk, butter}”, which appear together (customers often buy these together) more often than a specified threshold called support. The confidence indicates the probability of the consequent given the antecedent in a given rule, say “{bread}→{milk, butter}”, and is often used to restrict which rules are retained. There are other measures of interestingness besides confidence, such as lift, leverage and conviction, which we will not discuss here.The various types of existing AM methods typically address relatively simple dichotomous data sets containing only 1s and 0s. Applying AM to mine other types of data, such as clinical data, has been done in several, previous studies [6]. These approaches utilized the support measure to control the size and shape of the search space of associations. However, restricting the search space by applying one strict minimum support threshold, thus generating frequent item sets, as has traditionally been done in AM, is not necessarily an optimal strategy when analyzing clinical data. The reason is primarily due to the nature of such data, where some features and associations, may only be present in a subset of samples, for instance certain types of tests that only apply to certain diseases. When generating frequent item sets, if setting the support threshold too high, there is a risk that one will not find associations which contain these non-frequent, but interesting, features. However, if setting the threshold low enough to detect these associations, the combinatorial explosion of the number of rules generated may prevent detailed manual inspection.In this paper we present Compass, a new hybrid approach using a combination of Self-Organizing Maps (SOM) and AM, which is suitable for mining unexpected patterns in clinical data and other similar types of data. In this approach, SOM is applied as a first step before the application of AM. The reason for applying SOM is that it acts as a navigating pointer to areas in the search space where one is more likely to find statistically significant association rules, hence acting as a Compass to reduce the search space.SOM, also known as Kohonen neural networks, is an unsupervised neural-network method which clusters the data into a set of interconnected nodes [7,8]. These nodes are typically arranged in two-dimensional maps with rectangular or hexagonal grids, but may also be arranged in multi-dimensional maps with other types of grids. The training process is started by randomly assigning to each node a model vector, which has the same dimensionality as the data samples (i.e. the same number of variables). All samples are then iteratively assigned to the nodes whose’ model vector is most similar (often measured with Euclidian distance), and gradually regressing the model vectors towards each sample that is assigned. At the end of the training process, each node will have a number of samples (clusters) assigned to it as well as a model vector. The patterns of interest are extracted from the model vectors. Since the time of the introduction of SOM in 1982, different variants of this method have found their way into many practical applications in different scientific fields including biology, economics and physics [9]. The idea of combining SOM with association rule mining is not new [10]. However, previous studies did not fully explore the potential of this approach on complex medical data. In particular, they did not explore the ability of SOM to address two important challenges in the context of association mining; handling numerical variables and dealing with a large output of association rules to find the most interesting and unexpected ones.Our method can handle numerical variables without resorting to a priori binning, i.e. grouping numerical variables into classes before the analysis. Such grouping is common when studying for example income, wherein people are assigned to a number of arbitrary and discrete classes depending on their level of income. While previous studies have explored novel ways to deal with numerical variables without binning, to our knowledge these approaches rely on setting a minimum support cut-off to find frequent item sets [14]. Our approach makes it also much easier by manual inspection to identify new, interesting and unexpected patterns by introducing the concept of Associative Variable Groups (AVGs). AVGs are simply groups of variables which are much more likely to contain statistically significant association rules than any variables randomly chosen from the data, and which arise naturally using SOM as the initial step. Instead of generating a long list of possibly thousands of association rules as the output, our method generates a list of AVGs, which drastically reduces the size of the output and makes it much more comprehensible for subsequent manual inspection.

@&#CONCLUSIONS@&#
