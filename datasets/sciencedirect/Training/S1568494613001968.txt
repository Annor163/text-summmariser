@&#MAIN-TITLE@&#
Incorporating linear discriminant analysis in neural tree for multidimensional splitting

@&#HIGHLIGHTS@&#
NTLD classifier gives good classification accuracy in case of multi-class problem.NTLD generates shallower tree than NT in case of multi-class problem.NTLD is computationally fast.NTLD does not require the ad-hoc parameters.

@&#KEYPHRASES@&#
Decision tree,Linear discriminant analysis,Neural network,Neural tree,Pattern classification,

@&#ABSTRACT@&#
In this paper, a new hybrid classifier is proposed by combining neural network and direct fractional-linear discriminant analysis (DF-LDA). The proposed hybrid classifier, neural tree with linear discriminant analysis called NTLD, adopts a tree structure containing either a simple perceptron or a linear discriminant at each node. The weakly performing perceptron nodes are replaced with DF-LDA in an automatic way. Taking the advantage of this node substitution, the tree building process converges faster and avoids the over-fitting of complex training sets in training process resulting a shallower tree together with better classification performance. The proposed NTLD algorithm is tested on various synthetic and real datasets. The experimental results show that the proposed NTLD leads to very satisfactory results in terms of tree depth reduction as well as classification accuracy.

@&#INTRODUCTION@&#
The classification is a machine learning procedure in which individual items are placed into groups based on the characteristics inherent in the items and a training set of previously labeled items. In general, decision trees (DT) [1] and neural networks (NN) [2] are two powerful tools for pattern recognition and attracted a lot of interest in last decades. These techniques learn to classify instances by mapping points in the feature space onto classes without explicitly characterizing data in terms of parameterized distributions. Decision trees are easy to understand, but sensitive to noise in feature measurements. In particular, the threshold values used in tests at internal nodes are critical and consequently the sequential hard splitting classification process of a DT can lead to performance degradation. Several neural network models have been successfully applied to various problems in pattern classification, such as multilayer perceptrons (MLPs) using equalized error backpropagation [3], radial basis functions (RBFs) [4], self-organizing maps (SOMs) [5] and a number of their variants. However, they suffer of difficult problems to solve, such as the structure and the size of the network (number of neurons and connections involved, number of hidden layers, etc.), computational complexity and convergence analysis.Neural trees were introduced to combine neural networks and decision trees in order to take advantages of both and to overcome some limitations. According to the existing literature, neural trees can be classified into two categories. The first category uses decision trees to form the structure of the neural network. The main idea is to construct a decision tree and then convert the tree into a neural network. In [6], a three-layered neural network has been proposed by extracting its hidden nodes from a decision tree. Some more details like complexity analysis and practical refinements about this idea have been given in [7]. An extension to this idea has been proposed in [8], where linear decision trees are used as the building blocks of a network. In [9], a modification of the ID3 algorithm [1] called continuous ID3 algorithm has been proposed. A continuous ID3 algorithm converts decision trees into hidden layers. In the learning process, new hidden layers are added to the network until a learning task becomes linearly separable at the output layer. Thus the algorithm allows self-generation of a feedforward neural network architecture. Cauchy training is used to train the resulting hybrid structure.The second category uses neural networks as building blocks in decision trees. In [10], a hybrid form has been proposed containing neural networks at the leaf nodes and univariate decision nodes as the non-leaf nodes in the tree. In [11], univariate decision nodes have been replaced by single layer perceptrons, i.e., linear multivariate decision nodes at the internal nodes. A new tree pruning algorithm has been proposed for this model based on a Lagrangian cost function [12]. The nonlinear multivariate decision tree with MLPs at the internal nodes has been proposed in [13]. In [14], a generalized neural tree (GNT) model has been proposed, where the activation values of each node are normalized so that these can be interpreted as a probability distribution. The main novelty of the GNT consists in the definition of a new training rule that performs an overall optimization of the tree. Each time the tree is increased by a new level, the whole tree is re-evaluated. An adaptive high-order neural tree (AHNT) has been proposed in [15] by composing high order perceptrons (HOP) instead of simple perceptrons in a neural tree model. First-order nodes divide the input space with hyperplanes, while HOPs divide the input space arbitrarily, but at the expense of higher computational cost.In [16], a structural adaptive intelligent tree, called ‘SAINT’ has been proposed where the input feature space is hierarchically partitioned by using a tree-structured network that preserves a lattice topology at each subnetwork. Experimental results reveal that ‘SAINT’ is very effective for the classification of large sets of real words, hand-written characters with high variations, as well as multi-lingual, multi-font, and multi-size large-set characters. In [17], it has been observed that the complexity of the nodes effect the size of tree. A tree with complex nodes may be quite small, while a tree with simple nodes may grow very large. They proposed a new class of DTs where decision nodes can be univariate, linear or nonlinear depending on the outcome of comparative statistical tests. The drawbacks of this method are the long training time and the need of appropriately selecting the parameters for the MLPs.Recently, a pre-pruning strategy for MLP based tree has been proposed in [18] by defining a uniformity index for modeling the degree of correctness at each node. Due to the uniformity index, it has been shown that a significant reduction in the depth of the tree is achieved with a good classification accuracy. However, no rule has been given to define the values of the parameters like the networks architecture (number of hidden layers and number of nodes for each layer) and the uniformity index for different context as well as each training set. In [19], a new algorithm family, called ‘Cline’ has been proposed by incorporating a number of different methods for building multivariate decision tree. Several algorithms are used at each node and the best one is selected at the respective node. In this way, it searches for the best fitting algorithm at each node/current subspace.Use of a single layer perceptron is a good choice to avoid the problem of selecting an optimal architecture for multilayer perceptron and the parameters for exhaustive search in heuristic methods. However, a single layer perceptron can get stuck into a local minima or it may keep on stalling in a flat region since it does not have a strong nonlinear function approximation property as in the case of MLPs. This fact generates a large depth tree or a non-converging training process. A solution to this problem has been provided in [20] by introducing split nodes. In case of the above situation, the split node simply divides the local training set into two parts by generating a hyperplane passing through the barycentre of the local training set. Such kind of split nodes ensure the convergence of the tree-building process but they are not able to provide any solution in case of large depth trees.The main novelty of the proposed work includes an implementation of the direct fractional-step discriminant analysis (DF-LDA) [21] into the neural tree for obtaining a multi-dimensional split in place of the above described two-dimensional split [20]. The proposed NTLD provides a good solution for convergence of the tree-building process as well as a reduction in the tree-depth. Moreover, there is no need to define a number of parameters such as number of hidden layers, number of nodes for each layer and the uniformity index as in [18], since we are using single layer perceptrons. A regularized linear discriminant analysis is used to handle the situation when the number of training vectors is smaller than the feature dimensionality (i.e., when the with-in-class scatter matrix of the samples is singular). The proposed hybrid classifier has been tested on various benchmark data-sets as well as on synthetically generated four-class chessboard data-set. A comparison study has been provided between the proposed NTLD, NT [20], DF-LDA, MLP [22] and Naive Bayes classifier [22] for evaluating the performance over other existing algorithms.In the proposed work a multi-dimensional split using DF-LDA is introduced to overcome the disadvantages of existing Neural tree algorithms [11,20]. In this section a brief overview of NT [11] algorithm, splitting nodes [20] and DF-LDA are given.A neural tree (NT) is a decision tree where each intermediate/non-terminal node is a simple perceptron. Being supervised learning model, the NT accomplishes the task of classification in two phases: (a) training and (b) classification.The NT is constructed by linearly partitioning the training set, consisting of feature vectors and their corresponding class labels, for generating the tree in a recursive manner. Such a procedure involves three steps: computing internal nodes, determining and labeling leaf nodes. An intermediate node groups the input patterns into different classes. When a child node becomes homogeneous i.e., all the patterns in a group belong to the same class, the associated child node to this group is made a leaf node. The related class is used as label for the leaf node. Thus, NTs are class discriminators which recursively partition the training set such that each generated path ends with a leaf node. The training algorithm together with the computation of the tree structure calculates the connection's weights for each node. Connection's weights are adjusted by minimizing an objective function as mean square error or some other error function. At each node the weights corresponding to the optimal value of the objective function are used during the classification of test patterns. The NT training phase is summarized in Algorithm 1. The descriptions about the functions “Train Perceptron” and “Classify_NT” mentioned in the training algorithm are explained below:It trains a single layer perceptron at nodevon the training set T′. The perceptron learns until the error is not reducing any more for a given number of iterations. A trained perceptron generates (o1, …, oc) activation values.It assigns the pattern to the class corresponding to the highest activation value. In other words, it divides T′ intoTˆ={T1,…,Tc},c≤Csubsets and generates next level of child nodesvˆ={v1,…vc},c≤Ccorresponding toTˆ. Returns(vˆ,Tˆ).Algorithm 1Training phase of NTSetSv={v0}andST′={T}while(Sv)dov=Pop(Sv)andT′=Pop(ST′)Train Perceptron(v,T′)(Svˆ,STˆ)=Classify_NT(v,T′)whileSTˆdoifT˜=Pop(STˆ)is homogeneous thenv˜=Pop(Svˆ)is set to leafelsePush(Sv,Pop(Svˆ))andPush(ST′,T˜)end ifend whileend whileFor the classification task, unknown patterns are presented to the root node. The class is obtained by traversing the tree in a top-down way. At each node, activation values are computed on the basis of connection's weights. Starting from the root, the activation values of the current node determine the next node to consider until a leaf node is reached. Each node applies the “winner-takes-all” rule so that(1)x∈classi⇔σ(wj,x)≤σ(wi,x)forallj≠iwhere σ is the sigmoidal activation function, wiis the vector of the weights for the connections from the inputs to the ith output, and x is the input pattern. This rule determines the class associated with a leaf node as well as the next internal node to be considered on the path to reach a leaf node. The classification phase is summarized in Algorithm 2.Algorithm 2Classification phase of NTSetv=v0while(v)≠leafnodedoInput x tovSetv=vicorresponding to argmaxi{o1, …, oC}end whileAssign x to cicorresponding tovThe symbols used in the above Algorithms 1 and 2 are explained in Table 1.A single layer perceptron is simple to train but sometimes, when the TS is non linearly separable, the training process falls into a local minimum or stalls in a flat region. In such a case, the neural tree using single layer perceptron to learn and split the training set may not generate a splitting rule and keep on passing the same local training set to the successive child nodes hence leading to non convergent tree building process. To handle such a situation Foresti and Pieroni [20] employed an univariate split node when perceptron fails to split the training set. This particular splitting rule at first computes the barycentres of two classes with higher cardinality and the component k which satisfy L1 norm is identified. Then it computes the splitting hyperplane orthogonal to the k axis and passing through the median point between the two barycentres (see Fig. 1). This kind of split node guarantees the convergence of tree building process but the two-dimensional split may generate deeper tree resulting in overfitting and hence degraded performance. A tree building process using split nodes has been demonstrated in Fig. 2for a five-class data-set. The split node each time chooses the two dominating classes and classifies the patterns of other classes also as belonging to these two classes. In this way the work done by perceptron is lost and the tree building process may generate a large number of nodes before reaching convergence.A LDA node searches for those vectors in the underlying space that best discriminate among classes (rather than those that best describe the data). Then the objective of LDA is to seek the direction Ψ not only maximizing the between-class scatter of the projected samples, but also minimizing the with-in-class scatter. These two objectives can be achieved simultaneously by maximizing the following function:(2)J(Ψ)=ΨTSBΨΨTSWΨwhere the between-class scatter matrix SBis defined as(3)SB=∑i=1Cni(mi−m)(mi−m)Tin which m is the k-dimensional sample mean for the whole set, while miis the sample mean for ith class. The with-in-class scatter matrix SWis defined by(4)SW=∑i=1CSiwhere the scatter matrix Sicorresponding to ith class is defined by(5)Si=∑x∈Di(x−mi)(x−mi)T,i=1,…C.However, the traditional LDA suffers two problems (1) the degenerated scatter matrices caused by the so-called “small sample size” (SSS) problem, (2) the classification performance is often degraded by the fact that their separability criteria are not directly related to their classification accuracy in the output space [23]. To avoid such problems a regularized version of LDA i.e. “direct fractional-step linear discriminant analysis” (DF-LDA) is used. The method DF-LDA uses variants of “direct-linear discriminant analysis” (D-LDA) [24] and fractional step- discriminant analysis (F-LDA) [23]. The traditional solution to the SSS problem requires the incorporation of a principal component analysis (PCA) step into the LDA framework, which may cause loss of significant discriminatory information. In the D-LDA framework, data are processed directly in the original high-dimensional input space avoiding the loss of significant discriminatory information due to the PCA pre-processing step. A solution to the second problem is introducing weight functions in LDA. Object classes that are closer together in the output space, and thus can potentially result in misclassification, should be more heavily weighted in the input space. This idea has been framed in [23] with the introduction of the fractional-step linear discriminant analysis algorithm (F-LDA), where the dimensionality reduction is implemented in a few small fractional steps allowing for the relevant distances to be more accurately weighted.The proposed NTLD classifier is basically a decision tree whose nodes are either single layer perceptrons without hidden nodes or split nodes using DF-LDA. The adopted perceptron takes k inputs and generates C outputs, called activation values. Winner-takes-all rule is applied and the class with the highest associated activation value is taken to be the winner class. Each perceptron is characterized by a sigmoid activation function σ(x)=1/(1+e−x). Thus the activation valueoiqof the qth pattern corresponding to the class ciis given by:(6)oiq=1/[1+exp(−∑j=1kwijxjq)]wherewijare the elements of the weight matrix W of the perceptron. A DF-LDA node projects the patterns into a new space in order to bring the patterns of same class nearer and of different classes farther. The Euclidean distance of a pattern is computed from the centroid of each class in the projected space. A pattern is classified as belonging to the class whose centroid is closest to the projected pattern. The detailed descriptions of the adopted training and testing strategies are given in the following subsections.Let T={(xj, ci)|j=1, …, n∧ci∈[1, C]} be the training set containing n number of k-dimensional xj={x1, x2, x3, …, xk} patterns, each of them belonging to one of the C classes. The training phase of the NTLD is described in the Algorithm 3. The notations used in Algorithm 3 have already been given in Section 2. The algorithm starts with a simple perceptron at root node (v0) and whole training set (T) as input to the root node. The stacksSvandST′initially holds onlyv0and T respectively. The last elements ofSvandST′are popped intovand T′ and processed i.e. a perceptron is trained at the nodevwith input as T′. The trained perceptron at nodevsplits the LTS T′ into further LTSsTˆ={T1,…,Tc},c≤Cgenerating next level of child nodesvˆ={v1,…vc},c≤Cwhich are held in the local stacksSTˆandSvˆrespectively. If the local stack|STˆ|=1, i.e., the perceptron is unable to split the LTS and passes it to the child node as it is, such perceptron is replaced with DF-LDA at this node. A DF-LDA is trained at nodevfor LTS T′ to split it into further LTSsTˆ={T1,…,Tc},c≤Cand corresponding child nodesvˆ={v1,…vc},c≤C. Each child node is popped intovˆand corresponding LTS intoTˆfrom the local stacksSvˆandSTˆand checked ifTˆis homogeneous i.e. all the patterns ofTˆbelongs to the same class. If yes then this node is marked as leaf node and labeled with the class of related patterns if not the nodevˆis pushed into the stackSvand the corresponding LTSTˆis pushed into the stackSS′. The algorithm runs until the stackSvcounts to 0, i.e., all the nodes becomes leaf nodes. Descriptions about the functions “Train LDA” and “Classify_LD” mentioned in Algorithm 3 are explained below:Algorithm 3Training phase of NTLD classifierSetSv={v0}andST′={T}while(Sv)dov=Pop(Sv)andT′=Pop(ST′)Train Perceptron(v,T′)(Svˆ,STˆ)=Classify_NT(v,T′)if|STˆ|=1thenTrain LDA(v,T′)(Svˆ,STˆ)=Classify_LD(v,T′)end ifwhileSTˆdoifT˜=Pop(STˆ)is homogeneous thenv˜=Pop(Svˆ)is set to leafelsePush(Sv,Pop(Svˆ)andPush(ST′,T˜)end ifend whileend whileIt employs DF-LDA to project the patterns into a new space such that the ratio J(Ψ) is maximized. After finding such a projection matrix, T′ is projected into the new space and the centroid of the classes are computed.Each pattern xj∈T′ is projected in the new space. The Euclidean distances (d1, …, dc) of the projected pattern with respect to classes centroid are computed. The pattern xjis assigned to the class corresponding to the minimum distance. Thus it divides T′ intoTˆ={T1,…,Tc},c≤Csubsets and generates next level of child nodesvˆ={v1,…vc},c≤Ccorresponding toTˆ. Returns(vˆ,Tˆ).The classification phase of NTLD is inherited from NT. The same top-down traversal technique is adopted. The pattern moves through the tree starting from the root node and adopting the path suggested by the classification given by each node (maximum activation value in case of perceptron/minimum Euclidean distance in case of DF-LDA) (see Fig. 3). When a leaf node is reached, the pattern is classified corresponding to the label of this node. The steps involved in the classification of a pattern x=(x1, x2, …, xk) at the current nodevare given in the Algorithm 4.Algorithm 4Classification phase of NTLD classifierSetv=v0while(v)≠leafnodedoInput x tovifv=Perceptron node thenSetv=vicorresponding toargmaxi=1,…,Coiend ififv=LDA node thenSetv=vicorresponding toargmini=1,…,Cdiend ifend whileAssign x to cicorresponding tovUsing the above described steps, all patterns from a testing set can be classified into their corresponding classes.

@&#CONCLUSIONS@&#
We have presented a hybrid classifier composed by simple perceptrons and linear discriminant classifiers in a tree structure. The main novelty in the proposed classifier is the adoption of multi-dimensional split using DF-LDA when perceptron is not efficient in classifying patterns. We have tested the proposed classifier on various data-sets and derived the following remarks:(1)The proposed NTLD classifier gives a good classification accuracy in case of multi-class problem (almost always the best except MLP in case of few data-sets). It performs almost similar to NT in case of two-class problems.NTLD generates shallower tree than NT in case of multi-class problem resulting in a faster classification.NTLD does not require the ad-hoc parameters like details about network architecture (number of hidden layers and nodes in each layer) as in case of MLP. Only one parameter i.e., learning rate is chosen using validation set.The proposed classifier is easy to implement and adopts the good properties of neural network as well as linear discriminant classifiers. It is more accurate in case of complex classification problems where number of classes is large.