@&#MAIN-TITLE@&#
Semi-supervised learning for refining image annotation based on random walk model

@&#HIGHLIGHTS@&#
A unified framework for refining annotation by fusing GMM with random walk.A semi-supervised learning is used to enhance the quality of training data.Gaussian mixture model is learned to estimate initial annotations.Integrating label and visual similarities of images associated with the labels.Random walk is implemented on graph to further mine the semantic correlation.

@&#KEYPHRASES@&#
Automatic image annotation,Semi-supervised learning,Gaussian mixture model,Expectation-maximization,Random walk,Image retrieval,

@&#ABSTRACT@&#
Automatic image annotation has been an active research topic in recent years due to its potential impact on both image understanding and semantic based image retrieval. In this paper, we present a novel two-stage refining image annotation scheme based on Gaussian mixture model (GMM) and random walk method. To begin with, GMM is applied to estimate the posterior probabilities of each annotation keyword for the image, during which a semi-supervised learning, i.e. transductive support vector machine (TSVM), is employed to enhance the quality of training data. Next, a label similarity graph is constructed by a weighted linear combination of label similarity and visual similarity of images associated with the corresponding labels. In this way, it can seamlessly integrate the information from image low-level visual features and high-level semantic concepts. Followed by a random walk process over the constructed label graph is implemented to further mine the correlation of the candidate annotations so as to capture the refining results, which plays a crucial role in semantic based image retrieval. Finally, extensive experiments carried out on two publicly available image datasets bear out that this approach can achieve marked improvement in annotation performance over several state-of-the-art methods.

@&#INTRODUCTION@&#
Automatic image annotation (AIA) is a promising solution to enable the semantic based image retrieval via keywords. It is generally believed that AIA refers to a process to automatically generate textual words to describe the content of a given image. In recent years, the state-of-the-art research on AIA has broadly proceeded along two categories. The first one views image annotation as a supervised classification problem [1] that treats each semantic concept as an independent class and constructs different classifiers for different concepts. This approach predicts the annotations of a new image by computing similarity at the visual level and propagating the corresponding keywords. The second category treats the words and visual tokens in each image as equivalent features in different modalities. Image annotation is then formalized by modeling the joint distribution of visual and textual features on the training data and predicting the missing textual features for a new image. As the representative work of this perspective, Duygulu et al. [2] propose a translation model (TM) to treat AIA as a process of translation from a set of blob tokens, obtained by clustering image regions, to a set of keywords. Jeon et al. [3] put forward cross-media relevance model (CMRM) to annotate images, assuming that the blobs and words are mutually independent given a specific image. Subsequently, CMRM is improved through continuous-space relevance model (CRM) [4], multiple-Bernoulli relevance model (MBRM) [5] and dual cross-media relevance model (DCMRM) [6], etc. In addition, several nearest-neighbor-based methods have also been proposed in the most recent years [7,8]. Despite most of these methods have achieved encouraging results, there are still two problems remain to be solved. First, in most cases, labeled images are often hard to obtain or create in large quantities while the unlabeled ones are easier to collect. So how to efficiently use the unlabeled images to improve the annotation performance is a key issue to formulate effective semantic models. Second, little effort focuses on the semantic context and semantic correlation between image annotations. Even if considered, the image visual information related to the annotation often tends to be ignored, which is liable to cause the phenomenon that different images with the same candidate annotations1Candidate annotations denote the initial annotations generated by some image annotation models.1would obtain the same refinement results.To address the above issues, we present a novel two-stage refining image annotation scheme based on Gaussian mixture model and random walk. On the one hand, GMM is employed to estimate the posterior probabilities of each annotation keyword for the image, which can be seen as the initial annotation stage. It is worth noting that a semi-supervised learning, viz. transductive support vector machine, is introduced into GMM to enable the unlabeled images to be fully exploited to improve the GMM performance to some degree. On the other hand, a random walk process over the constructed label graph is implemented to further mine the correlation of the candidate annotations for precise annotation, which can be regarded as the refining annotation stage. Experimental results on two publicly available image datasets validate the effectiveness of the proposed approach. To the best of our knowledge, this study is the first attempt to integrate GMM with random walk as well as semi-supervised learning in the task of refining image annotation.The rest of this paper is organized as follows. Section 2 discusses some related work about image annotation. In Section 3, Gaussian mixture model is first introduced, and then the TSVM as well as random walk is elaborated respectively, especially for the proposed SGMM-RW refining annotation framework. Experimental results are reported and analyzed in Section 4. Finally, the paper is ended with some important conclusions and future work in Section 5.

@&#CONCLUSIONS@&#
This paper has proposed a refining image annotation method by integrating GMM with random walk as well as TSVM. Specifically, GMM is first used to estimate posterior probabilities of each annotation keyword for the image, during which TSVM is employed to improve the quality of training dataset. Next, a label similarity graph is constructed by a weighted linear combination of label similarity and visual similarity of images associated with the corresponding labels, which enables to construct an accurate semantic correlation of initial annotations. Afterwards a random walk process over the built label graph is exploited to further mine the correlation of the candidate annotations so as to capture the refining results. Experiments on Corel5k and Mirflickr25k datasets have shown its outstanding annotation performance over several state-of-the-art methods.Except for the summary mentioned above, several important conclusions can be drawn as follows. First, similar to [5], when only a 36-dimensional feature vector for each image region/block is extracted for experiment, the results are nearly the same as that generated in previous text by trial and error, which indicates that there is no direct relationship between low-level visual features and high-level annotation performance, mainly involving the feature types and dimensions extracted. Second, measuring the similarity between pairwise concepts in the context of image annotation and retrieval is a challenging task if not impossible. Compared to WordNet and NGD, the pairwise similarity strategy proposed in this paper can efficiently avoid the noise data introduced by polysemous words. Third, since image segmentation is still an open issue in computer vision, Ncuts rather than JSEG is utilized to segment images into a number of meaningful regions here. So to explore more efficient image segmentation methods is helpful to boost the performance of SGMM-RW. Furthermore, image segmentation itself is a worthy of further research direction. Last but not the least, the proposed refining image annotation framework, in actual fact, can be extended with other methods to evolve new annotation models. Owing to this generality, we consider to connect the algorithm to process other multimedia data such as audio and video for our future work.