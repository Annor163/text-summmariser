@&#MAIN-TITLE@&#
Comparison of human and computer performance across face recognition experiments

@&#HIGHLIGHTS@&#
We review experiments comparing humans and machines in NIST face recognition tests.We introduce the cross-modal performance analysis (CMPA) framework.We apply the CMPA framework to the human–machine experiments in the NIST tests.We propose a challenge problem to develop algorithms with human level performance.

@&#KEYPHRASES@&#
Face recognition,Algorithm performance,Human performance,Challenge problem,

@&#ABSTRACT@&#
Graphical abstract

@&#INTRODUCTION@&#
Overall, humans are the most accurate face recognition systems. People recognize faces as part of social interactions, at a distance, in still and video imagery, and under a wide variety of poses, expressions, and illuminations. A holy grail in automatic face recognition is developing an algorithm that has performance equivalent to humans—this is equivalent to solving the general face recognition problem. While it is easy to state the problem, accuracy equivalent to humans, it is not obvious how to determine if an algorithm's recognition accuracy is better than a human. One of the key challenges is establishing a measurable goal line and knowing when the goal line is crossed.Since 2005, human and computer performance has been systematically compared as part of face recognition competitions conducted by the National Institute of Standards and Technology (NIST) [1–4]. The comparisons provided an assessment of accuracy for both humans and machines for each competition. However, there has not been a systematic analysis of these results across the competitions.To analyze the results across experiments, we introduce the cross-modal performance analysis (CMPA) framework, which is demonstrated on the NIST competitions. CMPA was adapted from techniques in neuroscience that were developed to compare output from different sensing modalities of brain activity; e.g., functional magnetic resonance imaging (fMRI) and human perceptual judgments [5,6]. These techniques can measure concordance between experimental data and computational models. In our study, the modalities compared are human and algorithm performance. In the psychology and neuroscience literature, face recognition algorithms can be referred to as computational models. The computational model can be designed to optimize performance or to model the human face recognition processes. The framework is sufficiently general that it provides a goal line for determining when machine performance reaches human levels.On frontal faces in high quality still images, our analysis shows that machine performance is superior to humans. For these images, machines represent a person's identity primarily by encoding information extracted from the face; information from the body, hair, and head is generally ignored. For video and extremely difficult-to-recognize face pairs, experiments show that humans take advantage of all available identity cues when recognizing people [7,8]. CMPA quantifies the potential for improving machine performance if all possible identity information is encoded by algorithms.Comparing machine and human performance started with independent experiments in NIST competitions. The synthesis of the results across experiments gives a greater understanding of the relative strengths of machines and humans. The CMPA framework provides a goal line for determining if algorithm and human performance is comparable on the general face recognition problem.We examine the relative performance of humans and machines for both still and video imagery. This review section presents the key details and conclusion for each study. The key details and conclusions were selected to lay the groundwork for the cross-experiment analysis in Section 3. The summary includes an overview of the images in the experiment, how the images were selected for measuring human performance, the key receiver operating characteristics (ROCs) comparing machine and humans, and the headline conclusions for each experiment. References are provided for full details on each experiment and the associated competitions.

@&#CONCLUSIONS@&#
The most studied area of face recognition is recognition from high quality still frontal face images. In our studies these are represented by images taken with a digital single lens reflex camera. For the seven experiments described in Section 2.2, our analysis shows that machines are superior to normal humans. For these experiments, the faces contain significant identity cues.The results on the video and extremely-difficult face pairs show conditions where human performance is superior to machines. In the video experiment, human and machines are at near parity when pose is the same in both video sequences and only face identity cues are considered. When there was a change in pose, human performance was superior to machine performance. On both the video and extremely-difficult face pair experiments, human performance is superior when non-face identity cues are dominant. These results suggest that humans effectively integrate non-face identity cues into the recognition process and that humans take advantage of the head and body in identifying someone. Because the automatic face recognition community has developed algorithms that compensate for changes in pose, future experiments should directly compare human and machine performance on changes in pose.The CMPA provides a high level summary across multiple experiments. One direction for future analysis is developing statistical models of both human and machine performance. One example is generalized linear models that allow for analysis that explicitly models the effect of covariates [23,24]. This class of analysis has been restricted to algorithm performance on a single data set. To extend this technique to the problem described in this paper, the models need to be able to analyze results on multiple data sets and incorporate human matching results.The analysis in Section 3 directly compared human and machine performance. There is more to learn from the interplay of machines and humans than what can be learned from relative performance comparisons. We will examine this interplay in the context of three topics. The first is the other-race effect, where algorithms have contributed to understanding the human face processing systems and human face processing has contributed to understanding machine performance. Second, it has been possible to improve techniques for the analysis of machine performance based on the design of human experiments. Finally, the effect of fusing machine and humans is reviewed and can reveal strategy differences in the way humans and machines perform the tasks.The other-race effect is a classic property of human face recognition. Our ability to recognize the identity of faces from our own race is better than our ability to recognize the identity of faces of other-races. The other-race effect for face recognition has been established in numerous human memory studies [25] and in meta-analyses of these studies [26–28].Phillips et al. [29] looked for an other-race effect in algorithms submitted to the FRVT 2006. They compared the performance of a fusion of East Asian algorithms and a fusion of Western algorithms matching identity in pairs of Caucasian and East Asian faces. The East Asian algorithm was a fusion of five algorithms submitted from East Asian countries, and the Western algorithm was a fusion of eight algorithms from Western countries.The study showed an other-race effect for the algorithms. Specifically, performance of face recognition algorithms varies as a function of the demographic origin of the algorithm and the demographic contents of the test population. The mechanisms underlying the other-race effect for humans are reasonably well understood and are based in early experience with faces of different races. Because the algorithms tested were black boxes, conclusions about the mechanisms underlying the algorithm effects are tentative, but the effects reported were not. The results point to an important performance variable combination that has not received much attention. The results also suggest a need to understand how the ethnic composition of a training set impacts the robustness of algorithm performance. From a practical perspective, algorithms need to be evaluated on face sets whose demographics match those at the location(s) where they will be used.Furl et al. [30] used computational models to investigate two competing hypotheses to account for the other-race effect in humans. First, the generic contact hypothesis links the magnitude of the other-race effect in individuals to the relative amount of contact they have with own versus other race faces. Thus, people who see many individuals of another race on a daily basis should have a smaller other-race effect than those who rarely see other-race individuals. This effect is modeled using a principal components analysis (PAC)-based face recognition algorithm, where the proportion of faces in the training set from two races was varied [31,14,13]. Second, the developmental contact hypothesis assumes also that experience is the cause of the other-race effect, but that experience early in life (up to about 5years of age) with other-race faces is the critical factor. The rationale for this hypothesis is that the neural system early in life tunes itself to the statistical structure of the environment as it selects a feature set that will optimally represent the stimuli encountered most frequently. Consistent with the predictions of a developmental contact hypothesis, experience-based models demonstrated an other-race effect only when the representational system was developed through experience that warped the perceptual space in a way that was sensitive to the overall structure of the model's experience with faces of different races. These models were based on combinations of PCA and Fisher discriminant analysis (FDA) applied as the system acquired features for representing faces [32–34]. The results from this study supported a developmental contact hypothesis for the formation of the other-race effect in humans.Traditionally, attempts to improve algorithm performance have emphasized methods that increase the degree of similarity between two images of the same person; i.e., by modeling the effects of changes in illumination between two images of the same face. Less consideration has been given to the effects of the composition of the different-identity distributions in producing stable estimates of algorithm performance. In human experiments, the faces in different-identity pairs always have the same sex, race, and approximate age. The reason for this condition is that humans rarely confuse faces of different sexes, races, or age groups. However, in the majority of face recognition algorithm competitions, different-identity pairs are cross demographic. In fact, in many cases, the majority of different-identity pairs are cross-demographic. Inspired by the design of human experiments, O'Toole et al. [35,36] looked at the effect of algorithm performance as cross-demographic face pairs were limited. Experiments were performed on the GBU face challenge. Performance was measured in four cases. First, performance was measured when there was no pruning of the different-identity face pairs. This is the control case. Second, the faces in all different-identity pairs were of the same sex. Third, the faces in all different-identity pairs were of the same race. Fourth, the faces in all different-identity pairs were of both the same race and sex. On the Bad partition, at a false accept rate of 1 in 1000, the verification rate was 0.79 for the control case, 0.74 and 0.74 for demographic matching on sex or race, and 0.69 for different-identity pairs with the same race and sex. In these experiments, performance was measured for the fusion algorithm [11]. A similar reduction in performance, measured as verification rate, was observed for face pairs in the Ugly partition.The simulations over the four cases showed that differences in the demographic composition of the different-identity distribution can significantly alter the estimates of algorithm performance. These estimates are important for predicting how algorithms will perform in real-world environments. Furthermore, these results pose a new and pressing challenge for the biometric community to find a method for tuning algorithm performance to the constantly changing demographic environments in which systems must operate reliably.We end this review with two related questions. “Is an algorithm a reasonable model for human face recognition?” and “Does fusing human and machines improve performance?” Fusion is an effective tool for improving performance when the models encode complementary features. In other words, there is qualitative diversity in the way the models (human and machine) encode and recognize faces. If an algorithm is a good model for human face recognition, then there will be similar approaches to recognizing people. Thus, fusing them will not significantly improve performance. O'Toole et al. [37] looked at fusing human and machine results. Experiments were performed on the difficulty set of images in the FRGC experiments, see Section 2.2.1 and Fig. 2(b). The fusion algorithm was based on least partial squares regression [38,39]. Fig. 12shows the key results from the study and the total error rate that was reported. The first two cases are controls, performance on humans and the FRGC submission from NJIT algorithm. The third case is fusing all seven algorithms from the FRGC. The best results were achieved by fusing humans and all seven algorithms in the study, with an error rate of 0.003.That study demonstrated that fusing algorithms and humans can substantially reduce the error rate. For the data set and algorithms in the study, the fused error rate was almost zero. Because of the large reduction in the error rate, the results support two significant conclusions. First, designing systems to effectively fuse humans and machines can significantly improve overall performance. Second, the mechanisms underlying the recognition process in machines and humans are qualitatively different.The three structural studies reviewed in this section demonstrate the potential for a broad interaction between human and machine studies. This has led to improvement in experiment design, deeper understanding of the principles of face processing, and the ability to effectively combine humans and machines.The cross-modal performance analysis framework was designed to compare human and machine performance across a series of experiments. Although this framework is useful in its own right, we apply this technique to establish goals for advancing face recognition technology.Over the last two decades, phenomenal progress has been made in automated face recognition from frontal images taken in mobile studio or mugshot environments. Results from the MBE 2010 report a false reject rates of 27 in 10,000 at a false accept rate of 1 in a 1000; and an identification rate of 0.93 from a gallery of 1.6million faces [40]. Between 1993 and 2010, the false reject rate at a false accept rate of 1 in a 1000 has decreased by a factor of two every two years [9].Clearly, this level of performance cannot be achieved for faces acquired under all conditions. The question then becomes: What is a reasonable performance bound or goal? We propose a goal based on human performance. Establishing a goal based on one set of images will not adequately characterize a problem. To provide the needed benchmarking data, performance should be characterized by a set of experiments, where each experiment focuses on a different aspect of the challenge. For example, the analysis in Section 3 is conducted on a set of 16 experiments. We formalize this concept as a Face Performance Index. The goal in designing a face performance index is to select a set of experiments that adequately characterize performance under the range of conditions that are relevant to a problem.How does the CMPA framework and a face performance index establish a performance benchmark relative to humans? The first step is to create a face performance index that consists of a set of experiments that adequately characterizes human performance. Initially, this face performance index could be a first order approximation. Later iterations of this index would provide better approximations.For each experiment, human and machine performance is measured, the appropriate performance statistic is computed, and plotted on a CMPA scatterplot. In our analysis the performance statistic was the AUC. The goal of combining CMPA and a face performance index is to spur progress. This is illustrated in Fig. 13, which is adapted from Fig. 11. The green region in the upper left region of the figure is the ‘goal box.’ The goal box is the region of the plot where algorithms perform better than humans and algorithm performance is better than random. When all of the experiments are in the goal box, then within the CMPA framework, the performance of an algorithm on a face performance index is better than humans. The fraction of experiments that are in the goal box is a measure of the success of an algorithm.To illustrate the process, we create a notational challenge problem. Here a face performance index is created by augmenting the 16 experiments from Section 3 with additional notational experiments, which are annotated with blue +s. The position of the +s represents performance at the start of a challenge problem, which assumes that performance of humans is superior. The goal of the challenge is to improve algorithm performance so the experiments represented by the +s are in the goal box, which is illustrated by the yellow arrows.In the analysis in this paper, performance is measured for average-recognizers who have not received training. One common assumption is that trained law enforcement officers and forensic examiners are better face recognizers. Russell et al. [41] reported the existence of four super-recognizers. However, there are few published papers that report the performance of law enforcement officers or forensic examiners and their results are mixed [42–44].The goal could be updated to have machines match the abilities of super-recognizers, trained law enforcement professionals, or forensic face examiners. The goal of the notational challenge is absolute, better than humans. By modifying the goal-box region, the goal can be relative, with machine performance better than humans by a fixed factor. These two modifications illustrate the flexibility of the CMPA and face performance index for formulating challenge problems.The discussions in this paper have focused on the recognition accuracy of machines and humans on a verification task. This ignores many other aspects of performance of a face recognition system. Face recognition systems can search millions of mugshots, adjust to changing watch lists on demand, and process face imagery work 24h a day. Humans are substantially better at recognizing familiar faces than unfamiliar faces. The algorithm development community has focused on recognition of unfamiliar faces. A challenge for the algorithm community is developing techniques that achieve human-level performance for familiar faces. This will most likely include developing an understanding of when this accuracy can be achieved.Accepted conventional wisdom in the face recognition community is that humans are the most robust face recognition system. Humans perform face recognition across numerous imaging conditions; i.e., changes in natural illumination, pose, expression, imaging artifacts, etc. Like algorithms, human performance varies greatly under natural imaging conditions [45]. Also, human recognition of a person improves as a face transition from unfamiliar to familiar, and humans intuitively integrate all identify cues during recognition. The CMPA and face performance index has the potential to assist in developing algorithms that have these human capabilities. In turn, algorithms have the potential to serve as computational models that assist in explaining human face processing.