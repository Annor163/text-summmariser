@&#MAIN-TITLE@&#
Detection of vocal disorders based on phase space parameters and Lyapunov spectrum

@&#HIGHLIGHTS@&#
Based on chaos theory two new sets of features are introduced for distinguishing between normal and pathological voices.Mathematical and statistical analyses of the proposed features are conducted.Genetic algorithm and linear discriminant analysis are employed to improve performance of the system.

@&#KEYPHRASES@&#
Voice pathology detection,Chaos theory,Genetic algorithm,Linear discriminant analysis,Support vector machine,

@&#ABSTRACT@&#
Previous studies have shown that the underlying process of speech generation exhibits nonlinear characteristics. Since linear features cannot represent a nonlinear system thoroughly, this paper employs new sets of non-linear measurement for assessing the quality of recorded voices. Such measurement could be exploited for implementing efficient and convenient systems for diagnosing laryngeal diseases without using invasive methods. Three sets of features based on mutual information, false neighbor fraction, and Lyapunov spectrum are investigated to this end. Furthermore, distributions of the proposed features and their discriminative property are investigated. Moreover, the described procedure benefits from the synergy between different concepts of pattern recognition. First, a genetic algorithm (GA) is invoked to find a-near optimum subset of features. Second, linear discriminant analysis (LDA) is applied to remove remaining redundancies and correlations between selected features. Finally, support vector machine (SVM) is employed for learning decision boundaries. Sensitivity and specificity of 99.3% and 94% respectively were achieved in the simulation results.

@&#INTRODUCTION@&#
Diseases related to human voice production system such as laryngeal cancer and polyps usually cause severe damages to patients’ health and even their social life. Fortunately, most of these diseases can be cured if they are detected at early stages. Since laryngeal syndromes are likely to cause anomalies in a patient's voice (such as breathiness and hoarseness which are two common symptoms of human speech system's malfunction), some well-versed specialists can discern the problem just by listening to the patient's voice and decide to prescribe direct examinations like laryngoscopy. However, these examinations are very expensive and time-consuming. Moreover, they cause discomfort to patients as they are invasive. Thus, some pre-examinations are worthwhile. One major drawback of perceptual examinations is their inherent subjectivity which makes them unreliable and difficult to quantify [1,2]. To overcome these problems, researchers have been seeking a reliable way – usually based on speech signal processing techniques – to distinguish between healthy and pathologic voices. This procedure can be implemented in a mobile device or even simple applications on electronic gadgets, so everybody – especially those who are at high risks due to genetic defects or professions (like professional singers) – can do the primary examinations easily.In order to distinguish between healthy and pathologic voices, firstly some features have to be found whose differences are distinctly vivid. Then, voices are categorized into healthy or pathological using some classifying methods such as SVM or neural network (NN). Over the past years, several measures have been introduced to quantify the abnormality of humans’ speech signals. These measures include fundamental frequency (f0) [3,4], pitch perturbation (jitter) and amplitude perturbation (shimmer) [5,6], harmonic to noise ratio (HNR) [7], low to high energy ratio (LHR) [8], glottal to noise excitation ratio (GNE) [9], normalized noise energy (NNE) [10], signal to noise ratio (SNR) [11], and mel-frequency cepstral coefficients (MFCC) [12–14]. Jitter and shimmer are measures of short-term (cycle-to-cycle) variation in the fundamental frequency and amplitude of a voice signal. All of these measures are based on the linear acoustic theory [15–18] and assume a planar sound propagation through vocal tract.Further researches have shown that the linear model is inadequate. In another word, airflow propagation through human's vocal tract is more likely to follow the fluid dynamic rules which lead to nonlinear models [19–21]. Specifically, phenomena such as sub-harmonic oscillation, sudden change of vocal fold behavior, and octave jumps show that voice signal presents highly non-linear vibratory systems and therefore the chaos theory may be employed [22–25]. For example, transitions between different voice attractors can be characterized in terms of bifurcations [23,26].In recent years, the chaos theory has been used as a powerful tool for analyzing nonlinear systems [27,28]. Correlation dimension (CD) [29] and the largest Lyapunov exponent (LLE) [28] are among the most popular nonlinear features. Zhang and Jiang deployed CD to discriminate between different types of speech signals [30]. Efficacy of nonlinear features for discriminating between pathological and healthy voices in the case of running voices was shown in [31]. Giovanni et al. used LLE to diagnose the unilateral laryngeal paralysis [32]. Zhang et al. applied CD and entropy measures to analyze the sustained vowels of patients before and after surgical excision of vocal polyps [33]. Vaziri et al. exploited CD and LLE for classifying voices [34]. Henríquez reported a very high accuracy based on non-linear features and neural networks [35]. Arias et al. argued about performance of this method and noticed that normal and pathological voices have different lengths in MEEI database. They argued that high accuracy of this method could be biased by the different lengths of the files [14].Employing nonlinear analysis of voices, this paper makes the following contributions:-While there is a whole spectrum of Lyapunov exponents, to the best of our knowledge, previous works have only used LLE. This work uses the whole spectrum. Furthermore, two new sets of features based on false neighbor fraction (FNF) and mutual information (MI) are proposed.Using a mathematical approach, it is shown that the proposed features have different distributions and therefore, they have good discriminative properties. This is achieved by employing the simplified model of additive uncorrelated noise for pathological voices.The final decision is achieved by synergy of GA for feature selection, LDA for feature reduction, and SVM for learning decision regions.The rest of this paper is organized as follows. Section 2 presents a brief introduction to the chaos theory and the employed mathematical model for pathological voices. Nonlinear features are defined in Section 3. Section 4 is devoted to the proposed method and analyzing the proposed features. Experimental results are presented in Section 5. Section 6 discusses the proposed method and conclusions are finally reported in Section 7.In order to apply chaotic analysis, it is necessary to construct the phase space of a system. Fortunately, Takens embedding theorem shows that if the embedding dimension is sufficiently high, the phase space can be constructed from time series samples [36]. The most important technique for constructing the phase space is the method of delays [37].Let x(t) denote time series produced by an unknown dynamic system. Then, m-dimensional vector ofs(t) can be constructed by successive time delays of x(t):(1)s(t)=[x(t),x(t+τ),…,x(t+(m−1)τ)]where τ and m are called time-delay and embedding dimension, respectively. In practice, these parameters are not known a-prior and should be estimated [38]. Fig. 1shows reconstructed phase space of a normal and some pathological samples.In order to compare distributions of features extracted from normal and pathological voices, a mathematical model is employed. Furthermore, to make this mathematical problem tractable, an admittedly simplified model of additive noise is employed. It is noteworthy that this model is only employed for providing mathematical insight into discriminating properties of the proposed features. In another words, there is no claim that all pathological impairments follow this assumption or that the only difference between normal and pathological voices is due to their noises. Also, the uncorrelated noise does not make the system chaotic, but as it is discussed in Section 4.2.1 it can alter chaotic structure of a signal.A pathological voice, xP(t), is modeled as the sum of two independent sources:(2)xP(t)=xN(t)+n(t)where xN(t) represents a normal subject voice without any pathological anomaly and n(t) is an i.i.d random signal with zero mean and variance of σ2.Anomalies in pathological voices stem from malfunctions of some parts of voice production system. Consequently, it is logical to assume that pathological and normal voices stem from systems with different dynamics. In this section, we will show that nonlinear features can reflect these differences.In dimension m, each vectors(t)mhas a nearest neighbors(t˜)mwith respect to a distance measure. Let us denote their distance bydm2:(3)dm2=||s(t)m−s(t˜)m||2Also, lets(t)m+1 ands(t˜)m+1be maps ofs(t)mands(t˜)min dimension m+1. Then, distance between these points in the new dimension is:(4)dm+12=dm2+(x(t+mτ)−x(t˜+mτ))2In other words, by going from dimension m to m+1 two neighbor points diverge with the factor of(5)D=||x(t+mτ)−x(t˜+mτ)||2If the value of D is larger than a specific threshold [39,40], we call these points false neighbors. It can be argued that these points are neighbors in dimension m, because of the folding that occurs by projecting a high dimensional attractor down to dimension m (Fig. 2).After all false neighbors are found, FNF is calculated according to:(6)FNF=No.offalseneighborsNo.ofallneighbors×100%Considering random variables ofs(t) ands(t+τ) with joint probability distribution of P(s(t),s(t+τ)) and marginal distribution of P(s(t)) and P(s(t+τ)), mutual information MIτis defined as:(7)MIτ=∑∑P(s(t),s(t+τ))logP(s(t),s(t+τ))P(s(t)),P(s(t+τ))Mutual information measures the mutual dependence of the pointss(t) ands(t+τ). In other words, it measures how much knowledge ofs(t) reduces uncertainty abouts(t+τ).Lyapunov exponents show the sensitivity of a system to initial conditions and are one of the most effective descriptors of chaos in nonlinear systems. A good survey on the theory of Lyapunov Spectrum can be found in [41]. Also, practical estimation of this spectrum using time series observation is covered in [42]. Eq. (8) presents the mathematical formula for calculation of LLE (λ1), which is the largest component among Lyapunov exponents:(8)λ1=limN→∞1N∑n=1Nd(s(t+1),s(t˜+1))d(s(t),s(t˜))whered(s(t),s(t˜))is the distance between a reference points(t), and its neighbor on a nearby trajectorys(t˜). Also,d(s(t+1),s(t˜+1))is the distance between the same points one time ahead, along the same trajectories. Fig. 3depicts these points.Based on the nonlinear features presented in the previous section, a method is proposed to discriminate between normal and pathological voices. Fig. 4illustrates block diagram of the suggested scheme.Prior to the calculation of features, phase space should be constructed. If the dimension of this phase space is m, then a total number of m different Lyapunov features could be extracted. Therefore, prior to feature extraction, a suitable dimension of phase space should be determined. An experiment was conducted to determine the number of Lyapunov exponents with discriminative abilities. t-Test statistics [43] of λmwas measured to that end. The result is presented in Fig. 5.According to Fig. 5, for m≤7, corresponding p-value falls well beyond 1% significance level. Different impairments may impose different levels of non-linearity; therefore, it is logical to use different dimensions for unfolding each type of signals. Unfortunately, this approach relies on the prior knowledge about type of signal, and thus would be infeasible in classification tasks. Therefore, the single embedding dimension of m=7 was adopted for all samples in the rest of simulations.Features of the proposed method can be categorized into two groups: features that are calculated on segments of audio samples and those that are calculated on the whole audio signal. In the first group, after these intermediate features are extracted, usually an average is performed to find the final features. Therefore, lengths of different samples do not affect the values of these features severely. Features based on FNF and Lyapunov spectrum fall into this category. On the other hand, features like mutual information simultaneously process the whole length of audio samples; therefore, it is quite possible that different lengths of audio samples might be reflected in the calculated features. This phenomenon could bias the result of classifier if samples from different classes have same within-class and different between-class durations. Examining the MEEI database shows that normal samples are three times as long as pathological samples. To show how this phenomenon may affect extracted features, an experiment was conducted. In the first scenario, mutual information of original normal and pathological voices was extracted and then, their discriminating abilities were measured using t-Test statistics. In the second scenario, normal voices were trimmed to have the same duration as pathological voices have and then, features were extracted. Fig. 6shows the result of this simulation.According to Fig. 6, it can be deduced that there is a strong relation between discriminating capabilities of these features and lengths of signals. To remove such phenomenon, normal and pathological voices were trimmed to have the same length of 1sec. This task was carried out as discarding the first 500ms of the normal samples (to remove their transient portions) and retaining their next first second.To extract features, the data was being normalized between ±1. Then MI-based features were calculated over the whole length of the samples for delay values of 0≤τ≤20. Furthermore, FNF and Lyapunov spectrum were calculated on segments with 200ms duration using TISEAN package [44]. The complete feature extraction procedure is shown in Fig. 7.Previous works have analyzed some of the non-linear features [45]. The same strategy with some corrections in accordance with the model of Eq. (2) is used to analyze discriminative property of the employed features.The termD=(x(t+mτ)−x(t˜+mτ))2discriminates between true and false neighbors. In a normal case, this term is:(9)DN=(xN(t+mτ)−xN(t˜+mτ))2Employing mathematical model of equation 2, in a voice with additive noise we would have:(10)DP=(xP(t+mτ)−xP(t′+mτ))2=(xN(t+mτ)+n(t+mτ)−xN(t′+mτ)−n(t′+mτ))2Apparently due to the presence of noise, in phase space of voice with additive noise another point is neighbor ofs(t)Expected value ofDPcan be calculated as:(11)E[DP]=E[(xN(t+mτ)−xN(t′+mτ))2]+E[(n(t+mτ)−n(t′+mτ))2]+2E[(xN(t+mτ)−xN(t′+mτ)(n(t+mτ)−n(t′+mτ)]After some manipulation, Eq. (11) is reduced to:(12)E[DP]=E[(xN(t+mτ)−xN(t′+mτ))2]+2σ2Because both pairs of(t,t˜)and (t, t′) are selected so that they are neighbors, it is very likely that:(13)E[(xN(t+mτ)−xN(t′+mτ))2]≈E[(xN(t+mτ)−xN(t˜+mτ))2]So Eq. (13) reduces to:(14)E[DP]≈E[DN]+2σ2Eq. (14) shows that voices with additive noise tend to have higher values of FNFs.Investigating Eq. (8) shows that LLE of data is a function of:(15)λ1∝d(s(t+1),s(t˜+1))2d(s(t),s(t))2=∑k=0m−1(x(t+1+kτ)−x(t˜+1+kτ))2∑k=0m−1(x(t+kτ)−x(t˜+kτ))2Expected value of (15) for normal and voice with additive noise are:(16)λN1∝∑k=0m−1(x(t+1+kτ)−x(t˜+1+kτ))2∑k=0m−1(x(t+kτ)−x(t˜+kτ))2(17)λP1∝∑k=0m−1(x(t+1+kτ)+n(t+1+kτ)−x(t′+1+kτ)−n(t′+1+kτ))2∑k=0m−1(x(t+kτ)+n(t+kτ)−x(t′+kτ)−n(t′+kτ))2Investigating Eqs. (16) and (17) reveals that in normal and pathological voices points of(t,t˜)and (t, t′) are paired, respectively. It can be argued that to some extent, the algorithm allows denominator to mitigate the effect of the noise. We show that this is not true for the numerator:(18)∑k=0m−1(x(t+1+kτ)+n(t+1+kτ)−x(t′+1+kτ)−n(t′+1+kτ))2=E[(x(t+1+kτ)−x(t′+1+kτ))2]+2σ2Points of t, t′ are selected in a way that their distance is minimized. But, it is quite possible that this minimum distance might be the result of the noise while in fact these points do not belong to the neighboring trajectories. In this case, the distance between t+1 and t′+1 would be large. Therefore, we would have:(19)E[d(sP(t+1),sP(t˜+1))2]>E[d(sN(t+1),sN(t˜+1))2]+2σ2According to Eq. (19), voices with additive noise are expected to have a higher value of LLE.While analysis of previous section were pure mathematical and did not rely on the database, this section presents discriminative properties of the features extracted from MEEI database. To this end, statistical significance of each feature was measured with t-Test. p-Values of these tests are presented in Table 1.Table 1 shows interesting results. First, features based on FNF and Lyapunov spectrum typically have lower p-values than those based on mutual information and thus are more discriminative. Second, MI feature becomes discriminative when its delay is either small (τ=0, 1) or large (τ≥13).Besides statistical tests, plotting probability density function (PDF) of features for both of classes can provide considerable insight into discriminative property of features. To this end, after trimming samples of the MEEI database, the proposed features are extracted. Also, to keep the plots to a minimum, two plots are presented from each type of feature. Fig. 8shows PDF of FNF for pathological and normal voices when embedding dimension (m) are 2 and 7.Fig. 9shows the distribution of λ1, λ7 for pathological and normal voices.Fig. 10shows the distribution of MIτfor pathological and normal voices for two values of τ=0 and 20.In extracted features, there are some points which are significantly different from the other observations. These data points may be considered to be outliers. Removing the outliers during training allows the algorithm to learn classification boundaries more accurately [46]. The distance-based method implemented in [47] was used to eliminate the outliers. Further investigations of features showed that their dynamic ranges are different. Thus, it is likely that features with higher values influence the cost function of the classifier more, even though they may have low discriminating role. To overcome this problem, the features were normalized. To this end, mean and variance of the features over the train set were calculated and then features were normalized according to Eq. (20):(20)xˆik=xik−mkσkValues of mkand σkwere retained for normalization of the test set.In any classification task, there are some features which are redundant or irrelevant. Redundant features are those which provide no further information than currently selected features, and also irrelevant features provide no useful information for classification. Furthermore, the higher number of features not only increases the computational complexity of the classifier, but also may diminish generalization properties of the classifier [43].Genetic algorithm was invoked to choose the near-optimum subset of features. Our implementation used accuracy of the classifier as the fitness function, a population size of 200 individuals with tournament selection [48], and two-point crossover [49]. To further improve performance of GA, selection operation was followed by elitism [50] which was implemented as directly selecting 1% of the mating population from the best chromosomes. Finally, a mutation with rate of 1% was implemented as replacing one of already selected features with one of the remaining ones. The algorithm stopped if after ten consecutive generations there was no improvement in the fitness function. Fig. 11demonstrates evolution of the accuracy for different generations of GA.It has been demonstrated that feature reduction can be beneficial in voice pathology detection/classification tasks [50–53]. While in feature selection, a subspace of the original space of features is selected, feature reduction maps the feature space into a new one where the new features have different characteristics. One of the main characteristics of this new space could be that the features are uncorrelated. Furthermore, it is possible to map most of the classification-related features into a relatively small number of new features. In other words, an appropriately chosen transform can remove redundancies between selected features. This simplifies the design of classifier and it may improve the classification results. LDA [54] was exploited for feature reduction.

@&#CONCLUSIONS@&#
