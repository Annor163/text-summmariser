@&#MAIN-TITLE@&#
A robust approach for multi-agent natural resource allocation based on stochastic optimization algorithms

@&#HIGHLIGHTS@&#
A decentralized machine learning method that deals with learning in non-communicating non-observable stochastic games.Embeds domain knowledge using a parameterized function that is automatically estimated using stochastic optimization.The proposed method was applied on a realistic large scale multiagent simulation model of natural resource allocation.Experiments revealed the robustness and superiority of the proposed method, compared to decentralized Q-learning variants.

@&#KEYPHRASES@&#
Multi-agent learning,Natural resource allocation,Stochastic optimization,Evolutionary methods,Genetic algorithms,Covariance Matrix Adaptation,

@&#ABSTRACT@&#
Natural resource allocation is a complex problem that entails difficulties related to the nature of real world problems and to the constraints related to the socio-economical aspects of the problem. In more detail, as the resource becomes scarce relations of trust or communication channels that may exist between the users of a resource become unreliable and should be ignored. In this sense, it is argued that in multi-agent natural resource allocation settings agents are not considered to observe or communicate with each other. The aim of this paper is to study multi-agent learning within this constrained framework. Two novel learning methods are introduced that operate in conjunction with any decentralized multi-agent learning algorithm to provide efficient resource allocations. The proposed methods were applied on a multi-agent simulation model that replicates a natural resource allocation procedure, and extensive experiments were conducted using popular decentralized multi-agent learning algorithms. Experimental results employed statistical figures of merit for assessing the performance of the algorithms with respect to the preservation of the resource and to the utilities of the users. It was revealed that the proposed learning methods improved the performance of all policies under study and provided allocation schemes that both preserved the resource and ensured the survival of the agents, simultaneously. It is thus demonstrated that the proposed learning methods are a substantial improvement, when compared to the direct application of typical learning algorithms to natural resource sharing, and are a viable means of achieving efficient resource allocations.

@&#INTRODUCTION@&#
Multi-agent learning is an area of study that deals with the theoretical challenge of understanding how agents learn and adapt their behaviour in the presence of other agents that are also learning. It has drawn much attention over the past decades, as the expansion of multi-agent modelling approaches across interdisciplinary domains (i.e. economics, robotics and others) created a need for robust multi-agent learning algorithms. Learning in multi-agent settings however is a hard task, since the actions of one agent affects the environment of the other and vice versa [29]. In such dynamic environments, the convergence properties of single agent learning algorithms do not necessarily hold [29,54], thus learning becomes difficult even for simple two-agent settings [10,44,29].Many approaches have been proposed to deal with multi-agent learning, with Fictitious Play, Bayesian Learning, No-Regret Learning, Targeted Learning, Evolutionary Learning being the most representative ones [42,27]. Although these approaches have been successfully applied to game theory and converge to near optimal policies for specific types of games, most of them assume a priori knowledge of transition probabilities from one stage of the game to the other [42,43,7]. A popular approach that does not make such assumptions is Reinforcement Learning, where agents gradually acquire knowledge through scalar rewards that result from the actions they perform [45].The most typical algorithm of reinforcement learning is Q-learning [49], that iteratively updates a utility-estimation function (i.e. Q-function) for state-action pairs and associates each action with a value (i.e. Q-value). Given that all actions have been sufficiently explored, Q learning is known to converge to optimal solutions for single-agent settings [27,7]. Early approaches of multi-agent reinforcement learning entail the generalization of Q-learning on multi-agent environments, called Decentralized-Q learning, where each agent ignores all the other agents. This approach however fails in many cases, as it ignores the nature of the multi-agent settings [43,42]. To that extent, variants of Q-learning have been proposed to address multi-agent learning.One of the first extensions of Q-learning towards multi-agent applications was presented in [31], where the Q-function was extended to encapsulate the value of joint actions. The same concept was adopted in [24], where each agent observes the other agents and keeps track of their Q-values [7]. Observation or communication is used in most multi-agent reinforcement learning (MARL) algorithms in order to assist learning of joint actions [4,46,31,24,32,14,35,25,2]. In more detail, in [4] agents select their actions taking under consideration the opinion of other agents, whereas in [32,14]Q-values are updated using a value function, estimated through observations. In [25] an agent's ‘attraction’ to a specific strategy is updated according to a mixture of reinforcement learning and belief learning techniques, using inter-agent signalling. Similarly, in [35] the pheromone concept of swarm intelligence is used to allow agents to communicate with each other and to jointly learn how to solve a problem. In the same context, agents communicate in order to adapt their actions using Bayesian learning in [1], while [53] uses a bargain scheme where an agent ‘announces’ his action to other agents.Several MARL algorithms exist that bias the Q-values by modifying them (or purposefully failing to update them) according to some predefined criteria. In [30], agents ignore penalties related to non-coordination when updating their Q-function, thus evaluating actions using the maximum reward received. A similar approach is used in [34], where two distinct learning rates for updating the Q-function are used, depending on the reward the agent receives, whereas in [38]Q-values are updated only if an action has been accessed for several times. In [26], the probability of choosing an action is biased by modifying the Q-function according to a heuristic value, that is estimated based on how frequently the maximum reward for that action was received.Although multi-agent learning algorithms provide near optimal solutions for several types of games (i.e. zero sum or common pay-off games [43]), they fail when they are scaled in real world problems [33]. This is because the complexity of learning is dramatically increased due to a variety of reasons related not only to the non-stationarity of the environment, but to the nature of the problem under study as well. In more detail, even though reinforcement learning algorithms are well defined and extensively studied for discrete action and state spaces, real world applications entail variables of continuous nature [45]. Although this problem can be alleviated using function approximation (e.g. [50]), there is no proof of convergence to optimal solutions for continuous state/action space problems [16]. Additionally, due to the stochasticity and the non-deterministic nature of real world phenomena, measurement uncertainties and noise are almost certain to occur if the problem entails a large number of agents or ambiguous signals [29,33]. Moreover, many real world problems also face the problem of aliasing, i.e. different environmental states that are similar and cannot be easily distinguished. Learning is even made more difficult due to physical or social constraints that may restrict the information available to the agents. For example, unlike game theory where agents are expected to have access to other agent's utilities or to an overall utility welfare, in real world scenarios this is hardly the case [33,29]. In natural resource sharing for example, as the resource becomes scarce relations of trust or communication channels that may exist between the users of the resource become unreliable and should be ignored [3]. In addition, the importance of an equilibrium solution (e.g. Nash) requires a different justification for real world applications, where it should be investigated if indeed a Nash equilibrium reflects a desirable solution to the problem under study [33]. For example, it may be impossible to describe a desirable solution by such a unique equilibrium and multiple equilibria may exist.A typical approach that has been successfully applied to alleviate these issues and scale multi-agent learning beyond illustrative academic problems is the enhancement of learning algorithms with domain knowledge about the specific problem under study [33]. Recently, we have proposed a multi-agent simulation model that deals with natural resource allocation, together with a simple multi-agent learning approach that encapsulates domain knowledge in the form of empirical rules [3]. In this paper, we provide a more elaborate formulation of the learning approach and extend it using stochastic derivative-free optimization methods. The proposed approach, called Adaptive Rectification – AR employs a parametrized rectification function that is trained using a problem specific metric and reveals the way agents should modify their actions in order to achieve efficient resource allocation. In contrast to [3], domain knowledge is not hard coded in terms of heuristic rules but are rather revealed using a supervised learning scheme. Two AR methods are introduced in this study, AR-RCGA and AR-CMA that employ different methods for the estimation of the parameters of the rectification function, namely the Covariance Matrix Adaptation (CMA) evolutionary strategy and Real Coded Genetic Algorithms (RCGA). The main contributions of the proposed methods are that: (a) they achieve the coordination of the actions of the agents towards the solution of the problem using exclusively local information, without employing inter-agent communication or observation. This is an advancement compared to most typical multi-agent learning algorithms and multi-agent resource allocation solutions, that rely mostly on inter-agent communication or observation (i.e. combinatorial auctions); (b) they can be used to scale any general purpose multi-agent learning algorithm that does not violate the imposed constraints (i.e. lack of inter-agent communication and observation), to the real world problem of natural resource allocation. This is achieved through the parametrized function that embeds domain knowledge about the problem under study.The proposed methods were applied to the multi-agent simulation (MAS) model presented in [3], that accurately simulates a real-world natural resource allocation phenomena (i.e. a water resource exploited by a community of users). Extensive experiments were conducted following a Monte Carlo procedure, that evaluated the robustness of the proposed methods in providing efficient resource allocation schemes for agent populations with varying water needs. For the decision making process of the agents, agent policies employing popular decentralized multi-agent learning algorithms were implemented, whereas experiments entailed simulations with and without the use of the proposed AR methods, in order to demonstrate the complexity of learning and the superiority of the proposed methods. The impact of the policies under study on the sustainability of the resource and on the survival of the users was evaluated using qualitative and quantitative figures of merit. In addition, the occurrence probabilities for characteristic events such as the depletion of the resource and the survival of the users were estimated, in order to evaluate the overall performance of the policies and investigate the robustness of the proposed methods. As it will be demonstrated, policies entailing decentralized learning algorithms fail to ensure the simultaneous survival of the agents and preservation of the resource. However, the use of the proposed methods proves to be adequate for producing efficient resource allocations schemes under any given scenario (i.e. for any water need of the population), despite the imposed constraints (i.e. lack of communication/observation). Thus the proposed methods provide a substantial improvement, when compared to the direct application of typical learning algorithms to natural resource sharing, and are a viable means of simultaneously preserving the resource and ensuring the survival of the agents.Multi-agent resource allocation (MARA) is a problem met across various interdisciplinary domains, and refers to the process of distributing a resource to a number of agents [11]. In this study multi-agent allocation of natural resources is investigated, which is a special case of the aforementioned problem. An abstract linguistic description of this problem can be given as follows:“Consider a natural resource that is being exploited by a community of users. What is the optimal resource allocation so that the resource is preserved and, at the same time, users maximize their utilities/profits?”.In this manuscript, we consider resource allocation of a water resource, where agents represent users of the resource draining water for irrigation purposes only (i.e. other types of outflows, such as industrial or urban resource outflows are not taken into account). The significance of this problem can be easily realized. Many countries that already face water shortages, a situation that is expected to get worse since water ecosystems gradually becoming scarce and the earth's population is constantly increasing. Thus, there is a growing need for learning algorithms that can provide efficient resource allocation schemes that would preserve resources and distribute the water more evenly for the benefit of the users. In addition to its importance, this specific problem is quite challenging and complex. In more detail, besides the complexity of ecosystem modelling, which can be characterized as a NP-complete problem [36], and the problems arising from the real world nature of the problem (i.e. non-stationarity, stochasticity, continuous variables, noise, uncertainties in observations, large number of agents), additional constraints emerge from the socio-economical aspects of the problem.More precisely, as water becomes scarce and cannot support all the users in the community, the self-preserving nature of individuals leads them to choices that improve immediate rewards, i.e. to self-lucrative behaviours [41]. This condition is further deteriorated, as economic pressure is exerted on the users of the resource, leading them to even more self-lucrative behaviours. Under such extreme conditions, relations of trust or communication channels that may exist between the users of the resource become unreliable and should be ignored in the decision making process [3]. In other words, due to the economic pressure exerted on the users and the depletion of the resource, the user's primary concern is their own survival and thus they select actions consulting their own self-interest and not any information they might obtain through communication. This assumption is in accordance with real world farmer behaviours and is verified by studying real-world ecosystems with economically stressed farmers [48]. In the same context, it is unrealistic to assume that agents may observe other agents’ utilities or actions to define their own policy [33]. From an multi-agent learning point of view, these aforementioned constraints can be modelled by assuming that agents do not communicate or observe each other and, rather, base their decisions using only local information. Agents of this type are considered independent reinforcement learners, where the main challenge is how to make all of them coherently choose their individual actions so that the resulting joint action provides an efficient solution (problem of coordination) [29].These constraints however radically increase the complexity of learning, as most multi-agent learning algorithms use inter-agent communication or observation to coordinate the agents towards efficient solutions (e.g. [10]). Moreover, they differentiate natural resource allocation from most MARA problems found in the literature, where solutions emerge through direct or indirect inter-agent communication [11]. In more detail, the dominant solution for MARA are combinatorial auctions, a centralized approach where the resource is distributed using inter-agent negotiation and communication through a control agent (e.g. [11,12,40,9]). Auctioning based solutions can be also found in the literature of multi-agent environmental modelling, for problems where inter-agent communication is feasible [1,53]. The vast majority of environmental multi-agent models however are used as simulation tools for evaluating environmental policies, without focusing on agent learning or means that would provide efficient solutions [6]. Furthermore, when communication amongst the users of a resource is lacking, the self-lucrative behaviour exhibited by the users leads to confrontations and, evidently, the depletion of the resource [37]. This is a condition referred to in the literature as ‘the tragedy of the commons dilemma’, that has been verified by laboratory experiments [37] and studies on real-world ecosystems [48]. It is thus evident that the development of efficient decentralized resource allocation algorithms that operate without assuming inter-agent communication, as the one proposed in this study, would also assist in the preservation of resources that were otherwise wasted.Finally, another parameter that should be seriously taken under consideration when designing a solution for natural resource allocation are the utilities of the agents. Since agents use the water they drain from the resource for irrigation purposes (e.g. to irrigate their fields) it becomes evident that their profit, and thus their survival, depends on the portion of the resource they have allocated (i.e. how much water they have drained). However, the profit/survival of the users and the quantity of water they have drained from the resource are not linearly related. Depending on the type of the cultivation, there is a minimum quantity of water that each crop should be irrigated with during a cultivation period or else it is destroyed. Users that gather less water than this minimum quantity are not considered to survive, either because their cultivation is destroyed or because their profit was not enough to sustain them financially. In terms of reinforcement learning theory, this means that the goal of the learning algorithm is not to maximize the utility of the agents but to ensure that their utility will exceed at least a certain threshold, under which they are economically destroyed. In this sense, we consider an efficient solution for the problem under study an allocation that (a) preserves at least a fragment of the resource and (b) ensures that most users attain their goals and sustain (economic) survival.In order to investigate multi-agent learning on natural resource allocation, a framework has to be defined for the multi-agent learning methods to be applied. This is an important design consideration, as the defined multi-agent framework has to be able to capture the dynamics of the problem under study [33]. A typical approach for modelling n-person dynamic games are stochastic games [42,33]. A stochastic game can be considered either as an extension of Markov Decision Processes (MDPs) to multiple agents or as the extension of matrix games, since each stage of a stochastic game can be considered as a matrix game [7]. It is described by a tuple 〈n, S, A1…n, T, R1…n〉 where:•n is the number of agents.S is a finite set of states and stthe state of the environment at time t.Aiis the set of actions available to agent i. Letait∈Ai, be the selected action of the ith agent on time t and A=∏Aithe joint action space. The joint action of n agents at time t is denoted byat=(a1t,a2t,…,ant)T.T:S×A×S→[0, 1] is a transition function defining transition probabilities between statesRi:S×A→ℝdenotes the reward function for the ith agent.Since in the multi-agent setting multiple agents select actions simultaneously, it is evident that the next state and rewards depend on the joint action atof all the agents, and thus the environment becomes non-stationary [7,42]. The objective of an agent i is to learn a policy πi(·) that maps states to a probability distribution over its actions set Aitowards a predefined goal. Let pj∈[0, 1] denote the probability of selecting the jth action of Aiand m denote the total number of actions in Ai, then the policy πi(·) of the ith agent is a vector containing the probabilities of all his actions, i.e. πi(·)=(p1, p2, …, pm). Let πi(S) denote the policy of the ith agent for all states, πi(st) denote the policy of agent i for a single state stand π=(π1, π2, …, πn)Tthe joint policy/strategy played by all the agents. Then the goal of the learning process is to converge to an optimal policy π*(·) that maximizes an agent's future reward, as discounted according to machine learning principles. A policy may be called pure, when the probability pjof selecting the jth action equals one (i.e. only the jth action is selected out of the action space Ai), or mixed otherwise.The proposed method is applied on the MAS model presented in [3], that simulates a real world problem of water resource allocation, where a water resource is exploited by a community of users. This specific MAS model was chosen, since it is calibrated so as to replicate a real world ecosystem that is a representative example of the tragedy of the commons dilemma. In more detail, it simulates the lake Koronia ecosystem water sharing scheme, where the lack of communication, the failure of centralized resource allocation policies and the self lucrative behaviour of the users’ led to the depletion of the lake in 2002 [48]. In addition, even though the model follows the basic principles of multi-agent theory, it also complies to all the constraints presented in Section 2. As a result, no inter-agent communication or observation is employed in any part of its design, and agents are considered to have access only to local knowledge. For the sake of completeness, a brief description of the MAS model follows.The MAS model consists of the water resource, and the agents which represent users of the resource (i.e. farmers) that drain water from the resource for irrigation purposes.Water resource: The water resource initially consists of L units of water and is modelled by a water balance equation that encapsulates the difference between water inflows (i.e. surface water, rainfalls, groundwater inflows) and water outflows (i.e. evapotranspiration, evaporation, urban and industrial needs) over a period of time. Irrigation outflows are not taken into account in the water balance equation, since they are simulated by the users actions and are far too complex to be mapped on a water balance equation. Additionally, to encapsulate the non-linear nature of the resource, once the resource water level l reaches a critical threshold T, the resource is considered partially empty and all agents receive the same amount of water WT, regardless of their demands. This is in analogy to a natural reservoir that gradually degrades to a series of disconnected ponds, resulting to loss of water pressure, exposed or un-submerged pipes, etc.Agents: Agents are designed so as to perform a single task, i.e. request predefined quantities of water from the resource. In the real world, users of a resource have no physical constraints in draining arbitrary quantities of water from the lake. Reinforcement learning approaches that utilize continuous spaces however are quite complex and do not guarantee convergence to optimal solutions, with the exception of specific types of problems [16]. The utilized MAS model employs a sparse discretization of the continuous action space [0, L], resulting in a discrete action space Aithat consists of 5 predefined quantities of water.Although a more dense sampling could be employed to better approximate the range [0, L] (i.e. 1000 actions), this specific partitioning (i.e. 5 actions) was selected since it entails low memory requirements and, additionally, is verified by studies of the lake Koronia ecosystem and by cross-comparing the output of the model with real-world data [3]. The resulting action space Aiof the ith agent is defined as(1)Ai=ai,1=oi*/6ai,2=oi*/2ai,3=oi*=g·WnNai,4=2·oi*ai,5=6·oi*where Wnis a parameter defining the total amount of water that all agents drain, N is the total number of agents, and o* describes the nominal amount of water an agent needs to cover his daily irrigation needs, according to his beliefs. These needs should ideally be derived from the type of cultivation and the field area. Unfortunately, in many water sharing scenarios users over-drain water either by over-irrigating without knowing which is the appropriate amount of water for their type of cultivation or in order to compensate for water losses during water transfer from the lake to their field (ignoring the fact that all the excessive water is wasted) [48]. To that extent the ‘greediness’ variable g is introduced in Eq. (1), to account for all these issues and in order to differentiate between the amounts of water each agent drains. Study of research reports of the lake Koronia ecosystem (e.g. [48]), revealed that five distinct categories with values g=1, 2, 4, 6, 10 should suffice to depict the different behaviours in the community of the users, with respect to their degree of greediness. This means an agent of greediness category 6 is considered to drain 6 times more water than an agent of greediness 1, and so on. It should be also noted that this specific greediness categorization is verified when comparing the output of the model with real-world data [3].According to the principles of reinforcement learning theory the simulation advances in iterations, repeating the following procedure (i denotes the agent index and t the iteration index):•Step 1: Each agent selects an action ai,tout of the action set Ai, according to his policy πi(·).Step 2: Each agent performs the selected action ai,t.Step 3: The resource passes to a new state and each agent gets a reinforcement signal (i.e. numerical reward).Step 4: Each agent perceives the new state of the resource through the rate of change of the resource.A simulation is considered to end after 160 iterations, in accordance with the 160 days duration of a typical cropping season of the year. It should be noted that this is the time window of interest that is examined by the MAS model. In this sense, if the resource is not completely drained during this period, it is not considered to be depleted.The action selection in Step 1 is achieved using a policy π(·), that aims at maximizing the future reinforcements an agent receives. The definition of this reinforcement signal (i.e. the reward) is an important design consideration, as it should be representative of the efficiency of a solution [33]. In accordance to the definition of an efficient solution for natural resource allocation (Section 2), this reward should be a signal that is maximized when agents gather enough water to meet their goals and, at the same time, the resource is preserved.To address this issue, a new figure of merit was defined based on empirical observations of the ecosystem under study [48], that is representative to the efficiency of a possible resource allocation. It is a utility metric denoted by VC, that evaluates the overall utility of the agents actions to the environment and to the agent community. In more detail, VCis given by(2)VC=(el/L)·∑i=0Nviwhere l is the volume of the resource at the end of the simulation, L is the initial volume of the resource, N is the overall number of agents andviis the individual utility of each agent. That utility is estimated by the formula(3)vi=F2∑m=0160ri,m·F1(ga)where ga is the relative deviation of an agent from his goal, F1(x)=1/e2xa decaying function that aims at punishing farmers that did not meet their goals, thus they did not survive, and F2(x)=2.8×10−2x+0.8 is an increasing function that aims at rewarding agents that managed to gather more water than others, thus possibly attaining higher profits and contributing to the higher overall VC. It should be noted that both F1(x) and F2(x) were derived from empirical data of the lake Koronia ecosystem [48]. High values of VCcorrespond to simulations where most of the users have gathered enough water to meet their goals (i.e. sustain their production) and at the same time, the resource was not depleted. On the other hand small values of VCcorrespond to simulations where users did not manage to gather enough water to sustain their cultivation, and thus they are financially ruined, and at the same time the resource was dried out. It is thus evident that as VCis maximized, the resource allocation is more and more efficient. In this case, the reward signal received by each agent is given by the formula(4)(Rit)global=VCFurthermore, the state of the environment in Step 4 is measured through the rate of change of the resource, denoted bycit, which is given by the formula:(5)cit=d(fit)dtwherefitis a feedback signal defined as(6)fit=aitpwhereaitis the action performed by agent i on day t and p=l/L is a decaying parameter, entailing the lake initial level L and current level l. Parameter p actually modifies the feedback signalfitaccording to the current lake level, thus implicitly providing knowledge about the current state of the resource. The parameter p is an indirect measure of the water level and could be related to other available physical quantities such as any visual cues an agent may have to get a hint about the water level (which is commonly the case) or even, the electrical current drawn by a water draining pump, that should typically increase as the water resource gets empty, etc. Thus, throughcit, each agent acquires an estimate about the rate that the resource is being drained (i.e. how quickly its water volume diminishes).Finally, it should be noted that Eq. (6) could be also used as a reward signal, as is the case in [3]. In more detail, the reward each agent receives could be alternatively given by(7)(Rit)local=fit=aitpThe maximization of Eq. (7) may not be representative of an efficient solution, but it is a signal based only on local knowledge which is essential for a decentralized solution as the one employed in this study. In the context of this paper, Eq. (7) will be used as the reinforcement received by agents, whereas Eq. (4) will be used for the estimation of the parameters of the AR layer (i.e. its training).Domain knowledge is a vital element in the design of multi-agent learning algorithms, that can be used to scale them up beyond illustrative academic problems and bias them towards effective learning solutions [10,33]. In this study, we propose a solution for natural resource allocation called Adaptive Rectification (i.e. AR), that operates as an additional layer of complexity on the decision making process of the agents (Fig. 1) and achieves efficient resource allocation without the assumption of inter-agent communication or observation.The underlying principle of AR is quite simple: instead of directly using the action prescribed by some policy as the response for a given state, the AR layer modifies this action with the use of a parametrized rectification functionF(·). The main advantage of AR is thatF(·)is trained using stochastic derivative-free optimization methods that encapsulate domain knowledge and reveal the optimal way actions should be modified in order to achieve efficient solutions. In this sense, AR can be used in conjunction with any policy or multi-agent learning algorithm, provided that they do not entail inter-agent communication or observation (see Section 2). Moreover, since the parametrized functionF(·)is trained off-line it does not increase the complexity or the memory requirements of the simulations, which is important when using tabular learning algorithms (e.g. Q-learning).Another significant advancement of the proposed approach is that AR achieves the coordination of the agents actions using local information, without employing any inter-agent communication or observation. This is opposite to most multi-agent learning algorithms or multi-agent resource allocation solutions, that rely on inter-agent communication or observation in order to coordinate the agents and converge towards an optimal policy [10,33]. In the context of multi-agent resource allocation this advantage will help reveal alternative resource allocation schemes in cases where inter-agent communication is restricted and auctioning cannot take place. Furthermore, the AR layer also aids learning by approximating the continuous action space of the problem. This is because AR generates new actions that extend the initial action set Aiand gradually approximates [0, L]. This generation of new actions is feasible, since Aiwas produced by a sparse discretization of the continuous action space implied by the nature of the problem under study. The extension of Airesults to a more fine grained approximation of the continuous action space, where more actions are made available to the agents thus smoothing state fluctuations and assisting the whole learning process [39]. It should be noted that this fine grained approximation could be achieved by a more dense discretization of the action space, however this approach is unrealistic as it would increase the memory requirements of the simulations. The proposed approach is similar to [39], where the action space was partitioned into 28 pieces and following an iterative procedure, a discrete policy learning algorithm was used to find the optimal actions in a single-agent setting. However a significant advantage of the proposed approach is that it employs stochastic optimization methods to reveal the way that actions should be modified in order to converge to efficient solutions.Letaitdenote the action that agent i should take on iteration t (based on his policy πi(·)). In the proposed approach, agent i is considered to actually take an action(ait)′, which is given by(8)(ait)′=ait·F(·)whereF(·)is a rectification function that encapsulates domain knowledge. This functionF(·)is defined by 3 input parametersw,y,z(i.e.F:ℝ3→ℝ), which represent all the available local information of the agent: the perceived rate of change of the resource at iteration t (denoted by parameterw), the water each agent has accumulated over time (parameter y) and the feedback accumulated over time (parameter z). The association between these inputs and the output ofF(·)however, that will eventually modify the agent's action according to Eq. (8), is unknown. Even though empirical rules can be imposed, as is the case in [3] where a heuristic function was defined based on the findings of the decision support system presented in [48], this approach does not guarantee the optimal behaviour ofF(·). To alleviate this issue, AR employs stochastic optimization methods that reveal the proper association between the inputs and output ofF(·), in order to achieve efficient resource allocation.Let,F(·)be defined as(9)F(w,y,z)=fk(w)·fk(y)·fk(z)where fk(x) is a function representing the association between each one of the input variables andF(·), and k is the index of this function. For the purposes of this study, the following basic functions are considered possible candidates for fk:(10)fk(x)=axb(k=1)abx(k=2)(logax)b(k=3)ax2+bx(k=4)aebx(k=5)wherea∈ℝandb∈ℝare parameters that denote the weight and order of each function, respectively. The rectification functionF(·), can be fully defined by the function type (i.e. k) and the function parameters (i.e. a and b) for each input variable. In this sense,F(·)can be fully described by a parameter vector:(11)D=[kwawbwkyaybykzazbz]TThe main question is how to define the optimal parameter vector D*so as to encapsulate the exact association between the local information of the agent (i.e. input parameters) and the way his actions should be modified. The estimation of D*can be then seen as a ‘black box’ unconstrained optimization problem, where the are no conditions about the objective function. In more detail, provided that all the parameters of a simulation are kept constant (i.e. number of agents, greediness parameters, policies), the outcome of the simulation would be defined only by Eq. (8), i.e. by the parameter vector D (Eq. (11)). Additionally, the outcome of a simulation can be described through the utility function VC(Eq. (2)) by a single numerical value, that increases as the allocation of the resource becomes more efficient. Combining these two observations, the entire simulation process can be approximated by a non-linear mapping function of D, denoted byS(D), that maps D vectors to VCvalues (S(D):ℝ9→ℝ). Evidently, D*will be the vector that maximizesS(D), i.e.D*=argmaxDS(D). For the purposes of this study, two stochastic optimization methods are employed for the estimation of D*: Real Coded Genetic Algorithms (RCGA) and Covariance Matrix Adaptation Evolutionary Strategy (CMA).The first approach for the estimation of the parameter vector D*is based on genetic algorithms (GAs), a stochastic search method that is inspired be the mechanisms of natural selection and natural genetics [23,15]. They are considered a robust search method, mainly due to their ability to exploit the information accumulated about an initially unknown search space in order to bias subsequent searches into useful subspaces [23]. A traditional genetic algorithm starts with a set of possible solutions to a given problem, called ‘chromosomes’, that are made up of a series of variables, called genes. The efficiency of each chromosome in solving the problem is defined using a fitness function and the algorithm proceeds in successive iterations, called ‘generations’, where the chromosomes are altered by genetic operators.In the context of this paper, a Real Coded Genetic Algorithm (RCGA) is used for the estimation of the optimal parameter vector D*. The term ‘real coded’ refers to the coding type used for the representation of solutions by chromosomes. Since all the elements of D*are real numbers it was chosen to code the chromosome as a finite length vector of real numbers that correspond to the elements of D*. This approach is typical when dealing with real world problems, as it allows domain knowledge to be easily integrated in the genetic algorithm [22]. Furthermore, the use of real coding representation will avoid the binary encoding and decoding of the solution, thus increasing the speed of the genetic algorithm [22]. To that extent, in accordance to the definition of the parameter vector D (Eq. (11)), a typical chromosome of the population is represented as(12)Di=[d1…dj…d9]where i denotes the index of the chromosome in the population anddj∈ℝdenotes the jth gene of Di. As is typical in Real Coded Genetic Algorithms, the search space should be confined by limiting the available range of values for dj. In this study this available range was experimentally defined so as dj∈[−5, 5]. Moreover, the fitness of each chromosome is evaluated using the non-linear mapping functionS(D).An initial population of size λ is generated by means of a random number generator that draws samples from a Gaussian distribution (λ=100 in the context of this paper). The algorithm proceeds iteratively and each iteration consists of a selection step, a crossover step and a mutation step. During the selection step, all chromosomes are evaluated usingS(D), and based on their fitness score a subset of the population is selected to breed a new generation of chromosomes. Only the ‘fittest’ chromosomes are considered to survive this procedure, thus the search is driven towards the region of the best chromosomes. For the purposes of this paper, the roulette selection method is used, where the probability of a chromosome being selected is proportional to its fitness (i.e. fittest chromosomes have more chances of being selected) [13]. The selected chromosomes are subsequently copied in the mating pool for the crossover step. It should be noted that the selection procedure is repeated until λ parents are selected. Moreover, to ensure that the best found solution will not disappear from one generation to the next, the elitism strategy is used that copies the 4 highest scoring chromosomes directly to the next generation without exposing them to further genetic operations [52].Subsequently, the parent chromosomes from the mating pool are selected to produce offsprings for the population of the next generation (crossover step). Crossover is a primary search operator that shares information between chromosomes, by combining the features of two or more chromosomes (i.e. parents) to create potentially better offsprings [23]. The underlying idea is that the exchanging of genetic material amongst good chromosomes is bound to generate even better chromosomes, thus even better estimates of D*. In more detail, randomly selected pairs of chromosomes are combined, with a crossover probability pc(set to 0.85 in the context of this paper), to create new offspring chromosomes. In this study the heuristic crossover operator presented in [51] is employed. Let D1 and D2 be two parent chromosomes, where D1 is the one with the best fitness, anddjD1,djD2the jth gene of each parent. The genesdjD1D2of the offspring chromosomeDD1D2are given by(13)djD1D2=r(djD1−djD2)+diD1where r∈[0, 1] is a random number drawn form a Gaussian distribution. According to the definition of Eq. (13), it is ensured that each offspring will be close to the parent chromosome with the highest score.The final step in each iteration is the mutation step, where the genes of all the offspring chromosomes of the population are randomly modified with a mutation probability pm(set to 0.05 in the context of this paper). This procedure results to an exploration of the search space, further to the exploration provided by the crossover step, that prevents the population from stagnating in local optima or suboptimal solutions. In this study a simple static Gaussian mutation is employed [47]. In more detail, the jth gene djof a chromosome D is mutated todjmaccording to(14)djm=dj+N(0,σ)where N(0, σ) is a random number drawn from a Gaussian distribution with mean zero and standard deviation σ. In this study, σ is fixed to (max(dj)−min(dj))/2, where max(dj) and min(dj) denote the upper and lower limits of the parameters range (i.e. [−5, 5]) respectively.The Real Coded Genetic Algorithm is terminated when a maximum number of generations is reached, which is set to 400 in the context of this paper. Evidently, the optimal parameter vector D*is the fittest chromosome of the population.The second approach for estimating D*is based on the Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [19,20], a population based stochastic optimization algorithm that is recognized as one of the most powerful derivative-free optimization methods for real valued problems [18]. Covariance Matrix Adaptation uses a Gaussian mutation distribution, denoted by N(m, σ2C), that is described by a symmetric covariance matrixC∈ℝNxN, a mean vectorm∈ℝNand a step-size parameter (i.e. standard deviation)σ∈ℝ. The underlying concept of the approach is that the orientation and the shape of the distribution (i.e. the covariance matrix) are iteratively updated based on the so called evolution path, so that the likelihood of previously successful search steps is increased [28].Each iteration of the algorithm consists of a population generation step, that is followed by a parameter adaptation step. During the population generation step, possible solutions for D are generated by drawing samples from a n-dimensional normal distribution N(m, σ2C). In more detail, considering that the covariance matrix C can be written in terms of its eigenvector decomposition, i.e. C=BW2BT, where W is a diagonal matrix with square roots of eigenvalues of C as diagonal elements, random populations of λ possible solutions Di, (i=1, …, λ) are generated according to(15)Di(g+1)=m(g)+σ(g)B(g)W(g)zi(g+1)where i denotes the index of the candidate solution, g the generation (iteration) index andzi∈ℝis a vector of random samples generated from a multivariate normal distribution with zero mean vector and identity covariance matrix (i.e. N(0, I)).The parameter adaptation step follows, where the parameters of the mutation distribution are updated so that the likelihood of previously successful candidate solutions are maximized. The mean vector m of the mutation distribution is updated by selecting and recombining the best candidate solutions. Selection is done by ranking the λ candidate solutions in order of ascending fitness, i.e.(16)S(D1:λ)≤⋯≤S(Dμ:λ)≤⋯≤S(Dλ:λ)where the notation S(Di:λ) denotes the ith best individual. The mean m is subsequently updated with the weighted average of the best μ candidate solutions, i.e.(17)m(g+1)=∑iμwiDi:λ(g+1)wherewi, 1<i<μ are strictly positive and normalized weights. The adaptation of C entails a rank-μ update rule, that assigns recent generations a higher weight and reduces the number of necessary generations from O(n2) to O(n) [21], and a rank-one update rule that uses an evolution path (i.e. cumulation) to exploit the correlation between successive generations [21]. Let pcdenote the evolution path parameter, which contains information about the correlation between consecutive steps. The covariance matrix C is updated by(18)C(g+1)=(1−c1−cμ)C(g)+c1pc(g+1)[pc(g+1)]T︸rank-one update+cμ∑iμwiyi:λyi:λT︸rank-μupdatewhere cμand c1 are the learning rates for the rank-one and rank-μ update rules respectively andyi:λ(g+1)=(Di:λ(g+1)−m(g))/σ(g). The evolution path parameter pc, that is required for the estimation of Eq. (18), is derived by means of exponential smoothing as follows:(19)pc(g+1)=(1−cc)pc(g)+cc(2−cc)μeffm(g+1)−m(g)σ(g)where ccis a variable defining the backward time horizon of the evolution path pcandcc(2−cc)μeffis a normalization constant.Whereas Eq. (18) adapts the shape of the distribution, the size of the distribution (i.e. variance) is adapted by the σ parameter. In more detail, σ is updated by(20)σ(g+1)=σ(g)·expcσdσ∥pσ(g+1)∥E∥N(0,I)∥−1where pσis a conjugate evolution path and E∥N(0, I)∥ is the expectation of the Euclidean norm of a N(0, I) distributed random vector. The evolution path parameter pσ, that is required for the estimation of Eq. (20), is derived using the same techniques as Eq. (19) and is given by(21)pσ(g+1)=(1−cσ)pσ(g)+cσ(2−cσ)μeff[C(g)]−12m(g+1)−m(g)σ(g)wherecσ(2−cσ)μeffis a normalization constant and cσis the backward horizon of the evolution path pσ.In this study, the population size λ was set to 10, using the heuristic λ=4+3·log(N) proposed in [21], and the remaining parameters were set to the default strategy parameters according to [17]. The algorithm stops after 100 iterations, and the optimal parameter vector D*is the best candidate solutionS(D1:λ)of the last generation.A series of extensive computer generated simulations were conducted, evaluating the efficiency and robustness of the proposed AR-RCGA and AR-CMA methods, i.e. their ability to reveal efficient resource allocation schemes regardless of the water needs of the population and regardless of the policy the agents use. To investigate their performance across the widest possible range of water needs of the population, a Monte Carlo procedure similar to [5] was employed, where agents were distributed to the various greediness degree categories by means of a random number generator. This procedure was repeated until all the possible combinations of agents assignment to greediness categories were achieved, creating agent populations (called simulation scenarios) with varying water needs.Furthermore, since the proposed methods operate as an additional layer of complexity on the decision making process of the agents (Fig. 1), they can only be assessed together with a policy and a learning algorithm. Although the literature of Q-learning variants is vast, very few algorithms can be used due to the imposed constraints described in Section 2. For example [4,46,31,24,32,14,35,25] either employ inter-agent communication or assume that agents observe each other to require full knowledge of other agent's utilities and strategies. In this study, four popular decentralized Q-learning variants [29] that do not entail inter-agent communication or observation were employed11For simplicity reasons, a policy is denoted together with the learning algorithm by π, with the subscript denoting the learning algorithm.:•Decentralized Q-learning (πDec−Q): This is a straightforward extension of the Q-learning algorithm to stochastic games with independent learners [34]. Each agent maintains his own Q table and updates its values according to:(22)Qi(st,ait)=(1−α)Q(st,ait)+α[r+γmaxait+1Qi(st+1,ait+1)]where α is the learning rate of the algorithm.Actions are subsequently selected with the aid of a Boltzmann distribution that addresses the exploitation/exploration trade off dilemma, using concepts derived from simulated annealing. In more detail, the probability of selecting an action j is estimated using(23)pj=eQi(ai,j)·τ−1∑keQi(ai,k)·τ−1where τ is the temperature parameter defining the balance between exploration and exploitation. High values of τ make the exploration random while very low values, e.g. τ=0.01, correspond to a greedy approach. When τ=1, as is the case for our experiments, Eq. (23) is called a Gibbs distribution, commonly used since it provides balanced results between constant exploration and constant exploitation. It should be noted that this policy resembles the SoftMax strategy, commonly met in the field of game theory [26].Hysteretic Q-learning (πHyst−Q): This is a popular decentralized variant of Q-learning that uses the concept of optimistic agents, found in distributed Q-learning [34]. The idea is that the agents should not be altogether blind to penalties at the risk of ignoring the noise due to environment. Instead they must be chiefly optimistic to reduce oscillations in the learned policy [29]. Hysteretic Q-learning employs a heuristic rule, which ensures that low rewards that result from actions that have been satisfying in the past will have less importance in the update of the Q-table. In more detail, if agent i executes the action a and goes from state s to state s′, then the update rule is given by:(24)Qi(s,a)=Qi(s,a)+Qi(s,a)+αδ,ifδ≥0Qi(s,a)+βδ,otherwisewhere α, β are learning rates and the heuristic δ is given by(25)δ=Ri+γmax(Qi(s′,a′))−Qi(s,a)Actions are subsequently selected using the soft-max policy strategy, presented above.Frequency Maximize Q-value (πFMQ): This policy selects the actions whose values are evaluated using the Q-learning variant known as Frequency Maximum Q value (FMQ) [26]. Under FMQ, the probability of choosing an action is biased by the frequency of the maximum rewards received for that action. In more detail, the importance E of each action is evaluated by adding the Q values to the FMQ heuristic, according to:(26)E(ait)=Q(ait)+c×F(ait)×Rmax(ait)where Rmax(a) is the maximum reward received so far when choosing action a, F(a) is the frequency of receiving the maximum reward corresponding to action a and c is a weighting factor controlling the importance of the FMQ heuristic in the evaluation. Actions are subsequently selected using the soft-max policy, presented above.Policy hill climbing (πPHC): This policy is a simple extension of Q-learning, which performs hill climbing in the search space [8]. It allows agents to learn mixed strategies by giving more weight to actions that the Q-learning layer believes is best according to a learning rate δ. In more detail, after the classical Q-learning layer the policy of each agent is iteratively improved using the formula(27)pj=pj+δ,ifj=argmaxQ(s,j)−δ|Ai|−1,otherwisewhere δ is a learning rate and |Ai| denotes the number of actions in the action space [8]. In this sense, Eq. (27) increases the probability that the agent will select greedy actions by subtracting an equal amount from all actions and adding what was subtracted to the action with the maximum Q-value.Four sets of experiments were conducted, demonstrating the advancement provided by the proposed methods. During the first set of experiments the AR layer was not employed and agents’ actions resulted only from the learning and policy layer (Fig. 1), demonstrating the problems arising when directly applying multi-agent learning algorithms to a resource allocation with non-communicating users. The two following sets employed the proposed AR-RCGA and AR-CMA methods, whereas the last set of experiments employed the heuristic learning algorithm presented in [3], referred to as Empirical throughout this manuscript. As previously mentioned, both AR-CMA and AR-RCGA methods must first undergo a training procedure, in order to estimate the optimal parameter vector D*(as described in Section 5). Training was conducted on a 50 agent population setting, consisted of agents that were uniformly distributed to the 5 greediness categories. To simplify the training process and ensure the convergence of the optimization algorithms, agents employed a stationary policy that constantly selected a specific action according to their greediness degree. Fig. 2presents the training progress for AR-RCGA and AR-CMA, with respect to the maximization of VC. AR-CMA clearly outperforms AR-RCGA, as it achieves higher VCvalues. This is attributed to the fact that convergence of real value coding genetic algorithms is still an open research issue, that requires the manual tuning of many parameters. On the contrary, covariance matrix adaptation managed to achieve higher VCvalues with minimal effort, using the default strategy parameters according to [17]. It should be noted however that the superiority of CMA is achieved in more training episodes, mainly due to the increased computational complexity of the method. Table 1presents the estimated D*, together with the corresponding action rectification functionF(·)of Eq. (8) for all the stochastic optimization methods considered in this study. In addition, the heuristic function employed in [3] is also reformulated in accordance to Eq. (9) and is given in Table 1, so that a direct comparison with the estimated optimal vectors for AR-RCGA and AR-CMA is feasible. It should be noted that both AR-RCGA and AR-CMA managed to encapsulate the inverse proportional linear relationship of the input parameters withF(·), which is in accordance with the empirical estimation of the heuristic function presented in [3].Furthermore, since Q-learning variants are employed in all sets of experiments, the reinforcement signal received by each agent should be properly defined. For the first set of experiments, where the AR layer was not employed, Eq. (4) was used as a reinforcement signal. However for the remaining sets of experiments that entail action rectification, agents received rewards for their action through Eq. (7). This is because the AR layer has already been trained using VC, thus the use of Eq. (4) for the agent's reward might bias the results in favour of the proposed AR methods (since it has been already used to train them). Moreover, Eq. (7) employs only local knowledge, thus is considered more suitable for a decentralized solution as the one proposed in this work.Finally, for every set of experiments and for every simulation scenario, the Q-learning layer was trained for a period of 500 training episodes. Results presented in this section are derived by statistical analysis of the last training episode i.e. after the agents have been properly trained. Fig. 3depicts the learning curve of each policy, with respect to the maximization of VCfor an agent population where agents are uniformly distributed to the 5 greediness categories (similar to the one used for the training of the AR methods). Due to the non-stationarity and stochasticity of the environment, without the AR layer the multi-agent learning algorithms employed encountered problems learning a mixed policy that maximizes VC. Plots that correspond to simulations without the use of the proposed AR methods exhibit slight oscillations, indicating learning instability, whereas all the entailed policies converge to relative low VCvalues. On the contrary, with the proposed method a stability in learning is observed, since learning curves in both AR methods exhibit an increasing trend without oscillations across all policies. Furthermore, the proposed AR methods achieve far more higher VCvalues across all policies. In more detail, all AR plots start (left end of plot Fig. 3) at higher VCvalues than the final VCvalues (right end of plot Fig. 3) of the policies without AR, indicating a significant advantage provided by the proposed methods.Initially, the policies were evaluated by examining the water level at the end of each simulation scenario, together with the percentage of agents actually gathering enough water to sustain their production and economically survive. These two criteria provide a direct insight of the immediate impact the policies inflict to the environment, as well as to the agent's community. Even though these criteria are highly correlated to each other (and should be examined together) initially they are examined independently in order to provide a more elaborate view of the difficulty of resource allocation.Fig. 4presents the average water level after 160 iterations for all the simulation scenarios, when agents employ the policies under study (with and without the AR layer). It is clear that when the AR layer was not employed, all policies resulted in very low lake levels, indicating that agents did not learn how to efficiently allocate water from the resource. The average water level across all policies is 455units of water, which corresponds to 4.5% of the resource's initial volume. It should be noted however that the corresponding median values across all policies are zero, denoting that for most simulation scenarios the resource was completely depleted, regardless the policy the agents used. This is in accordance with research findings regarding the exploitation of natural resources under the lack of communication [37]. Furthermore, this indicates that the multi-agent learning algorithms employed failed to coordinate the actions of the agents for the preservation of the resource.Results were significantly improved when the agents’ actions were modified by the proposed AR methods. In more detail, across all AR methods there is a significant increase of the water level of the resource, indicating that the AR layer coordinated the agents towards the preservation of the resource despite the lack of communication. This is because less water was drained by the agents in overall, thus action rectification (achieved through Eq. (8)) led to a more conservative draining scheme. In accordance with the findings of Fig. 2, the best performance was exhibited by AR-CMA, where in average 4 times more water was preserved. Furthermore, Empirical and AR-RCGA exhibited a relatively similar behaviour preserving 3.3 and 3.1 times more water, respectively. This similar behaviour indicates that genetic algorithms managed to encapsulate the association between the input and output parameters ofF(·), in a manner that is in accordance with the empirical observations used to define the heuristic function in [3]. Moreover, the respective median values of the average water levels across all policies were also increased. In more detail, the respective median values were increased to 250 for Empirical, 324 for AR-RCGA and 563 for AR-CMA. Considering that median is a robust estimator of the central tendency of a distribution, this improvement indicates that with the proposed AR methods further to the increase of the water volume, the resource is preserved for more simulation scenarios.Although the use of the water level as an evaluation criterion is straightforward, the evaluation of a policy with respect to the agents economic survival should be further explained. The appropriate amount of water required to sustain the agent's survival does not correspond to the agents optimal goalGi*(i.e. agents total request). Since agents drain water for irrigation purposes, the goalGi*of each agent during the cultivation period is to gather enough to sustain his cultivation and make a profit. Depending on the type of cultivation however, there is a threshold STdefining the minimum quantity of water that the crop should be irrigated with during the cultivation period or else it is destroyed. Agents that gather less water than this threshold are not considered to economically survive, either because their cultivation is destroyed or because their profit was not enough to sustain them financially. The ratio of the number of the agents N1 exceeding that minimum quantity of water ST, to the total number of farmers (N), i.e. SR=N1/N is defined as the ‘survival rate’ SR.Fig. 5presents the average survival rate for all the simulation scenarios, when agents employ each one of the policies under study (with and without the AR layer). When the AR layer was not employed, very few of the agents in average managed to gather enough water to sustain survival (i.e. 13% in average). The corresponding median values across all policies are zero, indicating that in most simulation scenarios none of the agents gathered enough water to sustain survival (i.e. tragedy of the commons [37]). This is attributed to the failure of the learning algorithm to converge towards an efficient solution, that should favour more conservative actions while the resource was dried out. Furthermore, this clearly reveals that the drained water was not efficiently distributed amongst the agents. This finding holds even for policies that exhibited the best performance in terms of water preservation, stressing the complexity of the problem. For example, πPHCmay achieved higher water levels but in terms of agent's survival it exhibited the worst performance, indicating that the drained water was not efficiently distributed amongst the users.The respective survival rates SRwere significantly improved when the proposed AR methods were employed, as depicted in Fig. 5. Across all AR methods the average values were increased for all policies under study, as at least 23% of the population managed to survive in most simulations (i.e. 1.7 times more agents when compared to simulations that the AR layer was not employed). Thus, further to the preservation of a significantly increased amount of water (as already shown in Fig. 4), a more efficient distribution of water was achieved and more agents managed to sustain survival. This is because, water otherwise wasted by greedy agents is now more efficiently utilized, as it is now redistributed to non-greedy agents, assisting them to meet their goals and survive. Thus, despite the lack of communication, the proposed AR methods coordinate the actions of the agents in order for the whole community to survive. A closer look of Fig. 5 reveals that when the Emprical method was employed, there was an increase of SRof 1.78 times in average, whereas AR-RCGA provided an increase of the survival rate of 2 times in average. Again, the best performance was exhibited by the AR-CMA, that increased the average number of surviving farmers to 33.5%, indicating that 2.4 times more agents manage to survive. It is thus verified that Covariance Matrix Adaptation provided a better estimate of D*, that not only manages to preserve a significant portion of the resource but, also, to increase the number of agents that meet their goals and survive.None of the criteria described above evaluates the performance of the policy both with respect to the environment and to the agent's community simultaneously. To that extent, a more general evaluation is provided by experimentally estimating the probability of joint events. More precisely, for every policy under study, the following probabilities were estimated: (a) the probability P(a) that at least 5% (500units) of the resource's volume will be preserved, (b) the probability P(b) that at least 10% of the agents will gather enough water to sustain survival and (c) the joint probability P(a, b) which denotes the probability that at least 10% of the agents survive and at least 5% (500units) of the resource's volume is preserved, at the same time. The selection of these specific probability values in the estimation of P(a), P(b) and P(a, b) stem from the fact that an allocation is considered satisfactory only if at least a fragment of the resource is preserved and, simultaneously, a fragment of the agents managed to sustain survival. Based on research reports of real world water allocation management models, in this study these fragments were set to 5% and 10% respectively.The experimentally calculated probabilities P(a), P(b) and P(a, b) are given in Table 2. It is clear that when the AR layer was not employed (i.e. 1st column), the probability of the resource being preserved (P(a), 1st row) attains low values for the πDec−Qpolicy and slightly higher values for πHyst−Q, πFMQand πPHCpolicies (i.e. 0.31, 0.33 and 0.31 respectively). This indicates that, e.g. under the πFMQpolicy, there is a 31% probability that 5% of the resources’ volume will be preserved. However, the achievement of these slightly higher P(a) values comes at the expense of the agent's survival. More precisely, as indicated in the 2nd row of Table 2, P(b)=0.03 in average, demonstrating that agents are highly unlikely to survive under any policy. This is a typical problem of the tragedy of the commons dilemma, demonstrating that even though some policies may result in higher water levels and preserve the resource, this comes at the expense of the survival of the agents and vice versa [37]. P(a, b) provides the joint probability of the agent's survival and the simultaneous preservation of the resource. Since P(a, b)=0.12 in average, it is clear that it is highly unlikely for any policy to ensure the preservation of the resource and the survival of the agents at the same time. This means that learning, as achieved through the direct application of learning algorithms, failed to train the agents so as to preserve the resource and, simultaneously, meet their goals and survive.Results were significantly improved when the heuristic learning algorithm and the proposed AR methods were employed (2nd column, 3rd column and 4th column of Table 2). For the Empirical method P(a)=0.88 in average, which denotes the heuristic learning function employed in [3] improves the probability that the resource will be preserved. This performance however is clearly outperformed by the proposed AR methods, as P(a) increased to 0.91 and 0.92 for AR-RCGA and AR-CMA respectively, in average. This indicates that with the proposed methods, it is almost certain that 5% of the resource will be preserved, regardless of the policy the agents adopt. Similar results are derived when examining P(b), where the performance of the Empirical method (i.e. P(b)=0.64 in average) was outperformed by the proposed AR methods (P(b)=0.65 and P(b)=0.66 in average for AR-RCGA and AR-CMA respectively). The advancement provided by the proposed AR methods can be realized by considering that without the AR layer, P(b)=0.03 in average.The robustness of the proposed AR methods can be realized through P(a, b), that expresses the probability that the resource will be preserved and the agents will survive, simultaneously. While the Empirical method increases P(a, b) to 0.62 in average, with AR-RCGA and AR-CMA P(a, b) is further increased to 0.64 and 0.66, respectively. This is in accordance with our previous findings, where it was made clear that the use of the proposed methods not only preserved the resource (see Fig. 4), but the water drained was more efficiently distributed amongst the agents resulting to an increase of their survival rate SR(see Fig. 5). Also, in accordance with our previous findings, Table 2 reveals that AR-CMA achieved the best increase when examining all probabilities, verifying that CMA was able to encapsulate the proper association between the input and output parameters ofF(·), and achieve efficient resource allocation.To further investigate the robustness of the proposed AR methods in achieving efficient resource allocations, additional experiments were conducted investigating P(a, b) when the required number of surviving agents used for its estimation is increased. In more detail, Fig. 6depicts the variation of P(a, b), while the requirement of the percentage of surviving agents is gradually increased to 50% (horizontal axis). Comparing the plots of Fig. 6, it is made clear that the proposed AR methods achieve higher values of P(a, b) in all cases, even if the portion of the surviving agents is increased to 50% of the population. Although a similar increase of P(a, b) is observed in both AR methods and the Empirical method, AR-CMA clearly provides the best results, as in average it achieves 4.5 times higher P(a, b) than the non-AR policy (compared to 3.4 times and 3.1 times of Empirical and AR-RCGA respectively. It is thus verified that the proposed methods can be considered as viable means of simultaneously preserving the resource and ensuring the survival of the agents, providing a substantial improvement, when compared to the direct application of typical learning algorithms to natural resource sharing.

@&#CONCLUSIONS@&#
This study deals with multi-agent leaning within a natural resource allocation framework, where a natural resource is being exploited by a community of users. This is a significant and complex problem, that added to the complexity of multi-agent learning on real world applications (i.e. non-stationarity, stochasticity, continuous variables, noise, uncertainties in observations, large number of agents), employs agents that are not considered to communicate or observe with each other.In this study, we extend our previous work by introducing two learning methods based on a novel approach, called Adaptive Rectification – AR that uses a parametrized rectification functionF(·)in order to embed domain knowledge in the reasoning process of the agents. The proposed methods employ stochastic optimization methods for the training of theF(·)function, namely Real Coded Genetic Algorithms for the AR-RCGA method and Covariance Matrix Adaptation Evolutionary Strategy for the AR-CMA method. The main contribution of the aforementioned approaches are that they achieve efficient resource allocation, despite the lack of communication and observation amongst the users of the resource. In addition, they can be used to extend general purpose multi-agent learning algorithms to natural resource allocation. Furthermore, they operate on a decentralized manner, thus making them suitable for other applications of multi-agent resource allocation where communication based solutions cannot be applied.The proposed methods were applied on a MAS model that simulates a real world resource allocation phenomena, where a water resource is the exploited by a community of users. Four popular decentralized Q-learning variants with a soft-max policy were used and following a Monte Carlo procedure, extensive experiments were conducted evaluating the performance of these policies with and without the proposed methods. It was shown that without the use of the proposed AR methods, none of the policies managed to provide efficient resource allocation schemes, as in the majority of the simulation scenarios the resource was depleted and none of the users managed to gather enough water to sustain survival. This is in accordance with research findings, which indicate that the lack of communication in resource sharing scenarios leads to the depletion of the resource (i.e. tragedy of the commons dilemma). In addition, these findings clearly demonstrate the problems that general purpose multi-agent learning algorithms experience, when operating under the lack of communication and in real world scenarios.Results were significantly improved when the proposed AR methods were employed. In more detail, 3.6 times in average more water was preserved in the resource, and for more simulation scenarios, across all policies. Additionally, at the same time, the number of agents that managed to sustain survival were increased by 2.2 times in average, across all policies. In this sense, the proposed AR methods are capable of revealing efficient resource allocation schemes that preserve the resource and at the same time, ensure the economic survival of the agents. Out of the two stochastic optimization methods considered in this study, covariance matrix adaptation provided the best results, verifying that it is one of the most powerful derivative free optimization methods. A probabilistic analysis of the results verified the aforementioned findings, demonstrating that with the proposed AR methods, the probability that the resource will be preserved and, simultaneously, a portion of the agents will meet their goals and economically survive is significantly increased (5.1 times in average across all policies). It is thus demonstrated that, the proposed AR methods are adequate in providing efficient resource allocations, that simultaneously preserve the resource and ensure the survival of the agents, and provide a substantial improvement, when compared to the direct application of typical learning algorithms to natural resource sharing.