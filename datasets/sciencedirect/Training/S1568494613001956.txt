@&#MAIN-TITLE@&#
Modelling of diesel engine performance using advanced machine learning methods under scarce and exponential data set

@&#HIGHLIGHTS@&#
Diesel engine models are built using advanced machine learning techniques and verified based on experimental data.A new hybrid inference is proposed for the selection of hyperparameters of kernel based extreme learning machine.Problems of data scarcity and exponentiality are eased by using logarithmic transformation of dependent variables to pre-process the data.A comparison among the models built by advanced and traditional methods is conducted.The models developed by advanced methods with the hybrid inference and logarithmic transformation are more accurate than traditional ones.

@&#KEYPHRASES@&#
Diesel engine modelling,Engine performance,Kernel based extreme learning machine,Hybrid inference,Data exponentiality,Data scarcity,

@&#ABSTRACT@&#
Traditional methods on creating diesel engine models include the analytical methods like multi-zone models and the intelligent based models like artificial neural network (ANN) based models. However, those analytical models require excessive assumptions while those ANN models have many drawbacks such as the tendency to overfitting and the difficulties to determine the optimal network structure. In this paper, several emerging advanced machine learning techniques, including least squares support vector machine (LS-SVM), relevance vector machine (RVM), basic extreme learning machine (ELM) and kernel based ELM, are newly applied to the modelling of diesel engine performance. Experiments were carried out to collect sample data for model training and verification. Limited by the experiment conditions, only 24 sample data sets were acquired, resulting in data scarcity. Six-fold cross-validation is therefore adopted to address this issue. Some of the sample data are also found to suffer from the problem of data exponentiality, where the engine performance output grows up exponentially along the engine speed and engine torque. This seriously deteriorates the prediction accuracy. Thus, logarithmic transformation of dependent variables is utilized to pre-process the data. Besides, a hybrid of leave-one-out cross-validation and Bayesian inference is, for the first time, proposed for the selection of hyperparameters of kernel based ELM. A comparison among the advanced machine learning techniques, along with two traditional types of ANN models, namely back propagation neural network (BPNN) and radial basis function neural network (RBFNN), is conducted. The model evaluation is made based on the time complexity, space complexity, and prediction accuracy. The evaluation results show that kernel based ELM with the logarithmic transformation and hybrid inference is far better than basic ELM, LS-SVM, RVM, BPNN and RBFNN, in terms of prediction accuracy and training time.

@&#INTRODUCTION@&#
Energy conservation and environmental protection are two major issues that governments and researchers have been focusing on. Diesel engines, as machines that can convert fuel into motion, closely relate to these two issues. Despite the superior fuel efficiency and high durability performance of diesel engines, the amount of fuel consumed and pollutants emitted during their combustion process bring along serious environmental problems to the world. For instance, the exhaust gases like nitrogen oxides (NOx) and carbon monoxide (CO) can attach to haemoglobin and reduce the oxygen transport efficiency [1]. Particulate matter can accumulate in the human respiratory system and cause various health problems [2]. Some evidences already showed that particulate matter is associated with life shortening [3]; in 2010 more than 3.2 millions of people around the world died prematurely due to the ambient particulate matter pollution [4]. World Health Organization also announced in 2012 that exposure to diesel engine exhaust may lead to lung cancer. Moreover, the International Energy Agency forecasted that the world oil consumption would increase from 87.4 million barrels per day in 2011 to 99.7 million barrels per day in 2035 [5]. Therefore, the reduction on both exhaust emissions and fuel consumption without deteriorating the engine performance has become the biggest challenge for many engine and vehicle manufacturers.Generally, there are several methods to deal with this challenge, but the most effective one must be through the improvement of the combustion process of diesel engines. However, it is very difficult to fully understand the combustion process due to its complex nature. Additional instruments must be installed on the engine for monitoring and controlling the engine parameters, while numerous tests and experiments must be conducted to investigate the relationship between the engine parameters, the engine performance and the exhaust emissions. Obviously, these are expensive, fuel- and time-consuming [6,7]. By creating a mathematical engine model, such effort can be reduced because all the costly and immeasurable data can be predicted by the model, and most of the expensive sensors can then be replaced by virtual sensors [8,9].In the current literature, different engine modelling techniques have been developed. They are basically classified into two classes: white- and black-box identifications. The white-box identification technique derives the engine models by resorting to some physical laws. Examples of white-box identification on diesel engine include those analytical multi-zone models [10–12], computational fluid dynamic models [13] and chemical kinetic models [14]. However, these mathematical engine models are very difficult to put into practice because excessive assumptions have been made in constructing the models. Many engine-specific parameters, for example, inlet valve flow coefficient and geometric circulation sectional area of inlet valve, must be provided in advance too. These physical parameters are usually difficult to estimate or predefine, particularly for non-factory engineers. Therefore, black-box model identification is more preferable nowadays because it can identify a complicated and unknown input/output relationship based on experimental data.In the field of black-box identification, many modelling algorithms using machine learning techniques have already been developed. The one that has been widely adopted on engine modelling is the artificial neural network (ANN). A summary for the applications of ANNs on the modelling of different combustion processes has been provided in Kalogirou's work [15], where 22 examples of general combustion process were analyzed. Although none of the examples has presented the modelling of the combustion process for the diesel engine, many recent studies [6,8,9,16–23] have already demonstrated it. For instance, Traver et al. [8] described the use of three different types of ANNs as a virtual sensor for the prediction of diesel engine emissions. Çelik et al. [16] successfully generated performance maps of a diesel engine using back-propagation neural network (BPNN) models. More recently, Canakci et al. [6], Ghobadian et al. [17], Oguz et al. [18], Shivakumar et al. [19], Ozgur et al. [20] and Mohamed Ismail et al. [21] still used BPNN to predict the performance, emission characteristics and other engine responses of diesel engines fuelled with biodiesel blends. Apart from BPNN, Xu et al. [22] presented the application of radial basis function neural network (RBFNN) in forecasting engine systems reliability and compared it with traditional multilayer perceptron model and Box–Jenkins autoregressive-integrated-moving average model, while Manjunatha et al. [23] also attempted to use RBFNN to predict and model the emissions from biodiesel engine. However, according to Wong et al. [7,24] and Haykin [25], traditional ANNs have actually many drawbacks for its learning process, such as multiple local minima, user burden on selection of optimal network structure, large training data size and over-fitting risk.Several advanced machine learning techniques, namely least squares support vector machine (LS-SVM) [26], relevance vector machine (RVM) [27,28], and extreme learning machine (ELM) [29,30], have therefore been introduced in the recent years to compensate for the aforementioned limitations of ANNs. They exhibit the major advantages of global optimum and higher generalization capability, which means that the prediction results should be more accurate than ANNs. Some recent researches successfully applied LS-SVM [7,24,31–33] and RVM [33–35] to the modelling tasks for automotive gasoline engines and other real world applications, and showed that they are superior to traditional ANNs. Examples [29,30,36] also showed that ELM, as the most emerging approach, is superior to ANNs and LS-SVM on many benchmark problems from both regression and classification areas, and a few studies [37–40] demonstrated the use of ELM on some engineering practices. Nevertheless, the application of these advanced machine learning techniques to diesel engine modelling is still very rare. As a result, this study aims to apply all these methods (i.e., LS-SVM, RVM and ELM) to model and predict the emission characteristics and combustion process parameters of a diesel engine. A comprehensive comparison among these methods with traditional ANNs, such as BPNN and RBFNN, is further carried out accordingly.In addition, since the modelling of the diesel engine is a regression task, two situations need to be considered. The first situation is that the model output is single dimension, which means that individual model is required for each engine performance output. The other one is with multi-dimension outputs, where one model is already sufficient to predict several engine performance outputs. LS-SVM and RVM can only handle the first case, while the others can deal with both. Hence, this study also attempts to construct the LS-SVM and RVM models under the first situations, and the ELM and ANN models under both situations.In the present paper, the three advanced machine learning techniques are briefly introduced in Section 2. Then, the experimental setup for the collection of the sample data is described in Section 3. The sample data collected are found to suffer from two problems, namely data scarcity (i.e., small data set) and data exponentiality (i.e., the output y grows up exponentially along input x), which seriously deteriorate the prediction accuracy. Therefore, the three advanced machine learning techniques cannot directly be applied to this problem domain. In this research, logarithmic transformation of dependent variables is proposed to pre-process the sample data sets so that the prediction accuracy of the models can be improved. This data pre-process technique is studied in Section 4. The application of the advanced methods and the traditional ANNs with the logarithmic transformation on diesel engine modelling, along with a newly proposed hybrid inference procedure for model parameter selection, are then presented in Section 5. Finally, the modelling results and the comparison among these methods are discussed in Section 6, with a summary in Section 7.LS-SVM [26] is, as its name, a least squares version of support vector machine (SVM) [41] which employs least-squares errors as an objective function of the optimization problem. Originally, SVM solves nonlinear function estimation by means of convex quadratic programs and the sparseness is obtained as a result of quadratic programming problems, which are inherently difficult to solve. LS-SVM modifies the original SVM formulation, where the equality constraints in the formulation are utilized instead of the inequality constraints being used in the ordinary SVM, leading to solutions of sets of linear equations that are easier to solve than quadratic programming problems. Most of the advantages of SVM, such as the global and unique solution and good generalization, are inherited to LS-SVM, with some addition benefits such as fewer hyperparameters are needed and less work is necessary. Therefore, LS-SVM is considered in this research instead of SVM.RVM [27] is an approach based on sparse Bayesian inference and has an identical formulation to SVM. It avoids the need to set additional hyperparameters, offers probabilistic prediction, and obtains highly sparse solutions. Although the training time of RVM is approximately in the cube of the sample numbers, an accelerated learning algorithm [28] was developed, which initializes with an empty model and then sequentially adds or deletes samples within the same principal framework to increase the marginal likelihood while at the same time modifies their weights. Owing to its accelerated algorithm and probability property, RVM is considered in the present study.ELM [29,30] is a new learning scheme for single-hidden-layer feedforward neural networks (SLFNs) which can overcome the insufficiently fast learning speed of the conventional learning algorithms. It was then extended to the generalized SLFNs where the hidden layer needs not be neuron alike [36]. The basic idea of ELM is that, the model has only one hidden layer, and the parameters of this hidden layer, including the input weights and biases of the hidden nodes, need not to be tuned. On the contrary, these hidden nodes parameters are assigned randomly, which means that they may be independent of the training data [42]. Afterwards, the output weights (linking the hidden layer to the output layer) are determined analytically using a Moore–Penrose generalized inverse [43]. The training time can then be extremely fast due to the small calculation steps. Moreover, since the output weights are calculated analytically using inverse matrix, the issues of local minima and improper learning rate suffered in traditional ANN are also overcome [30].Recently, a kernel based version of ELM has been developed [36], where the hidden layer feature mapping is determined by a kernel matrix. In such case, only the kernel function and its parameters need to be defined (i.e., the number of hidden nodes is not required). As a comparative study, both the basic ELM (ELMbasic) and the kernel based ELM (ELMkernel) are included in this paper.Since the machine learning techniques are data-driven, experiments were conducted to collect sample data for model training and testing. A naturally aspirated, water-cooled, 4-cylinder, direct-injection diesel engine was employed for the experiment, and its specifications are shown in Table 1.The engine was connected to an eddy-current dynamometer with a control system used for adjusting its speed and torque. Ultra low sulfur diesel fuel containing less than 10-ppm-wt sulfur was used for data sampling. The experimental setup is illustrated in Fig. 1.In order to analyze the combustion process, in-cylinder pressure, and calculate the heat release rate, a Kistler type 6056A water-cooled piezoelectric pressure transducer was used to measure the in-cylinder pressure. The measured pressure signals were amplified with a Kistler charge amplifier type 5011B, and then recorded and analyzed with a combustion analyzer (DEWETRON, DEWE-ORION-0816-100x). A Kistler crank-angle encoder was also employed for crank-angle signal acquisition at a rate of 720 signals per engine revolution. For the gaseous species in the engine exhaust including CO, CO2 and NOx, Anapol EU5000 exhaust gas analyzer was adopted and these exhaust gas emissions were measured on a continuous basis. The Anapol EU5000 used infra-sensors for measuring CO and CO2 concentrations and used chemical cells for measuring NO and NO2 to obtain the NOxconcentration. The gas analyzer was calibrated with standard and zero gases before each experiment, and the measured exhaust gas emissions were converted into brake-specific gas emissions (e.g., brake-specific carbon monoxide (BSCO) and brake-specific nitrogen oxides (BSNOx)) in this study. Particulate mass (PM) concentration was measured with a tapered element oscillating microbalance (TEOM, Series 1105, Rupprecht & Patashnick Co., Inc.). The exhaust gases from the engine were diluted before passing through the TEOM with a Dekati mini-diluter, which dilution ratio was around 8.The experiments were conducted at five engine speeds of 1200, 1400, 1600, 1800, and 2000rpm, and each at engine torque of 28, 70, 140, 210, and 252Nm, except for 2000rpm where 252Nm cannot be reached. The environment was kept almost constant with air temperature varying from 25°C to 28°C and humidity ranging from 60% to 70%. To ensure the repeatability and comparability of the measurements, the cooling water temperature was automatically controlled by a temperature controller to 80°C, and held to within ±2°C, while the lubricating oil temperature varied from 90 to 100°C, depending on the engine load. At each speed and torque, data were recorded after the engine had reached the steady state, which was indicated by the lubricating oil temperature and the coolant temperature. The data were recorded continuously for 5minutes for the purpose of reducing experimental uncertainties. Each test was carried out three times and the average values were used.Besides, in each test, the volumetric flow rate of fuel was measured using a measuring cylinder and then converted into mass consumption rate based on the density of the fuel (i.e., fuel flow rate). The brake thermal efficiency (BTE) was then calculated based on the engine speed, engine torque and fuel flow rate. The air–fuel ratio (AFR) in this study is evaluated based on the CO2 concentration.Since the collection of the experimental data is time-consuming and costly, while the range of the operating speed of the test diesel engine is relatively narrow (1200–2000rpm), only 24 sets of data corresponding to different engine speeds and torque were collected from the experiments. From the view point of the machine learning approaches, 24 sets are considered as small data set. The problem of small data set is always encountered in small-scale test laboratories. To analyze the collected sample data before modelling, the AFR, fuel flow rate are provided in Tables 2 and 3, and the characteristics of BTE, BSCO, BSNOx, PM concentration, peak pressure (Ppeak), location of Ppeak at crank angle after top dead centre (ATDC), peak heat release rate (HRRpeak), location of HRRpeak at crank angle ATDC, ignition delay and combustion duration are depicted in Fig. 2.The plots in Fig. 2 show the highly nonlinear characteristic of the sample data. Although several data (e.g. BTE and Ppeak) are quite easy to model as they are either directly or inversely proportional to the engine speed and torque, a certain amount of the rest suffer from the problem of data exponentiality (i.e., the condition of being exponential). For instance, the amount of PM concentration increases exponentially along the engine speed and torque while the amount of BSNOx decreases exponentially as the engine speed increases. It has to be noticed that the machine learning techniques only try to determine the nonlinear interpolation between two data points. When the sample data set is scarce, this nonlinear interpolation may easily be wrongly constructed. This phenomenon becomes more obvious and deteriorates the model predictability seriously if the data exponentiality appears at the same time, which is the case of this study.In the current literature, Tchistiakov et al. [44] already faced the data scarcity problem in their time-series modelling work. Wong et al. [45] also encountered a similar data scarcity problem in their data-driven modelling work. Interestingly, both of them solved their data scarcity problem by removing the unrepresentative adjustable variables using different methods, such as wavelet transform and kernel principal component analysis, so that the dimension of the data set could be reduced. However, as indicated in Section 3 that the adjustable variables of the test diesel engine are already very few, and the problem domain in this study is that the number of data points for each dependent variable (i.e., each engine performance output) is not large enough for the modelling techniques to determine the nonlinear interpolation. Hence, the traditional dimension reduction methods are not applicable. Furthermore, the problem of data exponentiality did not appear in their studies either. As a result, this paper attempts to apply another data pre-processing technique, logarithmic transformation of dependent variables, to alleviate the data scarcity and data exponentiality simultaneously.With the logarithmic transformation, the range and the variances between the data points of each dependent variable can be reduced. The two most common bases of logarithm, log10 and natural log (ln), are adopted in this study. To demonstrate the use of the logarithmic transformation, the data of PM concentration are taken as an illustrative example because PM is the worst case of data exponentiality in the data sets. Fig. 3compares the characteristic of the PM concentration data before and after taking the logarithmic transformation.Obviously, after using the transformation (either log10 or ln), the interpolation between two data points becomes smoother and more linear than before (comparing Fig. 3(a) with Fig. 3(b) and (c)). Hence, it should be easier to be modelled and predicted.The use of the logarithmic transformation for data processing is relatively simple. Before training the model, all the dependent variables of the sample data are pre-processed by taking ln (or log10). After the model predicts the outputs, the values are post-processed by using exponential function (or anti-log10). The post-processed results are then considered as the actual output results of the prediction model. Besides, this type of data transformation is suitable for this study only because all the values in the sample data are positive. For other applications involving both positive and negative data, reciprocal or power transformation is recommended.Before training the models, each input and output value, say v, in the data sets was normalized within the range [−1, +1] by using Eq. (1) in order to increase the model accuracy and prevent any parameter from dominating the output values [46].(1)N(v)=v*=2(v−vmin)vmax−vmin−1where v*, vmax and vmin are the normalized parameter, the upper limit of the input/output parameter before normalization and the lower limit of the input/output parameter before normalization, respectively.Moreover, all the values are normalized in model training, so the output values predicted by the models need to be de-normalized using the inverse of Eq. (1). Finally, since the training data of the dependent variables are pre-processed by the logarithmic transformation, the predicted output values also need to go through the inverse of logarithmic transformation after de-normalization in order to obtain the actual output values, as indicated in Section 4.2.As the data size of this study is small, the number of test cases is not large enough to demonstrate the performance of the modelling methods. K-Fold cross-validation was therefore adopted in this study. Regarding the selection for the number of fold, K, it has been proved in the work of Kohavi [47] that there is no any universal standard for choosing this number. Since the number of sample data sets is 24 in this study, to evenly divide this number, the suitable numbers of folds are 3, 4, 6 and 8. Nevertheless, if the number of fold is 3 or 4, the training data size will be too small for model training, while if the number of folds is 8, only 3 data sets are available in the test group, which is too less for model verification. As a result, six-fold cross-validation was employed because 6 is the most reasonable fold number for the current study.To use six-fold cross-validation, the 24 sample data sets of this study were uniformly divided into six groups, resulting in 4 data sets per group with at least one data set being randomly selected from each speed. Five out of the six groups were then used to train the model, while the remaining one group was used as the test data. This process was repeated for six times, with each group being used exactly once as the test data. In each of the six folds, the training time was logged and the model accuracy indices were calculated. The results from the six folds were finally combined and averaged to show how well and how accurately the models could perform.To summarize the whole procedure of diesel engine modelling, including the data pre-processing technique and six-fold cross-validation, a flowchart is provided in Fig. 4showing clearly how the data was processed.Before training the diesel engine models, some modelling parameters must be defined in advance. Some common parameters for all the methods are the inputs, outputs, kernel function (for kernel based methods) and activation function (for ANN based methods). Apart from these parameters, each machine learning technique has its own parameters that need to be tuned, for instance, the number of hidden neurons for ELMbasic, and the kernel parameters of the kernel function for ELMkernel, LS-SVM and RVM. The procedures for the selection of these parameter values are discussed in the following sub-sections.The input parameters are actually engine dependent and usually those controllable parameters are selected. In this study, only four input parameters were chosen due to the limitation of the test engine. They are the engine speed, engine torque, AFR and fuel flow rate. Although the AFR and fuel flow rate could not be easily adjusted for the test engine of this study, it was included so as to demonstrate the generality of the model because these two factors are important factors affecting engine performance and emissions. For the output parameters, normally those parameters that are difficult and costly to be measured are selected (i.e., those parameters that the user wants the engine models to predict). In this study, ten different engine performance parameters were presented, including the BTE, BSCO, BSNOx, PM concentration, Ppeak, location of Ppeak at crank angle after top dead centre (ATDC), HRRpeak, location of HRRpeak at crank angle ATDC, ignition delay and combustion duration.(2)i.e.,x=speedtorqueAFRfuel flow rate,y=BTEBSCOBSNOxPMPpeaklocation ofPpeakat crank angle ATDCHRRpeaklocation of HRRpeakat crank angle ATDCignition delaycombustion durationFor the kernel function of LS-SVM, RVM and ELMkernel, and the activation function of ELMbasic, common functions like linear function, polynomial function, multi-quadratic function, and Gaussian radial basis function (RBF) were tried. It was found that RBF performed the best among these common functions, so RBF was selected.According to the theory of ELMbasic[30], only the number of hidden neurons is necessary to be defined. Although zero error can be approximated for the training samples when the number of hidden neurons is equal to the number of training samples, overfitting may occur. To avoid this, a validation scheme, namely leave-one-out cross-validation (LOOCV), is used for the selection of hidden neuron number.On the other hand, in the algorithm of LS-SVM and ELMkernel, two hyperparameters (regularization factor and basis width parameter) are necessary. To select the best values for these hyperparameters, LOOCV or 10-fold cross-validation is usually applied [36]. However, unlike ELMbasic where the number of hidden neurons is an integer, the hyperparameters can be any positive real number. Thus, if LOOCV is used to choose the hyperparameter, the results may be limited to the trial values provided by the user. More importantly, the best values may not be covered too. Previously, Bayesian inference has been applied to LS-SVM to estimate its hyperparameters [31,48], so purely Bayesian inference is adopted for LS-SVM. However, the framework of Bayesian inference on ELMkernel has not been explored yet. The theory of using Bayesian inference to predict the hyperparameter values for ELMkernel should be similar to that for LS-SVM. Nevertheless, it was found that the optimization step in Bayesian inference easily falls into local minima. In order to select a suitable pair of values, this study proposes a new procedure, where LOOCV is first used to select the initial value of the hyperparameters (i.e., large scope search), and Bayesian inference is then applied to determine the most appropriate values (i.e., local meticulous search). This hybrid inference procedure, as discussed in the following sub-section (Section 5.2.3), is adopted for ELMkernel.For RVM, only the basis width value of the kernel needs be defined. Since there is only one parameter, LOOCV was used for tuning this value again.RBFNN requires the user to provide the values of two parameters, namely the number of neurons and the spread parameter. In this study, the neurons are automatically added to the network iteratively until the sum-squared error falls beneath the error goal. Therefore, error goal and the spread parameter were required instead. The error goal was set to be mean squared error of 0.001 to avoid overfitting problem, and the spread parameter, being the only one adjustable parameter left, was determined using LOOCV.Lastly, although the structure of the BPNN model strongly affects the modelling results, the optimal settings cannot be easily determined even with the use of computer-aided optimization method. Therefore, in order to highlight the advantages and superiority of the other modelling methods, as well as to make a reliable comparison among all the modelling techniques, the latest optimal structure proposed by Mohamed Ismail et al. [21] was chosen, which is listed in Table 4.The algorithms for all the modelling methods and their parameter selection procedures were implemented using MATLAB R2012a and executed under Windows 7 on a computer with Intel Core i7 processor (3.4GHz with 8MB L3 cache) and 8GB RAM onboard.The optimization problem that ELMkernel deals with in its algorithm can be formulated as [36]:(3)Minimize:12||β||2+C12∑t=1ntθi2Subject to:θi=yi−h(xi)β,i=1,…,ntwhereβis the weight vector, θiis the training error,xiis the input vector, yiis the target output, ntis the number of training samples, C is the regularization factor, andh(·) is the nonlinear mapping but never calculated explicitly in kernel based ELM.The cost function can be modified to the form shown in Eq. (4):(4)Minimize:F1=μEW+∑i=1ntζiED,iwhere(5)EW=12||β||2,ED,i=12θi2=12(ti−h(xi)β)2,i=1,…,ntIn Eqs. (4) and (5), EWand EDrepresent the regularization term and error term, so μ is now the regularization factor instead of C, and ζiis the variance of noise for θi. In practice, constant variance is usually assumed, i.e., ζi=ζ. With this assumption, the relationship between μ and ζ becomes C=ζ/μ.Eq. (4) is actually equivalent to the cost function of LS-SVM [48]. Therefore, the theory of applying Bayesian inference approach on ELMkernel also becomes the same as that for LS-SVM based on this equation. However, the use of LOOCV for the initial value determination leads to a slightly different procedure. The procedure of the proposed hybrid inference is summarized as follows:1.Normalize the inputs and outputs to zero mean and unit variance.Denoting Dtrain as the training data set, and MELM as the ELM model, with RBF being the kernel function, optimize the basis width (σi) by maximizing the posterior in Eq. (6):(6)P(Dtrain|MELM)∝μMPneffζMPnt−1(Ceff−1)(nt−Ceff)∏i=1neff(μMP+ζMPλG,i)where μMP, ζMP, Ceff, λG,iand neff are inferred in the following steps.Initialize C and σiusing LOOCV. (Note: In the traditional Bayesian inference, these values are defined by the user or generated randomly.)Solve for the non-zero eigenvalues λG,ifrom the eigenvalue problem:(7)(MΩMT)vG,i=λG,ivG,i,i=1,…,neff≤nt−1whereM=(Int−(1/nt)1v1vT), Ω is the kernel matrix prepared by σi, and vG,iis the eigenvectors. neff is also obtained here as the number of λG,i.Optimize C by minimizing the cost functionF2as given in Eqs. (8) and (9):(8)Minimize:F2=∑i=1nt−1logλG,i+1C+(nt−1)logF1*(9)F1*=12(y−mˆy1v)TVGDG+1CIneff−1VGT(y−mˆy1v)with λG,i=0 for i>neff, andF1*is an alternative expression forF1given in Eq. (4).The optimization of Eqs. (8) and (9) can be easily done by using any gradient descent method. The gradient is given as:(10)∂F2∂C=∑i=1nt−11λG,iC2+C+(nt−1)1F1*∂F1*∂C(11)∂F1*∂C=12C2(y−mˆy1v)TVGDG+1CIneff−2VGT(y−mˆy1v)With CMP being the optimized C, calculate μMP and ζMP using Eqs. (12) and (13):(12)μMP=nt−12F1(13)ζMP=CMPμMPWith μMP, ζMP and CMP, calculate Ceff from Eq. (14):(14)Ceff=1+∑i=1neffCMPλG,i1+CMPλG,iAfter μMP, ζMP, Ceff, λG,iand neff are obtained, substitute these variables to Eq. (6) and calculate the posterior.Repeat steps 3–9 until an optimized σiis obtained. The optimized σiand its corresponding CMP are the inferred hyperparameters for ELMkernel.To evaluate the modelling methods, the model prediction accuracy, the time complexity and the space complexity were employed as the performance indices.The model prediction accuracy is expressed by the root mean square error (RMSE), which is calculated against the experimental data sets using Eq. (15):(15)RMSE=1nd∑k=1nd(yk−Yk)2where Ykis the kth model prediction value that has passed through all the data processing stage, ykis the actual experimental value (i.e., desired value), and ndis the number of data points. In general, the smaller the error, the better the model prediction is.The time complexity is measured by the training time of the models, while the space complexity is presented by the number of variables in the computer memory required by the models (i.e., model size). Since the formulation of the modelling methods varies from each other, there is no unique method to count the number of variables. Three functions are therefore designed for the evaluation of the model size.The first function is designed for ELMbasic, RBFNN and BPNN. As mentioned in Section 5.2 that there is only one hidden layer in the structure of these methods, the formulation can then be presented by Eq. (16):(16)Yk=∑i=1nhβi,kh(wi,bi,X),k=1,…,nowhere Ykis one of the model prediction outputs corresponding to a new input dataX(i.e., unseen input), nhis the number of hidden nodes, βi,kis the output weight corresponding to Yi, h(·) is the activation function,wiis the input weight, biis the bias of the activation function, and nois the number of output variables.For this formulation, denoting niand nhas the number of input variables and number of hidden nodes respectively, there are nh×niinput weights, nhhidden node parameters (e.g., bias or basis width), and nhoutput weights. Under the first situation (single dimensional output), there are totally nomodels, so the evaluation function of the model size for ELMbasic, RBFNN and BPNN is given by:(17a)sizeELMbasicRBFNNBPNN(ni,nh,no)=(nh×ni+nh+nh)×no.Under the second situation (multi-dimensional output), there is only one model with nh×nooutput weights, so the evaluation function is:(17b)sizeELMbasicRBFNNBPNN(ni,nh,no)=nh×ni+nh+nh×no.The second function is designed for LS-SVM and RVM. With RBF as the kernel function, the formulation is:(18)Yk=∑i=1nsvαi,kexp−||X−xi||2σi,k2+bk,k=1,…,nowhere αi,kis the support value, bkis the bias,xiis the input vector of the training samples, σi,kis the basis width, and nsv is the number of support values which usually equals to the number of training samples for LS-SVM and RVM.LS-SVM and RVM can only provide single dimensional output, so only the first situation is considered. Moreover, in practice, usually one basis width is set for all nsv kernels (i.e., σi,k=σk). As a result, there are nsv×nisupport inputs, nsv support values, one basis width and one bias in one model. For noindividual models, the function of the model size for LS-SVM and RVM is obtained as:(19)sizeLS-SVMRVM(ni,nsv,no)=(nsv×ni+nsv+2)×no.The last function is designed for ELMkernel. The formulation of ELMkernel is simply a combination of Eqs. (16) and (18), which is shown as follows:(20)Yk=∑i=1ntβi,kexp−||X−xi||2σi2,k=1,…,nowhere ntis the number of training samples.By setting a common value of basis width (i.e., σi=σ), there are nt×nitraining inputs, one basis width, and ntoutput weights under the first situation. Hence, for nomodels, the resulting function of the model size is:(21a)sizeELMkernel(ni,nt,no)=(nt×ni+1+nt)×no,Similar to ELMbasic, for multi-dimensional output, there is one model with nt×nooutput weights, so the evaluation function is:(21b)sizeELMkernel(ni,nt,no)=nt×ni+1+nt×no.

@&#CONCLUSIONS@&#
In this research, ELM, LS-SVM and RVM-based models have been successfully constructed to predict the performance and emission characteristics of a diesel engine. They are new but non-straight forward applications. An experiment was setup to collect sample experimental data for model construction and verification. Although the sample data in this study suffered from the two problems of data scarcity and data exponentiality, a data pre-processing technique, logarithmic transformation of dependent variables, is applied to solve these problems simultaneously. Six-fold cross-validation is also adopted to increase the credibility of the modelling results. Evaluation results show that the logarithmic transformation technique can significantly improve the modelling prediction. It is believed that the model accuracies can be further improved with a larger data size and by deeply investigation on the data pre-processing method.One of the original work in this study is the integration of LOOCV and Bayesian inference on the selection of hyperparameters for ELMkernel. The limitations of using each method alone are solved by combining them together. Experimental results show that the accuracy is improved by 23.78% as compared to purely LOOCV, and 47.57% as compared to purely Bayesian inference. Another attractive work of this study is the comprehensive comparison among the ELMbasic, ELMkernel, LS-SVM, RVM, RBFNN, and BPNN under the same sample data sets. The comparative results show that the training time of ELMkernel with the logarithmic transformation and hybrid inference is 4, 28, 216, 488 and 1914 times faster, while its RMSEtest is 55.57%, 45.10%, 57.30%, 57.78% and 68.58%, better than those of ELMbasic, LS-SVM, RVM, RBFNN and BPNN, with the logarithmic transformation respectively. This indicates that ELMkernel with the logarithmic transformation and hybrid inference is superior to LS-SVM, RVM, RBFNN and BPNN with or without the logarithmic transformation for the sample data in this study. Therefore, it can be concluded that ELMkernel with the logarithmic transformation and hybrid inference is a promising technique for this application, and similar applications under such scarce and exponential data sets.Based on the diesel engine models developed by the advanced machine learning techniques, the optimal setup for diesel engines will be determined in the future using some advanced computer-aided optimization approaches such as genetic algorithms and particle swarm optimization. Moreover, as the models can be used as virtual sensors for controlling and monitoring the engine parameters, an online model will be further constructed for real-time engine control. In addition, it is recommended to apply ELMkernel with the logarithmic transformation and hybrid inference to other modelling applications too.