@&#MAIN-TITLE@&#
Classification of major construction materials in construction environments using ensemble classifiers

@&#HIGHLIGHTS@&#
This study focused on the automatic detection of construction materials in images.We investigated the performance of six single classifiers and ensemble classifiers.We tested the classifiers on three data sets: one each for concrete, steel, and wood.The ensemble classifiers performed better than the single classifiers overall.The ensemble classifier is able to enhance the detection of construction materials.

@&#KEYPHRASES@&#
Ensemble classifier,Color,Construction material detection,Data mining techniques,Image processing,

@&#ABSTRACT@&#
The automatic detection of construction materials in images acquired on a construction site has been regarded as a critical topic. Recently, several data mining techniques have been used as a way to solve the problem of detecting construction materials. These studies have applied single classifiers to detect construction materials—and distinguish them from the background—by using color as a feature. Recent studies suggest that combining multiple classifiers (into what is called a heterogeneous ensemble classifier) would show better performance than using a single classifier. However, the performance of ensemble classifiers in construction material detection is not fully understood. In this study, we investigated the performance of six single classifiers and potential ensemble classifiers on three data sets: one each for concrete, steel, and wood. A heterogeneous voting-based ensemble classifier was created by selecting base classifiers which are diverse and accurate; their prediction probabilities for each target class were averaged to yield a final decision for that class. In comparison with the single classifiers, the ensemble classifiers performed better in the three data sets overall. This suggests that it is better to use an ensemble classifier to enhance the detection of construction materials in images acquired on a construction site.

@&#INTRODUCTION@&#
The automatic detection of construction materials in images acquired on a construction site is essential for a wide range of construction applications, from the generation of a 3D as-built model to progress monitoring (see, for example, [1–5]). With the rapid deployment of image sensors on construction sites, images containing valuable project information are readily available. However, material detection in construction images is non-trivial and difficult. Construction materials in construction images may appear cluttered, occluded, or articulated, and the shapes and positions of construction materials are unpredictable.Color has been recognized as an efficient feature for distinction of a material of interest from the background. Color has obvious advantages over other features such as texture and shape, especially in complex environments, as it is independent of the shapes and positions of the objects [6–8]. In addition, it is simple and computationally efficient to implement because it requires only the color values of each pixel in an image. It is therefore expected that material detection using color would be more robust and accurate and would overcome problems associated with construction environments compared to other features such as texture and shape.In past research, color distribution has been used in efforts to detect construction materials in images acquired on a construction site. Neto et al. [9] proposed a method that employs edge detection and color to identify construction materials in images. In their approach, an edge detector algorithm detects edge pixels that belong to construction materials by comparing the RGB values in such pixels with the predetermined RGB values of the construction materials. After the edges have been detected, it groups the interior pixels to each set of edge pixels as an object. At the end of the operation, both of the resulting linked lists (the one for edges and the one for internal pixels) are stored under an object name. Zou and Kim [10] suggested a color-based method for identification of hydraulic excavators on a construction site. They use the hue feature to separate hydraulic excavators of different colors, and the saturation feature to differentiate each excavator of interest from its background, which consists of dark-colored soil and white snow. Simple thresholding methods using the hue and saturation features in conjunction with a method of calculating object centroid coordinates enable their system to produce accurate determinations of excavator idle time and working rate. The changing centroid coordinates of an excavator in successive images taken at constant time intervals are used as indicators of movement. Son and Kim [11] proposed an automated structural component recognition method that employs color and 3D data acquired from a stereo vision system for use in construction progress monitoring. The data processing first relies on color features to effectively extract information on structural components by employing color invariance, 2D object segmentation, and two-stage post-processing of removing unnecessary noise unrelated to the structure of interest and supplementing the image with information that may have been unintentionally eliminated. That information is then utilized to extract 3D coordinates for each color feature. The color image is used to guide the detection of features, while the 3D data are used to compensate for the pose of the feature.In recent years, data mining methods such as artificial neural networks (ANNs), Gaussian mixture models (GMMs), and support vector machines (SVMs) have been investigated as a way to detect construction materials in images by use of a color model. Zhu and Brilakis [12] applied ANNs to classify regions of concrete in images acquired on a construction site. The images were first divided into regions through image segmentation using color. Then the color and texture features of each region were calculated, and the regions were classified using a pre-trained ANNs classifier. Son et al. [5] performed a comparative analysis of three data mining algorithms (GMMs, ANNs, and SVMs) for detection of concrete regions in images acquired on construction sites. The results show that the accuracy of the SVM they employed is better than that of the GMM or the ANN in dealing with concrete detection.In previous studies, single classifiers have been employed to detect construction materials—and distinguish them from the background—by using color as a feature. However, a single classifier might not produce the optimal result in construction environments in which the color of one construction material is similar to that of others around it, or the inherent color property of a construction material is altered because of the effects of variation in illumination. For these reasons, the detection of construction materials still remains a challenging problem and there is still room for further improvement of detection performance. In order to solve such complex classification problems, heterogeneous ensemble classifiers (combined into a so-called multiple classifier system) have been proposed and they have been shown to be considerably successful in highly complex domains compared to ones with individual classifiers [13–17].An ensemble classifier is comprised of a set of individual classifiers whose predictions are combined to obtain a highly accurate classification. Systems of this type have been proposed as a way to achieve better classification performance than with a single classifier [15,17] and are expected to reduce the variance in the estimation errors made by the individual classifiers [18,19]. The effectiveness of ensemble classifiers for detection of materials in complex environments has been demonstrated in various fields (see, for example, [20–24]). To the knowledge of the authors, the applicability of ensemble classifiers to the detection of construction materials has not been explored thus far.The aim of this study was to improve the accuracy of detection of major construction materials such as concrete, steel, and wood by using ensemble classifiers. It was hypothesized that ensemble classifiers achieve higher accuracy than single classifiers in detecting construction materials in construction environments. This belief is based on the general expectation that ensemble classifiers can outperform individual classifiers [17]. The rest of the paper is organized as follows. Section 2 describes data collection and pre-processing. Section 3 describes the methods employed by the single classifiers and the scheme employed in the proposed ensemble classifier. In Section 4, the results of experiments on the performance of the proposed ensemble classifier are compared with that of single classifiers in terms of average prediction accuracy. Finally, the conclusions are presented in Section 5.Without a comprehensive data set, it cannot be concluded that an ensemble classifier yields better accuracy than single classifiers in detecting construction materials in construction environment. Because comprehensive data sets for construction material detection were not readily available, a total of three data sets (one each for concrete, steel, and wood) were generated.The appearance of a construction material’s surface colors can be affected by environmental factors such as changes in the direction and intensity of illumination. Since most construction sites are outdoors, the intensity of illumination varies unpredictably and uncontrollably, depending on the time of day, seasonal variations, and weather conditions (sunny, cloudy, or foggy), thereby resulting in large variations in the appearance of a construction material’s surface colors. To account for such variations, 108 photographs were taken at a total of 50 construction sites for concrete detection, 91 photographs were taken at a total of 80 construction sites for steel detection, and 50 photographs were taken at a total of 14 construction sites for wood detection. Fig. 1(a), (c), and (e) present examples of construction site images for concrete, steel, and wood detection. Digital cameras with resolutions ranging from 3 megapixels to 12 megapixels were used when collecting data. The photographs were intended to contain either of the three structural components (concrete, steel, and wood) in images of actual construction-site scenes in order to validate the effectiveness and robustness of the proposed method for use in applications such as the generation of 3D as-built models and progress monitoring. Therefore, the photographs were taken at a distance from structures so that images contained the entire structures.Each photograph was then divided into sub-regions of either 25×25 pixels or 50×50 pixels. Each sub-region was categorized and labeled as either a material of interest or the background, or as unable to say whether the sub-region was the material of interest or the background. Fig. 1(b), (d), and (f) display examples of the sub-regions for concrete, steel, and wood data sets. For example, the first, second, and third rows in Fig. 1(b) show the examples of sub-regions that were labeled as the concrete, the background, and unable to say whether the sub-region was the concrete or the background. Of the sub-regions, 48.2% from the concrete data set, 22.8% from the steel data set, and 49.0% from the wood data set were categorized as unable to say whether the sub-region was the material of interest or the background. These sub-regions were then excluded from the data set. As a result, every data set consisted of material or non-material pixels. The former are pixels associated with objects made of materials such as concrete, steel, and wood, while the latter are pixels related to the background. To assess whether the heterogeneous ensemble classifiers perform better than single classifiers, this study made a particular effort to collect and include as many materials as possible with color properties similar to those of the materials of interest. In each data set, the background included all kinds of scenery—bricks, construction equipment, fences, forms, pipes, safety nets, the sky, soil, traffic signs, trees, windows, and other construction-related materials. For example, the background of the concrete data set included objects made of materials such as steel and wood in order to evaluate results in the presence of different construction materials.In total, the data collected from the concrete, steel, and wood sub-regions and their background sub-regions amounted to over 113million pixels for concrete detection, 95million pixels for steel detection, and 35million pixels for wood detection. The first data set contained approximately 44million pixels of concrete and 69million pixels of background. The second data set consisted of 9million pixels of steel and approximately 85million pixels of background. The third data set had 10million pixels of wood and approximately 25million pixels of background. The characteristics of the three data sets are provided in Table 1. In summary, the data sets were well balanced in terms of time of day, season, and weather. The percentages of data collected during a.m. and p.m. hours were roughly comparable. There was greater variation in the percentages of data collected in different seasons, as well as in the percentages of data collected and under different weather conditions, as shown in Table 1.One issue that must be carefully considered in order to take full advantage of color-based material detection is how to effectively reduce the effect that variations in illumination which occur in outdoor environments will have on material color. The values of colors in the RGB (red, green, and blue) color space, the most prevalent choice for computer graphics, are particularly subject to deterioration as a result of changes in illumination [25]. Such variations caused by factors in outdoor environments may dramatically affect color properties, potentially impacting detection performance [26]. To deal with these potential artifacts, it is important to represent color in a way that minimizes the effect of variations in illumination. In this study, in order to represent color in an appropriate form, the color values in the RGB color space were converted to the HSI color space, which separates chrominance and luminance information. The HSI color space consists of three components: hue (H), saturation (S), and intensity (I). Hue and saturation are related to color, or chromaticity, and are illumination-independent components. These three components were used as input features to the classifiers. The HSI color space is defined as follows [27]:(1)H=arctan3(G-B)(R-G)+(R-B)(2)S=1-min(R,G,B)I(3)I=(R+G+B)3We considered six different types of classifiers: a support vector machine (SVM), an artificial neural network (ANN), commercial version 4.5 (C4.5), naïve Bayes (NB), logistic regression (LR), and k-nearest neighbors (KNN). In all of our experiments, we employed the algorithms from WEKA release 3.6.7 [28], which is a Java-based machine learning tool. Default parameter values in WEKA release 3.6.7 were used in these six classifiers. What follows is a brief description of these six classifiers.The SVM, inspired by statistical-learning theory, is one of the most powerful machine-learning techniques for solving a large number of complex binary classification problems [29]. It acts as a linear classifier in a high-dimensional feature space transformed through a projection from the original input feature space by using non-linear kernel functions of the original data set [30]. The high-dimensional space could theoretically be infinite, in which case linear discrimination is almost possible. In general, however, the resulting classifier is non-linear in the input space. An SVM learns the data in the high-dimensional feature space and achieves good generalization performance by finding a hyperplane that maximizes the margin between the classes with the help of multiplier parameters, for example, Lagrange multipliers [31]. For further details, see Vapnik [32] and Vladimir and Vapnik [33]. In this study, the radial basis function (RBF) was selected as the kernel function. With the RBF kernel function, the parameters C and γ were set to 1 and 0, respectively.In this study, a back-propagation (BP) neural network is used for classification. This is a feed-forward network that can have one or more hidden layers. It utilizes an iterative gradient-search technique designed to minimize the mean-square error between the actual and desired net outputs. The units in the hidden layers sum their inputs, add a constant, and take a fixed function of the result [34]. The output units are of the same form but with an output function. A three-layer network with one hidden layer was proven to be capable of computing any continuous likelihood function required in a classifier and of solving complex binary classification problems [35,36]. In this study, a three-layer network with one hidden layer was used for experimentation. It has been shown that a multiple hidden-layer neural network can approximate any function [37]. The number of nodes for the hidden layer was set to 3. The sigmoid transfer function was selected as the activation function. The learning rate, which determines the amount of weights that are updated, was set to 0.3; the momentum that was applied to the weights during updating was set to 0.2; and the training time, which indicates the number of epochs through which to train, was set to 500.C4.5 is a supervised-learning algorithm that is the advanced version of the decision tree algorithm ID3 [38]. It is used to generate a set of rules from the data. As such, it employs a divide-and-conquer approach [39,40]. C4.5 works in three main steps. First, the node at the top of the tree (the “root node”) considers all samples and passes information about them to the “branch nodes” (the nodes that have at least one node of the tree below them). The branch nodes generate rules for a group of samples based on an entropy measure. At this stage of the process, C4.5 constructs a tree by considering all attribute values and finalizes the decision rule by pruning. It uses a heuristic approach for pruning based on the statistical significance of splits. After fixing the best rule, the branch nodes send the final target value to the lowest node on each branch, which is called a “leaf node” [38,41]. In this study, a confidence factor, which is used for pruning, was set to 0.25, and the minimum number of instances per leaf node was set to 2.The NB classifier is a simple linear classifier that is based on the classical statistical “Bayes theorem.” The term ‘‘naïve’’ refers to the fact that it calculates the maximum posterior probability based on the assumption that the attributes of the training samples are independent and that there are no hidden or latent attributes which influence the prediction procedures [42]. This classifier is based on theoretically well-founded mathematical models used to predict an unseen case given a training sample [43]. The NB classifier will assign the most likely class to a given example as described by its feature vector. It assumes that the decision problem is posed in probabilistic terms and that all of the relevant probability values are known. This type of classifier is simple to implement, as neither numerical optimization nor matrix algebra is required for its use. NB is also efficient to train and use and is easy to update with new data.The LR is a generalization of linear regression [44]. It is used primarily to predict binary or multi-class dependent variables. Because the response variable is discrete, it cannot be modeled directly by linear regression. Therefore, rather than predicting a point estimate of the event itself, it builds a model to predict the odds of its occurrence. In a two-class problem, odds greater than 50% would mean that the case is assigned to the class designated as 1; otherwise, it is assigned to the class designated as 0. Although logistic regression is a powerful modeling tool, it assumes that the response variable (which is the logarithm of the odds, not the odds per se) is linear in the coefficients of the predictor variables. Furthermore, the modeler, based on his or her experience with the data and data analysis, must choose the right inputs and specify their functional relationship to the response variable.KNN is an instance-based learning approach. It finds the k-nearest neighbors and uses a majority vote to determine the class label [45]. The basic principle of this algorithm is that each as-yet-unseen instance is always compared with existing instances using a distance metric. Based on the distance metric, the closest existing instance is used to assign the class for the test sample [28]. The most straightforward approach is to assign the majority class among the nearest neighbors to the query [46]. The advantage of the KNN is its robustness when using noisy training data sets [41,47]. The major drawback of the KNN algorithm is the high computational cost, as it has to compute the distance of every query instance for all the training data. In this study, Euclidean distance was used as the distance metric, and k, which indicates the number of neighbors to use, was set to 1.In this study, an ensemble classifier used a voting scheme which internally makes use of several single (base) classifiers and combines them in an effort to produce better results. Next, a determination was made of the single classifiers to choose for incorporation into a proposed ensemble classifier. The motivation for using a voting-based ensemble classifier was to take advantage of information outputs generated by a number of different base classifiers, instead of relying on the output of a single classifier alone [48]. The output generated by the ensemble classifier was defined as the average of the prediction probabilities generated by the base classifiers. This was done because there is a consensus in the literature that the classification performance of this type of heterogeneous ensemble classifier is better than that of even the best-performing individual classifier, and that biased decisions can be avoided more easily as a result [49,50].When we design heterogeneous ensemble classifiers, we must select the subset of classifiers that can be combined to achieve better accuracy [51]. It is easy to see that such an optimal subset could be obtained by exhaustive enumeration, that is, by assessing on a validation set the classification accuracy provided by all possible subsets, and then choosing the subset exhibiting the best performance. In our case, there are six single classifiers; thus, the number of possible subsets is equal to 57. If the number of classifiers increases, the number of possible subsets also increases. Unfortunately, the evaluation of all possible combinations of base classifiers is impractical and requires high computational cost. Therefore, different strategies have been proposed in order to limit the computational complexity of the selection process [51]. Accordingly, measures for evaluating the base classifiers forming an ensemble classifier have been proposed for classifier selection purposes [52].Generally speaking, to have an effective ensemble classifier that is more accurate than any of its base classifiers, one needs base classifiers which are diverse (in the sense that they predict differently) but accurate [23,53–56]. In other words, heterogeneous ensemble classifiers are more effective when the base classifiers are diverse and have acceptable individual performances, and consequently are more likely to have a more robust generalization performance in terms of the principle of bias–variance tradeoff [57,58]. If individual base classifiers are rather inaccurate, their combination is meaningless and might not produce an accurate ensemble classifier. On the other hand, if there is no diversity among the classifiers, (that is, if the base classifiers all produce essentially the same outputs), then their combination will not significantly contribute to improved performance but will serve mainly to increase the complexity of the process.It is rather difficult to achieve both high diversity and accuracy in base classifiers, since high diversity means substantial disagreement among the outputs of the base classifiers, and high accuracy indicates significant agreement among the outputs of the base classifiers [54,56,59]. The two key ingredients to achieve an effective ensemble classifier are thus in contradiction with each other. Therefore, it is crucial to select base classifiers with a good tradeoff between the two characteristics. In this study, the weighted count of errors and correct results (WCEC) was employed for this purpose. The WCEC is one of the commonly employed measures of diversity of errors.The weighted count of errors and correct results (WCEC) is one of a number of measures of diversity of errors that are in use in assessing the choice of base classifiers to be used in an ensemble classifier [52]. The measures of diversity of errors emphasize that differences within the errors made by the member classifiers truly affect performance [60,52]. In the previous literature (see, for example, [52,61]), the WCEC measure was proven to be effective in assessing the choice of base classifiers to be used in an ensemble classifier. The WCEC measure not only gives a measure of the correctness of classifiers but also considers the diversity between two classifiers [61]. By using the WCEC, we were able to select base classifiers that are both diverse as well as accurate from among a number of competing classifiers.The main concept of the WCEC is that adopting an incorrect classifier generally has a negative effect on the ensemble classifier, whereas adopting the correct classifier is beneficial. The agreements generally have a significantly greater positive impact on the ensemble classifier’s performance than the disagreements. Based on this concept, the resulting ensemble classifier often yields more accurate classification results than the single classifiers, because it aggregates the benefits of multiple classifiers [62].The WCEC takes information on both incorrect results and correct results into account, with more emphasis placed on cases where the classifiers agree on the result, regardless of whether that result is correct or incorrect. One can subdivide the occurrences of the different combinations of outputs of the base classifiers as follows [52]:(4)WCECa,b=N11+12(N01+N10)-Ndifferent00-5Nsame00,where N is the total number of instances, N11 is the number of instances in which both classifiers a and b are correct, N10 (resp. N01) is the number of instances in which just the first (resp. second) classifier is correct, and N00 is the number of instances in which both classifiers a and b are incorrect. Furthermore,Nsame00is the number of instances in which both classifiers a and b are incorrect and have the same output, andNdifferent00is the number of instances in which both classifiers a and b are incorrect but have different outputs. This study deals with a binary classification problem; thus, the value ofNdifferent00is zero. Thus N=N11+N10+N01+N00 andN00=Nsame00+Ndifferent00. According to Aksela and Laaksonen [52], who proposed the WCEC measure, the weighting is arbitrary, and the presented values were chosen based on their suitability to penalize errors, and especially the identical errors. This study adopted the weight values, as suggested by Aksela and Laaksonen [52].Using WCEC as the measure of diversity of errors in order to devise a diverse ensemble classifier, we trained six classifiers (SVM, ANN, C4.5, NB, LR, and KNN) on the three different data sets. Then the WCEC was computed for every combination of two to six of the classifiers for each of the three data sets. For each combination of just two classifiers, the pairwise values were used as inputs to the WCEC. For combinations of three or more classifiers, the mean of all the pairwise values of the classifiers were used as inputs to the WCEC. Then the combination of classifiers that had the maximum value of the WCEC was chosen for the ensemble classifier.The use of an ensemble classifier based on the voting scheme starts with training of multiple base classifiers (so-called member classifiers). The information from multiple base classifiers is then combined to produce a final decision. Here, the information from the base classifiers included their predictions (binary decisions) for each instance and their prediction probabilities for each target class. In the case of the concrete data set, for example, the information from the base classifiers included a prediction probability that an object is made of concrete a prediction probability that it is not made of concrete, and a decision on whether it is or is not made of concrete. The prediction probabilities convey not only prediction information but also confidence information [63]. Once the base classifiers were selected, based on the measure of the weighted count of errors and correct results, they were combined to produce a final decision (prediction) for each instance and prediction probabilities for each target class.The average of the prediction probabilities for the different base classifiers was used as the prediction probability for the overall classification result. Use of the average of prediction probabilities, rather than the average of binary decisions, is known to have improved the performance of some ensemble classifiers [64]. In addition, it is a simple, low-cost, but effective approach that has been shown to be quite successful in various classification problems [65–67].Let D={D1,…,DL} denote the ensemble classifier, where L is the number of base classifiers of which it is composed. Let Ω={ω1,ω2} be the set of class labels. The individual outputs are estimates of the posterior probabilities; that is, the output di,j(x) of classifier Di, in support of the hypothesis that an instance x comes from class ωj, is an estimate of P(ωj|x), i=1,…,L; j=1,2 [60]. In this study, the conditional probability μj(x) that x is in class j was defined as the average of the posterior probabilities output by the base classifiers:(5)μj(x)=1L∑i=1Ldi,j(x)Then instance x was assigned to the class with the larger of the values of μ1(x) and μ2(x).

@&#CONCLUSIONS@&#
