@&#MAIN-TITLE@&#
On detecting the playing/non-playing activity of musicians in symphonic music videos

@&#HIGHLIGHTS@&#
We propose a semi-automatic annotation system for large symphonic orchestras videos.We leverage video redundancy, image clustering, and human annotation.Our method successfully deals with several intra-class variability issues.Human annotation effort reduced while maintaining high level of output quality.Comprehensive analysis of the impact of different modules on the overall performance.

@&#KEYPHRASES@&#
Cross-modal analysis,Music information retrieval,Human-object interaction,Diarization,Clustering,

@&#ABSTRACT@&#
Information on whether a musician in a large symphonic orchestra plays her instrument at a given time stamp or not is valuable for a wide variety of applications aiming at mimicking and enriching the classical music concert experience on modern multimedia platforms. In this work, we propose a novel method for generating playing/non-playing labels per musician over time by efficiently and effectively combining an automatic analysis of the video recording of a symphonic concert and human annotation. In this way, we address the inherent deficiencies of traditional audio-only approaches in the case of large ensembles, as well as those of standard human action recognition methods based on visual models. The potential of our approach is demonstrated on two representative concert videos (about 7 hours of content) using a synchronized symbolic music score as ground truth. In order to identify the open challenges and the limitations of the proposed method, we carry out a detailed investigation of how different modules of the system affect the overall performance.

@&#INTRODUCTION@&#
Rapidly developing multimedia technology has opened up new possibilities for bringing the full symphonic music concert experience out of the concert hall and into people’s homes. New emerging platforms, like RCO Editions11http://www.concertgebouworkest.nl/en/rco-editions/and the Berliner Philharmoniker’s Digital Concert Hall22http://www.digitalconcerthall.com/are enriching audio-visual recordings of symphonic music performances to make them more informative and accessible offline, in a non-linear fashion and from multiple perspectives. Such platforms rely on the new generation of automatic music data analysis solutions. For instance, loudness and tempo can be estimated continuously over time and visualized as animations [8]. Notes can be detected and analyzed to reveal and visualize repeated parts of a piece [21]. Sheet music scores can be synchronized to the audio recording to allow users to follow the scores while listening to the music [2]. Furthermore, the sound produced by different instruments can be isolated via source separation [11], which could be deployed to zoom in on a particular instrument or instrumental section [12].While the solutions mentioned above primarily rely on an analysis of the audio channel of the performance recording, the visual channel has remained underexploited. In addition to enabling the development of new functionalities of platforms like RCO Editions and Berliner Philharmoniker’s Digital Concert Hall not covered by audio analysis, the analysis of the visual channel could also help to resolve some of the critical challenges faced by audio analysis. For instance, achieving reliable sound source separation is challenging in the case of large ensembles where the sound produced by many different instruments overlaps both in time and frequency [7].In this paper, we focus on the analysis of the visual channel of the audio-visual recording of a symphonic music performance and address the problem of annotating the activity of individual musicians with respect to whether they play their instruments at a given timestamp or not. The envisioned output of the solution we propose in this paper is illustrated in Fig. 1, where playing and non-playing musicians are isolated as indicated by respectively the green and red rectangles.Knowing the playing (P) and non-playing (NP) labels for each musician allows the annotations of an audio-visual recording to be enriched in a way that is complementary and supportive to audio-only analysis. For instance, repeats and solo parts could be detected also by analyzing the sequences of P/NP labels to allow novel non-linear browsing functionalities (e.g., skip to solo trumpets, skip to “tutti”). The problem of performance-to-score synchronization, which is typically addressed through audio-to-audio alignment [22], could also be approached in a multimodal fashion by combining state-of-the-art auditory features and P/NP labels [5].Related methods operating on the visual channel typically deploy a standard classification paradigm and learn visual models for human actions [28,39]. The disadvantage of this approach in the problem context of symphonic music concert videos is that the models may not be generic enough to cover the wide variety of instruments used and the ways the P/NP activities of individual musicians could be visually recorded. Additionally, a realistic view at the reliability of solving this classification problem reveals the need for manual human intervention in order to correct unavoidable classification errors, in particular in a professional context when high-quality annotation output is required.The method we propose in this paper is geared not only towards neutralizing the disadvantage mentioned above, but also towards incorporating human intervention in the way that is as efficient and effective as possible. We implement our proposed solution to assign P/NP labels per musician to the timeline of a symphonic music performance as a modular framework so that we can provide answers to the following research questions:•RQ1: How reliably can we isolate clusters of images depicting individual musicians from the keyframes extracted from a music video?RQ2: How accurately can sequences of P/NP labels be generated?RQ3: What is the tolerance of the proposed framework to errors in different modules?RQ4: Is a static image informative enough to reveal whether a musician is playing an instrument?RQ5: What is the relation between the amount of human intervention and the quality of the obtained P/NP label sequences?The paper is organized as follows. We start by explaining in Section 2 the context in which we operate in this paper and that characterizes the realization and recording of a typical symphonic music performance. By taking into account the properties of the work context and the related limitations, we proceed in Section 3 by analyzing the usability of the existing related work and in Section 4 by stating our novel contribution and explaining the rationale behind our proposed framework. We introduce the notation, set the goals and make assumptions in Section 5. We present our method in Section 6 elaborating on the realization of different framework modules. After we explain the experimental setup in Section 7, we present our assessment of the framework in Section 8 where we also provide answers to the research questions posed above. We conclude with a discussion section in which we also present future research directions (Section 9).A symphonic orchestra consists of a large number of musicians organized in sections (string, brass, woodwind or percussion). Sections are further divided into instrumental parts. Each instrumental part consists of a number of musicians playing one particular instrument and following a specific musical score. For instance, in Fig. 2the instrumental parts “Violino I” and “Violino II” play different notes even if the instrument is the same (violin). According to the scores, when one musician belonging to one instrumental part is (not) playing, all the other musicians performing the same instrumental part are expected to be (in-)active as well. This usually holds even in the divisi case33http://en.wikipedia.org/wiki/Unison#Divisi.Performance recordings may differ depending on several factors like, for instance, the type of environment (indoor vs. outdoor), the number of cameras and whether camera motion occurs. In this paper we focus on the indoor case and we consider two possible types of recording: single- and multiple-camera recordings. The former is made from a fixed point of view and with a fixed zoom factor. In this way, the whole ensemble is always visible and each musician covers the same region of the video frames throughout the video. The latter typically involves multiple-cameras positioned around and on the stage, with the possibility to zoom and pan. This type of recording typically serves as input to a team of experts in order to create an edited video using a script (e.g., “when the 100th bar of the scores starts, the 3rd camera switches to a close-up on the first clarinet player”). Thereby, the visual channel mainly focuses on (parts of) the orchestra, but can also show the conductor and the audience in the concert hall.Both in the single- and multiple-camera recordings, depending on the camera position, some musicians appear frontally, some non-frontally, and some even from the back, (fully) occluding their instruments. As illustrated in Fig. 3, the setting of the orchestra on the stage is rather dense, resulting in significant occlusion of individual musicians and their instruments. A video frame taken from the visual recording of the performance can therefore contain multiple musicians, not necessarily belonging to the same section or instrumental part.The characteristics of the context in which we operate, as described above, have significant impact on the extent to which we can rely on the existing related work in conceptually developing our proposed solution, but also on the way how we approach the definition and implementation of the modules of our framework. This will be explained in more detail in the following sections.

@&#CONCLUSIONS@&#
