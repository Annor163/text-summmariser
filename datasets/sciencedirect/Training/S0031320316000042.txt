@&#MAIN-TITLE@&#
Generalized mean for robust principal component analysis

@&#HIGHLIGHTS@&#
We propose a robust principal component analysis.The generalized mean is used in the proposed method instead of the arithmetic mean.A novel method is also presented to solve our optimization problem.

@&#KEYPHRASES@&#
Generalized mean,Principal component analysis,Robust PCA,Dimensionality reduction,

@&#ABSTRACT@&#
In this paper, we propose a robust principal component analysis (PCA) to overcome the problem that PCA is prone to outliers included in the training set. Different from the other alternatives which commonly replace L2-norm by other distance measures, the proposed method alleviates the negative effect of outliers using the characteristic of the generalized mean keeping the use of the Euclidean distance. The optimization problem based on the generalized mean is solved by a novel method. We also present a generalized sample mean, which is a generalization of the sample mean, to estimate a robust mean in the presence of outliers. The proposed method shows better or equivalent performance than the conventional PCAs in various problems such as face reconstruction, clustering, and object categorization.

@&#INTRODUCTION@&#
Dimensionality reduction [1] is a classical problem in pattern recognition and machine learning societies, and numerous methods have been proposed to reduce the data dimensionality. Principal component analysis (PCA) [2] is one of the most popular unsupervised dimensionality reduction methods which tries to find a subspace where the average reconstruction error of the training data is minimized. It is useful in representation of input data in a low dimensional space and it has been successfully applied to face recognition [3,4], visual tracking [5], clustering [6,7], and so on.When automatically collecting a large data set, outliers may be contained in the collected data since it is very difficult to examine whether each sample of data is outlier or not [8]. It is well known that, in this case, the conventional PCA is sensitive to outliers because it minimizes the reconstruction errors of training data in terms of the mean squared error and a few outliers with large errors dominate the objective function. This problem has been addressed in many studies [8–16]. Among them, some studies utilized L1-norm instead of L2-norm in the formulation of optimization problem to improve the robustness of PCA against outliers [9–11]. In [9], the cost function for optimization was constructed based on L1-norm and a convex programming was employed to solve the problem. R1-PCA [10] was presented to obtain a solution with the rotational invariance, which is a fundamental desirable property for learning algorithms [17]. In [11], PCA-L1 was proposed, which maximizes an L1 dispersion in the reduced space and an extension of PCA-L1 using Lp-norm with arbitrary p was also proposed in [14]. Other method utilizing Lp-norm was also presented in [15]. On the other hand, some of robust PCAs were recently developed using information theoretic measures [12,13]. He et al. [12] proposed MaxEnt-PCA which finds a subspace where Renyi׳s quadratic entropy [18] is maximized. Renyi׳s entropy was estimated by a non-parametric Parzen window technique. In [13], HQ-PCA was developed based on the maximum correntropy criterion [19].In this paper, we propose a new robust PCA method based on the power mean or the generalized mean [20], which can become the arithmetic, geometric, and harmonic means depending on the value of its parameter. The proposed method, PCA-GM, is a generalization of the conventional PCA by replacing the arithmetic mean with the generalized mean. The proposed method can effectively prevent outliers from dominating objective function by controlling the parameter in the generalized mean. Moreover, it is rotational invariant because it still uses the Euclidean distance as the distance measure between data samples. In doing so, we also propose a generalized sample mean, which is an enhancement of the conventional algebraic sample mean against outliers to address the problem that the sample mean is easily affected by outliers. It is used in the proposed PCA-GM instead of the arithmetic mean. The optimization problems based on the generalized mean are efficiently solved using a mathematical property of the generalized mean. Recently, Candés et al. proposed a robust PCA [21], which is sometimes referred to as RPCA in the literature, where data matrix is tried to be represented as a sum of a low rank matrix, which corresponds to reconstructions of data, and a sparse matrix, which corresponds to reconstruction errors different from the methods mentioned above. It can model pixel-wise noise effectively using the sparse matrix, thus it has been known that RPCA is useful in the applications such as background modeling from surveillance video and removing shadows and specularities from face images [21] by using each element in the reconstruction error vector (the column of the sparse matrix). On the other hand, in this paper, we will utilize distance metric in removing the effect of outliers like the previously mentioned methods, and an entire sample is considered as an outlier if it has a large norm of the reconstruction error vector.The remainder of this paper is organized as follows. Section 2 briefly introduces PCA and the state-of-the-art robust PCAs. The proposed method is described in Section 3. It is demonstrated in Section 4 that the proposed method gives better performances in face reconstruction and clustering problems than other variants of PCA. Finally, Section 5 concludes this paper.Let us consider a training set of N n-dimensional samples{lbracesupsubxi}i=1N. Assuming that the samples have zero-mean, PCA is to find an orthonormal projection matrixW∈Rn×m(m⪡n)by which the projected samples{yi=WTxi}i=1Nhave the maximum variance in the reduce space. It is formulated as follows:WPCA=argmaxWtr(WTSW),whereS=1N∑i=1NxixiTis a sample covariance matrix andtr(A)is the trace of a square matrixA. The projection matrixWPCAcan be also found from the viewpoint of projection errors, i.e., it minimizes the average of the squared projection errors or reconstruction errors. Mathematically, it is represented as the optimization problem minimizing the following cost function:JL2(W)=1N∑i=1N∥xi−WWTxi∥22,where∥x∥2is the L2-norm of a vectorx. The two optimization problems are equivalent and easily solved by obtaining the m eigenvectors associated with the m largest eigenvalues ofS. Although PCA is simple and powerful, it is prone to outliers [8,9] becauseJL2(W)is based on the mean squared reconstruction error. To learn a subspace robust to outliers, Ke and Kanade [9] proposed to minimize an L1-norm based objective function as follows:JL1(W)=1N∑i=1N∥xi−WWTxi∥1,where∥x∥1is the L1-norm of a vectorx. They also present an iterative method to obtain the solution for minimizingJL1(W).Although L1-PCA minimizingJL1(W)can relieve the negative effect of outliers, it is not invariant to rotations. In [10], Ding et al. proposed R1-PCA, which is rotational invariant, at the same time is robust to outliers. It is to minimize the following objective function:JR1(W)=∑i=1Nρ(xiTxi−xiTWWTxi),whereρ(·)is a generic loss function and the Cauchy function or Huber׳s M-estimator [22] was used forρ(·)in [10]. Huber׳s M-estimatorρH(s)is defined as(1)ρH(s)={s2if|s|≤c,2c|s|−c2otherwisewhere c is the cutoff parameter that controls the regularization effect of weights in a weighted covariance matrix. Note thatρH(s)becomes a quadratic or a linear function of|s|depending on the value of s. The solution for minimizingJR1(W)was obtained by performing a subspace iteration algorithm [23].On the other hand, PCA-L1 was developed in [11] motivated by the duality between maximizing variance and minimizing reconstruction error. It maximizes an L1 dispersion among the projected samples,∑i=1N∥WTxi∥1. A novel and efficient method for maximizing the L1 dispersion was also presented in [11]. The method allows PCA-L1 to be performed by much less computational effort than R1-PCA.HQ-PCA is formulated based on the maximum correntropy criterion in terms of information theoretic learning. Without the zero-mean assumption, which is necessary in other variants of PCA, HQ-PCA maximizes the correntropy estimated between a set of training samples{xi}i=1Nand the set of their reconstructed samples{Wyi+m}i=1N, wheremis a data mean. Mathematically, HQ-PCA tries to maximize the following objective function:(2)argmaxW,m∑i=1Ng(x¯iTx¯i−x¯iTWWTx¯i),whereg(x)=exp(−x2/2σ2)is the Gaussian kernel andx¯i=xi−m. Note that HQ-PCA finds a data mean as well as a projection matrix. Using the Welsch M-estimatorρW(x)=1−g(x), HQ-PCA is regarded as a robust M-estimator formulation because it is equivalent to findingWHandmHthat minimize the following objective function:(3)JHQ(W,m)=∑i=1NρW(x¯iTx¯i−x¯iTWWTx¯i).In [13], the optimization problem in (2) was effectively solved in the half-quadratic optimization framework, which is often used to address nonlinear optimization problems in information theoretic learning.For ap≠0, the generalized mean or power meanMpof{ai>0,i=1,…,N}[20] is defined asMp{a1,…,aN}=(1N∑i=1Naip)1/p.The arithmetic mean, the geometric mean, and the harmonic mean are special cases of the generalized mean whenp=1,p→0, andp=−1, respectively. Furthermore, the maximum and the minimum values of the numbers can also be obtained from the generalized mean by makingp→∞andp→−∞, respectively. Note that as p decreases (increases), the generalized mean is more affected by the smaller (larger) numbers than the larger (smaller) ones, i.e., controlling p makes it possible to adjust the contribution of each number to the generalized mean. This characteristic is useful in the situation where data samples should be differently handled according to their importance, for example, when outliers are contained in the training set.In [24], it was shown that the generalized mean of a set of positive numbers can be expressed by a nonnegative linear combination of the elements in the set and, in this paper, it is further simplified as follows:(4)∑i=1Kaip=b1a1+⋯+bKaKbi=aip−1,i=1,…,K.Note that each weight bihas the same value of 1 ifp=1, where the generalized mean becomes the arithmetic mean. It is also noted that, if p is less than one, the weight biincreases as aidecreases. This means that, whenp<1, the generalized mean is more influenced by the small numbers in{ai}i=1K, and the extent of the influence increases as p decreases. This equation plays an important role in solving the optimization problems using the generalized mean.Most conventional PCAs commonly assume that training samples have zero-mean. To satisfy this assumption, all of the samples are subtracted by the sample mean, i.e.,xi−mSfori=1,…,N, wheremS=1N∑i=1Nxi. The conventional sample mean can be considered as the center of the samples in the sense of the least square, i.e.,(5)mS=argminm1N∑i=1N∥xi−m∥22.In (5), a small number of outliers in the training samples dominate the objective function because the objective function in (5) is constructed based on the squared distances. To obtain a robust sample mean in the presence of outliers, a new optimization problem is formulated by replacing the arithmetic mean in (5) with the generalized mean asmG=argminm(1N∑i=1N(∥xi−m∥22)p)1/p.This problem is equivalent to (5) ifp=1. As mentioned in the previous subsection, the contribution of a large number to the objective function decreases as p decreases. Thus, the negative effect of outliers can be alleviated ifp<1. From now on, we will callmGas the generalized sample mean. Using the fact that xpwithp>0being a monotonic increasing function of x forx>0, this problem can be converted to(6)mG=argminm∑i=1N(∥xi−m∥22)p.Although the minimization in (6) should be changed into the maximization whenp<0, we only consider positive values of p in this paper.The necessary condition formGto be a local minimum is that the gradient of the objective function in (6) with respect tomis equal to zero, i.e.,∂∂m∑i=1N(∥xi−m∥22)p=0.However, it is hard to find a closed-form solution of the above equation. Although any gradient-based iterative algorithms can be applied to obtainmG, they usually have slow convergence speed. Alternatively, we develop a novel method based on (4), which is more efficient than gradient-based iterative methods. Our method for solving the problem in (6) is an iterative one, similar to the expectation–maximization algorithm [25].In the derivation, we decompose (6) into the form of (4) and consider the weight biin (4) as a constant. Then, (6) can be approximated by a quadratic function of∥xi−m∥2which can easily be optimized. The details are as follows. Let us denote the value ofmafter t iterations asm(t). The first step of the update rule is, formclose to a fixedm(t), to represent the objective function in (6) as a linear combination of∥xi−m(t)∥22using (4), i.e.,∑i=1N(∥xi−m∥22)p≈∑i=1Nαi(t)∥xi−m∥22,where(7)αi(t)=(∥xi−m(t)∥22)p−1.Here, the approximation becomes exact whenm=m(t). Note that the objective function nearm(t)can be approximated as a quadratic function ofmwithout computing the Hessian matrix of the objective function. The next step is to findm(t+1)that minimizes the approximated function based on the computedαi(t), i.e.,∂∂m∑i=1Nαi(t)∥xi−m∥22=0.The solution of this equation is just the weighted average of the samples as follows:(8)m(t+1)=1∑j=1Nαj(t)∑i=1Nαi(t)xi.This update rule with the two steps is repeated until a convergence condition is satisfied. This procedure is summarized in Algorithm 1.Algorithm 1Generalized sample mean.1: Input:{x1,…,xN},p>0.2:t⟵0.3:m(t)⟵mS.4: repeat5:Approximation: For fixedm(t), computeα1(t),…,αN(t)according to (7).6:Minimization: Using the computedα1(t),…,αN(t), updatem(t+1)according to (8).7:t⟵t+1.8: until A stop criterion is satisfied9: Output:mG=m(t).To demonstrate the robustness of the generalized sample mean obtained by Algorithm 1, we randomly generated 100 samples from a two-dimensional Gaussian distribution with the meanmi=0and covariance matrixΣi=diag[0.5,0.5]for inliers and also generated 10 samples from another two-dimensional Gaussian distribution with the meanmo=[5,5]Tand covariance matrixΣo=diag[0.3,0.3]for outliers. Using the generated samples, the sample mean was computed and two generalized sample means were also obtained by Algorithm 1 withp=0.1andp=0.2, respectively. Fig. 1shows the arithmetic sample mean and the two generalized sample means together with the generated samples. It is obvious that the generalized sample means are located close to the mean of the inliers,[0,0]T, whereas the arithmetic sample mean is much more biased by the ten outliers. This illustrates that the generalized sample mean with an appropriate value of p is more robust to outliers than the arithmetic sample mean.For a projected sampleWTx, the squared reconstruction errore(W)can be computed ase(W)=x˜Tx˜−x˜TWWTx˜,wherex˜=x−m. We use the generalized sample meanmGform. To prevent outliers corresponding to largee(W)from dominating the objective function, we propose to minimize the following objective function:(9)JG(W)=(1N∑i=1N[ei(W)]p)1/p,whereei(W)=xi˜Txi˜−xi˜TWWTxi˜is the squared reconstruction error ofxiwith respect toW. Note thatJG(W)is formulated by replacing the arithmetic mean inJL2(W)with the generalized mean keeping the use of the Euclidean distance and it is equivalent toJL2(W)ifp=1. The negative effect raised by outliers is suppressed in the same way as in (6). Also, the solution that minimizesJG(W)is rotationally invariant because eachei(W)is measured based on the Euclidean distance. To obtainWG, we develop an iterative optimization method similar to Algorithm 1.Like the optimization problem formGin the previous subsection, under the assumption thatp>0, the optimization problem based on (9) is firstly converted as follows:(10)WG=argminWTW=I(1N∑i=1N[ei(W)]p)1/p=argminWTW=I∑i=1N[ei(W)]p,Next, let us denoteW(t)as the value ofW∈Rn×mafter the t-th iteration. Near a fixedW(t), the converted objective function in (10) can be approximated as a quadratic function ofWaccording to (4) as∑k=1N[ei(W)]p≈∑i=1Nβi(t)ei(W),where(11)βi(t)=[ei(W(t))]p−1.Here, the approximation becomes exact ifW=W(t). After calculating eachβi(t),W(t+1)can be computed by minimizing the approximated function as(12)W(t+1)=argminW∑i=1Nβi(t)ei(W)=argmaxWtr(WTSβ(t)W),whereSβ(t)=∑i=1Nβi(t)x˜ix˜iT.Algorithm 2PCA-GM.1: Input:{x1,…,xN},mG, m, p.2:t⟵0.3:W(t)⟵WPCA∈Rn×m.4: repeat5:Approximation: For fixedW(t), computeβ1(t),…,βN(t)using (11).6:Minimization: Using the computedβ1(t),…,βN(t), findW(t+1)by solving the eigenvalue problem in (12).7:t⟵t+1.8: until A stop criterion is satisfied9: Output:WG=W(t).To help understanding of Algorithm 2, we made another toy example as shown in Fig. 2(a) where 110 two dimensional samples are plotted. Among the samples, 100 samples are regarded as inliers and the others are regarded as outliers. The samples were generated as the following rule:xi~N(0,1),yi=xi+ϵi,where the random noise ϵiis sampled fromN(0,0.52)for inliers andN(0,32)for outliers, respectively. Fig. 2(b) shows the objective function of PCA-GM in (9) withp=0.3for the samples as shown in Fig. 2(a). We can see from Fig. 2(b) that the conventional PCA is prone to the ten outliers because its objective function is minimized aroundW=[cos60°sin60°]T. However, PCA-GM is robust to the outliers because its objective function is minimized atW=[cos48.9°sin48.9°]T, which is close to the solution without the outliersW⁎=[cos45°sin45°]T. Given an initial projection vectorW(0)=[cos30°sin30°]T, the approximation step in Algorithm 2 gives a quadratic function corresponding to the red dashed line in Fig. 2(b). In the second step, the next iterationW(1)is determined as[cos32.1°sin32.1°]Tby minimizing the approximate function. Interestingly, it can be said that the approximate function plays a similar role of an upper bound of the objective function aroundW(0)in this update rule. It is also noted that the approximated function at the local optimal pointW=[cos48.9°sin48.9°]Thas its minimum as the same location, which is denoted as the magenta dashed dotted line in Fig. 2(b). This means that Algorithm 2 converges to the local minimum point of the objective function for the problem shown in Fig. 2(a).In practice, whenei(W(t))is zero or very small for any i,[ei(W(t))]p−1is numerically unstable ifp<1, and Algorithm 2 cannot proceed anymore. This problem can also occur in Algorithm 1. It can be overcome by adding a small constant δ into eachei(W)as(13)ei(W)′=x˜iTx˜i−x˜iTWWTx˜i+δ,where δ should be small enough that the modified objective function is not affected too much. This perturbation also changesSβ(t)in (12) intoS^β(t)asS^β(t)=∑i=1Nβi(t)(x˜ix˜iT+δn),where n is the original dimensionality of data.

@&#CONCLUSIONS@&#
