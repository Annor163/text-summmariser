@&#MAIN-TITLE@&#
Geometric operations using sparse interpolation matrices

@&#HIGHLIGHTS@&#
The method applies to curves and (hyper) surfaces that may contain base point.We exploit sparseness of the parameterization and of the implicit equation.The interpolation matrix suffices for membership and sidedness predicates.Our Maple code implements exact as well as approximate computation.

@&#KEYPHRASES@&#
Matrix representation,Implicitization,Linear algebra,Sparse polynomial,Membership,Sidedness operation,

@&#ABSTRACT@&#
Based on the computation of a superset of the implicit support, implicitization of a parametrically given hypersurface is reduced to computing the nullspace of a numeric matrix. Our approach predicts the Newton polytope of the implicit equation by exploiting the sparseness of the given parametric equations and of the implicit polynomial, without being affected by the presence of any base points. In this work, we study how this interpolation matrix expresses the implicit equation as a matrix determinant, which is useful for certain operations such as ray shooting, and how it can be used to reduce some key geometric predicates on the hypersurface, namely membership and sidedness for given query points, to simple numerical operations on the matrix, without need to develop the implicit equation. We illustrate our results with examples based on our Maple implementation.

@&#INTRODUCTION@&#
A fundamental question in changing representation of geometric objects is implicitization, namely the process of changing the representation of a geometric object from parametric to implicit. It is a fundamental operation with several applications in computer-aided geometric design (CAGD) and geometric modeling. There have been numerous approaches for implicitization, including resultants, Gröbner bases, moving lines and surfaces, and interpolation techniques.In this work, we restrict attention to hypersurfaces and exploit a matrix representation of hypersurfaces in order to perform certain critical operations efficiently, without developing the actual implicit equation. Our approach is based on potentially interpolating the unknown coefficients of the implicit polynomial, but our algorithms shall avoid actually computing these coefficients. The basis of this approach is a sparse interpolation matrix, sparse in the sense that it is constructed when one is given a superset of the implicit polynomial’s monomials. The latter is computed by means of sparse resultant theory, so as to exploit the input and output sparseness, in other words, the structure of the parametric equations as well as the implicit polynomial.The notion that formalizes sparseness is the support of a polynomial and its Newton polytope. Consider a polynomial f with real coefficients in n variablest1,…,tn,denoted byf=∑acata∈R[t1,…,tn],a∈Nn,ca∈R,whereta=t1a1…tnan.The support of f is the set{a∈Nn:ca≠0};its Newton polytopeN(f)⊂Rnis the convex hull of its support. All concepts extend to the case of Laurent polynomials, i.e. with integer exponent vectorsa∈Zn.The main ingredient of our method is the Newton polytope of the implicit equation, or implicit polytope and the set of lattice points it contains, which we call implicit support. The vertices of the implicit polytope are called implicit vertices. The implicit polytope is computed from the Newton polytope of the sparse (or toric) resultant, or resultant polytope, of auxiliary polynomials defined by the parametric equations. Under certain genericity assumptions, the implicit polytope coincides with a projection of the resultant polytope, see Section 3. In general, a translate of the implicit polytope is contained in the projected resultant polytope, in other words, a superset of the implicit support is given by the lattice points contained in the projected resultant polytope, modulo the translation. A superset of the implicit support can also be obtained by other methods, see Section 2; the rest of our approach does not depend on the method used to compute this support.The predicted support is used to build a numerical matrix whose kernel is, ideally, 1-dimensional thus yielding (up to a nonzero scalar multiple) the coefficients corresponding to the predicted implicit support. This is a standard case of sparse interpolation of the polynomial from its values. When dealing with hypersurfaces of high dimension, or when the support contains a large number of lattice points, then exact solving is expensive. Since the kernel can be computed numerically, our approach also yields an approximate sparse implicitization method.Our method of sparse implicitization was developed in [12,13], see Section 2. It handles hypersurfaces given parametrically by polynomial, rational, or trigonometric parameterizations and, furthermore, automatically handles the case of base points. The standard version of the method requires to compute the monomial expansion of the implicit equation. However, it would be preferable if various operations and predicates on the hypersurface could be completed by using the matrix without developing the implicit polynomial. This is an area of strong current interest, since expanding, storing and manipulating the implicit equation can be very expensive, whereas the matrix offers compact storage and fast, linear algebra computations. This is precisely the premise of this work.The main contribution of this work is to show that the matrix representation can be very useful when based on sparse interpolation matrices. First, from the interpolation matrix we construct a matrix which is numeric except for its last row. When this matrix is non-singular, its determinant equals the implicit equation (up to a constant multiple). This allows us to use the (nonzero) sign of the determinant of the numeric matrix obtained by evaluating its symbolic last row, to decide sidedness for query points q that do not lie on the hypersurfacep(x)=0. Second, we use the interpolation matrix, independently of its rank, to reduce the membership testp(q)=0,for a query point q and a hypersurface defined implicitly byp(x)=0,to rank tests on numeric matrices.Moreover, we implement an alternative interpolation matrix using the linear relations between the implicit and parametric expressions of the normal to the curve or surface at any given point, see e.g., [4]. This method is also known as Hermite interpolation. The new matrix is not of smaller size, but the number of sample points is reduced. With curves/surfaces, our method requires about half/one third of the sample points, respectively. The matrices again can be used to numerically evaluate membership and sidedness.Our methods work either with parameterized objects or with objects given by a point cloud along with normals at the points. When the parametric equations are not known, support prediction is not possible hence we use bounds on the implicit degree or try successively larger simplices. Our algorithms have been implemented in Maple. To emphasize algorithms and practical results we detail code and experiments; for readability we omit proofs of the statements. All omitted proofs can be found in [11].The paper is organized as follows: Section 2 overviews previous work. Section 3 describes our approach to predicting the implicit support while exploiting sparseness, presents our implicitization algorithm based on computing a matrix kernel and focuses on the case of high dimensional kernels. In Section 4 we formulate membership and sidedness tests as numerical linear algebra operations on the interpolation matrix. Our Maple implementation is described in Section 5 along with some examples. We conclude with further work and open questions.This section overviews existing work.If S is a superset of the implicit support, then the most direct method to reduce implicitization to linear algebra is to construct a |S| × |S| matrix M, indexed by monomials with exponents in S (columns) and |S| different values (rows) at which all monomials get evaluated. Then the vector of coefficients of the implicit equation is in the kernel of M. This idea was used in [12,19,24]; it is also the starting point of this paper.An interpolation approach was based on integrating matrixM=SS⊤,over each parametert1,…,tn[6]. Then the vector of implicit coefficients is in the kernel of M. In fact, the authors propose to consider successively larger supports in order to capture sparseness. This method covers polynomial, rational, and trigonometric parameterizations, but the matrix entries take big values (e.g. up to 1028), so it is difficult to control its numeric corank, i.e. the dimension of its nullspace. Thus, the accuracy of the approximate implicit polynomial is unsatisfactory. When it is computed over floating-point numbers, the implicit polynomial does not necessarily have integer coefficients. They discuss post-processing to yield integer relations among the coefficients, but only in small examples.Our method of sparse implicitization was introduced in [12], where the overall algorithm was presented together with some results on its preliminary implementation, including the case of approximate sparse implicitization. The emphasis of that work was on sampling and oversampling the parametric object so as to create a numerically stable matrix, and examined evaluating the monomials at random integers, random complex numbers of modulus 1, and complex roots of unity. That paper also proposed ways to obtain a smaller implicit polytope by downscaling the original polytope when the corresponding kernel dimension was higher than one.One issue was that the kernel of the matrix might be of high dimension, in which case the equation obtained may be a multiple of the implicit equation. In [13] this problem was addressed by describing the predicted polytope and showing that, if the kernel is not 1 dimensional, then the predicted polytope is the Minkowski sum of the implicit polytope and an extraneous polytope. The true implicit polynomial can be obtained by taking the greatest common divisor (GCD) of the polynomials corresponding to at least two and at most all of the kernel vectors, or via multivariate polynomial factoring.Our implicitization method is based on the computation of the implicit polytope, given the Newton polytopes of the parametric polynomials. Then the implicit support is a subset of the set of lattice points contained in the computed implicit polytope. There are methods for the computation of the implicit polytope based on tropical geometry [23,24], see also [9]. Our method relies on sparse elimination theory so as to compute the Newton polytope of the sparse resultant. In the case of curves, our Maple implementation relies on [14] to compute the implicit support.In [10], they develop an incremental algorithm to compute the resultant polytope, or its orthogonal projection along a given direction. It is implemented in package ResPol11http://sourceforge.net/projects/respol. The algorithm exactly computes vertex- and halfspace-representations of the target polytope and it is output-sensitive. It also computes a triangulation of the polytope, which may be useful in enumerating the lattice points. It is efficient for inputs relevant to implicitization: it computes the polytope of surface equations within 1 s, assuming there are less than 100 terms in the parametric polynomials, which includes all common instances in geometric modeling. This is the main tool for support prediction used in this work.The use of matrix representations in geometric modeling and CAGD is not new. In [25] they introduce matrix representations of algebraic curves and surfaces and manipulate them to address the curve/surface intersection problem by means of numerical linear algebra techniques In [2], the authors make use of some generalized matrix-based representations of parameterized surfaces in order to represent the intersection curve of two such surfaces as the zero set of a matrix determinant. Their method extends to a larger class of rational parameterized surfaces, the applicability of a general approach to the surface/surface intersection problem in [16]. In [5] they introduce a new implicit representation of rational Bézier curves and surfaces in the 3-dimensional space. Given such a curve or surface, this representation consists of a matrix whose entries depend on the space variables and whose rank drops exactly on this curve or surface.This section describes how sparse elimination can be used to compute the implicit polytope by exploiting sparseness and how this can reduce implicitization to linear algebra. We also discuss how the quality of the predicted support affects the implicitization algorithm and develop the necessary constructions that allow us to formulate the membership and sidedness criteria in the next section.A rationally parameterized hypersurface inRn+1is defined by a mapϕ:Rn→Rn+1:(1)t:=(t1,t2,…,tn)↦ϕ(t)=(x0,…,xn),wherexi=fi(t)gi(t),i=0,…,n.We assume thatgcd(f0,g0,…,fn,gn)=1,which implies that the parameterization has finitely many base points. We also assume that the parametrization is proper or faithful, i.e., all but finitely many points on the hypersurface correspond to a single parameter value.The implicitization problem asks for the defining polynomialp(x0,…,xn)of the Zariski closure of the image of ϕ. Implicitization of planar curves and surfaces in three dimensional space corresponds ton=1andn=2respectively. In sparse implicitization we wish to computep(x0,…,xn)=0given its Newton polytopeP=N(p),or a polytope that contains it. This polytope’s computation should exploit the sparseness of the polynomials fi, gi. This is achieved by considering the sparse resultant.It is possible to reduce implicitization to eliminating the parameters t, y from the auxiliary polynomials(2)Fi:=xigi(t)−fi(t),i=0,…,n,Fn+1:=1−yg0(t)…gn(t),considered as polynomials in(R[x0,…,xn])[t,y]=K[t,y]. In this case y is a new variable andFn+1assures that the parametric denominators do not vanish, which avoids base points. Eliminating t, y may be done by taking the sparse resultant of the polynomials in expression (2). UsingFn+1implies that the sparse resultant would be a smaller multiple of the implicit equation, or, generically equal to the later. Dropping the assumption that the parameterization is faithful, leads to a sparse resultant which is a power of the implicit polynomial.LetAi⊂Zn+1,i=0,…,n+1be the supports of the polynomials Fiand consider the generic polynomials(3)F0′,…,Fn′,Fn+1′∈K[t,y],with the same supports Aiand symbolic coefficients cij. To ensure that the resultant is non-trivial, or, equivalently, that the image of the parameterization is a hypersurface, we assume that the supports Aiform an essential family [22], i.e., that they jointly generateZn+1and any k-subset of them generates a lattice of dimension at least k.Definition 1The sparse resultantR=ResA0,…,An+1(F0′,…,Fn+1′)of system (3) is a polynomial in the cijwith integer coefficients, namelyR∈Z[cij:i=0,…,n+1,j=1,…,|Ai|],which is unique up to sign and vanishes after a specialization of the coefficients cijinK,if and only if the specialized system has a common root in the toric variety overKdefined by the Ai’s.The resultantResA0,…,An+1(F0,…,Fn+1)can be obtained by specializing the symbolic resultantResA0,…,An+1(F0′,…,Fn+1′)to the coefficients of the Fi’s, provided that this specialization is generic enough. Then the implicit polytope P equals the projection Q of the resultant polytopeN(R)to the space of the implicit variables, i.e. the Newton polytope of the specialized resultant, up to some translation. When the specialization of the cijis not generic enough, then Q contains a translate of P. This follows from the fact that the method computes the same resultant polytope as the tropical approach, where the latter is specified in [23, Prop. 5.3].Note that the sparse resultantResA0,…,An+1(F0,…,Fn+1)of the specialized polynomials Fiis a polynomial in the implicit variables xi, that equals the implicit polynomial of the parametric hypersurface (2), provided that the resultant does not vanish identically. This cannot happen if the supports are essential and the polynomials Fihave generic coefficients relative to their supports Ai, even in the presence of base points. The latter do not affect the vanishing of the sparse resultant because the latter is defined from the supports Aiof the polynomialsFi=xigi(t)−fi(t)which ignore the specific supports of the polynomials gi, fi: even if these polynomials have common monomials, taking the support Aiimplies one symbolic coefficient for all of them in Fi. Examples 1 and 2 apply our method to surfaces with base points.Given Q, the implicit support is a subset of the lattice points it contains. For computing Q we employ [10] and software ResPol22http://sourceforge.net/projects/respol.In the rest of the paper we assume that all polytopes are translated to the positive orthant and have non-empty intersection with the coordinate axis. This allows us to consider points with zero coordinates.The predicted implicit polytope Q and the set S of lattice points it contains are used in our implicitization algorithm to construct a numerical matrix M. If the kernel of M is one dimensional, then its unique generator contains the coefficients of the monomials with exponents in S in the implicit polynomial p(x). Otherwise, the vectors in the kernel contain the coefficients of multiples of p(x) as we shall see below.Let us describe the construction of M. LetS:={s1,…,sν}; eachsj=(sj0,…,sjn),j=1,…,νis an exponent of a (potential) monomialmj:=xsj=x0sj0…xnsjnof the implicit polynomial. We denote bym=(m1,…,mν)the vector of potential monomials and evaluate it at generic pointsτk∈Cn,k=1,…,μ,μ≥ν; each mjis evaluated at τkusing the parametric expressions (1) as follows:∏i=0n(fi(τk)gi(τk))sji,j=1,…,νavoiding values that make the denominators close to 0. Letm(τk) denote the vectormevaluated at τk. Thus, we construct an μ × ν matrix M with rows indexed byτ1,…,τμand columns bym1,…,mν:(4)M=[m(τ1)⋮m(τμ)].To cope with numerical issues, especially when computation is approximate, we let μ ≥ ν; this overconstrained system increases numerical stability and reduces the probability of obtaining an empty or higher dimensional kernel due to a bad sampling, see below.When constructing the interpolation matrix M it is critical to ensure that the parametric hypersurface is sampled sufficiently generically. We implement sampling by choosing random valuesτk∈Cnfor the parameters, at which we evaluate the parametric expressions. It is possible to check a-posteriori the genericity of the sampling by testing the evaluated matrix. However, experiments indicate that our scheme produces satisfactory results especially if care is taken to address small examples, i.e., when ν is small. The following reasoning also confirms these results. We employ μ ≥ ν random sample points in some set G, typically the lattice points in a hypercube. The probability that a nonzero polynomial other than p(x), in the same monomial basis S, also vanishes at one of the sample points is inversely proportional to |G| [21]. Hence we have the following:Lemma 2[13]Any polynomial in the basis of monomials S indexing M, with coefficient vector in the kernel of M, is a multiple of the implicit polynomial p(x).As in [12], one of the main difficulties is to build M whose corank, or kernel dimension, equals 1, i.e. its rank is 1 less than its column dimension. For some inputs we obtain a matrix of corank > 1 when the predicted polytope Q is significantly larger than P. It can be explained by the nature of our method: we rely on a generic resultant to express the implicit equation, whose symbolic coefficients are then specialized to the actual coefficients of the parametric equations. If this specialization is not generic, then the implicit equation divides the specialized resultant. The following theorem establishes the relation between the dimension of the kernel of M and the accuracy of the predicted support. It remains valid even in the presence of base points. In fact, it also accounts for them since then P is expected to be much smaller than Q.Theorem 3[13]LetP=N(p)be the Newton polytope of the implicit equation, and Q the predicted polytope. Assuming M has been built using sufficiently generic evaluation points, the dimension of its kernel equalsr=#{a∈Zn+1:a+P⊆Q}=#{a∈Zn+1:N(xa·p)⊆Q}.The formula for the corank of the matrix in the previous theorem also implies that the coefficients of the polynomials xap(x) such that N(xap(x))⊆Q, form a basis of the kernel of M (see [13, Proof of Thm. 10]). This observation will be useful in the upcoming Lemma 8 but also implies the following corollary.Corollary 4[13]Let M be the matrix from (4), built with sufficiently generic evaluation points, and suppose the specialization of the polynomials in expression (3) to the parametric equations is sufficiently generic. Let{c1,…,cλ}be a basis of the kernel of M andg1(x),…,gλ(x)be the polynomials obtained as the inner productgi=ci·m. Then the greatest common divisor (GCD) ofg1(x),…,gλ(x)equals the implicit equation up to a monomial factor xe.The extraneous monomial factor xein the previous corollary is always a constant when the predicted polytope Q is of the formQ=P+Rand, as we assume throughout this paper, it is translated to the positive orthant and touches the coordinate axis. However, it is possible that Q strictly containsP+Rand the extraneous polytope R is a pointe∈Rn+1,or it is the Minkowski sum of point e and a polytope R′ which touches the axis. Let ∑βcβxβbe the GCD of the polynomials giin Corollary 4, and letγ=(γ0,…,γn),whereγi=minβ(βi),i=0,…,n. We can efficiently remove the extraneous monomial xeby dividing ∑βcβxβwith xγ, i.e. the GCD of monomials xβ.We modify slightly the construction of the interpolation matrix M to obtain a matrix denoted M(x) which is numerical except for its last row. This matrix will be crucial in the constructions to follow.Fix a set of generic distinct valuesτk,k=1,…,ν−1and recall thatm(τk) denotes the vector of potential monomialsmevaluated at τk. Let M′ be theν−1×νnumeric matrix whose rows arem(τk),k=1…,ν−1,and let M(x) be the ν × ν matrix obtained by appending the (symbolic) row vectormto matrix M′:(5)M(x)=[M′m].Algorithm 1 summarizes the construction of matrix M(x).Given a pointq∈Rn+1,let M(q) be the matrixM(q)=[M′m(q)],wherem(q) denotes the vectormof predicted monomials evaluated at point q. This evaluation is done directly in the implicit variables and does not require using the parametric expressions. We assume that q does not coincide with any of the pointsx(τk),k=1,…,ν−1used to build matrix M′, which implies that the rows of matrix M(q) are distinct. This can be checked efficiently. Obviously, when q lies on the hypersurfacep(x)=0,matrix M(q) is equivalent to matrix M in expression (4) in the sense that they both have the same kernel and rank.Remark 6Let M be a matrix as in expression (4) andcbe a vector in the kernel of M. Then, since the kernel is a vector space, vector λc, for anyλ∈R*,is also in the kernel of M. This also follows of the implicit equation being defined up to a nonzero scalar multiple. As a consequence we can set an arbitrary nonzero coordinate ofcequal to 1, and we can always find at least one such coordinate since we assume the kernel is nonempty. As a consequence the matrices M′ and M (and from the discussion above also M(q), wherep(q)=0), have the same kernel of dimension r, where r is given in Theorem 3.Matrix M(x) has an important property as shown in the following:Lemma 7Assuming M′ is of full rank, the determinant of matrix M(x) equals the implicit polynomial p(x) up to a constant.We shall exploit this property in the sequel where we define sidedness for a point q as the sign of the determinant of the matrix M(q). Another application of this matrix in potentially constructing a ray-shooting is briefly discussed in Section 6. Although we rely ondetM(x)being a scalar multiple of the implicit polynomial, this is used mainly in proving our results; we shall avoid to explicitly compute symbolic determinants whenever possible.In this section we formulate certain elementary geometric operations on the hypersurface defined by p(x) as matrix operations. In particular, we focus on membership and sidedness and present our results in Lemmas 8 and 11. A discussion on their numerical behavior can be found in Section 5.Given a parameterizationxi=fi(t)/gi(t),i=0,…,n,and a query pointq∈Rn+1,we want to decide ifp(q)=0is true or not, where p(x) is the implicit equation of the parameterized hypersurface. Our goal is to formulate this test using the interpolation matrix in (5).Working with matrices instead of polynomials, we cannot utilize Corollary 4 and Remark 5 to process the kernel polynomials. Thus, a kernel polynomial might be of the form xep(x). To avoid reporting a false positive when evaluating such a polynomial at a query point having zero coordinates, we restrict membership testing to pointsq∈(R*)n+1,whereR*=R∖{0}.Lemma 8Let M(x) be as in expression (5) andq=(q0,…,qn)be a query point in(R*)n+1. Then q lies on the hypersurface defined byp(x)=0if and only ifcorank(M(q))=corank(M′).Note that we do not require that M′ is of corank 1. Lemma 8 readily yields Algorithm 2 that reduces the membership testp(q)=0for a query pointq∈Rn+1,to comparing ranks of matrices M′ and M(q).Let us now consider the sidedness operation for the hypersurfacep(x)=0,which we define using the sign of the evaluated polynomial p(q), forq∈R:Definition 9Given a hypersurface inRn+1with defining equationp(x)∈R[x],and a pointq∈Rn+1such that p(q) ≠ 0, we defineside(q)=sign(p(q))∈{−1,1}.See Fig. 1for an example of applying Definition 9 to the folium of Descartes curve defined byy3−3xy+x3=0.We will use matrix M(x) defined in expression (5) to reduce sidedness in the sense of Definition 9, to the computation of the sign of a numerical determinant. The following lemma assures that this determinant is nonzero for relevant inputs.Lemma 10Suppose that the predicted polytope Q contains only one translate of the implicit polytope P. Let M(x) be a matrix as in expression (5) and letq∈(R*)n+1such that p(q) ≠ 0. ThendetM(q)≠0.Given matrix M(x) and a pointq∈(R*)n+1such that p(q) ≠ 0, the sign ofdet(M(q))must be consistent with side(q) in the following sense: for every pair of query points q1, q2, wheneverside(q1)=side(q2),we have thatsign(detM(q1))=sign(detM(q2)). This is shown in the following:Lemma 11Let M(x) be as in expression (5) and q1, q2be two query points in(R*)n+1not lying on the hypersurface defined byp(x)=0. Assuming that the predicted polytope Q contains only one translate of the implicit polytope P, thenside(q1)=side(q2)if and only ifsign(detM(q1))=sign(detM(q2)),where sign( · ) is an integer in{−1,1}.Algorithm 3summarizes the previous discussions for deciding sidedness for any two query points. The rank test at step 2 of the algorithm can be avoided if we directly compute sign(detM(qi))and proceed depending on whether this sign equals 0 (i.e.detM(qi)=0) or not.We have implemented our algorithms in Maple 14. A beta-version is publicly available33http://ergawiki.di.uoa.gr/index.php/Implicitization. This section discusses this implementation and demonstrates its usage through a few examples. For completeness and because the implicitization routines of our implementation have been revised along with the addition of the new routines that implement the geometric predicates, we include a few examples that demonstrate their use. The corresponding timings together with comparisons with other methods are shown in Tables 1–3.The main functions for computing implicit equations are imcurve and imgen. Both functions operate similarly: first they construct a square or rectangular M by evaluating the implicit monomials at random integers, random complex numbers of modulus one, or complex roots of unity. To compute the nullspace of M exactly we use Maple’s commands LinearSolve which appears much faster than the dedicated command Nullspace; approximate results are obtained by numerical methods, in particular SVD, using SingularValues. The user can choose the method of solving as well as the way of evaluating the potential monomials. To compute all lattice points contained in the predicted implicit polytope Q, we use the external Maple package convex44http://www.math.uwo.ca/~mfranz/convex.Function imcurve accepts only planar curves and computes the implicit polygon following [14]. Function imgen is more general since it can compute the implicit equation of parametric curves, surfaces or hypersurfaces in 4-dimensional space, and is generalizable to arbitrary dimension. It requires the predicted implicit polytope to be computed from an external method, such as ResPol. These functions take as arguments:•The list of parametric expressions.(imgen only) The set of vertices of the predicted polytope Q.The solving method parameter: “exact” stands for exact computations using LinearSolve, and “approx” for approximate using SingularValues.The sampling parameter: “int” stands for random integers, “unc” for random complex numbers of modulus 1 whose real and imaginary part are floating point numbers, and “ruf” for roots of unity evaluated as floating point numbers.The ratio between number of rows and columns of the matrix: an integer ≥ 1.An optional integer in {0, 1} controlling the verbosity of the output.For the user’s convenience the routine WriteRespolInput allows to generate input files for ResPol for computing Q. The interpolation matrix M is constructed by function imatrix which also constructs the membership matrix M′ by setting the input argument for the ratio between number of rows and columns of the matrix equal to 0.The user has the option to use functions iderivc and ideriv that implement sparse imlicitization using normals for curves and surfaces, respectively. Their usage and arguments are identical to those of imcurve and imgen.The membership test is implemented in function MembTest which computes the rank of matrix M′ and compares it with the rank of the ν × ν matrix M(q) for a query point q. Its arguments are:•The list of parametric expressions.The(ν−1)×νmatrix M′.The vectormof predicted monomials.The query point q.The solving method parameter: “exact” stands for exact computations using LinearSolve, and “approx” for approximate using SingularValues.An optional integer in {0, 1} controlling the verbosity of the output.The corresponding function using normals is DMembTest having the same arguments.The main function implementing the sidedness test is SideTest which given query points q1, q2, constructs matrices M(q1), M(q2), and decides sidedness by comparing the sign of their determinants. Its arguments are the same as MembTest with the addition of a second query point. The corresponding function using normals is DSideTest. Currently the sign of the determinant is implemented by just computing the determinants and extracting their sign. However, there exist specialized algorithms for computing the sign of determinants which claim to be faster in practice than computing the determinant value e.g. [1,3]. Especially if one is ready to accept a small probability of error, in the context of approximate computation, determinant sign is faster than determinant value, see [15].The matrices encountered are hard to handle numerically and often ill-conditioned. One reason is the large range span by the absolute values of all entries. The ratio of the maximum over the minimum absolute value of the entries can be as high as 10300 in the case of the bicubic surface. Determinant computation, when computing sidedness, is then especially affected and becomes numerically unstable, which motivates the discussion above for algorithms computing the sign. To cope with this problem we compute determinants modulo a few large prime numbers and decide sidedness if all results agree.In the following examples we demonstrate the basic usage of our code. All experiments were performed on a Intel i5-2500, 3.30 GHz CPU, Linux machine with 8 GB of memory. To sample the parametric objects in all examples we used random integers.In many cases we have deliberately used query points having zero coordinates to illustrate that our criteria may produce the correct answer for such inputs. However, a correct answer cannot be guaranteed since it is closely related to the quality of the predicted support which, obviously, cannot be known a priori.We start with two surfaces which have base points but are treatable by our method. This implies (see also Section 3) that base points do not always lead to a higher dimensional kernel, for even if Q is strictly larger that P, it can fit only a single translate of P.Example 1[17, Exam. 2] Consider the surface parameterization:x0=st2−tst2,y=st+sst2,x2=2s−2tst2,having a base point at (0, 0). ResPol predicts an implicit polytope with vertices: (2, 0, 0), (0, 2, 0), (0, 0, 2), (0, 0, 0). This polytope, as expected from the discussion in Section 3, is slightly bigger than the implicit polytope, in fact it is the union of P and the origin. However, it contains only one translate of P which implies that the interpolation matrix M is of corank 1. It is therefore suitable not only for interpolation and membership, but also for sidedness. The implicit polynomial turns out to be(1/2)z+(1/4)z2−2y−yz+y2−x−xz+2xy+x2.Consider its parameterization:x0=−1−t2+s21+t2+s2,x1=2s1+t2+s2,x2=2st1+t2+s2.It has the base point(0,−i),i=−1. ResPol predicts an implicit polytope with vertices: (0, 0, 2), (0, 0, 4), (0, 2, 0), (0, 4, 0), (2, 0, 2), (2, 2, 0) It contains three translates of the implicit polytope, hence the interpolation matrix M will have corank 3. This restricts us from deciding sidedness but membership is still possible.Let us show how to use the basic routines for implicitization, membership and sidedness.Example 3Consider the polynomial parametric surfacex0=12t2−12s2−14t4+32t2s2−14s4,x1=−ts−t3s+ts3,x2=23t3−2ts2.Define the auxiliary polynomialsF0=x0−12t2+12s2+14t4−32t2s2+14s4,F1=x1+ts+t3s−ts3,F2=x2−23t3+2ts2,and using as input the supports of these polynomials, ResPol predicts implicit verticesV={(3,2,2),(9, 0, 4), (0, 12, 0), (0, 0, 16), (4, 4, 0), (0, 0, 6), (8, 4, 0), (0, 8, 0), (3, 0, 4), (0, 2, 4)}. This polytope contains 400 lattice points. Issuing the following command in Maple:imgen([x0,x1,x2],V,``exact′′,``int′′,1,0);we obtain the degree 16 implicit equation of the surface in 5.7 s. To decide membership for point(−1,3,2),and sidedness for points(10,−1/2,5),(2,−3,6)we use the following commands:MembTest([x0,x1,x2],M′,m,(−1,3,2),``exact′′,0);SideTest([x0,x1,x2],M′,m,(10,−1/2,5),(2,−3,6),``exact′′,0);,where the membership matrix M′ (of size 399 × 400 and of full rank) and the vectormof monomials with exponents in V are computed by the auxiliary routines imatrix and MakeMonomials, respectively. In 5.4 s MembTest answers negatively. For SideTest we resort to modular determinant computation and after 250 s the answer is inconclusive (half of the six pairs of results are positive) which suggests we either try with a new matrix or use more prime numbers.Consider the following curve shown in Fig. 2:x0=−−3+t4+6t23(1+t2)2,x1=8t3(1+t2)2;ResPol predicts the implicit polytope with vertices (0, 0), (0, 2), (0, 4), (1, 0), (4, 0); this polytope contains 15 lattice points. Using function imcurve and exact computations, we obtain the implicit equation in 0.01 s. It is a polynomial of total degree 4 containing seven terms−27x4−54x2y2−27y4+18x2+18y2+8x+1.To test if point (1, 0) lies on the curve, function MembTest builds a 15 × 15 matrix M((1, 0)), whose rank is 14, equal to the rank of matrix M′ hence the routine informs us that point (1, 0) lies on the curve.To decide sidedness for points (0, 0),(−13,13),function SideTest builds non-singular matrices M((0, 0)) andM((−13,13))whose determinants have equal signs. Hence both points lie on the same side of the curve (Fig. 3).Consider the Enneper’s surface parameterized as:x0=13(s−s3+st2),x1=13(−t+t3−s2t),x2=13(s2−t2).Using ResPol we obtain the implicit polytope’s vertices (0, 2, 0), (6, 0, 0), (0, 6, 0), (0, 0, 9), (2, 0, 0), (0, 0, 1); this polytope contains 103 lattice points. Using function imgen and exact kernel computation, we compute the implicit equation in 0.3 s. It is a polynomial of total degree 9 containing 23 terms.In order to determine if the point (2, 2, 0) lies on the surface, we build an 102 × 103 matrix M of rank equal to 102, and an 103 × 103 matrix M((2, 2, 0)) of the same rank, therefore MembTest decides in 0.41 s that the given point lies on the surface. Given query point(4,2,−12),matrixM((4,2,−12))has rank 103 hence MembTest decides in 0.4 s that the given point does not belong to the surface. To decide sidedness for points (2, 5, 1) and(−1,3,2)SideTest using exact determinant computation answers negatively in 90 s.Consider the standard benchmark of the bicubic surface, and define the following in(R[xi])[t1,t2]:F0=x0−3s(s−1)2−(t−1)3−3t,F1=x1−3t(t−1)2−s3−3s,F2=x2+3t(t2−5t+5)s3+3t(t−1)+3(t3+6t2−9t+1)t12−t1(6t3+9t2−18t+3).It takes ResPol 0.1 s to output the implicit polytope’s vertices (0, 0, 0), (18, 0, 0), (0, 18, 0), (0, 0, 9); this polytope contains 715 lattice points. Given the predicted implicit support, we build a 715 × 715 matrix M of corank 1. The implicit equation of the bicubic surface is computed in 42 susing function imgen and exact kernel computation. It is a polynomial of degree 18 containing 715 terms which corresponds exactly to the predicted implicit support.Testing membership and sidedness with such big matrices can be a challenge for Maple. Here matrices are especially ill-conditioned which affects mostly determinant computations (1677 s, inconclusive results) and, to a lesser extend, membership (68 s).Consider the Bourgain hypersurface:x0=s,x1=2t−2ws−wt,x2=wt−t+ws,x3=w.ResPol predicts vertices (0, 0, 1, 1), (1, 0, 0, 1), (0, 1, 0, 0), (0, 0, 1, 0), (1, 0, 0, 2), (0, 1, 0, 1); this polytope contains six lattice points. MembTest instantly decides that point (1, 1, 1, 1) lies on the hypersurface since the matrices M′ and M((1, 1, 1, 1)) have the same rank 5. SideTest is as fast in deciding that(2,−5,1,1)and(−1,3,2,−4)lie on different sides of the hypersurface.Consider the hypersurface:x0=s+t+w,x1=st−wt+ws,x2=wts,x3=s4+t4+w4.ResPol predicts vertices (24, 0, 0, 0), (0, 12, 0, 0), (0, 12, 0, 0), (0, 0, 8, 0), (0, 0, 0, 6); this polytope contains 169 lattice points. In order to determine if the points (2,5,1,0) and(−1,3,2,−4)lie on the same side of the surface we construct two 169 × 169 non-singular matrices whose determinants have the same sign, hence the answer is positive. It takes SideTest 0.3 s to complete the test using approximate computations.In Tables 1–3 we compare: our methods of sparse implicitization (SI) and implicitization using normals (IN), against μ-bases method [8], Gröbner bases [7, Chap. 3], and elimination using the Dixon resultant [18] (only for surfaces). All methods are implemented in Maple. The second column (Deg.) contains the implicit degrees while the third one (Msize) the column size of the matrices. We include runtimes for testing membership and sidedness only for our methods, because including runtimes for evaluation of the implicit polynomial obtained by other methods is not very informative; it does not account for the computational time to obtain the implicit polynomial which is not required by our predicates.We conclude that on small inputs all exact methods are comparable. For larger input, Gröbner bases seem to be slower than most, whereas μ-bases seem to behave best. It is encouraging that in almost all instances, the two predicates studied here are answered within about the same time complexity. The use of normals does not seem to change significantly the performance of our method.We have shown that certain operations can be accomplished on the matrix representation of a hypersurface, namely by using the interpolation matrix that we constructed. Our current work includes the study of further operations, most notably ray shooting, either in exact or approximate form.To perform ray shooting, assume that a ray is parameterized by ρ > 0 and given byxi=ri(ρ),i=0,…,n,where the riare linear polynomials in ρ. We substitute xiby ri(ρ) in M(x) thus obtaining a univariate matrix M(ρ) whose entries are numeric except for its last row. Assuming that M(x) is not singular, imagine that we wish to developdetM(ρ)by expanding along its last row. Then in preprocessing we compute all minorsdetMνj,j=1,…,ν,corresponding to entries in the last row, where Mijis the submatrix of M(ρ) obtained by deleting its ith row and jth column, thus definingp(ρ)=∑j=1ν(−1)ν+jdet(Mνj)mj(ρ).Note that every mj(ρ) is a product of powers of linear polynomials in ρ. Now ray shooting is reduced to finding the smallest positive real root of a univariate polynomial. For this, we plan to employ state of the art real solvers which typically require values of the polynomial in hand. To guarantee that the intersection of the ray and the hypersurface belongs to a given patch, an inversion formula would be needed. In particular, given the intersection point xαwe want to find parameter values tαsuch thatxα=(tα)and tαlies among the values that define the patch.Another useful application asks for the implicit equation of curves and surfaces which are given as a point cloud. In the specific case where we are given the normal at every point, we can formulate a linear system whose solution is the vector of coefficients of the implicit equation.Future work includes studying the matrix structure, which generalizes the classic Vandermonde structure, since the matrix columns are indexed by monomials and the rows by values on which the monomials are evaluated. This reduces matrix-vector multiplication to multipoint evaluation of a multivariate polynomial. There are no fast algorithms for multivariate interpolation and evaluation over arbitrary points, in general. However, it is possible to evaluate a bivariate polynomial of degree δ per variable over δ2 points in O(δ2.91) [20]. For implicit equations of curves whose Newton polygon is not very long and skinny we may assumeν=Θ(δ2),hence this method yields a matrix-vector multiplication in O(ν2.5).Sparse interpolation is the problem of interpolating a multivariate polynomial when information of its support is given [26, Ch.14]. A probabilistic approach in O(ν2δn) requires as input only δ, a bound on the output degree per variable. The most difficult step in the type of algorithms by Zippel for interpolation is computing the multivariate support: in our case this is known, hence our task should be easier.

@&#CONCLUSIONS@&#
