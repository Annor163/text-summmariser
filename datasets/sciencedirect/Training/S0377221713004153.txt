@&#MAIN-TITLE@&#
Iterative approaches for a dynamic memory allocation problem in embedded systems

@&#HIGHLIGHTS@&#
Two Mid-term iterative approaches for addressing a dynamic memory allocation problem are proposed.They are compared with Short-term and Long-term iterative approaches form the literature, as well as with an ILP formulation.Statistic analysis is used for showing the superiority of the Mid-term approach.

@&#KEYPHRASES@&#
Memory allocation,Embedded systems,Metaheuristics,VNS,

@&#ABSTRACT@&#
Memory allocation has a significant impact on energy consumption in embedded systems. In this paper, we are interested in dynamic memory allocation for embedded systems with a special emphasis on time performance. We propose two mid-term iterative approaches which are compared with existing long-term and short-term approaches, and with an ILP formulation as well. These approaches rely on solving a static version of the allocation problem and they take advantage of previous works for addressing the static problem. A statistic analysis is carried out for showing that the mid-term approach is the best one in terms of solution quality.

@&#INTRODUCTION@&#
Technology offers more and more features allowing embedded systems (such as smart phones) to surf the Web or to process HD pictures. As a consequence, the design of embedded systems becomes more and more complex. There exist some CAD tools such as Gaut [6] to generate the architecture of a circuit from its specifications. However, the designs produced by a CAD software are generally not energy aware, which is of course a major drawback. An interesting work on buffer minimization for designing embedded system has been conducted in [14], where the objective is minimizing the total surface of the buffers used for communicating data between two tasks of the application.Designers want to minimize power consumption [1], and to some extent, electronics practitioners consider that minimizing power consumption is equivalent to minimizing the running time of the application to be executed by the embedded system [4]. Moreover, the power consumption of a given application can be estimated using an empirical model as in [9], and parallelization of data access is viewed as the main action point for minimizing execution time, and consequently power consumption.Memory allocation and data allocation are among the main challenges in the design of embedded systems. This paper is focused on memory allocation in embedded systems because this point has a significant impact on power consumption as shown by Wuytack et al. in [25]. One has to know that the process of memory allocation is done off-line, during the design of the embedded system. Once set, the final embedded system is implemented in a specific architecture and will not be modified on-line. Thus, it is possible to spend some time to optimize memory allocation without consequences on the running time of the final application.In 2012, Google has suggested a machine reassignment problem as the topic for the ROADeF/EURO Challenge [16]. This problem is a general memory allocation problem and may be considered close to the problem addressed in our study. The machine reassignment problem consists in assigning each process to exactly one machine considering the resource capacity constraints and hard constraints that make some allocations impossible. The objective is to improve the usage of a set of machines by minimizing the total cost, which is the sum of a load cost, a balance cost and several move costs. Thus, the set of machines with a limited resources capacity can be seem as the set of memory banks, and processes are the data structures to be allocated. The conflicts between the processes of a services are represented by conflicts between data structures. However, there are three major differences between the problem addressed in this paper and the machine reassignment problem proposed by ROADEF/EURO in 2012. The first one is that in the machine reassignment problem, the processes of a service have to be executed on distinct machines whereas no such limitations exist in our problem. The second one is the cost computation, especially the balance cost, that is very specific to machine reassignment problem, and that differs significantly from the cost computation for the problem addressed in this paper. Finally, the more significant difference is that the problem addressed in this paper is dynamic. As the application requirements vary over time, a solution is a series of allocations, where each allocation is computed by taking present and future requirements into account.A relevant problem of data allocation is addressed in [2,12], it consists in finding the optimal allocation of relations to multidisk database such that the expected query cost is minimized. Each query involves two relations and has a probability of occurrence. The register allocation problem [10] is close to the simple version of the memory allocation problem tackled in [21]. This problem consists in assigning variables in a computer program to hardware registers in a processor. The objective is to minimize the number of used registers considering that some variables cannot be assigned to the same register. In these problems, the relations and variables can be modeled as the data structures. Disks and registers can be represented as the memory banks. However, the main differences between these problems and our problem are the external memory available and the objective functions. Also, these problems relate to static memory allocation, whereas this paper addresses the dynamic memory allocation.This work is an extension of the general work conducted by Marı́a Soto during her PhD thesis [17]. Related problems are covered in [19–21,23].We have addressed various simpler versions of the memory allocation problem: in Soto et al. [22], we have proposed a mixed integer linear programming formulation for the general memory allocation problem and a variable neighborhood search (VNS) algorithm [13]. We have also studied an even more simplified version of this problem in Soto et al. [21].We have dealt with the dynamic memory allocation problem in Soto et al. [23] for which an integer linear programming formulation and two iterative approaches have been devised to address this dynamic problem. In this paper, we propose a new iterative approach which combines these two iterative approaches. Similar studies with iterative approaches but in a completely different domain have also been conducted on another problem in [24].This paper is organized as follows. Section 2 provides a short introduction to dynamic memory allocation problem. Section 3 briefly recalls the two previous iterative approaches and introduces the new method. The proposed approach is compared to previous works in Section 4, and computational results are also discussed. Finally, Section 5 presents conclusions and future work on this problem.This section presents the dynamic memory allocation problem. Readers interested in more details about this problem, about the ILP formulation and about theNP-hard status of this problem are referred to Soto et al. [23]. The considered memory architecture is similar to the one of a TI C6201 device [9]. It is composed of m memory banks whose capacity is cjkilo Bytes (kB) for all j∈{1,…,m} and an external memory denoted by m+1, which does not have a practical capacity limit. The processor needs q milliseconds for accessing data structures located in a memory bank, and it spends an access time which is p times longer when data structures are in the external memory.Time is split into T time intervals whose durations may be different. The application to be implemented is assumed to be given as C source code, whose n data structures (i.e. variables, arrays, structures) have to be loaded in memory banks or external memory. The size of data structure sifor i∈{1,…,n} is expressed in kB. During each time interval It, the application requires accessing a given subset Atof its data structures. Thus ei,tdenotes the number of times that data structure si∈Atis accessed during time interval It, for all t∈{1,…,T}. The processor can access all its memory bank simultaneously, which allows for parallel data loading. Thus, two data structures can be loaded in parallel provided that they are allocated to two different memory banks. Two data structures are said to be conflicting whenever they are required at the same time by the processor. Each conflict has a cost, which is equal to the number of times the conflict appears in the current time interval. This cost might be non-integer if the application source code has been analyzed by a statistic-based code-profiling software [8,11]. Thus, the number of conflicts in Itis denoted by ot, and dk,tis the cost of conflict (k,t)=(k1,k2) during time interval Itfor all k in {1,…,ot}, k1 and k2 in At, and t in {1,…,T}.A conflict between two data structures is said to be closed if both data structures are allocated to two different memory banks. In any other case, the conflict is said to be open and its cost has to be paid.Initially, all data structures are in the external memory and memory banks are empty. The time required for moving a data structure from the external memory to a memory bank (and vice versa) is v ms/kB. The time required for moving a data structure from a memory bank to another is ℓ ms/kB. We suppose that v⩾ℓ and v<p because a DMA (Direct Memory Access) controller is part of the memory architecture, which allows for a direct access to data structures.The cost of an allocation is expressed in milliseconds and is the sum of two terms. The first term is the access cost for executing operations in the application which also includes the cost of conflicts cost of all the data structures that are in a memory bank, plus the access cost of all the data structures allocated to the external memory minus the closed conflict cost. The second term is the cost of changing the allocation of structures from a time interval to the next one.The dynamic memory allocation problem consists in allocating a memory bank or the external memory to any data structure of the application for each time interval, so as to minimize the time spent accessing and moving data structures while satisfying the memory bank capacity.In Soto et al. [23], we have introduced two heuristic iterative approaches which iteratively build a solution. They rely on addressing static memory allocation problems. The first approach called Long-term addresses the MemExplorer problem (ME) [22] which consists in finding a memory allocation for data structures such that the time spent accessing these data is minimized, for a given number of capacitated memory banks and an external memory. The second approach named Short-term addresses the MemExplorer-Prime problem (ME′)[23] which is similar to MemExplorer but that considers an initial memory allocation for data structures. Thus, in addition to minimizing the time spent accessing data, the cost of changing the allocation of these data is also minimized. Table 1points out the differences between these two problems in terms of inputs, outputs, constraints and objective function.In [23] and [22], these two static problems have been tackled using a Variable Neighborhood Search-based approach hybridized with a Tabu Search. The VNS starts with greedy and random solutions and explores two different neighborhoods. The first neighborhood swaps data structures in memory banks provided that memory banks capacity are not exceeded, whereas the second one performs new allocations which may lead to exceed the memory bank capacity (in that case the allocation is repaired afterward). The Tabu Search procedure uses the same neighborhoods, with a dynamic tabu list whose tenure is smartly adapted during the search.For each time interval t, Long-term builds a preliminary allocation Ptcalled the seed allocation. The allocation for the time interval t, denoted by Xtis built upon that seed allocation. Sections 3.1 and 3.2 present two ways of building a solution based on the seed allocations at each iteration. The present paragraph focuses on the construction of the seed solution Ptat time interval t.The seed allocation is selected among two candidate allocations. The first one is the seed allocation for the previous time interval Pt−1. The second candidate is the allocation found by solving MemExplorer on an instance built by gathering all the data structures, number of accesses to data, conflicts and cost involved from the current interval t to the last one. The total cost of both candidate allocations is then computed. This cost is the sum of two sub-costs. The first sub-cost is the cost that we would have if the candidate allocation was applied from the current time interval to the last one. The second sub-cost is the cost that should be paid for changing the memory mapping from the allocation of the previous time interval to the considered candidate allocation. The candidate allocation associated with the minimum total cost is selected as the seed allocation, and referred to as Pt.Short-term builds an allocation for a time interval by solving MemExplorer-Prime on an instance built by gathering all the data structures, number of accesses to data, conflicts and costs involved only in the current time interval, and also by considering the allocation in the previous time interval.Long-term takes into account the application’s requirements (data structures, number of accesses to data, conflicts and costs) for the current and future time intervals.We now explain why Short-term and Long-term produce solutions that tend to have specific features. When the capacity of memory banks is large, Long-term tends to avoid moving data structures from a time interval to the next one. Indeed, for all t, a good seed allocation found at the first time intervals has a high probability to reach a total cost lesser than the other candidate allocation, and to be selected as a seed allocation for the next intervals. This is partly due to the fact that the changing cost of such an allocation is zero, as memory bank capacity allows for keeping the same allocation for all time intervals.By contrast, Short-term minimizes the cost incurred by accessing data structures and the cost for changing the current allocation of data structures from a time interval to the next one. In general, the cost for moving data structures involved in a conflict is much less than the cost to be paid if the conflict is open. Thus, Short-term is prone to change the allocation of data structures from a time interval to the next one, but often at the expense of global solution quality.Since Long-term has the advantage of stability over time, and Short-term has the advantage of flexibility, we are interested in a mid-term approach that combines the benefits of both approaches. We propose two mid-term approaches, the first one uses a rolling horizon and the other one weights the time interval requirements. These approaches are described below.Long-term solves MemExplorer for all time intervals and Short-term addresses MemExplorer-Prime only once. Therefore, the mid-term approach should solve MemExplorer and MemExplorer-Prime for an intermediate number of times intervals. Based on this idea, the first version of mid-term approach uses a rolling horizon of length h. Algorithm 1 presents Mid-term with a rolling horizon which proceeds as follows. An allocation of data structures to memory banks has to be set for each time interval sequentially from the first time interval to the last one. As in Long term, the allocation at time interval t is based on a seed allocation (Pt) which is the best one out of three allocations: the seed allocation of the previous time interval (Pt−1), the allocation of MemExplorer for the following h time intervals (Mt), and the allocation of MemExplorer-Prime for the same h time intervals with the allocation of the previous interval as initial allocation(Mt′).A complete solution is represented by a matrix X, which is composed of T+1 vectors Xtfor all t∈{0,…,T}. Vector Xtis the memory allocation at time interval t. Thus element xtiof vector Xtrepresents the allocation of data structure i at time interval t. Element ptiof vector Ptrepresents the seed allocation of data structure i at time interval t. Initially, all data structures are allocated to the external memory, hence x0i=m+1 for all i∈{1,…,n} (Algorithm 1 – line 3). As mentioned before, the seed allocation in the time interval t is denoted by Pt. The initial seed allocation P0 is the initial memory allocation X0 (line 4).For each time interval t, this algorithm first generates three candidate allocations. The first one is the seed allocation Pt−1 used to build the memory allocation of the previous time interval Xt−1 (line 7). The second one, Mtis the allocation obtained by solving MemExplorer for data structures, number of accesses to data, conflicts and access costs involved from the time interval t to time interval t+h−1 (line 8). The last candidate allocation,Mt′is generated by MemExplorer-Prime for h time intervals using the memory mapping of the previous time interval Xt−1, as the initial allocation. Allocation M′ttakes into account the transitional cost for changing the allocation of data structures from the allocation of the previous time interval and the access cost (line 9).For each of the three candidate allocations, the algorithm computes the total cost which is the sum of two sub-costs. The first one is the access cost that we would have if the candidate allocation was applied from the current time interval t to the last one. The second sub-cost is the transitional cost for moving data structures from time interval t−1 to t that should be paid if the candidate allocation would be applied (line 11).The selected allocation is the one whose total cost is minimum (line 12). The allocation Xtfor time interval t is initialized to the seed allocation Pt(line 13); then, the data structures that are not required until the current time interval are allocated to the external memory.Long-term is not bound to change data structures allocation, because future and present requirements are considered equally important. In particular, a very high cost conflict occurring in a quite distant future will have a significant impact on all the allocations to be computed before its actual occurrence. On the contrary, Short-term is prone to move data structures in order to find an allocation that only minimizes the cost of moving data structures and access cost incurred by the current time interval. This is of course done at the expense of anticipation of future requirements that are totally ignored, and it can be observed that this approach produces too many moves.To overcome these drawbacks, we propose another mid-term approach which attempts to balance the number of moves between two successive time intervals. We have then weighted the requirements of each time interval. Future requirements are less and less weighted as they are far away from the current time interval. This mid-term approach considers future requirements but giving more importance to the one appearing in the nearest future. Thus, we avoid the drawbacks of Long-term while taking advantage of the flexible nature of Short-term. Algorithm 2 shows Mid-term with weighted future requirements.First, the decay rate α∈[0,1] has to be set. At time interval t, the cost of all the conflicts that arise from time interval t+1 are multiplied by α, those arising from time interval t+2 are multiplied by α2, those arising from time interval t+k are multiplied by αk, until the end of the horizon. Long-term corresponds to α=1, whereas Short-term can be computed with α=0.Numerical experiments suggest that the best trade off between these two extreme strategies can be found based on the ratio between memory bank capacity and the size of data structures, but it also depends on p, ℓ and v. The decay rate is then empirically defined by:(1)α=1if∑j=1mcj∑i=1nsi⩾1min1,vp·ℓ·∑j=1mcj∑i=1nsiotherwiseWhenever the memory bank capacity is large enough to accommodate all data structures, Long-term tends to be very efficient as very few modifications from a time interval to the next one may be needed. Then, if the ratio∑j=1mcj∑i=1nsiincreases, then a large α is suitable. In addition, a high cost for moving data structures also leads to favor steady assignments for data structures. Consequently, a largevℓleads to high values for α. However, a large penalty cost p makes it more and more costly to access data structures that are in the external memory. Then, if memory bank capacity is modest, it might be profitable to move the data structures that are required at each time interval in the memory banks, hence favoring flexibility. So, if p is large, then a smaller decay rate α should be desirable to reach high-quality solutions.Initialization is the same as in the Mid-term approach with a rolling horizon (Algorithm 2 – line 3–4), and a time interval allocation is generated as in Algorithm 1.At each time interval t, Algorithm 2 generates three candidate allocations (line 7–9). The first one is the seed allocation Pt−1 used to build the memory allocation of the previous time interval Xt−1 (line 7). The second one, Mtis the allocation obtained by solving MemExplorer by weighting the requirements for the current time interval t to the last one as explained above (line 8). The cost of conflicts occurring at time interval t are multiplied by αr−tfor all r∈{t,…,T}. Hence, the future requirements are less and less weighted as they are far away from the current time interval. The last candidate allocation, M′tis the allocation found by MemExplorer-Prime weighting the requirements and using the memory mapping of the previous time interval Xt−1 as the initial allocation (line 9). The rest of the algorithm is similar to Algorithm 1.

@&#CONCLUSIONS@&#
