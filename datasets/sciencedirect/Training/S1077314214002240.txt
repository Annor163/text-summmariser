@&#MAIN-TITLE@&#
On performance limits of image segmentation algorithms

@&#HIGHLIGHTS@&#
We formulate image segmentation as a statistical parameter estimation problem.A modified Cramér–Rao bound is used to determine segmentation performance limit.The bound is based on the biased estimator assumption and Affine bias model.Fuzzy segmentation formulation is used, where hard segmentation is a special case.Synthetic and real-world images are experimented to show the accuracy of the bound.

@&#KEYPHRASES@&#
Image segmentation,Cramér–Rao bound,Affine bias model,Statistical parameter estimation,

@&#ABSTRACT@&#
Image segmentation is a very important step in image analysis, and performance evaluation of segmentation algorithms plays a key role both in developing efficient algorithms and in selecting suitable methods for the given tasks. Although a number of publications have appeared on segmentation methodology and segmentation performance evaluation, little attention has been given to statistically bounding the performance of image segmentation algorithms. In this paper, to determine the performance limits of image segmentation algorithms, a modified Cramér–Rao bound combined with the Affine bias model is employed. A fuzzy segmentation formulation is considered, of which hard segmentation is a special case. Experimental results are obtained where we compare the performance of several representative image segmentation algorithms with the derived bound on both synthetic and real-world image data.

@&#INTRODUCTION@&#
Image segmentation plays a critical role in image analysis. It subdivides an image into its constituent parts in order to extract information regarding objects of interest, and has an impact on all the subsequent image analysis tasks, such as object classification and scene interpretation [1]. Image segmentation is a challenging problem in computer vision, and a wide variety of methodologies for it have been presented. Surveys of image segmentation techniques can be found in [1,2]. Based on the image information being employed for the segmentation task, image segmentation algorithms can be classified into four categories: region-based segmentation [3–9], boundary- or edge-based segmentation [10–16], methods combining both region and boundary (edge) information [17–23], and thresholding (multi-thresholding) methods [24–28].While the development of efficient segmentation algorithms is highly desirable, the assessment of their performance is also very important. There are basically three groups of methods for segmentation evaluation [1]. Surveys of the evaluation techniques for image segmentation can be found in [1,29,30], where one can observe that much progress has been made recently in evaluating the segmentation results, but performance of such methods tends to vary as widely as the techniques themselves. As a result, the performance of the evaluation methods is far from being satisfactory. In [1], the authors listed some of the factors which limit the advancement of evaluation methods and, in turn, the performance improvement of segmentation algorithms. These factors include a lack of common mathematical models or general strategy for evaluation, the challenges in defining wide-ranging performance metrics and statistics, the difficulties in defining the ground truth, large costs in performing comprehensive evaluations and the fact that the testing data are not representative enough for actual applications.We note that given a specific image, among all the factors possibly affecting the performance assessment of segmentation algorithms, the most important factor is the image content. Therefore, an investigation of the performance bound, which is only associated with the available image data and is independent of the segmentation algorithms, will be very helpful to evaluate the efficiency of image segmentation techniques. A tight performance bound can tell us what the best achievable performance of any image segmentation algorithm is for the specific image content. Thus, performance bounds can serve as benchmarks for the image dataset and segmentation algorithms. They can also be used to study how the image content or image preprocessing operations affect segmentation performance. The gap between the actual segmentation error of an approach and a tight bound can provide us with the efficiency of that segmentation approach and available room for improvement.There do exist efforts on bounding the segmentation performance from a statistical perspective. The work in [31] was based on the finite normal mixture (FNM) model assumption, where the model parameters, means and variances, were estimated using Expectation–Maximization (EM) and Classification-Maximization (CM) algorithms. Cramér–Rao bounds (CRB) on the variances of these estimates were derived. However, the use of the FNM model may limit the application of the presented bound to many image segmentation scenarios where FNM does not represent the image contents accurately. Also, the unbiased estimator assumption made in [31] does not hold for many real-world segmentation algorithms, which will be seen in our experimental results. While studying multi-spectral image segmentation [65], the performance of the Markov random fields (MRFs)-based segmentation algorithms was predicted using false alarm rate which was based on Rissanen’s minimum description length (MDL) criterion. The analysis in [65] covered many detailed scenarios of segmentation, which, however, was mainly focused on the MRF image segmentation criteria. Besides, the computational complexity and the use of multi-spectral image data are additional factors which constrain its application.In this paper, we formulate image segmentation as a statistical parameter estimation problem and derive the CRB on the performance measure, namely on the mean square error (MSE) of the resulting pixel labels, based on the biased estimator assumption [32] and Affine bias model [47]. Here, by “biased estimator” we mean that the expectation of the segmentation result, i.e., the pixel label values, is not equal to the true pixel label values. In addition, while deriving the bound, an approximation is made when computing the expectation of the inverse Fisher information matrix to reduce the computational burden. Bootstrapping technique and empirical approximation to the second-order statistics are employed to overcome the difficulty that the probability distribution of the images is unknown. Our final goal is to derive a tight performance bound for the image segmentation problem and compare the bound with the performance of various segmentation algorithms when applied to different image datasets. The effect of the factors, such as the intensity contrast in an image on the segmentation result, are investigated via the bound, which give us insights into the achievable accuracy of a segmentation algorithm in segmenting a specific image.This paper is structured as follows. In Section 2, the image segmentation problem is shown to fit the varying coefficient model (VCM) [33] and image segmentation is formulated as a parameter estimation problem. In order to derive the biased bound in Section 4, the CRB based on the unbiased estimator assumption is discussed in Section 3 as a necessary intermediate step. In Section 4, the biased bound and the optimum parameters for the Affine bias model are determined, where the methods used to calculate the bound are also discussed. In Section 5, the derived biased bound is compared with several representative image segmentation algorithms using synthetic and real-world image data. We also show in Section 5 the comparison of these segmentation algorithms with the unbiased bound, and demonstrate the unsuitability of the unbiasedness assumption. Concluding remarks and suggestions for future work are provided in Section 6.The fundamental question asked in this paper is whether there exists a theoretical limit to image segmentation performance and, more importantly, how much room do we have to improve the existing algorithms. In this section, as a first step to attempt to answer this question, we model the image segmentation problem as a linear estimation problem using a VCM, where the parameters of interest, i.e., the pixel labels indicating which region a pixel belongs to, are considered to be the coefficients of the VCM.Consider a random variable s whose distribution is dependent on a parameter η. In the VCM, η can be expressed as(1)η=F0+h1F1(χ1)+⋯+hMFM(χM),where h1, h2, …, hMand χ1, χ2, …, χMare known as the predictors for η, and F1, F2, …, FMare functions that enable the representation of η. F0 is the intercept term. Thus, the model is linear in the regressors, while their coefficients are allowed to change smoothly with the value of other variables which are called “effect modifiers”. η is called the linear predictor, which is related to the mean Λ=E{s} via the link function η=κ(Λ). In the simplest case of the Gaussian model, κ(Λ)=Λ and the data s is normally distributed with mean η, and model (1) has the form(2)s=h1F1(χ1)+⋯+hMFM(χM)+ε,where E{ε}=0, var(ε)=σε2. A special case occurs when χk’s are the same variable, such as time, age or pixel coordinates (or indices) as used in our work. There are many ways to model the functions Fi(χi). For example, we could use flexible parametric representations, such as piecewise polynomials or otherwise and more generally nonparametric functions. In our work, the B-spline functions (tensor product B-splines) are employed.In this subsection, we model the image segmentation problem using VCM. Suppose we have an image with N pixels whose observed intensity values are y(x), where x are pixel indices and ordered through zig-scanning, starting from the top-left to bottom-right in an image, and x=1, 2, …, N. The image segmentation problem can be formulated, based on the Gaussian model (2), as(3)y(x)=s(x)+w′(x)=h1(x)F1(x)+⋯+hM(x)FM(x)+w(x),where s(x) are the noise-free intensity values of the pixel with the index x. The noise term w(x) consists of two parts, the image noise w′(x) and the fitting error ε. This model has the signal effect modifying variable x, and M is the number of basic regions in an image, and M⩽N. Fi(x) denotes the intensity of the ith basic region at pixel x. The term, basic region, used in this paper represents the fundamental constituent element of an image, which cannot be further decomposed into any other types of regions. From (3), we can see that the intensity of a pixel x, e.g., y(x), includes the contributions of intensities from one or multiple basic regions at pixel x. In (3), hi(x) is the label of pixel x, which can be interpreted as the membership function, representing the degree to which the pixel x belongs to the ith basic region, 0⩽hi(x)⩽1 and∑i=1Mhi(x)=1for every x. In the rest of the paper, the terms “label value” and “membership function value” will be used interchangeably.The above definition enables the model to represent a general image segmentation scenario, i.e., fuzzy segmentation [34] where each pixel can belong to different basic regions at the same time. As a special case of fuzzy segmentation, a pixel in hard or crisp segmentation has the membership function hi(x)∈{0, 1}, that is, its intensity is only contributed by one of the basic regions. In addition to providing a more general formulation, another important reason to study fuzzy segmentation is that the CRB fails to bound the MSE if the space of a parameter becomes finite [35], i.e., the hard segmentation case. We will show an illustrative example in Fig. 1to explain the model (3) in more details.From the above definitions, we can see that the basic regions can be overlapping but the segmented regions, i.e., fuzzy or hard regions obtained after segmentation is carried out, may not. The segmented regions might be different from the basic regions. The method used to calculate the bound in this paper is based on the basic regions, not on the segmented regions, in that we will discuss the estimation of the pixel labels hi(x) for each basic region.Going beyond the above definitions given in the image processing context, we notice that (3) is actually a very general model, which may be applicable to several related problems, such as unmixing [36], blind source separation [37] and image matting [38]. In this paper, we focus on the discussion of (3) in the image segmentation context.We assume that we have a powerful algorithm and the fitting error is very small compared with the additive noise, so the image noise dominates the noise term, i.e., w(x)=ε+w′(x)≈w′(x). (To some extent, the fitting error might be controllable by varying the factors such as fitting functions and knot deployment. This assumption has been justified by our experiments.) In this work, the noise is considered to be independent and identically distributed (i.i.d.) Gaussian random variable with zero mean and variance σ2. We have investigated the distributions of a large number of real-world images, and found that many of them satisfy the Gaussian assumption and, therefore, the bound derived from this assumption is applicable to them. When the Gaussian assumption is not valid, it is not straightforward to find the bound but the bound based on the Gaussian assumption is still a useful bound albeit not a tight bound due to the fact that the Gaussian distribution has the maximum entropy among all distributions with the same mean and variance. Thus, the bound obtained here based on the Gaussian assumption is still valuable under non-Gaussian distributions. In our work, Fi(x) is modeled using the 2D B-spline function with the coefficient vectorβi. Let Fi(x)=φ(x;βi) represent the intensity of the pixel x in the ith basic region, andφ(x;βi)=∑l=1mβilςl(x), where ςl(x) are B-spline basis functions and m is the number of knots in an image and l is the index of the knots which are ordered through zig-scanning starting from the top-left to bottom-right in an image. For simplicity, the knots in our work are uniformly deployed on the entire image plane.Thus, (3) can be written in a matrix form as(4)y(x)=h(x)T·φ(x;B)+w(x)=h(x)T·B·ς(x)+w(x),where T denotes the matrix transpose,h(x)=[h1(x), h2 (x), …, hM(x)]T, φ(x;B)=[φ (x;β1), φ(x;β2), …, φ (x;βi), …, φ(x;βM)]TandB=[β1T,β2T,…,βMT]T. Here,φ(x;βi)=βiT·ς(x), where βi=[βi1, βi2, …, βim]T,ς(x)=[ς1(x), ς2(x), …, ςm(x)]Tand i=1, 2, …, M.Fig. 1 shows an illustrative example of the models described in (3) and (4), where an image consists of two basic regions. These two basic regions are composed of two hard segmented regions and one fuzzy segmented region. For simplicity, we assume that the pixel intensities in the two basic regions are constant, and the two constants are different from each other. Also, there is no noise.In Fig. 1(a), the left-most image is the original one consisting of two basic regions, Basic Region 1 and Basic Region 2, marked by the oblique lines with different directions. The rectangle, marked by the cross lines, in the middle of this image is the overlapping area of the two basic regions, which represents the fuzzy segmented region. It includes the information from the two basic regions and is formed by weighing the pixel intensities of the two basic regions with different membership values. The two rectangles at the two sides of the fuzzy region in the original image represent two hard segmented regions. Conceptually, this original image can be considered as the weighted summation of two layers. The two images on the right hand side of the equality sign in Fig. 1(a) are the two layers corresponding to Basic Region 1 and Basic Region 2, respectively. More specifically, the layer corresponding to Basic Region 1 is formed by expanding the size of the Basic Region 1 to be the same as that of the original image, keeping the original pixel intensities of the Basic Region 1 fixed, and filling out the extended area with zero intensity values. The dark areas in the two layers consist of the pixels with the zero intensity values. The layer corresponding to Basic Region 2 is constructed in a similar manner.Given that a hard region is a special case of a fuzzy region, Fig. 1(a) can also be considered as being composed of two fuzzy regions (with membership values 0 and 1, respectively) and a transition region. In the transition region, the membership values of a pixel are between 0 and 1. For simplicity, in the rest of this paper, we will still call the special cases of fuzzy regions, where the membership value is 0 or 1, the hard regions, and call the regions, with membership values between 0 and 1, the fuzzy regions.A 2-D image can be considered as a 3-D object in Euclidean space, where two orthogonal axes form the pixel coordinate plane and the third axis corresponds to pixel intensity. In this paper, we index the pixel using a scalar x, instead of a 2-D coordinate vector, and denote the intensity of pixel x with y(x). Based on the 3-D abstraction, Fig. 1(b) shows the transverse surface along the dot-dashed horizontal line in the middle of the original image in Fig. 1(a). Equivalently, the original image can be considered as the aerial view of Fig. 1(b). The pixels x1 and x3 are in the two hard segmented regions with the intensities of y(x1) and y(x3), respectively. x2 denotes the index of a pixel in the fuzzy segmented region with the intensity of y(x2). Fig. 1(c) denotes the knot deployment of the layer corresponding to Basic Region 2. y(x2) is the weighted summation of the pixel intensities of Basic Regions 1 and 2. As mentioned before, we represent the image using the fitting coefficientsB, instead of using the original pixel intensity information. Formally, we havey(x1)=h1(x1)β1Tς(x1),y(x3)=h2(x3)β2Tς(x3)andy(x2)=h1(x2)β1Tς(x2)+h2(x2)β2Tς(x2).β1Tς(x2)andβ2Tς(x2)are determined by the two layers corresponding to Basic Regions 1 and 2, respectively. Here, we consider w(x) to be zero. In this illustrative example,h(x1)=[h1(x1), h2(x1)]T=[1, 0]T,h(x2)=[h1 (x2), h2(x2)]T=[0.4, 0.6]T, andh(x3)=[h1(x3), h2 (x3)]T=[0, 1]T.We notice that a similar formulation has been used in [39,40] for developing image segmentation algorithms. In [39,40], hi(x) is considered to be equal to or very close to 0 or 1, that is, hard segmentation, while in our formulation we consider a more general segmentation configuration, i.e., fuzzy segmentation where hi(x) lies in [0, 1]. Also, in our work we consider the basic regions to be different from the segmented regions, while there is no such a distinction in [39,40]. Additionally, in [39,40] it was argued that the pixel label, with the given Gibbsian distribution as the prior, is independent of the image content represented byB. In contrast, we do not make any assumptions on the dependence or the prior distribution.There are several advantages to represent the image using the fitting coefficientsB, instead of the original pixel intensity information: (i) we can denote basic regions with various shapes and sizes, i.e., different number of pixels, using a “uniform” representation, i.e., the basisς(x) and the fitting coefficientsβiwith known or controllable dimensions. Thus, the segmentation problem can be conveniently represented by linear models, like VCM, and the analysis can be simplified; (ii) fitting can reduce the impact of a small number of pixels with large difference in intensity from their neighboring pixels, i.e., outliers, so as to enhance the homogeneity of the image regions. It is also helpful in reducing the possibility of yielding regions with very small size, i.e., region with very few pixels; (iii) spatially varying intensity and interactions between the neighboring image areas can be taken into consideration by the fitted representation to some extent; (iv) the fitting procedure can represent the image content using much smaller number of coefficients compared with the number of original image pixels, and, therefore, simplifies the computation.From (4), we can see that there are two sets of parametersh(x) andBin the model, but we are only interested in the estimation ofh(x). We packh(x) into a large vectorhand obtainh=[h1(1), h2(1), …, hM(1), h1(2), h2(2), …, hM(2), …, h1(N), h2(N), …, hM(N)]T. In this paper, we assume that the segmentation algorithms are biased estimators, that is, the output,hˆ(x), of a segmentation algorithm is a biased estimator of the true pixel labelh(x). More details about this assumption as well as its justification are provided in Section 4 and Appendix B.In this section, we derive the Fisher information matrix and the Cramér–Rao bound under the unbiased estimator assumption. For an estimation problem with two unknown parameters, likehandBin our work, one parameter, say,h, can be considered to be the wanted parameter and the other one,B, can be considered as the unwanted one. Both of them are assumed to be random. Based on this formulation, the performance of four variations of the Bayesian bound for estimating the wanted parameter were compared in [41,42]. However, determination of all of the bounds requires either the computation of derivatives and expectation over the joint probability distributions of the observationyand the wanted parameter, or the observation and the whole parameter set, i.e., p(y,h) or p(y,h,B), which is a very challenging task given the variety of image contents. Herey=[y(1), …, y(N)]T.In our work, we assumehandBto be random so as to find a bound with reasonable complexity. We first determine the conditional CRB givenhandB, and then find the expectation of the conditional bound with respect tohandBto obtain the global one. We will see during the computation of expectation that it is not necessary to determine the joint probability p(h,B) and even to consider the potential dependence betweenhandB. Admittedly, the CRB is not a stretch bound for low SNRs [41]. In these situations, other bounds, like Barankin bound [60], could perform better. In this paper, we mainly focus on the discussion of CRB and its applications to the image segmentation problems in general. The Fisher information matrix is first derived in Subsection 3.1, which forms the basis of the CRB for the unbiased estimator (Subsection 3.2) and the biased estimator (Section 4).In this subsection, we derive the Fisher information matrix conditioned onhandB, and propose a scheme to deal with the singularity of the matrix which may exist in the image segmentation scenario. Assume that the noise w(x) is i.i.d. Gaussian random variable with zero mean and variance σ2, and the observed pixel intensity is also i.i.d. given the membership functionhand the fitting coefficientB. Then the conditional pdf of the observation is(5)p(y|h,B)=12πσ2Nexp-∑x=1N[y(x)-h(x)T·B·ς(x)]22σ2,and the Fisher information matrix is(6)JF(h)=1σ2blkdiagϖ1,ϖ2,…,ϖN.Here, blkdiag denotes a block diagonal matrix, where ϖi=B·ς(i)(B·ς(i))Tand i=1, 2, …, N. The detailed derivation is provided in Appendix A.We notice from (6) thatJF(h) could be singular. This can be verified by multiplying the first row ofJF(h) byβ2Tς(1)and the second row byβ1Tς(1), and the resulting two rows will be equivalent to each other. The reason behind it is that the dimension ofhis usually higher than the available observationy, especially for the case of single image segmentation, which can be seen more clearly from (4).We need a non-singular matrix in order to compute the inverse of the Fisher information matrix. To achieve this goal, we transform the multiple basic region segmentation problem, where M>2, to a binary basic region segmentation problem, i.e., M=2, by maintaining the information regarding the basic region of interest, say, the ith basic region, and by considering the remaining regions as a single “super” basic region. That is, when we are studying the pixel labels for the ith basic region, the membership functions and the fitting coefficients corresponding to the ith basic region remain fixed, and the rest of the basic regions are merged to form a “super” basic region whose membership functions and the fitting coefficients are recalculated based on the image contents of the “super” basic region. Thus, the segmentation model (4) can be written as(7)y(x)=h(x)T·B·ς(x)+w(x)=hi(x)·βiT·ς(x)+∑j=1,j≠iMhj(x)·βjT·ς(x)+w(x)=hi(x)·βiT·ς(x)+hiS(x)·βiST·ς(x)+w(x),where hi(x) andβiare the original parameters of the ith basic region, andhiS(x)andβiSTcorrespond to the “super” basic region.hiS(x)·βiST·ς(x)=∑j=1,j≠iMhj(x)·βjT·ς(x), withhiS(x)⩾0, andhi(x)+hiS(x)=1, i=1, 2, …, M.Based on (7), the Fisher information matrix ofhi=[hi(1), …, hi(N)], corresponding to the ith basic region, can be calculated as (8), by following a similar procedure as shown in Appendix A but with the “super” basic region considered.(8)JF(h)=1σ2diagυ1,υ2,…,υN.Here, diag denotes a diagonal matrix, andυj=βiTς(j)-βisTς(j)2where j=1, 2, …, N. Eq. (8) is not singular ifβiTς(x)-βisTς(x)≠0.Thus, for the case whereβiTς(x)-βisTς(x)≠0, we have(9)JF-1(hi)=σ2diag1υ1,1υ2,…,1υN.The same result can be obtained using the constrained CRB[43] with the “super” basic region scheme, where the constraint ishi(x)+hiS(x)=1.We discuss the invertibility of the Fisher information matrix in a more general case in the next subsection.In this subsection, we derive the CRB under the unbiased estimator assumption, and employ Jensen’s inequality for matrix measures [44] to simplify the expectation determination procedure, where we assume that the segmentation algorithms yield unbiased estimates of the pixel labels. Based on the formulation in Section 3.1, the unbiased bound of multiple basic region segmentation can be calculated in a region by region manner. For the ith basic region, we calculate the Fisher information matrixJF(hi) and its inverseJF-1(hi)which corresponds to the conditional bound of the covariance matrix ofhˆi. We find the expectation ofJF-1(hi)with respect tohandB, and obtain the global bound forhˆi. Repeating the procedure for all the basic regions and averaging the resulting bounds, we obtain the average unbiased bound for the entire image. In this way, we decompose the estimation problem with the dimensionality equal to MN into M sub-problems, each of which has the dimensionality N, the same size as the number of observations (the total number of pixels in an image). Therefore, we overcome the ambiguity due to the insufficient number of observations and can find unique solutions to the problem.Now, we study the bound on the covariance of the estimatehˆunder the unbiasedness assumption. The conditional covariance matrix ofhˆi, i.e.,Cov(hˆi|h,B), for the unbiased estimator can be written as(10)Cov(hˆi|h,B)=Ey|h,B{(hˆi-μˆhˆi|h,B)(hˆi-μˆhˆi|h,B)T}⩾JF-1(hi),whereμˆhˆi|h,B=E(hˆi|h,B), and the corresponding conditional boundCRBUnbiased(hˆi|h,B)is(11)CRBUnbiased(hˆi|h,B)=TrJF-1(hi)=σ2∑x=1N1βiTς(x)-βisTς(x)2,where Tr(Γ) denotes the trace of the matrix Γ.The global bound forhiis determined by finding the expectation ofCRBUnbiased(hˆi|h,B)with respect tohandB, i.e.,Eh,B{CRBUnbiased(hˆi|h,B)}. The average bound for the unbiased estimator for an individual basic region can be found by averaging the global bounds of all the basic regions, that is,(12)CRBUnbiased-Ave(hˆ)=1M∑i=1MEh,BCRBUnbiased(hˆi|h,B)=1M∑i=1MTrEBJF-1(hi),where the last equality holds sinceTrJF-1(hi)is not a function ofh.In our paper, we further averageCRBUnbiased-Ave(hˆ)over all the pixels in an image and the average pixel-level bound serves as the bound on the performance of image segmentation. Sincehi’s have the same dimensions, i.e., the number of pixels included in an image, we obtain the average pixel-level bound by dividingCRBUnbiased-Ave(hˆ)with the total number of pixels, N, in an image, which is shown in (13).(13)CRBP-Unbiased-Ave(hˆ)=1NCRBUnbiased-Ave(hˆ)=1MN∑i=1MTrEBJF-1(hi).We notice from (9) that it is not easy to find the expectation ofJF-1(hi)overB, so we employ an approximation when calculating the bound, by performing the expectation operation onJF(hi) first and then finding its inverse, i.e., (EB[JF(hi)])−1. According toTheorem 4.2(Jensen’s inequality for matrix measures) and theTracial Jensen inequalitiesin [44], we have(14)EBJF-1(hi)⩾EBJF(hi)-1andTrEBJF-1(hi)⩾Tr(EBJF(hi))-1,where(15)(EBJF(hi))-1=σ2diag1EBυ1,1EBυ2,…,1EBυN.Here, υj, as defined in (8), equals(βiTς(x)-βisTς(x))2where j=1, 2, …, N.Thus, a looser bound is found to facilitate the computation, which is called the modified CRB in this paper and is indicated by the superscript Mod. Therefore, from (13) we have(16)CRBP-Unbiased-AveMod(hˆ)=1MN∑i=1MTr(EBJF(hi))-1.We now discuss a special situation, whereEBβiTς(x)-βisTς(x)2in (15) has very small values, such that its inverse is very large. In this case, the resulting average CRB value might be large. We note that the very small values ofEBβiTς(x)-βisTς(x)2correspond to an extreme situation where two image basic regions are not distinguishable at pixel x. BecauseEBβiTς(x)-βisTς(x)2evaluates the average intensity difference between the two basic regions at pixel x (due to the expectation operation with respect toB), it reduces the effect when the two different basic regions have similar pixel intensities at x, i.e.,βiTς(x)-βisTς(x)2is very small, by making use of the intensity information of a group of pixels. Therefore, there are very few components ofEBβiTς(x)-βisTς(x)2in (15) with very small values, given that the two basic regions are reasonably separable at pixel x, which has also been verified by our experiments. Thus, in our work we simply ignore the contribution of thoseEBβiTς(x)-βisTς(x)2terms to the bound when they have very small values, i.e., we censor the outliers. This operation yields a reasonable tight bound. However, if we do not incorporate the expectation operation when calculating the bound, the performance of the resulting bound might deteriorate when the above two basic regions have similar intensities at pixel x, which can be seen in the experimental results shown in Figs. 2(c), 3(c), 4(c) and 5(c) in Section 5.From (15), we can see thatEBβiTς(x)-βisTς(x)2actually measures the square of the difference between the intensities at pixel x contributed by the basic region of interest and the “super” basic region. It indicates the interaction between different basic regions at pixel x. A smaller difference means a higher similarity between the two basic regions. This result corresponds to the image content which is more difficult to be segmented apart, and the variance of the segmentation label is larger. Here, the intensity difference evaluation is carried out by using the spline coefficients and the expectation operation, and, thus, the effect of the contribution of the neighboring pixels to the intensity at x, i.e., the correlation between neighboring pixels, is also taken into account. It is also interesting to notice that the separability of the two basic regions, which is reflected by the segmentation variance, is independent of the membership values and only related to the contrast between the intensities of the basic regions overlapping at a pixel. Additionally, a larger noise energy, i.e., larger σ2, has a bigger negative influence on the segmentation result, which corresponds to a higher value of the bound. We can see that the bound calculated from (15) and (16) is consistent with these intuitions.The bound (16) has been obtained under the unbiasedness assumption, and the calculation of it includes the determination of the expectation operation, EB[JF(hi)], which will be discussed in Subsection 4.3 and Appendix D. However, as we will see in the next section, a biased estimator is a more reasonable assumption for real-world image segmentation algorithms. Therefore, the result obtained in this section is not applicable in practice. However, it will be very useful in deriving the bound for the biased estimator case, as discussed in next section.In this section, we assume the estimator ofhto be biased, and derive the bound on the MSE of the segmentation results. We continue to consider the transformed binary segmentation problem here.From both theoretical and practical points of views, unbiased estimators do not often exist. Moreover, biased estimators often have the advantage of lower MSE over unbiased ones if they do exist [45]. MSE actually includes the tradeoff between bias and covariance. In addition, unbiased estimators tend to yield very large variance, especially for some ill-posed problems, such as image segmentation. Regularization is widely used to solve ill-posed problems and the resulting estimators are often biased [46]. Many state-of-the-art image segmentation algorithms are designed under a regularization framework, in which an objective function consisting of both a fidelity term and a penalty term is optimized, resulting in biased estimators.Following the same steps as when deriving the average bound for the unbiased estimators in the last section, we first write the expression of the conditional MSE in terms of bias and covariance,(17)E{‖hˆi-hi‖2|h,B}=‖Ψ(hi)‖2+Tr{Cov(hˆi|h,B)},whereΨ(hi)=E{hˆi|h,B}-hiis the bias vector ofhˆi.Under suitable regularity conditions [41] on P(y∣h,B), the covariance of a biased estimator ofhˆis bounded by the CRB[41](18)Cov(hˆi|h,B)⩾AJF-1(hi)AT,where(19)A=I+∂Ψ∂h,and I is the identity matrix.In our work, we assume that the behavior of the bias model can be approximated by an Affine function. The Affine model has been justified and employed to study the MSE bound for estimation problems in [47]. The details of the justification of the Affine bias assumption in image segmentation context can be found in Appendix B. Formally, we have(20)Ψ(hi)=Kihi+ui,whereKianduiare Affine parameters for the ith basic region. So, following the same steps as in the last section and considering (17)–(20), we have the conditional MSE bound,CRBBiased(hˆi|h,B), of a biased estimator forhˆias follows(21)E{‖hˆi-hi‖2|h,B}⩾CRBBiased(hˆi|h,B)=(Kihi+ui)T(Kihi+ui)+Tr(I+Ki)JF-1(hi)(I+Ki)T.Therefore, following a similar idea as in Section 3, the global MSE bound forhˆi, i.e.,CRBBiased(hˆi), is obtained by finding the expectation of the right hand side of the “larger or equal” sign in (21) with respect tohandB,(22)E{‖hˆi-hi‖2}⩾CRBBiased(hˆi)=Eh,B{CRBBiased(hˆi|h,B)}=∫(Kihi+ui)T(Kihi+ui)+Tr((I+Ki)JF-1(hi)(I+Ki)T)p(h,B)dhdB.The average MSE bound, i.e.,CRBBiased-Ave(hˆ), can be found by averaging the global bound for each basic region, and we, therefore, obtain(23)CRBBiased-Ave(hˆ)=1M∑i=1M∫(Kihi+ui)T(Kihi+ui)+Tr((I+Ki)JF-1(hi)(I+Ki)T)p(h,B)dhdB.In this subsection, we determine the optimumKi∗,ui∗of the Affine bias model which yields the minimum value of the bound in (22), that is,(24){Ki∗,ui∗}=argminKi,ui∫(Kihi+ui)T(Kihi+ui)+Tr((I+Ki)JF-1(hi)(I+Ki)T)p(h,B)dhdB.We first calculate the optimum pairKi∗,ui∗, substitute it intoCRBBiasedMod(hˆi), and thereafter we obtain the modified bound for the ith basic region(25)CRBBiasedMod(hˆi)∗=Tr(EB[JF(hi)])-1-(EB[JF(hi)])-1(EB[JF(hi)])-1+Cov(hi)-1(EB[JF(hi)])-1.Details of the above derivation for the parameters and the bound can be found in Appendix C.So the average MSE bound is(26)CRBBiased-AveMod(hˆ)=1M∑i=1MCRBBiasedMod(hˆi)∗=1M∑i=1MTr{(EB[JF(hi)])-1-(EB[JF(hi)])-1((EB[JF(hi)])-1+Cov(hi))-1(EB[JF(hi)])-1}.As before, we obtain the average pixel-level MSE bound by further averagingCRBBiased-AveMod(hˆ)with respect to the total number of pixels, N, in an image, and we have(27)CRBP-Biased-AveMod(hˆ)=1NCRBBiased-AveMod(hˆ)=1MN∑i=1MTr{(EB[JF(hi)])-1-(EB[JF(hi)])-1((EB[JF(hi)])-1+Cov(hi))-1(EB[JF(hi)])-1}.We notice from (27) that the decomposition of the terms containinghandBmakes the solution easily computable and no explicit expression of the joint probability p(h,B) is required. It also avoids the study of the dependence betweenhandB.Computation of (27) requires the determination of EB[JF(hi)] and Cov(hi) for the ith basic region. We discuss the schemes to calculate these quantities in details in Appendix D.

@&#CONCLUSIONS@&#
