@&#MAIN-TITLE@&#
The relationship between metacognitive experiences and learning: Is there a difference between digital and non-digital study media?

@&#HIGHLIGHTS@&#
Two experiments compared metacognitive accuracy for text reading across study media.Study media did not influence metacognition, learning, or metacognitive accuracy.Metacognition may be less important to “screen inferiority” than previously reported.

@&#KEYPHRASES@&#
Metacognition,Metacognitive experiences,Digital learning,Reading,Learning outcome,Judgement of learning,Prediction of performance,Confidence rating,

@&#ABSTRACT@&#
Technological development has influenced the ways in which learning and reading takes place, and a variety of technological tools now supplement and partly replace paper books. Previous studies have suggested that digital study media impair metacognitive monitoring and regulation (Ackerman & Goldsmith, 2011; Ackerman & Lauterman, 2012; Lauterman & Ackerman 2014). The aim of the current study was to explore the relationship between metacognitive experiences and learning for digital versus non-digital texts in a test situation where metacognitive experiences were assessed more broadly compared to previous studies, and where a larger number of potentially confounding factors were controlled for. Experiment 1 (N = 100) addressed the extent to which metacognitive monitoring accuracy for 4 factual texts was influenced by whether texts were presented on a paper sheet, a PC, an iPad, or a Kindle. Metacognitive experiences were measured by Predictions of Performance (PoP), Judgements of Learning (JoL), and Confidence Ratings (CR), and learning outcome was measured by recognition performance. Experiment 2 (N = 50) applied the same basic procedure, comparing a paper condition with a PC condition with the opportunity to take notes and highlight text. In both experiments, study media had no consistent effect on metacognitive calibration or resolution. The results give little support to previous claims that digital learning impairs metacognitive regulation.

@&#INTRODUCTION@&#
In today's society there is an increased use of digital equipment, with PC's and tablet devices now being used more frequently also in educational settings. This has opened up for new ways of learning, both at an individual level but also at a group level. For instance, there is currently a large interest in the development of collaborative e-learning environments and multidisciplinary learning groups (e.g., Dascalua, Bodea, Lytras, Ordoñez de Pablos, & Burlacua, 2014), and technology is also seen as an important element of knowledge management (e.g., Zhao & Ordóñez de Pablos, 2011). This development calls for more knowledge about if and to what extent cognition is influenced by digital versus non-digital presentation format (Carr, 2010). In an educational context, digitalization has resulted in an increased emphasis on students' digital competence. In parallel, there is an additional focus in today's schools on students' ability to engage in self-regulation, defined as the extent to which the learner is “metacognitively, motivationally and behaviourally active participants in their own learning process” (Zimmerman, 1986, p. 308). The combined focus on digital competence and self-regulation necessitates more knowledge about the relationship between learning and self-regulation in digital compared to traditional paper-based learning.According to the above definition, self-regulation refers to students' ability to regulate learning through metacognitive processes. From this perspective, self-regulation requires the ability to engage in metacognition, i.e., cognition about one's own cognition (Metcalfe, 2000). A distinction is often made between metacognitive monitoring, where metacognitive thoughts or feelings reflect aspects of ongoing cognitive processing, and metacognitive control, where the output of such monitoring is used to regulate cognitive processes and behaviour (Koriat, 2000, 2007). An example would be the decision to read a text once more if one felt that comprehension was low. Within such a framework, metacognitive monitoring is a prerequisite for metacognitive control and self-regulation.Self-regulation and metacognition have become central concepts in a wide variety of studies on online learning, e-learning and digital media use. For instance, a recent study by Pellas (2014) that explored student engagement and learning in a virtual reality learning environment, found that metacognitive self-regulation was one predictor of emotional and cognitive engagement. Similarly, in an experimental study looking at “preflective prompts” (i.e., a request for reflection before the learning task) in e-learning, Lehmann, Hähnlein, and Ifenthaler (2014) found that metacognitive awareness was a significant predictor of effective self-regulation.In this paper we focus on the relationship between learning and metacognitive monitoring in digital and non-digital learning contexts where the learning material is written texts. Previous research has shown that people's subjective preferences for reading texts are often in favour of paper-based rather than digital formats (Buzzetto-More, Sweat-Guy, & Elobaid, 2007; Jamali, Nicholas, & Rowlands, 2009; Spencer, 2006; Woody, Daniel, & Baker, 2010). A number of studies have also explored effects of study media on learning outcome (e.g., Mangen, Waldermo, & Brønnick, 2013) and on subjective experiences (e.g., Mangen, 2006), of which metacognitive experiences would be one example. However, to our knowledge only three studies to date have specifically compared how learning and metacognition is related in digital versus non-digital learning contexts (Ackerman & Goldsmith, 2011; Ackerman & Lauterman, 2012; Lauterman & Ackerman, 2014). Here the relationship between learning and metacognitive monitoring was measured as the degree of correspondence between memory performance and prediction of performance (PoP), reported either after the study participants had completed an entire text or at regular time intervals during text reading. Absolute monitoring accuracy, referred to as calibration bias, was calculated as the absolute difference between memory and total PoP. One of the studies (Ackerman & Goldsmith, 2011) also included a measure of relative monitoring accuracy, referred to as metacognitive resolution, which is the correlation between PoP's and recognition scores for a series of texts. The general finding in these studies was that participants in on-screen conditions showed more overconfidence than participants in on-paper conditions, interpreted as calibration bias being influenced by study media. This was found for both free and fixed study time. The only study that included a measure of metacognitive resolution, i.e., relative monitoring accuracy, found that this was not influenced by study media (Ackerman & Goldsmith, 2011). As to the question of whether study media influences learning outcome, results from the studies were mixed.If metacognitive monitoring and regulation are influenced by presentation format, this has potentially wide-ranging implications both for teaching and research. For instance, it could mean that educators should adjust their expectations of student performance depending on whether a test is conducted on screen or on paper, and also address how students' metacognitive skills in digital learning contexts can be improved. Furthermore, it may encourage researchers to include study media as a potentially relevant variable in research on study processes and metacomprehension. However, in our opinion there is reason to be cautious about drawing such inferences on the basis of the aforementioned studies alone. One reason is the relatively small total number of studies and participants, and the need to replicate the basic effect. Another reason is some potential shortcomings of the basic paradigm used. In the following, we outline each of these, and present two experiments that were specifically designed to address these concerns.One potential shortcoming of the above studies is that they only include one measure of metacognitive experiences, namely PoP. Because reading involves a wide range of cognitive activities, it is likely that a variety of different forms of metacognitive experiences may arise in conjunction with these activities both before, during, and after reading. In order to better capture possible differences in metacognitive experiences across study media and thereby increase the validity of the findings, one should therefore broaden the range of metacognitive measures applied.A related point is that only one of the studies measured metacognitive resolution. Whereas calibration bias refers to the person's ability to estimate their actual performance level, metacognitive resolution refers to the ability to discriminate between differences in memorability of individual knowledge units (Dunlosky & Metcalfe, 2009). The only measure of metacognitive resolution included in the study by Ackerman and Goldsmith (2011) was also based on PoP. Because PoP was measured either once for each text (Experiment 1) or every 5 min during reading (Experiment 2), each individual correlation was based on very few data points. This statistical limitation was also pointed out by the authors, who referred to recent criticisms of the use of gamma correlations in metacognition research (Benjamin & Diaz, 2008; Masson & Rotello, 2009). Moreover, without specifically controlling for which part of the text each PoP refers to, the degree of correspondence between PoP and performance does not necessarily reflect the relationship between metacognition and learning at the level of individual knowledge units. One possibility is to increase the number of times at which PoP is measured within a single text. A problem with this solution is that frequently measuring PoP may interfere with text reading itself and thus reduce the ecological validity of the reading situation.One should therefore look for procedures where metacognitive experiences can be measured more specifically in conjunction with different information units contained in the text, but where such measurement does not interfere with the reading process. One obvious candidate is the Judgment of Learning (JoL), which can be defined as “judgments made by participants at the end of a learning trial regarding the likelihood of remembering the acquired information on a subsequent memory test” (Koriat, 1997, p. 490). In other words, it refers to an item-specific prospective metamemory judgement (Metcalfe, 2000). What distinguishes it from other metamemory judgements (like for example Feelings of Knowing, Koriat, 1993) is that it is normally measured in the context of newly acquired knowledge rather than, for example, general semantic knowledge.Although JoL, like PoP, is a measure of the predicted accuracy of future performance, it could be argued that JoL cannot straightforwardly be applied as a measure of metacognitive experiences in text reading. This is because, unlike PoP, it is rarely rated during the learning situation itself but most often at the end of the learning session, in conjunction with the presentation of a series of memory items. However, the focus in the present study is not so much on the phenomenology of metacognitive experiences during the learning process, as on metacognitive experiences related to the text material and their relationship to learning outcome. From this perspective, it could even be an advantage to measure metacognitive experiences after rather than during text reading. This is because learning outcome mainly reflects long-term memory, whereas a metacognitive rating given during reading mainly reflects short-term memory. This point was raised by Thiede, Griffin, Wiley, and Anderson (2010), as an argument against measuring metacognitive accuracy as the relationship between a metacognitive rating given during reading and performance on a subsequent memory task.An alternative would be Confidence Ratings (CRs) conducted after participants have answered each of a series of recall/recognition questions (see, e.g., Norman & Price, 2015; for an introduction to CR measurement). In the context of memory, confidence refers to “the state of believing that a particular piece of information has been correctly retrieved from memory” (Miner & Reder, 1994). Because JoL is a prospective judgement related to a future event, whereas CR is a retrospective judgement related to a past event, JoL could be regarded as being more similar in phenomenology to PoP than is CR.In this study we have included ratings of both JoL and CR, in addition to PoP. JoL and CR were measured on a trial-by-trial basis. To avoid the problem associated with few data points for the PoP judgement, whilst also controlling for which parts of the text each PoP was related to, we measured PoP twice for each text, i.e., halfway through each text and upon completion of each text.The only digital learning condition included in the studies by Ackerman and colleagues (Ackerman & Goldsmith, 2011; Ackerman & Lauterman, 2012; Lauterman & Ackerman, 2014) involved the use of a PC. In our view, it would also be interesting to include a condition in which reading occurs on different electronic reading devices (ERDs), because these are becoming more common, also as learning tools among students (Morineau, Blanche, Tobin, & Guéguen, 2005; Rockinson-Szapkiw, Courduff, Carter, & Bennett, 2013).Existing studies looking at learning on ERDs have shown mixed results. According to some studies, learning is not influenced by whether a text is presented in the form of an e-book or a paper book (e.g., Rockinson-Szapkiw et al., 2013). Studies also report that the usefulness of ERDs stretches beyond that of PC's and paper books. For example, recent studies have reported that such devices “are a useful addition to laptops for the consumption of learning materials as well as for collaborative and social activities.” (Fischer, Smolnik, & Galletta, 2013, p. 1). However some potential limitations have also been pointed out. For example, Morineau et al. (2005) argued that due to differences in the sensori-motor properties of e-books and paper books there is a risk that e-book presence “hinders recall of assimilated information whilst the presence of the paper support tends to facilitate it” (ibid, p.329). Moreover, Fischer et al. (2013) pointed out that various ERDs need further development in order to become useful for the production of content.Even though existing research has looked at possible effects of ERDs on learning, no studies have specifically explored how it may affect the relationship between metacognitive experiences and learning outcome. It may be hypothesized that ERDs bridge the gap between on-screen and on-paper learning, in the sense that it shares some properties with the paper medium and others with the PC medium. When Ackerman and colleagues (Ackerman & Goldsmith, 2011; Ackerman & Lauterman, 2012) reported effects of study media on metacognitive calibration, with more overconfidence in a PC condition, the question arises as to whether the relationship between learning and metacognition in an ERD condition would most closely resemble the pattern observed in the paper or in the PC condition. It should be noted that different ERDs have different screen properties. For example, whereas the iPad-screen has a backlight that is comparable with a typical PC-screen, the Kindle screen has an E-ink display designed to resemble the visual properties of paper texts. Therefore, in Experiment 1 of the present study, we included two ERD study conditions (iPad and Kindle), in addition to a paper and a PC condition.As pointed out by Ackerman and Goldsmith (2011), metacognitive calibration and resolution might be contaminated by factors other than on-line monitoring of metacognitive experiences, such as more stable individual differences in prior knowledge about the topic area. More studies are therefore needed which take into account individual difference variables that may be hypothesized to influence metacognitive experiences and/or learning outcome. In the most recent study, Lauterman and Ackerman (2014) included Scholastic Aptitude Test scores as a control variable. However, a number of other candidate variables that are likely to influence performance in a reading situation also need to be controlled for. These include the person's own estimate of their degree of prior knowledge, interest in the topic, and effort while reading the text.An additional variable that may also influence learning and/or metacognition, and that may potentially also be influenced by study media, is learning strategies. Weinstein and Mayer (1986) define learning strategies as the behaviour and thoughts that take place during the learning process and that aims to promote learning. Learning strategies can be classified according to the depth of cognitive processing they involve, e.g., superficial processing strategies (e.g., memorization of information) or deeper processing strategies (e.g., organisation, elaboration and monitoring of information). The potential influence of information processing strategies on digital versus non-digital learning has been addressed by Liu (2005) and Morineau et al. (2005), who found that on-screen learning was associated with more shallow information processing, and on-paper learning with deeper information processing. As mentioned above, Ackerman and colleagues have repeatedly found that on-screen learners show more overconfidence than participants in an on-paper condition, with one potential explanation being that reading on screen may be associated with a more superficial approach. However strategy use was not directly measured in these studies. Therefore the current study included a self-report questionnaire (Anmarkrud & Bråten, 2009) that assessed these 2 forms of learning strategies in conjunction with text reading. We also included self-report measures of prior knowledge, interest in the topic, and effort while reading.When Ackerman and colleagues (Ackerman & Goldsmith, 2011; Ackerman & Lauterman, 2012; Lauterman & Ackerman, 2014) allowed their participants to take notes or mark the text while reading in order to control for the possible effect of note-taking on cognitive effort (Piolat, Olive, & Kellogg, 2005), they found that it did not interact with study media in the analyses of metacognitive resolution and calibration. However, we think that their testing and scoring procedure could be refined. First, although it was not stated explicitly in the articles, participants seem to have been given a score of one not only if they marked the text once, but also if they marked the text multiple times. The total score for each person therefore reflected only the number of texts for which note-taking and highlighting tools were used at all, and was not sensitive to differences in within-text note-taking and highlighting frequency among participants receiving a score above zero. Second, there was a difference in the number of note-taking and highlighting opportunities in the two conditions: The on-screen condition involved one extra highlighting tool (i.e., bold text) and one extra note-taking tool (i.e., inserting notes into the body text). Therefore, in Experiment 2 of the current study we gave the participants in the two conditions (PC and paper) an identical set of note-taking and highlighting tools (i.e., highlighting and commentary), and we calculated the absolute frequency, rather than the number of texts, with which each tool was used.In the current study, we conducted two experiments to explore the relationship between metacognitive experiences and recognition memory for on-paper versus on-screen learning by use of the measures described above. The difference between the two experiments were that Experiment 1 included 4 study conditions (paper, PC, iPad, Kindle) and did not include the opportunity for using note-taking and highlighting tools, whereas Experiment 2 included only 2 study conditions (paper, PC) but included the opportunity for using note-taking and highlighting tools. Such tools were not included in Experiment 1, partly due to the fact that ERDs are still relatively rare as learning tools in higher education, and that in a sample of university students some may have little experience with the use of ERD note-taking and highlighting tools.The main research question was whether the relationship between learning outcome and metacognitive experiences would be influenced by study media. Because previous results are inconsistent with respect to whether learning outcome itself will be influenced by study media, this was treated as an open question. Regardless of possible differences in learning outcome across study conditions, we predicted that participants in the on-screen conditions would show more overconfidence than participants in the on-paper condition, as measured by the difference between PoP and learning outcome (i.e., metacognitive calibration), cf. the findings by Ackerman and colleagues (Ackerman & Goldsmith, 2011; Ackerman & Lauterman, 2012; Lauterman & Ackerman, 2014). Moreover, we predicted the moment-by moment relationship between each participant's PoP and their corresponding learning outcome (i.e., metacognitive resolution) to be similar for our PoP measure and the PoP measure used by Ackerman and Goldsmith (2011). However, we predicted on-paper learners to show higher metacognitive resolution than on-screen learners when the moment-by moment relationship between each participant's metacognitive score and their learning outcome was measured using JoL. This is because JoL measured in conjunction with individual knowledge units involves a higher degree of within-participant variance and therefore avoids the statistical limitations of PoP reported previously. Due to the fact that CR is temporarily further removed from the learning situation than JoL and PoP, in combination with a lack of relevant previous studies involving retrospective metacognitive judgements, we did not draw specific hypotheses regarding the influence of study media on the relationship between recognition performance and CR.Based on the work of Liu (2005) and Morineau et al. (2005), we also predicted digital and non-digital learning to differ with respect to the involvement of shallow versus deep processing strategies.Even though it can be hypothesized that electronic reading devices may bridge the gap between on-screen and on-paper learning, there are no previous studies of metacognitive monitoring for texts presented on ERDs in comparison to PC screens and paper. Therefore the pattern of results in this particular condition was treated as an open question. The same applied to possible differences between study conditions in the use of note-taking and highlighting tools, since previous studies including such variables have applied different procedures for measurement and scoring.

@&#CONCLUSIONS@&#
