@&#MAIN-TITLE@&#
Data-driven detection and analysis of the patterns of creaky voice

@&#HIGHLIGHTS@&#
Analysis of a large volume of read and conversational speech data.Mutual-information based assessment of the features for creaky voice detection.Classification experiments showing clear improvement in performance over the state-of-the-art.Illustration of three main excitation patterns for creaky voice.

@&#KEYPHRASES@&#
Creaky voice,Vocal fry,Irregular phonation,Glottal source,

@&#ABSTRACT@&#
This paper investigates the temporal excitation patterns of creaky voice. Creaky voice is a voice quality frequently used as a phrase-boundary marker, but also as a means of portraying attitude, affective states and even social status. Consequently, the automatic detection and modelling of creaky voice may have implications for speech technology applications. The acoustic characteristics of creaky voice are, however, rather distinct from modal phonation. Further, several acoustic patterns can bring about the perception of creaky voice, thereby complicating the strategies used for its automatic detection, analysis and modelling. The present study is carried out using a variety of languages, speakers, and on both read and conversational data and involves a mutual information-based assessment of the various acoustic features proposed in the literature for detecting creaky voice. These features are then exploited in classification experiments where we achieve an appreciable improvement in detection accuracy compared to the state of the art. Both experiments clearly highlight the presence of several creaky patterns. A subsequent qualitative and quantitative analysis of the identified patterns is provided, which reveals a considerable speaker-dependent variability in the usage of these creaky patterns. We also investigate how creaky voice detection systems perform across creaky patterns.

@&#INTRODUCTION@&#
This paper presents an empirical investigation of the temporal excitation patterns related to the voice quality often referred to as creaky voice. Creaky voice is a raspy or croaking quality of the voice generally produced with a very low pitch and often with highly irregular periodicity (Laver, 1980). Creaky voice is used for a variety of functions in spoken communication and, hence, presents both an opportunity and a challenge (because of its distinctive acoustic characteristics) for speech technology.One major difficulty with studying creaky voice, and indeed voice quality in general, is the problem of reconciling the variation in the terminology used in the literature. Many studies use the terms irregular phonation (Slifka, 2006; Surana and Slifka, 2006a; Vishnubhotla and Espy-Wilson, 2006; Böhm et al., 2010) or glottalisation (Dilley et al., 1996; Redi and Shattuck-Hufnagel, 2001). However, both these terms are rather broad and indeed cover other classes of phonation (or laryngeal activity) aside from what we consider as creaky voice. In this paper, we interpret creaky voice based solely on the auditory criterion “a rough quality with the additional sensation of repeating impulses” as is done in Ishi et al. (2008a) (which is close to that in Laver, 1980). Note that the “…sensation of repeating impulses” part clearly discriminates the voice quality from harsh voice. Any speech region displaying such a quality will be treated in this paper as creaky voice. Note that such a criterion will contain both creak and creaky voice, and, hence, will not apply the discrimination of the two used in Laver (1980). The term vocal fry (perhaps used more by American researchers; Laver, 1980) is often used in the literature (Hollien and Wendahl, 1968; Ishi et al., 2008a; Wolk and Abdelli-Beruh, 2012), and is likely to corresponding closely to our working definition of creaky voice.Creaky voice has been studied in relation to various functions in speech communication, and most commonly with phrase or sentence boundary marking (Surana and Slifka, 2006; Drugman et al., 2013). Similarly creaky voice has been associated with turn-yielding in Finnish (Ogden, 2001). However, creaky voice is likely to be also implicated in a range of speech functions other than boundary marking. It has been studied in relation to hesitations in Swedish (Carlson et al., 2006), and also creaky voice-like properties have been observed as an allophonic variant of word medial oral stops (Zue and Laferriere, 1979; Crystal and House, 1988). Creaky voice has also been investigated in terms of emotion and affectively coloured speech (Yanushevskaya et al., 2005; Gobl and Ní Chasaide, 2003; Ishi et al., 2008b) and is likely to be a significant correlate of subtle variation in levels of activation and formality of the speaking setting (Kane et al., 2011). The use of creaky voice has also recently been shown to be increasingly common for young American females (Wolk and Abdelli-Beruh, 2012) and has also been linked to the portrayal of social status (Yuasa, 2010).As a consequence of its implication in these various roles, the proper handling of creaky voice, and the distinctive acoustic characteristics associated with it, has a significant importance for speech technology. For speech synthesis (see e.g. Silen et al., 2009; Drugman et al., 2012), this could result in improved naturalness for speakers who use creaky voice and also for the development of expressive speech synthesis. As it has been shown that listeners are sensitive to creaky voice in terms of recognising a speaker's identity (Böhm and Shattuck-Hufnagel, 2007), it is likely beneficial for speaker recognition systems (Espy-Wilson et al., 2006; Elliot, 2002) to exploit information to do with creaky voice. Detection of creaky voice may also benefit developments in emotion recognition and conversational analysis.Although the focus of this paper is on the acoustic characteristics of creaky voice, we include here a brief outline of some of the physiological characteristics reported as being associated with creaky voice. Laver (1980) provides one of the more comprehensive descriptions of creaky voice. Here he describes creaky voice as involving low subglottal pressure, high levels of adductive laryngeal tension (i.e. the muscular tension involved in bringing the vocal folds together) and typically involves low levels of longitudinal vocal fold tension (probably the main physiological parameter utilised for pitch variation). Edmondson and Esling (2006) provide some additional physiological evidence, in particular details on the presence of ventricular incursion. This involves the ventricular folds pushing down and covering the true vocal folds, causing an increased mass. This has the consequence of lowering the frequency of vibration and often causing secondary vibrations above the glottis (Moisik and Esling, 2011).The above described physiological settings have the effect of generating speech with rather distinct acoustic characteristics from modal voice.11We interpret modal voice, following Laver (1980), as the case of periodic vocal fold vibration, with full glottal closure and no audible frication.However, rather than displaying a single type of acoustic pattern, speech pertaining to the auditory criterion used here can involve more than one pattern. Redi and Shattuck-Hufnagel (2001), for instance (following Huber, 1988) separate four categories of glottalisation, two of which likely correspond the auditory criterion used in the present study. The first category involves a high degree of pulse-to-pulse irregularity, in both duration (jitter) and amplitude (shimmer). This is likely to be consistent with the “multi-pulsed” pattern reported in Ishi et al. (2008a, 2007). Note, however, that a high degree of irregularity in the duration and amplitudes of successive glottal pulses alone will not be sufficient to allow the “sensation of repeating impulses”. A certain proportion of the pulses will need to display durations corresponding to a very low F0, otherwise the speech will be perceived as harsh voice (Laver, 1980).The second pattern is categorised as having sustained low frequency with little or no superposition of formant oscillations between adjacent glottal pulses. Such a pattern has commonly been reported, with F0 values typically ranging from 40 to 70Hz, but at times with glottal periods as long as 100ms (Blomgren et al., 1998; Hollien and Wendahl, 1968). Previous experiments carried out by Titze (1994) found that human listeners begin to perceive individual pulses from around 70Hz. This suggests that a very low F0, below some auditory threshold in the vicinity of 70Hz, is sufficient to give the auditory criterion used in this paper, even in the absence of irregularity in duration or amplitude. What is unclear is the extent to which the two categories used in Redi and Shattuck-Hufnagel (2001) are overlapping. For instance, can the auditory criterion used here be achieved with highly irregular periodicity, even if many of the glottal pulses display a duration corresponding to significantly above 70Hz?Ishi et al. (2007) provide a further subdivision of the second pattern discussed by Redi and Shattuck-Hufnagel (2001), by identifying a single-pulse and a “double-beated” creaky pattern, both in combination with pressed phonation. Interestingly, for the “double-beated” pattern, Ishi et al. (2007) claim that the secondary pulses observed in the speech waveform are likely caused by an abrupt glottal opening, as evidenced by the electroglottographic (EGG) signal. For this pattern, they report a very short glottal open phase and a long glottal closed phase, consistent with a tense or pressed phonation type. The same authors (Ishi et al., 2010) report similar patterns in the phonation type known as “Rikimi” in Japanese. However, in that study creak is also observed within an overall laxer mode of phonation (not consistent with the definition of “Rikimi”) without the presence of excitations resulting from abrupt glottal openings.Besides the temporal patterning of creaky voice, other work has reported the longest glottal closed phase for creaky voice, compared to a range of other voice qualities (Gobl and Ní Chasaide, 1992). Furthermore, the same authors reported a negative correlation between the strength of the main glottal excitation with duration of the glottal return phase, for patterns showing a high level of diplophonia.Previous work by the present authors (Drugman et al., 2012), sought to model the characteristics of the creaky excitation in order to achieve natural rendering of creaky voice for speech synthesis. The study reported the strong presence of secondary residual peaks corresponding to an abrupt glottal opening. The two speakers analysed in that study mainly displayed creaky voice patterns corresponding to the second category in Redi and Shattuck-Hufnagel (2001), or the “double-beated” pattern in Ishi et al. (2007). The data-driven excitation model derived in Drugman et al. (2012) may therefore not be suited for the system of creaky voice involving a more irregular structure, such as for the first category patterns.There are three main aims of the current study:1To assess, via mutual information-based measures, the relevance of acoustic features related to creaky voice (proposed by the present authors, Drugman et al., 2012; Kane et al., 2013, as well as by others, Ishi et al., 2008a) for identifying creaky voice regions.To integrate these combined features within an efficient creaky voice detection system.To utilise the classification system with different feature groups to help identify the various temporal excitation patterns of creaky voice, and to analyse them both qualitatively and quantitatively.The paper is structured as follows: next the speech data are described (Section 2), followed by the acoustic features included in the analysis (Section 3), the mutual information of the various features is assessed in Section 4, with the results from the automatic detection experiment in Section 5, before an examination of the creaky patterns found in the data (Section 6) and providing a summary and a conclusion (Section 7).This section describes the various speech databases used in the present study. The speech data has been selected in order to cover a variety of factors including: read vs. conversational speech, different recording conditions and a range of languages.The first database consists of studio recorded read speech for the purpose of text-to-speech (TTS) synthesis development. 100 sentences, which were deemed to contain creaky voice, were selected from 3 corpora. The first was speech from an American male (BDL) taken from the ARCTIC database, the second was a Finnish Female (HS) and the third was a Finnish male (MV).Recordings of a male (Swe-M) and a female (Swe-F) speaker were selected from the SPONTAL corpus of Swedish conversational speech (Edlund et al., 2010). Each conversation lasted approximately 30min and audio was captured in a recording studio. Recordings were made of audio, video and motion capture, however the data in the current study is limited to just the audio recorded with a head-mounted Beyerdynamic Opus 54 cardioid which was used to obtain optimal recording quality.The conversations were recorded involving 2 American males (US-M1 and US-M2) and 2 American females (US-F1 and US-F2) engaged in natural dyadic conversations on the topic of food. The conversations, recorded in a quiet room, lasted around 10min and audio was recorded with headset microphones. Similar recordings were used in a recent sociological study on creaky voice (Yuasa, 2010).The final database of audio recordings consisted of conversation speech data of two Japanese female speakers (Jap-F1 and Jap-F2). The two engaged in a 30min conversation where they first watched some short animated films before talking carrying out the conversation. Audio was recorded on AKG C420 III PP MicroMic headset microphones wired through a BeachTek DXA-2S pre-amp connected to the video camera (Sony DCR-TRV38 Mini DV camera).It is not generally possible to obtain automatic annotation of creaky voice to the level of precision required to evaluate detection algorithms. Consequently, human annotation of creaky voice regions was carried out, closely following the procedure adopted in Ishi et al. (2008a). The binary decision on the presence of creaky voice was made based solely on the auditory criterion mentioned previously, i.e. “a rough quality with the additional sensation of repeating impulses”. Annotation was, however, guided by displays of the speech waveform, spectrogram and F0 contours. Due to the large volume of data included in the present study, the annotation was split and shared between the first two authors who both adhered to the annotation scheme described here. Note that this annotation was achieved at a very low level, in the sense that regions of creaky regions were labelled at the frame level. We therefore expect our detection algorithms to be accurate at that level.Table 1provides a summary of the speech data used in the present study. Included in the table is a column displaying the percentage of speaking time which was annotated as involving creaky voice. It can be observed that there is a strong cross-speaker variability in terms of the proportion of speech containing creaky voice. One American speaker used creaky voice in 3.6% of her speech, while another American female, recorded as part of the same database, used creaky voice in over 10% of her speech. Also included in Table 1 is the actual duration (in seconds) of creaky productions for each speaker. Note in the TTS data the Finnish sentences are considerably longer than the US English sentences. Also, in the conversational data the duration given is the time they participated in the conversation, where as the percentage of creak is determined from just those times where the person is actually speaking. For instance, speaker Jap-F2 speaks less than Jap-F1 over the course of a 30min conversation, but a larger proportion of her speaking time involved creaky phonation.This section describes the set of acoustic features relevant to creaky voice which are examined in the present study. This set consists of features developed by the present authors (Kane et al., 2013; Drugman et al., 2012) as well as features by Ishi et al. (2008a). A brief description of each of the features is given below.The first acoustic feature was originally presented in Drugman et al. (2012), and is designed to characterise the strong presence of secondary residual peaks often found in creaky voice. The block diagram (in Fig. 1) shows that two resonators are applied to the linear prediction (LP) residual. Both resonators have a centre frequency set to the speaker's mean F0, however each has different bandwidth settings. Resonator 1 is set with a bandwidth of around 1kHz and is used for providing a more robust F0 contour, even in creaky voice regions. Note that the F0 contour is derived by calculating a corrected autocorrelation function, r′(τ), from 50-ms Hanning-windowed frames of the Resonator 1 output:(1)r′(τ)=NN−τ·autoCorr(τ)where N is the window length (in samples) and τ is the number of autocorrelation lags. A correction of N/(N−τ) is applied to compensate for the decreasing properties of the autocorrelation function with increasing τ (as is used in Ishi et al., 2008a). The local fundamental period is then considered as the position of the maximum value in r′(τ) above the peak centred on τ=0.Resonator 2 is set with a bandwidth to 150Hz, and is used for calculating the H2-H1 feature. Again applying 50-ms frames Hanning-windowed frames, this time to Resonator 2 output, the amplitude spectrum is derived and the corresponding F0 value (derived from Resonator 1) is used to detect the amplitudes of the first two harmonics (H1 and H2). The difference in amplitude of these two harmonics (i.e. H2-H1) in dB is the first acoustic feature used in this study. The H2-H1 contour is smoothed using a 100-ms moving average filter to remove the effect of outliers.An illustration of the steps involved in the H2-H1 calculation process is given in Fig. 2. On left side, for modal phonation, one can observe no strong secondary peaks in the residual signal or in the Resonator 2 output (middle panel). Consequently, its amplitude spectrum (bottom panel) shows a prominent amplitude level at F0 compared to the second harmonic. Contrastingly, for a creaky voice segment (right side), strong secondary peaks can be observed in the residual signal (middle panel) which affects the Resonator 2 output, causing a greater harmonicity in the resulting amplitude spectrum (bottom panel). A combination of this increased harmonicity and a considerably lower F0 in creaky voice compared to the speaker's mean F0, imply that the amplitude of the first harmonic is considerably weaker to that of the second. Note that in this study both the H2-H1 and the F0 contour derived from Resonator 1 (named F0creak) are used in this study.One limitation of the H2-H1 parameter is that for highly irregular periodicity patterns (i.e. the category 1 (Redi and Shattuck-Hufnagel, 2001), multi-pulsed pattern (Ishi et al., 2008a)), the amplitude spectrum may not display any clear harmonics in which case the discriminating power of H2-H1 may be reduced.The second feature (Peak-Prom) was designed to avoid spectral and periodicity related measurements, and instead characterise each excitation peak in the time-domain. The Peak-Prom parameter essentially characterises the prominence of LP-residual peaks relative to its immediate neighbourhood. The output of Resonator 1 (see Fig. 1) is used, as its low-pass filtering effect makes the prominence measure more robust as opposed to measurements directly from the LP-residual. Furthermore, the large bandwidth (1kHz) ensures a rapid decay in the oscillations which facilitates the prominence measurement. Peak-Prom involves the use of a fixed, non-overlapping rectangular window whose duration is set to 30ms. This roughly corresponds to two periods at 70Hz. In this method correct polarity of the speech signal is assumed (this can be determined automatically for example using the method described in Drugman, 2013). Although with correct polarity the LP-residual displays positive peaks, the low-pass filtering effect of the resonator causes corresponding negative peaks in its output. The resonator output is inverted so that it instead displays strong positive peaks.For each frame the absolute maximum peak in the resonator output is identified and the frame is then shifted to be centred on this peak. By measuring the amplitude difference between the maximum peak (in the centre of the frame) and the next strongest peak one can obtain a parameter value which differentiates modal and creaky regions. In order to avoid selecting values in the vicinity of the main centre peak, the search for the next strongest peak is made outside a distance of 6ms on both sides of the centre of the frame. This corresponds to 40% of half the frame length which ensures that there is sufficient space for peaks to occur from neighbouring glottal pulses. A value is thus obtained for each frame producing the outputted parameter contour. This contour is then filtered with a 3-point median filter to remove misdetections due to transients in the signal.The Peak-Prom parameter calculation can be summarised in the following steps:1Apply a 30ms rectangular window to the Resonator 1 outputInvert the windowed frame to ensure positive peaksIdentify the maximum peak and shift the frame to be centred on this peakMeasure the next strongest peak outside the middle 12ms of the frame, and calculate the amplitude difference between it and the strongest peakRepeat with non-overlapping framesApply 3-point median filter to the extracted contourAn illustration of the Peak-Prom parameter is shown in Fig. 3. An example of the window (black line) is provided both in the modal voice region (first syllable) and in the creaky voice region (second syllable). The dashed red line shows the region within the window from which the second peak measurement is excluded. For the creaky voice window it can be noticed that aside from the prominent excitation peak in the centre of the frame, no other strong peaks can be observed (outside the exclusion region). As a result the Peak-Prom contour (dot-dashed green line) displays high values here. For the modal voice region, one can observe that several strong residual peaks are contained within the rectangular frame and, hence, Peak-Prom displays values close to 0. A limitation of the Peak-Prom feature is that if a given creaky voice pattern (corresponding to the category 1 (Redi and Shattuck-Hufnagel, 2001), multi-pulsed pattern (Ishi et al., 2008a)) contains a high proportion of glottal pulses with a duration significantly lower than 15ms, then the effectiveness of Peak-Prom may be reduced.Note that for the present study the features H2-H1, F0creak and Peak Prom are grouped as the KD (Kane–Drugman) features.The first features described in Ishi et al. (2008a) used in this study are the so-called Power-Peak (PwP) parameters. Note that all of the patterns from Ishi et al. (2008a) are calculated from the speech signal bandlimited to 100–1500Hz. A ‘very short-term’ power contour is measured, with a frame length of 4ms and shift of 2ms, in order to highlight the amplitude variation within individual pulses (see Fig. 4). Peaks are then detected in this contour and Power Peak (PwP) parameters are derived for each peak based on the previous (PwP-rising) and following (PwP-falling) 5 frames (i.e. 10ms) in the contour. The maximum power difference in each direction is used as the PwP value. In the original implementation a threshold is applied to this parameter to determine whether the peak can be used as a creak candidate location, however in the present study we simply use both PwP parameter values as acoustic features.The inter-pulse similarity measure (Ishi et al., 2008a) is used to discriminate glottal pulses corresponding to creaky voice from unvoiced regions. The parameter is derived using the locations of the peaks measured in the very short-term energy contour (Section 3.3). A cross-correlation function is applied to assess the similarity of adjacent bandlimited speech pulses:(2)IPS=maxCCorr(Fτ1,Fτ2);τ1−τ2<Tmaxwhere CCorr is the cross-correlation function,Fτ1andFτ2are the frames centred on successive candidate peak locations, and Tmaxis the maximum allowed distance between adjacent peaks, and is set to 100ms. Each frame is selected as the range of 5ms around the peak location. It is assumed that adjacent creaky voice pulses will show a high degree of similarity, as the vocal tract is unlikely to have significantly changed in such a short space of time and, hence IPS values should be high. Contrastingly, for unvoiced regions IPS values should be low, indicating a low level of similarity. The IPS parameter is illustrated in Fig. 5, where the values are shown as red stems. It can be observed that high IPS values are found in both the ‘modal’ voice region (up to 0.3s) and also in the creaky voice (0.33–0.5s).Note that both the IPS and PwP features are sampled on a glottal synchronous basis. These features are interpolated up to a fixed frame basis using a nearest neighbour approach for the current study.The Intra-frame periodicity (IFP) feature (Ishi et al., 2008a) was originally designed to help disambiguate creaky voice from other voiced regions. Unlike the IPS feature, IFP is calculated on a fixed frame basis using:(3)IFP=minNN−τ·autoCorr(τ);τ=j·τ0;j=1,2,…where N is the frame length (set to 32ms, with a 10ms shift), τ is the autocorrelation lag, autoCorr is the normalised autocorrelation function, and τ0 is the lag of the strongest autocorrelation peak. Note also that the search space for τ is limited to 15ms and that the factor N/(N−τ) is used to compensate for the decrease in amplitude with increasing τ in the autocorrelation function. Due to irregular periodicity and/or the very low F0 of creaky voice, IFP values in these regions will be close to 0 (see the last syllable in Fig. 5). Other voiced regions (for instance the first syllable in Fig. 5) will display IFP values close to 1.Although in many cases IFP is suitable for discriminating creaky voice from non-creaky voiced regions, its effectiveness for this purpose can be reduced somewhat when a speech region contains a very low F0 (but not quite sufficiently low for creaky voice, e.g. around 80Hz). Such effect was observed in a previous study (Drugman et al., 2012) and resulted in a high number of false detections for a speaker with an inherently low pitch.Note that in the present study the features PwP-fall, PwP-rise, IPS and IFP are given the group title Ishi's features, as they were all proposed in Ishi et al. (2008a).In addition to the acoustic features (KD) proposed by the present authors and those by Ishi et al. (2008a), three further features were included in particular to avoid false positives in unvoiced and silent regions. It was suggested (in a personal communication) by the authors of Ishi et al. (2008a) that creaky voice detected in regions of considerably lower energy (e.g. 20dB) than the maximum energy of an utterance, be discarded from the detection output. Consequently, we include a measure of signal energy (in dB) which has been normalised to the maximum energy of the utterance (Energy Norm). We also include ZeroXrate, which is a measure of the number of zero-crossings per ms. Unvoiced and silent regions are likely to display a significantly higher rate of zero-crossings compared with creaky voice regions (Kane et al., 2011). Note that both Energy Norm and ZeroXrate are measured on 32ms frames, with a 10ms shift. Finally, the Power Std feature is used as a measure of the variance in the very short-term power contour used in the calculation of the PwP features (Ishi et al., 2008a). The feature is derived as the standard deviation of the power (in dB), measured on 16 frames (corresponding to 32ms). As with the other features used in this study, these three features were sampled every 10ms.The goal of this section is to assess the relevance of the features described in Section 3 for the automatic detection of creaky regions. For this, we here make use of measures derived from Information Theory (Shannon, 1948) as they allow a quantification of the amount of discriminant information conveyed by the features. This is done independently of any subsequent classifier. Our approach allows for an integrated assessment of the discriminative power of each feature individually and also of the discriminative power in the context of the other features in terms of redundancy and synergy. These measures are first presented in Section 4.1. They are then used for an objective assessment of the features in Section 4.2.The problem of automatic classification consists of finding a set of features Xisuch that the uncertainty on the determination of classes C is reduced as much as possible (Huan and Motoda, 1998). For this, Information Theory (Cover and Thomas, 1991) allows the assessment of the relevance of features for a given classification problem, by making use of the following measures (where p(.) denotes a probability density function):•The entropy of classes C is expressed as:(4)H(C)=−∑cp(c)log2p(c)where c are the discrete values of the random variable C. H(C) can be interpreted as the amount of uncertainty on the class determination.The mutual information (MI) between one feature Xiand classes C is defined as:(5)I(Xi;C)=∑xi∑cp(xi,c)log2p(xi,c)p(xi)p(c)where xiare the discretised values of feature Xi. I(Xi;C) can be viewed as the information that feature Xiconveys about the considered classification problem, i.e. the intrinsic discrimination power of a given feature.The joint mutual information between two features Xi, Xj, and classes C can be expressed as:(6)I(Xi,Xj;C)=I(Xi;C)+I(Xj;C)−I(Xi;Xj;C)and corresponds to the information that features Xiand Xj, when used together, bring to the classification problem. The last term can be written as (Cover and Thomas, 1991):(7)I(Xi;Xj;C)=∑xi∑xj∑cp(xi,xj,c)·log2p(xi,xj)p(xi,c)p(xj,c)p(xi,xj,c)p(xi)p(xj)p(c)An important remark has to be underlined about the sign of this term. It can be noticed from Eq. (6) that a positive value of I(Xi;Xj;C) implies some redundancy between the features, while a negative value means that features exhibit some synergy (depending on whether their association brings respectively less or more than the addition of their own individual information).To evaluate the significance of the features described in Section 3, the following measures are computed:•the relative intrinsic information of one individual feature I(Xi;C)/H(C), i.e. the proportion of relevant information conveyed by the feature Xi,the relative redundancy between two features I(Xi;Xj;C)/H(C), i.e. the rate of their common relevant information,the relative joint information of two features I(Xi, Xj;C)/H(C), i.e. the proportion of relevant information they convey together.For this, Eqs. (4)–(7) are calculated. Probability density functions are estimated by a histogram approach using bins uniformly distributed between the possible extremum values. The number of bins is set to 50 for each feature dimension, which results in a trade-off between an adequately high number for an accurate estimation, while keeping sufficient samples per bin. Class labels correspond to the presence (c=1) or not (c=0) of a creakiness in the voice, as indicated by the manual annotation.

@&#CONCLUSIONS@&#
