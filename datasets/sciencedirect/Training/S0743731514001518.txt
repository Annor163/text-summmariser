@&#MAIN-TITLE@&#
A work stealing based approach for enabling scalable optimal sequence homology detection

@&#HIGHLIGHTS@&#
Comprehensive solution to scalable optimal homology detection.First implementation of suffix tree construction using distributed work stealing.Evaluation of many dynamic load balancing strategies for optimal homology detection.Parallel efficiency of 75%–100% for 2.56 M sequences on 8 K cores in 33 s.Peak rate of protein sequence alignments per second (PSAPS) of 2 M.

@&#KEYPHRASES@&#
Homology detection,Pairwise sequence alignment,Protein family identification,Dynamic load balancing,Work stealing,Distributed task counters,Parallel suffix tree construction,

@&#ABSTRACT@&#
Sequence homology detection is central to a number of bioinformatics applications including genome sequencing and protein family characterization. Given millions of sequences, the goal is to identify all pairs of sequences that are highly similar (or “homologous”) on the basis of alignment criteria. While there are optimal alignment algorithms to compute pairwise homology, their deployment for large-scale is currently not feasible; instead, heuristic methods are used at the expense of quality. Here, we present the design and evaluation of a parallel implementation for conducting optimal homology detection on distributed memory supercomputers. Our approach uses a combination of techniques from asynchronous load balancing (viz. work stealing, dynamic task counters), data replication, and exact-matching filters to achieve homology detection at scale. Results for 2.56 M sequences on up to 8K cores show parallel efficiencies of∼75%–100%, a time-to-solution of 33 s, and a rate of∼2.0Malignments per second.

@&#INTRODUCTION@&#
The field of bioinformatics and computational biology is currently experiencing a data revolution. The exciting prospect of making fundamental biological discoveries is fueling the rapid development and deployment of numerous cost-effective, high-throughput next-generation sequencing (NGS) technologies that have cropped up in a span of three to four years  [5,47,21,37,27]. Touted as next-generation sequencing, to now “3rd generation” technologies, these instruments are being aggressively adopted by large sequencing centers and small academic units alike. The result is that the DNA and protein sequence repositories are being bombarded with both raw sequence information (or “reads”) and processed sequence information (which could be in the form of DNA and amino acid/open reading frames types of data). Traditional databases such as the NCBI GenBank  [46] and UniProt  [8] are continuing to report a Moore’s law-like growth trajectory in their database sizes, roughly doubling every 18 months. In what seems to be a paradigm-shift, individual projects are now capable of generating billions of raw sequence data that need to be analyzed in the presence of already annotated sequence information. Path-breaking endeavors such as personalized genomics  [40], cancer genome atlas  [33], and the Earth Microbiome Project  [16] foretell the continued explosive growth in genomics data and discovery.While it is clear that the field of computational life sciences is becoming a Big Data field, the algorithmic advancements essential for implementing complex data analytics at scale have lagged behind  [11,35]. With a few notable exceptions in sequence search routines  [41,29,14,34] and phylogenetic tree construction  [36], bioinformatics continues to be largely dominated by serial tools originally designed for desktop computing.In this paper, we visit the problem of sequence homology detection—more specifically, given a set ofnsequences, detect all pairs of sequences that share a high degree of sequence homology as defined by a set of alignment criteria. The sequences are themselves typically short—typically a few hundred to few thousand characters in length.Also sometimes referred to as the “all-against-all” sequence comparison model of sequence analysis, the use-case for sequence homology detection arises routinely in the context of a number of bioinformatics applications. For instance, it arises in the context of genome sequencing projects, where the goal is to reconstruct an unknown (target) genome by aligning the short DNA sequences (aka. “reads”) originally sequenced from the target genome  [13]. The expectation is for reads sequenced from the same genomic location to exhibit significant end-to-end overlap, which can be detected using sequence alignment computation. A similar use-case also arises in the context of transcriptomics studies  [49] where the goals are to identify genes, and measure their level of activity (aka. expression) under various experimental conditions. A third, emerging use-case arises in the context of functionally characterizing metagenomics communities  [19]. Here, the goal is to identify protein families  [6,45] that are represented in a newly sequenced environmental microbial community (e.g., human gut, soil, ocean). This is achieved by first performing sequence homology detection on the set of predicted protein sequences (aka. Open Reading Frames (ORFs)) obtained from the community, and subsequently identifying groups of ORFs that are highly similar to one another  [53,51].At its core, the sequence homology detection problem involves the computation of a large number of pairwise sequence alignment (PSA) operations. A brute force computation of alln2pairs is not only infeasible but also generally not needed as with the sequence diversity expected in most practical inputs only a small fraction of pairs tend to survive the alignment test with a high quality alignment. The key is in identifying such a subset of pairs for PSA computation, using computationally less expensive means, without missing out on valid pairs. To this end, there are several effective filtering techniques using exact matching data structures  [2,23]. Yet, even after deploying some of the most effective pair filters, several billions of PSAs remain to be computed even for modest input sizes ofn≈106. The most rigorous way of computing a PSA, which is to use optimality guaranteeing dynamic programming algorithms such as Smith–Waterman  [17,31,44], is also computationally expensive—the algorithm takesO(m×n)time for aligning two sequences of lengthsmandnrespectively. In practice, each alignment task takes up to a few milliseconds on modern day CPUs. In the interest of saving time, current methods resort to faster, albeit approximation heuristic techniques such as BLAST  [2], FASTA  [39], or USEARCH  [12]. This has been the approach in nearly all the large scale genome and metagenome projects conducted over the last 4–5 years, ever since the adoption of NGS platforms. On the other hand, several studies have shown the importance of deploying optimality-guaranteeing methods for ensuring high sensitivity (e.g.,  [38,43]). For example, a recent study of an arbitrary collection of 320 K ocean metagenomics amino acid sequences shows that a Smith–Waterman-based optimal alignment computation could detect 36% more homologous pairs than was possible using a BLAST-based run under similar parameter settings  [52]. Improving sensitivity of homology detection becomes particularly important when dealing with such environmental microbial datasets  [35] due to the sparse nature of sampling in the input.In this paper, we evaluate the key question of feasibility of conducting a massive number of PSAs through the more rigorous optimality-guaranteeing dynamic programming methods at scale. To define feasibility, we compare the time taken to generate the data to the time taken to detect homology from it. Consider the following calculation: The Illumina/Solexa HiSeq 2500,11While there are other faster technologies, we use Illumina as a representative example.which is one of the more popular sequencers today, can sequence×109reads in ∼11 days  [21]. A brute-force all-against-all comparison would imply×1018PSAs. Whereas using an effective exact matching filter such as the suffix tree could provide 99.9% savings (based on our experiences  [52,22,24]). This would still leave×1015PSAs to perform. Assuming a millisecond for every PSA, this implies a total of 277 M CPU hours. To complete this scale of work in time comparable to that of data generation (11 days), we need the software to be running on 106 cores with close to 100% efficiency. This calculation yields a target of 109 PSAPS to achieve, where PSAPS is defined as the number of Pairwise Sequence Alignments Per Second.In addition to achieving large PSAPS counts, achieving fast turn-around times (in minutes) for small- to mid-size problems also becomes important in practice. This is true for use-cases–in which a new batch of sequences needs to be aligned against an already annotated set of sequences, or in analysis involving already processed information (e.g., using open reading frames from genome assemblies to incrementally characterize protein families)–where the number of PSAs required to be performed could be small (when compared to that generated in de novo assembly) but needs to be performed multiple times due to the online/incremental nature of the application.Some key challenges exist in the design of a scalable parallel algorithm that can meet the scale of 109 PSAPS or more. Even though the computation of individual PSAs are mutually independent, the high variance in sequence lengths and the variable rate at which those PSA tasks are identified using an exact matching filter can result in load imbalance (as will be elaborated in Section  3.3). In addition, the construction of the exact matching filter (such as the suffix tree) and the use of it to generate pairs for PSA computation on-the-fly need to be done in tandem with task processing (PSA computation), in order to reduce the memory footprint.22Note that it is not reasonable to assume that all of the generated pairs from the filter can be computed and stored prior to PSA calculations.In this paper, we present the design of a scalable parallel framework that can achieve orders of magnitude higher PSAPS performance than any contemporary software. Our approach uses a combination of techniques from asynchronous load balancing (viz. work stealing and dynamic task counters), remote memory access using PGAS, data replication, and exact matching filters using the suffix tree data structure  [50] in order to achieve homology detection at scale. Several factors distinguish our method from other work: (i) We choose the all-against-all model as it finds a general applicability in most of the large-scale genome and metagenome sequencing initiatives, occupying an upstream phase in numerous sequence analysis workflows; (ii) To ensure high quality of the output, each PSA is evaluated using the optimality-guaranteeing Smith–Waterman algorithm  [44] (as opposed to the traditional use of faster sub-optimal heuristics such as BLAST); (iii) We use protein/putative open reading frame inputs from real world datasets to capture a more challenging use-case where a skewed distribution in sequence lengths can cause nonuniformity in PSA tasks; and (iv) To the best of our knowledge, this effort represents the first use of work stealing with suffix tree filters.The key contributions are as follows:1.Comprehensive solution to scalable optimal homology detection at the largest reported scale of 8 K cores (previous highest was 2 K cores  [52]);A new implementation of suffix tree construction that uses the distributed memory work stealing approach for dynamic load balancing;Design and evaluation of different dynamic load balancing strategies–viz. work stealing, work stealing with iterators, and dynamic task counters–for scalable PSA computation;Results demonstrating parallel efficiency of ∼75%–100% for 2.56 M sequences and core counts up to 8 K cores; our results also show that we could analyze 2.56 M sequences at 8 K cores in 33 s with a PSAPS rate of 2 M.The paper is organized as follows: Section  2 presents the sequence homology problem in more detail and addresses the current state of computational solutions for the problem of homology detection. Section  3 presents the overall system architecture of our solution. Section  4 describes and experimentally evaluates our parallel algorithm. Key findings and future line of research are outlined in Section  5.Lets1ands2denote two sequences of lengthsn1andn2, respectively over a fixed input alphabetΣ. For DNA,Σ={a,c,g,t}. For amino acid/protein sequences, the alphabet contains one symbol for each of the 20 amino acids. Lets[i…j]denote the substring starting at indexiand ending atjins, and let the string indexing start at 1. The suffixiof stringsis the substrings[i…|s|]. As convenient, we will use the terms “strings” and “sequences” interchangeably.An alignment between two sequences is an order-preserving way to map characters in one sequence to characters in the other sequence or to gap symbols. There are many models for computing alignments—the most common models are global alignment, where all characters from both sequences need to be involved, and local alignment, where the aligning portions can be restricted to a pair of substrings from the two sequences. An alignment is scored based on the number of character substitutions (matches or mismatches) and the number of characters aligned with gaps (insertions or deletions). For DNA sequences, positive scores are given to matches and negative scores to penalize gaps and mismatches. For protein/amino acid sequences, scoring is typically based on a predefined table called a “substitution matrix” which scores each possible|Σ|×|Σ|combination  [1]. An optimal alignment is one which maximizes the alignment score.Computing an optimal alignment between two sequencess1ands2can be achieved inO(n1×n2)time andO(n1+n2)space using various dynamic programming algorithms  [31,44,20]. Faster, approximation heuristic methods such as BLAST  [2], FASTA  [39], or USEARCH  [12] are available to speedup the alignment process in near linear time, although they run the risk of producing sub-optimal alignments.The sequence homology detection problem is as follows: Given a sequence setS={s1,s2,…,sn}, identify all pairs of sequences that are “homologous”. There are several ways to define homology depending on the type of sequence data and the intended use-case. Since for this paper, we deal with protein/amino acid sequences, we use the following definition consistent with some of the previous works in the area  [53,51,52]: Two sequencess1ands2of lengthsn1andn2, respectively, are homologous if they share a local alignment whose score is at leastτ1%of the ideal score (withn1matches), and the alignment covers at leastτ2%ofn2characters. The above is assumingn1≤n2w.l.o.g. The parametersτ1andτ2are user-specified, with defaults for protein sequences set asτ1=40%andτ2=80%[52]. Note that for DNA sequences, these cutoffs typically tend to be higher as more similarity is expected at the nucleotide level. The lower cutoffs used in protein sequences make the homology detection process more time consuming because more pairs of sequences typically need to be evaluated. Many methods that happen to use even fast alignment heuristics such as USEARCH  [12] and CD-HIT  [26] do not even allow specifying such lower settings due to computational constraints. If one were to deploy dynamic programming methods to evaluate alignments, an optimal alignment will be computed regardless of the specified cutoff thus making the solution more generic. The key lies in scaling the number of alignments computed to the extent that evaluation of the identified pairs becomes feasible. However, to the best of our knowledge, no such parallel implementations exist. Consequently, all the genome and metagenome scale projects so far have resorted to BLAST-like heuristics to compute homology. On the other hand, several studies have shown the importance of deploying optimal alignment methods to ensure high sensitivity (e.g.,  [38,43,52]).Let us visit here the question of what makes homology detection through optimal alignment computation challenging from a scalability point of view. An exhaustive, brute-force evaluation of alln2pair combinations is not feasible given the large values ofnexpected in practice (even if alignment heuristics are to be used). As a result, filters need to be used to identify only a subset of pairs for which alignment computation is likely to produce satisfactory results (as per the pre-defined cutoffs). A popular filtering data structure is that of the look-up table  [3], which is also internally used in numerous programs that are variants of BLAST and FASTA  [39,12,26,4]. While it is easy to construct and process this data structure, its use is restricted to identifying short, fixed-length exact matches between pairs of sequences. This is owing to its space complexity, which is exponential in the length of the exact match sought after—more specifically,O(|Σ|k)wherekis the length of the exact match. Furthermore, a smaller value ofk(typically, 3 or 4 used in practice) significantly increases the number of pairwise sequence alignments (PSAs), as more pairs of sequences are likely to share a shorter exact match by random chance.The use of suffix trees33Since we have multiple sequences as input, the appropriate data structure here is the “generalized suffix tree”, which is nothing but a unified suffix tree corresponding to all suffixes of all the input sequences; however, for convenience, we simply use the term suffix tree in this paper.[50] overcomes these limitations as its space complexity is linear in the input size, and it has the ability to allow detection of arbitrarily long exact matches in constant time per matching pair  [23]. It has also demonstrated a high selectivity as a filter for identifying pairs, typically identifying less than 0.1% of the totaln2pairs for both DNA and protein sequences [52,22,24].There are, however, some design challenges presented by the use of suffix trees. First, constructing suffix trees on massively parallel distributed memory machines is nontrivial, owing to the inherent irregularity of the underlying data access patterns [23,15,30]. Second, although the data structure has a linear space complexity, the constant of proportionality is high, typically around 40–50. Therefore, the data structure needs to be generated and stored in a distributed manner in order for scalability. Third, despite the high selectivity of pairs, the number of pairs identified could still be in several billions or more for modest sized inputs containing millions of sequences, precluding the possibility of storing them before processing them for alignment.The algorithm presented in this paper improves on our previous efforts  [52,9] and tackles the challenges outlined above through the use of work stealing and task counters. Wu et al.  [52] use a hierarchy of master and worker processes on a compute cluster to balance the load of generating pairs from a precomputed, out-of-core sequence filter while concurrently aligning the generated pairs. They report scaling up to 2 K processors. Daily et al.  [9] were the first to apply a work stealing technique to scale sequence homology to over 100 K processors but did so by simulating an arbitrary filter which did not introduce compute overhead or load imbalance, thus their work focused primarily on the work stealing of the brute forcen2set of sequence pairs. Our work represents the first comprehensive solution to scalable optimal homology detection given an input set of sequences; nothing is computed beforehand and no portions of the pipeline are simulated. Our pipeline applies work stealing to the creation and processing of the suffix tree filter concurrently with the pair alignments. Lastly, while there are numerous solutions available for hardware acceleration of individual PSA computations on various specialized multicore platforms such as GPUs, FPGAs, etc. (reviewed in  [42]), the implementation presented in this paper does not incorporate those (future work).In this section we explore many ways of solving the problem of optimal homology detection. We first attempt to reduce the task space using known filtering techniques. Then we propose a solution to the load balancing issue caused by using the filters in addition to the load imbalance inherent to the problem.As noted in Section  2, exact matching filters need to be used in practice to reduce the task space fromn2PSAs. One of the most effective filters designed to date is the suffix tree filter used by  [52]; however the search for better filters is an open area of research. We describe the suffix tree filter as well as an alternative length-based filter in the following sections.Using suffix trees to identify “promising” sequence pairs for alignment computation is detailed in  [52]. We improve upon their work by not precomputing and storing the suffix trees to disk, and instead generate the suffix tree on-the-fly and use it to identify promising pairs when different subtrees of the suffix tree become available.To build the suffix tree in parallel, we independently construct subtrees of the suffix tree. We first partition all suffixes of the input sequences into|Σ|k“buckets” based on their firstkcharacters, wherekis a short, fixed-length parameter e.g., 5 for amino acid sequences. We represent a suffix as a 3-tuple of the sequence index, the offset from the start of the sequence, and the bucket index. The reason for the sequence index and offset are clear, however our choice of associating the bucket index with each suffix was for memory considerations as well as for ease of implementation. With respect to memory, the number of suffixes (3-tuples) depends on the size of the input sequences, whereas the number of buckets depends on|Σ|kwhich grows quickly as eitherkor|Σ|becomes large. Our implementation allows for much largerkthan would normally be allowed given memory constraints. With respect to ease of implementation, we can store the suffixes as a contiguous array instead of using a sparse representation of the buckets. This contiguity enables easy sorting of the suffixes as well as the direct exchange of the suffixes subtrees when load balancing.Once the buckets are constructed, by definition of the suffix tree, each such bucket contains suffixes that fall into a distinct subtree rooted at a depth ofkof the tree. The idea is to subsequently process all buckets in parallel so that the individual subtrees corresponding to buckets can be constructed in an independent manner. A challenge here is that the size of each bucket is not necessarily uniform as it is input dependent, and the amount of work is proportional to the number of suffixes contained in the tree. Consequently, one option is to statically partition the buckets onto each process in an attempt to balance the total number of suffixes to be handled on each process. However, this would require global knowledge as to the size of each bucket, and if|Σ|kis large this approach is not feasible. As an alternative, we partition the buckets based on the bucket index modulo the number of processes, then we apply work stealing to further load balance this problem. The initial static distribution of the buckets is a simple calculation. In addition, since adjacent buckets, e.g., “AAB”, “AAC” wherek=3, tend to be similar in size when they share a common prefix (here “AA”), the initial distribution keeps adjacent buckets from being stored on the same process in case their shared prefix occurs frequently. Lastly, each subtree requires a variable amount of suffixes to be present in memory, along with their corresponding sequences, before processing begins. This may increase the amount of communication in our implementation, especially when sequences are not stored locally (discussed more in detail in Section  3.2). Non-local sequences are always fetched as needed, which works well for aligning two sequences with at most two fetches, but in the case of suffix subtree processing which may require many fetches, we cache non-local sequences until the subtree processing is complete. Caches are not shared between processes and are discarded once the subtree is no longer being processed. The suffix subtrees are themselves constructed in a depth-first manner by recursively bucketing the set of suffixes at increasing node depths. We do not construct or use any auxiliary suffix tree data structures such as suffix links. A depth-first traversal of the constructed subtree generates the promising pairs similar to the algorithm of Gusfield  [18, p. 147] for generating maximal repeated pairs.The suffix tree filter, although generally effective in terms of reducing the number of alignments to perform, takes a non-negligible time to create and process the suffix subtrees. One way to achieve further savings in the number of PSAs performed, without impacting the final output, is as follows: we can rule out pairs based upon the length of the two sequences involved in the potential alignment. As a user-supplied heuristic, if the two sequences could not possibly produce a positive optimal score because the sequences differ too greatly in length, or if one of the sequence lengths is less than the minimal length cutoff, the pair is discarded. This length-based filter calculation is in fact used by the suffix tree filter as an additional filter after it has identified a promising pair using the tree alone. We explore the merit of using the length based filter on its own in Section  4.Using work stealing as in  [9] required the tasks to be explicitly enumerated and stored for a total ofn2tasks stored acrossPprocesses. The largest dataset explored by the authors was 2.56 M sequences which resulted in approximately 3.28 trillion tasks. The tasks were stored as two 8-byte integer sequence identifiers. This could be reduced to a single 8-byte integer using a combinatorial number system of degree 2, but even so this would require nearly 24 TB of aggregate memory or at minimum nearly 800 compute nodes with 32 GB of usable memory each. This solution of computing and storing the enumerated pairs does not scale with respect to memory constraints, even if we are able to filter out pairs—eventually larger datasets will produce enough pairs to invalidate this approach.An alternative is to dynamically generate and process the pairs using a dynamic load balancing scheme. The strategy in  [52] was to use a hierarchy of masters and workers in such a way to handle pairs being generated faster than they could be consumed. We use a similar strategy but apply it using work stealing, dynamically creating new work to be consumed as suffix trees are processed.A significant challenge in the design of parallel homology detection is the management of the sequence data. The strategy in  [9] was to store the sequence database once per compute node rather than once per worker process. In a hybrid MPI + pthread model this is accomplished by running one MPI process per compute node to hold the sequences and then using pthreads to access the read-only sequence database. In a standard MPI model, the sequences can be stored in shared memory.By storing the entire sequence database per compute node, the authors did not address memory constraints such that the sequences would not fit within a single compute node. This is a problem as the database sizes continue to grow faster than the amount of memory per node. Our solution to this problem relies on a PGAS model rather than a shared-nothing MPI model or a hybrid MPI + pthread model. The PGAS model provides a shared memory interface to the sequence database while transparently distributing the sequences across compute nodes.Using the PGAS model, the aggregate memory of multiple compute nodes is available with the trade-off of having to communicate sequences that are no longer local. We reduce the chances of having non-local sequences by replicating the sequence database once per subset of nodes such that each subset of nodes has enough aggregate memory to store the complete sequence dataset. As an improvement over  [52], non-local sequences are communicated using one-sided operations rather than periodic collective communications or the alternative of using non-blocking two-sided operations which would require explicit progress. We use an efficient one-sided communication library  [48] which performs better than the one-sided primitives of the MPI-2 standard, making this a viable implementation strategy.There is significant incidence of load imbalance throughout this problem. We look at the causes and solutions in detail next.For the suffix tree filter, each suffix is placed in a bucket based on its firstkcharacters resulting in at most|Σ|kbuckets. Each bucket is processed to yield a distinct subtree of the suffix tree, which is subsequently processed to generate sequence promising pairs.kmust be sufficiently large to create enough work to distribute. Subtree creation is linearly proportional to the sum of the length of all suffixes that constitute the subtree. Pair generation on the other hand takes time linearly proportional to the number of output pairs. Since the sizes of the buckets may not be uniform, load imbalance could occur. Further, the number of pairs generated by a tree is completely dependent on the content of the trees, which also varies (quadratic in the worst case).The length-based filter does not directly cause load imbalance since it requires negligible computation time on its own. However, when used as part of the brute force strategy, it will reject pairs as they enqueue for computation and will ultimately alter the already imbalanced workload but in a similarly imbalanced way.Fig. 1shows the histogram and normalized cumulative distribution of alignment processing times for all-against-all alignment of 15,000 sequences obtained from a metagenomics sequence database  [7]. We observe from Fig. 1(a) that a significant fraction of tasks are of the order of milliseconds or lower, with a non-negligible fraction consuming well above a millisecond. The large number of tasks together with the wide disparity in the task processing times exacerbates problems associated with static load balancers due to small errors in the estimation of alignment times. The alignments include a few large tasks taking few tenths to over one second.Fig. 1(b) shows the cumulative distribution of time spent in processing all tasks that can be processed under a particular time. As we anticipated, despite their counts, the smallest alignment operations consume a negligible fraction of the total processing time. On the other hand, alignment operations that can be processed in 1–100 ms consume almost 90% of the total processing time. This shows that the alignment operations critical to load balanced execution vary by up to two orders of magnitude in their processing time.The means to load balance computations fall into three broad categories, namely static partitioning, dynamic repartitioning, and asynchronous repartitioning. In static partitioning, the work is collectively distributed among available compute resources based on available load information. Dynamic repartitioning is similar to static repartitioning; however, rather than performing once at the beginning of the computation it is performed periodically and collectively. The last strategy is to asynchronously migrate work between compute resources without exchanging information collectively. In the case of homology detection, as shown by the characteristics in Fig. 1 as well as due to the dynamic nature of suffix subtree processing and pair alignments, the best load balancing approach would also need to be dynamic and asynchronous. Examples of asynchronous load balancing include work stealing and distributed task counters.Work stealing: Scalable work stealing as a general approach to asynchronous load balancing is detailed by Dinan et al.  [10] and Lifflander et al.  [28] while its application to sequence alignment is covered in  [9]. Briefly, work stealing models a shared task pool. The task can be represented by any fixed-size datatype including structures. The implementation of Dinan et al. places a portion of the task pool on each process in a double-ended queue (deque) which is split into shared and private portions. Tasks can be released from the private portion to the shared portion without locks; acquiring tasks from the shared portion to the private portion requires locking. Tasks may also create additional tasks as part of their execution; however dynamically adding tasks to the pool is done into the private portion and the process becomes lock free. When a worker runs out of tasks in both the private and shared portions of its deque, it becomes a thief. Thieves choose a random victim and attempt to steal half of their tasks, if available. A termination detection algorithm is used to end the task pool execution. The implementation of work stealing in Lifflander et al.  [28] and Daily et al.  [9] uses an MPI + pthreads execution model and an active message programming model instead of the PGAS model used by Dinan et al.; however it follows the same model of a shared task pool. The implementation requires one core per compute node be reserved as a progress thread. Even so, it was shown to scale to over 100 K cores with 75% efficiency  [9].Work stealing with iterators: A special form of work stealing can be utilized when the tasks are a finite countable set and can therefore be represented as a contiguous sequence of natural numbers. Instead of implementing the task pool with one deque per worker, each worker stores a range of numbers from the task set as a[low..high]interval. Therefore, a steal operation splits the victims range in half and only transfers two integer values instead of half of a queue’s tasks. This results in both memory and communication bandwidth savings. We use a combinatorial number system of degree 2 in order to translate a non-negative index to a lexicographically ordered 2-combination which represents the two sequences to align as described by  [9,25]. We explore using work stealing iterators to improve the efficiency of work stealing for sequence alignments.Distributed task counters: Work stealing iterators are a form of a distributed task counter. Many high-speed interconnects provide hardware-accelerated implementations of an atomic integer fetch-and-add instruction which can be used to implement a distributed task counter. A process requesting a new task increments the value of the counter while reading the old value. The atomicity of the instruction guarantees that each calling process reads a unique counter value. We translate the counter value into a pair of sequence IDs using the same combinatorial number system of degree 2 as with work stealing iterators. Using distributed task counters does not necessarily require one core per node to be reserved, especially on high speed interconnects. This can result in improved efficiency with respect to work stealing. Further, although less important, distributed task counters only allocate space for the counter on a single process which avoids the need to allocate portions of the task pool on each process. We explore using distributed task counters to improve the efficiency of work stealing for sequence alignments.

@&#CONCLUSIONS@&#
We presented a design of a scalable parallel framework which achieves orders of magnitude higher PSAPS performance and at greater scale than contemporary software using the generally applicable all-against-all sequence alignment model. This represents a comprehensive solution to scalable optimal homology detection. This achievement was facilitated using the work stealing dynamic load balancing technique, a one-sided asynchronous PGAS model for data transfer, and a distributed hash table to eliminate duplicate work. Our results demonstrate a promising step towards analyzing biological sequences as fast as they can be generated on contemporary sequencing hardware.