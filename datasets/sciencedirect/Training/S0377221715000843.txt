@&#MAIN-TITLE@&#
Rankings and university performance: A conditional multidimensional approach

@&#HIGHLIGHTS@&#
In this paper we give a contribution to the birth of a new generation of rankings.We integrate new kind of information and use a new ranking technique.We implement confidence bounds for “managerial efficiencies”.Our approach overcomes four main criticisms of university rankings.We provide an illustration on European universities.

@&#KEYPHRASES@&#
Rankings,European universities,Conditional directional distances,Robust frontiers,Bootstrap,

@&#ABSTRACT@&#
University rankings are the subject of a paradox: the more they are criticized by social scientists and experts on methodological grounds, the more they receive attention in policy making and the media. In this paper we attempt to give a contribution to the birth of a new generation of rankings, one that might improve on the current state of the art, by integrating new kind of information and using new ranking techniques. Our approach tries to overcome four main criticisms of university rankings, namely: monodimensionality; statistical robustness; dependence on university size and subject mix; lack of consideration of the input–output structure. We provide an illustration on European universities and conclude by pointing on the importance of investing in data integration and open data at European level both for research and for policy making.University rankings are the subject of a paradox: the more they are criticized by social scientists and experts on methodological grounds, the more they receive attention in policy making and the media. Rather than adding to the large literature on the methodological shortcomings of the existing rankings, this paper tries to give a contribution to the birth of a new generation of rankings, one that might improve on the current state of the art both in substantive and methodological bases. We provide two contributions: integrating new kind of information and using new ranking techniques.The main criticisms (that we report in their historical order of introduction in the literature) addressed to university rankings, which we examine in detail in Section 2, can be summarized as follows:(a)MonodimensionalityStatistical robustnessDependence on university size and subject mixLack of consideration of the input–output structure.According to several authors, world rankings suffer from focusing only on the research dimension, which is more visible and easier to measure using external observations. A call for integrating the existing rankings with the educational perspective is in order. Yet several studies call into question the statistical properties of the rankings, irrespective of their substantive content, while others show that rankings systematically distort the representation in favour of large and established universities, and of universities in which scientific and technological disciplines, with particular reference to medical disciplines, are dominant. Finally, a few authors have raised the issue of whether it is acceptable to rank universities worldwide, without any consideration of the differences in resources made available to them by their respective national governments, or their input–output structure.In this paper we provide an experiment that addresses all these issues, with reference to universities in Europe. The experiment might be replicated in USA and in several Asian countries, which have data comparable to the ones we use here.First, we reduce monodimensionality by integrating data on research output (basically, scientific publications) with data on the teaching mission of universities. This is a major departure from existing rankings. The integration has been made possible by the creation of the Eumida (European Universities Micro Data) census of Higher Education Institutions (HEIs) in Europe, a project supported by the European Commission and Eurostat. In addition, we use data that refer to the quality of research. Thus by integrating data on education and research, and by including data not only on students but on degrees, we address the monodimensionality issue. In future studies other indicators (not available for this study) might be included, such as third mission, regional engagement and research infrastructures, leading to even more comprehensive analyses.Second, we propose a ranking technique that is based on estimators that are robust to extreme values and outliers (as illustrated in Section 4) and delivers confidence intervals for the estimates (as illustrated in Appendix B), allowing the analyst to fully understand the statistical properties of the ranking score we propose.Third, we address the dependence of rankings on size and subject mix by using a novel technique, called directional conditional efficiency analysis. As illustrated in the methodological section, this technique permits the estimation of efficiency measures net of the impact of size of universities (as proxied by the number of students) and net of the subject mix. This is another major departure from existing rankings. While our data do not allow any estimation in the fields of Social Sciences and Humanities (SSH), due to the limitations of current databases, for the first time we consider the subject mix of universities, as proxied by the specialization index of universities.Fourth, the ranking we propose is based on an explicit input–output structure. We take benefit from the data in the Eumida dataset, that include academic and non-academic staff and personnel and non-personnel expenditures, to compute technical efficiency indicators in a multi-input multi-output framework. In this framework a university ranked high is one that makes the best possible use of its resources, on which it may have little discretionary power.A consolidated literature has applied Data Envelopment Analysis (DEA) in the education sector (see e.g. Sarrico and Dyson, 2000; Sarrico, Teixeira, Rosa, and Cardoso, 2009 and Grosskopf, Hayes, and Taylor, 2014 and the references cited therein).From a methodological point of view, this paper implements in the context of universities rankings the conditional directional distance approach by Daraio and Simar (2014) extending it to derive confidence bounds on the “managerial” efficiency scores robustly estimated. Indeed, as rightly emphasized by Grosskopf et al. (2014, p. 24): “Policy makers are interested in using efficiency scores [...] so it is crucially important to strengthen existing strategies for generating confidence bands around efficiency scores [...]”.Recently, Daraio, Bonaccorsi, and Simar (2015) propose a robust directional distance approach to analyze economies of scale and specialization in European universities and find that both size and specialization have a statistical significant effect on the efficiency. In this paper we make a step further and estimate the efficiency in the production of research quality taking into account also the volume of scientific production and the teaching realized. Research quality is hence the main output of interest. It is measured by a factor built taking into account international collaborations, normalized impact of research, high quality and excellence rate of publications. By applying a robust directional distance technique, we consider as non-discretionary outputs the volume of teaching and research carried out as well. We examine how European universities can improve their efficiency in the production of research quality, given the resources they are using and taking into account the level of teaching and research they produce while moving along a direction which is representative of the median case at European level.Summing up, we believe that by integrating new data and adopting a novel technique there might be a leap forward in the way in which the activities and performances of universities are examined.The paper unfolds as follows. Section 2 proposes an outline of the critical literature on university rankings. Section 3 introduces the main sources of data and lists the variables analyzed. Section 4 illustrates the methodology and is complemented by Appendix B. Finally, Section 5 presents the main results, while Section 6 concludes the paper.In this section we present the main lines of criticism to university rankings in the four chapters anticipated in Section 1. Other classifications are certainly possible. For the sake of clarity, criticisms classified in categories (a, monodimensionality) and (d, lack of input–output structure) deal with the substantive content of rankings, i.e., the data included (or missing), while studies under (b, statistical robustness) and (c, dependence on size and subject mix) mainly address methodological issues, i.e. how the data are processed in order to arrive at a ranking. Our classification clearly does not exhaust other lines of criticism: for example, we do not have any solution to the issue of English language bias, as well as for the lack of appropriate inclusion of Social Sciences and Humanities in rankings. Also we do not address the more general criticism according to which rankings are a disciplinary device created to impose neoliberal market-oriented values and practices onto an institution, the university, hitherto governed by the public ethos. At the same time our classification is reasonably comprehensive.The argument is that universities all around the world perform several institutional missions: teaching, research, and third mission. Rankings that programmatically focus only on research outputs of universities are therefore biased. Even admitting that the third mission has been legitimized and institutionalized more recently, and is certainly less relevant (quantitatively) than the other two missions, it is felt that ignoring the teaching output altogether severely distorts the reality. Therefore, there is a demand for including information on teaching as well as research outputs of universities. Existing rankings include only a small set of indicators, whose meaning in terms of overall education activity of universities is questionable: the Alumni Nobel and Field prizes (10 percent) in ARWU (Academic Ranking of World Universities), student/staff ratios (20 percent weight), international students (5 percent) and international staff (5 percent) in QS (Quacquarelli Symonds) World University Rankings, and income per academic (2.25 percent), undergraduates admitted per academic (4.5 percent), ratio of international to domestic students (2.5 percent), ratio of international to domestic staff (2.5 percent) in THE (Times Higher Education Rankings). These proxies are considered unreliable and highly volatile by most analysts, as it is witnessed by the lack of consistency across various rankings, with the exception of the few top positions (Saisana, D’Hombres, and Saltelli, 2011; Salmi and Saroyan, 2007).In fact, several authors have questioned the correspondence between rankings and quality of education, stating that in general “what is incorporated into the rankings is what is measurable, not what is valid” (Cremonini, Westerheijden, and Enders, 2008). The over reliance on research indicators may induce biased decisions (Bastedo and Bowman, 2010).It is well known that the Shanghai ranking, the first global university ranking, originated from a specific need to provide information on research quality of universities which were considered target for Chinese students and decision makers (Liu, 2009). Therefore it did not incorporate any consideration of the teaching dimension, with the exception of prizes to Alumni, which is however biased toward large and old universities. Other rankings, such as Times Higher Education Supplement, introduced a few items related to education. However, the criticism hits the point: global league tables are largely based on the research output and ignore or underestimate the importance of education (Moed, Burger, Frankfort, and van Raan, 1985).Needless to say, including data on education calls into question the issue of quality and in particular on what accounts for quality and whether it can be captured by quantitative measures. Without entering into the theoretical debate, we can say that there is an agreement that data on the completion of studies are an acceptable indicator of quality.11In our case, the information on degrees is the only available quantitative proxy for teaching quality, based on comparable data coming from national statistical authorities at European level. Indeed, comparable data at European level on placement of students would be a better proxy of teaching quality, but unfortunately are not available. Nevertheless, several studies in efficiency analysis suggest to focus on educational degrees. According to Johnes (2006) degrees include elements of quality, since they are the result of the completion of the curriculum. This line of reasoning has also been followed by Daghbashyan, Deiaco, and McKelvey (2014).While the number of students is certainly an indicator of teaching output (i.e. students are subject to teaching activities during their stay) but not necessarily of quality, a university’s completion and degree of achievement are strongly correlated with the quality of students it takes in.From a public policy perspective, it would be important to consider that two universities, ranked similarly with respect to research excellence, have largely different social importance depending on the number of students who receive a degree from them, that is, who have completed the curriculum. In fact, education is one of the avenues through which new knowledge generates an impact on society.From the methodological point of view, rankings collapse a variety of indicators into a single measure. This raises a number of technical issues that are the subject of disciplines such as statistics, information theory and decision theory. According to several authors, the validity, reliability and comparability of information incorporated into the measures fail to satisfy properties for acceptance (Bowden, 2000; Florian, 2007; Van Dyke, 2005).One line of reasoning has stressed the importance of not using just one ranking but multiple ones. More generally, Van Leeuwen, Visser, Moed, Nederhof, and van Raan (2003) have underlined the importance of ‘using multiple indicators instead of only one’ (Van Leeuwen et al., 2003, p. 276). In a famous and controversial paper, Van Raan (2005) warned against the construction of rankings, on the basis of the argument that bibliometric information is biased and subject to errors, so that people do not have ‘competence to understand what is measured’ (Van Raan, 2005, p. 134; see the reply in Liu, Cheng, and Lin, 2005).A second line of research within this chapter has introduced the notion of probabilistic ranking. According to Lubrano (2009) an important methodological problem of rankings is that they assume a deterministic setting, while the underlying indicators are average values from distributions. As Goldstein and Spiegelhalter (1996) puts it, on the contrary, ‘the mean has no special status’ (Goldstein and Spiegelhalter, 1996, p. 395). In other words, rankings suppress the intrinsic variability of indicators at lower levels of aggregation, giving an impression of stable hierarchies among universities, without explicitly testing for the statistical representativeness of differences. As it has been noted ‘an overinterpretation of a set of rankings where there are large uncertainty intervals, can lead both to unfairness and to inefficiency and unwarranted conclusions about changes in ranks’ (Goldstein and Spiegelhalter, 1996, p. 405).A third direction has been pioneered by Saisana et al. (2011), who developed a methodology to test the robustness of rankings. Being based on elementary indicators aggregated into composite indicators, rankings utilize only one of a number of possible combinations of indicators and of aggregation rules. One problem, often raised in the literature, is that the weights used for the aggregation of individual indicators are arbitrary and lack theoretical foundation (see e.g. Provan and Abercromby, 2000). Using a simulation technique, Saisana et al. (2011) show that, in general, rankings are robust in the top positions but less reliable elsewhere, that Shanghai rankings are more robust than Times Higher Education Supplement rankings, and that for a certain number of universities the variability induced by changes in the construction of the composite indicator is so large that all existing rankings are meaningless.This line of criticism argues that the rankings are not objective, since they systematically favour old and large universities (Hazelkorn, 2007; 2009). In addition, they favor universities in which scientific, technical and medical disciplines (STEM) are dominant. It has been shown, in fact, that controlling for differences in the subject mix may lead to completely different rankings.With respect to size, the existence of a correlation between the output and the impact of publications has been identified since long time (Hemlin, 1996). Basically, most rankings use absolute numbers of publications and citations as the main element.The issue of subject mix and the disciplinary composition of universities has also been repeatedly raised in the literature (see Toutkoushian and Webber, 2011 for a discussion). Different disciplines have largely different distributions of scientific output. According to Bornmann, de Moya Anegon, and Mutz (2013) universities that focus on disciplines such as life sciences have an advantage over universities with a wider variety of disciplines such as engineering, simply because the former have a higher citation volume than the latter. As a consequence, according to several authors (see e.g. Buela-Casal, Gutierrez-Martinez, Bermudez-Sanchez, and Vadillo-Mugnoz, 2007), there should be separate individual rankings for each school or department, rather than having a composite measure. Marginson (2007) has proposed a general principle: ‘when comparing research and scholarly capacity or performance, use primarily discipline-based measures rather than whole of institution measures’ (Marginson, 2007, p. 19). One important reason to work in this direction is that rankings give a premium to comprehensive research universities. Isomorphic pressures may reduce the diversity of the system penalizing programmatic diversity and specialist universities (Marginson and van der Wende, 2007). Thus the issue here is not to use several rankings or to check their robustness or to avoid aggregation but rather use separate disciplinary rankings.Another line of criticism argues that rankings simply ignore the amount of resources that universities receive. According to OECD (Organisation for Economic Cooperation and Development) data, governments allocate to higher education widely different amount of resources, resulting in large gaps in student/staff ratios, as well as in cost per student (Porter and Toutkoushian, 2006). Accordingly, it is argued that rankings are, at least partially, a reflection of the economic status of countries. If this is the case, they would give no information as to how to improve the system within countries (Docampo, 2012). Furthermore, they might lead to wrong implications for the allocation of resources (Stake, 2006).Bornmann, Lutz, and Daniel (2013) have shown that 80 percent of the variance between the universities is explained by differences between the countries in which the universities are located, in particular by differences in GDP per capita. This leads to ask whether rankings measure the differential performance of universities, or rather reflect the divide in scientific performance among countries, a factor upon which individual universities have little power. A related and subtle criticism has been proposed by Cremonini et al. (2008), who argue that rankings want to reframe higher education as a consumer good, while the appropriate reference model should be one of investment. In other words, rankings offer only information on the output, while they fail to account for the relation between inputs and outputs, and between outputs and social outcomes.Safon (2013) has shown that the position in rankings is largely determined by underlying factors such as “age, scope, activity in hard sciences, university in U.S., English-speaking country, annual income, orientation toward research, and reputation” (p. 238). As it is clear from this list, only a few of these factors, such as orientation toward research and, partially, reputation, are under the control of universities, while others mostly depend on historical factors (age, scope and activity in hard sciences) or on country-level factors (English and annual income). While it will not be possible to control for all contextual factors and isolate those that are under the control of universities (an issue that has been prominent for decades in the literature in industrial economics and strategic management and is still largely unsolved), some improvement can be pursued.As a matter of fact, most of these authors challenge the notion that rankings can be built mainly on the basis of output data. Rather, the appropriate notion to be used in order to compare universities at the international level is the one of efficiency, or the relation between input and output.22This topic has been addressed in national contexts by various contributions (see e.g. Abbott and Doucouliagos, 2003; Flegg, Allen, Field, and Thurlow, 2004; Johnes, 2008; 2013; Worthington and Lee, 2008).The joint consideration of outputs and resources employed is the starting point for university strategy (Bonaccorsi and Daraio, 2007) and for the positioning of universities with respect to their peers (Bonaccorsi and Daraio, 2008).We exploit a large database, recently constructed by the European University Micro Data (Eumida) Consortium under a European Commission tender, supported by DG EAC (Directorate General for Education and Culture), DG RTD (Directorate General for Research and Innovation), and Eurostat.This database is based on official statistics produced by National Statistical Authorities in all 27 EU countries (with the exception of France and Denmark) plus Norway and Switzerland. The Eumida project, relying on the results of the Aquameth project (Bonaccorsi and Daraio, 2007; Daraio et al., 2011) included two data collections: Data Collection 1 (DC 1) included all higher education institutions that are active in graduate and postgraduate education (i.e. universities), but also in vocational training. Data refer to 2008, or to 2009 in some cases. Thus all institutions delivering ISCED (International Standard Classification of Education) 5a and 6 degrees are included, and the subset of those delivering ISCED 5b degrees that have a stable organization (i.e. mission, budget, staff). There are 2457 institutions identified in Data Collection 1: these constitute the perimeter of higher education institutions in Europe. On these institutions a large set of uniform variables have been collected.Of these, 1364 are defined research active institutions: of these only 850 are also doctorate awarding. They are the object of Data Collection 2 (DC 2), for which a larger set of variables were collected. This means that a significant portion of research active institutions is found outside the traditional perimeter of universities, that is in the domain of non-university research (particularly in countries with dual higher education systems).We integrate the EUMIDA data, in particular the DC 2 dataset, with the Scimago data (SIR World Report 2011, period analyzed 2005–2009) that include institutions having published at least 100 scientific documents of any type, that is, articles, reviews, short reviews, letters, conference papers, etc., during the period 2005–2009 as collected by Scopus database.33The integration has been carried out within the Smart.CI.EU (Sapienza microdata architecture for education, research and technology studies. A Competence-based data Infrastructure on European Universities), an experimental data infrastructure created within a research project funded by Sapienza University of Rome and owned at the Department of Computer, Control and Management Engineering Antonio Ruberti, Sapienza University of Rome.From Scimago data we used the following variables:–number of publications in Scopus (PUB);Specialization index (SPEC) of the university that indicates the extent of thematic concentration/dispersion of an institution’s scientific output; its values range between 0 and 1, indicating generalistic vs. specialized institutions respectively. This indicator is computed according to the Gini Index and in our analysis it is used as a proxy of the specialization of the university.International Collaboration (IC), percent of a university’s output realized in collaboration with foreign institutions (calculation based on affiliations with more than one country address).High Quality Publications (Q1), percent of publications that a university publishes in the first quartile (25 percent) in their categories as ordered by Scimago Journal Rank indicator.Normalized Impact (NI), in percent shows the relationship between a university’s average scientific impact and the world average set to a score of 1.Excellence Rate (EXC), percent of university output that is included in the 10 percent of the most cited paper in their respective scientific fields.Table 1defines and describes the inputs, outputs and conditioning factors that are used in the following analysis. The choice of these variables has been carried out by making a compromise between relevance of the factors and availability of data. For instance, capital expenditures would be an interesting input to include in the analysis but unfortunately there were not available data.44As a consequence, the omission of capital expenditure might cause possible distortion in the comparison of university performance. A factorial analysis has been done on the inputs listed in Table 1 (NACSTA, ACSTAF, PEREXP, NOPEX) and on the base of its results (see Appendix A) an input factor was calculated for the empirical investigation.As usually used in applied econometrics, the size is computed as the logarithm of the total volume of the activity, that in our case is proxied by the sum of enrolled students at all undergraduate and post-graduate levels.Table 2reports some descriptive statistics (25th percentile, median, average, 75th percentile and standard deviation) on the sample that will be analyzed in the paper. It would have been interesting also to consider in the analysis other relevant variables, such as external research funding of universities. Unfortunately, this information was not available and hence has not been included in the analyses.55A potentially interesting line of future research could consist in formulating a network problem, either in terms of reallocation within university systems or thinking about actual success of graduates as ultimate outcome of interest (see e.g. Grosskopf, Hayes, Taylor, and Weber, 2012).We model European universities in a production activity framework. In this setup, universities are the producing units (hereafter ‘units’) and produce a set of outputsY∈Rqby combining a set of resources (inputs)X∈Rp. The production activity is characterized by the attainable set Ψ, the set of combination of the production plans (x, y) that are technically achievable:(4.1)Ψ={(x,y)∈Rp×Rq|xcanproducey}.We know (see Daraio and Simar, 2007) that the set Ψ can be described as:(4.2)Ψ={(x,y)∈Rp×Rq|HXY(x,y)>0},where HXY(x, y) is the probability of observing a unit (X, Y) dominating the production plan (x, y), i.e.HXY(x,y)=Prob(X≤x,Y≥y).The efficient boundary of Ψ is of interest and several ways have been proposed in the literature to measure the distance of the unit (x, y) to the efficient frontier. One of the most flexible approach is the directional distance introduced by Chambers, Chung, and Färe (1996). Given a directional vector for the inputsdx∈R+pand a direction for the outputsdy∈R+q,the directional distance is defined as:(4.3)β(x,y;dx,dy)=sup{β>0|(x−βdx,y+βdy)∈Ψ},or equivalently (as reported also in Daraio and Simar, 201466See the references cited there.):(4.4)β(x,y;dx,dy)=sup{β>0|HXY(x−βdx,y+βdy)>0}.Hence, we measure the distance of unit (x, y) to the efficient frontier in an additive way and along the path defined by(−dx,dy).This way of measuring the distance generalizes the ‘oriented’ radial measures proposed by Farrell (1957). Indeed by choosingdx=0anddy=y(ordx=xanddy=0), we recover the traditional output (respectively input) oriented radial distances. As we shall also discuss later, the flexibility of this approach relies in the possibility of setting some elements of the vector dxand/or of the vector dyto zero, for focusing on the distances to the frontier along certain particular paths (for instance if some inputs or outputs are non-discretionary, not under the control of the units, etc.).Consistent nonparametric estimators of Eq. (4.4) can be found in Daraio and Simar (2014) which analyzes in details the case when some directions are set to zero, as well as statistical issues in this context.1.Multi-input multi-output activity of universitiesThe approach described in Section 4.1 permits to model the activity of universities as multi-input multi-output production units. Ideally, we would like to compare European universities taking into account all their outputs of teaching, research and ‘third mission’. The data described in Section 3 are of great value at this purpose; however in our database third mission dimensions have a low coverage and for that reason where excluded. We run an exploratory data analysis (see Appendix A for further details) and given the high correlations observed among variables we ended up with the following variables to proxy the activity of universities. One input, FINP (a factor including NACSTA, ACSTAF, PEREXP, NOPEXP); and three outputs: TODEG5 (proxy of the teaching activity), FRES (a factor of research including PUB and TODEG6) and FQUAL (a factor of quality of research including IC, NI, Q1 and EXC). See Table 1.Target settingFor a discussion about the choice of a direction to approach the efficient frontier, see Färe, Grosskopf, and Margaritis (2008). The direction can be different for each unit (like in the radial cases) or it can be the same for all the units. Färe et al. (2008) argue that a common direction would be a kind of egalitarian evaluation reflecting some social welfare function.In this paper we select the same direction for all the units, setting a reference with respect to the European standard. The reference is made with respect to the median value calculated at European level on the analyzed sample.We adopt then an output directional distance in which the inputs are given (FINP), two outputs TODEG5 and FRES are non-discretionary (that means that are considered in the estimation of the production possibility set Ψ but are not active in the maximization) and one output, FQUAL, is the target. This means that universities are compared on their ability to produce quality of research (FQUAL, a factor of IC, NI, Q1 and EXC), given the inputs used and taking into account their teaching activity (TODEG5) and the volume of their research (FRES).In this paper we attempt to investigate how European universities are doing in the production of Excellent science, a pillar of the European Research Area. This attempt is possible thanks to the availability of comparable micro-data on European universities and their integration with the scientific production outputs described in Section 3. The path along which we compare European university performance to reach the efficient frontier is the same for all universities and corresponds to the median value of FQUAL, computed at European level.It is worth noting that in our case, by using only one discretionary output (all others are non-discretionary and their directions are set to zeros) the resulting ranks are independent of the direction value. That is, since only one direction is active, it does not matter (at a scaling factor) which direction we choose, e.g. the mean, the median and so on, this has no influence on the resulting rankings that will keep unchanged. Nevertheless, if we would add any other “active” direction (asking one of our non-discretionary output to become active), then the direction will play a role and the obtained ranking would probably be different. The flexibility of directional distances is that they allow us to model the phenomenon as we want. In our case, it is an output direction that focuses only on quality of research keeping the other outputs as fixed. Other modeling strategies could be chosen and this is thanks to directional distances flexibility. Moreover, a nice property of a constant (e.g. unitary, mean, median) direction vector is that it greatly facilitates aggregation.77We did not exploit this property in this paper, but consider it as an interesting topic for further research, along the lines of Färe, Grosskopf, and Primont (2007).See Fig. 1for an illustration. In Fig. 1 stars are the units and the arrows show the path of units to reach the efficient frontier; u is a university and u′ its projection onto the efficient frontier: given its value of TEACH and FRES (non-discretionary outputs), the unit has to improve in the production of FQUAL going from u toward u′.It may be useful for policy makers to measure, in original units of the outputs, the estimated distance of a unit to the frontier. This allows us to appreciate the efforts to be achieved in increasing the outputs and decreasing the inputs to reach the efficient frontier. This measure is given by what we call the ‘gaps’ to efficiency. They are directly given by:(4.5)Gx=β^(x,y;dx,dy)dx,andGy=β^(x,y;dx,dy)dy.Taking size and subject mix into accountFrom the literature review reported in Section 2 we know that both size and scientific specialization (SPEC) have a significant impact on the performance of European universities. In our model then we will condition the efficiency estimation to these factors, to account for their influence on the distribution of inefficiency, that is the distance of the units from the efficient boundary. In Section 4.4 we detail how to include these factors in the directional distance framework described above.We aim at comparing how European universities are doing in the production of Quality of Research (FQUAL), given their inputs and taking into account their teaching (TODEG5) and their volume of research (FRES), the latter outputs considered as non-discretionary outputs, and conditioning the comparison to the impact of SIZE and SPEC.A ‘fair’ comparison of university performanceThe conditional directional methodology described in Section 4.4 is useful also to make a step further in the comparison of European university performance. As illustrated in Section 4.5, we can ‘depurate’ the efficiency scores from the influence of SIZE and SPEC. These latter are two of the most important sources of heterogeneity of university performance. Our goal is to compare universities in Europe on the base of their ‘managerial’ ability, that is measured as the residual of the conditional efficiency score net of SIZE and SPEC effects.88However, also other variables could be considered, such as the localization of universities. The investigation of this variable effect is left for future research.Accounting for statistical robustnessOne of the main methodological issues of the literature on rankings is the statistical robustness of the proposed approach. In this paper we account for statistical robustness by applying directional distances estimators robust to extremes and outliers (see Section 4.3) and estimating bootstrap error bounds on the managerial efficiency scores (see Appendix B).Quantile frontiers for evaluating the performance of units by using oriented radial measures (input or output) are currently used (see Simar and Wilson, 2014 for a recent survey). Their adaptation to directional distance is quite natural after the representation given in (4.4). In place of looking at the support of the distribution HXYwe benchmark the unit against a point which leaves on average α × 100 percent of points above the frontier. This benchmark is the α-quantile frontier. Formally the α-order directional distance is defined as(4.6)βα(x,y;dx,dy)=sup{β>0|HXY(x−βdx,y+βdy)>1−α}.Here a valueβα(x,y;dx,dy)=0indicates a point (x, y) on the α-quantile frontier, a positive value is a point below the quantile frontier and a negative value is a point above the quantile frontier. We see clearly that when α → 1 we recover the full frontier definition.As shown in Daouia, Simar, and Wilson (2014), the directional distance and its estimate, by contrast with the radial measure, have the desired property of always being monotonically increasing with the input variables (the inefficiency score increases when the inputs increase, all other variables being fixed), see details in Daouia et al. (2014).A nonparametric estimator99Denoted in Section 5βα, FDH because it is the robust version of a directional distance based on a nonconvex FDH (Free Disposal Hull, Deprins, Simar, and Tulkens, 1984) estimator.can be found in Daraio and Simar (2014) which detail the case with some non-discretionary inputs and/or outputs that we apply in this paper.The projection of any (x, y) ∈ Ψ on the estimated α-quantile frontier is given by the points(x^α∂,y^α∂)defined as(4.7)x^α∂=x−β^α(x,y;dx,dy)dx,andy^α∂=y+β^α(x,y;dx,dy)dy.Since the resulting estimator will not envelop all the data points, the resulting frontier is more robust to outliers and extreme data points than its full version above.For the partial frontiers, the gaps appear as being the difference between (x, y) and the projections on the α-quantile frontier given in (4.7). They are particularly useful to detect outliers in the direction given by (dx, dy). This will be the case in the input direction ifGα,x=β^α(x,y;dx,dy)dxhas some elements with large negative values: the point (x, y) is well below the estimated α-frontier in the input direction, and/or a very large negative value in some elements of the vectorGα,y=β^α(x,y;dx,dy)dywarns a point being well above the quantile frontier.It is well known that nonparametric efficiency analysis gain in precision when working in space with lower dimensions (this is the usual “curse of dimensionality” of nonparametric techniques, see e.g. Daraio and Simar (2007), for a discussion). In our application, the original data are transformed before entering into the analysis, to reduce the dimension of the problem (by using input and/or output factors as defined in Daraio and Simar, 2007, p. 148 and followings). In this case, once the gaps have been computed for the variables used in the analysis, there is a need to evaluate the corresponding gaps in the original inputs and outputs. This can be achieved by transforming back the gaps in the factors into the original units. For more details, see Appendix A.In this section we introduce in the production model described above external or environmental factorsZ∈Rr. These variables are neither inputs nor outputs, and they are not under the direct control of the manager. However, they may influence the production process. A natural way for introducing these variables through conditional efficiency measures could be as follows.1010See Daraio and Simar (2007) for more details.The idea is very simple, we only have to replace HXY(x, y) in the above unconditional model byHXY|Z(x,y|Z=z)=Prob(X≤x,Y≥y|Z=z)where we condition to the value z of the external factors that the unit (x, y) has to face. In our setup here, this permits to define a conditional directional distance β(x, y; dx, dy|z). Daraio and Simar (2014) provide a nonparametric estimator ofHXY|Z(x,y|Z=z)when some directions are set to zero as well as its robust version1111Denoted in Section 5βα, FDH|Z.that we apply in this paper.This approach has been applied to our European university data for including SIZE and SPEC in the multidimensional evaluation of university performance.Many of the existing studies for investigating the effect of external environmental factors are based on simple two-stage regression analyses where estimated efficiency scores (input or output oriented) are regressed in a second stage against the Z variables. However we know from the literature (Simar and Wilson, 2007) that this is valid only under a restrictive ‘separability’ assumptions where it is assumed that the frontier of the attainable set is not changing with the values of z. As indicated in Badin, Daraio, and Simar (2012), the use of the estimated conditional efficiency scores for this second stage regression, does not require this assumption. We can evidently do the same here with conditional directional distances. The flexible second stage regression can be written as the following location-scale nonparametric regression model (the presentation here follows Daraio and Simar, 2014):(4.8)β(X,Y;dx,dy|Z=z)=μ(z)+σ(z)ɛ,whereE(ɛ)=0andV(ɛ)=1andμ(z)=E(β(X,Y;dx,dy|Z=z))andσ2(z)=V(β(X,Y;dx,dy|Z=z)).These two functions can be estimated nonparametrically from a sample of observations{Zi,β^(Xi,Yi;dx,dy|Zi)},i=1,…,nby using, e.g., Nadaraya–Watson or local linear estimates (see Daraio and Simar, 2014 for technical details). As shown with simulated samples in Badin et al. (2012), the analysis ofμ^(z)as a function of z will enlighten the potential effect of Z on the average efficiency, with the help ofσ^(z)which may indicate the presence of heteroskedasticity.An important result of the above approach is the analysis of the ‘residuals’. For a given unit we can define the error term:(4.9)ɛ=β(X,Y;dx,dy|Z=z)−μ(Z)σ(Z)This can be viewed as the unexplained part of the conditional efficiency score. If Z is independent of ɛ, this quantity can be interpreted as a ‘pure’ or ‘managerial’ efficiency measure of the unit since it is the remaining part of the conditional efficiency after removing the location and scale effect due to Z. It is called ‘managerial’ because it depends only upon the managers of units ability and not upon the environmental factors, and it represents an advanced and robust interpretation of the Leibenstein (1966)X-inefficiency theory.The label ‘managerial’ does not convey any analogy between universities and private firms. We fully recognize that universities are not maximizing an objective function under conditions of competition. We also recognize that universities are, at least in most European countries, governed by a complex governance in which the academic side is dominant with respect to management and/or stakeholders. Thus the label ‘managerial’ should not be interpreted literally. The label only emphasizes that part of the efficiency of universities may depend on internal decisions, with respect to adjustments in inputs (e.g. recruitment of academic staff) or outputs (e.g. offering of new courses in specific disciplines).What we have done is a kind of whitening of the conditional efficiency scores, from the effects due to the environmental-external conditions Z. We can use these quantities (the estimated ɛ, indicated asɛ^), which are standardized (mean zero and variance one), to compare the units among them on a fair base: a large value ofɛ^indicates a unit which has poor performance, even after eliminating the main effects of the environmental factors. A small (negative) value, on the contrary, indicates very good managerial performance of the unit. It allows us to rank the units facing different external conditions (SIZE and SPEC), because the main effects of these factors have been eliminated. Extreme (unexpected) values ofɛ^would also warn for potential outliers.As explained above, if we want to make an analysis which is robust to extreme and outlying data points, it is preferable to use the robust version of the efficiency scores βα(x, y; dx, dy) selecting a value of α near 1 to provide a robust version of the full frontier. In addition, by using the nonparametric estimator of these α-efficiency scores, we can build bootstrap confidence bounds for the resulting managerial efficiency scores, as described in details in Appendix B. The idea is to adapt the bootstrap algorithm provided in Daraio and Simar (2014) to our setup here.

@&#INTRODUCTION@&#


@&#CONCLUSIONS@&#
