@&#MAIN-TITLE@&#
Efficacy of utilizing a hybrid algorithmic method in enhancing the functionality of multi-instance multi-label radial basis function neural networks

@&#HIGHLIGHTS@&#
Proposing a hybrid algorithm based on GBMO for image classification.Introducing a new hybrid neural network for MIML problems.Utilizing SNPOM for decreasing the train and test times.

@&#KEYPHRASES@&#
Multi-instance multi-label learning,RBF neural network,Gases Brownian Motion Optimization algorithm,SNPOM method,

@&#ABSTRACT@&#
The facts show that multi-instance multi-label (MIML) learning plays a pivotal role in Artificial Intelligence studies. Evidently, the MIML learning introduces a framework in which data is described by a bag of instances associated with a set of labels. In this framework, the modeling of the connection is the challenging problem for MIML. The RBF neural network can explain the complex relations between the instances and labels in the MIMLRBF. The parameters estimation of the RBF network is a difficult task. In this paper, the computational convergence and the modeling accuracy of the RBF network has been improved. The present study aimed to investigate the impact of a novel hybrid algorithm consisting of Gases Brownian Motion optimization (GBMO) algorithm and the gradient based fast converging parameter estimation method on multi-instance multi-label learning. In the current study, a hybrid algorithm was developed to estimate the RBF neural network parameters (the weights, widths and centers of the hidden units) simultaneously. The algorithm uses the robustness of the GBMO to search the parameter space and the efficiency of the gradient. For this purpose, two real-world MIML tasks and a Corel dataset were utilized within a two-step experimental design. In the first step, the GBMO algorithm was used to determine the widths and centers of the network nodes. In the second step, for each molecule with fixed inputs and number of hidden nodes, the parameters were optimized by a structured nonlinear parameter optimization method (SNPOM). The findings demonstrated the superior performance of the hybrid algorithmic method. Additionally, the results for training and testing the dataset revealed that the hybrid method enhances RBF network learning more efficiently in comparison with other conventional RBF approaches. The results obtain better modeling accuracy than some other algorithms.

@&#INTRODUCTION@&#
Data classification methods play a key role in the analysis and interpretation of images. Generally speaking; the traditional data classification approaches assign a specific class to an image based on its global features. As frequently has been demonstrated, these approaches are functionally inadequate because they fail to cope with real-world problems. As a consequence, multi-instance multi-label framework develops a new and flexible learning model to handle this problem. This section aims to compare the overall features of MIML with other rival frameworks:Multi-instance learning (MIL): This framework is a weakly supervised learning [1]. The input is represented by sets of bags and each bag is identified by a specific label. Characteristically, a bag will be considered to be positive if it contains at least one positive instance. It is claimed that MIL can handle the ambiguities existing in the input space. In fact, in the past decade, numerous algorithms have been proposed for training classifiers capable of handling problems like boosting algorithm [2], support vector machine (SVM) and neural networks. Additionally, the MIL framework has a wide spread application in computer vision such as object detection [2], tracking and classification [3].Multi label learning (single-instance multi-label learning): The operational basis in multi-label learning consists of a set of labels assigned to a specific instance. In fact, each image is defined by an instance [4].Traditional supervised learning (single-instance single-label learning): In this approach, an object is represented by an instance. The instance is a feature vector associated with a particular class label [5]. This framework has a limited scope and it cannot formulize many real-world problems. The reason is that objects consist of complex features may be assigned to multiple labels simultaneously. For this framework, algorithms such as decision trees, neural networks, k-nearest neighbor classifiers and support vector machines are used. As an illustration, an image may belong to several classes at the same time and such cross-classification prevents it from being represented by a single label.Multi-instance multi-label learning (MIML): This is a learning framework which describes the training data by a bag of instances associated with a set of labels. This framework can satisfy a wider range of applications compared with other existing frameworks mentioned above. In facts, it can deal with many real-world learning problems. Here, a training image contains multiple patches or segments and each patch can be described by a feature vector; and the related image may belong to multiple classes. Apparently, the traditional supervised learning approach is a degenerated version of MIML; since each instance can only be described and represented by a single label. MIML has a few algorithms including RBF neural network [6]. Fig. 1illustrates the algorithms.MIMLBoost and MIMLSVM reduce the MIML problem to MISL and SIML problem respectively [7]. MIMLBoost uses MIBoosting [8] to solve MIL problem. These algorithms lose information during the reduction. MIMLSVM employs a cluster to map a bag of instances into a single instance. The clustered medoid solves the ML learning by utilizing MLSVM [9]. The MIML-KNN algorithm uses k-nearest neighbor method to make predictions which are based on the neighboring and citing examples [10]. The algorithm considers the correlation between instances and labels of MIML example and it is superior to MIMLSVM and MIMLBoost algorithms.D-MIMLSVM transforms a single instance or single-label example into MIML learning [11]. The method is very useful when we cannot extract sufficient information of the real problems. The algorithm defines an objective function to balance the loss between the labels and predictions. This algorithm can achieve better performance in comparison with MIMLBoost and MIMLSVM algorithms. The M3MIML algorithm can exploit the relation between the instances and labels using a maximum margin method[12]. The algorithm is faster than the MIMLBoost algorithm. The M3MIML algorithm obtains superior performance compared with MIMLSVM and MIMLBoost.Markov-MIML learning presents an efficient algorithm to reduce the computational cost for MIML learning [13]. The algorithm is a nearest neighbor approach to learn correct labels based on neighbor information as well as the affinities in a Markov chain. The MIML-KNN method determines k nearest neighbors of the unseen object and maximum posterior is used to calculate the label of object. The Markov chain enjoys the class probability of each object. It can spread through the neighbor during iterations to represent the importance of a set of labels to an object. The performance of Markov-MIML is better than M3MIML for some evaluation metrics. Moreover, it is faster than M3MIML and MIMLBoost.The MIMLNN algorithm uses two layers of multilayer Perceptrons (MLPs) [14]. The back-propagation limits the performance of the algorithm. The gradient descent has several drawbacks such as the dependency of the error surface, the starting point and the tuning parameters. The MIMLRBF method uses radial basis function (RBF) neural networks to estimate the relationship between instances and labels [6]. The algorithm utilizes a k-medoids clustering for the examples of each class. In the first step, the clustering algorithm uses Hausdorff metric to measure the distance bags and the classes can be coded to the medoids. In the second step, the weights of second layer are optimized by a sum of square error function. The MIMLRBF method achieves poor performance when an imbalance problem appears in the number of samples in class. The M3MIML only assumed linear models to solve the MIML learning problem while other nonlinear models such as RBF can handle the limitation.The improved MIMLRBF (IMIMLRBF) neural network improves the MIMLRBF to handle this problem. The algorithm applies an improved k-medoids clustering on the dataset [15]. The cluster number can be calculated by the data density for each class. The IMIMLRBF take much more time than MIMLRBF, while the performance of algorithm is better than that of MIMLRBF for unbalanced samples. EnMIMLNN replaces the back-propagation neural network in MIMLNN with RBF [14]. The EnMIMLNN algorithm combines different distance metrics to determine the distance between the proteins. The algorithm achieves better performance than MIMLNN, MIMLBOOST and MIMLKNN.MIML problem can be solved using a Gaussian process. The algorithm considers a likelihood function and covariance matrix to represent, simultaneously, the connection between the instances and each label as well as the correlation among labels [16]. The performance of the algorithm is better than MIMLRBF. The efficiency of the algorithm is weaker than MIMLRBF and MIMLSVM because the Gaussian process spends more time.The MIML framework has been used successfully in a variety of applications including audio analysis, text categorization, bioinformatics [17] and computer vision. The video annotation task is inherently a MIML learning problem. As a matter of fact, the En-MIMLSVM is a new approach for the video annotation task [18]. The MIML learning is used for automatic tag recommendation and each Web page is divided into a bag of instances [19]. For automatic object detection tasks, a large number of training images are labeled by a multi-task multi-label multi-instance learning (MTML-MIL) [20].The performance of RBF neural network depends on the center and the width of radical basis function and the weight values. The RBF neural network learning has significant drawbacks. It uses the results of the optimal solution to determine the structure parameters. In this section, a new hybrid search method is proposed to achieve better results for RBF model in the MIMLRBF. Our proposed algorithm is developed by the GBMO algorithm and the SNPOM method (GBM-SNPOM). In the search strategy, the search capability of GBMO and efficiency of the gradient search are exerted. The proposed algorithm applies the global search of GBMO and the fast convergence of SNPOM. In the same vein, the algorithm combines the fast convergence rate and strong global search capability which can improve speed of convergence, stability, and modeling accuracy of RBF. Therefore, in this paper, the GBM-SNPOM algorithm was used to optimize the RBF learning strategy. The purpose of training in the proposed algorithm is to determine the spread and centers of the nodes of hidden layer and the weights of the output layer. The spread and centers are nonlinear parameters and weights are linear parameters. Proper values are set for these training parameters of RBF with the GBMO algorithm. The parameters can be estimated by a fast convergence optimization. The nonlinear parameters are estimated by LMM; the LSM is used for linear optimization using SVD for weights estimation. The findings demonstrated the superior performance of the hybrid algorithmic method. Additionally, the results for training and testing the dataset revealed that the hybrid method enhances RBF network learning more effectively in comparison with conventional RBF approaches. In the same vein, better modeling accuracy was obtained considering the other algorithms.The rest of this paper is organized as follows. Section 2 includes MIMLRBF structure and evolutionary algorithms. Section 3 introduces the architecture and the training phase of the proposed algorithm. In Section 4, the result of the scene and text datasets are reported. The last section includes the conclusions and suggestions for future research. The goals of the paper are listed as follows:(1)Proposing a new hybrid algorithm by introducing a new training algorithm based on the GBMO and the SNPOM method.Introducing a new hybrid neural network based on the robustness of evolutionary algorithm and the efficiently of the gradient search.Utilizing the SNPOM to increase the speed of convergence.In this section, we introduce the details of MIMLRBF structure. In Section 2.2, we briefly review some evolutionary algorithms.MIMLRBF is composed of two levels of nodes [6]. In the first level, each instance is described by a vector. In the second level, each output node is considered as a possible class. Fig. 2shows the MIMLRBF neural network and the input with a bag of instances X={x1, x2, …, xn} where each instance xnis a d-dimensional feature vector X={xn1, xn2, …, xnd}T. The typical architecture of a MIMLRBF neural network is shown in Fig. 2.The MIML training set isS={xi,yi|1≤i≤N}and the training set with the lth label is Ul. The k-MEDOIDS algorithm is used to partition Ulinto Mlgroups of bagsGjl(1≤j≤Ml). Mlis set to be fraction α of the number of instances in Ul,Ml=α×Ul. The average Hausdorff distance between two bags A and B may be computed by:(1)aveH(A,B)=∑a∈Aminb∈Bdist(a,b)+∑b∈Bmina∈Adist(a,b)A+B,and the medoidCjlof clusterGjlis defined by(2)Cjl=argminA∈Gji∑B∈GjiaveH(A,B).The width of nodes σj (standard deviation) is a parameter controlling the basis function (·). In this network all the (1≤j≤M) have the same σ value, which is the average distance between each pair of medoids in the input level. σ is calculated by:(3)σ=μ×∑p=1M−1∑q=p+1MaveH(Cp,Cq)M(M−1)2where μ is the parameter of scaling factor and The number of prototype vectors in the hidden layer isM=∑l=1QMl. Finally, for the weights between first and second layers, we use the SVD method and ϕj(xi) which is the activation of the jth basis function on xi.(4)ϕjXi=exp−aveH(Xi,Cj)22σj21≤j≤MThe weights are optimized by minimizing the following sum-of-squares errors functions(5)E=12∑i=1N∑l=1Q(yl(Xi)−tli)2,wheretliis the desired output of ith instance of lth class. The weight matrix between first and second layers isW=[wjl]M+1×Q. The actual output of xion the lth class isyl(xi)=∑j=0Mwjlϕj(xi). Differentiating the error function of Eq. (5) with respect towjland, setting the derivative to zero gives the normal equations for the least sum-of-squares problem as follows:(6)(ϕTϕ)W=ϕTT.The weight parameters are computed by Eq. (6) using singular value decomposition (SVD) [6]. In this algorithm, the output neurons are connected with basis functions corresponding to all classes. The hidden layer performs clustering on training instances of each possible class. The weight parameters are determined by minimizing the sum-of-squares error function. The test phase is used for prediction based on the trained neural network. The weight matrix can be obtained by optimizing the error functionJ(W)=||Yij−WϕijT||2where ϕijis the result of jth radial basis function andYijis jth element of ith target vector. The solution is the smallest Euclidian norm(W*)T=ϕ+YijTwhereϕ+ is pseudo inverse matrix of ϕij. In this method,ϕ+ should be a full rank; otherwise, it causes large variations in W* for noisy data [6]. In the real problem SVD increases error in RBF.In the noisy training data, the performance of MIMLRBF reduces because the SVD method augments the error during the network. The steepest descent (SD) method can optimize the weights of output layer [21]. The weights can be computed by the gradient of J. The MIMLRBF-SD method performs better than MIMLRBF. The approach is very effective while consume more time than MIMLRBF. In this paper, an evolutionary algorithm and the structured nonlinear parameters optimization method were used to increase the speed and efficiency of the proposed algorithm.The evolutionary algorithms were developed to solve optimization problems in wide range of applications [22]. An evolutionary algorithm is defined as a population-based meta-heuristic algorithm which uses an evolution process of biological populations. One of the evolutionary algorithms is called genetic algorithm (GA) which was proposed by Holland [23]. There have been a number of other evolutionary algorithms such as the particle swarm optimization algorithm suggested by Kennedy and Eberhart [24] in 1995, and the cultural evolutionary algorithm (CE) developed by Reynolds and Jin [25] in the early 1990s. Another type of evolutionary algorithm was the ant colony optimization algorithm (ACO) which was used to discover the particulars of ant's behaviors [26]. The process of musicians’ speed and the performance was investigated by enlisting harmony search (HS) algorithm [27]. differential evolution (DE) algorithm originally was introduced by Storn and Price [28]. The algorithm was used to search problem space by population-based stochastic search method. DE uses mutation, crossover, and selection operators at each generation. Recently, some meta-heuristic algorithms have been designed for optimization. For instance, the charged system search (CSS) [29], the gravitational search algorithm (GSA) by Rashedi et al., and the imperialist competitive algorithm (ICA) proposed by Atashpaz-Gargari and Lucas [30]. Finally, a novel algorithm designed by Abdechiri et al. named Gases Brownian Motion Optimization (GBMO) [31] utilizes the characteristics of gas molecules and their movements.The GBMO algorithm is based on the laws of gases. Gases Brownian motion and turbulent rotational motion of molecules are used for finding optimal solutions [31]. In the algorithm, the agents are molecules and their performance is measured by their positions. Each position presents a part of solution, and the algorithm is directed by properly adjusting the Gases Brownian Motion and turbulent rotational motion. All these molecules move in the space of the problem, and this motion causes a movement of all molecules towards the solution. During the early part of the search, high temperature in the gas causes increases in kinetic energy and velocity of molecules. So, the Gases Brownian Motion causes global exploration of the full range of the search space. Furthermore, the turbulent rotational motion has a good performance for local search.In the GBMO algorithm, local exploitation is more accurate in the optimum solution. In low temperature turbulent rotational motion plays a role global search and Gases Brownian Motion play a role local search in the space problem. In the GBMO algorithm, the search is faster than that of the PSO, ICA and GA algorithms because the velocity of molecules is very fast. Simultaneously, the local and global search capabilities pave the way for that the search to exploit and explore the solution space more effectively.Evolutionary algorithms have some advantages for optimization problem; including, powerful global search, insensitivity to function, and non-obligatory (drivability, continuity). Here, a new algorithm was developed using these characteristics in order to improve RBF learning. Generally, evolutionary algorithms require a great number of iterations and converge very slowly. We use the SNOPM to solve this problem and accelerate the rate of convergence of the parameters optimization in RBF network.The performance of RBF neural network depends on the center and the width of radical basis function and the weight values. The RBF neural network learning has significant drawbacks because of finding the optimal solution to determine the structure parameters. Incorrect solution of parameters cause decline in the accuracy and divergence. RBF network can model efficiently nonlinear dynamic model of the complex system while the identification of model is a difficult optimization problem containing many local minimums. The RBF learning is sensitive to initial value and can trap in a local optimum.Three types of algorithms are applied to estimate RBF parameters. The first, the nonlinear optimization algorithms use an exhaustive search and consume more time to find a good estimation such as Levenberg–Marquardt algorithm [32]. The second, the centers and spreads are estimated randomly and the weights can be estimated by linear least-squares method (LSM). The solutions of these algorithms are not optimal. The third, the methods divide the parameters search space into subspaces, including nonlinear subspace and linear subspace. The methods combine nonlinear optimization and linear optimization. The parameters of model can be optimized simultaneously by LMM and LSM. The SNPOM is one of these method which divides search space into two subspaces [33,34]. The SNPOM can accelerate the computational convergence of the parameter optimization in RBF network.In this section, we propose a new hybrid search method to achieve better results for RBF model. Our proposed algorithm is developed by the GBMO algorithm and the SNPOM method (GBM-SNPOM). In the search strategy, the search capability of GBMO and efficiency of the gradient search was used. Our algorithm applies the global search of GBMO and the fast convergence of SNPOM. The algorithm combines the fast convergence rate and strong global search capability which can improve speed of convergence, stability and modeling accuracy of RBF. Therefore, in this paper, the GBM-SNPOM algorithm was used to optimize the RBF learning strategy.The purpose of training in the proposed algorithm is to determine the spread and centers of the nodes of hidden layer and the weights of the output layer. The spread and centers are nonlinear parameters and weights are linear parameters. We set proper values for these training parameters of RBF with the GBMO algorithm. The GBMO algorithm encodes all of these parameters into a molecule. The parameters of the network are real-coded and optimized by a gradient-based fast-converging parameter estimation method. However, it is well-known that evolutionary algorithms generally require a great number of iterations and they converge very slowly. Moreover, there are usually a large number of parameters (i.e. centers and widths) to be optimized in an RBF network. It is difficult to find satisfactory results for estimating parameters of the RBF network. The architecture is evolved by the GBMO algorithm and the parameters are optimized by the SNPOM for each generation of the GBMO. The network parameters are not optimized directly by GBMO. The parameters can be estimated by a fast convergence optimization. The nonlinear parameters are estimated by LMM and the LSM is used for linear optimization using SVD for weights estimation. The parameter classification of the algorithm is introduced by linear parameters θLand nonlinear parameters θNin GBM-SNPOM. The set of linear parameters θLconsist of the weightswi. The set of nonlinear parameter consists of θNis the parameters Ciand σi. The SNPOM rewrites the RBF model [35] as forms (7) and (8) to estimate the parameters.(7)y=fθL,θN,X,(8)y=Ψ(θN,X)TθLThe proposed hybrid training algorithm for RBF networks is implemented as follows:Step 1:Initializing parameters: The GBMO starts with an initial population of N molecules representing a set of parameters.Step 2:Setting the range: Random radius of turbulence for every molecule is set within the range [0,1].Step 3:Assigning a temperature: A temperature will be assigned to the system. This guarantees the convergence of the GBMO algorithm because temperature can influence the velocity of molecules. It should be noted that the temperature has an important role in controlling the exploitation and exploration ability of the algorithm since it can influence the velocity of molecules.Step 4:Calculating the fitness function: The fitness function is the mean square error (MSE) for the training or testing data, which is obtained by the SNPOM. The parameters are optimized by the RBF model (7) and (8). The objective function is taken to be:The optimization calculation on the search forθNk+1at the kth iteration, followed by the immediate updates of linear weightsθNk+1using the LSM is as follows:(12)θLk+1=∑t=1MΨ¯t,k+1Ψ¯t,k+1T−1∑t=1MΨ¯t,k+1y¯(t),Ψ¯t,k+1=Ψ(θNk+1,X¯(t))and the nonlinear parametersθNk+1are updated by the following formula.(13)θNk+1=θNk+βkdk,where dkis the search direction, and βkis a scalar step. The search direction dkis obtained from a solution of the set of linear equations:(14)[J(θNk)TJ(θNk)+γkI]dk=−J(θNk)TF(θLk,θNk)whereJ(θNk)is the Jacobian matrix ofF(θLk,θNk)with respect toθNkand the γkis non-negative damping factor to control both the magnitude and direction of dk, I, is identity matrix. LMM is used to solve (14) to obtain the search direction. The size of γkis determined by Isqnonlin in Matlab [36]. The line search is used to calculate βksuch as extrapolation method [36].Step 5: Updating the velocity and position. In GBMO algorithm,xidandvidwould be calculated as follows:Step 6: Fitness evaluating step. The values of fitness functions for the molecules are evaluated.Step 7: Updating the turbulent rotational motion. Every molecule, besides the Brownian motion, has a turbulent rotational motion (vibration in specific radius for which the radius is a random number in the interval [0,1]). The turbulent rotational motion is:With a=0.5 and b=0.2, it generates a chaotic sequence in (0,1) andxidis the current position in turbulent rotational motion, thus the GBMO is a stochastic search algorithm.Step 8:Evaluating and comparing step: To evaluate and compare the values of the objective function with the new positions of molecules.Step 9: Updating the mass and temperature value. We update the molecules’ masses by the following equations:The temperature will be updated by T=T−1/mean(fiti(t)) where fiti(t) represents the fitness value of the molecule i at the time t.Step 10: Repeating the search steps 4–10 until T=0. The learning process is shown in Fig. 3.Our algorithm uses the advantages of global and local optimization algorithm. The local optimization which is based on the gradient method can lead to speed of convergence. Our algorithm can reduce the risk of trapping in the local optimum using the powerful global search of GBMO algorithm.

@&#CONCLUSIONS@&#
