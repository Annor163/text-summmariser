@&#MAIN-TITLE@&#
Fuzzy-neural self-adapting background modeling with automatic motion analysis for dynamic object detection

@&#HIGHLIGHTS@&#
We propose a novel fuzzy-neural background modeling robust to scene changes.The model involves self-adapting threshold and learning rates mechanism.The system involves scene analysis to automatically update the model parameters.An automatic optical flow-matting process improves dynamic object segmentation.The model shows competitive performance compared with state-of-the-art models.

@&#KEYPHRASES@&#
Object detection,Background modeling,Video analysis,Self organized maps,Fuzzy system,Optical flow,

@&#ABSTRACT@&#
In this paper we propose a system that involves a Background Subtraction, BS, model implemented in a neural Self Organized Map with a Fuzzy Automatic Threshold Update that is robust to illumination changes and slight shadow problems. The system incorporates a scene analysis scheme to automatically update the Learning Rates values of the BS model considering three possible scene situations. In order to improve the identification of dynamic objects, an Optical Flow algorithm analyzes the dynamic regions detected by the BS model, whose identification was not complete because of camouflage issues, and it defines the complete object based on similar velocities and direction probabilities. These regions are then used as the input needed by a Matte algorithm that will improve the definition of the dynamic object by minimizing a cost function. Among the original contributions of this work are; an adapting fuzzy-neural segmentation model whose thresholds and learning rates are adapted automatically according to the changes in the video sequence and the automatic improvement on the segmentation results based on the Matte algorithm and Optical flow analysis. Findings demonstrate that the proposed system produces a competitive performance compared with state-of-the-art reported models by using BMC and Li databases.

@&#INTRODUCTION@&#
Segmentation of dynamic objects for video analysis has turned out to be an indispensable task for different kind of applications such as surveillance systems [1,2], automatic robot navigation [3,4]; entertainment industry [5,6], etc. Nevertheless, because most of these segmentation algorithms are application oriented it is complicated to affirm which of them produce the most accurate definition of dynamic objects.As reported on many surveys, the most common approach used to identify dynamic objects in video sequences is based on Background Subtraction, BS, algorithms [7–9]. The first stage of a BS algorithm is to build a background model, B, considering N initial frames of the video sequence, then each incoming frame is subtracted from B and the result is defined as the foreground, F, which may contains the dynamic objects. A very important step on a BS algorithm is the B maintenance to assure that new video circumstances are incorporated into B to avoid false positive regions on F. These subtraction and maintenance steps continue until the end of the video. Therefore, segmentation models based on BS must define optimal threshold values on the subtraction step and learning factors on the maintenance stage.The most common algorithm used in BS models is based on Gaussian Mixture Models, GMM. Yoshinaga presented in [10] a GMM spatio-temporal BS model that performs a region level statistical analysis with a sensitive selection of 9 parameters which are continuously updated as the video is analyzed. Yoshinaga's model was validated with the Background Models Challenge, BMC, database [11] achieving the best performance reported in the literature with this database. Chen and Ellis proposed in [12] a model based on GMM with an adaptive parameter update defined as SAM. SAM implements special filters to suppress image noise and sudden illumination changes. Also the model can handle shadows and reflection highlight issues and a final morphological operation was incorporated to fill holes in the final F. The segmentation results reported with SAM were not very accurate arguing that the different segmentation issues that affect the F cannot be solved simultaneously by only one BS model because of the different needs associated with the semantic interpretation of the F and B. Spampinato et al. proposed in [13] a texton based kernel density estimation based on color and texture features. Texton was validated with three different databases: I2R (available at http://perception.i2r.a-star.edu.sg/bk_model/bk_index.html), Fish4Knowledge and BMC. Texton reported problems with night videos, scare illumination, severe dynamic background and rain/snow scenarios. Models based on biological process have also achieved very accurate definition of dynamic objects, this is the case of Maddalena and Petrosino that proposed the 3dSOBS in [14]. 3dSOBS is a BS model based on the SOM neural architecture where each pixel is represented by a map of 3×3×5 neurons. Similar to Yoshinaga, the 3dSOBS needs a careful initial definition of parameters (in this case 12 parameters) to produce the F definition. The DR-SOM, proposed in [15], is a neural inspired model approach based on the mechanisms of the visual cortex. The neural architecture of DR-SOM is mainly SOM. DR-SOM reported very accurate segmentation results on dynamic background and illumination changes. Similar than SAM, DR-SOM has the capability to auto-adapt its parameters as the video is analyzed. DR-SOM was validated with BMC achieving the second best performance compared against State of the Art, SoA, models.Based on the works aforementioned we can appreciate that even when some of them auto-adapt their parameters as the analysis is carried out, the models tend to be very sensitive to their initial definition. Besides, most of these models are only validated with one video database, leaving as an open question how will be their F results with different video scenarios by using the same initial parameter definition. In addition, both statistical and neural inspired algorithms have in common that the model of each pixel is defined by a number of statistical distributions or neurons. However, the parameters in the neural models represent a lower computational burden. For these reasons, a new neural BS algorithm, where the B maintenance is highly adaptive is proposed in this paper. Depending on the difference between the input frame and the background model the learning parameters are treated differently considering three possible scenarios: a stable scene, scene with normal changes and a scene with drastic changes. The last stage in most BS models considers morphological operations to improve the segmentation results. In our proposed model, an automatic motion analysis and an optimization function is used to improve the F results. Therefore the proposed system is completely automatic and auto adaptive.In order to test our auto-adaptive feature to any video condition, we decided to perform our validation by using two different databases with the same initial parameter definition. These databases are BMC [11] and Li [16]. We compare our proposed model with SoA demonstrating our high accuracy in the F definition and competitive performance. As our knowledge, this validation has not been performed previously by any other authors, therefore, it represents a novel BS model validation.The rest of this paper proceeds as follows: Section 2 describes the different modules of the dynamic segmentation model proposed in this paper explaining how they are combined in an automatic way; Section 3 presents qualitative and quantitative results and Section 4 expose our conclusions.Our proposed BS model is based on the SOM neural architecture working in combination with a Fuzzy System. Neural Networks (NN) and Fuzzy Systems are among the soft computing theories most frequently adopted in computer vision literature [17]. NN has demonstrated their adaptability to changes in the environment and their capacity to learn and incorporate new representations of the input space, whereas Fuzzy Systems allows handling the imprecision and uncertainty inherent in the background subtraction approach to define and update the parameters of the model. This work was initially presented in [18] where a Fuzzy System continually defines the optimal threshold values as the video is analyzed, but the learning rates (LRs) are maintained constant. In this paper, we developed the idea to adapt the LRs automatically according to a continuous evaluation of two parameters: the difference of the Value color component (of the HSV color space) between the input frame and the B; and the number of pixels detected on F. Because of this analysis, the system is defined as SOM-DVA, SOM with Difference Value Analysis. We observe that a video may present three different situations: a stable scenario, scene with normal changes and scene with drastic changes. Depending on these situations, the LRs can be adapted differently in order to accelerate the B maintenance stage and produce a better definition of dynamic objects. A block diagram of our proposed model is shown in Fig. 1. A six-rule fuzzy system determines the BS threshold values Th1 and Th2. Then, the BS model performs a pixel–neuron comparison between the input frame and the B model and decides, at a pixel resolution, if they belong to F or B. Next, the scene is analyzed to update the LRs values that will be used in the weights update of the SOM. In the last stage of our model, a motion analysis based on the Optical Flow, OF, algorithm, encloses within an ellipse the dynamic objects detected by the SOM. These objects are later automatically analyzed by an optimization Matte algorithm to improve their definition in F. The next sections will explain in more detail all these different stages.Let each pixel of the video frame be represented by its HSV color information p(x, y, t)=[H, S, V]Twhere x and y are the spatial coordinates, and t denotes time. The HSV color space was selected based on the work developed by Karasulu and Korukoglu in [19] where it is indicated that HSV is a most common cylindrical-coordinate representation of points because of its low computational complexity and its good quantization color space that produces better segmentation results compared with other color spaces. In SOM-DVA there is a one to one correspondence between the pixels of the image and the neurons of the SOM. Therefore each pixel of the first frame is mapped into a neuron W(x, y, t) to build the initial B model. From the second frame, the result of the Euclidean distance in the HSV color hexcone is verified with the following rule to determine if the pixel is part of a moving object, F(1)Ifepw(x,y,t)>Th1and|p(x,y,t)V–W(x,y,t)V|>Th2thenp(x,y,t)∈F(x,y,t)whereepw(x,y,t)=p(x,y,t)−W(x,y,t)is the Euclidean distance and the second part of the rule reduces shadow pixels detected erroneously as F. If this rule is false the B model is updated by(2)W(x,y,t+1)=W(x,y,t)+LRj[p(x,y,t)−W(x,y,t)],j=1,2.where LR1 refers to the learning rate of the winning neuron, and LR2 corresponds to the learning rate of the 8 neighborhood neurons. The weights of the neurons W(x, y, t) represent the background model of the scenario. An example of a B pixel and its neighborhood update is shown in Fig. 2. Because the HSV color information of the pixel under analysis (input frame) and the corresponding neuron (background model) is similar, the winning neuron weight W is updated with the information of the input pixel with a LR1 weighting, whereas the 8 neighborhood neurons are updated with the LR2 parameter.The thresholds Th1 and Th2 are defined by the following fuzzy rules proposed and broadly explained in [18],R1.IfSBis LowANDVBis HighThenTh1 and Th2 are High.R2. IfSBis MediumANDVBis MediumThenTh1 and Th2 are Medium.R3. IfSBis HighANDVBis LowThenTh1 and Th2 are Low.R4. IfSBis MediumANDVBis HighThenTh1 and Th2 are Medium.R5. IfSBis LowANDVBis MediumThenTh1 is High and Th2 is Medium.R6.IfSBis HighANDVBis MediumThenTh1 is Low and Th2 is Medium.The aforementioned situations that may occur in a video scene: a stable scene, a scene with normal changes and a scene with drastic changes; can be detected by an analysis of the V difference between each pixel and neuron defined as Difference Value Analysis, DVA,(3)DVA=1M×N∑x=1M∑y=1Np(x,y,t)V−W(x,y,t)V,where M×N is the size of the frame to process.A stable scene is detected with Eq. (4)(4)if∑x=1N∑y=1MFN×M≤ThStable→LR1=0.004andLR2=0.002where ThStable=0.18.A scene with normal changes is defined as follows(5)if∑x=1N∑y=1MFN×M>ThStableand∑x=1N∑y=1MFN×M≤ThDCandDVA>0.1→LR1=DVAandLR2=DVA2where ThDC=0.26.In a scene with drastic changes, it is required to update the B model drastically with the incoming input information and the LRs must be adjusted in a fast way, this is performed by Eq. (6)(6)if∑x=1N∑y=1MFN×M>ThDCandDVA>0.1→B=Inputframe,LR1=DVAandLR2=DVA2By implementing Eqs. (3)–(6) the LRs are maintained high adaptive according to the video scene variations, accelerating the B update when drastic changes have occurred. The values of the thresholds in Eqs. (4)–(6) were determined by analysis of a set of different videos.In order to improve the definition of F, some researches only perform morphological operations in the final F results [8,12,15,16], increasing in some cases the False Positive, FP, detection rate. An important contribution in this work is that instead just applying morphological operators to improve the F results, we propose the use of the Optical Flow algorithm to perform a motion analysis on the F results of SOM-DVA. The OF algorithm automatically identifies the dynamic objects and will enclose them within an ellipse. These regions will be analyzed by a Matte (also known as Matting) algorithm to increase the detection of True Positives, TP, in F. Most of the time Matte algorithms are implemented in a supervised way [20,21], in this work the Matte algorithm is implemented in an unsupervised manner. In order to differentiate between our proposed models with and without the OF and Matte analysis, they are defined as SOM-DVAM and SOM-DVA respectively.OF algorithms are widely used in the literature to analyze motion information, but an important contribution in this paper is the way how this information is used. The Lucas Kanade (LK) algorithm is a gradient-based method commonly used to compute optical flow [22]. The LK algorithm looks into a small neighborhood (commonly within a 3×3 window) and assumes that the optical flow on those pixels is the same (smoothness assumption). Since the OF analysis over the entire F region, the foreground previously defined by the SOM-DVA model, will increase the computational load of our model, we propose to use the corner detector algorithm (CDA) developed by Noble in [23] to find Interest Points (IP) in the F region and only calculate the OF on them. Considering that the videos used to validate segmentation algorithms have an average acquisition frame rate of 30 frames per second, it was decided to compute the CDA every 10 frames because they will not change significantly in this short period of time. The IPs are calculated by Eq. (7).(7)IPR=DetHTr(H)+εwhere IPR stands for Interest Point Response, H represents the two-component information variation in F(8)H=∑x,ygw(x,y)FxFxFxFyFxFyFyFy,Fx=∂F∂x,Fy=∂F∂y,wheregw(x,y)is a Gaussian window and ɛ is a small constant used to avoid a singular denominator. If IPR is greater than an established threshold (commonly 1000), it is considered as an IP.(9)IfIPR>1000⇒IPOnce all the IPs have been defined, their OF information, u, is calculated by(10)uK=(CTC)−1CTftwhere(11)C=∂FIPK∂x1∂FIPK∂y1⋮⋮∂FIPK∂x9∂FIPK∂y9,ft=−∂FIPK∂t1⋮−∂FIPK∂t9and K is the number of IP.We propose to analyze the OF of each IP with a probability function that will define the probability of whether IP is moving from left to right or right to left in order to have a better understanding of the scene. This probability function, PKl, is defined by(12)PKl=1σ2πe−(uK−μ)22σ2,l=1,2where μ represents the mean and σ the standard deviation of the OF, and l denotes if the object is moving from right to left, 1, or from left to right, 2. In this work, close disjoint IPs that move in the same direction based on Eq. (12) are merged and be considered as part of only one dynamic object that presents camouflage issues. The merge operation is performed with a dilatation function on its corresponding F region, FD=Dilation(F). After dilation, there will be i different dynamic objects identified, DO.The area, centroid, orientation, major and minor axis of each DO will be calculated to define the ellipse that encloses it. At each time t, the total area of all DOs, DOTA(t) is compared against the previous total area, DOTA(t−1), and three possible situations may occur:i)If DOTA(t)<DOTA(t+1) it is possible that an object presents a camouflage issue. Therefore, the previous calculated area is considered to define the ellipse that will enclose it. The OF information will be used to update correctly the ellipse's centroid.If DOTA(t)>DOTA(t+1) may be two different interest objects that were initially together are now separated. Therefore, the current DOs information is used to define each ellipse.If DOTA(t)≈DOTA(t+1) the total area did not change much; we use the actual DO information to define the ellipse.This analysis is defined as the memory of previous dynamics regions area(s) and is summarized in the following equation(13)ellipse(DOt)=ellipse(DOt−1)ifDOTA(t)<DOTA(t−1)ellipse(DOt)ifDOTA(t)>DOTA(t−1)orDOTA(t)≈DOTA(t−1)In order to consider new dynamic objects and also eliminate those that have disappeared, this memory will reset each 10 frames as in the case of the CDA definition.Once the ellipse of each DO has been defined, we can proceed with the automatic input definition for the Matte algorithm. The input of a Matte algorithm is defined as trimap, and it is formed of three parts, foreground, F, background, B and unknown, U. The U region is the one that contains a mixture of F and B pixels that needs to be improved. Therefore, in this work we propose to maintain the F region previously defined by the SOM-DVA model, the U region will be defined as the area within each ellipse that was not classified previously as F, and B is the original background except the background inside of the ellipse defined by the motion analysis described in Section 2.2.Mathematically, in a Matting algorithm, the image I is modeled by a combination of F and B parameterized by an alpha matte value [24](14)I=αF+(1−α)Bwhere α can be any value in [0,1] and represent the pixel's foreground opacity. When α=1, the pixel belongs to the F region, if α=0 it is a B pixel. Then, the matting algorithm estimates α values in the unknown region U of the image to define F and B regions within it. The proposed matting algorithm used in this work is a non-supervised version based on Qifeng et al. [25] where the optimization function has the following form(15)h=αTLα+λ∑i∈s−vαi2+λ∑i∈v(αi−1)2where v is a binary vector of pixel indices corresponding to the F region, s is a binary vector that contains the indices of all F and B defined pixels, λ is a constant that defines the influence of those defined regions and L is the un-normalized graph Laplacian matrix defined by L=D−A where A is an affinity matrix, D is a diagonal matrix whose values are defined asdi=∑j=1naij,aij∈A. In other words, the second term of Eq. (15) considers the pixels that have been already classified as B, the third term the pixels already classified as F and the first term considers the affinity between all the pixels of the image to find an optimal classification in the U region. The incomplete Cholesky factorization and the preconditioned Conjugate Gradients methods are used to solve (15).

@&#CONCLUSIONS@&#
Because many factors affect the performance of a video segmentation algorithm perfect segmentation of dynamic objects is a very challenging task. In order to deal with some of these factors, this paper presented an approach based on the neural SOM architecture that auto adjusts its parameter as the video is being analyzed. A fuzzy system defines optimal threshold values and a scene analyzes decides the learning factor parameters whose values affect the weight updates of the neurons. Besides, our proposed model includes motion analysis with the OF algorithm followed by a Matte analysis in order to increase the definition of dynamic objects. Most algorithms that implement a Matte algorithm require an initial user intervention to define the region where a detailed analysis is necessary. In this paper, we proposed a system that automatically defines this region, eliminating the action of the user. Therefore, our system is completely unsupervised with automatic parameter updates.Another important contribution of this work is that our model analyzes the video and identifies three different scenarios. Using this analysis, the LRs are adapted differently. Thus, the adaptation to changes is faster compared with SoA models as demonstrated by the results in Fig. 4. Also, many SoA models just implement morphological operations on the final stage of the BS model to increase the FP detection. In our proposed model, this improvement on F is carried out with the automatic OF-Matte analysis. The metric results validated this idea because the performance is increased in Re, F-Measure and Ssim.A common approach used by most BS models is to carefully define the thresholds and/or learning rates for each database (or video) to be analyzed. In this paper, we demonstrated that our model can adapt better to changes in the scene because of its adapting mechanism. We consider that this is a major contribution because the results that we reported are very competitive with SoA models, which need pre-training stages or manual adjustment to determine their parameters in order to achieve their best performance every time they change the video to be analyzed.