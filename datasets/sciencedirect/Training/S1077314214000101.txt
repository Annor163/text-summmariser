@&#MAIN-TITLE@&#
Identifying the writer of ancient inscriptions and Byzantine codices. A novel approach

@&#HIGHLIGHTS@&#
We date ancient documents by automatically identifying their writer.The proposed writer identification method does not require training set or database.First, a new approach is introduced using the letter’s contour curvature.Proper statistical methods are introduced for classifying the documents per writer.Application to 46 ancient inscriptions and 23 Byzantine codices with full success.

@&#KEYPHRASES@&#
Writer identification,Dating ancient inscriptions,Dating Byzantine codices,Curve fitting,Curvature,Letters similarity,Pattern recognition for manuscript classification,

@&#ABSTRACT@&#
In this paper, a novel methodology is presented aiming at the automatic identification of the writer of ancient inscriptions and Byzantine codices. This identification can offer unambiguous dating of these ancient manuscripts. The introduced methodology is also applicable to contours of complexes of letters or any class of similar curves. The method presented here initially estimates the normalized curvature at each pixel of a letter contour. Subsequently, it performs pair-wise comparisons of the curvatures sequences that correspond to two realizations of the same alphabet symbol. Then, it introduces a new Proposition that, on the basis of the previous results, offers a closed solution to the problem of matching two equinumerous digital contours in the Least Squares sense. Next, a criterion is employed quantifying the similarity of two realizations of the same alphabet symbol. Finally, a number of statistical criteria are introduced for the automatic identification of the writer of ancient manuscripts. The introduced method did not employ any reference manuscript neither the number of distinct hands who had written the considered set of manuscripts nor any related information whatsoever; it also performs quite efficiently even if a small number of realizations (less than 6) of certain alphabet symbols appear in a tested document. The only a priori knowledge is the alphabet of the language under consideration. We would like to stress that otherwise the method does not depend at all on the language itself. Namely it does not take into account if the alphabet is Latin, Greek, Etruscan, etc. The methodology and the related, developed information system has been applied to 46 ancient inscriptions of the Classical and Hellenistic era and 23 Byzantine codices, offering 100% accurate results, in the sense that the obtained results are in full agreement with prominent scholars in the field of Archaeology, History and Classical Studies.

@&#INTRODUCTION@&#
The most important source for the science of History is the preserved set of written documents. Concerning Antiquity, this set mainly includes ancient stone inscriptions [1–3] and manuscripts. Dating the content of these documents is absolutely crucial for History and Archaeology. One of the most prominent historians, Professor Christian Habicht has recently written, “Proper historical use of inscriptions can only be made if they can be dated”. However, writers of ancient inscriptions and manuscripts, as a rule, did not sign or date their documents. On the other hand manuscripts are the handwritten texts on papyrus or parchment that were the vehicles transmitting the literature of the ancient world through the middle ages, until mechanical reproduction by printing presses took over. These documents pose questions similar to those raised by inscriptions, and some new ones. For example, the Homeric Iliad survives mainly through a handful of large, beautiful manuscript volumes. These were all produced in Constantinople during the 10th or 11th century, but made their way to different libraries in Europe: Venice, El Escorial in Spain, London, Geneva, Florence, Rome. Each of these manuscripts, too, contains many different texts – the text of Homer’s poem, and several different commentary texts, as well as shorter notes in the margins and between lines. One main goal of the present paper is to perform quantitative analysis on the scribal hands, so that the relationships among these volumes and their relative dates of production are obtained. Most importantly, the results obtained in this paper might be able to ascertain something of the nature of the source-documents these scribes were working from as they produced these deluxe, comprehensive editions. So far, as a rule, dating the content of ancient inscriptions and manuscripts is a very difficult task and it is based on the scholars’ instinct and frequently subjective considerations [1]. Thus, it is not a surprise that ancient documents dating, often causes scientific disputes and disagreements.The idea emerged among the authors to achieve dating of ancient documents by means of automatic identification of their writer. In fact, as far as ancient inscriptions are concerned, their writers carved the stones as a career; in other words, writing inscriptions was a profession in antiquity. This has the following important consequences: (a) The number of different hands who cut the stones to write a document, was particularly limited; thus, one may expect that the tenths of thousands of inscriptions unearthed in Attica may belong to few hundreds of different writers. (b) If one succeeds in attributing a set of inscriptions to a writer, then it is very easy to determine his career period, since, as a rule, the content of at least one inscription will reveal it. Evidently, the greater the number of the inscriptions correctly attributed to this hand, the greater is the probability to date his career. (c) If a new inscription is found and/or studied, the content of which cannot be dated, if the developed methodology attributes it to a writer, then the content of the inscription gains a date immediately, which is clearly the time period during which the writer was active professionally. We note that the working careers of most ancient writers covered about 20–25years.Similar remarks hold true for those who preserved ancient texts by copying them manually. For example, as has already been pointed out, a few hands preserved Homers’ poems by copying them in Constantinople during the 10th or 11th century; a few other writers added their comments in the manuscripts. If one may by automatic means identify the various hands that had written text in these documents, one immediately obtains a chronological classification as well as additional most valuable information as stated in the previous Section 1.1.Therefore, the importance of developing an ensemble of methods and a related information system that performs correct writer identification is absolutely essential. The authors’ research group has already made a first attempt towards this direction with very fruitful results, in connection with ancient inscriptions [1–3]. There is, however, substantial novelty in the present work and manuscript, namely: (1) The method presented here is essentially different than the methods introduced in [3,2]. We would like to emphasize that, as described in [4], when a new independent statistical method is developed for the solution of a problem including stochastic processes, then one may be asymptotically certain for the results offered in common with the already existing methods. In other words, every new approach that confirms already existing results offered by other statistical methods increases the degree of confidence that these results are correct. (2) The methods developed by the authors and presented in [3,2], work really well, when there is a sufficient number (usually >5) of realizations for each alphabet symbol on a tested inscription. Without this limitation, the method introduced here seems to work quite well even if a very small number (1–5) of realizations exist on an inscription. As it will become evident from the subsequent analysis, the comparison of two documents in order to test if they have been written by the same writer, is exhaustive; in other words, all pairs of realizations of the same alphabet symbol in the two compared documents are processed. Thus, for example, suppose that one has to decide if two manuscripts D1 and D2 have been written by the same writer or not. Suppose, moreover, that there are four (4) realizations of a certain alphabet symbol L in D1 and five (5) realizations of L in D2. Then, the method introduced here, generates 5×5=25 and 4×5=20 proper quantities, which may offer a quite reliable relative answer, after the proposed statistical processing. The same hold true for documents D1 and D2 including two (2) and ten (10) realizations of an arbitrary L, respectively, etc. (see Section 7.3). The previous work of the authors deals with ancient inscriptions; the present work tackles both the problem of ancient inscriptions, as well as the problem of Byzantine codices. The essential difference between these two cases will become evident in the main text of the manuscript. (3) The method proposed here is exhaustive in association with all realizations appearing on a document.In any case, the authors are dealing with the problem of identifying the writer of documents, for which no prior information whatsoever is known. In other words, there are no reference documents at all. Equivalently, absolutely no training set of documents were available for the development of the introduced system, nor any supporting database. We would like to stress that the method presented here tries to accomplish writer identification based on individual letter realizations and/or repeated complexes of letters. We emphasize that the introduced analysis is based on the individual characters or blocks of characters, extracted as described in Section 3. Strictly speaking, the introduced approach is not text-independent. However, the only prerequisite for the application of the entire set of actions for writer identification, which is presented here, is knowledge of the set of the fundamental symbols. Equivalently, in order to apply the introduced method is sufficient to be able to distinguish the different symbols of an alphabet, say the Greek or Latin or of an earlier such set of symbols, as the Babylonian, the Etruscan one, etc. In fact, the only reason for knowing how to distinguish distinct letters or complexes of letters is the need to make comparisons only between realizations of the same symbols or complexes of symbols. Beyond that, no text-dependency at all exists in the introduced method.Concerning the approach introduced here, we would like to make the following two remarks: (a) It is assumed that the main text appearing on a single stone or on a single papyrus has been written by the same hand. (b) Every single papyrus consists of the main text as well as a number of comments written on the boarders of each page by either the person copying the original text or by scholars/researchers afterwards. In the present work, we completely ignore the comments. However, the authors intend to attempt classification of the comments in the near future.In the last years the research topic of automated writer identification and verification is very active, mainly concerning hand written text. Concerning on-line writer identification, Ref. [5] proposes a method for dynamic writer identification that uses the relation between static and dynamic information in a handwritten text. In [6] a system for on-line writer identification from data captured from a whiteboard is presented. The developed system employs Gaussian Mixture Models in order to process the text-independent features from the whiteboard handwriting, while the data selected from different writers are used to train a Universal Background Model that provides a writer-specific model. Finally, the method introduced in [7] aims to improving identification accuracy when only a small number of characters is available by employing temporal sequence codes and shape codes which take advantage of speed and pressure applied during the writing process.Much more work is done in off-line identification and verification. More specifically researchers apply morphological approach [8], Gabor filters and grayscale co-occurrence approach [9] or texture identification [10]. The work of [11–13] is based on feature extraction, while Hidden Markov Models are applied to [14–16]. A wavelet based Generalized Gaussian Density (GGD) method is presented in [17]. Refs. [18,19] tackle writer identification and writer verification tasks using local features. In [20] a Fourier Transform approach is proposed and in [21] a dichotomy transformation is performed. Ref. [22] measures the individuality of handwritten characters through identification and verification models. Textural and allograph prototype approach is described in [23,24].Writer identification using information retrieval is treated by allograph prototype matching approaches [25] or by discrete character prototype distribution approaches [26], offering satisfying results. More recently, researchers [27] use continuous character prototype distribution approach with fuzzy c-means algorithm in order to estimate the probability that a character has been generated by a prototype. Other scholars [28] use a combination of local descriptors and learning techniques or directional morphological features [29]. Finally in [30] special consideration is taken to certain characteristics of graphological type, such as skew, slant, pressure, thinning area, etc. in order to classify calligraphic handwritten scripts according to their writer.More recently, automatic writer identification has been applied to documents of historical significance and/or to text based on symbols other than Latin ones [31–36].The methodology presented here may also be applied in the case of optimal matching and classification of arbitrary shapes, provided that they undertake Euclidean transformations and scaling only. Consequently, we will make a synopsis of previous work in this field. Indeed, concerning curvature driven classification of planar curves, in [37] it is shown that the curvature of a curve expressed as a function of the curve length, fully determines the differential affine invariants of this curve. Moreover, a new curve with coordinates the curvature of the original curve and its derivative with respect to the curve length, is invariant to locally Euclidean transformations. In [38], in a broader context, the results of paper [37] are extended to include any group of transformations, not necessarily Euclidean. On the basis of these results, in [39], curvature signatures of the control points of a curve (or, simply, of its vertices) are employed to determine locally-Euclidean equivalence relations between a pair of curves. Then, the similarity of a set of curves is determined by means of the similarity of the corresponding equivalence relations. The non-reciprocity between curvature signatures and two-dimensional Euclidean transformations is shown and studied in [40], where families of non-equivalent curves of the same signature are explicitly constructed. In [41], a method is introduced, more relative to the one presented here, according to which the sequences of the curvature values of planar curves is adopted in order to guide a “proper”, orientation invariant comparison of planar shapes.In connection with the aforementioned methods and their limitations, especially in comparison with the approach introduced here, we would like to point out the following: Curvature-driven methods of curve comparison do not necessarily generate invariants, which may give rise to quantities suitable for Statistical processing. More specifically, metrics that measure differences between such classifying invariants, do not share the same statistical properties with the metrics that express curve difference/similarity. Moreover, the invariants employed in such approaches are associated with differentiable transformations and, consequently, do not work in the case where topological anomalies exist. In the case of pretty noisy images of Ancient inscriptions and Byzantine codices, one may expect to encounter topological anomalies. Hence, these are additional reasons for which we have preferred to develop the method of writer identification introduced in the present manuscript.One major difficulty of the problem treated here, is that there is no training set available, namely there are no reference manuscripts at all, in contrast with most approaches introduced so far (see the above Section 2.1). This means that the authors ought to achieve writer identification without having any text written by the sought-for hands to use it as a prototype, without employing a database and without having any clue whatsoever concerning the texts or their writers. In addition the authors belonging to the disciplines of Computer Engineering and Mathematics knew nothing about the number of distinct hands who had written the considered manuscripts.There are various additional difficulties in achieving automatic writer identification of ancient documents, such as: (1) Inscriptions and sometimes Byzantine codices may suffer from serious wear (see Figs. 1a and 1b). (2) Quite frequently, the similarity between two specific samples of an alphabet symbol of the same writer is smaller than the similarity between other pairs of samples by different writers. (3) We point out that there is substantial variability in the way a writer in antiquity forms an alphabet symbol on a document: for example, in the same manuscript, a number of ‘Ω’ realizations may be closed, while another ensemble of ‘Ω’ may be open (see Fig. 2). Similar remarks hold for ‘E’ and other alphabet symbols. In other manuscripts, alphabet symbol kappa looks like the Latin ‘U’, while a few lines below looks like ‘K’. In ancient inscriptions, very frequently, the same writer connects the middle cross bar of ‘A’ a number of times with the left leg only, other times with the right leg only, while sometimes he does not connect it with any leg. We stress that this variability does not concern the middle cross bar of ‘A’ only, but it extends to the relative position of the legs, the form of the top of ‘A’, etc. (4) Even if the form of an alphabet symbol on a document looks standard, still the contour of the various realizations is, as a rule, noisy; statistically much more noisy in the case of inscriptions (see Fig. 3), although there are codices where the presence of this noise on the letters’ contours is serious (see Fig. 1b). This noise is due to many factors: the writer’s fatigue and mood, the quality of the employed stone or writing material, the precise form of the writing instrument (chisel, pen, etc.), the age of the writer, etc.In addition, the nature of the problem asks for results as close to certainty as possible. Therefore it is necessary to develop a statistically strict approach for the quantification and test of the hypothesis that two documents have been written by the same writer. For this reason, it is not possible to restrict and bias the comparison of two documents by choosing a specific set of features a priori.Writer identification and verification methods fall into two broad categories: text-independent and text-dependent ones [42]. The methods that perform text-independent writer identification, do not take into consideration the fact that the processed image represents a text; equivalently, they generate an ensemble of features to be processed statistically from the image itself, which do not take into account the fact that the image contains a text. On the other hand, text-dependent methods use the comparison between individual characters or words; as a rule, these methods require a database or at least a training set (e.g. [43]). One may say that in the text-dependent case, one may use a specific part of a text written by a certain writer (e.g. a signature), in order to identify him/her.There are two main reasons for which we have not employed a completely text – independent approach, like the one introduced in [42]: The first reason has to do with the fact that, as a rule, inscriptions images are very noisy; at the same time, codices’ text manifests considerable distortion. In particular, in both cases, the background is non-uniform, a fact that imposes artifacts in text independent document representations. The second reason has to do with the fact that, when using text-independent descriptors, it is necessary to project the text components to representations unrelated to intrinsic characteristics of the writing style. On the contrary, in the present work, we have the intention to take into account the writing style of each individual character, for many historical and archaeological reasons, too. Namely, our effort is that the selected text representation reflects any variation of the writing style.In addition, in the writer identification case treated here, there is no training set available whatsoever. For these reasons we have selected the entire letters shapes, after suppressing randomness in position, rotation and scaling, to be the fundamental components of the documents comparison. The only dependency of the introduced method on the content of the text has to do with the knowledge of the Greek alphabet symbols.Two of the authors (Professors S. Tracy and C. Blackwell) have chosen a number of important ancient Greek inscriptions and Byzantine codices respectively. High-resolution images of each such document have been obtained, as described in Section 8.Next, the individual letters from each manuscript have been extracted. The extraction has been achieved by means of a semi-automatic method similar to the one introduced in [36,44,45]. Each letter realization extracted from the document has been immediately embedded into a rectangular frame.In order that the introduced method is applicable, the contour of each body of an arbitrary letter realization must have a clear well defined form. Dedicated novel algorithms performing proper filtering/actions towards this direction have been developed by the authors (see Sections 4, 5 and Fig. 5b).Subsequently, the following actions have been taken:1.LetCbe the contour of an arbitrary alphabet symbol realization, appearing on a specific document. We divideCinto contiguous chains of pixels, slightly overlapping. Next, we approximate each such chain of pixels with proper polynomial functions for the x and y coordinates separately, with independent variable the chain’s arc-length, S. By the end of this process, each contourCof an arbitrary alphabet symbol realization is uniquely associated with a sequence of curvature values, which represent the trend of the curvature at each pixel ofC, in a very satisfactory manner (see Section 4). We take special care to normalize the values of the curvature sequence, by multiplying each one of them with the length of the corresponding contour.Suppose, now, that we consider two contours C1, C2 of two different realizations of the same alphabet symbol, which give rise to two distinct sequences of normalized curvatures K1, K2, like those described in the above Step 1. We have developed and applied a mathematical approach and a corresponding algorithm, which achieves direct optimal matching of K1 and K2 via minimization of a properly chosen error function. Subsequently, contours C1 and C2 are, also, optimally fit by means of an introduced Criterion (See Section 5).At this stage, we introduce another substantially different criterion in Section 6, which is particularly suitable for statistical considerations. More specifically, we consider two realizations of the same alphabet symbol, we optimally fit them and at the best fitting position we compute the ratio of the area of their intersection with the area of their union; this ratio constitutes both a strong fitting error, as well as a quantity pretty suitable for the subsequent statistical analysis.Employing the criterion defined in the previous Step 3, we have stated and tested statistical hypotheses, which eventually indicated the number of different writers who wrote all the available documents. At the same time, a “representative” document has been uniquely associated with each writer.Each one of the remaining documents has been classified to a corresponding hand, on the basis of novel maximum likelihood estimations.We would like to emphasize that no a priori knowledge concerning the documents is necessary for the application of this approach. The only demand is to ask for comparisons of contour couples C1 and C2 belonging to realizations of the same alphabet symbol. Beyond this, the proposed method is essentially content/text-independent. In addition, the method is applicable to contours of repeated complexes of letters or to any analogous class of similar two-dimensional curves.The curvature of the various letter’s realizations on an (ancient) document very frequently plays an essential role in the identification of the writer of the document. Moreover, the contour of each written or scribed letter is actually a curve. Furthermore, the knowledge of the curvature in every point of a curve uniquely represents this curve. In addition, repetition of similar curvature implementations in a given area of the same alphabet symbol’s realizations could frequently be typical of one person’s writing style (for example, see Figs. 4a and 4b for typical realizations of letters ‘Ω’ and ‘P’). As a result, the curvature of the contour of a given letter’s realization throughout a document can be used towards an effort to distinguish the writing style of the person that wrote it. In this section, we present a method for computing a sequence of properly normalized curvature values for a letter contour, as well as for comparing the corresponding curvature sequences of two letters’ contours. This will lead to determining corresponding points, in the contours of the two compared letters.Consider two distinct codices or inscriptions, say D1 and D2 and any alphabet symbol, say L, whose realizations appear in both documents. Let us choose an arbitrary realizationLi1of L in D1 andLj2of L in D2, where the superscripts 1, 2 indicate the inscription or codex and the subscript is the cardinal number of the realization in the corresponding document. We segment the images ofLi1andLj2, by application of the method presented in [46,3]. We extract the contours ofLi1andLj2, so that it is always in proper form; namely, any pixel of the contour has precisely two neighboring pixels, no right angles between consecutive pixels is allowed, as well as no isolated formation of pixels are permitted (see Fig. 5b). Since there is no image segmentation algorithm that can generate contours in such a form, proper morphological filters have been applied in the letters’ images to ensure this form of the contour. In fact, let I be the image of the entire frame where a letter is embedded; let, moreover, C be the contour of this letter. Then, we apply the following morphological transformations on I: (a) An erosion via a diamond-shaped structural element of diagonal length 5 (five) pixels, thus obtaining image IE(b) We apply dilation on IEvia a square structural element of side 2 (two) pixels, thus obtaining image ID. (c) Next, we have applied dilation on ID, by means of a cross-like structural element with 5 (five) pixels in height and width, thus obtaining IDC. (d) Finally, we obtain the contour of the letter in its proper form by subtracting sets IDCand ID.LetCi1be theLi1outer contour in such a proper form andCj2the outer contour ofLj2. We suppose that each one of these outer contoursCi1andCj2are Jordan polygons. In many cases, each outer contour is a single closed curve (see the realizations of ‘A’ in Fig. 5a). In various other cases, the outer contour of the alphabet symbol realization consists of two or more disjoint curves (see the realizations of ‘Ω’ in Fig. 5a). In the following, we will treat the two cases separately.First, we divide the contours, say,Ci1,Cj2into NOslightly overlapping contiguous chains of pixels, of almost equal length. We will employ the symbolismΣ11,Σ21,…,ΣNO1for the chains ofCi1.We optimally approximate each chain separately with a polynomial of ν degree, in the Least Squares sense. The choice of the values of NOand ν is guided by the desire to keep the approximation error as small as possible, while at the same time the approximating curve does not manifest considerable undulations. The approximation is achieved as follows: We first define the sequence which describes the chain length at each pixel; namely, if (xq,yq), q=1,2,…,NOare the coordinates of the centers of the pixels of an arbitrary chain, then the distance of two consecutive pixels centers isδSq=(xq+1-xq)2+(yq+1-yq)2. Using δSq, we define the sequence of arc-lengths on each chain as follows: s1=0,sq=∑n=1q-1δSnand by means of it, we define the approximating polynomials of νth degreeX(s)=∑n=0ναn(s)ν-nandY(s)=∑n=0νβn(s)ν-n. The optimal values of the coefficients αn, βn, n=0,1,2,…,ν are obtained by minimization of the error functionε=∑q=1NO{(xq-Xq)2+(yq-Yq)2}, via the Nelder–Mead algorithm where Xq=X(sq), Yq=Y(sq) and q=1,2,…,NO.At this point, we approximate the curvature at an arbitrary point of the chainΣ11,Σ21,…,ΣNO1, by the curvature of the corresponding polynomial curves (Xq,Yq). Indeed, if the polynomial curve isR→(s)=X(s)iˆ+Y(s)jˆ, then its curvature isκ(s)=dT→dsdR→ds,T→=dR→dsdR→ds, where we have used this formula instead ofκ=d2R→ds2, since s is not the actual arc-length of the polynomial curve, but the arc-length of the approximated chain.By the aforementioned approach, we are able to spot the curvature in the middle points of chainΣq1, with a satisfactory precision. In the end points of each such chain, however, (a) the approximation ofΣq1by the polynomial curveR→(s)is not particularly good and (b) successive chains’ curvature approximations are discontinuous. To set ideas, suppose that each chain consists of 130pixels and thatΣ11andΣ21overlap in 30pixels; then, in practice, the last 10pixels ofΣ11are not very well approximated by the correspondingR→(s), while the same holds true for the first 10pixels ofΣ21. At the same time, the pixels ofΣ11with cardinal number from 111 to 120 coincide with the pixels ofΣ21with cardinal number from 11 to 20; however, the two obtained approximating curvatures are not equal. To circumvent this discontinuity, we attribute to these 10 overlapping pixels the curvature:κ1(111)κ1(112)⋯κ1(120)10/119/11⋮1/11+κ2(11)κ2(12)⋯κ2(20)1/112/11⋮10/11.In the general case, the pixels of chainΣq1with cardinal number from (p+1) to (p+μ) coincide with the pixels ofΣq+11with cardinal number from (r+1) to (r+μ) (see Fig. 6); then, ifκq(p+1),κq(p+2),…,κq(p+μ)are the approximate curvatures of the overlapping points calculated viaΣq1, whileκq+1(r+1),κq+1(r+2),…,κq+1(r+μ)are the approximate curvatures of the very same points computed via the chainΣq+11, then we attribute to each one of these points the overall unique curvature given by:κq(p+1)κq(p+2)⋯κq(p+μ)μμ+1μ-1μ+1⋮1μ+1+κq+1(r+1)κq+1(r+2)⋯κq+1(r+μ)1μ+12μ+1⋮μμ+1.We note that other masks, besides the linear one, work quite well, too. By the end of this process, a unique value of curvature is attributed to each pixel of the contour and jumps in curvature values of successive pixels have been significantly suppressed in a satisfactory manner (see Fig. 7).As a final step, we normalize the obtained curvature sequence in order to account for possible differences in scale between documents D1 and D2. In fact, it is well-known that if a simple scaling by a factorλis applied to a curve, then the curvature at each point of this curve is divided byλ, while the length of the curve is multiplied by the same factorλ. Hence, we employ as a final sequence of values of the curvature at each point of contourCi1the previously evaluated curvature value multiplied by the length of the contourCi1. In this way, a kind of suppression of the scaling factorλtakes place. When the compared contours of the alphabet symbol realizations are quite similar, which is the case of interest, this curvature normalization is optimal. In all subsequent analysis, when we will refer to the curvature value sequenceκp1, we will consider the aforementioned sequence of normalized curvature at each point p ofCi1.The very same process is applied to the outer contourCj2of the arbitrary letter implementationLj2and their corresponding chainsΣq2, q=1,2,…,NO, thus obtaining a sequence of normalized curvature valuesκq2for each point q ofCj2.Consider, now, two arbitrary realizations of the same alphabet symbol, their contoursCi1,Cj2and the corresponding sequences of curvature valuesκp1andκq2. In general, the two realizations have arbitrary orientation and size, within certain limits; thus, the correspondence of the pixels of the two contours is not at all evident. Hence, we will employ the curvature sequencesκp1andκq2, in order to establish the proper correspondence between the two contours.To achieve that, let us first consider that the two contours consist of the same number of pixels NC. Then, for each non-negative integer τ we consider the error functionεκ(τ)=∑n=0NC-1(κn1-κ̃n,τ2)2whereκ̃n,τ2is the cyclic extension ofκn2namely, for any fixedτ,0⩽τ⩽NC, the sequenceκ̃n,τ2=κNC-τ2κNC-τ+12⋯κNC-12κ02κ12⋯κNC-τ-12. At this point, we spot the value of τ, say τO, which renders εκ(τ) minimum. We adopt that the relation which maps the first pixel ofCi1to the τOth pixel ofCj2, the second pixel ofCi1to the (τO+1)th pixel ofCj2and so on, offers the optimal similarity correspondence between the two contours. Following a quite standard path, a fast, efficient technique for spotting the minimum of εκ(τ) is to expand the squares, in which case one obtains:(4.1)εκ(τ)=∑n=0NC-1(κn1)2+∑n=0NC-1(κn2)2-2∑n=0NC-1κn1κ̃n,τ2.But, the sequence∑n=0NC-1κn1κ̃n,τ2is the cyclic convolution of sequencesκn1,κn2; thus, if DFT{κ} is the Discrete Fourier Transform of sequence κ and DFT−1{K} is the Inverse Fourier Transform of sequence K, then∑n=0NC-1κn1κ̃n,τ2=DFT-1{DFT{κn1}DFT{κn2}}. Consequently, the minimum of εκ(τ) is immediately obtained by spotting the maximum ofDFT-1{DFT{κn1}DFT{κn2}}.The previous analysis deals with the case where the two contours of the alphabet symbol realizations consist of the same number of pixels. Let us, now, suppose that the two contours consist of a different number of pixels. Then, we first perform a preliminary resize of the larger contour, so that the two letters are close in size. After this preliminary scaling, let the first contour consist of N1 pixels while the second of N2 pixels, with N1>N2, without any loss of generality. We will properly remove the correct number of extra pixels from the first contour, so as to make them equinumerous. In fact, let δ=N1−N2; we note that, due to the performed preliminary resize, number δ is as a rule small, in comparison with N1 and N2. Thus, we distribute the δ pixels in the first contour as follows: we divide the first contour into δ contiguous sub-contours C1,C2,…,Cδwhere each one of C1,C2,…,Cδ−1 consists of[N1δ]pixels (where [x] stands for the integer part of x), while Cδcomprises the remainingN1-(δ-1)[N1δ]pixels. At each Ci, we spot the pixel of minimum curvature and we remove it, thus eventually obtaining a reduced contour of the first letter, sayC‾i1consisting of N2 pixels.In this way, we have formed two versions of the contours of the two alphabet symbols realizations, which consist of the same number of pixels. Therefore, the aforementioned method is immediately applicable and offers the optimal τOand, hence, the optimal one-to-one correspondence between the pixels of the two contoursCi1,Cj2, as far as the difference in curvature is concerned.It is known that the writers of ancient inscriptions cut letters on the stone by exerting a proper number of strokes each time. This frequently generated alphabet symbol realizations, which consist of a number of distinct, simply connected regions. There is also the possibility that multiply connected regions may appear in letter realizations of Byzantine codices. In either case, an alphabet symbol realization may as well be divided into disjoint simply connected regions, mainly due to wear (see Figs. 1a, 1b, 3 and 8).In general, concerning inscriptions, it seems that the greater number of applied strokes was four, and that there are two main categories of alphabet symbols according to the kind of strokes: the rectilinear alphabet symbols and the curvilinear ones. The rectilinear alphabet symbols are those generated by strokes ideally executed along a straight line. In this category belong the alphabet symbols A, Γ, Δ, E, Z, H, I, K, Λ, M, N, Ξ, Π, Σ, T, Y, X. On the contrary, we consider as curvilinear alphabet symbols those that are generated with at least one stroke, which requires a curvilinear movement of the chisel. In this category belong the alphabet symbols B, Θ, O, P, Φ, Ψ, Ω. We must point out, however, that rarely, there are writers who sometimes violated this general rule. For example, there is a very limited number of writers who carved letter P with three rectilinear strokes, others who carved O with four rectilinear strokes, others who carved Y with two curvilinear and one rectilinear strokes, etc.To handle the case of alphabet symbols’ realizations consisting of multiple strokes with multiply disjoint outer curves, we employ the notion of critical points introduced in [2]. We simply note that the critical points correspond to local maxima of the curvature value in the outer contour, having a value greater than a statistically estimated threshold. The critical points define the sides of each stroke (see Fig. 8).Next, we emphasize that the analysis introduced in Section 4.1, also applies in the present case, with the only difference being that curvature considerations and curvature comparisons are made for the outer contour of each stroke separately. In particular, the error (4.1) describing the overall square difference of the curvature sequences ofCi1andCj2is given by:εκ(τ)=∑p=1Np∑n=0NpC-1κn,p1-κ̃n,τ,p22where p runs through the sides of the strokes of the considered letter,NpCis the common number of pixels of the pth side of the two compared letters, Npis the number of stroke sides of the letters in hand,κn,p1is the curvature sequence of the pth side of the letterLi1andκ̃n,τ,p2is the cyclic extension ofκn,p2, namely of the curvature sequence of the pth side of the letterLj2. Evidently, the process that makes the curvature sequences in the two letters equinumerous is applied side-wise.We will explain the content of this formula by means of a specific example: consider for a moment two realizations of the alphabet symbol ‘P’ and suppose that both realizations consist of two strokes, one rectilinear and one curvilinear. Then, the error function indicating the overall curvature square difference, consists of four terms: (1) the term who adds the curvature difference between the left side of the vertical, rectilinear stroke, (2) the term describing the squared difference of the curvature of the right side of the vertical stroke, (3) the sum of square differences of the left side of the curvilinear stroke and (4) the sum of square differences of the right side of the curvilinear stroke.In the present section, we will introduce a method for placing the contours of two realizations of an alphabet symbol; this placement/fitting is very near to optimal. In the process of doing so, we will employ the results of the previous Section 4, in order to establish the optimal correspondence between the points of the contours of two realizations of an alphabet symbol, from the curvature point of view. To achieve this correspondence, we will consider the following two cases separately: (a) The case where the contours of the two letters consist of the same number of pixels and (b) The case in which the two contours are not equinumerous. This second case will be treated via proper reduction to the first one.Indeed, first consider any two digital curves consisting of the same number of points N, say (xi,yi) and (pi,qi), i=1,2,…,N. We want to perform the following transformations to curve (pi,qi): (a) rotation, (b) parallel translation and (c) scaling, so that its transformed version optimally fits (xi,yi) in the Least Squares sense. In fact, in order to rotate (pi,qi), we first parallel translate it, so that its center of mass coincides with the origin (0,0) and then we apply rotation. In this way, we obtain an intermediate transformed version (γi,δi) given byγiδi=cosφ-sinφsinφcosφpi-p¯qi-q¯,wherei=1,2,…,N,p¯=∑i=1NpiN,q¯=∑i=1NqiNand φ the rotation angle.Next, we move (γi,δi) near the center of mass of points (xi,yi), via parallel translation by (xδ,yδ), thus generating curve (αi,βi), i=1,2,…,N. Evidently, ∀i=1,…,Nαi=(pi-p¯)cosφ-(qi-q¯)sinφ+xδβi=(pi-p¯)sinφ+(qi-q¯)cosφ+yδOur intention is that (αi,βi) optimally fits (xi,yi) in the Least Squares sense, which is equivalent to minimizing quantityεRT(φ,xδ,yδ)=∑i=1N(xi-αi)2+(yi-βi)2Minimization of εRT(φ,xδ,yδ) admits a close solution as it will be demonstrated later on.Then, ifα¯=∑i=1NαiNandβ¯=∑i=1NβiN, we resize curve (αi,βi) by a factorλ, with center of scaling the point(α¯,β¯), thus generating curve(ri,ti)=(λ(αi-α¯)+α¯,λ(βi-β¯)+β¯). Among all these curves (ri,ti), we must spot the one that optimally fits (xi,yi) in the Least Squares sense. The results associated with the aforementioned actions are given in the following:PropositionConsider two equinumerous sets of co-planar points (xi,yi) and (pi,qi), i=1,2,…,N. In order to optimally fit the digital curve (pi,qi), after a proper transformation of it, to (xi,yi) in the Least Squares sense, we proceed as follows:We first rotate all (pi,qi) by angle φ as a rigid body, we parallel translate them by (xδ,yδ) and we ask for minimization of quantity:Minimization of εRT(φ,xδ,yδ) has the closed solution:tanφO=∑i=1N{(pi-p¯)(yi-y¯)-(qi-q¯)(xi-x¯)}∑i=1N{(pi-p¯)(xi-x¯)+(qi-q¯)(yi-y¯)}xδ0=x¯yδ0=x¯(ii)Employing the above quantities φO,xδO,yδO, we apply the corresponding transformations to digital curve (pi,qi), thus obtaining the digital curve (αi,βi), i=1,2,…,N via the formula:(i)Quantity εRT(φ,xδ,yδ) is evidently continuously differentiable and bounded below by zero. Therefore, it obtains its minima in its domain and at its minimum it holds∂εRT∂xδ=∂εRT∂yδ=∂εRT∂φ=0. These lead to the following equations:Evidently, the uniqueness of the solution in-π2,π2ensues that this indeed corresponds to the minimum.(ii)After obtaining the optimal rotation angle (5.5) and translation formulae (5.3) and (5.4), the goal is to apply optimal scaling without violating the values (φO,xδO,yδO). To achieve this, we first move (pi,qi) as a rigid body to (αi,βi), i=1,2,…,N by means of the transformation:To circumvent this difficulty, we define the fitting error by means of the formula:εRTO(λ)=∑i=1N(xi-λsign(xi-a¯,ai-a¯)(ai-a¯)-a¯)2+(yi-λsign(yi-β¯,βi-β¯)(βi-β¯)-β¯)2where sign(x,y) is the function which returns the sign of xy. In this way, by changing the value ofλ, one can achieve a minimum in the fitting error, while at the same time quantityεRTO(λ)remains continuously differentiable. This is so, because multiplication by a positive factor cannot change the quadrant where a point belongs, which means that the term-λsign(xi-a¯,ai-a¯)(ai-a¯), from the moment it obtains the correct sign, it never changes it.After obtaining the derivative with respect toλand setting it equal to zero, we obtain:∂εRTO∂λ=0⇔∑i=1N(ai-a¯)sign(xi-a¯,ai-a¯)(xi-λsign(xi-a¯,ai-a¯)(ai-a¯)-a¯)+∑i=1N(βi-β¯)sign(yi-β¯,βi-β¯)(yi-λsign(yi-β¯,βi-β¯)(βi-β¯)-β¯)=0⇔λO=∑i=1N|(xi-a¯)(ai-a¯)|+|(yi-β¯)(βi-β¯)|∑i=1N(ai-a¯)2+(βi-β¯)2The uniqueness of the solution, together with the fact that the error functionεRTO(λ)is bounded below, implies that these value ofλoffers the unique minimum.□At this point, we will employ the Proposition and the results of Section 4 to obtain a first estimation of the best fitting position of the two considered letters. In fact, in Section 4 we have spotted the correspondence between the starting points of the two contours of the considered alphabet symbols realizations.We would like to point out that this relative position of the two letters contours is not optimal, since the contour of the greater length has δ pixels missing. In addition, the fitting error values are not ready to be handled from the statistical point of view. For this reason, we have employed a decisive criterion for the similarity of two alphabet symbols realizations. This criterion that will be described in the next section employs the analytic solution offered by the Proposition, since the optimal relative position of the two contours is pretty close to the one offered by the Proposition.In the previous Section 5, we have determined a near-to-optimal relative position of two contours of an alphabet symbol. In the present section, starting from this near-to-optimal position, we will determine the optimal relative placement of the two letters’ contours.Consider two realizations L1 and L2 of the same alphabet symbol appearing on the same document or two distinct documents. Let C1 be the outer contour of L1 and C2 the outer contour of L2, both in the proper form as described in the beginning of Section 4. In all cases encountered so far, each one of C1, C2 can be thought of as simple polygons. Notice that C1 or C2 may enclose other polygon curves, as in the case of the second ‘ω’ in Fig. 2. We would like to stress that, in all cases, the Proposition of Section 5 is applied only to the outer contours C1 and C2 (See Figs. 9a, 10a, 11a, 12a).We place C1 and C2 in their optimal relative position, which is obtained by application of the methodology introduced in Sections 4 and 5 (see Fig. 9b, Fig. 10b, Fig. 11b and Fig. 12b). We arbitrarily consider contour C1 as fixed and, then, we apply rotation, parallel translation and scaling to C2 pretty close to this equilibrium position. Namely, if xδ, yδ, φ,λare the corresponding transformation parameter values offered by application of the methodology of Sections 4 and 5, then we define a small 4-cube around this point and a fine partition of it. A proper transformation of C2 corresponds at each point of this partition. For each transformed version of C2, we consider the transformed internal of L2 realization and we define the criterion:(6.1)δEn=areaoftheintersectionoftheinternalsofL1andofthetransformedL2areaoftheunionoftheinternalsofL1andofthetransformedL2,where subscript n runs through all partition points of the defined 4-cube. We spot the maximum of all δEn, say δEBand we determine the transformation parametersxδB,yδB, φB,λBthat correspond to this maximum. We adopt the criterion that δEBbest expresses the similarity between contours C1 and C2. We note that, in the ideal case where C1 and transformed C2 coincide, then δEB=1, while when the two letter realizations are radically different, then criterion δEBbecomes characteristically small (see Figs. 13 and 14).In the case where the letter consists of more than one simply connected regions, the criterion described in Section 6.1 is properly modified. Multiple connected realizations of letters appear much more frequently in ancient inscriptions, since their writer, as a rule, exerted a number of distinct strokes on the stone in order to carve the letter; however, even in Byzantine codices multiple connected letters may also appear, though essentially less frequently. For this reason, we will employ the term “stroke” for each simply connected region of an alphabet symbol realization even if it appears in a Byzantine codex. In fact, let us consider two realizations M1 and M2, consisting ofN1SandN2Sdistinct strokes respectively, where each stroke is, as always, a simply connected region (see Fig. 8 and [3]).Then, as in Section 6.1, we arbitrarily consider one realization as fixed and we apply to the other realization the entire set of transformations, namely rotation, translation and scaling. However, in the present case, the error function is slightly different than in Section 6.1. In fact, for M1, we consider the union, say U1, of itsN1Sstrokes; similarly, for an arbitrary position of the transformed M2, we consider the union, say U2, of itsN2Sstrokes. Then, we apply the concept of Section 6.1 to U1 and U2. Namely, we define and maximize the error function(6.2)δEnS=areaoftheintersectionofU1andU2areaoftheunionofU1andU2,thus obtaining the optimal parameter values of the transformations that offer the best matching position of M1 with the transformed M2. We note that the developed method ensures that the relative position between M1 and the transformed M2 is indeed unambiguously optimal, with the desired precision defined by the step of the 4-cube partition. Once more, the value ofδEnSat the optimal position, for which we will once more employ the symbol δEB, constitutes a particularly good criterion of the degree of similarity of realizations M1 and M2. As in Section 6.1, the closer the value of δEBto 1, the greater the similarity of realizations M1 and M2; conversely, the smaller the positive value of δEB, the more the two realizations differ (see Figs. 15 and 16).In the following section, we will employ the distribution of δEB, in order to set statistical criteria towards deciding if two documents have been written by the same writer or not.The essence of the approach for attributing each document to its writer that will be described below incorporates two steps: First, in Sections 7.1 and 7.2, we will determine the number of distinct hands that have written the entire ensemble of the given documents. At the same time, a first representative document will be assigned to each hand. Next, we will attribute each remaining documents to the corresponding writer by a maximum likelihood criterion described in Section 7.3.Let us consider two distinct documents D1 and D2 and a specific alphabet symbol, say L,N1Lrealizations of which appear in D1, whileN2Lrealizations of it appear in D2.We arbitrarily choose a first realization of L in D1, we call it L1,1 and we perform all pair-wise comparisons of it with all other realizations of L on D1 namely L1,2, L1,3,…,L1,N1L. These comparisons are performed by means of the methodology introduced in Sections 4–6. In particular, the performed comparisons have offered the ensemble of values ofδE1,pB,p=2,3,…,N1L(defined in Section 6.1, Eq. (6.1) for a single outer contour curve and in Section 6.2, Eq. (6.2) for multiple disjoint outer contour curves).Next, we consider realization L1,2 as fixed and we compare it withL1,3,L1,4,…,L1,N1L,thus, obtaining criteria valuesδE2,pB,p=3,…,N1Land so forth untilL1,N1L-1is compared withL1,N1L.We assume that the obtained(N1L-1)N1L2values ofδEp,qB,p=1,2,…,N1L,q=p+1,…,N1Lcome from a normal distribution; the performed Colmogorof–Smirnof test did not violate this assumption (a=0.01). We let δμ1 be the mean value ofδEp,qBand δS1 be their standard deviation.Subsequently, we make all pair-wise comparisons of realizations L1,i,i=1,2,…,N1Lwith realizations L2,j,j=1,2,…,N2Land we obtain another set of best fitting valuesδZp,qB,p=1,2,…,N1L,q=1,2,…,N2L. We again assume that the obtainedN1L·N2Lvalues ofδZp,qBcome from a normal distribution; the performed Colmogorof–Smirnof test did not violate this assumption (a=0.01). We let δμ2 be the mean value ofδZp,qBand δS2 be their standard deviation.Then, if μ1, μ2 are the means of the two normal populations and ifN1,1L=(N1L-1)N1L2andN1,2L=N1L·N2L, it is well known that quantity(7.1)tL=(δμ1-δμ2)-(μ1-μ2)(δS1)2N1,1L+(δS2)2N1,2Lfollows a Student distribution with d degrees of freedom, where d is the integral part of(7.2)(δS1)2N1,1L+(δS2)2N1,2L2(δS1)2N1,1L2N1,1L-1+(δS2)2N1,2L2N1,2L-1.In general, the population means μ1, μ2 are not known. However, if we make the hypothesis that the two documents D1 and D2 have been written by the same hand, then μ1=μ2. Consequently, quantity tLin (7.1) has a well-defined value and therefore, the validity of the hypothesis can be testedH0:μ1=μ2against the complementaryH1:μ1≠μ2The degree of confidence with which these hypotheses will be tested is chosen by heuristic arguments. In fact, a safe upper bound for the number of writers of ancient inscriptions is a few hundreds, given that inscribing stones was a profession up to the Hellenistic Period. At the same time, one may expect that a quite complete database of inscriptions should include some tenths of thousands of them. Thus, since multiple comparisons will be performed, as it will be described below, if we also take the Bonferoni approach [47] into consideration, a logical value for the threshold of the level of significance α isαT=10-4n, where n is the number of the performed statistical tests associated with the number of different alphabet symbols, which are taken into consideration each time. Since similar arguments hold for the Byzantine codices too, the same αTvalue will be adopted as a threshold, also for the considered codices. Hence, ifP(-∞<x<-|tL|∨+|tL|<x<+∞)<αTthen H0 is rejected and H1 is adopted; otherwise, one cannot, most probably, reject H0.Now, we proceed as follows in order to take into account the entire set of ν considered documents D1,D2,…,Dν: we make the aforementioned pair-wise comparisons of D1 with all other documents D2,D3,…,Dνfor a single letter, say the alphabet symbol ‘A’. Among all documents for which H0 is rejected for ‘A’, we keep the one with minimum P(−∞<x<−|tA|∨+|tA|<x<+∞) We let this document belong to another hand, different than the hand who wrote D1 and we re-symbolize itD∼2. In other words, we attribute to two different writers, those documents that give the smallest “two-sided tail area”, provided that this area is smaller than the chosen threshold αT.Next, we apply all pair-wise comparisons of all remaining documents with both D1 andD∼2, using the same threshold αTand for the same alphabet symbol ‘A’. Each comparison of Diwith D1 andD∼2offers one two-sided tail area; we choose the maximum of these two areas and we symbolize it Pi, i=3,4,…,ν. We once more keep this document, if any, that offers the smallest maximum area Pi, provided that this area is smaller than the chosen threshold of significance level αT. We attribute this document to a third writer and we re-symbolize the document asD∼3. In other words, we demand that documentD∼3differs from D1 andD∼2enough to ensure rejection of H0 for both documents, while, at the same time, manifests the greater possible difference from D1 andD∼2among all yet non-classified documents.We repeat the aforementioned process by considering the triplet D1,D∼2,D∼3and by making three pair-wise comparisons with all remaining, non-classified documents; for each such comparison, we obtain three corresponding two-sided tails probabilitiesP∼1,P∼2,P∼3; letP∼Mbe the maximum ofP∼1,P∼2,P∼3. IfP∼M<αT, then we proceed to another non-classified document and so forth. If noP∼M<αTis found, then the process stops and we move to the next alphabet symbol, as described below. Thus, by the end of this process, we end up with, say,D1,D∼2,…,D∼vAdocuments belonging to νA different writers, as far as alphabet symbol ‘A’ is concerned.Subsequently, we consider another alphabet symbol that appears in all considered documents, say ‘Ω’. Then, we apply all pair-wise comparisons of the non-classified documents with the previous νAdocuments, employing the same criterion in connection with alphabet symbol ‘Ω’. Namely, if Diis an arbitrary non-classified document, its comparison with D1 will offer a two-sided tail area, sayP∼1, its comparison withD∼2a corresponding areaP∼2and, eventually, its comparison withD∼vAa two-sided tail areaP∼vA. If the maximumP∼MiofP∼1,P∼2,…,P∼vAis smaller than αT, then this document potentially corresponds to a new writer. Now, we choose another non-classified document Djand we check if the corresponding maximum areaP∼Mjis smaller than αT; if yes, we consider that Djpotentially belongs to another writer. Otherwise, we proceed to the next non-classified document. After exhausting all non-classified documents, we choose the one of minimumP∼Mkand we consider that this document belongs to a new writer. Therefore, we enrich the set of documents that represent different hands by one and we repeat the procedure for the remaining documents and the same alphabet symbol ‘Ω’, until no maximum areaP∼Mismaller than αTis found.Next, we proceed to considering the next alphabet symbol, say ‘K’, appearing to all considered documents and we apply the same method as in ‘Ω’. In this way, probably, the number of distinct hands may be increased, together with a corresponding number of representative documents. This process continues until all alphabet symbols appearing in the tested documents are exhausted. By the end of the process, this method offers a number of distinct writers, say W1,W2,…,Wμ. In addition, the method associates a corresponding representative document, sayD∼1W1,D∼1W2,…,D∼1Wμ, to each writer; the upper-script indicates the writer and the subscript the cardinal number of the document attributed to him/her. Since the introduced approach is exhaustive and takes into consideration all available alphabet symbols, we assume that we have determined the precise number of different writers of the given set of documents. The next step is to attribute the remaining documents to one of the writers W1,W2,…,Wμ.Consider an arbitrary, not yet classified document Diand all realizations of the alphabet symbol ‘A’ on it. We also consider the first writer’s representative documentD∼1W1. For these two inscriptions, we have applied the method described in Sections 5, 6, 7.1 and we have obtained two sets of values as follows: Set (1) the set of valuesδEp,qAfor documentD∼1W1,p=1,2,…,N1A,q=p+1,p+2,…,N1A, whereN1Ais the number of ‘A’ realizations appearing inD∼1W1and Set (2) the set of valuesδZp,qAfor the pair-wise comparisons of all ‘A’ realizations onD∼1W1and Di, where p runs through all ‘A’ realizations onD∼1W1and q through all ‘A’ realizations on Di. We have also computed the mean value and standard deviation of these sets of values, symbolized viaδμ1AandδS1Afor the first set andδμ1,iAandδS1,iAfor the second set. Now, quantityt1,iA=(δμ1A-δμ1,iA)-(μ1-μ1,i)(δS1A)2N1,1A+(δS1,iA)2N1,iA,(7.3a)whereN1,1A=(N1A-1)N1A2(7.3b)andN1,iA=N1A·NiA(NiAis the number of ‘A’ realizations in Di), follows a Student distribution with d1 degrees of freedom defined by a formula exactly analogous to (7.2). At this point, if we make the assumption that documentsD∼1W1and Dihave been written by the same hand, then the population means are equal, namely μ1=μ1,i. In this case, quantityt1,iAhas a well-defined value, evidently given byt1,iA=(δμ1A-δμ1,iA)(δS1A)2N1,1A+(δS1,iA)2N1,iA,and the same holds true for the value of the probability density functionfd1A(t1,iA)of the underlying Student distribution with d1 degrees of freedom.Next, we proceed to the comparison of Diwith the remainingD∼1Wp, p=2,3,4,…,μ, where, we repeat that μ is the number of distinct writers of the considered documents, evaluated in Section 7.2, as far as alphabet symbol ‘A’ realizations are concerned. In this way, we obtain a sequence of well defined valuestp,iAand corresponding Student probability density functionsfdpA(tp,iA).At this point, we make the plausible assumption that the closer the value oftp,iAis to 0, i.e. to the center of symmetry of the underlying Student distribution, the greater the likelihood that documentsD∼1Wpand Dihave been written by the same hand, as far as alphabet symbol ‘A’ is concerned.Subsequently, we proceed to the next alphabet symbol, realizations of which appear in all the considered documents; let this alphabet symbol be ‘Ω’. We computetp,iΩandfdpΩ(tp,iΩ)in an exactly analogous manner.We apply the same approach in connection with all other available alphabet symbols, say Q in number, thus obtaining a sequence oftp,iLandfdpL(tp,iL), where L belongs to the set of alphabet symbols simultaneously appearing in all considered documents. Now, for an arbitrary writer, having the representative documentD∼1Wp, and for a non-classified document Di, we define an overall likelihood function expressing the letters’ stylistic similarity of these two documents via the product of the aforementioned Q probability density functions, namely via:ξp,i=fdpA(tp,iA)·fdpΩ(tp,iΩ)·fdpK(tp,iK)·…·fdpL(tp,iL)Q.We note that we have used the Q-root in order to obtain a quantitative expression, independent of the number Q of alphabet symbols appearing in all tested documents. We would also like to point out that it is rather evident from the previous statistical analysis that the introduced method performs in a satisfactory manner, even if a small number of realizations of certain alphabet symbols (1–5) appear in a document to-be-classified. This is so, for ifNiL=8andNjL=3, then, according to the general form of formula (7.3b), the statistical tests and/or the subsequent maximum likelihood considerations apply to 8×8=64 and 8×3=24 quantities, which are satisfactory number of samples from a statistical point of view.Criterion for attributing a non-classified document to one of the already spotted writers. We attribute document Dito the writer Wq, for whom quantity ξq,iis maximum among all ξp,i.Suppose that the application of the previous methodology has offered μ distinct hands and that ν1 documents have been attributed to the first hand W1, ν2 to hand W2,…,νμdocuments have been attributed to writer Wμ. Let the documents attributed to an arbitrary hand Wiform a setΔi={D∼1Wi,D∼2Wi,…,D∼νiWi}Then, we randomly choose one of the considered documents, say the documentD∼jWpand we remove this document from the corresponding set Δp. Next, we make all pair-wise comparisons ofD∼jWpwith all documents belonging to Δ1 and for all available alphabet symbols, thus obtaining a sequence of ν1 likelihood values ξ1,j,k, k=1,2,…,ν1. Letξ̃1,jbe the maximum of ξ1,j,kover k. Evidently, quantityξ̃1,jexpresses the maximum likelihood of similarity of documentD∼jWpwith the documents of set Δ1, namely with the documents of writer W1.We repeat the same process for set Δ2, thus obtaining a maximum likelihoodξ̃2,jand so on, until quantityξ̃μ,jis obtained, associated with writer Wμ. Then, we compute the maximum ofξ̃1,j,ξ̃2,j,…,ξ̃μ,jand suppose that this maximum corresponds to writer Wq. If q=p, namely if documentD∼jWpis again attributed to Wp, then, one may claim that the classification achieved so far is self-consistent, as far as documentD∼1Wpis concerned. Otherwise, the method reports a non-consistent classification in the approach.We repeat this process for all available documents. If all documents give consistent results, then we claim that the overall approach is self-consistent. Otherwise, one may define a degree of inconsistency according to the number of documentsD∼jWp, which were classified in a non-consistent manner.In this section, we will first give a very brief description of the data set, consisting of ancient inscriptions and Byzantine codices, upon which the introduced methodology has been applied. Two of the authors, whose main disciplines is Archaeology/Epigraphy and/or Classical Studies, namely Prof. Stephen Tracy and Prof. Christopher Blackwell, selected a number of really important ancient documents, so that the rest of the authors (dealing with the discipline of Computer Engineering and Mathematics) could test the efficiency of the developed methodology. In particular, Prof. Stephen Tracy has chosen a number of ancient inscriptions, while Prof. Christopher Blackwell has selected a number of Byzantine codices.We stress that both Prof. St. Tracy and Prof C. Blackwell took special care and they were very rigorous at not disclosing any information whatsoever to the rest of the authors, concerning the documents upon which the methodology has been tested. Consequently, the entire information about the documents that is presented in Sections 8.1 and 8.2 below, has been disclosed and written by these two authors, after the application of the method and the presentation of the related results by the rest of the team.At this point, we would like to make a comment about the availability of these ancient documents to other potentially interested researchers: In association with the Byzantine codices, an electronic application to Prof. Blackwell is, as a rule, sufficient. Concerning the ancient inscriptions, the process of gaining access to the inscriptions themselves or their images is far more complicated. First of all, we would like to point out that the images of the inscriptions were taken by the authors of the paper, by means of a strict photographic protocol, at their own expenses and after a quite serious effort. In order that a researcher gains access to this and/or related material, he/she must apply to the corresponding Ephorate of Prehistoric and Classical Antiquities and the Central Archaeological Council himself/herself to obtain an approval of accessibility.A list of the selected inscriptions is given below, where each inscription is labeled by its code number in Agora and the Epigraphic Museum of Greece (Original ID).The 46 inscriptions chosen by Professor Tracy and presented in Table 1below represent a fairly broad time period, namely from mid-fourth century B.C. to late second century B.C. and a variety of different styles of lettering. We, once more, emphasize, that Prof. Tracy took special care not to disclose any information whatsoever about the inscriptions to the rest of the authors. In particular, he revealed nothing about the era of each inscription, its content, the place where it was unearthed or any other information he already knew.The authors have themselves photographed the inscriptions under a strict photographic protocol, obtaining high-resolution images (of more than 250pixels per cm) of these documents.A list of the selected Byzantine codices is given below, where each manuscript is labeled by its code number given by the Technical Institution it comes from (Original ID).We repeat that Prof. Blackwell took special care not to disclose any information whatsoever about these manuscripts to the rest of the authors. This set of codices is presented in Table 2below:The authors specialized in the fields of Computer Engineering and Mathematics have applied the steps of the methodology discussed previously. In particular, they made exhaustive pair-wise Bonferoni-type comparisons of all inscriptions, as described in Sections 7.1 and 7.2, thus reaching the conclusion that all 46 inscriptions have been written by ten (10) distinct writers. More specifically, the two inscriptions for which the hypothesis that they belong to the same cutter has been rejected by the greater confidence level, while at the same time, the corresponding level of significance was smaller than αT, were I33 and I39; evidently, these two inscriptions have been attributed to two different hands, we call hand 1 (H1) with representativeD∼1the inscription I33 and hand 2 (H2), with representativeD∼2the inscription I39. Next, they have compared the remaining 44 inscriptions with I33 and I39 via the same method; they selected the inscription for which the hypothesis that was made either by H1 or by H2 was rejected by the highest degree of confidence, again, provided that the corresponding level of significance was smaller than αT. This inscription was I17, which we assume it was made by another hand H3; evidently the representativeD∼3of H3 is I17. They continued this process, classifying I31 to another hand H4 (D∼4=defI31), I23 to hand H5 (D∼5=defI23), I36 to H6 (D∼6=defI36), I22 to hand H7 (D∼7=defI22), I10 to hand H8 (D∼8=defI10), I2 to hand H9 (D∼9=defI2) and I3 to hand H10 (D∼10=defI3); at this point, all statistical hypotheses that the remaining inscriptions were made by the already spotted hands, could not be rejected. In fact, the associated two-tailed areas were essentially larger (more than fifty times) than αT; this indicates that the whole method performs well for a wide range of threshold values αT.As a next step, they have classified the remaining 36 inscriptions to hands H1,…,H10, by the maximum likelihood criteria introduced in Section 7.3. This process continued until all 36 inscriptions have been classified as indicated in Table 3. We would like to point out that the introduced method furnished maximum likelihood values that offered a quite clear-cut distinction between inscriptions made by the same writer and the rest of the documents. For example, when an inscription, say 7587, was compared with all the already classified ones, then, this comparison offered maximum likelihood of the order of 10−2 when 7587 was compared with inscriptions generated by the same hand, while it offered maximum likelihood of the order of 10−5 or less, when it was compared with inscriptions inscribed by another writer. Clearly, this jump in the maximum likelihood values was the more evident, the grater the number of realizations of the employed alphabet symbols. However, it is worthwhile reporting that this jump was quite clear-cut, even in association with inscriptions containing a limited number of realizations (less than 6); the reason for this favorable behavior has already been stated in the previous sections and mainly in Section 7.Prof. Tracy, as well as other prominent epigraphists fully agree with this classification, with no exception whatsoever. Conclusively, the method offered 100% successful identification of the writer of each given inscription.Once more, the authors specialized in the fields of Computer Engineering and Mathematics applied the approach described in Sections 7.1 and 7.2 and reached the conclusion that there are four (4) distinct hands, who have written all 23 documents; the corresponding representative Byzantine codices for each one of these hands areD∼1=defBC12,D∼2=defBC7,D∼3=defBC23, andD∼4=defBC14. After that, the two-tailed areas of the performed comparisons were essentially larger (more than fifty times) than αT, a fact suggesting that no more distinct writers exist. This indicates that the choice of the value of αThas a minor impact in the performance of the method. The remaining 19 Byzantine codices have been classified to one of these 4 hands by the maximum likelihood criterion introduced in Section 7.3.Prof. Christopher Blackwell and the experts of Center of Hellenic Studies (CHS) of Harvard University fully agree with this classification. In this sense, the introduced method offers 100% successful classification on the tested set of codices (See Table 4).At this point, we would like to point out that the Consistency criterion introduced in Section 7.3 has been fully satisfied for both the 46 inscriptions and the 23 Byzantine codices. We would also like to once more emphasize that no training sets (i.e. reference inscriptions or reference Byzantine codices) have been used in this classification. In addition, no information concerning the number of different hands, nor any related database were available.

@&#CONCLUSIONS@&#
