@&#MAIN-TITLE@&#
Estimating Reputation Polarity on Microblog Posts

@&#HIGHLIGHTS@&#
We find that reputation polarity of a post is different from sentiment.We model reputation polarity using feature classes from communication theory.We introduce new features based on the replies to a post.We propose different ways to operationalise the RepLab 2012 and 2013 tasks.

@&#KEYPHRASES@&#
Social media analysis,Online reputation analysis,

@&#ABSTRACT@&#
In reputation management, knowing what impact a tweet has on the reputation of a brand or company is crucial. The reputation polarity of a tweet is a measure of how the tweet influences the reputation of a brand or company. We consider the task of automatically determining the reputation polarity of a tweet. For this classification task, we propose a feature-based model based on three dimensions: the source of the tweet, the contents of the tweet and the reception of the tweet, i.e., how the tweet is being perceived. For evaluation purposes, we make use of the RepLab 2012 and 2013 datasets. We study and contrast three training scenarios. The first is independent of the entity whose reputation is being managed, the second depends on the entity at stake, but has over 90% fewer training samples per model, on average. The third is dependent on the domain of the entities. We find that reputation polarity is different from sentiment and that having less but entity-dependent training data is significantly more effective for predicting the reputation polarity of a tweet than an entity-independent training scenario. Features related to the reception of a tweet perform significantly better than most other features.

@&#INTRODUCTION@&#
Social media monitoring and analysis has become an integral part of the marketing strategy of businesses all over the world (Mangold & Faulds, 2009). Companies can no longer afford to ignore what is happening online and what people are saying about their brands, their products and their customer service. With growing volumes of online data it is infeasible to manually process everything written online about a company. Twitter is one of the largest and most important sources of social media data (Jansen, Zhang, Sobel, & Chowdury, 2009). Tweets can go viral, i.e., get retweeted by many other Twitter users, reaching many thousands of people within a few hours. It is vital, therefore, to automatically identify tweets that can damage the reputation of a company from the possibly large stream of tweets mentioning the company.Tasks often considered in the context of online reputation management are monitoring an incoming stream of social media messages and profiling social media messages according to their impact on a brand or company's reputation. We focus on the latter task. In particular, we focus on the problem of determining the reputation polarity of a tweet, where we consider three possible outcomes: positive, negative, or neutral. Knowing the reputation polarity of a single tweet, one can either aggregate this knowledge to understand the overall reputation of a company or zoom in on tweets that are dangerous for the reputation of a company. Those tweets need counteraction (van Riel & Fombrun, 2007).The reputation polarity task is a classification task that is similar to, but different in interesting ways, from sentiment analysis. For example, a post may have a neutral sentiment but may be negative for reputation polarity. Consider, for instance, the statement The room wifi doesn't work., which is a factual statement that may negatively impact the reputation of a hotel.There are two standard benchmarking datasets for reputation polarity, the RepLab 2012 dataset (Amigó, Corujo, Gonzalo, Meij, & de Rijke, 2012a) and the RepLab 2013 dataset (Amigó et al., 2013), made available as part of RepLab, a community-based benchmarking activity for reputation analysis. In view of the distinction that we have just made between sentiment analysis and reputation polarity, it is interesting to observe that the best performing reputation polarity classifiers at RepLab are sentiment-based. The main research question we address is:RQ1 Can we improve the effectiveness of baseline sentiment classifiers by adding additional information?The RepLab 2012 and 2013 datasets have different training and testing scenarios: the 2012 dataset uses a training and testing setup that is independent of individual brands or companies (“entities”), while this dependence is introduced in the 2013 dataset. We ask:RQ2 How do different groups of features perform when trained on entity-(in) dependent or domain-dependent training sets?Our last research question is exploratory in nature. Having introduced new features and interesting groups of features, we ask:RQ3 What is the added value of features in terms of effectiveness?Without further refinements, RQ3 is a very general research question. One of the contributions of this paper, however, is the way in which we model the task of determining the reputation polarity of a tweet as a three-class classification problem: we build on communication theory to propose three groups of features, based on the sender of the tweet, on the message (i.e., the tweet itself), and on the reception of the message, that is, how the tweet is being perceived.While we use and compare some features that are known from the literature (Naveed, Gottron, Kunegis, & Alhadi, 2011), a second contribution that we make in this paper consists of new features to capture the reception of messages-this is where the difference between reputation polarity and sentiment analysis really shows.Furthermore, as we will see below, reputation polarity class labels are highly skewed and data for some features is missing; our third contribution below consists of an analysis of sampling methods to alleviate the problem of skewness.Another important contribution that we make concerns the way in which we operationalize the reputation management task. Social media analysts use company-specific knowledge to determine the reputation polarity (Corujo, 2012). In line with this, we discover that sets of tweets pertaining to different entities may be very different in the sense that different features are effective for modeling the reputation polarity. We therefore provide an operationalization of the reputation polarity task using the RepLab 2012 dataset in which we train and test on company-dependent datasets instead of using a generic training set. We find that we can avoid overtraining and that training on far fewer data points (94.4% less) per entity gives up to 37% higher scores. The observation transfers to the RepLab 2013 dataset which is operationalized in precisely that way.Finally, this paper adds a new point of view for the business analysis perspective: here our biggest contribution is the difference in performance of features when trained on entity or domain dependent or independent data. Features pertaining to the author of the message seem to be generalizable while others are not.We proceed with a definition of the reputation polarity task in Section 2. Section 3 introduces our features and reputation polarity model. We detail our experiments, results and analysis in Sections 4 and 5, respectively. Section 6 provides an overview of related work, and we conclude in Section 7.The current practice in the communication consultancy industry is that social media analysts manually perform labeling and classification of the content being analyzed (Amigó et al., 2012a). Two of the most labor intensive tasks for reputation analysts are monitoring and profiling of media for a given company, product, celebrity or brand (“entity”). The monitoring task is the (continuous) task of observing and tracking the social media space of an entity for different topics and their importance for the reputation of the entity. Here, the retrieval and aggregation of information concerning the entity is most important. Technically, the monitoring task can be understood as consisting of two steps as follows:(Cluster)cluster the most recent social media posts about an entity thematically, andassign relative priorities to the clusters.In this paper we focus on the profiling task, which is the (periodic) task of reporting on the status of an entity's reputation as reflected in social media. To perform this task, social media analysts need to assess the relevance of a social media post for an entity and the likely implications on the entity's reputation that the post has. Specifically, when working on Twitter data as we do in this paper, the profiling task consists of two subtasks, i.e., to assess for a given tweet.(Relevance)whether the tweet is relevant to the given entity, andwhether the tweet has positive, negative, or no implications for the entity's reputation.The relevance assessment subtask is very similar to WePS3 (Amigó et al., 2010) and to the retrieval task assessed at the TREC Microblog 2011 and 2012 track (Ounis, Macdonald, Lin, & Soboroff, 2011). The polarity subtask is new, however, and so far, it has received little attention from the research community. It is a three-class classification task: a tweet can have a negative, positive, or no implication at all (i.e., it is neutral) for the reputation of an entity. This class label is what we call the reputation polarity of a tweet.After having defined and motivated the reputation polarity task, we now turn to modeling the task.In this section we provide our model for estimating reputation polarity. For the remainder of the paper we are working with Twitter data; details of our experimental setup are provided in Section 4.We treat the reputation polarity task as a three-class classification problem. We introduce baseline features based on the literature, i.e., mainly using sentiment classifiers, in Section 3.1. We go beyond the baseline features by introducing different types of feature, that we group together in a manner inspired by the transmission model from communication theory (Shannon & Weaver, 1949). A similar grouping of features has been used by (Balahur et al., 2010) to manually distinguish opinion and sentiment in news. They analyze annotation procedures and find that three different views need to be addressed. In each communication act, we have a sender who sends a message and a receiver who receives this. So, we have three types of feature:(Sender)features based on the sender of the tweet that we are trying to classify,features based on the (content of the) tweet itself, andfeatures based on the reception of a tweet.In Sections 3.2 and 3.3 we introduce the sender and message features, respectively. We explain different ways to compute reception features in Section 3.4. In Section 3.5 we explain how we combine the features in a classification paradigm. Table 1provides an overview of our features and their types.We use two approaches to estimate the sentiment score of a tweet. We start with a simple, but effective, way of estimating the sentiment of short texts that is based on manually created sentiment word lists (Liu, 2012). After that we consider a more sophisticated approach, based on SentiStrength, a state of the art sentiment analysis tool for social media (Thelwall, Buckley, & Paltoglou, 2012).We begin by introducing our notation. We use p to denote negative (−1), neutral (0), or positive (1) reputation polarity of a given tweet.11In sentiment analysis researchers usually only score for negative and positive, assuming that the negative and positive will cancel another out and create a score for neutral. We do the same. The classifier still classifies as −1, 0, or 1.We write W to denote the vocabulary of all words; w stands for an element of W. A tweet T is contained in the set of all tweetsT. We also consider the subsetT^⊆T. This is the subset of tweets for which the reputation polarity needs to be estimated. We write react(T) to denote the set of reactions (replies or retweets) available for tweet T. Impact features are learnt with a learning rate δi. Specifically, we use a simple linear decay function for our learning rate so thatδi=δ0·1i. Finally, we use a polarity filter that returns an item x only if it has the same sign as polarity p:(1)PF(x,p)={xif0.35em0exsign(x)=sign0.35em0ex(p)0otherwise,where sign(x) is the sign of x.We write sent(T, R) to denote the sentiment of a tweet; superscripts indicate different scoring functions introduced in the following sections: sentWWL(T, R) and sentSS(T, R) use weighted word lists and SentiStrength, respectively. R denotes the term scoring function.Letsent_word(w,p)be the sentiment score of a term w based on sentiment wordlists for different polarities p. The wordlists are distinct for the different polarities. This can be the basis of an overall sentiment score, by summing the sentiment of terms22We need to aggregate over the sentiment of terms since a term can be in both positive and negative word lists, depending on the context, e.g., homeopathic.:(2)sentWWL(T,sent_word(·,·))=∑w∈T∑p∈{−1,1}sent_word(w,p).In specific cases in our discussions below, we formalize the association between words and sentiment using a scoring functionR:W×{−1,1}→[0,1]that maps a word w and polarity p tosent_word(w,p):(3)sentWWL(T,R)=∑w∈T∑p∈{−1,1}R(w,p).Below, we consider different scoring functions Ri, whereR0(w,p)=sent_word(w,p).SentiStrength (Thelwall et al., 2012) is a word list-based sentiment scoring system. It generates sentiment scores based on predefined lists of words and punctuation with associated positive or negative term weights. Word lists are included for words bearing sentiment, negations, words boosting sentiment, question words, slang and emoticons. The standard setting of SentiStrength has been optimized for classifying short social web texts by training on manually annotated MySpace data. Thelwall et al. (2012) provide extensive details of the features used and of the training methodology.We used the standard out-of-the-box setting of SentiStrength. We write sentSS(T, Ri) to denote usage of SentiStrength with the term weights Ri. The score of a single term w is denotedsent_wordSS(w,·).According to social media analysts, the sender is a key factor in determining the impact of a message (Corujo, 2012). How do we capture the sender? Information about the sender can be provided by the sender herself, providing nominal features such as the time zone and location she is in, and the language she speaks. The intuition behind those features is that if a sender located in Europe talks about a brand only distributed in the US in German, this does not impact the reputation of a company as much. It can also be an artefact of her standing in the Twitter community, such as the number of followers or the number of lists the sender has been added to, both of which are numerical features.Other sender features we use are directly associated with the creation and validation of the account: whether the account has been verified (nominal), the age of the account (numerical), and whether the automatic transmission of the geographical location (nominal) has been enabled. In particular, the verification and account age are important to identify spammers: young, unverified accounts are probably more likely to be spam accounts than verified accounts. Verified accounts are never accounts from the general public (Twitter, 2014). The location of the sender the moment the tweet was sent may indicate that she is in the vicinity of the brand, or as mentioned above, in a non-relevant are. All features are encoded in the JSON-formatted data obtained through the Twitter API. The account age is the number of days the account existed prior to the last tweet in the collection.Moving on to the message features, we use several metadata message features. We use numerical features derived from tweets such as the number of links, usernames, and hashtags. Those features are extracted from the tweet: usernames begin with an @, hashtags with a #, and we used regular expressions to extract the number of urls and punctuation marks. The intuition behind the features stems from the idea of monitoring the quality of tweets (Weerkamp & de Rijke, 2012) or the potential of being retweeted (Naveed et al., 2011). Tweets with many hashtags often hijack trending topics, and are spam-like. Intuitively, they should not have a large impact on the reputation of a company. Similarly, tweets that are of a very colloquial nature do not necessarily have a large impact on the reputation. However, tweets with a question are engaging (Naveed et al., 2011). The tweet can be favourited (a nominal feature) by other users. The number of times a tweet was favourited is a lower bound of the number of times a tweet was actually read. This indicates the reach of a tweet. This information is provided in the JSON formatted data downloaded from Twitter.We further use textual message features, such as the identified language, the number of punctuation marks, and discriminative terms. We use language identification (Carter, Weerkamp, & Tsagkias, 2013) to identify the language (a nominal feature) of the tweet, which may be different to the language set as standard by the user. As our final textual message feature we select discriminative terms. We either use five or ten terms with the highest log likelihood ratio (llr (5), or llr (10)) of the two models built on the texts of messages in the positive and negative classes, respectively, in the training set (Manning, Raghavan, & Schütze, 2008).We move on to our reception features, the third column in Table 1. Reception features are meant to estimate how a tweet is being received. An initial operationalization of this idea is simply to determine the sentiment of a tweet. But we do not stop there. In communication theory (Barnlund, 1970), the reception of a message is said to depend on the responses that it generates, and in particular on the sentiment in these responses (and not just in the originating message). Below, we present an algorithm that aims to capture the iterative nature of this perspective by taking into account the sentiment in the reactions to a message. Here, a reaction to a tweet is either a direct reply or a retweet to the tweet.Our reception features, then, come in two groups: a group of baseline features that provide initial estimates of the reception of a tweet by computing its sentiment score in different ways (see Section 3.1) second group that iteratively re-estimates the reception based on the initial estimations provided by the features in the first group. Below, we refer to the first group as baseline or sentiment features (WWL, SS) and the second group as impact features (I-WWL, I-SS, I-WWL-RP, I-SS-RP).Algorithm 1Impact features, computed using the EM algorithm.As pointed out above, we assume that a tweet's perceived impact on the reputation of an entity is reflected in the sentiment of the replies that it generates. This estimation, in turn, can be used to update the word lists used for sentiment analysis with the terms in the tweet, assigning the added words to the classes predicted for the tweets in which they are contained. The updated word lists can then be used to re-estimate the impact in the replies of the same tweet, but also other tweets. We assume that the overall, combined reputation polarity of reactions to a tweet is the same as the reputation polarity of the tweet itself.Essentially, this approach assumes that there is an entity-specific latent word list that denotes different terms for reputation. This list is updated iteratively, so that estimating the impact of a tweet is an process that can be computed using a variant of the Expectation Maximization algorithm described below. Here, the latent parameters are the entity and polarity specific scoring function R based on a word list. The goal of the algorithm is to maximize the impact(T) of a tweet T.Algorithm 1 provides a schematic overview of the process. There are three key phases, initialization, expectation and maximization, which we explain below.Recall that we record sentiment scores in a scoring function R; this is the latent scoring function at iteration 0 for the polarity p:(4)R0(w,p)=PF(sent(w,sent_word(·,·)),p).The maximization step is the estimation of the impact as solicited in the reactions react(T) to a tweet T. To estimate the impact, we estimate the average sentiment of the replies based on iteratively altered word lists. For iterations i > 0,(5)impacti(T)=1|react(T)|∑Tr∈react(T)sent(Tr,Ri−1).For the sentiment estimation at every round i, (senti−1) we can use the approaches listed in Section 3.1. The maximization step can be performed by the sentiment classifier by retraining based on the current word list. We do not retrain; instead, we treat the word lists as the algorithms' own positive and negative word lists.The estimation of the latent variable Ri(w, p) for term w and polarity p is done by interpolating the variableRi−1(w,p)with the average polarity of the tweets in which the term occurs. Formally,(6)R^i(w,p)=Ri−1+δi1|T^|∑T∈T^PF(impacti(T),p),where δiis the interpolation factor andδi≤δi−1. We normalize the scoring function Risuch that(7)Ri(w,p)=R^i(w,p)∑wi∈WR^i(wi,p).The impact polarity of a tweet T is therefore estimated as sent(T, RN), where N is the number of iterations of Algorithm 1. Using sent(T, R0) is equivalent to simply estimating the sentiment, as explained in Section 3.1.We write I-WWL and I-SS to refer to the impact features as computed by Algorithm 1, where the sentiment of a tweet T has been estimated using the Weighted-WordList (sentWWL(T, N)) and SentiStrength (sentSS(T, N)), respectively. Similarly, the impact features I-WWL-RP and I-SS-RP use only the replies to tweets in the computation of Algorithm 1.We detail our parameter settings in Section 4.We mentioned before that the basic underlying assumption of the impact algorithm is the reflected in the sentiment of the replies that is generates. The user study and the examples in Appendix C support this assumption. Nevertheless, we would like to point out (constructed) examples where this assumption clearly fails. Consider a tweet that conveys a message that implies a bad reputation related to a product. The answers to that tweet express a disapproval sentiment with respect to the opinion expressed in the original tweet:(Tweet)The X user interface is terrible. It blows.I hate it when people like you blame their own stupidity on an innocent UI.WTF? Stop being so dismissive and change it, dummy. X is open source after all.Let us assume that those are the only tweets in the collection to estimate the impact scores. If terrible, blows, hate, stupidity, dismissive, dummy are the only sentiment terms used, we would enforce the negativity of the terms terrible and blows: making the tweet even more negative than it actually is, even though the replies are actually defending and improving the reputation of the product. This counteracts our assumption. Additionally, Answer 2, is neutral about the product, however, not about the author of the original tweet. Assuming there are no replies, the impact will be estimated as negative.While, on a small scale those examples clearly fail to be covered by the impact, there are two silver linings. First, the power of large amounts of data. The iterations on only one tweet and its responses may not necessarily give a complete picture of the distribution of the data. Other tweets with different response patterns will reduce the influence of the hopefully few tweets that violate our assumption. Second, the impact features are only one group of features. We have presented other features and feature groups that, in particular in an entity-dependent training scenario (see Section 4), may cover up the failures of the impact features.As pointed out above, we model the task of estimating the reputation polarity of a tweet as a three-class classification problem. We use decision trees to combine and learn the features. Decision trees are known to perform well when faced with nominal and missing data (Russell, Norvig, Candy, Malik, & Edwards, 1996).33In preliminary experiments on the RepLab 2012 and 2013 datasets, we examined the performance of support vector machines and random forests. Both performed much lower than decision trees, due to a large number of missing features.They are essential to our setting because they are human and non-expert understandable. This characteristic is vital for social media analysts who need to explain successes and failures of their algorithms to their customers.To answer our research questions as formulated in the introduction, we run a number of experiments. We use two datasets. The first, RepLab 2012, was introduced at CLEF 2012 (Amigó et al., 2012a). Based on lessons learnt, the second dataset, RepLab 2013, was introduced at CLEF 2013 (Amigó et al., 2013). A detailed description of the datasets can be found in Appendices A.1 and A.2. We detail the preprocessing of our data in Section 4.1. We then describe our approaches to sampling to address the strong class imbalance in our datasets (Section 4.2). Section 4.3 outlines the operationalization of the task and the different training procedures. Based on this, our experiments, parameter settings, and evaluation procedures are explained in Sections 4.4,4.5,4.6.We separate punctuation characters from word characters (considering them as valuable tokens) and keep mentions, hashtags, and smilies intact. Language identification is done using the method described in Carter et al. (2013). We use publicly available sentiment word lexicons in English (Hu & Liu, 2004; Liu, Hu, & Cheng, 2005) and Spanish (Pérez-Rosas, Banea, & Mihalcea, 2012) to estimate the weighted word list baselines (WWL).As we will see below, the data that we work with displays a strong imbalance between classes. In particular, far more tweets are labeled with positive than negative reputation in the RepLab 2012 dataset. To deal with this, we consider two strategies, both relying on the following notation. Let Scbe the sample size for each polarity class (p∈{−1,0,1}), and let M denote the size of the largest () polarity class.=APTARANORMALmax{Sp|p∈{−1,0,1}}and m denote the size of the smallest polarity class. We oversample for each polarity class p by selecting each data point Kptimes (whereKp=⌊MSp⌋), and pad this with kp(wherekp=M0.35em0exmod0.35em0exSp) randomly sampled data points from the polarity class p. As an alternative we also consider undersampling by randomly selecting mmodSpdata points from the majority classes until we have at most the number of data points in the minority class (Chawla, 2010).

@&#CONCLUSIONS@&#
