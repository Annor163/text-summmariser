@&#MAIN-TITLE@&#
Stochastic modeling of the NLMS algorithm for complex Gaussian input data and nonstationary environment

@&#HIGHLIGHTS@&#
An accurate stochastic model for the NLMS algorithm is developed.This model considers a nonstationary environment, i.e., a time-varying plant.Model expressions describing the steady-state algorithm behavior are obtained.Analytical solutions for the normalized autocorrelation-like matrices are proposed.Simulation results for different scenarios confirm the accuracy of the model.

@&#KEYPHRASES@&#
Adaptive filtering,Nonstationary environment,Normalized least-mean-square (NLMS) algorithm,Stochastic modeling,

@&#ABSTRACT@&#
This paper presents a stochastic model for the normalized least-mean-square (NLMS) algorithm operating in a nonstationary environment with complex-valued Gaussian input data. To derive this model, several approximations commonly used in the modeling of algorithms with normalized step size are avoided, thus giving rise to very accurate model expressions describing the algorithm behavior in both transient and steady-state phases. Such accuracy comes mainly from the strategy used for computing the normalized autocorrelation-like matrices arising from the model development, for which analytical solutions are also derived here. In addition, based on the proposed model expressions, the impact of the algorithm parameters on its performance is discussed, clarifying the tracking properties of the NLMS algorithm in a nonstationary environment. Through simulation results, the effectiveness of the proposed model is assessed for different operating scenarios.

@&#INTRODUCTION@&#
Since its inception in 1937 by Kaczmarz [1] (currently also credited to [2] and [3]), the normalized least-mean-square (NLMS)11The terminology of normalized LMS was coined by Bitmead and Anderson in 1980 [4].algorithm has been successfully used in many practical applications, such as network and acoustic echo cancellation, noise cancellation, active noise control, channel equalization, and adaptive array beamforming [5–12]. This success is mainly because of its low computational complexity, inherent numerical stability, very good tracking properties, and high robustness in a wide range of operating conditions. Basically, such characteristics come from the use of a normalized step size in contrast to the LMS algorithm, which makes the NLMS algorithm less susceptible to variations in the input signal power [5–7]. However, due to the use of this normalized step size, the stochastic modeling of the NLMS algorithm becomes a very difficult task (as compared with the LMS one) which is still an open research problem.In short, the stochastic modeling of adaptive algorithms aims to determine analytical expressions (i.e., a mathematical model) that describe with a certain accuracy the algorithm behavior under different operating conditions, thus providing the basis for a deeper understanding of the algorithm considered. Moreover, since stochastic models allow predicting the algorithm behavior, they can be used for tuning the algorithm parameters as well as for performance assessment, avoiding the need of exhaustive Monte Carlo (MC) simulations. Furthermore, from the model expressions, some cause-and-effect relationships between algorithm parameters and performance metrics can be established, giving support to consistent design guidelines [13–16]. Lastly, analytical models can reveal undesired (anomalous) algorithm behavior, allowing to modify the algorithm either for correcting such a behavior or customizing it for specific operating conditions [17].Nevertheless, the modeling of adaptive algorithms presents several mathematical challenges, which are mostly circumvented by using a certain number of simplifying assumptions (e.g., based on the step-size value, filter length, or even statistical characteristics of the input data) during the model development [7]. However, the use of several simplifying assumptions generally gives rise to models with limited practical applicability (i.e., restricted to very specific operating conditions). On the other hand, to obtain accurate models for a wide range of operating conditions, a reduced number of simplifying assumptions should be considered in the model derivation, thereby implying more mathematical challenges [18]. In summary, there is a tradeoff between mathematical complexity, number/consistency of the simplifying assumptions, and accuracy of the obtained model.Concerning the modeling of the NLMS algorithm, as pointed out by [13] and [19], one of the major challenges resides in the determination of the normalized autocorrelation-like matrices arising from the model development. Regarding the calculus of such matrices, for the case of real-valued input data, several approaches22The pioneer research work on this subject dates back to 1986 [20], when a very accurate methodology has been presented; however, analytical solutions are described there only for some particular conditions.can be found in the open literature considering different approximations and, consequently, resulting in models with different accuracy levels (i.e., exhibiting better or worse match with MC simulations) [13,20–29]. In contrast, for complex-valued input data (as occurs in several practical applications [5–9]), the results presented in the literature are very scarce and eventually exhibit certain inconsistencies [30,31]. For instance, in [30], the authors claim that analytical solutions for the calculus of the normalized autocorrelation-like matrices are presented; however, this claim is not plausible since such solutions involve the generalized exponential integral [32] related to the algorithm regularization parameter [21]. In [31], disregarding now the regularization parameter, a solution for the normalized autocorrelation-like matrix (required in the model expression that describes the mean weight behavior) has been derived; however, it is possible to show that this solution diverges when filters with order greater than two are used. Therefore, in the case of complex-valued Gaussian input data, the determination of analytical solutions for the normalized autocorrelation-like matrices is still an open issue.Besides the calculus of the normalized autocorrelation-like matrices, it is important to emphasize that other challenges arise during the derivation of model expressions describing the algorithm behavior in steady state. As a consequence, the steady-state analysis of the NLMS algorithm has been addressed only in [7,22,23,25,26,29,30]; however, model expressions obtained in most of these research works focuses exclusively on the case of real-valued input data. For complex-valued input data, model expressions describing the steady-state algorithm behavior are derived only in [7] and [30]; nevertheless, neither [7] nor [30] present analytical expressions. In particular, the results presented in [7] require the determination of an expected value (e.g., see [7, (21.21)]) whose solution has not yet been determined in the literature. Likewise, expressions developed in [30] involve the calculus of the normalized autocorrelation-like matrices as well as the inverse of a full matrix (e.g., see [30, (15)]), for which solutions have not been provided. Therefore, for complex-valued input data, the derivation of analytical expressions describing the steady-state behavior of the NLMS algorithm remains an open issue.In this context, focusing on the methodology devised in [20], the present research work aims to address the following goals:i)to obtain model expressions (mean weight behavior, learning curve, and weight-error covariance matrix) for the NLMS algorithm operating under complex-valued data;to determine analytical solutions for the normalized autocorrelation-like matrices considering complex-valued Gaussian input data;to derive a model comprising both nonstationary and stationary environments;to obtain analytical expressions describing the algorithm behavior in steady state;to investigate how the algorithm parameters affect its performance in a nonstationary environment.This paper is organized as follows. Section 2 revisits the NLMS algorithm and introduces the general formulation of the nonstationary environment considered. Section 3 derives the proposed model, from which an analysis of the steady-state algorithm behavior is also carried out. Still, some particular results for uncorrelated input data are presented in this section. Section 4 shows some simulation results in order to assess the accuracy of the model expressions. Finally, Section 5 presents remarks and conclusions of this research work.This section revisits the general expressions of the NLMS algorithm considering complex-valued data, and introduces a brief description of the nonstationary environment used for the model development.The weight update equation of the NLMS algorithm for complex-valued data is given by [6](1)w(n+1)=w(n)+μx(n)e⁎(n)xH(n)x(n)+εwherew(n)=[w1(n)w2(n)⋯wM(n)]Tdenotes the M-dimensional adaptive weight vector,x(n)=[x(n)x(n−1)⋯x(n−M+1)]T, the input vector, μ, the step-size control parameter,e(n), the error signal, andε>0, a regularization parameter that prevents division by zero [5,6]. By considering a system identification problem (see Fig. 1), the error signal can be expressed as(2)e(n)=[wo(n)−w(n)]Hx(n)+η(n)whereη(n)represents the measurement noise andwo(n)=[wo,1(n)wo,2(n)⋯wo,M(n)]T, a time-varying weight vector characterizing the nonstationary plant.A fundamental feature of the adaptive filters is their capability to track variations in the underlying signal statistics, thus motivating the development of models that describe the algorithm tracking behavior. For such, a model based on a first-order Markov process is used [33]. This model states that the system to be identified (plant) undergoes random variations according to [6,7](3)wo(n+1)=αwo(n)+ϕ(n)where0<α⩽1,wo(0)characterizes an arbitrary starting vector, andϕ(n)=[ϕ1(n)ϕ2(n)⋯ϕM(n)]Tdenotes a plant perturbation vector.Note that the value of parameter α in (3) must be close to 1, implying that many iterations of the Markov model are required to produce a significant change in the plant weight vector [6]. Particularly, forα=1, (3) reduces to the random-walk model commonly considered in the literature [5–7]; as a consequence, such a condition follows as a special case. Still, makingα=1andϕ(n)=0∀n in (3), the plant weight vector becomes time invariant and, hence, one returns to the special case of a stationary environment.Now, having the expressions characterizing both the NLMS algorithm and the nonstationary environment considered at hand, we can proceed to the derivation of model expressions that describe the algorithm behavior. For such, the following assumptions are firstly stated:A1.The input data is obtained from a zero-mean Gaussian process with varianceσx2and autocorrelation matrixR=E[x(n)xH(n)][6,7].The regularization parameter can be neglected in the stochastic modeling under certain conditions, i.e., asε≪E[xH(n)x(n)][14,19].The measurement noise is obtained from a zero-mean process with varianceση2, which is uncorrelated with any other signal in the system [5–7].The plant perturbation vectorϕ(n), assumed uncorrelated with any other signal in the system, is obtained from a zero-mean process with varianceσϕ2and autocorrelation matrixΦ=E[ϕ(n)ϕH(n)][5–7]. Also, it is assumed thatϕ(n)andϕ(m)are uncorrelated forn≠m.For a slow adaptation condition,w(n)andx(n)are assumed uncorrelated [5–7].An expression that describes the mean weight behavior of the NLMS algorithm can be determined by substituting (2) into (1), taking the expected value of both sides of the resulting expression, and using Assumptions A2–A5. Thus,(4)E[w(n+1)]=(I−μR1)E[w(n)]+μR1E[wo(n)]where I denotes the identity matrix with dimensionM×Mand(5)R1=E[x(n)xH(n)xH(n)x(n)].Note that, from (3) and Assumption A4, the term in (4) related to the nonstationarity of the plant can be written as(6)E[wo(n)]=αnwo(0).The main point now is to determine the normalized autocorrelation-like matrixR1, which is given by (5). However, the procedure to compute such an expected value [as well as those in (21) and (22)] requires the knowledge of the probability density function of the input data; therefore, as a consequence of assuming Gaussian input data (see Assumption A1), distinct solutions are obtained depending on whether the input data is complex- or real-valued [33]. Particularly, for the case of complex correlated Gaussian input data,R1can be written as33The solution presented in [17, Appendix I] has been reproduced here to make this paper self-contained.[17](7)R1=QHQHwhere Q is the eigenvector matrix arising from the eigendecomposition of R (i.e.,R=QΛQH[6]) and H represents a diagonal matrix whose elements are given by(8)hi,i=1G[−Aiωi−∑l=1l≠iMBl,ilog(ωlωi)]with(9)ωi=−1λi,(10)G=∏k=1Mλk,(11)Ai=1∏k=1k≠iM(ωi−ωk)and(12)Bl,i=1(ωl−ωi)∏k=1k≠lM(ωl−ωk).Therefore, from the model expression (4), along with (6) and the solution of the autocorrelation-like matrixR1[given by (7)–(12)], the mean weight behavior of the NLMS can be properly predicted for both nonstationary and stationary environments.In order to derive an expression that describes the algorithm learning curve [mean-square error (MSE)], one first defines the weight-error vector as(13)v(n)=w(n)−wo(n).Thus, using (13), the error signal (2) can be rewritten as(14)e(n)=−vH(n)x(n)+η(n).Then, carrying out the producte(n)e⁎(n), taking the expected value of both sides of the resulting expression, and considering Assumptions A3 and A5, one obtains (see [5–7])(15)J(n)=E[|e(n)|2]=Jmin+Jex(n)with the minimum MSE attainable in steady state given by(16)Jmin=ση2and the excess MSE (EMSE), by(17)Jex(n)=λTk′(n)where vectorsλandk′(n)contain, respectively, the eigenvalues of R and the diagonal elements of(18)K′(n)=QHK(n)QforK(n)=E[v(n)vH(n)]denoting the weight-error covariance matrix.Therefore, from the model expressions (15)–(17), the algorithm learning curve can be properly predicted if the diagonal elements of the transformed weight-error covariance matrixK′(n)are known.Since (17) requires the knowledge of the diagonal elements ofK′(n)and taking into consideration that such a model expression can differ from one algorithm to another, a recursive expression fork′(n)is derived here considering the NLMS algorithm. For such, using (3), (13), and (14), the weight update equation (1) is first rewritten in terms ofv(n)as(19)v(n+1)=v(n)−μx(n)xH(n)v(n)xH(n)x(n)+ε+μx(n)η⁎(n)xH(n)x(n)+ε+(1−α)wo(n)−ϕ(n).Then, determining the productv(n+1)vH(n+1), taking the expected value of both sides of the resulting expression, and considering Assumptions A2–A5, we get(20)K(n+1)=K(n)−μK(n)R1−μR1K(n)+μ2R2+μ2JminR3−μ(1−α){R1[K˜(n)−Ko(n)]+[K˜H(n)−Ko(n)]R1}+(1−α){[K˜(n)−Ko(n)]+[K˜H(n)−Ko(n)]}+(1−α)2Ko(n)+Φwith(21)R2=E{x(n)xH(n)K(n)x(n)xH(n)[xH(n)x(n)]2},(22)R3=E{x(n)xH(n)[xH(n)x(n)]2},(23)Ko(n)=E[wo(n)woH(n)],and(24)K˜(n)=E[w(n)woH(n)].Now, considering (7), along with the results obtained of Appendices A and B, pre- and post-multiplying (20) byQHand Q , respectively, and taking the diagonal elements of the resulting expression, we get the following recursive expression fork′(n):(25)k′(n+1)=Bk′(n)+gs+2(1−α)A[k˜′(n)−ko′(n)]+(1−α)2ko′(n)+ϕ′whereϕ′characterizes a vector containing the diagonal elements ofQHΦQ, A denotes a diagonal matrix given by(26)A=I−μH,B, a full matrix defined as(27)B=I−2μH+μ2(T+P),and(28)g=μ2Jmin.Note that (25) requires the knowledge of vectorsko′(n)andk˜′(n), which are related to the diagonal elements of (23) and (24), respectively. Thus, a recursive expression forko′(n)is obtained by determining the outer product of (3), computing the expected value of both sides, pre- and post-multiplying byQHand Q , respectively, and taking the diagonal elements of the resulting expression, which yields(29)ko′(n+1)=α2ko′(n)+ϕ′.Similarly, from (1)–(3), we get(30)k˜′(n+1)=αAk˜′(n)+αμHko′(n).In (25)–(30), matrices H, T, and P, and vector s come from the calculus of the normalized autocorrelation-like matricesR1,R2, andR3(see Section 3.1, Appendices A and B), implying that the determination of H, T, P, and s depends on the statistical characteristics of the input data. In particular, for complex correlated Gaussian input data, H is determined as in Section 3.1 [see (8)], while the elements of the diagonal matrix T are obtained by (see Appendix A)(31)ti,i=1G(−Ai2ωi+∑l=1l≠iMBl,iωl−ωi{ωl[1+log(ωiωl)]−ωi}),the off-diagonal44As shown in Appendix A, the main diagonal elements of matrix P are also determined by using (31).elements of the symmetric matrix P by(32)pi,j=1G(Aiωj−ωi{log(−ωi)+2ωj[1−log(−ωj)]ωj−ωi}+Bj,i{2ωi[1−log(−ωi)]ωj−ωi−log(−ωj)}+∑l=1l≠i,jMBl,i{(ωl−ωi)(ωl+ωi−2ωj)ωj[1−log(−ωj)](ωj−ωi)2(ωl−ωj)+(ωl+ωj−2ωi)ωi[1−log(−ωi)](ωj−ωi)2+ωl[1−log(−ωl)]ωl−ωj}),and the elements of vector s by (see Appendix B)(33)si=1G(−Ailog(−ωi)+∑l=1l≠iMBl,i{ωl[1−log(−ωl)]−ωi[1−log(−ωi)]})withωi, G,Ai, andBl,igiven by (9), (10), (11), and (12), respectively.Therefore, from the model expressions (15)–(17), along with (8)–(12) and (25)–(33), the algorithm learning curve can be properly predicted for both nonstationary and stationary environments.In this section, we firstly derive model expressions relating algorithm parameters with some performance metrics (namely, steady-state EMSE and misadjustment); afterwards, based on these expressions, we discuss important philosophical aspects about the steady-state algorithm behavior.Concerning the steady-state EMSE, makingn→∞in (17), the resulting expression reads as(34)Jex(∞)=λTk′(∞)which requires the determination ofk′(∞). Thus, lettingn→∞in (25), we get(35)k′(∞)=Bk′(∞)+gs+2(1−α)A[k˜′(∞)−ko′(∞)]+(1−α)2ko′(∞)+ϕ′with(36)ko′(∞)=limn→∞ko′(n)=1(1−α2)ϕ′and(37)k˜′(∞)=limn→∞k˜′(n)=μα(1−α2)(I−αA)−1Hϕ′.Then, substituting (36) and (37) into (35), using (26), and solving the resulting expression fork′(∞), we obtain(38)k′(∞)=(I−B)−1[gs+2μ(1+α)−1(I−αA)−1Hϕ′].Note that (38) involves the inverse of the diagonal matrix (I−αA), which has a trivial solution, as well as the inverse of the full matrix (I−B), whose solution cannot be obtained analytically for correlated input data. Hence, aiming to avoid the use of numerical methods in the determination of such an inverse, the following approximation is considered:(39)(I−B)−1=12μC−1{I+μ2P[I−μ2C−1P]−1C−1}≅12μC−1(I+μ2PC−1)which is obtained by using the matrix inversion lemma [34] and assuming a slow adaptation condition, with(40)C=H−μ2Tdenoting a diagonal matrix.Finally, substituting (39) and (28) into (38), and the resulting expression into (34), the steady-state EMSE can now be rewritten as(41)Jex(∞)=Jex,1(∞)+Jex,2(∞)where(42)Jex,1(∞)≅μ2JminλTC−1(I+μ2PC−1)srepresents the portion of the EMSE present in both stationary and nonstationary cases, and(43)Jex,2(∞)≅1(1+α)λTC−1(I+μ2PC−1)(I−αA)−1Hϕ′denotes the portion of the EMSE related to the nonstationary environment considered. According to [5–7],Jex,1(∞)is due to the weight vector noise and is called estimation error, whereasJex,2(∞)is due to the weight vector lag and is termed lag error.Note, from (41)–(43), that the misadjustment can be straightforwardly obtained through [6,7](44)M=Jex(∞)JminwhereJmindenotes the minimum MSE attainable in steady state [see (16)].Here, important philosophical aspects coming from the model expressions (41)–(43) are discussed, aiming to provide some insights on the steady-state algorithm behavior operating under the nonstationary environment considered. In this context, to assess the impact of the plant variation speed on the adaptive algorithm, Fig. 2plots steady-state EMSE curves obtained from (41)–(43) forα=0.999and three different values ofσϕ2. (The remaining parameter values are the same as used later in Example 2 of Section 4, considering 40 dB SNR.) Observe from this figure that, asσϕ2is increased (implying faster changes in the plant weights), the algorithm exhibits higher steady-state EMSE values, mainly due to the lag errorJex,2(∞). Following the same presentation pattern, Fig. 3plots (41)–(43) forσϕ2=10−8and three different values of α. Notice from this figure that the convex characteristic of the steady-state EMSE curve vanishes as α is decreased, e.g., forα=0.95.For more details about the convex characteristic of the steady-state EMSE, Fig. 4plots separate curves of (41), (42), and (43) forα=0.999andσϕ2=10−8. Notice that, under certain conditions, the overall steady-state EMSEJex(∞)is a convex function with respect to the step size, which can be explained as follows. Firstly, it is well-known [5–7] that the estimation errorJex,1(∞)is an increasing function of μ and, hence, small values of the estimation error are achieved with small step sizes. On the other hand, the lag errorJex,2(∞)exhibits a decreasing55Actually, if the approximation (39) is not used to obtain (43),Jex,2(∞)will exhibit a slightly increasing characteristic asμ>1, thus implying thatJex,2(∞)becomes a convex function. Nevertheless, sinceμ>1is not used in the practice, this aspect does not impact on the discussion presented here.characteristic with respect to μ, since larger step-size values are required to properly track the variations of the plant. Therefore, we conclude that there exists a tradeoff between the estimation error, the lag error, and the step size. Such a tradeoff indicates that the optimum step-size value (in the EMSE sense) occurs in the region whereJex,1(∞)is approximately equal toJex,2(∞).In contrast to the development presented so far, we describe in this section some particular results obtained specifically for complex uncorrelated Gaussian input data. In this context, makingλi=σx2∀i in [17, (40)], (59), (60), and (62), and then solving the resulting integrals [32], we now get the following solutions for:(45)H=IM,(46)T=IM(M+1),(47)P=11TM(M+1),and(48)s=1M(M−1)σx2where 1 represents an M-dimensional vector of 1's. Thereby, the model expressions describing the mean weight behavior, learning curve, and weight-error covariance matrix [i.e., (4), (15), and (25)] can also be used to predict the algorithm behavior under complex uncorrelated Gaussian input data.Because of the particular structure of (45)–(48), a model expression for the steady-state EMSE can be determined [disregarding the approximation given in (39)] by substituting (45)–(48) into (38) and the resulting expression into (34), using the Sherman–Morrison identity [5, (6.59)], and makingλ=σx21andϕ′=σϕ21. Thus, considering (41), the estimation error can now be written as(49)Jex,1(∞)=μ(2−μ)M(M−1)Jminand the lag error as(50)Jex,2(∞)=2M2σx2σϕ2(1+α)[(1−α)M+αμ](2−μ).Notice from (49) that the estimation error does not depend on the variance of the input data, thereby evidencing the immunity of the NLMS algorithm against variations in the input signal power in contrast to the LMS algorithm (see [6, (5.98)] or [7, (16.10)]). On the other hand, observe that the lag error (50) is directly proportional toσx2, which may compromise the algorithm performance if the input signal power reaches a very high value. Moreover, since (50) is also an increasing function of M andσϕ2, higher-order filters and/or plants that exhibit fast variations will lead to larger lag errors. Finally, except for the common term (2−μ), we verify that (49) is directly proportional to μ while (50) is inversely proportional to μ, ratifying the existence of a tradeoff between the estimation error, the lag error, and the step size.As discussed, under certain conditions, the steady-state EMSE curve is convex with respect to the step size (e.g., see Fig. 4). This fact indicates the existence of an optimum step size, i.e., a step-size value that leads to a minimum EMSE in steady state. For this case, an analytical expression for the optimum step size can be determined by substituting (49) and (50) into (41), differentiating it with respect to μ and equaling the resulting expression to zero, which yields(51)μo=M(M−1)σx2σϕ2α(1+α)Jmin(1+(1+α)[(1−α)M+2α]JminM(M−1)σx2σϕ2−1−(1−α2)Jmin(M−1)σx2σϕ2).Note that, forM≫1andα≅1, (51) can be simplified to(52)μo≅M2σx2σϕ22Jmin(1+4JminM2σx2σϕ2−1)corroborating the results described in [7].In order to gain insights on the behavior of the optimum step sizeμo, Fig. 5plots the model expression (51) as a function of the plant perturbation varianceσϕ2. (The remaining parameter values are the same as used later in Example 2 of Section 4, considering uncorrelated input data and 40 dB SNR.) Notice from this figure that, for higher values ofσϕ2(implying fast changes in the plant weights), the model expression (51) leads to larger step-size values; in contrast, for smaller values ofσϕ2(i.e., slow changes in the plant weights), the optimum step size approaches zero. Furthermore, (51) presents saturation for both very low and very high values ofσϕ2, conditions in which the EMSE is mainly affected by the estimation error and the lag error, respectively. In particular, for very high values ofσϕ2, the saturation occurs forμoaround 1, i.e., the step-size value that provides the fastest convergence speed for the NLMS algorithm [35].In this section, aiming to assess the accuracy of the proposed model, four examples are presented considering different operating scenarios, i.e., covering different types of plants with several lengths, distinct SNR values, and various eigenvalue spreads of the input autocorrelation matrix. In these examples, the results obtained from MC simulations (average of 200 independent runs) are compared with model predictions. For such examples, a zero-mean Gaussian input signalx(n)with varianceσx2=1is used, which is obtained from [6](53)x(n)=−a1x(n−1)−a2x(n−2)+v(n)wherea1anda2denote the AR(2) coefficients, andv(n)is a white noise with variance given by(54)σv2=(1−a21+a2)[(1+a2)2−a12].Here, the signal-to-noise ratio (SNR) is defined as(55)SNR=10log10(σx2ση2).Throughout this example, assuming a nonstationary environment, the accuracy of the proposed model is verified [via learning curve (MSE)] for different input data correlation levels and SNR values. For such, we use a complex-valued plantwoAwithM=32weights obtained from a Rayleigh channel model [7]. The eigenvalue spread values considered for the input autocorrelation matrix areχ=34.73[obtained from (53) fora1=−0.2anda2=0.7] andχ=1097.69(by changinga2to 0.97). Five different values of SNR are also used, i.e., 10, 20, 30, 40, and 60 dB SNR. For this scenario, the initial condition chosen for the adaptive weight vector isw(0)=woA, the step size,μ=0.15, and the regularization parameter,ε=10−3. In order to obtain a time-varying plant, we assumeα=0.999,σϕ2=10−8, andwo(0)=woAin (3).Fig. 6depicts the learning curves (MSE) obtained by MC simulations and from the proposed model for different input data correlation levels and SNR values. In this figure, a very good accuracy of the proposed model is verified (in both transient and steady-state phases) forχ=34.73[Fig. 6(a)] andχ=1097.69[Fig. 6(b)], irrespective of the SNR value considered. Hence, in the next examples, one considers only the cases of 20 and 40 dB SNR. Also, notice from Fig. 6 that the model expression describing the steady-state MSE [i.e., (15) along with (41)–(43)] exhibit a very good match with the simulation results.This example has two main goals, namely: i) to assess the accuracy of the model expressions describing the mean weight behavior (4), the learning curve (EMSE) (17), and the steady-state EMSE (41)–(43); and ii) to ratify our claim that under certain conditions there exists an optimum step size that leads to a minimum EMSE in steady state. For these purposes, we consider here a complex-valued plantwoBwithM=32weights obtained from a cable modem channel [36, Channel 1] (see Fig. 7). The input signal is obtained from (53) witha1=−0.2anda2=0.9, resulting in an eigenvalue spread ofχ=232.04for the input autocorrelation matrix. In this scenario, the step-size value isμ=0.15, which according to Fig. 4 results in the minimum steady-state EMSE. Here, the initial conditions for the adaptive weight vector and for the first-order Markov process arew(0)=woBandwo(0)=woB, respectively, while the remaining parameters are the same as in Example 1. Especially to achieve Goal ii), results predicted by (41)–(43) are compared with those obtained from MC simulations, averaging the last 100 EMSE values in steady state.For this operating scenario, Fig. 8depicts (for the sake of clarity) the mean behavior of the real [Fig. 8(a)] and imaginary [Fig. 8(b)] parts of four adaptive filter weights obtained through MC simulations and from the prediction model (4). (In Fig. 8, the case of 40 dB SNR has been omitted, since the results are very similar to 20 dB SNR.) From these figures, one observes a very good match between MC simulations and model predictions for both real and imaginary parts of the adaptive filter weights. In addition, Fig. 9shows learning curves (EMSE) obtained by MC simulations and from the model expression (17), where a very good accuracy is also verified for both transient and steady-state phases.Fig. 10illustrates steady-state EMSE curves (obtained by MC simulations and prediction model) as a function of μ, confirming (under certain conditions) the existence of an optimum step-size value [see Fig. 10(b)]. Notice from this figure that the model expressions (34) and (38) exhibit a very good match with the results obtained from MC simulations even for larger step-size values. In contrast, due to the use of the approximation (39), a mismatch is verified between simulations and the results predicted from the analytical expressions (41)–(43) as the step-size value increases.Here, to verify the effectiveness of the proposed model under a more realistic operating scenario, two plants encountered in microwave communication systems are used. Such plants termedwoC(M=64) andwoD(M=128) are obtained from [37, Channels 3 and 7], respectively (see Fig. 11). The eigenvalue spreads of the input autocorrelation matrices areχ=326.03forM=64andχ=394.43forM=128, both obtained by considering the AR(2) coefficients given in Example 2. Parameters α, μ, and ε are the same as in Example 2, withw(0)andwo(0)corresponding to the plant considered. Particularly for this example, three different values of the plant perturbation variance are considered, i.e.,σϕ2=10−10,σϕ2=10−8, andσϕ2=10−6, aiming to assess the proposed model vis-à-vis plants that undergo slow, moderate, and fast changes, respectively.Fig. 12shows the results obtained from MC simulations and model predictions, considering the plants given in Fig. 11. Specifically in Figs. 12(a) and (b), the accuracy of the proposed model is verified through learning curves (EMSE) for the plantwoC; following the same presentation pattern, Figs. 12(c) and (d) depict the evolution of the EMSE for the plantwoD. Notice the very good accuracy achieved by the proposed model for these more realistic plants, irrespective of the value of variance considered for the plant perturbation vector. Furthermore, we can observe that fast time-varying plants (obtained by increasingσϕ2) lead to higher steady-state EMSE values due to the tracking lag, corroborating the results discussed in Section 3.4.In this example, we assess the proposed model [via learning curve (EMSE)] for complex uncorrelated Gaussian input data [i.e., settinga1=a2=0in (53)] and a stationary environment, which is obtained by makingα=1andσϕ2=0in (3). For such, we now consider the solutions of the normalized autocorrelation-like matrices given by (45)–(48) along with the model expressions (17) and (25). Here, the plant used is the same as in Example 2 (see Fig. 7),w(0)=0,ε=0, and three different step-size values are considered, i.e.,μ=0.1,μ=0.25, andμ=0.5. In addition, to verify the accuracy of the model expression describing the steady-state EMSE, results obtained from MC simulations (averaging the last 100 EMSE values in steady state) are compared with those predicted from the model expression (49).For this operating scenario, Fig. 13depicts learning curves (EMSE) obtained through MC simulations and from the proposed model, considering complex uncorrelated Gaussian input data. Notice again the very good accuracy achieved for the step-size values used. In addition, Fig. 14shows the steady-state EMSE curves (as a function of μ) obtained by MC simulations and from the model expression (49) for both SNR values considered. From these figures, a very good match can also be observed even for step-size values close to the stability upper bound.Based on the results previously presented, one can infer that the proposed model presents a very good accuracy under different operating conditions, i.e., taking into account several types of plant and lengths, a wide range of eigenvalue spreads of the input autocorrelation matrix, as well as different SNR andσϕ2values.In this paper, considering complex Gaussian input data, an analytical stochastic model for the NLMS algorithm operating under a nonstationary environment was presented, from which a model for a stationary environment follows as a special case. In contrast to other models from the literature that, in general, do not properly predict the algorithm transient phase, the proposed model accurately predicts the algorithm behavior in both transient and steady-state phases for a wide range of operating conditions. Such accuracy is mainly due to the minimal number of simplifying assumptions considered and the strategy used for computing the normalized autocorrelation-like matrices. Regarding these matrices, the approach adopted here preserves in the model the effect of the step size normalization, which is verified through a reduction in the eigenvalue spread of the normalized autocorrelation-like matrices. Moreover, expressions describing the steady-state EMSE, misadjustment, and the step size for the minimum EMSE are derived here. Based on these model expressions, important philosophical aspects of the NLMS algorithm operating under a nonstationary environment are discussed. Through simulation results, the effectiveness of the proposed model was confirmed for both nonstationary and stationary environments. Lastly, taking into account the accuracy of the proposed model, one can conclude that the proposed model can be used to predict the algorithm behavior in several applications, avoiding the use of exhaustive MC simulations for tuning the algorithm parameters as well as for performance assessment.

@&#CONCLUSIONS@&#
