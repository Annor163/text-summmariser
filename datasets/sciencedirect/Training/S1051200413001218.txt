@&#MAIN-TITLE@&#
Compressed sensing signal recovery via forward–backward pursuit

@&#HIGHLIGHTS@&#
We propose a greedy CS recovery method with iterative forward and backward steps.The forward step being larger than the backward one, the support grows iteratively.The sparsity level is not required a priori in contrast to SP and CoSaMP.Recovery accuracy is improved over BP, SP and OMP, in run times are equal to OMP.We obtain better recovery accuracy than BP on a sparse image recovery problem.

@&#KEYPHRASES@&#
Compressed sensing,Forward–backward search,Sparse signal reconstruction,Greedy algorithms,Two stage thresholding,

@&#ABSTRACT@&#
Recovery of sparse signals from compressed measurements constitutes anℓ0norm minimization problem, which is unpractical to solve. A number of sparse recovery approaches have appeared in the literature, includingℓ1minimization techniques, greedy pursuit algorithms, Bayesian methods and nonconvex optimization techniques among others. This manuscript introduces a novel two stage greedy approach, called the Forward–Backward Pursuit (FBP). FBP is an iterative approach where each iteration consists of consecutive forward and backward stages. The forward step first expands the support estimate by the forward step size, while the following backward step shrinks it by the backward step size. The forward step size is larger than the backward step size, hence the initially empty support estimate is expanded at the end of each iteration. Forward and backward steps are iterated until the residual power of the observation vector falls below a threshold. This structure of FBP does not necessitate the sparsity level to be known a priori in contrast to the Subspace Pursuit or Compressive Sampling Matching Pursuit algorithms. FBP recovery performance is demonstrated via simulations including recovery of random sparse signals with different nonzero coefficient distributions in noisy and noise-free scenarios in addition to the recovery of a sparse image.

@&#INTRODUCTION@&#
Despite the conventional acquisition process which captures a signal as a whole prior to dimensionality reduction via transform coding, Compressed Sensing (CS) aims at acquisition of sparse or compressible signals directly in reduced dimensions. Mathematically, the “compressed” observations are obtained via an observation matrix Φ(1)y=Φx,where x is a K-sparse signal of length N, K is the number of nonzero elements in x, y is the observation vector of length M, and Φ is anM×Nrandom matrix withK<M<N. Once y is observed, the goal of CS is to recover x, which is analytically ill-posed following the dimensionality reduction via Φ. Exploiting the sparse nature of x, CS reformulates (1) as a sparsity-promoting optimization problem(2)x=argmin‖x‖0subject toy=Φx,where‖x‖0, called theℓ0norm by abuse of terminology, denotes the number of nonzero elements in x. As direct solution of (2) is computationally intractable, a number of alternative and approximate solutions have emerged in the literature. An overview of mainstream methods is available in [1], which broadly categorizes CS algorithms as convex relaxation techniques, greedy pursuits, Bayesian methods and nonconvex optimization techniques. Theoretical exact recovery guarantees have also been developed mainly under the Restricted Isometry Property (RIP) [2–4] for some of the algorithms. RIP also provides a basis for understanding what type of observation matrices should be employed. Random matrices with Gaussian or Bernoulli entries, or matrices randomly selected from the discrete Fourier transform satisfy RIP with high probabilities [2,3].Convex relaxation methods [2,5–9] replace theℓ0minimization in (2) with its closest convex approximation, theℓ1minimization. Following this modification, recovery is tractable via convex optimization algorithms such as linear programming, as proposed by Basis Pursuit (BP) [8], which is historically the first convex relaxation algorithm. Greedy pursuit algorithms, such as Matching Pursuit (MP) [10], Orthogonal MP (OMP) [11], Compressive Sampling MP (CoSaMP) [12], Subspace Pursuit (SP) [13] and Iterative Hard Thresholding (IHT) [14,15], employ iterative greedy mechanisms. In addition, [16] provides a framework called Two Stage Thresholding (TST), into which algorithms such as SP and CoSaMP fall.The manuscript at hand proposes a two stage iterative greedy algorithm, called the Forward–Backward Pursuit (FBP). As the name indicates, FBP employs forward selection and backward removal steps which iteratively expand and shrink the support estimate of x. With this structure, FBP falls into the general category of TST-type algorithms, while iterative expansion of the support estimate is investigated for the first time in this concept. Despite their similar structures, FBP has a major advantage over SP and CoSaMP: Since its forward step is larger than the backward one, FBP iteratively expands the support estimate, removing the need for an a priori estimate of K, which is mostly unknown. Additionally, the backward step of FBP can remove some possibly misplaced indices from the support estimate, which is an advantage over forward greedy algorithms such as OMP. In parallel, the simulation results in this manuscript demonstrate that FBP can perform better than SP, OMP and BP in most scenarios, which indicates that SP is not necessarily the globally optimum TST scheme as proposed in [16].A forward–backward greedy approach for the sparse learning problem, FoBa, has been investigated in [17]. Though both FoBa and FBP consist of iterative forward and backward steps, the algorithms have some fundamental differences: FoBa employs strict forward and backward step sizes of one. On the contrary, the forward step size of FBP is greater than one, while the backward step size might also be. By increasing the difference between the forward and backward step sizes, FBP terminates in less iterations. Second, FoBa takes the backward step after a few forward steps based on an adaptive decision. FBP employs no adaptive criterion for taking the backward step, which immediately follows each forward step. (Note that using an adaptive criterion is not trivial when the step sizes are greater than one.) Finally, FoBa has been applied for the sparse learning problem, while we propose and evaluate FBP for sparse signal recovery from compressed measurements.This manuscript is organized as follows: First, we give a brief overview of greedy pursuit algorithms. The FBP algorithm is introduced in Section 3. Section 4 demonstrates FBP recovery performance in comparison to the BP, SP and OMP algorithms via simulations involving sparse signals with different nonzero coefficient distributions, phase transitions, noiseless and noisy observations, and a sparse image. We conclude with a brief summary in Section 5. A preliminary version of this work, [18], has been presented at EUSIPCOʼ2012.In this section, we summarize OMP, SP and TST, which are important for our purposes because of their resemblance to the FBP algorithm. Beforehand, we define the notation that is used throughout the paper:Tkdenotes the estimated support of x after the kth iteration, whileT˜kstands for the expanded support after the forward selection step of the kth iteration.y˜kis the approximation of y after the kth iteration andrkis the residue of y after the kth iteration.ΦJdenotes the matrix consisting of the columns of Φ indexed byJ, andxJis the vector of the elements of x indexed byJ. Finally,Φ⁎stands for the conjugate of the matrix Φ. Note that each column of Φ is sometimes referred to as an atom in the rest.OMP is a forward greedy algorithm that searches for the support of x by identifying one element per iteration. It starts with an empty support estimate:T0=∅andr0=y. At the iteration k, OMP expandsTk−1with the index of the dictionary atom closest tork−1, i.e. it selects the index of the largest magnitude entry ofΦ⁎rk−1. Next,y˜kis computed via orthogonal projection of y ontoΦTkand the residue is updated asrk=y−y˜k. The iterations are carried out until the termination criterion is met. In this work, we stop OMP when‖rk‖2⩽ε‖y‖2similar to the termination criterion of FBP.SP and CoSaMP combine selection of multiple columns per iteration with pruning, keeping K-element support sets throughout the iterations. At iteration k, SP first expandsTk−1with the indices of the K largest magnitude elements ofΦ⁎rk−1, obtaining the extended supportT˜kof size 2K. (Alternatively, CoSaMP expandsTk−1by 2K elements.) In the second step, the orthogonal projection coefficients of y ontoΦT˜kare computed, andTkis obtained by pruning the indices of all but the K largest magnitude projection coefficients fromT˜k.rkis finally computed using the approximationy˜kwhich is obtained by orthogonal projection of y ontoΦTk. The iterations are stopped when‖rk‖2⩾‖rk−1‖2. CoSaMP and SP are both provided with RIP-based exact recovery guarantees. On the other hand, they employ equal forward and backward step sizes, which lead to a fixed support size between the iterations. This necessitates an a priori estimate of the sparsity level K. This is an important handicap in most practical cases, where K is either unknown or it is not desired to fix it.Recently, Maleki and Donoho have introduced the TST framework [16], into which algorithms such as SP and CoSaMP fall. TST algorithms employ a two stage iterative scheme which first updates the sparse estimate and then prunes it by thresholding. Next, the optimal coefficients are computed by orthogonal projection of y onto the pruned support. This is followed by a second thresholding operation that yields a new sparse estimate. [16] also presents a tuned SP algorithm, which turns out to be the empirically optimal TST scheme for sparse signals with constant amplitude nonzero elements. This algorithm employs a tuned support size which is decided on-the-fly depending on the pre-computed phase transition curves for the particular M and N values of interest. The motivation behind this tuning is to select the support estimate at least as large as the largest sparsity level SP can exactly recover. However, as the results in [16] also indicate, choosing the support size of SP larger than the actual sparsity level degrades the recovery performance. Hence, this tuned SP algorithm is subject to perform worse than the oracle-SP, which incorporates the actual sparsity level.Forward–backward pursuit is an iterative two stage algorithm. The first stage of FBP is the forward step which expands the support estimate byα>1atoms, where we call α the forward step size. These α indices are chosen as the indices of the dictionary atoms which are maximally correlated with the residue, following the motivation of obtaining the best match to it. Then, FBP computes the orthogonal projection of the observed vector onto the subspace defined by the support estimate. Next, the backward step prunes the support estimate by removingβ<αindices with smallest contributions to the projection. Similar to α, we refer to β as the backward step size. The orthogonality of the residue to the subspace defined by the pruned support estimate is ensured by a second projection of the residue onto this subspace. These forward and backward steps are iterated until the energy of the residue either vanishes or is less than a threshold which is proportional to the energy of the observed vector.The FBP algorithm can now be outlined as follows: We initialize the support estimate asT0=∅, and the residue asr0=y. At iteration k, first the forward step expandsTk−1by indices of the α largest magnitude elements inΦ⁎rk−1. This builds up the expanded support setT˜k. Then the projection coefficients are computed by the orthogonal projection of y ontoΦT˜k. The backward step prunesT˜kby removing the β indices with the smallest magnitude projection coefficients. This produces the final support estimateTkof the kth iteration. Finally, the projection coefficients w for the vectors inΦTkare computed via the orthogonal projection of y ontoΦTk, and the residue is updated asrk=y−ΦTkw. The iterations are carried on until‖rk‖2<ε‖y‖2. After termination of the algorithm at the lth iteration,Tlgives the support estimate for x, while w contains the corresponding nonzero values. The pseudo-code of FBP is given in Algorithm 1.As for the termination parameter ε, we choose a very small value in practice (10−6for the experiments below) when the observations are noise-free. For noisy observations, ε should be selected depending on the noise level. To avoid the algorithm running for too many iterations in case of a failure, the maximum size of the support estimate is also limited byKmax. Note that, the specific choice ofKmaxhas no significant effect on the recovery accuracy given it is a bit larger than the underlying sparsity level K (to assure the correct solution may be found before the support size reachesKmax). Since K cannot be known in practice, we may setKmaxeither large enough or simply asKmax=M. In addition, the phase transitions of FBP may also be used for obtaining a large enough estimate forKmaxgiven N and M in a specific scenario. That is, given the empirical phase transition curve,Kmaxcan be chosen such that it corresponds to a sparsity ratio which lies over the phase transition curve for the particular M and N values.An important issue for the performance of FBP is the choice of the forward and backward step sizes. The forward step size α should be chosen larger than 1. It is possible to choose α as large as problem-specific constraints allow, while a reasonable approach would obviously be selecting it small in comparison to the observation length M in order to avoid linearly dependent subsets in the expanded support estimate after the forward step. As for the backward step, β should be smaller than α by the definition of FBP, since the support estimate should be enlarged at each iteration. In order to find an empirically optimal rule for choosing α and β, we present phase transition curves of FBP with various α and β choices among the simulation results below. It turns out that choosingα∈[0.2M,0.3M]andβ=α−1leads to the optimal recovery performance in practice, whereas the algorithm is also quite robust to other choices of α and β as well. In particular, choosingβ<α−1speeds up the algorithm without a severe decrement in the recovery accuracy as demonstrated below.Forward greedy algorithms, such as OMP and other MP variants, which enlarge the support estimate iteratively via forward selection steps, have a fundamental drawback by definition: Since they possess no backward removal mechanism, any index that is inserted into the support estimate cannot be removed. That one or more incorrect elements remain in the support until termination may cause the recovery to fail. FBP, on the contrary, employs a backward step, which provides means for removal of atoms from the support estimate. This gives FBP the ability to cover up for the errors made by the forward step. To illustrate, consider a well-known example: Let x be the summation of two equal magnitude sinusoids with very close frequencies,f1andf2, and Φ be an overcomplete sinusoidal dictionary, containing atoms with frequenciesf1,f2andf3=(f1+f2)/2among others. The first iteration of OMP selects the component with frequencyf3. Then, during the next iterations, the algorithm tries to cover for this error by choosing components other than the two correct ones and fails. Instead, assume we run FBP withα=3andβ=1. During the forward step of the first iteration, FBP selects all the three components with frequenciesf1,f2andf3. Following orthogonal projection, the backward step will eliminatef3, and the recovery will be successful after the first iteration.11Note that the success of FBP in this case depends on the choice of α and β, however, this example still illustrates the motivation behind the backward removal step in a very simple way.In contrast to SP and CoSaMP, the FBP algorithm does not require an a priori estimate of the sparsity level K. Unlike the tuned TST, it does not necessitate a tuning of the support size either. As explained above, FBP enlarges the support estimate byα−βindices at each iteration until termination of the algorithm, which depends on the residual power, and not on the sparsity level. Hence, neither the forward and backward steps nor the termination criterion require an estimate of the sparsity level. In addition, the forward and backward step sizes of FBP may be chosen proportional to M with a simple empirical strategy as demonstrated below, while the recovery performance is quite robust to this choice. These make the FBP algorithm easily applicable in practice in contrast to SP and CoSaMP. This, however, comes at a cost: The theoretical guarantees cannot be provided in a way similar to SP or CoSaMP, which make use of the support size being fixed as K after the backward step. For the time being, we cannot provide a complete theoretical analysis of FBP, and leave this as future work. Note that, however, most of the theoretical analysis steps of SP or CoSaMP also hold for FBP.This section is reserved for the demonstration of the FBP recovery performance in comparison to BP, SP, OMP. For this purpose, we run recovery simulations involving different nonzero coefficient distributions, noiseless and noisy observations, and a sparse image. First, we compare the exact recovery rates, average recovery error and run times of FBP with those of OMP, SP and BP for signals with nonzero elements drawn from the Gaussian and uniform distributions. In order to generalize the results to a wide range of M and K along with different nonzero element distributions, we provide the empirical phase transition curves, which are obtained using the procedure in [16]. Meanwhile, these phase transition curves also serve for the purpose of investigating optimal α and β choices. We then demonstrate recovery from noisy observations, and finally test our proposal on a sparse image to illustrate the recovery performance for a realistic coefficient distribution.Results of the 1D simulations are presented as averages over three different data sets, where the nonzero entries of the test samples in each set are selected from different random ensembles. The nonzero entries of the Gaussian sparse signals are drawn from the standard Gaussian distribution. Nonzero elements of the uniform sparse signals are distributed uniformly in[−1,1], while the constant amplitude random sign (CARS) sparse signals have nonzero elements with unit magnitude and random sign. During the experiments, a different observation matrix Φ is drawn from the Gaussian distribution with mean 0 and standard deviation1/Nfor each test signal. All experiments are performed in the MATLAB environment. For fair comparison of the run times, algorithms share similar structures. The tests are run on a modest laptop with Pentium Dual-Core CPU at 2.3 GHz and 2 GB memory under Windows 7.As for the termination parameters,ε=10−6in the noise-free case, while it depends on the signal-to-noise ratio (SNR) under noisy conditions.Kmax, which is not critical for the recovery performance as discussed in Section 3, is chosen large enough, particularly eitherKmax=MorKmax>M/2. Note that the same ε andKmaxare also used for OMP.First, we compare the exact recovery rates, recovery error and run times of FBP using various α and β values with those of OMP, SP and BP. In these simulations, the signal and observation sizes are fixed asN=256andM=100while K varies in[10,45]. For each K, recovery simulations are repeated over 500 randomly generated Gaussian and uniform sparse signals. The recovery error is expressed in terms of Average Normalized Mean-Squared-Error (ANMSE), which is defined as(3)ANMSE=1500∑i=1500‖xi−xˆi‖22‖xi‖22wherexˆiis the recovery of the ith test vectorxi. In addition, we present the exact recovery rates, which represent the ratio of perfectly recovered test samples to the whole test data. The exact recovery condition is selected as‖x−xˆ‖2⩽10−2‖x‖2following [16]. In these tests, we selectKmax=55to allow for exact recovery of sparse signals up to aboutM/2=50nonzero elements. Note that, for the specific N and M values in this experiment, the phase transition occurs well belowM/2(see the phase transitions below), hence choosingKmax>M/2is sufficient.Figs. 1 and 2depict the reconstruction performance of FBP with various α and β choices for the Gaussian sparse signals in comparison to OMP, BP and SP. Fig. 1 is obtained by varying α in[2,30], whileβ=α−1. That is, the forward step size varies, while the support estimate is expanded by one element per iteration. For Fig. 2, α is selected as 20, and β is altered in[13,19], changing the increment in the support size per iteration for a fixed forward step size. The run times of the FBP, SP and OMP algorithms are also compared, while BP is excluded as it is incomparably slower than the other algorithms. Analogous results are provided in Figs. 3 and 4for the uniform ensemble as well.According to Fig. 1, increasing α while keeping the support incrementα−βfixed improves the recovery performance of FBP. We observe that the exact recovery rates of FBP are significantly better than the other candidates for all choices of α, even including the modest choiceα=2. BP, SP, and OMP start to fail at aroundK=25, where FBP is still perfect for all choices of α. Moreover, forα⩾20, the FBP failures begin only whenK>30. As for ANMSE, FBP is the best performer whenα⩾20. With this setting, BP can beat FBP in ANMSE only whenK>40. In addition, FBP yields better recovery rates than OMP and SP for all choices of α.In Fig. 2, we observe that increasing β for a fixed α improves the recovery performance. In this case, the exact recovery rate of FBP significantly increases with the backward step size, while ANMSE remains mostly unaltered. This indicates that when β is increased, nonzero elements with smaller magnitudes, which do not significantly change the recovery error, can be more precisely recovered. In comparison to the other algorithms involved in the simulation, FBP is the best performer forβ>15. Similar to the previous test case, BP can produce lower ANMSE than FBP only forK>40.Investigating Figs. 3 and 4, which depict recovery results for the uniform sparse signals, we observe a similar behavior as well. FBP yields better exact recovery rates than the other algorithms when α and β are large enough, i.e.α⩾5in Fig. 3, andβ⩾16in Fig. 4, while BP can perform better than FBP in terms of the average error whenK>30.As for the run times, we expectedly observe that increasing α or β slows down FBP. This is due to the decrease in the increment of the support size per iteration, which increases the number of iterations and the number of required orthogonal projection operations. Moreover, the dimensions of the orthogonal projection operations also increase with the forward step size. On the other hand, increasingα−βdecreases the number of necessary iterations, as a result of which FBP terminates faster. More important, the run time of FBP, SP and OMP are very close whenα=20andβ⩽α−2. In caseα=20andβ=17, the speed of FBP and OMP are almost the same, whereas the exact recovery rate of FBP is significantly better than the other algorithms involved. Note that the speed of FBP can be improved by removing the orthogonal projection after the backward step, at the expense of a slight degradation in the recovery performance.Phase transitions are important for empirical evaluation of CS recovery algorithms over a wide range of the sparsity level and the observation length. Below, we present the empirical phase transition curves of the FBP algorithm in comparison to those of the OMP, SP, and BP algorithms. These graphs are obtained from recovery simulations involving 200 Gaussian, uniform or CARS sparse signals each. Below, we first depict phase transitions of FBP with different α and β choices in order to investigate the optimality of these over the observation length. These simulations provide us an empirical strategy about how to choose the FBP step sizes in relation to M. Next, we compare FBP with two different settings to BP, OMP and SP for the three test sets. In this test set, we setKmax=M. This choice is reasonable here, since we are interesting in recovering signals with a very wideK/Mrange.To explain how we obtain the phase transitions, let us first define normalized measures for the observation length and the sparsity level:λ=M/Nandρ=K/M. To obtain the phase transition curves, we keep the signal length fixed atN=250, and alter M and K to sample the{λ,ρ}space forλ∈[0.1,0.9]andρ∈(0,1]. For each{λ,ρ}tuple, we randomly generate 200 sparse instances and run FBP, OMP, BP and SP algorithms for recovery. The exact recovery condition being‖x−xˆ‖2⩽10−2‖x‖2as before, exact recovery rate is obtained for each{λ,ρ}tuple and each algorithm. The phase transitions are then obtained using the methodology described in [16]. That is, for each λ, we employ a generalized linear model with logistic link to describe the exact recovery curve over ρ, and then find the ρ value which yields50%exact recovery probability.Phase transitions provide us important means for finding an empirical way of choosing α and β optimally. As discussed in [16], the phase transition curve is mostly a function of λ. That is, it remains unaltered when N changes. Moreover, the transition region turns out to be narrower with increasing N. These claims are also supported by some other publications in the literature [19–22]. Hence, in order to find an optimal set of step sizes for FBP, we need to have a look at the phase transitions using different α and β parameters. For a better understanding of their optimality, α and β should not be fixed but be proportional to M. Trying to find fixed α and β values is subject to fail mainly for very low or very high λ values. In other words, it would not be possible to find a fixed optimal set{α,β}for the whole λ range even when we fix N. This is, however, possible when α is proportional to M, and β is related to the chosen α value. In order to find an optimal choice, we run two distinct sets of simulations: First, we vary α in[0.1M,0.4M], whereasβ=α−1. Then we fixα=0.2M, and select β either in[0.7α,0.9α]or asα−1.The phase transitions obtained by the procedure described above are depicted in Fig. 5for the Gaussian, uniform, and binary sparse signals. These graphs indicate that the performance of FBP fundamentally improves with α and β, except for very high α choices. Another exception is the recovery of CARS sparse signals, which constitute the hardest problem for this type of algorithms [16,13]. For this case, the gain with α is not significant, while the phase transitions remain unaltered when β changes. Another important observation that can be deduced from these results is that the performance of FBP is quite robust to the choice of forward and backward step size choices.Concentrating on the forward step, the graphs on the left side of Fig. 5 reveal that the phase transitions are slightly improved with α untilα=0.3Mfor the uniform and Gaussian sparse signals. Choosingα=0.4M, in contrast, improves the phase transitions only for the mid-λ region, while the results get worse especially for the high λ values.22We do not increase α over 0.4M, however note that doing so would even further narrow the mid-λ range where the recovery is slightly improved, and widen the high λ region where the performance is degraded.The reason for this degradation is that the size of the expanded support estimate exceeds M after the forward step for large K and α values, which leads to an ill-posed orthogonal projection problem, and causes the recovery to fail. According to Fig. 5,α=0.3Mis reasonable for a globally optimum FBP recovery accuracy, while this value might be increased if the problem lies in the mid-λ region. On the other hand, taking into account the computational complexity, we observe no significant decrement in the recovery performance whenα=0.2M. Hence, we selectα=0.2Mbelow for faster termination, and show that even this choice already leads to better phase transitions than OMP, BP, and SP for the Gaussian and uniform sparse signals.33In fact, the α values evaluated in the previous section do also cover a wide range including the detailed investigation of the choice of β forα=0.2M=20.As for the backward step, the recovery accuracy decreases slightly with β for the Gaussian and uniform sparse signals. Though this degradation increases slightly with λ, we observe that the recovery performance of FBP is quite robust to the choice of the backward step size in addition to the forward step size. Remember that theβ/αratio commands the increment in the support size per FBP iteration, and reducing this ratio accelerates the recovery process. Therefore, these results reveal that it is possible to reduce the complexity of FBP by decreasingβ/α. The phase transition comparison below states that the phase transition curves of FBP are still better than those of the BP, SP, and OMP algorithms for the recovery of uniform and Gaussian sparse signals with reducedβ/αrates. Similarly, recovery results from the previous section also reveal that FBP does not only provide better recovery rates than the other candidates, but is also as fast as them withα=20andβ=17, which corresponds toα=0.2Mandβ=0.85α.Fig. 6compares the phase transition curve of FBP to those of OMP, BP and SP for the Gaussian, uniform and CARS sparse signals, where FBP is run with bothα=0.2M,β=α−1andα=0.2M,β=0.8α. For the Gaussian and uniform distributions, FBP outperforms the other algorithms, while for the CARS case BP is better than FBP and the other greedy algorithms. As a consequence of its strong theoretical guarantees and convex structure, the phase transition of BP is robust to the coefficient distribution. On the other hand, the performances of the greedy candidates SP, FBP and OMP degrade for the CARS case, while the FBP and OMP curves show the highest variation among different distributions. We observe that when the nonzero values cover a wide range, as for the Gaussian distribution, the performances of FBP and OMP are boosted. In contrast, nonzero values of equal magnitudes are the most challenging case for these algorithms. This is related to the involved correlation-maximization step, i.e. choosing the largest magnitude elements ofΦ⁎rk−1, which becomes more prone to errors when the nonzero elements of the underlying sparse signals span a narrower range [23].Next, we simulate recovery of sparse signals from noisy observationsy=Φx+n, which are obtained by contamination of white Gaussian noise component n at SNR values varying from 5 to 40 dB. Based on our conclusions from above, FBP is run with bothα=20,β=19andα=20,β=17, which correspond toα=0.2M,β=α−1andα=0.2M,β=0.85α, respectively. ε is selected with respect to the noise level, such that the remaining residual power is equal to the noise power. The simulation is repeated for 500 Gaussian and 500 uniform sparse signals, whereN=256andM=100. The sparsity levels are selected asK=30andK=25for the Gaussian and uniform sparse signals, respectively.Kmaxis 55 as in the first set of simulations. Fig. 7depicts the recovery error for the noisy Gaussian and uniform sparse signals, while the run times are compared in Fig. 8. Note that we express the recovery error in the decibel (dB) scale, calling it the distortion ratio, in order to make it better comparable with SNR. Clearly, FBP yields the most accurate recovery for both β values, while BP can do slightly better than FBP only when SNR is 5 dB.44Note that all algorithms almost completely fail at these very low SNR values.In addition, we observe that reducing β does not significantly change the recovery performance. The run times reveal that FBP is not only the most accurate algorithm in this example, but is also as fast as OMP withα=20andβ=17. As above, this result also supports that β can be reduced for speeding up the recovery process without a significant decrement in the recovery accuracy.In order to evaluate the FBP recovery performance in a more realistic case, we demonstrate recovery of the512×512image “bridge”. The recovery is performed using8×8blocks. The aim for such processing is breaking the recovery problem into a number of smaller, and hence simpler, problems. The image “bridge” is first preprocessed such that each8×8block is K-sparse in the 2D Haar Wavelet basis, Ψ, whereK=12, i.e. for each block only theK=12largest magnitude wavelet coefficients are kept. Note that, in this case the signal is not itself sparse, but has a sparse representation in a basis Ψ. Hence, the reconstruction dictionary becomes the holographic basisV=ΦΨ. From each block,M=32observations are taken, where the entries of Φ are randomly drawn from the Gaussian distribution with mean 0 and standard deviation1/N. The parameters are selected asKmax=20andε=10−6. Two sets of FBP parameters are tested,α=10,β=7andα=10,β=9. These selections correspond toα=0.3M,β=α−1andα=0.3M,β=0.7α.55Since M is small in this case, there is no significant run time difference between choosingα=0.2Mandα=0.3M. Therefore, we demonstrate FBP withα=0.3M. Note that the recovery PSNR that can be obtained withα=0.2Mis about 31.5 dB, which is also better than the PSNR value BP yields.Fig. 9shows the preprocessed image “bridge” on the upper left. On the upper right is the BP recovery. FBP recovery withα=10,β=7can be found on lower left, and FBP recovery withα=10,β=9is next to it. In this example, BP provides a Peak Signal-to-Noise Ratio (PSNR) value of 29.9 dB, while the much simpler FBP improves the recovery PSNR up to 32.5 dB. A careful investigation of the recovered images shows that FBP is able to improve the recovery at detailed regions and boundaries. This example demonstrates that the simpler FBP algorithm is able to perform more accurate and faster recovery of a signal with realistic nonzero coefficient distribution than the much more sophisticatedℓ1norm minimization approach.

@&#CONCLUSIONS@&#
