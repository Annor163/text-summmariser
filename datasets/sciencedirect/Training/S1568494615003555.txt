@&#MAIN-TITLE@&#
GSETSK: a generic self-evolving TSK fuzzy neural network with a novel Hebbian-based rule reduction approach

@&#HIGHLIGHTS@&#
This paper attempts to achieve a compact, recent, and interpretable fuzzy rule bases.It proposes a novel rule pruning method that is simple and computationally efficient.It proposes a merging approach to improve the interpretability of the knowledge base.GSETSK adopts an online data-driven incremental-learning-based approach.GSETSK derives an up-to-date and interpretable rule base with high level of accuracy.

@&#KEYPHRASES@&#
Self-evolving,Fuzzy neural network,Neuro-fuzzy systems,

@&#ABSTRACT@&#
Takagi–Sugeno–Kang (TSK) fuzzy systems have been widely applied for solving function approximation and regression-centric problems. Existing dynamic TSK models proposed in the literature can be broadly classified into two classes. Class I TSK models are essentially fuzzy systems that are limited to time-invariant environments. Class II TSK models are generally evolving systems that can learn in time-variant environments. This paper attempts to address the issues of achieving compact, up-to-date fuzzy rule bases and interpretable knowledge bases in TSK models. It proposes a novel rule pruning method which is simple, computationally efficient and biologically plausible. This rule pruning algorithm applies a gradual forgetting approach and adopts the Hebbian learning mechanism behind the long-term potentiation phenomenon in the brain. It also proposes a merging approach which is used to improve the interpretability of the knowledge bases. This approach can prevent derived fuzzy sets from expanding too many times to protect their semantic meanings. These two approaches are incorporated into a generic self-evolving Takagi–Sugeno–Kang fuzzy framework (GSETSK) which adopts an online data-driven incremental-learning-based approach.Extensive experiments were conducted to evaluate the performance of the proposed GSETSK against other established evolving TSK systems. GSETSK has also been tested on real world dataset using the high-way traffic flow density and Dow Jones index time series. The results are encouraging. GSETSK demonstrates its fast learning ability in time-variant environments. In addition, GSETSK derives an up-to-date and better interpretable fuzzy rule base while maintaining a high level of modeling accuracy at the same time.

@&#INTRODUCTION@&#
Neuro-fuzzy computing is a popular framework for solving problems with a soft computing approach due to its capability to combine the human-like reasoning style of fuzzy systems with the connectionist structure and learning ability of neural networks [1]. Neuro-fuzzy hybridization is also widely known as fuzzy neural networks (FNN) or neuro-fuzzy systems (NFS). The main strength of the neuro-fuzzy approach is that it can provide insights to the user about the symbolic knowledge embedded within the network [1]. More specifically, the hybrid network can generalize from training data, learn/tune system parameters, and generate the fuzzy rules to create a linguistic model of the problem domain. Extensive reviews of FNNs are discussed in fuzzy rule learning [1] and neuro-fuzzy works [2–5]. There are two types of fuzzy model; namely the Mamdani model [3] and the Takagi–Sugeno–Kang (TSK) model [6–8].The main advantage of the TSK model over the Mamdani model is its ability to achieve higher system modeling accuracy. Hence, there is a continuing trend of using TSK fuzzy models for solving function approximation and regression-centric problems. In practice, these problems are online, meaning that the data are not all available prior to training but are sequentially presented to the learning system. Many dynamic systems such as SONFIN [9], FITSK [10], DENFIS [11], FLEXFIS [12], eTS [13], and others [14–21] have been developed to provide solutions for such online problems.Existing dynamic TSK models proposed in the literature can be broadly classified into two classes. Class I TSK models are essentially fuzzy systems that are generally limited to time-invariant environments. In real life, time-variant problems, which most often occurred in many areas of engineering, usually possess non-stationary, temporal data streams which are modified continuously by underlying data-generating processes. Dynamic approaches such as FITSK [10] and DENFIS [11] belong to this class. DENFIS [11] and FLEXFIS [12] implicitly assume prior knowledge of the upper bound and lower bound of the data set to normalize data before learning [22], which is unsuitable for time-variant environments. Kukolj and Levi [16] proposed a heuristic self-organizing network which is based on k-means for structure learning. Quah and Quek [10] proposed a simple process that presupposes an even space partitioning of the linguistic labels for fast learning. However both [10] and [16] require the number of clusters or rules to be specified prior to training, which is an impossible task in time-variant problems.Class II TSK models are generally evolving systems [13] that can learn in time-variant environments. Many Class II systems such as [9,17–19,22] do not consider the interpretability of the knowledge bases. They generally employ back-propagation or gradient descent algorithms to heuristically tune the widths of their antecedent membership functions, which can result in highly overlapping and indistinguishable fuzzy sets. Thus, the semantic meaning of the derived fuzzy sets is deteriorated. Besides, many systems such as SONFIN [9], FLEXFIS [12], eTS [13], and [15] do not have a rule pruning algorithm, which may lead to the collection of obsolete rules over time and thus degrade the level of human interpretability of the resultant fuzzy rule base.Some approaches have been proposed to address these issues. In [23,24], a merging approach using the Jaccard index as a similarity measure to merge strongly overlapping fuzzy sets is employed. However this method does not prevent the fuzzy sets from growing overly large, and thus the fuzzy labels of the fuzzy sets represent may become obscure and poorly defined. In Simpl_eTS [20], a simple rule pruning algorithm which monitors the population of each rule is proposed. If a rule amounts to less than 1% of the total data samples at that current moment, it is considered as obsolete, and it will be pruned. This approach considers the contribution of old data and new data equally in determining the obsolete rules, thus it cannot detect drifts and shifts in online data streams [25]. In systems such as eTS+ [14] and xTS [21], the age of a cluster is used to determine if a rule (cluster) is obsolete. However, the age of the cluster in [14,21] is determined by a self-driven formula which does not incorporate the membership degrees of the samples forming that cluster. In [25], the authors proposed an enhanced version of the formula to calculate the age of a cluster in [14,21] by incorporating the membership degrees of the samples. This approach is reasonable, but not biologically plausible. In this paper, an alternative rule pruning method which is simple, computationally efficient and biologically plausible is proposed. This rule pruning algorithm applies a gradual forgetting approach and adopts the Hebbian learning mechanism behind the long-term potentiation phenomenon [26] in the brain. A merging approach which can prevent derived fuzzy sets from expanding too many times to protect their semantic meanings is also proposed.Ensemble methods have been in hot topics in recent years [27–32]. Ensemble is a learning paradigm where multiple networks are jointly used to solve a problem. In general, an ensemble consist of multiple individually trained networks that are used to predict a certain problem, and each individual predicted results are then combined using either the average or weighted average of the results. It has been shown that ensemble methods does provide a better accuracy result as compared to an individual predicting network [30]. However, to require multiple networks to be trained requires a longer duration, hence it is not recommended in time critical applications, and interpretability of the result is impossible. Multiple networks each have its own rule base hence an average or weighted average does not summarize a meaningful rule base for interpretation.This paper focuses on solving the issues of achieving compact, up-to-date fuzzy rule bases and interpretable knowledge bases in TSK models. Thus, this paper presents an evolving TSK neural-fuzzy framework which can work in time-variant environments to address these issues. The framework is termed the generic self-evolving Takagi–Sugeno–Kang fuzzy framework (GSETSK). GSETSK starts with an initial empty rule base. New rules are sequentially added to the rule base by a fuzzy clustering algorithm called as multidimensional-scaling growing clustering which is completely data-driven. Highly overlapping membership functions are merged by a novel merging approach and obsolete rules are constantly pruned by a Hebbian-based rule pruning algorithm to derive a compact fuzzy rule base while maintaining a high level of modeling accuracy. GSETSK does not require prior knowledge of the numbers of clusters or rules present in the training data set. For parameter tuning, GSETSK employs a localized version of the recursive least-square algorithm [33] for high-accuracy online learning performance.The paper is organized as follows. Section 2 briefly discusses the general structure of the GSETSK and its neural computations. Section 3 presents the novel structural learning phase of GSETSK. Section 4 presents its parameter learning phase. Section 5 evaluates the performance of the GSETSK model using five different simulations and benchmark its performances against other established neural fuzzy systems. Section 6 concludes the paper.This section introduces the structure and functions of GSETSK. The GSETSK model is basically an FNN [3] that consists of six layers of computing nodes as shown in Fig. 1. They are: Layer I (the input layer), Layer II (the input linguistic layer), Layer III (the rule layer), Layer IV (the normalization layer), Layer V (the consequent layer) and Layer VI (the output layer). From Fig. 1, the structure of the proposed GSETSK model defines a set of TSK-type IF-THEN fuzzy rules. The fuzzy rules are incrementally constructed by presenting the training observations …(X(T), d(t))… sequentially (one-by-one), where X(t) and d(t) denote the vectors containing the inputs and the corresponding desired outputs, respectively, at any time t. Each fuzzy rule Rkin GSETSK has the form as shown in (1)(1)Rk:IFx1isIL1,j1kAND…xiisILi,jikAND…xnisILn,jnkTHENyk=b0k+b1kx1+⋯+bikxi+⋯+bnkxn,where X=[x1, …, xi, …, xn]Trepresents the numeric inputs of GSETSK;ILi,jik(ji=1,…,Ji(t),k=1,…,K(t))denotes the jith linguistic label of the input xithat is part of the antecedent of rule Rk; Ji(t) is the number of fuzzy sets of xi; K(t) is the number of fuzzy rules at time t; ykis the crisp output of rule Rk; n is the number of inputs; [b0k, …, bnk] represents a set of consequent parameters of rule Rk.For simplicity, the proposed GSETSK network is modeled as a multiple input single output (MISO) network, as a multiple input multiple output (MIMO) network is fundamentally an aggregation of MISOs. For clarity of subsequent discussion, the output of a node in Fig. 1 is denoted as Z with the superscripts denoting its layer and the subscripts denoting its origin; for exampleZiIis the output of the ith node in layer I. All the outputs of a layer are propagated to the inputs of the connecting nodes at the next layer.Each input node Iimay connect to a different number of input linguistic nodes Ji(t). Hence the total number of nodes in layer II at each time t is∑i=1nJi(t). Also, at each time t, layer III consists of K(t) rule nodes Rk. It should be noted that K(t) and Ji(t) change over time, increasing to accommodate new data or decreasing to keep a compact fuzzy rule base. Each rule node Rkis directly connected to a normalization node Nkin layer IV. Subsequently, each normalization node Nkis directly connected to a consequent node Ckin layer V. Hence, the numbers of nodes in layer III, layer IV and layer V are the same. For clarity of subsequent discussion, the variables i and j are used to refer to arbitrary nodes in layers I and II, and the variable k for layers III, IV, and V respectively. The output node at layer VI is a summation node which connects to all nodes in layer V.Detailed mathematic functions of each layer of GSETSK are presented in the following sections.Layer I: Input layer(2)ZiI=xi,i=1,…,n.Layer I nodes are termed as linguistic nodes. Each node receives only one input as one dimension of the vectored data input, and outputs to several nodes of the next layer. In this paper, singleton input is employed.Layer II: Input linguistic layer(3)Zi,jiII=μi,j(ZiI)=μi,ji(xi),i=1,…,n,ji=1,…,Ji(t),whereμi,jiis a fuzzy membership function of the fuzzy linguistic nodeILi,ji. Layer II nodes are termed as input-label nodes. They constitute the antecedent of the fuzzy rules in GSETSK. The labelILi,jidenotes the jth linguistic label of the ith linguistic variable input. The input linguistic layer measures the matching degree of each input with its corresponding linguistic nodes. Each linguistic node in this layer has a Gaussian membership function with its center and width dynamically computed during the structural learning phase. With the use of Gaussian membership function, [3] can be expressed as in(4)Zi,jiII=μi,j(xi)=e−xi−mi,ji2σi,ji2,i=1,…,n,ji=1,…,Ji(t),wheremi,jiandσi,jiare, respectively, the center and the width of the Gaussian membership function of the jth linguistic label of the ith linguistic variable input xi.Layer III: Rule layerEach node in rule-base layer represents a single Sugeno-type fuzzy rule and is termed as a rule node. The net output or the firing strength of a rule node Rkis computed based on the activation of its antecedents as in(5)rk=ZkIII=minZ1,j1kII,…,Zi,jikII,…,Zn,jnkII,k=1,…,K(t),whereZi,jikIIis the output of the jth linguistic label of the ith linguistic variable input xithat connects to the kth rule; rkis the forward firing strength of Rk.Layer IV: Normalization layer(6)wk=ZkIV=ZkIII∑k=1K(t)ZkIII,k=1,…,K(t),wherewkis the normalized firing strength.Layer V: Consequence layer(7)ZkV=ZkIVfk(X),k=1,…,K(t),where fk(X) is a linear function of consequent node Ck.Layer VI: Summation layer(8)ZVI=∑k=1K(t)ZkV,whereZkVis the output of consequent node Ckin layer V. The output node in this layer corresponds to the output of the GSETSK model.Although the GSETSK appears structurally similar to other evolving networks such as SONFIN [9], FLEXFIS [12], and eTS [13], there are distinct differences between them. SONFIN uses back-propagation to tune its membership functions, which can result in highly overlapping and indistinguishable membership functions. The number of membership functions and fuzzy rules in FLEXFIS and eTS will grow monotonically, especially when solving time-variant problems. Ouyang et al. [15] proposed a merge-based fuzzy clustering algorithm to merge highly similar clusters. However, this algorithm does not prune irrelevant rules, which results in a continuously growing fuzzy rule base over time. In contrast, the GSETSK employs a Hebbian-based rule pruning algorithm which takes into consideration the backward connections from layer VI to layer III via layer V as presented in Sections 2.2 and 2.3. This novel rule pruning algorithm ensures a compact and up-to-date fuzzy rule base in the GSETSK network.The backward connections from layer VI to layer III via layer V in the GSETSK solely serve for the purpose of computing the potentials of the fuzzy rules in GSETSK. These fuzzy rule potentials will subsequently be used to determine if the rules will be pruned or kept. Inspired by the learning algorithm in POPFNN [34], the GSETSK adopts the Hebbian learning mechanism to compute its fuzzy rule potentials. However, POPFNN and its family of networks [35–37] are of Mamdani-type fuzzy neural networks in which the output of each fuzzy rule is a set of fuzzy linguistic labels. The Hebbian learning algorithm employed in POPFNN is based on the firing strengths of the rules nodes (forward firing) and the membership values derived at the output-label nodes (backward firing).In contrast, the GSETSK model adopts the TSK fuzzy model and the output of each rule in GSETSK has the form of a linear function of the input vector. Hence, a novel approach to compute the fuzzy rule potentials based on the observed training data pair (X(t), d(t)) is proposed in GSETSK. At each rule node Rk, the forward firing strength rkhas been described in (5), and the backward firing strengthrkbackis computed in two steps as follows.1.Computing output error of each fuzzy rule:Layer V (Backward operation): At time t, the desired output d(t) is directly transmitted to each consequent node Ckin layer V. The output of the linear function of the consequent node Ckin response to the input X(t) is a crisp value given in (9).(9)yk(t)=b0k(t)+b1k(t)x1(t)+⋯+bik(t)xi(t)+⋯+bnk(t)xn(t),k=1,…,K(t),where [b0k(t), …, bnk(t)] represents a set of consequent parameters of rule Rkat time t. Note that ykis the output of the fuzzy rule Rk. It is different fromZkV, which is the output of the consequent node Ck. For each rule Rk, the difference between the computed output yk(t) and the desired output d(t) is computed in(10)ek(t)=|d(t)−yk(t)|,k=1,…,K(t),where ek(t) is the output error of rule Rkat time t.Determining backward firing strength of each fuzzy rule:Layer V (Backward operation): The values e1(t), …, ek(t), …, eK(t)(t) will then be used to form a Gaussian membership function with the mean of 0 and the width at time t formulated in(11)σback(t)=∑k=1K(t)ek(t)K(t)π.This membership function measures how closely the computed output yk(t) can approximate the desired output d(t). Denote μ(0, σback(t)) as the Gaussian membership function with center 0 and width σback(t). Fig. 2shows such a Gaussian membership function, which can be approximated by an isosceles triangle with unity height and the length of its bottom edge equal to2σback(t)π[38].The backward firing strength ofrkbackof rule Rkat time t is then determined by(12)rkback(t)=μ(0,σback(t),ek(t))=exp−ek(t)2σback(t)2.In Mamdani-type models such as POPFNN, the backward firing strength of a fuzzy rule is defined by how close the desired output is to the centers of the membership functions in the rules output-label nodes. The idea in GSETSK is similar. At layer V, the Gaussian function μ(0, σback(t)) is formulated to measure the degree of closeness between the desired output d(t) and the computed output yk(t). When μ(0, σk(t), ek(t))=1, ek(t)=0, and yk(t)=d(t). The smaller the value of ek(t), the greater the value of μ(0, σk(t), ek(t)). That also means the closer the computed output yk(t) is to the desired output d(t), the greater the backward firing strength of rule Rk. It can be observed from (11) that the width σback(t) is constructed using the average of the errors of all rules at time t. This approach is built on the idea that the existing fuzzy rules in GSETSK at time t should be compared against each other in terms of how well they can approximate the desired output. However, it should be noted that the backward firing strength only forms a part of the formula to calculate the fuzzy rule potentials as presented in Section 2.3.GSETSK is an online model which functions by interleaving reasoning (testing) and learning (training) activities. At any time t, GSETSK carries out the activities as follows:1.It performs structural learning to formulate the fuzzy rules and to learn the membership functions using the input X(t) as presented in Section 3.1.It performs forward reasoning to approximately infer the output y(t+1) based on the input X(t) and its knowledge of the past.It performs tuning of the network parameters using the recursive least square algorithm as presented in Section 4.It performs backward computing to update its fuzzy rules potentials to keep an up-to-date knowledge base by pruning outdated rules.GSETSK relies on fuzzy rules potentials in its rule pruning algorithm to delete obsolete fuzzy rules that can no longer describe the current observed data characteristics. The potential Pkof a fuzzy rule Rkin GSETSK indicates its importance or influence in the entire rule base of the system. At any time t, the potential Pkof a fuzzy rule Rkcan be recursively calculated based on the current training data (X(t), d(t)) as shown in (13).(13)Pk(t)=Pk(t−1)+rk(X(t))×rkback(d(t)),k=1,…,K(t),having Pk(t−1) as the potential of rule Rkat time (t−1); rk(X(t)) is the forward firing strength of rule Rkas given in (5); andrkback(d(t))is the backward firing strength of rule Rkas given in (12).Eq. (13) indicates that the importance of a fuzzy rule Rkin GSETSK is reinforced if its input antecedents and computed output can closely mimic the information expressed in the training pair (X(t), d(t)). This totally complies with the Hebbian learning mechanism behind the long-term potentiation phenomenon [26] in the brain. The mechanism is based on the Hebb theory which states that the synaptic connections of the associative memories formed in the brain are strengthened when the coincident pre-synaptic and post-synaptic activities occur.To account for complex time-variant data sets, GSETSK needs to separate its new learning from its old learning to avoid catastrophic forgetting [39]. More specifically, GSETSK needs to decay the effects of its old learning as new data pairs become available. This is achieved by a forgetting mechanism that gradually removes the outdated rules from GSETSK. This helps to maintain a set of up-to-date fuzzy rules that best describes the current characteristics of the incoming data. Furthermore, the rule base will be more compact and can be better interpreted by human experts. This is done by adding a forgetting factor to the original formulation described in (13) and is now given below(14)Pk(t)=λPk(t−1)+rk(X(t))×rkback(d(t)),λ∈(0,1],k=1,…,K(t),where λ is the forgetting factor. The smaller λ is, the faster the effects of old learning decay. The rule Rkwill be pruned if falls below the predefined parameter thresP. The details of the rule pruning algorithm in GSETSK will be presented in Section 3.3.As mentioned, at each arrival of data observations (X(t), d(t)), GSETSK performs its learning process which consists of two phases, namely structural and parameter learning. This section describes the structural learning phase of GSETSK.GSETSK proposes a clustering technique known as multidimensional-scaling growing clustering (MSGC), initially proposed by the author in [40], to partition the input space from the training data to formulate its fuzzy rules. The multidimensional scaling approach of MSGC has been inspired by the human cognitive process model described in [41]. The multidimensional scaling generates a spatial representation of a stimulus domain which facilitates the operation of the fundamental cognitive process within the domain. Initially there is no rule in the rule base of the GSETSK network. New rules are sequentially added to the rule base if the existing rules are not sufficient to describe the new data. Highly overlapping membership functions will be merged and obsolete rules will be constantly pruned based on their fuzzy potentials.Fig. 3shows the flow chart of the proposed GSETSK model's learning process. Details of individual parts of the learning process will be discussed in the following sections.The MSGC has the following advantages: (1) it does not require the number of cluster/fuzzy rules to be specified prior to training and (2) it can account for the time-varying characteristics of the incoming data. In MSGC, each fuzzy rule is a cluster which is identified in the multidimensional input space. After a cluster is identified, the corresponding 1-D membership function for each input dimension is derived by decomposing the multidimensional cluster. The clustering process is described as follows. Assume the arrival of a new training data pair (X(t), d(t)), where X(t)=[x1(t), …, xi(t), …, xn(t)]T. Initially, there is no cluster identified, i.e. K(t)=0. If (X(t), d(t)) is the first incoming training observation (i.e. t=1), MSGC immediately creates a new cluster and projects the newly created cluster to the 1-D inputs to form the Gaussian membership function as described by (15) and (16), shown on the left portion of the flow chart in Fig. 3.(15)mi,Ji(t+1)=xi(t),(16)σi,Ji(t+1)=vi,wheremi,Ji(t+1)andσi,Ji(t+1)are the center and width of the input labelILi,Ji(t+1),Ji(t+1)=1, respectively, andviis a predefined constant which can be set to some arbitrary values or based on a users prior knowledge. A new cluster corresponds to a new rule node in layer III.For the next training observations, MSGC will determine whether a new rule should be created to cover the new data or not, based on the rule firing strengths as computed using (5), shown on the center portion of the flow chart in Fig. 3. At time t, MSGC performs a partial activation of the GSETSK network via the forward connections of layers I–III to derive the firing strengths rk(X(t)) where k=1, …, K(t). The maximum firing strength is then determined by using (17).(17)ρ=argmax1≤k≤K(t)rk(X(t)),where ρ indicates that the ρth rule achieves the maximum firing strength among all existing fuzzy rules in the rule base.A new rule is created if rρ(X(t))<θ, where θ∈(0, 1) is a predefined threshold. θ controls the number of rules created. The higher the value of θ, the more rules are being created. In order to achieve a balance between having highly distinguishable clusters (rules) and using a sufficient number of rules, θ is predefined at 0.4. After a rule (cluster) is created, the corresponding 1-D Gaussian membership function for each input dimension is formulated. The center of the new membership function in the ith dimension is set using (15). However, to determine the width of the new membership function in the ith dimension,σi,Ji(t+1), an extra step is taken as follows. Denote ϵ as the ϵth input label in the ith dimension that has the largest matching degree with xi(t). ϵ can be found using(18)ϵ=argmax1≤j≤Ji(t)e−(xi(t)−mi,j)2σi,j2.The width of the new membership function in the ith dimension,σi,Ji(t+1)can be determined by(19)σi,Ji(t+1)=η|xi(t)−mi,ϵ|,where mi,ϵis the center of the membership function that is nearest to xi(t), and η≥0 is a predefined constant that determines the degree of overlap between two arbitrary membership functions. It can be observed that the widthσi,Ji(t+1)is directly proportional to the distance between xi(t) and the center of the nearest fuzzy set. The greater η, the bigger the width of a newly created fuzzy set. η is set at 0.5 in all experiments in this paper. The widths of the 1-D membership functions will not be tuned during the parameter learning phase of the GSETSK, therefore they are carefully set using (19) to make sure the membership functions are sufficient to cover the entire input space. Any highly overlapping fuzzy sets will be merged as presented in Section 3.2. It should be noted that the min operation in (5) can ensure that, for any rule Rk, when the matching degreeZi,jikIIin any arbitrary ith input dimension is small, the firing strength will be small. This subsequently leads to the weakening of the fuzzy rule Rk's potential, which is computed using (14). As a result, Rkcan potentially be pruned and replaced by a new fuzzy rule which has new membership functions that represent the current data better. This dynamic mechanism ensures highly distinguishable fuzzy sets that can well-represent data with time-varying characteristics in GSETSK.The MSGC technique employed in GSETSK is sufficient to maintain a consistent and compact rule base by performing the procedure CheckRuleBase which consists of two steps, namely CheckSimilarity and MergeMembership, as shown in the bottom part of the flow chart in Fig. 3. Denoteμ(mi,Ji(t+1),σi,Ji(t+1))as the new membership function in the ith dimension. Afterμ(mi,Ji(t+1),σi,Ji(t+1))is created using (15) and (19), the step CheckSimilarity is carried out to measure the similarity betweenμ(mi,Ji(t+1),σi,Ji(t+1))and its nearest membership function μ(mi,ϵ, σi,ϵ).To determine the similarity measure of two Gaussian fuzzy sets, a fuzzy subset-hood measure [42] is computed. The fuzzy subset-hood measure which defines the degree that fuzzy set A is a subset of fuzzy set B can be approximated by (20)[10].(20)S(A,B)=maxx∈U(min(μA(x),μB(x)))maxx∈U(μA(x)).At time t, the procedure CheckRuleBase is performed as follows:Algorithm 1procedureCheckRuleBaseBegin Perform CheckSimilarity to determineS(μ(mi,Ji(t+1),σi,Ji(t+1)),μ(mi,ϵ,σi,ϵ)), which is the similarity between the newly created fuzzy set and its nearest membership function μ(mi,ϵ, σi,ϵ).ifS(μ(mi,Ji(t+1),σi,Ji(t+1)),μ(mi,ϵ,σi,ϵ))>thresAReplace the newly created membership function with the ϵth one; set Ji(t+1)=Ji(t)else ifS(μ(mi,Ji(t+1),σi,Ji(t+1)),μ(mi,ϵ,σi,ϵ))>thresBMergeMembership; set Ji(t+1)=Ji(t)elseAccept the newly created membership function; set Ji(t+1)=Ji(t)+1end ifend ProcedureIn the above procedure, thresA and thresB(thresA>thresB) are two predefined similarity thresholds to determine three actions as illustrated in Fig. 4. These two thresholds determine the number of membership functions created. The higher the value of thresA and thresB, the more membership functions are created. However, thresA is preset to 0.8 in all experiments in GSETSK, which has the semantic meaning that if the matching degree between the new membership function and the ϵth membership function is over 80% then the new membership function should be replaced by the ϵth one. Similarly, thresB is preset to 0.7.The MergeMembership step in the CheckRuleBase procedure is to merge two highly overlapping membership functions into a Gaussian function with a larger width. However, to maintain the meaning of a membership function and to prevent a membership function from expanding too many times, a willingness parameter (WP) is employed. WP indicates the willingness of a membership function to expand/merge with another membership function. WP decreases each time the membership function performs an expansion. At time t, a membership function will not be allowed to merge if its WP(t)<0. The parameter WP maintains the semantic meaning of a fuzzy set by preventing its width from growing overly large. For the ϵth fuzzy set, its WP at time t is determined as follows.(21)WP(t)=WP(tu)−(1.5−WP(tu))︸always≥1×1−Sμmi,Ji(t+1),σi,Ji(t+1),μ(mi,ϵ,σi,ϵ)︸always≥0,WP(0)=0.5,where tuindicates the last time when the ϵth fuzzy set expands.The initial value of WP is set to 0.5 to make sure WP always decreases. The smaller the similarity measure betweenμmi,Ji(t+1),σi,Ji(t+1)and μ(mi,ϵ, σi,ϵ) in (21) (meaning the harder it is for the two membership functions to merge), the faster the WP of the ϵth fuzzy set decreases. Note that the ϵth fuzzy set only expands whenSμmi,Ji(t+1),σi,Ji(t+1)∈(thresB,thresA]. Fig. 5illustrates how WP behaves.Consider that a Gaussian membership function can be approximated by an isosceles triangle with unity height and the length of its bottom edge equal to2σπ[38]; the width and center of the new membership function after merging two arbitrary membership functions (m1, σ1) and (m2, σ2), (m1>m2) are determined by(22)σnew=m1−m2+(σ1+σ2)π2π,(23)mnew=m1+m2+(σ1+σ2)π2.Merging two membership functions will create a new one with a larger width which can cover a larger region. This leads to fewer fuzzy sets in each dimension. In addition, the fuzzy sets are highly distinguishable. The MSGC clustering technique helps to ensure a consistent and compact knowledge base in the GSETSK network.As stated in Section 2.2, the rule pruning process in the GSETSK is to remove obsolete fuzzy rules that no longer can model the current data characteristics, and to maintain a compact and current rule base. This can improve the level of human interpretability of the resultant fuzzy rule base. The computed fuzzy rules potentials Pk, k=1, …, K(t) as described in (14) are employed to determine which rules will be pruned. At time t, the rule Rkwill be pruned if Pk(t)<thresP, where thresP is a predefined parameter. The greater thresP is, the more obsolete rules in GSETSK will be pruned. thresP is preset to 0.5 in all experiments in GSETSK. It should be noted that the potential of a newly created rule is defined as unity. The semantic meaning of setting thresP to 0.5 is that if a rule loses half of its initial potential, it should be pruned. Parameters such as thresA, thresB, and thresP can be set to the constants specified above in any experiment, as they only serve to provide the semantic meanings for these constants.After a set of obsolete rules are pruned, the number of rules K(t+1) will be updated accordingly. The rule pruning process may result in obsolete fuzzy label(s) which are not connected to any rule node(s). Therefore, GSETSK will scan through each ith input dimension to remove any obsolete label and update Ji(t) accordingly.In this phase, only the consequent parameters in the consequent nodes at layer V will be tuned. It is shown in Fig. 3 being the last phase of learning before getting another new training tuple into the network. In GSETSK, the output node at layer VI based on the observed data pair (X, D) is shown in(24)y=∑k=1K(t)ZkV=∑k=1K(t)wkfk(X)=∑k=1K(t)wk[b0k+b1kx1+⋯+bikxi+⋯+bnkxn],where Bk=[b0k, …, bik, …, bnk]Tis the parameter vector of the consequent node Ck;wkis the normalized firing strength at the normalization node Nk.Assuming that the GSETSK network models a system with T number of training samples (X(1), d(1)), …, (X(t), d(t)), …, (X(T), d(T)), GSETSK adapts a localized version of the recursive linear-least-squares (RLS) algorithm [33] as presented in [10] to reduce the space complexity and the computation cost as well as to enhance the training speed. Assuming a rule Rkstays in the fuzzy rule base after T number of training samples, and assuming that the GSETSK has only two inputs x1 and x2, a local approximation that can represent the input–output relationships at the consequent node Ckis shown in(25)wk(1)wk(1)x1(1)wk(1)x2(1)⋮⋮⋮wk(t)wk(t)x1(t)wk(t)x2(t)⋮⋮⋮wk(T)wk(T)x1(T)wk(T)x2(T)×b0kb1kb2k=wk(1)d(1)⋮wk(t)d(t)⋮wk(T)d(T)Eq. (25) can be represented in the form of(26)A×B=D.Denote apas the pth row of the matrix A. Using RLS, B can be iteratively estimated as(27)B(p+1)=Bp+C(p+1)a(p+1)T(d(p+1)−a(p+1)Bp)C(p+1)=1λCp−Cpa(p+1)Ta(p+1)Cpλ+a(p+1)Cpa(p+1)Twith initial condition C0=θI where θ is a large positive number and I is the identity matrix of dimension β×β, where β is the number of consequent parameters of one rule, λ is the forgetting factor. The localized version of RLS algorithm empowers the GSETSK with fast training ability [10]. For each newly created rule RK(t+1), its parameters are determined by the weighted average of the parameters of the other rules [13]. The weights are the normalized firing strengths of the existing rules. More specifically, the parameters for the rule RK(t+1) are initialized as in(28)bi,K(t+1)=∑k=1K(t)wkbi,k,i=1,…,n,where [b0k, …, bik, …, bnk]Tis the parameter vector of the rule Rkandwkis the normalized firing strength of the rule Rk. Extensive experiments were conducted to evaluate the performance of the proposed GSETSK against other established neural fuzzy systems. The results are presented in the next section.

@&#CONCLUSIONS@&#
