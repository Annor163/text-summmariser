@&#MAIN-TITLE@&#
Employing distance-based semantics to interpret spoken referring expressions

@&#HIGHLIGHTS@&#
An interpretation process that considers multiple alternatives.A mechanism for combining uncertainty from a variety of sources.Distance functions with probabilistic semantics that represent similarity measures.Two evaluation experiments to assess our system's performance.

@&#KEYPHRASES@&#
Spoken language understanding,Numerical approach,Semantic interpretation,Distance-based semantics,Performance evaluation,

@&#ABSTRACT@&#
In this paper, we present Scusi?, an anytime numerical mechanism for the interpretation of spoken referring expressions. Our contributions are: (1) an anytime interpretation process that considers multiple alternatives at different interpretation stages (speech, syntax, semantics and pragmatics), which enables Scusi? to defer decisions to the end of the interpretation process; (2) a mechanism that combines scores associated with the output of the different interpretation stages, taking into account the uncertainty arising from a variety of sources, such as ambiguity or inaccuracy in a description, speech recognition errors and out-of-vocabulary terms; and (3) distance-based functions with probabilistic semantics that represent lexical similarity between objects’ names and similarity between stated requirements and physical properties of objects (viz colour, size and positional relations). We considered two approaches for combining these descriptive attributes, viz multiplicative and additive, and determined whether prioritizing certain interpretation stages and descriptive attributes affects interpretation performance. We conducted two experiments to evaluate different aspects of Scusi?'s performance: Interpretive, where we compared Scusi?'s understanding of descriptions that are mainly ambiguous or inaccurate with people's understanding of these descriptions, and Generative, where we assessed Scusi?'s understanding of naturally occurring spoken descriptions. Our results show that Scusi?'s understanding of the descriptions in the Interpretive trial is comparable to that of people; and that its performance is encouraging when given arbitrary spoken descriptions in diverse scenarios, and excellent for the corresponding written descriptions. In both experiments, Scusi? significantly outperformed a baseline system that maintains only top same-score interpretations.

@&#INTRODUCTION@&#
People often express themselves ambiguously or inaccurately (Trafton et al., 2005; Moratz and Tenbrink, 2006; Funakoshi et al., 2012). An ambiguous reference to an object matches several objects well, while an inaccurate reference matches one or more objects partially. For instance, in a household domain, a reference to a “big blue mug” is ambiguous if there is more than one big blue mug in the room, and inaccurate if there are two mugs in the room, one big and red, and one small and blue. In addition, ambiguous or inaccurate references may result from different parse trees (e.g., due to variants in prepositional attachments), or from misheard utterances in spoken interactions. Computer systems that interact with people in natural language must be able to cope with such issues. This does not mean that a system must always obtain an intended interpretation of an utterance (although that would be nice), but when it misunderstands an utterance, the misunderstanding should be plausible.Like Funakoshi et al. (2012), Lison and Kruijff (2009) and Ross (2010), we posit that simply considering the best interpretation of an utterance would not address these problems. In fact, our approach is part of a growing trend which harnesses numerical formalisms to consider multiple interpretations in order to handle the uncertainty inherent in real-world problems, e.g., (Funakoshi et al., 2012; Lison and Kruijff, 2009; Ross, 2010). However, we defer decisions to the end of the interpretation process, which like Punyakanok et al.'s (2008) approach, allows us to make global, rather than local, decisions. The score associated with an interpretation typically represents its goodness, which in turn enables the ranking of candidate interpretations. Additionally, scores may be used, in combination with utilities, to determine a course of action, e.g., ask a clarification question, or perform a requested action. Considering multiple interpretations, coupled with numerical scores, enables a system to recover from situations where the highest-scoring interpretation is not the intended one (e.g., due to speech recognition errors), or detect situations where several interpretations are similarly plausible.In this paper, we present a numerical mechanism for the interpretation of spoken referring expressions which considers multiple interpretations at different levels (i.e., speech, syntax, semantics and pragmatics), and incorporates the physical reality of the context at the pragmatics level of the interpretation process. The disparate processes carried out at each level of interpretation require our mechanism to combine scores and probabilities obtained from several sources. Specifically, our mechanism combines scores returned by an Automatic Speech Recognizer (ASR), probabilities estimated by a probabilistic parser, scores that estimate the complexity of an interpretation, and scores that represent how well the properties of candidate objects (viz lexical item, colour, size and location) match a user's requirements (Section 4). The scores may be viewed as subjective probabilities, which represent one's state of certainty regarding the truth of a proposition (Pearl, 1988).Our mechanism produces a ranked list of interpretations at each of the above levels, culminating in instantiated objects in a specific context. For example, consider the description “the large blue mug” uttered in a room which contains a large aqua mug, a small blue mug and a large red mug, denoted mug0, mug1 and mug2 respectively, among other objects. Our mechanism yields alternative interpretations where in principle the referent may be each of the objects in the room, and each interpretation is associated with a score. Interpretations comprising mug0, mug1 or mug2 as referents have a higher score than interpretations comprising other objects (which have a low score); and interpretations where the referent is a blue object have a higher score than interpretations where the referent has another colour (this helps in situations where the name of a referent has been misheard, but its colour was heard correctly). Since none of the mugs in our example match the description precisely, their ranking depends on how well the given requirements match the actual colour and size of the mugs, and on the relative importance of colour and size. These rankings and their scores support the determination of a course of action by a robotic agent. For instance, if the score of, say, the mug0 interpretation is significantly higher than the scores of the other mugs, the agent can simply retrieve mug0. If the scores of the interpretations involving the three mugs are similar, the agent should ask a clarification question. However, if the description was uttered in a room with no mugs, the scores of all the candidate objects would be low, which may prompt the agent to look for a mug in a different room. The implementation of such a decision process is the next step in this project.Our mechanism was evaluated in two experimental settings: an Interpretive web-based setting to determine how well our system's understanding of given written referring expressions matches people's understanding; and the more common Generative setting, e.g., (Gandrabur et al., 2006; Thomson et al., 2008; DeVault et al., 2009), to assess our system's performance in various scenarios where people give spoken descriptions of designated referents.The contributions of this paper are:•An anytime interpretation process that considers multiple alternatives at different interpretation stages (speech, syntax, semantics and pragmatics), which enables our system to defer decisions to the end of the interpretation process.A mechanism that combines the probabilities and scores associated with the output of the different interpretation stages, taking into account the uncertainty arising from a variety of sources, such as ambiguity or inaccuracy in a description, speech recognition errors and out-of-vocabulary terms (Section 4).Distance-based functions with probabilistic semantics that represent lexical similarity between objects’ names and similarity between stated requirements and physical properties of objects, viz colour, size and topological and projective positional relations (Section 4).Two approaches for combining these distance-based functions, viz multiplicative and additive (Section 4); and experiments that determine whether prioritizing certain interpretation stages and descriptive attributes affects interpretation performance (Section 5.2).Evaluation with two corpora that illustrate a variety of conditions, which, to the best of our knowledge, are more diverse than those examined to date (Moratz and Tenbrink, 2006; Funakoshi et al., 2012; Kelleher and Costello, 2008) (Section 5).This paper is organized as follows. Section 2 presents the interpretation process, and Section 3 details the semantic interpretation procedure. The estimation of the score of an interpretation appears in Section 4. Our evaluation experiments are described in Section 5. Related research and concluding remarks are given in Sections 6 and 7 respectively.Our interpretation mechanism is implemented in a module called Scusi?, which processes spoken input in four stages: speech recognition, parsing, semantic interpretation and pragmatic interpretation (Fig. 1(a)). In the first stage of our interpretation process, Scusi? runs Automatic Speech Recognition (ASR) software (Microsoft Speech SDK 6.1) to generate candidate texts from a speech signal (up to 50 texts are produced). Each text is assigned a score that reflects the certainty of the ASR regarding the words in the text given the speech wave. The second stage applies Charniak's probabilistic parser (http://bllip.cs.brown.edu/resources.shtml#software) to generate parse trees from the texts. The parser generates up to 50 parse trees for each text, associating each parse tree with a probability. During semantic and pragmatic interpretation, parse trees are successively mapped into two representations based on Conceptual Graphs (Sowa et al., 1984): Uninstantiated Concept Graphs (UCGs) and Instantiated Concept Graphs (ICGs). UCGs are obtained from parse trees deterministically – one parse tree generates one UCG (but a UCG may have several parent parse trees), and each UCG can generate many ICGs (an ICG may have several parent UCGs, which are obtained from similar parse trees).A UCG represents syntactic information pertaining to the head nouns in the parent parse tree and their modifiers, and relations between concepts, which are directly derived from syntactic information in the parse tree and prepositions. A UCG also stores semantic information about head-noun modifiers whose meaning is known to Scusi?, viz colour and size.11Other noun modifiers are simply denoted by the label attribute. Although at present these modifiers are not meaningful to Scusi?, the system can often overcome this limitation by taking advantage of redundant information in a description (Section 5). Additional attributes, such as shape and texture, may be incorporated into our framework by adding suitable information to Scusi?'s knowledge base.Fig. 1(b) illustrates the generation of a candidate UCG and a matching ICG for the description “the blue mug on the table near the vase”, where, as shown in the parse tree, the mug is expected to be near the vase (an alternative interpretation would place the table near the vase). The nouns “mug”, “table” and “vase” in the parse tree are mapped to the concepts mug, table and vase in the UCG respectively, with the mug having a colour attribute (represented by coordinates in the CIE94 colour space, Appendix A). The prepositions “on” and “near” in the parse tree are respectively mapped to the relations on and near in the UCG. In order to favour simple interpretations, the score associated with a UCG is the reciprocal of the number of nodes in it.An ICG represents pragmatic information comprising instantiated concepts and relations between these concepts in the current context. Instantiated concepts are objects in the physical context (e.g., a particular room), and instantiated relations correspond to positional relations. The objects and positional relations within an ICG mirror those within its parent UCG(s), but in contrast to the generic names of UCG concepts (e.g., mug), an ICG contains specific objects (e.g., mug3, which is a candidate match for mug, colour: blue in the example in Fig. 1(b)). The score of an ICG reflects how well its objects and relations match the specifications represented in its parent UCG(s) and the context. For instance, the score of the ICG in Fig. 1(b) reflects how well mug3, table2 and vase1 match the concepts mug, table and vase in the UCG respectively (a cup yields a worse match with the concept mug than a mug), how well the colour of mug3 matches the specified colour blue, whether mug3 is on table2, and how close it is to vase1.The generation of ICGs is described in Section 3. In the mean time, it is worth noting that when generating pragmatic interpretations, the first step consists of proposing candidate ICG objects and positional relations for each UCG concept and relation respectively. Scusi? then checks whether a proposed combination of objects and positional relations satisfies the specifications in the UCG and matches the context. So, all the mugs and cups in the room are good candidates for the concept mug (with blue mugs being better candidates), all the tables and desks are good candidates for table, and all the vases for vase. When the relation Location_on is tested for a particular mug–table combination, it will produce a good match only for the mugs that are on tables, and when the relation Location_near is tested for a mug–vase combination, only the mugs that are near vases will be winners. So, if mug3 in Fig. 1(b) is blue, it is a good candidate in terms of lexical match and colour, but if it is not on table2 or near vase1, then the ICG in Fig. 1(b) will receive a low score. In contrast, if mug3 was on table1, the ICG representing this situation would have a better score.The consideration of all possible options at each stage of the interpretation process is computationally intractable. Hence, Scusi? employs an anytime algorithm combined with a thresholding approach to generate interpretations in real time, and a stochastic optimization strategy to avoid getting stuck in local maxima.The anytime algorithm ensures that the system can return a list of ranked interpretations at any point in time (after a start-up phase). At each stage of the interpretation process, the algorithm applies a selection–expansion cycle to add an element to a search graph as follows (Fig. 1(a)). First, it probabilistically selects a level to be expanded (speech, text, parse tree or UCG), and then it selects a node in the search graph at this level. For instance, if the speech level is selected, the next text produced by the ASR (in descending order of score) is returned. If the text level is selected, the algorithm chooses a text node to be expanded, and returns the next parse tree for it. This text node may be one that has produced no children yet (e.g., Text T3 in Fig. 1(a)), or one that has already generated children (e.g., T1). When a node is expanded for the first time, a priority buffer containing at most kmax ranked children is created, but only a single (top-ranked) child from this buffer is incorporated in the search graph. Every time a node is expanded, the next child from its buffer is added to the search graph. For example, when UCG U12 in Fig. 1(a) is expanded for the second time, the ICG-generation module returns the next ICG in the UCG's buffer, i.e., I122 (Section 3). The selection–expansion process is repeated until one of the following happens: all options are fully expanded, a time limit is reached, or a specific number of iterations is performed, which is the option chosen for the experiments described in Section 5. At any point after completing an expansion, the anytime algorithm can return a list of ranked ICGs with their ancestor sub-interpretations (text, parse tree(s) and UCG(s)).To encourage the early generation of complete interpretations, when selecting a level to be expanded, preference is given to later stages in the search. That is, the probability of expanding the UCG level is higher than that of expanding the parse tree level, which in turn is higher than the probability of expanding the Text level, and finally the Speech Wave level. The stochastic optimization strategy is employed within a level during node selection: the most promising node is chosen most often, but not always. Specifically, nodes with a proven “track record” are preferred, i.e., nodes that have previously produced children with high probabilities/scores.The thresholding approach comprises two efficiency measures. The first one, inspired by the branch and bound algorithm, does not expand sub-interpretations whose scores are significantly worse than the score of the top Nth ICG, as they are unlikely to yield ICGs that are better than the current top N ICGs (N is currently set to 10). The second efficiency measure is based on the observation that the scores of the texts returned by the ASR drop significantly after a certain number of texts, as do the probabilities of the parse trees returned by the probabilistic parser. We harness this observation to prevent the procreation of unpromising nodes at all levels as follows. When the probability or score of the next child of a parent node n drops below a particular threshold relative to the probability or score of the best child of n, no additional children of n are generated. For example, given an ASR output threshold of 0.6, if the score of a textual output for the speech wave is less than 60% of the score of the first (best) textual output, then this text is ignored, and no more texts are generated for the speech wave. At present, we employ the following thresholds, which were set manually: 0.45 for ASR output, 0.1 for parse trees, and 0 for UCGs and ICGs (a parse tree generates only one UCG, and all ICGs are considered). Clearly, the values of these parameters can be learned in the same way we learn weights for the stages of the interpretation process and descriptive attributes (Section 5.2) – a task we propose to undertake in the future.UCGs are generated from parse trees in a rule-based manner as follows: parse tree nodes corresponding to head nouns are converted to UCG concepts; prepositions and syntactic relations are converted to UCG relations; and noun modifiers are incorporated as attributes of the concepts corresponding to head nouns (Fig. 1(b)).22In the future, we will investigate an approach based on a chunking parser, similar to that of Meena et al. (2012), for the generation of UCGs.The process of generating ICGs from a UCG and computing their score is carried out by Algorithm 1, which has two main stages: concept and relation postulation (Steps 2–8) and ICG construction (Steps 9–14). This algorithm generates a buffer containing up to kmax ICGs ranked in descending order of score the first time a UCG is expanded. Every time a new ICG is requested for that UCG, the next-ranked ICG is returned.Algorithm 1Generate candidate ICGs for a UCGRequire: UCG U comprising concepts and relations, contextC1: Initialize bufferIUof size kmax{Postulate objects and relations for the ICG}2: for all concepts and relations u in Udo3:   Initialize a list of candidate concepts and relations Lu←∅4:   for all instantiated objects and relations k in the knowledge basedo5:     Compare the requirements in u with the intrinsic attributes of k, yielding a score for the match6:     Insert k in the list Luin descending order of score7:   end for8: end for{Construct ICGs}9: forj=1 to kmaxdo10:   Generate the “next best” ICG Ijby going down each list Luin turn11:   Calculate the positional scores of the object-relation-object trigrams in ICG Ij12:   Combine the scores obtained in Step 5 with the scores obtained in Step 11 (Eqs. (3) and (4), Section 4)13:   Insert Ijinto bufferIUin descending order of score14: end forIn this stage, the algorithm consults Scusi?'s knowledge base to propose a list of instantiated nodes (concepts and relations) for each UCG node; this list is sorted in descending order of score, which reflects how well the instantiated nodes match the corresponding node in the parent UCG. The objects in the knowledge base, which define the context, are represented by means of the terms commonly used to refer to them, their colour and dimensions, and their position in a three-dimensional space, e.g., a room. Relations are represented by their reference terms and by procedures that implement their operational semantics, e.g., what does it mean in the physical world for an object to be near or on a landmark?In Step 5, for each concept (relation) u in UCG U, we estimate the match score between each instantiated concept (relation) k in Scusi?'s knowledge base and u. To this effect, we compare the intrinsic attributes of each instantiated object k (i.e., lexical item, colour and size) with the requirements stated in concept u, and the lexical item of each instantiated relation with the term used for relation u. For instance, given the UCG concept cup, the instantiated concepts {cup0, …, cup5} have a perfect lexical match with the UCG concept, while {mug0, …, mug4} have a good lexical match. If the UCG concept also had a colour: blue attribute, then the colour coordinates of the objects in the knowledge base (including mugs and cups) would be matched against the colour coordinates for blue (as indicated in Section 1, matches with non-cup-like concepts are useful when the head noun has been misheard, but there are objects that match the specified colour). The score for lexical, colour and size match is calculated using the functions outlined in Table 2 and described in Appendix A.This stage of the algorithm yields a list of candidate instantiated concepts (relations) Lusorted in descending order of score for each UCG concept (relation) u. Table 1shows the top objects in the sorted lists generated for the description in Fig. 1(b) “the blue mug on the table near the vase” in a context that comprises cup0 and mug3 (which are blue), and cup1 and mug2 (which are not blue), three tables, two vases and other objects (the reasonable candidates appear in larger font). The blue mug is ranked first, followed by the blue cup, the other mug and cup, and then all the other objects in the scene (which have a very low match score with the lexical item “mug”, with blue objects preceding objects of other colours). The three tables have a perfect lexical match with the second noun in the description, hence they have the same score (followed by vases, mugs, cups and other objects – all with a very low lexical match score). Similarly, the two vases have a perfect lexical match with the third noun (followed by tables, mugs, cups and other objects). There are three equal-score relations which match the preposition on, and one relation that matches near, with other relations yielding poor lexical matches with these prepositions.In this stage, the algorithm uses the list of instantiated concepts (relations) built for each concept (relation) in a UCG to construct candidate ICGs for this UCG, and sorts these ICGs in descending order of score. First, Step 10 applies an enumerative process to generate different combinations of concepts and relations from the Lulists maintained for the UCG nodes. This is done by iteratively selecting one candidate instantiated concept (relation) from each list starting at the top. For instance, the concepts and relations in Table 1 are combined as follows to build candidate ICGs. First, the top line {mug3, table0, vase0, Location_on, Location_near}, which has the highest total score, is used. The next two combinations are generated by replacing table0 with table1 and table2, as the tables have the same score, yielding {mug3, table1, vase0, Location_on, Location_near} and {mug3, table2, vase0, Location_on, Location_near}; and so on.The scores of the object–relation–object trigrams in each ICG are estimated in Step 11. These scores reflect the extent to which the relationships between neighbouring object nodes in an ICG match the context (Section 4). For example, given the referring expression “the cup on the table”, if, according to the knowledge base, the coordinates of mug3 place it on table2, an ICG that contains [mug3 – Location_on – table2] is assigned a high score for positional match; whereas if the coordinates of cup1 place it on a shelf, the ICGs containing cup1 are assigned low scores for positional match (the calculation of the scores for positional relations is outlined in Table 2and described in Appendix B). In Step 12, these positional scores are combined with the scores calculated in Step 5 to obtain the total score of an ICG produced for a given UCG. If an ICG has more than one parent UCG, the scores obtained from the different parent UCGs are combined (Section 4.1.1). The resultant ICG is inserted in a buffer for its parent UCGs, which is sorted in descending order of the ICGs’ scores.As mentioned above, the score of an ICG may be viewed as a subjective probability representing Scusi?'s state of certainty regarding the match between a candidate ICG and the intended meaning of a description. Scusi? ranks candidate ICGs according to this score, which is estimated on the basis of how well an ICG matches a heard description and the contextual information. At present, the context comprises the type, colour, dimensions and position of the objects in a room. This information could be provided by a scene analysis system, but it is currently obtained from a virtual representation generated with the SweetHome software package (http://sweethome3d.com). In the future, the context will also take into account salience from dialogue history.Given a speech signal S and a contextC,Sc(I|S,C), the score of an ICG I, is estimated as follows:(1)Sc(I|S,C)∝Sc(I|U,C)WISc(U|P)WUPr(P|T)WPSc(T|S)WTwhere a UCG, parse tree and plain-text interpretation are denoted by U, P and T respectively, and the probability returned by the parser and the scores produced by the ICG and UCG stages and the ASR are each assigned a weight to adjust their effect on the final score of ICG I. These weights were determined in a modest learning experiment performed on a small development corpus, where we compared Scusi?'s performance using weights learned by two irrevocable search algorithms, viz a genetic algorithm and steepest ascent hill climbing, with that of a baseline were all the weights equal 1 (Section 5.2).The ASR returns an estimate for Sc(T|S) which is in the [0, 1] range, and the parser returns the probability Pr(P|T). To estimate Sc(U|P) andSc(I|U,C)we adopt an approach inspired by the Minimum Message Length (MML) principle (Wallace, 2005), which balances model complexity against data fit. MML operationalizes Occam's Razor, which may be stated as follows: “If you have two theories, both of which explain the observed facts, you should use the simplest until more evidence comes along”. Thus, MML favours simple models that explain the data well. In our case, these are simple Concept Graphs that match a description and the context well. We model Concept Graph simplicity by penalizing UCGs with a high number of nodes as follows (the conversion from parse trees to UCGs is deterministic, so no other factor needs to be considered):(2)Sc(U|P)=1Nwhere N is the number of nodes.Sc(I|U,C)represents data fit, i.e., how well an ICG I matches its parent UCG U and the contextC. These scores are mapped to the [0, 1] range to have a compatible range with that of the scores obtained for UCGs and texts and the probabilities returned by the parser. This compatible range in turn facilitates the weight-learning process described in Section 5.2.We distinguish between two types of features that influenceSc(I|U,C): intrinsic and positional.•Intrinsic featuresrepresent the attributes of individual nodes in an ICG. They determine how well an object (obtained from a room or context) or a relation in a candidate ICG matches the specifications in the corresponding concept or relation in its parent UCG (recall that a UCG concept node is composed of a head noun and its modifiers, and a relation node is built from a preposition or prepositional expression, e.g., “to the left of”). Each object in Scusi?'s knowledge base has a type, colour (represented by CIE94 coordinates (Rubner et al., 2001)) and size (represented by dimensions of the bounding box circumscribing the object (Skubic et al., 2004)). Hence, the intrinsic features handled by Scusi? at present are: lexical item, colour and size (Appendix A). In addition, Scusi? identifies unknown attributes, which contain noun modifiers that Scusi? does not understand. For instance, if the description in Fig. 1 had been “the blue ceramic mug on the table near the vase”, “ceramic” would have been an unknown attribute, which would have reduced the score of all the matches with candidate objects. It is worth noting that when unknown attributes are due to ASR error, this behaviour affects the interpretations generated from the incorrect text.Positional featuresrepresent the relative location of objects. They model how well each object–position–object triple in an ICG matches the current context. For instance, given the description “the blue mug on the table near the vase” in Fig. 1(b), the relation mug3–Location_on–table2 in the candidate ICG matches the context if the z coordinate of the bottom of mug3 is the same as the z coordinate of the top of table2. Now, if mug3 is also near vase1, then both positional relations in the ICG in Fig. 1(b) will match the context. However, if mug3 is not near vase1 or any vase, but table2 is near, say, vase3, then the following ICG (obtained from a different parse tree and UCG) would yield the best match: mug3–Location_on–table2–Location_near–vase3. At present, Scusi? handles positional relations represented by the topological prepositions on, in, near, at and in/at/near the center/corner/edge/end of, which designate a region relative to a landmark; and positional relations represented by the projective prepositions in front/back of, behind, to the left/right of, above and under, which describe a region projected from the landmark in a particular direction (Appendix B). The specification of the direction depends on the frame of reference being used (absolute, intrinsic or viewer centered) (Kelleher and Costello, 2008; Coventry and Garrod, 2004; Liu et al., 2010). In addition, projective relations depend on whether the landmark has a “face” (e.g., TV) or is human centric (e.g., chair) (Liu et al., 2010). In these cases, there is a high potential for ambiguity between the viewer-centered and intrinsic frames of reference. The consideration of these factors is left for future work, and like Kelleher and Costello (2008), in this research we adopt a viewer-centered frame of reference for projective relations.Coventry and Garrod (2004) propose three factors that influence the use of positional prepositions: (1) the geometry of the situation, (2) control and support (e.g., a referent is in a landmark if moving the landmark controls the movement of the referent), and (3) domain knowledge (about the manner in which the referent and the landmark interact). In this paper, we consider only the first factor, as positional information may be obtained from a scene analysis system, while the extra-geometric information relies on additional user experiences. The representation and acquisition of this information and its incorporation into our geometric formalism is the subject of future work.We consider two schemes for estimating the score of an ICG on the basis of intrinsic and positional features: Multiplicative and Additive. This score is later mapped to the [0, 1] range as described in Section 4.1.Multiplicative scheme. This scheme is similar to that used in Eq. (1):(3)ScMULT(I|U,C)=∏i=1M∏j=1MSc(loc(ki,kj))Wlocδ(loc(ui,uj))×∏i=1MSc(ui,lex|ki)Wlex×Sc(ui,col|ki)Wcolδ(ui,col)×Sc(ui,size|ui,lex,ki)Wsizeδ(ui,size)×Sc(ui,unk)Wunkδ(ui,unk)where•M is the number of objects in ICG I.Sc(ui,lex|ki) denotes the lexical similarity between the term ui,lex and terms commonly used to refer to object ki; Sc(ui,col|ki) denotes the visual similarity between the term ui,col and the colour of object ki; Sc(ui,size|ui,lex, ki) denotes how well the term ui,size reflects the size of object ki; Sc(ui,unk) is the penalty associated with a term that Scusi? does not understand (currently set to a very small value ϵ); and Sc(loc(ki, kj)) reflects how well the location of referent kimatches location loc with respect to landmark kj. Note that size is conditioned on lexical item, because size depends on both the described concept and the candidate object. For example, given a request for a large desk, a candidate table should be deemed large compared to desks, rather than compared to tables. In contrast, if a red table is requested, we only need to determine how close to red is the colour of a candidate table.δ(ui, f) equals 1 if feature f was specified for object i, and 0 otherwise; similarly, δ(loc(ui, uj)) equals 1 if relation loc was specified between objects kiand kj, and 0 otherwise.the weights Wlex, Wcol, Wsize, Wunk and Wloc reflect the influence of lexical item, colour, size, unknown attribute and location respectively on the score of an interpretation. Like the weights assigned to the different stages of the interpretation process, these weights were experimentally determined (Section 5.2).The second and third lines in Eq. (3) represent how well each object kiin ICG I matches the lexical item, colour and size specified in its parent concept uiin UCG U, and whether uihas unknown attributes that could not be matched. The first line represents how well the relative location of two objects kiand kjin contextC(e.g., a room) matches their specified location in UCG U (e.g., on(ki, kj)). For instance, given the ICG in Fig. 1(b), the second line in Eq. (3) estimates how suitable the term “mug” is for designating mug3 and how well “blue” matches the colour of mug3 (no size was specified), and how suitable the terms “table” and “vase” are for designating table2 and vase1 respectively (no colour or size was specified). The first line estimates to what extent mug3 could be said to be on table2 and near vase1; if the mug is on the table but far from the vase, the first score will be high and the second one low.This scheme is rather unforgiving of partial matches or mismatches, e.g., the score of a lexical match between “mug” and cup1, which is less than 1, is substantially reduced when raised to an exponent greater than 1; and a mismatch of a single attribute in an ICG significantly lowers the score of the ICG. This motivates the more forgiving Additive scheme.Additive scheme. This scheme estimatesScADD(I|U,C)using the following formulation:(4)ScADD(I|U,C)=∑i=1M∑j=1MSc(loc(ki,kj))Wlocδ(loc(ui,uj))+∑i=1MSc(ui,lex|ki)Wlex+Sc(ui,col|ki)Wcolδ(ui,col)+Sc(ui,size|ui,lex,ki)Wsizeδ(ui,size)+Sc(ui,unk)Wunkδ(ui,unk)The anytime and stochastic operation of our algorithm, and the diverse sources from which the probabilities or scores are obtained, highlight two issues that must be addressed when calculating the score of an interpretation: (1) multiple parent nodes, and (2) probabilities or scores of different magnitudes.As shown in Fig. 1(a), an ICG may have more than one parent UCG, which in turn may have more than one parent parse tree. To take into account multiple parents when estimating the score of ICGs and UCGs, we employ a normalized weighted average of the score of the child node as shown in Eqs. (5) and (6), where the weights are the probabilities or scores of the parent nodes.(5)Sc(Ui|P)∝∑jSc(Ui|Pj)·Pr(Pj),wherePjis a parent parse tree ofUi(6)Sc(Ii|U,C)∝∑jSc(Ii|Uj,C)·Sc(Uj),whereUjis a parent UCG ofIiThe resultant scores are then mapped to the [0, 1] range.There are large variations in the probabilities and scores returned by the different interpretation stages. In particular, the probabilities returned by the parser are several orders of magnitude smaller than the scores returned by the other stages. In order to obtain probabilities and scores of a similar magnitude, we adopt two approaches: (1) adjusting the probabilities returned by the parser by calculating their standardized score zi, and (2) normalizing the scores of the ICGs by introducing a factor that depends on the weights assigned to different descriptive attributes. The second approach takes advantage of specific information about ICGs. Such information is not available about parse trees, which motivates the use of the more generic first approach.Adjusting parse tree probabilities. Given a probability pireturned by the parser, we calculate its z-score zi=(pi−μ)/σ, where μ is the mean and σ the standard deviation of the probabilities returned by the parser for our development corpus. The ziscores are then transformed to the (0, 1] range using a sigmoid functionziNorm=1/(1+e−zi), which yields scores that increase linearly between the values [−2, 2] of zi.Normalizing ICG scores. The ICG scores obtained by the Multiplicative scheme are often in a small band in a very low range, while the ICG scores obtained by the Additive scheme are typically greater than 1. In order to expand the range of the former, and map the latter into the (0, 1] range, we incorporate the following normalizing factor φ into their formulation:φ=∑i=1M∑j=1MWlocδ(loc(ui,uj))+∑i=1MWlex+Wcolδ(ui,colour)+Wsizeδ(ui,size)+Wunkδ(ui,unk)This factor is incorporated into the Multiplicative and Additive schemes as follows:•Multiplicative scheme.(7)ScMULT(I|U,C)=ScMULT(I|U,C)1/φAdditive scheme.(8)ScADD(I|U,C)=1φScADD(I|U,C)The above adjustments are made in the following order: the parse tree probabilities are adjusted before using them in the computation of the scores of UCGs, which takes into account multiple parents. The normalization factor φ is applied to adjust the score of an ICG derived from a single UCG, and the final score of this ICG is derived by considering multiple parents.Modern approaches advocate the estimation of scores such as those in Eqs. (3) and (4) from data. However, it may be quite difficult to perform data-driven estimation of the similarity between the lexical item, colour, size and location in a description and the corresponding attributes of candidate objects in the physical world, and it may also be unnecessary, as the functions for performing this estimation have generally accepted distance-based semantics. For instance, a referent is deemed to be on a landmark when the bottom of the referent rests on the top of the landmark. As another example, given a request for a blue mug, a royal blue mug should have a higher match score than a green mug. However, as noted by Coventry and Garrod (2004), the details of these perceptions may vary between people, e.g., while it is generally agreed that two adjacent objects are near each other, the perception of when they stop being near depends on the individual and the context. Still, there is general agreement about relative attribute values, e.g., which object is nearer.On the basis of these observations, we devise simple distance-based comparison functions to estimate the extent to which candidate objects match a description. These functions are largely based on approximations of the physical properties of objects, and implement generally accepted distance-based semantics. Our approach is similar to that adopted by Funakoshi et al. (2012), while significantly expanding the range of functions being handled. Table 2 displays an overview of our intrinsic and positional functions. The intrinsic functions, which estimate the scores in lines 2 and 3 of Eqs. (3) and (4), are described in detail in Appendix A. Our functions for positional relations, which resemble those by Coventry and Garrod (2004) and Kelleher and Costello (2008), estimate the scores in line 1 of Eqs. (3) and (4), and are detailed in Appendix B. As seen in Section 5, despite being rather coarse grained, these functions produce reasonable interpretations for the descriptions in our corpus.

@&#CONCLUSIONS@&#
