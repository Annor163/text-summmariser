@&#MAIN-TITLE@&#
Computation of graph edit distance: Reasoning about optimality and speed-up

@&#HIGHLIGHTS@&#
Graph Edit Distance (GED) is the most used error-tolerant graph matching method.Bipartite graph matching algorithm (BP) is the most used algorithm to solve GED.2 new versions of BP published that reduce the runtime but restrictions are imposed.We study how much extend these restrictions limits their applicability.Empirical validation shows new versions reduce runtime but keep recognition ratio.

@&#KEYPHRASES@&#
Error-tolerant graph matching,Bipartite graph matching algorithm,Fast Bipartite,Square Fast Bipartite,Hungarian method,Jonker–Volgenant solver,

@&#ABSTRACT@&#
Bipartite graph matching has been demonstrated to be one of the most efficient algorithms to solve error-tolerant graph matching. This algorithm is based on defining a cost matrix between the whole nodes of both graphs and solving the nodes' correspondence through a linear assignment method (for instance, Hungarian or Jonker–Volgenant methods). Recently, two versions of this algorithm have been published called Fast Bipartite and Square Fast Bipartite. They compute the same distance value than Bipartite but with a reduced runtime if some restrictions on the edit costs are considered. In this paper, we do not present a new algorithm but we compare the three versions of Bipartite algorithm and show how the violation of the theoretically imposed restrictions in Fast Bipartite and Square Fast Bipartite do not affect the algorithm's performance. That is, in practice, we show that these restrictions do not affect the optimality of the algorithm and so, the three algorithms obtain similar distances and recognition ratios in classification applications although the restrictions do not hold. Moreover, we conclude that the Square Fast Bipartite with the Jonker–Volgenant solver is the fastest algorithm.

@&#INTRODUCTION@&#
Attributed graphs have been used in some pattern recognition fields such as object recognition [1–3] scene view alignment [4–6] multiple object alignment [7,8], object characterization [9,10] interactive methods [11,12] image registration [13], tracking [14] among a great amount of other applications. Interesting reviews of techniques and applications are [15,16] and [17]. Error-tolerant graph-matching algorithms compute the correspondences between nodes of two Attributed Graphs that minimises some kind of objective function. One of the most widely used methods to evaluate an Error-correcting graph isomorphism is the Graph Edit Distance [18–21]. The basic idea behind the Graph Edit Distance is to define a dissimilarity measure between two graphs based on the minimum amount of required distortion to transform one graph into the other. To this end, a number of distortion or edit operations, consisting of insertion, deletion and substitution of both nodes and edges are defined. Then, for every pair of graphs (Gpand Gq), there is a sequence of edit operations that transforms one graph into the other. To quantitatively evaluate which sequence is the best, edit cost functions are introduced. The basic idea is to assign a penalty cost to each edit operation according to the amount of distortion introduced in the transformation. Unfortunately, the time and space complexity to compute the minimum of these objective functions is very high. For this reason, almost 20years ago appeared the Graduated Assignment algorithm [22] that computes a sub-optimal solution of the Error-Tolerant Graph Matching problem in O(n6), beingnthe number of nodes of both graphs.Other methods different from graph edit distance have been presented in which the flexibility to cope with any kind of domains in node and edges and different structures is reduced but also their computational cost. One example are the spectral methods [23,24], which are based on the eigendecomposition of the adjacency or Laplacian matrix of a graph. In this framework, graphs are unlabelled or only allow severely constrained label alphabets. Other common constraints include restrictions to ordered graphs [25], planar graphs [26,27], bounded-valence graphs [28], trees [29] and graphs with unique node labels [30]. Finally, a general optimisation framework based on a graduated non-convexity and concavity procedure (GNCCP) [31] has been applied to solve the error-tolerant graph matching. They present a comparison to well-know methods like [22] showing good achievements.The Graph Edit Distance is applicable to a wide range of real-world applications since any type of attributes can be used. In recent years, a number of methods addressing the high computational complexity of graph edit distance computation have been proposed. Probabilistic relaxation labelling [32,33] adopts a Bayesian perspective on Graph Edit Distance and iteratively applies edit operations to improve a maximum a posteriori criterion. As an alternative to this hill climbing approach, genetic algorithms have been proposed for optimization in [34]. In [35] a randomized construction of initial mappings is followed by a local search procedure. In [36], a linear programming method for computing the edit distance of graphs with unlabelled edges is reported. And also, dominant sets have been applied to sub-optimally compute the edit distance [37]. Finally, in [38] the graph edit distance is approximated by the Hausdorff distance.Recently, a new algorithm called Bipartite (BP) [39] and two other versions of this algorithm have appeared that are called Fast Bipartite [41] (FBP) and Square Fast Bipartite [42] (SFBP). Note, this algorithm solves the error-tolerant graph-matching problem between attributed and undirected graphs. Therefore, it has not any relation between matching graphs that are classified as bipartite. These three algorithms have a cubic computational cost with respect to the number of nodes and are implemented in two main steps. In the first one, a cost matrix is defined and in a second one, a linear solver on this matrix is applied to find the final distance and node correspondence. Due to BP, FBP or SFBP do not consider the structural information globally but only locally, the obtained distance tends to be larger than the exact one. Some methods [43,44] improve this distance and obtain a new correspondence starting from the correspondence computed by BP, FBP or SFBP at the expense of increasing the runtime.Table 1summarises the main properties and differences of these algorithms extracted from [41] and [42]. Two linear solvers have been used; the Hungarian method [45,46] and the Jonker–Volgenant solver [48], which was published later. It is not guaranteed that both methods obtain exactly the same correspondence, although there is a clear tendency of obtaining similar assignations. Therefore, the distance obtained by the three algorithms through the Hungarian method can be different of the distance obtained by the same algorithms but through the Jonker–Volgenant solver. The Hungarian method is usually slower than the Jonker–Volgenant solver but always converges. The Jonker–Volgenant is faster but it has convergence problems in some cost matrices. In this way, the Hungarian method always converges while applied to the cost matrices defined by BP and FBP algorithms but the Jonker–Volgenant method not always finishes on the cost matrix defined by FBP. The Computational Cost of FBP and SFBP is slightly lower than BP but in the expense of introducing some restrictions on the edit costs [41,42]. In Table 1,nandmrepresent the order of both graphs. Finally, it was shown in [41] that the real runtime of these solvers clearly depends on the cost matrix and so, the runtime of comparing two graphs depends on the order of presentation of these graphs. We call this property a non-symmetric runtime. In this way, it is worth to consider the number of nodes of the graphs to decide in which order the graphs are introduced into the matching algorithm.In some classification applications or tests on databases, it would be useful to set some edit costs such that the edit costs restrictions theoretically imposed to FBP and SFBP do not hold with the aim of increasing the recognition ratio. The aim of this paper is to present a real comparison of the three methods and to show to which extend the fact of not holding the edit cost restrictions affects on the runtime, optimality and recognition ratio. That is, we want to show the applicability of these algorithms on real graph problems not only from the runtime point of view but also from the recognition ratio point of view. To do so, we performed two types of experiments. The first ones are applied on synthetic graphs and we want to discover which is the increase of the obtained graph edit distance when we move away from the edit cost restrictions. The second ones are applied to public graph databases and we want to show the relation between edit costs (although they may violate the restrictions), recognition ratio and run time.The outline of the paper is as follows: in the next section, we define the Attributed graphs and the Graph Edit Distance. In Section 3, we comment the two most well known linear assignment solvers. In Section 4, we summarise algorithms BP, FBP and SFBP and also the Hungarian and Jonker–Volgenant linear solvers. In Section 5, we move on the experimental part to present a comparison of the applicability of these three algorithms. Section 6 concludes the paper.In this section, we first define Attributed graphs and Error-tolerant graph matching and then we explain the Graph Edit Distance.An Attributed Graph is defined as a triplet G=(Σν, Σe,γv,γe), where Σv={va | a = 1,…,n} is the set of vertices and Σe={eab|a,b∈1,…,n} is the set of edges. Functions γv:Σv→Δv and γe:Σe→Δe assign attribute values in any domain to vertices and edges. The order of graph G is n. We call E(va) to the number of neighbours of node va, that is, the number of outgoing edges. Finally, we define the neighbours of a node va, named Na, on an attributed graph G, as another graphNa=ΣvNa,ΣeNa,γvNa,γeNa. The definition of the neighbours of a node is needed to define two different local sub-structures in Section 4. Na has the structure of an attributed graph but it is only composed of nodes connected to va by an edge. Formally,ΣvNa=vb|eab∈Σe,ΣeNa=∅(empty set) andγvNavb=γvvb,∀vb∈ΣvNa.Let Gp=(Σvp, Σep,γvp,γep) and Gq=(Σvq, Σeq,γvq,γeq) be two Attributed graphs of initial order n and m. To allow maximum flexibility in the matching process, graphs are extended with null nodes to be of order n+m. We refer to null nodes of Gp and Gq byΣ^vp⊆ΣvpandΣ^vq⊆Σvqrespectively. We assume that null nodes have indices a∈[n+1,…,n+m} and i∈[m+1,…,n+m} for graphs Gp and Gq, respectively. Let T be a set of all possible bijections between two vertex sets Σvp and Σvq. We define the non-existent or null edges asΣ^ep⊆ΣepandΣ^eq⊆Σeq. Isomorphism fp,q:Σvp→Σvq, assigns one vertex of Gp to only one vertex of Gq. The isomorphism between edges, denoted by fep,q, is defined accordingly to the isomorphism of their terminal nodes. That is, fep,q(eabp)=eijq⇒fp,q(vap)=viq∧fp,q(vbp)=vjqwherevap,vbp∈Σvp−Σ^vpandviq,vjq∈Σvq−Σ^vq. We define the non-existent or null edges byΣ^ep⊆ΣepandΣ^eq⊆Σeq.One of the most widely used methods to evaluate an error-correcting graph isomorphism is the Graph Edit Distance [1,20]. The dissimilarity is defined as the minimum amount of required distortion to transform one graph into the other. To this end, a number of distortion or edit operations, consisting of insertion, deletion and substitution of both nodes and edges are defined. Edit cost functions are introduced to quantitatively evaluate the edit operations. The basic idea is to assign a penalty cost to each edit operation according to the amount of distortion that it introduces in the transformation. Deletion and insertion operations on nodes (on edges) are transformed to assignations of a non-null node (non-null edge) of the first or second graph to a null node (null edge) of the second or first graph. Substitutions simply indicate node-to-node (edge-to-edge) assignations. Using this transformation, given two graphs Gpand Gq, and a bijection between their nodes, fp,q, the graph edit cost is given by:(1)EditCostGpGqfp,qKvKe=∑vap∈Σvp−Σ^vpviq∈Σvq−Σ^vqCvsvapviq+∑vap∈Σvp−Σ^vpviq∈Σ^vqKv+∑vap∈Σ^vpviq∈Σvq−Σ^vqKv+∑eabp∈Σep−Σ^epeijq∈Σeq−Σ^eqCeseabpeijq+∑eabp∈Σep−Σ^epeijq∈Σ^eqKe+∑eabp∈Σ^epeijq∈Σeq−Σ^eqKeWhere fp,q(vap)=viqand fep,q(eaip)=eijqwhere Cvsand Cesare functions that represent the cost of substituting the involved nodes or edges. Constant Kvis the cost of deleting node vapof Gpor inserting node viqof Gq. Likewise for the edges, Keis the cost of assigning edge eabpof Gpto a non-existing edge of Gqor assigning edge eabqof Gqto a non-existing edge of Gp. Note that we have not considered the cases in which two null nodes or null edges are mapped, this is because this cost is zero by definition.The Graph Edit Distance is defined as the minimum cost under any bijection in T:(2)EditDistGpGqKvKe=minfp,q∈TEditCostGpGqfp,qKvKeWe say the optimal correspondence, fp,q*, is one of the correspondences (may be not unique) that obtains the minimum cost,(3)fp,q*=argminfp,q∈TEditCostGpGqfp,qKvKeGraph Edit Distance has been defined on any kind of attributes domain on nodes and edges in [1] and [20]. Moreover, in its original form, costs of inserting nodes or edges do not have to be the same than deleting them. We have assumed equivalent and constant costs for deleting and inserting nodes or edges, Kvand Kebecause in this case, we assure that Eq. (2) is symmetric with respect to the order of presentation of graphs. Moreover, if we force Kvand Keto be non-negative constants and we define Cvsand Cesto be distance functions such that 0≤Cvs≤2·Kvand 0≤Ces≤2·Ke, then the Edit distance fulfils the distance function restrictions [20].The Linear Assignment problem considers the task of finding an optimal assignment of the elements of a set A to the elements of another set B, where both sets have the same cardinality k=|A|=|B|. Let us assume there is a k X k cost matrix C. The matrix element Ci,jcorresponds to the cost of assigning the i-th element of A to the j-th element of B. An optimal assignment is the one that minimises the sum of the assignment costs and so, the linear assignment problem can be stated as finding the permutation P that minimises ∑i=1kCi,P(i). Munkres' algorithm [45] solves the assignation problem in O(k3), in the worst case. It is a refinement of an earlier version by Kuhn [46] and is also referred to as Kuhn–Munkres or Hungarian algorithm. The algorithm repeatedly finds the maximum number of independent optimal assignments and in the worst case the maximum number of operations needed by the algorithm is O(k3). Since this local exploration is performed through columns (or rows), in cases the cost matrix is not symmetric, the real run time drastically depends on the order of exploration (columns or rows). Later, an algorithm to solve this problem applied to non-square matrices were presented [47].The Jonker–Volgenant algorithm [48] has received significant attention recently due to, although the theoretical computational cost is similar to the Hungarian method, O(k3), it is highly effective in practice. This is because in a first step, the algorithm computes a large number of initial assignments and in a second step, only few shortest paths are needed to obtain the optimal solution. The original algorithm was developed for integer costs. When it is used for real (floating point) costs, sometimes the algorithm takes an extremely long time. This drawback is emphasised when the cost matrix has a high number of cells with equivalent values. Similar to the Hungarian method, it is supposed that the input cost matrix is square. Usually, the implemented codes accept non-square matrices because they convert the non-square matrices to square matrices and fill the extended cells with zero values. Although extending the non-square matrices to square matrices makes the Jonker–Volgenant algorithm to be applicable to a higher amount of problems, it also increases the non-convergence problems.These three algorithms return a sub-optimal value of the edit distance (Eq. (2)) and the sub-optimal correspondence (Eq. (3)). To do so, the first step of these algorithms is to obtain a cost matrix. We call, CBP, CFBPand CSFBPthe cost matrices of the three algorithms (explained at the end of this section). The second step is to apply a linear solver such as the Hungarian method or the Jonker–Volgenant method (Section 3) to these matrices and to obtain the correspondence fp,q*. And the third step is to compute the Edit distance cost given this correspondence and both graphs, EditDistance(Gp,Gq) = EditCost(Gp,Gq,fp,q* ).With the aim of explaining the cost matrix, we define values Ca,i, Ca,ε and Cε,i as follows:Ca,irepresents the cost of mapping two nodes and their local sub-structures. It is defined as:Ca,i=Cvs(vap,viq)+Ccs(vap,viq);vap∈Σvp−Σ^vpandviq∈Σvq−Σ^vq.Ca,ε represents the cost of deleting a node from Gpand its local sub-structures. It is defined as:Ca,ε=Kv+Ccd(vap,viq);vap∈Σvp−Σ^vpandviq∈Σ^vq.Cε,i represents the cost of inserting a node from Gqand its local sub-structures. It is defined as:Cε,i=Kv+Cci(vap,viq);vap∈Σ^vpandviq∈Σvq−Σ^vq.That is, the whole values on the cost matrices depend on two disjoint costs. The first one only depends on the nodes and the second one depends on the rest of the local sub-structure. As commented in Section 2, Cvsis a distance function defined through the node attribute values and Kvgauges the importance of deleting or inserting nodes in the matching process. Finally, Ccsis the cost involved by the substitution of two the local sub-structure and also Ccdand Cciare the costs to delete and insert it, respectively. These costs depend on the type of sub-structure associated to the node. In this paper, we propose the most used local sub-structures, viz, the degree centrality and the clique centrality. The degree centrality is composed of the set of neighbouring edges and the clique centrality is composed of the set of neighbouring edges and also the neighbouring nodes. Fig. 1shows these two local sub-structures.Other sub-structures have been presented in [49,51,54]. In this paper, the costs are defined as follows:The degree centrality: The local sub-structure is composed by only the edges connected to the node. The distance between these sub-structures is based on counting the number of edges. Attributes on edges are not taken into consideration. That is,Ccsvapviq=Ke·Evap−Eviq;vap∈Σvp−Σ^vpandviq∈Σvq−Σ^vq.Ccdvapviq=Ke·Evap;vap∈Σvp−Σ^vpandviq∈Σ^vq.Ccivapviq=Ke·Eviq;vap∈Σ^vpandviq∈Σvq−Σ^vq.The clique centrality: The neighbour nodes of vapand viqare Napand Niq(Section 2). Thus, the distance between Napand Niqis defined as the distance between two graphs (Eqs. (1) and (2)). Yet, we have to consider that in the definition of the neighbour structures Napand Niq, there are no edges. Besides, edges that connect the central node with the neighbouring nodes in the local sub-structure have to be taken into consideration in the cost value. To solve these two requirements, costs are defined as follows,Ccsvapviq=EditDistanceNap,Niq,Kv+Ke,0;vap∈Σvp−Σ^vpandviq∈Σvq−Σ^vq.Ccdvapviq=Kv+Ke·Evap;vap∈Σvp−Σ^vpandviq∈Σ^vq.Ccivapviq=Kv+Ke·Eviq,vap∈Σ^vpandviq∈Σvq−Σ^vq.Note function EditDistance is computed through the BP algorithm on both sets of nodes. Having defined these costs, the cost matrix for Bipartite [39] isMatrix CBPis composed of four quadrants. Each quadrant represents a different Edit distance operation. Quadrant Q1 denotes the combination of costs of substituting nodesvap∈Σvp−Σ^vpbyviq∈Σvq−Σ^vqand their local sub-structures. The diagonal of quadrant Q2 denotes the whole costs of deleting nodes vapand its local sub-structures. That this, the substitution ofvap∈Σvp−Σ^vpbyviq∈Σ^vq. Similarly, the diagonal of quadrant Q3 denotes the whole costs of inserting nodes viqand its adjacent vertices. That is, the substitution ofvap∈Σ^vpbyviq∈Σvq−Σ^vq. Q4 quadrant is filled with zero values since the substitution between null elements has a zero cost.The cost matrix for Fast Bipartite [41] is composed of only one quadrant and if m≠n it is not square. Nevertheless, this is not a problem for the linear assignation methods since they fill with zeros the non-square matrices to make it square [47].The Square Fast Bipartite [42] defines two square matrices and uses one of them depending on the order of the involved graphs. Whether m≥n, the cost matrix is:Whether m≤n, the cost matrix is:Finally, FBP and SFBP have the restriction that Edit costs have to be defined such that the Edit distance is a distance function [42]. Thus three restrictions have to hold. 1) Insertion and deletion costs have to be symmetric. 2) Cvs(vap,viq) and Ces(eabp,eijq) have to be defined as a distance measure. 3) Cvs(vap,viq)≤2·Kvand Ces(eabp,eijq)≤2·Ke. Note, 2·Kv(and 2·Ke) is the cost of inserting and deleting nodes (and edges).As commented in the introduction, we performed two types of experiments. The first ones are applied on synthetic graphs and we want to discover which is the speedup of FBP and SFBP with respect to BP given several values of Kvand Ke. We also study the graph edit distance obtained by the three algorithms. The main aim is to analyse the values of Kvand Kesuch that FBP and SFBP are faster than BP while keeping similar values of the Graph Edit Distance. The second ones are applied on public graph databases and we want to show the relation between substitution, deletion and insertion edit costs, recognition ratio and runtime. The Matlab code of these algorithms is available at [53].Algorithms BP and FBP are non-symmetric runtime algorithms but SFBP is symmetric [42] (summarised on last column of Table 1). In our implementation (and also in the implementation in [42]) the fastest option is the one such that the order of Gpis higher than the order of Gq(n≥m) while searching for mapping fp,q*. Since we want to compute the fastest option, we experimented on two combinations: n=20 and m=20 and also n=20 and m=10. Tests have been executed on an I7 processor, Windows and Matlab 2014a. Nodes have only one attribute that is a real number from 0 to 99 and in average each node is connected to half of the other ones by an unattributed edge. The node attributes and the node connections have an equal distribution. The whole costs and runtimes we show are the average of 1000 executions (1000 pairs of graphs). Cost Cvs(vap,viq) is the Euclidean distance between the attributes of the involved nodes, Ces(eabp,eijq)=0 (edges are unatributed) and constants Kvand Keare parameters of the tests.We nameDLinearSolverMatchAlg,LocStructKvKe=mean∀Gp→GqEditDistGpGqKvKewhere: MatchAlg is the used matching algorithm: {BP, FBP, SFBP}. LinearSolver is the linear solver: {h,jv} (h: Hungarian andjv: Jonker–Volgenant).LocStruct is the used local structure: {c,d} (c: Clique centrality and d: Degree centrality). That is, we averaged the Edit Distance of 1000 tests given a specific combination of insertion and deletion constants Kvand Keand also the combination of the three methods: Matching algorithm, Local structure and Linear solver.Figs. 2 and 3show the increase of the distance obtained by matching methods FBP and SFBP with respect to BP. In Fig. 2 the order of graphs is n=20 and m=20 and in Fig. 3 the order is n=20 and m=10. Note that we do not want to validate the quality of BP since, this quality has been tested in [39]. Moreover, if we would like to obtain the exact distance, we would have to use a A* algorithm with a huge runtime. More sub-optimal the method is, higher the obtained distance is. Therefore, in our case, we consider algorithms FBP and SFBP perform similarly than BP if this increase is zero. In this experiments, we do not want to compare the local sub-structures methods (clique versus degree centralities) since this comparison was performed in [51] together with other more complex local sub-structures. We neither want to compare Hungarian method versus Jonker–Volgenant since this comparison can be seen in [42]. But we want to know, how much the distance increases when the Edit costs restrictions are violated. That is, we want to know the quality of the obtained distance when we cannot insure |γv(vap)−γv(viq)|≤2·Kv, given the specific definition of our attributed graphs. Table 2shows the ratios we used to validate the increase of sub-optimality. In the whole experiments, we divided the mean distance obtained by FBP or SFBP to the mean distance obtained by BP. A ratio higher than one means FBP or SFBP obtains a higher value than BP and so it is more sub-optimal than BP. Therefore, lower these values are, better FBP or SFBP perform.Figs. 2 and 3 show us that when Kvand Keare very low, distances obtained by FBP or SFBP tend to be much higher than the ones obtained through BP. Therefore, if our application forces us to impose these so small costs, then FBP and SFBP cannot be used. Note these curves depend on Ke. This is because when Kedecreases, FBP and SFBP consider the same number of substituted nodes and their local structures. Contrarily, BP can consider that it is cheaper to delete and insert a node and its local structure than substituting them. Although edges do not have attributes these operations applied on the local structures depend on Ke. Nevertheless, we can consider the sub-optimality problems in these tests are solved at value Kv=12 approximately (the ratio is ≈1). We name this threshold Kvprac. Note that from the theoretical point of view, the minimum acceptable value is Kv=50 since |γv(vap)−γv(viq)|<2·Kv=100. We name this threshold Kvtheor. Therefore, in these experiments we have obtainedKvprac≈14·Kvtheor. This reduction is really important since it shows that the practical minimum value is 25% lower than the theoretical threshold and so FBP and SFBP have more opportunities to be used in practical situations. Finally, in the Jonker–Volgenant plots in Fig. 3, values seem to be random and this is because the maximum difference between these values is lower than 0.01.Fig. 4shows the ratio of runtime given two different combinations of executions. Note that both plots have an opposite point of view. We have taken only two examples two show that the ratio of runtime can increase or decrease when Kv≤Kvprac. But we are not interested on computing the matching algorithm in the domain 0≤Kv≤Kvpracsince we realised the obtained correspondences where much sub-optimal than BP but we want to compute FBP or SFBP in the domain Kv>Kvprac. In the whole 12 experiments shown in Figs. 2 and 3 we realised that the runtime ratio keeps flat when Kv>Kvpracand Ke>Kvprac.Tables 3 and 4show the average runtime in seconds of the previously commented experiments. In Table 3 we set Ke=Kv=Kvprac=12 and in Table 4 we set Ke=Kv=20. Several aspects of these tables can be commented. First of all, the difference between both tables is really low. As commented before, when Kv>Kvpracwe have a constant runtime. Note the large difference on the runtime between clique and degree centralities. Through these experiments, we know degree centrality makes the graph-matching algorithm much faster than clique centrality. Nevertheless, to analyse if it is worth to use clique or degree centralities although this runtime gap, in the next section, we show the differences in the recognition ratio while performing experiments in real databases. Moreover, FBP and SFBP runtime is almost similar and both are lower than BP. Finally, Jonker–Volgenant tends to be faster than Hungarian but FBP with Jonker–Volgenant have convergence problems (marked with --- in Table 3 and 4) and the algorithm does not find the solution in some pairs of graphs.From Tables 3 and 4, we conclude that the fastest combination is SFBP and Jonker–Volgenant in both local structures: clique and degree and also when the order of both graphs is equivalent or the first graph is larger than the second one. Remember that the case where the first graph is smaller than the second one is not explored since in [41] they arrived at the conclusion that this parameterization is always slower than the inverse case. To finish this section, we show in Table 5the Speedup of FBP and SFBP versus BP. That is,TimeBPTimeFBPandTimeBPTimeSFBP. The runtime values are the average of values in Tables 3 and 4. We realise the maximum Speedup is achieved in the cases where the runtime was minimum.In these experiments, we compare BP versus SFBP and we do not use FBP since the synthetic experiments showed us that it obtains similar classification, runtime and Edit costs than SFBP. Moreover, we only have used Jonker–Volgenant solver and the Hungarian method was discarded since we have deducted Jonker–Volgenant is faster than Hungarian and always converges applied on BP and SFBP. Besides, in [50], Jonker–Volgenant is reported to be faster than Hungarian. We have used the following five public graph databases: Letter high, Letter med, Letter low, Grec and Coil Rag. The most common features and details of these databases are summarised in [52]. For instance, the number of graphs in the test, reference and validation sets, the average and maximum number of nodes and edges or the type and number of attributes on nodes and edges.Given a database, if we want to assure BP returns the same distance than SFBP, condition Cvs(vap,viq)≤2·Kvhas to hold for all pairs of graphs composed of a graph in the test set Gpand a graph in the reference set Gpand for all pairs of nodes in these graphs vap∈Gpand viq∈Gq. For this reason, we analysed these five datasets from the point of view of the substitution Edit cost. Table 6shows the main features with respect to Cvsand Fig. 5shows the histogram of node substitution costs Cvsof the whole dataset (red vertical line indicates the mean) of the first four databases (Coil Rag has a similar distribution).Fig. 6shows the recognition ratio using 3-NN and classification runtime of five datasets. The horizontal axis is the node insertion and deletion cost Kv. We have considered 6 different values of Kv:116MeanCvs,18MeanCvs,14MeanCvs,12MeanCvs, Mean{Cvs} (red line) and Kvtheor. Note, only in the last case in which Kv=Kvtheor, the Edit cost restriction imposed to SFBP holds. For simplicity reasons, in the whole experiments Ke=Kv. From the classification runtime plots, we reinforce the conclusion we extracted in the experiments with synthetic graphs, which is that Clique centrality is really much slower than Degree centrality and also that BP is slower than SFBP. Moreover, we cannot deduct any relation between Kvand the graph-matching runtime. The recognition ratio plots show a concave function through the axis Kv=Ke. We call KvmaxRRthe value of Kvsuch that the maximum recognition ratio is achieved. In [54], an optimisation method is presented to achieve this maximum. The most important knowledge that we want to extract from these plots is the value of Kvprac. As commented in the previous section, it is the value of Kvsuch that from this value to zero, SFBP becomes so sub-optimal that obtains lower recognition ratio than BP. Horizontal blue line (dashed red line) indicates the values of Kvsuch that BP obtains higher recognition ratio than SFBP using Degree (Clique) centrality. The right end of these lines indicates Kvprac. We realise in the whole cases these horizontal lines do not have values in KvmaxRR. A blue circle (red square) indicates KvmaxRRachieved using Degree (Clique) centrality. The main conclusion of these experiments is that in the tested datasets Kvprac≤KvmaxRR≤Kvtheor. Therefore, it is worth to use SFBP instead of BP when Kvis validated such that the recognition ratio obtains the maximum value since SFBP achieves similar recognition ratio results than BP but with a reduced runtime. Note that in Fig. 6, we show the recognition ratio but not the mean cost given different values of Kvand the four combinations of algorithms. In fact, we cannot deduct a relation between these two metrics. That is, forcing the cost to be closer to the optimal one does not imply an increase of recognition ratio. Therefore, if we deduct the maximum recognition ratio is achieved at the same point Kvfor both algorithms BP and SFBP, we cannot deduct that, given this specific Kv, the costs are similar.When Kvis very low, BP algorithm deletes and inserts almost the whole nodes but the SFBP algorithm can delete a maximum of |n−m| nodes if n>m or insert them, otherwise. Therefore, SFBP has a maximum number of |n−m| nodes to be deleted or inserted independently of Kv. Note that SFBP with Clique centrality obtains a larger recognition ratio than BP in databases Letter Medium, GREC and COIL RAG in the case that Kvis very low. This is because, in these extreme conditions, SFBP keeps substituting min(n,m) nodes (that most of these substitutions might be correct) but BP substitutes almost non of them.

@&#CONCLUSIONS@&#
Fast Bipartite and Square Fast Bipartite are two new versions of Bipartite algorithm that have recently presented to solve the error-tolerant graph matching. They obtain the same distance than Bipartite with reduced runtime if the insertion and deletion Edit costs on nodes and edges are defined such that they are larger than half of the maximum value of substitution costs on nodes and edges. This theoretical restriction could reduce the applicability of these algorithms since in some recognition applications, the best recognition rate could appear in values that are lower than half of the maximum of substitution costs. The aim of this paper is to show that, in practice, considering insertion and deletion costs lower than this theoretical value does not affect on the optimality of the algorithms nor the recognition ratio.In the practical validation, we have seen that the maximum recognition ratio is achieved in insertion and deletion cost values that are lower than the theoretical threshold. Therefore, the best results are obtained when the theoretical restriction does not hold. This fact could discourage to use these new algorithms. Nevertheless, we have seen that the insertion and deletion costs where the optimality of these algorithms decreases are really much smaller than the values where the recognition ratio is maximised. Therefore, Bipartite, Fast Bipartite and Square Fast Bipartite obtain the same Edit distance and node correspondence in the insertion and deletion values where the recognition ratio is maximised. Considering these experiments, we conclude that we can use these new algorithms using the cost values where the recognition ratio is maximised because there is a considerable speedup.