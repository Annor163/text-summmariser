@&#MAIN-TITLE@&#
Ant Colony Optimization based clustering methodology

@&#HIGHLIGHTS@&#
A novel ACO based methodology (ACO-C) is proposed for spatial clustering.It works in data sets with no a priori information.It includes solution evaluation, neighborhood construction and data set reduction.It has a multi-objective framework, and yields a set of non-dominated solutions.Experimental results show that ACO-C outperforms other competing approaches.

@&#KEYPHRASES@&#
Clustering,Ant Colony Optimization,Multiple objectives,Data set reduction,

@&#ABSTRACT@&#
In this work we consider spatial clustering problem with no a priori information. The number of clusters is unknown, and clusters may have arbitrary shapes and density differences. The proposed clustering methodology addresses several challenges of the clustering problem including solution evaluation, neighborhood construction, and data set reduction. In this context, we first introduce two objective functions, namely adjusted compactness and relative separation. Each objective function evaluates the clustering solution with respect to the local characteristics of the neighborhoods. This allows us to measure the quality of a wide range of clustering solutions without a priori information. Next, using the two objective functions we present a novel clustering methodology based on Ant Colony Optimization (ACO-C). ACO-C works in a multi-objective setting and yields a set of non-dominated solutions. ACO-C has two pre-processing steps: neighborhood construction and data set reduction. The former extracts the local characteristics of data points, whereas the latter is used for scalability. We compare the proposed methodology with other clustering approaches. The experimental results indicate that ACO-C outperforms the competing approaches. The multi-objective evaluation mechanism relative to the neighborhoods enhances the extraction of the arbitrary-shaped clusters having density variations.

@&#INTRODUCTION@&#
Cluster analysis is the organization of a collection of data points into clusters based on similarity [1]. Clustering is usually considered as an unsupervised classification task. That is, the characteristics of the clusters and the number of clusters are not known a priori, and they are extracted during the clustering process. In this work we focus on spatial data sets in which a priori information about the data set (the number of clusters, shapes and densities of the clusters) is not available. Finding such clusters has applications in geographical information systems [2], computer graphics [3], and image segmentation [4]. In addition, clusters of spatial defect shapes provide valuable information about the potential problems in manufacturing processes of semiconductors [5,6].We consider spatial clustering as an optimization problem. Our aim is to obtain compact, connected and well-separated clusters. To the best of our knowledge, there is not a single objective function that works well for any kind of geometrical clustering structure. Therefore, we first introduce two solution evaluation mechanisms for measuring the quality of a clustering solution. The main idea behind both mechanisms is similar, and each mechanism is based on two objectives: adjusted compactness and relative separation. The first objective measures the compactness and connectivity of a clustering solution, and the second objective is a measure for separation. The difference between the two mechanisms is the degree of locality addressed in the calculations. The main advantage of these objectives is that the length of an edge is evaluated relatively, that is, it is scaled relative to the lengths of other edges within its neighborhood. This scaling permits us to evaluate the quality of the clustering solution independent of the shape and density of the clusters.We implement the proposed solution evaluation mechanisms in a clustering framework based on Ant Colony Optimization (ACO). In order to find the target clusters, we use two complementary objective functions (adjusted compactness and relative separation) in a multiple-objective context. Hence, the output of ACO-C is a set of non-dominated solutions. Different from the literature, we are not interested in finding all non-dominated solutions or the entire Pareto efficient frontier. ACO-C has two pre-processing steps: neighborhood construction and data set reduction. Neighborhood construction extracts the local connectivity, proximity and density information inherent in the data set. Data set reduction helps reduce the storage requirements and processing time for the clustering task. Our experimental results indicate that ACO-C finds the arbitrary-shaped clusters with varying densities effectively, where the number of clusters is unknown.Our contributions to the literature are as follows:1.The proposed solution evaluation mechanisms allow us to quantify the quality of a clustering solution having arbitrary-shaped clusters with different densities in an optimization context. The use of these evaluation mechanisms is not restricted to ACO. They can be used in other metaheuristics and optimization-based clustering approaches.The proposed ACO-based methodology introduces a general, unified framework for the spatial clustering problem without a priori information. It includes the solution evaluation mechanism, extraction of local properties, data set reduction, and the clustering task itself.ACO-C is a novel methodology for the clustering problem in which there is no a priori information, that is,•the number of clusters is unknown,clusters may have arbitrary shapes,there may be density variations within the clusters, anddifferent clusters may have density differences.We provide the related literature in Section 2. Section 3 introduces the solution evaluation mechanisms. The details of ACO-C are explained in Section 4. Section 5 is about the empirical performance of ACO-C. First, we set the algorithm parameters using a full factorial design. Then, we compare ACO-C with some well-known algorithms. Finally, we conclude in Section 6.The clustering algorithms can be classified into partitional, hierarchical, density-based algorithms, and metaheuristics (simulated annealing, tabu search, evolutionary algorithms, particle swarm optimization, ACO, and so on). [1,7,8] provide comprehensive reviews of clustering approaches.In this section, we present the related literature on the solution evaluation mechanisms and ant-based clustering algorithms.A good clustering solution has compact and connected clusters that are well-separated from each other. However, quantifying and measuring the clustering objectives (compactness, connectivity and separation) for a data set is not a trivial task. We review the solution evaluation mechanisms in the literature under four categories: partitional approaches, graph-based approaches, clustering validity indices, and multi-objective approaches.Partitional approaches consider objective functions such as minimization of total variance/distance between all pairs of data points, or minimization of total variance/distance between data points and a cluster representative such as k-means [9,10] or k-medoids [11]. In these approaches, the number of clusters needs to be given as input, and the resulting clusters have spherical or ellipsoid shapes in general.In order to handle the data sets with arbitrary-shaped clusters and density variations, graph-based approaches are proposed. Objective functions used are minimization of the maximum edge length in a cluster, maximization of the minimum/maximum/average distance between two clusters, and so on [12,13]. A typical complication for such objective functions is illustrated in Fig. 1(a). In Fig. 1(a) the maximum edge length within the spiral clusters is larger than the distance between these two clusters. In this case elimination of the longest edge causes division of the spiral clusters.Another research stream in solution evaluation makes use of cluster validity indices. Validity indices are used to quantify the quality of a clustering solution and to determine the number of clusters in a data set [14,15]. In an effort to find the target clusters, some researchers use validity indices as objective functions in genetic algorithms [16–21]. However, most of the validity indices assume a certain geometrical structure in the cluster shapes. When a data set includes several different cluster structures, such as arbitrary shapes and density differences, these indices may fail. An example is provided in Fig. 1(b). The clustering solutions generated by DBSCAN [22] are evaluated using Dunn index [23] with different MinPts settings (within a range of 1–15). The number of clusters found with each setting is shown, e.g. 30 clusters are found when MinPts is set to two. Dunn index measures the minimum separation to maximum compactness ratio, so a higher Dunn index implies better clustering. Although the highest Dunn index (0.31) is achieved for the solutions with two and four clusters, the target solution has three clusters with a Dunn index of 0.09. Hence, Dunn index is not a proper objective function for such a data set.Maulik and Bandyopadhyay [24] evaluates the performance of three clustering algorithms, namely k-means, single-linkage, and simulated annealing (SA) by using four cluster validity indices, namely Davies-Bouldin index, Dunn index, Calinski-Harabasz index, and index I. Compared to other validity indices, index I is found to be more consistent and reliable in finding the correct number of clusters. However, the four cluster validity indices are limited to extracting spherical clusters only. To handle different geometrical shapes, Bandyopadhyay et al. [25] uses a point symmetry-based distance measure in a genetic algorithm. The algorithm has difficulty in handling asymmetric clusters and density differences within a cluster.Since a single objective is often unsuitable to extract target clusters, multi-objective (MO) approaches are considered to optimize several objectives simultaneously. To the best of our knowledge, VIENNA [26] is the first multi-objective clustering algorithm, which is based on PESA [27]. It optimizes two objective functions, total intra-cluster variance and connectedness. However, the algorithm requires the target number of clusters. One of the well-known MO clustering algorithms is the multi-objective clustering with automatic k-determination (MOCK) [28]. MOCK is based on evolutionary algorithms, and uses compactness and connectedness as two complementary objective functions. It can detect the number of clusters in the data set. The output of the algorithm is a set of non-dominated clustering solutions. However, it is capable of finding well-separated clusters having hyperspherical shapes. Improvements in this algorithm and its applications have been investigated [29,30]. Saha and Bandyopadhyay [31] also considers the clustering problem in a multi-objective framework. They optimize Xie-Beni (XB) index [32] and Sym-index [21] simultaneously, and introduce a multi-objective SA algorithm. This work is also limited to finding symmetric clusters. Saha and Bandyopadhyay [33] proposes several connectivity-based validity indices based on the relative neighborhood graph. In addition to Sym-index and index I, [34] uses one of the connectivity-based validity indices in [33] as the third objective. Adding this connectivity measure helps extraction of arbitrary shapes and asymmetric clusters.There are additional solution approaches proposed for MO clustering such as differential evolution [35,36], immune-inspired method [37], and particle swarm optimization [38]. In these studies clustering objectives are either cluster validity indices such as XB index, Sym-index and FCM index, or compactness-connectivity objectives as in [28].To the best of our knowledge, [39] is the only study that applies ACO to MO clustering problem. In this algorithm, there are two ant colonies working in parallel. Each colony optimizes a single objective function, either compactness or connectedness. The number of clusters is required as input. In addition, they test the proposed algorithm using the Iris data set only.ACO was introduced by [40–42]. It is inspired from the behaviors of real ants. As ants search for food on the ground, they deposit a substance called pheromone on their paths. The concentration of the pheromone on the paths helps direct the colony to the food sources. Ant colony finds the food sources effectively by interacting with the environment. Solution representation, solution construction and pheromone update mechanisms are the main design choices of ACO.In the clustering literature, several ant-based clustering algorithms have been proposed. For a comprehensive review about ant-based and swarm-based clustering one can refer to [43]. In this study, we categorize the related studies into three: ACO-based approaches, approaches that mimic ants’ gathering/sorting activities, and other ant-based approaches.ACO-based approaches [44–52] are built upon the work of [42]. In these studies the total intra-cluster variance/distance is considered as the objective function, and the number of clusters is required a priori. An ant constructs a solution by assigning a data point to a cluster. The desirability of assigning a data point to a cluster is represented by the amount of pheromone. Ants update the pheromone in an amount proportional to the objective function value of the solution they generate. The proposed algorithms are capable of finding the clusters with spherical and compact shapes. There are also hybrid algorithms using ACO [53–55]. For instance, Kuo et al. [53] modifies the k-means algorithm by adding a probabilistic centroid assignment procedure. Huang et al. [54] introduces a hybridization of ACO and particle swarm optimization (PSO). In this approach, PSO helps optimize continuous variables, whereas ACO directs the search to promising regions using the pheromone values. Another hybrid algorithm that combines k-means, ACO and PSO is [55]. In [55] the cluster centers obtained by ACO and PSO are used to initialize k-means. In these hybrid studies the number of clusters is required as input, and the resulting clusters are compact and spherical.The approaches that mimic ants’ gathering/sorting activities [56] form another research stream. ACO uses an explicit objective function whereas these approaches have an implicit objective function, and clusters emerge as the result of the gathering/sorting activities. An ant picks up a point in the space and drops it off near the points that are similar to it. These picking up and dropping off operations are performed using the probabilities that are calculated based on the similarity of the points in the neighborhood. Hence, ants work as if they are forming a topographic map. After forming this pseudo-topographic map, a cluster retrieval operation is applied to find the final clusters. Lumer and Faieta [57] generalizes this method for exploratory data analysis. [58,48,59–64] are the extensions and modifications of the algorithms proposed by [55,56]. For instance, Yang and Kamel [59] uses parallel and independent ant colonies aggregated by a hypergraph model. In [64] the local similarity of a point is measured by entropy rather than distance.Other ant-based approaches use the emergent behavior of the ants. Azzag et al. [65] introduces a hierarchical ant-based algorithm to build a decision tree. Ants move a data point close to the similar points and away from the dissimilar ones on the decision tree. In [66,67] ants generate tours by inserting edges between data points. Pheromone is updated for each edge connecting a pair of points. The closer the distance between two points, the larger the amount of pheromone released. In the first phase of the algorithm, the edges between similar points become denser in terms of pheromone concentration. The next phase is the cluster retrieval process by using a hierarchical clustering algorithm. Ghosh et al. [68] introduces a clustering approach based on aggregation pheromone. Aggregation pheromone leads the data points to accumulate around the points with higher pheromone density. In order to obtain the desired number of clusters, merging operations are performed using the average-linkage agglomerative hierarchical clustering algorithm. Another ant-based clustering algorithm is the chaotic ant swarm optimization proposed by Wan et al. [69]. It combines the chaotic behavior of a single ant and self-organizing behavior of the ant colony. Given the number of clusters, the proposed approach optimizes the total intra-cluster variation. Although it provides some improvement over PSO and k-means, the resulting clusters are still spherical.Our aim is to obtain compact, connected and well-separated clusters. For this purpose, we introduce two solution evaluation mechanisms: Clustering Evaluation Relative to Neighborhood (CERN) and Weighted Clustering Evaluation Relative to Neighborhood (WCERN).This objective is built upon the trade-off between the connectivity and relative compactness:(a)Connectivity: Basically, connectivity is the degree to which neighboring data points are placed in the same cluster [26,28]. Then, we first need to define the neighborhood of a point. There are several neighborhood construction algorithms such as k-nearest neighbors (KNN), ɛ-neighborhood [22], NC algorithm [70], and so on. When there are arbitrary-shaped clusters with density differences, NC outperforms KNN and ɛ-neighborhood. It provides a unique neighborhood for each data point. NC also generates subclusters (closures), which are formed by merging the data points having common neighbors. These closures can be used as the basis of a clustering solution. For these reasons, we use the NC algorithm to determine the neighborhoods of individual data points.Let Cmand Clpbe the sets of points in cluster m and closure p, respectively. Connectivity of cluster m with respect to closure p is connectmp=|Cm∩Clp|/|Clp| if Cm∩Clp≠∅. In the ideal case, connectivity takes a value of one, which means that cluster m and closure p fully overlap. The connectivity of cluster m is calculated asconnectm=∏p=1ncconnectmp,where nc is the total number of closures. In this calculation, if Cm∩Clp=∅, then closure p is part of a cluster other than m, and, in this case, we take connectmp=1 so that the value of connectmis not affected by such unrelated closure and cluster pairs. Merging multiple closures that are in the same cluster results in a connectivity value of one, whereas it is less than one when there are divided clusters.Relative compactness: We define the relative compactness of cluster m as the most inconsistent edge within its neighborhood. In relative compactness calculation we consider the edges in the minimum spanning tree (MST) of a cluster. MST is a graph in which the sum of the edge lengths is the minimum, and the graph is connected with no cycles. These two properties together allow us to define compactness of a cluster in an efficient way. Then, we compare each edge in the MST with the edges in the neighborhood. More formally, relative compactness of cluster m isr_comp_cm=max(i,j)∈MSTmdijmax(k,l)∈MSTm(i)or(k,l)∈MSTm(j){dkl}where (i,j) is the edge between points i and j, dijis the Euclidean distance between points i and j, MSTmand MSTm(i) are the sets of edges in the MST of the points in cluster m and in the neighborhood of point i in cluster m, respectively.When the number of clusters increases, relative compactness improves (decreases) whereas the connectivity deteriorates (decreases). Combining connectivity and compactness, adjusted compactness of cluster m is obtained as compm=r_comp_cm/connectm. The overall compactness of a clustering solution is found asmaxm{compm}.A good clustering solution must have well-separated clusters. We define the relative separation based on the local properties of clusters. Let the nearest cluster to cluster m be n such that (m(i*), n(j*))=argmin{dij:i∈Cm, j∈Cn, m≠n}. The relative separation of cluster m isr_sep_cm=mindm(i*),n(j*)max(k,l)∈MSTm(i*)if|Cm|>1or(k,l)∈MSTn(j*)if|Cn|>1{dkl},if |Cm|>1or|Cn|>11,otherwise.The overall separation of a clustering solution isminm{r_sep_cm}.CERN minimizes the adjusted compactness and maximizes the relative separation.WCERN is similar to CERN; both compactness and separation are calculated relative to the neighborhoods. The only difference between CERN and WCERN is that the edge lengths are used as a weight factor in compactness and separation calculations in WCERN. Hence, relative compactness and relative separation of cluster m are calculated asr_comp_wm=max(i,j)∈MSTmdij2max(k,l)∈MSTm(i)or(k,l)∈MSTm(j){dkl}andr_sep_wm=mindm(i*),n(j*)2max(k,l)∈MSTm(i*)if |Cm|>1or(k,l)∈MSTn(j*)if |Cn|>1{dkl},if|Cm|>1or|Cn|>1dm(i*),n(j*),otherwise.Similar to CERN, WCERN minimizes the adjusted compactness, and maximizes the relative separation.ACO-C is a clustering methodology in a multi-objective framework. It has two pre-processing steps: neighborhood construction and data set reduction. Neighborhood construction helps extract the local information inherent in the data set. This local information is used in the solution evaluation. Data set reduction ensures the scalability of the approach.In ACO-C an ant is a search agent. Ants construct tours by inserting edges between pairs of data points. Connected points in a tour form a cluster. During edge insertion each point is connected to exactly two points. This makes it easier to extract arbitrary-shaped clusters and reduces computational requirements.The outline of the ACO-C methodology is presented in Fig. 2where max_iter and no_ants denote the maximum number of iterations and the number of ants, respectively.•Step 0. Pre-processing.We construct the neighborhood of each data point and obtain closures (subclusters) using the NC algorithm [70]. NC closures have two properties: (1) A closure is either a cluster itself or a subset of a cluster (divided cluster). (2) There may be an outlier mix on the boundary of a closure. Hence, we focus on the merging operations and outlier detection in the clustering. In order to allow outlier detection and closure merging, we extend NC neighborhoods with distant neighbors and nowhere. Distant neighbors are the nearest pair of data points between two adjacent closures. Nowhere is a dummy point used for outlier detection. If a data point is connected to nowhere twice, then it is classified as an outlier. If a data point is connected to nowhere once, then it is the start/end point of a cluster. An example for neighborhood definition is provided in Fig. 3. Points j, k, l, m and n are neighbors of point i generated by NC, and point p is the distant neighbor of point i. Neighborhood of point i is extended by point p and nowhere. Note that not every point has a distant neighbor.The interior points of a closure are already connected, hence it is sufficient to consider only the points on the boundaries of the closures for merging and outlier detection. Exclusion of interior points in a closure decreases the number of points in a data set and contributes to the scalability of ACO-C. We use the boundary extraction algorithm in [71].•Step 1. Initialization of parameters.The parameters of ACO-C, including the number of ants (no_ants), the number of maximum iterations (max_iter), and the evaporation rate (ρ) are initialized. We conduct a factorial design in Section 5 to determine the values of these parameters.Step 2. Solution construction.When an ant starts clustering, the set of unvisited points, Do, is initialized as the entire data set, D. For each point in the data set, the set of currently available neighbors, NCSi, is initialized as the set of its neighbors, NSi. The current number of clusters (m) is set to one. There are two substeps in solution construction: point selection and edge insertion.Point selection: Every time an ant starts a new tour, a new cluster is formed. When a new cluster, Cm, is initialized, a point, say point i, is selected at random from the set of unvisited points, Do. Then, the related sets are updated as Cm=CmU{i}, Do=Do/{i}, and NCSk=NCSk/{i} for ∀k∈Do. If NCSiis non-empty or point i is not nowhere, we continue with edge insertion. Otherwise, the construction of the current cluster is finished, and a new cluster is initialized by incrementing the cluster index, m, by 1.Edge insertion: An ant inserts an edge between point i and a point selected from NCSi. The pheromone concentration on edge (i,j), τij, represents the tendency of edge (i,j) to occur in a cluster. Hence, the probability of selecting edge (i,j) is calculated aspij=τij/∑k∈NCSiτikfor ∀j∈NCSi. Then, the ant continues edge insertion starting from point j. The initial pheromone concentration is inversely proportional to the evaporation rate, τij=1/ρ for ∀i∈D, j∈NSi.Point selection and edge insertion substeps are repeated until Do is empty. The details of Step 2 are presented in Fig. 4.Step 3. Solution evaluation.The performance of a clustering solution is evaluated using CERN and WCERN as described in Section 3.Step 4. Local search.In order to strengthen the exploitation property of ACO-C we apply local search to each clustering solution constructed. Conditional merging operations are performed in the local search. Let clusters m and n are adjacent clusters considered for merging, and let comp and sep be the adjusted compactness and relative separation of the current clustering solution, respectively. The adjusted compactness and relative separation after merging are comp’ and sep’, respectively. If comp’≤comp and sep’≥sep, clusters m and n are merged. The clustering solutions at the end of the local search form the set of solutions constructed by the ants (SC) in the current iteration.Step 5. Pheromone update.Pheromone update is performed for each solution component (edge) so that the effect of the solution component is well-reflected in the pheromone concentration.There are two important properties about our clustering problem: (1) We are interested in arbitrary-shaped clusters with different densities, so reflecting the local density, connectivity and proximity relations are crucial in finding the target clusters. (2) We use adjusted compactness and relative separation as two complementary objective functions. Hence, we use the following pheromone update mechanism. For each data point i, the incumbent (minimum) adjusted compactness obtained so far for point i, inc_compi, and the incumbent (maximum) relative separation obtained so far for point i, inc_sepi, are kept in the memory. We check whether or not the adjusted compactness and relative separation of the cluster to which edge (i,j) belongs are better than the corresponding incumbent values. More pheromone is released if an incumbent improves.For all the edges in the clustering solution, E, the amount of pheromone released is proportional to the amount of improvement in the incumbents. The initial incumbent adjusted compactness and relative separation for each point are taken from the closures of the NC algorithm. Formally, the pheromone values are updated asτij=(1−ρ)τij+ρwijτij∀i∈D,j∈NSi,wherewij=min{inc_compi,inc_compj}comp(i,j)+sep(i,j)max{inc_sepi,inc_sepj},if(i,j)∈E0,otherwise.Step 6. Non-dominated set update.Let s1 and s2 be two clustering solutions generated by the ants. The aim is to minimize the maximum adjusted compactness and to maximize the minimum relative separation.Definition 1Solution s1 dominates solution s2 ifcomps1<comps2 and seps1≥seps2, orcomps1≤comps2 and seps1>seps2.Definition 2If there does not exist any other clustering solutions dominating solution s1, then solution s1 is called a non-dominated solution.We update the current set of non-dominated solutions (SN) at the end of each iteration using Definitions 1 and 2. We also update the incumbent compactness and separation (inc_compiand inc_sepi) for the data set.If the maximum number of iterations is not exceeded, Steps 2–6 are repeated. Otherwise, ACO-C terminates with the non-dominated solutions in set SN.In this section, we test the performance of ACO-C empirically. First, we present the test data sets and the performance evaluation criteria. Second, using some pilot data sets, we conduct a full factorial experiment in order to set the ACO-C parameters. Third, we elaborate on the impact of data set reduction. Finally, we compare the performance of ACO-C with other clustering algorithms.The algorithm was coded in Matlab 7.9 and run on a PC with Intel Core2 Duo 2.33GHz processor and 2 GB RAM.We tested ACO-C using 32 data sets compiled from several sources [72–74]. These include 2- and higher dimensional data sets with various shapes of clusters (circular, elongated, spiral, etc), intra-cluster and inter-cluster density variations, and outliers. Some example data sets are presented in Fig. 5.We evaluated the accuracy of the clustering solution using Jaccard index (JI) and Rand index (RI). We define these measures as follows:a: the number of point pairs that belong to the same target cluster and are assigned to the same cluster in the solution.b: the number of point pairs that belong to the same target cluster but are assigned to different clusters in the solution.c: the number of point pairs that belong to different target clusters but are assigned to the same cluster in the solution.d: the number of point pairs that belong to different target clusters and are assigned to different clusters in the solution.JI is one of the well-known external clustering validity indices. It takes values between zero and one, one indicating the target clustering is achieved. RI is also known as the simple matching coefficient. While JI focuses on point pairs correctly assigned to the same cluster, RI also takes into account the point pairs correctly assigned to different clusters. Both indices penalize the division of clusters as well as mixing them. We report the maximum JI and RI values in the set of non-dominated solutions.The three parameters of ACO-C are no_ants, max_iter, and ρ. We set max_iter to twice the number of points in the data set, and recorded the iteration number in which the target clustering was found. We used a full factorial experimental design in order to determine the best settings for no_ants and ρ. We also studied the impact of the solution evaluation function (EF) on the performance of ACO-C. The three factors used in the experimental design and their levels are presented in Table 1. We conducted the full factorial experiment using a subset of 15 data sets. These 15 data sets were selected to represent different properties of all data sets.Before discussing the full factorial design results, we present the ACO-C results for the example data set given in Fig. 5(d). The three non-dominated clustering solutions found by ACO-C are presented in Fig. 6. These solutions include the target clustering with a JI value of one. The resulting non-dominated solutions can be interpreted as clustering of points in different resolutions.We also checked the convergence of ACO-C in the example data set. In Fig. 7the set of non-dominated solutions stays the same after iteration number 70. This implies that convergence is achieved.The main effect plots for the maximum RI and execution time are presented in Fig. 8. The low setting of the evaporation rate slows the convergence down and prevents ACO-C from missing the target solutions. However, the time spent in ACO-C increases three times with this setting. Increasing the number of ants used in ACO-C provides a slight improvement in the maximum RI in return for increase in the time. Considering the trade-offs between the performance and time, the experiments are performed with the parameter settings ρ=0.01 and no_ants=5.In Fig. 8(a) and (b) WCERN performs better than CERN in terms of the maximum RI and time. On the other hand, only CERN finds the target clustering in some data sets, i.e. data sets in Fig. 5(d) and (e). Hence, CERN ensures finding the target clustering that is visible in low resolution. WCERN is more powerful in extracting clusters that are visible in higher resolution such as data sets in Fig. 5(f) and (h). CERN and WCERN complement each other in finding the target clusters, so we run ACO-C using both evaluation mechanisms. We consider the union of the non-dominated solutions obtained by both as the final solution set.Note that there is no significant interaction among the factors.We tested the impact of data set reduction using 32 data sets. The boundary extraction algorithms in [71] were used for data set reduction. The number of points in the original data set was compared with the number of points after reduction.Table 2shows the data set reduction percentages for 2- and higher dimensional data sets. The reduction percentages vary depending on the shape of the clusters. The highest data set reduction percentages are achieved when clusters are convex, as in Fig. 5(f) and (h). When there are non-convex clusters as in Fig. 5(g), the reduction percentages are lower.In Section 5.4 clustering is performed on the data sets after the reduction.The performance of ACO-C is compared with the results of k-means, single-linkage, DBSCAN, NC closures, and NOM [75]. In our comparisons k-means represents the partitional clustering algorithms, and single-linkage the hierarchical clustering algorithms. DBSCAN is selected as a representative of the density-based clustering algorithms. The number of clusters is an input for k-means and single-linkage, therefore we run k-means and single-linkage for several values of the number of clusters. This number varies between 2 and 10% of the number of points in the data set with increments of 1, and the one with the best JI value is selected for each algorithm. In the same manner, for DBSCAN, among several MinPts settings the one with the best JI value is selected for comparison. NOM is a graph theoretical clustering algorithm. It also uses the neighborhoods constructed by the NC algorithm, hence we can elaborate on the impact of ACO-C better. For ACO-C, we consider the union of the non-dominated solutions obtained with CERN and WCERN settings, and the sum of the execution times with CERN and WCERN is considered as the execution time of ACO-C.The results for 32 data sets are summarized in Table 3. ACO-C finds the target clusters in 29 data sets out of 32. Single-linkage and NOM follow ACO-C with 24 and 18 data sets, respectively. ACO-C has the best average JI and RI values, followed by NOM, NC, DBSCAN and single-linkage. This indicates that ACO-C is able to form clusters that are close to the target clusters on the average. Moreover, the standard deviations of JI and RI are the smallest, and the minimum values of JI and RI are the highest for ACO-C. This indicates that even in the worst case ACO-C performs better than the competing approaches.Typically, ACO-C has difficulty in detecting target clusters when there is noise, as for the data set in Fig. 5(g). The relative solution evaluation mechanisms of both CERN and WCERN are sensitive to density and distance changes, so these points are labeled as separate clusters. Although ACO-C yields the general structure of the target clusters in such data sets, it forms clusters by enclosing the noise as well.The number of non-dominated solutions generated by CERN and WCERN varies between 1 and 6 for different data sets. Hence, the size of the non-dominated sets is reasonable for practical use.The main limitation of ACO-C is the longer execution times compared to k-means, single-linkage and DBSCAN, partly due to the Matlab implementation. In this respect, improvements are required.

@&#CONCLUSIONS@&#
