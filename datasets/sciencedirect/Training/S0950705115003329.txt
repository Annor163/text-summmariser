@&#MAIN-TITLE@&#
Double-quantitative fusion of accuracy and importance: Systematic measure mining, benign integration construction, hierarchical attribute reduction

@&#HIGHLIGHTS@&#
IP-Accuracy is mined by systematic double-quantitative fusion of causality measures.IP-Accuracy GrC integration is constructed to gain benign granulation monotonicity.IP-Accuracy attribute reduction is studied to establish a hierarchical reduct system.

@&#KEYPHRASES@&#
Rough set theory,Granular computing,Attribute reduction,Uncertainty measure,Double quantification,

@&#ABSTRACT@&#
Uncertainty measure mining and applications are fundamental, and it is possible for double-quantitative fusion to acquire benign measures via heterogeneity and complementarity. This paper investigates the double-quantitative fusion of relative accuracy and absolute importance to provide systematic measure mining, benign integration construction, and hierarchical attribute reduction. (1) First, three-way probabilities and measures are analyzed. Thus, the accuracy and importance are systematically extracted, and both are further fused into importance-accuracy (IP-Accuracy), a synthetic causality measure. (2) By sum integration, IP-Accuracy gains a bottom-top granulation construction and granular hierarchical structure. IP-Accuracy holds benign granulation monotonicity at both the knowledge concept and classification levels. (3) IP-Accuracy attribute reduction is explored based on decision tables. A hierarchical reduct system is thereby established, including qualitative/quantitative reducts, tolerant/approximate reducts, reduct hierarchies, and heuristic algorithms. Herein, the innovative tolerant and approximate reducts quantitatively approach/expand/weaken the ideal qualitative reduct. (4) Finally, a decision table example is provided for illustration. This paper performs double-quantitative fusion of causality measures to systematically mine IP-Accuracy, and this measure benignly constructs a granular computing platform and hierarchical reduct system. By resorting to a monotonous uncertainty measure, this study provides an integration-evolution strategy of granular construction for attribute reduction.

@&#INTRODUCTION@&#
Rough set theory (RS-Theory)[35,36,59,63,70,71] represents a fundamental granular computing (GrC) pattern for handling uncertainty issues. The initial Pawlak-Model [35] acts only as a qualitative model, so it lacks the quantitative mechanism regarding fault-tolerance and robustness. Thus, quantitative models exhibit improvements and have applications, and they could in part be unified by the subsethood measure [61]. In particular, the probabilistic rough set (PRS)[1,27,28,30,51,57,58,60,78] introduces the probability uncertainty measure into RS-Theory, which forms the basis of mainstream quantitative models. PRS offers measurability, generality, and flexibility and exhibits a series of concrete models, including the decision-theoretic rough set (DTRS)[64], game-theoretic rough set [1,2], variable precision rough set [81], Bayesian rough set [47], and parameterized rough set [6]. With the exception of PRS, the graded rough set [25,62] depends on the grade measure to become another basic type of quantitative model.Herein, DTRS is introduced as a model example. DTRS utilizes conditional probability and the Bayesian risk decision to establish three-way decisions and threshold-quantitative semantics [64]. As a result, DTRS improves upon some basic models and provides a quantitative exploration platform. In terms of relevant studies, three-way decisions were analyzed in [17–19,58,60,79]; model development and threshold calculation were discussed in [15,16,27,45,48]; attribute reduction was studied in [10,12,31,65,74,75]; and model applications (regarding clustering, regression, and semi-supervised learning) were addressed in [13,23,24,26,66]. In fact, three-way decisions have been expanded into three-way decision theory, and this fundamental theory has been the subjectofextensive study and used in a number ofapplications [5,7,14,17–19,29,55,58,60,67,69,79]. In particular, shadowed sets offer useful insight into the three-way decision mechanism and are thus related in some way to three-way decision theory [38,40,41,80].Note that all quantitative models depend on underlying uncertainty measures. For RS-Theory, uncertainty measures underlie to a large extent quantitative applications, so their mining and applications have become a fundamental subject [4,6,8,21,34,43,49,51,61,76,78]. In fact, ratio|[x]∩X||[x]|acts as the core measure in the bottom approximate space (Approx-Space), and it corresponds to rough membership [37], probability [64,72], misclassification degree [81], and precision [73,77]. The extensively used|[x]∩X||[x]|is essentially a relative measure to exhibit information concentration. In contrast, a sort of absolute measure also exists to manifest data vividness, for example, the grade measure. Relative and absolute measures adopt different quantitative views for measurement, thus underlying quantitative applications (especially those regarding the approximation/error). Usually, both hold heterogeneity and complementarity, and thus, each relies on its essential benefit to occupy its own dominant environment. Therefore, their double-quantitative integration/fusion exhibits systematicness and superiority for gaining efficient applications. For this important topic, relative probability/precision and absolute grade were utilized for the double-quantitative information architecture and model construction [15,72,73,77]. In this paper, three-way probabilities and measures in Approx-Space are analyzed from the perspective of probability statistics and application semantics, respectively; then, relative accuracy and absolute importance – two causality measures – are systematically extracted to implement double-quantitative fusion. Thus, we systematically mine a synthetic causality measure, importance-accuracy (IP-Accuracy), which is verified as being benignly monotonous in its further granular integration construction.Attribute reduction holds optimization and generalization to underlie practical applications regarding knowledge discovery and data mining. Thus, it is always an essential subject in RS-Theory [3,11,20,31,42,50,72,75]. Classical qualitative Pawlak-Reduction mainly depends on the classification-positive region (C-POS) and granulation monotonicity. However, quantitative C-POS change usually exhibits non-monotonicity and further uncertainty. Thus, quantitative model-based attribute reduction transcends basic qualitative reduction, exhibiting several reduction anomalies [32,52,65]. The difficulty of this reduction approach originates from the constructional region complexity in quantitative expansion. To address this challenging topic, DTRS-Reduction was researched in [10,12,31,65,74,75], and relevant hierarchical reduction theory was established in [74,75]. Note that uncertainty measures apply to attribute reduction, especially metrical reduct construction and heuristic algorithm development. In particular, information reduction is performed in entropy theory [31,49,76]. By virtue of the metrical merit, uncertainty measure-based attribute reduction has substantial research potential and application prospects. In contrast, the difficulty of metrical reduction technology mainly exists in semantics-based measure mining, metrical hierarchical construction, and granulation monotonicity discovery. In this paper, the synthetic causality measure IP-Accuracy first performs gradual granular integration; then, IP-Accuracy with its inherent monotonicity is fully utilized to establish a hierarchical reduct system, including qualitative/quantitative reducts, tolerant/approximate reducts, reduct hierarchies, and heuristic algorithms.GrC [22,68] is one of the most powerful structural methodologies and can effectively process hierarchical information by utilizing its trialistic characteristics, i.e., multiple granules, levels, and perspectives. GrC has been the focus of much study as a result of information granulation [39,41,44,54,56]; in particular, GrC was also deeply concerned with RS-Theory [9,16,21,33,45,46,53,63,73,77]. In fact, RS-Theory is a fundamental GrC pattern; thus, GrC could effectively apply to RS-Theory, including double quantification and attribute reduction mentioned above. (1) Double quantification refers to the initial dual viewpoint and further multiple granules/hierarchies, thus adhering to GrC. In this sense, this paper utilizes GrC to perform double-quantitative fusion mining of causality measures. (2) Attribute reduction mainly depends on knowledge granulation. The latter is a sort of hierarchical transformation of knowledge structures, so it leads to structural coarsening, granular merging, regional uncertainty, and informational monotonicity. Thus, knowledge granulation closely follows GrC, while GrC could thoroughly probe the essence of attribute reduction. Focusing on IP-Accuracy, the objective of this paper is to make a novel bottom-top GrC construction: a hierarchical integration-evolution from micro bottom Approx-Space to macro top attribute reduction. In particular, the challenges posed by the granulation monotonicity of IP-Accuracy and the relevant equality conditions are the emphasis of this research, and they are the foundation of the hierarchical reduct system.According to the above, this paper mainly utilizes GrC to investigate the double-quantitative fusion of relative accuracy and absolute importance, and it provides systematic measure mining, benign integration construction, and hierarchical attribute reduction. Thus, there are four relevant parts. (1) Three-way probabilities and measures are analyzed. Then, accuracy and importance are systematically extracted to perform double-quantitative analysis and fusion, and thus IP-Accuracy is produced. (2) IP-Accuracy implements a bottom-top GrC construction. The relevant granulation monotonicity and hierarchical structure are revealed. (3) IP-Accuracy attribute reduction is explored based on decision tables. A hierarchical reduct system is thereby established, including qualitative/quantitative reducts, reduct hierarchies, and heuristic algorithms. (4) Finally, a decision table example is provided for relevant illustration. This paper performs double-quantitative fusion of causality measures to systematically mine IP-Accuracy, and this measure benignly constructs a GrC platform and hierarchical reduct system. By resorting to a monotonous uncertainty measure, this study provides an integration-evolution strategy of granular construction for attribute reduction. In particular, IP-Accuracy and its reducts hold benign monotonicity and statistical systematicness; accordingly, the novel tolerant and approximate reducts quantitatively approach/expand/weaken the ideal qualitative reduction by a direct and statistical mean.This paper is organized as follows. Section 2 is devoted to IP-Accuracy’s systematic mining; in Section 3, the GrC for IP-Accuracy is constructed; Section 4 establishes attribute reduction for the measure; Section 5 provides a relevant illustration, and Section 6 concludes this paper. First, the main abbreviations in this paper are listed in Table 1.This section performs systematic mining to acquire a benign uncertainty measure: importance-accuracy (IP-Accuracy), which informs later sections regarding GrC construction and attribute reduction.For in-depth mining, this subsection systematically inspects Approx-Space’s metrics from the probability statistics and application semantics. Thus, three-way probabilities and measures produce; after comparative analyses, two causality measures naturally emerge. Herein, the narrow measure only applies to three-way measures (i.e., accuracy, importance, and generality).Universe U is non-empty and finite. Equivalence relation R corresponds to partitionU/R={[x]R:x∈U}, and U/R (or mere R) means knowledge. Thus, (U, R) constitutes Approx-Space, where [x]Ris the basic granule. Furthermore, given conceptX⊆U,0.35em0ex(U,R,X)is called the bottom system (Bottom-System). From the causality mechanism, granule [x]Rand concept X represent the factor and result, respectively.Lemma 2.1.1∀S ∈ 2U, letp(S)=|S||U|, then (U, 2U, p) constitutes a probability space.Probability space (U, 2U, p) underlies Bottom-System (U, R, X), especially from the mathematical statistics perspective. For example, conditional probability p(X|[x]R) could be naturally well-defined:p(X|[x]R)=p(X∩[x]R)p([x]R)=|X∩[x]R|/|U||[x]R|/|U|=|X∩[x]R||[x]R|.Definition 2.1.2Three-Way ProbabilitiesIn Bottom-System(U,R,X),0.35em0ex∀[x]R∈U/R,(2){p(X|[x]R)=|X∩[x]R||[x]R|,p([x]R|X)=|[x]R∩X||X|,p([x]R)=|[x]R||U|,mean the likelihood, posterior, and prior probabilities, respectively.Three-way probabilities exhibit different probability semantics and causality connotation. Prior p([x]R) solely measures cause uncertainty of knowledge R. Conditional probabilities exhibit two forms. For the decisiveness mechanism, likelihood p(X|[x]R) and posterior p([x]R|X) reflect causality in the cause-to-effect and effect-to-cause directions, respectively; thus, both directly describe the correlative relationship between the granule and concept. For the descriptive style, they exhibit relativity and absoluteness, respectively. In fact, the likelihood probability depends on information concentration to express a sort of relative possibility regarding causality; in contrast, the posterior probability depends on data directness to express a sort of absolute possibility regarding causality.Three-way probabilities originate from the theoretical statistics framework of probability space (U, 2U, p). In fact, these metrics can also be directly described from the applicable semantics perspective.Definition 2.1.3Three-Way MeasuresIn Bottom-System(U,R,X),0.35em0ex∀[x]R∈U/R,(3){aX([x]R)=|[x]R∩X||[x]R|,iX([x]R)=|[x]R∩X||X|,g([x]R)=|[x]R||U|,are called granule [x]R’s accuracy, importance, and generality regarding concept X, respectively.In essence, three-way measures correspond to three mappings from Approx-Space to the one-dimensional line [0, 1], where concept X may be the mapping parameter. From discriminative perspectives, they hold inherent semantics in Bottom-System (U, R, X).(1)As a granular local property, accuracy describes granular relative precision for concept X.As a granular global property in concept X, importance reflects the granular absolute contribution for X.As a granular global property in universe U, generality reflects a sort of granular absolute scale.In particular, the measure in Bottom-System usually has three basic semantics functions, i.e., (1) measure the granule by the concept; (2) measure the concept by the granule; (3) set up a sort of confidence for a causality reasoning rule from the granule to concept. The three measurement clues have high coordination, and Items (1) (3) are extensively used. In the above discussions, accuracy and importance perform their granular descriptions. For the rule confidence, accuracy aX([x]R) and importance iX([x]R) provide a sort of relative and absolute confidence for causality rule[x]R0.25em0ex⇒0.25em0exX: IF x ∈ [x]RTHEN x ∈ X, respectively.Thus far, three-way probabilities and measures are established from the probability statistics and application semantics, respectively. Next, they are contrastively summarized in Table 2.Along the probability clue, DTRS produces with the likelihood probability and Bayesian decision [58,60,64]. Moreover, we have investigated three-way weighted entropy and three-way attribute reduction [76], which systematically integrate and hierarchically promote three-way probabilities. In particular,|[x]R∩X||[x]R|and|[x]R∩X||X|adopt different views to hold heterogeneity and complementarity for causality inference. Therefore, their system and fusion are worth analyzing for more powerful rough reasoning. Next, the two topics are successively discussed, mainly in terms of the measure approach.This subsection analyzes accuracy and importance, two causality measures, in a mathematical plane as well as their relationship from a systematic viewpoint.In Bottom-System(U,R,X),0.35em0ex|[x]R|and |[x]R∩X| act as the direct and basic cardinality; their relevant measure variablesVandWconstruct a fundamental two-dimensional plane(V,W)(Fig. 1) with the triangle range. In Fig. 1, accuracy and importance correspond to the slope and contraction, respectively. Next, letA=aX([x]R),0.35em0exI=iX([x]R). Herein, the factotum is mainly utilized to highlight relevant variables.Proposition 2.2.1(1)A=WV,∂A∂V=−WV2≤0,∂A∂W=1V>0,dA=−WV2dV+1VdW.I=W|X|,dIdW=1|X|>0,dI=1|X|dW.For system(V,W), accuracyAand importanceIbecome binary and univariate functions, respectively. Thus, their derivative and differential are provided by continuous expansion; furthermore, accuracy and importance exhibit monotonicity regarding the single variable.Accuracy and importance are independent. Furthermore, their application system(A,I)is two-dimensional and so becomes complete for the quantitative Approx-Space.AandIhave no mutual determination/derivation relationships. In fact,IA=|[x]R||X|is not a constant for knowledge granules. Hence, accuracy and importance are independent, and their system(A,I)is two-dimensional. As a result,(A,I)becomes complete for the quantitative Approx-Space because the latter is two-dimensional and its base is related to(V,W).□Because of the independence, accuracy and importance hold heterogeneity, which could also be explained by their relativity and absoluteness. Furthermore, they have quantitative completeness and complementarity for the quantitative Approx-Space. Thus, they directly constitute a system via Cartesian product integration. Clearly, the measure system(A,I)adheres to Approx-Space’s inherent quantitative essence, thus implying essential effectiveness and adequate performance.Proposition 2.2.3IA=V|X|.(1)A=IV|X|,∂A∂I=|X|V>0,∂A∂V=−IV2|X|≤0,dA=|X|VdI−IV2|X|dV.I=AV|X|,0.35em0ex∂I∂A=V|X|>0,∂I∂V=A|X|>0,dI=V|X|dA+A|X|dV.In Proposition 2.2.3, accuracy and importance are systematically inspected. Although they never hold mutual determination, their basic non-linear relationship is constructed byV, i.e.,IA=V|X|. Thus, the accuracy change depends on importance, while the importance change depends on accuracy. The change dependency becomes decisive ifVis unchanged.Theorem 2.2.4System EquivalenceMeasure system(A,I)and cardinality system(V,W)have a non-linear transformation and thus become equivalent.{A=WV,I=W|X|.0.25em0ex⇔0.25em0ex{V=IA|X|,W=I|X|.The above equivalence verifies the non-linear transformation between two systems(A,I)and(V,W).□There are two bearing systems for accuracy and importance. The cardinality system originates from the definition of both measures and thus approaches Approx-Space; in contrast, the measure system mainly cares for both measures’ inherent systematicness, thus exhibiting direct applicability. In fact, the two systems become equivalent, although both have different emphases.In Approx-Space, relative accuracy is extensively utilized [37,72,73,77,81]. However, its direct concentration also implies its simplicity and singleness. In view of the heterogeneity and complementarity from relativity and absoluteness, double-quantification [15,72,73,77] advocates measure diversity and superiority; thus, this suggests adding or fusing absolute information on the basis of relative information. In fact, importance provides the absolute estimation regarding causality; hence, accuracy and importance’s double-quantitative fusion is worth further exploration, and this is therefore the next topic.According to three-way probabilities and measures (Section 2.1), accuracy and importance become two causality measures. In fact, they correspond to relativity/concentration and absoluteness/directness, respectively. Based on a system analysis (Section 2.2), their heterogeneity and complementarity are further verified. Therefore, their benign fusion is valuable for causality inference. In this subsection, both are fused into a benign uncertainty measure, IP-Accuracy, by adopting the weighted product technology. Herein, IP-Accuracy is mainly described by its rule confidence role.Definition 2.3.1IP-AccuracyIn Bottom-System(U,R,X),0.35em0ex∀[x]R∈U/R,(5)IA=iaX([x]R)=iX([x]R)×aX([x]R)is called granule [x]R’s importance-accuracy (IP-Accuracy) regarding concept X.(6)IA=W2|X|V=|[x]R∩X|2|X∥[x]R|.(7)∑[x]R∈U/RiX([x]R)=1.IP-Accuracy, which corresponds to a mapping from Approx-Space to [0, 1], is formally constructed by the product fusion of importance and accuracy, thus exhibiting cardinality essence (Formula (6)). In the sense of weight conservation (Formula (7)), the algebraic product could be semantically explained as a sort of weighted product. Thus, IP-Accuracy’s fusion scientificity and measurement semantics could be explained as follows.(1)First, accuracy cannot completely represent the causality between the granule and concept. For example, for two granules with the same accuracy, the granule with greater importance necessarily plays a more vital function in the causal reasoning. Then, importance could also represent the causality by virtue of its essence. In view of metrical conservation, each granule occupies only the part importance shares regarding sum 1; thus, importance could be viewed as a sort of objective weight coefficient to be appended to accuracy. The weighted pattern effectively underlies the product fusion of importance and accuracy; as a result, the corresponding IP-Accuracy more synthetically represents the causality and is thus novel and scientific.From the rule semantics, accuracy and importance give the causality rule only relative and absolute confidences, respectively. Furthermore, the fusional IP-Accuracy could provide a sort of synthetic confidence by integrating both the relativeness and absoluteness. For example,aX([x]R)=0.5endows reasoning rule[x]R0.25em0ex⇒0.25em0exXwith relative confidence 0.5; in contrast,iX([x]R)=0.2endows the reasoning rule with absolute confidence 0.2, which means that the granule [x]Rholds a 20% contribution for determining X (regardless of [x]R’s scale); furthermore,iaX([x]R)=0.2×0.5=0.1could more stably assess the reliability of the rule. Therefore, IP-Accuracy holds the basic rule semantics: the comprehensive reliability for the decision rule. Moreover, by virtue of the importance weight, IP-Accuracy could represent the fusional measure semantics: the granular effective precision for concept X.From the double-quantitative perspective, IP-Accuracy effectively fuses two fundamental causality measures, accuracy and importance, so it has measurement merit as a result of combining metrical absoluteness and relativeness. Moreover, the superiority of IP-Accuracy could be further verified by its granulation monotonicity, which is provided in Section 3. Thus, IP-Accuracy underlies causality-based rough reasoning, so it could be utilized to comprehensively describe reasoning rules.In a word, IP-Accuracy becomes a systematic synthetic causality measure to describe a rule’s reliability and granular availability. From the mathematical viewpoint, IP-Accuracy’s causality superiority mainly benefits from its non-linear promotion in system(V,W). According to Formula (3) and Fig. 1, accuracy and importance exhibit only the pure linear cardinality form. In contrast, according to Formula (6), IP-Accuracy uses the non-linear cardinality style. Thus, this non-linear improvement implies more information connotations than IP-Accuracy contains. According to Formulas (3) and (6), the relevant causality strengthening mechanism is analyzed as follows, and it radically underlies the later benign granulation monotonicity (Section 3).(1)Except for |[x]R∩X|, IP-Accuracy synthetically considers both |[x]R| and |X|, rather than only one. Thus, it becomes more comprehensive than accuracy and importance, especially when considering causality.IP-Accuracy strengthens the nuclear interaction information, i.e., |[x]R∩X|. In fact, it uses the two-order form |[x]R∩X|2, which carries more effective information than the one-order |[x]R∩X|. Thus, IP-Accuracy becomes more powerful than accuracy and importance in terms of causality.Finally, IP-Accuracy’s derivative and differential are provided by continuous expansion.Proposition 2.3.4(1)∂(IA)∂I=A≥0,∂(IA)∂A=I≥0,d(IA)=AdI+IdA.∂(IA)∂V=−W2|X|V2≤0,∂(IA)∂W=2W|X|V≥0,d(IA)=−W2|X|V2dV+2W|X|VdW.∂(IA)∂V=−1|X|A2≤0,0.35em0ex∂(IA)∂W=2|X|A≥0,1em0exd(IA)=−1|X|A2dV+2|X|AdW.According to Formulas (5) and (6), Proposition 2.3.4 provides mathematical analysis results in systems(A,I)and(V,W). Furthermore, Corollary 2.3.5 expresses IP-Accuracy’s derivative and differential from the accuracy perspective; in particular, this result further reflects that IP-Accuracy contains more basic information than accuracy.In Section 2, IP-Accuracy – a synthetic causality measure – is systematically mined in Approx-Space by the double-quantitative fusion of accuracy and importance. In micro Bottom-System (U, R, X), IP-Accuracy mainly depends on the concept to describe granules, so granular concept IP-Accuracy (Gr-Con IP-Accuracy) is the full name in this case. Attribute reduction usually concerns a macro top with knowledge and concept systems. To explore attribute reduction, Approx-Space needs promoting to comprehensive hierarchical structures. Thus, this section progressively performs IP-Accuracy’s granular integration construction in three granular levels (i.e., the micro bottom, meso middle, and macro top). Concretely, Gr-Con IP-Accuracy will be integrated into knowledge’s concept IP-Accuracy and further into knowledge’s classification IP-Accuracy; as a result, the relevant GrC hierarchical structure is finally established. Herein, IP-Accuracy has three levels, whose relevant abbreviations are listed in Table 1, and it is used directly without confusions. Note that granulation monotonicity acts as a crucial criterion to evaluate an uncertainty measure in RS-Theory, especially in attribute reduction [31,49,51,76]. Thus, IP-Accuracy’s granulation monotonicity and relevant equality conditions are the next research emphasis.Herein, necessary granulation preliminaries are first provided. Decision table (D-Table)(U,C∪D)serves as a basic framework; the relevant factotum also highlights the attribute subset with knowledge. Thus,C*⊆Ccorrespondingly replaces the previous knowledge R; moreover, let k ≥ 2. Suppose{U/C={[x]Ci:i=1,⋯,n};U/D={Xj:j=1,⋯,m}.C2⊆C1⊆Crefers to the knowledge deduction, denoted byC10.25em0ex⇒0.25em0exC2;0.25em0ex⇒corresponds to a knowledge-granular coarsening process and a mathematical partial order. IfC10.25em0ex⇒0.25em0exC2, then C2 depends on and is generally rougher than C1 (while knowledge C1 deduces and is generally finer than knowledge C2); thus,C10.25em0ex⇒0.25em0exC2is called knowledge coarsening (Kn-Coarsening).Proposition 3.1.1In Kn-CoarseningC10.25em0ex⇒0.25em0exC2,1em0ex∀[x]C1∈U/C1,∃[x]C2∈U/C2,2em0exs.t.,0.35em0ex[x]C1⊆[x]C2.∀[x]C2∈U/C2,∃k∈N,0.35em0exs.t.,0.35em0ex[x]C2=⋃t=1k[x]C1t.Kn-CoarseningC10.25em0ex⇒0.25em0exC2usually includes two types of granular transformation:(8){Granule0.35em0exmerging0.35em0ex(Gr-Merging):[x]C11∪⋯∪[x]C1k0.25em0ex→=0.25em0ex[x]C21,Granule0.35em0expreservation0.35em0ex(Gr-Preservation):[x]C110.25em0ex→=0.25em0ex[x]C21.From the granule viewpoint, Kn-Coarsening becomes a sort of rough hierarchical transformation between knowledge structures. Thus, it involves two types of granular actions: Gr-Merging and Gr-Preservation. Herein, the granular superscript represents the granular ordinal number, e.g., t in[x]C1tand k in[x]C1k. In fact, Gr-Merging becomes a radical feature of Kn-Coarsening, and representative[x]C11∪⋯∪[x]C1k0.25em0ex→=0.25em0ex[x]C21could be directly utilized for complete granulation monotonicity verification.Positive, negative, boundary regions (POS, NEG, BND) of X regarding knowledgeC*are defined by:(9){POSC*(X)={x:[x]C*⊆X},NEGC*(X)={x:[x]C*⊆¬X},BNDC*(X)={x:[x]C*∩X,¬X0.25em0ex≠0.25em0ex⌀}.(10){POSC*(D)=⋃X∈U/DPOSC*(X),BNDC*(D)=U−POSC*(D).Thus, the dependency degree is given by:(11)γC*(D)=|POSC*(D)||U|,Proposition 3.1.4In Kn-CoarseningC10.25em0ex⇒0.25em0exC2,(12){POSC2(X)⊆POSC1(X),POSC2(D)⊆POSC1(D),γC2(D)≤γC1(D).In the qualitative Pawlak-Model, C-POS becomes the union of classification concepts’ POS, while the dependency degree becomes the classical measure to evaluate the classification quality. In particular, C-POS, POS, and measure γ all have granulation monotonicity (Proposition 3.1.4).Aiming at meso Middle-System(U,C,X), this subsection utilizes Gr-Con IP-Accuracy in micro Bottom-System(U,C*,X)to integratedly construct knowledge’s concept IP-Accuracy, and the corresponding granulation monotonicity is emphatically verified.Definition 3.2.1Kn-Con IP-AccuracyIn Middle-System(U,C,X),(13)iaX(C*)=∑[x]C*∈U/C*iaX([x]C*)is called knowledgeC*’s IP-Accuracy regarding concept X, i.e., knowledge’s concept IP-Accuracy (Kn-Con IP-Accuracy).By using the arithmetic sum, Kn-Con IP-Accuracy is naturally established by integrating Gr-Con IP-Accuracy because knowledge consists of knowledge granules. In fact, Kn-Con IP-Accuracy promotes Gr-Con IP-Accuracy from the GrC viewpoint. Clearly, Kn-Con IP-AccuracyiaX(C*)holds three types of harmonious semantics functions.(1)Measure reasoningC*0.25em0ex⇒0.25em0exXin a confidence sense.Assess knowledgeC*by concept X, a systematic parameter.Systematically estimate concept X by knowledgeC*, a sort of granule system.Next, we make efforts to inspect IP-Accuracy’s granulation monotonicity in(U,C,X). For this purpose, the relevant mathematical mechanism regarding Gr-Merging is first uncovered for IP-Accuracy.Lemma 3.2.2(14)∑t=1kat2bt≥(∑t=1kat)2∑t=1kbt.The above inequality reaches the equal sign, if and only if (iff),(15)a1b1=⋯=akbk,0.35em0exi.e.,0.35em0exa1b1=⋯=akbk=∑t=1kat∑t=1kbt.For Gr-Merging:[x]C11∪⋯∪[x]C1k0.25em0ex→=0.25em0ex[x]C21,(16)iaX([x]C21)≤iaX([x]C11)+⋯+iaX([x]C1k).The above equality holds, iff Gr-Merging concerns the equal granular accuracy. This equality requirement could be further divided into three conditions:(17){aX([x]C11)=⋯=aX([x]C1k)=aX([x]C21)=1;aX([x]C11)=⋯=aX([x]C1k)=aX([x]C21)=0;aX([x]C11)=⋯=aX([x]C1k)=aX([x]C21)∈(0,1).According to Formulas (6) and (14),iaX([x]C11)+⋯+iaX([x]C1k)=|[x]C11∩X|2|[x]C11|×|X|+⋯+|[x]C1k∩X|2|[x]C1k|×|X|2em0ex≥(|[x]C11∩X|+⋯+|[x]C1k∩X|)2|[x]C11|×|X|+⋯+|[x]C1k|×|X|=|([x]C11∩X)∪⋯∪([x]C1k∩X)|2|[x]C11∪⋯∪[x]C1k|×|X|=|([x]C11∪⋯∪[x]C1k)∩X|2|[x]C21|×|X|2em0ex=|[x]C21∩X|2|[x]C21|×|X|=iaX([x]C21).Thus, Formula (16) holds for G-Merging. Moreover, the equality condition (Formula (15)) refers to the equal granular accuracy, and it is further divided into G-Merging’s three special cases regarding 0 and 1 (Formula (17)).□Lemma 3.2.2, whose proof is given in Appendix A, establishes an important mathematical basis. Thus, IP-Accuracy gains Gr-Merging monotonicity. Concretely, the constructional granular IP-Accuracy never increases for the merged granules’ IP-Accuracy sum (Formula (16)). Next, the equality conditions (Formula (17)) are further analyzed, mainly from the set-regional perspective.Proposition 3.2.4Given Gr-Merging:[x]C11∪⋯∪[x]C1k0.25em0ex→=0.25em0ex[x]C21.(1)aX([x]C11)=⋯=aX([x]C1k)=aX([x]C21)=1, iff[x]C11,⋯,[x]C1k,[x]C21⊆X, iff[x]C11,⋯,[x]C1k,[x]C21⊆POSC1(X),POSC2(X).aX([x]C11)=⋯=aX([x]C1k)=aX([x]C21)=0, iff[x]C11,⋯,[x]C1k,[x]C21⊆¬X, iff[x]C11,⋯,[x]C1k,[x]C21⊆NEGC1(X),NEGC2(X).IfaX([x]C11)=⋯=aX([x]C1k)=aX([x]C21)∈(0,1), then[x]C11,⋯,[x]C1k,[x]C21∩X,¬X0.25em0ex≠0.25em0ex⌀, i.e.,[x]C11,⋯,[x]C1k,[x]C21⊆BNDC1(X),BNDC2(X).According to Formula (15), the monotonicity equality could be equivalently achieved by three accuracy conditions (Formula (17)); the latter are further transformed into set-regional manifestations (Proposition 3.2.4). Concretely, Items (1) (2) mean that Gr-Merging is equivalently inside X/POS and¬X/NEG, respectively; Item (3) shows that the third equality condition is only a strong case inside BND. These results underlie further region preservation deduction.Theorem 3.2.5Gr-Preservation InvarianceFor Gr-Preservation[x]C110.25em0ex→=0.25em0ex[x]C21,(18)iaX([x]C21)=iaX([x]C11).In Kn-CoarseningC10.25em0ex⇒0.25em0exC2,(19)iaX(C2)≤iaX(C1).iaX(C2)=iaX(C1), iff each Gr-Merging exists in one of the following three cases:(1)in POS’s interior regarding bothC1andC2;in NEG’s interior regarding bothC1andC2;in BND’s interior regarding bothC1andC2, but on the premise of equal granular accuracy.Kn-Coarsening usually consists of Gr-Merging and Gr-Preservation (Proposition 3.1.2). IP-Accuracy exhibits Gr-Merging monotonicity (Theorem 3.2.3) and Gr-Preservation invariance (Theorem 3.2.5). Thus, IP-Accuracy naturally holds Kn-Coarsening monotonicity, i.e., granulation monotonicity. For the monotonicity equality condition, all existing Gr-Merging satisfy condition (17) and thus equivalently exists in the above three cases. Thus, the equality condition achieves a natural evolution from internal Gr-Merging to integral Kn-Coarsening.□IP-Accuracy preservation unidirectionally deduces set-region preservation in Kn-Coarsening. In other words, forC10.25em0ex⇒0.25em0exC2, ifiaX(C2)=iaX(C1), then(20){POSC2(X)=POSC1(X),NEGC2(X)=NEGC1(X),BNDC2(X)=BNDC1(X);however, the opposite usually does not hold.IP-Accuracy holds granulation monotonicity (Theorem 3.2.6); i.e., it never increases in Kn-Coarsening. In particular, monotonicity acquires its equality condition from the regional perspective; concretely,iaX(C2)=iaX(C1)means the arbitrary and conditional Gr-Merging in POS/NEG and BND, respectively. Thus, Corollary 3.2.7 clearly provides a basic connection between the measure and region. In fact, the perfect granulation monotonicity and equality condition regarding Kn-Coarsening promote the corresponding results regarding Gr-Merging (Theorem 3.2.3); thus, they hold a strong mechanism and underlie later attribute reduction.In practice, Kn-Coarsening’s integration monotonicity could be similarly inspected for other measures. However, accuracy and importance never have a perfect Gr-Merging mechanism, benign Kn-Coarsening monotonicity, or clear monotonicity equality condition. Thus, IP-Accuracy’s granulation monotonicity (Theorem 3.2.6) fully reflects IP-Accuracy’s superiority, which mainly benefits from the benign weight fusion and non-linear cardinality promotion (Section 2.3).In a regional way, POS and, further, C-POS underlie attribute reduction. To approach POS and C-POS, we finally mine Kn-Con IP-Accuracy’s fundamental subpart, which also holds granulation monotonicity.Definition 3.2.8Kn-Con Sub-IP-AccuracyIn Middle-System(U,C,X)whereC*⊆C,(21){IiaX(C*)=∑[x]C*⊆XiaX([x]C*);EiaX(C*)=∑[x]C*⊆¬XiaX([x]C*);BiaX(C*)=∑[x]C*∩X,¬X≠⌀iaX([x]C*).are called knowledgeC*’s internal, external, and boundary IP-Accuracy regarding concept X, respectively. Herein, the three types of sub-IP-Accuracy are simply denoted by (Kn-Con) IIP-Accuracy, EIP-Accuracy, BIP-Accuracy, respectively.(1)IiaX(C*)=|POSC*(X)||X|.EiaX(C*)=0.BiaX(C*)=∑[x]C*⊆BNDC*(X)iaX([x]C*).iaX(C*)=IiaX(C*)+BiaX(C*).Herein, Kn-Con IP-AccuracyiaX(C*)is further divided into IIP-AccuracyIiaX(C*), EIP-AccuracyEiaX(C*), and BIP-AccuracyBiaX(C*). According to Proposition 3.2.9 (whose proof is given in Appendix B), EIP-Accuracy is equal to 0, IIP-Accuracy becomes|POSC*(X)||X|to act as a main part of IP-Accuracy, and BIP-Accuracy becomes the sum of all Gr-Con IP-Accuracy regarding concept X’s BND.Theorem 3.2.10Monotonicity IIIn Kn-CoarseningC10.25em0ex⇒0.25em0exC2,(22)IiaC2(X)≤IiaC1(X);(23)IiaC2(X)=IiaC1(X),0.35em0exiff0.35em0exPOSC2(X)=POSC1(X).According to Propositions 3.1.4 and 3.2.9,IiaC2(X)=|POSC2(X)||X|≤|POSC1(X)||X|=IiaC1(X); moreover, the monotonicity equality corresponds to conditionPOSC2(X)=POSC1(X).□Kn-Con IIP-Accuracy represents IP-Accuracy’s internal sum regarding POS and also has granulation monotonicity. Thus, IIP-Accuracy more resembles the classical regional pattern when compared to IP-Accuracy.Aiming at macro Top-System(U,C,D), this subsection further utilizes Kn-Con IP-Accuracy in meso Middle-System(U,C,X)to integratedly construct classification’s IP-Accuracy, and corresponding granulation monotonicity is naturally organized. In particular, preservation deduction of relevant targets becomes a new focus to underlie the later reduct hierarchies.Definition 3.3.1Kn-Cl IP-AccuracyIn Top-System(U,C,D),(24)iaD(C*)=∑j=1miaXj(C*)is called knowledge C*’s IP-Accuracy regarding classificationD, i.e., knowledge’s classification IP-Accuracy (Kn-Cl IP-Accuracy).By using the arithmetic sum, Kn-Cl IP-Accuracy is further established by integrating Kn-Con IP-Accuracy because classification consists of concepts. In fact, Kn-Cl IP-Accuracy further promotes Kn-Con IP-Accuracy from the GrC viewpoint. Similarly, Kn-Cl IP-AccuracyiaD(C*)also holds three harmonious semantics functions.(1)Measure reasoningC*0.25em0ex⇒0.25em0exDin a confidence sense.Measure knowledgeC*by classificationD: a systematic parameter by integrating concepts Xj.Systematically measure classificationDby knowledgeC*: a sort of granule system.In Kn-CoarseningC10.25em0ex⇒0.25em0exC2,(25)iaD(C2)≤iaD(C1).iaD(C2)=iaD(C1), iffiaC2(Xj)=iaC1(Xj)for∀j∈{1,⋯,m}, iff each Gr-Merging exists in one of the two conditions (whereC1andC2are equivalently concerned):(1)in POS’s interior of only one concept;in BND’s interior of all concepts, but on the premise of equal granular accuracy for each concept.First, Kn-Cl IP-Accuracy becomes Kn-Con IP-Accuracy’s algebraic sum integration (regarding concepts). According to Kn-Con IP-Accuracy’s monotonicity (Theorem 3.2.6),iaD(C2)≤iaD(C1)becomes natural. Thus, we only need to further refine the equality condition “all Gr-Merging satisfies equal accuracy regarding all concepts Xj”.For this purpose, we next adopt a two-stage discussion strategy to identify Gr-Merging’s regional range. Concretely, disjoint C-POS and C-BND are first concerned, and concepts are then considered. Herein, the regional notions mainly refer to knowledgeC1, but they are equivalent to those regarding knowledgeC2.Consider Gr-Merging[x]C11∪⋯∪[x]C1k0.25em0ex→=0.25em0ex[x]C21.(1)If it is distributed in both C-POS and C-BND, then exist Xj, s.t., it has at least two granules to contain different accuracy values regarding Xj. Hence, Gr-Merging is only in either C-POS or C-BND.Suppose it is in C-POS, the union of multiple POS. If it is distributed in different POS, we could obtain the contradiction (from granular unequal accuracy), which is similar to above. Hence, it is only in the POS of a sole concept, and thus G-Merging equal accuracy is naturally realized for all concepts.Suppose it is in C-BND, the union of multiple BND. By virtue of the similar contradiction proof, its granules must exist in the intersection of all BND and must have the same accuracy for each concept.□Kn-Cl IP-Accuracy holds granulation monotonicity (Theorem 3.3.2), which integratedly promotes Kn-Con IP-Accuracy’s granulation monotonicity (Theorem 3.2.6). Herein, the monotonicity equality’s regional mining becomes complex. By analyzing IP-Accuracy’s connotation, two specific regional equality conditions regarding classification-quantitative preservation are finally established to define Gr-Merging’s range. In fact, the conversion between the metrical and regional descriptions performs the GrC integration construction from the set-regional to classification-regional levels. As a result, Theorem 3.3.2’s two items have summarized and promoted Theorem 3.2.6’s three items, and this GrC integration can be illustrated by Theorem 3.3.2’s proof.Corollary 3.3.3IP-Accuracy preservation unidirectionally deduces C-POS preservation in Kn-Coarsening. In other words, forC10.25em0ex⇒0.25em0exC2, ifiaD(C2)=iaD(C1), thenPOSC2(D)=POSC1(D); however, the opposite usually does not hold.IP-Accuracy preservation implies the arbitrary and conditional Gr-Merging in C-POS and C-BND, respectively (Theorem 3.3.2). In contrast, C-POS preservation implies the arbitrary Gr-Merging in both C-POS and C-BND. Thus, this corollary is proved.□IP-Accuracy preservation is stronger than C-POS preservation, and this conclusion is inferred by the regional equality conditions (Theorem 3.3.2).Corollary 3.3.4In Kn-Coarsening, IP-Accuracy preservation unidirectionally deduces set-region preservation regarding classification’s all concepts.Kn-Cl IP-Accuracy becomes Kn-Con IP-Accuracy’s sum (regarding all concepts), and both hierarchical measures have granulation monotonicity. Thus, by the contradiction proof, the former’s preservation is equivalent to the latter’s preservation regarding all concepts. According to Corollary 3.2.7, for concept Xj, Kn-Con IP-Accuracy preservation unidirectionally deduces set-region preservation. Hence, Kn-Cl IP-Accuracy preservation unidirectionally deduces set-region preservation regarding classification’s all concepts.□Corollary 3.3.4 is derived from the monotonicity integration, as well as Kn-Con IP-Accuracy preservation’s strongness (for set-region preservation). Now, there are three concerned preservation targets in the above two corollaries. In Corollary 3.3.3, IP-Accuracy preservation unidirectionally deduces C-POS preservation; then, the former unidirectionally deduces set-region preservation in Corollary 3.3.4; moreover, set-region preservation naturally unidirectionally induces C-POS preservation.In Section 3.2, Kn-Con IP-Accuracy consists of three parts, i.e., Kn-Con IIP-Accuracy, EIP-Accuracy, and BIP-Accuracy. Note that Kn-Con IP-Accuracy is promoted to Kn-Cl IP-Accuracy. Thus, Kn-Cl IP-Accuracy naturally consists of three corresponding promotional parts. Next, the relevant definition, essence, and monotonicity are straightforwardly provided by virtue of the relevant integration and promotion.Definition 3.3.5Kn-Cl Sub-IP-AccuracyIn(U,C,D)whereC*⊆C,(26){IiaD(C*)=∑j=1mIiaXj(C*);EiaD(C*)=∑j=1mEiaXj(C*);BiaD(C*)=∑j=1mBiaXj(C*).are called knowledgeC*’s internal, external, boundary IP-Accuracy regarding classificationD, respectively; in particular, the three types of Sub-IP-Accuracy are simply denoted as Kn-Cl IIP-Accuracy, EIP-Accuracy, BIP-Accuracy, respectively.(1)IiaD(C*)=∑j=1m|POSC*(Xj)||Xj|≤∑j=1m|POSC*(Xj)|=|POSC*(D)|.EiaD(C*)=0.BiaD(C*)=∑j=1m∑[x]C*⊆BNDC*(Xj)iaXj([x]C*).iaD(C*)=IiaD(C*)+BiaD(C*).Kn-Cl IP-AccuracyiaD(C*)consists of IIP-AccuracyIiaD(C*), EIP-AccuracyEiaD(C*), BIP-AccuracyBiaD(C*). Furthermore, EIP-Accuracy is equal to 0, IIP-Accuracy becomes∑j=1m|POSC*(Xj)||Xj|to act as a main part of IP-Accuracy, and BIP-Accuracy becomes the sum of all Gr-Con IP-Accuracy regarding all BND and concepts.In Kn-CoarseningC10.25em0ex⇒0.25em0exC2,(27)IiaD(C2)≤IiaD(C1);IiaD(C2)=IiaD(C1),0.35em0exiff0.35em0exPOSC2(Xj)=POSC1(Xj),∀j∈{1,⋯,m},2em0ex2em0ex2em0exiff0.35em0exPOSC2(D)=POSC1(D).In particular, IIP-Accuracy preservation aims to keep both C-POS and C-BND and is thus called classification-qualitative preservation.In view of the homogeneous structure, IIP-Accuracy preservation (i.e., classification-qualitative preservation) is equivalent to C-POS preservation (Theorem 3.3.7); however, the two features exhibit the metrical and regional styles, respectively. As a result, IIP-Accuracy preservation could be used to define a qualitative reduct (Definition 4.2.3), which is equivalent to the Pawlak-Reduct but has a metrical connotation. In fact, C-POS could also fully correspond to its dependency degreeγC*(D). In fact,{γC*(D)=∑j=1m|POSC*(Xj)||U|;IiaD(C*)=∑j=1m|POSC*(Xj)||Xj|.Thus,IiaD(C*)has homogeneous monotonicity about and also sharply contrasts withγC*(D): C-POS’s metrical form. As a result, when compared toγC*(D),0.35em0exIiaD(C*)contains Xj’s individual information, thus becoming rational and optimal. Therefore,IiaD(C*)underlies the better reduction heuristic information and heuristic reduction algorithm, which could be reflected by Formula (30) and Algorithm 1’s parallel generalization (for IIP-Accuracy), respectively.Finally, we plan to summarize the relevant preservation deduction, which radically underlies the reduction target hierarchy and attribute reduction hierarchy. For this purpose, the preservation deduction of IP-Accuracy and IIP-Accuracy is first provided.Corollary 3.3.8IP-Accuracy preservation unidirectionally deduces IIP-Accuracy preservation.IP-Accuracy consists of IIP-Accuracy and BIP-Accuracy, and both IP-Accuracy and IIP-Accuracy have granulation monotonicity. Thus, that IP-Accuracy reaches its coarsening maximum unidirectionally implies that IIP-Accuracy reaches its own coarsening maximum; this result could be verified by the contradiction proof and BIP-Accuracy factor.□Herein, IP-Accuracy preservation is also stronger than IIP-Accuracy preservation (Corollary 3.3.8). In fact, the former is stronger than C-POS preservation (Corollary 3.3.3), while the latter is equivalent to C-POS preservation (Theorem 3.3.7). Thus, except for the above metrical proof, this metrical preservation relationship (between IP-Accuracy and IIP-Accuracy) could also be equivalently proved by the relevant regional method. In fact, the equivalent deduction transformation corresponds to the next preservation deduction commutativity.For IP-Accuracy/IIP-Accuracy, we make great efforts to reveal the granulation monotonicity and also to mine the relevant equality conditions in the regional way. As a result, we establish a fundamental connection between the relevant metrical and regional preservation. These basic works underlie the relationship discussions between the novel IP-Accuracy/IIP-Accuracy reduction and classical regional reduction (Section 4). Now, it is time to summarize the deduction relationships of these preservation targets by Fig. 2.In Fig. 2, there are two types of preservation targets. Concretely, metrical targets include IP-Accuracy and IIP-Accuracy; in contrast, regional targets include Set-Region preservation, C-POS preservation, and distributional preservation (i.e., POS or NEG or BND preservation). Thus, there are two unidirectional deduction lines, i.e., Line I: IP-Accuracy preservation ⇒ Set-Region preservation ⇒ Distributional preservation ⇒ C-POS preservation; and Line II: IP-Accuracy preservation ⇒ IIP-Accuracy preservation; moreover, there is a bidirectional deduction line, i.e., Line III: C-POS preservation ⇔ IIP-Accuracy preservation. In particular, Lines I and III compose a natural deduction line: IP-Accuracy preservation ⇒ Set-Region preservation ⇒ Distributional preservation ⇒ C-POS preservation ⇒ IIP-Accuracy preservation; thus, this new line exhibits commutativity with Line II for the two preservation states: IP-Accuracy preservation and IIP-Accuracy preservation.Note that preservation targets’ deduction radically underlies relevant preservation reducts’ hierarchy. In particular, some reduction hierarchies were provided in [74,75] for the two-category case. In Section 4, some reduction hierarchies will be investigated, and they apply to the more general multi-category case.Gr-Con IP-Accuracy, Kn-Con IP-Accuracy/IIP-Accuracy, Kn-Cl IP-Accuracy/IIP-Accuracy are provided in Sections 2.3, 3.2, and 3.3, respectively. In this subsection, the three IP-Accuracy levels are summarized to establish the relevant GrC hierarchical structure. Herein, the systematic structural figure is first provided by Fig. 3.Next, this hierarchical structure figure is utilized to explain IP-Accuracy’s GrC construction. There are four fundamental granular systems, three GrC levels (micro bottom, meso middle, macro top), two main RS-Theory terms (Approx-Space and attribute reduction), and two GrC-hierarchical evolution lines.(1)Bottom-System(U,C*,X), at the micro level, mainly concerns the knowledge-granule and concept. Thus, Gr-Con IP-AccuracyiaX([x]C*)is constructed by fusing accuracy and importance. Herein,iaX([x]C*)can systematically measure the following: reasoning[x]C*0.25em0ex⇒0.25em0exX, granule[x]C*(based on concept X), and concept X (based on granule[x]C*).In fact, the relevant system and level correspond to Approx-Space.Middle-System(U,C,X), at the meso level, mainly concerns the knowledge and concept. Thus, Kn-Con IP-AccuracyiaX(C*)is constructed by integrating Gr-Con IP-AccuracyiaX([x]C*)because knowledgeC*is actually a granular system. Herein,iaX(C*)can systematically measure reasoningC*0.25em0ex⇒0.25em0exX, knowledgeC*(based on concept X), and concept X (based on knowledgeC*). Moreover, IIP-AccuracyIiaX(C*)is provided to describe POS. As a result, Kn-Con IP-Accuracy and IIP-Accuracy achieve basic granulation monotonicity.Top-System(U,C,D), at the macro level, mainly concerns knowledge and classification. Thus, Kn-Cl IP-AccuracyiaD(C*)is naturally constructed by integrating Kn-Con IP-AccuracyiaX(C*)because classificationDis actually concepts system{X1,⋯,Xm}. Herein,iaD(C*)can systematically measure reasoningC*0.25em0ex⇒0.25em0exD, knowledgeC*(based on classificationD), and classificationD(based on knowledgeC*). Moreover, Kn-Cl IIP-AccuracyIiaD(C*)is provided to describe C-POS. Thus, Kn-Cl IP-Accuracy and IIP-Accuracy also achieve basic granulation monotonicity. In particular, D-Table attribute reduction exactly corresponds to the top system and macro level; as a result, the perfect granulation monotonicity underlies further attribute reduction.For the above three systems, IP-Accuracy utilizes the natural sum integration to realize the valuable GrC-hierarchical promotion, i.e.,iaX([x]C*)→iaX(C*)→iaD(C*); this evolutionary line is marked by Line I. Meanwhile, IP-Accuracy’s GrC evolution could also be completed by establishing a new connection based on the fourth system. The added system(U,C*,D), which is also at the meso middle level, mainly concerns knowledge-granule, classification, and Cl-Gr IP-Accuracyia[x]C*(D), as well as Cl-Gr IIP-AccuracyIia[x]C*(D). Herein,ia[x]C*(D)can systematically measure the following: reasoning[x]C*0.25em0ex⇒0.25em0exD, classificationD(based on granule[x]C*), and granule[x]C*(based on classificationD). According to next Definition 3.4.1, Cl-Gr IP-Accuracyia[x]C*(D)integrates Con-Gr IP-Accuracyia[x]C*(X)in Bottom-System and is further integrated into Cl-Kn IP-AccuracyiaC*(D)in Top-System. Thus,ia[x]C*(D)could also be used to realize the new GrC-hierarchical evolution, i.e.,ia[x]C*(X)→ia[x]C*(D)→iaC*(D); this promotional line is marked by Line II.Line II integrates the classification first and knowledge second, while Line I adopts the contrary order. Thus, different terms usually have different semantics emphases. However, according to Theorem 3.4.2, both lines have commutativity, which radically originates from the algebraic sum commutativity; as a result, both lines obtain the same final measure in macro Top-System. In particular, Line II could establish the set-operation property regarding the classification system, which sharply contrasts with Line I’s granulation monotonicity. Therefore, with the exception of granulation monotonicity, Top-System could finally achieve more classification system properties.In summary, we adopt a bottom-top strategy to perform IP-Accuracy’s GrC construction. In particular, IP-Accuracy granulation monotonicity (Theorems 3.3.2 and 3.3.7) and relevant preservation deduction (Fig. 2) not only fully reflect our basic GrC works but also effectively underlie the later attribute reduction (Section 4).Next, we provide the relevant definition and property for Middle-System(U,C*,D), which are mainly concerned in the above explanation Items (4) (5). For the new measure, IIP-Accuracy could gain the relevant construction and analysis, which are similar to IP-Accuracy’s (especially in Items (4) (5)).Definition 3.4.1(1)Con-Gr IP-Accuracyia[x]C*(Xj)=iaXj([x]C*).Cl-Gr IP-Accuracyia[x]C*(D)=∑j=1mia[x]C*(Xj);Cl-Gr IIP-Accuracyia[x]C*(D)=∑POSC*(Xj)⊇[x]C*ia[x]C*(Xj).Cl-Kn IP-AccuracyiaC*(D)=∑[x]C*∈U/C*ia[x]C*(D);Cl-Kn IIP-AccuracyIiaC*(D)=∑[x]C*∈U/C*Iia[x]C*(D).(28){iaC*(D)=iaD(C*);IiaC*(D)=IiaD(C*).Note that IP-Accuracy’s GrC hierarchical system could be compared to and comprehended by a practical assessment pattern, at least in some sense. Thus, a relevant example is finally provided to gain more thorough explanations.There are m players Xjand n judges [x]i. For players Xj, judges [x]iare endowed with objective systematic weightsIij, and they also have their own subject judgementAij. Thus, the weighted productIij×Aijleads to the effective judgement, while the further weighted sum provides the synthetic assessment. Concretely, there are four systems and relevant assessments.(1)In system ([x]i, Xj) (with one judge and one player), dataIij×Aijcould be used for the judge to synthetically assess the player (based on the systematic weight) and also for the player to synthetically evaluate the judge. Thus,Aij,Iij,Iij×Aijcorrespond to accuracy, importance, and IP-Accuracy in Bottom-System, respectively. In particular, the former three’s subjectivity, objectivity, effectiveness generally correspond to the latter three’s individuality, systematicness, synthesis, respectively.In system({[x]1,⋯,[x]n},Xj)(with systematic judges and one player), data∑i=1nIij×Aijcould be used for the judges system to synthetically assess the player and also for the latter to synthetically evaluate the former. Thus, the weighted sum information corresponds to Kn-Con IP-Accuracy in Middle-System one.In system([x]i,{X1,⋯,Xm})(with one judge and systematic players), data∑j=1mIij×Aijcould be used for the judge to synthetically assess the players system and also for the latter to synthetically evaluate the former. Thus, the weighted sum information corresponds to Cl-Gr IP-Accuracy in Middle-System two.In system({[x]1,⋯,[x]n},{X1,⋯,Xm})(with systematic judges and systematic players), data∑j=1m[∑i=1nIij×Aij]=∑i=1n[∑j=1mIij×Aij]could be used for the judges system to synthetically assess the players system and also for the latter to synthetically evaluate the former. Thus, the weighted sum information corresponds to Kn-Cl/Cl-Kn IP-Accuracy in Top-System. □IP-Accuracy’s GrC construction is hierarchically completed in Section 3. IP-Accuracy’s/IIP-Accuracy’s granulation monotonicity (Theorem 3.3.2/3.3.7) not only verifies IP-Accuracy’s fusion superiority but also underlies further reduction construction. In particular, the monotonicity equality conditions and their equivalent regional manifestations, as well as the in-depth preservation deduction (Fig. 2), solidly underlie IP-Accuracy/IIP-Accuracy reducts and their hierarchies (regarding regional reducts).In this section, IP-Accuracy/IIP-Accuracy and its monotonicity are utilized to finally investigate attribute reduction regarding D-Table(U,C,D). Concretely, qualitative/quantitative reducts, their hierarchies, and heuristic algorithms are systematically provided. Herein, IP-Accuracy/IIP-Accuracy is at the macro top, and constraint Cl-Kn is usually omitted. In particular, the hierarchical reduction theory has been established in [74,75]. In this section, we will give a well-researched and descriptive emphasis to the hierarchical reducts, in view of multiple reducts’ diversity.This subsection provides some reduct preliminaries. First, the classical Pawlak-Reduct [35,36] is reviewed; then, generalized reducts are proposed based on a monotonicity target; finally, hierarchical reducts [74,75] are introduced. Herein, letB⊆C.Lemma 4.1.1The following two items are equivalent:(1)POSB′(D)0.25em0ex≠0.25em0exPOSB(D),∀B′⊂B;POSB−{b}(D)0.25em0ex≠0.25em0exPOSB(D),∀b∈B.Herein, Items (1) (2) correspond to the maximality and independence regarding C-POS preservation, respectively. They could be equivalently used for Pawlak-Reduct.Bis Pawlak-Reduct ofCif it satisfies following Items (1) (2) or (1) (3) regarding C-POS preservation.(1)POSB(D)=POSC(D);POSB′(D)0.25em0ex≠0.25em0exPOSB(D),∀B′⊂B;POSB−{b}(D)0.25em0ex≠0.25em0exPOSB(D),∀b∈B.In view of C-POS granulation monotonicity, Pawlak-Reduct is naturally established by preserving C-POS, thus becoming a qualitative reduct. Herein, Item (1) reflects the joint sufficiency regarding C-POS preservation, and Items (2) (3) could be arbitrarily chosen for the individual necessity. The following core-reduct relationship is basic.Core(C)=⋂B∈Red(C)B.Among all reduction algorithms, heuristic algorithms are appreciated in view of their high efficiency. The basic heuristic algorithm mainly utilizes the dependency degree, and(29)sig(c,C*;D)=γC*(D)−γC*−{c}(D)becomes the corresponding reduction-heuristic information for conditional attribute c.The above Pawlak-Reduct results mainly originate from the C-POS granulation monotonicity (Proposition 3.1.4). In fact, they have powerful generalization. Thus, we primarily propose a type of generalized reducts via a monotonicity target. Note that Ref. [11] generalizes multiple types of exiting attribute reducts. In contrast, the generalized reducts proposed below mainly focus on and promote fundamental monotonicity.Definition 4.1.4Let MT be an isomorphism between complete lattices(2C,⊆)and (L, ⪯); herein, ifC2⊆C1⊆C, thenMT(C2)⪯MT(C1), whileMT(⌀),MT(C)become the least and greatest elements in L, respectively. Thus, MT is called a monotonicity target (MT) regardingC. For Kn-CoarseningC10.25em0ex⇒0.25em0exC2, monotonicity target preservation (MT-Preservation) is defined byMT(C2)=MT(C1).Note that Ref. [74] utilizes the complete lattice to establish the structural target (ST), which is multi-dimensional. In this paper, we propose the monotonicity target (MT), which is also related to the complete lattice but is integral. For example, C-POS is MT in the mapping sense, where the relevant mapping changes⌀to⌀. Clearly, MT holds granulation monotonicity, and MT-Preservation is naturally defined. Thus, the relevant MT-Preservation reducts could be further established.Lemma 4.1.5The following two items are equivalent:(1)MT(B′)0.25em0ex≠0.25em0exMT(B),∀B′⊂B;MT(B−{b})0.25em0ex≠0.25em0exMT(B),∀b∈B.Bis MT-Preservation Reduct ofCif it satisfies following Items (1) (2) or (1) (3) regarding MT-Preservation.(1)MT(B)=MT(C);MT(B′)0.25em0ex≠0.25em0exMT(B),∀B′⊂B;MT(B−{b})0.25em0ex≠0.25em0exMT(B),∀b∈B.CoreMT(C)=⋂B∈RedMT(C)B.Lemma 4.1.5 and Proposition 4.1.7 are proved in Appendixes C and D, where MT granulation monotonicity plays the key role. In fact, they extend Lemma 4.1.1 and Proposition 4.1.3, respectively. Moreover, MT-Preservation Reduct extends Pawlak-Reduct. Thus, we establish a more general framework of attribute reduction by virtue of the national target with granulation monotonicity. MT-Preservation Reduct and its property apply to extensive monotonicity reducts. In particular, all of the reducts proposed later fall into the MT-Preservation Reduct category, so MT-Preservation Reduct provides a unified normal pattern. For these reducts, we mainly emphasize their target monotonicity and preservation while neglecting their natural definition equivalence and core-reduct relationship. As a result, we could more thoroughly focus on multiple reduct types and their systematic hierarchies.Theorem 4.1.8Hierarchical ReductStrong and weak reducts are defined by the strong and weak reduction targets, respectively. The strong reduct’s core and set are denoted byCorestrong(C)andRedstrong(C), respectively, and so are the weak symbolsCoreweak(C)andRedweak(C). Thus, two items hold as follows:(1)Corestrong(C)⊇Coreweak(C);∀Bstrong∈Redstrong(C),∃Bweak∈Redweak(C), s.t.,Bweak⊆Bstrong.According to the hierarchical reduction theory [74,75], the strong reduct has a strong reduction target, and its core includes weak reduct’s core. Moreover, in contrast to Item (2) (Theorem 4.1.8), the weak reduct could exist by transcending the inclusion relationship from the strong reduct. Moreover, denoteBstrong0.25em0ex⇒0.25em0exBweak; then, relation ⇒ becomes a partial order with the natural transitivity, but it transcends simple set relations ⊆ and ⊇.This subsection utilizes IP-Accuracy and IIP-Accuracy to establish two types of qualitative reducts and further studies their relationships for qualitative Pawlak-Reduct.Definition 4.2.1IP-Accuracy ReductBis IP-Accuracy Reduct ofCif it satisfies following Items (1) (2) or (1) (3) regarding IP-Accuracy preservation.(1)iaD(B)=iaD(C);iaD(B′)0.25em0ex≠0.25em0exiaD(B),∀B′⊂B;iaD(B−{b})0.25em0ex≠0.25em0exiaD(B),∀b∈B.IP-Accuracy’s granulation monotonicity and relevant equality conditions are provided in Theorem 3.3.2. In view of Gr-Merging’s accuracy requirement and region feature, the objective of IP-Accuracy preservation is actually to quantitatively keep the classification. By virtue of this reduction target, IP-Accuracy Reduct is naturally established to become a qualitative reduct but exhibits a metrical style.Theorem 4.2.2IP-Accuracy Reduct is stronger than Pawlak-Reduct. Thus,(1)Coreia(C)⊇Core(C).∀Bia∈Redia(C),∃B∈Red(C), s.t.,B⊆Bia.IP-Accuracy preservation unidirectionally deduces C-POS preservation (Corollary 3.3.3 or Fig. 2), so IP-Accuracy Reduct is stronger than Pawlak-Reduct. Thus, the above hierarchical reduction description becomes clear.Bis IIP-Accuracy Reduct ofCif it satisfies following Items (1) (2) or (1) (3) regarding IIP-Accuracy preservation.(1)IiaD(B)=IiaD(C);IiaD(B′)0.25em0ex≠0.25em0exIiaD(B),∀B′⊂B;IiaD(B−{b})0.25em0ex≠0.25em0exIiaD(B),∀b∈B.IIP-Accuracy Reduct is equivalent to Pawlak-Reduct. Thus,(1)CoreIia(C)=Core(C);RedIia(C)=Red(C).IIP-Accuracy’s granulation monotonicity and relevant equality conditions are provided in Theorem 3.3.7. In view of Gr-Merging’s C-POS result, the objective of IIP-Accuracy preservation is actually to qualitatively keep the classification. By virtue of this reduction target, IIP-Accuracy Reduct is naturally established to become a qualitative reduct with a metrical style. Clearly, IIP-Accuracy Reduct is equivalent to Pawlak-Reduct, according to Theorem 3.3.7 or Fig. 2.Theorem 4.2.5IP-Accuracy Reduct is stronger than IIP-Accuracy Reduct. Thus,(1)Coreia(C)⊇CoreIia(C);∀Bia∈Redia(C),∃BIia∈RedIia(C), s.t.,BIia⊆Bia.According to Corollary 3.3.8 or Fig. 2, Theorem 4.2.5 clearly exhibits the hierarchy between IP-Accuracy and IIP-Accuracy Reducts. Moreover, this hierarchy conclusion could be inferred by Theorems 4.2.2 and 4.2.4.The above reduction hierarchies mainly focus on only three reduct types: IP-Accuracy Reduct, IIP-Accuracy Reduct, and Pawalk-Reduct. Note that the hierarchy radically originates from the reduction target deduction. Thus, more reduction hierarchies could be established by Fig. 2 and become later research content (Section 4.4).Aiming at qualitative IP-Accuracy and IIP-Accuracy Reducts, heuristic reduction algorithms are next constructed. Clearly, underlying IP-Accuracy and IIP-Accuracy (and their monotonicity) could be fully utilized. Thus, IP-Accuracy and IIP-Accuracy are first used to construct two types of reduction-heuristic information:(30){sigia(c,C*;D)=iaD(C*)−iaD(C*−{c});sigIia(c,C*;D)=IiaD(C*)−IiaD(C*−{c}).Moreover, the reduction core is easily calculated to underlie attribute addition algorithms. Thus, both the heuristic information and reduction core are utilized to develop the relevant heuristic reduction algorithm. Next, the IP-Accuracy-Based heuristic algorithm for IP-Accuracy Reduct is provided, i.e., Algorithm 1, while the parallel algorithm for IIP-Accuracy Reduct could be similarly developed.Algorithm 1IP-Accuracy-Based heuristic algorithm for IP-Accuracy ReductInput: D-Table(U,C∪D);Output: IP-Accuracy ReductBia∈Redia(C).1: ComputeCoreia(C).2:Bia=Coreia(C).3: whileiaD(Bia)0.25em0ex≠0.25em0exiaD(C)do4:∀c∈C−Bia, calculatesigia(c,Bia;D);choosec0=arg0.12em0exAPTARANORMALmaxc∈C−Biasigia(c,Bia;D);letBia=Bia∪{c0}.5: end while6: returnBia.Based on the attribute addition strategy, Algorithm 1 mainly utilizes IP-Accuracy heuristic informationsigia(c,C*;D)to calculate IP-Accuracy Reduct. Concretely, Step 1 gives the basic core; Step 2 chooses the core as the reduct basis; Steps 3 and 4 further seek an attribute subset to satisfy the IP-Accuracy preservation target, and the added c0 is randomly chosen insideC−Biavia the highest IP-Accuracy heuristic information. As a result, Algorithm 1 could usually yield one IP-Accuracy Reduct (regardless of independence/maximality), thus becoming convergent and effective.Herein, the two heuristic reduct algorithms (i.e., Algorithm 1 and its parallel) are correspondingly inspired by their own bearing measures. Moreover, they could alternatively be inspired by their opposite bearing measures. In other words, IP-Accuracy/IIP-Accuracy Reduct could be heuristically calculated by IIP-Accuracy/IP-Accuracy. For Pawlak-Reduct, Algorithm 1 effectively provides a heuristic algorithm because IP-Accuracy Reduct is stronger than Pawlak-Reduct (Theorem 4.2.2); in contrast, the parallel algorithm for IIP-Accuracy Reduct directly provides a heuristic algorithm because IIP-Accuracy Reduct is equivalent to Pawlak-Reduct (Theorem 4.2.4).In Section 4.2, two qualitative reducts are proposed, and both are compared to Pawlak-Reduct. For qualitative reducts, the qualitative equality with the measure criterion becomes somewhat strict, especially in certain practical environments. In practice, there is not only calculation error but also data noise. Thus, the relevant quantitative reduct with some metrical change exhibits radical improvements and underlies extensive applications. Moreover, the quantitative reduct expansion could enlarge IP-Accuracy Reduct’s application scope because IP-Accuracy preservation is relatively strict (Fig. 2). Usually, the quantitative reduct is indirectly realized by the quantitative model and its quantitative function. Herein, we alternatively resort to a new approach; i.e., the quantitative reduct is directly established by the monotonous measure and its quantitative function. Thus, this subsection mainly depends on IP-Accuracy/IIP-Accuracy to construct quantitative reducts, including the novel tolerant and approximate reducts.Theorem 4.3.1(1)DefineU/⌀=UandCKSC={U/C*:C*⊆C}. Then,(CKSC,⇒)constitutes a complete lattice;U/CandU/⌀serve as the least and greatest elements, respectively.Define mappingkn:2C→CKSC,0.35em0ex∀C*∈2C,0.35em0exkn(C*)=U/C*. Then, surjective kn constructs an order-preservation mapping between complete lattices (2C, ⊇) and (CKSC, ⇒). Furthermore, by virtue of kn, both complete lattices exhibit a complete lattice homomorphism.By virtue of IP-AccuracyiaD(C*), define mappingmia:CKSC→[1,iaD(C)],0.35em0ex∀U/C*∈CKSC,0.35em0exmia(U/C*)=iaD(C*). Then, non-surjective mia constructs an order-preservation mapping between complete lattices(CKSC,0.25em0ex⇒)and([1,iaD(C)],≥).IP-AccuracyiaD(C*)essentially determines mappingiaD:2C→[1,iaD(C)],0.35em0ex∀C*∈2C,0.35em0exiaD(C*)=iaD(C*). Thus,iaDbecomes the composite mapping of kn and mia, i.e.,iaD=mia∘knandiaD(C*)=(mia∘kn)(C*)=mia(kn(C*))=mia(U/C*)=iaD(C*). Additionally,iaDconstructs an order-preservation mapping between complete lattices(2C,⊇)and([1,iaD(C)],≥). As a result,iaDholds the monotonicity and boundedness, i.e.,{iaD(C1)≥iaD(C2),0.35em0exif0.35em0exC⊇C1⊇C2;iaD(C*)∈[1,iaD(C)],∀C*⊆C.In Theorem 4.3.1, three complete lattices and their order preservation mappings are thoroughly constructed for the conditional attribute, coarsening knowledge, and IP-Accuracy measure. These algebraic results become natural but in-depth. Herein,CKSCmeans the coarsening knowledge set. Moreover, extreme⌀andU/⌀={U}lead toia⌀(D)=∑j=1mia⌀(Xj)=∑j=1m|U∩Xj|2|U∥Xj|=∑j=1m|Xj||U|=1;thus, IP-AccuracyiaD(C*)has the lower bound 1. Next, these relevant mathematical results are exhibited in Fig. 4. There, the three complete lattices and their representative elements (including the least and greatest elements) are located at three different levels; furthermore, the three order preservation mappings connect these levels. In particular, IP-Accuracy’s granulation monotonicity regarding coarsening order relation ⇒ is also expressed by a non-increasing mapping figure.Theorem 4.3.2(31)APTARANORMALlimC*→CiaD(C*)=iaD(C),i.e.,0.35em0ex∀ɛ>0,∃Cɛ,0.35em0exif0.35em0exC*⊇Cɛ0.35em0ex(i.e.,0.35em0exC*0.25em0ex⇒0.25em0exCɛ),0.35em0exthen0.35em0exiaD(C)−iaD(C*)<ɛ.In Theorem 4.3.2, IP-Accuracy’s limit form is provided to describe IP-Accuracy’s granulation monotonicity. In particular, the relevant ɛ-Cɛdescription could be reflected by Fig. 4. This limit result reflects a sort of generalized continuity for IP-Accuracy’s granulation monotonicity, thus underlying the next quantitative reduct construction.Definition 4.3.3Define mappingsdriaD,0.35em0exeriaD:2C→[0,1],0.35em0ex∀C*∈2C,(32){driaD(C*)=iaD(C)−iaD(C*)iaD(C)=1−iaD(C*)iaD(C),eriaD(C*)=1−driaD(C*)=iaD(C*)iaD(C).Herein,driaD(C*)anderiaD(C*)are called knowledgeC*’s discrepancy and equality rates regarding knowledgeC’s IP-Accuracy, respectively.(33){driaD(C*)∈[0,1−1iaD(C)];eriaD(C*)∈[1iaD(C),1];driaD(C*)=0, iffiaD(C*)=iaD(C), ifferiaD(C*)=1.Discrepancy and approximation rates hold granulation monotonicity. Concretely, in Kn-CoarseningC10.25em0ex⇒0.25em0exC2,(34){driaD(C2)≥driaD(C1);eriaD(C2)≤eriaD(C1);driaD(C2)=driaD(C1), iffiaD(C2)=iaD(C1), ifferiaD(C2)=eriaD(C1).Complementary discrepancy and equality rates are proposed to measure discrepancy and equality degrees between knowledgeC*’s and knowledgeC’s IP-Accuracy, respectively. Herein,C’s P-Accuracy acts as the ideal target value, thus becoming the comparison criterion. Moreover, a controllable measure style of relative ratio is adopted to form the relevant interval range with1iaD(C). In particular, the relevant granulation monotonicity is provided and thus leads to the corresponding quantitative reducts as follows.Definition 4.3.6IP-Accuracy Tolerant ReductGive IP-Accuracy tolerance rateδ∈[0,1−1iaD(C)].Bis IP-Accuracy δ-Tolerant >Reduct ofCif it satisfies following Items (1) (2) or (1) (3) regarding IP-Accuracy δ-tolerance preservation.(1)driaD(B)≤δ;driaD(B′)>δ,0.35em0ex∀B′⊂B;driaD(B−{b})>δ,0.35em0ex∀b∈B.Give IP-Accuracy approximation rateη∈[1iaD(C),1].Bis IP-Accuracy η-Approximate Reduct ofCif it satisfies following Items (1) (2) or (1) (3) regarding IP-Accuracy η-approximation preservation.(1)eriaD(B)≥η;eriaD(B′)<η,0.35em0ex∀B′⊂B;eriaD(B−{b})<η,0.35em0ex∀b∈B.Tolerance rate δ first provides the tolerance threshold for objective discrepancy ratedriaD(C*); thus, IP-Accuracy δ-tolerance preservation (i.e.,driaD(C*)∈[0,δ]) becomes the reduction target to determine IP-Accuracy Tolerant Reduct. Similarly, equality rateeriaD(C*)holds the objectivity, approximation rate η provides the subjective approximation threshold, and IP-Accuracy η-approximation preservation (i.e.,eriaD(C*)∈[η,1]) becomes the reduction target. In particular, both IP-Accuracy δ-tolerance and η-approximation preservation hold convergence for IP-Accuracy preservation: a sort of quantitative preservation of classification (Theorem 3.3.2); thus, both IP-Accuracy Tolerant and Approximate Reducts are locally enlarged by surrounding the ideal knowledge state of this sort of quantitative preservation. Based on the above analyses, both reducts hold not only the directness regarding the qualitative reduction target but also the statistical property regarding IP-Accuracy.Note that IP-Accuracy Tolerant/Approximate Reduct adheres to MT-Preservation Reduct (Definition 4.1.6). In fact, IP-Accuracy δ-tolerance also has monotonicity, which comes from the discrepancy rate’s monotonicity (Theorem 4.3.5). In the lattice sense, ifC2⊆C1then(driaD(C2)∈[0,δ])⪯(driaD(C1)∈[0,δ]), which accords withdriaD(C2)≥driaD(C1)(Formula (34)). Moreover, IP-Accuracy η-approximation’s monotonicity could be similarly analyzed.Theorem 4.3.8IP-Accuracy Tolerant and Approximate Reducts become equivalent whenδ+η=1.The two quantitative reducts exhibit clear definition duality and hold the equivalent transformation whenδ+η=1. Thus, IP-Accuracy Tolerant Reduct is mainly used as a representative.Theorem 4.3.9Quantitative Reduct Features(1)Approach: IP-Accuracy δ-Tolerant (η-Approximate) Reduct approaches IP-Accuracy Reduct, when δ → 0 (η → 1).Expansion: IP-Accuracy δ-Tolerant (η-Approximate) Reduct expands IP-Accuracy Reduct and could degenerate into the latter whenδ=0(η=1).Weakening: IP-Accuracy Tolerant (Approximate) Reduct is weaker than IP-Accuracy Reduct.(1) Item (1) is natural and could also be verified by Theorem 4.3.2 and Fig. 4. (2) According to Monotonicity V (Theorem 4.3.5), whenδ=0(η=1), IP-Accuracy δ-tolerance (η-approximate) preservation degenerates into IP-Accuracy preservation (i.e.,driaD(C*)=0). (3) IP-Accuracy preservation (i.e.,driaD(C*)=0) unidirectionally deduces IP-Accuracy δ-tolerance (η-approximate) preservation.□(1)Coreiaδ(C),Coreiaη(C)⊆Coreia(C);∀Bia∈Redia(C),0.35em0ex∃Biaδ∈Rediaδ(C),0.35em0ex∃Biaη∈Rediaη(C), s.t.,Biaδ,Biaη⊆Bia.IP-Accuracy Tolerant and Approximate Reducts originate from IP-Accuracy Reduct by introducing the applicable tolerance and approximation, respectively. Theorem 4.3.9 summarizes both quantitative reducts’ three fundamental features regarding qualitative IP-Accuracy Reduct, i.e., the approach, expansion, and weakening; in fact, they correspond to δ → 0 (η → 1),δ=0(η=1), δ > 0 (η < 1), respectively. Thus, quantitative reducts also exhibit directness regarding the qualitative reduct in a statistical way. Therefore, when compared to qualitative IP-Accuracy Reduct, IP-Accuracy Tolerant and Approximate Reducts become two essential types of quantitative reducts, thus holding efficient and extensive quantitative applications. In particular, Corollary 4.3.10 endows the above weakening feature with the hierarchical description. More hierarchical results of quantitative reducts will be intensively reflected in Section 4.4.For quantitative reducts’ heuristic algorithms, a representative is provided for calculating IP-Accuracy Tolerant Reduct. Clearly, the discrepancy rate could be chosen to construct key heuristic information:(35)sigiaδ(c,C*;D)=driaD(C*)−driaD(C*−{c}).Thus, we naturally develop the following Algorithm 2, which accords with the previous Algorithm 1. Herein, attribute’s sorting and choice based onsigiaδ(c,C*;D)are essentially equivalent to those based onsigia(c,C*;D), in view of the order-preservation of relevant granulation monotonicity.Algorithm 2IP-Accuracy discrepancy rate-based heuristic algorithm for IP-Accuracy Tolerant ReductInput: D-Table(U,C∪D)and tolerance rateδ;Output: IP-Accuracy Tolerant ReductBiaδ∈Rediaδ(C).1: ComputeCoreiaδ(C).2:Biaδ=Coreiaδ(C).3: whiledriaD(Biaδ)>δdo4:∀c∈C−Biaδ, calculatesigiaδ(c,Biaδ;D);choosec0=arg0.12em0exAPTARANORMALmaxc∈C−Biaδsigiaδ(c,Biaδ;D);letBiaδ=Biaδ∪{c0}.5: end while6: returnBiaδ.By simulating the above quantitative reducts regarding IP-Accuracy, we could similarly study the quantitative reducts regarding IIP-Accuracy, which more approach Pawlak-Reduct. Within the IIP-Accuracy framework, the discrepancy and equality rates becomedrIia(C*)anderIia(C*), while the tolerance and approximation rate thresholds δ′, η′ are usually in the range [0, 1]. The naturally defined IIP-Accuracy Tolerant and Approximate Reducts also approach/expand/weaken IIP-Accuracy Reduct. In particular, the two quantitative reducts mainly surround the classification-qualitative preservation (Theorem 3.3.7). Finally, we provide the reduction hierarchy for the IIP-Accuracy and IP-Accuracy quantitative reducts.Proposition 4.3.11IIP-Accuracy Tolerant/Approximate Reduct is usually weaker than IP-Accuracy Tolerant/Approximate Reduct, especially for appropriate tolerance/approximation rate thresholds.In Section 4.2, we provide two qualitative reduct types (IP-Accuracy/IIP-Accuracy Reduct) and their hierarchies regarding qualitative Pawlak-Reduct. In Section 4.3, we provide four quantitative reduct types (IP-Accuracy/IIP-Accuracy Tolerant/Approximate Reduct) and their hierarchies regarding qualitative IP-Accuracy/IIP-Accuracy Reduct. All of these constructional reducts belong to MT-Preservation Reduct because the underlying target holds granulation monotonicity; in particular, they hold some statistical systematicness because IP-Accuracy/IIP-Accuracy holds statistical systematicness; moreover, Tolerant/Approximate Reduct holds the directness. In this section, these reduct types’ hierarchy relationships are summarized. For this purpose, the above metrical and regional reduction fruits are first utilized to provide the following reduct hierarchy figure regarding IP-Accuracy, i.e., Fig. 5.Next, Fig. 5 is utilized to put forth some explanations:(1)All nine reduct types apply to the extensive multiple-category case. The qualitative reducts concern five reduct types (in the bottom half), which completely correspond to the previous preservation deduction targets (Fig. 2); herein, the set-region preservation reduct and distributional preservation reduct are naturally defined. In contrast, the quantitative reducts concern four reduct types (in the top half), which are provided in Section 4.3.Arrows represent the reduction hierarchy relation ⇒ (from reduction strengthening to weakening), which correspond to the deduction relation of reduction preservation targets. Thus, all of the reduct hierarchies become clear. In particular, IP-Accuracy Reduct and IIP-Accuracy Tolerant/Approximate Reduct become the strongest and weakest reduct types, respectively. To realize the hierarchical transformation of both, there are actually three different approaches, including IP-Accuracy Reduct ⇒ IP-Accuracy Tolerant Reduct ⇒ IIP-Accuracy Approximate Reduct.Reduct hierarchies closely adhere to GrC. In fact, different hierarchical reducts correspond to different results regarding the granular scale, presentative precision, and applicable generalization. Thus, IP-Accuracy Reduct exhibits the finest scale, the highest formal precision, and the narrowest generalization, while IIP-Accuracy Tolerant/Approximate Reduct exhibits the relevant opposites.According to hierarchical reduction theory [74,75], Fig. 5 could be further utilized to make hierarchical descriptions regarding the core and reduct, such as those in Corollary 4.3.10. Note that a strong reduct can provide some guidance for seeking weak reducts. Thus, the strongest IP-Accuracy Reduct is utilized to finally develop a hierarchical heuristic algorithm for searching IIP-Accuracy Approximate Reduct, which becomes Algorithm 3.Algorithm 3IP-Accuracy Reduct-based heuristic algorithm for IIP-Accuracy Approximate ReductInput: D-Table(U,C∪D), approximation rate thresholdη′;Output: IIP-Accuracy Approximate ReductBIiaη′∈RedIiaη′(C).1: Give IP-Accuracy ReductBia.2: YieldCoreIiaη′(C).3: SearchBIiaη′in rangeCoreIiaη′(C)⊆BIiaη′⊆Biato satisfy IIP-Accuracyη′-approximation preservation.4: returnBIiaη′.In Algorithm 3, Step 1 provides a strong IP-Accuracy Reduct for the hierarchical heuristics, Step 2 yields the core of weak IIP-Accuracy Approximate Reduct, and Step 3 performs the hierarchical heuristic search to gain the weak target: IIP-Accuracy η′-approximation preservation. As a result, this algorithm is convergent and effective and could achieve all IP-Accuracy Approximate Reducts included in the givenBia(regardless of independence/maximality). In particular, the hierarchical heuristic algorithm sharply contrasts with the metrical heuristic algorithm (e.g., Algorithm 2). The two algorithm types mainly differ in the heuristic formation respect; for heuristic development, the hierarchical structure and monotonous measure are utilized, respectively. Moreover, the two types of heuristic formation could be synthetically utilized to develop more efficient algorithms; for example, Algorithm 3 could be efficiently improved by introducing IIP-Accuracy into Step 3’s search.D-Table usually originates from a real-world example, and the relevant RS-Theory approach could depend on D-Table to bring extensive practical application performance. In this section, we mainly resort to D-Table to illustrate this paper’s approach. For convenience, the granule-statistical information and knowledge-structural hierarchy are provided after basic processing; thus, they are directly utilized to illustrate the three-way measures and fusional IP-Accuracy, IP-Accuracy’s GrC construction and granulation monotonicity, qualitative/quantitative reducts and their hierarchies.Example 2For D-Table(U,C∪D), the granule-statistical information regarding knowledgeCand concept X is provided in Table 3. Herein,|U|=100,0.35em0ex|U/C|=n=12,0.35em0ex|X|=40.Table 3 focuses on Bottom-System(U,C,X)via eight columns. Thus, these columns’ data are concretely utilized to illustrate the relevant subjects. First, the 1st column provides twelve granules’ serial number (i.e., i in[x]Ci), which comes from the interaction cardinality ordering in the 2nd column. The 4th, 5th, 6th columns exhibit generality, importance, and accuracy, respectively. The three-way measures completely correspond to three-way probabilities (i.e., the prior, posterior, and likelihood probabilities); in fact, they radically originate from and equivalently correspond to the two-initial data: the interaction and granule cardinalities (in the 2nd and 3rd columns). Moreover, the 7th column provides IP-Accuracy, which embodies the corresponding data product of 5th and 6th columns.Accuracy usually acts as the previous single causality measure, but exhibits only the relative causality. In this paper, importance with absolute causality is introduced to fuse accuracy, so IP-Accuracy is further mined to represent synthetic causality. Herein, IP-Accuracy’s improvements are explained based on accuracy.(1)There are two specific groups of granules with the same accuracy, i.e.,[x]C1,0.35em0ex[x]C4with accuracy 0.5 and[x]C6,0.35em0ex[x]C10with 1.0. In the usual accuracy environment,[x]C1and[x]C4cannot be discriminated, so they are treated equally in the same degree. However, they play different roles in the causality applications (such as rule reasoning) becauseiX([x]C1)>iX([x]C4). For causality,[x]C1actually holds more absolute contributions (based on importance) than[x]C4and therefore should gain more systematic attentions and synthetic availability/reliability. Furthermore, the fusional IP-Accuracy could provide a synthetic causality estimation, and[x]C1receives high reasoning trust becauseiaX([x]C1)>iaX([x]C4). The other granular group ([x]C6and[x]C10) also achieves a similar reasonable result.When compared to[x]C7and[x]C8,[x]C1and[x]C2depend on their higher importance and IP-Accuracy to gain higher attention and trust, though their accuracy is lower. As a result,[x]C10.25em0ex⇒0.25em0exXand[x]C20.25em0ex⇒0.25em0exXbecome more reliable than[x]C70.25em0ex⇒0.25em0exXand[x]C80.25em0ex⇒0.25em0exX, especially from the systematic and synthetic viewpoint.In this example, the granule-statistical information comes from Bottom-System(U,C,X)and is thus the finest one. Herein,C={a,b,c,d,e,f},U/D={X,¬X}; thus,(n,m)=(12,2),(|X|,|¬X|)=(40,60). Next, Fig. 6 provides the relevant knowledge-structural hierarchy. This hierarchy figure evolutes the initial large D-Table and thus becomes smaller and simpler. Herein, attributes a, b, c have more complete descriptions, while attributes d, e, f provide only some auxiliary functions.First, this hierarchy figure’s construction is simply explained.(1)For Kn-Coarsening fromC, the twelve granules regardingCare the finest, so they establish a basis for the knowledge-structural description. In particular, they depend on their accuracy to exhibit the following ordering: (6) (10) (7) (8) (2) (1) (4) (3) (9) (5) (11) (12); thus, they are represented by twelve serial points.Each attribute subset corresponds to a knowledge-structure, and the latter implies Gr-Merging and Gr-Preservation for the twelve granules. Thus, for each attribute subset and its knowledge-structure, small rectangles (containing serial points) mark Gr-Merging, and two vertical lines are used to defineX,0.35em0ex¬X.Knowledge’s coarsening relation is represented by the arrow to connect all knowledge-structures. Thus, this hierarchy figure is formed. In particular, the bottom half has part Kn-Coarsening regarding{a,b,c,d,e,f}0.25em0ex⇒0.25em0ex{a,b,c,d,e}0.25em0ex⇒0.25em0ex{a,b,c,d}0.25em0ex⇒0.25em0ex{a,b,c}; in contrast, the top half has all Kn-Coarsening regarding {a, b, c}’s subsets.Then, we calculate each knowledge’s IP-Accuracy, IIP-Accuracy, discrepancy and equality rates. Note that this calculation process radically reflects the relevant GrC construction, which is related to the sum integration. The computational results are shown in Table 4. Herein, we provide the presentative calculation process regarding knowledge {a, b, c}, where only five granules exist.(1)iaX(C*)=440×44+540×57+2540×2551+640×622+040×016=0.5366,ia¬X(C*)=060×04+260×27+2660×2651+1660×1622+1660×1616=0.6910;0.35em0exIia¬X(C*)=440×44=0.1000,Iia¬X(C*)=1660×1616=0.2667.iaD(C*)=iaX(C*)+ia¬X(C*)=0.5366+0.6910=1.2276;IiaD(C*)=IiaX(C*)+Iia¬X(C*)=0.1000+0.2667=0.3667.Similar to the above two steps, the initial twelve granules are utilized to obtain:iaD(C)=iaX(C)+ia¬X(C)=0.5437+0.6958=1.2395;0.35em0exIiaD(C)=IiaX(C)+Iia¬X(C)=0.1000+0.2667=0.3667. The two data regardingC– 1.2395 and 0.3667 – establish the comparative criterion to calculate the discrepancy and equality rates. Thus, regarding{a,b,c},0.35em0exdriaD=1.2395−1.22761.2395=0.0096=0.96%,eriaD=99.04%;0.35em0exdrIiaD=0.3667−0.36670.3667=0.00%,erIiaD=100.00%,Thus far, basic data processing has been completed. Next, Table 4 (as well as Fig. 6) is fully utilized to illustrate IP-Accuracy’s relevant GrC terms (Section 3), including the hierarchy construction, granulation monotonicity, equality condition, and preservation deduction.(1)Kn-Con IP-Accuracy aims to calculate all Gr-Con IP-Accuracy’s sum, and this process holds an integration from the granule to knowledge. The 2nd column verifies Kn-Con IP-Accuracy’s granulation monotonicity (Theorem 3.2.6), andiaX(C*)changes from maximum 0.5437 to limit 0.4000 in Kn-Coarsening. Herein, the same Kn-Con IP-Accuracy emerges in Kn-Coarsening{a,b,c,d,e,f}0.25em0ex⇒0.25em0ex{a,b,c,d,e}. The strict Kn-Coarsening has three groups of Gr-Merging, and they all satisfy the theoretical equality conditions (Theorem 3.2.6); in particular, Gr-Merging[x]{a,b,c,d,e,f}1∪[x]{a,b,c,d,e,f}40.25em0ex→=0.25em0ex[x]{a,b,c,d,e}1is in BND and C-BND, and the three granules concerned have the same accuracy, 0.5. The 3rd column could similarly verify Kn-Con IIP-Accuracy’s granulation monotonicity (as well as the equality condition on POS) (Theorem 3.2.10). Thus, Middle-System(U,C,X)’s inspection is finished.The 4th column’s data become the 2nd column’s double-data sum; clearly, this fact reflects Kn-Cl IP-Accuracy’s integration regardingX,¬X. The 4th column could verify Kn-Cl IP-Accuracy’s granulation monotonicity (Theorem 3.3.2). Concretely,iaD(C*)changes from maximum 1.2395 to limit 1; moreover, Kn-Coarsening{a,b,c,d,e,f}0.25em0ex⇒0.25em0ex{a,b,c,d,e}shows the theoretical equality conditions. Meanwhile, the 5th column – which comes from the 3rd column’s sum – could be used to exhibit Kn-Cl IIP-Accuracy’s granulation monotonicity (Theorem 3.3.7); in particular, there are more groups of Gr-Merging to illustrate the equality conditions.Top-System(U,C,D)is continuously used to illustrate preservation deduction (Fig. 2). OnlyC0.25em0ex⇒0.25em0ex{a,b,c,d,e}exhibits IP-Accuracy preservation. In contrast,C0.25em0ex⇒0.25em0ex{a,b,c,d,e},C0.25em0ex⇒0.25em0ex{a,b,c,d},C0.25em0ex⇒0.25em0ex{a,b,c},C0.25em0ex⇒0.25em0ex{a,c}all hold IIP-Accuracy preservation; they and only they have C-POS preservation. Thus, as is shown by this example, IP-Accuracy preservation unidirectionally deduces IIP-Accuracy preservation (Corollary 3.3.8), while the latter is equivalent to C-POS preservation (Theorem 3.3.7).Finally, Table 4 (as well as Fig. 6) is utilized to calculate qualitative/quantitative reducts and analyze their hierarchies (Section 4).(1){a, b, c, d, e} has IP-Accuracy preservation and has no proper subsets to satisfy the target; hence, it becomes the sole IP-Accuracy reduct. Only{a,b,c,d,e},0.35em0ex{a,b,c,d},0.35em0ex{a,b,c},0.35em0ex{a,c}exhibit IIP-Accuracy preservation; hence, the smallest {a, c} becomes the sole IIP-Accuracy reduct. For this two-category example, the three types of regional and qualitative reducts (i.e., the set-region preservation reduct, distributional preservation reduct, and Pawlak-Reduct) become equivalent, and they are further equivalent to IIP-Accuracy Reduct; hence, they also have the sole reduct {a, c}.The 6th and 7th columns provide the discrepancy and equality rates regarding IP-Accuracy and IIP-Accuracy, respectively. Thus, the granulation monotonicity and granulation boundedness are verified for relevant rates, e.g.,driaDis monotonous in [0.00%, 19.32%]. Letδ=2.5%∈[0.00%,19.32%]andη=97.5%∈[80.68%,100.00%]. Thus, only{a,b,c,d,e},0.35em0ex{a,b,c,d},0.35em0ex{a,b,c},0.35em0ex{b,c}exhibit IIP-Accuracy δ-tolerance preservation; hence, the smallest {b, c} becomes the sole IP-Accuracy Tolerance Reduct. In fact, {b, c} is also the sole IP-Accuracy Approximate Reduct. Similarly, letδ′=30%∈[0%,100%]andη′=70%∈[0%,100%]; then, {c} becomes the sole IIP-Accuracy Tolerance/Approximate Reduct.In particular, all reduct results are appended to Fig. 5. Thus, these relevant reducts naturally verify the relevant reduct hierarchies, which are exactly and completely in Fig. 5. For example, {a, b, c, d, e}⊇{a, c}, {b, c} could verify Theorems 4.2.2, 4.2.5, and Corollary 4.3.10. Finally, the hierarchical heuristic Algorithm 3 is appropriately illustrated. Concretely, the strongest IP-Accuracy Reduct {a, b, c, d, e} could provide the heuristic information regarding the internal inclusion to quickly search the weakest IIP-Accuracy Approximate Reduct {c}. □

@&#CONCLUSIONS@&#
Aiming at uncertainty measure mining and applications, this paper conducts systematic double-quantitative fusion of accuracy and importance to mine IP-Accuracy and further explores IP-Accuracy’s granular integration construction and attribute reduction applications. Herein, IP-Accuracy’s benign mining adopts the double-quantitative fusion strategy, while the later integration construction and attribute reduction mainly utilize the GrC technology. As a result, three-way probabilities and measures are systemically developed, causality-based IP-Accuracy is synthetically mined, a fundamental GrC platform is benignly constructed, and a hierarchical reduction system is fully established.By resorting to a monotonous uncertainty measure, this study provides an integration-evolutionary GrC construction strategy for attribute reduction. Note that IP-Accuracy and its reduction hold benign monotonicity and statistical systematicness. As a result, the innovative tolerant and approximate reducts, in a direct and statistical way, quantitatively approach/expand/weaken the ideal qualitative reduct; thus, they efficiently and extensively apply to quantitative environments, such as those with data noise. Moreover, MT-Preservation Reduct provides a unified normal pattern for monotonicity targets, thus holding great generalization significance.Several problems are retained for in-depth explorations.(1)The uncertainty measure fusion is fundamental for the uncertainty presentation and measurement. As is noted in Section 2.1, Approx-Space’s metrics usually play a dual role for the probability statistics and application semantics. However, only their semantics statuses are utilized for IP-Accuracy’s fusion mining and integration construction. Thus, their statistics positions and relevant uncertainty fusions are worth exploring further. In other words, the metrical mining and fusional application could proceed from the statistics viewpoint and even from the synthetic viewpoint.IP-Accuracy becomes the central uncertainty measure. It is mined mainly by a sort of system detection for three-way measures. In fact, accuracy and importance act as main causality factors in Bottom-System, and their benign fusion benefits from the weight function and synthetic semantics. As a result, fusional IP-Accuracy holds a non-linear promotion, and its evolutionary hierarchical form holds granulation monotonicity in both Middle-System and Top-System. Thus, the surplus absolute generality is worth introducing to mine more monotonous uncertainty measures. In other words, double-quantification work needs to be carried out in an in-depth manner.In this paper, IP-Accuracy’s GrC construction becomes one of the main contributions, and relevant granulation monotonicity underlies the in-depth attribute reduction applications. To establish attribute reduction, we actually provide a mature granular integration technology – an effective bottom-up strategy – for the evolutionary GrC construction. Thus, this hierarchical promotion approach is worth generalizing for extensive monotonous uncertainty measures, including existing and potential measures. In particular, when introduced into RS-Theory, classical entropy never has granulation monotonicity [76]; in contrast, weighted entropy could perform a granular integration strategy to construct the relevant conditional entropy and mutual information and to acquire final granulation monotonicity [76].IP-Accuracy attribute reduction is systematically investigated; herein, the tolerant and approximate reducts – two novel quantitative reducts – have wide potential for development in theory and in application. This metrical approach holds directness and statistics, so it differs from the model-regional method; thus, it is worth comparing to the latter, especially regarding quantitative performance. In particular, the illustrative example concerns only the two basic categories. Thus, further practical verification (especially regarding extensive multiple categories) is needed for both the efficiency of the relevant granulation monotonicity, reduct hierarchy, and heuristic algorithm and applicability of the tolerant/approximate reducts. Moreover, MT-Preservation Reduct is worth exploring further.This paper is devoted to the application of basic research, and the primary approach and theoretical contribution are thoroughly illustrated by a D-Table example based on granule-statistical information and the knowledge-structural hierarchy. In fact, offering a detailed, more real-world example could highlight the essence and relevance of our approach. In other words, it is worthwhile to verify the content of this paper with an exact real-world example, especially to illustrate the inherent effectiveness and efficiency. Furthermore, it is worth applying in a practical environment.Based on the work in this paper and taking into account the above issues, we are engaged in follow-up research and have already made some progress, including double quantification regarding accuracy and generality, metrical integration regarding the probability distribution, uncertainty fusion regarding weighted entropy, the GrC and reduction construction regarding information entropy and mutual information. Moreover, practical applications are concerned.