@&#MAIN-TITLE@&#
A hybrid approach based on stochastic competitive Hopfield neural network and efficient genetic algorithm for frequency assignment problem

@&#HIGHLIGHTS@&#
Our algorithm owns good adaptability to deal with some practical problems.Our algorithm obtains better or comparable performance than other algorithms.The solutions are helpful for hybrid between neural nets and evolutionary algorithms.

@&#KEYPHRASES@&#
Frequency assignment problem (FAP),Genetic algorithm (GA),Hopfield net,Hybrid algorithm,

@&#ABSTRACT@&#
This paper presents a hybrid efficient genetic algorithm (EGA) for the stochastic competitive Hopfield (SCH) neural network, which is named SCH–EGA. This approach aims to tackle the frequency assignment problem (FAP). The objective of the FAP in satellite communication system is to minimize the co-channel interference between satellite communication systems by rearranging the frequency assignment so that they can accommodate increasing demands. Our hybrid algorithm involves a stochastic competitive Hopfield neural network (SCHNN) which manages the problem constraints, when a genetic algorithm searches for high quality solutions with the minimum possible cost. Our hybrid algorithm, reflecting a special type of algorithm hybrid thought, owns good adaptability which cannot only deal with the FAP, but also cope with other problems including the clustering, classification, and the maximum clique problem, etc. In this paper, we first propose five optimal strategies to build an efficient genetic algorithm. Then we explore three hybridizations between SCHNN and EGA to discover the best hybrid algorithm. We believe that the comparison can also be helpful for hybridizations between neural networks and other evolutionary algorithms such as the particle swarm optimization algorithm, the artificial bee colony algorithm, etc. In the experiments, our hybrid algorithm obtains better or comparable performance than other algorithms on 5 benchmark problems and 12 large problems randomly generated. Finally, we show that our hybrid algorithm can obtain good results with a small size population.

@&#INTRODUCTION@&#
The frequency assignment problem (FAP) has received lots of attention these years due to its various applications including satellite communication systems, mobile telephone and TV broadcasting. Frequency assignment problems have arisen in many different situations in the field of wireless communications [1]. In this paper, we focus on the FAP in satellite communication system. In satellite communication system, the reduction of the cochannel interference has become a major factor for determining the system design [2]. Furthermore, due to the necessity of accommodating as many satellites as possible in geostationary orbit, this interference reduction has become an even more important issue with the increase in number of geostationary satellites [3]. To deal with interference reduction in practical situations, the rearrangement of frequency assignments is considered as an effective measure [4].FAP is a NP-complete combinatorial optimization problem and a lot of approaches have been proposed [5]. The application of neural networks in frequency assignment problems was first proposed by Sengoku et al. [6] and Kunz [7]. Then Kurokawa and Kozuka firstly proposed a Hopfield neural network (HNN) that consist of M×M neurons for FAP [8–10]. But the neural network of Kurokawa and Kozuka focuses only on minimization of the total interference [11,12]. Funabiki and Nishikawa [32] proposed a gradual neural network(GNN) that consists of N×M neurons. The disadvantage of GNN is its heavy computation, especially in large problems. Salcedo-Sanz et al. [3] combined a binary Hopfield neural network with simulated annealing (HopSA) for the FAP. The algorithm also cannot deal with large problems because of the excessive computation time. Recently Wang et al. [11] proposed a stochastic competitive Hopfield neural network (SCHNN) [11]. They introduced the stochastic dynamics to help the network escape from local minima. However, avoiding local minima only by the dynamics is not efficient. In these neural networks, the minimizations of the total interference and the largest interference are set in one energy function. In the process of evolution, the energy function will be gradually smaller and stable. Because the two objectives are not fully synchronized, the energy function cannot guarantee that the values of the two objectives decrease at the same time, there may even be a situation that one objective becomes better while another objective becomes worse. So in the process of solving FAP, the better way is to reserve and deal with these two objectives’ smallest solutions in each iteration, but it cannot be done in the neural network with only one energy function. There are also many genetic algorithms (GAs) proposed for FAP. Bremermann and Fraser can be considered as the pioneers of GAs [13,14]. Then the algorithm was considerably developed by Holland and Goldberg [15,16]. Cuppini, Kim and Lai were among the first papers found in literature to have applied GA to solve the channel allocation problem [17,18]. Then a lot of new genetic algorithms were proposed for FAP and one of the most recent one is Revueltal [19]. Ngo et al. and Beckmann et al. proposed a new strategy known as the Combined Genetic Algorith (CGA). CGA starts by estimating the lower bound z on bandwidth and its computation time is highly dependent on the z[20,21]. Then Lima et al. introduced two new Dynamic Channel Allocation (DCA) strategies using genetic algorithm. These two strategies are then compared with existing methods [22]. A novel DCA was applied using a Genetic Algorithm for the Wireless Access Network. The authors then categorised the existing channel allocation methods in terms of a Channel Allocation Matrix [23]. Performance of soft computing methods such as genetic programming was improved by embedding it with other potential soft computing methods such as neural network, M5 and statistical methods [24–31]. Furthermore, GA is a search and optimization method and it may get good solutions for FAP. The major differences of the genetic algorithms for FAP lie in the representation of different steps mentioned and the implementation. That is to say, the solutions obtained by GA may be unstable, since the solutions may be influenced by the initialization of the population, the operators of crossover and mutation, the population size and so on. The local restraint is also the weakness of GA. So we cannot get the suitable answer only by GA.In this paper, we first propose an efficient genetic algorithm (EGA) with five optimal strategies to cope with FAP. Then we try to find a hybrid algorithm which can overcome the shortcomings of the Hopfield-type neural network and the genetic algorithm while keeping their advantages. Three hybridizations between SCHNN and EGA are proposed and compared on five benchmark problems and three large problems in Group 4. The best hybrid algorithm among the three hybridizations is discovered and obtains good performance on all cases. In our research, the hybrid algorithm helps SCHNN to escape from local minima efficiently and increases the ability of EGA to find better solutions by using the neural network as an additional “mutation” operator that injects new good solutions. After that, two strategies for adding the outputs of SCHNN into EGA are discussed. Then SCH–EGA is also compared with other algorithms proposed before. On all cases, SCH–EGA obtains better or comparable performance. At last, in order to show the efficient performance of our hybrid algorithm with a small population, we test the effects of population sizes. In this experiment, our hybrid algorithm can obtain good performance with a small population scale, which reduces computation a lot.The contributions of this paper are: (1) a novel SCH–EGA algorithm is proposed to cope with the frequency assignment problem and obtains better results than other algorithms. As the good applicability, SCH–EGA can also cope with many other problems including the clustering, classification and the maximum clique problem etc; (2) three hybridizations between the neural network and the genetic algorithm are compared and we believe the comparison can also be helpful for hybrid algorithms between the Hopfield-type neural networks and other evolutionary algorithms, such as the ant colony optimization algorithm, the particle swarm optimization algorithm, the artificial bee colony algorithm, etc.; (3) five optimal strategies are proposed to built an efficient genetic algorithm which can also deal with the FAP very well.In previous methods for frequency assignment problems, researchers always focused on optimizing the powerful searching ability separately on the aspect of groups or individuals. The algorithms originated from GA almost realized their searching based on the characteristic of population evolution. As an independent algorithm, Hopfield-type neural networks were improved generally on the basis of their neural properties. Differently with these previous researches, in this study, Hopfield-type neural networks treated as chromosomes of genetic algorithm are embedded into genetic algorithm to generate hybrid searching capability for optimization problems. Several hybrid methods we studied reveal the different properties and capability of SCH–EGA, which could produce inspiration to the research of hybrid algorithm. As our algorithm integrates the searching capability of both GA and SCHNN, it shows superior performance on solving FAP. To our knowledge, there are few algorithms adopting the similar hybrid optimization method to solve FAP.The rest of the paper is organized as follows: in the next section we define and analyze the FAP. In Section 3, we briefly introduce some methods to solve the problem. Section 4 we propose an efficient genetic algorithm(EGA) to cope with FAP. Then the hybrid approach based on stochastic competitive Hopfield neural network (SCHNN) and Efficient Genetic algorithm (EGA) is described. Section 5 shows the performance of the SCH–EGA algorithm, by solving a set of benchmark problems and comparing the results obtained with previous algorithms for the FAP. Finally, Section 6 ends the paper with concluding remarks.In this section, FAP follows the problem formulation by Mizuike [4]. First, we describe the FAP in satellite communications systems as a combinatorial optimization problem with three constraints and two objectives.Given two adjacent satellite systems (Fig. 1), FAP consists of reducing the inter-system cochannel interference by rearranging the frequency assignment on carriers in system #2(M segments, N carriers), while the assignment in system #1(M segments) remains fixed. Because each carrier usually occupies a different length in a frequency band, Mizuike et al. introduced the segmentation of carriers so that each carrier can be described by a collection of consecutive unit segments. The interference between two M-segment systems is described by a M×M interference matrix IM (Fig. 3), in which the ijth element eijstands for the cochannel interference when segment i in system #2 uses a common frequency with segment j in system #1.The three constraints of FAP are:(1)Every segment in system #2 must be assigned to a segment in system #1.Each segment in system #1 can be assigned by at most one segment in system #2.All segments of each carrier in system #2 must be assigned to consecutive segments in system #1 in the same order.The two objectives of FAP are shown as follows. Note that the first objective has a higher priority over the second one.(1) Minimize the largest element of the interference matrix selected in the assignment. (2) Minimize the total interference of all the selected elements.Fig. 2shows the characteristics of the combinatorial optimization for FAP. Three and four carriers are utilized there in each satellite system, respectively. The assignment in system #1 remains fixed, while the segments in system #2 are rearranged. Then the new interferences between the two systems are generated. The cochannel interference is evaluated by each pair of carriers using the same frequency. And the interference is calculated by the interference matrix IM.As Fig. 2shows, the initial assignment and the optimum assignment, totally two schemes, exist for the FAP. In the initial assignment, segment S21 in system #2 corresponds to the segment S11 in system #1. Then the interference generated by S21 equals the element e11 in IM in Fig. 3. As the length of carrier C21 is one, only S21 is occupied by C21. So the interference generated by C21 equals to the interference generated by S21. C22 owns two segments, S22 and S23. The segments S22 and S23 in system #2 correspond to S12 and S13 in system #1, respectively. Then the interferences generated by C22 are the elements e22 and e33. In the same way, the interference of C23 equals to the element e44 and the interferences of C24 are the elements e55 and e66. So in the initial assignment, the interferences of the six segments in system #2 are 20, 10, 30, 0, 50, 50. The largest interference is 50 and the total interference is 160.In the optimum assignment, S21 in system #2 corresponds to S16 in system #1. So the interference generated by C21 equals to the element e16. By the same token, the interferences C22 generates are the elements e24 and e35. The interference C23 generates equals to the element e41. The interferences C24 generates are the element e52 and e63. Thus, in the optimum assignment, the interferences generated by the six segments in system #2 are 30, 5, 25, 0, 15, 25, respectively. Then the largest interference is 30 and the total interference is 100. Both the largest interference and the total interference are smaller than that in the initial assignment.As it is discussed above, there are two objectives for FAP and both of them are to minimize values. Note that the first objective owns higher priority than the second one. But in FAP, the two objectives are not fully synchronized which means that a solution with a smaller largest interference may have a larger total interference. And there may be also many cases that solutions with same largest elements have different total interferences. So the two objectives should be considered together.Neural network models are common solutions for solving FAP. The neural network is composed of a large number of massively connected simple processing elements. The processing element is called a neuron because it performs the function of a simplified biological neuron model. In the neural network model, the number of carriers and segments should be determined first. We define N as the number of carriers and M as the number of segments. Then a N×M matrix can be built for the neural network model. Each element in the N×M matrix has one input Uijand one output Vij. The input of the element is connected with outputs of several processing elements, including the processing element itself. The interconnections between neurons are given by the motion equation(1)dUidtϑE(V1,V2,…,Vn)ϑViwhere Uiand Viare the input and the output of neuron i respectively, and n is the number of required neurons for the problem.Many neural network approaches were proposed for FAP and they dealt with FAP very well [33,34]. In the early neural networks for FAP, the models were built as a M×M binary matrix in a M-segments system, which has a same structure with the interference matrix IM. In the M×M matrix, there is one and only one neuron with an output in each row, others are zero. The output Vij=1 means that the segment i in system #1 corresponds to the segment j in system #2. Thus, there are M elements with output distributing on different rows and different columns. The largest interference is the largest element selected in the M output elements. And the total interference is the sum of all the elements with an output.Funabiki and Nishikawa [32] proposed a gradual neural network (GNN) that consists of N×M neurons. In the N×M neural network model, N stands for the number of carriers and M is the number of segments. All the neurons in the matrix have one input value Uijand one output value Vij. The input value is a real value and the output value is a binary value. The “one output” (Vij=1) means that the carrier i in system #2 is assigned to segments j−(j+ci−1). Note that cirepresents the number of segments or the length of carrier i. The “zero output” (Vij=0) indicates no assignment. The form of the N×M matrix neural network has many advantages for FAP. First of all, this model itself has met the third constraint of the FAP. Thus, once the N×M neural network model is built, only the first two constraints need to be considered. And second, this model reduces much space and computation, especially if the problems are very large. The last advantage of this model is that it is very simple so that it satisfies many other models. Thus, this N×M model has been used for many different neural network approaches for FAP. In our paper, this N×M model is combined with our genetic algorithm easily to cope with FAP and obtains good performance.Fig. 4shows the output of the N×M model in the optimum assignment. It is clear that there is one and only one “one output” in each row. The neuron with an output Vij=1 means that the carrier i occupies the segments j-(j+ci−1). Note that ciis the length of carrier i.In the N×M neural network, building the largest interference matrix and the total interference matrix is the basic process for solving FAP. The largest interference matrix and the total interference matrix are also N×M matrixes. Each element Vijin the largest interference matrix should be calculated as follows: Vij= max{ekj,…,ek+ci−1,j+ci−1}, where k is the first segment number of carrier i in the M×M interference matrix IM. For example, V(2, 2) means that the carrier 2 is assigned to the segments S12 and S13 in system #1. The interferences generated by the two segments are 10 and 30. So 30 is chosen as the largest interference value for V(2, 2) in the largest interference matrix. For the total interference matrix, each element Vijin the largest interference matrix equals to the sum of the elements carrier i is assigned to. That is to say, each element Vijin total interference matrix is calculated according to the rule Vij=sum{ekj,…,ek+ci−1,j+ci−1}, where k is the first segment number of carrier i in the M×M interference matrix IM. So the total interference of the V(2, 2) is the sum of 10 and 30. The largest interference and total interference matrixes are shown in Fig. 5(a) and (b), respectively.Energy function is the common approach for setting the constraints in neural networks. As the N×M binary matrix neural network model that Funabiki and Nishikawa proposed has satisfied the third constraints, two energy functions are built for the first two constraints. The first constraint is that each first segment of N carriers in system #2 must be assigned to one of M segments in System #1, that is to say, in the neural network matrix, each group(each row) must have one output. The first energy function is given by(2)E1=∑i=1N∑q=1MViq−12When every carrier is assigned, E1 becomes zero.The second constraint is that each segment in system #1 can be assigned by at most one segment in system #2. In other words, when carrier i is assigned to segments j−(j+ci−1), carrier p(p≠i) must not be assigned to segments (j−cp+1)-(j+ci−1). The second energy function is built for the second constraint.(3)E2=∑i=1N∑j=1M∑p=1p≠iN∑q=j−cp+1j+ci−1VijVpqWhen the second constraint is satisfied, E2 becomes zero. Then a total energy is built by the summation of E1 and E2(4)E=A2∑i=1N∑q=1MViq−12+B2∑i=1N∑j=1M∑p=1p≠iN∑q=j−cp+1j+ci−1VijVpqwhere A and B are coefficients. We believe that it has met all the constraints when E becomes zero. Note that for FAP, the state that E becomes zero is just a basic requirement, the main work is to find a solution that has the smallest interference.A lot of neural network models were proposed to cope with FAP. All the neural network models have both one input matrix U and one output matrix V. The output matrix updates depending on the input matrix U. And the results of the output matrix V affect the input matrix U. By the promoting of the updating for each other, the input matrix U and the output matrix V go to a stable states. The stable states may be the best solution for FAP, and it may also be the local minima state. To escape from the local minima, many methods were proposed such as the climbing hill algorithms. The main differences between different models are the updating strategies for U and V. Nobuo Funabiki et al. do a good job for FAP by using the neural network. They proposed a gradual neural network (GNN) which cannot only satisfy the third constraint of FAP, but also reduce much more computation than the M×M matrix. To the updating strategies, GNN mainly uses a gradual expansion scheme for the goal function optimization. In the N-carrier-M-segment model, the NM neurons are classified into P groups by the cost order. In the beginning stage, only the neurons in the first group update. That is to say, the NM neurons updates asynchronous. With the increase of iterations, neurons in the second group update with the neurons in the first group at the same time. In the end, all the neurons will update in one iteration. The GNN reduces the required number of neurons, but the multiphase searching leads to heavy computation burden. So GNN cannot be applied for FAP with large size.Recently Wang et al. proposed a stochastic competitive Hopfield neural network (SCHNN) for solving FAP. The SCHNN also uses the N×M matrix as the basic neural model. In SCHNN, the updating strategies of Uijand Vijare shown in Eqs. (5) and (6). In the ith row, the Vij=1 if the Uijis the maximum in the row according to Eq. (6). Other Vip(p ≠ j) equals to 0. If the numbers of the maximum elements are more than one, it randomly choose a Uijin the maximum elements as the result, the outputs of other elements become zero Vip=0(p ≠ j). One and only one neuron within each group (row) is fired at every time t.(5)uij′(t)=α(s)·uij(t)(6)vij(t+1)=1,uij′=maxk=1,…,M{uik′(t)}0,otherwiseIn Eq. (5), s is the updating step number defined above, α(s) is a random multiplier which will be discussed about later, anduij′(t)is the transient variable. The multiplier α(s) in Eq. (5) is given by α(s)=random(h(s), 1)whereh(s)=1−T·e−s/λ.In SCHNN, it satisfies the rule that large carriers with many segments should be assigned as early as possible, otherwise, it would be difficult to assign them after many carriers have been already assigned [32]. Thus, the updating rule of the input matrix is as shown in Eq. (7)(7)uij(t)=−W2∑p=1,p≠iN∑q=max(j−cp+1,M)min(j+ci−1),Mcpvpq(t)−W3d′Consider Eqs. (5) and (7) as a whole, in SCHNN, the input matrix U and the output matrix V evolve together and the finally solutions can be obtained. SCHNN can cope with FAP very well and the solutions obtained by SCHNN are better than other algorithms. But many disadvantages still exist. SCHNN cannot escape from local minima efficiently, which is the common problems for neural networks. And the two objectives of FAP are set in one energy functions, but the two objectives are not fully synchronized which means that a solution with a small total interference while the largest interference element may be more larger. In the following sections, we propose an efficient genetic algorithm (EGA) to help SCHNN escape from local minima and the hybrid algorithm SCH–EGA can obtain better performance.Genetic algorithms (GAs) are a family of computational models which work by mimicking the evolutionary principles and chromosomal processing in natural genetics. GA can deal with the frequency assignment problem very well as an energy minimization problem.The basic idea of GA is that better solutions can be obtained by many iterations of the crossovers, mutations and eliminations in the population. There are several basic steps in GA. The first step is the determination of the parameters. The parameters include the population scale, the definition of the fitness function, the structure of individuals and the definition of the operations. As two objectives exist in FAP, two fitness functions should be made. The first fitness function is about the largest interference in the assignment and the second one is about the total interference. When the initialization is complete, the operations of the crossover, mutation and elimination begin. In the crossover step, the individuals in pairs exchange parts of their structure to generate new individuals. In the mutation step, new individual arises based on the mutation rules. The last operation is the elimination step. Before elimination, the fitness values for each individual in population will be calculated. The individuals with low fitness values will be eliminated. The individuals with high fitness functions will be kept for next generation. Then the next iteration begins until the best solution is found.GA is particularly suitable for solving NP problems and it always can obtain better solutions. Many different GAs were proposed for FAP. The major difference lies in the representation of different steps and its implementation. So the definitions of the operations and the fitness functions are very important for the performance. At the same time, GA with more individuals in population may generate better solutions, but it also requires much more computation. Thus, for the GA we proposed, not only the good performance for FAP should be guaranteed, but also the small computation should be promised.In our research, five optimal strategies are proposed to build an efficient genetic algorithm (EGA) which is a component of our hybrid algorithm. GA has been successfully used in solving various problems because its simplicity and global perspective. Skillful definitions of the operations and fitness functions are always needed in GA to get good performance [35,36]. The definitions sometimes may be very complex and require much computation. However, EGA can not only obtain good results but also require a little computation. At the same time, EGA can combine with SCHNN easily. First, the five optimal strategies for EGA are shown as follows:The first strategy IS is defined as the foundation of EGA and it determines the structure of every individual. According to the strategy IS, each individual in EGA should be a N×M binary matrix and one and only one for each group (row) must have one output. The one with an output means that its value is 1. In this way, each individual in EGA has the same structure as in the SCHNN. So the strategy IS also guarantees the agreement between individuals in EGA and outputs in SCHNN.The calibration strategy is aimed to modify each individual to satisfy the second constraint of FAP. And this strategy will be executed after the crossover and mutation operations. When the calibration step is over, all individuals in population satisfy the three constraints of FAP, which means that each individual may be a solution for FAP. We just need to choose the best one in population as the final result. There may be some repetitions generated by strategy CS and these repetitions will be eliminated in each iteration. The Calibration Strategy is shown in Algorithm 1.Algorithm 1Calibration Algorithm1:Each individual is a N×M binary matrix, and there is one and only one output each row;2:for each individual in population do3:forj=1 to Mdo4:fori=1 to Ndo5:ifVij==1then6:ifVpq==1 (p=1 to N and p≠i, q=j to (j+ci−1))then7:Vpq=0;8:Vp((q+1)%N)=1;9:end if10:end if11:end for12:end for13:end forAs there are two objectives for FAP, two fitness functions should be built simultaneously. The first fitness function is about the largest element of the interference matrix selected in the assignment. And the second fitness function is about the total interference of all the selected elements. The fitness values will be higher if the largest interference value or the total interference value is smaller.Strategy ES is proposed to eliminate the repetitions and those individuals with low fitness values. And the individuals with high fitness values will be kept for next generation. The detailed elimination steps are shown as follows: First, the two fitness values for each individual in population should be calculated. Then all individuals will be sorted by the two fitness values, respectively. The parameter Lnumis set as the number of individuals chosen for next generation by the first fitness function and parameter Tnumis the number of individuals chosen by the second fitness function. The sum of Lnumand Tnumis the population size Inum. After sorting individuals by their fitness values, the repetitions in population will be eliminated first. Then Lnumindividuals with least largest interferences will be chosen as in the population for next generation. After that Tnumindividuals with least total interferences will be chosen. If the individual has been chosen by the largest interference, it will be ignored and the next individual will be checked. At last, the individuals kept for next generation are those with small largest interferences or total interferences.The population should be kept in a small scale, which means that the value of Inumshould not be very large. In our research, EGA can obtain good performance with a small population size. In fact, with the increase of individuals in population, the improvements of solutions are limited, but the computation increases a lot.Thus, the basic steps of EGA are shown in Algorithm 2.Algorithm 2EGA Algorithm1:Set Inum, Lnum, Tnum;2:Define the two fitness functions and the operations;3:Initialize the population;4:whilestability criteria is not satisfieddo5:Crossover;6:Mutation;7:Calibration for every individual;8:Calculate the two fitness values for each individual;9:Elimination;10:end whileIn EGA, the system and the parameters should be set first. That means the structure of each individual, the population size Inum, the parameters Lnumand Tnumare determined above all. Then operations in EGA and the two fitness functions will be defined. The operations should be simple and efficient enough. At last, the repetitions and individuals with low fitness values will be eliminated and the next iteration begins. Specifically, Inumindividuals are randomly generated first. All individuals in population are N×M binary matrixes. For each individual, there is one and only one output in each group (row). The value of the output is 1 and other values are 0. After the initialization, the crossover operation begins. In this step, two individuals are set as one pair. In each pair, the two individuals exchange one row which is randomly chosen. Then two new individuals are generated by the crossover operation for each pair. Note that the new individuals generated by the crossover operation also satisfy the first and the third constraints of FAP. At the same time, the computational cost in this operation is small. After that, the mutation operation starts. One individual in population is randomly selected and the outputs for each row are generated. The individual generated in mutation step can also satisfy the first constraint that one and only one output should exist in each row. Calibration step is conducted after the crossover and mutation operations. By the calibration step, each individual is modified to satisfy the second constraint. As all individuals in population have satisfied the first and third constraints, each individual after the calibration step may be a solution for FAP which has satisfied all the three constraints. Last step in EGA is the elimination operation. In this step, the repetitions will be eliminated first. Then Lnumindividuals with least largest interferences and Tnumindividuals with the least total interferences will be kept for next generation. If the best solution in EGA is not good enough, next iteration will begin.A lot of hybrid algorithms have been proposed to deal with FAP and the solutions have become much better [37,35]. As the good performance of neural networks and genetic algorithms for FAP, many hybrid algorithms about the two algorithms are proposed [3,38]. Thus, we propose a stochastic competitive Hopfield neural network-efficient genetic algorithm (SCH–EGA). In our experiments, SCH–EGA can obtain better or comparable performance than other algorithms. In fact, our hybrid algorithm owns good adaptability. That means our hybrid algorithm can not only deal with the frequency assignment problem, but also cope with the problems of clustering, classification, the maximum clique problem and so on.In order to guarantee the good generality which can be used for hybridizations between neural networks and other evolutionary algorithms, our hybrid algorithm is just in a linear structure. In our research, three hybridizations between SCHNN and EGA were proposed for different purposes. The first hybrid algorithm is designed to explore the performance that using a Hopfield-type neural network to improve the ability of the genetic algorithm to search better solutions. And we call it N-E algorithm for short. The neural network in this hybridization will be seen as an additional “mutation” operator which can inject new good solutions into the evolutionary process. Specifically, the outputs of SCHNN will be added into EGA as high quality individuals to participate in the operations in EGA. The detailed hybrid algorithm (N-E) is shown in Algorithm 3.Initializations for SCHNN and EGA are the first two steps in the first hybrid algorithm. For the initialization of SCHNN, we need to build a 2-dimensional neural network with N×M neurons. N stands for the number of the carriers and M stands for the number of segments. The binary output Vij=1 of neuron ij represents that carrier i in system #2 is assigned to segments from j to (j+ci−1) in system #1. Note that cirepresents the number of segments or the length of carrier i. Then the energy function and the updating rules are defined as Eqs. (4), (6) and (7). In the initialization part of EGA, Inumindividuals will be randomly generated. Each individual in population is a binary N×M matrix. Then the operations of the crossover, mutation, calibration and the elimination are defined as before. In fact, the three hybridizations between SCHNN and EGA share the same initialization step.Algorithm 3N-E Algorithm1:Initialize the parameters;2:Set t=0;3:whilet< the max number of iterations or stability criteria are not satisfied do4:fori=1 to Ndo5:forj=1 to Mdo6:s=⌊t/N⌋;7:Compute the Uijwith Eq. (5);8:end for9:forj=1 to Mdo10:Update Vijusing Eq. (6);11:end for12:end for13:Add the solution of neural network to the population as an individual;14:Crossover;15:Mutation;16:Calibration;17:Calculate the two fitness values for each individual;18:Elimination;19:t=t+1;20:end whileAs EGA relies on combinations of individuals to seek better solutions, the better individuals in population, the better solutions EGA may obtain. So in the first hybrid algorithm, the outputs of SCHNN will be added into the population as high quality individuals to participate in the operations with other individuals in population. In our research, two strategies were compared for this operation. The first strategy is that the output will be added into population in each iteration. The second strategy is that the poor outputs should be ignored and the individuals which will be added into population should be the high quality individuals as good as possible. The two strategies will be explored in the experiment section. As the good performance of SCHNN for FAP, many high quality outputs will be generated from SCHNN. And then these outputs may help EGA to obtain better solutions. As the good outputs of SCHNN will be added into EGA, we just need to consider whether EGA has found the best solution or it runs to the maximum iterations. When the algorithm ends, the best solution in EGA is the finally result of the first hybrid algorithm.Unlike the first hybrid algorithm, the second hybridization which is called E-N algorithm for short is aimed to test whether the genetic algorithm can help a neural network to escape from the local minima efficiently and obtain better results by combination. The core idea of the second hybrid algorithm is that the best solution in EGA should be chosen to compare with the output of SCHNN. If the solution from EGA is better than the output in SCHNN, the output will be replaced by the solution. Otherwise, the output remains the same and the next iteration begins. In the second hybrid algorithm, SCHNN and EGA will be conducted respectively. And the two fitness values for each individual in EGA will be calculated. The solution with the least largest and total interference in EGA will be taken as the representative to compare with the output of SCHNN. If the output of SCHNN does not change in many steps, or the algorithm runs to the maximum step, the algorithm will be ended. The output of SCHNN is the finally solution. The second hybrid algorithm is shown in Algorithm 4.Algorithm 4E-N Algorithm1:Initialize the parameters;2:Set t=0;3:whilet< the max number of iterations or stability criteria are not satisfieddo4:fori=1 to Ndo5:forj=1 to Mdo6:s=⌊t/N⌋;7:Compute the Uijwith Eq. (5);8:end for9:forj=1 to Mdo10:Update Vijusing Eq. (6);11:end for12:end for13:Crossover;14:Mutation;15:Calibration;16:Calculate the two fitness values for each individual;17:Elimination;18:ifthe solution of neural network is worse than the best individual in populationthen19:Replace the solution of neural network by the best individual;20:end if21:t=t+1;22:end whileThe third hybridization (N-E-N) is a combination of the first two hybrid algorithms. In the third hybrid algorithm, not only the outputs of SCHNN should be added into population to participate the operations, but also the best solutions in EGA will compare with the outputs of SCHNN. So the third algorithm helps the SCHNN and EGA simultaneously. When the best solution in EGA does not change in many iterations or the algorithm runs to the maximum iteration steps, the algorithm will be ended. Best solution in EGA is the finally result we needed. The third hybrid algorithm is shown in Algorithm 5.Algorithm 5N-E-N Algorithm1:Initialize the parameters;2:Set t=0;3:whilet< the max number of iterations or stability criteria are not satisfieddo4:fori=1 to Ndo5:forj=1 to Mdo6:s=⌊t/N⌋;7:Compute the Uijwith Eq. (5);8:end for9:forj=1 to Mdo10:Update Vijusing Eq. (6);11:end for12:end for13:Add the solution of neural network to the population as an individual;14:Crossover;15:Mutation;16:Calibration;17:Calculate the two fitness values for each individual;18:Elimination;19:ifthe solution of neural network is worse than the best individual in populationthen20:Replace the solution of neural network by the best individual;21:end if22:t=t+1;23:end whileIn this section, we have shown the three hybridizations between SCHNN and EGA. The comparison is important for our hybrid algorithm. As the comparison has good generality, we believe that it can also helpful for hybridizations between neural networks and other evolutionary algorithms. The first hybrid algorithm is proposed to explore the performance of improving the quality of the genetic algorithm to search better solutions by adding the outputs from a Hopfield neural network as an additional “mutation” operator. In this hybridization, many outputs as high quality individuals from the neural network will participate in the operations in the genetic algorithm. The second hybrid algorithm is aimed to help the neural network to escape from local minima efficiently and obtain better results by the genetic algorithm. The best solutions from the genetic algorithm will be compared with the outputs of the neural network. If the output is worse than the result, it will be replaced by the result. In EGA, the operations require a little computation. So compared to the neural network, the computation increasing by EGA is a little. The third hybridization combines the first two hybrid algorithms. Not only the outputs from the neural network will be added to the genetic algorithm as high quality individuals, but also the best solutions in EGA will be compared with the outputs in neural network. In our research, all the three hybrid algorithms obtain good performance and the third algorithm is the best one. That means that the combination between the neural network and the genetic algorithm is helpful for both the two algorithms.Many experiments were conducted for our research. First, the performances of the three hybridizations between SCHNN and EGA were tested and we discovered that the third hybrid algorithm is the best one. Second, two strategies that adding the outputs of SCHNN to EGA were discussed. Then many algorithms proposed before were compared with SCH–EGA. The last experiment shows the effects of population size for our hybrid algorithm and the good performance of SCH–EGA with a small population scale.In our experiments, the algorithms are programmed with Matlab 7.0 for utilizing its powerful capability of matrix computing. Simulations were implemented on a PC(Core(TM) i5-3450 3.10GHz, 8.0G RAM). We tested our SCH–EGA approach on four groups with a total of 17 cases like in Wang et al. (2008). Benchmark instances BM1-BM5 are from Funabiki and Nishikawa [32], where these were called instances 1–5, respectively. The benchmark instances BM1–BM5 are valuable to evaluate the performance of optimization algorithms, and have widely adopted in previous researching works. The comparison results on these benchmark instances could directly release the algorithm solving capability. Moreover, to reveal the robustness for different kinds of instances, the instances in Group 2–4 are randomly generated. These instances are generated in the following steps according to the method and cases parameters as [11].(1)Choose the number of carriers N and the number of segments M for the instance.Select the values of the range of carrier length and the range of interference between the two systems.Generate a set of carrier length ci(i=1, …, N) and interference matrix IM(M×M) using uniformly distributed random values in the range defined in the step above.According to the steps above, three instances in Group 2 are generated to observe the influence of magnitude of the interference by varying the interference from 1–10 to 1–1000, while the number of carriers N, the number of segments M, and the range of carrier length are fixed to 50, 200 and 1–10, respectively. Group 3 is designed to show the effects of the carrier length and the number of segments M. Group 4 (Case 15–17) is designed to test the effects of the number of carriers N and the performance on large problems. The three groups of random instances are ranked to form different types of instances with different complexity, which products efficient evaluation for frequency assignment problems, as described in [11]. The four groups in our simulations are shown in Table 1.The first experiment is set to compare the three hybridizations between SCHNN and EGA. First, the three hybridizations were tested on the five benchmark problems. The results include the largest interference, the total interference and the running time. As the benchmark problems are very simple, all the three hybrid algorithms obtain the same largest interferences on BM1–BM5. But many differences of the total interferences on BM3 - BM5 exist. The results of the total interferences on BM3, BM4 and BM5 are shown in Fig. 6.From Fig. 6, it can be found that on the three benchmark problems, the results obtained by the second hybrid algorithm (E-N) are worse than the other two algorithms. And the third algorithm (N-E-N) can get the best solutions. This phenomenon becomes more obvious with the enlargements of problems. On BM3, the total interference of both the first and the third hybrid algorithms is 85 while it in the second algorithm is 88. That is mainly because the operations in EGA are very simple and the simplicity limits the ability of EGA to search better solutions. When SCHNN runs to the local minima, EGA cannot help SCHNN escape from the local minima. Although the simplicity limits the ability of EGA, it reduces computation a lot and we can improve the ability by adding the outputs of SCHNN as high quality individuals. The better performance of the first and the third algorithms shows the importance that adding the outputs of SCHNN to EGA. The phenomenon that the third algorithm obtains better solution than the first algorithm (N-E) shows the necessity that helping SCHNN to escape from local minima by EGA. As the third hybridization is the combination of the first two hybridizations and obtains the best performance, both adding the outputs to EGA and helping SCHNN to escape from minima by EGA have beneficial effects. That is to say, the hybridization between the two algorithms makes up for the defects in SCHNN and EGA while fully using the advantages of the two algorithms. That also proves that the hybridization between the two algorithms for FAP is very sensible.As the five benchmark problems are not complex enough, the three hybridizations were also tested on large problems. Table 2shows the results of the three algorithms on Group 1 (BM1–BM5) and Group 4 (Case 15–17).In Table 2, it is clear that the third hybrid algorithm obtains the best performance and its advantages are more obvious on large problems. At the same time, the difference of the running time on one case between the three hybridizations is a little. Since the three hybrid algorithms are in the serial structure, the running time is the sum of SCHNN and EGA. Thus, there is a little difference in the running time between the three hybrid algorithms. Then we choose the third hybrid algorithm as the best hybridization between SCHNN and EGA.This experiment is very important for our hybrid algorithm. Three hybridizations between SCHNN and EGA were compared on five benchmark problems and three large problems. In the experiment, the performance of the third algorithm is the best and the second algorithm is a little worse than the two algorithms. The results show that our hybrid algorithm cannot only help the genetic algorithm to find better solutions by adding high quality individuals from the neural network, but also help the neural network to escape from the local minima efficiently by the genetic algorithm. As the hybrid algorithms between neural networks and evolutionary algorithms are very common, we believe that this experiment can be helpful for hybridizations between neural networks and other evolutionary algorithms, such as the artificial bee colony algorithm, ant colony optimization algorithm, the particle swarm optimization algorithm and so on.As the first experiment shows, the third hybridization obtains the best performance among all the three hybridizations. In the third hybrid algorithm, the outputs of SCHNN will be sent to the population as high quality individuals. Two strategies shown as follow are compared for the adding way in this experiment.•Successive Strategy: This strategy is that the outputs of SCHNN should be added to the population in each iteration. It means that whether the output of SCHNN is good or bad, it needs to participate in the operations with other individuals in EGA. This strategy is very simple and promises the good performance of the hybrid algorithm.Skip Strategy: This strategy is used to avoid some poor outputs to be added to the EGA. In fact, it is clear that the outputs of SCHNN will be better and better with the increase of iterations, which means that the outputs in the first iterations may be not good. Adding each output into the population may increase the computation a lot and be not efficient. Thus, the second strategy is proposed to cope with this problem.The function used in the second strategy is shown in Eq. (8). Actually, it has been used inuij′(t)=α(s)·uij(t). Neural networks have an important property that the solutions will be better with the increase of iterations. So in the first iterations there are a lot of poor outputs which do not need to be added into EGA to participate in the operations. The second strategy (Skip Strategy) uses the functions h(x) and α(t) to deal with this problem. The graphs of the two functions are shown in Fig. 7. Initially, h(0)=0, therefore α(0) is a random value that selected from the range[0,1]. Then the value of h gradually approximates to 1. The parameter Tvalis defined as the threshold value. In one iteration, if the value of α(t) is less than Tval, the output of SCHNN does not need to be added into EGA. And the output should be added into the population if the value of α(t) is larger than Tval. From Fig. 7, it is clear that in the first iterations, only a little outputs will be added into EGA. And with the increase of iterations, the probability of adding outputs to EGA increases. Thus, by function Eq. (8), the second strategy can reduces some computations, especially in the first iterations. But this strategy itself increases some computations which generated from Eq. (8).(8)α(t)=random(h(t/N),1)where h(x)=1−e−x/λ.As far as we known, the function Eq. (8) was proposed by Wang et al. and plays very well for the random problems [33]. This algorithm provides a mechanism for hill-climbing by varying probability choosing outputs to added to EGA as high quality individuals. Fig. 7 depicts how the value of t changes for λ=60 and Tval=0.5. The function Eq. (8) is based on the exponential function h(x) and generates the randomness. At first, many values generated by the α(t) are less than the threshold value Tval. Thus, these individuals will not be added into EGA to participate in the operators with other individuals. With the incrase of iterations, the values of h(x) approximate to 1 and the values of α(t) approximate to 1, too. That means that nearly each output of SCHNN needs to be added into the population.As the running time of SCH–EGA on the benchmark problems are very short, we test the two strategies on Group 4, totally three large problems. The results are shown in Fig. 8. It can be found that the hybrid algorithm using the second strategy reduces the computation a little. Compared with the running time, the differences nearly are negligible. There may be two reasons leading to this results. First, although the second strategy avoids some outputs adding to the population, the computation by Eq. (8) in the second strategy also requires many time. Second, the operations in EGA is simple and efficient and the computation in EGA is a little, so adding one individual in each iteration does not increase computation a lot. Thus, So the advantage of the second strategy is not obvious.In our experiment, we compared SCH–EGA with many other well-known algorithms proposed before for solving frequency assignment problem. As the basic algorithms our algorithm originated from, HopSA, GNN, SCHNN and EGA were compared to show the improvement of our algorithm. CGA and DCA were chosen to show the performance difference in the aspect of similar algorithms. All of these algorithms were applied on Group 1 (BM1–BM5) [11]. Table 3shows the comparison results. Each result includes the largest interference and the total interference. As the five benchmark problems are not very complex, all the algorithm get same results on the first three benchmark problems. For BM4, the largest interference of EGA is a little larger than other algorithms, but the total interference is smaller except DCA and SCH–EGA. The reason is mainly because the simplicity of the operations in EGA limits the ability to search better solutions. SCH–EGA algorithm obtains the smallest total interferences on both BM4 and BM5. HopSA gets smaller total interference than other algorithms except SCH–EGA on BM5. But the largest interference of HopSA is larger than other algorithms. As the first objective of FAP owns a higher priority, the performance of HopSA is not good. As shown in Table 3, CGA could obtain better solutions than HopSA and GNN on BM4 and BM5 instances. That shows the heuristic assignment strategy applied in CGA is more efficient than the original strategies in these algorithms. The total interferences obtained by CGA are obviously bigger than EGA and SCH–EGA. DCA could get smaller total interference than EGA on BM4, but its results on BM5 is worse. Generally, SCH–EGA obtains better or comparable performance than other algorithms on the five benchmark problems. Noticeably, EGA also obtains good performance on these problems.Fig. 9(a) and (b) shows the evolutions of SCH–EGA, SCHNN and EGA on BM5. It is clear that all the three algorithms obtain their final results with the increase of iterations. In Fig. 9(a), the three algorithms obtain the same largest interference at last. But the iterations when SCH–EGA obtains to the final solution are more less. In Fig. 9(b), the advantage of SCH–EGA is more obvious. Not only the results of SCH–EGA are better than the other two algorithms, but also the iterations when it find the final results are less.As the five benchmark problems are not complex, SCHNN, EGA and SCH–EGA were tested on Group 2–4, totally 12 large problems. The results are shown in Table 4. Three cases (Case 6, 7, 8) are included in Group 2. All the three cases have the fixed numbers of carriers N, segments M and the carrier length. The differences between the three cases are the interference matrixes. In Group 2, the results of SCH–EGA are better than the two algorithms. At the same time, there is a little difference of the running time for SCH–EGA on the three cases. Thus, the conclusion can be made that the cost matrix is not the main factors for FAP.The Group 3 includes the Case 9, 10, 11, 12, 13, 14, totaly 6 problems. In Group 3, all the cases have the same number of carriers N and the range of the interference 1–100. The ranges of carrier length enlarge with the increase of the number of segments M. The main characteristic of the results in this group is that the running time increases a lot with the enlargements of M. In fact, with the increase of M, the interference matrix IM and each neuron in the neural network will also enlarge. This is the main factor of the increase for the running time. From Table 4, we also find that with the enlargements of M, the increase of SCHNN are much larger than the EGA. This mainly because there are much computations in the neural network and EGA is efficient with a little computation.In Group 4(Case 15, 16, 17), the numbers of carriers N and segments M become very large and the range of interference is fixed to 1–100. In this group, M decreases with the enlargements of N and the running time becomes longer. That means the number of carriers N plays an important role for FAP. In reality, the numbers of carriers N and segments M determine the structure of the neurons, individuals and the interference matrix. With the increase of carriers, the enlargements of computation is reasonable.In this experiment, SCH–EGA was compared with HopSA, GNN, SCHNN, and EGA on five benchmark problems. SCH–EGA obtained the best or comparable performance than other algorithms. EGA also got good performance. As the benchmark problems are not very complex, SCH–EGA was compared with SCHNN and EGA on 12 large problems. The advantages of SCH–EGA become more obvious with the enlargements of the problems. We also get the conclusion that the numbers of carriers N and segments M are the key factor in FAP while the interference matrix is not very important. As SCH–EGA is a hybrid algorithm, the running time in each iteration is a little longer than SCHNN and EGA. But with enlargement of the problems, this disadvantage becomes less important.At last, an experiment was conducted on Group 1 and Group 4 to explore the influence of population sizes for the hybrid algorithm. Parameter Inumstands for the number of individuals. 20, 40, 60 and 80 are chosen as the specific numbers for Inum. Each result in the experiment includes three constants, which are the largest interference, total interference and the running time. Fig. 10shows the results for different sizes on BM3-BM5.In Fig. 10, three graphs in the left side show the total interferences of the hybrid algorithm with different population sizes. And the three graphs in the right side depict the running time corresponding to the graphs in the left side. The first graph in the left side shows the results for four different population scales on BM3 and the running time is shown in the right side. On BM3, the running time of SCH–EGA with 20 individuals is the shortest and the result in this setting is the worst. When the population size Inumincreases to 40, the result is better than before and the running time is longer. Then the result remains the same when Inumincreases to 60 and 80. But the running time increases a lot. Actually, the running time increases nearly in the linear growth with the enlargements of Inum. The running time with 80 individuals is about 5 times longer than it with 20 individuals and 3 times longer than it with 40 individuals. On BM4 and BM5, the algorithm obtains the best results when 60 is set as the value of Inum. If Inumincreases again, the best results may not change any more. Thus, too large population size is not needed in our hybrid algorithm. That is to say, a small population scale in SCH–EGA can guarantee the little computation and satisfy the demand to obtain good solutions. Many reasons lead to this conclusion. First, we propose the calibration strategy to modify all individuals to satisfy the three constraints which means that each individual in population may be a proper solution for FAP. Then operations between the possible solutions improve the efficiency that finding better solutions. At the same time, repetitions in the population will be eliminated in each iteration and this rule guarantees the diversities of the possible solutions. And the diversity for the population is very helpful. As SCH–EGA can obtain good solutions with a little computation, it is an efficient algorithm.Table 5shows the detailed results in this experiment. From Table 5 it can be found that with the enlargements of the cases, the running time increases a lot with the increase of the population scale. But the solutions are not changed better and better. This phenomenon becomes more obvious with the enlargements of the cases. So conclusion can be made that SCH–EGA with a proper population size Inumwhich is not very large can obtain good results and guarantee the little computation.In this paper, we first propose five optimal strategies to build an efficient genetic algorithm(EGA). Then a hybrid stochastic competitive Hopfield neural network-efficient genetic algorithm (SCH–EGA) approach for the frequency assignment problem in the satellite communication systems is presented. As SCH–EGA owns good adaptability, it can also deal many other problems including the clustering, classification, the maximum clique problem and so on. In order to find a good hybridization between the Hopfield-type neural network and the genetic algorithm, three hybridizations were proposed and compared on 5 benchmark problems and 3 large problems and then we discovered the best hybridization. In the hybrid algorithm, the advantages of the two algorithms are kept and the disadvantages of the two algorithms are solved. We believe this comparison can also be helpful for the hybridizations between the Hopfield-type neural networks and other evolutionary algorithms, such as the ant colony optimization algorithm, the particle swarm optimization algorithm, artificial bee colony algorithm and so on. In the experiments, SCH–EGA was compared with other algorithms, such as the SCHNN, GNN, HopSA, EGA, CGA and DCA on five benchmark problems. On all cases, our SCH–EGA obtains better or comparable results than other algorithms.Due to the hybrid characteristic in SCH–EGA, our algorithm is dependent on the properties of both Hopfield-type neural net and genetic algorithm. Similarly, the application of our algorithm on optimization problems should satisfy the restrictions of each embedded algorithm separately, especially the Hopfield-type neural networks, which limits our algorithm suitability to some extent. Furthermore, since SCH–EGA is a general optimization algorithm, our future work will apply the SCH–EGA on other practical optimization problems, and improve it to get better suitability.

@&#CONCLUSIONS@&#
