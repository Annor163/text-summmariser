@&#MAIN-TITLE@&#
Bio-insect and artificial robot interaction using cooperative reinforcement learning

@&#HIGHLIGHTS@&#
We present a cooperative reinforcement learning technique using a fuzzy logic-based expertise measurement system to entice bio-insects towards desired goal areas.We address the fuzzy logic-based expertise measurement system for sharing knowledge among the robots.We conduct two experiments. In the first experiment, the robots entice the bio-insect without sharing knowledge, while in the second experiment, the robots entice the bio-insect with sharing knowledge. The second experiment shows better results than the first experiment.From the experiments, we can conclude that sharing knowledge using fuzzy-logic-based expertise measurement system is an efficient way for our task.

@&#KEYPHRASES@&#
Reinforcement learning,Fuzzy control,Intelligent interaction,

@&#ABSTRACT@&#
In this paper, we propose fuzzy logic-based cooperative reinforcement learning for sharing knowledge among autonomous robots. The ultimate goal of this paper is to entice bio-insects towards desired goal areas using artificial robots without any human aid. To achieve this goal, we found an interaction mechanism using a specific odor source and performed simulations and experiments [1]. For efficient learning without human aid, we employ cooperative reinforcement learning in multi-agent domain. Additionally, we design a fuzzy logic-based expertise measurement system to enhance the learning ability. This structure enables the artificial robots to share knowledge while evaluating and measuring the performance of each robot. Through numerous experiments, the performance of the proposed learning algorithms is evaluated.

@&#INTRODUCTION@&#
In the field of robotics, numerous efforts to establish artificial intelligence have been taken by numerous researchers. However, there has still been no dominant result due to the difficulty of creating artificial intelligence for robots [2,3]. This is especially true in our environment context, which involves complex and unpredictable elements and makes it difficult to apply artificial intelligence in robot applications. The project, called BRIDS (Bio-insect and artificial Robot Intelligence based on Distributed Systems) [1,4], seeks to study interactions between bio-insects and artificial robots to establish a new architectural framework for improving the intelligence of robots. In this project, we use living bio-insects which have their own intelligence to survive in nature. Because of their own intelligence, behavior of the bio-insect also involves complex and unpredictable elements. Therefore, studying an interaction between a living insect from nature and artificial robot will provide an idea of how to enhance the intelligence of robots. In this paper, as a specific task for the interaction between bio-insects and artificial robots, we would like to entice bio-insects towards desired goal areas using artificial robots without any human aid. Thus, the potential contribution of this research lies in the field of robot intelligence; it establishes a new learning framework for an intelligent robot based on cooperative reinforcement learning, which constitutes a type of coordination for a community composed of bio-insects and artificial robots. The research on bio-insect and artificial robot interaction will provide a fundamental theoretical framework for human and robot interactions.The main focus of our early results [1] was on how to address the uncertain and complex behavior of a bio-insect under a constructed framework for robot intelligence. The first goal was to find available interaction mechanisms between a bio-insect and an artificial robot. Contrary to our expectation, the bio-insect did not react to light, vibration, or movement of the robot. From various trials and errors, we eventually found an interaction mechanism using a specific odor source from the bio-insect's habitat. Additionally, to develop a framework, we made an artificial robot that can spread the specific odor source towards a bio-insect. Then, to evaluate interaction ability of the mechanism, we conducted experiments using the artificial robot, which was manually controlled by a human operator. In the experiment, by the human operator the artificial robot was considered that the robot has enough knowledge to entice the bio-insect towards desired point, and the experiment result showed that the artificial robot can entice the bio-insect. The second goal was to entice a bio-insect towards the desired goal area using an artificial robot without human aid. To achieve the second goal, we conducted two types of experiments to entice a bio-insect towards the desired goal using an artificial robot without human aid. The first type of experiment was conducted using fuzzy logic based reinforcement learning and second type of experiment used simple regular reward based reinforcement learning. From the experimental results, we found that fuzzy logic based reinforcement learning showed more efficient results. However, it took a huge amount of learning time for the robot to acquire the necessary knowledge though it eventually obtained the knowledge by the learning process. Furthermore, due to the complex and unpredictable behaviors of a bio-insect, the single reinforcement learning process was insufficient to enable a reliable and efficient learning. Thus, for an efficient learning (i.e., to improve the success rate in a faster learning time), we decided to use a cooperative learning mechanism in multi-agent domain. In Ref. [4], we conducted experiments using two artificial robots. In the experiments, we used fuzzy logic based expertise measurement system for sharing knowledge and obtained successful results. This paper is an extended version of Ref. [4]. In this extended version, we have generalized a fuzzy logic based expertise measurement system and presented more detailed explanation of the system. In the previous version, the effect of sharing knowledge might not be clear and the experiment result appeared quite optimal at the beginning. Therefore, we have newly conducted two types of experiments: Experiment A – without sharing knowledge case and Experiment B – sharing knowledge case. In the Experiment A, individual agents 1 and 2 have performed to entice a bio-insect together without sharing knowledge. The Experiment B has focused on sharing knowledge between the two artificial robots to entice a bio-insect together. Also, the number of action points has been increased and the artificial robots have needed more trials-and-errors to find out suitable angle direction and optimal distance range to entice the bio-insect. After conducting experiments, we have obtained newly experimental results.Note that we use the term “cooperative learning” to represent a learning by sharing data among multiple autonomous robots. When a robot is faced with given commands for which the robot lacks a sufficient knowledge base and is required to act alone, the robot may not be successful in implementing the commands. Or the robot takes a long time to complete the task. However, if there are several other robots and each of the robots possesses their own specialized knowledge about the task, then the given commands can be more readily completed by mutual cooperation. Moreover, when the robots learn knowledge from trials and errors, some of the robots may have more specialized knowledge than the others, as seen in human society. If the robots have the ability to share knowledge, then the performance of the robots would be enhanced. For these reasons, cooperative learning has recently received a lot of attention due to the various benefits it provides.In a relationship between a predator and prey, a predator needs to learn hunting skills to survive in nature. By trials-and-errors, the predator will obtain useful knowledge to capture prey, and success rate of hunting will be increased as shown in Refs. [5,6]. However, due to complex and unpredictable elements in nature, the predator cannot be always successful in hunting prey even though the predator has enough knowledge in hunting. For example, weather conditions, species of prey, and physical elements of the predator and prey, etc. are different whenever the predator hunts a prey. The elements affect the success rate of hunting of the predator [7,8] and make it difficult to capture a prey. At least, the predator can learn available hunting skills by its own trial and error process, and it can survive in nature.From a behavioral point of view, the basic concept of reinforcement learning is similar to learning mechanism of animal using positive and negative reward through trial and error process. This process is similar to the relation between an artificial robot and a bio-insect. The artificial robot needs to find out how to entice the bio-insect by trial and error process. As a predator cannot fully capture a prey due to complex and unpredictable environment, the artificial robot cannot entice the bio-insect at all times because of complex and unpredictable elements of the bio-insect. At least, the artificial robot learns to have useful knowledge to entice the bio-insect. Because of these similarities, therefore, we consider that this approach is a fully adaptable and useful solution. In addition, due to the merit of this process, the reinforcement learning has a lot of attention and has been applied to various fields. Using the reinforcement learning, they have controlled helicopter flight [9], movement of elevator [10], humanoid robots [11], soccer robot [12], and traffic signal control [13]. Also, they have applied into spoken dialogue system [14], packet routing [15], production scheduling [16], traveling salesman problem [17], and resource allocation [18].As a basic step, we have focused on using a living bio-insect, called stag beetle, as a target in interaction with robots. Based on interaction mechanism we found, we attempt to control movement of the bio-insect without any human aid. To achieve this goal, the robots need to learn how to control behavior of the bio-insect. The problem is that a bio-insect contains own low level intelligence composed of ganglia to survive in nature. Therefore, behavior of the bio-insect also contains complex and unpredictable elements, and the elements make it hard to control the bio-insect. In our previous experimental results using a robot controlled manually by human operator [1], we achieved only 80% success rate. This result implies that reactions of the bio-insect are not always equal to what we expected, and the amount of reaction is different at every trial. In these conditions, the robots need to learn precise knowledge to entice the bio-insect towards desired goal area. If we know what the bio-insect thinks of future movement in current situation, then the robots may entice the bio-insect in an efficient way. However, the robots have some clues, which only were acquired from behavioral reaction of the bio-insect. To overcome these difficulties, we applied reinforcement learning and fuzzy logic in this paper.To apply the reinforcement learning into real robot, a generation of a precise reward is a crucial issue for an accurate learning. To deal with this complexity of the environment, we found that the fuzzy logic could be one of the profitable approaches for generating a reward. We expect that this process will make robots do more active learning than a sole reinforcement learning by adequately generating a reward from behavioral reactions after interacting with the bio-insect. Therefore, we adopt the fuzzy logic into our learning structure. Also, when sharing knowledge for a cooperative learning, classifying and finding experts in each specific field among agents also contain complex elements. Therefore, we also use fuzzy logic to measure performance of each robots. Then, based on the developed fuzzy rules, the system calculates performance of each robot in each specific fields.This paper is organized as follows. In Section 2, we briefly introduce a project entitled BRIDS (Bio-insect and artificial Robot Interaction based on Distributed Systems). In this section, we also present the main purpose and goal of the research. In Section 3, we present the fuzzy logic-based cooperative reinforcement learning using an expertise measurement system. Using the aforementioned structure, we present experimental setup and results in Section 4. In Section 5, we present a discussion of our experimental results. Finally, Section 6 provides a conclusion of this paper.The BRIDS seeks to study bio-insects and artificial robots interaction to establish a new architectural framework for improving the intelligence of mobile robots. One of the main research goals is to drive or entice a bio-insect through the coordination of a group of mobile robots towards a desired point. The research includes the establishment of hardware/software for the bio-insect and artificial robot interaction and the synthesis of distributed sensing, distributed decision-making, and distributed control systems for building a network composed of a bio-insect and artificial robots. Fig. 1explains how to compose and connect the subsystems.Distributed sensing is used in the recognition and detection of the bio-insect, as well as in the construction of a wireless sensor network to locate the artificial robots and bio-insect. The distributed decision contains the learning of the repetitive reactions of bio-insect for a certain form of input. It aims at finding which commands and actuations drive the bio-insect towards a desired point or drive the bio-insect away from the target position. The reinforcement learning algorithm will be designed to generate either a penalty or reward based on a set of actions. The distributed decision stores the state of current actions and their outputs, which are closely associated with the future event, into memory. Then, it selects commands and outcomes of past actions for the current closed-loop learning. Thus, the synthesis of the recursive learning algorithm based on the storage and selection procedure along with the learning domain will be main point of interest in the distributed decision. The distributed control includes the control and deployment of the multiple-mobile robots via coordination, as well as the design of the optimally distributed-control algorithm based on the coordination. It learns how the bio-insect reacts based on the relative speed, position, and orientation between the multiple-mobile robots and the bio-insect. Thus, the ultimate goal of this research is to establish a new theoretical framework for robot learning via a recursive sequential procedure of the distributed sensing, decision and control systems. Fig. 2illustrates the structure of the BRIDS.As a candidate of the bio-insect, we selected a stag beetle shown in Fig. 4(c). As shown in Fig. 3, the stag beetle, called Serrognathus platymelus castanicolor Motschulsky as a scientific name, has strong physical strength, good movement over flat surfaces, and long life on extreme environment. When we place the bio-insect to experimental platform, it normally stays on the experimental platform without movements while hiding its own antenna. In that case, the robot cannot entice the bio-insect, because the bio-insect do not react from any stimulation. After a few minutes later, the bio-insect tries to measure odor source in the air and starts moving somewhere. When the bio-insect opens its own antenna, then the bio-insect begins to follow the specific odor source. Therefore, before taking an experiment, we have to check condition of the bio-insect whether the bio-insect will follow the robot or not. Also, when a bio-insect is suddenly shocked by collision with a robot or something, the bio-insect tries to run away strongly or do not move. From the observed results, we have understood that unknown other characteristic may affect the behavior of the bio-insect.For an artificial robot, we redesigned e-puck robots to produce the desired actuation by adding one more micro controller unit. One of the crucial points in hardware development is how to interact between the bio-insect and the artificial robot. Eventually, as a solution, we selected a specific odor source from habitat of the bio-insect and used it an actuation mechanism as illustrated in Fig. 4(a). The actuation mechanism contains two air-pump motors to produce airflow and one plastic bottle containing the specific odor source. When a bio-insect smells the specific odor source generated from the actuation mechanism, the bio-insect follows the specific odor source. The actuation mechanism was installed into the redesigned e-puck robots as shown in Fig. 4(b). To control the redesigned e-puck robots, we used Bluetooth access point for a communication link. A machine vision camera captures the image of experimental platform continuously; then, using the captured images, a host computer finds the current positions and heading angles of bio-insects and artificial robots. Then, using own designed intelligence structure the robots decide actions to entice the bio-insect without human aid. The whole structure of the hardware platform is illustrated in Fig. 4(c).It is notable that the BRIDS project was initially targeting to develop a distributed or decentralized decision and control platform, however it was not easy to design fully distributed or decentralized hardware platform at the present time. Thus, at this moment, our main goal is to develop an intelligent learning algorithm, and we seek to verify the developed learning algorithms in a centralized hardware setup as shown in Fig. 4. The development of BRIDS with a fully distributed hardware platform remains as a future work. This paper only focuses on development, test, and verifications of intelligent learning algorithms under rather ideal setups.In the field of cooperative reinforcement learning, the area of expertise (AOE) concept was recently proposed in [19], where the framework evaluates the performance of each robot from several points of view and obtains generalized knowledge from the expert robots. In [20], they report a similar concept and introduce an advice-exchange structure focused on sharing knowledge based on previous experience. In a different way, the AOE is focused on which robot has more expertise in defined area, and then, the robots share the knowledge. In [20], there are two different aspects on expertise. A behavioral and knowledge-based approach focuses on a better and more rational behavior, while a structural approach examines better and more reliable knowledge for evaluating expertise. For evaluating the expertise of each robot [19,21,22], present various methods that were used to measure and calculate expertise. These measurements help the AOE evaluate the expertise of all the robots in each specific area. After evaluating the knowledge of each robot, robots then share knowledge with each other using a weight strategy-sharing concept. Based on the AOE structure [23], presents a simple experiment using two robots that use an adaptive weight strategy sharing and a regret measure. In this paper, we adopt the AOE method proposed in [19] into our framework, because it is suitable for evaluating knowledge and efficient way for sharing knowledge among the multiple robots.In this subsection, we design a cooperative reinforcement learning structure using a fuzzy logic-based expertise measurement system. The structure of new learning logic is composed of two parts: expertise measurement part and sharing knowledge part. In the expertise measurement part, it helps to evaluate the performance of each robot using various measurements in the specific field. From the outcomes of each robot in enticing the bio-insect towards specific directions, the learning logic can evaluate which robot possesses a higher expertise in specific fields. Here, the specific fields mean specific expert domains of each robot. If robots are required to learn how to complete complex tasks without any given knowledge, the robots try to learn how to fulfill the given tasks. Among the robots, some of robots may have more knowledge in domain A and other robots may have more knowledge in domain B because the robots rely on randomly chosen action. During the tasks, some of robots may have outstanding knowledge in each different domain. If the robots can determine which robot is an expert in specific domain and share knowledge, then the performance of the robots will be increased comparing than non-sharing knowledge case. Then, based upon the evaluated performance, the robots share knowledge with each other. The following Fig. 5depicts the whole structure of the system.Fig. 5(a) represents a fuzzy-logic-based reinforcement learning structure for a robot, which is composed of reinforcement learning and fuzzy logic. The Fig. 5(b) represents a core of the cooperative reinforcement learning part using fuzzy logic. During each episode, the expertise measurement part stores every robot's specific criteria defined as expertise measurements introduced in Section III(c). Using the criteria, the expertise measurement system evaluates each robot's performance with a score based on fuzzy logic and fuzzy rules. Then, using the evaluated score of each robot, the robots share knowledge. The following subsections introduce the specific processes of the expertise measurement system.Reinforcement learning [24–26] is a reward signal-based trial-and-error iteration process (see Fig. 6). Based on a discrete set of states S, a set of robot actions A, a set of transition probability T:T(s,a,sˆ), policy p:s→ a, and an immediate reward signal τ, an optimal policy is searched using a Q-learning structure. The Q-learning structure helps robots learn how to entice a bio-insect towards the desired direction under defined specific fields as seen in the following equation:(1)Qt+1k,l(s,a)←(1−α)Qtk,l(s,a)+α(τt+1k,l+ΓmaxaˆQtk,l(sˆ,aˆ))where α is the learning rate (0≤α≤1), Γ is the discount factor (0≤Γ≤1), t is iteration step, k denotes a robot, and l is a specific field. One of merits of the Q-learning structure is that it adopts a learning rate α. The learning rate α is a weighting parameter between previously acquired knowledge and newly acquired knowledge by a reward. When α is near 1, then the Q-learning structure fully updates newly acquired rewards as part of the exploration process. Conversely, when α approaches 0, then the structure passes over the newly acquired rewards. In this case, the structure depends on previous knowledge that a robot has learned as part of the exploitation process. The value α can be useful for our experiment because the robots require precise learning knowledge of the complex behavior of bio-insects. If the robots can control α during the experiment, then performance of the experiment will be enhanced. In these experiments, we choose the approach where α decreases with an increase in the number of episodes. Additionally, we adaptively update the specific fields where a robot is an expert. Using the evaluated performance of each robot, we know which robot is an expert in the field. Using an initialized Qk,l(s, a) table, (1), the kth robot updates its own table using the calculated immediate reward at the current state s by the selected actionaˆwithin a specific field l.To understand the behavior of a bio-insect as a result of a given action, we apply a fuzzy logic to generate rewards for the behavior of a bio-insect, because the fuzzy logic is a good approach for understanding an imprecise environment, such as understanding emotion of human behavior [27] and human mind [28]. When the kth robot recognizes the current state, then, with a possible set of actionsA, it chooses an actionaˆin the current state s. After the action is executed, the reaction information, including the variation in distanceΔdtbbetween the sub-goal point for the bth bio-insect and the bth bio-insect and the variation in distanceΔetkbetween the bth bio-insect and the kth robot, is collected. Here,ΔdtbandΔetkare calculated using the following equations, respectively:(2)Δdtb=∥qt_sb−qt_sGoal,b∥−∥qt_eb−qt_eGoal,b∥(3)Δetk=∥qt_sb−qt_sk∥−∥qt_eb−qt_ek∥whereqtb,qtk, andqtGoal,bindicate the positions of the bth bio-insect, the kth artificial robot, and the sub-goal for bth bio-insect, respectively whereqt∈ℝ2, {t_s, t_e}∈t (t_s and t_e indicate the start time and end time of the selected actionaˆat the iteration step t, respectively), and ∥·∥ is the Euclidean norm.To generate suitable rewards for robots, using only the parameterΔdtb, which means variation in distance between the sub-goal point and the bio-insect, was insufficient. Due to complex and unpredictable elements of a bio-insect, the bio-insect may move towards the desired goal point with wrongly chosen action when a bio-insect did not react from a robot. In this case, if we use only the value, a wrongly generated reward may be accumulated to each robot. To avoid such case, we additionally use the parameterΔetk, which means the variation in distance between the bth bio-insect and the kth robot. We consider that the parameterΔetkis also one of crucial clues that the specific odor source lets a bio-insect follow towards the spreading direction. Therefore, using this approach, the system can generate more specific reward signal.To generate a reward signal, we divide two types of situations: positive case - the artificial robot entices the bio-insect towards the right place and negative case - the artificial robot entices the bio-insect towards a wrong place. Because of complex and unpredictable elements of the bio-insect, the bio-insect occasionally moves towards a place without any clues. Therefore, to generate a precise reward signal, we focus on specific behaviors as follows. Positive case: If the bio-insect followed the artificial robot (Δetkis VG) and the artificial robot enticed the bio-insect towards the right place (Δdtbis VG), then we consider that this is a very good case A. Negative Cases: If the bio-insect did not follow the artificial robot (Δetkis VB or BD) and the artificial robot moved the bio-insect towards the right place (Δdtkis VG), then we consider that this is a very bad case E. If the bio-insect followed the artificial robot (Δetkis VG or GD) and the artificial robot enticed the bio-insect towards a wrong place, then we consider that this is very bad case E. The other rules have considered as meaningless cases C and the rules that are slightly related with above positive and negative cases have been classified as B or D. Based on the above regulation for generating rewards, detailed fuzzy rules are developed as shown in Table 4 in the appendix.Based on the fuzzy rules described in Table 3 in the appendix, the input variables Δetand Δdtare changed by the following membership functions (4) and (5) as depicted in Fig. 7(a) and (b).(4)μd={VGd,GDd,NMd,PRd,VPd}(5)μe={VGe,GDe,NMe,PRe,VPe}(6)μoutput={A,B,C,D,E}where VG, GD, NM, PR, and VP indicate very good, good, normal, poor, and very poor, respectively. In the fuzzy sets, VG, GD, NM, BD, VB, A, B, C, D, and E represent each fuzzy membership function, and input variables are changed by linguistic process. Next, the calculated valuesΔdtbandΔetkare converted by a fuzzification process using the defined fuzzy sets as depicted in Fig. 7(a) and (b).After the fuzzification process, the converted values are calculated using (7) and (8) with a max-min composition process. Then using the fuzzy rules shown in Table 3 in the appendix, all of the values are expressed into output fuzzy sets as depicted in Fig. 7(c) using (8). All the outputs are combined into the aggregation of output fuzzy sets as union process in set theory.(7)μi=min[min[μdi(Δdtb),μei(Δetk)],μoutputi](8)μo(u)=⋃i=125μiwhere parameter i represents number of fuzzy rules and k denotes robot.An immediate reward is calculated using the center of mass method as follows:(9)τt+1k,l=∫uμo(u)du∫μo(u)duBased on the reinforcement learning structure, the fuzzy logic generates a reward signalτt+1k,lfor the kth robot from the collected reaction of the bio-insect in specific field l. Then using the reward, a robot updates the Qk,l(s, a) table and tries to optimize the Q-table as knowledge.When we examine the performance of each robot, various indexes can be used as measurements. In our structure, we choose the following three measurements: average reward, positive average reward, and percentage of positive rewards. Average reward is calculated as follows:(10)τavgk,l=∑t=1Mk,lτt+1k,lMk,lwhere Mk,lis the number of iterations for kth robot in the specific field l.We define a positive reward as shown below.(11)τ_pstt+1k,l=τt+1k,l,ifτt+1k,l>δ0,otherwise.Here, the range of the reward is −1≤τ≤1.Using the defined positive reward, the average positive reward is calculated as(12)τk,l_pstavg=∑t=1Mk,lτ_psttk,lMk,lSimilarly, the percentage of positive rewards is calculated in the following equations. For counting the number of positive rewards, the equations below check whether the current rewardτt+1k,lis positive reward or not.(13)τ_cntt+1k,l=1,ifτt+1k,l>δ0,otherwise.Then, the percentage of positive rewards is calculated as(14)τk,l_cntavg=∑t=1Mk,lτ_cnttk,lMk,lUnder the expertise measurement values, the expertise measurement system evaluates the performance of all robots using the following fuzzy sets and fuzzy rules as described in Table 4 in the appendix.(15)μavg={GDa,NMa,PRa}(16)μpst={GDp,NMp,PRp}(17)μcnt={GDc,NMc,PRc}(18)μexp={A,B,C,D,E}For determining an expert among agents in each specific field, we use three types of measurements. In that case, each measurement equally contributes to judge all agents. Therefore, one of measurements contains NM or BD, then the output will be decreased proportionally. For example, if all measurements are GD or one measurements is NM and others are GD, then the output is A. Then, one of measurements contains more NM or BD, then output will be decreased proportionally as B, C, and D. Eventually, when one measurement is NM, and others are BD or all measurements are BD, then the output is E. Based on the above regulation for expertise measurement system, detailed fuzzy rules are described in Table 4.After the fuzzification process, the converted values are calculated using (19) and (20) with a max-min composition process. Then, using the fuzzy rules shown in Table 4 in the appendix, all of the values are expressed into the output fuzzy sets as depicted in Fig. 8(d) using (20). All the outputs are combined into the aggregation of the output fuzzy sets as union process in set theory.(19)μi=min[min[μavgi(τavgk,l),μpsti(τk,l_pstavg),μcnti(τk,l_cntavg)],μexpi](20)μexp(u)=⋃i=127μiwhere parameter i represents number of fuzzy rules, k denotes robot, and l denotes number of specific field.The final output Sk,lis the score of each robot and is calculated using the center of mass method:(21)Sk,l=∫uμexp(u)du∫μexp(u)duThen, knowledge of each robot is merged as(22)Sl←∑k=1NSk,lwhere N is the number of robots and k denotes a robot k∈1, …, NFinally, all robots have the shared knowledge as follows:(23)Ql←∑k=1NSk,lSl·Qk,lThe whole procedures of the fuzzy logic-based expertise measurement system for cooperative reinforcement learning are described in the Algorithms 1 and 2. In the algorithms, B denotes the number of bio-insects b∈{1, …, B}, L denotes the number of specific fields l∈{1, …, L}, N denotes the number of robots k∈{1, …, N}, and Mk,ldenotes the number of iterations of the kth robot in specific field l.From literature search, we find several different reinforcement learning approaches. In [29], it was explained why a cooperative learning is more attractive compared with independent learning case; and in [30], they consider two types of agents that try to complete their opposed goals. Using opposite goals, one agent tries to maximize its own reward while another agent tries to minimize its own reward. [31] presents a learning structure for competitive and cooperative behavior in artificial soccer game based on temporal difference learning algorithms in reinforcement learning field. In [32], they adopt an average reward-based learning mechanism under different action and task levels. Each level is composed of a hierarchical structure, and the action level performs a given task under the overall current task. In [33], they present a team Q-learning algorithm composed of parallel Q-tables for maximizing the common goal and in [34] an integrated sequential Q-learning using a genetic algorithm was presented. Using a fitness function, they evaluate the current performance and try to find a better performance under selection, crossover, and mutation processes.In this paper, in order to move the bio-insect towards a given goal point, the robots need to achieve a common goal together because the robots are supposed to entice the bio-insect. Therefore, [31–34] may be utilized in our task. On the other hand, [29] and [30] cannot be used because they try to achieve opposite goals.From our experiments for finding interaction mechanism between a bio-insect and a robot, we found that one of the crucial criteria was an actuation direction to the bio-insect using the specific odor source. Because, the bio-insect relies on collected information from its own antenna, which is located in its own head to detect smell in the air, the probability to entice the bio-insect is different according to the actuation directions. When a robot tries to spread a specific odor source at heading direction of the bio-insect, the bio-insect followed well the robot with a high probability. Contrary to this, when the robot tries to spread the specific odor source at the rear of bio-insect, the bio-insect followed the robot with low probability. Due to this problem, we have used an enticing mechanism to interact with bio-insect. However, it is important to check which actuations of the robots affect more on the movement of the bio-insect. For example, if two robots locate at the heading side and the rear side of the bio-insect, then since the heading direction of the bio-insect is right direction that robots need to entice, the bio-insect only follows a robot located on heading side with a high probability. However, in that case, even though the bio-insect follows a robot located on heading direction, two robots (robots locating at the heading side and rear side) will receive same positive reward since the bio-insect was actuated towards the desired direction. Due to this problem, in achieving a common goal, the multiple robots may get some problems in finding the right actions. To handle this problem, in the fuzzy-logic-based expertise measurement system that was introduced in previous sub-sections, each robot only tries to entice a bio-insect at a chosen action point, and each agent receives a reward and records achieved performance by expertise measurement. After an episode has been completed, the robots share knowledge based on own recorded performance using expertise measurement system. Then, in the next episode, the robots will entice the bio-insect based on shared knowledge.As an interaction mechanism between a bio-insect and an artificial robot, we found a specific odor source that helps the bio-insect follow the artificial robot [1]. Using the interaction mechanism, each robot learns how to entice a bio-insect towards the desired goal point under cooperative manner. To realize this concept, we conduct the following two experiments using a bio-insect and two artificial robots: Experiment A – without sharing knowledge as a control group and Experiment B – with sharing knowledge as a experimental group using the fuzzy logic based expertise measurement system as described in previous section to measure effect of sharing knowledge.In examining the performance of the cooperative reinforcement learning, we consider that it is more favorable to increase the number of artificial robots. Because, when the number of artificial robots is increased, then the number of clues for obtaining knowledge is also increased by sharing the obtained knowledge. It means that the total learning time can be reduced if they share knowledge efficiently. However, in our experiments, only two robots were utilized for single bio-insect due to limited space around the bio-insect. In examining the performance of the cooperative reinforcement learning, we build the following experimental platforms illustrated in Fig. 9for Experiments. As shown in Fig. 9(a) and (c), robot 1 and robot 2 work as a group for bio-insect 1. In the Experiment A, individual agents 1 and 2 perform to entice the bio-insect together without sharing knowledge. The Experiment B focuses on sharing knowledge between the two artificial robots. In the experiments, the robot 1 and robot 2 try to entice the bio-insect 1 towards a given sub-goal point while avoiding artificial walls and common restricted areas. Each sub-goal point is given by Algorithm 3. All sub-goal points and areas are illustrated in Fig. 9(b). Especially, after the robots have conducted the experiment at every episode, they share their knowledge using the fuzzy logic-based expertise measurement systems only in the Experiment B. Then, in the next episode, the robots try to entice the bio-insect using the shared knowledge.To recognize the current state among the bio-insects and robots, we define states that consist of a heading angle and a goal direction for the bio-insect, as illustrated in Fig. 10(a). The heading angle and the goal direction are divided into eight equal parts, each separated by 45° with drawn dotted lines in Fig. 10(a). To entice the bio-insect, the number of actuation points is illustrated in Fig. 10(b). The action points consist of three different distance ranges as d1, d2, and d3 and eight different directions separated by 45°. At chosen points among the action points, robots spread a specific odor source towards the bio-insect. To avoid collision, the robots move around a related bio-insect at a restricted distance range among them.

@&#CONCLUSIONS@&#
