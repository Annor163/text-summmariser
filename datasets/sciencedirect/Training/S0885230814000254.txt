@&#MAIN-TITLE@&#
Feature extraction based on the high-pass filtering of audio signals for Acoustic Event Classification

@&#HIGHLIGHTS@&#
We propose a new front-end for Acoustic Event Classification tasks (AEC).The proposed front-end relies on the high pass filtering of acoustic event signals.Results show that it outperforms the baseline system in clean and noisy conditions.

@&#KEYPHRASES@&#
Acoustic Event Classification,High-pass filtering,Auditory filterbank,

@&#ABSTRACT@&#
In this paper, we propose a new front-end for Acoustic Event Classification tasks (AEC). First, we study the spectral characteristics of different acoustic events in comparison with the structure of speech spectra. Second, from the findings of this study, we propose a new parameterization for AEC, which is an extension of the conventional Mel-Frequency Cepstral Coefficients (MFCC) and is based on the high pass filtering of the acoustic event signal. The proposed front-end have been tested in clean and noisy conditions and compared to the conventional MFCC in an AEC task. Results support the fact that the high pass filtering of the audio signal is, in general terms, beneficial for the system, showing that the removal of frequencies below 100–275Hz in the feature extraction process in clean conditions and below 400–500Hz in noisy conditions, improves significantly the performance of the system with respect to the baseline.

@&#INTRODUCTION@&#
In recent years, the problem of automatically detecting and classifying acoustic non-speech events has attracted the attention of numerous researchers. Although speech is the most informative acoustic event, other kind of sounds (such as laughs, coughs, keyboard typing, etc.) can give relevant cues about the human presence and activity in a certain scenario (for example, in an office room). This information could be used in different applications, mainly in those with perceptually aware interfaces such as smart-rooms (Temko and Nadeu, 2006), automotive applications (Muller et al., 2008), mobile robots working in diverse environments (Chu et al., 2006) or surveillance systems (Clavel et al., 2005). Additionally, acoustic event detection and classification systems, can be used as a pre-processing stage for Automatic Speech Recognition (ASR) in such a way that this kind of sounds can be removed prior to the recognition process increasing its robustness. In this paper, we focus on Acoustic Event Classification (AEC).Several front-ends have been proposed in the literature, some of them based on short-term features, such as Mel-Frequency Cepstral Coefficients (MFCC) (Temko and Nadeu, 2006; Zieger, 2008; Zhuang et al., 2010; Kwangyoun and Hanseok, 2011), log filterbank energies (Zhuang et al., 2010), Perceptual Linear Prediction (PLP) (Portelo et al., 2009), log-energy, spectral flux, fundamental entropy and zero-crossing rate (Temko and Nadeu, 2006). Other approaches are based on the application of different temporal integration techniques over these short-term features (Meng et al., 2007; Mejía-Navarrete et al., 2011; Zhang and Schuller, 2012). Finally, other relevant works in the literature have shown that the activation coefficients produced by the application of Non-Negative Matrix Factorizarization (NMF) on audio spectrograms can be used as acoustic features for AEC and other related tasks (Weninger et al., 2011; Cotton and Ellis, 2011). In order to distinguish between the different acoustic classes, some classification tools are then applied over these acoustic features, as for example, Gaussian Mixture Models (GMM) (Temko and Nadeu, 2006), Hidden Markov Models (HMM) (Cotton and Ellis, 2011), Support Vector Machines (SVM) (Temko and Nadeu, 2006; Mejía-Navarrete et al., 2011), Radial Basis Function Neural Networks (RBFNN) (Dhanalakshmi et al., 2008) and Deep Neural Networks (DNN) (Kons and Toledo, 2013). The high correlation between the performance of different classifiers suggests that the main problem is not the classification technique, but a design of a suitable feature extraction process for AEC (Kons and Toledo, 2013).In fact, as pointed in Zhuang et al. (2010), conventional acoustic features are not necessarily the more appropriate for AEC tasks because they have been design according to the spectral characteristics of speech which are quite different from the spectral structure of acoustic events. To deal with this issue in Zhuang et al. (2010) it is proposed a boosted feature selection method to construct a more suitable parameterization for AEC.In this work, we follow a different approach. Based on the empirical study of the spectral characteristics of different acoustic events in comparison with the structure of speech spectra, we propose a new parameterization for AEC, which is an extension of the conventional MFCC and is based on the high pass filtering of the acoustic event signal. The proposed front-end has been tested in clean and noisy conditions achieving, in both scenarios, significant improvements with respect to the baseline system.This paper is organized as follows: in Section 2 the main spectral characteristics of acoustic events are described. Section 3 is devoted to the explanation of the proposed parameterization. Section 4 describes the experiments and results to end with some conclusions and ideas for future work in Section 5.As it is well known, the spectrograms of speech signals are characterized by the presence of a higher energy in the low-frequency regions of the spectrum. However, in general, non-speech sounds do not show this speech spectral structure. In fact, in many cases, their relevant spectral contents are located in other frequency bands, as it will be shown in the empirical study of the spectral characteristics of several AEs performed in this section.As an example, Fig. 1represents the spectrograms of two instances of the same acoustic event, Phone ring. Although it is possible to extract conclusions about the spectral nature of this AE by means of the visual inspection of these spectrograms, their high variability due in part to the intrinsic frequency characteristics of the acoustic event and in part to the presence of noise (microphone, environment noise, etc.), motivates us to use an automatic method such as Non-Negative Matrix Factorization (NMF) (Lee and Seung, 1999), which is capable of providing a more compact parts-based representation of the magnitude spectra of the AEs.Given a nonnegative matrixVe∈ℝ+F×T, where each column is a data vector (in our case, Ve contains the short-term magnitude spectrum of a set of audio signals), NMF approximates it as a product of two nonnegative matrices We and He, such that(1)Ve≈WeHewhereWe∈ℝ+F×KandHe∈ℝ+K×Tand F, T and K represent frequency bins, frames and basis components, respectively. This way, each column of Ve can be written as a linear combination of the K building blocks (columns of We), weighted by the coefficients of activation located in the corresponding column of He. In this work, we are interested on retrieving the matrix We as it contains the building blocks or Spectral Basis Vectors (SBVs) which encapsule the frequency structure of the data in Ve (Smaragdis, 2004).For each for the acoustic events considered, their SBVs were obtained by applying NMF to the corresponding matrix Ve composed by the short-term magnitude spectrum of a subset of the training audio files belonging to this particular class. The magnitude spectra were computed over 20ms windows with a frameshift of 10ms. In total, 364,214 magnitude spectral examples were used for performing NMF, which corresponds to approximately 60min of audio. The NMF matrices were initialized using a multi-start initialization algorithm (Cichocki et al., 2009), in such a way that 10 pairs of uniform random matrices (We and He) were generated and the factorization producing the smallest Euclidean distance between Ve and (WeHe) was chosen for initialization. Then, these initial matrices were refined by minimizing the KL divergence between the magnitude spectra Ve and their corresponding factored matrices (WeHe) using an iterative scheme and the learning rules proposed in Lee and Seung (1999) until the maximum number of iterations (in our case, 200) was reached.The number of basis vectors K was set taking into account a trade-off between an accurate reconstruction of the magnitude spectra (i.e. the average approximation error between Ve and (WeHe) computed over all AEs) and a good visualization of the SBVs. In particular, we used K=23 which corresponds to the case in which the relative change in the average approximation error between two successive numbers of SBVs is less than 2%. It is also worth mentioning that when the number of basis vectors increases, NMF tends to place more and smaller bands in the areas of the spectrum with high energy (i.e. provides more resolution in these regions) and therefore reduces the overall reconstruction error (Bertrand et al., 2008). Nevertheless, for the purpose of this analysis, a larger value of K does not provide relevant information and produces a worse visualization of the SBVs.Fig. 2represents the 23 SBVs of four different non-speech sounds (Laugh, Applause, Phone ring and Spoon/cup jingle) and two different kind of noises (Restaurant and Subway). From this figure, the following observations can be extracted:•The spectral content of the AEs are very different each other, presenting, in general, relevant components in medium-high frequencies. As it is well-known that the spectral components of speech are concentrated in low frequencies, it is possible to infer that the parameterizations designed for speech (as the conventional MFCC) are not suitable enough for representing non-speech sounds.In all cases, low frequency components are presented to a greater or lesser extent, so this part of the spectrum seems not to very discriminative when comparing different types of AEs.Comparing the SBVs of the non-speech sounds, it can be observed that large differences can be found in the medium-high part of the spectrum, suggesting that these frequency bands are more suitable (or at least, they cannot be negligible) than the lower part of the spectrum for discriminating between different acoustic events.Different environment noises present very different spectral characteristics. For example, in the case of Restaurant, most of the frequency content is located in the band below 1kHz, whereas the SBVs of the Subway noise are distributed in two different regions of the spectrum: a low frequency band below 750Hz and a medium-high band of frequencies between 2 and 3kHz. The analysis of other kind of noises (Airport, Babble, Train and Exhibition Hall) yields to similar observations. This way, the distortion produced over the AE signals due to the presence of additive noise will vary considerably depending of the nature of the noise. As a consequence of this fact, some noises will be presumably more harmful than others, producing more noticeable degradations in the performance of the AEC system.The observation of the SBVs of the different acoustic events shown in Section 2 motivated us to derive an extension of the conventional MFCC more suitable for AEC. As it is well known, MFCC is the most popular feature extraction procedure in speech and speaker recognition and also in audio classification tasks. The basic idea behind the new front-end is to take explicitly into account the special relevance of certain frequency bands of the spectrum into the feature extraction procedure through the modification of the characteristics of the conventional mel-scaled auditory filterbank.One of the main conclusions drawn from the empirical study in Section 2 is that medium and high frequencies are specially useful for discriminating between different acoustic events. For this reason, this band of frequencies should be emphasized in some way into the parameterization process. This can be accomplished by high-pass filtering the short-term frames of the signal (using the appropriate filter) prior to the application of the auditory filterbank and the cepstral parameters computation. However, in this work, we have adopted a straightforward method which consists of modifying the auditory filterbank by means of the explicit removal of a certain number of the filters placed on the low frequency region of the spectrum. In Fig. 3it can be observed the upper frequency of the complete stopband as a function of the number of removed filters in the auditory filterbank for the Mel scale.In practice, this procedure consists of setting to a small value the energies corresponding to the outputs of the low-pass filters which are required to be removed. This threshold must be different to zero in order to avoid numerical problems with the logarithm, being, in our particular case, equal to 2−52 (the value of the roundoff level eps in the programming language Matlab).Once the high-pass filtering is carried out following the procedure previously described and the remaining log filterbank energies are computed, a Discrete Cosine Transform (DCT) is applied over them as in the case of the conventional MFCC yielding to a set of cepstral coefficients.11Another alternative to this method was considered in which the cepstral coefficients were obtained by applying the logarithm and the DCT exclusively on the outputs of the non-removed filters. The first method was finally adopted in this work because a preliminary experimentation showed that it outperformed this second approach.Finally, it is applied a temporal feature integration technique which consists of dividing the sequence of cepstral coefficients into sliding windows of several seconds length and computing the statistics of these parameters (in this case, mean, standard deviation and skewness) over each window. These segment-based parameters are the input to the acoustic event classifier, which is based on Support Vector Machines (SVM). The whole process is summarized in Fig. 4.

@&#CONCLUSIONS@&#
