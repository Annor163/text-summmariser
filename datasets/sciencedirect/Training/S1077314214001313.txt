@&#MAIN-TITLE@&#
Hierarchical Bayesian models for unsupervised scene understanding

@&#HIGHLIGHTS@&#
Two novel hierarchical models for unsupervised scene understanding are presented.We investigate hierarchical structures for modelling context within our models.These models are compared against unsupervised, weakly and fully supervised models.Our new models are competitive with the supervised models for scene recognition.We also show the models operating on a large underwater dataset collected by a robot.

@&#KEYPHRASES@&#
Scene understanding,Unsupervised learning,Clustering,Hierarchical Bayesian models,Topic models,Variational Bayes,

@&#ABSTRACT@&#
For very large datasets with more than a few classes, producing ground-truth data can represent a substantial, and potentially expensive, human effort. This is particularly evident when the datasets have been collected for a particular purpose, e.g. scientific inquiry, or by autonomous agents in novel and inaccessible environments. In these situations there is scope for the use of unsupervised approaches that can model collections of images and automatically summarise their content. To this end, we present novel hierarchical Bayesian models for image clustering, image segment clustering, and unsupervised scene understanding. The purpose of this investigation is to highlight and compare hierarchical structures for modelling context within images based on visual data alone. We also compare the unsupervised models with state-of-the-art supervised and weakly supervised models for image understanding. We show that some of the unsupervised models are competitive with the supervised and weakly supervised models on standard datasets. Finally, we demonstrate these unsupervised models working on a large dataset containing more than one hundred thousand images of the sea floor collected by a robot.

@&#INTRODUCTION@&#
In many real-world applications involving the collection of visual data, obtaining ground truth from a human expert can be very costly or even infeasible. For example, remote autonomous agents operating in novel environments like extra-planetary rovers and autonomous underwater vehicles (AUVs) are very effective at collecting huge quantities of visual data. Sending all of this data back to human operators quickly is hard since communication is usually bandwidth limited. In these situations it may be desirable to have algorithms operating on these vehicles that can summarise the data in unsupervised but semantically meaningful ways.Similarly, many scientific datasets may contain terabytes of visual data that require expert knowledge to label it in a manner which is suitable for scientific inference. Obtaining such knowledge for large datasets can be a large drain on research resources. Again, it would be desirable to have algorithms that can separate this data automatically and in semantically meaningful ways, so the attention of the domain experts can be focused on subsets of the visual data for further labelling. In Section 6 we present a large visual dataset collected by an AUV that exhibits exactly this problem.Recently there has been much focus on the computer vision problem of scene understanding, whereby multiple sources of information and various contextual relationships are used to create holistic scene models. Typically the aim in scene understanding is to improve scene recognition tasks while taking advantage of scene labels or annotations [1–4], accompanying caption or body text [5], or even contextual relationships between image labels and low-level visual features [6,7]. Most of these approaches are weakly supervised, semi-supervised or supervised in nature, and not much attention has been given to fully unsupervised, visual-data only holistic scene understanding.In this article we wish to explore how unsupervised, or visual-data only, techniques can be applied to the problem of scene understanding. To this end we experiment with well established unsupervised models for clustering, such as Bayesian mixture models [8] and latent Dirichlet allocation [9]. These models cluster coarse whole-image descriptors, or cluster individual parts of images (but not simultaneously). We also explore models that can cluster data on multiple levels simultaneously (e.g. image segments or parts, and images), which are similar to the models presented in [4,10]. These models discover the relationships between objects in images, and then define scene types as distributions of these objects. Also, by knowing the scene type, contextual information is used to aid in finding objects within scenes. Finally we present a new model that can cluster multiple sources of visual information, such as segment and image descriptors. This model takes advantage of holistic image descriptors, which may encode spatial layout, as well as modelling scene types as distributions of objects.All of these models are compared on standard computer vision datasets as well as a large AUV dataset for scene and object discovery. Emphasis is placed on scene category discovery, since we compare these unsupervised methods to state-of-the-art weakly-, semi- and supervised techniques for scene understanding. We also compare these models for object discovery in two of the experiments.In the next section we review the most relevant literature to place this work in context. We then present the hierarchical Bayesian models we use for unsupervised scene understanding in Section 3 and in Section 4 we present variational Bayes algorithms for learning these models. In Section 5 we describe the image and image-segment descriptors we use, since these play a large part in the performance of these purely visual-data driven models. Then in Section 6 we empirically compare all of the aforementioned models, and summarise our results in Section 7.Visual context, such as the spatial structure of images, and position and co-occurrence of objects within scenes provide semantic information that aids object and scene recognition in our visual cortex [11,12]. Similarly, semantic information about images can be derived from the volumes of textual data that accompanies these images in the form of tags, captions and paragraph text on the Internet. Consequently, there has recently been a lot of research focusing on holistic image “understanding”, where these sources of information are fused in order to improve scene and object recognition tasks.An early attempt at combining annotation information with scene modelling proposed in [1] extends latent Dirichlet allocation (LDA) [9] to use both visual and textual data for inferring image tags in untagged images. This is essentially a “weak” form of supervision, where the exact image classes are unknown, but some semantically relevant information is still used in training the model. Subsequent research, [2–4,13–15] (amongst others) present hierarchical Bayesian models that can simultaneously classify scenes and recognise objects. These models can be supervised at the scene level, object level, or both. They can also use “weak” labels, or annotations, at the image or object levels [2–4]. Typically the features used to represent each image in these models are the proportions of super-pixel clusters (objects) contained within the images. The super-pixels are usually described by a combination of bag-of-words (BoW) features, such as quantised SIFT descriptors and quantised attributes like colour, texture and shape.Li et al. [3] present a hierarchical Bayesian model that has a principled way of dealing with “noisy” or irrelevant object tags. Essentially a trade off is made between the model’s certainty of the distribution of tags that correspond to a visual object class, and the distribution of tags that are irrelevant to the current object class. If an object class has a strong associated posterior distribution over the corresponding tags, a new tag that has low likelihood under this posterior is likely to be declared as irrelevant by an indicator variable. This model can also infer tags for images when they are missing. Quite a different approach to modelling textual and visual information is presented in [5]. They use a kernel canonical correlation analysis model (CCA) that attempts to learn the latent subspace that connects visual features with unaligned text (e.g. web pages with images). They then use this learned subspace for scene classification tasks.Fei-Fei and Li [16] also present a model where the scene and object levels are classified in the same framework, but are linked through a higher “event” level, such as a particular sporting event. For example, the objects in an image may be a person, skis, etc., and the scene may be of a snowy mountain. Naturally, these are both related to a “skiing” event, which is simultaneously inferred. These higher level contextual relationships were shown to aid image classification. Similarly, it has been found in works such as [17,18] that knowledge of scene-type context can aid object recognition. In [17] the authors use a hidden Markov model (HMM) to classify a scene, and give certain objects a priori more probability of being detected conditioned on the scene type. For example, it is more likely you would find a coffee machine in a kitchen. Similarly, certain objects commonly co-occur, and so detection of one object (street) may be used to aid detection of another object (building), as demonstrated by [19]. They use tree-like models to infer the contextual and spatial relationships of, and between, labelled objects to aid inference in unlabelled test sets. It is worth noting at this juncture that while object discovery can be an important part of scene understanding, emphasis has usually been placed on scene recognition in the scene understanding literature. Object recognition and discovery performance is usually presented in a qualitative fashion. Conversely, scene recognition is not given much attention in the object recognition and discovery literature, which focuses on quantitative measures of recognition and object purity.Many models for scene understanding explicitly model the spatial layout of scenes [14,15,19,20], or may make use of non-parametric processes or random fields to enforce segment-label contiguity [4,21–24]. In [14] the authors present a supervised model, the context-aware topic model (CA-TM), that is similar to hierarchical Bayesian models like [13], but it also learns the absolute (as opposed to relative) position of objects within a scene type. For example, it learns that sky objects are at the top, and buildings are at the sides, of street scenes, etc. Hence it takes advantage of both scene and spatial context for classification and object recognition. It can be both supervised at the scene and, optionally, at the object level. A model with a similar concept is presented in [20], however their emphasis is on object detection/discovery rather than scene recognition. The models presented in this article do not explicitly model scene spatial layout, however, the image descriptors used encode this information, see Section 5 for details.Recently [6] combined Beta-process sparse-code dictionary learning, topic modelling and image classification in one generative framework. Essentially this framework models images from the pixel level to scene level. This is quite an impressive feat, and results in a very complex model. This model can also be used for unsupervised image clustering, but not necessarily object detection/segmentation. It can also use image annotations where available. While the classification results are impressive, each iteration of learning (Gibbs sampling) takes on the order of minutes, when it is usually milliseconds or seconds for other models. A similar concept is presented in [7], however they use a Bayesian co-clustering framework to incorporate semantic knowledge from image labels for visual dictionary learning. They can directly relate image features to semantic concepts, and show better performance than [6].Scene understanding is a very active area of research, however much of the literature is concerned with weakly supervised, semi-supervised (a few strong labels) or supervised approaches to image understanding. Some of the aforementioned models can be used in a fully unsupervised, visual data only setting, though they may operate in a reduced capacity. For instance, the model in [13] loses its ability to perform scene recognition/discovery and reverts to just clustering segments when image labels are not present. Also [4,6] can be used as unsupervised models when no annotation data is available, however they were not rigorously tested in such situations. The only publication, to the authors’ knowledge, that presents a model exclusively designed for unsupervised scene understanding is [25]. This model is also reviewed and used for comparison in this work.There has been more work on unsupervised object discovery, where scene recognition/discovery is not an important consideration. For instance [26,20] cluster segments from multiple image segmentations in order to find the “purest” instances of objects. Though [20] can also make use of object spatial layout, and labelled categories where available, which is a similar approach taken by [27]. A comprehensive review of clustering models such as K-means and spectral clustering, and topic models such as LDA and non-negative matrix factorisation (NMF) applied to object discovery is provided in [28]. They test these models on single and multiple object per image tasks, and with different BoW feature normalisations. Similarly, there has been much work on unsupervised scene discovery, where typically whole-scene descriptors are used without explicitly modelling image parts [29–31].From the aforementioned literature it is apparent that performance for scene and object recognition can be greatly increased by taking advantage of joint scene and object contextual cues. However, it is also apparent that not much attention has been given to achieving holistic scene understanding in a completely unsupervised manner. The work presented in this article reviews and introduces various approaches for a more fully-fledged unsupervised scene understanding framework in the absence of any annotations or related textual information.In this section we present and discuss the structure of a number of hierarchical Bayesian models of increasing complexity that we apply to unsupervised scene understanding tasks. We start with Bayesian Gaussian mixture models (BGMMs) [8,32] and latent Dirichlet allocation [9], but with Gaussian clusters or topics (G-LDA), for scene or segment clustering. We then present two novel models for simultaneous image and segment clustering. The first is the simultaneous clustering model (SCM), which is similar to the models presented in [4,10]. The second is the multiple-source clustering model (MCM) that can cluster both image and segment descriptors.We will present BGMMs in the context of clustering images, but these models can equally be applied to clustering segments.Firstly, a BGMM assumes all images in a dataset,W=wii=1Iwherewi∈RDim, are drawn from a weighted sum of T Gaussian distributions;(1)wi∼∑t=1TπtNwi|ηt,Ψt-1.Hereπ=πtt=1Tare the mixture weights, whereπt∈0,1and∑t=1Tπt=1. Also,ηtandΨtare the means and inverse covariances (precisions) for each Gaussian cluster respectively.An auxiliary indicator variable is also introduced,Y=yii=1Iwhereyi∈1,…,T, which assigns each observation to a Gaussian component according to the following conditional relationship;(2)wi|yi∼∏t=1TNwi|ηt,Ψt-11yi=t.Here1·is an indicator function that evaluates to 1 if the expression in the brackets is true, or 0 otherwise. Theyiare distributed according to a Categorical distribution,(3)yi∼Categπ=∏t=1Tπt1yi=t.Because this is a Bayesian model, prior distributions are placed on all of the model parameters as well,(4)π∼Dirα,(5)ηt∼Nh,δΨt-1,(6)Ψt∼WΦ,ξ,whereW·is a Wishart distribution, and only a single scalar parameter is given to the Dirichlet distribution as shorthand for a symmetric Dirichlet prior.The following describes the generative process of the Bayesian Gaussian Mixture model:1.Draw T cluster parametersηtandΨtfrom (5) and (6) respectively.Draw mixture weightsπ∼Dirα.For each image,i∈1,…,I:(a)Choose an image clusteryi∼Categπ.Draw an observation from the chosen clusterwi|yi=t∼Nηt,Ψt.The graphical model of this process is shown in Fig. 1. The actual (posterior) hyper-parameters (α̃t,h̃t, etc.), number of clusters (T), and indicator assignments (yi) are learned using variational Bayes [8], which is discussed in Section 4.There exist generalisations of this BGMM whereT→∞using a Dirichlet process instead of a symmetric Dirichlet over the mixture weights [33]. Such a model is the variational Dirichlet Process (VDP) presented in [32]. We will not describe such a model here, however we do use the VDP in the experiments in Section 6. We have found that the variational Bayes realisation of the VDP and BGMM yield very similar results.The BGMM and VDP do not take advantage of any contextual or structural information when applied to clustering images or segments. They simply cluster images or segments as if they were all in one “bag”. We use the BGMM/VDP as baseline Bayesian unsupervised methods for comparison to other, more sophisticated methods.Latent Dirichlet allocation (LDA) [9] was originally formulated for modelling text, and typically has a Multinomial or Categorical cluster (topic) distribution. Because the image and segment descriptors we have used in this article are best modelled with Gaussian clusters, we now present a version of smoothed LDA with Gaussian clusters (G-LDA).Using again the application of clustering images, G-LDA assumes data originates in J distinct groups or photo albums,W=Wjj=1J(these groups are known as “documents” in the text modelling literature). Each of these albums containsIjimages,Wj=wjii=1Ij. Also, the mixture weights are specific to each group,πjj=1J, but the clusters are shared between all groups,(7)wji∼∑t=1TπjtNwji|ηt,Ψt-1.The differences between G-LDA and the BGMM are perhaps illustrated more clearly in Fig. 1, and by the following generative process:1.Draw T cluster parametersηtandΨtfrom (5) and (6) respectively.For each group or album,j∈1,…,J:(a)Draw mixture weightsπj∼Dirα.For each image,i∈1,…,Ij:i.Choose an image clusteryji∼Categπj.Draw an observation from the chosen clusterwji|yji=t∼Nηt,Ψt.G-LDA is quite similar to the BGMM except that is has mixture weights specific to each group or album. So each album of images has a specific proportion of scene-types (image clusters). Alternatively, if we used this model to cluster image segments, then each image would be described as a particular proportion of objects (segment-clusters). Hence, G-LDA models album context for image clustering, and image context when applied to segment clustering.Generalisations of LDA toT→∞also exist, such as the hierarchical Dirichlet process (HDP) [34]. These models typically aid in the selection of T because of the hierarchical nature of the prior used. However, we have found this not to be an issue with G-LDA because of the heavy complexity penalties introduced by the Gaussian cluster priors (see Section 4 and Appendix A). Hence, we have elected to stay with the more simple, conjugate LDA-based model for this article.Most standard computer vision datasets are not divided into photo albums, and so in Section 6 we mostly use G-LDA for clustering segments. However, the AUV dataset is comprised of multiple surveys, which we do use as albums.Now we present the novel simultaneous clustering model (SCM), which can simultaneously cluster image segments and images unlike the BGMM and G-LDA. Like G-LDA, the SCM models albums j, but does not explicitly model image descriptors (wji). Instead it models images as distributions of objects, or segment-clusters. This is a “bag-of-segments” representation since the layout, or order, of the segments in the image is not modelled.Each image is comprised ofNjinon-overlapping segments,Xji=xjinn=1Njiwherexjin∈RDseg, which are drawn from a mixture of K Gaussians or “object” types. The segment cluster weightsβt=βt1,…,βtKare specific to each scene-type, t, as opposed to each image in the case of G-LDA;(8)SCM:xjin|yji=t∼∑k=1KβtkNxjin|μk,Λk-1,(9)G-LDA:xjin∼∑k=1KβikNxjin|μk,Λk-1.So where G-LDA models segments as a Gaussian mixture specific to each image, i, the SCM models segments as a Gaussian mixture specific to a scene type or cluster of images, t. Like in the BGMM, an indicator variable for each segment observation,zjin∈1,…,K, is used to assign the observation to a segment-cluster (object-type). This indicator variable also has a Categorical distribution, but is conditioned on the scene indicator,(10)zjin|yji∼∏t=1TCategzjin|βt1yji=t,note how this is similar to (2). Consequently, each image is described as a set of object types,Zji=zjinn=1Nji, which is inherently a Multinomial distribution. This means that each scene-type, t, will have its own unique distribution of objects,βt. The BGMM and G-LDA represent a scene-type as a Gaussian cluster, whereas the SCM represents a scene-type as a Multinomial cluster.All the SCM parameters have prior distributions;(11)πj∼GDira,b,(12)βt∼Dirθ,(13)μk∼Nm,γΛk-1,(14)Λk∼WΩ,ρ.We have chosen to use a generalised Dirichlet distribution,GDir·, [35,36] over the scene-type mixture weights. It can be represented as a truncated stick breaking process, which is also used to approximate a Dirichlet process [37],(15)πjt=vjt∏s=1t-1(1-vjs),vjt∼Betaa,bift<T1ift=T,wherevjt∈[0,1]are “stick-lengths” for each album. Here we have also elected to just choose a scalar value for the hyper-parameters a and b, like in the case of the symmetric Dirichlet. We use a generalised Dirichlet for the SCM because it is a heavier prior (has twice the number of parameters) than the symmetric Dirichlet. This helps variational Bayes select an appropriate number of scene types, T, when using a Multinomial scene-type representation. This is similar in concept to using a HDP, but without the complexity. This modelling choice is explored empirically in Section 6.The entire generative model of the SCM is represented in Fig. 2, and is;1.Draw T image cluster parametersβtfrom (12).Draw K segment cluster parametersμkandΛkfrom (13) and (14) respectively.For each group or album,j∈1,…,J:(a)Draw mixture weightsπj∼GDira,b.For each image,i∈1,…,Ij:i.Choose an image clusteryji∼Categπj.For each image segmentn∈1,…,Nji:A.Choose a segment clusterzjin|yji=t∼Categβt.Draw an observation from the segment clusterxjin|zjin=k∼Nμk,Λk.The SCM is similar in some ways to the model presented in [4], for instance it represents an image as a distribution of object types. However, the SCM retains the image-album context of G-LDA (when the latter is applied to clustering images). The SCM models segments as having scene-type context, i.e. a tree is more likely to appear in a forest scene than an indoor scene. This is unlike G-LDA (when applied to segments), which only models segments as having specific image context. Hence the SCM has better object co-occurrence modelling facility than G-LDA. Distributions of objects are captured at the scene-type level,βt, which involves many images, as opposed to just the single image level,βi.The final model we present in this article is the multiple-source clustering model (MCM). This model has also been presented in [25]. It essentially combines the SCM segment representation with the image-level representation of G-LDA.Like the SCM, the MCM models segments,xjin, as a scene-type specific mixture of Gaussians,βt. But unlike the SCM, the MCM also models image descriptors,wji, as a group specific mixture of Gaussians, like image-level G-LDA. So now scene clusters, or types, are represented by both the proportions of objects (segment clusters) within them,βt, and a Gaussian component that describes the overall scene appearance, parameterised byηtandΨt.The difference between the SCM and the MCM can be visualised by the graphical models in Fig. 2. We also illustrate how images are modelled under the SCM and MCM in Fig. 3. The generative process of the MCM is;1.Draw T image cluster parametersβt,ηtandΨtfrom (12), (5) and (6) respectivelyDraw K segment cluster parametersμkandΛkfrom (13) and (14) respectively.For each group or album,j∈1,…,J:(a)Draw mixture weightsπj∼GDira,b.For each image,i∈1,…,Ij:i.Choose an image clusteryji∼Categπj.Draw an image observation from the chosen image clusterwji|yji=t∼Nηt,Ψt.For each image segmentn∈1,…,Nji.A.Choose a segment clusterzjin|yji=t∼Categβt.Draw a segment observation from the segment clusterxjin|zjin=k∼Nμk,Λk.The type of context that the MCM models is similar to the SCM. The only real difference being that scene types have a joint Multinomial-Gaussian representation. This allows the MCM to more effectively model global scene attributes that may not be captured by just object co-occurrence. For instance, the image descriptors introduced in Section 5 capture the coarse spatial structure of an image.In this section variational Bayes inference algorithms are derived for learning the posterior latent variables of the SCM and MCM, i.e., posterior hyper-parameters, labels, and number of clusters. We do not present the variational Bayes algorithms for the BGMM, VDP or G-LDA since these can be found in [8,32,38,39]. Also, many of the updates are similar between these models.Typically, to learn the posterior latent variables of the types of Bayesian models presented in the previous section, the models’ log-marginal likelihood is maximised with respect to the set of model hyper-parameters,Ξ. The log-marginal likelihood takes the general form:(16)logpX|Ξ=log∫pX,Z,Θ|ΞdZdΘ,whereXare observable,Zare latent indicators, andΘare the set of model parameters. This integral is intractable in the case of all of the presented models, and so an approximation of this marginal log-likelihood is usually made. In the case of variational Bayes, this approximation is called free energy,F. The approximation starts by representing the full joint distribution of the model with a set of factored distributions:(17)pX,Z,Θ|Ξ≈qZqΘ.By optimising the free energy functionalFqZ,qΘ, the Kullback–Leibler divergence,KLp‖q, is minimised between the approximation and the true joint in (17). This optimisation typically results in an expectation maximisation-like algorithm [40] that alternates between finding the expected distribution (or assignments) for the indicators,qZ, and the optimum value of the variational posterior hyper-parameters,Ξ̃, that governqΘ. The expectation and maximisation steps are alternated untilFconverges. For more information on this general variational Bayes algorithm see [38,40]. Also, for the exact form ofFfor the SCM and MCM see Appendix A.Applying the variational Bayes learning algorithm to the SCM yields the following expectation step for the segment indicators,Z,(18)qzjin=k=1Zzjinexp∑t=1Tqyij=tEqβlogβtk+Eqμ,ΛlogNxjin|μk,Λk-1.HereZzjinis a normalisation constant which is straight-forwardly computed from the sum over k of the un-normalised components of (18). Also,Eq·denotes the expectation with respect to the variational distribution,q·. These expectations are given in Appendix B. In (18) the term with the sum over the weighted image label probabilities,qyjin=t, assigns more or less likelihood of the current segment observation belonging to the segment cluster based on the probability of the image belonging to a scene-type, t. For example, if the image is of a forest type, then the current object is more likely to be a tree trunk than a building. This is how scene-type context is modelled for objects.Similarly, the following expectation step is obtained for the image indicators,Y,(19)qyji=t=1ZyjiexpEqπlogπjt+∑k=1KEqβlogβtk∑n=1Njiqzjin=k.Again,Zyjiis simply calculated by summing the un-normalised components over t, and the expectations are given in Appendix B. From (19) we can see that an image is assigned to a scene-type, t, by the number and co-occurrence of object types within it. This is indicated by the term containing sums overqzjin=k, which is essentially a Multinomial log-likelihood.OptimisingFfor the parameters of the SCM leads to the following variational posterior updates for the mixture weights,πj, and image cluster parameters,βt;(20)ãjt=a+∑i=1Ijqyji=t,b̃jt=b+∑i=1Ij∑s=t+1Tqyji=s,θ̃tk=θ+∑j=1J∑i=1Ijqyji=t∑n=1Njiqzjin=k.These updates are essentially just the prior with added observation counts, or sufficient statistics. The sum forb̃jtin (20) must be performed in descending cluster size order, as per [32]. The variational posterior Gaussian–Wishart hyper-parameters for the segment clusters are,(21)γ̃k=γ+Nk,m̃k=1γ̃kγm+Nkx¯k,ρ̃k=ρ+Nk,Ω̃k-1=Ω-1+NkRk+γNkγ̃kx¯k-mx¯k-m⊤,where(22)Nk=∑j=1J∑i=1Ij∑n=1Njiqzjin=k,x¯k=1Nk∑j=1J∑i=1Ij∑n=1Njiqzjin=kxjin,Rk=1Nk∑j=1J∑i=1Ij∑n=1Njiqzjin=kxjin-x¯kxjin-x¯k⊤.The expectation steps (18) and (19) and the maximisation steps (21) are alternated untilFfor the SCM converges.Fis given in Appendix A.Since the MCM is similar to the SCM, apart from the image observation model, many of the variational updates are the same. The image indicator,Y, updates have a different form because of the image observations,(23)qyji=t=1ZyjiexpEqπlogπjt+∑k=1KEqβlogβtk∑n=1Njiqzjin=k+Eqη,ΨlogNwji|ηt,Ψt-1.This equation has the same general form as (19), but with an added Gaussian log-likelihood term corresponding to the image descriptors,wji.The maximisation steps for the model parameters are also the same as the MCM apart from those for the Gaussian-Wishart prior over the image observation,wji, clusters. However, these update equations are the same as those in (21), though the sums in (22) are only over j and i, and involve the image indicators,yji.If the number of clusters, T and K, is known a priori or set to some large value, the label and posterior hyper-parameter updates can simply be iterated untilFconverges to a local maximum. Some of the clusters will not accrue any observations because of the variational Bayes complexity penalties that naturally arise inF. We have found that better clustering results can be obtained if we guide the search for the segment clusters.The segment-cluster search heuristic we use is a much faster, greedy version of the exhaustive heuristic presented in [32]. The SCM and MCM start withK=1segment cluster, and iterate until convergence. Then the segment cluster is split in a direction perpendicular to its principal axis. The two resulting clusters are then refined by running variational Bayes over them for a limited number of iterations (we use a maximum of 15).Fis estimated with this newly proposed split, and if it has increased in value, the split is accepted and the whole model is again iterated until convergence. Otherwise, the algorithm terminates. The exhaustive heuristic proceeds by trialling every possible cluster split between each model convergence stage, and only accepts the split that maximisesF. When K becomes large, this search heuristic becomes the dominant computational cost of the whole inference algorithm.In our greedy “split-tally” heuristic, we guess which cluster to split first by ranking all clusters’ approximate contribution toF(details in Appendix C). Also, a tally is kept of how many times a cluster has previously failed a split trial. Clusters that have not yet failed splits are prioritised for splitting. The first cluster split to increaseFis accepted, and the tally for the original cluster is reset. All clusters must eventually fail to be split for the algorithm to terminate. We have found this split-tally heuristic greatly reduces run-time, without significantly impacting performance, mostly because of the tally. To our knowledge, this is the first time a tally has been used in such a heuristic. A similar heuristic was also trialled to search for T in the MCM, however we found that it was better to randomly initialise it to some large value,Ttrunc>T, since both heuristics would interact. Also, there is no intuitive way to split the purely Multinomial image clusters of the SCM, so it is also randomly initialised.We also use this split-tally heuristic for searching for the number of clusters in the VDP and G-LDA, when applied to image or segment clustering, in Section 6.Because all of the aforementioned models are Bayesian we need to choose priors in the form of initial hyper-parameter values. These priors are then updated as evidence is presented to the learning algorithm. By prioritising simplicity, we have chosen the following values for the prior hyper-parameters;(24)α,a,b,θ,γ,δ=1,ρ=Dseg,ξ=Dim,m=mean(X),h=mean(W),Ω=(ρCw,s)-1IDseg,Φ=(ξCw,iλcov(W)max)-1IDim.The values for the first three equations in (24) have been chosen to be their minimum integer value in the support of their respective distributions. We have found that apart fromθ, changing these values only has a minor affect on the posterior clusters. For the SCM in Section 6 we do vary the value ofθ.The prior parameter,θ, essentially controls how many different objects (segment clusters) we expect to be in a particular scene-type a priori. For low values ofθ, we would expect only a few objects within each scene-type, i.e. we expectzjinto only take a few values of k for a particular scene-type, t. Therefore more image clusters may be required to represent all possible object-types, k, since only a few can exist in a scene-type. We would expect the opposite to occur for high values ofθ.The Wishart matrix priors in the last two equations of (24) are just scaled identity matrices. This has the effect of making the algorithms expect isotropic Gaussian clusters in the data a priori. Alsoλcov(W)maxis the largest Eigenvalue of the covariance of the image descriptors. This value is not used for the segment descriptors since they are whitened, see Section 5.Cw,iandCw,s(i for image, s for segment) are tunable parameters that encode the a priori “width” of the isotropic clusters. These tuning parameters were found to have the largest effect on the number of (Gaussian) clusters found by the algorithms. In Section 6 we will show clustering performance with varyingθ,Cw,iandCw,s.The aforementioned algorithms rely on highly discriminative visual descriptors since they are driven by visual-data alone. We have chosen unsupervised feature learning algorithms for this task as they are easily implemented and have excellent performance in a number of scene recognition tasks, e.g. [41].We use a modified sparse coding spatial pyramid matching (ScSPM) [41] method to encode the image descriptorswji. Fig. 4(a) demonstrates how these image descriptors are created. For all experiments we use the original 1024-base Caltech-101 dictionary supplied by [41] to encode dense16×16pixel SIFT patches with a stride of 8 pixels. We have found little to no reduction in classification and clustering performance using this pre-learned dictionary as compared to learning dictionaries for each specific dataset. This is similar to the observation made in [42].We use orthogonal matching pursuit (OMP) with 10 activations in place of the original sparse coding encoding method used in [41] for the larger AUV dataset. It is much less computationally demanding and does not affect scene clustering performance greatly. We use the original pyramid with a [1,2,4] pooling region configuration, which leads to a 21,504 dimensional (sparse) code for each image. This is far too large to use with a Gaussian cluster model, but we have found that these codes are highly compressible with (randomised) PCA. Typically we can compress them toDim=20while still achieving excellent image clustering performance.Out of the many segment descriptors tried, it was found that pooling dense independent component analysis (ICA) [43] codes within segments gave the best results. The following procedure was used to create a descriptor for each segment within an image:1.Extract square patches centred on every pixel in the image.(Optional) remove the DC offset, and contrast normalise the patches.Use a random subset of all of the patches in the dataset to train an ICA dictionary,D, and its pseudo-inverse,D+.UseD+to create a code (or filter response),al, for all of the patches. This is a fast matrix multiplication operation, so is feasible for patches centred on every pixel,l∈[1,L], in an image. L is the total number of pixels in an image.Over-segment the image, obtaining sets of pixelsSjin. The results presented here used a fast mean-shift segmentation method [44].Obtain segment descriptors by mean pooling all of the ICA dictionary responses in a segment in the following manner:(25)x̃jin=1#Sjin∑l∈Sjinlog|al|.These transformations greatly improved segment clustering performance. We conjecture that the absolute value makes the descriptors invariant to 90 degree phase shifts inrl. The logarithm transforms the range back to(-∞,∞).Obtain the final segment descriptors,xjin, by PCA whitening all thex̃jin. We perform dimensionality reduction as part of this whitening stage, toDseg=15, which preserves more than 90% of the spectral power.This process is graphically demonstrated in Fig. 4(b).A bag-of-words representation was not chosen for the segments as it would require a Multinomial cluster distribution as opposed to Gaussian. We found this representation to be less powerful for model selection in this unsupervised application (this is demonstrated in Section 6 at the image level). We have chosen to leave a comprehensive comparison between these ICA-based features and bag-of-words features for object detection as future work. However, we do compare the image ScSPM representation against a bag-of-words image representation for use in spectral clustering in Table 1, where we can observe tangible benefits. Naturally, the performance of these algorithms is largely influenced by the representation chosen.Both the image and segment descriptors take about 1s each per image to calculate. The ScSPM and ICA features are complementary; the ScSPM descriptors encode the spatial layout and structural information of an image (the “gist”), whereas the ICA features encode fine-grained colour and texture information.

@&#CONCLUSIONS@&#
