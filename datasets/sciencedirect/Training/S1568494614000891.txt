@&#MAIN-TITLE@&#
Adaptive directional local search strategy for hybrid evolutionary multiobjective optimization

@&#HIGHLIGHTS@&#
A novel adaptive local search method is developed for hybrid evolutionary multiobjective algorithms.An efficient directional local search operator is also proposed.Local search probability is adapted based on effectiveness of local search operator.The present adaptive method is applied to uni- and multi-modal test problems.The present method successively allocate computational budget to evolutionary and local operators.

@&#KEYPHRASES@&#
Evolutionary multiobjective optimization,Memetic algorithms,Directional operator,Adaptive local search,

@&#ABSTRACT@&#
A novel adaptive local search method is developed for hybrid evolutionary multiobjective algorithms (EMOA) to improve convergence to the Pareto front in multiobjective optimization. The concepts of local and global effectiveness of a local search operator are suggested for dynamic adjustment of adaptation parameters. Local effectiveness is measured by quantitative comparison of improvements in convergence made by local and genetic operators based on a composite objective. Global effectiveness is determined by the ratio of number of local search solutions to genetic search solutions in the nondominated solution set. To be consistent with the adaptation strategy, a new directional local search operator, eLS (efficient Local Search), minimizing the composite objective function is designed. The search direction is determined using a centroid solution of existing neighbor solutions without making explicit calculations of gradient information. The search distance of eLS decreases adaptively as the optimization process converges. Performances of hybrid methods NSGA-II+eLS are compared with the baseline NSGA-II and NSGA-II+HCS1 for multiobjective test problems, such as ZDT and DTLZ functions. The neighborhood radius and local search probability are selected as adaptation parameters. Results show that the present adaptive local search strategy can provide significant convergence enhancement from the baseline EMOA by dynamic adjustment of adaptation parameters monitoring the properties of multiobjective problems on the fly.

@&#INTRODUCTION@&#
Multiobjective optimization methods are getting increased attention, as consideration of trade-offs between conflicting objectives is becoming more critical in multidisciplinary design optimization (MDO) of modern engineering problems. Evolutionary algorithms (EAs) are well suited for multiobjective optimization problems (MOP), because they are based on a population rather than single solution, and therefore can more naturally adapt to generate distributed solutions on the Pareto front. During the last two decades, evolutionary multiobjective optimization algorithms (EMOAs) have shown great progress and success [1–4].A well-known drawback of EAs is their slow convergence near the optimum solution. This behavior is still true for EMOAs. The convergence behavior of EMOAs can be graphically explained in a two-dimensional design space of a two objective minimization problem in Fig. 1[5], in which the concentric ellipses are iso-contour lines of each objective function. The size of a descent cone is a set of search directions in which any of the objectives are not decreasing with properly selected step sizes. Since the descent cone angle decreases as design converges, a random search is expected to have slow convergence near the Pareto front. Another possible reason of slow convergence of EMOAs is that the relative size of the descent zone decreases exponentially as the number of objectives increases. To cope with the convergence issue, hybrid methods incorporating local search into EMOAs have been suggested. The hybrid methods are also referred to as memetic EMO algorithms [6].Local search methods used in the hybrid EMO algorithms can be put into two categories: neighborhood-based local searches and directional local searches. For all the references in the introduction section, combinatorial optimization methods are explicitly mentioned as combinatorial, and unmarked optimization methods in the text are for continuous optimization.Neighborhood-based local search schemes generate perturbed solutions around a baseline solution and try to find better solutions than the baseline in terms of either a composite objective or a Pareto-dominance relation. Deb and Goel [7] applied a hill climbing local search method to only final solutions obtained by EMO algorithms. A composite objective function is used with weighting factors determined by the location of each solution in the objective space so that a solution movement by the local search results in a better spread (diversity) of solutions. Ishibuchi and Murata [8,9] suggested the multiobjective genetic local search (MOGLS) algorithm for combinatorial optimization, in which a local search is applied to each offspring in every generation using a composite objective function with the same random weights for parental selection and evaluation of local search solutions. Jaszkiewicz [10] improved the MOGLS by adding mating restriction to its parental selection mechanism. Knowles and Corne [11] suggested a memetic Pareto archived evolution strategy (M-PAES) by including a crossover operator in PAES [12]. In M-PAES, all new solutions generated by genetic and local searches are accepted or rejected based on the Pareto-dominance relation and the grid-type partition of the objective space. In Ishibuchi and Narukawa [13], it was reported that the composite function approach gives better results than the Pareto-dominance relation approach for combinatorial optimization problems. Minella et al. [14] gave a comprehensive review of multi-objective algorithms for flowshop scheduling problems and identified two state of the art algorithms: MOSA (multi-objective simulated annealing) [15] and MOGALS [16]. Later, Dubois-Lacoste et al. [17] devised a new multi-objective local search method for flowshop scheduling problems hybridizing two-phase (TP) and Pareto local search (PLS) algorithms that is superior to the algorithms discussed by Minella et al. [14]. Liefooghe et al. [18] presented a more recent review of dominance-based multiobjective local search algorithms that are adopting a neighborhood structure and Pareto dominance relation in the context of combinatorial optimization.In continuous optimization, the covariance matrix adaptation evolutionary strategy (CMA-ES) [48] is the state of the art algorithm for single-objective problems. A multi-objective variant of CMA-ES (MO-CMA-ES) was proposed by Igel et al. [53]. CMA-ES and its variants can also be considered as a neighborhood-based local search method.Unlike the neighborhood-based methods, directional local search methods conduct a local search along a search direction, which can be determined either by sensitivity information of objective functions with respect to decision variables or by approximation of an improving search direction using neighborhood solutions. They will allow a very accurate local search if objective functions are differentiable and accurate gradient information can be obtained. An important issue for directional local search methods is whether to calculate the objective function gradient vector explicitly or not. In general cases without any efficient sensitivity analysis code available, an explicit calculation of the gradient vector of an objective function by a first-order finite differencing requires Ndvadditional function calls, where Ndvis the number of decision variables. The computational cost would easily become prohibitive if Ndvis large and objective function evaluations are expensive.Bosman and de Jong [19] tested three different techniques to use gradient information. They reported that using nondominated improving directions is superior to using gradients of objectives separately. Harada et al. [20] proposed a new gradient-based local search method called the Pareto Descent Method, which finds Pareto descent directions and moves solutions in such directions. In both Refs. [19,20], hybrid methods using EMO algorithms and gradient-based local searches are adopted with an explicit calculation of gradient information by finite differencing. Search directions are randomly selected among descent directions in the objective space. Both references, however, show difficulties in improving convergences especially for problems of a large number of decision variables Ndv, mainly because of excessive computational burden for gradient calculation by finite differencing and very expensive line searches. Since modern engineering MDO problems tend to have a large Ndvand expensive objective functions, we prefer not to explicitly calculate gradient information for directional local searches.As an alternative to explicit calculation of gradient information, search directions can be approximated by utilizing neighboring individuals. Brown and Smith [5] suggested an approximation method for the local Jacobian matrix using the difference between a point of interest and local neighbors. Wanner et al. [21] proposed a quadratic polynomial approximation of all objective functions around an initial solution for local search with sample solutions gathered in the optimization process. Recently, Lara et al. [22] compared hybrid methods with and without explicit gradient information and showed that using the gradient information was advantageous for the problems they tested, assuming that an explicit gradient calculation is equivalent to five function calls. In Schutze et al. [51], however, the authors reported that using the gradient information was not advantageous for the test problems. The effectiveness of using explicitly calculated gradient information depends heavily on how to take the cost of sensitivity analysis into account in the computational budget.Another major issue for the hybrid methods is that we do not know a priori if conducting a local search would be beneficial for a specific optimization problem at hand. It would be effective for unimodal problems or in the last stage of an optimization process near the true optimum; however, it might be a waste of computational budget for multimodal problems, where an exploratory search is more desirable than exploitation by a local search for successful optimization. Goel and Deb [23] compared two approaches: (1) modifying final solutions by EMOA with local searches (posteriori approach) and (2) conducting local searches for all solutions obtained by genetic operations during an EMOA run (online approach). They reported that the posteriori approach is superior to the online approach for the test problems, mainly because of too much computational cost spent during local searches for the online approach. They also mentioned that a balance between local and evolutionary searches is essential to achieving best results. Ishibuchi et al. [24] conducted a parametric study to show the importance of a balance between local and global searches in the context of multiobjective combinatorial optimization. However, Minella et al. [14] pointed out that the approach of Ishibuchi et al. [24] is inferior to state of the art algorithms in multi-objective flowshop problems.An effective use of genetic and local search operators within the available computational budget can be implemented by an appropriate selection of local search probability pls. Hart [25] proposed fitness-based and distribution-based adaptive methods for dynamic adjustment of plsin single-objective optimization problems. Fitness-based adaptive methods allocate higher plsfor individuals with better fitness. In distribution-based adaptive methods, solutions far apart from each other are selected to apply local search. Lozano et al. [26] presented a fitness-based adaptive strategy that assigns plscomparing fitness of the worst individual in the population and new individuals generated by genetic operators in the framework of single objective steady-state real coded genetic algorithms. Molina et al. [27] suggested the concept of local search chain to adjust the number of function evaluations allowed to a local search method. In Schutze et al. [51], a local search with a constant plsis applied after 75% of generations have been completed. In Lara et al. [22], plsis set so that a constant number of solutions undergo local search operations throughout the optimization. A pre-defined schedule with a periodic sawtooth function is used by Synhya et al. [28]. Five different dynamic scheduling patterns of plswere tested by Ishibuchi et al. [29] on combinatorial optimization problems: constant, stepwise increase/decrease, and linear increase/decrease. They found that better results are obtained by varying the probability rather than by keeping it constant for almost all test problems, and that the best schedule is problem-dependent. An adaptive operator allocation scheme was suggested by Bosman and de Jong [30], in which the numbers of improved solutions by three different gradient-based techniques and a variation operator of a baseline EMOA were monitored as a measure of the effectiveness of each operator. Then available computational resources were redistributed proportional to the number of improved solutions per evaluation to allow a more effective operator to spend more resources. Successful results were reported for numerical experiments; however, the quantitative amount of the improvement was not taken into account for the resource redistribution. Lopez et al. [31] introduced a balance control mechanism incorporating a local search based on the number of nondominated solutions in a population. This offered, however, only limited adaptivity: parameters for the balance control were selected so that the local search was applied up to 10 solutions for a population of size 100, which is equivalent to setting the upper limit of plsas 10%.The objective of the present study is to develop an adaptive hybrid EMOA-local search method to enhance the convergence of EMOAs by balanced usage of computational budget on genetic and local search operators. The adaptive strategy quantifies improvements in convergence by local and global search operators and evaluates local and global effectiveness of the local search to dynamically adjust adaptation parameters such as a neighborhood radius r and local search probability pls. The improvements in solution convergence made by genetic and local search operators are measured based on a composite objective, which is an evenly weighted sum of normalized objective functions. We also devise a novel efficient directional local search method, eLS (efficient Local Search), which is designed to minimize additional usage of computational budget for objective function evaluations by adopting an gradient-free approximation of search direction using existing neighbor solutions and a line search method using only two function evaluations. The eLS method also allows adaptive search distances, which decrease as the optimization process converges. The performance of hybrid EMOA+eLS is compared with that of hybrid EMOA+HCS1 by Lara et al. [22]. NSGA-II was used as the baseline EMOA for the hybridization.The remainder of this paper is composed as follows: Section 2 describes details of the new directional operator without utilizing explicit gradient information of a composite objective function. An adaptive local search strategy for the hybrid method is suggested in Section 3. Numerical results for adaptation of neighborhood radius are presented in Section 4. Numerical results of the adaptation of local search probability are presented in Section 5. Finally, Section 6 gives concluding remarks.Multiobjective optimization problems (MOPs) are defined in their general form as follows:(1)Minimize(f1(x),f2(x),…,fM(x))Subject togj(x)≤0,j=1,2,…,Jwhere M and J are the number of objective functions and inequality constraints, respectively. In this study, only continuous objective functions and decision variables are considered. The dominance relation between solutions of MOPs is defined by the following two conditions:Solution a dominates solution b for minimization problems if(1)fj(a)≤fj(b) for all j∈{1, …, M}fk(a)<fk(b) for at least one k∈{1, …, M}In other words, solution a dominates solution b if a is not worse than b in all objectives and a is strictly better than b in at least one objective; this is written symbolically as “a≺b” Similarly, if a is dominated by b, it is represented as “a≻b” A solution that is not dominated by any other solutions is said to be “nondominated”; the nondominated solutions form the Pareto optimal set.In this paper, NSGA-II (nondominated sorting genetic algorithm II) [4] is used as a baseline EMO algorithm because of its popularity and widespread usage in engineering MDO applications. NSGA-II is an elitism approach utilizing the fast nondominated sorting. The crowded distance sorting is used for diversity preservation. Fig. 2(a) shows the main procedure of NSGA-II. In Fig. 2, P stands for parents set of size Npop, Q stands for children set of size Npop, and R=p∪Q is the combined set of parents and children. t is for generation number.In the present hybrid EMOA approach, a local search operator is treated as another mutation operator in the baseline EMOA and applied to each individual in P with the local search probability of pls. In other words, initial solutions for the local search are selected as all the parent solutions regardless of their dominance relations. The selection of initial solutions for local search may have a large effect on the performance of local search operations [32]. Any other possible choices of the initial solutions (for instance, selecting only nondominated solutions in P) are not tested in this study and remain for future work.The local search operator was not applied during the first ten generations because conducting the local search in the very beginning of the optimization process may cause genetic shift and premature convergence especially for multimodal problems.A set of all new local search solutions, L, is added to R, the combined set of parents and children, to join the selection process for parents of the next generation as illustrated in Fig. 2(b). If the cost (required number of function evaluations) of a local search operation is Cls, |L| is about Npop×pls×Cls.Adding local search solutions to the combined set R is a simple and effective way to address one of the issues of multiobjective memetic algorithms: the use of local search may cause complications for achieving solution diversity [6]. Ishibuchi and Narukawa [13], López-Ibáñez et al. [50] and Lara et al. [22] among others are adopting this approach. By having local search solutions go through the selection process composed of the ranking and pruning of the baseline EMOA, diversity can be imposed to the combined set of solutions. It also allows a chance for quality assessment of local search solutions among the parents and children solutions, since even if a solution is improved in terms of a composite objective it may not make actual improvement from the current nondominated solutions depending on the convex or concave shape of the Pareto front surface [13].Lara et al. [22] proposed a novel local search operator, with or without explicit gradient calculations. The local search is composed of two procedures: hill climbing and sidestepping. The hill climbing tries to improve convergence to the Pareto front. The sidestepping is a lateral search along the Pareto front surface for better solution spread. To determine search directions without explicit gradient calculations, they utilize randomly generated neighbors of an initial solution within a given radius in the design space. When a randomly generated neighbor dominates or is dominated by the initial solution, a local search is conducted along the direction of the difference vector between the two solutions. When all the neighbor solutions generated by Nndtrials are mutually nondominating with the initial solution, it is considered that the initial solution is close to the Pareto front and it would not be easy to improve the solution further toward the Pareto front according to the arguments regarding Fig. 1. So the mode of local search is switched from the hill climbing to the sidestepping. They named the method as HCS (Hill Climber with Sidestep). HCS1 and HCS2 stand for HCS without and with explicit gradient calculations, respectively.In HCS1, a neighborhood set S1 is defined for an initial solution x0 as follows:(2)S1(x0,r)={x∈Rn|x0i−r≤xi≤x0i+r∀i=1,…,Ndv},The overall procedure of HCS1 is presented in Fig. 3(a). Nndin Fig. 3(a) is a prespecified parameter for the maximum number of trials of a random neighbor solution generation and was set as 3 or 5 depending on the baseline EMOAs combined with HCS1 in [22]. In the present study, Nndis set as 3. The line search procedure for HCS1 is shown in Fig. 3(b), in which any convex single objective is minimized using a quadratic polynomial fitting. Nmax in Fig. 3(b) is a maximum number of trial of function convexity in a line search. In this study we set Nmax=2. The number of function calls for each HCS1 run varies from 3 to Nnd+2M−2. When a neighbor solution xn∈S1 generated at the first trial is in the zone of dominance and at least one objective function is convex, the number of function calls is 3. When Nndtrials of random generation of a neighbor solution do not provide a neighbor solution in the zone of dominance, sidestepping is conducted generating 2M−2 new solutions making the maximum number of function calls Nnd+2M−2. For example, when Nnd=3 and M=2, the maximum number of function calls is 5. HCS1 was combined with EMOAs such as NSGA-II and SPEA2 and shown to improve performances for unimodal functions such as CONV1, CONV2 and DTLZ2 [22].The present local search method is basically a hill climbing method; the sidestepping is not adopted in order to reduce the number of additional function calls. Also, instead of random generation of new neighbor solutions, neighbor solutions are selected from existing solutions obtained during the optimization process. This would also help to reduce the required number of function calls of the local search. Therefore, the new local search method is named as the eLS (efficient Local Search) method. Overall procedure of eLS is presented in Fig. 4.The neighborhood set S for eLS is defined as follows:(3)S2(x0,r)={x∈Rn|x∈Hand(x0i−r≤xi≤x0i+r∀i=1,…,Ndv)and(x≺x0orx≻x0)},where H is a set of all solutions gathered in the optimization history. Compared to Eq. (2), Eq. (3) includes the condition to be in the zone of dominance in the definition of S2. In other words, members of S2 are located in either the descent cone (−,−) or the ascent cone (+,+) in Fig. 1. |S2|max, the upper bound of |S2|, is set as max(100, Ndv).An important issue for eLS is how to determine the neighborhood radius r. In general, the smaller the neighborhood radius is, the more accurate the resulting search directions will be. The radius r taken in Lara et al. [22] for random generation of neighbor solutions was 0.05 or 0.1 depending on baseline EMO algorithms. In eLS, however, there is no guarantee to have |S2|>0 for a small r especially in an early stage of an optimization process, in which case existing solutions are sparcely scattered in the design space. Specifying a radius small enough for accurate search directions and large enough to have |S2|>0 would be very difficult and problem dependent.To cope with this neighborhood radius issue, we propose to select neighbor solutions in H that are closest to the initial solution until |S2|max is reached, and to select a centroid solution xc of S2 to calculate a descent search direction. This approach allows an adaptive variation of the local search distance |x0−xc|, which will get reduced regardless of the specified value of r as more solutions are gathered near the Pareto front and also near the initial solution as an optimization process converges. In actual implementation, S2 is filled with the latest solutions in H until |S2|max is reached. The reason for taking the latest solution is that more recent solutions are likely to be closer to the Pareto front than older solutions. Also, it costs much less computational time than sorting all the solutions in H for distance from initial solutions of local search.Once S2 is built, a centroid solution xcof S2 is calculated by a weighted summation of decision vectors using the following relation:(4)xc=∑S2j=1w0jxj∑S2j=1w0j,where the weighting factor w0jis defined as the inverse of Euclidean distance in the design space between an initial solution x0 and a neighbor solution xjin order to emphasize genotypically closer solutions to the initial solution in the weighted sum:(5)w0j=1|xj−x0|When the centroid xc is calculated, a solution closest to xc in the design space is selected from S2 and newly set as xc. Once xc is determined in S2, a line search is conducted.The line search procedure of eLS is similar to that of HCS1 except that the normalized composite objective Z in Eq. (6) is minimized instead of conducting a separate line search for each objective function.(6)Z(x)=∑m=1Mfm(x)−fmminfmmax−fmmax,where fm(x) is the mth objective function for the solution x, andfmmaxandfmminare the maximum and minimum values of the mth objective function for nondominated individuals in the population. The composite objective function is employed in this study to be consistent with the adaptive local search strategy described in Section 3.The composite function in Eq. (6) corresponds to an evenly-weighted scalar function. The use of random weighting factors, instead of uniform weighting factors, might be beneficial for avoiding potential bias of the local search procedure [24]. Here we choose, however, to set the even weighting factors in order to keep consistency in the quantitative comparison of performances by directional and genetic search operators, which will be described later in Section 3 for our adaptation strategy. In addition, the use of uniform weighting factors for the composite objective does not necessarily result in uniform search directions in the objective space because the search direction is also determined from S2, solutions of which are mainly generated stochastically by genetic operators.In multi-objective problems, a composite or aggregated objective function approach may have difficulties in finding solutions on a nonconvex Pareto front [33, pp. 55–56]. General local search methods combined with an EMOA, including the present directional local search approach, however, do not suffer from the nonconvexity issue because EMOAs can deal with nonconvex Pareto fronts without any problem. If an initial solution for the local search lies off the nonconvex Pareto front and a search direction is properly selected, application of a local search operation can find an improved solution closer to the nonconvex Pareto front.A line search is conducted for Z in the direction from xl and xm, that are defined asxl=x0,xm=xcifxc≺x0xl=xc,xm=x0otherwiseZ is approximated as a quadratic polynomial of a scalar parameter α[34] following the procedure shown in Fig. 4(b):(7)Z(a)=aa2+bα+cThe third solution xr for the quadratic fitting is determined along the search direction as xr=xm+(xm−xl), and a function evaluation is made at xr. If the following condition for convexity is satisfied for the three solutions as illustrated in Fig. 5,(8)Z(αm)−Z(αl)αm−αl<Z(αr)−Z(αm)αr−αm,the coefficients a, b, and c in Eq. (7) are calculated by the three Z values, and the second function evaluation is made for Z(α*) with the optimum step size α*=−b/2a (see Fig. 5(c)). If the convexity condition in Eq. (8) is not satisfied the step size is doubled, and the three-solution set for line search is shifted to right such that (xl, xm, xr)new:=(xm, xr, xr+2(xr−xm))old. Then, Z for the new xr is computed and the convexity condition is checked again. The convexity check is conducted up to Nmax times as shown in Fig. 4(b). In this study, Nmax is set as 2 for both eLS and HCS1.Clsis defined as the number of additional function evaluations needed to conduct a line search for a later use in an adaptive local search strategy. The minimum value of Clsis 2 if Z is convex in the first check of convexity, and the maximum value of Clsis Nmax+1, which is 3 in the present study with Nmax=2. From our empirical observations, Cls=2 for each line search in eLS for most test cases. This is because xl and xm are already selected to be descending in Z. The search distance |xm−xl| is relatively large in the early stage of optimization stage, so it is much easier to obtain a large Zrmaking the profile convex. In the later stage of optimization process, it is more likely that the local search is being conducted in a valley rather than on a ridge and the search distance becomes relatively small due to the adaptive nature of eLS, which will also make Z show a locally convex variation. Therefore,C¯ls, average of Cls, in Table 1is set as two, as a reference cost for eLS.When Z shows a convex behavior, the final output of the local search is determined as follows:(9)Zopt=Z(α*)If Z is not convex in the line search procedure repeated for Nmax times, the final output function of the local search is determined as the best solution found in the line search procedure:(10)Zopt=min(Z(αm),Z(αr))The improvement by a local search is measured in terms of the difference between initial and final values of Z:(11)impls=Z0−ZoptIn summary, eLS is designed to minimize additional usage of computational budget for objective function evaluations in the local search procedure by determining gradient-free descent directions with pre-existing neighbor solutions and by using a line search method based on a quadratic polynomial fitting with only two function calls. Some unique features of eLS compared to HCS1 are listed below:(1)Sidestepping is not employed in the local search to save computational budget.Neighbor solutions are selected from the existing solution pool H instead of being newly generated.The neighbor solution set S2 is filled with recent solutions in H, which are expected to be close to the initial solution x0.A centroid solution of S2 is used in the line search. A weighted average procedure is proposed for calculating the centroid using the inverse distance as a weighting factor. This allows adaptation of local search distances.The composite objective function is used for the local search instead of using each objective function separately.Items (1) and (2) above help to drastically reduce the number of function calls from that of HCS1. Items (3) and (4) contribute to adaptation of the search distance.In this study, we only consider unconstrained MOPs with bounded decision variables. However, general constraint treatment techniques for line search methods [35] can be readily applied if necessary, with additional computational cost in the line search.An adaptation strategy for adjusting local search parameters should be applicable to both unimodal and multimodal problems without any a priori knowledge on MOPs at hand. In other words, an adaptation algorithm should be able to detect property of MOPs on the fly and adjust parameters accordingly. For example, if a local search is more effective than genetic operators the local search probability should be increased, and vice versa. In this study, a new adaptation method is tested for two parameters: neighborhood radius r and local search probability pls. Adaptation of the parameters is based on local and global effectiveness of a local search operator. The local effectiveness is measured by quantitative comparison of improvements made by genetic operators and a local search operator. The global effectiveness is determined by the ratio of numbers of nondominated solutions by local search and genetic search operations.Before we proceed to present a strategy for the adaptive local search, a measure of average improvement by genetic operators such as crossover and mutation in terms of the composite objective Z is defined. For this purpose, we compare a mating pair P1, P2 and their two children C1 and C2 regarding the dominancy relations and differences in the composite objective function Z as illustrated in Fig. 6. The improvement ΔZij≥0, i, j∈{1,2} is defined as(12)ΔZij=ZPt−ZCtifCtdominatesPj0otherwiseAfter calculating ΔZijfor all mating couples of parents and their children in a generation, the average improvement by evolutionary operators impevolis calculated by(13)impevol=∑k=1Npop/2max(ΔZ11+ΔZ22,ΔZ12+ΔZ21)kNpop−Nnocountwhere Nnocountis defined as the number of parent solutions in the mating process of each generation that are mutually nondominating with their two children. For example, in Fig. 6, Nnocountis increased by one because P1 is neither dominating or being dominated by C1 or C2 and P2 is dominated by C2. Also, ΔZ11, ΔZ21, and ΔZ22 are all zero, and ΔZ12 is non-zero from Eq. (12).The metric of evolutionary improvement impevolprovides a single averaged value of distributed improvements in terms of convergence for a population of each generation. We can consider some extreme situations such that(1)All the children solutions are dominating their parents but the improvement is small.Only one child is making improvement from its parents, but the improvement is very large.Situation 2 above may have a larger value of impevolthan situation 1, or vice versa. The amount of variance of the improvements may warrant special attention for the measure of overall improvement and monitoring convergence property of optimization; this subject is left for future study.The metric impevolconcerns only convergence aspect of improvement made by evolutionary operators because it is meant to be compared with improvement in Z by the local search given in Eq. (11). And as mentioned in Section 2, we are not considering enhancing diverisity or sidestepping by eLS.Other progress measures such as the S metric [36] and a running convergence metric [37] could be alternatives to the above-suggested impevolmetric. Employment of other metrics for adaptive local search remains for future work.Given the measures of convergence improvement by the directional local search operator and evolutionary operators, defined in Eqs. (11) and (13), respectively, the local effectiveness of a local search operator to evolutionary operators can now be determined. Fig. 7shows three zones in the objective space representing zones of success, failure and nondeterminacy for a local search operator. For evaluation of local effectiveness (worthiness) of conducting a local seach, we introduce the required improvement in Z, impreqCls×impevol, to impose a condition that a local search should make (at least) Clstimes more improvement than evolutionary operators since it spends Clstimes more computational cost than evolutionary operators do.The center point of the dotted cross in Fig. 7 represents the initial solution in the objective space, and the other points are possible locations of a final solution of a directional local search. If the local search achieves at least the required improvement impreq, the local search is defined as successful. If the final solution is dominated by the initial solution, the local search defined as a failure. If the local search is not successful and the final solution is not dominated by the initial solution, the solution lies in the zone of non-determinacy: we cannot tell whether the local search is a success or a failure at this moment. In summary, the success and failure of a local search is determined as follows:if impls≥impreqthe local search is a success; Nsuccess:=Nsuccess+1otherwiseif the initial solution dominates the final solutionthe local search is a failure; Nfailure:=Nfailure+1endifendifThe numbers of success (Nsuccess) and failure (Nfailure) are counted for all the local search operation conducted in each generation. The difference of numbers of successes and failures, Nsuccess−Nfailure, defines the local effectiveness in a generation and is used as major indicator in our adaptation strategy. If Nsuccess−Nfailure>0, we define the local searchs in the generation as locally effective.Ishibuchi and Narukawa [13] discuss the use of the weighted scalar and the Pareto ranking approaches in a local search, with the concept of movable areas as shown in Fig. 8(a) and (b). In their S-MOGLS algorithm, a local search solution is accepted (the movement of the initial solution to the local search solution is accepted) if it lies in the movable area, and rejected otherwise. If the local search solution is accepted, the initial solution is replaced by the local search solution. It is noted that the movable area of the Pareto ranking approach is much smaller than that of the weighted scalar approach, because the movable area of the Pareto ranking approach shrinks exponentially with the number of objectives, whereas that of the weighted scalar approach is always a half of the objective space. A modified method allowing a movable area between the weighted scalar and the Pareto ranking approaches, is also considered by them (see Fig. 8(c)). Selection of a proper angle for the modified movable area is, however, problem dependent.Recalling our definitions of the zones of success and failure illustrated in Fig. 7, our strategy for evaluation of a local search can be interpreted as a combination of the weighted scalar approach and the Pareto ranking approach: we are using the weighted scalar approach to determine the success of a local search with the movable area shifted by impreq. And we are adopting the Pareto ranking approach to determine failure of a local search.It should be also noted that one major difference between the approach of Ishibuchi and Narukawa [13] and the present one is that we are using the zones in the objective space just for counting the numbers of success and failure of local search runs in order to dynamically adjust adaptive parameters. Regardless of success or failure of a local search, all the solutions newly generated during the local search are added to R, the combined population set, for competition to next generation. In previous works [13,22] only accepted final solutions join the process for selection of parents for the next generation. Our approach allows good quality solutions newly generated during the local search to have a chance to make a contribution to the optimization, because even if byproduct solutions of a local search are worse than the final local search solution xopt in terms of the composite objective function, they may still be competitive in R.In addition to the local effectiveness of a local search operator, a global effectiveness needs to be considered for a robust adaptive local search. This is to make an adaptation more conservative even when the local search is locally effective (Nsuccess>Nfailure). The global effectiveness can be evaluated by checking the number of local search solutions in the nondominated solutions, which can be considered as final solution outputs of the optimization.The basic idea behind the global effectiveness is that the number ratio of local search solutions to evolutionary search solutions in the set of nondominated solutions should be higher than the computational cost ratio spent for the local search and evolutionary search. If it is not the case, we can tell that the local search is not working properly in a global perspective even if it is locally effective. A constraint for the global effectiveness can be written as(14)Bls≥C¯ls×pls,where Blsis the ratio of the number of local search solutions to the number of evolutionary search solutions contained in the set of nondominated solutions as defined in Table 1. For EMOAs with external archive for non-dominated solutions such as MOGAS [49], Blsis the defined in the external archive. For NSGA-II, Blsis the defined for the nondominated solutions before the selection process.Another constraint related to the global effectiveness is an upper limit constraint on Blsitself. It is based on empirical observations that at least half of the nondominated solution set needs to be filled with solutions by evolutionary operators to prevent genetic shift and premature convergence. Thus, an additional constraint is introduced:(15)Bls≤1.0The two constraints for the global effectiveness in Eqs. (14) and (15) can be put together as follows:(16)C¯ls×pls≤Bls≤1.0The neighborhood radius r and local search probability plsare selected as adaptation parameters in this study. Specific adjustment method for each parameter is presented in the following subsections. The adaptation methods are based on local and global effectiveness of a local search at each generation. If the local search is globally effective in a generation, an adaptation parameter is adjusted (increased or decreased) based on the local effectiveness. Otherwise, the extent of application of a local search is reduced because the local search solutions are not good enough to remain in the nondominated solution set or the local search is too prevailing. The detailed algorithm of the present adaptation strategy for r and plsis presented in Fig. 9. Performance indices and parameters for the adaptive local search strategy are listed in Table 1. The adaptive strategy can be combined with any EMOAs and local search methods.If any of the global effectiveness constraints in Eq. (16) is violated, r is reduced by Δr. If both the constraints in Eq. (16) are satisfied, r is adjusted based on the difference between Nsuccessand Nfailureas follows:(17)r≔r+Δr(Nsuccess−Nfailure)If any of the global effectiveness constraints in Eq. (16) is violated, plsis reduced by 10%. Otherwise plsis adjusted based on Nsuccess−Nfailureas follows:(18)pls≔pls+Δpls(Nsuccess−Nfailure)As test functions for validation of the proposed algorithms, the ZDT [38] functions and DTLZ functions [39] with real decision variables and two or three objectives are adopted, for they have been widely used for evaluating performance of EMOAs. The number of decision variables Ndvis set as 10 for multimodal functions such as ZDT4, DTLZ1, and DTLZ3 functions and 100 for all other test functions.For the baseline multiobjective evolutionary algorithm, we use the SBX recombination operator with ηc=15 and pc=1, and the polynomial mutation operator with ηm=20 and pm=1/Ndv. The population size and number of generations are set to be 100 and 200, respectively, so that the total number of function evaluation is 20,000. For the hybrid method, the same population size and maximum number of evaluations are imposed.There are many performance metrics for measuring convergence and diversity of Pareto-front solutions obtained by multiobjective optimization algorithms. The hypervolume indicator measures the hypervolume in the objective space that is weakly dominated by an approximate set. Here we use the hyp_ind program which is contained in the performance assessment tools of PISA [40]. The hyp_ind calculates the hypervolume difference indicator, IH−, the difference in hypervolume between a reference set R and an approximate set A under consideration, and therefore the smaller is the better. The reference set, R, is composed of non-dominated solutions from the set of pooled Pareto solutions from multiple runs of all the different algorithms. fmin is defined as (f1min,f2min,…fMmin), wherefmminmeans the lower bound of the mth objective function for all solutions in R.fmax=(f1max,f2max,…,fMmax)is defined similarly as the upper bound the mth objective for all solutions in R. fmin and fmax are used to normalize objective function values of the solutions in R and A. fmax is set as the reference point for the hypervolume indicator.As another metric for convergence and diversity, the Inverted Generational Distance (IGD) [41] is adopted. The IGD measures both convergence and diversity of solutions on the Pareto front.(19)IGD=∑Nri=1diNr,where diis the minimum Euclidian distance in the objective function space between a reference solution i and nondominated solutions, and Nris the number of reference solutions, which is set as 500 for two-objective test functions and 990 for three-objective test functions. The reference solutions are uniformly distributed on the Pareto-optimal front.In order to evaluate the performances of stochastic algorithms, each algorithm is run 30 times. For statistical tests, the Kruskal Wallis test [42] is adopted with a confidence level of 95% (i.e., p value below 0.05). We use a program for the Kruskal Wallis test included in PISA for conducting a nonparametric test for difference between multiple independent samples.In this section, we shall compare performances of NSGA-II and hybrid NSGA-II–eLS method using different neighborhood radii, r=0.1, 0.3 or 1.0 and adaptive r. Since all the decision variables are normalized into a domain of [0,1] in the present study, r of 1.0 means that there is no constrained radius for S. For adaptation of r, the initial value of r is set as 1.0, and Δr is 0.1. The range of r in the adaptation were set as [0.1, 1.0] as also shown in Table 1. The lower limit of r, or rmin, was selected so that |S2|>0 in the mid and final stages of the optimization process in the test problems of the present study. In general cases, a smaller rmin can be selected (rmin=0.01 for instance), and when |S2|=0, r can be gradually increased until |S2|>0. If no local search is conducted due to the absence of neighborhood solutions, r cannot bounce back when needed. The local search probability plsis kept constant and set as 5%.Fig. 10(a) compares trend of the search distance |x0−xc| averaged at each generation for different neighborhood radii. It is noted that the search distance decreases as the optimization converges, which shows the adaptive nature of the search distance of eLS. It can be also noted that for smaller radii the local search operator is not conducted in the early stage of the optimization process. This is due to the fact that it gets harder to find neighborhood solutions as radius decreases in the early stage of the optimization, which is clearly shown in Fig. 10(b), where trends of the averaged size of |S2| for constant and adaptive r are compared. |S2| gets smaller for smaller radii especially in the early stage of optimization process.Box plots for IH− and IGD are depicted in Fig. 11for the ZDT test functions. A box plot represents minimum, first quartile, third quartile and maximum values of multiple runs. A dot in the box indicates second quartile or a median value. In overall, the IGD metric shows very similar trends to IH− for all the results presented in this paper. For unimodal ZDT problems such as ZDT1, 2, 3 and 6, the performance of the hybrid method gets better as the neighborhood radius increases from 0.1 to 1. For ZDT4, which is multimodal, there is no drastic performance difference among the algorithms and neighborhood radii tested, which means the local search is not effective for the highly multimodal problem and pls=5% is low enough not to degrade performance of the hybrid EMOA. It is noted that r=0.1 gives the best results for ZDT4.Table 2shows the number of algorithms that are better than each algorithm in a statistically significant way in terms of IH−. The statistical test for the IGD metric was not conducted because IGD behaves very similarly to IH− for all the results in this paper. Overall, the hybrid method with radius r=1.0 shows the best performance. The case with adaptive r is the second best, and r=0.3 is the third. The fact that the r=1.0 case is the best means conducting the local search in the early stage of the optimization process with a large search distance is advantageous for better performances in the unimodal problems.Typical trends of r-adaptation and corresponding Blsvalue are shown in Fig. 12. The missing parts in the curve of Blsin Fig. 12 and other figures mean that the value of Blsis 0 there. For ZDT1, r remains the maximum value except a couple of dips to rmin, and Blsremains around 0.1 for majority of the optimization generations. Meanwhile, for the ZDT4 function r rapidly drops to rmin from the initial value of 1.0 and Blsremains very low around 0.02–0.03 except the first one third of the optimization process due to its severe multimodality.Box plots for IH− and IGD are depicted in Fig. 13for the multimodal DTLZ test functions such as DTLZ1 and DTLZ3. Table 3shows the number of algorithms that are better than each algorithm in a statistically significant way. The hybrid methods show similar performance with the baseline EMOA for the DTLZ1 function and perform better than the baseline EMOA for the DTLZ3 function. Although it is generally expected that a local search would not be beneficial for multimodal problems, the hybrid method with small plsgives better results than the baseline EMOA does for the DTLZ3 problem. The adaptation history of r is shown in Fig. 14. The value of r drops rapidly at the beginning of the optimization process to rmin. The Blsvalue remains so small that it does not satisfy the constraint in Eq. (14) for most part of the optimization process. In this case, Blsneeds to be larger than 0.1 to satisfy the global effectiveness constraint of Eq. (14), since pls=5%.Box plots for IH− and IGD are depicted in Fig. 15for the unimodal DTLZ test functions such as DTLZ2 and DTLZ4. For the unimodal problems, the performance of the hybrid method gets better as r increases. Table 4shows the number of algorithms that are better than each algorithm in a statistically significant way in terms of IH−. Overall, r=1.0 gives the best performance and results in a drastic improvement from the baseline EMOA, and the case of adaptive r shows similar performance with the r=1.0 case. The adaptation trends in Fig. 16shows r remains at rmax for the first 1/3 of the process and drops to rmin as Blsgets reduced.

@&#CONCLUSIONS@&#
