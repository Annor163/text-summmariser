@&#MAIN-TITLE@&#
Improved accelerated PSO algorithm for mechanical engineering optimization problems

@&#HIGHLIGHTS@&#
A new improved accelerated particle swarm optimization algorithm is proposed (IAPSO).Individual particles memories are incorporated in order to increase swarm diversity.Balance between exploration and exploitation is controlled through two selected functions.IAPSO outperforms several recent meta-heuristic algorithms, in terms of accuracy and convergence speed.New optimal solutions, for some benchmark engineering problems, are obtained.

@&#KEYPHRASES@&#
Meta-heuristic,Particle swarm optimization,Diversity,Memory,Engineering problems,

@&#ABSTRACT@&#
This paper introduces an improved accelerated particle swarm optimization algorithm (IAPSO) to solve constrained nonlinear optimization problems with various types of design variables. The main improvements of the original algorithm are the incorporation of the individual particles memories, in order to increase swarm diversity, and the introduction of two selected functions to control balance between exploration and exploitation, during search process. These modifications are used to update particles positions of the swarm. Performance of the proposed algorithm is illustrated through six benchmark mechanical engineering design optimization problems. Comparison of obtained computation results with those of several recent meta-heuristic algorithms shows the superiority of the IAPSO in terms of accuracy and convergence speed.

@&#INTRODUCTION@&#
A large number of optimization problems out coming from numerous engineering and scientific fields are generally formulated as nonlinear constrained optimization problems. Frequently, the objective function and constraints are neither smooth nor continuous. Hence, deterministic methods such as the steepest descent method, the Newton method and the quasi Newton method, are inefficient, inaccurate and unstable when used to solve these complex optimization problems. During the last decades, a huge number of meta-heuristic algorithms have been proposed in order to overcome these defects. Most of these algorithms are inspired form physical and natural phenomena and based on a combination of several rules and randomness. Such algorithms are simulated annealing (SA) algorithm [1], genetic algorithm (GA) [2,3], particle swarm optimization (PSO) [4,5], ant colony optimization (ACO) [6], harmony search (HS) [7], artificial bee colony (ABC) [8], firefly algorithm (FA) [9], etc. and more recently league championship algorithm (LCA) [10], water cycle algorithm (WCA) [11] and mine blast algorithm (MBA) [12].Among all these meta-heuristics, PSO algorithm has attracted scientific researchers due to its efficiency to solve complex optimization problems, arising in various fields, and ease of implementation. PSO algorithm was for the first time introduced by Eberhart and Kennedy [4,5] and presented as a population based algorithm which mimics the behavior of a swarm of birds or a school of fish. In PSO algorithm, each member of the swarm, called particle, is characterized by a position and a velocity vectors. A particle adjusts, during the optimization process, its velocity with respect to its own experience and the experience of the whole swarm. The particle position is then updated according to its previous position and its new velocity. However, despite its advantages, in some complex and intricate optimization problems, PSO can be trapped in local optima and can in a later period of the iterative process have a weak convergence rate. In order to overcome these limitations, a great effort of scientific researchers has therefore been devoted to the development of diverse approaches and strategies to improve the PSO algorithm. These based PSO algorithms have therefore been applied on diverse scientific and engineering problems. Early modifications of the original PSO, are conducted by its own creators and their collaborators, leading to two principal models which are the inertia weight model [13,14] and the constriction factor model [15]. The main goal of both approaches is to balance exploration and exploitation, through searching process, and then improving convergence rate and solution quality.Next, several other based PSO algorithms have been developed and applied on benchmark and real life optimization problems. Angel et al. [16] proposed a modified PSO algorithm, called particle evolutionary swarm optimization (PESO). Two perturbation operators are successively added to the standard PSO in order to prevent premature convergence and improving diversity. He and Wang [17] presented a co-evolutionary PSO, named CPSO, for solving constrained engineering optimization problems. Two kinds of swarms are employed and evolved simultaneously using the standard PSO algorithm. The first kind of multiple swarms is used to evolve decision solutions, while the second one of single swarm is used to adapt penalty factors for solution evaluation. A modified co-evolutionary particle swarm optimization using Gaussian distribution (CPSO–GD) was developed by Krohling and Coelho [18]. The authors suggested a Gaussian probability distribution to generate the accelerating coefficients of the PSO with constriction factor [15]. Aguirre et al. [19] extended the PESO algorithm [16] by introducing a new ring neighborhood structure and a technique based on feasibility and sum of constraints violation to handle constraints. The resulting algorithm is named: constrained optimization via PSO algorithm: COPSO. He and Wang [20] presented a hybrid PSO algorithm (HPSO) with a feasibility-based rule to solve constrained optimization problem. In this work, the PSO algorithm is hybridized with the simulated annealing algorithm (SA) to avoid premature convergence. Worasucheep [21] introduced a constrained particle swarm optimization with stagnation detection and dispersion (PSO-DD) based on the standard PSO and using some techniques to handle stagnation and constraints. Cagnina et al. [22] used the standard PSO algorithm with a simple technique to handle constrains. The algorithm is called: simple constrained PSO (SiC-PSO). Wang and Yin [23] proposed a ranking selection-based PSO (RSPSO) to solve engineering design problems with mixed variables. The authors have integrated a ranking selection scheme into the standard PSO to control the search behavior of a swarm in different search phases and on categorical variables. Yildiz [24] developed a hybrid approach based on PSO and receptor editing property of immune system (PSRE). The authors of [25] presented a modified particle swarm optimization (MPSO) which included the heuristic crossover derived from genetic algorithm to compete with standard PSO in order to improve local search. Zahara and Kao [26] hybridized PSO with a well-known local search namely Nelder–Mead Simplex method and applied it for solving constrained optimization problems. This hybrid version was named as NM-PSO. Liu et al. [27] proposed a hybrid algorithm called PSO-DE, which incorporates standard PSO with differential evolution (DE) to solve constrained numerical and engineering optimization problems. The use of DE is mainly to fight stagnation problem. Coelho [28] introduced an improved quantum-behaved PSO (QPSO) approaches using mutation operator with Gaussian probability distribution. The algorithm is called G-QPSO. Gaussian mutation operator is used instead of random sequences in QPSO in order to prevent premature convergence to local optima. For solving real life engineering problems, recently, Davoodi et al. [29] derived a hybrid algorithm combining improved quantum-behaved particle swarm optimization (IQPSO) with simplex algorithms: (IQPSOS) for solving load flow problems. The simplex method is used as a local search to fine tune the obtained solution from QPSO. Similarly, in [30], PSO and DE are used for the parametric identification of seismic isolators. A complete survey of the state of the art in particle swarm optimization can be found in [31–33].All the above developed variants of the PSO algorithm propose to modify acceleration coefficients, add some operators or combine other existing algorithms with PSO to produce new hybrid algorithms. The goal of these variants is mainly to improve diversity, prevent premature convergence and increase convergence rate. However, the majority of these algorithms is time-consuming, due to integration of complex procedures, and need a large number of iterations to reach the optimum or a near-optimum solution. In order to reduce computation effort and improve the convergence of the PSO, Yang [34] has proposed a simplified based PSO model called accelerated particle swarm optimization (APSO). The main modification is the removal of particles velocities vectors from the standard PSO model, hence particles of the swarm are characterized only by their position vectors. In addition, Yang has proposed to replace the contribution of particles personal best positions, which are used to increase diversity, by some randomness. However, despite the successful test of this algorithm on some benchmark functions and its high convergence speed, the APSO suffers from premature convergence and weak diversity, particularly when handling highly nonlinear optimization problems. Motivated by these disadvantages, in the present study we propose some modifications on the APSO in order to alleviate the problem of premature convergence and to improve swarm diversity. These modifications are the incorporation of the individual particles’ memories, in order to increase swarm diversity, and the introduction of two selected functions to control balance between exploration and exploitation during search process. The resulting algorithm is called improved accelerated particle swarm optimization (IAPSO).The remainder of this paper is organized as follows. Section 2, presents a brief review of the standard PSO algorithm. In Section 3, the APSO [34] is presented and discussed. The proposed IAPSO algorithm and related formulations are detailed in Section 4. Section 5 of the paper covers performance evaluation of the proposed algorithm through six benchmark mechanical engineering optimization problems. The last section provides a clear conclusion of the study.Particle swarm optimization is a population based algorithm developed by Eberhart and Kennedy in 1995 [4,5]. The PSO algorithm is inspired by social behavior of bird flocking, or fish schooling. Meanwhile, it gained popularity among scientific researches thanks to its ability to quickly converge to a reasonably good solution and its simplicity of implementation.Mathematically, PSO can be described as follows: consider a search space D∈ℜdand f an objective function. d is the number of design variables. As mentioned above, PSO is a population-based algorithm. The population is called the swarm and its individuals are called the particles. The swarm is defined as a set X={x1, x2, …, xN}Tof N particles. Each particle is assigned a position vector, defined as xi={xi1, xi2, …, xid}∈D, i=1, 2, …, N, and a velocity vector denoted asvi={vi1,vi2,…,vid}.In order to allow particles visiting all search space, each particle, updates its velocity according to the memory gained so far. In fact, PSO maintains a memory set P={p1, p2, …, pN}T, where piis the best position ever visited by the ith particle. In addition, as PSO is based on information exchange allowing particles to mutually communicate their experiences, the best position ever visited by all particles is shared. Let g be the index of the global best position in P then the global best position is defined aspg=argmini(pi).At each iteration step t, the particle velocity and position are updated successively using respectively the two following equations:(1)vit+1=vit+αr1(pit−xit)+βr2(pg−xit)(2)xit+1=xit+vit+1where r1 and r2 are two uniformly random numbers independently distributed in the interval [0, 1] which are used to maintain the diversity of the swarm. α and β are respectively the cognitive and the social parameters (or acceleration parameters) which drive particles toward local and global best positions, usually set as α≈β≈2. However, some researchers report that we might even better choose a larger cognitive parameter α than a social parameter β but respecting α+β≤4 [15].The accelerated particle swarm optimization (APSO) is a simplified version of the standard PSO developed by Yang [34]. While PSO algorithm uses both the particle best piand the global best pgto update the velocity vi, as shown in Eq. (1); the APSO algorithm uses only the global best position. According to Yang, this simplification is due to the fact that individual best is used to increase the diversity in the quality solutions and hence, can be replaced simply by some randomness [34]. Consequently, the velocity of the ith particle is updated as follows:(3)vit+1=vit+αε+β(pg−xit)where ɛ is a random vector uniformly distributed in the range [0,1]. The position of the ith particle is updated using Eq. (2). Moreover, in order to increase the convergence even further, the update of the location is simply written in a single step as follows:(4)xit+1=(1−β)xit+βpg+αεTypically, the parameter α=0.1L∼0.5L, where L is the scale of each variable, while β=0.1∼0.7 is sufficient for most applications [35].As can be seen, the APSO is described only by a single equation (Eq. (4)). Thus, there is no need to deal with velocity. In addition, Yang has suggested a further improvement of the APSO by reducing the randomness as iterations proceed. This is done by introducing the parameter α as a monotonically decreasing function such as:(5)α=α0e−γtor(6)α=α0γt,(0<γ<1)where α0=0.5∼1 is the initial value of the randomness parameter and γ is a control parameter.According to [34], in the case of highly nonlinear and multimodal optimization problems, the APSO algorithm suffers from premature convergence due to its weak diversity. Moreover, the APSO algorithm lacks memory since the update of particle position does not consider the memory gained by each particle during the search process. To overcome these disadvantages, three modifications will be substituted to the APSO algorithm.The first modification is to replace the current position xiby the best position piin Eq. (4), providing a memory to the APSO algorithm. We can never underestimate the significance of using local memory. Indeed, using personal bests of all particles in the population, which represent the memory of the swarm, will enhance diversity of the swarm. The particles current positions, in this case, are regarded as a set of explorers whose purpose is the exploration of search space. This will allow particles to explore broadly the whole of the search space, mainly promising sub-regions, and alleviating premature convergence.Similarly, the second change is also included to improve diversity of the swarm. In fact, the random vector ɛ, Eq. (4), is substituted by the line vector Riof the matrix R=[R1R2 ⋯RN]T. The matrix R is a (N×d) normally distributed random matrix computed at each iteration t. Each jth column (j=1, …, d) of R is randomly generated with a mean zero and a standard deviation σj, where σjis the standard deviation of the jth design parameter of all the particles best positions P={p1, p2, …, pN}T. Fig. 1shows schematically the computation of σjfor N particles best positions. The approach of taking the standard deviation for the normal distribution is used in [36]. The goals of using Riis to improve diversity of the swarm and to take part of the collective memory of all particles since it is computed based on particles best positions.The standard APSO uses two constant parameters α and β, Eq. (4), as described in the above section. Whereas, in the present work, we propose as a third modification to use these two parameters as two monotonically functions with respect to the iteration counter t.The function α(t) is used to control particles exploration of the search space. It is considered as a decreasing step function updated during the iterative process repeatedly after each s iterations. The mathematical expression of α(t) is as follows:(7)α(t)=αmax−αmax−αmintmaxtwhere tmax is the maximum iteration number. As can be noted, during the iterative process α(t) decreases between the upper and lower limits αmax and αmin, respectively. Indeed, a large value of α favor global exploration, while a small value tends to enable exploitation. The use of α(t) as a step function allows the diversity of the swarm to be preserved during the s iterations and hence, helps to avoid premature convergence. Furthermore, contrarily to the APSO, where α must be scaled with respect to design parameters, in IAPSO, there is no need to scale it.In the same way, the function β(t) is considered as an increasing function expressed as follows:(8)β(t)=βmin+(βmax−βmin)sinπ2ttmaxwhere βmin and βmax are respectively minimal and maximal values of β(t) at first and last iterations. The function β(t) is employed to control balance between global and local explorations during the search process. In fact, a small value of β gives height impact to personal best pi(i=1, …, N) on the swarm allowing global exploration, whereas, a large value gives more influence of the global best pgon the new particles positions, which favors solutions refinement. It is worth pointing that the function β(t) has been chosen after numerical simulations showing its superiority compared to several other tested functions.Taking into account of the above modifications, mathematically the fundamental equation of the proposed algorithm is as follows:(9)xit+1=(1−β(t))pit+β(t)pgt+α(t)RitAs can be noted, the IAPSO algorithm uses a simple equation (Eq. (9)) to update the current position of each particle in the swarm. The position vector is updated based on the memory gained by each particle as well as the knowledge gained by the swarm as a whole, both are controlled by the function β. In addition, some randomness, generated based on the current particles best positions, is substituted to enhance diversity of the population and also controlled by the function α. Thus, based on the personal memory and the social behavior of the swarm, each particle position is updated accordingly, allowing particle to adapt to its environment by returning to promising regions of the search space previously discovered, and searching for better positions over time.Similar to other stochastic optimization algorithms, the IAPSO is defined for unconstrained problems. However, during search process, particles may violate either the limits of the design variables or the problem specific constraints. For any particle with a position xiexceeding the boundary of the variable range, its position is reset to the upper (or lower) boundary value. On the other hand, to take into account constraints violation in this work, the penalty function method [37] is adopted thanks to its simple principle and easy implementation, especially for continuous constrained problems.Generally, a constrained optimization problem is described as follows:(10)minimizef(x)Subject to:gk(x)≤0,k=1,…,mwhere f is the objective function and gkis the kth inequality constraint.Integration of penalty functions into the objective function will transform the above constrained problem to an unconstrained one. The penalized objective function fpis then written as follows:(11)fp(x)=f(x)+λ∑k=1mδk[gk(x)]2where λ>0 (e.g. λ=1015) is a penalty factor andδk=1ifconstraintgkisviolatedδk=0ifconstraintgkissatisfied.The step-wise procedure for the implementation of the proposed IAPSO algorithm is given as follows:Step 1: Define the optimization problem and initialize the optimization parameters.•The problem is defined as shown in Eq. (10) and transformed as illustrated in Eq. (11), number of design variables (d), and limits of design variables (Ub, Lb).Initialize the population size (N), maximum number of iterations (tmax), the control functions α=αmax, β=βmin and the iteration counter t=0.Step 2: Initialize the population.•Generate a random populationX0={x10,x20,…,xN0}Taccording to the population size and number of design variables. For the IAPSO, initial location of the ith particle is given as:(12)xi0=Lb+rand(Ub−Lb)where rand is a uniformly distributed random number between 0 and 1.Set initial particles best positions as initial population:pi0=xi0for i=1, …, N.Find the global best position pgsuch thatfp(pg)=mini(fp(pi0)).Step 3: Repeat until the stopping criteria are met: (t=tmax).•Update iteration counter (t=t+1), control functions α and β using respectively Eqs. (7) and (8). Calculate Rtas illustrated in Fig. 1.Update particle positionxitusing Eq. (9) and calculate its fitnessfp(xit).Update the best particle positionpitat current iteration (t) and global best particle position pg.In this section, a set of six benchmark mechanical engineering design optimization problems, frequently used by the specialized literature, have been chosen to test the performance of the proposed IAPSO algorithm. The considered benchmark problems include objective functions and constraints of various types and nature (quadratic, cubic, polynomial and nonlinear) with several numbers and types of design variables (continuous, mixed, discrete and integer). The mathematical formulations of the test problems can be consulted in the Appendix A of this paper.The obtained optimization results have been compared with other well-known optimizers particularly the APSO algorithm [34]. It is worth pointing that numerical results obtained by APSO algorithm are carried out during this study. The APSO parameters are tuned according to those advised in [34]. Results were compared in terms of statistical results and number of function evaluations (NFEs). In this paper, the computational cost which is considered as the best NFEs corresponding to the obtained best solution, is calculated by the product of the number of swarm's particles and the maximum number of iterations (i.e. NFEs=N×tmax).The proposed algorithm was coded in MATLAB programming software and the simulations and numerical solutions were run on a T5870 @ 2GHz Intel Core™ 2 Duo process with 4 GB random access memory (RAM). The task of optimizing each mechanical design problems was carried out by using 25 independent runs. The user parameters of IAPSO, for considered engineering optimization problems, are presented in Table 1. These parameters are empirically selected after numerous experiments and fine-tuned using the following guidelines:•The control function β is defined such as βmin≤β≤βmax. Typically βmin≈0.1 to 0.3 and βmax≈0.5 to 0.9. A lower value of β at first iterations allows exploration and then gradually increases during iterations to a higher value for more exploitation. In other word, β controls the influence of the global best particle and personal best on the current solution.The decreasing function α, which controls diversification of the swarm, is bounded such as αmin≤α≤αmax. Typically, αmax≈0.5 to 2 and αmin≈0.2 to 0.6. A large value of α, at first iterations, allows more exploration of the search space and a lower value is used to refine solutions quality.To maintain a constant level of exploration, during optimization process, the parameter s is used. Typically 1≤s≤5.The size of the swarm N and the maximum number of iterations tmax depend on the complexity of the optimization problems. In fact, for highly complex problems, a choice of 20–50 particles and 200–400 iterations may be sufficient. However, for simple to medium complex problem a reasonable choice is to set 10–25 particles and 10–50 iterations.This problem aims to design a welded beam for minimum cost, subject to some constraints [38]. Fig. 2shows the welded beam structure which consists of a beam A and the weld required to hold it to member B. The objective is to find the minimum fabrication cost, considering four design variables: x1, …, x4 and constraints of shear stress τ, bending stress in the beam σ, buckling load on the bar Pc, and end deflection on the beam δ.The algorithms earlier used to optimize this problem include: genetic algorithm (GA) based co-evolution model GA1 [39], GA through the use of dominance-based tour tournament selection GA2 [40], hybrid genetic algorithm (HGA) [41], CPSO-GD [18], HPSO [20], NM–PSO [26], cultural algorithms with evolutionary programming CAEP [42], society and civilization algorithm SC [43], differential evolution DE [44] and more recently LCA [10], WCA [11] and MBA [12].Table 2shows the comparison of the best solutions for previously reported studies and the proposed algorithm in terms of design variables, function values and constraints accuracy. The statistical optimization results for reported algorithms are given in Table 3. From Table 3, the proposed algorithm stably detects the best known solution recently obtained by LCA and strongly dominates the standard APSO algorithm [34]. Moreover, in terms of statistical results, IAPSO offered better results using fewer number of function evaluations (NFEs) compared to all other considered algorithms. However, the best solution is obtained with a standard deviation value (SD) greater than that given by LCA. It is worth pointing that the best solution obtained by NM-PSO violates the third constraint and then cannot be compared to other best solutions.Fig. 3illustrates the function values with respect to the number of iterations for the welded beam design problem. Note that the proposed IAPSO shows a good converging behavior thanks to the efficiency of the searching model and the perfect balance between exploration and exploitation during optimization process.As a second example, the design optimization of the cylindrical pressure vessel capped at both ends by hemispherical heads (Fig. 4) is considered [45,46]. The vessel will be designed for a working pressure of 3000psi and a minimum volume of 750ft3 regarding the provisions of ASME boiler and pressure vessel code. The objective is to minimize the total manufacturing cost of the vessel including materials, forming and welding costs. There are four design variables: thickness of the shell (x1), thickness of the head (x2), the inner radius (x3), and the length of the cylindrical section of the vessel (x4). The variables x1 and x2 are discrete values which are integer multiples of 0.0625 and x3 and x4 are continuous values.To solve this mixed nonlinear problem we round the value of x1 and x2 to their integer part and multiply them by 0.0625. The proposed IAPSO algorithm was applied to the pressure vessel optimization problem and the optimal results were compared to earlier methods as shown in Table 4. This problem has been solved previously using GA1, GA2, CPSO, HPSO, NM–PSO, G-QPSO, QPSO, PSO-DE, co-evolutionary differential evolution CDE [47], LCA, WCA and MBA and compared with the proposed IAPSO as given in Table 5.As can be seen from Table 4, results indicate that IAPSO is superior to other optimizers in term of the best solution. Furthermore, the proposed algorithm is the most stable algorithm reporting always the smallest value of the standard deviation (SD), the most dependable algorithm in terms of the mean and the worst performance as shown in Table 5. Moreover, IAPSO converge to the optimal solution with the lowest NFEs compared to all other methods. It is worth pointing that solutions obtained respectively by NM-PSO, WCA and MBA are infeasible (see Table 4) since the design variables x1 and x2 have not discrete values. Furthermore, the best solutions reported by NM-PSO and HPSO violate constraints g1, g2 and g3 respectively. To conclude, we can say that IAPSO is the most efficient algorithm for optimizing the design features of a pressure vessel. Fig. 5illustrates the function values with respect to the number of iterations for the pressure vessel design optimization problem. As before, IAPSO shows a good converging behavior, thanks to the efficiency of search model, despite the infeasibility of the initial population.The weight of the speed reducer [48] as shown in Fig. 6is to be minimized subject to constraints on bending stress of the gear teeth, surfaces stress, transverse deflections of the shafts and stresses in the shafts. The design variables (x1, …, x7) are respectively the face width, module of teeth, number of teeth in the pinion, length of the first shaft between bearings, length of the second shaft between bearings and the diameter of the first and second shafts. The third variable is integer, the rest of them are continuous. This is an example of a mixed integer optimization problem. The mathematical formulation of this complex problem [49] having 11 constraints makes the search of feasible solutions very hard. Again two cases of the problem are investigated where the only difference between them is in the allowable range for variable x5.The speed reducer problem (case 1) previously was optimized using COPSO, SiC-PSO, LCA, modified bacterial foraging optimization algorithm MBFOA [50], methods based on differential evolution strategies DES [51] and the simple evolutionary algorithm SES [52]. The statistical results of these algorithms were compared with the proposed IAPSO and are given in Table 6. As we can see, IAPSO algorithm exhibits the best performance among all rivals. Indeed, after only 6000 function evaluations, IAPSO stably converge to a new optimal solution better than all known solutions so far, which is:x*=(3.49999999999760, 0.7, 17, 7.3, 7.8, 3.35021466609630, 5.28668322975692) and f(x*)=2996.34816496772, with the lowest standard deviation value compared to those of other optimizers. Moreover, during simulation, despite the IAPSO starts with initial infeasible solutions, as shown in Fig. 7, it converges rapidly to the neighborhood of the optimal solution, thanks to its effectiveness and the perfect tuning of the exploration and exploitation parameters.Similarly, the speed reducer problem (case 2) previously was optimized using ABC, LCA, WCA, MBA, PSO–DE, SC, differential evolution with level comparison DELC [53], differential evolution with dynamic stochastic selection DEDS [54], hybrid evolutionary algorithm and adaptive constraint handling technique HEAA [55], (μ+λ)-ES [56], modified differential evolution (MDE) [57,58]. The comparison of the best solutions of reported methods is presented in Table 7. Note that IAPSO converge once again to a new solution better than all known solutions so far. Moreover, the IAPSO outperformed other considered optimization engines as shown in Table 8. In fact, in terms of computation effort and stability, it reaches the best solution faster than other reported algorithms using only 6000 function evaluations with a very lower standard deviation value. Fig. 8depicts the reduction of function values versus the number of iterations for speed reducer design problem (case 2), where IAPSO shows the same convergence behavior as the previous case.The weight of a tension/compression spring [39,59], as shown in Fig. 9, is to be minimized subject to constraints on minimum deflection, shear stress, surge frequency, limits on outside diameter and on design variables (see Appendix A). The design variables x1, x2, and x3 are respectively the wire diameter, the mean coil diameter and the number of active coils.This problem was solved by several researchers using different optimization algorithms such as GA1, GA2, CAEP, CPSO, HPSO, NM–PSO, G-QPSO, QPSO, PSO–DE, PSO, DELC, DEDS, HEAA, SC, DE, ABC, and (μ+λ)-ES, etc. Table 9shows the comparison for the best solution obtained by the proposed IAPSO and above cited algorithms. In terms of solution quality, it is clear that the best solution obtained by IAPSO is similar to the best-known solution obtained by the LCA. It is worth pointing that the best solution reported by NM-PSO is infeasible since the first two constraints are violated. Moreover, as displayed in Table 10, the proposed IAPSO converge to the best solution with much less computing effort than LCA method and a satisfactory standard deviation value. Fig. 10depicts the reduction of function values versus the number of iterations for tension/compression spring design problem. Note that during numerical simulation, in early iterations, all particles positions were in the infeasible region, however, after only fewer iterations, particles are directed to the feasible region and the optimal solution is reached. This may be owing to the effectiveness of the searching criteria and exploration ability of the IAPSO to locate promoting regions of problem domain.Gear train design problem [45] aims to minimize the cost of the gear ratio of the gear train as shown in Fig. 11. The decision variables of the problem are x1, x2, x3 and x4 which are respectively the number of teeth for gear A, B, D and F. The constraints are only limits on design variables (side constraints). Design variables to be optimized are in integer form since each gear has to have an integer number of teeth. It is well known that constrained problems with discrete variables may increase the complexity of the problem. For the present problem, the lower and upper bounds of integer design variables are 12 and 60, respectively. The mathematical formulation of the problem is reported in Appendix A.Table 11shows the comparison for the best solution of cuckoo search (CS) algorithm [60], the unified particle swarm optimization (UPSO) algorithm [61] and MBA in terms of the value of design variables and function value. As can be seen, the IAPSO algorithm converges to the best known solution similar to the three other optimization techniques. Statistical results illustrated in Table 12show that IAPSO exceeded all optimizers with respect to the computation effort (NFEs) and displays once again a stable behavior by reporting a very low standard deviation value. Fig. 12demonstrates the reduction of function values with respect to the number of iterations. Whereas, IAPSO algorithm starts with infeasible solutions, the particles were able to quickly reach the optimal solution thanks to the efficiency of the searching criteria and the balance between exploration and exploitation.The mass of a multiple disk clutch brake [62], as shown in Fig. 13, is to be minimized. The design variables x1, …, x5 are respectively inner radius, outer radius, thickness of the disk, actuating force and number of friction surfaces. All design variables are discrete and restricted to take the following values:x1=60,61,…,80;x2=90,91,…,110;x3=1,1.5,…,3;x4=600,610,…,1000andx5=2,3,…,9.This discrete optimization problem (see Appendix A) previously was optimized using ABC, WCA, NSGA-II [63] and TLBO [64]. The comparison for the best solution and the statistical optimization results, given by such algorithms, are presented in Tables 13 and 14, respectively. As it can be seen from Table 13, IAPSO, TLBO and WCA algorithms converge to the same optimal solution. However, in term of statistical results, the IAPSO shows superiority compared with other optimization algorithms in terms of computation effort (NFEs) and stability (SD). Fig. 14depicts the reduction of function values versus the number of iterations for the multiple disk clutch brake design problem. It is worth noting that after only a few iterations, the population was converged to the neighborhood of the optimal solution and due to the searching criteria, particles are prompt efficiently conducted to reach the optimal solution.

@&#CONCLUSIONS@&#
