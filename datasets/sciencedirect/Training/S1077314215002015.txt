@&#MAIN-TITLE@&#
Fixed-point Gaussian Mixture Model for analysis-friendly surveillance video coding

@&#HIGHLIGHTS@&#
The fGMM is mathematically derived to eliminate the floating-point calculations.A division simulation algorithm is proposed to replace the division operations.The fGMM is implemented in FPGA to process HD videos in real-time.

@&#KEYPHRASES@&#
Surveillance video,Video coding,Analysis-friendly,Background modeling,Fixed-point Gaussian Mixture Model,

@&#ABSTRACT@&#
With the recent explosion in the use of video surveillance in security, social and industrial applications, it is highly desired to develop “smart” cameras which are capable of not only supporting high-efficiency surveillance video coding but also facilitating some content analysis tasks such as moving object detection. Usually, background modeling is one of fundamental pre-processing steps in many surveillance video coding and analysis tasks. Among various background models, Gaussian Mixture Model (GMM) is considered as one of the best parametric modeling methods for both video coding and analysis tasks. However, a number of floating-point calculations and division operations largely limit its application in the hardware implementation (e.g., FPGA, SOC). To address this problem, this paper proposes a fixed-point Gaussian Mixture Model (fGMM), which can be used in the hardware implementation of the analysis-friendly surveillance video codec in smart cameras. In this paper, we first mathematically derive a fixed-point formulation of GMMs by introducing several integer variables to replace the corresponding float ones in GMM so as to eliminate the floating-point calculations, and then present a division simulation algorithm and an approximate calculation to replace the division operations. Extensive experiments on the PKU-SVD-A dataset show that fGMM can achieve comparable performance with the float GMM on both surveillance video coding and object detection tasks, and outperforms several state-of-the-art methods remarkably. We also implemented fGMM in FPGA. The result shows that the FPGA implementation of our fGMM can process HD videos in real-time, just requiring 140 MHz user logic and 622 MHz DDR3 memory with 64-bit data bus.

@&#INTRODUCTION@&#
In recent years, surveillance cameras have been widely deployed and video surveillance systems have reached every corner of our modern society. For example, more than 5.9 million surveillance cameras have been deployed in UK; while there are up to more than 30 million surveillance cameras in China. Obviously, with the explosion in the use of video surveillance, it is highly deserved to develop “smart” cameras which, in addition to image capture circuitry, are capable of extracting application-specific information from the captured images [1]. Such smart cameras can be used in a wide range of security, social and industrial applications, including unattended surveillance [1], in-home nanny monitoring [2], road surveillance and traffic control [3], robot guidance and vehicle control [4].Usually, existing surveillance systems adopt the common video codecs such as H.264/AVC with general settings to compress the captured videos for weeks or months. Some systems even set the compression rate to 300:1 or higher to further reduce network bandwidth and storage capacity. On the other hand, these systems often conduct the video analysis task (e.g., object detection) on the whole frames after decoding the bit-stream from cameras. The better the quality of the frames, the higher the performance of video analysis and search might achieve. Thus a high compression ratio may inevitably influence the accuracy of video analysis. To partially solve this dilemma, as pointed out by [2], the video codec in a “smart” video surveillance system should be analysis-friendly (as Fig. 1shows), which can not only support high-efficiency surveillance video coding but also facilitates some content analysis tasks such as object detection.To achieve high-efficiency coding performance, a background-model-based coding method was proposed in [5,6] for surveillance videos, in which the background pictures are modeled from the original input frames so as to provide better reference for encoding the following frames, and these background pictures can be encoded into the bitstream as non-display frames to guarantee the decoding match. Experimental results showed that this method could obtain a remarkable compression gain on surveillance videos over H.264/AVC High Profile. It should be noted that this background-model-based coding technology has been adopted in IEEE 1857-S (AVS) and is expected to be implemented in hardware codecs for different video surveillance applications [2]. Meanwhile, background modeling is also one of fundamental pre-processing steps in surveillance video analysis (e.g., [3,7–10]). Many practical evidences have validated that a clean background can be very helpful for analysis tasks such as foreground segmentation and object detection. Therefore, to develop such a smart video surveillance system with the analysis-friendly video codec, it is crucial to design a background modeling method suitable for both video coding and content analysis.Basically, background modeling in surveillance video coding should be done with low computational complexity and low memory cost. Till to now, two background modeling methods have been proposed in the literature for surveillance video coding. In [11], Zhang et al. proposed a Segment-and-Weight based Running Average (SWRA) method to approximately calculate the background by assigning a larger weight on the frequently-appearing values in the averaging process. As one of low-complexity background modeling methods, it can offer rather good reference for video coding, and more importantly, can be easily implemented in the hardware codec. In [12], Paul et al. proposed to embed the Gaussian Mixture Model (GMM) background generation procedure and the background prediction into H.264/AVC. Compared with SWRA, GMM is computationally complex since it has many floating-point calculations and division operations.For video analysis, there are many methods in the literature [9], e.g., Frame Differencing, Temporal Median Filtering, Gaussian Mixture Models (GMMs), Bayesian, Kernel Density Estimation (KDE), Codebook methods. Among them, GMM [13–15] is considered as one of the best parametric background modeling methods, due to its good performance and high robustness over different scenes. Recently, many new background modeling algorithms, such as ViBe [16], PBAS [17], SOBS [18], Eigenbackground [19] and Neural-fuzzy model based approach [20], have been proposed. Totally speaking, they show promising performance on several video analysis tasks, and mostly even outperform GMMs. However, ViBe and PBAS are not able to generate backgrounds and thus cannot be embedded into the codec so as to provide prediction reference for video coding, while SOBS and Eigenbackground need massive calculations and thus are difficult to implement in the codecs with low computational complexity. On the other hand, as a variant of the simple running average method, SWRA [11] is not analysis-friendly. It cannot model dynamic background effectively and thus does not satisfy the needs for many video analysis tasks. Thus to the best of our knowledge, GMM should be the best background modeling method for both video coding and video analysis that can be found in the literature till to now.However, the floating-point calculations and division operations in GMM still present a significant obstacle for its wide application in the hardware implementation of video codecs. For example, if we want to empower the cameras with the analysis-friendly video codec, we need to implement the GMM in FPGA and even SOC. To solve this problem, some recent works (e.g., 21–24) proposed several hardware implementation methods of GMM. Among them, [21] is the newest one and it can process the HD video in real-time. Totally speaking, all these methods adopt the similar strategy by using data conversion to accelerate the processing speed (i.e., using the fixed number of bits to represent the integer and fractional parts). Obviously, this strategy is not an optimization of the GMM model itself. What is more, due to the approximate representation of the fractional parts in the model, the quality of the constructed background will decrease inevitably.In this paper, we propose a fixed-point Gaussian Mixture Model (fGMM) method for analysis-friendly surveillance video coding. Different from previous works, the proposed fGMM eliminates the floating-point calculations and division operations while being capable of achieving comparable performance. Towards this end, we mathematically derive a fixed-point formulation of GMMs by introducing several integer variables to replace the corresponding float ones in GMM so as to eliminate the float-point calculations. After this, we adopt a division simulation algorithm and an approximate calculation to replace the division operations. Experimental results show that this approximate calculation has little influence on the quality of the modeled background pictures. In addition, our analysis also shows that fGMM saves 46% memory cost compared with its float version.Extensive experiments were performed on the PKU-SVD-A dataset11http://mlg.idm.pku.edu.cn/resources/pku-svd-a.html. This dataset consists of more than 10 videos with different resolutions (ranging from SD, 720p, 1600 × 1200, and 1920 × 1080) and is online publically available. Experimental results show that both in surveillance video coding and object detection tasks, the proposed fGMM can achieve comparable performance with the float GMM, and also outperforms several state-of-the-art methods (e.g., [11,21,24]) remarkably. We also implemented our fGMM in FPGA. The result shows that fGMM has lower hardware requirements (e.g., user logic, memory) to process HD video in real-time, compared with [21].Our main contributions can be summarized as follows:(1)A fixed-point formulation of GMMs is mathematically derived by introducing several integer variables to replace the corresponding float ones in GMM so as to eliminate the floating-point calculations. Different from other data conversion methods, the proposed fGMM does not introduce any approximate presentation to convert float into integer.A division simulation algorithm and an approximate calculation are further proposed for fGMM to replace the division operations. As a result, fGMM can complete all the calculations using integers to achieve comparable performance with the float version of GMM.Without floating-point calculations and division operations, fGMM can be easily implemented in hardware devices. In our work, fGMM is implemented in FPGA to process HD videos in real-time, just requiring 140 MHz user logic and 622 MHz DDR3 memory with 64-bit data bus.The remainder of this paper is organized as follows: Section 2 briefly reviews the Gaussian Mixture Model. The proposed fGMM is presented in Section 3. Experimental results are shown in Section 4. Section 5 mainly presents the FPGA implementation of fGMM and Section 6 concludes this paper.Gaussian Mixture Model (GMM), firstly proposed by Stauffer and Grimson [13], is a widely-used parametric background modeling method. Its basic idea is to use several Gaussian distributions to describe a pixel, some of which represent the background in the scene while the others characterize the foreground. To facilitate the online model learning, KaewTraKulPong and Bowden [14] proposed to use Expectation Maximization (EM) to update the parameters. Thus this section briefly reviews the GMM in [14] and then discuss the possible problems when used in hardware implementation.GMM models each pixel by a mixture of K Gaussian distributions. For each Gaussian distribution, there is a weight parameter w representing the time proportion this pixel stays. The greater the value of w is, the longer this pixel stays in this distribution. The Gaussian function is shown as follows:(1)f(x)=1σ2πe−(x−μ)22σ2,where σ is the standard deviation. A smaller value of σ means a more stable distribution. The background pixel value tends to be the one which stays the longest and keeps static in the video. Therefore, static background tends to form tight clusters in the Gaussian distributions while moving ones form widen clusters. The measurement of this widen or tight cluster is called the fitness valueϕkN. It is calculated by(2)ϕkN=wkNσkN,where wkand σkare the weight and the standard deviation of the kth Gaussian distribution respectively.To allow the model to adapt to the changes in the video, an update scheme is applied. In this scheme, every new pixel value is checked through the existing model components in order of fitness, while the first matched Gaussian component will be updated. If it does not find a matched component, a new one will be added with a large standard deviation and a small value of the weighting parameter. The match rule is(3)abs(x−μk)≤Th×σk,where abs(i) is the absolute value function, x means the value of the current pixel, μkis the mean value of the kth Gaussian distribution, and Th is a threshold value (always setting as 2 or 3). After finding the matched Gaussian distribution, the parameters will be updated using the EM algorithm.To update the parameters of Gaussian distributions, the EM algorithm [14] begins the estimation of the GMM by using sufficient statistics update equations, then switches to L-recent window version after the first L samples are processed. That is, it uses different updating strategies in different processing stages. When the window size does not reach the threshold L, the EM algorithm can be characterized as follows:(4)wkN+1=wkN+1N+1(p(wk|xN+1)−wkN),(5)μkN+1=μkN+p(wk|xN+1)∑i=1N+1p(wk|xi)(xN+1−μkN),(6)ΣkN+1=ΣkN+p(wk|xN+1)∑i=1N+1p(wk|xi)((xN+1−μkN)(xN+1−μkN)−ΣkN).Otherwise, the EM algorithm is shown as follows:(7)wkN+1=wkN+1L(p(wk|xN+1)−wkN),(8)μkN+1=μkN+1L(p(wk|xN+1)xN+1wkN+1−μkN),(9)ΣkN+1=ΣkN+1L(p(wk|xN+1)(xN+1−μkN)(xN+1−μkN)wkN+1−ΣkN).From (4) to (9),wkNandμkNdenote the weight and the mean value of the kth GaussianMkrespectively when the window size equals to N,ΣkNis the square ofσkNwhileσkNmeans the variance ofMk. xNis the pixel value of the Nth frame, andp(wk|xN+1)is used to represent whetherMkis the first match of the current pixel, which can be calculated as follows:(10)p(wk|xN+1)={1,Mkisthefirstmatch;0,otherwise.

@&#CONCLUSIONS@&#
