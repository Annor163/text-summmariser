@&#MAIN-TITLE@&#
Comparing shapes through multi-scale approximations of the matching distance

@&#HIGHLIGHTS@&#
This paper deals with the concepts of persistence diagram and matching distance.We present multi-scale approaches to approximate the matching distance.Experiments show the capability of the proposed methodologies for shape retrieval.

@&#KEYPHRASES@&#
Persistence diagram,Bottleneck distance,Shape analysis,Dissimilarity criterion,

@&#ABSTRACT@&#
Two of the main ingredients of topological persistence for shape comparison are persistence diagrams and the matching distance. Persistence diagrams are signatures capturing meaningful properties of shapes, while the matching distance can be used to stably compare them. From the application viewpoint, one drawback of these tools is the computational cost for evaluating the matching distance. In this paper we introduce a new framework for the matching distance estimation: It preserves the reliability of the entire approach in comparing shapes, extremely reducing the computational cost. Theoretical results are supported by experiments on 3D-models.

@&#INTRODUCTION@&#
Interpreting and comparing shapes are challenging issues in computer vision, computer graphics and pattern recognition [32,34]. Topological persistence [4,22] – including persistent homology [21] and size theory [24] – has proven to be a successful approach for these tasks, with applications to shape retrieval and classification.One step in dealing with shape retrieval and classification is to define a measure of the (dis) similarity between shapes in a given database. According to topological persistence, this can be done by extracting from each element in the database a battery of persistence diagrams, shape descriptors which can be used to encode meaningful shape properties. In general, different shapes can provide the same persistence diagram: This can be seen as an equivalence with respect to the properties captured by that descriptor. The problem of determining the (dis) similarity between two shapes can be tackled by comparing the associated persistence diagrams: This can be done according to stable metrics between these descriptors, such as the Hausdorff distance or the matching (a.k.a. bottleneck) distance[14,18]. In this way a (pseudo) metric over the database can be defined, and used to quantitatively assess the (dis) similarity between different shapes [5,9,13].Throughout the years, topological persistence has been successfully linked to a number of concrete problems concerning shape classification and retrieval, ranging from the retrieval of real-world, multi-component trademark images [11] to the analysis and classification of tumor cells [31] and melanocytic lesions [23]. This is due to properties such as robustness against different types of noise [2,26] and partial occlusion of shapes [19,20].The main goal of this paper is to speed up the computational machinery underlying the assessment of a matching distance-based (dis) similarity metric over a dataset of shapes.From the theoretical viewpoint, there is a good reason to consider the matching distance as the “right one” to compare persistence diagrams: It is optimal[30]. More precisely, it is possible to define a very natural notion of stability according to which, among all the distances between persistence diagrams that are stable in this sense, the matching distance turns out to be the most discriminative one (see also [18, Thm. 32] for details).Nevertheless, evaluating the matching distance between two persistence diagrams is computationally expensive (see Section 3 for a precise formulation of this claim), making its usage not practicable when a (dis) similarity metric has to be defined on large datasets. To overcome this problem, in past works other distances have been used to compare persistence diagrams, such as the Hausdorff distance [12]. However, having the aforementioned optimality property of the matching distance in mind, we would prefer to deal with “good” approximations of it, which should be faster to compute and able to preserve its discriminative power, rather than be content with less informative metrics.In [10], the authors introduce a multi-scale strategy for the approximation of a matching distance-based (dis) similarity metric. For each pair of persistence diagrams associated with different elements in a database, the idea is to compute a rough estimation of their matching distance, which is faster to obtain than its exact computation. This estimation is based on a “dissimilarity criterion” that is explained in detail in Section 3. It has been implemented using an algorithmic procedure which allows for a progressive refinement of such an estimation, whenever it is not sufficient to distinguish between persistence diagrams (and hence shapes) which are too similar.In this paper we extend that work in two main respects:•From the algorithmic viewpoint, we introduce a new scheme generalizing the one proposed in [10]. It is based on a randomized strategy to apply the aforementioned dissimilarity criterion. The outcome is a more flexible tool allowing us to obtain statistically better performances in terms of retrieval results.As for experiments, we enlarged their setting by considering a new dataset of triangle meshes and new batteries of persistence diagrams, obtaining even in this case satisfactory results.The present work is organized as follows. In Section 2 we recall the necessary definitions and results needed in the rest of the paper. In Section 3 we review the multi-scale construction of a matching distance-based (dis) similarity metric presented in [10], together with the dissimilarity criterion which is formalized in Theorem 3.1. Section 4 and Section 5 are devoted to present the schema we have developed for the application of the dissimilarity criterion. The experimental setting is described in Section 6, ranging from the chosen datasets to the selected batteries of persistence diagrams. In Section 7 experimental results are provided. A final discussion on the obtained results concludes the paper (Section 8).In the classical formulation of persistence [22], the shape of an object is usually modeled as a topological space X endowed with a continuous functionφ:X→R. The role ofφis to describe certain geometrical properties of X considered relevant to characterize the associated shape [9,24]. The topological properties of X are then captured by analyzing the sublevel sets filtration induced byφon X, i.e. a family of subspacesXu=φ-1((-∞,u]),u∈R, nested by inclusion. The core of persistent homology is to focus on the occurrence and the longevity of topological events along this filtration, such as the appearance and disappearance of connected components, tunnels and voids. For these reasons, the functionφ:X→Ris called a filtering (or measuring) function. The levels ofφat which these events correspond, i.e. the birth or the death of a homological class, are called homological critical values. As for homological classes, we distinguish between the classes which are born at a certain levelu∈Rand never die along the filtration, and the classes having a bounded lifetime. The former, called essential homological classes, represent the homological features of X; the latter represent the homological features of X with respect to the geometrical shape property described byφ. The importance of these “bounded” classes depends on their lifetime: The basic paradigm here is that the longer a feature survives, the more meaningful or coarse the feature is for shape description. Vice-versa, noise and shape details are characterized by a shorter life.The geometric-topological information of the shape provided by the chosen pair(X,φ)can be encoded in an algebraic structure known in the literature as persistent homology group. Givenu⩽v∈R, we consider the inclusion ofXuintoXv. This inclusion induces a homomorphism of homology groupsHk(Xu)→Hk(Xv)for everyk∈Z. Its image consists of the k-homological classes that live at least from the level u to the level v and is called the kth-persistent homology group of(X,φ)at(u,v). If X satisfies some mild conditions [8] (which will be assumed to hold throughout the paper) this group is finitely generated: In this case, we denote its rank byβk(u,v).A simple and compact description of persistent homology groups is provided by the associated persistence diagrams. Roughly, these are multi-sets of points obtained by pairing homological critical values corresponding to the birth-level and the death-level of one or more independent k-homological classes along the filtration induced byφ, and vertical lines representing essential classes. In this paper, for the sake of simplicity, we focus on sub-diagrams of persistence diagrams, known in the literature as ordinary persistence diagrams (from now on simply called persistence diagrams), in which lines are not present [15]. However, we underline that all the results we present here can be extended to the classical persistence diagrams (see Remark 3.1).Assume that a pair(X,φ)is fixed. To formalize the definition of a persistence diagram, letΔ+‾=Δ+∪ΔwhereΔ+={(u,v)∈R×R:u<v}, andΔ={(u,v)∈R×R:u=v}. With each point(u,v)∈Δ+let us associate a multiplicity, that is the number of independent k-homological classes born at the level u and dead at the level v[22,25]. Obviously, the sole points with a positive multiplicity are the ones having as coordinates homological critical values appropriately paired. These points are called proper points.Definition 2.1MultiplicityLetk∈Zand(u,v)∈Δ+. The multiplicityμk(u,v)of(u,v)is the finite non-negative number defined by(2.1)μk(u,v)=limε→0+βk(u+ε,v-ε)-βk(u-ε,v-ε)+-βk(u+ε,v+ε)+βk(u-ε,v+ε).The kth-persistence diagramDkis the multiset of all points(u,v)∈Δ+such thatμk(u,v)>0, counted with their multiplicity, union the points ofΔ, counted with infinite multiplicity.Fig. 1shows an example of persistence diagrams fork=0,1. The surfaceX⊂R3is filtered by the height functionφ. The associated persistence diagramsD0andD1have only one proper point. The abscissa ofp∈D0(q∈D1, respectively) corresponds to the level at which a new connected component (tunnel, respectively) is born along the filtration, while its ordinate identifies the level at which this connected component (tunnel, respectively) merges with the existing one (is closed on one side, respectively). To see, for instance, thatμ0(p)=1, ifp=(u¯,v¯), it is sufficient to observe that, for everyε>0sufficiently small, it holds thatβ0(u¯+ε,v¯-ε)=2,β0(u¯-ε,v¯-ε)=β0(u¯+ε,v¯+ε)=β0(u¯-ε,v¯+ε)=1, and apply Definition 2.1. In an analogous way, it can be observed thatμ1(q)=1.Two persistence diagrams can be compared by means of the matching distance.Seen that this comparison is generally performed by fixing a certain homological degree, we drop in what follows any reference to the integer k.The matching distance between two persistence diagramsD,D′measures the cost of finding a correspondence between their points. In doing this, the cost of taking a pointp∈Dto a pointp′∈D′is measured as the minimum between the cost of moving one point onto the other and the cost of moving both points onto the diagonal. Matching a proper point p with a point ofΔ, which can be interpreted as the destruction of p, is allowed by the fact that the number of proper points for two persistence diagrams is usually different. See Fig. 2for an example.Definition 2.3Matching distanceThe matching distance between two persistence diagramsD,D′is defined asdmatch(D,D′)=minσmaxp∈Dd(p,σ(p)),whereσvaries among all the bijections between D andD′and(2.2)du,v,u′,v′=minmax|u-u′|,|v-v′|,maxv-u2,v′-u′2for everyu,v,u′,v′∈Δ+‾.The main interest in this metric is due to the fact that persistence diagrams are robust with respect to the matching distance. A visual interpretation of this property is given in Fig. 3, where the 0th-persistence diagram of a woman surface model filtered by the height function is considered(a)together with a noisy version(b). In both diagrams, the two points which are farthest from the diagonal represent the components born once the filtration includes the woman’s hands (they do not touch the rest of the body) and dying at the height of the armpits.Looking at Fig. 3, the stability of persistence diagrams with respect to the matching distance can be explained as follows: Small changes in the considered filtering function produce only small changes in the position of points far from the diagonal, and possibly produce variations close to the diagonal.More formally, consider two filtering functionsφ,φ′:X→R. If we measure the distance betweenφandφ′by theL∞-norm, and the matching distance between the corresponding persistence diagrams D andD′, the stability result upper bounds the latter distance by the former, i.e.dmatch(D,D′)⩽‖φ-φ′‖∞[18,14].From a computational viewpoint, getting the matching distance between two persistence diagrams takesO(h2.5), being h the total amount of their proper points [17].This means that, in applications involving large databases, computing the matching distance for any possible shape comparison is not a viable option. In fact, noisy or detailed shape models may give origin to persistence diagrams with a large number of proper points. In light of this, we want to reduce this computational complexity by considering, at first, only a rough estimation of the metric induced by the matching distance over a database, to be possibly refined whenever it is not sufficient to distinguish between different shapes.The main point here is that, in most cases, determine that two shapes are very dissimilar does not require the exact computation of the matching distance between the associated persistence diagrams. For example, only a fast glance is usually needed to decide whether an elephant is different from an ant. In our framework, this could be equivalent to a rough estimation of the matching distance – and hence faster than its actual computation – between the persistence diagrams associated with the “elephant shape” and the “ant shape”, respectively. On the other hand, a higher level of accuracy could be necessary to distinguish, e.g., the “wolf shape” from the “German shepherd shape”. This would lead to a sharper estimation of the matching distance between the associated persistence diagrams, possibly to its exact computation.In light of these reasonings, we introduce a multi-scale construction of our matching distance-based (dis) similarity metric.Let D be a persistence diagram, and A its subset of proper points. For everyp=(u,v)∈Δ+and everyδ>0, letQδ(p)be the open square centered at p of side length equal to2δ, and let us denote by♯(Qδ(p),A)the number of points of A contained inQδ(p). This notation is maintained throughout the paper.Theorem 3.1Dissimilarity criterionLetD,D′be two persistence diagrams, andA,A′the respective subsets of proper points. If there exist a pointp=(u,v)∈Δ+and two real numbersδ,ε>0such thatQδ+ε(p)⊂Δ+and♯(Qδ(p),A)-♯(Qδ+ε(p),A′)>0, thendmatch(D,D′)⩾ε.The assumption♯(Qδ(p),A)>♯(Qδ+ε(p),A′)implies that, for every bijectionσ:D→D′there exists at least one proper pointq¯=(u¯,v¯)∈Dsuch thatq¯∈Qδ(p)andσ(q¯)=q¯′=(u¯′,v¯′)∈D′, withq¯′∉Qδ+ε(p). Then, from (2.2) it holds that(3.1)d(q¯,q¯′)⩾minε,maxv¯-u¯2,v¯′-u¯′2⩾minε,v¯-u¯2=ε.Indeed, in (3.1), the first inequality holds because at least one between|u¯-u¯′|and|v¯-v¯′|is not smaller than the difference between the semi-sides ofQδ(p)andQδ+ε(p); the second inequality is obvious; the equality follows from both the facts thatv¯-u¯>(v-δ)-(u+δ), being(u¯,v¯)∈Qδ(p)and(u+δ,v-δ)∈Δ+the bottom right vertex ofQδ(p), and(v-δ-ε)-(u+δ+ε)⩾0, i.e.(v-δ)-(u+δ)⩾2ε, being(u+δ+ε,v-δ-ε)∈Δ+‾the bottom right vertex ofQδ+ε(p). Hencemaxq∈Dd(q,σ(q))⩾εfor every bijectionσand, by Definition 2.3, the theorem is proved □.The matching distance between two classical persistence diagrams is computed on the basis of bijective correspondences between the multisets of points (proper and ofΔ), and the ones between the multisets of vertical lines (called corner lines or points at infinity) [17]. Accordingly, Theorem 3.1 can be extended to classical persistence diagrams by working with open squares for points inΔ+, and open rectangles with infinite height for vertical lines.Fig. 4shows an example of Theorem 3.1 in action. Figs. 4(a) and (b) display two persistence diagrams D andD′. Points belonging to A andA′are yellow and blue colored, respectively. In Fig. 4(c) the two multisets of points are superimposed, and the two squaresQδ(p)(yellow) andQδ+ε(p)(blue) are depicted. It holds that♯(Qδ(p),A)-♯(Qδ+ε(p),A′)=1, hence Theorem 3.1 implies thatdmatch(D,D′)⩾ε.The issue here is to find a suitable way to apply Theorem 3.1, so to improve our framework. To this end, two different schemes have been designed, which we describe in Section 4 and 5, respectively.We conclude the section with a remark which will be useful later.Remark 3.2Definition 2.3 of the matching distance implies that, given two persistence diagramsD,D′, and the respective subsets of proper pointsA,A′, ifH=A∪A′, thendmatch(D,D′)⩽(V-U)/2, withU=min(u,v)∈Hu,V=max(u,v)∈Hv. Indeed,(V-U)/2upper bounds the cost of the bijection between D andD′, taking all the points of H ontoΔ. Sincedmatchis realized by the cheapest bijection between D andD′, we have the claim.This section is devoted to review the scheme introduced in [10] to estimate from below the matching distance between two persistence diagrams by virtue of Theorem 3.1. Moreover, we describe the pseudo-code and the computational complexity of our algorithm. We called this procedure a Refinement Prefixed Grid Scheme (RPGS).Algorithm 1RPGS (A,A′,N)1: Input:A,A′– lists of proper points; N – number of iterations2:Res⇐03: set U to be minimum x-coordinate of points inA∪A′4: set V to be maximum y-coordinate of points inA∪A′5:ω⇐(V-U)/106:Side⇐V-U+2ω7: forn=1toNdo8:t⇐5+n9:sSide⇐Side/t10: compute matrices mA andmA′11:fori=2to(t-4)do12:forj=(i+3)to(t-1)do13: compute MA andMA′for the parameters i and j14:r1⇐(MA<mA′(i,j))15:r2⇐(MA′<mA(i,j))16:if(r1orr2)and(Res<sSide)then17:Res⇐sSide18:endif19:endfor20:endfor21: endfor22: returnResAn implementation of RPGS is Algorithm 1, which takes as input the lists A andA′of proper points of two persistence diagrams D andD′, and a parameter N which is a natural number. Algorithm 1 runs a number of iterations equal to N. It starts by creating a squared region Q, which contains all the points of A andA′. Q has(U-ω,U-ω),(U-ω,V+ω),(V+ω,V+ω),(V+ω,U-ω)as vertices, where U and V are as in Remark 3.2, andωan arbitrarily small positive real number (see e.g. Fig. 5(a)). At the nth iteration a grid consisting oft2squared cells,t=n+5, is constructed on Q. The algorithm computes thet×tmatrices mA andmA′whose entries are the number of points of A andA′in every cell of the grid, respectively. A pointp=(u,v)belonging to A (A′, respectively) is counted in the entry(i,j)of mA (mA′, respectively) if(⌈(u-U+ω)/sSide⌉,⌈(v-U+ω)/sSide⌉)=(i,j), where sSide is as in line 9. Moreover, the algorithm sets the variables MA andMA′to the sum of the entries of the submatricesmA[i-1,i,i+1;j-1,j,j+1]andmA′[i-1,i,i+1;j-1,j,j+1], respectively. Finally, the algorithm evaluates Theorem 3.1 on each squared cell compared with the concentric square having side length three times greater, provided that both squares are contained inΔ+‾; the maximum value for which the theorem holds is then returned.An example of RPGS in action is shown in Fig. 5. Let us assume that A is the set of yellow points, andA′the set of blue points. In the first iteration (Fig. 5(a)) we evaluate Theorem 3.1 once. Variables storing the number of points of A andA′in the blue square areMA=3,MA′=2, respectively; variables storing the number of points of A andA′in the yellow square aremA(2,5)=1,mA′(2,5)=0. Hence,MA>mA′(2,5)andMA′>mA(2,5), and we cannot provide an estimate of the matching distance because none of the assumptions in lines 14 and 15 hold. Moving to the second iteration, shown in Fig. 5(b), the algorithm applies Theorem 3.1 three times, getting the same result. Eventually, in the third iteration (Fig. 5(c)) there is at least one case for which our assumptions hold:MA′=1,mA(4,7)=2, thusMA′<mA(4,7). In other words, the number of blue points in the blue square is less than that of yellow points in the yellow square. This implies that any bijection between A andA′matches at least one pair of points whose mutual distance is larger than the side length sSide of the yellow square. Hence, sSide is a lower bound for the matching distance between D andD′.Set h to the total number of points in A andA′. With reference to Algorithm 1, the computational complexity C of RPGS can be formalized asC(h,N)=c0+c1·h+∑n=1Nc2+2c3(n+5)2+c4·h+∑i=2n+4∑j=i+3n+4c5,withc0,…,c5constants,c0+c1·hthe cost of lines1-6,2c3(n+5)2+c4·hthe cost of line 10, andc5that of lines 8–9.Making some simple mathematical manipulations we obtain thatC(h,N)=c0+c1·h+N(c2+c4·h)+2c3·∑n=1N(n+5)2+∑n=1N∑i=1n+3∑j=1n-i+1c5.Now, by counting the total number of squares on which the theorem is evaluated on a run of the algorithm, which is∑n=1N∑i=1n∑j=1n-i+11=∑n=1N∑i=1n(n-i+1)=∑n=1Nn(n+1)2=N3+3N2+2N6,we can conclude that the computational complexity of RPGS isO(N3).In this section we elaborate on a more generic procedure for the application of Theorem 3.1 to approximate the matching distance between two persistence diagrams. In particular, we start by presenting the most general formulation of our approach (Algorithm 2), and then we describe how we implemented its main building blocks to obtain more efficient schemes of computation. Our choices lead to produce Algorithm 4, called a Randomized Bisection Squares Scheme (RBSS). Its computational cost depends on the choice between two possible structures for membership tests, kD-tree and integral image methods, which we describe in detail. Eventually, the cost of RBSS using each of these structures is compared with that of the matching distance.In what follows, byA,A′,H,h,U,V,Qwe denote the same concepts as in the previous Section 4.Algorithm 2DissimilarityScheme (A,A′,R,S,IQS,CIR)1: Input:A,A′– lists of proper points; R – number of outer squares; S – number of inner squares; IQS – structure used to store points and make queries; CIR – structure used to choose the side length and the center of each square2:r⇐r1⇐r2⇐03:H⇐A∪A′4:Q⇐IQS(H)5: fori=1toRdo6:p⇐(CIR(xmin(H),xmax(H)),CIR(ymin(H),ymax(H)))7:d⇐py-px8:ifd>c*(V-U)then9:δ⇐CIR(0,d)10:forj=1toSdo11:η⇐CIR(0,δ)12:if#(Qη(p),A)-#(Qδ(p),A′)>0then13:r1⇐δ-η14:endif15:if#(Qη(p),A′)-#(Qδ(p),A)>0then16:r2⇐δ-η17:endif18:r⇐max(r,r1,r2)19:endfor20:endif21: end for22: returnrAlgorithm 2 shows the skeleton of the most general scheme.The overall idea can be sketched as follows. We first select R random points inQ∩Δ+(first for loop, lines 5–6). For each such point p, we build a squareKp⊂Δ+(“outer” square), centered at p (first if-then construct, lines 8–9). Then S squares (“inner” squares), concentric withKp, sayL1p,…,LSp, are taken insideKp(second for loop, lines 10–11). Finally, we evaluate Theorem 3.1 on the squareKpcompared withLjp,j=1,…,S(lines 12–17). Algorithm 2 returns the maximum value r verifying our theorem.Before going on, let us first disambiguate the pseudo-code and explain the followings key points:•Line 2 initializes return values of Theorem 3.1. In particular r stores the highest lower bound for the matching distance, whiler1andr2memorize the temporary values related to the two possible theorem applications (lines 12 and 15);Line 4 initializes the structure used to make membership tests and to count the number of proper points of a given set (A orA′) belonging to a given squared area (IQS(H)stands for “Initialize Query Structure on the set H”);The value c in Line 8, which is a scalar in the interval]0,m[⊂R, with m arbitrarily small, represents the discrete step we use in our measurement. Moreover the whole expression stands for “Take p only if it is over the main diagonal by at least c times V−U”;The expressionCIR(m,M)(that is, “Choose In Rangem…M”) in lines 6, 9 and 11 denotes the choice of a value inside the interval[m,M]⊂R.A first formulation of the overall computational complexity C of Algorithm 2 is(5.1)C(h,R,S)=IQ(h)+Eout(R)·Eint(S)·EQ(h),withIQ(h)the cost of building a structure to make membership tests,Eout(R)the cost of evaluating R outer squares,Eint(S)the cost of evaluating S inner squares, andEQ(h)the cost of executing a query on such a structure.In what follows we discuss in detail how we implemented Algorithm 2 to optimize each computational cost in (5.1). Our procedure generates a Randomized Bisection Squares Scheme (RBSS) whose pseudo-code is shown in Algorithm 4.The first block we explain is the one which realizes the search of the pair of squares (inner and outer) optimizing the matching distance estimate. Let us start with the following remark:Remark 5.1Assume that the side length2δand the center p of an outer square are fixed. If Theorem 3.1 holds for this square compared with some inner square, then there is only one optimal side length of the inner square such thatε(semi-difference of square’s sides) is maximal and#(Qδ-ε(p),A)-#(Qδ(p),A′)>0(Theorem 3.1 holds). This is a straightforward consequence of the monotonicity of#(Qδ-ε(p),A)-#(Qδ(p),A′)with respect toε.We can exploit information from Remark 5.1 to formulate Algorithm 3, which finds the optimal length for an inner square’s side with a fixed outer square. One can use the bisection method [29] to find the maximal difference in the square’s side length up to a given errorτ. Suchτvalue depends on S with regard to the number of steps of the algorithm, and on a real interval to calculate the effective value. Without further constraints, the best choice ofτisτ=δ-ηS.Algorithm 3BisectionSquare (p,δ,η,τ,A,A′)1: Input: p – center of square;δ,η– range for bisection search;τ– desired tolerance;A,A′– lists of proper points2: Setτto be the tolerance for the bisection method3: Use bisection method to find minimalγs.t.#(Qγ(p),A)-#(Qδ(p),A′)>0holds4: returnδ-γThe invocation of a bisection scheme to find the optimal pair of outer/inner squares (lines 11–14 of Algorithm 4) improves the overall computational complexity (5.1) reducing the cost ofEintfromO(S)down toO(log2S)(which means that we are evaluatingO(S)inner squares inO(log2S)steps).It is worth mentioning that this method can be easily transposed to the opposite situation, when the inner square is fixed and the outer one may vary. Accordingly, we can provide a scheme which does not degrade the overall complexity and use both these approaches, fixing in turn the outer and the inner square. For the sake of simplicity we provide here only the method for the inner square. However, in the experiments we use both methods.The procedure to choose outer squares is quite different from the bisection scheme we have just presented. Indeed, the only constraint we have to satisfy in choosing outer squares is that their bottom-right vertex must be contained inΔ+. This requirement directly contributes to the computational complexity, which cannot be better than linear in the number of outer squares we want to consider, meaning thatEout(R)=O(R).In Algorithm 4 the choice of outer squares is determined by selecting squares’ centers (line 6 in the for loop). We observe that a first optimization of our scheme can be obtained by taking only squares containing at least one proper point fromA∪A′: We show in Section 7 how we achieve this.The optimal estimates for inner and outer squares are computed separately as explained in the previous subsection. We use the procedureRandom(b,e)(lines 9–10 of Algorithm 4) which returns random numbers from the interval[b,e]. For a pointp=(px,py), the measure of inner squares’ sides centered at p is set randomly up to 30% of the maximal possible length, which isd=py-px. The measure of the outer squares’ sides length is set also randomly to at least 70% of d.Algorithm 4RBSS (A,A′,R,IQS)1: Input:A,A′– lists of proper points; R – number of outer squares; IQS – structure used to store points and make queries2:r⇐r1⇐r2⇐03:H⇐A∪A′4:Q⇐IQS(H)5: fori=1toRdo6:p⇐(Random(xmin(H),xmax(H)),Random(ymin(H),ymax(H)))7:d⇐py-px8:ifd>c*(V-U)then9:δ⇐d·Random(0.7,1)10:η⇐d·Random(0,0.3)11:τ⇐δ-ηS12:r1⇐BisectionSquare(p,δ,η,τ,A,A′)13:r2⇐BisectionSquare(p,δ,η,τ,A′,A)14:r⇐max(r,r1,r2)15:endif16: endfor17: returnrThe final computational cost of RBSS depends on how we implement line 4 of Algorithm 4. We propose two different structures to make membership tests: kD-tree and integral image methods.We need a fast method to retrieve the number of points in a given square ofΔ+. Using kD-trees [1] is probably one of the best possible choices. Indeed, it is linear in terms of memory usage (with respect to the number of points) and the kD-tree construction does not increase the overall computational complexity, beingO(ylog2y), with y the cardinality of the set on which the kD-tree is built. In our context two kD-trees are needed, one for the points of A and one for the points ofA′. Therefore, by using the kD-tree structure in Algorithm 4 we can update the overall complexity (5.1) as(5.2)C(h,S,R)=O(hlog2h)+O(R)·O(log2(S))·O(h),withO(h)the cost of executing a query on a kD-tree.Instead of using kD-trees, we may retrieve the number of points in R squares by using the integral image (or summed area) method [16,28]. Integral image algorithm uses pre-computed arrays to quickly retrieve the sum of values in rectangular subsets of a given image. In our case we consider squares instead of rectangles. Such an approach requires that the sides of the considered squares lie on a known grid. A further restriction is that vertices have integer coordinates, so we can use a two dimensional array.In order to use the integral image method, we divide the square Q by the smallest possible grid. The number of the grid cells is set toλ2. It is then possible to count the number of points in the considered squares using this method. Preprocessing for the integral image is linear in the number of cells, and the cost of executing a query (retrieving the number of points in the square) isO(1).Using this approach the computational complexity of RBSS turns out to be(5.3)C(λ,R)=O(λ2)+O(R)·O(log2(λ))·O(1)We remark that, when evaluating the computational complexity, the parameterλhas an effect on the range of the errorτ. More precisely, by choosing the integral image scheme, the best possible choice ofτisτ=V-Uλ.We conclude this section with some final remarks on the computational costs in (5.2) and (5.3), by discussing how to keep them lower than the costO(h2.5)of the matching distance.Let us begin by considering the computational complexityC(h,S,R)of our algorithm with the usage of kD-trees, see (5.2). Keeping it not grater thanO(h2.5)requires thatO(R)·O(log2(S))·O(h)⩽O(h2.5),or equivalentlyO(R)·O(log2(S))⩽O(h1.5).From the above relation we can deduce that, once we fix the number S of inner squares, the number R of outer squares can range between 1 andh1.5; while, fixing the number R of outer squares, the number S of inner squares can range between 1 and2h1.5.Let us now analyze the computational complexityC(λ,R)due to the usage of the integral image approach, see (5.3). In order to keep it asymptotically lower thanO(h2.5), the following inequalities have to be verified:(5.4)O(λ2)⩽O(h2.5)O(R)·O(log2(λ))⩽O(h2.5)These reasonings lead us to conclude that the optimal values forλ, i.e. the number of rows and columns in the grid, would be in the range between 2 andh1.25, while for R betweenh2.5log2h1.25andh2.5.The previous considerations aboutS,λand R are summarized in Table 1, assuming that the minimum possible value for R to be taken is 1.To conclude we observe that, as in the case of RGPS, we can ensure that the computational complexity of RBSS is smaller than the complexity of classical computation of matching distance. We can do this simply by forcing the parameter R to be smaller than eitherh1.5orh2.5, according to the usage of kD-trees or the integral image scheme. Depending on the data, varying the ratio between S and R in the former case, as well as the ratio betweenλand R in the latter one, would give a more accurate estimate for the matching distance computation.Our goal is to validate the theoretical framework introduced in the previous Sections 4 and 5. Experiments on 3D-models represented by triangle meshes will give evidence that our algorithms are able to reduce the computational complexity in defining a matching distance-based metric over a given database, preserving the quality of results in terms of retrieval performance.To test the proposed framework we considered two datasets of 3D-surface mesh models. As a first one, we opted for the Non-Rigid World Benchmark [6] (from now on Db1). This database contains 148 three-dimensional models such as cats, dogs, wolves, horses, lions and gorillas, in a variety of poses for non-rigid, shape similarity experiments. Fig. 6shows five models belonging to the “cat” class (first row), together with some representatives for other classes in the database (second row). The second database (from now on Db2) has been introduced in [3]. It is a collection of 228 3D-surface mesh models grouped in 12 classes, each one in turn composed by 19 elements: A null model (cat0, david0, dog0,…, victoria0, wolf0) from the Non-Rigid World Benchmark, together with 18 deformations given by six non-rigid, possibly non-metric-preserving transformations applied to the null model at three different strength levels. Fig. 7shows some elements for the “centaur” class.To select the considered filtering functions we followed [3]. In that paper, the authors exploit the modularity of the persistence framework to study under different perspectives the models belonging to the two datasets. Indeed, persistence diagrams inherit their invariance properties (with respect to groups of transformations) directly from the corresponding filtering functions. Therefore, to obtain different invariance properties, it is sufficient to change the filtering function. In particular, the ones chosen in [3] fit the different purposes the two datasets are designed for: Db1 is suited to analyze non-rigid shape similarity, while Db2 has been created to deal with noise and other deformations which do not preserve the metric properties of shapes (e.g. the Riemannian metric).According to the previous reasonings, for Db1 we considered two filtering functions which are well known to be robust with respect to non-rigid shape changes. The first one, which we denote byφHK, is chosen to be the heat kernel signature [7,33], computed using the first 10 eigenfunctions of the Laplace–Beltrami operator and a fixed time t=1000, and the second one,φG, the integral geodesic distance [27]. The invariance to scale comes from the a priori normalization of the models.As for Db2, to define the considered filtering functions we proceeded as follows: For each triangle mesh M of vertices{P1,…,Pn}, the center of mass B is computed, and the model is normalized to be contained in a unit sphere. Further, a vectorw→is defined asw→=∑i=1n(Pi-B)‖Pi-B‖∑i=1n‖Pi-B‖2.We can think of the vectorw→as a generalization of the center of mass: Its computation is rotation and translation invariant, so that its relative position with respect to the corresponding triangle mesh does not change when rigid movements are taken into account. Moreover, models in Db2 are generic enough (no point-symmetries occur, etc.) to guarantee thatw→is well-defined and stably oriented for all shapes. Three filtering functionsφL,φP,φBare computed on the vertices ofM:φLis the distance from the line parallel tow→and passing through the center of mass B,φPis the distance from the plane orthogonal tow→and passing through B, andφBis the distance from B (see Fig. 8as an example). The values ofφL,φPandφBare then normalized so that they range in the interval[0,1]. These filtering functions are translation and rotation invariant, as well as scale invariant because of a priori normalization of the models.Suppose to have fixed a database, say Db={Mi}, and a scheme, say Scm. For each corresponding filtering functionφ, we can induce a metric over Db by computing the matching distancesdijφ=dmatch(Di,Dj)for everyi,j=1,…,|Db|, withDi,Djpersistence diagrams associated withMi,Mj, respectively, and|Db|the number of models in Db. To approximate such a metric, we apply Scm to get a lower bound for eachdijφ, sayResijφ. This procedure is controlled by a threshold,threshφ, obtained as follows: For every class in the database, 4 elements are (randomly) selected, and an average of the matching distances on this small subset is evaluated. The final value ofthreshφis then the average over all the classes in the database. In this perspective, the valuethreshφrepresents the average matching distance between two elements of the same class.Now, ifResijφ>threshφ, then we can assume that the shapes ofMiandMjare quite dissimilar (compared with respect toφ) and therefore it is sufficient to have just an estimation ofdijφ: We opted for((V-U)/2+Resijφ)/2, with V and U taken as in Remark 3.2. In plain words, our estimation is the average between the lower bound (according to Theorem 3.1) and the upper bound (according to Remark 3.2) ofdijφ. IfResijφ⩽threshφ, then the exact value ofdijφis computed. The overall process is described in Algorithm 5.Algorithm 5MetricApprox (A,A′,Exp,thresh)1: Input:A,A′– lists of proper points; Exp – control parameter for the selected scheme; thresh – dissimilarity threshold2:Res=Scm(A,A′,Exp)%Scm is either RPGS or RBSS3: ifRes>thresh4:Val=[(V-U)/2+Res]/25: else6:Val=dmatch(D,D′)7: end if8: returnVal

@&#CONCLUSIONS@&#
