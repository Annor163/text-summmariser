@&#MAIN-TITLE@&#
A novel algorithm using affine-invariant features for pose-variant face recognition

@&#HIGHLIGHTS@&#
Multi-scale autoconvolution features maintain invariance to noise and pose changes.Histogram equalization reduces the effect on MSA features caused by illumination.The number of (α,β) pairs affects the size of MSA features and recognition rates.MSA+PCA algorithm exhibits excellent abilities on completely different databases.

@&#KEYPHRASES@&#
Face recognition,Pose variance,Affine-invariant feature,Multi-scale autoconvolution,Principal component analysis,Illumination change,

@&#ABSTRACT@&#
Pose variation is the major factor significantly affecting recognition efficiency in the field of face recognition. In this paper, we propose a novel algorithm for pose-variant face recognition. We first use the affine-invariant multi-scale autoconvolution (MSA) transformation to extract pose-invariant features. Following this, we use principal component analysis (PCA) to decorrelate the feature sets and reduce the number of required MSA dimensions. The PCA components with sufficiently large corresponding eigenvalues are then passed to the k nearest classifier. Compared with other pose-variant face recognition techniques, our proposed algorithm exhibits a superior ability to pose changes, illumination changes, and different face databases, moreover records a higher recognition rate.

@&#INTRODUCTION@&#
Human face recognition continues to be one of the most active areas in the fields of computer vision and pattern recognition because of its wide range of applications in traditional security as well as numerous scenarios involving image tagging and searching. As one of the most important biometric techniques, face recognition has a clear advantage of being natural and passive over other techniques such as fingerprint recognition and iris recognition, which require cooperative subjects. In order to benefit from the unintrusive nature of this technique, a face recognition system must possess the capability to identify an uncooperative face in arbitrary poses, e.g., a forward-looking face, an overlooking face, a side-looking face, and an upward-looking face, etc., as shown in Fig. 1.Owing to the complex three-dimensional (3D) structure of a human face, pose variations present a serious challenge to current face recognition systems. In particular, the innate facial characteristics distinguishing one face from another do not vary significantly among people. However, the extent of image variation caused by pose variations is often greater than that of dissimilarities due to the innate characteristics. Pose-invariant face recognition algorithms thus need to address the task of extracting innate characteristics from a variety of poses [1].Considerable algorithmic progress has been made in face recognition involving forward-looking face images [2–4]. This progress can be attributed to greater interest among the researchers in pose-variant face recognition following its identification as an important problem. Consequently, a few promising methods have been proposed to solve the problem of pose-variant face recognition, such as Tied Factor Analysis (TFA) [5], 3D Modular Discriminant analysis (3DMDA) [6], and Eigen Light-Field (ELF) [7]. However, none of these methods has succeeded in solving the pose problem in face recognition. Hence, further research is needed to attain pose-variant face recognition.In general, face recognition techniques involving poses fall into two categories: methods based on 3D models, and methods based on two-dimensional (2D) techniques [8]. The use of 3D models to tackle pose variations has been successful because human heads are 3D objects. Moreover, 3D facial data is considered invariant against changes of pose and illumination. The advantages of using 3D models in face recognition have been highlighted in [9]. However, 3D face recognition algorithms do not go beyond 2D methods in terms of performance. This is compounded by the weakness of these methods with regard to data collection and complexity. Therefore, 3D face recognition technology is not widely used [8]. 2D techniques include pose transformation in image space [7,10,11] and pose-tolerant feature extraction [5,12–14]. The former involves synthesizing virtual views into possible poses from a limited number of real views (often from a single view) by using reference faces as prior knowledge. These techniques can effectively handle pose variations within small-to-medium rotations usually limited to 45°. However, large pose variations can cause image discontinuities in 2D image space to the extent that they cannot be reliably handled. Pose-tolerant feature extraction attempts to transform the image space into a feature space where pose variation can be tolerated to a greater degree. In this respect, various pose-invariant transformation methods have been proposed [15]. However, these transformation methods are only invariant to zoom, rotation, and scale transformation, or a combination of these. A single instance of these transformations or their combination is not sufficient to express the actual changes of pose in real life, which can be satisfactorily approximated by an affine transformation. Rahtu et al. [16] have proposed an affine-invariant feature extraction method called “multi-scale autoconvolution” (MSA), which can attain more accurate identification results than other affine-invariant methods and cross-weighted moments. Several researchers have applied MSA features to recognition applications. However, only a few researchers have used MSA for face recognition.Zhao et al. [17] were the first to use MSA features in pose-variant face recognition. However, they found that MSA features are sensitive to illumination changes and adapt poorly to different face databases. Hence, based on the past research, we propose a new face recognition algorithm in this paper. Our algorithm uses histogram equalization to mitigate the effects of illumination changes, and utilizes PCA to reduce the number of MSA dimensions and render MSA features more adaptable.Our contributions in this paper are as follows:1.We propose a new MSA+PCA pose-variant face recognition algorithm and verify its effectiveness using four common multi-pose face databases: the Olivetti Research Laboratory (ORL) database maintained by Cambridge University, the Audio Visual Technologies Group (GTAV) database at the Computer Vision Center of the Universitat Politécnica de Catalunya, the Pose, Illumination, and Expression (PIE) database of the Robotics Institute at Carnegie-Mellon University, and the datasets available at Pointing’04 Workshop at Cambridge University.We analyze the applicability of MSA features to multi-pose face recognition tasks.We study the influence of illumination changes on MSA features and offer solutions.We analyze the influence of (α,β) pairs on MSA features as well as a range of suitable values, while pointing out the relationship between the number of (α,β) pairs and the recognition rate.We verify the stability of MSA features against noise (of varying density and type).The remainder of this paper is structured as follows: in Section 2, we present our pose- variant face recognition algorithm, and assess its effectiveness in Section 3. Section 4 is devoted to exhibiting the ability of our proposed algorithm through a series of experiments, whereas we offer our conclusions as well as directions for future research in Section 5.In this section, we introduce the MSA transform, test the classification performance of MSA features, and propose our face recognition algorithm.We briefly summarize the MSA transform. Detailed derivation can refer to [16].Let f(x):IR2→IR be an image intensity function in L1(IR2)∩L2(IR2) and letp(x)=1‖f‖L1f(x)be the normalized version of f, so that∫p(x)dx=1. Then, p(x) is a probability density function, and we may take X0, X1, X2 to be independent random variables with values in IR2, so thatP(Xj=xj)=1fL1f(xj).For α,β∈R, we define a random variable Uα,β=α(X1−X0)+β(X2−X0)+X0.Therefore, we obtain Uα,β=αX1+βX2+γX0, where γ=1−α−β. Now, we can easily show that Uα,βhas a probability density function(1)PUα,β=u=1fL13(fα∗fβ∗fγ)(u),where fα(x)=α−2f(x/α) for α≠0 andfα=fL1δ0for α=0.For α,β∈R, we define the MSA transform of f by F(α,β)=E{f(Uα,β)}.Writing this out in terms of the probability density function givesF(α,β)=∫f(u)P(Uα,β=u)du=1fL13∫f(u)(fα∗fβ∗fγ)(u)du.Taking the Fourier transform and using the convolution and correlation theorems, we obtain:(2)F(α,β)=1f̂(0)3∫IR2f̂(-ξ)f̂(αξ)f̂(βξ)f̂(γξ)dξ,which holds for all (α,β).In this paper, the (α,β) values refer to the experimental values of Rahtu et al., i.e., in the set {−1, −0.75, −0.5, −0.25, 0, 0.25, 0.5, 0.75, 1}, we select any possible combinations of two elements to form 29 (α,β) pairs.Given a gray-scale face image function f and (α,β) pairs, we can obtain its MSA feature values using Eq. (2). In this sub-section, we test the classification performance of MSA features using an experiment, the flow of which is shown in Fig. 2.The dataset we adopted for the experiment was the Pointing’04 multi-pose face database [18]. The images in Figs. 1 and 3were part of the database.K-nearest classifiers, support vector machines (SVMs), and neural networks (NNs) are the typical classifiers used for face recognition. Therefore, we used them in our experiment to assess their influence on recognition rates.The face images in Pointing’04—taken at (v,h) angles (90,0), (60,30), (30,−15), (−15,−60), (−15,90), (0,45), (15,0), (30,−45), (60,−90), and (60,60)—were used as the training image sets to train the classifiers. All the other 83 head-pose images were used to assess the performance of the classifiers. Table 1shows the recognition rate of each.The SVMs and NNs classifiers are more complex than the k-nearest classifier and usually deliver good recognition results. However, in our experiment, as shown in Table 1, different types of classifiers had little effect on the recognition results, which showed that MSA features are only slightly dependent on the types of classifiers.The low recognition rates of the three classifiers (40.7%, 42.9%, and 43.7%) are unsatisfactory with regard to our requirements on MSA features. Therefore, we carried out another experiment to investigate the reasons for the poor performance.We chose five sets of face images from the Pointing’04 database: person1, person2, person3, person4, and person5. Each set contained five different poses. The (v,h) angles for these were (0,0) (−90,0) (90,0) (0,−90) (0,90). Partial poses of the five subjects are shown in Fig. 3. We extracted the MSA features of each subject in the five different poses. The MSA features of different faces showed great similarity, as shown in Fig. 4.From Fig. 4, we can see that the MSA values for different subjects at different poses were almost identical because of the great similarity of human faces.Due to the great similarity of human faces, the classification performance of MSA features was poor (as shown in Table 1). Furthermore, we noticed the following in Fig. 4: the images of different subjects recorded similar MSA values, which renders classification difficult for any classifier. In pattern recognition field, the basic principle involves increasing the distinction among human faces on the premise of maintaining the invariance. Therefore, we adopt the classical PCA method for our algorithm. The function of PCA is to decorrelate the feature sets and reduce the number of required MSA dimensions. Dimension reduction is carried out such that only the PCA components with sufficiently large corresponding eigenvalues are passed to the classifiers.Following PCA, the values in Fig. 5that are close to zero present the non-principal components that contribute little to classification. The other values represent the principal components that are sufficiently distinct from one another for a classifier, all the while maintaining its invariance.Our proposed MSA+PCA algorithm is as follows.Algorithm: MSA+PCAStep 1: Calculate MSA valuesInput: I is a gray-scale image matrix obtained histogram equalization, and ind is a column vector containing the (α,β) values.1: Calculate the density function p1,p2,p3 of I.2: Calculate the probability density function P1, P2, P3 by a double convolution.3: Calculate the discrete Fourier transform pairs of P1, P2, P3 to obtain the resultant F.Output: F is a vector containing MSA values corresponding to the given coefficients (α,β) defined in the matrix ind.Step 2: Calculate PCA componentsInput: F.1: Load F.2: Construct the training datasets RD and the test datasets ED.3: Calculate the covariance matrix G of RD.4: Calculate the eigenvalue EV and eigenvector EC of G.5: Sort EC, which has the max EV; obtain the transformation matrix TM.6: RD×TM; obtain the primary components R1 for the training datasets.7: ED×TM; obtain the primary components R2 for the test datasets.Output: PCA components R1,R2.Step 3: Training and classificationInput: R.1: Load R.2: Train the k-nearest classifier with R1.3: Test the classifier with R2.4: Obtain the classification outputs g.Output: Classification results g.In this section, we theoretically analyze the performance of our proposed algorithm.Many researchers have adopted MSA features for simple target recognition, such as letters, road-oriented markings, objects from the Columbia Object Image Library (COIL)-100 database, etc. Because the human face includes rich textures, facial expressions, accessories, etc., we analyze the applicability of MSA features to multi-pose face recognition.Let us assume that a face image f(x) undergoes an affine transformation such that f′(x)=f(Tx+t), where T is a nonsingular matrix and t is a translation vector. Similarly, we define X0′,X1′,X2′ in an affine space:X0′=TX0+t,X1′=TX1+t,X2′=TX2+t,Uα,β′=α(X1′-X0′)+β(X2′-X0′)+X0′.Hence,Uα,β′=TUα,β+t,F′(α,β)=F(α,β).The MSA eigenvectors F(α,β) in the original image space and F′(α,β) in the affine image space are equivalent. Therefore, the transform coefficients F(α,β) are invariant against the affine transformation of the image coordinates. This provides a method to obtain affine-invariant features for an image function f. Since multi-pose faces in real life satisfy affine transformations, we can adopt the MSA transform in pose-variant face recognition tasks.We extracted the MSA features of the subjects shown in Fig. 1 and found that the MSA eigenvectors extracted showed satisfactory pose-invariance and robustness; i.e., for images of the same subject in different poses, the MSA features remained stable, as shown in Fig. 6.Given the image of a human face, we can calculate the MSA values by Eq. (2). By changing the (α,β) pairs, we can obtain an infinite number of affine-invariant MSA features. However, this raises the question of the (α,β) value to select with respect to an infinite number of (α,β) pairs.When projected to the (α,β) plane, the equality F(α,β)=F(β,α) causes diagonal symmetry with respect to the line α−β=0, i.e., symmetry in the direction of the β-axis with respect to the line α+2β=1, and that in the direction of the α-axis with respect to the line 2α+β=1. By using these symmetries, we can find small regions in the (α,β) plane, that essentially provide all MSA transform values. There are several possible values, one of which is shown in Fig. 7.Fig. 7 shows the probable region in the (α,β) plane. However, there still remain an infinite number of possible choices. The convoluted nature of the MSA transform produces a smooth continuous surface. Therefore, (α,β) pairs that lie in close proximity to one another provide highly correlated transform values.We adopt three sets of (α,β) pairs to observe their influence on MSA values. The first collection of sets, called pair1, is (0,−0.5), (0.4,0.8), (−0.2,0.9), and (0.3,−0.5); the second collection is (−1,1), (−0.8,0.8), (−0.6,0.6), (−0.6,0.8), (−0.4,0.6), (−0.4,0.4), (−0.2,0.2), (−0.2,0.4), (−0.2,0.6), (0,0.2), (0,0.4), (0.2,0.2), and (0.2,0.4), and is called pair2; the third collection consists of the 29 (α,β) pairs used by Rahtu et al., and is called pair3. The results in Fig. 8show that different (α,β) pairs only affect the MSA values and the size of the feature vectors.The Face Recognition Vendor Test (FRVT) 2002 [19] confirmed that illumination and pose variations are the two major problems plaguing existing face recognition systems. Thus, a pertinent question in the context of our research relates to the influence of illumination changes on MSA features.Suppose f(x):IR2→IR,f⩾0 is an image intensity function in IR2. Let x0,x1,x2 be three random points in f, then, uα,β=α(x1−x0)+β(x2−x0)+x0,where (α,β) are the coordinates of u in the space spanned by the vectors x1−x0 and x2−x0 with origin at x0.Now let A{T,t} be an affine transformation. If we take A to the three points x0, x1, x2, then x0′=Tx0+t, x1′=Tx1+t, x2′=Tx2+t, and u′α,β=α(x1′−x0′)+β(x2′−x0′)+x0′=α(Tx1−Tx0)+β(Tx2−Tx0)+Tx0+t.Thus, u′α,β=Tuα,β+t.Let x0,x1,x2 be samples of the random variables X0,X1,X2, and define a new random variable Uα,β=α(X1−X0)+β(X2−X0)+X0.Similarly,U′α,β=α(X1′−X0′)+β(X2′−X0′)+X0′, where X0′=TX0+t, X1′=TX1+t, X2′=TX2+t.Hence U′α,β=TUα,β+t. Substituting x=U′α,β, we obtainf′(Uα,β′)=f(T-1Uα,β′-T-1t)=f(T-1(TUα,β+T)-T-1t)=f(Uα,β).The MSA transformation treats an image as a probability density function, and the probability distribution remains stable when the image undergoes an affine transformation A{T,t}. Thus, the MSA transform is sensitive to illumination changes caused by affine transformation.We chose five images from the PIE database [20], which reflects extreme changes in illumination, called illumination1, illumination2, illumination3, illumination4, and illumination5, as shown in Fig. 9. We then extracted the MSA features of these images. The results are shown in Fig. 10. The illumination changes showed a significant effect on MSA values. The blue1For interpretation of color in Fig. 10, the reader is referred to the web version of this article.1curve represents the lightest face, whereas the pink curve represents the darkest one.We can typically reduce the illumination effect according to different lighting models. However, in real-world conditions, it is difficult to obtain a model for it. Therefore, we reduced the illumination effect by equalizing the histogram. The illumination effect can thus be correspondingly reduced, as shown in Fig. 11.The images in face recognition tasks are typically noisy [21]. Therefore, in this section, we study the influence of noise on MSA features.We first added some Gaussian noise (with 0 mean and 0.01variance) to the face in Fig. 1 using five different density values (0.01, 0.03, 0.05, 0.07, and 0.09), called noise1, noise2, noise3, noise4, and noise5, respectively. The noise density used was the probability of a pixel of being disturbed by noise.Following this, we repeated the same experiment with different types of noise—Gaussian noise, Salt-and-Pepper noise, Poisson noise, and Speckle noise—with a density of 0.05. The results in Figs. 12 and 13show that the MSA features remain invariant to noise of different densities and types.

@&#CONCLUSIONS@&#
In this paper, we proposed a new pose-variant face recognition algorithm based on MSA features and the PCA dimension-reduction method. The proposed algorithm was analyzed theoretically and assessed experimentally using four common multi-pose face databases.In the theoretical part of this paper, we proposed an MSA+PCA algorithm and analyzed its applicability to pose-variant face recognition tasks. We also evaluated the influence of (α,β) pairs, illumination changes, and noise on MSA features. The results of experiments using the ORL, GTAV, PIE, and Pointing’04 multi-pose face databases showed that the proposed algorithm attains higher recognition rates than prevalent pose-variant face recognition methods, and has a clear advantage of weakly dependence on the database and classifiers.In pattern recognition field, a balance is often required between performance and computational complexity. Higher performance usually implies higher computational complexity. The limitation of our proposed algorithm is that it is computationally expensive during MSA feature extraction because it involves more (α,β) pairs. During our experiment using MATLAB 7.8, the computation time for images of each subject in the Pointing’04 database (with a resolution of 384×288) was 427.486505s, given the 29 (α,β) pairs and standard personal computer configurations. The computation time for the PCA and subsequent classification was 5.014438s. The reduction of computational complexity will involve reducing the number of (α,β) pairs. However, this will result in lower recognition rates.In the future, we intend to investigate methods to mitigate the complexity of our proposed algorithm. Furthermore, we will apply the MSA+PCA algorithm to real-time multi-pose face recognition tasks by extracting MSA features in offline mode, and conducting training and classification in online mode.