@&#MAIN-TITLE@&#
An effective hybrid harmony search-based algorithm for solving multidimensional knapsack problems

@&#HIGHLIGHTS@&#
A harmony memory consideration rule is developed.Global-best pitch adjustment rule and parallel updating strategy are employed.The fruit fly optimization (FFO) scheme is integrated into the improved HS as a local search strategy.

@&#KEYPHRASES@&#
Multidimensional knapsack problem,Harmony search,Fruit fly optimization,Evolutionary algorithms,Binary algorithm,Meta-heuristic,

@&#ABSTRACT@&#
This study presents an effective hybrid algorithm based on harmony search (HHS) for solving multidimensional knapsack problems (MKPs). In the proposed HHS algorithm, a novel harmony improvisation mechanism is developed with the modified memory consideration rule and the global-best pitch adjustment scheme to enhance the global exploration. A parallel updating strategy is employed to enrich the harmony memory diversity. To well balance the exploration and the exploitation, the fruit fly optimization (FFO) scheme is integrated as a local search strategy. For solving MKPs, binary strings are used to represent solutions and two repair operators are applied to guarantee the feasibility of the solutions. The HHS is calibrated based on the Taguchi method of design-of-experiment. Extensive numerical investigations based on well-known benchmark instances are conducted. The comparative evaluations indicate the HHS is much more effective than the existing HS and FFO variants in solving MKPs.

@&#INTRODUCTION@&#
The multidimensional knapsack problem (MKP) is a well-known combinatorial optimization, which is also an NP-hard problem [1]. The objective is to maximize the total profit of the selected given items with all resource constraints satisfied. A variety of practical applications can be formulated as MKPs, for example, capital budgeting [1], cargo loading [2], resource allocating [3], cutting stock [4], etc. Thus, it is significant in the development of effective and efficient algorithms for solving MKPs.Exact algorithms were always applied to deal with MKPs in early studies, such as the branch and bound algorithm [2] and dynamic programming (DP) [5]. Because of the NP-hardness property of MKPs, these exact methods perform poorly when the scales come to be large, although they can produce optimal solutions in solving small-scale problems. In recent decades, some meta-heuristic algorithms have been employed to solve MKPs, including tabu search (TS) [6–8], genetic algorithm (GA) [9,10], simulated annealing (SA) [11], ant colony optimization (ACO) [12–15], particle swarm optimization (PSO) [16–18], and estimation of distribution algorithm (EDA) [19], etc.Harmony search (HS) firstly presented by Geem et al. [20] is one of new meta-heuristics. It imitates the musical improvisation process to search for a perfect state of harmony. Since firstly proposed, HS has drawn much attention and it has been successfully applied to various mathematical and engineering optimization problems [21–25]. For the 0–1 knapsack problems: Zou et al. [26] proposed a novel global harmony search algorithm (NGHS) in which position updating and genetic mutation operators are applied to enhance its ability. Wang et al. [27] considered various adaptive mechanisms in the basic HS algorithm and developed a scalable adaptive strategy to improve its robustness. Abdesslem Layeb presented another modified algorithm named QIHSA [28] where quantum representation was integrated to achieve a better balance between the exploration and exploitation.Inspired by the intelligent behavior of fruit flies in finding food, a new evolutionary optimization approach, namely fruit fly optimization algorithm (FFO), has recently proposed by Pan [29]. The FFO algorithm, because of its easy implementation and quick convergence, has been applied to diverse fields such as financial distress [30], power load forecasting [31] web auction logistics service [32] and PID controller tuning [36,37]. As for MKPs, to the best of our knowledge, there is only one research by Wang et al. [33], who proposed a binary fruit fly optimization algorithm (bFOA) in which three main search processes (smell-based process, search-based process and vision-based process) are designed to perform evolutionary search.Overall, despite identifying the high-performance regions in the search space well, the HS algorithm has difficulty in performing local search [22] whereas the FFO algorithm can be expert in local search but easily become trapped in local optima [31]. In view of these, in this paper, the FFO scheme is integrated into the HS as a local search strategy. During every evolution, HS operators are performed on the population group for global search and then each harmony in HM moves independently based on the FFO scheme to find an improving neighbor nearby. In this way, the resulting algorithm, named a hybrid harmony search-based algorithm (HHS), achieves good balance between the exploration capability of the HS algorithm and the exploitation ability of the FFO scheme. Additionally, combined with the parallel updating strategy, a novel improvisation rule with the modified memory consideration rule and the global-best pitch adjustment rule is developed to enhance the exploration ability of the HS operator. Furthermore, for solving MKPs, binary strings are used to represent the solutions, and two repair operators are applied to guarantee their feasibility. Finally, extensive numerical testing results and comparisons are provided to demonstrate the effectiveness of the proposed HHS.The rest of this paper is organized as follows: In Section 2, the basic HS and FFO algorithms are introduced, while the mathematical information of the MKP is also presented. Then, the details of the proposed HHS and two repair operators are presented in Section 3. In Section 4, the experimental design and comparisons are given. Finally, Section 5 presents the conclusion and identifies future work.In the basic HS algorithm, each solution, represented by a n-dimensional real-valued vector, is analogous to a harmony and stored in the harmony memory (HM). The basic HS algorithm is composed of five steps described in detail below:Step 1: Initialize the problem and algorithm parameters. Generally, the bound-constrained optimization problems can be defined as: Minimize f(X) subject tox(i)∈LBi,UBi,i=1,2,…,n. Here, f(X) is the objective function; X is the set of decision variables x(i); LBiand UBiare the lower and upper bounds for x(i), respectively, and n is the number of decision variables. The four critical parameters are initialized in this step, including harmony memory size (HMS), the harmony memory considering rate (HMCR), the pitch adjusting rate (PAR) and the bandwidth(bw).Step 2: Initialize the harmony memory (HM). The HM consists of HMS harmony vectors, and each decision variable is randomly generated as: hmi,j=LBi+r×(UBi−LBi), where i=1, 2, …, HMS, j=1, 2, …, n, and r is a random real-valued number between 0 and 1. Thus, the HM can be formulated in a matrix:(1)HM=hm1,1hm1,2⋯hm1,nhm2,1hm2,2⋯hm2,n⋮⋮⋯⋮hmHMS,1hmHMS,2⋯hmHMS,nStep 3: Generate a new harmony. The new harmony vector HMnew=(hmnew,1, hmnew,2, …, hmnew,n) is generated by applying three basic rules including memory consideration, pitch adjustment and random selection. Decision variables in the generating vector will be sequentially produced following these three rules.Firstly, in the memory consideration rule, the value of each decision variable is randomly chosen from the existing values stored in the HM with a probability HMCR or generated randomly from the possible range with a probability 1−HMCR. The memory consideration rule is given as follows:(2)hmnew,j=hma,jwith HMCR probabilityLBj+r×(UBj−LBj)with 1−HMCR probabilitywhere a is a random integer uniformly generated between 1 and n.Then each decision variable obtained from the HM should conduct a pitch adjustment rule with a probability PAR. The pitch adjustment rule is showed as follows:(3)hmnew,j=hmnew,j±r×bwwhere r is a random real-valued number between 0 and 1.Step 4: Update the harmony memory. The newly generated harmony is compared with the existing harmonies in HM and then replace the worst harmony if it has a better fitness value in terms of the objective function.Step 5: Check the termination criterion. The evolutionary search repeats from Step 3 to Step 4 until it meets the termination criterion and then output the best harmony vector in HM.The FFO mimics the food finding behavior of fruit fly swarms, and consists of two main foraging processes: smell-based foraging process and vision-based foraging process. In the smell-based foraging phase, a group of fruit flies smell the food sources and they are randomly located. Then, fruit flies use sensitive vision to find the best food source location and fly towards it in the vision foraging phase. The basic FFO was presented for financial distress [30]. It is adapted to solve high-dimensional optimization problems by Pan [34] as follows:Step 1: Initialize the fruit fly group location and set the control parameters. The fruit fly group location, gl=(l1, l2, …, ln), is the set of decision variables, while n is the number of decision variables. The location is randomly initialized in the search space with control parameters, including maximum iteration and population size.Step 2: Apply the smell-based process. A population size (PS) of food sources, Xi=(xi,1, xi,2, …, xi,n), are found which are generated around the current group location based on a random food finding distance (rd). This foraging phase procedure can be summarized as follows:for (i=1 to PS)for (j=1 ton)xi,j=lj±rdendforendforStep 3: Apply the vision-based process. Find the best food source location with best fitness value and then all fruit flies fly towards it. Thus, the current fruit fly swarm location is updated.Step 4: Check the stopping criterion. If the stopping criterion is reached, terminate the algorithm. Otherwise, repeat the implementation of Steps 2–4.Mathematically, the multidimensional knapsack problem can be formulated as follows [4]:(4)Maximize∑j=1ncjdj(5)Subject to∑j=1nri,jdj≤si,i=1,2,…,m,j=1,2,…,n(6)dj∈0,1,j=1,2,…,nwhere n and m are the number of all the given items and knapsack constraints, respectively. In Eq. (1), cjdenotes the profit of the jth item, and djindicates whether the jth item is selected into the knapsack. ri,jand sirepresent consumption of the jth item and capacity at the ith resource constraint in Eq. (2), respectively. As is shown in Eq. (3), if the jth item is selected into the knapsack, then dj=1; otherwise, dj=0. Generally, the MKP can be regarded as a statement of any 0–1 integer programming problem with non-negative coefficients and ri,j≤si,∑j=1nri,j>si.This section proposes an effective hybrid algorithm based on HS for MKPs. A novel improvisation process combined with the parallel updating strategy is presented to enhance the efficiency and exploration ability. The FFO search strategy is integrated to conduct a neighborhood-based search around each harmony in HM. In this way, both the exploration capability of the HS scheme and the exploitation ability of the FFO strategy are considered. Furthermore, two repair operators are introduced to guarantee the feasibility of solutions. The flowchart of the HHS is displayed in Fig. 1, and the details of the proposed algorithm are given below.To deal with MKPs, the HM is formed by HMS binary solutions where each bit of harmony vector is initialized stochastically to be “1” or “0”. As in the HM matrix shown in Eq. (4), hmi,j∈{0, 1}, in which i∈{1, 2, …, HMS}, j∈{1, 2, …, n}. “1” represents the corresponding item is selected into the knapsack, while “0” indicates the corresponding item is not selected. For example, a binary HM matrix can be displayed as follows:HM=HM1HM2⋮HMHMS=01⋯111⋯0⋮⋮⋯⋮10⋯0To improvise a new harmony, each element of it is independently generated by applying memory consideration rule, pitch adjustment rule and random selection. For MKPs, each bit of solutions has only two values, i.e., “0” and “1”, and the parameterbwin the pitch adjustment process in basic HS is not applicable. Accordingly, the memory consideration and pitch adjustment rules can be updated as follows:(7)hmnew,j=hma,j∈(0,1)ifr1<HMCRRelse(8)hmnew,j=hmnew,j−1ifr2<PARhmnew,jelsewhere hmnew,jis the jth bit of the generated harmony vector; a is a random integer belonging to [1, HMS]; R is a re-initialized number to be “0” or “1” with 50% probability, and r1 and r2 are independent random real-valued numbers between 0 and 1.According to Eqs. (7) and (8), it can be demonstrated that the improvisation process executes a mutation operation (“0”→“1” or “1”→“0”) with a total mutation probability(2×HMCR×PAR+1−HMCR)/2.On the one hand, although the mutation operation here can perform local search, its efficiency and effectiveness are not satisfactory [27]. On the other hand, we equip the HS algorithm with the FFO scheme, as it delivers good performance in local search. Consequently, we abandon the mutation operation property and further enhance the global search ability with the modified memory consideration rule and the global-best pitch adjustment rule.In the modified memory consideration rule, HMCR is the probability indicating whether the generated harmony HMnew=(hmnew,1, hmnew,2, …, hmnew,n) (not each bit of it) is chosen from the HM or re-initialized in the possible range. For the first case, the elements in HMnew are not determined by the same harmony vector in HM, and they are also sequentially and independently selected from different harmonies. Such a modification not only abandons the mutation operation validly, but also allows the algorithm to have more chance to search in a broader range for global optima based on the re-initialization strategy with 1−HMCR probability.In addition, the original pitch adjustment rule, which is also transformed into mutation operator mentioned above, spoils the performance in the binary HS algorithm, so that it was removed in [38]. In this paper, we retain this operator by employing the global-best pitch adjustment rule presented by Omran and Mahdavi [21,39]. Updating the decision variable by using the best harmony vector in HM, this modification eliminates the parameterbw,so it is properly suitable for binary problems. Moreover, this increases the search efficiency, since the search space around the best harmony vector could be the most promising region. The global-best pitch adjustment rule is displayed as follows:(9)hmnew,j=hmB,k,k∈[1,n]where k is an arbitrary integer between 1 and n, and HMBrepresents the best harmony vector in HM. It should be noted that hmnew,jis updated by randomly selecting among any of the decision variables in HMB. However, specially for MKPs, which have different global optimum value in different dimensions, such a modified pitch adjustment rule will break the building structure in HMB. Thus, to well retain good information stored in HMB, we modify the pitch adjustment rule as: hmnew,j=hmB,j, where hmnew,jis determined by the corresponding decision variable hmB,j.Furthermore, the parallel updating strategy [35] is combined with the harmony improvisation process. And with this strategy, the HM will be parallel updated by making use of the multiple newly generated harmonies in each iteration. Because the present worse harmony vectors in the HM are easily to be replaced by the newly generated ones, this strategy takes advantage of them in a bigger probability to create new solutions to enrich the harmony memory diversity, according to the fact that the evolutionary efficiency not only depends on the better individuals but also the worse ones.The computational improvisation scheme can be summarized as follows:for (k=1 togn)if (rand()<HMCR)for (i=1 ton) dotunek,i=hma,i,a∈(1, 2, …, HMS)if (rand()<PAR)tunek,i=hmB,iendifendforelsefor (i=1 ton) doifrand()<0.5tunek,i=1elsetunek,i=0endforendforwhere gn is the number of generating harmonies in the parallel updating strategy; tune is a two-dimensional array to store the generated harmony vectors; a is a stochastic integer between [1, HMS], while B is the index of the best harmony in current HM.According to the modified HS algorithm described above, it focuses much on the global exploration. To well balance the exploration ability and exploitation capability, the FFO scheme is performed on each harmony in HM after the HM is updated by the global HS-based search. The FFO strategy applied here contains the smell-based process and the vision-based process. As the initial group locations are binary harmony vectors, the traditional FFO should be specially designed to operate on them [33], and the main difference lies in the smell-based search process.In the smell-based search process, s neighbors are randomly generated through the mutation operation in values of L bits that are randomly selected. Consider a problem with eight items and L=3 for an example. Supposing the second, fourth, and seventh bits are randomly selected for mutations, the mutation operation can be illustrated in Fig. 2.After generating s neighbors in the smell-based search process, each harmony finds its best neighbor and then it will be replaced by the best neighbor if its value is worse and remain unchanged otherwise in the vision-based search process.The generated solutions may be illegal because resource constraints in MKPs may not be satisfied. In this paper, two repair operators are introduced here to guarantee the feasibility of the individuals.Repair operator 1 (RO1): The general technique is a greedy-like heuristic based on pseudo-utility ratios, which are calculated by the surrogate duality approach [40]. The surrogate relaxation problem of the MKP can be described as follows:(10)Maximize∑j=1ncjdj(11)subject to∑j=1n∑i=1mwiri,jdj≤∑i=1mwisi(12)dj∈0,1,j=1,2,…,nwherewi(i=1,2,…,m)is a set of positive real numbers used as a surrogate multiplier (or weight). These weight values can be viewed as dual variables in solving the LP relaxation of the original MKP. Specifically, the weight values also can be regarded as shadow prices of the ith constraint in the LP relaxation of the original MKP, and the pseudo-utility of each variable can be formulated as:uj=cj/∑i=1mwiri,j.Then, uj(j∈[1, n]) is sorted in descending order based on the pseudo-utility of each variable in it. This repair operator can be conducted in two phases: the DROP phase and the ADD phase. In the DROP phase, it examines each bit of the solution in the increasing order of ujand changes the value of bits from one to zero until the solution is feasible. Alternatively, the second phase, denoted DROP phase, examines each bit in decreasing order of ujand changes a bit from zero to one as long as the modified solution is feasible.Repair operator 2 (RO2): RO1 depends on solving the LP relaxation problem of MKP, so it is inefficient to solve MKP problems when m+n is large. Wang et al. [19] proposed a repair operator for solving MKP problems. Considering the specific knowledge of the MKP, it requires a matrix qi,j, which is the value of a unit volume denoted value ratio of the jth item in the ith constraint. qi,jcan be formulated as follows:(13)qi,j=cj/ri,j,i∈[1,m],j∈[1,n]This repair can be briefly described as follows: First, it uses an assistant matrix F, each row of which is an arrangement of all of the items in descending order of value ratio in all constraints. Second, in the DROP phase, it examines each bit and changes it from 1 to 0 in ascending order of qi,juntil the solution is legal. Third, it examines each bit and changes it from 0 to 1 in descending order of qi,juntil all constraints are legal in the ADD phase.The complete computational procedure of the presented HHS algorithm is outlined in Fig. 3.Three different sets of widely used benchmark instances which are all available from ORLIB are executed to evaluate the HHS. Test set 1 consists of eight small scale problems wit m=2–30 and n=15–105. Test set 2 contains 11 large-scale problems with m=15–100 and n=100–2500, while 20 medium-scale problems with m=5 or 10 and n=100 are included in test set 3. In addition, the algorithm is denoted as HHS1 when RO1 is employed and as HHS2 when RO2 is employed. All the compared algorithms are coded in Visual C++ and run on a 3.2GHz Intel Core i5 processor.The proposed HHS algorithm contains six parameters including hms, HMCR, PAR, gn, S and L. Among these, gn, S and L are not included in HS scheme, but they are closely related to hms in HHS. Therefore, in this subsection, the Taguchi method of design of experiment (DOE) [41] with instance Mk_gk06 in test set 2 is applied to investigate the influence of these four parameters on the performance of the algorithm. Different combinations of the values are given in Table 1.HHS1 is conducted 50 runs with 100,000 evaluations independently for each combination parameter values. To eliminate the disturbance of HMCR and PAR, they are set as constants recommended in [42], i.e., HMCR=0.74 and PAR=0.1. The orthogonal array L16 factor levels and the obtained average profits (AVG) are listed in Table 2. According to the obtained AVG values in Table 2, the trend of each factor level is illustrated in Fig. 4using the statistical analysis tool Minitab and the response values analyzing the significance rank of each parameter are given in Table 3.It can be clearly observed from Table 3 and Fig. 4 that all the four parameters significantly influence the performance of the HHS. More specifically, the algorithm with smaller S values and larger gn values achieves better performance. As for hms, when its factor level is equal to 4, the result is much inferior to the other three levels. It can be demonstrated that a small value of hms allows the algorithm to evolve in a deep search, while a large value favors the algorithm searching in a large domain. Parameter L is associated with local FFO search strategy which is beneficial to local exploitation when L is small. However, a too-small L value may be useless for the repair process. Fig. 4 indicates that the performance of the algorithm is significantly better when L=3.According to DOE test-based investigation, we set the parameters for test set 2 as hms=40, S=2, gn=20, L=3. These four parameters will be same set in test set 1, while for test set 3, parameter L is differently set as 10.Regarding the HS scheme in the HHS, two control parameters HMCR and PAR should be properly tuned, which strongly affect the performance of the HS algorithm [21,27,43,44]. A large HMCR value favors choosing values from the historic ones stored in the harmony memory, while a small HMCR value increases the diversity of the HM. The previous researches suggest a large value for HMCR, and our preliminary experiments indicate that our algorithm performs uniformly better when HMCR is set as 0.90. PAR is the adjustment rate for using the best harmony vector in HM. A small PAR value is required to help the algorithm search in the whole HM for global exploration in the early stage. In the final search phase, a large PAR value enhances the local exploitation ability around the best harmony vector. So, the PAR value is updated dynamically the same as in [43].(14)PAR(t)=PARmin+(PARmax−PARmin)×t/Max_genwhere t is the generation number, and PAR(t) is the pitch adjusting rate for generation t and PARmax and PARmin are the minimum and maximum adjustment rates, respectively. We set them as PARmin=0.01 and PARmax=0.99.Recently, Wang [33] proposed an algorithm named bFOA with better performance than the other existing approach [19]. To discuss the effect of two repair operators, they are separately applied in proposed HHS and bFOA. Same as [33], the maximum number of evaluations is set as 100,000 and each algorithm is run 20 times independently.Comparative results are summarized in Tables 4 and 5. HHS1 and bFOA1 are the algorithms united with RO1, and HHS2 and bFOA2 are the algorithms united with RO2. Min.Dev and Ave.Dev denote the minimum and average percentage deviations from best-known values, respectively; Var.Dev represents the variance of the deviations. Bold values in the tables denote better results.From Tables 4 and 5, in terms of Min.Dev, Ave.Dev and Var.Dev, our algorithm performs better than bFOA for all instances, not only with RO1 but also with RO2. And comparing RO2, both algorithms applying RO1 are superior in achieving better results. Apparently, our HHS1 is more powerful in solving the MKP than HHS2. Thus, in the following computational tests, we apply HHS united with RO1.In this subsection, the performance of the proposed HHS algorithm is investigated by comparing it with NGHS [26], ABHS [27] and bFOA [33]. To make the comparison as fair as possible, the repair operator RO1 is united in all four algorithms. Twenty independent replications are carried out for each instance, and the maximum number of evaluations for each replication is set to 100,000.Eight small-size instances in test set 1 are used to test the performance of the four algorithms in this subsection. Table 6presents comparative results where the AVG column is the average value and SR denotes the ratio of the running times hitting the best-known value of 20 independent runs.From Table 6, it can be seen that the AVG and SR obtained by these four algorithms can reach their best values for all of the instances except Weing7, Weing8 and Knap39. Compared to the other two HS variants for these three instances, the performance of our HHS is better, and especially for Weing8 and Knap39, the SR in our HHS reaches 100. However, as for Weing7, the proposed HHS is surpassed by bFOA in terms of the SR value. We conclude that the proposed HHS performs better than ABHS and NGHS on test set 1 instances.In this subsection, the proposed HHS is investigated using medium and large scale instances in test sets 2 and 3. We compare these four algorithms in terms of AVG, Min.Dev, Ave.Dev, and Var.Dev. Moreover, to investigate whether the results obtained by the HHS are significantly different from those of the other three algorithms, the z-test with 5% significance level is applied for the HHS against ABHS, NGHS, and bFOA, respectively. The h values displayed in the tables represent the results of z-tests. An h value of 1 or −1 indicates that the HHS is significantly better or worse than the compared algorithms, while an h value of 0 implies that HHS performances are not significantly different. The results obtained by the four algorithms and the z-test values are reported in Tables 7–12.The results on the test set 2 are reported in Table 7, and the z-test values are given in Table 8. It can be easily seen from Tables 7 and 8 that the proposed HHS performs best among the four compared algorithms. When compared to ABHS, the HHS obtains significantly better results for almost all test instances except MK_gk01 (where there is no statistically significant difference between the two algorithms). Compared to NGHS, the HHS outperforms for all instances, but NGHS is the only algorithm that finds the best-known value for MK_gk01. Comparing HHS with bFOA, the HHS produces seven significantly better and four competitive results.Tables 9 and 10 present the results on the test set 3 with m=5. It can be observed that the proposed HHS algorithm performs best among the four algorithms. Out of 10 instances, it yields 8, 10 and 9 significantly better and 2, 0, and 1 competitive results than the ABHS, NGHS and bFOA algorithms, respectively. As for the test set 3 instances with m=10, we can see from Tables 11 and 12 that the proposed HHS algorithm outperforms both NGHS and bFOA for all instances. While compared to ABHS, the HHS algorithm produces eight significantly better, one competitive and one significantly worse results. Thus, the experimental studies illustrate that overall the proposed HHS is superior to the existing ABHS, NGHS and bFOA algorithms on medium and large scale problems.

@&#CONCLUSIONS@&#
In this paper, a hybrid algorithm based on harmony search is proposed for solving multidimensional knapsack problems. In the proposed algorithm, a novel harmony improvisation rule combined with the parallel updating strategy is presented. Moreover, the FFO scheme is integrated into the improved HS as a local search strategy to well balance the exploration ability and exploitation capability. In experimental study, the influence of the two repair operators and parameter settings were investigated, and appropriate values were suggested. The performance of the HHS was extensively investigated by using a large number of widely used benchmark instances and the results demonstrate the effectiveness and robustness of the proposed HHS. Our future work will generalize the proposed HHS algorithm to solve flow shop scheduling problems [45,46].