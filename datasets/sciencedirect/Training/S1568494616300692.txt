@&#MAIN-TITLE@&#
An enhanced particle swarm optimization with levy flight for global optimization

@&#HIGHLIGHTS@&#
Enhanced PSO with levy flight.Random walk of the particles.High convergence rate.Provides solution accuracy and robust.

@&#KEYPHRASES@&#
Particle swarm optimization,Levy flight,Nature-inspired strategy,Global optimization,

@&#ABSTRACT@&#
Hüseyin Haklı and Harun Uguz (2014) proposed a novel approach for global function optimization using particle swarm optimization with levy flight (LFPSO) [Hüseyin Haklı, Harun U guz, A novel particle swarm optimization algorithm with levy flight. Appl. Soft Comput. 23, 333–345 (2014)]. In our study, we enhance the LFPSO algorithm so that modified LFPSO algorithm (PSOLF) outperforms LFPSO algorithm and other PSO variants. The enhancement involves introducing a levy flight method for updating particle velocity. After this update, the particle velocity becomes the new position of the particle. The proposed work is examined on well-known benchmark functions and the results show that the PSOLF is better than the standard PSO (SPSO), LFPSO and other PSO variants. Also the experimental results are tested using Wilcoxon's rank sum test to assess the statistical significant difference between the methods and the test proves that the proposed PSOLF method is much better than SPSO and LFPSO. By combining levy flight with PSO results in global search competence and high convergence rate.

@&#INTRODUCTION@&#
In the last few decades, many nature-inspired evolutionary algorithms have been developed for solving most engineering design optimization problems which are highly nonlinear, involving many design variables and complex constraints. Due to the capability of global search and less time consumption of metaheuristic algorithms are attracted very much now days to solve real world problems. Nature-inspired algorithms [3–5] imitate the behaviors of the living things in the nature, so they are also called as Swarm Intelligence (SI) algorithms. For example, Ant Colony Optimization (ACO) [6,9], imitates the food searching paths of ants in nature, Bees Algorithm [7] developed by Pham DT in 2005 imitate the food foraging behavior of honey bee colonies, particle swarm optimization (PSO), introduced by Eberhart and Kennedy [8] in 1995, simulates the social behavior of bird flock or fish school.The power of all popular metaheuristics comes from the fact that they imitate the best characteristics in nature. Two important characteristics are selection of the fittest and adaptation to the environment. These two properties are translated into exploitation and exploration. Exploitation directs to search around the current best solutions and select the best solutions, while exploration gives assurance that the algorithm can explore the search space efficiently.Among the several metaheuristic algorithms, particle swarm optimization (PSO) algorithm has been used a lot for the benefit of ease of implementation and small number of parameters to be controlled. It has been widely applied to solve many scientific and real-world problems [10–17]. However, PSO algorithm suffers from two major problems of trapping in local minima and premature convergence and decreased convergence rate in the later period of evolution. To overcome those problems, several variants of particle swarm optimization algorithms were developed in the literature. Shi and Eberhart [19] introduced an effective linearly time varying parameter called inertia weight into original PSO and they proved that modified PSO produces global optimum value by making use of global search capability. Liang et al. [18] used a new learning strategy to update particle's velocity by utilizing all other particles’ best experience and as a result of premature convergence was avoided by preserving the diversity of the swarm. Zhan et al. [20] used orthogonal learning strategy to guide the particles to fly in better directions based on particles’ own experience and its neighborhood's experiences. Xinchao [21] introduced a new velocity updating strategy based on the perturbed gbest which resulted the prevention of loss of swarm diversity. Wang et al. [22] utilized special velocity update procedure to prevent premature convergence problem. In [23], Tsoulos made modified velocity update method by adding stopping rule, similarity check and some local search. Ratnaweera et al. [24] proposed a novel particle swarm optimization algorithm called HPSO-TVAC in which social and cognitive part of the particles swarm were considered to estimate the particles’ velocity and particles are reinitialized whenever they are unable to explore the search space. Yang et al. [25] proposed a new velocity updating formula, where inertia weight is dynamically changed based on the generation and evolution state. In [26] authors used full information of the entire neighbourhood for guiding the particle to fly in better directions. Topological structure is used to determine particles’ neighbourhood in [27]. Liang et al. [28] utilized dynamic multiple swarms and small neighborhood's information to diversify the swarm. A new PSO algorithm based on fusion global-local-topology (FGLT-PSO) was proposed in [29]. Hüseyin Haklı and Harun U guz [1] proposed a novel particle swarm optimization algorithm with levy flight in which randomness phenomenon of levy flight and the advantages of original PSO algorithm are utilized. Levy flight is heavily applied with various global optimization methods such as cuckoo search algorithm [30–33], firefly algorithm [34], bat algorithm [35], etc. In our study, we enhance the particle swarm optimization algorithm with levy flight by using PSOLF method to update the particles’ velocity.The rest of the paper is organized as follows. In Section 2 a simple PSO algorithm is explained and in Section 3 Levy flight is presented. The proposed PSOLF algorithm is presented in Section 4 and conducted experiments and results are shown in Section 5. As a final, the paper is concluded with summary in Section 6.The particle swarm optimization (PSO) [8] is a population based stochastic optimization technique for the solution of continuous optimization problems. It is inspired by social behaviors in flocks of birds and schools of fish. In PSO, a set of software agents called particles search for good solutions to a given continuous optimization problem.Each particle is a solution of the considered problem. For a d-dimensional problem, each particle i maintains two vectors:Position vectorXi=[xi1,xi2…xid]andVelocity vectorVi=[vi1,vi2…vid].Each particle uses its own best experience (pbest) and the best experience of all particles (gbest) to choose how to move in the search space.For a d-dimensional search space,pbest of particle i is represented as:pbesti=[pi1,pi2,pi3,…,pid]gbest is represented asgbest=[g1,g2,g3,…,gd.].In practice, in the initialization phase each particle is given a random initial position and an initial velocity. The position of the particle represents a solution of the problem and has therefore a value, given by the objective function. While moving in the search space, particles memorize the position of the best solution they found. At each iteration of the algorithm, each particle moves with a velocity that is a weighted sum of three components: the old velocity, a velocity component that drives the particle towards the location in the search space where it previously found the best solution so far, and a velocity component that drives the particle towards the location in the search space where the neighbour particles found the best solution so far. The velocity and position of the particle i at each iteration are calculated as follows:(1)Vi(t+1)=ω×Vi(t)+c1×rand()⊕(pbesti−Xit)+c2×rand()⊕(gbest−Xit)(2)Xi(t+1)=Xi(t)+Vi(t+1)whereVi(t+1), velocity of particle i at iteration t+1;Vi(t), velocity of particle i at iteration t;Xit, position value of the ith particle at iteration t;Xi(t+1), position value of the ith particle at iteration t+1; c1, cognitive weighting factor; c2, social weighting factors are acceleration coefficients; rand (), stochastic components of the algorithm, which are in the interval [0,1]; ω, inertia weight; ⊕, element-by-element multiplication.c1 and c2 values are used to ensure that particles affected more either locally or globally. Inertia weight (ω) plays the important role in controlling effect of previous velocity increases of the particles. ω was added to PSO by Shi and Eberhart in 1998 [19] to provide the balance between exploitation and exploration. It is calculated using the following equation:(3)ω(t+1)=ωmax−ωmax−ωmintmax*twhere ωmax, maximum Inertia weight; ωmin, minimum inertia weight; tmax, maximum number of iteration; t, the current iteration.Usually, ωmax is set to 0.9 and ωmin is set to 0.4 for providing the balance between global search and local search. ω is allowed to a linearly decreasing over the course of search [19].After updating velocity and position, for each particle, Xmax and Xmin represent maximum and minimum values of search range, respectively is assigned. Then each particle's fitness values are calculated, and if necessary, pbest and gbest updates are performed, and the same procedure is continued until the stop criteria are provided.Levy flight follows [1,2,30–35]; the generation of random numbers with levy flight consists of two steps: the choice of a random direction and the generation of steps which obey the chosen levy distribution. Random walks are drawn from Levy stable distribution. This distribution is a simple power-law formula L(s)∼|s|−1−βwhere 0<β<2 is an index.Definition 5.1Mathematically, a simple version of Levy distribution can be defined as:L(s,γ,μ)=γ2πexp−γ2(s−μ)1(s−μ)3/2,if0<μ<s<∞0,ifs≤0whereμ parameter is location or shift parameter, γ>0 parameter is scale (controls the scale of distribution) parameter.In general, Levy distribution should be defined in terms of Fourier transform.F(k)=exp−αkβ,0<β≤2where α is a parameter within [−1,1] interval and known as skewness or scale factor. An index of o stability β∈(0, 2) is also referred to as Levy index. The analytic form of the integral is not known for general β except for a few special cases.For random walk, the step length S can be calculated by Mantegna's algorithm as(7)S=uv1/βwhere u andvare drawn from normal distributions. That is(8)u∼N(0,σu2),v∼N(0,σv2),where(9)σu=Γ(1+β)sin(πβ/2)Γ[1+β/2]β2β−1/21/βThen the step size is calculated by(10)step size=0.01×SHere the factor 0.01 comes from the fact that L/100 should the typical step size of walks where L is the typical length scale; otherwise, Levy flights may become too aggressive, which makes new solutions (even) jump out side of the design domain (and thus wasting evaluations).Researchers investigated particle swarm optimization algorithm to avoid the problem of premature convergence and trapping in local optima. Some researchers improved PSO's performance by designing different techniques for updating velocity. In order to improve the performance of PSO algorithm, this study aims at updating velocity using levy flight method. Similar to original PSO, initially particles are randomly distributed within the search space, fitness values of all particles are evaluated and particles’ pbest as well as the swarm gbest are found. Then for each particle, velocity and position are updated based on the random probability. The particles’ velocity and position are updated as in the original PSO by Eqs. (1) and (2), respectively with probability greater than or equal to 0.5. If the random value is less than 0.5, then particle's velocity is updated as given in the Eq. (12) and then particle's velocity becomes its position. By employing levy flight method in updating the particle's velocity, particle takes long jump towards its pbest and gbest thereby enhancing the diversity of the swarm and facilitating the algorithm to perform global exploration throughout the search space. In Levy flight method β parameter takes major role in distribution. By applying different values for β, the random distribution is changed differently. In our study, we choose constant value for β(i,e., 1.5).Loss of diversity is thus avoided by using random phenomenon of levy flight while updating the velocity. As the performance of PSO algorithm is enhanced by incorporating the advantages of random walk into the PSO, it improves particles’ positions in each iteration through high exploration and exploitation of the search space.In our study, ω is defined as(11)ω=0.1+0.8×1−iterationMaxiterThe next position of each particle computed in Eq. (13) is based on Eq. (12) which involves computing the Levy flight random walk on the current position and using the computed values of pbest (best position found by the particle)and gbest (best position found by the swarm).(12)Vi(t+1)=ω×Levywalk(Xit)+c1×rand()⊕(pbesti−Xit)+c2×rand()⊕(gbest−Xit)(13)Xi(t+1)=Vi(t+1)where(14)Levywalk(Xit)=Xi(t)+step⊕random(size(Xi))where(15)step=stepsize⊕Xi(t)and stepsize is the value obtained from Eq. (10). ⊕ represents element-by-element multiplication.After updating particles’ velocity and position, fitness value is calculated. If the fitness value for the new particle is better than its pbest fitness value, then update pbest. Otherwise pbest value is not updated. Then gbest for the entire swarm is found. Repeat the same procedure until maximum number of function evaluations is reached or the global optimum value is obtained. The pseudocode of the PSOLF algorithm is given in Fig. 1and flowchart is given in Fig. 2.In this section, the detailed evaluation of the proposed PSOLF algorithm is presented. For comparison, two other algorithms are used. They are standard PSO (SPSO) [36], recently developed by M. Omran, which is one of the state of the art of PSOs being trapped in local optima and another is LFPSO [1] proposed by Hüseyin Haklı. Since the proposed paper is an enhanced PSO algorithm with levy flight, we are comparing our proposed algorithm with the above two algorithms. The source code for SPSO is available at [36] and LFPSO program code was obtained from the corresponding author [1] upon request, thanks to him for providing the code. The goal of this paper is to improve the performance of PSO algorithm for solving both unimodal functions and multimodal functions.All the three algorithms SPSO, LFPSO, proposed PSOLF are executed in Matlab 8.2 with Windows OS environment using Intel Core i3, 3.30GHz, 3.41GB RAM. The population size is automatically calculated based on the dimension for SPSO. For more information [36] can be referred. The control parameter values for LFPSO are set as per the suggestions of the authors in [1]. The control parameter settings for the above algorithms are shown in Table 1.Twenty one benchmark test functions listed in Table 2are used for evaluating the performance of the proposed PSOLF algorithm. The selected benchmark functions are categorized into three categories such as unimodal, multimodal and rotated multimodal functions. The first category, unimodal functions having a single optimal solution, includes seven functions namely f1 (Sphere), f2 (Schwefel2.22), f4 (Noise), f15 (Sum Square), f16 (Step), f17 (Quartic) and f3 (Rosenbrock) is a simple unimodal in 2D or 3D search space but also can be considered as a multimodal function in high-dimensional cases. The second category, multimodal functions having two or more local optima, includes ten complex high-dimensional functions such as f5 (Schwefel2.26), f6 (Rastrigin), f7 (Ackley), f8 (Griewank), f9 (Penalized1), f10 (Penalized2), f18 (Levy), f19 (Schaffer), f20 (Alpine) and f21 (Non-continuous Rastrigin). The third category contains four rotated multimodal functions f11 (Rotated Schwefel), f12 (Rotated Rastrigin), f13 (Rotated Ackley) and f14 (Rotated Griewank). Table 2 gives global optimal solution (column 7), global optimal value (column 6), search space range and initialization range (column 5), dimension being used (column 4), function name and category (column 2) and formula (column 3) for each test function.For each test function the three algorithms SPSO, LFPSO and PSOLF are executed independently for about 50 times on nearly 30 and 50 dimensions. The mean, standard deviation and mean CPU time (in seconds) for each function test results over 50 runs at the maximum function evaluations (FEs) with dimension 30 and 50 are given in Tables 3 and 4, respectively. The best results produced by the algorithms for each test function are shown in boldface. For comparison to become easier and clear, result values below 10−18 are considered as 0.As given in Table 3 for the dimension 30, PSOLF algorithm gets optimal results for 16 benchmark functions among all 21 functions. PSOLF algorithm outperforms other two algorithms for the 11 benchmark functions (i.e) f4, f6, f7, f8, f11, f12, f13, f14, f19, f20, and f21. SPSO algorithm acquires better results than LFPSO, PSOLF on f3 function only whilst LFPSO obtains better results for the functions f5, f9, f10, and f18 functions (i.e) 4 out of the 21 functions. Both algorithms LFPSO and PSOLF get optimal results on f1, f15, f16 and f17 functions whilst SPSO and PSOLF get optimal results on f1, f2, f15 and f17 functions. In general, algorithm's performance in finding global optimal or near global optimal values using SPSO is 5 functions namely f1, f2, f3, f15 and f17, LFPSO is 8 functions such as f1, f5, f9, f10, f15, f16, f17, and f18, and PSOLF is 16 functions f1, f2, f4, f6, f7, f8, f11, f12, f13, f14, f15, f16, f17, f19, f20, and f21. SPSO performs well on Sphere and Schwefel2.22 which are simple unimodal functions. Since Rosenbrock is considered as multimodal function in high-dimensional search space, SPSO algorithm achieves better results 3.55E+00(2.41E+00) than 2.39E+01(2.47E−01), 2.69E+01(1.01E+00) which are obtained by LFPSO and PSOLF, respectively. SPSO algorithm achieves better results on function f10 1.51E−12(7.45E−12) than PSOLF. But SPSO algorithm fails to achieve optimum results for the multimodal and rotated multimodal functions as it gets stuck in local optima. While SPSO algorithm is being trapped in local minima for multimodal and rotated multimodal functions, LFPSO algorithm provides better solution than SPSO but this improvement is less when compared with PSOLF algorithm. The proposed PSOLF algorithm performs well for unimodal, multimodal and rotated multimodal functions except Rosenbrock, Schwefel2.26, Penalized1, Penalized2 and Levy functions.When examining the values given in Table 4 for the dimension 50, PSOLF still continues to achieve best results on f1, f2, f4, f6, f7, f8, f11, f12, f13, f14, f15, f16, f17, f19, f20, and f21 (i.e) 16 out of the 21 functions. PSOLF algorithm escapes the local minima and provides better result than SPSO and LFPSO for both unimodal and multimodal functions.For the 50 dimension, while SPSO algorithm performs better than the PSOLF algorithm for only function f11, both of the algorithms obtain optimum result for the function f1, f2, f15, and f17. For the remaining 11 benchmark functions, PSOLF algorithm gives better results than the SPSO algorithm. LFPSO achieves optimal results for the functions f3, f5, f9, f16, f17 and f18 but PSOLF algorithm performs well for the multimodal functions, unimodal functions and rotated functions also. SPSO algorithm gets stuck in the local minima while PSOLF algorithm escapes the local minima and obtains the better result than SPSO and LFPSO.In summary, proposed PSOLF algorithm performs much better than SPSO and LFPSO algorithms and reaches optimal solutions for most of the test functions. The small standard deviation values for the PSOLF algorithm seems to be robust as it obtains optimal solutions for all runs when compared to SPSO and LFPSO algorithms.In order to analyze the scalability of proposed PSOLF algorithm, PSOLF algorithm is tested with higher dimensions including 100, 500 and 1000. For conducting this experiment, functions f1, f2, f3, f4, f6, f8, f15, f16, f17, f19, f20 and f21 are used. The mean, standard deviation and mean CPU time (in seconds) values for 10 independent runs for the benchmark functions obtained by the algorithm are reported in Table 5. While increasing the dimension, it is revealed that as the dimension increases, the success of the PSOLF algorithm continues to give best solution. This is achieved by using the same parameter settings as used for above experiments and it does not require any increase in population size or number of function evaluations. Thus, it is concluded that PSOLF algorithm is insensitive to growing dimensions and has a superior scalability.In order to determine whether there is a statistical significance difference between SPSO and PSOLF and also between LFPSO and PSOLF, Wilcoxon's test is performed for 50 independent runs results. The test is conducted with significance level at α=0.05, and obtained results in terms of p-value, h-value and z-value are shown in Tables 6 and 7for each test function. As given in Table 6 for the functions from f1 to f11, there is a significant difference between SPSO and PSOLF algorithm's performance for dimension 30 and for dimension 50 excluding function f3. Similarly, results obtained by PSOLF algorithm is statistically significant in compared to LFPSO algorithm for dimension 30 and 50. It is also noted by z-value that SPSO algorithm performs well on functions f3 and f10 for dimension 30 and on function f3 for dimension 50 in compared to PSOLF algorithm and LFPSO's performance is better on functions f3, f5, f9 and f10 than PSOLF as we discussed earlier in Section 5.2. From the test values for functions f12–f21 given in Table 7, test results imply that there is a statistical significant difference between SPSO and PSOLF for dimension 30 and 50 except f18 whereas LFPSO and PSOLF are also statistically different for dimension 30 and 50 except f16 since for f16, both algorithms produce the same optimal result of 0 in all runs. Thus it shows that PSOLF's performance is better compared with SPSO and LFPSO.Figs. 3–5graphically present the comparison of convergence characteristics for the three methods SPSO, LFPSO and PSOLF in solving the benchmark test functions for dimension 30. Fig. 3 plots the convergence graphs for Sphere, Schwefel2.22, Rosenbrock, Noise, Schwefel2.26, Rastrigin, Ackley and Griewank functions. Fig. 4 plots the convergence behaviors for Penalized1, Penalized2, RotatedSchwefel2.26, RotatedRastrigin, Quartic, Levy, Schaffer and Alphine functions. Fig. 5 plots the convergence behaviors for RotatedAckley, RotatedGriewank, SumSquare, Step and Non-continuous Rastrigin functions.On Sphere function, all the three algorithms SPSO, LFPSO and PSOLF obtain best results. PSOLF indeed reaches optimal result of 0 at about 22,000 FEs. This shows that PSOLF algorithm converges quickly and also achieves the global optimal result as shown in Fig. 3(a).On Rastrigin and Griewank functions, PSOLF converge very fast at about 1350 and 1550 FEs respectively achieving optimal result of 0, as seen in Fig. 3(f) and (h). SPSO and LFPSO do not perform well and get stuck in local minima.On Schwefel2.22 function, SPSO does improve solution continually; LFPSO does not obtain optimal value for this function. PSOLF seems to be good in achieving optimal result 0 at about 41,100 FEs as shown in Fig. 3(b).On Rosenbrock function, SPSO converges quickly and improves solution subsequently. LFPSO initially improves and then gets trapped in local minima similarly PSOLF gets stuck in local minima, hence both algorithms reach close results as shown in Fig. 3(c).Even SPSO does not progress solution for unimodal Nosie and Ackley functions, LFPSO and PSOLF get better results as shown in Fig. 3(d) and (g), respectively.On Schwfel2.26 function, SPSO converges very fast but gets trapped in local minima and PSOLF improves solution slowly but gets stuck in local minima. LFPSO achieves better result than SPSO and PSOLF.As seen from Figs. 4 and 5, the proposed PSOLF algorithm obtains global optimum of 0 for unimodal, multimodal and rotated functions such as Quartic, Schaffer, Rotated Rastrigin, Alpine, Rotated Ackley, Rotated Griewank, Sum Square, Step and Non-continuous Rastrigin at about 11200, 5300, 1625, 39,750, 2375, 1525, 21,775, 325 and 1650 FEs, respectively. LFPSO algorithm also performs well on functions Quartic, Sum Square, and Step getting optimal or near optimal solutions at about 200,000, 200,000 and 71,680 FEs, respectively. SPSO reaches optimal value at about 200,000 and 200,000 FEs for functions Sum Square, and Step (simple unimodal functions) respectively. On levy, Penalized1 functions, LFPSO outperforms SPSO and PSOLF which are converging very fast and getting stuck in local minima as shown in Fig. 4(n) and (i).On Penalized2 function, both algorithms LFPSO and SPSO achieves optimal result of 0 at about 64,880, 4860 FEs respectively, whilst PSOLF performs poorly on this function, as seen in Fig. 4(j).As a conclusion on examining the convergence behavior of the three methods, the proposed PSOLF algorithm offers best performance, converges very fast and achieves optimal results quickly.In order to analyze the computational complexity of the proposed algorithm, the average time required in reaching the optimal result or maximum number of functions evaluations on the test functions is given in Tables 3–5. As PSOLF algorithm converges quickly, it takes less than 3s for the most of the functions. On seeing Table 5, as dimension increases, PSOLF does not take much time and it scales with O(n) where n is the problem dimension.Table 8shows the results obtained by the proposed PSOLF algorithm and various PSO variants such as CLPSO [18], HPSO-TVAC [24], FIPSO [26], SPSO-40 [36], LPSO [27], DMS-PSO [28], LFPSO [1]. The results in Table 8 are obtained from 25 experimental results with dimension 30. Results of the Algorithms CLPSO [18], HPSO-TVAC [24], FIPSO [26], SPSO-40 [36], LPSO [27], DMS-PSO [28], are directly from [20] and LFPSO [1] results’ from [1]. For SPSO-40, the number of particle is not found automatically, the number of particles is set to 40. The results given in Table 8 are displayed without error rate.When examining Table 8, the proposed PSOLF algorithm reaches the optimal solution and PSOLF algorithm works well compared to the other seven stated algorithms and got the first rank.

@&#CONCLUSIONS@&#
