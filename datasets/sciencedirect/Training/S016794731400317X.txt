@&#MAIN-TITLE@&#
Mixture-based clustering for the ordered stereotype model

@&#HIGHLIGHTS@&#
New methodology for clustering rows and columns from a matrix of ordinal data.Establishes likelihood-based methods via finite mixtures with the stereotype model.Tests the reliability of this methodology through a simulation study.Illustrates this new approach with two examples.Reviews and compares the performance several model choice measures.

@&#KEYPHRASES@&#
Biclustering,Cluster analysis,Dimension reduction,EM-algorithm,Finite mixture model,Fuzzy clustering,Likert scale,Ordinal data,Stereotype model,

@&#ABSTRACT@&#
Many of the methods which deal with the reduction of dimensionality in matrices of data are based on mathematical techniques such as distance-based algorithms or matrix decomposition and eigenvalues. Recently a group of likelihood-based finite mixture models for a data matrix with binary or count data, using basic Bernoulli or Poisson building blocks has been developed. This is extended and establishes likelihood-based multivariate methods for a data matrix with ordinal data which applies fuzzy clustering via finite mixtures to the ordered stereotype model. Model-fitting is performed using the expectation–maximization (EM) algorithm, and a fuzzy allocation of rows, columns, and rows and columns simultaneously to corresponding clusters is obtained. A simulation study is presented which includes a variety of scenarios in order to test the reliability of the proposed model. Finally, the results of the application of the model in two real data sets are shown.

@&#INTRODUCTION@&#
An ordinal variable is one with a categorical data scale which describes order, and where the distinct levels of such a variable differ in degree of dissimilarity more than in quality (Agresti, 2010). This is different from nominal variables which vary in quality, not in quantity, and thus the order of listing the categories is irrelevant. For example, Likert scale responses in a questionnaire might be “disagree”, “neither agree nor disagree” or “agree”. In his seminal paper, Stevens (1946) called a scale ordinal if “any order-preserving transformation will leave the scale form invariant”. Although the collection and use of ordinal variables is common, most of the current methods for analyzing them treat the data as if they were nominal (Hoffman and Franke, 1986) or continuous data (Agresti, 2010). On the one hand, treating an ordered categorical variable as ordinal rather than nominal provides advantages in the analysis such as simplifying the data description and allowing the use of more parsimonious models. The nominal approach ignores the intrinsic ordering of the data and thus the statistical results are less powerful than they could be. On the other hand, models for continuous variables have similarities to those for ordinal variables although the use of them with ordinal variables has disadvantages such as the treatment of the output categories as equally spaced, which they may not be (see Agresti, 2010, Sections 1.2–1.3 for a list of advantages from treating an ordinal variable as ordinal rather than nominal or continuous).Categorical data analysis methods were first developed in the 1960s and 1970s (Bock and Jones, 1968; Snell, 1964), including loglinear models and logistic regression (see the review by Liu and Agresti, 2005). An increasing interest in ordinal data has since produced the articles by Goodman (1979) and McCullagh (1980) on loglinear modeling relating to ordinal odds ratios, and logit modeling of cumulative probabilities respectively. Recently, new ordinal data analysis methods have been introduced such as the proportional odds model version of the cumulative logit model, and the stereotype model with ordinal scores (Agresti, 2010, Chap. 3 and 4) from which new lines of research have developed. Two recent examples of these are the application of a stereotype model in a case-control study by Ahn et al. (2009), and a new methodology to fit a stratified proportional odds model by Mukherjee et al. (2008). In particular, the stereotype model is a paired-category logit model which is an alternative when the fit of cumulative logits and adjacent-categories logit models in their proportional odds version is poor. Anderson (1984) proposed this model as nested between the adjacent-categories logit model and the standard baseline-category logits model (see the review by Agresti, 2002, Chapter 6).In the research literature, multiple algorithms and techniques have been developed which deal with the clustering of data such as hierarchical clustering (Johnson, 1967; Kaufman and Rousseeuw, 1990), association analysis (Manly, 2005) and partition optimization methods such as thek-means clustering algorithm (Jobson, 1992; Lewis et al., 2003; McCune and Grace, 2002). There has been research on cluster analysis for ordinal data based on latent class models (see Agresti and Lang, 1993; Moustaki, 2000; Vermunt, 2001; DeSantis et al., 2008; Breen and Luijkx, 2010; McPartland and Gormley, 2013 and the review by Agresti, 2010, Section 10.1). There are a number of clustering methods based on mathematical techniques such as distance metrics (Everitt et al., 2001), association indices (Wu et al., 2008; Chen et al., 2011), matrix decomposition and eigenvalues (Quinn and Keough, 2002; Manly, 2005; Wu et al., 2007). However, these do not have a likelihood based formulation, and do not provide a reliable method of model selection or assessment. A particularly powerful model-based approach to one-mode clustering based on finite mixtures, with the variables in the columns being utilized to cluster the subjects in the rows, is provided by McLachlan and Basford (1988), McLachlan and Peel (2000), Everitt et al. (2001), Böhning et al. (2007), Wu et al. (2008) and Melnykov and Maitra (2010).The simultaneous clustering of rows and columns into row clusters and column clusters is called biclustering (or block clustering or two-mode clustering). Biclustering models based on doublek-means have been developed in Vichi (2001) and Rocci and Vichi (2008). A hierarchical Bayesian procedure for biclustering is given in DeSarbo et al. (2004). Biclustering using mixtures has been proposed for binary data in Pledger (2000), Arnold et al. (2010) and Labiod and Nadif (2011), and for count data in Govaert and Nadif (2010). An approach via finite mixtures for binary and count data using basic Bernoulli or Poisson building blocks has been developed in Govaert and Nadif (2010) and Pledger and Arnold (2014). This work expanded previous research for one-mode fuzzy cluster analysis based on finite mixtures (McLachlan and Basford, 1988; McLachlan and Peel, 2000; Everitt et al., 2001) to a suite of models including biclustering. Finally, Matechou et al. (2011) have recently developed biclustering models for ordinal data using the assumption of proportional odds and having a likelihood-based foundation. The main difference with our work is that we use the assumption of ordinal stereotype model which has the advantage of allowing us to determine a new spacing of the ordinal categories, dictated by the data.In this article, we present an extension of the likelihood-based models proposed in Pledger and Arnold (2014) by applying them to matrices with ordinal data by using finite mixtures to define a fuzzy clustering. We use the ordered stereotype model introduced by Anderson (1984) in order to formulate the ordinal approach, which has rarely been used so far. Two possible reasons for this lack of use might be the absence of standard software for model fitting and its unusual structure including the product of parameters in the linear predictor (Kuss, 2006). The plan of the article is as follows. Section  2 has definitions of the models and its formulation including fuzzy clustering via finite mixtures. Model fitting by using the iterative EM algorithm is described in Section  3. Section  4 presents a review of several model comparison measures and a comparison of eleven information criteria performance. Two real-life examples and simulation studies are given in Section  5, and we conclude with a discussion in Section  6.In this section, we give the standard definition of the ordered stereotype model (Section  2.1) followed by a modification to include clustering (Section  2.2). The likelihood for the suite of basic models is provided next (Section  2.3).For a set ofmordinal response variables each withqcategories measured on a set ofnunits, the data can be represented by an×mmatrixYwhere, for instance, thenrows represent the subjects of the study and themcolumns are the different questions in a particular questionnaire. Although the number of categories might be different, we assume the sameqfor all such questions. If each answer is a selection fromqordered categories (e.g. strongly agree, agree, neutral, disagree, strongly disagree), thenyij∈{1,…,q},i=1,…,n,j=1,…,m.The ordered stereotype model (Anderson, 1984) for the probability thatyijtakes the categorykis characterized by the following log odds(1)log(P[yij=k∣x]P[yij=1∣x])=μk+ϕkδ′x,i=1,…,n,j=1,…,m,k=2,…,q,where the inclusion of the following monotone increasing constraint(2)0=ϕ1≤ϕ2≤⋯≤ϕq=1preserves the variable responseYis ordinal (see Anderson, 1984). The vectorxis a set of predictor variables which can be categorical or continuous, and the vector of parametersδrepresents the effects ofxon the log odds of the response variable for the categorykrelative to the baseline category. The first category is the baseline category,pis the number of covariates, the parameters{μ2,…,μq}are the cut points, and{ϕ2,…,ϕq}are the parameters which can be interpreted as the “scores” for the categories of the response variableyij. We restrictμ1=ϕ1=0andϕq=1to ensure identifiability. With this construction, the category response probabilities in the ordered stereotype model are as follows(3)P[yij=k∣x]=exp(μk+ϕkδ′x)∑ℓ=1qexp(μℓ+ϕℓδ′x)fork=1,…,q,where the probability for the baseline category, as defined in (3), satisfiesP[yij=1∣x]=1−∑ℓ=2qP[yij=ℓ∣x]and therefore, sinceμ1=ϕ1=0, this probability can be defined asP[yij=1∣x]=11+∑ℓ=2qexp(μℓ+ϕℓδ′x).Greenland (1994) showed that the stereotype model is appropriate when the progression of the response variable occurs through various stages. Agresti (2010) (see Chapter 4) showed that the stereotype model is equivalent to an ordinal model, such as the proportional odds version of the adjacent-categories logit model, when the scores{ϕk}are a linear function of the different categories of the response variable. An advantage of the stereotype model is that it is more parsimonious than the baseline-category logit model or the multinomial logistic regression model. In addition, the ordered stereotype model is more flexible than the models including the proportional odds structure such as the version for the cumulative logit model (Agresti, 2010, Section 4.3.4) as a result of the{ϕk}parameters. However, the parameters are more difficult to estimate due to the intrinsic nonlinearity which arises from the product of parametersϕkδ′xin the predictor.The structure of the linear predictor in the ordered stereotype model can include the predictor variablesxas numerical covariates, or they may simply be related to the effect of the row and column on the observationyij. We consider this latter situation and build upδ′xonly taking into account the row and column effects by using a linear formulation. To do this, we define{α1,…,αn}and{β1,…,βm}as the sets of parameters quantifying the main effects of thenrows andmcolumns respectively, and the set{γ11,…,γnm}are the associations between the different rows and columns. In this way, we can formulate the following saturated model(4)log(P[yij=k]P[yij=1])=μk+ϕk(αi+βj+γij),k=2,…,q,i=1,…,n,j=1,…,m,where∑i=1nαi=∑j=1mβj=0and we impose sum-to-zero constraints on each row and column of the association (or pattern detection) matrixγ. This model has2q+nm−4independent parameters. The relationship between models (3) and (4) is shown in Appendix A. The most common submodels to formulate from the saturated model are the main effect model (γij=0, with2q+n+m−5parameters), the row effect model (βj=γij=0,2q+n−4parameters), the column effect model (αi=γij=0,2q+m−4parameters) and the null model (αi=βj=γij=0,2q−3parameters).The main problem with the model in (4) is that the specific row and column effects in this suite of models over-parametrizes the data structure. This model is not parsimonious and it requires a lot of parameters for describing all the effects. A way to reduce the dimensionality of the problem is to introduce fuzzy clustering via finite mixtures. Hence, we obtain the following model formulation including row clustering, column clustering or biclustering.•Row clusteringlog(P[yij=k∣i∈r]P[yij=1∣i∈r])=μk+ϕk(αr+βj+γrj),k=2,…,q,r=1,…,R,j=1,…,m.Column clusteringlog(P[yij=k∣j∈c]P[yij=1∣j∈c])=μk+ϕk(αi+βc+γic),k=2,…,q,i=1,…,n,c=1,…,C.Biclusteringlog(P[yij=k∣i∈r,j∈c]P[yij=1∣i∈r,j∈c])=μk+ϕk(αr+βc+γrc),k=2,…,q,r=1,…,R,c=1,…,C,Finally, in the same way as before, we can formulate the probability of the data responseyijbeing equal to the categorykconditional on the appropriate clustering as,•Row clustering(5)θrijk=P[yij=k∣i∈r]=exp(μk+ϕk(αr+βj+γrj))∑ℓ=1qexp(μℓ+ϕℓ(αr+βj+γrj)),k=1,…,q,r=1,…,R,j=1,…,m.Column clustering(6)θicjk=P[yij=k∣j∈c]=exp(μk+ϕk(αi+βc+γic))∑ℓ=1qexp(μℓ+ϕℓ(αi+βc+γic)),k=1,…,q,c=1,…,C,i=1,…,n.Biclustering(7)θricjk=P[yij=k∣i∈r,j∈c]=exp(μk+ϕk(αr+βc+γrc))∑ℓ=1qexp(μℓ+ϕℓ(αr+βc+γrc)),k=1,…,q,r=1,…,R,c=1,…,C.In this section, we summarize the likelihood functions for the cases of row clustering, column clustering and biclustering. The formulation of the complete data log-likelihood is given in each case.As we noted in the previous section, the unknown data in the case of the row-clustered model is the actual membership of the rows among theRrow-clusters. Thus, the incomplete data likelihood only sums over all possible partitions of rows intoRclusters:L(Ω∣{yij})=∑r1=1R⋯∑rn=1Rπr1⋯πrn∏i=1n∏j=1m∏k=1q(θrijk)I(yij=k),whereΩis the parameter vector for the case of row clustering,πriis the a priori row membership probability of rowi,θrijkis the probability of the data response defined in (5). Assuming independence among rows and, conditional on the rows, independence over the columns, we can simplify the previous incomplete data likelihood toL(Ω∣{yij})=∏i=1n[∑r=1Rπr∏j=1m∏k=1q(θrjk)I(yij=k)].We define the unknown row group memberships through the following indicator latent variables,(8)Zir=I(i∈r)={1ifi∈r0ifi∉ri=1,…,n,r=1,…,R,wherei∈rindicates that rowiis in row groupr. It follows that∑r=1RZir=1,i=1,…,n,and since their a priori row membership probabilities are{πr}(Zi1,…,ZiR)∼Multinomial(1;π1,…,πR),i=1,…,n.These indicator latent variables fulfill the following convenient identity∏r=1RaiZir=∑r=1RaiZirfor anyai≠0.Consequently, the complete data log-likelihood of this model using the known data{yij}and the unknown data{zir}is as follows(9)lc(Ω∣{yij},{zir})=∑i=1n∑r=1Rzirlog(πr)+∑i=1n∑j=1m∑k=1q∑r=1RzirI(yij=k)log(θrjk).The model for the case of clustering the columns but not the rows is similar. It assumes independence among columns and, conditional on the columns, independence over the rows. Analogous toZirfor row clustering (see (8)) we define the following indicator latent variables for the unknown data(10)Xjc=I(j∈c)={1ifj∈c0ifj∉cj=1,…,m,c=1,…,C.The complete data log-likelihood of this model using the known data{yij}and the unknown data{xjc}is as follows(11)lc(Ω∣{yij},{xjc})=∑j=1m∑c=1Cxjclog(κc)+∑i=1n∑j=1m∑k=1q∑c=1CxjcI(yij=k)log(θick),whereΩis the parameter vector for the case of column clustering andκcis the a priori column membership probability.In the case of clustering the rows and the columns simultaneously, the incomplete data likelihood sums over all possible partitions of rows intoRclusters and over all possible partitions of columns intoCclusters, and is given byL(Ω∣{yij})=∑c1=1C⋯∑cm=1Cκc1⋯κcm∑r1=1R⋯∑rn=1Rπr1⋯πrn∏i=1n∏j=1m∏k=1q(θricjk)I(yij=k).HereΩis the parameter vector for the case of biclustering andθricjkis the probability of the data response expressed in (7). Assuming independence among rows and, conditional on the rows, independence over the columns, we can simplify the previous incomplete data likelihood to(12)L(Ω∣{yij})=∑c1=1C⋯∑cm=1Cκc1⋯κcm∏i=1n[∑r=1Rπr∏j=1m∏k=1q(θrjk)I(yij=k)],which sums over the possible column cluster partitions. Similarly, if we assume independence among columns and, conditional on the columns, independence over the rows, we obtain the following simplified expression:(13)L(Ω∣{yij})=∑r1=1R⋯∑rn=1Rπr1⋯πrn∏j=1m[∑c=1Cκc∏i=1n∏k=1q(θick)I(yij=k)].We define the unknown data through the indicator latent variables described in (8) and (10). Consequently, the complete data log-likelihood of this model using the known data{yij}and the unknown data{zir}and{xjc}is as follows:(14)lc(Ω∣{yij},{zir},{xjc})=∑i=1n∑j=1m∑k=1q∑r=1R∑c=1CzirxjcI(yij=k)log(θrck)+∑i=1n∑r=1Rzirlog(πr)+∑j=1m∑c=1Cxjclog(κc).We estimate the MLEs from this expression by using the EM algorithm. In the E-step, the expected value of the first term is approximated using the variational approximation employed by Govaert and Nadif (2005) (see Appendix C for details). With the aim of ensuring a solution avoiding approximations, we use the resulting MLEs from the EM algorithm as starting points to numerically maximize the incomplete-data log-likelihood (12) (or (13)). We note that during the maximization a convenient transformation for the row and column membership parameters{πr}and{κc}issr=logit(πr/∑ℓ=rRπℓ)forr=1,…,R−1andqc=logit(κc/∑ℓ=cCκℓ)forc=1,…,C−1respectively. This transformation means that the parameterssrandqcare unconstrained, taking values over the whole real line.In this section, we develop a model fitting procedure using the EM algorithm (Dempster et al., 1977; McLachlan and Krishnan, 1997). One of the most common uses of the EM algorithm is in the case of the estimation of the parameters for a finite mixture-density model with incomplete data which in this case is the actual unknown cluster membership of each row and/or column. This method performs a fuzzy assignment of rows and/or columns to clusters based on the posterior probabilities. In this section, we develop this in detail for the case of clustering the rows but not the columns. It has a easy interpretation which helps explain of our methodology. The development for other two cases: clustering the columns but not the rows and biclustering are described in the Appendices B and C.We apply the E-Step in the EM algorithm by considering theZiras latent variables. In this manner, we use their a priori probabilities{πr}and the current values for the parameters so as to evaluate their expected values,Ẑir, which are the posterior probabilities that rowiis a member of row groupr. The conditional expectation of the complete data log-likelihood at iterationtcan be expressed as follows(15)Q(Ω∣Ω(t−1))=E{zir}∣{yij},Ω(t−1)[ℓc(Ω∣{yij},{zir})]=∑i=1n∑r=1Rlog(πr(t−1))E[zir∣{yij},Ω(t−1)]+∑i=1n∑j=1m∑k=1q∑r=1RI(yij=k)log(θrjk(t−1))E[zir∣{yij},Ω(t−1)].The latent variableZiris a Bernoulli random variable so thatE[zir∣{yij},Ω(t−1)]=P[zir=1∣{yij},Ω(t−1)],and applying Bayes’ rule to this expression we obtain(16)Ẑir(t)=P[zir=1∣{yij},Ω(t−1)]=P({yij},Ω(t−1)∣zir=1)P(zir=1)∑ℓ=1RP({yij},Ω(t−1)∣ziℓ=1)P(ziℓ=1)=π̂r(t−1)∏j=1m∏k=1q(θ̂rjk(t−1))I(yij=k)∑ℓ=1R{π̂ℓ(t−1)∏j=1m∏k=1q(θ̂ℓjk(t−1))I(yij=k)}.This is the expected value of the latent variableZirwhich defines the posterior probability that rowiis in groupronce we have observed{yij}. Finally, we complete the E-step by substituting the previous expression in the complete data log-likelihood at the iterationtexpressed in (15),(17)Q̂(Ω∣Ω(t−1))=∑i=1n∑r=1RẐir(t)log(π̂r(t−1))+∑i=1n∑j=1m∑k=1q∑r=1RẐir(t)I(yij=k)log(θ̂rjk(t−1)).The M-step of the EM algorithm is the global maximization of the previous expression (17) obtained in the E-step. For the case of finite mixture models, the updated estimation of the term containing the row-cluster proportions{π1,…πR}and the one containing the rest of the parametersΩare computed independently. Thus, the M-step has two separate parts.Finally, the maximum-likelihood estimator for the parameterπrin the case that the indicator variables{Z1r,…,Znr}were observable isπ̂r=1n∑i=1nzir,r=1,…,R.However, the datazirare unobserved in our case. In that manner, we use their conditional expectation which we found in the E-step (16) to replace in the previous expression for the iterationt,(18)π̂r(t)=1n∑i=1nE[zir∣{yij},Ω(t−1)]=1n∑i=1nẐir(t),r=1,…,R.To estimate the remaining parametersΩ, we must numerically maximize the conditional expectation of the complete data log-likelihood (15). In the case of row clustering,Ω̂=argmaxΩ[∑i=1n∑j=1m∑k=1q∑r=1RẐirI(yij=k)log(θrjk)],where the maximization is conditional on the constraints on the parameters. We repeat the two step iteration of the EM algorithm until convergence, that is until there is a small relative change in the likelihood between two consecutive iterations:‖L(Ω(t+1)∣{yij})−L(Ω(t)∣{yij})‖‖L(Ω(t)∣{yij})‖≈0.A disadvantage of mixture modeling is that the associated likelihood surface may be multimodal. A comprehensive search over different starting points is used to avoid finding only a local maximum. Particularly in our case, the iterative process is repeated 10 times with random starting points and the best MLE (those that lead to higher log-likelihood value) are kept. We have run experiments testing up to 100 random starting points and it was sufficient with 10 repetitions to avoid convergence to local optima.Finally, we have implemented the EM algorithm for the ordered stereotype model including clustering via finite mixtures and set up the simulation study by using the statistical package R 2.15.1 (R Development Core Team, 2010). The maximization was carried out by using the quasi-Newton method provided as an option in optim().The increasing constraint that enforces the scoresϕ1,…,ϕqto be increasing in the stereotype model defined in (2) must be imposed during the estimation procedure. Such a constraint is complex to impose during optimization and hence for convenience we reparametrizeϕ1,…,ϕqas follows.We first setνk=logit(ϕk)fork=2,…,q−1, which implies that−∞≤ν2≤ν3≤⋯≤νq−1≤∞.We then setνk=νk−1+eukfor−∞<uk<∞,k=3,…,q−1.In that manner, our parameter vector{ϕ1=0,ϕ2,…,ϕq−1,ϕq=1}is replaced with{ν2,u3,…,uq−1}which has the same number of parameters but it is more convenient because the new parameter vectorRq−2is completely unconstrained. This makes the optimization process more straightforward. Once we find the MLEs ofν2,u3,…,uq−1, we can transform back to the original set of parameters byϕk={0k=111+e−ν2k=2expit[logit(ϕ2)+∑ℓ=3keuℓ]k=3,…,q−11k=q,whereexpit(x)=(1+e−x)−1is the inverse of the logit function.There are two main approaches to the comparison of a set of candidate likelihood-based models once they are fitted, in order to decide which one (or group of them) best approximates the (unknown) true model. One approach is to carry out a hypothesis test by using the likelihood ratio as a test statistic (LRT). Another approach uses information criteria which are based on a penalized form of the likelihood function where the penalty increases as the number of parameters in the model increases. Some of the most common information criteria measures are Akaike’s Information Criterion (AIC, Akaike, 1973), its small-sample modification (AICc, Akaike, 1973; Hurvich and Tsai, 1989; Burnham and Anderson, 2002), the Bayes’ Information Criterion (BIC, Schwarz, 1978) or its Integrated Classification Likelihood version (ICL-BIC, Biernacki et al., 1998).Unlike the LRT, information criteria quantify the differences in goodness of fit between a set of candidate likelihood-based models (comparative measure of fit), but give no absolute measure of fit. The use of LRT is more computationally demanding than the information criteria because the LRT requires bootstrapping to obtain thep-value. This quantification of the significance is not assessed with the information criteria. However, the LRT does not lead to a suitable significance test in our approach. This occurs because regularity conditions do not hold for−2log(LRT)to have its usual asymptotic distribution under the null hypothesis for mixture densities. Thus, model selection by LRT tends to overestimate the number of clusters (Stahl and Sallis, 2012). There has been a lot of published research to formulate theoretical results on the null distribution of the LRT for finite mixture model through simulation and bootstrapping studies (see the review in McLachlan and Peel, 2000, Section 6.5). One of the most common way may be using randomization tests (McLachlan, 1987; Manly, 2007; Gotelli and Graves, 1996) to obtain the asymptotic null distribution. However, there is a lack of research on this topic focused on mixtures based on densities from ordinal variables and it might be a field to explore for future research.We set up a simulation study to empirically establish a relationship between our likelihood-based methodology for ordinal data and the performance of eleven information criteria in order to determine which was most reliable. We evaluate the following information criteria’s performances: AIC,AICc, BIC, ICL-BIC,AICu(McQuarrie et al., 1997), AIC3 (Bozdogan, 1994), CLC (Biernacki and Govaert, 1997), CAIC (Bozdogan, 1987), NEC (Biernacki et al., 1999), AWE (Banfield and Raftery, 1993) andL(Figueredo and Jain, 2002). Their definitions are given in Table 1.The results we are interested in are the percentage of simulated experiments where the eleven information criteria correctly determine the true number of row/column clusters in a set of diverse scenarios. The scenarios are determined by varying the sample size/subjects (n=50,100,500) and number of measures/questions (m=5,10). In addition, we made variations in the number of row clusters (R=2,3,4), column clusters (C=2,3,4) and the space between theq=4score parameters{ϕk}. The five scenarios for{ϕk}may be described by: equal spacing between any pair of adjacent score parameters (Scenario 1), one pair of adjacent score parameters are very close in value (Scenario 2), one of the mixing cluster proportions is close to zero (Scenario 3), one pair of adjacent score parameters have the same value (Scenario 4), and the same as the first scenario but increasing the number of measures tom=10(Scenario 5). All the parameters for each scenario in the row clustering and biclustering cases are shown in Tables D.10 and D.11 in Appendix D.For each scenario, we drew (h=100) data sets, and selected the best model for each data set using each information criterion. Therefore, we worked with a total of 4500 and 6000 data sets for row clustering and biclustering respectively. The EM algorithm to obtain the estimators is repeated 10 times with random starting points and the estimates with the highest likelihood are kept.Fig. 1is a histogram displaying the percentage of cases in which each information criterion determines the true number of row clusters across the five scenarios and the factors used in the experimental control. The best performance was AIC (correctly selecting the number of row clusters in 93.8% of cases), followed byAICc(89.8%) andAICu(82.4%). In the case of biclustering, the results are very similar as AIC also performs the best, although with a lower percentage of correctly selecting the number of row and column clusters than the row clustering case (86.1%).AICcandAICualso perform very well with percentages close to AIC: 85.6% and 84.2% respectively. BIC is underestimating the number of clusters (incorrectly selecting a smaller number of clusters in 56% and 63.2% of cases in row clustering and biclustering respectively). A very poor performance is obtained by ICL-BIC (correctly selecting the number of clusters in 33.1% and 31.3% of cases in row clustering and biclustering). Our results are in accordance with Fonseca and Cardoso (2007) for the categorical case.Tables 2 and 3show the best 5 information criteria performances in the case of row clustering and biclustering respectively. In both cases, we can observe that AIC is the best measure over the 5 scenarios and the ranking positions are exactly the same over the 5 scenarios. The best performance is scenario 5 which has the largest number of measuresm. On the other hand, the worse achievement is scenario 3 which has one of the mixing cluster proportion close to zero and, therefore, the percentage of underestimated number of clusters is higher. Regardless of this challenging scenario configuration, the AIC andAICcperformances are still quite satisfactory (over 75% of fitted cases in row and biclustering).Our conclusion is that AIC is the best information criteria when dealing with ordinal data and we fit likelihood-based finite mixture models with the ordinal stereotype model as the components in the mixture. It is important to remark these results are just evaluating the fact of obtaining the right number of clusters in the mixture, but it does not imply that they are the best clustering structure for the data.

@&#CONCLUSIONS@&#
