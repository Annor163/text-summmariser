@&#MAIN-TITLE@&#
Coverage-based resampling: Building robust consolidated decision trees

@&#HIGHLIGHTS@&#
Coverage-based resampling determines the number of samples to be used based on dataset’s class distribution.Consolidated trees achieve better results for most performance measures when using higher coverage values.CTC ranks first against multiple genetics-based and classical algorithms for rule induction.CTC combined with SMOTE tops state of the art techniques designed to tackle class imbalance.

@&#KEYPHRASES@&#
Comprehensibility,Consolidated decision trees,Class imbalance,Resampling,Inner ensembles,

@&#ABSTRACT@&#
The class imbalance problem has attracted a lot of attention from the data mining community recently, becoming a current trend in machine learning research. The Consolidated Tree Construction (CTC) algorithm was proposed as an algorithm to solve a classification problem involving a high degree of class imbalance without losing the explaining capacity, a desirable characteristic of single decision trees and rule sets. CTC works by resampling the training sample and building a tree from each subsample, in a similar manner to ensemble classifiers, but applying the ensemble process during the tree construction phase, resulting in a unique final tree. In the ECML/PKDD 2013 conference the term “Inner Ensembles” was coined to refer to such methodologies. In this paper we propose a resampling strategy for classification algorithms that use multiple subsamples. This strategy is based on the class distribution of the training sample to ensure a minimum representation of all classes when resampling. This strategy has been applied to CTC over different classification contexts. A robust classification algorithm should not just be able to rank in the top positions for certain classification problems but should be able to excel when faced with a broad range of problems. In this paper we establish the robustness of the CTC algorithm against a wide set of classification algorithms with explaining capacity.

@&#INTRODUCTION@&#
In data mining, a classification problem occurs when an object needs to be assigned to a predefined group or class based on a number of observed attributes related to that object [1].Class imbalance has been considered one of the main problems in data mining in recent years [2–6]. The class imbalance problem occurs when at least one of the classes (minority class/es) is underrepresented in the original training sample compared to the remaining classes. The imbalance can be either intrinsic (directly related to the nature of the data, such as the diagnosis of rare diseases) or extrinsic. Extrinsic imbalance can be caused by limitations in the data collection process [7]. Class imbalance is present in several real problems, such as medical diagnosis [8], insurance fraud detection [9], customer churn prevention [10], traffic incident detection [11] and DNA sequencing [12].Class imbalance has a detrimental effect on classification algorithms that maximize overall accuracy [3]. In the presence of class imbalance, such algorithms might build a trivial classifier that classifies all examples as majority class, obtaining a high overall accuracy but misclassifying all minority class examples (which is usually the class of interest). This is the case with the well known C4.5 decision tree algorithm [13] and its pruning mechanism. This mechanism iteratively deletes leaf nodes by looking for the deletion that maximizes accuracy gain until no deletion increases accuracy. In the presence of class imbalance, the deleted branches are usually responsible for correctly classifying the minority class examples [14]. Class imbalance can also amplify the effects of other classification problems such as concept complexity [15], high dimensionality combined with small sample size [3] or small disjuncts [16].The CTC (Consolidated Tree Construction) algorithm [17] was proposed for an insurance fraud detection problem where class imbalance was present [9]. CTC creates a set of subsamples from a training sample and builds a decision tree from each subsample in a similar manner to Bagging [18] but applying the ensemble process when building the tree by voting on the split on each of the tree’s nodes. Abbasian et al. [19] recently coined the term “Inner Ensembles” for similar procedures and suggested extending it to other algorithms, such as Bayesian networks and K-means. Unlike ensemble algorithms, the final model of the CTC algorithm is a simple decision tree understandable by humans. The mining of understandable patterns is a current trend in data mining, as highlighted in a recent special issue of a high-ranking journal in the field of artificial intelligence [6]. Consolidated trees are more stable and less complex than the C4.5 trees they are based on. Consolidated trees change far less when induced from different training samples and are thus more stable [20]. The complexity of the trees, represented as the amount of internal nodes, is smaller in consolidated trees. These features are important because, as Turney [21] and Domingos [22] pointed out separately “engineers are disturbed when different batches of data from the same process result in radically different decision trees. The engineers lose confidence in the decision trees, even when we can demonstrate that the trees have high predictive accuracy.” and “a single decision tree can easily be understood by a human as long as it is not too large”.In the work presented in this paper a novel resampling methodology is proposed and applied to the CTC algorithm. This methodology uses the notion of coverage, the minimum percentage of instances from any class of the training sample present in the subsample set with a different class distribution, to determine the amount of subsamples needed. Thus, instead of setting a fixed amount, the number of subsamples is determined by the data set’s class distribution, the subsample type and the chosen coverage value. The greater the class imbalance present in the training set, the more subsamples are necessary to achieve the same coverage. The results achieved by CTC using this new resampling strategy are compared to those published by Fernández et al. [23]. They proposed a taxonomy of sixteen rule-based evolutionary algorithms, dividing them into 3 main categories and 5 families. The discriminating ability of the algorithms was tested in three different contexts: a set of 30 standard (mostly multi-class) data sets, 33 two-class imbalanced data sets and the same two-class data sets preprocessed with SMOTE to balance the class distribution. All the data sets were taken from the KEEL repository.2http://sci2s.ugr.es/keel/datasets.php.2For each of the three contexts an intra-family comparison was performed and the best ranking algorithms of each family of the taxonomy were compared, along with a fixed set of six classical non-evolutionary classification algorithms. All twenty-two algorithms used in their work (whether rule-based or not) are explanatory, which makes them natural rivals to CTC. This makes that experiment an ideal environment to test CTC with the coverage-based resampling strategy.The main contribution of this paper is the use of the notion of coverage. Depending on the difficulty of the problem (defined by the class distribution in the data set) and the characteristics of the subsamples to be created, the coverage determines the adequate number of samples to build consolidated trees. In previous works, CTC has never been used with data sets with such high degree of class imbalance and such small size. Coverage-based resampling ensures that the number of samples does not fall short of representing all classes to a minimum degree, independently of the class distribution. Furthermore, we have generalized this strategy in the context of multi-class data sets, where class imbalance is also present but usually not studied. In the analysis performed in this work in three classification contexts, a coverage value of 99% has been determined to be the most adequate for the CTC algorithm. Also, although applying SMOTE had previously never improved CTC’s performance in a significant manner, the combination of coverage-based resampling with the use of SMOTE has been able to do so.In this work we want to establish CTC’s robustness by showing that it ranks in the top positions for different classification contexts compared against a wide range of algorithms, all with explaining capability. The significance of CTC’s performance compared to its competitors is backed up by performing rigorous statistical testing following the guidelines established in the field of machine learning research [24–26].The rest of the paper is organized as follows. Section 2 gives an overview of the related work in the fields of class imbalance, tree and rule induction algorithms and the CTC algorithm. Section 3 presents the coverage-based resampling and states the hypothesis of this work. Sections 4 and 5 respectively describe the experimental setup and the analysis of results. Finally, Section 6 gives this work’s conclusions and details future work.

@&#CONCLUSIONS@&#
This paper presents a new resampling strategy that is applicable to any algorithm that requires the use of multiple subsamples, such as bagging, boosting or CTC. Instead of using fixed numbers of subsamples without taking the data sets’ aspects into account, this methodology uses the subsample’s change in class distribution from the training sample and the subsample’s size and type (with or without replacement) to compute the number of subsamples to ensure that the combination of subsamples covers a certain percentage of the training sample. We refer to this notion as the coverage value.In this paper we also applied the coverage methodology to CTC, an algorithm that requires the use of several subsamples of the original training sample (as a lot of multiple classifier systems), but with a resulting single decision tree. We used two subsample types with a balanced class distribution. The subsample types differ in size with one subsample type being the same size as the minority class in the original training sample (sizeofMinClass) and the other one being the same amount multiplied by the number of classes (maxSize). The discriminating capacity of CTC with a wide range of coverage values was tested for three supervised classification contexts, these being standard data set classification, imbalanced two-class classification and imbalanced two-class classification preprocessed with SMOTE. We compared our results to those published by Fernández et al. [23] where they compared evolutionary and classical algorithms, all with explaining capacity.On the one hand, comparing the performance of CTC using different subsample sizes, statistically significant differences always favor the use of bigger, maxSize subsamples so for following comparisons this subsamples were used. We decided to use a coverage value of 99%, based on the average values of the performance measures.Comparing the performance of CTC with the original imbalanced and preprocessed data sets, there are statistically significant differences in favor of using SMOTE prior to CTC.In summary, CTC ranks first for imbalanced two-class classification and standard data sets if kappa is used as the performance measure, while it ranks 4th if accuracy is used as the measure and it ranks 3rd for imbalanced classification if SMOTE is used to balance the class distribution prior to generating the subsamples. When CTC does not rank first, no statistically significant differences are found between the best ranking algorithm and CTC. The Consolidated Tree Construction algorithm proves to be the most robust of the 22 algorithms compared (16 evolutionary and 6 classical) for the three classification contexts comprising 96 data sets. It is the only one of the algorithms to place in the top third for all classification problems and it also places first globally. Depending on the application context, a lower coverage could be used if computational cost were an issue. CTC with a coverage of 50%, using a lower proportion of examples from the training set, still ranks first against the evolutionary and classical algorithms.Additional experiments compare the performance of CTC in the context of imbalanced data set classification with several state of the art methods for tackling class imbalance. CTC ranks seventh behind several combinations of resampling and classifier algorithms. However, CTC with SMOTE resampling ranks first, with statistically significant differences with most of the competing algorithms.For future work, we propose extending the application of the coverage-based resampling to other algorithms that use multiple subsamples. We would also like to study the performance of those algorithms with coverage values closer to 100%. We are also interested in extending the research presented in this paper by using subsamples with a class distribution other than a fully balanced distribution, with the aim of improving the true negative rate and, thus, the geometric mean. We would also like to apply the consolidation methodology (also known as inner ensemble methodology) to other decision tree or rule induction algorithms. Finally, in view of the good results that some of the algorithms tackling class imbalance obtain using resampling strategies other than SMOTE, we would like to analyze the performance of CTC with some of these preprocessing methods; such as EUSCHC or SMOTE+ENN.